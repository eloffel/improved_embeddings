advanced data analysis

from an elementary point of view

cosma rohilla shalizi

for my parents

and in memory of my grandparents

3

contents

introduction

introduction
to the reader
concepts you should know

part i regression and its generalizations

regression basics
statistics, data analysis, regression

1
1.1
1.2 guessing the value of a random variable
1.3
1.4
1.5
1.6

the regression function
estimating the regression function
linear smoothers
further reading
exercises

the truth about id75

2
2.1 optimal linear prediction: multiple variables
2.2
2.3
2.4
2.5

shifting distributions, omitted variables, and transformations
adding probabilistic assumptions
id75 is not the philosopher   s stone
further reading
exercises

model evaluation

errors, in and out of sample

3
3.1 what are statistical models for?
3.2
3.3 over-fitting and model selection
3.4
3.5 warnings
3.6

cross-validation

further reading
exercises

4
4.1

smoothing in regression
how much should we smooth?

4

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

11

11
11
14

15

17
17
18
19
23
28
39
39

41
41
46
55
58
60
60

61
61
62
66
70
74
77
78

84
84

contents

adapting to unknown roughness

4.2
4.3 kernel regression with multiple inputs
4.4
4.5
4.6
4.7

interpreting smoothers: plots
average predictive comparisons
computational advice: npreg
further reading
exercises

how do we simulate stochastic models?
repeating simulations

simulation

5
5.1 what is a simulation?
5.2
5.3
5.4 why simulate?
5.5

further reading
exercises

the bootstrap
stochastic models, uncertainty, sampling distributions
the bootstrap principle
resampling
id64 regression models
bootstrap with dependent data
con   dence bands for nonparametric regression
things id64 does poorly

6
6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8 which bootstrap when?
6.9

further reading
exercises

7
7.1
7.2
7.3
7.4
7.5
7.6
7.7

8
8.1
8.2
8.3
8.4
8.5
8.6
8.7

splines
smoothing by penalizing curve flexibility
computational example: splines for stock returns
basis functions and degrees of freedom
splines in multiple dimensions
smoothing splines versus kernel regression
some of the math behind splines
further reading
exercises

additive models
additive models
partial residuals and back-   tting
the curse of dimensionality
example: california house prices revisited
interaction terms and expansions
closing modeling advice
further reading
exercises

5

85
92
94
94
96
99
100

113
113
114
118
119
125
125

126
126
128
139
141
146
147
147
148
149
150

152
152
154
160
162
163
163
165
166

168
168
169
172
174
178
180
181
181

6

contents

testing regression speci   cations
testing functional forms

9
9.1
9.2 why use parametric models at all?
9.3

further reading

10 weighting and variance
10.1 weighted least squares
10.2 heteroskedasticity
10.3 estimating conditional variance functions
10.4 re-sampling residuals with heteroskedasticity
10.5 local id75
10.6 further reading

exercises

id28

11
11.1 modeling conditional probabilities
11.2 id28
11.3 numerical optimization of the likelihood
11.4 generalized linear and additive models
11.5 model checking
11.6 a toy example
11.7 weather forecasting in snoqualmie falls
11.8 id28 with more than two classes

exercises

12 glms and gams
12.1 generalized linear models and iterative least squares
12.2 generalized additive models
12.3 further reading

exercises

trees

13
13.1 prediction trees
13.2 regression trees
13.3 classi   cation trees
13.4 further reading

exercises

part ii distributions and latent structure

   the fundamental theorem of statistics   

14 density estimation
14.1 histograms revisited
14.2
14.3 error for density estimates
14.4 kernel density estimates
14.5 conditional density estimation
14.6 more on the expected log-likelihood ratio

191
191
201
205

206
206
208
217
225
225
230
231

232
232
233
237
239
240
242
245
257
258

260
260
266
266
266

267
267
270
279
285
285

291

293
293
294
295
298
304
305

contents

14.7 simulating from density estimates
14.8 further reading

exercises

15 relative distributions and smooth tests
15.1 smooth tests of goodness of fit
15.2 relative distributions
15.3 further reading

exercises

principal components analysis

16
16.1 mathematics of principal components
16.2 example 1: cars
16.3 example 2: the united states circa 1977
16.4 latent semantic analysis
16.5 pca for visualization
16.6 pca cautions
16.7 random projections
16.8 further reading

exercises

factor models

17
17.1 from pca to factor analysis
17.2 the graphical model
17.3 roots of factor analysis in causal discovery
17.4 estimation
17.5 id113
17.6 the rotation problem
17.7 factor analysis as a predictive model
17.8 factor models versus pca once more
17.9 examples in r
17.10 rei   cation, and alternatives to factor models
17.11 further reading

exercises

18 nonlinear id84
18.1 why we need nonlinear id84
18.2 local linearity and manifolds
18.3 locally linear embedding (lle)
18.4 more fun with eigenvalues and eigenvectors
18.5 calculation
18.6 example
18.7 further reading

exercises

19 mixture models
19.1 two routes to mixture models
19.2 estimating parametric mixture models

7

308
313
315

317
317
330
341
342

343
343
350
354
357
360
363
363
365
366

369
369
371
374
376
381
382
383
386
387
391
398
398

400
400
402
406
410
413
421
421
423

424
424
428

8

contents

19.3 non-parametric mixture modeling
19.4 worked computating example
19.5 further reading

exercises

20 id114
20.1 conditional independence and factor models
20.2 directed acyclic graph (dag) models
20.3 conditional independence and d-separation
20.4
20.5 examples of dag models and their uses
20.6 non-dag id114
20.7 further reading

independence and information

exercises

part iii causal id136

21 graphical causal models
21.1 causation and counterfactuals
21.2 causal id114
21.3 conditional independence and d-separation revisited
21.4 further reading

exercises

identifying causal e   ects

22
22.1 causal e   ects, interventions and experiments
22.2
22.3
22.4 summary
exercises

identi   cation and confounding
identi   cation strategies

estimating causal e   ects

instrumental-variables estimates

23
23.1 estimators in the back- and front- door criteria
23.2
23.3 uncertainty and id136
23.4 recommendations
23.5 further reading

exercises

24 discovering causal structure
24.1 testing dags
24.2 testing conditional independence
24.3 faithfulness and equivalence
24.4 causal discovery with known variables
24.5 software and examples
24.6 limitations on consistency of causal discovery
24.7 pseudo-code for the sgs algorithm

433
433
452
452

454
454
455
457
464
466
468
472
473

475

477
477
478
481
482
484

485
485
487
489
504
505

507
507
515
516
516
517
518

519
520
521
522
523
528
534
534

contents

24.8 further reading

exercises

part iv dependent data

time series

25
25.1 what time series are
25.2 stationarity
25.3 markov models
25.4 autoregressive models
25.5 id64 time series
25.6 cross-validation
25.7 trends and de-trending
25.8 breaks in time series
25.9 time series with latent variables
25.10 longitudinal data
25.11 multivariate time series
25.12 further reading

exercises

simulation-based id136

26
26.1 the method of simulated moments
26.2
26.3 further reading

indirect id136

exercises

appendices

appendix a data-analysis problem sets

bibliography

references
acknowledgments

part v online appendices

appendix b id202 reminders

appendix c big o and little o notation

appendix d taylor expansions

appendix e multivariate distributions

9

535
536

537

539
539
540
545
549
554
556
556
560
562
570
570
570
572

596
596
603
603
604

607

607

684
685
706

709

711

723

725

728

10

contents

appendix f algebra with expectations and variances

appendix g propagation of error

appendix h optimization

appendix i

  2 and likelihood ratios

appendix j

rudimentary id207

appendix k missing data

appendix l

programming

appendix m generating random variables

739

741

743

774

776

779

807

843

introduction

to the reader

this book began as the notes for 36-402, advanced data analysis, at carnegie
mellon university. this is the methodological capstone of the core statistics se-
quence taken by our undergraduate majors (usually in their third year), and by
undergraduate and graduate students from a range of other departments. the
pre-requisite for that course is our class in modern id75, which in
turn requires students to have taken classes in introductory statistics and data
analysis, id203 theory, mathematical statistics, id202, and multi-
variable calculus. this book does not presume that you once learned but have
forgotten that material; it presumes that you know those subjects and are ready
to go further (see p. 14, at the end of this introduction). the book also presumes
that you can read and write simple functions in r. if you are lacking in any of
these areas, this book is not really for you, at least not now.

ada is a class in statistical methodology: its aim is to get students to under-
stand something of the range of modern1 methods of data analysis, and of the
considerations which go into choosing the right method for the job at hand (rather
than distorting the problem to    t the methods you happen to know). statistical
theory is kept to a minimum, and largely introduced as needed. since ada is
also a class in data analysis, there are a lot of assignments in which large, real
data sets are analyzed with the new methods.

there is no way to cover every important topic for data analysis in just a
semester. much of what   s not here     sampling theory and survey methods, ex-
perimental design, advanced multivariate methods, id187, the in-
tricacies of categorical data, graphics, data mining, spatial and spatio-temporal
statistics     gets covered by our other undergraduate classes. other important
areas, like networks, inverse problems, advanced model selection or robust esti-
mation, have to wait for graduate school2.

the mathematical level of these notes is deliberately low; nothing should be
beyond a competent third-year undergraduate. but every subject covered here
can be pro   tably studied using vastly more sophisticated techniques; that   s why

1 just as an undergraduate    modern physics    course aims to bring the student up to about 1930

(more speci   cally, to 1926), this class aims to bring the student up to about 1990   1995, maybe 2000.
2 early drafts of this book, circulated online, included sketches of chapters covering spatial statistics,

networks, and experiments. these were all sacri   ced to length, and to actually    nishing.

11

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

12

introduction

this is advanced data analysis from an elementary point of view. if reading these
pages inspires anyone to study the same material from an advanced point of view,
i will consider my troubles to have been amply repaid.

a    nal word. at this stage in your statistical education, you have gained two
kinds of knowledge     a few general statistical principles, and many more speci   c
procedures, tests, recipes, etc. typical students are much more comfortable with
the speci   cs than the generalities. but the truth is that while none of your recipes
are wrong, they are tied to assumptions which hardly ever hold3. learning more
   exible and powerful methods, which have a much better hope of being reliable,
will demand a lot of hard thinking and hard work. those of you who succeed,
however, will have done something you can be proud of.

organization of the book

part i is about regression and its generalizations. the focus is on nonparametric
regression, especially smoothing methods. (chapter 2 motivates this by dispelling
some myths and misconceptions about id75.) the ideas of cross-
validation, of simulation, and of the bootstrap all arise naturally in trying to come
to grips with regression. this part also covers classi   cation and speci   cation-
testing.

part ii is about learning distributions, especially multivariate distributions,
rather than doing regression. it is possible to learn essentially arbitrary distri-
butions from data, including conditional distributions, but the number of ob-
servations needed is often prohibitive when the data is high-dimensional. this
motivates looking for models of special, simple structure lurking behind the high-
dimensional chaos, including various forms of linear and non-linear dimension
reduction, and mixture or cluster models. all this builds towards the general
idea of using id114 to represent dependencies between variables.

part iii is about causal id136. this is done entirely within the graphical-
model formalism, which makes it easy to understand the di   erence between causal
prediction and the more ordinary    actuarial    prediction we are used to as statis-
ticians. it also greatly simpli   es    guring out when causal e   ects are, or are not,
identi   able from our data. (among other things, this gives us a sound way to
decide what we ought to control for.) actual estimation of causal e   ects is done
as far as possible non-parametrically. this part ends by considering procedures
for discovering causal structure from observational data.

part iv moves away from independent observations, more or less tacitly as-

3    econometric theory is like an exquisitely balanced french recipe, spelling out precisely with how
many turns to mix the sauce, how many carats of spice to add, and for how many milliseconds to
bake the mixture at exactly 474 degrees of temperature. but when the statistical cook turns to raw
materials, he    nds that hearts of cactus fruit are unavailable, so he substitutes chunks of
cantaloupe; where the recipe calls for vermicelli he uses shredded wheat; and he substitutes green
garment dye for curry, ping-pong balls for turtle   s eggs and, for chalifougnac vintage 1883, a can of
turpentine.        stefan valavanis, quoted in roger koenker,    dictionary of received ideas of
statistics    (http://www.econ.uiuc.edu/~roger/dict.html), s.v.    econometrics   .

introduction

13

sumed earlier, to dependent data. it speci   cally considers models of time se-
ries, and time series data analysis, and simulation-based id136 for complex or
analytically-intractable models.

parts iii and iv are mostly independent of each other, but both rely on parts

i and ii.

the appendices contain data-analysis problem sets; mathematical reminders;
statistical-theory reminders; some notes on optimization, id205, and
missing data; and advice on writing r code for data analysis.

r examples

the book is full of worked computational examples in r. in most cases, the
code used to make    gures, tables, etc., is given in full in the text. (the code is
deliberately omitted for a few examples for pedagogical reasons.) to save space,
comments are generally omitted from the text, but comments are vital to good
programming (  l.9.1), so fully-commented versions of the code for each chapter
are available from the book   s website.

exercises and problem sets

there are two kinds of assignments included here. mathematical and computa-
tional exercises go at the end of chapters, since they are mostly connected to
those pieces of content. (many of them are complements to, or    ll in details of,
material in the chapters.) there are also data-centric problem sets, in appendix
a; most of these draw on material from multiple chapters, and many of them are
based on speci   c papers.

solutions will be available to teachers from the publisher; giving them out to

those using the book for self-study is, sadly, not feasible.

to teachers

the usual one-semester course for this class has contained chapters 1, 2, 3, 4, 5,
6, 7, 8, 9, 10, 11, 12, 16, 17, 19, 20, 21, 22, 23, 24 and 25, and appendices e and
l (the latter quite early on). other chapters have rotated in and out from year
to year. one of the problem sets from appendix a (or a similar one) was due
every week, either as homework or as a take-home exam.

corrections and updates

the page for this book is http://www.stat.cmu.edu/~cshalizi/adafaepov/.
the latest version will live there. the book will eventually be published by cam-
bridge university press, at which point there will still be a free next-to-   nal draft
at that url, and errata. while the book is still in a draft, the pdf contains
notes to myself for revisions, [[like so]]; you can ignore them.

[[also
marginal
notes-to-
self]]

14

introduction

concepts you should know

if more than a few of these are unfamiliar, it   s unlikely you   re ready for this book.
id202: vectors; arithmetic with vectors; inner or dot product of
vectors, orthogonality; linear independence; basis vectors. linear subspaces. ma-
trices, matrix arithmetic, multiplying vectors and matrices; geometric meaning
of id127. eigenvalues and eigenvectors of matrices. projection.

calculus: derivative, integral; fundamental theorem of calculus. multivari-
able extensions: gradient, hessian matrix, multidimensional integrals. finding
minima and maxima with derivatives. taylor approximations (app. d).

id203: random variable; distribution, population, sample. cumula-
tive distribution function, id203 mass function, id203 density func-
tion. speci   c distributions: bernoulli, binomial, poisson, geometric, gaussian,
exponential, t, gamma. expectation value. variance, standard deviation.

joint distribution functions. conditional distributions; conditional expecta-
tions and variances. statistical independence and dependence. covariance and
correlation; why dependence is not the same thing as correlation. rules for arith-
metic with expectations, variances and covariances. laws of total id203,
total expectation, total variation. sequences of random variables. stochastic pro-
cess. law of large numbers. central limit theorem.

statistics: sample mean, sample variance. median, mode. quartile, per-
centile, quantile. inter-quartile range. histograms. contingency tables; odds ratio,
log odds ratio.

parameters; estimator functions and point estimates. sampling distribution.
bias of an estimator. standard error of an estimate; standard error of the mean;
how and why the standard error of the mean di   ers from the standard deviation.
consistency of estimators. con   dence intervals and interval estimates.

hypothesis tests. tests for di   erences in means and in proportions; z and t
tests; degrees of freedom. size, signi   cance, power. relation between hypothesis
tests and con   dence intervals.   2 test of independence for contingency tables;
degrees of freedom. ks test for goodness-of-   t to distributions.

likelihood. likelihood functions. maximum likelihood estimates. relation be-

tween con   dence intervals and the likelihood function. likelihood ratio test.

regression: what a linear model is; distinction between the regressors and
the regressand. predictions/   tted values and residuals of a regression. interpre-
tation of regression coe   cients. least-squares estimate of coe   cients. relation
between maximum likelihood, least squares, and gaussian distributions. matrix
formula for estimating the coe   cients; the hat matrix for    nding    tted values.
r2; why adding more predictor variables never reduces r2. the t-test for the sig-
ni   cance of individual coe   cients given other coe   cients. the f -test and partial
f -test for the signi   cance of groups of coe   cients. degrees of freedom for resid-
uals. diagnostic examination of residuals. con   dence intervals for parameters.
con   dence intervals for    tted values. prediction intervals. (most of this material
is reviewed at http://www.stat.cmu.edu/~cshalizi/talr/.)

part i

regression and its generalizations

15

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

1

regression: predicting and relating

quantitative features

1.1 statistics, data analysis, regression

statistics is the branch of mathematical engineering which designs and analyses
methods for drawing reliable id136s from imperfect data.

the subject of most sciences is some aspect of the world around us, or within
us. psychology studies minds; geology studies the earth   s composition and form;
economics studies production, distribution and exchange; mycology studies mush-
rooms. statistics does not study the world, but some of the ways we try to under-
stand the world     some of the intellectual tools of the other sciences. its utility
comes indirectly, through helping those other sciences.

this utility is very great, because all the sciences have to deal with imperfect
data. data may be imperfect because we can only observe and record a small
fraction of what is relevant; or because we can only observe indirect signs of what
is truly relevant; or because, no matter how carefully we try, our data always
contain an element of noise. over the last two centuries, statistics has come
to handle all such imperfections by modeling them as random processes, and
id203 has become so central to statistics that we introduce random events
deliberately (as in sample surveys).1

statistics, then, uses id203 to model id136 from data. we try to mathe-
matically understand the properties of di   erent procedures for drawing id136s:
under what conditions are they reliable? what sorts of errors do they make, and
how often? what can they tell us when they work? what are signs that some-
thing has gone wrong? like other branches of engineering, statistics aims not
just at understanding but also at improvement: we want to analyze data better:
more reliably, with fewer and smaller errors, under broader conditions, faster,
and with less mental e   ort. sometimes some of these goals con   ict     a fast,
simple method might be very error-prone, or only reliable under a narrow range
of circumstances.

one of the things that people most often want to know about the world is how
di   erent variables are related to each other, and one of the central tools statistics
has for learning about relationships is regression.2 in your id75 class,

1 two excellent, but very di   erent, histories of how statistics came to this understanding are hacking

(1990) and porter (1986).

2 the origin of the name is instructive (stigler, 1986). it comes from 19th century investigations into
the relationship between the attributes of parents and their children. people who are taller (heavier,
faster, . . . ) than average tend to have children who are also taller than average, but not quite as tall.

17

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

18

regression basics

you learned about how it could be used in data analysis, and learned about its
properties. in this book, we will build on that foundation, extending beyond
basic id75 in many directions, to answer many questions about how
variables are related to each other.

this is intimately related to prediction. being able to make predictions isn   t the
only reason we want to understand relations between variables     we also want to
answer    what if?    questions     but prediction tests our knowledge of relations.
(if we misunderstand, we might still be able to predict, but it   s hard to see how
we could understand and not be able to predict.) so before we go beyond linear
regression, we will    rst look at prediction, and how to predict one variable from
nothing at all. then we will look at predictive relationships between variables,
and see how id75 is just one member of a big family of smoothing
methods, all of which are available to us.

1.2 guessing the value of a random variable

we have a quantitative, numerical variable, which we   ll imaginatively call y .
we   ll suppose that it   s a random variable, and try to predict it by guessing a
single value for it. (other kinds of predictions are possible     we might guess
whether y will fall within certain limits, or the id203 that it does so, or
even the whole id203 distribution of y . but some lessons we   ll learn here
will apply to these other kinds of predictions as well.) what is the best value to
guess? more formally, what is the optimal point forecast for y ?

to answer this question, we need to pick a function to be optimized, which
should measure how good our guesses are     or equivalently how bad they are,
i.e., how big an error we   re making. a reasonable, traditional starting point is
the mean squared error:

so we   d like to    nd the value    where mse(m) is smallest. start by re-writing
the mse as a (squared) bias plus a variance:

(y     m)2(cid:105)

mse(m)     e(cid:104)
(y     m)2(cid:105)

mse(m) = e(cid:104)

= (e [y     m])2 + v [y     m]
= (e [y     m])2 + v [y ]
= (e [y ]     m)2 + v [y ]

(1.1)

(1.2)

(1.3)
(1.4)
(1.5)

notice that only the    rst, bias-squared term depends on our prediction m. we
want to    nd the derivative of the mse with respect to our prediction m, and

likewise, the children of unusually short parents also tend to be closer to the average, and similarly
for other traits. this came to be called    regression towards the mean,    or even    regression towards
mediocrity   ; hence the line relating the average height (or whatever) of children to that of their
parents was    the regression line,    and the word stuck.

1.3 the regression function

19

then set that to zero at the optimal prediction
t rueregf unc:

=    2 (e [y ]     m) + 0

dmse

dm

(cid:12)(cid:12)(cid:12)(cid:12)m=  

dmse

dm

= 0
2(e [y ]       ) = 0

   = e [y ]

(1.6)

(1.7)

(1.8)
(1.9)

so, if we gauge the quality of our prediction by mean-squared error, the best
prediction to make is the expected value.

1.2.1 estimating the expected value

of course, to make the prediction e [y ] we would have to know the expected value
of y . typically, we do not. however, if we have sampled values, y1, y2, . . . yn, we
can estimate the expectation from the sample mean:

yi

(1.10)

if the samples are independent and identically distributed (iid), then the law of
large numbers tells us that

(1.11)

and algebra with variances (exercise 1.1) tells us something about how fast the
convergence is, namely that the squared error will typically be v [y ] /n.

of course the assumption that the yi come from iid samples is a strong one,
but we can assert pretty much the same thing if they   re just uncorrelated with a
common expected value. even if they are correlated, but the correlations decay
fast enough, all that changes is the rate of convergence (  25.2.2.1). so    sit, wait,
and average    is a pretty reliable way of estimating the expectation value.

1.3 the regression function

of course, it   s not very useful to predict just one number for a variable. typically,
we have lots of variables in our data, and we believe they are related somehow.
for example, suppose that we have data on two variables, x and y , which might
look like figure 1.1.3 the feature y is what we are trying to predict, a.k.a.
the dependent variable or output or response or regressand, and x is
the predictor or independent variable or covariate or input or regressor.
y might be something like the pro   tability of a customer and x their credit
rating, or, if you want a less mercenary example, y could be some measure of

3 problem set a.30 features data that looks rather like these made-up values.

(cid:98)       1

n

n(cid:88)

i=1

(cid:98)       e [y ] =   

20

regression basics

improvement in blood cholesterol and x the dose taken of a drug. typically we
won   t have just one input feature x but rather many of them, but that gets
harder to draw and doesn   t change the points of principle.

figure 1.2 shows the same data as figure 1.1, only with the sample mean
added on. this clearly tells us something about the data, but also it seems like
we should be able to do better     to reduce the average error     by using x,
rather than by ignoring it.

let   s say that the we want our prediction to be a function of x, namely f (x).
what should that function be, if we still use mean squared error? we can work
this out by using the law of total expectation, i.e., the fact that e [u ] = e [e [u|v ]]
for any random variables u and v .

mse(f ) = e(cid:104)

(y     f (x))2(cid:105)

= e(cid:2)e(cid:2)(y     f (x))2|x(cid:3)(cid:3)
= e(cid:104)v [y     f (x)|x] + (e [y     f (x)|x])2(cid:105)
= e(cid:104)v [y |x] + (e [y     f (x)|x])2(cid:105)

(1.12)

(1.13)

(1.14)

(1.15)

when we want to minimize this, the    rst term inside the expectation doesn   t
depend on our prediction, and the second term looks just like our previous opti-
mization only with all expectations conditional on x, so for our optimal function
  (x) we get

  (x) = e [y |x = x]

(1.16)

in other words, the (mean-squared) optimal conditional prediction is just the
conditional expected value. the function   (x) is called the regression function.
this is what we would like to know when we want to predict y .

some disclaimers

it   s important to be clear on what is and is not being assumed here. talking
about x as the    independent variable    and y as the    dependent    one suggests
a causal model, which we might write

y       (x) +  

(1.17)
where the direction of the arrow,    , indicates the    ow from causes to e   ects, and
  is some noise variable. if the gods of id136 are very kind, then   would have a
   xed distribution, independent of x, and we could without loss of generality take
it to have mean zero. (   without loss of generality    because if it has a non-zero
mean, we can incorporate that into   (x) as an additive constant.) however, no
such assumption is required to get eq. 1.16. it works when predicting e   ects from
causes, or the other way around when predicting (or    retrodicting   ) causes from
e   ects, or indeed when there is no causal relationship whatsoever between x and

1.3 the regression function

21

plot(all.x, all.y, xlab = "x", ylab = "y")
rug(all.x, side = 1, col = "grey")
rug(all.y, side = 2, col = "grey")

figure 1.1 scatterplot of the (made up) running example data. rug() adds
horizontal and vertical ticks to the axes to mark the location of the data;
this isn   t necessary but is often helpful. the data are in the
basics-examples.rda    le.

y 4. it is always true that

y |x =   (x) +  (x)

(1.18)

4 we will cover causal id136 in detail in part iii.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0xy22

regression basics

plot(all.x, all.y, xlab = "x", ylab = "y")
rug(all.x, side = 1, col = "grey")
rug(all.y, side = 2, col = "grey")
abline(h = mean(all.y), lty = "dotted")

figure 1.2 data from figure 1.1, with a horizontal line at y.

where  (x) is a random variable with expected value 0, e [ |x = x] = 0, but as
the notation indicates the distribution of this variable generally depends on x.

it   s also important to be clear that if we    nd the regression function is a con-
stant,   (x) =   0 for all x, that this does not mean that x and y are statistically

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0xyyi

(1.19)

1.4 estimating the regression function

23

independent. if they are independent, then the regression function is a constant,
but turning this around is the logical fallacy of    a   rming the consequent   5.

1.4 estimating the regression function

we want the regression function   (x) = e [y |x = x], but what we have is a pile
of training examples, of pairs (x1, y1), (x2, y2), . . . (xn, yn). what should we do?

if x takes on only a    nite set of values, then a simple strategy is to use the

conditional sample means:(cid:98)  (x) =
(cid:98)  (x)     e [y |x = x].

(cid:88)

i:xi=x

1

#{i : xi = x}

reasoning with the law of large numbers as before, we can be con   dent that

unfortunately, this only works when x takes values in a    nite set. if x is
continuous, then in general the id203 of our getting a sample at any par-
ticular value is zero, as is the id203 of getting multiple samples at exactly
the same value of x. this is a basic issue with estimating any kind of function
from data     the function will always be undersampled, and we need to    ll
in between the values we see. we also need to somehow take into account the
fact that each yi is a sample from the conditional distribution of y |x = xi, and
generally not equal to e [y |x = xi]. so any kind of function estimation is going
to involve interpolation, extrapolation, and de-noising or smoothing.

di   erent methods of estimating the regression function     di   erent regression
methods, for short     involve di   erent choices about how we interpolate, extrapo-
late and smooth. these are choices about how to approximate   (x) with a limited
class of functions which we know (or at least hope) we can estimate. there is no
guarantee that our choice leads to a good approximation in the case at hand,
though it is sometimes possible to say that the approximation error will shrink as
we get more and more data. this is an extremely important topic and deserves
an extended discussion, coming next.

1.4.1 the bias-variance tradeo   

suppose that the true regression function is   (x), but we use the function (cid:98)   to
use    to make predictions. we   ll begin by expanding (y    (cid:98)  (x))2, since the mse

make our predictions. let   s look at the mean squared error at x = x in a slightly
di   erent way than before, which will make it clearer what happens when we can   t

at x is just the expectation of this.

(y    (cid:98)  (x))2
= (y       (x) +   (x)    (cid:98)  (x))2
= (y       (x))2 + 2(y       (x))(  (x)    (cid:98)  (x)) + (  (x)    (cid:98)  (x))2

(1.20)

(1.21)

5 as in combining the fact that all human beings are featherless bipeds, and the observation that a

cooked turkey is a featherless biped, to conclude that cooked turkeys are human beings.

regression basics

24
eq. 1.18 tells us that y       (x) =  , a random variable which has expectation
zero (and is uncorrelated with x). taking the expectation of eq. 1.21, nothing
happens to the last term (since it doesn   t involve any random quantities); the
middle term goes to zero (because e [y       (x)] = e [ ] = 0), and the    rst term
becomes the variance of  , call it   2(x):

mse((cid:98)  (x)) =   2(x) + (  (x)    (cid:98)  (x))2

(1.22)

thing we estimate from earlier data. but if those data are random, the regression

subscript reminds us of the    nite amount of data we used to estimate it. what

the   2(x) term doesn   t depend on our prediction function, just on how hard it is,
intrinsically, to predict y at x = x. the second term, though, is the extra error
we get from not knowing   . (unsurprisingly, ignorance of    cannot improve our
predictions.) this is our    rst bias-variance decomposition: the total mse

tional on a particular estimated regression function. what can we say about the
prediction error of the method, averaging over all the possible training data sets?

our predictions are systematically o   , and a variance   2(x), the unpredictable,
   statistical       uctuation around even the best prediction.

at x is decomposed into a (squared) bias   (x)     (cid:98)  (x), the amount by which
all this presumes that (cid:98)   is a single    xed function. really, of course, (cid:98)   is some-
function we get is random too; let   s call this random function (cid:99)mn, where the
we have analyzed is really mse((cid:99)mn(x)|(cid:99)mn = (cid:98)  ), the mean squared error condi-
mse((cid:99)mn(x)) = e(cid:104)
(y     (cid:99)mn(x))2|x = x
= e(cid:104)e(cid:104)
(cid:105)|x = x
(y     (cid:99)mn(x))2|x = x,(cid:99)mn =(cid:98)  
= e(cid:104)
(cid:105)
  2(x) + (  (x)     (cid:99)mn(x))2|x = x
=   2(x) + e(cid:104)
(cid:105)
(  (x)     (cid:99)mn(x))2|x = x
+ e(cid:104)(cid:99)mn(x)
(  (x)     e(cid:104)(cid:99)mn(x)
=   2(x) + e(cid:104)
(cid:105)     (cid:99)mn(x))2(cid:105)
(cid:105)
(cid:105)
+ v(cid:104)(cid:99)mn(x)
(cid:105)(cid:17)2
(cid:16)
  (x)     e(cid:104)(cid:99)mn(x)

=   2(x) +

(1.23)

(1.24)

(1.25)

(1.26)

(1.27)

(1.28)

(cid:105)

(cid:105)

this is our second bias-variance decomposition     i pulled the same trick as
before, adding and subtracting a mean inside the square. the    rst term is just
the variance of the process; we   ve seen that before and it isn   t, for the moment,

approximation bias or approximation error. the third term, though, is the
variance in our estimate of the regression function. even if we have an unbiased
), if there is a lot of variance in our estimates, we can

of any concern. the second term is the bias in using (cid:99)mn to estimate        the
method (  (x) = e(cid:104)(cid:99)mn(x)
(cid:105)
ple, if e(cid:104)(cid:99)mn(x)

the approximation bias depends on the true regression function. for exam-
= 42 + 37x, the error of approximation will be zero at all x if
  (x) = 42+37x, but it will be larger and x-dependent if   (x) = 0. however, there
are    exible methods of estimation which will have small approximation biases for

expect to make large errors.

(cid:105)

1.4 estimating the regression function

25

all    in a broad range of regression functions. the catch is that, at least past
a certain point, decreasing the approximation bias can only come through in-
creasing the estimation variance. this is the bias-variance trade-o   . however,
nothing says that the trade-o    has to be one-for-one. sometimes we can lower
the total error by introducing some bias, since it gets rid of more variance than
it adds approximation error. the next section gives an example.
in general, both the approximation bias and the estimation variance depend
on n. a method is consistent6 when both of these go to zero as n            
that is, if we recover the true regression function as we get more and more data.7
again, consistency depends not just on the method, but also on how well the
method matches the data-generating process, and, again, there is a bias-variance
trade-o   . there can be multiple consistent methods for the same problem, and
their biases and variances don   t have to go to zero at the same rates.

1.4.2 the bias-variance trade-o    in action

let   s take an extreme example: we could decide to approximate   (x) by a con-
stant   0. the implicit smoothing here is very strong, but sometimes appropriate.
for instance, it   s appropriate when   (x) really is a constant! then trying to es-
timate any additional structure in the regression function is just wasted e   ort.
alternately, if   (x) is nearly constant, we may still be better o    approximating
it as one. for instance, suppose the true   (x) =   0 + a sin (  x), where a (cid:28) 1 and
   (cid:29) 1 (figure 1.3 shows an example). with limited data, we can actually get
better predictions by estimating a constant regression function than one with the
correct functional form.

1.4.3 ordinary least squares id75 as smoothing

let   s revisit ordinary least-squares id75 from this point of view. we   ll
assume that the predictor variable x is one-dimensional, just to simplify the
book-keeping.

we choose to approximate   (x) by b0 +b1x, and ask for the best values   0,   1 of

6 to be precise, consistent for   , or consistent for conditional expectations. more generally, an

7 you might worry about this claim, especially if you   ve taken more id203 theory     aren   t we

estimator of any property of the data, or of the whole distribution, is consistent if it converges on
the truth.

just saying something about average performance of the (cid:99)mn, rather than any particular estimated
inequality, pr (|x     e [x] |     a)     v [x] /a2, each (cid:99)mn(x) comes arbitrarily close to e(cid:104)(cid:99)mn(x)
(cid:105)

regression function? but notice that if the estimation variance goes to zero, then by chebyshev   s

with

arbitrarily high id203. if the approximation bias goes to zero, therefore, the estimated
regression functions converge in id203 on the true regression function, not just in mean.

26

regression basics

ugly.func <- function(x) {

1 + 0.01 * sin(100 * x)

}
x <- runif(20)
y <- ugly.func(x) + rnorm(length(x), 0, 0.5)
plot(x, y, xlab = "x", ylab = "y")
curve(ugly.func, add = true)
abline(h = mean(y), col = "red", lty = "dashed")
sine.fit = lm(y ~ 1 + sin(100 * x))
curve(sine.fit$coefficients[1] + sine.fit$coefficients[2] * sin(100 * x), col = "blue",

add = true, lty = "dotted")

legend("topright", legend = c(expression(1 + 0.1 * sin(100 * x)), expression(bar(y)),

expression(hat(a) + hat(b) * sin(100 * x))), lty = c("solid", "dashed",
"dotted"), col = c("black", "red", "blue"))

figure 1.3 when we try to estimate a rapidly-varying but small-amplitude
regression function (solid black line,    = 1 + 0.01 sin 100x +  , with
mean-zero gaussian noise of standard deviation 0.5), we can do better to use
a constant function (red dashed line at the sample mean) than to estimate a
more complicated model of the correct functional form   a +   b sin 100x (dotted
blue line). with just 20 observations, the mean predicts slightly better on
new data (square-root mse, rmse, of 0.52) than does the estimate sine
function (rmse of 0.55). the bias of using the wrong functional form is less
than the extra variance of estimation, so using the true model form hurts us.

those constants. these will be the ones which minimize the mean-squared error.

mse(a, b) = e(cid:104)
(y     b0     b1x)2(cid:105)
= e(cid:104)e(cid:104)
= e(cid:104)v [y |x] + (e [y     b0     b1x|x])2(cid:105)
(e [y     b0     b1x|x])2(cid:105)
= e [v [y |x]] + e(cid:104)

(y     b0     b1x)2|x

(cid:105)(cid:105)

(1.29)

(1.30)

(1.31)

(1.32)

llllllllllllllllllll0.00.20.40.60.80.51.01.52.02.5xy1+0.1sin(100x)ya^+b^sin(100x)1.4 estimating the regression function

27

the    rst term doesn   t depend on b0 or b1, so we can drop it for purposes of
optimization. taking derivatives, and then bringing them inside the expectations,

   mse

   b0

= e [2(y     b0     b1x)(   1)]
0 = e [y       0       1x]
  0 = e [y ]       1e [x]

so we need to get   1:

   mse

   b1

= e [2(y     b0     b1x)(   x)]

0 = e [xy ]       1e(cid:2)x 2(cid:3) + (e [y ]       1e [x])e [x]
= e [xy ]     e [x] e [y ]       1(e(cid:2)x 2(cid:3)     e [x]2)

  1 =

cov [x, y ]

v [x]

(1.33)

(1.34)
(1.35)

(1.36)

(1.37)

(1.38)

(1.39)

using our equation for   0. that is, the mean-squared optimal linear prediction is

  (x) = e [y ] +

cov [x, y ]

v [x]

(x     e [x])

(1.40)

now, if we try to estimate this from data, there are (at least) two approaches.
one is to replace the true, population values of the covariance and the variance
with their sample values, respectively

1
n

i

(cid:88)
(yi     y)(xi     x)
(cid:88)
(xi     x)2    (cid:98)v [x] .
(cid:88)

i

i

1
n

(yi     b0     b1xi)2
(cid:80)
(cid:99)  1 =
(cid:98)v [x]
i (yi     y)(xi     x)
(cid:99)  0 = y    (cid:99)  1x

1
n

and

1
n

the other is to minimize the in-sample or empirical mean squared error,

you may or may not    nd it surprising that both approaches lead to the same
answer:

(1.45)
(1.46)
provided that v [x] > 0, these will converge with iid samples, so we have a
consistent estimator.

we are now in a position to see how the least-squares id75 model

(1.41)

(1.42)

(1.43)

(1.44)

28

regression basics

is really a weighted averaging of the data. let   s write the estimated regression
function explicitly in terms of the training data points.

= y +(cid:99)  1(x     x)
(cid:32) 1

=

1
n

(cid:98)  (x) =(cid:99)  0 +(cid:99)  1x
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)

(cid:18)

1
n

1
n

=

=

i=1

i=1

=

i=1
1
n

i=1

yi +

yi +

yi +

1 +

(cid:33)

n

1
n

(x     x)

(cid:80)
(cid:80)
i (yi     y)(xi     x)
i (xi     x)2
n(cid:88)
(xi     x)(yi     y)
n(cid:88)
(xi     x)yi     (x     x)

(x     x)
n    2
x
(x     x)
n    2
x
(x     x)(xi     x)

(cid:19)

n    2
x

i=1

i=1

    2
x

(nx     nx)y

(1.51)

yi

(1.52)

(1.47)

(1.48)

(1.49)

(1.50)

in words, our prediction is a weighted average of the observed values yi of the
regressand, where the weights are proportional to how far xi and x both are from
the center of the data (relative to the variance of x). if xi is on the same side of
the center as x, it gets a positive weight, and if it   s on the opposite side it gets a
negative weight.

figure 1.4 adds the least-squares regression line to figure 1.1. as you can see,
this is only barely slightly di   erent from the constant regression function (the
slope is x is 0.014). visually, the problem is that there should be a positive slope
in the left-hand half of the data, and a negative slope in the right, but the slopes
and the densities are balanced so that the best single slope is near zero.8

mathematically, the problem arises from the peculiar way in which least-
squares id75 smoothes the data. as i said, the weight of a data point
depends on how far it is from the center of the data, not how far it is from the
point at which we are trying to predict. this works when   (x) really is a straight
line, but otherwise     e.g., here     it   s a recipe for trouble. however, it does sug-
gest that if we could somehow just tweak the way we smooth the data, we could
do better than id75.

1.5 linear smoothers

the sample mean and the least-squares line are both special cases of linear
smoothers, which estimates the regression function with a weighted average:

(cid:88)

(cid:98)  (x) =

yi(cid:98)w(xi, x)

(1.53)

i

these are called linear smoothers because the predictions are linear in the re-
sponses yi; as functions of x they can be and generally are nonlinear.

8 the standard test of whether this coe   cient is zero is about as far from rejecting the null hypothesis

as you will ever see, p = 0.89. remember this the next time you look at id75 output.

1.5 linear smoothers

29

plot(all.x, all.y, xlab = "x", ylab = "y")
rug(all.x, side = 1, col = "grey")
rug(all.y, side = 2, col = "grey")
abline(h = mean(all.y), lty = "dotted")
fit.all = lm(all.y ~ all.x)
abline(fit.all)

figure 1.4 data from figure 1.1, with a horizontal line at the mean
(dotted) and the ordinary least squares regression line (solid).

as i just said, the sample mean is a special case; see exercise 1.7. ordinary

id75 is another special case, where (cid:98)w(xi, x) is given by eq. 1.52. both

of these, as remarked earlier, ignore how far xi is from x. let us look at some
linear smoothers which are not so silly.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0xy30

regression basics

1.5.1 k-nearest-neighbor regression

at the other extreme from ignoring the distance between xi and x, we could do
nearest-neighbor regression:

(cid:98)w(xi, x) =

(cid:26) 1 xi nearest neighbor of x

0 otherwise

(1.54)

this is very sensitive to the distance between xi and x. if   (x) does not change
too rapidly, and x is pretty thoroughly sampled, then the nearest neighbor of
x among the xi is probably close to x, so that   (xi) is probably close to   (x).
however, yi =   (xi) + noise, so nearest-neighbor regression will include the noise
into its prediction. we might instead do k-nearest neighbor regression,

(cid:26) 1/k xi one of the k nearest neighbors of x

(cid:98)w(xi, x) =

(1.55)

0

otherwise

again, with enough samples all the k nearest neighbors of x are probably close
to x, so their regression functions there are going to be close to the regression
function at x. but because we average their values of yi, the noise terms should
tend to cancel each other out. as we increase k, we get smoother functions    
in the limit k = n and we just get back the constant. figure 1.5 illustrates this
for our running example data.9 to use k-nearest-neighbors regression, we need to
pick k somehow. this means we need to decide how much smoothing to do, and
this is not trivial. we will return to this point in chapter 3.

because k-nearest-neighbors averages over only a    xed number of neighbors,
each of which is a noisy sample, it always has some noise in its prediction, and is
generally not consistent. this may not matter very much with moderately-large
data (especially once we have a good way of picking k). if we want consistency,
we need to let k grow with n, but not too fast; it   s enough that as n        , k        
and k/n     0 (gy  or    et al., 2002, thm. 6.1, p. 88).

1.5.2 kernel smoothers

changing k in a k-nearest-neighbors regression lets us change how much smooth-
ing we   re doing on our data, but it   s a bit awkward to express this in terms of a
number of data points. it feels like it would be more natural to talk about a range
in the independent variable over which we smooth or average. another problem
with id92 regression is that each testing point is predicted using information
from only a few of the training data points, unlike id75 or the sample
mean, which always uses all the training data. it   d be nice if we could somehow
use all the training data, but in a location-sensitive way.

there are several ways to do this, as we   ll see, but a particularly useful one is

9 the code uses the k-nearest neighbor function provided by the package fnn (beygelzimer et al.,

2013). this requires one to give both a set of training points (used to learn the model) and a set of
test points (at which the model is to make predictions), and returns a list where the actual
predictions are in the pred element     see help(knn.reg) for more, including examples.

1.5 linear smoothers

31

library(fnn)
plot.seq <- matrix(seq(from = 0, to = 1, length.out = 100), byrow = true)
lines(plot.seq, knn.reg(train = all.x, test = plot.seq, y = all.y, k = 1)$pred,

col = "red")

lines(plot.seq, knn.reg(train = all.x, test = plot.seq, y = all.y, k = 3)$pred,

col = "green")

lines(plot.seq, knn.reg(train = all.x, test = plot.seq, y = all.y, k = 5)$pred,

col = "blue")

lines(plot.seq, knn.reg(train = all.x, test = plot.seq, y = all.y, k = 20)$pred,

col = "purple")

legend("center", legend = c("mean", expression(k == 1), expression(k == 3),

expression(k == 5), expression(k == 20)), lty = c("dashed", rep("solid",
4)), col = c("black", "red", "green", "blue", "purple"))

figure 1.5 points from figure 1.1 with horizontal dashed line at the mean
and the k-nearest-neighbor regression curves for various k. increasing k
smooths out the regression curve, pulling it towards the mean.     the code
is repetitive; can you write a function to simplify it?

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0xymeank=1k=3k=5k=2032

regression basics

kernel smoothing, a.k.a. kernel regression or nadaraya-watson regres-
sion. to begin with, we need to pick a id8110 k(xi, x) which satis   es
the following properties:
1. k(xi, x)     0;
2. k(xi, x) depends only on the distance xi     x, not the individual arguments;

3. (cid:82) xk(0, x)dx = 0; and
4. 0 <(cid:82) x2k(0, x)dx <    .

these conditions together (especially the last one) imply that k(xi, x)     0 as
|xi   x|        . two examples of such functions are the density of the unif(   h/2, h/2)
distribution, and the density of the standard gaussian n (0,
h) distribution.
here h can be any positive number, and is called the bandwidth. because
k(xi, x) = k(0, xi     x), we will often write k as a one-argument function,
k(xi     x). because we often want to consider similar kernels which di   er only by
bandwidth, we   ll either write k( xi   x

   

the nadaraya-watson estimate of the regression function is

h ), or kh(xi     x).
(cid:88)

k(xi, x)
j k(xj, x)

yi

(cid:98)  (x) =
(cid:98)w(xi, x) =

i

(cid:80)
(cid:80)

k(xi, x)
j k(xj, x)

(1.56)

(1.57)

i.e., in terms of eq. 1.53,

(notice that here, as in id92 regression, the sum of the weights is always 1.
why?)11

what does this achieve? well, k(xi, x) is large if xi is close to x, so this will
place a lot of weight on the training data points close to the point where we are
trying to predict. more distant training points will have smaller weights, falling
o    towards zero. if we try to predict at a point x which is very far from any of
the training data points, the value of k(xi, x) will be small for all xi, but it will
typically be much, much smaller for all the xi which are not the nearest neighbor

of x, so (cid:98)w(xi, x)     1 for the nearest neighbor and     0 for all the others.12 that is,

far from the training data, our predictions will tend towards nearest neighbors,
rather than going o    to      , as id75   s predictions do. whether this

10 there are many other mathematical objects which are also called    kernels   . some of these meanings

are related, but not all of them. (cf.    normal   .)

11 what do we do if k(xi, x) is zero for some xi? nothing; they just get zero weight in the average.
what do we do if all the k(xi, x) are zero? di   erent people adopt di   erent conventions; popular
ones are to return the global, unweighted mean of the yi, to do some sort of interpolation from
regions where the weights are de   ned, and to throw up our hands and refuse to make any
predictions (computationally, return na).

12 take a gaussian kernel in one dimension, for instance, so k(xi, x)     e   (xi   x)2/2h2
e   (xj   xi)2/2h2 (cid:28) e   l2/2h2

nearest neighbor, and |xi     x| = l, with l (cid:29) h. so k(xi, x)     e   l2/2h2
for any other xj , k(xi, x)     e   l2/2h2
that we   re using a kernel like the gaussian, which never quite goes to zero, unlike the box kernel.

, a small number. but now
.     this assumes

e   (xj   xi)l/2h2

. say xi is the

1.5 linear smoothers

33

is good or bad of course depends on the true   (x)     and how often we have to
predict what will happen very far from the training data.

figure 1.6 shows our running example data, together with kernel regression
estimates formed by combining the uniform-density, or box, and gaussian kernels
with di   erent bandwidths. the box kernel simply takes a region of width h around
the point x and averages the training data points it    nds there. the gaussian
kernel gives reasonably large weights to points within h of x, smaller ones to points
within 2h, tiny ones to points within 3h, and so on, shrinking like e   (x   xi)2/2h.
as promised, the bandwidth h controls the degree of smoothing. as h        , we
revert to taking the global mean. as h     0, we tend to get spikier functions    
with the gaussian kernel at least it tends towards the nearest-neighbor regression.
if we want to use kernel regression, we need to choose both which kernel to
use, and the bandwidth to use with it. experience, like figure 1.6, suggests that
the bandwidth usually matters a lot more than the kernel. this puts us back
to roughly where we were with id92 regression, needing to control the degree
of smoothing, without knowing how smooth   (x) really is. similarly again, with
a    xed bandwidth h, kernel regression is generally not consistent. however, if
h     0 as n        , but doesn   t shrink too fast, then we can get consistency.

34

regression basics

lines(ksmooth(all.x, all.y, "box", bandwidth = 2), col = "red")
lines(ksmooth(all.x, all.y, "box", bandwidth = 1), col = "green")
lines(ksmooth(all.x, all.y, "box", bandwidth = 0.1), col = "blue")
lines(ksmooth(all.x, all.y, "normal", bandwidth = 2), col = "red", lty = "dashed")
lines(ksmooth(all.x, all.y, "normal", bandwidth = 1), col = "green", lty = "dashed")
lines(ksmooth(all.x, all.y, "normal", bandwidth = 0.1), col = "blue", lty = "dashed")
legend("bottom", ncol = 3, legend = c("", expression(h == 2), expression(h ==

1), expression(h == 0.1), "box", "", "", "", "gaussian", "", "", ""), lty = c("blank",
"blank", "blank", "blank", "blank", "solid", "solid", "solid", "blank",
"dashed", "dashed", "dashed"), col = c("black", "black", "black", "black",
"black", "red", "green", "blue", "black", "red", "green", "blue"), pch = na)

figure 1.6 data from figure 1.1 together with kernel regression lines, for
various combinations of kernel (box/uniform or gaussian) and bandwidth.
note the abrupt jump around x = 0.75 in the h = 0.1 box-kernel (solid blue)
line     with a small bandwidth the box kernel is unable to interpolate
smoothly across the break in the training data, while the gaussian kernel
(dashed blue) can.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0xyh=2h=1h=0.1boxgaussian1.5 linear smoothers

35

1.5.3 some general theory for linear smoothers

some key parts of the theory you are familiar with for id75 models
carries over more generally to linear smoothers. they are not quite so important
any more, but they do have their uses, and they can serve as security objects
during the transition to non-parametric regression.

throughout this sub-section, we will temporarily assume that y =   (x) +  ,
with the noise terms   having constant variance   2, no correlation with the noise
at other observations. also, we will de   ne the smoothing, in   uence or hat
matrix   w by   wij =   w(xi, xj). this records how much in   uence observation yj

had on the smoother   s    tted value for   (xi), which (remember) is (cid:98)  (xi) or (cid:98)  i for
it is easy to get the standard error of any predicted mean value (cid:98)  (x), by    rst

1.5.3.1 standard error of predicted mean values

short13, hence the name    hat matrix    for   w.

working out its variance:

(cid:35)

j=1

(cid:34) n(cid:88)
v [(cid:98)  (x)] = v
n(cid:88)
n(cid:88)
n(cid:88)

j=1

j=1

=

=

=   2

w(xj, x)yj

v [w(xj, x)yj]

w2(xj, x)v [yj]

w2(xj, x)

(1.58)

(1.59)

(1.60)

(1.61)

j=1

the second line uses the assumption that the noise is uncorrelated, and the last
the assumption that the noise variance is constant. in particular, for a point xi

which appeared in the training data, v [(cid:98)  (xi)] =   2(cid:80)
notice that this is the variance in the predicted mean value, (cid:98)  (x). it is not an

estimate of v [y | x = x], though we will see how conditional variances can be
estimated using nonparametric regression in chapter 10.

j w2
ij.

notice also that we have not had to assume that the noise is gaussian. if we
did add that assumption, this formula would also give us a con   dence interval
for the    tted value (though we would still have to worry about estimating   ).

1.5.3.2 (e   ective) degrees of freedom

for id75 models, you will recall that the number of    degrees of free-
dom    was just the number of coe   cients (including the intercept). while degrees
of freedom are less important for other sorts of regression than for linear models,
they   re still worth knowing about, so i   ll explain here how they are de   ned and
calculated.
13 this is often written as   yi, but that   s not very logical notation; the quantity is a function of yi, not

an estimate of it; it   s an estimate of   (xi).

36

regression basics

the    rst thing to realize is that we can   t use the number of parameters to de   ne
degrees of freedom in general, since most linear smoothers don   t have parameters.
instead, we have to go back to the reasons why the number of parameters matters
in ordinary linear models14. we   ll start with an n    p data matrix of predictor
variables x (possibly including an all-1 column for an intercept), and an n    1
column matrix of response values y. the ordinary least squares estimate of the
p-dimensional coe   cient vector    is

this lets us write the    tted values in terms of x and y:

     =(cid:0)xt x(cid:1)   1
(cid:98)   = x     
(cid:16)
x(cid:0)xt x(cid:1)   1

=

xt y

xt(cid:17)

y

= wy

or hat matrix, in the special case of ordinary least squares.

notice that w depends only on the predictor variables in x; the observed re-

(1.65)
contributes to each    tted(cid:98)  i. this is what, a little while ago, i called the in   uence
where w is the n    n matrix, with wij saying how much of each observed yj
sponse values in y don   t matter. if we change around y, the    tted values (cid:98)   will
dom. once x (and thus w) are    xed, however, (cid:98)   has to lie in a p-dimensional
geometrically, the dimension of the space in which (cid:98)   = wy is con   ned is the

linear subspace in this n-dimensional space, and the residuals have to lie in the
(n     p)-dimensional space orthogonal to it.

also change, but only within the limits allowed by w. there are n independent
coordinates along which y can change, so we say the data have n degrees of free-

rank of the matrix w. since w is an idempotent matrix (exercise 1.5), its rank
equals its trace. and that trace is, exactly, p:

(cid:16)
xt(cid:17)
x(cid:0)xt x(cid:1)   1
xt x(cid:0)xt x(cid:1)   1(cid:17)
(cid:16)

tr w = tr

= tr

= tr ip = p

(cid:98)   = wy
df ((cid:98)  )     tr w

since for any matrices a, b, tr (ab) = tr (ba), and xt x is a p    p matrix15.

for more general linear smoothers, we can still write eq. 1.53 in matrix form,

we now de   ne the degrees of freedom16 to be the trace of w:

this may not be an integer.

14 what follows uses some concepts and results from id202; see appendix b for reminders.
15 this all assumes that xt x has an inverse. can you work out what happens when it does not?
16 some authors prefer to say    e   ective degrees of freedom   , to emphasize that we   re not just counting

parameters.

(1.62)

(1.63)

(1.64)

(1.66)

(1.67)

(1.68)

(1.69)

(1.70)

1.5 linear smoothers

37

covariance of observations and fits

eq. 1.70 de   nes the number of degrees of freedom for linear smoothers. a yet more
general de   nition includes nonlinear methods, assuming that yi =   (xi) +  i, and
the  i consist of uncorrelated noise of constant17 variance   2. this is

df ((cid:98)  )     1

  2

n(cid:88)

i=1

cov [yi,(cid:98)  (xi)]

in words, this is the normalized covariance between each observed response yi and

the corresponding predicted value,(cid:98)  (xi). this is a very natural way of measuring

how    exible or stable the regression model is, by seeing how much it shifts with
the data.

if we do have a linear smoother, eq. 1.71 reduces to eq. 1.70.

(cid:35)

(cid:34)

n(cid:88)

j=1

yi,

wijyj

cov [yi,(cid:98)  (xi)] = cov
n(cid:88)

=

wijcov [yi, yj]

j=1

= wiiv [yi] =   2wii

here the    rst line uses the fact that we   re dealing with a linear smoother, and
the last line the assumption that  i is uncorrelated and has constant variance.
therefore

  2wii = tr w

(1.75)

df ((cid:98)  ) =

1
  2

n(cid:88)

i=1

as promised.

1.5.3.3 prediction errors

bias

because linear smoothers are linear in the response variable, it   s easy to work out
(theoretically) the expected value of their    ts:

wije [yj]

n(cid:88)

e [(cid:98)  i] =
e [(cid:98)  ] = we [y]

j=1

in matrix form,

(1.77)
this means the smoother is unbiased if, and only if, we [y] = e [y], that is, if
e [y] is an eigenvector of w. turned around, the condition for the smoother to
be unbiased is

(in     w)e [y] = 0

(1.78)

17 but see exercise 1.10.

(1.71)

(1.72)

(1.73)

(1.74)

(1.76)

regression basics

38
in general, (in   w)e [y] (cid:54)= 0, so linear smoothers are more or less biased. di   erent
smoothers are, however, unbiased for di   erent families of regression functions.
ordinary id75, for example, is unbiased if and only if the regression
function really is linear.

in-sample mean squared error

when you studied id75, you learned that the expected mean-squared
error on the data used to    t the model is   2(n     p)/n. this formula generalizes
to other linear smoothers. let   s    rst write the residuals in matrix form.

y    (cid:98)   = y     wy
= iny     wy
= (in     w)y

(1.79)
(1.80)
(1.81)

(1.82)

(1.83)

(1.84)

the in-sample mean squared error is n   1 (cid:107)y    (cid:98)  (cid:107)2, so
(cid:107)(in     w)y(cid:107)2
yt (in     wt )(in     w)y

(cid:107)y    (cid:98)  (cid:107)2 =

1
n

=

1
n
1
n

taking expectations18,

(cid:20) 1

n

e

(cid:21)

(cid:107)y    (cid:98)  (cid:107)2

tr(cid:0)(in     wt )(in     w)(cid:1) +
(cid:0)tr in     2 tr w + tr (wt w)(cid:1) +
(cid:0)n     2 tr w + tr (wt w)(cid:1) +

1
n

1
n

=

=

  2
n
  2
n
  2
n

(cid:107)(in     w)e [y](cid:107)2

(cid:107)(in     w)e [y](cid:107)2(1.85)

1
n
(cid:107)(in     w)e [y](cid:107)2

=

(1.86)
the last term, n   1 (cid:107)(in     w)e [y](cid:107)2, comes from the bias: it indicates the dis-
tortion that the smoother would impose on the regression function, even without
noise. the    rst term, proportional to   2, re   ects the variance. notice that it in-
volves not only what we   ve called the degrees of freedom, tr w, but also a second-
order term, tr wt w. for ordinary id75, you can show (exercise 1.9)
that tr (wt w) = p, so 2 tr w    tr (wt w) would also equal p. for this reason, some
people prefer either tr (wt w) or 2 tr w     tr (wt w) as the de   nition of degrees of
freedom for linear smoothers, so be careful.

1.5.3.4 inferential statistics

many of the formulas underlying things like the f test for whether a regression
predicts signi   cantly better than the global mean carry over from id75
to linear smoothers, if one uses the right de   nitions of degrees of freedom, and one
believes that the noise is always iid and gaussian. however, we will see ways of
doing id136 on regression models which don   t rely on gaussian assumptions
at all (ch. 6), so i won   t go over these results.

18 see app. f.2 for how to    nd the expected value of quadratic forms like this.

1.6 further reading

39

1.6 further reading

in chapter 2, we   ll look more at the limits of id75 and some ex-
tensions; chapter 3 will cover some key aspects of evaluating statistical models,
including regression models; and then chapter 4 will come back to kernel regres-
sion, and more powerful tools than ksmooth. chapters 10   8 and 13 all introduce
further regression methods, while chapters 11   12 pursue extensions.

good treatments of regression, emphasizing linear smoothers but not limited
to id75, can be found in wasserman (2003, 2006), simono    (1996),
faraway (2006) and gy  or    et al. (2002). the last of these in particular provides
a very thorough theoretical treatment of non-parametric regression methods.
(1989,   2.7.3), and ye (1998).

on generalizations of degrees of freedom to non-linear models, see buja et al.

historical notes

all the forms of nonparametric regression covered in this chapter are actually
quite old. kernel regression was introduced independently by nadaraya (1964)
and watson (1964). the origin of nearest neighbor methods is less clear, and
indeed they may have been independently invented multiple times     cover and
hart (1967) collects some of the relevant early citations, as well as providing a pi-
oneering theoretical analysis, extended to regression problems in cover (1968a,b).

exercises

1.1

suppose y1, y2, . . . yn are random variables with the same mean    and standard deviation
  , and that they are all uncorrelated with each other, but not necessarily independent19
or identically distributed. show the following:

(cid:3) = n  2.
(cid:3) =   2/n.

1. v(cid:2)(cid:80)n
2. v(cid:2)n   1(cid:80)n
3. the standard deviation of n   1(cid:80)n
4. the standard deviation of        n   1(cid:80)n

i=1 yi

i=1 yi

   
   
n.
i=1 yi is   /
n.
i=1 yi is   /

can you state the analogous results when the yi share mean    but each has its own
standard deviation   i? when each yi has a distinct mean   i? (assume in both cases that
the yi remain uncorrelated.)
suppose we use the mean absolute error instead of the mean squared error:

1.2

(1.87)
is this also minimized by taking m = e [y ]? if not, what value      minimizes the mae?
should we use mse or mae to measure error?

mae(m) = e [|y     m|]

1.3 derive eqs. 1.45 and 1.44 by minimizing eq. 1.43.
1.4 what does it mean to say that gaussian kernel regression approaches nearest-neighbor

regression as h     0? why does it do so? is this true for all kinds of kernel regression?

1.5 prove that w from eq. 1.65 is idempotent, i.e., that w2 = w.
1.6

show that for ordinary id75, eq. 1.61 gives the same variance for    tted values
as the usual formula.

19 see appendix e.4 for a refresher on the di   erence between    uncorrelated    and    independent   .

40

regression basics

1.7 consider the global mean as a linear smoother. work out the in   uence matrix w, and

show that it has one degree of freedom, using the de   nition in eq. 1.70.

1.9

1.8 consider k-nearest-neighbors regression as a linear smoother. work out the in   uence ma-
trix w, and    nd an expression for the number of degrees of freedom (in the sense of eq.
1.70) in terms of k and n. hint: your answers should reduce to those of the previous
problem when k = n.
suppose that yi =   (xi) +  i, where the  i are uncorrelated have mean 0, with constant
v [    i] = (  2/n) tr (wwt ). show

i=1
that this reduces to   2p/n for ordinary id75.

variance   2. prove that, for a linear smoother, n   1(cid:80)n
to (cid:80)n

1.10 suppose that yi =   (xi) +  i, where the  i are uncorrelated and have mean 0, but
i . consider modifying the de   nition of degrees of freedom
i =   2). show that this

i (which reduces to eq. 1.71 if all the   2

each has its own variance   2

i=1 cov [yi,     i] /  2

still equals tr w for a linear smoother with in   uence matrix w.

2

the truth about id75

we need to say some more about how id75, and especially about how
it really works and how it can fail. id75 is important because

1. it   s a fairly straightforward technique which sometimes works tolerably for

prediction;

2. it   s a simple foundation for some more sophisticated techniques;
3. it   s a standard method so people use it to communicate; and
4. it   s a standard method so people have come to confuse it with prediction and

even with causal id136 as such.

we need to go over (1)   (3), and provide prophylaxis against (4).

[[todo: discuss the geometry: smoothing on to a linear surface; only projec-

tion along    matters;    tted values constrained to a linear subspace]]

2.1 optimal linear prediction: multiple variables

we have a numerical variable y and a p-dimensional vector of predictor variables
or features (cid:126)x. we would like to predict y using (cid:126)x. chapter 1 taught us that the
mean-squared optimal predictor is is the conditional expectation,

  ((cid:126)x) = e(cid:104)

y | (cid:126)x = (cid:126)x

(cid:105)

(2.1)

instead of using the optimal predictor   ((cid:126)x), let   s try to predict as well as
possible while using only a linear1 function of (cid:126)x, say   0 +       (cid:126)x. this is not
an assumption about the world, but rather a decision on our part; a choice,
not a hypothesis. this decision can be good       0 + (cid:126)x       could be a tolerable
approximation to   ((cid:126)x)     even if the linear hypothesis is strictly wrong. even if
no linear approximation to    is much good mathematically, but we might still
want one for practical reasons, e.g., speed of computation.

(perhaps the best reason to hope the choice to use a linear model isn   t crazy
is that we may hope    is a smooth function. if it is, then we can taylor expand2

1 pedants might quibble that this function is actually a   ne rather than linear. but the distinction is

specious: we can always add an extra element to (cid:126)x, which is always 1, getting the vector (cid:126)x(cid:48), and
then we have the linear function   (cid:48)    (cid:126)x(cid:48).

2 see appendix d on taylor approximations.

41

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

42

the truth about id75

it about our favorite point, say (cid:126)u:

(cid:18)      

p(cid:88)

   xi

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)u

(cid:19)

(xi     ui) + o((cid:107)(cid:126)x     (cid:126)u(cid:107)2)

  ((cid:126)x) =   ((cid:126)u) +

or, in the more compact vector-calculus notation,

  ((cid:126)x) =   ((cid:126)u) + ((cid:126)x     (cid:126)u)         ((cid:126)u) + o((cid:107)(cid:126)x     (cid:126)u(cid:107)2)

(2.2)

(2.3)

if we only look at points (cid:126)x which are close to (cid:126)u, then the remainder terms
o((cid:107)(cid:126)x     (cid:126)u(cid:107)2) are small, and a linear approximation is a good one3. here,    close
to (cid:126)u    really means    so close that all the non-linear terms in the taylor series are
comparatively negligible   .)

whatever the reason for wanting to use a linear function, there are many
linear functions, and we need to pick just one of them. we may as well do that
by minimizing mean-squared error again:

(cid:20)(cid:16)

(cid:17)2(cid:21)

m se(  ) = e

y       0     (cid:126)x      
(cid:105)

(cid:104) (cid:126)x, y

   = v   1cov

(2.4)

(cid:105)

(2.5)

(cid:104) (cid:126)x, y
(cid:104) (cid:126)x, y
(cid:105)

=

i

(2.6)

going through the optimization is parallel to the one-dimensional case we worked
through in   1.4.3, with the conclusion that the optimal    is

where v is the covariance matrix of (cid:126)x, i.e., vij = cov [xi, xj], and cov

is the vector of covariances between the regressors and y , i.e. cov
cov [xi, y ]. we also get

  0 = e [y ]           e(cid:104) (cid:126)x

(cid:105)

just as in the one-dimensional case (exercise 2.1). these conclusions hold without
assuming anything at all about the true regression function   ; about the distri-
bution of x, of y , of y |x, or of y       (x) (in particular, nothing needs to be
gaussian); or whether data points are independent or not.

multiple regression would be a lot simpler if we could just do a simple regression
for each regressor, and add them up; but really, this is what multiple regression
does, just in a disguised form. if the input variables are uncorrelated, v is diagonal
(vij = 0 unless i = j), and so is v   1. then doing multiple regression breaks up into
a sum of separate simple regressions across each input variable. when the input
variables are correlated and v is not diagonal, we can think of the multiplication
by v   1 as de-correlating (cid:126)x     applying a linear transformation to come up
with a new set of inputs which are uncorrelated with each other.4
3 if you are not familiar with the big-o notation like o((cid:107)(cid:126)x     (cid:126)u(cid:107)2), now would be a good time to read

appendix c.

4 if (cid:126)z is a random vector with covariance matrix i, then w (cid:126)z is a random vector with covariance

matrix wt w. conversely, if we start with a random vector (cid:126)x with covariance matrix v, the latter
has a    square root    v1/2 (i.e., v1/2v1/2 = v), and v   1/2 (cid:126)x will be a random vector with covariance

2.1 optimal linear prediction: multiple variables

43

notice:    depends on the marginal distribution of (cid:126)x (through the covariance
matrix v). if that shifts, the optimal coe   cients    will shift, unless the real
regression function is linear.

(cid:104) (cid:126)x, y

(cid:105)

2.1.1 collinearity

the formula    = v   1cov
makes no sense if v has no inverse. this will
happen if, and only if, the predictor variables are linearly dependent on each
other     if one of the predictors is really a linear combination of the others. then
(as we learned in id202) the covariance matrix is of less than    full rank   
(i.e.,    rank de   cient   ) and it doesn   t have an inverse. equivalently, v has at least
one eigenvalue which is exactly zero.

so much for the algebra; what does that mean statistically? let   s take an
easy case where one of the predictors is just a multiple of the others     say
you   ve included people   s weight in pounds (x1) and mass in kilograms (x2), so
x1 = 2.2x2. then if we try to predict y , we   d have

(cid:98)  ( (cid:126)x) =   1x1 +   2x2 +   3x3 + . . . +   pxp

p(cid:88)
p(cid:88)

i=3

= 0x1 + (2.2  1 +   2)x2 +

  ixi

= (  1 +   2/2.2)x1 + 0x2 +

  ixi

=    2200x1 + (1000 +   1 +   2)x2 +

i=3

p(cid:88)

i=3

(2.7)

(2.8)

(2.9)

  ixi

(2.10)

in other words, because there   s a linear relationship between x1 and x2, we
make the coe   cient for x1 whatever we like, provided we make a corresponding
adjustment to the coe   cient for x2, and it has no e   ect at all on our prediction.
so rather than having one optimal linear predictor, we have in   nitely many of
them.5

there are three ways of dealing with collinearity. one is to get a di   erent data
set where the regressors are no longer collinear. a second is to identify one of the
collinear variables (it usually doesn   t matter which) and drop it from the data set.
this can get complicated; principal components analysis (chapter 16) can help
here. thirdly, since the issue is that there are in   nitely many di   erent coe   cient
vectors which all minimize the mse, we could appeal to some extra principle,

(cid:16) (cid:126)xv   1/2(cid:17)(cid:16)

(cid:104) (cid:126)x, y

(cid:105)(cid:17)

matrix i. when we write our predictions as (cid:126)xv   1cov

, we should think of this as

v   1/2cov

. we use one power of v   1/2 to transform the input features into
uncorrelated variables before taking their correlations with the response, and the other power to
decorrelate (cid:126)x.     for more on using covariance matrices to come up with new, decorrelated
variables, see chapter 16.

5 algebraically, there is a linear combination of two (or more) of the regressors which is constant. the

coe   cients of this linear combination are given by one of the zero eigenvectors of v.

(cid:104) (cid:126)x, y

(cid:105)

44

the truth about id75

beyond prediction accuracy, to select just one of them. we might, for instance,
prefer smaller coe   cient vectors (all else being equal), or ones where more of the
coe   cients were exactly zero. using some quality other than the squared error
to pick out a unique solution is called    regularizing    the optimization problem,
and a lot of attention has been given to regularized regression, especially in the
   high dimensional    setting where the number of coe   cients is comparable to, or
even greater than, the number of data points. see appendix h.3.5, and exercise
7.2 in chapter 7.

2.1.2 the prediction and its error

once we have coe   cients   , we can use them to make predictions for the expected
value of y at arbitrary values of (cid:126)x, whether we   ve an observation there before or
not. how good are these?

if we have the optimal coe   cients, then the prediction error will be uncorrelated

(cid:104)

with the regressors:

cov

y     (cid:126)x      , (cid:126)x

(cid:105)

(cid:104)
(cid:104)

(cid:105)     cov
(cid:105)     vv   1cov

(cid:104) (cid:126)x    (v   1cov
(cid:105)

y, (cid:126)x

(cid:104)

(cid:104) (cid:126)x, y

(cid:105)

), (cid:126)x

(cid:105)

(2.11)

(2.12)

= cov

= cov

y, (cid:126)x

y, (cid:126)x

(2.13)
moreover, the expected prediction error, averaged over all (cid:126)x, will be zero (exer-
cise 2.2). but the conditional expectation of the error is generally not zero,

= 0

and the conditional variance is generally not constant,

y     (cid:126)x       | (cid:126)x = (cid:126)x1

y     (cid:126)x       | (cid:126)x = (cid:126)x2

v(cid:104)

(cid:105) (cid:54)= 0

e(cid:104)

y     (cid:126)x       | (cid:126)x = (cid:126)x

(cid:105) (cid:54)= v(cid:104)

(cid:105)

(2.14)

(2.15)

the optimal linear predictor can be arbitrarily bad, and it can make arbitrarily

big systematic mistakes. it is generally very biased6.

2.1.3 estimating the optimal linear predictor

to actually estimate    from data, we need to make some probabilistic assumptions
about where the data comes from. a fairly weak but often su   cient assumption
is that observations ( (cid:126)xi, yi) are independent for di   erent values of i, with un-
changing covariances. then if we look at the sample covariances, they will, by
the law of large numbers, converge on the true covariances:

(cid:104) (cid:126)x, y

(cid:105)

xt y     cov
xt x     v

1
n
1
n

(2.16)

(2.17)

6 you were taught in your linear models course that id75 makes unbiased predictions.

this presumed that the linear model was true.

n(cid:88)

2.1 optimal linear prediction: multiple variables

45

where as before x is the data-frame matrix with one row for each data point and
one column for each variable, and similarly for y.

(cid:98)   = (xt x)

   1

xt y       

(2.18)

so, by continuity,

and we have a consistent estimator.

on the other hand, we could start with the empirical or in-sample mean squared

error

(2.19)

mse(  )     1
n

(yi     (cid:126)xi      )2

and minimize it. the minimizer is the same (cid:98)   we got by plugging in the sample
mse, but it doesn   t let us say anything about the convergence of (cid:98)  . for that,

covariances. no probabilistic assumption is needed to minimize the in-sample

we do need some assumptions about (cid:126)x and y coming from distributions with
unchanging covariances.

i=1

(one can also show that the least-squares estimate is the linear predictor with
the minimax prediction risk. that is, its worst-case performance, when everything
goes wrong and the data are horrible, will be better than any other linear method.
this is some comfort, especially if you have a gloomy and pessimistic view of
data, but other methods of estimation may work better in less-than-worst-case
scenarios.)

2.1.3.1 unbiasedness and variance of ordinary least squares estimates

the very weak assumptions we have made still let us say a little bit more about

the properties of the ordinary least squares estimate (cid:98)  . to do so, we need to think
about why (cid:98)      uctuates. for the moment, let   s    x x at a particular value x, but
the key fact is that (cid:98)   is linear in the observed responses y. we can use this

allow y to vary randomly (what   s called       xed design    regression).

by writing, as you   re used to from your id75 class,

here   is the noise around the optimal linear predictor; we have to remember that
while e [ ] = 0 and cov
= 0

 | (cid:126)x = (cid:126)x

 , (cid:126)x

or that v(cid:104)

 | (cid:126)x = (cid:126)x

(cid:105)

is constant. even with these limitations, we can still say that

(cid:104)

y = (cid:126)x       +  

= 0, it is not generally true that e(cid:104)
(cid:105)
(cid:98)   = (xt x)

   1
   1

xt y
xt (x   +  )

= (xt x)
=    + (xt x)

   1

this directly tells us that (cid:98)   is an unbiased estimate of   :

xt  

e(cid:104)(cid:98)  |x = x

(cid:105)

=    + (xt x)

   1

xt e [ ]

=    + 0 =   

(2.20)

(cid:105)

(2.21)

(2.22)

(2.23)

(2.24)

(2.25)

(2.26)

(2.27)

46

we can also get the variance matrix of (cid:98)  :

the truth about id75

v(cid:104)(cid:98)  |x = x

(cid:105)

= v(cid:104)
= v(cid:104)

(cid:105)
(cid:105)

   + (xt x)

   1

xt   | x
xt   | x = x

   1

(xt x)
   1

= (xt x)

(2.28)
let   s write v [ |x = x] as a single matrix   (x). if the linear-prediction errors are
uncorrelated with each other, then    will be diagonal. if they   re also of equal
variance, then    =   2i, and we have

xt v [  | x = x] x(xt x)

   1

v(cid:104)(cid:98)  |x = x

(cid:105)

(cid:19)   1

(cid:18) 1

n

=   2(xt x)   1 =

  2
n

xt x

(2.29)

said in words, this means that the variance of our estimates of the linear-regression
coe   cient will (i) go down as the sample size n grows, (ii) go up as the linear
regression gets worse (  2 grows), and (iii) go down as the regressors, the compo-
nents of (cid:126)x, have more sample variance themselves, and are less correlated with
each other.

(cid:105)(cid:105)

if we allow x to vary, then by the law of total variance,

v(cid:104)(cid:98)  
(cid:105)
continuous, v(cid:104)(cid:98)  

+ v(cid:104)e(cid:104)(cid:98)  |x

= e(cid:104)v(cid:104)(cid:98)  |x
(cid:105)     n   1  2v   1, and points (i)   (iii) still hold.

(cid:105)(cid:105)

  2
n

xt x

e

=

n

(cid:34)(cid:18) 1

(cid:19)   1(cid:35)

as n        , the sample variance matrix n   1xt x     v. since matrix inversion is

(2.30)

2.2 shifting distributions, omitted variables, and transformations

2.2.1 changing slopes

   

i said earlier that the best    in id75 will depend on the distribution
of the regressors, unless the conditional mean is exactly linear. here is an illustra-
tion. for simplicity, let   s say that p = 1, so there   s only one regressor. i generated
x +  , with       n (0, 0.052) (i.e. the standard deviation of the
data from y =
noise was 0.05). figure 2.1 shows the lines inferred from samples with three dif-
ferent distributions of x: x     unif(0, 1), x     n (0.5, 0.01), and x     unif(2, 3).
some distributions of x lead to similar (and similarly wrong) regression lines;
doing one estimate from all three data sets gives yet another answer.

2.2.1.1 r2: distraction or nuisance?

this little set-up, by the way, illustrates that r2 is not a stable property of the
distribution either. for the black points, r2 = 0.92; for the blue, r2 = 0.70; and
for the red, r2 = 0.77; and for the complete data, 0.96. other sets of xi values
would give other values for r2. note that while the global linear    t isn   t even a
good approximation anywhere in particular, it has the highest r2.

this kind of perversity can happen even in a completely linear set-up. suppose

2.2 shifting distributions, omitted variables, and transformations

47

   

figure 2.1 behavior of the conditional distribution y |x     n (
x, 0.052)
with di   erent distributions of x. the dots (in di   erent colors and shapes)
show three di   erent distributions of x (with sample values indicated by
colored    rug    ticks on the axes), plus the corresponding regression lines. the
solid line is the regression using all three sets of points, and the grey curve is
the true regression function. (see code example 1 for the code use to make
this    gure.) notice how di   erent distributions of x give rise to di   erent
slopes, each of which may make sense as a local approximation to the truth.

now that y = ax +  , and we happen to know a exactly. the variance of y will
be a2v [x] + v [ ]. the amount of variance our regression    explains        really,
the variance of our predictions     will be a2v [x]. so r2 = a2v[x]
a2v[x]+v[ ] . this goes
to zero as v [x]     0 and it goes to 1 as v [x]        . it thus has little to do with
the quality of the    t, and a lot to do with how spread out the regressor is.

0.00.51.01.52.02.53.00.00.51.01.52.02.53.0xylllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllunif[0,1]n(0.5, 0.01)unif[2,3]union of abovetrue regression line48

the truth about id75

x1 <- runif(100)
x2 <- rnorm(100, 0.5, 0.1)
x3 <- runif(100, 2, 3)
y1 <- sqrt(x1) + rnorm(length(x1), 0, 0.05)
y2 <- sqrt(x2) + rnorm(length(x2), 0, 0.05)
y3 <- sqrt(x3) + rnorm(length(x3), 0, 0.05)
plot(x1, y1, xlim = c(0, 3), ylim = c(0, 3), xlab = "x", ylab = "y", col = "darkgreen",

pch = 15)

rug(x1, side = 1, col = "darkgreen")
rug(y1, side = 2, col = "darkgreen")
points(x2, y2, pch = 16, col = "blue")
rug(x2, side = 1, col = "blue")
rug(y2, side = 2, col = "blue")
points(x3, y3, pch = 17, col = "red")
rug(x3, side = 1, col = "red")
rug(y3, side = 2, col = "red")
lm1 <- lm(y1 ~ x1)
lm2 <- lm(y2 ~ x2)
lm3 <- lm(y3 ~ x3)
abline(lm1, col = "darkgreen", lty = "dotted")
abline(lm2, col = "blue", lty = "dashed")
abline(lm3, col = "red", lty = "dotdash")
x.all <- c(x1, x2, x3)
y.all <- c(y1, y2, y3)
lm.all <- lm(y.all ~ x.all)
abline(lm.all, lty = "solid")
curve(sqrt(x), col = "grey", add = true)
legend("topleft", legend = c("unif[0,1]", "n(0.5, 0.01)", "unif[2,3]", "union of above",

"true regression line"), col = c("black", "blue", "red", "black", "grey"),
pch = c(15, 16, 17, na, na), lty = c("dotted", "dashed", "dotdash", "solid",

"solid"))

code example 1: code used to make figure 2.1.

notice also how easy it is to get a very high r2 even when the true model is

not linear!

2.2.2 omitted variables and shifting distributions

that the optimal regression coe   cients can change with the distribution of the
predictor features is annoying, but one could after all notice that the distribution
has shifted, and so be cautious about relying on the old regression. more subtle is
that the regression coe   cients can depend on variables which you do not measure,
and those can shift without your noticing anything.

mathematically, the issue is that

e(cid:104)

y | (cid:126)x

(cid:105)

= e(cid:104)e(cid:104)

y |z, (cid:126)x

(cid:105)

(cid:105)| (cid:126)x

(2.31)

now, if y is independent of z given (cid:126)x, then the extra conditioning in the inner
expectation does nothing and changing z doesn   t alter our predictions. but in
general there will be plenty of variables z which we don   t measure (so they   re

2.2 shifting distributions, omitted variables, and transformations

49

library(lattice)
library(mass)
x.z = mvrnorm(100, c(0, 0), matrix(c(1, 0.1, 0.1, 1), nrow = 2))
y = x.z[, 1] + x.z[, 2] + rnorm(100, 0, 0.1)
cloud(y ~ x.z[, 1] * x.z[, 2], xlab = "x", ylab = "z", zlab = "y", scales = list(arrows = false),

col.point = "black")

figure 2.2 scatter-plot of response variable y (vertical axis) and two
variables which in   uence it (horizontal axes): x, which is included in the
regression, and z, which is omitted. x and z have a correlation of +0.1.

not included in (cid:126)x) but which have some non-redundant information about the
response (so that y depends on z even conditional on (cid:126)x). if the distribution of
(cid:126)x given z changes, then the optimal regression of y on (cid:126)x should change too.
here   s an example. x and z are both n (0, 1), but with a positive correlation
of 0.1. in reality, y     n (x + z, 0.01). figure 2.2 shows a scatterplot of all three
variables together (n = 100).
now i change the correlation between x and z to    0.1. this leaves both

marginal distributions alone, and is barely detectable by eye (figure 2.3).

figure 2.4 shows just the x and y values from the two data sets, in black for
the points with a positive correlation between x and z, and in blue when the
correlation is negative. looking by eye at the points and at the axis tick-marks,
one sees that, as promised, there is very little change in the marginal distribution
of either variable. furthermore, the correlation between x and y doesn   t change
much, going only from 0.75 to 0.55. on the other hand, the regression lines are
noticeably di   erent. when cov [x, z] = 0.1, the slope of the regression line is 1.2
    high values for x tend to indicate high values for z, which also increases y .
when cov [x, z] =    0.1, the slope of the regression line is 0.74, since extreme
values of x are now signs that z is at the opposite extreme, bringing y closer

   3   2   1012   2   1012   202xzy50

the truth about id75

new.x.z = mvrnorm(100, c(0, 0), matrix(c(1, -0.1, -0.1, 1), nrow = 2))
new.y = new.x.z[, 1] + new.x.z[, 2] + rnorm(100, 0, 0.1)
cloud(new.y ~ new.x.z[, 1] * new.x.z[, 2], xlab = "x", ylab = "z", zlab = "y",

scales = list(arrows = false))

figure 2.3 as in figure 2.2, but shifting so that the correlation between x
and z is now    0.1, though the marginal distributions, and the distribution
of y given x and z, are unchanged.

back to its mean. but, to repeat, the di   erence is due to changing the correlation
between x and z, not how x and z themselves relate to y . if i regress y on x

and z, i get (cid:98)   = 0.99, 1 in the    rst case and (cid:98)   = 0.98, 1 in the second.

we   ll return to omitted variables when we look at causal id136 in part iii.

2.2.3 errors in variables

often, the predictor variables we can actually measure, (cid:126)x, are distorted versions
of some other variables (cid:126)u we wish we could measure, but can   t:

(cid:126)x = (cid:126)u + (cid:126)  

(2.32)

with (cid:126)   being some sort of noise. regressing y on (cid:126)x then gives us what   s called
an errors-in-variables problem.

in one sense, the errors-in-variables problem is huge. we are often much more
interested in the connections between actual variables in the real world, than
with our imperfect, noisy measurements of them. endless ink has been spilled, for
instance, on what determines students    test scores. one thing commonly thrown
into the regression     a feature included in (cid:126)x     is the income of children   s

   2   1012   2   1012   2   10123xzy2.2 shifting distributions, omitted variables, and transformations

51

figure 2.4 joint distribution of x and y from figure 2.2 (black, with a
positive correlation between x and z) and from figure 2.3 (blue, with a
negative correlation between x and z). tick-marks on the axes show the
marginal distributions, which are manifestly little-changed. (see
accompanying r    le for commands.)

families. but this is rarely measured precisely7, so what we are really interested
in     the relationship between actual income and school performance     is not
what our regression estimates. typically, adding noise to the input features makes

them less predictive of the response     in id75, it tends to push (cid:98)  

closer to zero than it would be if we could regress y on (cid:126)u .

on account of the error-in-variables problem, some people get very upset when
they see imprecisely-measured features as inputs to a regression. some of them,
in fact, demand that the input variables be measured exactly, with no noise
whatsoever. this position, however, is crazy, and indeed there   s a sense in which
errors-in-variables isn   t a problem at all. our earlier reasoning about how to
   nd the optimal linear predictor of y from (cid:126)x remains valid whether something
like eq. 2.32 is true or not. similarly, the reasoning in ch. 1 about the actual
regression function being the over-all optimal predictor, etc., is una   ected. if we
will continue to have (cid:126)x rather than (cid:126)u available to us for prediction, then eq. 2.32
is irrelevant for prediction. without better data, the relationship of y to (cid:126)u is just
one of the unanswerable questions the world is full of, as much as    what song the
sirens sang, or what name achilles took when he hid among the women   .

now, if you are willing to assume that (cid:126)   is a very well-behaved gaussian with
known variance, then there are solutions to the error-in-variables problem for
id75, i.e., ways of estimating the coe   cients you   d get from regressing

7 one common proxy is to ask the child what they think their family income is. (i didn   t believe that

either when i    rst read about it.)

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   3   2   10123   4   202xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll52

the truth about id75

y on (cid:126)u . i   m not going to go over them, partly because they   re in standard
textbooks, but mostly because the assumptions are hopelessly demanding.8

2.2.4 transformation

let   s look at a simple non-linear example, y |x     n (log x, 1). the problem
with smoothing data like this on to a straight line is that the true regression
curve isn   t straight, e [y |x = x] = log x. (figure 2.5.) this suggests replacing
the variables we have with ones where the relationship is linear, and then undoing
the transformation to get back to what we actually measure and care about.

we have two choices: we can transform the response y , or the predictor x. here
transforming the response would mean regressing exp y on x, and transforming
the predictor would mean regressing y on log x. both kinds of transformations
can be worth trying. the best reasons to use one kind rather than another are
those that come from subject-matter knowledge: if we have good reason to think
that that f (y ) =   x +  , then it can make a lot of sense to transform y . if
genuine subject-matter considerations are not available, however, my experience
is that transforming the predictors, rather than the response, is a better bet, for
several reasons.
1. mathematically, e [f (y )] (cid:54)= f (e [y ]). a mean-squared optimal prediction of
f (y ) is not necessarily close to the transformation of an optimal prediction of
y . and y is, presumably, what we really want to predict.

2. imagine that y =

x + log z. there   s not going to be any particularly nice
transformation of y that makes everything linear, though there will be trans-
formations of the features. this generalizes to more complicated models with
features built from multiple covariates.

   

3. suppose that we are in luck and y =   (x) +  , with   independent of x,
and gaussian, so all the usual default calculations about statistical id136
apply. then it will generally not be the case that f (y ) = s(x) +   , with   
a gaussian random variable independent of x. in other words, transforming
y completely messes up the noise model. (consider the simple case where
we take the logarithm of y . gaussian noise after the transformation implies
log-normal noise before the transformation. conversely, gaussian noise before
the transformation implies a very weird, nameless noise distribution after the
transformation.)

figure 2.6 shows the e   ect of these transformations. here transforming the
predictor does, indeed, work out more nicely; but of course i chose the example
so that it does so.

to expand on that last point, imagine a model like so:

8 non-parametric error-in-variable methods are an active topic of research (carroll et al., 2009).

j=1

  ((cid:126)x) =

cjfj((cid:126)x)

(2.33)

q(cid:88)

2.2 shifting distributions, omitted variables, and transformations

53

x <- runif(100)
y <- rnorm(100, mean = log(x), sd = 1)
plot(y ~ x)
curve(log(x), add = true, col = "grey")
abline(lm(y ~ x))

figure 2.5 sample of data for y |x     n (log x, 1). (here x     unif(0, 1),
and all logs are natural logs.) the true, logarithmic regression curve is
shown in grey (because it   s not really observable), and the id75
   t is shown in black.

if we know the functions fj, we can estimate the optimal values of the coe   cients
cj by least squares     this is a regression of the response on new features, which
happen to be de   ned in terms of the old ones. because the parameters are out-
side the functions, that part of the estimation works just like id75.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0   6   4   202xy54

the truth about id75

figure 2.6 transforming the predictor (left column) and the response
(right) in the data from figure 2.5, shown in both the transformed
coordinates (top) and the original coordinates (middle). the bottom    gure
super-imposes the two estimated curves (transformed x in black,
transformed y in blue). the true regression curve is always in grey. (r code
deliberately omitted; reproducing this is exercise 2.4.)

models embraced under the heading of eq. 2.33 include id75s with
interactions between the regressors (set fj = xixk, for various combinations of
i and k), and polynomial regression. there is however nothing magical about
using products and powers of the regressors; we could regress y on sin x, sin 2x,
sin 3x, etc.

to apply models like eq. 2.33, we can either (a)    x the functions fj in advance,
based on guesses about what should be good features for this problem; (b)    x the

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   6   4   20   6   4   202log(x)yllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0   6   4   202xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.002468xexp(y)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0   6   4   202xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0   6   4   202xy2.3 adding probabilistic assumptions

55

functions in advance by always using some    library    of mathematically convenient
functions, like polynomials or trigonometric functions; or (c) try to    nd good
functions from the data. option (c) takes us beyond the realm of id75
as such, into things like splines (chapter 7) and additive models (chapter 8).
it is also possible to search for transformations of both sides of a regression model;
see breiman and friedman (1985) and, for an r implementation, spector et al.
(2013).

2.3 adding probabilistic assumptions

the usual treatment of id75 adds many more probabilistic assump-
tions, namely that

y | (cid:126)x     n ( (cid:126)x      ,   2)

(2.34)

and that y values are independent conditional on their (cid:126)x values. so now we
are assuming that the regression function is exactly linear; we are assuming that
at each (cid:126)x the scatter of y around the regression function is gaussian; we are
assuming that the variance of this scatter is constant; and we are assuming that
there is no dependence between this scatter and anything else.

none of these assumptions was needed in deriving the optimal linear predictor.
none of them is so mild that it should go without comment or without at least
some attempt at testing.

leaving that aside just for the moment, why make those assumptions? as
you know from your earlier classes, they let us write down the likelihood of the
observed responses y1, y2, . . . yn (conditional on the covariates (cid:126)x1, . . . (cid:126)xn), and then
estimate    and   2 by maximizing this likelihood. as you also know, the maximum
likelihood estimate of    is exactly the same as the    obtained by minimizing the
residual sum of squares. this coincidence would not hold in other models, with
non-gaussian noise.

we saw earlier that (cid:98)   is consistent under comparatively weak assumptions
make the extra statistical assumptions, so that (cid:98)   is also the maximum likelihood

    that it converges to the optimal coe   cients. but then there might, possibly,
still be other estimators are also consistent, but which converge faster. if we

estimate, we can lay that worry to rest. the id113 is generically (and certainly
here!) asymptotically e   cient, meaning that it converges as fast as any other
consistent estimator, at least in the long run. so we are not, so to speak, wasting
any of our data by using the id113.
a further advantage of the id113 is that, as n        , its sampling distribution is
itself a gaussian, centered around the true parameter values. this lets us calculate
standard errors and con   dence intervals quite easily. here, with the gaussian
assumptions, much more exact statements can be made about the distribution of

(cid:98)   around   . you can    nd the formulas in any textbook on regression, so i won   t

get into that.

we can also use a general property of id113s for model testing. suppose we have
two classes of models,     and   .     is the general case, with p parameters, and   

56

the truth about id75

is a special case, where some of those parameters are constrained, but q < p of
them are left free to be estimated from the data. the constrained model class   
is then nested within    . say that the id113s with and without the constraints

are, respectively, (cid:98)   and (cid:98)  , so the maximum log-likelihoods are l((cid:98)  ) and l((cid:98)  ).
because it   s a maximum over a larger parameter space, l((cid:98)  )     l((cid:98)  ). on the

other hand, if the true model really is in   , we   d expect the constrained and
unconstrained estimates to be converging. it turns out that the di   erence in log-
likelihoods has an asymptotic distribution which doesn   t depend on any of the
model details, namely

(cid:104)

(cid:105)
l((cid:98)  )     l((cid:98)  )

2

;   2

p   q

(2.35)

that is, a   2 distribution with one degree of freedom for each extra parameter
in     (that   s why they   re called    degrees of freedom   ).9

this approach can be used to test particular restrictions on the model, and so
it is sometimes used to assess whether certain variables in   uence the response.
this, however, gets us into the concerns of the next section.

2.3.1 examine the residuals

by construction, the errors of the optimal linear predictor have expectation 0
and are uncorrelated with the regressors. also by construction, the residuals of a
   tted id75 have sample mean 0, and are uncorrelated, in the sample,
with the regressors.

if the usual probabilistic assumptions hold, however, the errors of the optimal

linear predictor have many other properties as well.

1. the errors have a gaussian distribution at each (cid:126)x.
2. the errors have the same gaussian distribution at each (cid:126)x, i.e., they are in-
dependent of the regressors. in particular, they must have the same variance
(i.e., they must be homoskedastic).

3. the errors are independent of each other. in particular, they must be uncor-

related with each other.

when these properties     gaussianity, homoskedasticity, lack of correlation    
hold, we say that the errors are white noise. they imply strongly related prop-
erties for the residuals: the residuals should be gaussian, with variances and
covariances given by the hat matrix, or more speci   cally by i     x(xt x)   1xt
(  1.5.3.2). this means that the residuals will not be exactly white noise, but they
should be close to white noise. you should check this! if you    nd residuals which
are a long way from being white noise, you should be extremely suspicious of
your model. these tests are much more important than checking whether the
coe   cients are signi   cantly di   erent from zero.

9 if you assume the noise is gaussian, the left-hand side of eq. 2.35 can be written in terms of various
residual sums of squares. however, the equation itself remains valid under other noise distributions,
which just change the form of the likelihood function. see appendix i.

2.3 adding probabilistic assumptions

57

every time someone uses id75 with the standard assumptions for
id136 and does not test whether the residuals are white noise, an angel loses
its wings.

2.3.2 on signi   cant coe   cients

if all the usual distributional assumptions hold, then t-tests can be used to decide
whether particular coe   cients are statistically-signi   cantly di   erent from zero.
pretty much any piece of statistical software, r very much included, reports the
results of these tests automatically. it is far too common to seriously over-interpret
those results, for a variety of reasons.

begin with exactly what hypothesis is being tested when r (or whatever) runs
those t-tests. say, without loss of generality, that there are p predictor variables,
(cid:126)x = (x1, . . . xp), and that we are testing the coe   cient on xp. then the null
hypothesis is not just      p = 0   , but      p = 0 in a linear, gaussian-noise model
which also includes x1, . . . xp   1, and nothing else   . the alternative hypothesis
is not just      p (cid:54)= 0   , but      p (cid:54)= 0 in a linear, gaussian-noise model which also
includes x1, . . . xp   1, but nothing else   . the optimal linear coe   cient on xp will
depend not just on the relationship between xp and the response y , but also on
which other variables are included in the model. the test checks whether adding
xp really improves predictions more than would be expected, under all these
assumptions, if one is already using all the other variables, and only those other
variables. it does not, cannot, test whether xp is important in any absolute sense.
even if you are willing to say    yes, all i really want to know about this variable
is whether adding it to the model really helps me predict in a linear approxima-
tion   , remember that the question which a t-test answers is whether adding that
variable will help at all. of course, as you know from your regression class, and
as we   ll see in more detail in chapter 3, expanding the model never hurts its
performance on the training data. the point of the t-test is to gauge whether
the improvement in prediction is small enough to be due to chance, or so large,
compared to what noise could produce, that one could con   dently say the variable
adds some predictive ability. this has several implications which are insu   ciently
appreciated among users.

in the    rst place, tests on individual coe   cients can seem to contradict tests on
groups of coe   cients. adding multiple variables to the model could signi   cantly
improve the    t (as checked by, say, a partial f test), even if none of the coe   cients
is signi   cant on its own. in fact, every single coe   cient in the model could be
insigni   cant, while the model as a whole is highly signi   cant (i.e., better than a
   at line).

statistically signi   cant. remember that the t-statistic is (cid:98)  i/se((cid:98)  i), the ratio of
(cid:105)
the estimated coe   cient to its standard error. we saw above that v(cid:104)(cid:98)  |x = x
=
   1     n   1  2v   1. this means that the standard errors will shrink as
n (n   1xt x)
the sample size grows, so more and more variables will become signi   cant as we

in the second place, it   s worth thinking about which variables will show up as

  2

58

the truth about id75

get more data     but how much data we collect is irrelevant to how the process
we   re studying actually works. moreover, at a    xed sample size, the coe   cients
with smaller standard errors will tend to be the ones whose variables have more
variance, and whose variables are less correlated with the other predictors. high
input variance and low correlation help us estimate the coe   cient precisely, but,
again, they have nothing to do with whether the input variable actually in   uences
the response a lot.

to sum up, it is never the case that statistical signi   cance is the same as
scienti   c, real-world signi   cance. the most important variables are not those with
the largest-magnitude t statistics or smallest p-values. statistical signi   cance is
always about what    signals    can be picked out clearly from background noise10.
in the case of id75 coe   cients, statistical signi   cance runs together
the size of the coe   cients, how bad the id75 model is, the sample
size, the variance in the input variable, and the correlation of that variable with
all the others.

of course, even the limited    does it help linear predictions enough to bother
with?    utility of the usual t-test (and f -test) calculations goes away if the stan-
dard distributional assumptions do not hold, so that the calculated p-values are
just wrong. one can sometimes get away with using id64 (chapter 6)
to get accurate p-values for standard tests under non-standard conditions.

2.4 id75 is not the philosopher   s stone

the philosopher   s stone, remember, was supposed to be able to transmute base
metals (e.g., lead) into the perfect metal, gold (eliade, 1971). many people treat
id75 as though it had a similar ability to transmute a correlation
matrix into a scienti   c theory. in particular, people often argue that:

1. because a variable has a signi   cant regression coe   cient, it must in   uence the

response;

2. because a variable has an insigni   cant regression coe   cient, it must not in   u-

ence the response;

3. if the input variables change, we can predict how much the response will change

by plugging in to the regression.

all of this is wrong, or at best right only under very particular circumstances.

we have already seen examples where in   uential variables have regression coef-
   cients of zero. we have also seen examples of situations where a variable with no
in   uence has a non-zero coe   cient (e.g., because it is correlated with an omitted
variable which does have in   uence). if there are no nonlinearities and if there are
no omitted in   uential variables and if the noise terms are always independent of
the predictor variables, are we good?

10 in retrospect, it might have been clearer to say    statistically detectable    rather than    statistically

signi   cant   .

2.4 id75 is not the philosopher   s stone

59

no. remember from equation 2.5 that the optimal regression coe   cients de-
pend on both the marginal distribution of the predictors and the joint distribution
(covariances) of the response and the predictors. there is no reason whatsoever to
suppose that if we change the system, this will leave the conditional distribution
of the response alone.

a simple example may drive the point home. suppose we surveyed all the cars
in pittsburgh, recording the maximum speed they reach over a week, and how
often they are waxed and polished. i don   t think anyone doubts that there will
be a positive correlation here, and in fact that there will be a positive regression
coe   cient, even if we add in many other variables as predictors. let us even
postulate that the relationship is linear (perhaps after a suitable transformation).
would anyone believe that polishing cars will make them go faster? manifestly
not. but this is exactly how people interpret regressions in all kinds of applied
   elds     instead of saying polishing makes cars go faster, it might be saying
that receiving targeted ads makes customers buy more, or that consuming dairy
foods makes diabetes progress faster, or . . . . those claims might be true, but the
regressions could easily come out the same way were the claims false. hence, the
regression results provide little or no evidence for the claims.

similar remarks apply to the idea of using regression to    control for    extra
variables. if we are interested in the relationship between one predictor, or a few
predictors, and the response, it is common to add a bunch of other variables to
the regression, to check both whether the apparent relationship might be due to
correlations with something else, and to    control for    those other variables. the
regression coe   cient is interpreted as how much the response would change, on
average, if the predictor variable were increased by one unit,    holding everything
else constant   . there is a very particular sense in which this is true: it   s a predic-
tion about the di   erence in expected responses (conditional on the given values
for the other predictors), assuming that the form of the regression model is right,
and that observations are randomly drawn from the same population we used to
   t the regression.

in a word, what regression does is probabilistic prediction. it says what will
happen if we keep drawing from the same population, but select a sub-set of
the observations, namely those with given values of the regressors. a causal or
counter-factual prediction would say what would happen if we (or someone)
made those variables take those values. sometimes there   s no di   erence between
selection and intervention, in which case regression works as a tool for causal
id13611; but in general there is. probabilistic prediction is a worthwhile en-
deavor, but it   s important to be clear that this is what regression does. there are
techniques for doing causal prediction, which we will explore in part iii.

every time someone thoughtlessly uses regression for causal id136, an angel
not only loses its wings, but is cast out of heaven and falls in extremest agony
into the everlasting    re.

11 in particular, if our model was estimated from data where someone assigned values of the predictor
variables in a way which breaks possible dependencies with omitted variables and noise     either by
randomization or by experimental control     then regression can, in fact, work for causal id136.

60

the truth about id75

2.5 further reading

if you would like to read a lot more     about 400 pages more     about linear
regression from this perspective, see the truth about id75, at http:
//www.stat.cmu.edu/~cshalizi/talr/. that manuscript began as class notes
for the class before this one, and has some overlap.

there are many excellent textbooks on id75. among them, i would
mention weisberg (1985) for general statistical good sense, along with faraway
(2004) for r practicalities, and hastie et al. (2009) for emphasizing connections
to more advanced methods. berk (2004) omits the details those books cover, but
is superb on the big picture, and especially on what must be assumed in order
to do certain things with id75 and what cannot be done under any
assumption.

for some of the story of how the usual probabilistic assumptions came to have
that status, see, e.g., lehmann (2008). on the severe issues which arise for the
usual inferential formulas when the model is incorrect, see buja et al. (2014).
id75 is a special case of both additive models (chapter 8), and of
locally linear models (  10.5). in most practical situations, additive models are a
better idea than linear ones.

historical notes

because id75 is such a big part of statistical practice, its history has
been extensively treated in general histories of statistics, such as stigler (1986)
and porter (1986). i would particularly recommend klein (1997) for a careful
account of how regression, on its face a method for doing comparisons at one
time across a population, came to be used to study causality and dynamics. the
paper by lehmann (2008) mentioned earlier is also informative.

2.1

1. write the expected squared error of a linear predictor with slopes (cid:126)b and intercept b0

exercises

as a function of those coe   cients.

2. find the derivatives of the expected squared error with respect to all the coe   cients.
3. show that when we set all the derivatives to zero, the solutions are eq. 2.5 and 2.6.

show that the expected error of the optimal linear predictor, e(cid:104)

y     (cid:126)x      

, is zero.

(cid:105)

2.2

2.3 convince yourself that if the real regression function is linear,    does not depend on the
marginal distribution of x. you may want to start with the case of one predictor variable.

   
2.4 run the code from figure 2.5. then replicate the plots in figure 2.6.
2.5 which kind of transformation is superior for the model where y |x     n (

x, 1)?

3

evaluating statistical models: error and

id136

3.1 what are statistical models for? summaries, forecasts,

simulators

there are (at least) three ways we can use statistical models in data analysis: as
summaries of the data, as predictors, and as simulators.

the least demanding use of a model is to summarize the data     to use it for
data reduction, or compression. just as the sample mean or sample quan-
tiles can be descriptive statistics, recording some features of the data and saying
nothing about a population or a generative process, we could use estimates of a
model   s parameters as descriptive summaries. rather than remembering all the
points on a scatter-plot, say, we   d just remember what the ols regression surface
was.

it   s hard to be wrong about a summary, unless we just make a mistake. (it
may not be helpful for us later, but that   s di   erent.) when we say    the slope
which minimized the sum of squares was 4.02   , we make no claims about any-
thing but the training data. that statement relies on no assumptions, beyond our
calculating correctly. but it also asserts nothing about the rest of the world. as
soon as we try to connect our training data to anything else, we start relying on
assumptions, and we run the risk of being wrong.

probably the most common connection to want to make is to say what other
data will look like     to make predictions. in a statistical model, with random
variables, we do not anticipate that our predictions will ever be exactly right, but
we also anticipate that our mistakes will show stable probabilistic patterns. we
can evaluate predictions based on those patterns of error     how big is our typical
mistake? are we biased in a particular direction? do we make a lot of little errors
or a few huge ones?

statistical id136 about model parameters     estimation and hypothesis test-
ing     can be seen as a kind of prediction, extrapolating from what we saw in a
small piece of data to what we would see in the whole population, or whole pro-
cess. when we estimate the regression coe   cient   b = 4.02, that involves predicting
new values of the dependent variable, but also predicting that if we repeated the
experiment and re-estimated   b, we   d get a value close to 4.02.

using a model to summarize old data, or to predict new data, doesn   t commit
us to assuming that the model describes the process which generates the data.
but we often want to do that, because we want to interpret parts of the model

61

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

62

model evaluation

as aspects of the real world. we think that in neighborhoods where people have
more money, they spend more on houses     perhaps each extra $1000 in income
translates into an extra $4020 in house prices. used this way, statistical models
become stories about how the data were generated. if they are accurate, we
should be able to use them to simulate that process, to step through it and
produce something that looks, probabilistically, just like the actual data. this is
often what people have in mind when they talk about scienti   c models, rather
than just statistical ones.

an example: if you want to predict where in the night sky the planets will be,
you can actually do very well with a model where the earth is at the center of
the universe, and the sun and everything else revolve around it. you can even
estimate, from data, how fast mars (for example) goes around the earth, or where,
in this model, it should be tonight. but, since the earth is not at the center of the
solar system, those parameters don   t actually refer to anything in reality. they
are just mathematical    ctions. on the other hand, we can also predict where the
planets will appear in the sky using models where all the planets orbit the sun,
and the parameters of the orbit of mars in that model do refer to reality.1

this chapter focuses on evaluating predictions, for three reasons. first, often
we just want prediction. second, if a model can   t even predict well, it   s hard to
see how it could be right scienti   cally. third, often the best way of checking a
scienti   c model is to turn some of its implications into statistical predictions.

3.2 errors, in and out of sample

with any predictive model, we can gauge how well it works by looking at its errors.
we want these to be small; if they can   t be small all the time we   d like them to
be small on average. we may also want them to be patternless or unsystematic
(because if there was a pattern to them, why not adjust for that, and make
smaller mistakes). we   ll come back to patterns in errors later, when we look at
speci   cation testing (chapter 9). for now, we   ll concentrate on the size of the
errors.

to be a little more mathematical, we have a data set with points zn = z1, z2, . . . zn.

(for regression problems, think of each data point as the pair of input and output
values, so zi = (xi, yi), with xi possibly a vector.) we also have various possible
models, each with di   erent parameter settings, conventionally written   . for re-
gression,    tells us which regression function to use, so m  (x) or m(x;   ) is the
prediction we make at point x with parameters set to   . finally, we have a loss
function l which tells us how big the error is when we use a certain    on a
certain data point, l(z,   ). for mean-squared error, this would just be

l(z,   ) = (y     m  (x))2

(3.1)

1 we can be pretty sure of this, because we use our parameter estimates to send our robots to mars,

and they get there.

3.2 errors, in and out of sample

but we could also use the mean absolute error

l(z,   ) = |y     m  (x)|

63

(3.2)

or many other id168s. sometimes we will actually be able to measure how
costly our mistakes are, in dollars or harm to patients. if we had a model which
gave us a distribution for the data, then p  (z) would a id203 density at z,
and a typical id168 would be the negative log-likelihood,     log m  (z). no
matter what the id168 is, i   ll abbreviate the sample average of the loss
over the whole data set by l(zn,   ).

what we would like, ideally, is a predictive model which has zero error on

true, ideal model has non-zero error.2 this corresponds to the    rst,   2
in the bias-variance decomposition, eq. 1.28 from chapter 1.

future data. we basically never achieve this:
    the world just really is a noisy and stochastic place, and this means even the
x, term
    our models are usually more or less mis-speci   ed, or, in plain words, wrong.
we hardly ever get the functional form of the regression, the distribution of
the noise, the form of the causal dependence between two factors, etc., exactly
right.3 this is the origin of the bias term in the bias-variance decomposition.
of course we can get any of the details in the model speci   cation more or less
wrong, and we   d prefer to be less wrong.
    our models are never perfectly estimated. even if our data come from a perfect
iid source, we only ever have a    nite sample, and so our parameter estimates
are (almost!) never quite the true, in   nite-limit values. this is the origin of
the variance term in the bias-variance decomposition. but as we get more and
more data, the sample should become more and more representative of the
whole process, and estimates should converge too.

so, because our models are    awed, we have limited data and the world is stochas-
tic, we cannot expect even the best model to have zero error. instead, we would
like to minimize the expected error, or risk, or generalization error, on new
data.

what we would like to do is to minimize the risk or expected loss

e [l(z,   )] =

l(z,   )p(z)dz

(3.3)

(cid:90)

to do this, however, we   d have to be able to calculate that expectation. doing
that would mean knowing the distribution of z     the joint distribution of x and
y , for the regression problem. since we don   t know the true joint distribution,
we need to approximate it somehow.

a natural approximation is to use our training data zn. for each possible model

2 this is so even if you believe in some kind of ultimate determinism, because the variables we plug in

to our predictive models are not complete descriptions of the physical state of the universe, but
rather immensely coarser, and this coarseness shows up as randomness.

3 except maybe in fundamental physics, and even there our predictions are about our fundamental

theories in the context of experimental set-ups, which we never model in complete detail.

64

model evaluation

  , we can could calculate the sample mean of the error on the data, l(zn,   ), called
the in-sample loss or the empirical risk. the simplest strategy for estimation
is then to pick the model, the value of   , which minimizes the in-sample loss.
this strategy is imaginatively called empirical risk minimization. formally,

(cid:99)  n     argmin

       

l(zn,   )

(3.4)

this means picking the regression which minimizes the sum of squared errors,
or the density with the highest likelihood4. this is what you   ve usually done
in statistics courses so far, and it   s very natural, but it does have some issues,
notably optimism and over-   tting.

the problem of optimism comes from the fact that our training data isn   t
perfectly representative. the in-sample loss is a sample average. by the law of
large numbers, then, we anticipate that, for each   ,
l(zn,   )     e [l(z,   )]

(3.5)
as n        . this means that, with enough data, the in-sample error is a good
approximation to the generalization error of any given model   . (big samples are
representative of the underlying population or process.) but this does not mean
that the in-sample performance of      tells us how well it will generalize, because
we purposely picked it to match the training data zn. to see this, notice that the
in-sample loss equals the risk plus sampling noise:

l(zn,   ) = e [l(z,   )] +   n(  )

(3.6)

here   n(  ) is a random term which has mean zero, and represents the e   ects
of having only a    nite quantity of data, of size n, rather than the complete
id203 distribution. (i write it   n(  ) as a reminder that di   erent values of
   are going to be a   ected di   erently by the same sampling    uctuations.) the
problem, then, is that the model which minimizes the in-sample loss could be one
with good generalization performance (e [l(z,   )] is small), or it could be one
which got very lucky (  n(  ) was large and negative):

(e [l(z,   )] +   n(  ))

(3.7)

we only want to minimize e [l(z,   )], but we can   t separate it from   n(  ), so

we   re almost surely going to end up picking a (cid:99)  n which was more or less lucky

(  n < 0) as well as good (e [l(z,   )] small). this is the reason why picking the
model which best    ts the data tends to exaggerate how well it will do in the
future (figure 3.1).
again, by the law of large numbers   n(  )     0 for each   , but now we need
to worry about how fast it   s going to zero, and whether that rate depends on
  . suppose we knew that min     n(  )     0, or max   |  n(  )|     0. then it would

4 remember, maximizing the likelihood is the same as maximizing the log-likelihood, because log is
an increasing function. therefore maximizing the likelihood is the same as minimizing the negative
log-likelihood.

(cid:99)  n = argmin

       

3.2 errors, in and out of sample

65

n <- 20
theta <- 5
x <- runif(n)
y <- x * theta + rnorm(n)
empirical.risk <- function(b) {

mean((y - b * x)^2)

}
true.risk <- function(b) {

1 + (theta - b)^2 * (0.5^2 + 1/12)

}
curve(vectorize(empirical.risk)(x), from = 0, to = 2 * theta, xlab = "regression slope",

ylab = "mse risk")

curve(true.risk, add = true, col = "grey")

figure 3.1 empirical and generalization risk for regression through the
origin, y =   x +  ,       n (0, 1), with true    = 5, and x     unif(0, 1). black:
mse on a particular sample (n = 20) as a function of slope, minimized at
     = 4.37. grey: true or generalization risk (exercise 3.2). the gap between
the curves is the text   s   n(  ).

0246810123456regression slopemse risk66

model evaluation

follow that   n((cid:99)  n)     0, and the over-optimism in using the in-sample error to
approximate the generalization error would at least be shrinking. if we knew
how fast max   |  n(  )| was going to zero, we could even say something about how
much bigger the true risk was likely to be. a lot of more advanced statistics and
machine learning theory is thus about uniform laws of large numbers (showing
max   |  n(  )|     0) and rates of convergence.
learning theory is a beautiful, deep, and practically important subject, but also
a subtle and involved one. (see   3.6 for references.) to stick closer to analyzing
real data, and to not turn this into an advanced id203 class, i will only
talk about some more-or-less heuristic methods, which are good enough for many
purposes.

3.3 over-fitting and model selection

the big problem with using the in-sample error is related to over-optimism, but
at once trickier to grasp and more important. this is the problem of over-   tting.
to illustrate it, let   s start with figure 3.2. this has the twenty x values from a
gaussian distribution, and y = 7x 2     0.5x +  ,       n (0, 1). that is, the true
regression curve is a parabola, with additive and independent gaussian noise.
let   s try    tting this     but pretend that we didn   t know that the curve was
a parabola. we   ll try    tting polynomials of di   erent degrees in x     degree 0
(a    at line), degree 1 (a id75), degree 2 (quadratic regression), up
through degree 9. figure 3.3 shows the data with the polynomial curves, and
figure 3.4 shows the in-sample mean squared error as a function of the degree of
the polynomial.

notice that the in-sample error goes down as the degree of the polynomial
increases; it has to. every polynomial of degree p can also be written as a poly-
nomial of degree p+1 (with a zero coe   cient for xp+1), so going to a higher-degree
model can only reduce the in-sample error. quite generally, in fact, as one uses
more and more complex and    exible models, the in-sample error will get smaller
and smaller.5

things are quite di   erent if we turn to the generalization error. in principle, i
could calculate that for any of the models, since i know the true distribution, but
it would involve calculating things like e [x 18], which won   t be very illuminating.
instead, i will just draw a lot more data from the same source, twenty thousand
data points in fact, and use the error of the old models on the new data as their
generalization error6. the results are in figure 3.5.

what is happening here is that the higher-degree polynomials     beyond degree
2     are not just a little optimistic about how well they    t, they are wildly

5 in fact, since there are only 20 data points, they could all be    t exactly if the degree of the
polynomials went up to 19. (remember that any two points de   ne a line, any three points a
parabola, etc.     p + 1 points de   ne a polynomial of degree p which passes through them.)

6 this works, yet again, because of the law of large numbers. in chapters 5 and especially 6, we will

see much more about replacing complicated probabilistic calculations with simple simulations, an
idea sometimes called the    monte carlo method   .

3.3 over-fitting and model selection

67

x = rnorm(20)
y = 7 * x^2 - 0.5 * x + rnorm(20)
plot(x, y)
curve(7 * x^2 - 0.5 * x, col = "grey", add = true)

figure 3.2 scatter-plot showing sample data and the true, quadratic
regression curve (grey parabola).

over-optimistic. the models which seemed to do notably better than a quadratic
actually do much, much worse. if we picked a polynomial regression model based
on in-sample    t, we   d chose the highest-degree polynomial available, and su   er
for it.

in this example, the more complicated models     the higher-degree polynomi-
als, with more terms and parameters     were not actually    tting the generalizable

llllllllllllllllllll   2   101201020304050xy68

model evaluation

plot(x, y)
poly.formulae <- c("y~1", paste("y ~ poly(x,", 1:9, ")", sep = ""))
poly.formulae <- sapply(poly.formulae, as.formula)
df.plot <- data.frame(x = seq(min(x), max(x), length.out = 200))
fitted.models <- list(length = length(poly.formulae))
for (model_index in 1:length(poly.formulae)) {

fm <- lm(formula = poly.formulae[[model_index]])
lines(df.plot$x, predict(fm, newdata = df.plot), lty = model_index)
fitted.models[[model_index]] <- fm

}

figure 3.3 twenty training data points (dots), and ten di   erent    tted
regression lines (polynomials of degree 0 to 9, indicated by di   erent line
types). r notes: the poly command constructs orthogonal (uncorrelated)
polynomials of the speci   ed degree from its    rst argument; regressing on them is
conceptually equivalent to regressing on 1, x, x2, . . . xdegree, but more numerically
stable. (see ?poly.) this builds a vector of model formulae and then    ts each one
in turn, storing the    tted models in a new list.

llllllllllllllllllll   2   101201020304050xy3.3 over-fitting and model selection

69

mse.q <- sapply(fitted.models, function(mdl) {

mean(residuals(mdl)^2)

})
plot(0:9, mse.q, type = "b", xlab = "polynomial degree", ylab = "mean squared error",

log = "y")

figure 3.4 empirical mse vs. degree of polynomial for the data from the
previous    gure. note the logarithmic scale for the vertical axis.

features of the data. instead, they were    tting the sampling noise, the accidents
which don   t repeat. that is, the more complicated models over-   t the data.
in terms of our earlier notation,    is bigger for the more    exible models. the
model which does best here is the quadratic, because the true regression func-
tion happens to be of that form. the more powerful, more    exible, higher-degree

llllllllll02468125102050100200polynomial degreemean squared error70

model evaluation

polynomials were able to get closer to the training data, but that just meant
matching the noise better. in terms of the bias-variance decomposition, the bias
shrinks with the model degree, but the variance of estimation grows.

notice that the models of degrees 0 and 1 also do worse than the quadratic
model     their problem is not over-   tting but under-   tting; they would do better
if they were more    exible. plots of generalization error like this usually have a
minimum. if we have a choice of models     if we need to do model selection    
we would like to    nd the minimum. even if we do not have a choice of models,
we might like to know how big the gap between our in-sample error and our
generalization error is likely to be.

there is nothing special about polynomials here. all of the same lessons apply
to variable selection in id75, to k-nearest neighbors (where we need
to choose k), to kernel regression (where we need to choose the bandwidth), and
to other methods we   ll see later. in every case, there is going to be a minimum
for the generalization error curve, which we   d like to    nd.

(a minimum with respect to what, though? in figure 3.5, the horizontal axis
is the model degree, which here is the number of parameters [minus one for the
intercept]. more generally, however, what we care about is some measure of how
complex the model space is, which is not necessarily the same thing as the number
of parameters. what   s more relevant is how    exible the class of models is, how
many di   erent functions it can approximate. linear polynomials can approximate
a smaller set of functions than quadratics can, so the latter are more complex,
or have higher capacity. more advanced learning theory has a number of ways
of quantifying this, but the details get pretty arcane, and we will just use the
concept of complexity or capacity informally.)

3.4 cross-validation

e(cid:104)

the most straightforward way to    nd the generalization error would be to do
what i did above, and to use fresh, independent data from the same source    
a testing or validation data-set. call this z(cid:48)
m, as opposed to our training data

zn. we    t our model to zn, and get(cid:99)  n. the loss of this on the validation data is
m, is independent of(cid:99)  n. so

where now the sampling noise on the validation set,   (cid:48)
this gives us an unbiased estimate of the generalization error, and, if m is large,
a precise one. if we need to select one model from among many, we can pick the
one which does best on the validation data, with con   dence that we are not just
over-   tting.

(cid:105)
l(z,(cid:99)  n)

m((cid:99)  n)

+   (cid:48)

(3.8)

the problem with this approach is that we absolutely, positively, cannot use any
of the validation data in estimating the model. since collecting data is expensive
    it takes time, e   ort, and usually money, organization, e   ort and skill     this
means getting a validation data set is expensive, and we often won   t have that
luxury.

3.4 cross-validation

71

capa <- na.omit(read.csv("http://www.stat.cmu.edu/~cshalizi/uada/13/hw/01/calif_penn_2011.csv"))
half_a <- sample(1:nrow(capa), size = nrow(capa)/2, replace = false)
half_b <- setdiff(1:nrow(capa), half_a)
small_formula = "median_house_value ~ median_household_income"
large_formula = "median_house_value ~ median_household_income + median_rooms"
small_formula <- as.formula(small_formula)
large_formula <- as.formula(large_formula)
msmall <- lm(small_formula, data = capa, subset = half_a)
mlarge <- lm(large_formula, data = capa, subset = half_a)
in.sample.mse <- function(model) {

mean(residuals(model)^2)

}
new.sample.mse <- function(model, half) {

test <- capa[half, ]
predictions <- predict(model, newdata = test)
return(mean((test$median_house_value - predictions)^2))

}

code example 2: code used to generate the numbers in figure 3.7.

3.4.1 data splitting

the next logical step, however, is to realize that we don   t strictly need a separate
validation set. we can just take our data and split it ourselves into training and
testing sets. if we divide the data into two parts at random, we ensure that they
have (as much as possible) the same distribution, and that they are independent
of each other. then we can act just as though we had a real validation set. fitting
to one part of the data, and evaluating on the other, gives us an unbiased estimate
of generalization error. of course it doesn   t matter which half of the data is used
to train and which half is used to test.
  a.13, and code example 2 shows the code used to make figure 3.7.

figure 3.7 illustrates the idea with a bit of the data and linear models from

3.4.2 k-fold cross-validation (cv)

the problem with data splitting is that, while it   s an unbiased estimate of the
risk, it is often a very noisy one. if we split the data evenly, then the test set has
n/2 data points     we   ve cut in half the number of sample points we   re averaging
over. it would be nice if we could reduce that noise somewhat, especially if we
are going to use this for model selection.

one solution to this, which is pretty much the industry standard, is what   s
called k-fold cross-validation. pick a small integer k, usually 5 or 10, and
divide the data at random into k equally-sized subsets. (the subsets are often
called    folds   .) take the    rst subset and make it the test set;    t the models to
the rest of the data, and evaluate their predictions on the test set. now make
the second subset the test set and the rest of the training sets. repeat until each
subset has been the test set. at the end, average the performance across test sets.
this is the cross-validated estimate of generalization error for each model. model

72

model evaluation

cv.lm <- function(data, formulae, nfolds = 5) {

data <- na.omit(data)
formulae <- sapply(formulae, as.formula)
n <- nrow(data)
fold.labels <- sample(rep(1:nfolds, length.out = n))
mses <- matrix(na, nrow = nfolds, ncol = length(formulae))
colnames <- as.character(formulae)
for (fold in 1:nfolds) {

test.rows <- which(fold.labels == fold)
train <- data[-test.rows, ]
test <- data[test.rows, ]
for (form in 1:length(formulae)) {

current.model <- lm(formula = formulae[[form]], data = train)
predictions <- predict(current.model, newdata = test)
test.responses <- eval(formulae[[form]][[2]], envir = test)
test.errors <- test.responses - predictions
mses[fold, form] <- mean(test.errors^2)

}

}
return(colmeans(mses))

}

code example 3: function to do k-fold cross-validation on linear models, given as a vector (or
list) of model formulae. note that this only returns the cv mse, not the parameter estimates
on each fold.

selection then picks the model with the smallest estimated risk.7 code example
3 performs k-fold cross-validation for linear models speci   ed by formulae.

the reason cross-validation works is that it uses the existing data to simulate
the process of generalizing to new data. if the full sample is large, then even the
smaller portion of it in the testing data is, with high id203, fairly represen-
tative of the data-generating process. randomly dividing the data into training
and test sets makes it very unlikely that the division is rigged to favor any one
model class, over and above what it would do on real new data. of course the
original data set is never perfectly representative of the full data, and a smaller
testing set is even less representative, so this isn   t ideal, but the approximation is
often quite good. k-fold cv is fairly good at getting the relative order of di   erent
models right, that is, at controlling over-   tting.8 figure 3.8 demonstrates these
points for the polynomial    ts we considered earlier (in figures 3.3   3.5).

cross-validation is probably the most widely-used method for model selection,
and for picking control settings, in modern statistics. there are circumstances
where it can fail     especially if you give it too many models to pick among    

7 a closely related procedure, sometimes also called    k-fold cv   , is to pick 1/k of the data points at
random to be the test set (using the rest as a training set), and then pick an independent 1/k of the
data points as the test set, etc., repeating k times and averaging. the di   erences are subtle, but
what   s described in the main text makes sure that each point is used in the test set just once.

8 the cross-validation score for the selected model still tends to be somewhat over-optimistic, because

it   s still picking the luckiest model     though the in   uence of luck is much attenuated. tibshirani
and tibshirani (2009) provides a simple correction.

3.4 cross-validation

73

but it   s the    rst thought of seasoned practitioners, and it should be your    rst
thought, too. the assignments to come will make you very familiar with it.

3.4.3 leave-one-out cross-validation

suppose we did k-fold cross-validation, but with k = n. our testing sets would
then consist of single points, and each point would be used in testing once. this
is called leave-one-out cross-validation. it actually came before k-fold cross-
validation, and has three advantages. first, because it estimates the performance
of a model trained with n     1 data points, it   s less biased as an estimator of the
performance of a model trained with n data points than is k-fold cross-validation,
which uses k   1
k n data points. second, leave-one-out doesn   t require any random
number generation, or keeping track of which data point is in which subset. third,
and more importantly, because we are only testing on one data point, it   s often
possible to    nd what the prediction on the left-out point would be by doing
calculations on a model    t to the whole data. (see p. 3.4.3 below.) this means
that we only have to    t each model once, rather than k times, which can be a
big savings of computing time.
the drawback to leave-one-out cv is subtle but often decisive. since each
training set has n     1 points, any two training sets must share n     2 points. the
models    t to those training sets tend to be strongly correlated with each other.
even though we are averaging n out-of-sample forecasts, those are correlated
forecasts, so we are not really averaging away all that much noise. with k-fold
cv, on the other hand, the fraction of data shared between any two training
sets is just k   2
n   1 , so even though the number of terms being averaged is
smaller, they are less correlated.

k   1 , not n   2

there are situations where this issue doesn   t really matter, or where it   s over-
whelmed by leave-one-out   s advantages in speed and simplicity, so there is cer-
tainly still a place for it, but one subordinate to k-fold cv.9

a short-cut for linear smoothers

m(xi) =(cid:80)

suppose the model m is a linear smoother (  1.5). for each of the data points
i, then, the predicted value is a linear combination of the observed values of y,
j   w(xi, xj)yj (eq. 1.53). as in   1.5.3, de   ne the    in   uence   ,    smooth-
ing    or    hat    matrix   w by   wij =   w(xi, xj). what happens when we hold back
data point i, and then make a prediction at xi? well, the observed response at i
can   t contribute to the prediction, but otherwise the linear smoother should work

9 at this point, it may be appropriate to say a few words about the akaike information criterion, or
aic. aic also tries to estimate how well a model will generalize to new data. one can show that,
under standard assumptions, as the sample size gets large, leave-one-out cv actually gives the same
estimate as aic (claeskens and hjort, 2008,   2.9). however, there do not seem to be any situations
where aic works where leave-one-out cv does not work at least as well. so aic should really be
understood as a very fast, but often very crude, approximation to the more accurate cross-validation.

74

as before, so

model evaluation

m(   i)(xi) =

(   wy)i       wiiyi

1       wii

(3.9)

the numerator just removes the contribution to m(xi) that came from yi, and the
denominator just re-normalizes the weights in the smoother. now a little algebra
says that

the quantity on the left of that equation is what we want to square and average
to get the leave-one-out cv score, but everything on the right can be calculated
from the    t we did to the whole data. the leave-one-out cv score is therefore

yi     m(   i)(xi) =

yi     m(xi)
1       wii
(cid:19)2

(cid:18) yi     m(xi)

1       wii

n(cid:88)

i=1

1
n

(3.10)

(3.11)

thus, if we restrict ourselves to leave-one-out and to linear smoothers, we can
calculate the cv score with just one estimation on the whole data, rather than
n re-estimates.
an even faster approximation that this is what   s called    generalized    cross-
validation, which is just the in-sample mse divided by (1     n   1 tr   w)2. that is,
rather than dividing each term in eq. 3.11 by a unique factor that depends on
its own diagonal entry in the hat matrix, we use the average of all the diagonal
entries, n   1 tr   w. (recall from   1.5.3.2 that tr   w is the number of e   ective degrees
of freedom for a linear smoother.) in addition to speed, this tends to reduce the
in   uence of points with high values of   wii, which may or may not be desirable.

some caveats are in order.

3.5 warnings

1. all of the model-selection methods i have described, and almost all others in
the literature, aim at getting models which will generalize well to new data,
if it follows the same distribution as old data. generalizing well even when
distributions change is a much harder and much less well-understood problem
(qui  nonero-candela et al., 2009). it is particularly troublesome for a lot of
applications involving large numbers of human beings, because society keeps
changing all the time     variables vary by de   nition, but the relationships
between variables also change. (that   s history.)

2. all of the standard theory of statistical id136 you have learned so far
presumes that you have a model which was    xed in advance of seeing the
data. if you use the data to select the model, that theory becomes invalid, and
it will no longer give you correct p-values for hypothesis tests, con   dence sets
for parameters, etc., etc. typically, using the same data both to select a model
and to do id136 leads to too much con   dence that the model is correct,
signi   cant, and estimated precisely.

3.5 warnings

75

3. all the model selection methods we have discussed aim at getting models which
predict well. this is not necessarily the same as getting the true theory of the
world. presumably the true theory will also predict well, but the converse
does not necessarily follow. we have seen (fig. 1.3), and will see again (  9.2),
examples of false but low-capacity models out-predicting correctly speci   ed
models at small n, because the former have such low variance of estimation.

the last two items     combining selection with id136, and parameter inter-

pretation     deserve elaboration.

3.5.1 id136 after selection

you have, by this point, learned a lot of inferential statistics     how to test various
hypotheses, calculate p-values,    nd con   dence regions, etc. most likely, you have
been taught procedures or calculations which all presume that the model you
are working with is    xed in advance of seeing the data. but, of course, if you do
model selection, the model you do id136 within is not    xed in advance, but is
actually a function of the data. what happens then?

this depends on whether you do id136 with the same data used to select
the model, or with another, independent data set. if it   s the same data, then all of
the inferential statistics become invalid     none of the calculations of probabilities
on which they rest are right any more. typically, if you select a model so that it
   ts the data well, what happens is that con   dence regions become too small10,
as do p-values for testing hypotheses about parameters. nothing can be trusted
as it stands.

the essential di   culty is this: your data are random variables. since you   re
doing model selection, making your model a function of the data, that means
your model is random too. that means there is some extra randomness in your
estimated parameters (and everything else), which isn   t accounted for by formulas
which assume a    xed model (exercise 3.4). this is not just a problem with formal
model-selection devices like cross-validation. if you do an initial, exploratory data
analysis before deciding which model to use     and that   s generally a good idea
    you are, yourself, acting as a noisy, complicated model-selection device.

there are three main approaches to this issue of post-selection id136.

1. ignore it. this can actually make sense if you don   t really care about doing in-
ference within your selected model, you just care about what model is selected.
otherwise, i can   t recommend it.

2. beat it with more statistical theory. there is, as i write, a lot of interest among
statisticians in working out exactly what happens to sampling distributions
under various combinations of models, model-selection methods, and assump-
tions about the true, data-generating process. since this is an active area of
research in statistical theory, i will pass it by, with some references in   3.6.

10 or, if you prefer, the same con   dence region really has a lower con   dence level, a lower id203

of containing or covering the truth, than you think it does.

76

model evaluation

3. evade it with an independent data set. remember that if the events a and b
are probabilistically independent, then pr (a|b) = pr (a). now set a =    the
con   dence set we calculated from this new data covers the truth    and b =
   the model selected from this old data was such-and-such   . so long as the
old and the new data are independent, it doesn   t matter that the model was
selected using data, rather than being    xed in advance.
the last approach is of course our old friend data splitting (  3.4.1). we divide
the data into two parts, and we use one of them to select the model. we then
re-estimate the selected model on the other part of the data, and only use that
second part in calculating our inferential statistics. experimentally, using part of
the data to do selection, and then all of the data to do id136, does not work
as well as a strict split (faraway, 2016). using equal amounts of data for selection
and for id136 is somewhat arbitrary, but, again it   s not clear that there   s a
much better division.

of course, if you only use a portion of your data to calculate con   dence regions,
they will typically be larger than if you used all of the data. (or, if you   re running
hypothesis tests, fewer coe   cients will be signi   cantly di   erent from zero, etc.)
this drawback is more apparent than real, since using all of your data to select
a model and do id136 gives you apparently-precise con   dence regions which
aren   t actually valid.

the simple data-splitting approach to combining model selection and id136
only works if the individual data points were independent to begin with. when
we deal with dependent data, in part iv, other approaches will be necessary.

3.5.2 parameter interpretation

in many situations, it is very natural to want to attach some substantive, real-
world meaning to the parameters of our statistical model, or at least to some of
them. i have mentioned examples above like astronomy, and it is easy to come
up with many others from the natural sciences. this is also extremely common
in the social sciences. it is fair to say that this is much less carefully attended to
than it should be.

to take just one example, consider the paper    luther and suleyman    by prof.
murat iyigun (iyigun, 2008). the major idea of the paper is to try to help explain
why the protestant reformation was not wiped out during the european wars
of religion (or alternately, why the protestants did not crush all the catholic
powers), leading western europe to have a mixture of religions, with profound
consequences. iyigun   s contention is that the european christians were so busy
   ghting the ottoman turks, or perhaps so afraid of what might happen if they did
not, that con   icts among the europeans were suppressed. to quote his abstract:

at the turn of the sixteenth century, ottoman conquests lowered the number of all newly initiated
con   icts among the europeans roughly by 25 percent, while they dampened all longer-running
feuds by more than 15 percent. the ottomans    military activities in   uenced the length of intra-
european feuds too, with each ottoman-european military engagement shortening the duration
of intra-european con   icts by more than 50 percent.

3.6 further reading

77

to back this up, and provide those quantitative    gures, prof. iyigun estimates
id75 models, of the form11

yt =   0 +   1xt +   2zt +   3ut +  t

(3.12)

where yt is    the number of violent con   icts initiated among or within continental
european countries at time t   12, xt is    the number of con   icts in which the
ottoman empire confronted european powers at time t   , zt is    the count at
time t of the newly initiated number of ottoman con   icts with others and its
own domestic civil discords   , ut is control variables re   ecting things like the
availability of harvests to feed armies, and  t is gaussian noise.

the qualitative idea here, about the in   uence of the ottoman empire on the
european wars of religion, has been suggested by quite a few historians before13.
the point of this paper is to support this rigorously, and make it precise. that
support and precision requires eq. 3.12 to be an accurate depiction of at least
part of the process which led european powers to    ght wars of religion. prof.
iyigun, after all, wants to be able to interpret a negative estimate of   1 as saying
that    ghting o    the ottomans kept christians from    ghting each other. if eq.
3.12 is inaccurate, if the model is badly mis-speci   ed, however,   1 becomes the
best approximation to the truth within a systematically wrong model, and the
support for claims like    ottoman conquests lowered the number of all newly
initiated con   icts among the europeans roughly by 25 percent    drains away.

to back up the use of eq. 3.12, prof. iyigun looks at a range of slightly di   erent
linear-model speci   cations (e.g., regress the number of intra-christian con   icts
in year t on the number of ottoman attacks in year t     1), and slightly di   er-
ent methods of estimating the parameters. what he does not do is look at the
other implications of the model: that residuals should be (at least approximately)
gaussian, that they should be unpredictable from the regressor variables. he does
not look at whether the relationships he thinks are linear really are linear (see
chapters 4, 8, and 9). he does not try to simulate his model and look at whether
the patterns of european wars it produces resemble actual history (see chapter
5). he does not try to check whether he has a model which really supports causal
id136, though he has a causal question (see part iii).

i do not say any of this to denigrate prof. iyigun. his paper is actually much
better than most quantitative work in the social sciences. this is re   ected by the
fact that it was published in the quarterly journal of economics, one of the most
prestigious, and rigorously-reviewed, journals in the    eld. the point is that by
the end of this course, you will have the tools to do better.

3.6 further reading

data splitting and cross-validation go back in statistical practice for many decades,
though often as a very informal tool. one of the    rst important papers on the

11 his eq. 1 on pp. 1473; i have modi   ed the notation to match mine.
12 in one part of the paper; he uses other dependent variables elsewhere.
13 see   1   2 of iyigun (2008), and macculloch (2004, passim).

78

model evaluation

subject was stone (1974), which goes over the earlier history. arlot and celisse
(2010) is a good recent review of cross-validation. faraway (1992, 2016) reviews
computational evidence that data splitting reduces the over-con   dence that re-
sults from model selection even if one only wants to do prediction. gy  or    et al.
(2002, chs. 7   8) has important results on data splitting and cross-validation,
though the proofs are much more advanced than this book.

some comparatively easy starting points on statistical learning theory are
kearns and vazirani (1994), cristianini and shawe-taylor (2000) and mohri et al.
(2012). at a more advanced level, look at the tutorial papers by bousquet et al.
(2004); von luxburg and sch  olkopf (2008), or the textbooks by vidyasagar (2003)
and by anthony and bartlett (1999) (the latter is much more general than its title
suggests), or read the book by vapnik (2000) (one of the founders). hastie et al.
(2009), while invaluable, is much more oriented towards models and practical
methods than towards learning theory.

on model selection in general, the best recent summary is the book by claeskens
and hjort (2008); it is more theoretically demanding than this book, but includes
many real-data examples.

the literature on doing statistical id136 after model selection by accounting
for selection e   ects, rather than simple data splitting, is already large and rapidly
growing. taylor and tibshirani (2015) is a comparatively readable introduction
to the    selective id136    approach associated with those authors and their
collaborators. tibshirani et al. (2015) draws connections between this approach
and the bootstrap (ch. 6). berk et al. (2013) provides yet another approach to
post-selection id136; nor is this an exhaustive list.

white (1994) is a thorough treatment of parameter estimation in models which
may be mis-speci   ed, and some general tests for mis-speci   cation. it also brie   y
discusses the interpretation of parameters in mis-speci   ed models. that topic
deserves a more in-depth treatment, but i don   t know of a really good one.

exercises

3.1

suppose that one of our model classes contains the true and correct model, but we also
consider more complicated and    exible model classes. does the bias-variance trade-o   
mean that we will over-shoot the true model, and always go for something more    exible,
when we have enough data? (this would mean there was such a thing as too much data
to be reliable.)

3.2 derive the formula for the generalization risk in the situation depicted in figure 3.1, as
given by the true.risk function in the code for that    gure. in particular, explain to
yourself where the constants 0.52 and 1/12 come from.
   optimism    and degrees of freedom suppose we get data of the form yi =   (xi) +  i,
where the noise terms  i have mean zero, are uncorrelated, and all have variance   2. we

use a linear smoother (  1.5) to estimate (cid:98)   from n such data points. the    optimism    of

3.3

the estimate is

(cid:34)

e

1
n

n(cid:88)

i=1

i    (cid:98)  (xi))2

(cid:48)

(y

(cid:35)

(cid:34)

    e

1
n

n(cid:88)

i=1

(cid:35)

(yi    (cid:98)  (xi))2

(3.13)

exercises

79

where y (cid:48)
i is an independent copy of yi. that is, the optimism is the di   erence between
the in-sample mse, and how well the model would predict on new data taken at exactly
the same xi values.
1. find a formula for the optimism in terms of n,   2, and the number of e   ective degrees

2. when (and why) does e(cid:2) 1

of freedom (in the sense of   1.5.3).

(cid:80)n
i=1 (y (cid:48)

n

i    (cid:98)  (xi))2(cid:3) di   er from the risk?

3.4 the perils of post-selection id136, and data splitting to the rescue14 generate a 1000  
101 array, where all the entries are iid standard gaussian variables. we   ll call the    rst
column the response variable y , and the others the predictors x1, . . . x100. by design,
there is no true relationship between the response and the predictors (but all the usual
linear-gaussian-modeling assumptions hold).

1. estimate the model y =   0 +   1x1 +   50x50 +  . extract the p-value for the f test
of the whole model. repeat the simulation, estimation and testing 100 times, and plot
the histogram of the p-values. what does it look like? what should it look like?

2. use the step function to select a linear model by forward stepwise selection. extract the
p-value for the f -test of the selected model. repeat 100 times and plot the histogram
of p-values. explain what   s going on.

3. again use step to select a model based on one random 1000  101 array. now re-estimate
the selected model on a new 1000    101 array, and extract the new p-value. repeat
100 times, with new selection and id136 sets each time, and plot the histogram of
p-values.

14 inspired by freedman (1983).

80

model evaluation

x.new = rnorm(20000)
y.new = 7 * x.new^2 - 0.5 * x.new + rnorm(20000)
gmse <- function(mdl) {

mean((y.new - predict(mdl, data.frame(x = x.new)))^2)

}
gmse.q <- sapply(fitted.models, gmse)
plot(0:9, mse.q, type = "b", xlab = "polynomial degree", ylab = "mean squared error",

log = "y", ylim = c(min(mse.q), max(gmse.q)))

lines(0:9, gmse.q, lty = 2, col = "blue")
points(0:9, gmse.q, pch = 24, col = "blue")

figure 3.5 in-sample error (black dots) compared to generalization error
(blue triangles). note the logarithmic scale for the vertical axis.

llllllllll02468110100100010000polynomial degreemean squared errorexercises

81

extract.rsqd <- function(mdl) {

c(summary(mdl)$r.squared, summary(mdl)$adj.r.squared)

}
rsqd.q <- sapply(fitted.models, extract.rsqd)
plot(0:9, rsqd.q[1, ], type = "b", xlab = "polynomial degree", ylab = expression(r^2),

ylim = c(0, 1))

lines(0:9, rsqd.q[2, ], type = "b", lty = "dashed")
legend("bottomright", legend = c(expression(r^2), expression(r[adj]^2)), lty = c("solid",

"dashed"))

figure 3.6 r2 and adjusted r2 for the polynomial    ts, to reinforce
  2.2.1.1   s point that neither statistic is a useful measure of how well a model
   ts, or a good criteria for picking among models.

llllllllll024680.00.20.40.60.81.0polynomial degreer2llllllllllr2radj282

model evaluation

2

3

4

5

11274

11275

3

4

11275

2

5

11274

median house value median household income median rooms

909600

748700

773600

579200

209500

253400

111667

66094

87306

62386

56667

71638

6.0

4.6

5.0

4.5

6.0

6.6

median house value median household income median rooms

748700

773600

253400

66094

87306

71638

4.6

5.0

6.6

median house value median household income median rooms

909600

579200

209500

111667

62386

56667

6.0

4.5

6.0

income only

income + rooms

rmse(a     a)
1.6215652    105
1.2831218    105

rmse(a     b)
1.6078767    105
1.2576588    105

figure 3.7 example of data splitting. the top table shows three columns
and seven rows of the housing-price data used in   a.13. i then randomly
split this into two equally-sized parts (next two tables). i estimate a linear
model which predicts house value from income alone, and another model
which predicts from income and the median number of rooms, on the    rst
half. the third table fourth row shows the performance of each estimated
model both on the    rst half of the data (left column) and on the second
(right column). the latter is a valid estimate of generalization error. the
larger model always has a lower in-sample error, whether or not it is really
better, so the in-sample mses provide little evidence that we should use the
larger model. having a lower score under data splitting, however, is evidence
that the larger model generalizes better. (for r commands used to get these
numbers, see code example 2.)

exercises

83

little.df <- data.frame(x = x, y = y)
cv.q <- cv.lm(little.df, poly.formulae)
plot(0:9, mse.q, type = "b", xlab = "polynomial degree", ylab = "mean squared error",

log = "y", ylim = c(min(mse.q), max(gmse.q)))

lines(0:9, gmse.q, lty = 2, col = "blue", type = "b", pch = 2)
lines(0:9, cv.q, lty = 3, col = "red", type = "b", pch = 3)
legend("topleft", legend = c("in-sample", "generalization", "cv"), col = c("black",

"blue", "red"), lty = 1:3, pch = 1:3)

figure 3.8 in-sample, generalization, and cross-validated mse for the
polynomial    ts of figures 3.3, 3.4 and 3.5. note that the cross-validation is
done entirely within the initial set of only 20 data points.

llllllllll02468110100100010000polynomial degreemean squared errorlin   samplegeneralizationcvn(cid:88)

(cid:98)  (x) =

4

using nonparametric smoothing in

regression

having spent long enough running down id75, and thought through
evaluating predictive models, it is time to turn to constructive alternatives, which
are (also) based on smoothing.

recall the basic kind of smoothing we are interested in: we have a response
variable y , some input variables which we bind up into a vector x, and a col-
lection of data values, (x1, y1), (x2, y2), . . . (xn, yn). by    smoothing   , i mean that
predictions are going to be weighted averages of the observed responses in the
training data:

yiw(x, xi, h)

(4.1)

i=1

most smoothing methods have a control setting, here written h, that says how
much to smooth. with k nearest neighbors, for instance, the weights are 1/k if
xi is one of the k-nearest points to x, and w = 0 otherwise, so large id116 that
each prediction is an average over many training points. similarly with kernel
regression, where the degree of smoothing is controlled by the bandwidth.

why do we want to do this? how do we pick how much smoothing to do?

4.1 how much should we smooth?

when we smooth very little (h     0), then we can match very small,    ne-grained
or sharp aspects of the true regression function, if there are such. that is, less
smoothing leads to less bias. at the same time, less smoothing means that each of
our predictions is going to be an average over (in e   ect) fewer observations, mak-
ing the prediction noisier. smoothing less increases the variance of our estimate.
since

(total error) = (noise) + (bias)2 + (variance)

(4.2)

(eq. 1.28), if we plot the di   erent components of error as a function of h, we
typically get something that looks like figure 4.1. because changing the amount
of smoothing has opposite e   ects on the bias and the variance, there is an optimal
amount of smoothing, where we can   t reduce one source of error without increas-
ing the other. we therefore want to    nd that optimal amount of smoothing, which
is where cross-validation comes in.

you should note, at this point, that the optimal amount of smoothing depends

84

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

4.2 adapting to unknown roughness

85

curve(2 * x^4, from = 0, to = 1, lty = 2, xlab = "smoothing", ylab = "generalization error")
curve(0.12 + x - x, lty = 3, add = true)
curve(1/(10 * x), lty = 4, add = true)
curve(0.12 + 2 * x^4 + 1/(10 * x), add = true)

figure 4.1 decomposition of the generalization error of smoothing: the
total error (solid) equals process noise (dotted) plus approximation error
from smoothing (=squared bias, dashed) and estimation variance
(dot-and-dash). the numerical values here are arbitrary, but the functional
forms (squared bias     h4, variance     n   1h   1) are representative of kernel
regression (eq. 4.12).

on the real regression curve, on our smoothing method, and on how much data we
have. this is because the variance contribution generally shrinks as we get more
data.1 if we get more data, we go from figure 4.1 to figure 4.2. the minimum
of the over-all error curve has shifted to the left, and we should smooth less.

strictly speaking, parameters are properties of the data-generating process
alone, so the optimal amount of smoothing is not really a parameter. if you do
think of it as a parameter, you have the problem of why the    true    value changes
as you get more data. it   s better thought of as a setting or control variable in
the smoothing method, to be adjusted as convenient.

4.2 adapting to unknown roughness

figure 4.3, which graphs two functions, r and s. both are    smooth    functions in
the mathematical sense2. we could taylor-expand both functions to approximate
their values anywhere, just from knowing enough derivatives at one point x0.3 if

1 sometimes bias changes as well. noise does not (why?).
2 they are    c      : continuous, with continuous derivatives to all orders.
3 see app. d for a refresher on taylor expansions.

0.00.20.40.60.81.00.00.51.01.52.0smoothinggeneralization error86

smoothing in regression

curve(2 * x^4, from = 0, to = 1, lty = 2, xlab = "smoothing", ylab = "generalization error")
curve(0.12 + x - x, lty = 3, add = true)
curve(1/(10 * x), lty = 4, add = true, col = "grey")
curve(0.12 + 2 * x^2 + 1/(10 * x), add = true, col = "grey")
curve(1/(30 * x), lty = 4, add = true)
curve(0.12 + 2 * x^4 + 1/(30 * x), add = true)

figure 4.2 consequences of adding more data to the components of error:
noise (dotted) and bias (dashed) don   t change, but the new variance curve
(dotted and dashed, black) is to the left of the old (greyed), so the new
over-all error curve (solid black) is lower, and has its minimum at a smaller
amount of smoothing than the old (solid grey).

instead of knowing the derivatives at x0 we have the values of the functions at a
sequence of points x1, x2, . . . xn, we could use interpolation to    ll out the rest of
the curve. quantitatively, however, r is less smooth than s     it changes much
more rapidly, with many reversals of direction. for the same degree of accuracy
in the interpolation r needs more, and more closely spaced, training points xi
than does s.

now suppose that we don   t get to actually get to see r and s, but rather just
r(x)+  and s(x)+  , for various x, where   and    are noise. (to keep things simple
i   ll assume they   re constant-variance, iid gaussian noises, say with    = 0.15.)
the data now look something like figure 4.4. can we recover the curves?

as remarked in chapter 1, if we had many measurements at the same x, then
we could    nd the expectation value by averaging: the regression function   (x) =
e [y |x = x], so with multiple observations xi = x, the mean of the corresponding
yi would (by the law of large numbers) converge on   (x). generally, however, we
have at most one measurement per value of x, so simple averaging won   t work.
even if we just con   ne ourselves to the xi where we have observations, the mean-

0.00.20.40.60.81.00.00.51.01.52.0smoothinggeneralization error(cid:98)  (x) =

=

n(cid:88)
n(cid:88)

i=1

i=1

yiwi

4.2 adapting to unknown roughness

87

squared error would always be   2, the noise variance. however, our estimate
would be unbiased.

smoothing methods try to use multiple measurements at points xi which are
near the point of interest x. if the regression function is smooth, as we   re assuming
it is,   (xi) will be close to   (x). remember that the mean-squared error is the
(cid:54)= x is going to
sum of bias (squared) and variance. averaging values at xi
introduce bias, but averaging independent terms together also reduces variance.
if smoothing gets rid of more variance than it adds bias, we come out ahead.

here   s a little math to see it. let   s assume that we can do a    rst-order taylor

expansion (figure d.1), so

and

  (xi)       (x) + (xi     x)  (cid:48)(x)

yi       (x) + (xi     x)  (cid:48)(x) +  i

(4.3)

(4.4)

now we average: to keep the notation simple, abbreviate the weight w(xi, x, h)
by just wi.

(4.5)

(4.6)

(4.7)

(4.8)

(  (x) + (xi     x)  (cid:48)(x) +  i)wi
n(cid:88)

n(cid:88)

wi i +   (cid:48)(x)

wi(xi     x)

i=1

i=1

=   (x) +

n(cid:88)
(cid:98)  (x)       (x) =
n(cid:88)
e(cid:2)((cid:98)  (x)       (x))2(cid:3) =   2

i=1

wi i +   (cid:48)(x)

n(cid:88)
      (cid:32)

i=1

wi(xi     x)
n(cid:88)

(cid:33)2      

(remember that: (cid:80) wi = 1; e [ i] = 0;   is uncorrelated with everything; and

i=1

i=1

(4.9)

i + e
w2

  (cid:48)(x)

wi(xi     x)

v [ i] =   2.)

the    rst term on the    nal right-hand side is an estimation variance, which will
tend to shrink as n grows. (if we just did a simple global mean, wi = 1/n for all
i, so we   d get   2/n, just like in baby stats.) the second term, an expectation,
is bias, which grows as xi gets further from x, and as the magnitudes of the
derivatives grow, i.e., this term   s growth varies with how smooth or wiggly the
regression function is. for smoothing to work, wi had better shrink as xi     x and
  (cid:48)(x) grow.4 finally, all else being equal, wi should also shrink with n, so that
the over-all size of the sum shrinks as we get more data.

4 the higher derivatives of    also matter, since we should really keep more than just the    rst term in

the taylor expansion. the details get messy, but eq. 4.12 below gives the upshot for kernel
smoothing.

88

smoothing in regression

to illustrate, let   s try to estimate r(1.6) and s(1.6) from the noisy observations.
we   ll try a simple approach, just averaging all values of r(xi) +  i and s(xi) +   i
for 1.5 < xi < 1.7 with equal weights. for r, this gives 0.71, while r(1.6) = 0.83.
for g, this gives 1, with s(1.6) = 0.96. (see    gure 4.5.) the same window size
creates a much larger bias with the rougher, more rapidly changing r than with
the smoother, more slowly changing s. varying the size of the averaging window
will change the amount of error, and it will change it in di   erent ways for the
two functions.

(cid:21)

(cid:20) 1

if one does a more careful second-order taylor expansion like that leading to

eq. 4.9, speci   cally for kernel regression, one can show that the bias at x is

  (cid:48)(x)f(cid:48)(x)

  (cid:48)(cid:48)(x) +

e [(cid:98)  (x)       (x)|x1 = x1, . . . xn = xn] = h2

2

f (x)

  2
k + o(h2)

where f is the density of x, and   2
density corresponding to the kernel5. the   (cid:48)(cid:48) term just comes from the second-
order part of the taylor expansion. to see where the   (cid:48)f(cid:48) term comes from,
imagine    rst that x is a mode of the distribution, so f(cid:48)(x) = 0. as h shrinks, only

k =(cid:82) u2k(u)du, the variance of the id203
training points where xi is very close to x will have any weight in(cid:98)  (x), and their
small). so, at mode, e [w(xi, x, h)(xi     x)(cid:98)  (x)]     0. away from a mode, there

distribution will be roughly symmetric around x (at least once h is su   ciently

will tend to be more training points on one side or the other of x, depending
on the sign of f(cid:48)(x), and this induces a bias. the tricky part of the analysis is
concluding that the bias has exactly the form given above.6

(4.10)

one can also work out the variance of the kernel regression estimate,

v [(cid:98)  (x)|x1 = x1, . . . xn = xn] =

  2(x)r(k)

+ o((nh)   1)

where r(k)    (cid:82) k 2(u)du. roughly speaking, the width of the region where the
points available to estimate (cid:98)  (x). each of these has a yi value, equal to   (x) plus

kernel puts non-trivial weight is about h, so there will be about nhf (x) training

noise of variance   2(x). the    nal factor of r(k) accounts for the average weight.
putting the bias together with the variance, we get an expression for the mean

nhf (x)

(4.11)

squared error of the kernel regression at x:

(cid:20) 1

2

(cid:21)2

m se(x) =   2(x)+h4

  (cid:48)(cid:48)(x) +

  (cid:48)(x)f(cid:48)(x)

f (x)

(  2

k)2+

  2(x)r(k)

nhf (x)

+o(h4)+o((nh)   1)

(4.12)
eq. 4.12 tells us that, in principle, there is a single optimal choice of bandwidth
h, an optimal degree of smoothing. we could    nd it by taking eq. 4.12, di   eren-
tiating with respect to the bandwidth, and setting everything to zero (neglecting

5 if you are not familiar with the    order    symbols o and o, see appendix c.
6 exercise 4.1 sketches the demonstration for the special case of the uniform (   boxcar   ) kernel.

the o terms):

4.2 adapting to unknown roughness

89

(cid:21)2

(cid:20) 1

2

         n

0 = 4h3

  (cid:48)(cid:48)(x) +

4f (x)(  2

h =

k)2       2(x)r(k)
(  2
nh2f (x)

  (cid:48)(x)f(cid:48)(x)

k)2(cid:104) 1

f (x)
2   (cid:48)(cid:48)(x) +   (cid:48)(x)f(cid:48)(x)
  2(x)r(k)

f (x)

(cid:105)2

            1/5

(4.13)

(4.14)

of course, this expression for the optimal h involves the unknown derivatives   (cid:48)(x)
and   (cid:48)(cid:48)(x), plus the unknown density f (x) and its unknown derivative f(cid:48)(x). but
if we knew the derivative of the regression function, we would basically know the
function itself (just integrate), so we seem to be in a vicious circle, where we need
to know the function before we can learn it.7

one way of expressing this is to talk about how well a smoothing procedure
would work, if an oracle were to tell us the derivatives, or (to cut to the chase)
the optimal bandwidth hopt. since most of us do not have access to such oracles,

we need to estimate hopt. once we have this estimate,(cid:98)h, then we get our weights
be the oracle   s mse, plus an extra term which depends on how far(cid:98)h is to hopt,
actual mse, using (cid:98)h, approaches the oracle   s mse, which it gets from hopt.

and our predictions, and so a certain mean-squared error. basically, our mse will

what would be really nice would be an adaptive procedure, one where our

and how sensitive the smoother is to the choice of bandwidth.

this would mean that, in e   ect, we are    guring out how rough the underlying
regression function is, and so how much smoothing to do, rather than having to
guess or be told. an adaptive procedure, if we can    nd one, is a partial8 substitute
for prior knowledge.

4.2.1 bandwidth selection by cross-validation

the most straight-forward way to pick a bandwidth, and one which generally
manages to be adaptive, is in fact cross-validation; k-fold cv is usually somewhat
better than leave-one-out, but the latter often works acceptably too. the usual
procedure is to come up with an initial grid of candidate bandwidths, and then
use cross-validation to estimate how well each one of them would generalize. the
one with the lowest error under cross-validation is then used to    t the regression
curve to the whole data9.

7 you may be wondering why i keep talking about the optimal bandwidth, when eq. 4.14 makes it

seem that the bandwidth should vary with x. one can go through pretty much the same sort of
analysis in terms of the expected values of the derivatives, and the qualitative conclusions will be the
same, but the notational overhead is even worse. alternatively, there are techniques for
variable-bandwidth smoothing.

8 only partial, because we   d always do better if the oracle would just tell us hopt.
9 since the optimal bandwidth is     n   1/5, and the training sets in cross-validation are smaller than
the whole data set, one might adjust the bandwidth proportionally. however, if n is small enough
that this makes a big di   erence, the sheer noise in bandwidth estimation usually overwhelms this.

90

smoothing in regression

cv_bws_npreg <- function(x, y, bandwidths = (1:50)/50, nfolds = 10) {

require(np)
n <- length(x)
stopifnot(n > 1, length(y) == n)
stopifnot(length(bandwidths) > 1)
stopifnot(nfolds > 0, nfolds == trunc(nfolds))
fold_mses <- matrix(0, nrow = nfolds, ncol = length(bandwidths))
colnames(fold_mses) = bandwidths
case.folds <- sample(rep(1:nfolds, length.out = n))
for (fold in 1:nfolds) {

train.rows = which(case.folds != fold)
x.train = x[train.rows]
y.train = y[train.rows]
x.test = x[-train.rows]
y.test = y[-train.rows]
for (bw in bandwidths) {

fit <- npreg(txdat = x.train, tydat = y.train, exdat = x.test, eydat = y.test,

bws = bw)

fold_mses[fold, paste(bw)] <- fit$mse

}

}
cv_mses = colmeans(fold_mses)
best.bw = bandwidths[which.min(cv_mses)]
return(list(best.bw = best.bw, cv_mses = cv_mses, fold_mses = fold_mses))

}

code example 4: cross-validation for univariate kernel regression. the colnames trick: com-
ponent names have to be character strings; other data types will be coerced into characters when
we assign them to be names. later, when we want to refer to a bandwidth column by its name,
we wrap the name in another coercing function, such as paste.     the is just demo of how
cross-validation for bandwidth selection works in principle; don   t use it blindly on data, or in
assignments. (that goes double for the vector of default bandwidths.)

code example 4 shows how it would work in r, with a one predictor variable,
borrowing the npreg function from the np library (hay   eld and racine, 2008).10
the return value has three parts. the    rst is the actual best bandwidth. the
second is a vector which gives the cross-validated mean-squared errors of all the
di   erent bandwidths in the vector bandwidths. the third component is an array
which gives the mse for each bandwidth on each fold. it can be useful to know
things like whether the di   erence between the cv score of the best bandwidth
and the runner-up is bigger than their fold-to-fold variability.

figure 4.7 plots the cv estimate of the (root) mean-squared error versus band-
width for our two curves. figure 4.8 shows the data, the actual regression func-
tions and the estimated curves with the cv-selected bandwidths. this illustrates
why picking the bandwidth by cross-validation works: the curve of cv error
against bandwidth is actually a pretty good approximation to the true curve
of generalization error (which would look like figure 4.1), so optimizing the cv
error is close to optimizing the generalization error.

notice, by the way, in figure 4.7, that the rougher curve is more sensitive

10 the package has methods for automatically selecting bandwidth by cross-validation     see   4.6

below.

4.2 adapting to unknown roughness

91

to the choice of bandwidth, and that the smoother curve always has a lower
mean-squared error. also notice that, at the minimum, one of the cross-validation
estimates of generalization error is smaller than the true system noise level; this
shows that cross-validation doesn   t completely correct for optimism11.

we still need to come up with an initial set of candidate bandwidths. for
reasons which will drop out of the math in chapter 14, it   s often reasonable
to start around 1.06sx/n1/5, where sx is the sample standard deviation of x.
however, it is hard to be very precise about this, and good results often require
some honest trial and error.

4.2.2 convergence of kernel smoothing and bandwidth scaling

go back to eq. 4.12 for the mean squared error of kernel regression. as we said,
it involves some unknown constants, but we can bury them inside big-o order
symbols, which also absorb the little-o remainder terms:

m se(h) =   2(x) + o(h4) + o((nh)   1)

(4.15)

the   2(x) term is going to be there no matter what, so let   s look at the excess
risk over and above the intrinsic noise:

m se(h)       2(x) = o(h4) + o((nh)   1)

(4.16)

that is, the (squared) bias from the kernel   s only approximately getting the curve
is proportional to the fourth power of the bandwidth, but the variance is inversely
proportional to the product of sample size and bandwidth. if we kept h constant
and just let n        , we   d get rid of the variance, but we   d be left with the bias.
to get the mse to go to zero, we need to let the bandwidth h change with n    
call it hn. speci   cally, suppose hn     0 as n        , but nhn        . then, by eq.
4.16, the risk (generalization error) of kernel smoothing is approaching that of
the ideal predictor.

what is the best bandwidth? we saw in eq. 4.14 that it is (up to constants)

hopt = o(n   1/5)
if we put this bandwidth into eq. 4.16, we get
m se(h)     2(x) = o

n   1/5(cid:17)4(cid:19)
(cid:18)(cid:16)

(cid:18)
n   1(cid:16)

n   1/5(cid:17)   1(cid:19)

+o

(cid:16)

n   4/5(cid:17)

+o

= o

(4.17)

(cid:16)
n   4/5(cid:17)

= o

(cid:16)
n   4/5(cid:17)

(4.18)
that is, the excess prediction error of kernel smoothing over and above the system
noise goes to zero as 1/n0.8. notice, by the way, that the contributions of bias
and variance to the generalization error are both of the same order, n   0.8.

is this fast or slow? we can compare it to what would happen with a parametric
model, say with parameter   . (for id75,    would be the vector of

11 tibshirani and tibshirani (2009) gives a fairly straightforward way to adjust the estimate of the

generalization error for the selected model or bandwidth, but that doesn   t in   uence the choice of the
best bandwidth.

92

smoothing in regression

slopes and the intercept.) the optimal value of the parameter,   0, minimizes the
mean-squared error. at   0, the parametric model has mse

m se(  0) =   2(x) + b(x,   0)

(4.19)

where b is the bias of the parametric model; this is zero when the parametric
model is true12. since   0 is unknown and must be estimated, one typically has
n). because the error is minimized at   0, the    rst derivatives
of m se at   0 are 0. doing a second-order taylor expansion of the parametric

(cid:98)         0 = o(1/
model contributes an error o(((cid:98)         0)2), so altogether

   

m se((cid:98)  )       2(x) = b(x,   0) + o(1/n)

(4.20)
this means parametric models converge more quickly (n   1 goes to zero faster
than n   0.8), but they typically converge to the wrong answer (b2 > 0). kernel
smoothing converges more slowly, but always converges to the right answer13.

this doesn   t change much if we use cross-validation. writing (cid:100)hcv for the band-

width picked by cross-validation, it turns out (simono   , 1996, ch. 5) that

given this, one concludes (exercise 4.2) that the mse of using (cid:100)hcv is also

hopt

(4.21)

    1 = o(n   1/10)

(cid:100)hcv     hopt

o(n   4/5).

4.2.3 summary on kernel smoothing in 1d

suppose that x and y are both one-dimensional, and the true regression func-
tion   (x) = e [y |x = x] is continuous and has    rst and second derivatives14.
suppose that the noise around the true regression function is uncorrelated be-
tween di   erent observations. then the bias of kernel smoothing, when the kernel
has bandwidth h, is o(h2), and the variance, after n samples, is o((1/nh)   1).
the optimal bandwidth is o(n   1/5), and the excess mean squared error of using
this bandwidth is o(n   4/5). if the bandwidth is selected by cross-validation, the
excess risk is still o(n   4/5).

4.3 kernel regression with multiple inputs

for the most part, when i   ve been writing out kernel regression i have been
treating the input variable x as a scalar. there   s no reason to insist on this,

12 when the model is wrong, the optimal parameter value   0 is often called the pseudo-truth.
13 it is natural to wonder if one couldn   t do better than kernel smoothing   s o(n   4/5) while still having

no asymptotic bias. resolving this is very di   cult, but the answer turns out to be    no    in the
following sense (wasserman, 2006). any curve-   tting method which can learn arbitrary smooth
regression functions will have some curves where it cannot converge any faster than o(n   4/5). (in
the jargon, that is the minimax rate.) methods which converge faster than this for some kinds of
curves have to converge more slowly for others. so this is the best rate we can hope for on truly
unknown curves.

14 or can be approximated arbitrarily closely by such functions.

4.3 kernel regression with multiple inputs

93

however; it could equally well be a vector. if we want to enforce that in the
notation, say by writing (cid:126)x = (x1, x2, . . . xd), then the kernel regression of y on (cid:126)x
would just be

(cid:98)  ((cid:126)x) =

n(cid:88)

i=1

(cid:80)n
k((cid:126)x     (cid:126)xi)
j=1 k((cid:126)x     (cid:126)xj)

yi

(4.22)

in fact, if we want to predict a vector, we   d just substitute (cid:126)yi for yi above.

to make this work, we need id81s for vectors. for scalars, i said
that any id203 density function would work so long as it had mean zero,
and a    nite, strictly positive (not 0 or    ) variance. the same conditions carry
over: any distribution over vectors can be used as a multivariate kernel, provided
it has mean zero, and the variance matrix is    nite and    positive de   nite   15. in
practice, the overwhelmingly most common and practical choice is to use product
kernels16.

a product kernel simply uses a di   erent kernel for each component, and then

multiplies them together:

k((cid:126)x     (cid:126)xi) = k1(x1     x1

i )k2(x2     x2

i ) . . . kd(xd     xd
i )

(4.23)

now we just need to pick a bandwidth for each kernel, which in general should
not be equal     say (cid:126)h = (h1, h2, . . . hd). instead of having a one-dimensional error
curve, as in figure 4.1 or 4.2, we will have a d-dimensional error surface, but we
can still use cross-validation to    nd the vector of bandwidths that generalizes best.
we generally can   t, unfortunately, break the problem up into somehow picking the
best bandwidth for each variable without considering the others. this makes it
slower to select good bandwidths in multivariate problems, but still often feasible.
(we can actually turn the need to select bandwidths together to our advantage.
if one or more of the variables are irrelevant to our prediction given the others,
cross-validation will tend to give them the maximum possible bandwidth, and
smooth away their in   uence. in chapter 14, we   ll look at formal tests based on
this idea.)

kernel regression will recover almost any regression function. this is true even
when the true regression function involves lots of interactions among the input
variables, perhaps in complicated forms that would be very hard to express in
id75. for instance, figure 4.9 shows a contour plot of a reasonably
complicated regression surface, at least if one were to write it as polynomials in
x1 and x2, which would be the usual approach. figure 4.11 shows the estimate
we get with a product of gaussian kernels and only 1000 noisy data points. it   s
not perfect, of course (in particular the estimated contours aren   t as perfectly
smooth and round as the true ones), but the important thing is that we got this
without having to know, and describe in cartesian coordinates, the type of shape
we were looking for. kernel smoothing discovered the right general form.

15 remember that for a matrix v to be    positive de   nite   , it must be the case that for any vector

(cid:126)a (cid:54)= (cid:126)0, (cid:126)a    v(cid:126)a > 0. covariance matrices are automatically non-negative, so we   re just ruling out the
case of some weird direction along which the distribution has zero variance.

16 people do sometimes use multivariate gaussians; we   ll glance at this in chapter e.

94

smoothing in regression

there are limits to these abilities of kernel smoothers; the biggest one is that
they require more and more data as the number of predictor variables increases.
we will see later (chapter 8) exactly how much data is required, generalizing the
kind of analysis done   4.2.2, and some of the compromises this can force us into.

4.4 interpreting smoothers: plots

in a id75 without interactions, it is fairly easy to interpret the coe   -
cients. the expected response changes by   i for a one-unit change in the ith input
variable. the coe   cients are also the derivatives of the expected response with
respect to the inputs. and it is easy to draw pictures of how the output changes
as the inputs are varied, though the pictures are somewhat boring (straight lines
or planes).

as soon as we introduce interactions, all this becomes harder, even for para-
metric regression. if there is an interaction between two components of the input,
say x1 and x2, then we can   t talk about the change in the expected response for
a one-unit change in x1 without saying what x2 is. we might average over x2
values, and in   4.5 below we   ll see next time a reasonable way of doing this, but
the    at statement    increasing x1 by one unit increases the response by   1    is just
false, no matter what number we    ll in for   1. likewise for derivatives; we   ll come
back to them next time as well.

what about pictures? with only two input variables, we can make wireframe
plots like figure 4.11, or contour or level plots, which will show the predictions
for di   erent combinations of the two variables. but what if we want to look at
one variable at a time, or there are more than two input variables?

a reasonable way to produce a curve for each input variable is to set all the
others to some    typical    value, like their means or medians, and to then plot the
predicted response as a function of the one remaining variable of interest (figure
4.12). of course, when there are interactions, changing the values of the other
inputs will change the response to the input of interest, so it   s a good idea to
produce a couple of curves, possibly super-imposed (figure 4.12 again).

if there are three or more input variables, we can look at the interactions of any
two of them, taken together, by    xing the others and making three-dimensional
or contour plots, along the same principles.

the fact that smoothers don   t give us a simple story about how each input is
associated with the response may seem like a disadvantage compared to using
id75. whether it really is a disadvantage depends on whether there
really is a simple story to be told, and/or how much big a lie you are prepared
to tell in order to keep your story simple.

4.5 average predictive comparisons

suppose we have a id75 model

y =   1x1 +   2x2 +  

(4.24)

4.5 average predictive comparisons

95

and we want to know how much y changes, on average, for a one-unit increase
in x1. the answer, as you know very well, is just   1:

[  1(x1 + 1) +   2x2]     [  1x1 +   2x2] =   1

(4.25)

this is an interpretation of the regression coe   cients which you are very used to
giving. but it fails as soon as we have interactions:

y =   1x1 +   2x2 +   3x1x2 +  

(4.26)

now the e   ect of increasing x1 by 1 is
[  1(x1 +1)+  2x2 +  3(x1 +1)x2]   [  1x1 +  2x2 +  3x1x2] =   1 +  3x2 (4.27)
the right answer to    how much does the response change when x1 is increased
by one unit?    depends on the value of x2; it   s certainly not just      1   .

we also can   t give just a single answer if there are nonlinearities. suppose that

the true regression function is this:

e  x

+  

y =

1 + e  x

(4.28)
which looks like figure 4.13, setting    = 7 (for luck). moving x from    4 to    3
increases the response by 7.57   10   10, but the increase in the response from x =
   1 to x = 0 is 0.499. functions like this are very common in psychology, medicine
(dose-response curves for drugs), biology, etc., and yet we cannot sensibly talk
about the response to a one-unit increase in x. (we will come back to curves
which look like this in chapter 11.)

more generally, let   s say we are regressing y on a vector (cid:126)x, and want to assess
the impact of one component of the input on y . to keep the use of subscripts and
superscripts to a minimum, we   ll write (cid:126)x = (u, (cid:126)v ), where u is the coordinate
we   re really interested in. (it doesn   t have to come    rst, of course.) we would like
to know how much the prediction changes as we change u,
y | (cid:126)x = (u(1), (cid:126)v)

y | (cid:126)x = (u(2), (cid:126)v)

(4.29)

and the change in the response per unit change in u,

y | (cid:126)x = (u(2), (cid:126)v)

y | (cid:126)x = (u(1), (cid:126)v)

(4.30)

e(cid:104)
e(cid:104)

(cid:105)     e(cid:104)
(cid:105)     e(cid:104)

u(2)     u(1)

(cid:105)
(cid:105)

both of these, but especially the latter, are called the predictive comparison.
note that both of them, as written, depend on u(1) (the starting value for the
variable of interest), on u(2) (the ending value), and on (cid:126)v (the other variables,
held    xed during this comparison). we have just seen that in a linear model
without interactions, u(1), u(2) and (cid:126)v all go away and leave us with the regression
coe   cient on u. in nonlinear or interacting models, we can   t simplify so much.

once we have estimated a regression model, we can choose our starting point,
ending point and context, and just plug in to eq. 4.29 or eq. 4.30. (or problem

96

smoothing in regression

9 in problem set a.14.) but suppose we do want to boil this down into a single
number for each input variable     how might we go about this?

one good answer, which comes from gelman and pardoe (2007), is just to av-
erage 4.30 over the data17. more speci   cally, we have as our average predictive

comparison for u(cid:80)n
where i and j run over data points, (cid:98)   is our estimated regression function, and

(cid:80)n
j=1 ((cid:98)  (uj, (cid:126)vi)    (cid:98)  (ui, (cid:126)vi))sign(uj     ui)
(cid:80)n
(cid:80)n
j=1 (uj     ui)sign(uj     ui)

the sign function is de   ned by sign(x) = +1 if x > 0, = 0 if x = 0, and =    1 if
x < 0. we use the sign function this way to make sure we are always looking at
the consequences of increasing u.

(4.31)

i=1

i=1

the average predictive comparison is a reasonable summary of how rapidly we
should expect the response to vary as u changes slightly. but we need to remember
that once the model is nonlinear or has interactions, it   s just not possible to boil
down the whole predictive relationship between u and y into one number. in
particular, the value of eq. 4.31 is going to depend on the distribution of u (and
possibly of v), even when the regression function is unchanged. (see exercise 4.3.)

4.6 computational advice: npreg

the homework will call for you to do nonparametric regression with the np pack-
age     which we   ve already looked at a little. it   s a powerful bit of software, but
it can take a bit of getting used to. this section is not a substitute for reading
hay   eld and racine (2008), but should get you started.

we   ll look at a synthetic-data example with four variables: a quantitative re-
sponse y , two quantitative predictors x and z, and a categorical predictor w ,
which can be either    a    or    b   . the true model is

z

y =   + 20x 2 +

(4.32)
with       n (0, 0.05). code example 5 generates some data from this model for
us.

10ez/(1 + ez)

the basic function for    tting a kernel regression in np is npreg     conceptually,
it   s the equivalent of lm. like lm, it takes a formula argument, which speci   es
the model, and a data argument, which is a data frame containing the variables
included in the formula. the basic idea is to do something like this:

if w = a
if w = b

(cid:26)

demo.np1 <- npreg(y ~ x + z, data = demo.df)

the variables on the right-hand side of the formula are the predictors; we use
+ to separate them. kernel regression will automatically include interactions be-
tween all variables, so there is no special notation for interactions. similarly, there
is no point in either including or excluding intercepts. if we wanted to transform

17 actually, they propose something a bit more complicated, which takes into account the uncertainty

in our estimate of the regression function, via id64 (chapter 6).

4.6 computational advice: npreg

97

make.demo.df <- function(n) {

demo.func <- function(x, z, w) {

20 * x^2 + ifelse(w == "a", z, 10 * exp(z)/(1 + exp(z)))

}
x <- runif(n, -1, 1)
z <- rnorm(n, 0, 10)
w <- sample(c("a", "b"), size = n, replace = true)
y <- demo.func(x, z, w) + rnorm(n, 0, 0.05)
return(data.frame(x = x, y = y, z = z, w = w))

}
demo.df <- make.demo.df(100)

code example 5: generating data from eq. 4.32.

either a predictor variable or the response, as in lm, we can do so. run like this,
npreg will try to determine the best bandwidths for the predictor variables, based
on a sophisticated combination of cross-validation and optimization.

let   s look at the output of npreg:

x

summary(demo.np1)
##
## regression data: 100 training points, in 2 variable(s)
##
z
## bandwidth(s): 0.08108232 2.428622
##
## kernel regression estimator: local-constant
## bandwidth type: fixed
## residual standard error: 2.228451
## r-squared: 0.9488648
##
## continuous kernel type: second-order gaussian
## no. continuous explanatory vars.: 2

the main things here are the bandwidths. we also see the root mean squared
error on the training data. note that this is the in-sample root mse; if we wanted
the in-sample mse, we could do

demo.np1$mse
## [1] 4.965993

(you can check that this is the square of the residual standard error above.) if

we want the cross-validated mse used to pick the bandwidths, that   s

demo.np1$bws$fval
## [1] 16.93204

the fitted and residuals functions work on these objects just like they do

in lm objects, while the coefficients and confint functions do not. (why?)

the predict function also works like it does for lm, expecting a data frame
containing columns whose names match those in the formula used to    t the model:

predict(demo.np1, newdata = data.frame(x = -1, z = 5))
## [1] 22.60836

98

smoothing in regression

with two predictor variables, there is a nice three-dimensional default plot

(figure 4.14).

id81s can also be de   ned for categorical and ordered variables.
these can be included in the formula by wrapping the variable in factor()
or ordered(), respectively:

demo.np3 <- npreg(y ~ x + z + factor(w), data = demo.df)

again, there   s no point, or need, to indicate interactions. including the extra

variable, not surprisingly, improves the cross-validated mse:

demo.np3$bws$fval
## [1] 3.852239

with three or more predictor variables, we   d need a four-dimensional plot,
which is hard. instead, the default is to plot what happens as we sweep one vari-
able with the others held    xed (by default, at their medians; see help(npplot)
for changing that), as in figure 4.15. we get something parabola-ish as we sweep
x (which is right), and something near a step function as we sweep z (which is
right when w = b), so we   re not doing badly for estimating a fairly complicated
function of three variables with only 100 samples. we could also try    xing w at
one value or another and making a perspective plot     figure 4.16.

the default optimization of bandwidths is extremely aggressive. it keeps adjust-
ing the bandwidths until the changes in the cross-validated mse are very small,
or the changes in the bandwidths themselves are very small. the    tolerances   
for what count as    very small    are controlled by arguments to npreg called tol
(for the bandwidths) and ftol (for the mse), which default to about 10   8 and
10   7, respectively. with a lot of data, or a lot of variables, this gets extremely
slow. one can often make npreg run much faster, with no real loss of accuracy,
by adjusting these options. a decent rule of thumb is to start with tol and ftol
both at 0.01. one can use the bandwidth found by this initial coarse search to
start a more re   ned one, as follows:

bigdemo.df <- make.demo.df(1000)
system.time(demo.np4 <- npreg(y ~ x + z + factor(w), data = bigdemo.df, tol = 0.01,

ftol = 0.01))

##
##

user
30.251

system elapsed
30.547

0.122

this tells us how much time it took r to run npreg, dividing that between
time spent exclusively on our job and on background system tasks. the result of
the run is stored in demo.np4:

demo.np4$bws
##
## regression data (1000 observations, 3 variable(s)):
##
##
factor(w)
## bandwidth(s): 0.05532488 1.964943 1.535065e-07
##
## regression type: local-constant

x

z

4.7 further reading

99

## bandwidth selection method: least squares cross-validation
## formula: y ~ x + z + factor(w)
## bandwidth type: fixed
## objective function value: 0.9546005 (achieved on multistart 2)
##
## continuous kernel type: second-order gaussian
## no. continuous explanatory vars.: 2
##
## unordered categorical kernel type: aitchison and aitken
## no. unordered categorical explanatory vars.: 1

the bandwidths have all shrunk (as they should), and the cross-validated mse
is also much smaller (0.95 versus 3.9 before). figure 4.16 shows the estimated
regression surfaces for both values of the categorical variable.

the package also contains a function, npregbw, which takes a formula and a
data frame, and just optimizes the bandwidth. this is called automatically by
npreg, and many of the relevant options are documented in its help page. one can
also use the output of npregbw as an argument to npreg, in place of a formula.
as a    nal piece of computational advice, you will notice when you run these
commands yourself that the bandwidth-selection functions by default print out
lots of progress-report messages. this can be annoying, especially if you are em-
bedding the computation in a document, and so can be suppressed by setting a
global option at the start of your code:

options(np.messages = false)

4.7 further reading

simono    (1996) is a good practical introduction to kernel smoothing and related
methods. wasserman (2006) provides more theory. li and racine (2007) is a
detailed treatment of nonparametric methods for econometric problems, over-
whelmingly focused on kernel regression and kernel density estimation (which
we   ll get to in chapter 14); racine (2008) summarizes.
while kernels are a nice, natural method of non-parametric smoothing, they are
not the only one. we saw nearest-neighbors in   1.5.1, and will encounter splines
(continuous piecewise-polynomial models) in chapter 7 and trees (piecewise-
constant functions, with cleverly chosen pieces) in chapter 13; local linear models
(  10.5) combine kernels and linear models. there are many, many more options.

kernel regression was introduced, independently, by nadaraya (1964) and watson
(1964); both were inspired by kernel density estimation.

historical notes

100

4.1

suppose we use a uniform (   boxcar   ) kernel extending over the region (   h/2, h/2). show
that

smoothing in regression

exercises

(cid:20)

e [(cid:98)  (0)] = e

  (x)

=   (0) +   
  (cid:48)(cid:48)(0)

(cid:19)(cid:21)
(cid:18)

(cid:12)(cid:12)(cid:12)(cid:12)x    
(cid:20)

(0)e

(cid:48)

,

h
2

    h
2

(cid:18)
(cid:12)(cid:12)(cid:12)(cid:12)x    
(cid:20)
(cid:12)(cid:12)(cid:12)(cid:12)x    
(cid:18)

x

(cid:19)(cid:21)
(cid:19)(cid:21)

h
2

,

    h
2

show that e(cid:2)x(cid:12)(cid:12)x    (cid:0)    h

2 , h
2

2

e

+

x 2

(cid:1)(cid:3) = o(f(cid:48)(0)h2), and that e(cid:2)x 2(cid:12)(cid:12)x    (cid:0)    h

+ o(h2)

    h
2

h
2

,

2 , h
2

4.2 use eqs. 4.21, 4.17 and 4.16 to show that the excess risk of the kernel smoothing, when

conclude that the over-all bias is o(h2).
the bandwidth is selected by cross-validation, is also o(n   4/5).

4.3 generate 1000 data points where x is uniformly distributed between    4 and 4, and y =
e7x/(1 + e7x) +  , with   gaussian and with variance 0.01. use non-parametric regression

to estimate (cid:98)  (x), and then use eq. 4.31 to    nd the average predictive comparison. now

re-run the simulation with x uniform on the interval [0, 0.5] and re-calculate the average
predictive comparison. what happened?

(4.33)

(4.34)

(cid:1)(cid:3) = o(h2).

exercises

101

par(mfcol = c(2, 1))
true.r <- function(x) {

sin(x) * cos(20 * x)

}
true.s <- function(x) {

log(x + 1)

}
curve(true.r(x), from = 0, to = 3, xlab = "x", ylab = expression(r(x)))
curve(true.s(x), from = 0, to = 3, xlab = "x", ylab = expression(s(x)))
par(mfcol = c(1, 1))

figure 4.3 two curves for the running example. above,
r(x) = sin x cos 20x ; below, s(x) = log 1 + x (we will not use this
information about the exact functional forms).

0.00.51.01.52.02.53.0   1.00.00.51.0xr(x)0.00.51.01.52.02.53.00.00.40.81.2xs(x)102

smoothing in regression

x = runif(300, 0, 3)
yr = true.r(x) + rnorm(length(x), 0, 0.15)
ys = true.s(x) + rnorm(length(x), 0, 0.15)
par(mfcol = c(2, 1))
plot(x, yr, xlab = "x", ylab = expression(r(x) + epsilon))
curve(true.r(x), col = "grey", add = true)
plot(x, ys, xlab = "x", ylab = expression(s(x) + eta))
curve(true.s(x), col = "grey", add = true)

figure 4.4 the curves of fig. 4.3 (in grey), plus iid gaussian noise with
mean 0 and standard deviation 0.15. the two curves are sampled at the
same x values, but with di   erent noise realizations.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.0   1.00.01.0xr(x)+ellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.00.00.51.01.5xs(x)+hexercises

103

par(mfcol = c(2, 1))
x.focus <- 1.6
x.lo <- x.focus - 0.1
x.hi <- x.focus + 0.1
colors = ifelse((x < x.hi) & (x > x.lo), "black", "grey")
plot(x, yr, xlab = "x", ylab = expression(r(x) + epsilon), col = colors)
curve(true.r(x), col = "grey", add = true)
points(x.focus, mean(yr[(x < x.hi) & (x > x.lo)]), pch = 18, cex = 2)
plot(x, ys, xlab = "x", ylab = expression(s(x) + eta), col = colors)
curve(true.s(x), col = "grey", add = true)
points(x.focus, mean(ys[(x < x.hi) & (x > x.lo)]), pch = 18, cex = 2)
par(mfcol = c(1, 1))

figure 4.5 relationship between smoothing and function roughness. in
both panels we estimate the value of the regression function at x = 1.6 by
averaging observations where 1.5 < xi < 1.7 (black points, others are
   ghosted    in grey). the location of the average in shown by the large black
diamond. this works poorly for the rough function r in the upper panel (the
bias is large), but much better for the smoother function in the lower panel
(the bias is small).

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.0   1.00.01.0xr(x)+ellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.00.00.51.01.5xs(x)+h104

smoothing in regression

figure 4.6 error of estimating r(1.6) (solid line) and s(1.6) (dashed) from
averaging observed values at 1.6     h < x < 1.6 + h, for di   erent radii h. the
grey is   , the standard deviation of the noise     how can the estimation
error be smaller than that?

rbws <- cv_bws_npreg(x, yr, bandwidths = (1:100)/200)
sbws <- cv_bws_npreg(x, ys, bandwidths = (1:100)/200)
plot(1:100/200, sqrt(rbws$cv_mses), xlab = "bandwidth", ylab = "root cv mse",

type = "l", ylim = c(0, 0.6), log = "x")

lines(1:100/200, sqrt(sbws$cv_mses), lty = "dashed")
abline(h = 0.15, col = "grey")

figure 4.7 cross-validated estimate of the (root) mean-squard error as a
function of the bandwidth (solid curve, r data; dashed, s data; grey line,
true noise   ). notice that the rougher curve is more sensitive to the choice of
bandwidth, and that the smoother curve is more predictable at every choice
of bandwidth. cv selects bandwidths of 0.02 for r and 0.095 for s.

0.010.020.050.100.200.501.000.00.20.40.60.81.0radius of averaging windowabsolute value of error0.0050.0100.0200.0500.1000.2000.5000.00.10.20.30.40.50.6bandwidthroot cv mseexercises

105

x.ord = order(x)
par(mfcol = c(2, 1))
plot(x, yr, xlab = "x", ylab = expression(r(x) + epsilon))
rhat <- npreg(bws = rbws$best.bw, txdat = x, tydat = yr)
lines(x[x.ord], fitted(rhat)[x.ord], lwd = 4)
curve(true.r(x), col = "grey", add = true, lwd = 2)
plot(x, ys, xlab = "x", ylab = expression(s(x) + eta))
shat <- npreg(bws = sbws$best.bw, txdat = x, tydat = ys)
lines(x[x.ord], fitted(shat)[x.ord], lwd = 4)
curve(true.s(x), col = "grey", add = true, lwd = 2)
par(mfcol = c(1, 1))

figure 4.8 data from the running examples (circles), true regression
functions (grey) and kernel estimates of regression functions with
cv-selected bandwidths (black). r notes: the x values aren   t sorted, so we
need to put them in order before drawing lines connecting the    tted values; then
we need to put the    tted values in the same order. alternately, we could have used
predict on the sorted values, as in   4.3.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.0   1.00.01.0xr(x)+ellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.00.00.51.01.5xs(x)+h106

smoothing in regression

x1.points <- seq(-3, 3, length.out = 100)
x2.points <- x1.points
x12grid <- expand.grid(x1 = x1.points, x2 = x2.points)
y <- matrix(0, nrow = 100, ncol = 100)
y <- outer(x1.points, x2.points, f)
library(lattice)
wireframe(y ~ x12grid$x1 * x12grid$x2, scales = list(arrows = false), xlab = expression(x^1),

ylab = expression(x^2), zlab = "y")

figure 4.9 an example of a regression surface that would be very hard to
learn by piling together interaction terms in a id75 framework.
(can you guess what the mystery function f is?)     wireframe is from the
graphics library lattice.

   3   2   10123   3   2   101230.20.40.60.8x1x2yexercises

107

x1.noise <- runif(1000, min = -3, max = 3)
x2.noise <- runif(1000, min = -3, max = 3)
y.noise <- f(x1.noise, x2.noise) + rnorm(1000, 0, 0.05)
noise <- data.frame(y = y.noise, x1 = x1.noise, x2 = x2.noise)
cloud(y ~ x1 * x2, data = noise, col = "black", scales = list(arrows = false),

xlab = expression(x^1), ylab = expression(x^2), zlab = "y")

figure 4.10 1000 points sampled from the surface in figure 4.9, plus
independent gaussian noise (s.d. = 0.05).

   2   1012   2   10120.00.20.40.60.81.0x1x2y108

smoothing in regression

noise.np <- npreg(y ~ x1 + x2, data = noise)
y.out <- matrix(0, 100, 100)
y.out <- predict(noise.np, newdata = x12grid)
wireframe(y.out ~ x12grid$x1 * x12grid$x2, scales = list(arrows = false), xlab = expression(x^1),

ylab = expression(x^2), zlab = "y")

figure 4.11 gaussian kernel regression of the points in figure 4.10. notice
that the estimated function will make predictions at arbitrary points, not
just the places where there was training data.

   3   2   10123   3   2   101230.00.20.40.60.8x1x2yexercises

109

new.frame <- data.frame(x1 = seq(-3, 3, length.out = 300), x2 = median(x2.noise))
plot(new.frame$x1, predict(noise.np, newdata = new.frame), type = "l", xlab = expression(x^1),

ylab = "y", ylim = c(0, 1))

new.frame$x2 <- quantile(x2.noise, 0.25)
lines(new.frame$x1, predict(noise.np, newdata = new.frame), lty = 2)
new.frame$x2 <- quantile(x2.noise, 0.75)
lines(new.frame$x1, predict(noise.np, newdata = new.frame), lty = 3)

figure 4.12 predicted mean response as function of the    rst input
coordinate x1 for the example data, evaluated with the second coordinate x2
set to the median (solid), its 25th percentile (dashed) and its 75th percentile
(dotted). note that the changing shape of the partial response curve
indicates an interaction between the two inputs. also, note that the model
can make predictions at arbitrary coordinates, whether or not there were
any training points there.

   3   2   101230.00.20.40.60.81.0x1y110

smoothing in regression

curve(exp(7 * x)/(1 + exp(7 * x)), from = -5, to = 5, ylab = "y")

figure 4.13 the function of eq. 4.28, with    = 7.

plot(demo.np1, theta = 40, view = "fixed")

figure 4.14 plot of the kernel regression with just two predictor variables.
(see help(npplot) for plotting options.

   4   20240.00.20.40.60.81.0xyx   0.50.00.5z   30   20   1001020y   1001020[theta= 40, phi= 10]exercises

111

plot(demo.np3)

figure 4.15 predictions of demo.np3 as each variable is swept over its
range, with the others held at their medians.

   1.0   0.50.00.51.005101520x y   30   20   10010203005101520z yab05101520factor(w) yll112

smoothing in regression

x.seq <- seq(from = -1, to = 1, length.out = 50)
z.seq <- seq(from = -30, to = 30, length.out = 50)
grid.a <- expand.grid(x = x.seq, z = z.seq, w = "a")
grid.b <- expand.grid(x = x.seq, z = z.seq, w = "b")
yhat.a <- predict(demo.np4, newdata = grid.a)
yhat.b <- predict(demo.np4, newdata = grid.b)
par(mfrow = c(1, 2))
persp(x = x.seq, y = z.seq, z = matrix(yhat.a, nrow = 50), theta = 40, main = "w=a",

xlab = "x", ylab = "z", zlab = "y", ticktype = "detailed")

persp(x = x.seq, y = z.seq, z = matrix(yhat.b, nrow = 50), theta = 40, main = "w=b",

xlab = "x", ylab = "z", zlab = "y", ticktype = "detailed")

figure 4.16 the regression surfaces learned for the demo function at the
two di   erent values of the categorical variable. note that holding z    xed, we
always see a parabolic shape as we move along x (as we should), while
whether we see a line or something close to a step function at constant x
depends on w, as it should.

x   1.0   0.50.00.51.0z   30   20   100102030y   2002040w=ax   1.0   0.50.00.51.0z   30   20   100102030y   1001020w=b5

simulation

you will recall from your previous statistics courses that quantifying uncertainty
in statistical id136 requires us to get at the sampling distributions of things
like estimators. when the very strong simplifying assumptions of basic statistics
courses do not apply1, there is little hope of being able to write down sampling
distributions in closed form. there is equally little help when the estimates are
themselves complex objects, like kernel regression curves or even histograms,
rather than short,    xed-length parameter vectors. we get around this by using
simulation to approximate the sampling distributions we can   t calculate.

5.1 what is a simulation?

a mathematical model is a mathematical story about how the data could have
been made, or generated. simulating the model means following that story,
implementing it, step by step, in order to produce something which should look
like the data     what   s sometimes called synthetic data, or surrogate data,
or a realization of the model. in a stochastic model, some of the steps we need
to follow involve a random component, and so multiple simulations starting from
exactly the same inputs or initial conditions will not give exactly the same outputs
or realizations. rather, the model speci   es a distribution over the realizations,
and doing many simulations gives us a good approximation to this distribution.
for a trivial example, consider a model with three random variables, x1    
2), with x1        x2, and x3 = x1 + x2. simulating from
n (  1,   2
this model means drawing a random value from the    rst normal distribution for
x1, drawing a second random value for x2, and adding them together to get x3.
the marginal distribution of x3, and the joint distribution of (x1, x2, x3), are
implicit in this speci   cation of the model, and we can    nd them by running the
simulation.

1), x2     n (  2,   2

in this particular case, we could also    nd the distribution of x3, and the joint
distribution, by id203 calculations of the kind you learned how to do in
your basic id203 courses. for instance, x3 is n (  1 +   2,   2
2). these

1 +   2

1 as discussed ad nauseam in chapter 2, in your linear models class, you learned about the sampling

distribution of regression coe   cients when the linear model is true, and the noise is gaussian,
independent of the predictor variables, and has constant variance. as an exercise, try to get parallel
results when the noise has a t distribution with 10 degrees of freedom.

113

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

114

simulation

analytical id203 calculations can usually be thought of as just short-cuts
for exhaustive simulations.

5.2 how do we simulate stochastic models?

5.2.1 chaining together random variables

stochastic models are usually speci   ed by sets of conditional distributions for one
random variable, given some other variable or variables. for instance, a simple
id75 model might have the speci   cation

x     u(xmin, xmax)

y |x     n (  0 +   1x,   2)

(5.1)
(5.2)

if we knew how to generate a random variable from the distributions given
on the right-hand sides, we could simulate the whole model by chaining together
draws from those conditional distributions. this is in fact the general strategy for
simulating any sort of stochastic model, by chaining together random variables.2
you might ask why we don   t start by generating a random y , and then gen-
erate x by drawing from the x|y distribution. the basic answer is that you
could, but it would generally be messier. (just try to work out the conditional
distribution x|y .) more broadly, in chapter 20, we   ll see how to arrange the
variables in complicated id203 models in a natural order, so that we start
with independent,    exogenous    variables, then    rst-generation variables which
only need to be conditioned on the exogenous variables, then second-generation
variables which are conditioned on    rst-generation ones, and so forth. this is also
the natural order for simulation.

the upshot is that we can reduce the problem of simulating to that of gener-

ating random variables.

5.2.2 random variable generation

5.2.2.1 built-in random number generators

r provides random number generators for most of the most common distributions.
by convention, the names of these functions all begin with the letter    r   , followed
by the abbreviation of the functions, and the    rst argument is always the number
of draws to make, followed by the parameters of the distribution. some examples:

rnorm(n, mean = 0, sd = 1)
runif(n, min = 0, max = 1)
rexp(n, rate = 1)
rpois(n, lambda)
rbinom(n, size, prob)

2 in this case, we could in principle    rst generate y , and then draw from y |x, but have fun    nding
those distributions. especially have fun if, say, x has a t distribution with 10 degrees of freedom. (i
keep coming back to that idea, because it   s really a very small change from being gaussian.)

5.2 how do we simulate stochastic models?

115

a further convention is that these parameters can be vectorized. rather than
giving a single mean and standard deviation (say) for multiple draws from the
gaussian distribution, each draw can have its own:

rnorm(10, mean = 1:10, sd = 1/sqrt(1:10))

that instance is rather trivial, but the exact same principle would be at work

here:

rnorm(nrow(x), mean = predict(regression.model, newdata = x), sd = predict(volatility.model,

newdata = x))

where regression.model and volatility.model are previously-de   ned parts
of the model which tell us about conditional expectations and conditional vari-
ances.

of course, none of this explains how r actually draws from any of these distri-
butions; it   s all at the level of a black box, which is to say black magic. because
ignorance is evil, and, even worse, unhelpful when we need to go beyond the
standard distributions, it   s worth opening the black box just a bit. we   ll look
at using transformations between distributions, and, in particular, transforming
uniform distributions into others (  5.2.2.3). appendix m explains some more
advanced methods, and looks at the issue of how to get uniformly-distributed
random numbers in the    rst place.

5.2.2.2 transformations

if we can generate a random variable z with some distribution, and v = g(z),
then we can generate v . so one thing which gets a lot of attention is writing
random variables as transformations of one another     ideally as transformations
of easy-to-generate variables.

example: from standard to customized gaussians

suppose we can generate random numbers from the standard gaussian distri-
bution z     n (0, 1). then we can generate from n (  ,   2) as   z +   . we can
generate   2 random variables with 1 degree of freedom as z 2. we can generate
  2 random variables with d degrees of freedom by summing d independent copies
of z 2.

in particular, if we can generate random numbers uniformly distributed be-
tween 0 and 1, we can use this to generate anything which is a transformation of
a uniform distribution. how far does that extend?

5.2.2.3 quantile method

suppose that we know the quantile function qz for the random variable z we
want, so that qz(0.5) is the median of x, qz(0.9) is the 90th percentile, and in
general qz(p) is bigger than or equal to z with id203 p. qz comes as a pair
with the cumulative distribution function fz, since

qz(fz(a)) = a, fz(qz(p)) = p

(5.3)

116

simulation

in the quantile method (or inverse distribution transform method), we
generate a uniform random number u and feed it as the argument to qz. now
qz(u ) has the distribution function fz:

pr (qz(u )     a) = pr (fz(qz(u ))     fz(a))

= pr (u     fz(a))
= fz(a)

(5.4)
(5.5)
(5.6)

where the last line uses the fact that u is uniform on [0, 1], and the    rst line
uses the fact that fz is a non-decreasing function, so b     a is true if and only if
fz(b)     fz(a).
example. the cdf of the exponential distribution with rate    is 1    e     z. the
quantile function q(p) is thus     log (1   p)
. (notice that this is positive, because
1    p < 1 and so log (1     p) < 0, and that it has units of 1/  , which are the units
    exp(  ). this is
of z, as it should.) therefore, if u unif(0, 1), then     log (1   u )
the method used by rexp().

  

  

(cid:17)     

example: power laws

the pareto distribution or power law is a two-parameter family, f (z;   , z0) =
if z     z0, with density 0 otherwise. integration shows that the cumu-
     1
z0

(cid:16) z
lative distribution function is f (z;   , z0) = 1    (cid:16) z

(cid:17)     +1

z0

. the quantile function
     1 . (notice that this has the same units as

z0

    1

therefore is q(p;   , z0) = z0(1     p)
z, as it should.)

example: gaussians

the standard gaussian n (0, 1) does not have a closed form for its quantile func-
tion, but there are fast and accurate ways of calculating it numerically (they   re
what stand behind qnorm), so the quantile method can be used. in practice, there
are other transformation methods which are even faster, but rely on special tricks.
since qz(u ) has the same distribution function as z, we can use the quantile
method, as long as we can calculate qz. since qz always exists, in principle
this solves the problem. in practice, we need to calculate qz before we can use
it, and this may not have a closed form, and numerical approximations may be
intractable.3 in such situations, we turn to more advanced methods, like those
described in appendix m.

5.2.3 sampling

a complement to drawing from given distributions is to sample from a given
collection of objects. this is a common task, so r has a function to do it:

3 in essence, we have to solve the nonlinear equation fz (z) = p for z over and over for di   erent p    

and that assumes we can easily calculate fz .

5.2 how do we simulate stochastic models?

117

sample(x, size, replace = false, prob = null)

here x is a vector which contains the objects we   re going to sample from.
size is the number of samples we want to draw from x. replace says whether
the samples are drawn with or without replacement. (if replace=true, then
size can be arbitrarily larger than the length of x. if replace=false, having a
larger size doesn   t make sense.) finally, the optional argument prob allows for
weighted sampling; ideally, prob is a vector of probabilities as long as x, giving
the id203 of drawing each element of x4.

as a convenience for a common situation, running sample with one argument

produces a random permutation of the input, i.e.,

sample(x)

is equivalent to

sample(x, size = length(x), replace = false)

for example, the code for k-fold cross-validation, code example 3, had the

lines

fold.labels <- sample(rep(1:nfolds, length.out = nrow(data)))

here, rep repeats the numbers from 1 to nfolds until we have one number
for each row of the data frame, say 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2 if there were twelve
rows. then sample shu   es the order of those numbers randomly. this then would
give an assignment of each row of df to one (and only one) of    ve folds.

5.2.3.1 sampling rows from data frames

when we have multivariate data (which is the usual situation), we typically
arrange it into a data-frame, where each row records one unit of observation,
with multiple interdependent columns. the natural notion of sampling is then to
draw a random sample of the data points, which in that representation amounts
to a random sample of the rows. we can implement this simply by sampling row
numbers. for instance, this command,

df[sample(1:nrow(df), size = b), ]

will create a new data frame from b, by selecting b rows from df without
replacement. it is an easy exercise to    gure out how to sample from a data frame
with replacement, and with unequal probabilities per row.

4 if the elements of prob do not add up to 1, but are positive, they will be normalized by their sum,

e.g., setting prob=c(9,9,1) will assign probabilities ( 9

19 , 9

19 , 1

19 ) to the three elements of x.

118

simulation

5.2.3.2 multinomials and multinoullis

if we want to draw one value from a multinomial distribution with probabilities
p = (p1, p2, . . . pk), then we can use sample:

sample(1:k, size = 1, prob = p)

if we want to simulate a    multinoulli    process5, i.e., a sequence of independent
and identically distributed multinomial random variables, then we can easily do
so:

rmultinoulli <- function(n, prob) {

k <- length(prob)
return(sample(1:k, size = n, replace = true, prob = prob))

}

of course, the labels needn   t be the integers 1 : k (exercise 5.1).

5.2.3.3 probabilities of observation

often, our models of how the data are generated will break up into two parts.
one part is a model of how actual variables are related to each other out in the
world. (e.g., we might model how education and racial categories are related to
occupation, and occupation is related to income.) the other part is a model of
how variables come to be recorded in our data, and the distortions they might
undergo in the course of doing so. (e.g., we might model the id203 that
someone appears in a survey as a function of race and income.) plausible sampling
mechanisms often make the id203 of appearing in the data a function of
some of the variables. this can then have important consequences when we try
to draw id136s about the whole population or process from the sample we
happen to have seen (see, e.g., app. k).

income <- rnorm(n, mean = predict(income.model, x), sd = sigma)
capture.probabilities <- predict(observation.model, x)
observed.income <- sample(income, size = b, prob = capture.probabilities)

5.3 repeating simulations

because simulations are often most useful when they are repeated many times,
r has a command to repeat a whole block of code:

replicate(n, expr)

here expr is some executable    expression    in r, basically something you could

type in the terminal, and n is the number of times to repeat it.

for instance,

5 a handy term i learned from gustavo lacerda.

5.4 why simulate?

119

output <- replicate(1000, rnorm(length(x), beta0 + beta1 * x, sigma))

will replicate, 1000 times, sampling from the predictive distribution of a gaus-
sian id75 model. conceptually, this is equivalent to doing something
like

output <- matrix(0, nrow = 1000, ncol = length(x))
for (i in 1:1000) {

output[i, ] <- rnorm(length(x), beta0 + beta1 * x, sigma)

}

but the replicate version has two great advantages. first, it is faster, because
r processes it with specially-optimized code. (loops are especially slow in r.)
second, and far more importantly, it is clearer: it makes it obvious what is being
done, in one line, and leaves the computer to    gure out the boring and mundane
details of how best to implement it.

5.4 why simulate?

there are three major uses for simulation: to understand a model, to check it,
and to    t it. we will deal with the    rst two here, and return to    tting in chapter
26, after we   ve looked at dealing with dependence and hidden variables.

5.4.1 understanding the model; monte carlo

we understand a model by seeing what it predicts about the variables we care
about, and the relationships between them. sometimes those predictions are easy
to extract from a mathematical representation of the model, but often they aren   t.
with a model we can simulate, however, we can just run the model and see what
happens.

our stochastic model gives a distribution for some random variable z, which
in general is a complicated, multivariate object with lots of interdependent com-
ponents. we may also be interested in some complicated function g of z, such
as, say, the ratio of two components of z, or even some nonparametric curve    t
through the data points. how do we know what the model says about g?

assuming we can make draws from the distribution of z, we can    nd the
distribution of any function of it we like, to as much precision as we want. suppose
that   z1,   z2, . . .   zb are the outputs of b independent runs of the model     b di   erent
replicates of the model. (the tilde is a reminder that these are just simulations.)
we can calculate g on each of them, getting g(   z1), g(   z2), . . . g(   zb). if averaging
makes sense for these values, then

b(cid:88)

i=1

1
b

g(   zi)             
b      

e [g(z)]

(5.7)

by the law of large numbers. so simulation and averaging lets us get expectation

120

simulation

(cid:80)b
values. this basic observation is the seed of the monte carlo method.6 if our
simulations are independent, we can even use the central limit theorem to say
i=1 g(   zi) has approximately the distribution n (e [g(z)] , v [g(z)] /b).
that 1
b
of course, if you can get expectation values, you can also get variances. (this
is handy if trying to apply the central limit theorem!) you can also get any
higher moments     if, for whatever reason, you need the kurtosis, you just have
to simulate enough.

you can also pick any set s and get the id203 that g(z) falls into that

set:

1
b

b(cid:88)

i=1

1s(g(   zi))             

b       pr (g(z)     s)

(5.8)

the reason this works is of course that pr (g(z)     s) = e [1s(g(z))], and we can
use the law of large numbers again. so we can get the whole distribution of any
complicated function of the model that we want, as soon as we can simulate the
model. it is really only a little harder to get the complete sampling distribution
than it is to get the expectation value, and the exact same ideas apply.

5.4.2 checking the model

an important but under-appreciated use for simulation is to check models after
they have been    t. if the model is right, after all, it represents the mechanism
which generates the data. this means that when we simulate, we run that mecha-
nism, and the surrogate data which comes out of the machine should look like the
real data. more exactly, the real data should look like a typical realization of the
model. if it does not, then the model   s account of the data-generating mechanism
is systematically wrong in some way. by carefully choosing the simulations we
perform, we can learn a lot about how the model breaks down and how it might
need to be improved.7

5.4.2.1    exploratory    analysis of simulations

often the comparison between simulations and data can be done qualitatively
and visually. for example, a classic data set concerns the time between eruptions
of the old faithful geyser in yellowstone, and how they relate to the duration of
the latest eruption. a common exercise is to    t a regression line to the data by
ordinary least squares:

library(mass)
data(geyser)
fit.ols <- lm(waiting ~ duration, data = geyser)

6 the name was coined by the physicists who used the method to do calculations relating to designing

the hydrogen bomb; see metropolis et al. (1953). folklore among physicists says that the method
goes back at least to enrico fermi in the 1930s, without the cutesy name.

7    might   , because sometimes (e.g.,   1.4.2) we   re better o    with a model that makes systematic

mistakes, if they   re small and getting it right would be a hassle.

5.4 why simulate?

121

plot(geyser$duration, geyser$waiting, xlab = "duration", ylab = "waiting")
abline(fit.ols)

figure 5.1 data for the geyser data set, plus the ols regression line.

figure 5.1 shows the data, together with the ols line. it doesn   t look that
great, but if someone insisted it was a triumph of quantitative vulcanology, how
could you show they were wrong?

we   ll consider general tests of regression speci   cations in chapter 9. for now,
let   s focus on the way ols is usually presented as part of a stochastic model for
the response conditional on the input, with gaussian and homoskedastic noise.
in this case, the stochastic model is waiting =   0 +   1duration +  , with      
n (0,   2). if we simulate from this id203 model, we   ll get something we can

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123455060708090100110durationwaiting122

simulation

rgeyser <- function() {

n <- nrow(geyser)
sigma <- summary(fit.ols)$sigma
new.waiting <- rnorm(n, mean = fitted(fit.ols), sd = sigma)
new.geyser <- data.frame(duration = geyser$duration, waiting = new.waiting)
return(new.geyser)

}

code example 6: function for generating surrogate data sets from the linear model    t to
geyser.

compare to the actual data, to help us assess whether the scatter around that
regression line is really bothersome. since ols doesn   t require us to assume a
distribution for the input variable (here, duration), the simulation function in
code example 6 leaves those values alone, but regenerates values of the response
(waiting) according to the model assumptions.

a useful principle for model checking is that if we do some exploratory data
analyses of the real data, doing the same analyses to realizations of the model
should give roughly the same results (gelman, 2003; hunter et al., 2008; gelman
and shalizi, 2013). this is a test the model fails. figure 5.2 shows the actual
histogram of waiting, plus the histogram produced by simulating     reality is
clearly bimodal, but the model is unimodal. similarly, figure 5.3 shows the real
data, the ols line, and a simulation from the ols model. it   s visually clear that
the deviations of the real data from the regression line are both bigger and more
patterned than those we get from simulating the model, so something is wrong
with the latter.

by itself, just seeing that data doesn   t look like a realization of the model isn   t
super informative, since we   d really like to know how the model   s broken, and
so how to    x it. further simulations, comparing more detailed analyses of the
data to analyses of the simulation output, are often very helpful here. looking
at figure 5.3, we might suspect that one problem is heteroskedasticity     the
variance isn   t constant. this suspicion is entirely correct, and will be explored in
  10.3.2.

5.4.3 sensitivity analysis

often, the statistical id136 we do on the data is predicated on certain assump-
tions about how the data is generated. we   ve talked a lot about the gaussian-
noise assumptions that usually accompany id75, but there are many
others. for instance, if we have missing values for some variables and just ignore
incomplete rows, we are implicitly assuming that data are    missing at random   ,
rather than in some systematic way that would carry information about what the
missing values were (see app. k). often, these assumptions make our analysis
much neater than it otherwise would be, so it would be convenient if they were
true.

as a wise man said long ago,    the method of    postulating    what we want has

5.4 why simulate?

123

hist(geyser$waiting, freq = false, xlab = "waiting", main = "", sub = "", col = "grey")
lines(hist(rgeyser()$waiting, plot = false), freq = false, lty = "dashed")

figure 5.2 actual density of the waiting time between eruptions (grey bars,
solid lines) and that produced by simulating the ols model (dashed lines).

many advantages; they are the same as the advantages of theft over honest toil   
(russell, 1920, ch. vii, p. 71). in statistics, honest toil often takes the form of
sensitivity analysis, of seeing how much our conclusions would change if the
assumptions were violated, i.e., of checking how sensitive our id136s are to the
assumptions. in principle, this means setting up models where the assumptions
are more or less violated, or violated in di   erent ways, analyzing them as though
the assumptions held, and seeing how badly wrong we go. of course, if that

waitingdensity4050607080901001100.000.010.020.030.04124

simulation

plot(geyser$duration, geyser$waiting, xlab = "duration", ylab = "waiting")
abline(fit.ols)
points(rgeyser(), pch = 20, cex = 0.5)

figure 5.3 as in figure 5.1, plus one realization of simulating the ols
model (small black dots).

was easy to do in closed form, we often wouldn   t have needed to make those
assumptions in the    rst place.

on the other hand, it   s usually pretty easy to simulate a model where the
assumption is violated, run our original, assumption-laden analysis on the sim-
ulation output, and see what happens. because it   s a simulation, we know the
complete truth about the data-generating process, and can assess how far o    our
id136s are. in favorable circumstances, our id136s don   t mess up too much

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123455060708090100110durationwaitinglllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll5.5 further reading

125

even when the assumptions we used to motivate the analysis are badly wrong.
sometimes, however, we discover that even tiny violations of our initial assump-
tions lead to large errors in our id136s. then we either need to make some
compelling case for those assumptions, or be very cautious in our id136s.

5.5 further reading

simulation will be used in nearly every subsequent chapter. it is the key to the
   bootstrap    technique for quantifying uncertainty (ch. 6), and the foundation
for a whole set of methods for dealing with complex models of dependent data
(ch. 26).

many texts on scienti   c programming discuss simulation, including press et al.
(1992) and, using r, jones et al. (2009). there are also many more specialized
texts on simulation in various applied areas. it must be said that many references
on simulation present it as almost completely disconnected from statistics and
data analysis, giving the impression that id203 models just fall from the
sky. guttorp (1995) is an excellent exception.

for further reading on methods of drawing random variables from a given
distribution, on monte carlo, and on generating uniform random numbers, see
appendix m. for doing statistical id136 by comparing simulations to data,
see chapter 26.

when all (!) you need to do is draw numbers from a id203 distribution
which isn   t one of the ones built in to r, it   s worth checking cran   s    task
view    on id203 distributions, https://cran.r-project.org/web/views/
distributions.html.

for sensitivity analyses, miller (1998) describes how to use modern optimiza-
tion methods to actively search for settings in simulation models which break
desired behaviors or conclusions. i have not seen this idea applied to sensitivity
analyses for statistical models, but it really ought to be.

5.1 modify rmultinoulli from   5.2.3.2 so that the values in the output are not the integers

from 1 to k, but come from a vector of arbitrary labels.

exercises

6

the bootstrap

we are now several chapters into a statistics class and have said basically nothing
about uncertainty. this should seem odd, and may even be disturbing if you are
very attached to your p-values and saying variables have    signi   cant e   ects   .
it is time to remedy this, and talk about how we can quantify uncertainty for
complex models. the key technique here is what   s called id64, or the
bootstrap.

6.1 stochastic models, uncertainty, sampling distributions

statistics is the branch of mathematical engineering which studies ways of draw-
ing id136s from limited and imperfect data. we want to know how a neuron
in a rat   s brain responds when one of its whiskers gets tweaked, or how many rats
live in pittsburgh, or how high the water will get under the 16th street bridge
during may, or the typical course of daily temperatures in the city over the year,
or the relationship between the number of birds of prey in schenley park in the
spring and the number of rats the previous fall. we have some data on all of these
things. but we know that our data is incomplete, and experience tells us that
repeating our experiments or observations, even taking great care to replicate the
conditions, gives more or less di   erent answers every time. it is foolish to treat
any id136 from the data in hand as certain.

if all data sources were totally capricious, there   d be nothing to do beyond
piously qualifying every conclusion with    but we could be wrong about this   . a
mathematical discipline of statistics is possible because while repeating an ex-
periment gives di   erent results, some kinds of results are more common than
others; their relative frequencies are reasonably stable. we thus model the data-
generating mechanism through id203 distributions and stochastic processes.
when and why we can use stochastic models are very deep questions, but ones
for another time. if we can use them in our problem, quantities like the ones
i mentioned above are represented as functions of the stochastic model, i.e., of
the underlying id203 distribution. since a function of a function is a    func-
tional   , and these quantities are functions of the true id203 distribution
function, we   ll call these functionals or statistical functionals1. functionals
could be single numbers (like the total rat population), or vectors, or even whole

1 most writers in theoretical statistics just call them    parameters    in a generalized sense, but i will

126

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

6.1 stochastic models, uncertainty, sampling distributions

127

curves (like the expected time-course of temperature over the year, or the regres-
sion of hawks now on rats earlier). statistical id136 becomes estimating those
functionals, or testing hypotheses about them.

these estimates and other id136s are functions of the data values, which
means that they inherit variability from the underlying stochastic process. if we
   re-ran the tape    (as the late, great stephen jay gould used to say), we would get
di   erent data, with a certain characteristic distribution, and applying a    xed pro-
cedure would yield di   erent id136s, again with a certain distribution. statis-
ticians want to use this distribution to quantify the uncertainty of the id136s.
for instance, the standard error is an answer to the question    by how much
would our estimate of this functional vary, typically, from one replication of the
experiment to another?    (it presumes a particular meaning for    typically vary   ,
as the root-mean-square deviation around the mean.) a con   dence region on a
parameter, likewise, is the answer to    what are all the values of the parameter
which could have produced this data with at least some speci   ed id203?   ,
i.e., all the parameter values under which our data are not low-id203 out-
liers. the con   dence region is a promise that either the true parameter point lies
in that region, or something very unlikely under any circumstances happened    
or that our stochastic model is wrong.

to get things like standard errors or con   dence intervals, we need to know the
distribution of our estimates around the true values of our functionals. these
sampling distributions follow, remember, from the distribution of the data,
since our estimates are functions of the data. mathematically the problem is well-
de   ned, but actually computing anything is another story. estimates are typically
complicated functions of the data, and mathematically-convenient distributions
may all be poor approximations to the data source. saying anything in closed
form about the distribution of estimates can be simply hopeless. the two classical
responses of statisticians were to focus on tractable special cases, and to appeal
to asymptotics.

your introductory statistics courses mostly drilled you in the special cases.
from one side, limit the kind of estimator we use to those with a simple math-
ematical form     say, means and other linear functions of the data. from the
other, assume that the id203 distributions featured in the stochastic model
take one of a few forms for which exact calculation is possible, analytically or
via tabulated special functions. most such distributions have origin myths: the
gaussian arises from averaging many independent variables of equal size (say,
the many genes which contribute to height in humans); the poisson distribu-
tion comes from counting how many of a large number of independent and
individually-improbable events have occurred (say, radioactive nuclei decaying
in a given second), etc. squeezed from both ends, the sampling distribution of
estimators and other functions of the data becomes exactly calculable in terms
of the aforementioned special functions.

try to restrict that word to actual parameters specifying statistical models, to minimize confusion. i
may slip up.

128

the bootstrap

that these origin myths invoke various limits is no accident. the great results
of id203 theory     the laws of large numbers, the ergodic theorem, the
central limit theorem, etc.     describe limits in which all stochastic processes
in broad classes of models display the same asymptotic behavior. the central
limit theorem, for instance, says that if we average more and more independent
random quantities with a common distribution, and that common distribution
isn   t too pathological, then the average becomes closer and closer to a gaussian2.
typically, as in the clt, the limits involve taking more and more data from
the source, so statisticians use the theorems to    nd the asymptotic, large-sample
distributions of their estimates. we have been especially devoted to re-writing
our estimates as averages of independent quantities, so that we can use the clt
to get gaussian asymptotics.

up through about the 1960s, statistics was split between developing general
ideas about how to draw and evaluate id136s with stochastic models, and
working out the properties of inferential procedures in tractable special cases
(especially the linear-and-gaussian case), or under asymptotic approximations.
this yoked a very broad and abstract theory of id136 to very narrow and con-
crete practical formulas, an uneasy combination often preserved in basic statistics
classes.

the arrival of (comparatively) cheap and fast computers made it feasible for
scientists and statisticians to record lots of data and to    t models to it, so they
did. sometimes the models were conventional ones, including the special-case as-
sumptions, which often enough turned out to be detectably, and consequentially,
wrong. at other times, scientists wanted more complicated or    exible models,
some of which had been proposed long before, but now moved from being the-
oretical curiosities to stu    that could run overnight3. in principle, asymptotics
might handle either kind of problem, but convergence to the limit could be un-
acceptably slow, especially for more complex models.

by the 1970s, then, statistics faced the problem of quantifying the uncertainty
of id136s without using either implausibly-helpful assumptions or asymp-
totics; all of the solutions turned out to demand even more computation. here
we will examine what may be the most successful solution, bradley efron   s pro-
posal to combine estimation with simulation, which he gave the less-than-clear
but persistent name of    the bootstrap    (efron, 1979).

6.2 the bootstrap principle

remember (from baby stats.) that the key to dealing with uncertainty in param-
eters and functionals is the sampling distribution of estimators. knowing what
distribution we   d get for our estimates on repeating the experiment would give
us things like standard errors. efron   s insight was that we can simulate repli-

2 the reason is that the non-gaussian parts of the distribution wash away under averaging, but the

average of two gaussians is another gaussian.

3 kernel regression (  1.5.2), kernel density estimation (ch. 14), and nearest-neighbors prediction

(  1.5.1) were all proposed in the 1950s or 1960s, but didn   t begin to be widely used until about 1980.

6.2 the bootstrap principle

129

figure 6.1 schematic for model-based id64: simulated values are
generated from the    tted model, then treated like the original data, yielding
a new estimate of the functional of interest, here called q0.01.

cation. after all, we have already    tted a model to the data, which is a guess
at the mechanism which generated the data. running that mechanism generates
simulated data which, by hypothesis, has the same distribution as the real data.
feeding the simulated data through our estimator gives us one draw from the
sampling distribution; repeating this many times yields the sampling distribu-
tion. since we are using the model to give us its own uncertainty, efron called
this    id64   ; unlike the baron munchhausen   s plan for getting himself
out of a swamp by pulling on his own bootstraps, it works.

figure 6.1 sketches the over-all process:    t a model to data, use the model to
calculate the functional, then get the sampling distribution by generating new,
synthetic data from the model and repeating the estimation on the simulation
output.

to    x notation, we   ll say that the original data is x. (in general this is a whole
data frame, not a single number.) our parameter estimate from the data is     . sur-
rogate data sets simulated from the    tted model will be   x1,   x2, . . .   xb. the cor-
responding re-estimates of the parameters on the surrogate data are     1,     2, . . .     b.

data.00168-0.002490.0183-0.005870.0139estimator   tted modelq0.01 = -0.0326parameter calculationsimulationsimulated data.00183-0.003780.00754-0.00587-0.00673estimatorq0.01 = -0.0323re-estimate130

the bootstrap

the functional of interest is estimated by the statistic4 t , with sample value
  t = t (x), and values of the surrogates of   t1 = t (   x1),   t2 = t (   x2), . . .   tb = t (   xb).
(the statistic t may be a direct function of the estimated parameters, and only
indirectly a function of x.) everything which follows applies without modi   ca-
tion when the functional of interest is the parameter, or some component of the
parameter.

in this section, we will assume that the model is correct for some value of   ,
which we will call   0. this means that we are employing a parametric model-
based bootstrap. the true (population or ensemble) values of the functional is
likewise t0.

6.2.1 variances and standard errors

the simplest thing to do is to get the variance or standard error:

(cid:100)var(cid:2)  t(cid:3) = v(cid:2)  t(cid:3)
(cid:98)se(  t) = sd(  t)

(6.1)

(6.2)

that is, we approximate the variance of our estimate of t0 under the true but
unknown distribution   0 by the variance of re-estimates   t on surrogate data from

the    tted model (cid:98)  . similarly we approximate the true standard error by the

standard deviation of the re-estimates. the logic here is that the simulated   x
has about the same distribution as the real x that our data, x, was drawn from,
so applying the same estimation procedure to the surrogate data gives us the
sampling distribution. this assumes, of course, that our model is right, and that
     is not too far from   0.

a code sketch is provided in code example 7. note that this may not work
exactly as given in some circumstances, depending on the syntax details of, say,
just what kind of data structure is needed to store   t.

we can use id64 to correct for a biased estimator. since the sampling

distribution of   t is close to that of(cid:98)t, and(cid:98)t itself is close to t0,

6.2.2 bias correction

e(cid:104)(cid:98)t
(cid:105)     t0     e(cid:2)  t(cid:3)    (cid:98)t

(6.3)

the left hand side is the bias that we want to know, and the right-hand side the
was what we can calculate with the bootstrap.

in fact, eq. 6.3 remains valid so long as the sampling distribution of (cid:98)t     t0
is close to that of   t    (cid:98)t. this is a weaker requirement than asking for (cid:98)t and
  t themselves to have similar distributions, or asking for (cid:98)t to be close to t0. in

statistical theory, a random variable whose distribution does not depend on the
parameters is called a pivot. (the metaphor is that it stays in one place while

4 t is a common symbol in the literature on the bootstrap for a generic function of the data. it may

or may not have anything to do with student   s t test for di   erence in means.

6.2 the bootstrap principle

131

rboot <- function(statistic, simulator, b) {

tboots <- replicate(b, statistic(simulator()))
if (is.null(dim(tboots))) {

tboots <- array(tboots, dim = c(1, b))

}
return(tboots)

}
bootstrap <- function(tboots, summarizer, ...) {

summaries <- apply(tboots, 1, summarizer, ...)
return(t(summaries))

}
bootstrap.se <- function(statistic, simulator, b) {

bootstrap(rboot(statistic, simulator, b), summarizer = sd)

}

code example 7: code for calculating bootstrap standard errors. the function rboot generates
b bootstrap samples (using the simulator function) and calculates the statistic on them (using
statistic). simulator needs to be a function which returns a surrogate data set in a form
suitable for statistic. (how would you modify the code to pass arguments to simulator and/or
statistic?) because every use of id64 is going to need to do this, it makes sense to
break it out as a separate function, rather than writing the same code many times (with many
chances of getting it wrong). the bootstrap function takes the output of rboot and applies
a summarizing function. bootstrap.se just calls rboot and makes the summarizing function
sd, which takes a standard deviation. important note: this is just a code sketch, because
depending on the data structure which the statistic returns, it may not (e.g.) be feasible to just
run sd on it, and so it might need some modi   cation. see detailed examples below.

bootstrap.bias <- function(simulator, statistic, b, t.hat) {

expect <- bootstrap(rboot(statistic, simulator, b), summarizer = mean)
return(expect - t.hat)

}

code example 8: sketch of code for bootstrap bias correction. arguments are as in code
example 7, except that t.hat is the estimate on the original data. important note: as with
code example 7, this is just a code sketch, because it won   t work with all data types that might
be returned by statistic, and so might require modi   cation.

the parameters turn around it.) a su   cient (but not necessary) condition for eq.

6.3 to hold is that(cid:98)t     t0 be a pivot, or approximately pivotal.

6.2.3 con   dence intervals

a con   dence interval is a random interval which contains the truth with high
id203 (the con   dence level). if the con   dence interval for g is c, and the
con   dence level is 1       , then we want

pr (t0     c) = 1       

(6.4)

no matter what the true value of t0. when we calculate a con   dence interval, our
inability to deal with distributions exactly means that the true con   dence level,
or coverage of the interval, is not quite the desired con   dence level 1       ; the

132

the bootstrap

closer it is, the better the approximation, and the more accurate the con   dence
interval.5

when we simulate, we get samples of   t, but what we really care about is the
distribution of   t. when we have enough data to start with, those two distributions
will be approximately the same. but at any given amount of data, the distribution
of   t      t will usually be closer to that of   t    t0 than the distribution of   t is to that of
  t. that is, the distribution of    uctuations around the true value usually converges
quickly. (think of the central limit theorem.) we can use this to turn information
about the distribution of   t into accurate con   dence intervals for t0, essentially by
re-centering   t around   t.

speci   cally, let q  /2 and q1     /2 be the   /2 and 1       /2 quantiles of   t. then

1        = pr

= pr
    pr

= pr

(cid:17)

(cid:16)
(cid:17)
(cid:16)
q  /2       t     q1     /2
(cid:16)
(cid:17)
q  /2       t       t       t     q1     /2       t
(cid:16)
(cid:17)
q  /2       t       t     t0     q1     /2       t
(cid:16)
(cid:17)
q  /2     2   t        t0     q1     /2     2   t
2   t     q1     /2     t0     2   t     q  /2

= pr

(6.9)
the interval c = [2   t     q  /2, 2   t     q1     /2] is random, because   t is a random
quantity, so it makes sense to talk about the id203 that it contains the true
value t0. also, notice that the upper and lower quantiles of   t have, as it were,
swapped roles in determining the upper and lower con   dence limits. finally,
notice that we do not actually know those quantiles exactly, but they   re what we
approximate by id64.

this is the basic bootstrap con   dence interval, or the pivotal ci. it is
simple and reasonably accurate, and makes a very good default choice for    nding
con   dence intervals.

6.2.3.1 other bootstrap con   dence intervals

the basic bootstrap ci relies on the distribution of   t       t being approximately
the same as that of   t     t0. even when this is false, however, it can be that the
distribution of

   =

(cid:98)se(  t)
  t     t0

is close to that of

  t       t
se(  t)

     =

5 you might wonder why we   d be unhappy if the coverage level was greater than 1       . this is

certainly better than if it   s less than the nominal con   dence level, but it usually means we could
have used a smaller set, and so been more precise about t0, without any more real risk. con   dence
intervals whose coverage is greater than the nominal level are called conservative; those with less
than nominal coverage are anti-conservative (and not, say,    liberal   ).

(6.5)

(6.6)

(6.7)

(6.8)

(6.10)

(6.11)

6.2 the bootstrap principle

133

equitails <- function(x, alpha) {
lower <- quantile(x, alpha/2)
upper <- quantile(x, 1 - alpha/2)
return(c(lower, upper))

}
bootstrap.ci <- function(statistic = null, simulator = null, tboots = null,

b = if (!is.null(tboots)) {

ncol(tboots)

}, t.hat, level) {
if (is.null(tboots)) {

stopifnot(!is.null(statistic))
stopifnot(!is.null(simulator))
stopifnot(!is.null(b))
tboots <- rboot(statistic, simulator, b)

}
alpha <- 1 - level
intervals <- bootstrap(tboots, summarizer = equitails, alpha = alpha)
upper <- t.hat + (t.hat - intervals[, 1])
lower <- t.hat + (t.hat - intervals[, 2])
cis <- cbind(lower = lower, upper = upper)
return(cis)

}

code example 9: sketch of code for calculating the basic bootstrap con   dence interval. see
code example 7 for rboot and bootstrap, and cautions about blindly applying this to arbitrary
data-types.

this is like what we calculate in a t-test, and since the t-test was invented by
   student   , these are called studentized quantities. if    and      have the same
distribution, then we can reason as above and get a con   dence interval

(cid:0)  t     (cid:98)se(  t)q     (1       /2),   t     (cid:98)se(  t)q     (  /2)(cid:1)

this is the same as the basic interval when (cid:98)se(  t) = se(  t), but di   erent otherwise.

to    nd se(  t), we need to actually do a second level of id64, as follows.

(6.12)

1. fit the model with     ,    nd   t.
2. for i     1 : b1

1. generate   xi from     
2. estimate     i,   ti
3. for j     1 : b2
   
ij from     i
1. generate x
   
2. calculate t
ij

4. set     i = standard deviation of the t
5. set     ij = t

3. set (cid:98)se(  t) = standard deviation of the   ti

   
ij     ti
    i

for all j

   
ij

4. find the   /2 and 1       /2 quantiles of the distribution of the     
5. plug into eq. 6.12.

134

the bootstrap

boot.pvalue <- function(test, simulator, b, testhat) {

testboot <- rboot(b = b, statistic = test, simulator = simulator)
p <- (sum(testboot >= testhat) + 1)/(b + 1)
return(p)

}

code example 10: bootstrap p-value calculation. testhat should be the value of the test statis-
tic on the actual data. test is a function which takes in a data set and calculates the test statis-
tic, presuming that large values indicate departure from the null hypothesis. note the +1 in the
numerator and denominator of the p-value     it would be more straightforward to leave them
o   , but this is a little more stable when b is comparatively small. (also, it keeps us from ever
reporting a p-value of exactly 0.)

the advantage of the studentized intervals is that they are more accurate than
the basic ones; the disadvantage is that they are more work! at the other extreme,
the percentile method simply sets the con   dence interval to

(q  t(  /2), q  t(1       /2))

(6.13)

this is de   nitely easier to calculate, but not as accurate as the basic, pivotal ci.
all of these methods have many variations, described in the monographs re-
ferred to at the end of this chapter (  6.9).

6.2.4 hypothesis testing

for hypothesis tests, we may want to calculate two sets of sampling distributions:
the distribution of the test statistic under the null tells us about the size of the test
and signi   cance levels, and the distribution under the alternative tells us about
power and realized power. we can    nd either with id64, by simulating
from either the null or the alternative. in such cases, the statistic of interest, which
i   ve been calling t , is the test statistic. code example 10 illustrates how to    nd a
p-value by simulating under the null hypothesis. the same procedure would work
to calculate power, only we   d need to simulate from the alternative hypothesis,
and testhat would be set to the critical value of t separating acceptance from
rejection, not the observed value.

6.2.4.1 double bootstrap hypothesis testing

when the hypothesis we are testing involves estimated parameters, we may need
to correct for this. suppose, for instance, that we are doing a goodness-of-   t test.
if we estimate our parameters on the data set, we adjust our distribution so that
it matches the data. it is thus not surprising if it seems to    t the data well!
(essentially, it   s the problem of evaluating performance by looking at in-sample
   t, which gave us so much trouble in chapter 3.)

some test statistics have distributions which are not a   ected by estimating
parameters, at least not asymptotically. in other cases, one can analytically come
up with correction terms. when these routes are blocked, one uses a double
bootstrap, where a second level of id64 checks how much estimation

6.2 the bootstrap principle

135

doubleboot.pvalue <- function(test, simulator, b1, b2, estimator, thetahat,

testhat, ...) {
for (i in 1:b1) {

xboot <- simulator(theta = thetahat, ...)
thetaboot <- estimator(xboot)
testboot[i] <- test(xboot)
pboot[i] <- boot.pvalue(test, simulator, b2, testhat = testboot[i],

theta = thetaboot)

}
p <- (sum(testboot >= testhat) + 1)/(b1 + 1)
p.adj <- (sum(pboot <= p) + 1)/(b1 + 1)
return(p.adj)

}

code example 11: code sketch for    double bootstrap    signi   cance testing. the inner or second
bootstrap is used to calculate the distribution of nominal bootstrap p-values. for this to work, we
need to draw our second-level bootstrap samples from     , the bootstrap re-estimate, not from     ,
the data estimate. the code presumes the simulator function takes a theta argument allowing
this. exercise: replace the for loop with replicate.

improves the apparent    t of the model. this is perhaps most easily explained in
pseudo-code (code example 11).

6.2.5 model-based id64 example: pareto   s law of wealth

inequality

the pareto or power-law distribution6, is a popular model for data with    heavy
tails   , i.e. where the id203 density f (x) goes to zero only very slowly as
x        . the id203 density is

(6.14)

(6.15)

where x0 is the minimum scale of the distribution, and    is the scaling exponent
(exercise 6.1). the pareto is highly right-skewed, with the mean being much
larger than the median.

if we know x0, one can show that the maximum likelihood estimator of the

exponent    is

     = 1 +

n(cid:80)n

i=1 log xi
x0

and that this is consistent (exercise 6.3), and e   cient. picking x0 is a harder
problem (see clauset et al. 2009)     for the present purposes, pretend that the
oracle tells us. the    le pareto.r, on the book website, contains a number of
functions related to the pareto distribution, including a function pareto.fit for
estimating it. (there   s an example of its use below.)

pareto came up with this density when he attempted to model the distribution

6 named after vilfredo pareto (1848   1923), the highly in   uential economist, political scientist, and

proto-fascist.

(cid:18) x

(cid:19)     

x0

f (x) =

       1
x0

136

the bootstrap

sim.wealth <- function() {

rpareto(n = n.tail, threshold = wealth.pareto$xmin, exponent = wealth.pareto$exponent)

}
est.pareto <- function(data) {

pareto.fit(data, threshold = x0)$exponent

}

code example 12: simulator and estimator for model-based id64 of the pareto dis-
tribution.

of personal wealth. approximately, but quite robustly across countries and time-
periods, the upper tail of the distribution of income and wealth follows a power
law, with the exponent varying as money is more or less concentrated among the
very richest individuals and households7. figure 6.2 shows the distribution of net
worth for the 400 richest americans in 20038.

source("http://www.stat.cmu.edu/~cshalizi/adafaepov/code/pareto.r")
wealth <- scan("http://www.stat.cmu.edu/~cshalizi/adafaepov/data/wealth.dat")
x0 <- 9e+08
n.tail <- sum(wealth >= x0)
wealth.pareto <- pareto.fit(wealth, threshold = x0)

taking x0 = 9   108 (again, see clauset et al. 2009), the number of individuals

in the tail is 302, and the estimated exponent is      = 2.34.

how much uncertainty is there in this estimate of the exponent? naturally, we   ll
bootstrap. we need a function to generate pareto-distributed random variables;
this, along with some related functions, is part of the    le pareto.r on the course
website. with that tool, model-based id64 proceeds as in code example
12.

using these functions, we can now calculate the bootstrap standard error, bias

and 95% con   dence interval for     , setting b = 104:

pareto.se <- bootstrap.se(statistic = est.pareto, simulator = sim.wealth, b = 10000)
pareto.bias <- bootstrap.bias(statistic = est.pareto, simulator = sim.wealth,

t.hat = wealth.pareto$exponent, b = 10000)

pareto.ci <- bootstrap.ci(statistic = est.pareto, simulator = sim.wealth, b = 10000,

t.hat = wealth.pareto$exponent, level = 0.95)

this gives a standard error of   0.078, matching the asymptotic approximation

reasonably well9, but not needing asymptotic assumptions.

7 most of the distribution, for ordinary people, roughly conforms to a log-normal.
8 for the data source and a fuller analysis, see clauset et al. (2009).
9    in asymptopia   , the variance of the id113 should be (        1)2

, in this case 0.076. the intuition is

n

that this variance depends on how sharp the maximum of the likelihood function is     if it   s sharply
peaked, we can    nd the maximum very precisely, but a broad maximum is hard to pin down.
variance is thus inversely proportional to the second derivative of the negative log-likelihood. (the
minus sign is because the second derivative has to be negative at a maximum, while variance has to
be positive.) for one sample, the expected second derivative of the negative log-likelihood is
(       1)   2. (this is called the fisher information of the model.) log-likelihood adds across

6.2 the bootstrap principle

137

plot.survival.loglog(wealth, xlab = "net worth (dollars)", ylab = "fraction of top 400 above that worth")
rug(wealth, side = 1, col = "grey")
curve((n.tail/400) * ppareto(x, threshold = x0, exponent = wealth.pareto$exponent,

lower.tail = false), add = true, lty = "dashed", from = x0, to = 2 * max(wealth))

figure 6.2 upper cumulative distribution function (or    survival function   )
of net worth for the 400 richest individuals in the us (2000 data). the solid
line shows the fraction of the 400 individuals whose net worth w equaled or
exceeded a given value w, pr (w     w). (note the logarithmic scale for both
axes.) the dashed line is a maximum-likelihood estimate of the pareto
distribution, taking x0 = $9    108. (this threshold was picked using the
method of clauset et al. 2009.) since there are 302 individuals at or above
the threshold, the cumulative distribution function of the pareto has to be
reduced by a factor of (302/400).

1e+092e+095e+091e+102e+105e+100.0020.0050.0200.0500.2000.500net worth (dollars)fraction of top 400 above that worth138

the bootstrap

ks.stat.pareto <- function(x, exponent, x0) {

x <- x[x >= x0]
ks <- ks.test(x, ppareto, exponent = exponent, threshold = x0)
return(ks$statistic)

}
ks.pvalue.pareto <- function(b, x, exponent, x0) {

testhat <- ks.stat.pareto(x, exponent, x0)
testboot <- vector(length = b)
for (i in 1:b) {

xboot <- rpareto(length(x), exponent = exponent, threshold = x0)
exp.boot <- pareto.fit(xboot, threshold = x0)$exponent
testboot[i] <- ks.stat.pareto(xboot, exp.boot, x0)

}
p <- (sum(testboot >= testhat) + 1)/(b + 1)
return(p)

}

code example 13: calculating a p-value for the pareto distribution, using the kolmogorov-
smirnov test and adjusting for the way estimating the scaling exponent moves the    tted distri-
bution closer to the data.

asymptotically, the bias is known to go to zero; at this size, id64

gives a bias of 0.0051, which is e   ectively negligible.

we can also get the con   dence interval; with the same 104 replications, the 95%
ci is 2.17, 2.48. in theory, the con   dence interval could be calculated exactly, but
it involves the inverse gamma distribution (arnold, 1983), and it is quite literally
faster to write and do the bootstrap than go to look it up.

a more challenging problem is goodness-of-   t; we   ll use the kolmogorov-smirnov
statistic.10 code example 13 calculates the p-value. with ten thousand bootstrap
replications,

signif(ks.pvalue.pareto(10000, wealth, wealth.pareto$exponent, x0), 4)
## [1] 0.0101

(cid:113) (0.01)(0.99)

104

ten thousand replicates is enough that we should be able to accurately es-
timate probabilities of around 0.01 (since the binomial standard error will be

    9.9    10   4); if it weren   t, we might want to increase b.

simply plugging in to the standard formulas, and thereby ignoring the e   ects of
estimating the scaling exponent, gives a p-value of 0.171, which is not outstanding
but not awful either. properly accounting for the    exibility of the model, however,
the discrepancy between what it predicts and what the data shows is so large
that it would take a big (one-in-a-hundred) coincidence to produce it. we have,

independent samples, giving us an over-all factor of n. in the large-sample limit, the actual
log-likelihood will converge on the expected log-likelihood, so this gives us the asymptotic variance.
(see also   h.5.5.)

10 the pareto.r    le contains a function, pareto.tail.ks.test, which does a goodness-of-   t test for

   tting a power-law to the tail of the distribution. that di   ers somewhat from what follows, because
it takes into account the extra uncertainty which comes from having to estimate x0. here, i am
pretending that an oracle told us x0 = 9    108.

6.3 resampling

139

therefore, detected that the pareto distribution makes systematic errors for this
data, but we don   t know much about what they are. in chapter 15, we   ll look at
techniques which can begin to tell us something about how it fails.

6.3 id64 by resampling

the bootstrap approximates the sampling distribution, with three sources of ap-
proximation error. first, simulation error: using    nitely many replications to
stand for the full sampling distribution. clever simulation design can shrink this,
but brute force     just using enough replicates     can also make it arbitrarily
small. second, statistical error: the sampling distribution of the bootstrap re-
estimates under our estimated model is not exactly the same as the sampling
distribution of estimates under the true data-generating process. the sampling
distribution changes with the parameters, and our initial estimate is not com-
pletely accurate. but it often turns out that distribution of estimates around the
truth is more nearly invariant than the distribution of estimates themselves, so
subtracting the initial estimate from the bootstrapped values helps reduce the
statistical error; there are many subtler tricks to the same end. third, speci   ca-
tion error: the data source doesn   t exactly follow our model at all. simulating
the model then never quite matches the actual sampling distribution.

efron had a second brilliant idea, which is to address speci   cation error by
replacing simulation from the model with re-sampling from the data. after all,
our initial collection of data gives us a lot of information about the relative
probabilities of di   erent values. in a sense the empirical distribution is the least
prejudiced estimate possible of the underlying distribution     anything else im-
poses biases or pre-conceptions, possibly accurate but also potentially mislead-
ing11. lots of quantities can be estimated directly from the empirical distribution,
without the mediation of a model. efron   s resampling bootstrap (a.k.a. the
non-parametric bootstrap) treats the original data set as a complete popula-
tion and draws a new, simulated sample from it, picking each observation with
equal id203 (allowing repeated values) and then re-running the estimation
(figure 6.3, code example 14). in fact, this is usually what people mean when
they talk about    the bootstrap    without any modi   er.

everything we did with model-based id64 can also be done with re-
sampling id64     the only thing that   s changing is the distribution the
surrogate data is coming from.

the resampling bootstrap should remind you of k-fold cross-validation. the
analog of leave-one-out cv is a procedure called the jack-knife, where we repeat
the estimate n times on n   1 of the data points, holding each one out in turn. it   s
historically important (it dates back to the 1940s), but generally doesn   t work as
well as resampling.

an important variant is the smoothed bootstrap, where we re-sample the

11 see   14.6 in chapter 14.

140

the bootstrap

figure 6.3 schematic for the resampling id64. new data is
simulated by re-sampling from the original data (with replacement), and
functionals are calculated either directly from the empirical distribution, or
by estimating a model on this surrogate data.

resample <- function(x) {

sample(x, size = length(x), replace = true)

}
resample.data.frame <- function(data) {

sample.rows <- resample(1:nrow(data))
return(data[sample.rows, ])

}

code example 14: a utility function to resample from a vector, and another which resamples
from a data frame. can you write a single function which determines whether its argument is a
vector or a data frame, and does the right thing in each case/

data points and then perturb each by a small amount of noise, generally gaus-
sian12.

12 we will see in chapter 14 that this corresponds to sampling from a kernel density estimate.

data0.00168-0.002490.0183-0.005870.0139estimatorempiricaldistributionq0.01 = -0.0392parameter calculationre-samplingsimulated data0.001830.00183-0.00249-0.00249-0.00587estimatorq0.01 = -0.0354re-estimate6.4 id64 regression models

141

back to the pareto example

let   s see how to use re-sampling to get a 95% con   dence interval for the pareto
exponent13.

wealth.resample <- function() {

resample(wealth[wealth >= x0])

}
pareto.ci.resamp <- bootstrap.ci(statistic = est.pareto, simulator = wealth.resample,

t.hat = wealth.pareto$exponent, level = 0.95, b = 10000)

the interval is 2.17, 2.48; this is very close to the interval we got from the model-

based bootstrap, which should actually reassure us about the latter   s validity.

6.3.1 model-based vs. resampling bootstraps

when we have a properly speci   ed model, simulating from the model gives more
accurate results (at the same n) than does re-sampling the empirical distribution
    parametric estimates of the distribution converge faster than the empirical
distribution does. if on the other hand the model is mis-speci   ed, then it is rapidly
converging to the wrong distribution. this is of course just another bias-variance
trade-o   , like those we   ve seen in regression.

since i am suspicious of most parametric modeling assumptions, i prefer re-
sampling, when i can    gure out how to do it, or at least until i have convinced
myself that a parametric model is a good approximation to reality.

6.4 id64 regression models

let   s recap what we   re doing estimating regression models. we want to learn
the regression function   (x) = e [y |x = x]. we estimate the model on a set of
predictor-response pairs, (x1, y1), (x2, y2), . . . (xn, yn), resulting in an estimated

curve (or surface) (cid:98)  (x),    tted values (cid:98)  i = (cid:98)  (xi), and residuals,  i = yi    (cid:98)  i. for

any such model, we have a choice of several ways of id64, in decreasing
order of reliance on the model.
    simulate new x values from the model   s distribution of x, and then draw y
    hold the x    xed, but draw y |x from the speci   ed distribution.

    hold the x    xed, but make y equal to (cid:98)  (x) plus a randomly re-sampled  j.

from the speci   ed conditional distribution y |x.

    re-sample (x, y) pairs.

13 even if the pareto model is wrong, the estimator of the exponent will converge on the value which

gives, in a certain sense, the best approximation to the true distribution from among all power laws.
econometricians call such parameter values the pseudo-truth; we are getting a con   dence interval
for the pseudo-truth. in this case, the pseudo-true scaling exponent can still be a useful way of
summarizing how heavy tailed the income distribution is, despite the fact that the power law makes
systematic errors.

142

the bootstrap

the    rst case is pure model-based id64. (so is the second, sometimes,
when the regression model is agnostic about x.) the last case is just re-sampling
from the joint distribution of (x, y ). the next-to-last case is called re-sampling
the residuals or re-sampling the errors. when we do that, we rely on the
regression model to get the conditional expectation function right, but we don   t
count on it getting the distribution of the noise around the expectations.

the speci   c procedure of re-sampling the residuals is to re-sample the  i, with

replacement, to get    1,    2, . . .    n, and then set   xi = xi,   yi = (cid:98)  (  xi) +    i. this

surrogate data set is then re-analyzed like new data.

6.4.1 re-sampling points: parametric model example

a classic data set contains the time between 299 eruptions of the old faithful
geyser in yellowstone, and the length of the subsequent eruptions; these variables
are called waiting and duration. (we saw this data set already in   5.4.2.1, and
will see it again in   10.3.2.) we   ll look at the id75 of waiting on
duration. we   ll re-sample (duration, waiting) pairs, and would like con   dence
intervals for the regression coe   cients. this is a con   dence interval for the coef-
   cients of the best linear predictor, a functional of the distribution, which, as we
saw in chapters 1 and 2, exists no matter how nonlinear the process really is. it   s
only a con   dence interval for the true regression parameters if the real regression
function is linear.

before anything else, look at the model:

library(mass)
data(geyser)
geyser.lm <- lm(waiting ~ duration, data = geyser)

estimate

std. error

t value pr(     t   )

(intercept)

duration

99.3

-7.8

1.960

0.537

50.7

-14.5

0

0

the    rst step in id64 this is to build our simulator, which just means

sampling rows from the data frame:

resample.geyser <- function() {
resample.data.frame(geyser)

}

we can check this by running summary(geyser.resample()), and seeing that

it gives about the same quartiles and mean for both variables as summary(geyser)14,
but that the former gives di   erent numbers each time it   s run.

next, we de   ne the estimator:

14 the minimum and maximum won   t match up well     why not?

6.4 id64 regression models

143

est.geyser.lm <- function(data) {

fit <- lm(waiting ~ duration, data = data)
return(coefficients(fit))

}

we can check that this function works by seeing that coefficients(geyser.lm)
matches est.geyser.lm(geyser), but that est.geyser.lm(resample.geyser()
is di   erent every time we run it.

put the pieces together:

geyser.lm.ci <- bootstrap.ci(statistic=est.geyser.lm,

simulator=resample.geyser,
level=0.95,
t.hat=coefficients(geyser.lm),
b=1e4)

lower

upper

(intercept)

96.60

102.00

duration

-8.72

-6.95

notice that we do not have to assume homoskedastic gaussian noise     fortu-

nately, because that   s a very bad assumption here15.

6.4.2 re-sampling points: non-parametric model example

nothing in the logic of re-sampling data points for regression requires us to use
a parametric model. here we   ll provide 95% con   dence bounds for the kernel
smoothing of the geyser data. since the functional is a whole curve, the con   dence
set is often called a con   dence band.

we use the same simulator, but start with a di   erent regression curve, and

need a di   erent estimator.

evaluation.points <- data.frame(duration = seq(from = 0.8, to = 5.5, length.out = 200))
library(np)
npr.geyser <- function(data, tol = 0.1, ftol = 0.1, plot.df = evaluation.points) {

bw <- npregbw(waiting ~ duration, data = data, tol = tol, ftol = ftol)
mdl <- npreg(bw)
return(predict(mdl, newdata = plot.df))

}

now we construct pointwise 95% con   dence bands for the regression curve.

15 we have calculated 95% con   dence intervals for the intercept   0 and the slope   1 separately. these

intervals cover their coe   cients all but 5% of the time. taken together, they give us a rectangle in
(  0,   1) space, but the coverage id203 of this rectangle could be anywhere from 95% all the
way down to 90%. to get a con   dence region which simultaneously covers both coe   cients 95% of
the time, we have two big options. one is to stick to a box-shaped region and just increase the
con   dence level on each coordinate (to 97.5%). the other is to de   ne some suitable metric of how
far apart coe   cient vectors are (e.g., ordinary euclidean distance),    nd the 95% percentile of the
distribution of this metric, and trace the appropriate contour around     0,     1.

144

the bootstrap

main.curve <- npr.geyser(geyser)

# we already defined this in a previous example, but it doesn't hurt
resample.geyser <- function() { resample.data.frame(geyser) }

geyser.resampled.curves <- rboot(statistic=npr.geyser,

simulator=resample.geyser,
b=800)

resampling

that data frame and re-estimating

code example 15: generating multiple kernel-regression curves for the geyser data,
by
the model on each simulation.
geyser.resampled.curves stores the predictions of those 800 models, evaluated at a common
set of values for the predictor variable. the vector main.curve, which we   ll use presently to get
con   dence intervals, stores predictions of the model    t to the whole data, evaluated at that same
set of points.

for this end, we don   t really need to keep around the whole kernel regression
object     we   ll just use its predicted values on a uniform grid of points, extending
slightly beyond the range of the data (code example 15). observe that this will
go through bandwidth selection again for each bootstrap sample. this is slow,
but it is the most secure way of getting good con   dence bands. applying the
bandwidth we found on the data to each re-sample would be faster, but would
introduce an extra level of approximation, since we wouldn   t be treating each
simulation run the same as the original data.

figure 6.4 shows the curve    t to the data, the 95% con   dence limits, and
(faintly) all of the bootstrapped curves. doing the 800 bootstrap replicates took
4 minutes on my laptop16.

6.4.3 re-sampling residuals: example

as an example of re-sampling the residuals, rather than data points, let   s take a
id75, based on the data-analysis assignment in   a.14. we will regress
gdp.growth on log(gdp), pop.growth, invest and trade:

penn <- read.csv("http://www.stat.cmu.edu/~cshalizi/uada/13/hw/02/penn-select.csv")
penn.formula <- "gdp.growth ~ log(gdp) + pop.growth + invest + trade"
penn.lm <- lm(penn.formula, data = penn)

(why make the formula a separate object here?) the estimated parameters are

16 speci   cally, i ran system.time(geyser.resampled.curves <- rboot(statistic=npr.geyser,

simulator=resample.geyser, b=800)), which not only did the calculations and stored them in
geyser.resampled.curves, but told me how much time it took r to do all that.

6.4 id64 regression models

145

plot(0, type = "n", xlim = c(0.8, 5.5), ylim = c(0, 100), xlab = "duration (min)",

ylab = "waiting (min)")

for (i in 1:ncol(geyser.resampled.curves)) {

lines(evaluation.points$duration, geyser.resampled.curves[, i], lwd = 0.1,

col = "grey")

}
geyser.npr.cis <- bootstrap.ci(tboots = geyser.resampled.curves, t.hat = main.curve,

level = 0.95)

lines(evaluation.points$duration, geyser.npr.cis[, "lower"])
lines(evaluation.points$duration, geyser.npr.cis[, "upper"])
lines(evaluation.points$duration, main.curve)
rug(geyser$duration, side = 1)
points(geyser$duration, geyser$waiting)

figure 6.4 kernel regression curve for old faithful (central black line),
with 95% con   dence bands (other black lines), the 800 bootstrapped curves
(thin, grey lines), and the data points. notice that the con   dence bands get
wider where there is less data. caution: doing the bootstrap took 4 minutes
to run on my computer.

146

the bootstrap

resample.residuals.penn <- function() {

new.frame <- penn
new.growths <- fitted(penn.lm) + resample(residuals(penn.lm))
new.frame$gdp.growth <- new.growths
return(new.frame)

}
penn.estimator <- function(data) {

mdl <- lm(penn.formula, data = data)
return(coefficients(mdl))

}
penn.lm.cis <- bootstrap.ci(statistic = penn.estimator, simulator = resample.residuals.penn,

b = 10000, t.hat = coefficients(penn.lm), level = 0.95)

code example 16: re-sampling the residuals to get con   dence intervals in a linear model.

x

(intercept)

5.71e-04

log(gdp)

5.07e-04

pop.growth

-1.87e-01

invest

trade

7.15e-04

3.11e-05

code example 16 shows the new simulator for this set-up (resample.residuals.penn)17,

the new estimation function (penn.estimator)18, and the con   dence interval cal-
culation (penn.lm.cis):

lower

upper

(intercept)

-1.62e-02

1.71e-02

log(gdp)

-1.46e-03

2.49e-03

pop.growth

-3.58e-01

-1.75e-02

invest

trade

4.94e-04

9.37e-04

-1.94e-05

8.21e-05

doing ten thousand id75s took 45 seconds on my computer, as

opposed to 4 minutes for eight hundred kernel regressions.

6.5 bootstrap with dependent data

if the data points we are looking at are vectors (or more complicated structures)
with dependence between components, but each data point is independently gen-
erated from the same distribution, then dependence isn   t really an issue. we

17 how would you check that this worked?
18 how would you check that this worked?

6.6 con   dence bands for nonparametric regression

147

re-sample vectors, or generate vectors from our model, and proceed as usual. in
fact, that   s what we   ve done so far in several cases.

if there is dependence across data points, things are more tricky. if our model
incorporates this dependence, then we can just simulate whole data sets from
it. an appropriate re-sampling method is trickier     just re-sampling individual
data points destroys the dependence, so it won   t do. we will revisit this question
when we look at time series in chapter 25.

6.6 con   dence bands for nonparametric regression

many of the examples in this chapter use id64 to get con   dence bands
for nonparametric regression. it is worth mentioning that there is a subtle issue
with doing so, but one which i do not think really matters, usually, for practice.
the issue is that when we do nonparametric regression, we accept some bias
in our estimate of the regression function. in fact, we saw in chapter 4 that min-
imizing the total mse means accepting matching amounts of bias and variance.
so our nonparametric estimate of    is biased. if we simulate from it, we   re sim-
ulating from something biased; if we simulate from the residuals, those residuals
contain bias; and even if we do a pure resampling bootstrap, we   re comparing the
bootstrap replicates to a biased estimate. this means that we are really looking
at sampling intervals around the biased estimate, rather than con   dence intervals
around   .

the two questions this raises are (1) how much this matters, and (2) whether
there is any alternative. as for the size of the bias, we know from chapter 4 that
the squared bias, in 1d, goes like n   4/5, so the bias itself goes like n   2/5. this
does go to zero, but slowly.
[[living with it vs. hall and horowitz (2013) paper, which gives 1       coverage
at 1        fraction of points. essentially, construct naive bands, and then work out
by how much they need to be expanded to achieve desired coverage]]

6.7 things id64 does poorly

the principle behind id64 is that sampling distributions under the true
process should be close to sampling distributions under good estimates of the
truth. if small perturbations to the data-generating process produce huge swings
in the sampling distribution, id64 will not work well, and may fail spec-
tacularly. for model-based id64, this means that small changes to the
underlying parameters must produce small changes to the functionals of interest.
similarly, for resampling, it means that adding or removing a few data points
must change the functionals only a little19.

re-sampling in particular has trouble with extreme values. here is a simple
19 more generally, moving from one distribution function f to another (1      )f +  g mustn   t change the

functional very much when   is small, no matter in what    direction    g we perturb it. making this
idea precise calls for some fairly deep mathematics, about di   erential calculus on spaces of functions
(see, e.g., van der vaart 1998, ch. 20).

the bootstrap

148
example: our data points xi are iid, with xi     u nif (0,   0), and we want to
estimate   0. the maximum likelihood estimate      is just the sample maximum of
the xi. we   ll use resampling to get a con   dence interval for this, as above     but
i will    x the true   0 = 1, and see how often the 95% con   dence interval covers
the truth.

max.boot.ci <- function(x, b) {

max.boot <- replicate(b, max(resample(x)))
return(2 * max(x) - quantile(max.boot, c(0.975, 0.025)))

}
boot.cis <- replicate(1000, max.boot.ci(x = runif(100), b = 1000))
(true.coverage <- mean((1 >= boot.cis[1, ]) & (1 <= boot.cis[2, ])))
## [1] 0.87

that is, the actual coverage id203 is not 95% but about 87%.
if you suspect that your use of the bootstrap may be setting yourself up for
a similar epic fail, your two options are (1) learn some of the theory of the
bootstrap from the references in the    further reading    section below, or (2) set
up a simulation experiment like this one.

6.8 which bootstrap when?

this chapter has introduced a bunch of di   erent bootstraps, and before it closes
it   s worth reviewing the general principles, and some of the considerations which
go into choosing among them in a particular problem.

when we bootstrap, we try to approximate the sampling distribution of some
statistic (mean, median, correlation coe   cient, regression coe   cients, smoothing
curve, di   erence in mses. . . ) by running simulations, and calculating the statistic
on the simulation. we   ve seen three major ways of doing this:
    the model-based bootstrap: we estimate the model, and then simulate from x
    resampling residuals: we estimate the model, and then simulate by resampling
    resampling cases or whole data points: we ignore the estimated model com-
pletely in our simulation, and just re-sample whole rows from the data frame.

residuals to that estimate and adding them back to the    tted values;

the estimated model;

which kind of bootstrap is appropriate depends on how much trust we have in
our model.

the model-based bootstrap trusts the model to be completely correct for some
parameter value. in, e.g., regression, it trusts that we have the right shape for the
regression function and that we have the right distribution for the noise. when we
trust our model this much, we could in principle work out sampling distributions
analytically; the model-based bootstrap replaces hard math with simulation.

resampling residuals doesn   t trust the model as much. in regression problems,
it assumes that the model gets the shape of the regression function right, and
that the noise around the regression function is independent of the predictor

6.9 further reading

149

variables, but doesn   t make any further assumption about how the    uctuations
are distributed. it is therefore more secure than model-based bootstrap.20

finally, resampling cases assumes nothing at all about either the shape of the
regression function or the distribution of the noise, it just assumes that each data
point (row in the data frame) is an independent observation. because it assumes
so little, and doesn   t depend on any particular model being correct, it is very
safe.

the reason we do not always use the safest bootstrap, which is resampling
cases, is that there is, as usual, a bias-variance trade-o   . generally speaking, if
we compare three sets of bootstrap con   dence intervals on the same data for the
same statistic, the model-based bootstrap will give the narrowest intervals, fol-
lowed by resampling residuals, and resampling cases will give the loosest bounds.
if the model really is correct about the shape of the curve, we can get more
precise results, without any loss of accuracy, by resampling residuals rather than
resampling cases. if the model is also correct about the distribution of noise, we
can do even better with a model-based bootstrap.

to sum up: resampling cases is safer than resampling residuals, but gives wider,
weaker bounds. if you have good reason to trust a model   s guess at the shape of
the regression function, then resampling residuals is preferable. if you don   t, or
it   s not a regression problem so there are no residuals, then you prefer to resample
cases. the model-based bootstrap works best when the over-all model is correct,
and we   re just uncertain about the exact parameter values we need.

6.9 further reading

davison and hinkley (1997) is both a good textbook, and the reference i consult
most often. efron and tibshirani (1993), while also very good, is more theoretical.
canty et al. (2006) has useful advice for serious applications.
bootstraps for time series, see   25.5.

all the bootstraps discussed in this chapter presume iid observations. for

software

for professional purposes, i strongly recommend using the r package boot (canty
and ripley, 2013), based on davison and hinkley (1997). i deliberately do not
use it in this chapter, or later in the book, for pedagogical reasons; i have found
that forcing students to write their own id64 code helps build character,
or at least understanding.

the bootstrap vs. robust standard errors

for id75 coe   cients, econometricians have developed a variety of
   robust    standard errors which are valid under weaker conditions than the usual

20 you could also imagine simulations where we presume that the noise takes a very particular form

(e.g., a t-distribution with 10 degrees of freedom), but are agnostic about the shape of the regression
function, and learn that non-parametrically. it   s harder to think of situations where this is really
plausible, however, except maybe gaussian noise arising from central-limit-theorem considerations.

150

the bootstrap

assumptions. buja et al. (2014) shows their equivalence to resampling cases. (see
also king and roberts 2015.)

historical notes

the original paper on the bootstrap, efron (1979), is extremely clear, and for
the most part presented in the simplest possible terms; it   s worth reading. his
later small book (efron, 1982), while often cited, is not in my opinion so useful
nowadays21.

as the title of that last reference suggests, the bootstrap is in some ways a
successor to an older method, apparently dating back to the 1940s if not before,
called the    jackknife   , in which each data point is successively held back and
the estimate is re-calculated; the variance of these re-estimates, appropriately
scaled, is then taken as the variance of estimation, and similarly for the bias22.
the jackknife is appealing in its simplicity, but is only valid under much stronger
conditions than the bootstrap.

show that x0 is the mode of the pareto distribution.

6.1
6.2 derive the maximum likelihood estimator for the pareto distribution (eq. 6.15) from the

exercises

density (eq. 6.14).
show that the id113 of the pareto distribution is consistent.
1. using the law of large numbers, show that      (eq. 6.15) converges to a limit which

depends on e [log x/x0].

2. find an expression for e [log x/x0] in terms of    and from the density (eq. 6.14).
hint: write e [log x/x0] as an integral, change the variable of integration from x to
z = log (x/x0), and remember that the mean of an exponential random variable with
rate    is 1/  .

find con   dence bands for the id75 model of   6.4.1 using
1. the usual gaussian assumptions (hint: try the intervals="confidence" option to

6.3

6.4

predict);

2. resampling of residuals; and
3. resampling of cases.

6.5

(computational) writing new functions to simulate every particular linear model is some-
what tedious.

1. write a function which takes, as inputs, an lm model and a data frame, and returns
a new data frame where the response variable is replaced by the model   s predictions
plus gaussian noise, but all other columns are left alone.

2. write a function which takes, as inputs, an lm model and a data frame, and returns
a new data frame where the response variable is replaced by the model   s predictions
plus resampled residuals.

21 it seems to have done a good job of explaining things to people who were already professional

statisticians in 1982.

22 a    jackknife    is a knife with a blade which folds into the handle; think of the held-back data point

as the folded-away blade.

exercises

151

3. will your functions work with npreg models, as well as lm models? if not, what do you

have to modify?

hint: see code example 3 in chapter 3 for some r tricks to extract the name of the
response variable from the estimated model.

7

splines

7.1 smoothing by penalizing curve flexibility

let   s go back to the problem of smoothing one-dimensional data. we have data

points (x1, y1), (x2, y2), . . . (xn, yn), and we want to    nd a good approximation (cid:98)  
trolled how smooth we made (cid:98)   indirectly, through the bandwidth of our kernels.

to the true conditional expectation or regression function   . previously, we con-

but why not be more direct, and control smoothness itself?

a natural way to do this is to minimize the spline objective function

n(cid:88)

i=1

l(m,   )     1
n

(cid:90)

(yi     m(xi))2 +   

(m(cid:48)(cid:48)(x))2dx

(7.1)

the    rst term here is just the mean squared error of using the curve m(x) to
predict y. we know and like this; it is an old friend.

the second term, however, is something new for us. m(cid:48)(cid:48) is the second derivative
of m with respect to x     it would be zero if m were linear, so this measures the
curvature of m at x. the sign of m(cid:48)(cid:48)(x) says whether the curvature at x is concave
or convex, but we don   t care about that so we square it. we then integrate this
over all x to say how curved m is, on average. finally, we multiply by    and add
that to the mse. this is adding a penalty to the mse criterion     given two
functions with the same mse, we prefer the one with less average curvature. we
will accept changes in m that increase the mse by 1 unit if they also reduce the
average curvature by at least   .

the curve or function which solves this minimization problem,

l(m,   )

(7.2)

(cid:98)     = argmin

m

is called a smoothing spline, or spline curve. the name    spline    comes from
a simple tool used by craftsmen to draw smooth curves, which was a thin strip of
a    exible material like a soft wood; you pin it in place at particular points, called
knots, and let it bend between them. (when the gas company dug up my front
yard and my neighbor   s driveway, the contractors who put everything back used
a plywood board to give a smooth, curved edge to the new driveway. that board
was a spline, and the knots were pairs of metal stakes on either side of the board.
figure 7.1 shows the spline after concrete was poured on one side of it.) bending
the spline takes energy     the sti   er the material, the more energy has to go into
bending it through the same shape, and so the material makes a straighter curve

152

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

7.1 smoothing by penalizing curve flexibility

153

figure 7.1 a wooden spline used to create a smooth, curved border for a
paved area (shadyside, pittsburgh, october 2014).

between given points. for smoothing splines, using a sti   er material corresponds
to increasing   .
it is possible to show (  7.6 below) that all solutions to eq. 7.1, no matter what
the data might be, are piecewise cubic polynomials which are continuous and have

continuous    rst and second derivatives     i.e., not only is (cid:98)   continuous, so are
(cid:98)  (cid:48) and (cid:98)  (cid:48)(cid:48). the boundaries between the pieces sit at the original data points. by

analogy with the craftman   s spline, the boundary points are called the knots of
the smoothing spline. the function is continuous beyond the largest and smallest
data points, but it is always linear in those regions.1

i will also assert, without proof, that, with enough pieces, such piecewise cu-
bic polynomials can approximate any well-behaved function arbitrarily closely.
finally, smoothing splines are linear smoothers, in the sense of chapter 1: pre-
dicted values are linear combinations of the training-set response values yi     see
eq. 7.21 below.

7.1.1 the meaning of the splines

look back to the optimization problem. as           , any curvature at all becomes
in   nitely costly, and only linear functions are allowed. but we know how to min-
imize mean squared error with linear functions, that   s ols. so we understand
that limit.
on the other hand, as        0, we decide that we don   t care about curvature. in
that case, we can always come up with a function which just interpolates between
the data points, an interpolation spline passing exactly through each point.
more speci   cally, of the in   nitely many functions which interpolate between those
points, we pick the one with the minimum average curvature.

at intermediate values of   ,(cid:98)     becomes a function which compromises between

1 can you explain why it is linear outside the data range, in terms of the optimization problem?

154

splines

having low curvature, and bending to approach all the data points closely (on
average). the larger we make   , the more curvature is penalized. there is a bias-
variance trade-o    here. as    grows, the spline becomes less sensitive to the data,
with lower variance to its predictions but more bias. as    shrinks, so does bias,
but variance grows. for consistency, we want to let        0 as n        , just as,
with kernel smoothing, we let the bandwidth h     0 while n        .

we can also think of the smoothing spline as the function which minimizes the
mean squared error, subject to a constraint on the average curvature. this turns
on a general corresponds between penalized optimization and optimization under
constraints, which is explored in appendix h.3. the short version is that each
level of    corresponds to imposing a cap on how much curvature the function
is allowed to have, on average, and the spline we    t with that    is the mse-
minimizing curve subject to that constraint.2 as we get more data, we have more
information about the true regression function and can relax the constraint (let
   shrink) without losing reliable estimation.

it will not surprise you to learn that we select    by cross-validation. ordinary
k-fold cv is entirely possible, but leave-one-out cv works quite well for splines.
in fact, the default in most spline software is either leave-one-out cv, or the even
faster approximation called    generalized cross-validation    or gcv (see   3.4.3).

7.2 computational example: splines for stock returns
the default r function for    tting a smoothing spline is smooth.spline:

smooth.spline(x, y, cv = false)

where x should be a vector of values for input variable, y is a vector of values
for the response (in the same order), and the switch cv controls whether to pick   
by generalized cross-validation (the default) or by leave-one-out cross-validation.
the object which smooth.spline returns has an $x component, re-arranged in
increasing order, a $y component of    tted values, a $yin component of original
values, etc. see help(smooth.spline) for more.

as a concrete illustration, figure 7.2 looks at the daily logarithmic returns3
of the s&p 500 stock index, on 5542 consecutive trading days, from 9 february
1993 to 9 february 20154.

functions m where(cid:82) (m(cid:48)(cid:48)(x))2dx is at most some maximum level c.    would then be the lagrange

2 the slightly longer version: consider minimizing the mse (not the penalized mse), but only over

multiplier enforcing the constraint. the constrained but unpenalized optimization is equivalent to
the penalized but unconstrained one. in economics,    would be called the    shadow price    of average
curvature in units of mse, the rate at which we   d be willing to pay to have the constraint level c
marginally increased.

3 for a    nancial asset whose price on day t is pt and which pays a dividend on that day of dt, the

log-returns on t are log (pt + dt)/pt   1. financiers and other professional gamblers care more about
the log returns than about the price change, pt     pt   1, because the log returns give the rate of
pro   t (or loss) on investment. we are using a price series which is adjusted to incorporate dividend
(and related) payments.

4 this uses the handy pdfetch library, which downloads data from such public domain sources as the

federal reserve, yahoo finance, etc.

7.2 computational example: splines for stock returns

155

require(pdfetch)

## loading required package: pdfetch

sp <- pdfetch_yahoo("spy", fields = "adjclose", from = as.date("1993-02-09"),

to = as.date("2015-02-09"))

sp <- diff(log(sp))
sp <- sp[-1]

we want to use the log-returns on one day to predict what they will be on the
next. the horizontal axis in the    gure shows the log-returns for each of 2527 days
t, and the vertical axis shows the corresponding log-return for the succeeding day
t + 1. a linear model    tted to this data displays a slope of    0.0642 (grey line in
the    gure). fitting a smoothing spline with cross-validation selects    = 0.0127,
and the black curve:

(intercept)

sp.today
0.0003716837 -0.0640901257

sp.today <- head(sp, -1)
sp.tomorrow <- tail(sp, -1)
coefficients(lm(sp.tomorrow ~ sp.today))
##
##
sp.spline <- smooth.spline(x = sp.today, y = sp.tomorrow, cv = true)
sp.spline
## call:
## smooth.spline(x = sp.today, y = sp.tomorrow, cv = true)
##
## smoothing parameter
## equivalent degrees of freedom (df): 5.855613
## penalized criterion (rss): 0.7825304
## press(l.o.o. cv): 0.0001428132
sp.spline$lambda
## [1] 0.01299752

spar= 1.346847

lambda= 0.01299752 (11 iterations)

(press is the    prediction sum of squares   , i.e., the sum of the squared leave-
one-out prediction errors.) this is the curve shown in black in the    gure. the
blue curves are for large values of   , and clearly approach the id75;
the red curves are for smaller values of   .

the spline can also be used for prediction. for instance, if we want to know
what the return to expect following a day when the log return was +0.01, we do

predict(sp.spline, x = 0.01)
## $x
## [1] 0.01
##
## $y
## [1] 0.0001948564

r syntax note:

the syntax for predict with smooth.spline spline di   ers slightly from the syntax
for predict with lm or np. the latter two want a newdata argument, which should
be a data-frame with column names matching those in the formula used to    t
the model. the predict function for smooth.spline, though, just wants a vector
called x. also, while predict for lm or np returns a vector of predictions, predict

156

splines

plot(as.vector(sp.today), as.vector(sp.tomorrow), xlab = "today's log-return",

ylab = "tomorrow's log-return", pch = 16, cex = 0.5, col = "grey")

abline(lm(sp.tomorrow ~ sp.today), col = "darkgrey")
sp.spline <- smooth.spline(x = sp.today, y = sp.tomorrow, cv = true)
lines(sp.spline)
lines(smooth.spline(sp.today, sp.tomorrow, spar = 1.5), col = "blue")
lines(smooth.spline(sp.today, sp.tomorrow, spar = 2), col = "blue", lty = 2)
lines(smooth.spline(sp.today, sp.tomorrow, spar = 1.1), col = "red")
lines(smooth.spline(sp.today, sp.tomorrow, spar = 0.5), col = "red", lty = 2)

figure 7.2 the s& p 500 log-returns data (grey dots), with the ols linear
regression (dark grey line), the spline selected by cross-validation (solid
black,    = 0.0127), some more smoothed splines (blue,    = 0.178 and 727)
and some less smooth splines (red,    = 2.88    10   4 and 1.06    10   8).
incoveniently, smooth.spline does not let us control    directly, but rather a
somewhat complicated but basically exponential transformation of it called
spar. (see help(smooth.spline) for the gory details.) the equivalent    can
be extracted from the return value, e.g.,
smooth.spline(sp.today,sp.tomorrow,spar=2)$lambda.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   0.10   0.050.000.050.10   0.10   0.050.000.050.10today's log   returntomorrow's log   return7.2 computational example: splines for stock returns

157

for smooth.spline returns a list with an x component (in increasing order) and a
y component, which is the sort of thing that can be put directly into points or
lines for plotting.

7.2.1 con   dence bands for splines

continuing the example, the smoothing spline selected by cross-validation has
a negative slope everywhere, like the regression line, but it   s asymmetric     the
slope is more negative to the left, and then levels o    towards the regression
line. (see figure 7.2 again.) is this real, or might the asymmetry be a sampling
artifact?

we   ll investigate by    nding con   dence bands for the spline, much as we did for
kernel regression in chapter 6 and problem set a.27, problem 5. again, we need
to bootstrap, and we can do it either by resampling the residuals or resampling
whole data points. let   s take the latter approach, which assumes less about the
data. we   ll need a simulator:

sp.frame <- data.frame(today = sp.today, tomorrow = sp.tomorrow)
sp.resampler <- function() {

n <- nrow(sp.frame)
resample.rows <- sample(1:n, size = n, replace = true)
return(sp.frame[resample.rows, ])

}

this treats the points in the scatterplot as a complete population, and then
draws a sample from them, with replacement, just as large as the original5. we   ll
also need an estimator. what we want to do is get a whole bunch of spline curves,
one on each simulated data set. but since the values of the input variable will
change from one simulation to another, to make everything comparable we   ll
evaluate each spline function on a    xed grid of points, that runs along the range
of the data.

grid.300 <- seq(from = min(sp.today), to = max(sp.today), length.out = 300)
sp.spline.estimator <- function(data, eval.grid = grid.300) {

fit <- smooth.spline(x = data[, 1], y = data[, 2], cv = true)
return(predict(fit, x = eval.grid)$y)

}

this sets the number of evaluation points to 300, which is large enough to give

visually smooth curves, but not so large as to be computationally unwieldly.

now put these together to get con   dence bands:

sp.spline.cis <- function(b, alpha, eval.grid = grid.300) {

spline.main <- sp.spline.estimator(sp.frame, eval.grid = eval.grid)
spline.boots <- replicate(b, sp.spline.estimator(sp.resampler(), eval.grid = eval.grid))
cis.lower <- 2 * spline.main - apply(spline.boots, 1, quantile, probs = 1 -

alpha/2)

cis.upper <- 2 * spline.main - apply(spline.boots, 1, quantile, probs = alpha/2)
return(list(main.curve = spline.main, lower.ci = cis.lower, upper.ci = cis.upper,

x = eval.grid))

}

5   25.5 covers more re   ned ideas about id64 time series.

158

splines

the return value here is a list which includes the original    tted curve, the
lower and upper con   dence limits, and the points at which all the functions were
evaluated.

figure 7.3 shows the resulting 95% con   dence limits, based on b=1000 boot-
strap replications. (doing all the id64 took 45 seconds on my laptop.)
these are pretty clearly asymmetric in the same way as the curve    t to the whole
data, but notice how wide they are, and how they get wider the further we go
from the center of the distribution in either direction.

7.2 computational example: splines for stock returns

159

sp.cis <- sp.spline.cis(b = 1000, alpha = 0.05)
plot(as.vector(sp.today), as.vector(sp.tomorrow), xlab = "today's log-return",

ylab = "tomorrow's log-return", pch = 16, cex = 0.5, col = "grey")

abline(lm(sp.tomorrow ~ sp.today), col = "darkgrey")
lines(x = sp.cis$x, y = sp.cis$main.curve, lwd = 2)
lines(x = sp.cis$x, y = sp.cis$lower.ci)
lines(x = sp.cis$x, y = sp.cis$upper.ci)

figure 7.3 bootstrapped pointwise con   dence band for the smoothing
spline of the s & p 500 data, as in figure 7.2. the 95% con   dence limits
around the main spline estimate are based on 1000 bootstrap re-samplings of
the data points in the scatterplot.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   0.10   0.050.000.050.10   0.10   0.050.000.050.10today's log   returntomorrow's log   return160

splines

7.3 basis functions and degrees of freedom

7.3.1 basis functions

splines, i said, are piecewise cubic polynomials. to see how to    t them, let   s
think about how to    t a global cubic polynomial. we would de   ne four basis
functions6,

b1(x) = 1
b2(x) = x
b3(x) = x2
b4(x) = x3

(7.3)
(7.4)
(7.5)
(7.6)

and chose to only consider regression functions that are linear combinations of
the basis functions,

4(cid:88)

  (x) =

  jbj(x)

(7.7)

j=1

such regression functions would be linear in the transformed variables b1(x), . . . b4(x),
even though it is nonlinear in x.
function to each data point xi and gather the results in an n    4 matrix b,

to estimate the coe   cients of the cubic polynomial, we would apply each basis

bij = bj(xi)

(7.8)

then we would do ols using the b matrix in place of the usual data matrix x:

     = (bt b)   1bt y

(7.9)

since splines are piecewise cubics, things proceed similarly, but we need to be a
little more careful in de   ning the basis functions. recall that we have n values of
the input variable x, x1, x2, . . . xn. for the rest of this section, i will assume that
these are in increasing order, because it simpli   es the notation. these n    knots   
de   ne n + 1 pieces or segments: n     1 of them between the knots, one from       
to x1, and one from xn to +   . a third-order polynomial on each segment would
seem to need a constant, linear, quadratic and cubic term per segment. so the
segment running from xi to xi+1 would need the basis functions
1(xi,xi+1)(x), (x     xi)1(xi,xi+1)(x), (x     xi)21(xi,xi+1)(x), (x     xi)31(xi,xi+1)(x)
(7.10)
where as usual the indicator function 1(xi,xi+1)(x) is 1 if x     (xi, xi+1) and 0
otherwise. this makes it seem like we need 4(n + 1) = 4n + 4 basis functions.

however, we know from id202 that the number of basis vectors we
need is equal to the number of dimensions of the vector space. the number of
adjustable coe   cients for an arbitrary piecewise cubic with n + 1 segments is
indeed 4n + 4, but splines are constrained to be smooth. the spline must be
continuous, which means that at each xi, the value of the cubic from the left,

6 see app. b.11 for brief reminders about basis functions.

7.3 basis functions and degrees of freedom

161

de   ned on (xi   1, xi), must match the value of the cubic from the right, de   ned
on (xi, xi+1). this gives us one constraint per data point, reducing the number of
adjustable coe   cients to at most 3n+4. since the    rst and second derivatives are
also continuous, we are down to just n + 4 coe   cients. finally, we know that the
spline function is linear outside the range of the data, i.e., on (      , x1) and on
(xn,   ), lowering the number of coe   cients to n. there are no more constraints,
so we end up needing only n basis functions. and in fact, from id202, any
set of n piecewise cubic functions which are linearly independent7 can be used as
a basis. one common choice is

b1(x) = 1
b2(x) = x

bi+2(x) =

(x     xi)3

+     (x     xn)3
xn     xi

+

    (x     xn   1)3

+     (x     xn)3

+

xn     xn   1

(7.11)
(7.12)

(7.13)

where (a)+ = a if a > 0, and = 0 otherwise. this rather unintuitive-looking basis
has the nice property that the second and third derivatives of each bj are zero
outside the interval (x1, xn).

now that we have our basis functions, we can once again write the spline as a

weighted sum of them,

m(x) =

m(cid:88)

  jbj(x)

(7.14)

j=1

and put together the matrix b where bij = bj(xi). we can write the spline
objective function in terms of the basis functions,

nl = (y     b  )t (y     b  ) + n    t      

(7.15)

where the matrix     encodes information about the curvature of the basis func-
tions:

   jk =

b(cid:48)(cid:48)
j (x)b(cid:48)(cid:48)

k (x)dx

(7.16)

(cid:90)

notice that only the quadratic and cubic basis functions will make non-zero
contributions to    . with the choice of basis above, the second derivatives are
non-zero on, at most, the interval (x1, xn), so each of the integrals in     is going
to be    nite. this is something we (or, realistically, r) can calculate once, no
matter what    is. now we can    nd the smoothing spline by di   erentiating with
respect to   :

0 =    2bt y + 2bt b      + 2n          

bt y =(cid:0)bt b + n     (cid:1)     
     =(cid:0)bt b + n     (cid:1)   1

bt y

(7.17)

(7.18)

(7.19)

7 recall that vectors (cid:126)v1, (cid:126)v2, . . . (cid:126)vd are linearly independent when there is no way to write any one of

the vectors as a weighted sum of the others. the same de   nition applies to functions.

162

splines

notice, incidentally, that we can now show splines are linear smoothers:

(cid:98)  (x) = b     

= b(cid:0)bt b + n     (cid:1)   1

bt y

(7.20)

(7.21)

once again, if this were ordinary id75, the ols estimate of the co-
e   cients would be (xt x)   1xt y. in comparison to that, we   ve made two changes.
first, we   ve substituted the basis function matrix b for the original matrix of
independent variables, x     a change we   d have made already for a polynomial
regression. second, the    denominator    is not xt x, or even bt b, but bt b+n     .
since xt x is n times the covariance matrix of the independent variables, we are
taking the covariance matrix of the spline basis functions and adding some extra
covariance     how much depends on the shapes of the functions (through    ) and
how much smoothing we want to do (through   ). the larger we make   , the less
the actual data matters to the    t.

in addition to explaining how splines can be    t quickly (do some matrix arith-
metic), this illustrates two important tricks. one, which we won   t explore further
here, is to turn a nonid75 problem into one which is linear in an-
other set of basis functions. this is like using not just one transformation of
the input variables, but a whole library of them, and letting the data decide
which transformations are important. there remains the issue of selecting the
basis functions, which can be quite tricky. in addition to the spline basis8, most
choices are various sorts of waves     sine and cosine waves of di   erent frequen-
cies, various wave-forms of limited spatial extent (   wavelets   ), etc. the ideal is
to chose a function basis where only a few non-zero coe   cients would need to be
estimated, but this requires some understanding of the data. . .

the other trick is that of stabilizing an unstable estimation problem by adding a
penalty term. this reduces variance at the cost of introducing some bias. exercise
7.2 explores this idea.

e   ective degrees of freedom

in   1.5.3.2, we de   ned the number of e   ective degrees of freedom for a linear
smoother with smoothing matrix w as just tr w. thus, eq. 7.21 lets us calculate
the e   ective degrees of freedom of a spline, as tr
. you
should be able to convince yourself from this that increasing    will, all else being
equal, reduce the e   ective degrees of freedom of the    t.

   1bt(cid:17)

b(bt b + n     )

(cid:16)

7.4 splines in multiple dimensions

suppose we have two input variables, x and z, and a single response y. how could
we do a spline    t?

8 or, really, bases; there are multiple sets of basis functions for the splines, just like there are multiple

sets of basis vectors for the plane. phrases like    b splines    and    p splines    refer to particular
choices of spline basis functions.

m1(cid:88)

m2(cid:88)

7.5 smoothing splines versus kernel regression

163

one approach is to generalize the spline optimization problem so that we pe-
nalize the curvature of the spline surface (no longer a curve). the appropriate
penalized least-squares objective function to minimize is
l(m,   ) =

(cid:90) (cid:34)(cid:18)    2m

(yi     m(xi, zi))2 +   

(cid:18)    2m

(cid:18)    2m

(cid:19)2(cid:35)

n(cid:88)

(cid:19)2

(cid:19)2

dxdz

+ 2

+

   x2

   x   z

   z2

i=1

(7.22)
the solution is called a thin-plate spline. this is appropriate when the two
input variables x and z should be treated more or less symmetrically9.

an alternative is use the spline basis functions from section 7.3. we write

m(x) =

  jkbj(x)bk(z)

(7.23)

j=1

k=1

doing all possible multiplications of one set of numbers or functions with another
is said to give their outer product or tensor product, so this is known as a
tensor product spline or tensor spline. we have to chose the number of terms
to include for each variable (m1 and m2), since using n for each would give n2
basis functions, and    tting n2 coe   cients to n data points is asking for trouble.

7.5 smoothing splines versus kernel regression

for one input variable and one output variable, smoothing splines can basically do
everything which kernel regression can do10. the advantages of splines are their
computational speed and (once we   ve calculated the basis functions) simplicity,
as well as the clarity of controlling curvature directly. kernels however are easier
to program (if slower to run), easier to analyze mathematically11, and extend
more straightforwardly to multiple variables, and to combinations of discrete and
continuous variables.

7.6 some of the math behind splines

above, i claimed that a solution to the optimization problem eq. 7.1 exists, and
is a continuous, piecewise-cubic polynomial, with continuous    rst and second
derivatives, with pieces at the xi, and linear outside the range of the xi. i do not
know of any truly elementary way of showing this, but i will sketch here how it   s
established, if you   re interested.

eq. 7.1 asks us to    nd the function which minimize the sum of the mse and

9 generalizations to more than two input variables are conceptually straightforward     just keep

adding up more partial derivatives     but the book-keeping gets annoying.

10 in fact, there is a technical sense in which, for large n, splines act like a kernel regression with a

speci   c non-gaussian kernel, and a bandwidth which varies over the data, being smaller in
high-density regions. see simono    (1996,   5.6.2), or, for more details, silverman (1984).

11 most of the bias-variance analysis for kernel regression can be done with basic calculus, as we did in

chapter 4. the corresponding analysis for splines requires working in in   nite-dimensional function
spaces called    hilbert spaces   . it   s a pretty theory, if you like that sort of thing.

(cid:90)

l =

164

splines

a certain integral. even the mse can be brought inside the integral, using dirac
delta functions:

(cid:90) (cid:34)

n(cid:88)

i=1

l =

  (m(cid:48)(cid:48)(x))2 +

1
n

(yi     m(xi))2  (x     xi)

dx

(7.24)

(cid:35)

in what follows, without loss of generality, assume that the xi are ordered, so
x1     x2     . . . xi     xi+1     . . . xn. with some loss of generality but a great gain
in simplicity, assume none of the xi are equal, so we can make those inequalities
strict.

the subject which deals with maximizing or minimizing integrals of functions
is the calculus of variations12, and one of its basic tricks is to write the integrand
as a function of x, the function, and its derivatives:

where, in our case,

l =   (m(cid:48)(cid:48)(x))2 +

l(x, m, m(cid:48), m(cid:48)(cid:48))dx

n(cid:88)

i=1

1
n

(yi     m(xi))2  (x     xi)

(7.25)

(7.26)

n(cid:88)

i=1

this sets us up to use a general theorem of the calculus of variations, to the e   ect
that any function   m which minimizes l must also solve l   s euler-lagrange
equation:

   l
   m

    d
dx

   l
   m(cid:48) +

d2
dx2

   l
   m(cid:48)(cid:48)

= 0

(7.27)

(cid:12)(cid:12)(cid:12)(cid:12)m=   m

in our case, the euler-lagrange equation reads

    2
n

(yi       m(xi))  (x     xi) + 2  

d2
dx2

  m(cid:48)(cid:48)(x) = 0

remembering that   m(cid:48)(cid:48)(x) = d2   m/dx2,

d4
dx4

  m(x) =

1
n  

(yi       m(xi))  (x     xi)

n(cid:88)

i=1

(7.28)

(7.29)

the right-hand side is zero at any point x other than one of the xi, so the fourth
derivative has to be zero in between the xi. this in turn means that the function
must be piecewise cubic. now    x an xi, and pick any two points which bracket
it, but are both greater than xi   1 and less than xi+1; call them l and u. integrate

12 in addition to its uses in statistics, the calculus of variations also shows up in physics (   what is the

path of least action?   ), control theory (   what is the cheapest route to the objective?   ) and
stochastic processes (   what is the most probable trajectory?   ). gershenfeld (1999, ch. 4) is a good
starting point.

7.7 further reading

our euler-lagrange equation from l to u:

(cid:90) u

l

d4
dx4

  m(x)dx =

  m(cid:48)(cid:48)(cid:48)(u)       m(cid:48)(cid:48)(cid:48)(l) =

(cid:90) u
n(cid:88)
yi       m(xi)

1
n  

i=1

l

n  

(yi       m(xi))  (x     xi)

165

(7.30)

(7.31)

that is, the third derivative makes a jump when we move across xi, though (since
the fourth derivative is zero), it doesn   t matter which pair of points above and
below xi we compare third derivatives at. integrating the equation again,

yi       m(xi)

  m(cid:48)(cid:48)(u)       m(cid:48)(cid:48)(l) = (u     l)

n  

(7.32)
letting u and l approach xi from either side, so u    l     0, we see that   m(cid:48)(cid:48) makes
no jump at xi. repeating this trick twice more, we conclude the same about
  m(cid:48) and   m itself. in other words,   m must be continuous, with continuous    rst
and second derivatives, and a third derivative that is constant on each (xi, xi+1)
interval. since the fourth derivative is zero on those intervals (and unde   ned at
the xi), the function must be a piecewise cubic, with the piece boundaries at the
xi, and continuity (up to the second derivative) across pieces.

to see that the optimal function must be linear below x1 and above xn, suppose
that it wasn   t. clearly, though, we could reduce the curvature as much as we want
in those regions, without altering the value of the function at the boundary, or
even its    rst derivative there. this would yield a better function, i.e., one with a
lower value of l, since the mse would be unchanged and the average curvature
would be smaller. taking this to the limit, then, the function must be linear
outside the observed data range.

we have now shown13 that the optimal function   m, if it exists, must have all
the properties i claimed for it. we have not shown either that there is a solution,
or that a solution is unique if it does exist. however, we can use the fact that
solutions, if there are any, are piecewise cubics obeying continuity conditions to
set up a system of equations to    nd their coe   cients. in fact, we did so already
in   7.3.1, where we saw it   s a system of n independent linear equations in n
unknowns. such a thing does indeed have a unique solution, here eq. 7.19.

7.7 further reading

there are good discussions of splines in simono    (1996, ch. 5), hastie et al. (2009,
ch. 5) and wasserman (2006,   5.5). wood (2006, ch. 4) includes a thorough prac-
tical treatment of splines as a preparation for additive models (see chapter 8
below) and generalized additive models (see chapters 11   12). the classic ref-
erence, by one of the inventors of splines as a useful statistical tool, is wahba
(1990); it   s great if you already know what a hilbert space is and how to navigate
one.

13 for a very weak value of    shown   , admittedly.

166

splines

historical notes

the    rst introduction of spline smoothing in the statistical literature seems to
be whittaker (1922). (his    graduation    is more or less our    smoothing   .) he
begins with an    inverse id203    (we would now say    bayesian   ) argument
for minimizing eq. 7.1 to    nd the most probable curve, based on the a priori
hypothesis of smooth gaussian curves observed through gaussian error, and gives
tricks for    tting splines more easily with the mathematical technology available
in 1922.

the general optimization problem, and the use of the word    spline   , seems to
have its roots in numerical analysis in the early 1960s; those spline functions were
intended as ways of smoothly interpolating between given points. the connec-
tion to statistical smoothing was made by schoenberg (1964) (who knew about
whittaker   s earlier work) and by reinsch (1967) (who gave code). splines were
then developed as a practical tool in statistics and in applied mathematics in the
1960s and 1970s. silverman (1985) is a still-readable and insightful summary of
this work.

in econometrics, spline smoothing a time series is called the    hodrick-prescott
   lter   , after two economists who re-discovered the technique in 1981, along with
a fallacious argument that    should always take a particular value (1600, as it
happens), regardless of the data. see paige and trindade (2010) for a (polite)
discussion, and demonstration of the advantages of cross-validation.

7.1 the smooth.spline function lets you set the e   ective degrees of freedom explicitly. write

a function which chooses the number of degrees of freedom by    ve-fold cross-validation.

exercises

7.2 when we can   t measure our predictor variables perfectly, it seems like a good idea to try
to include multiple measurements for each one of them. for instance, if we were trying to
predict grades in college from grades in high school, we might include the student   s grade
from each year separately, rather than simply averaging them. multiple measurements
of the same variable will however tend to be strongly correlated, so this means that a
id75 will be nearly multi-collinear. this in turn means that it will tend to
have multiple, mutually-canceling large coe   cients. this makes it hard to interpret the
regression and hard to treat the predictions seriously. (see   2.1.1.)
one strategy for coping with this situation is to carefully select the variables one uses in the
regression. another, however, is to add a penalty for large coe   cient values. for historical
reasons, this second strategy is called ridge regression, or tikhonov id173.
speci   cally, while the ols estimate is

n(cid:88)

i=1

1
n

(cid:98)  ols = argmin
n(cid:88)

(cid:34)

  

1
n

i=1

(cid:98)  rr = argmin

  

(yi     xi      )2 ,

(cid:35)

p(cid:88)

j=1

  2
j

+   

(yi     xi      )2

(7.33)

(7.34)

the regularized or penalized estimate is

exercises

1. show that the matrix form of the ridge-regression objective function is

2. show that the optimum is

   1(y     x  )t (y     x  ) +     t   
n

(cid:98)  rr = (xt x + n  i)

   1xt y

167

(7.35)

(7.36)

(this is where the name    ridge regression    comes from: we take xt x and add a    ridge   
along the diagonal of the matrix.)

3. what happens as        0? as           ? (for the latter, it may help to think about the

case of a one-dimensional x    rst.)

4. let y = z + , with z     u (   1, 1) and       n (0, 0.05). generate 2000 draws from z and
y . now let xi = 0.9z +   , with        n (0, 0.05), for i     1 : 50. generate corresponding
xi values. using the    rst 1000 rows of the data only, do ridge regression of y on the xi
(not on z), plotting the 50 coe   cients as functions of   . explain why ridge regression
is called a shrinkage estimator.

5. use cross-validation with the    rst 1000 rows to pick the optimal value of   . compare the
out-of-sample performance you get with this penalty to the out-of-sample performance
of ols.

for more on ridge regression, see appendix h.3.5.

then

[[todo:
re-
organize:
bring curse
of
dimen-
sionality
up,
additive
models
as
promise,
so
order
lectures?]]

same
as

com-

8

additive models

8.1 additive models

the additive model for regression is that the conditional expectation function
is a sum of partial response functions, one for each predictor variable. formally,
when the vector (cid:126)x of predictor variables has p dimensions, x1, . . . xp, the model
says that

e(cid:104)

y | (cid:126)x = (cid:126)x

(cid:105)

p(cid:88)

=    +

fj(xj)

(8.1)

j=1

this includes the linear model as a special case, where fj(xj) =   jxj, but it   s
clearly more general, because the fjs can be arbitrary nonlinear functions. the
idea is still that each input feature makes a separate contribution to the response,
and these just add up (hence    partial response function   ), but these contribu-
tions don   t have to be strictly proportional to the inputs. we do need to add a
restriction to make it identi   able; without loss of generality, say that e [y ] =   
and e [fj(xj)] = 0.1

additive models keep a lot of the nice properties of linear models, but are
more    exible. one of the nice things about linear models is that they are fairly
straightforward to interpret: if you want to know how the prediction changes
as you change xj, you just need to know   j. the partial response function fj
plays the same role in an additive model: of course the change in prediction from
changing xj will generally depend on the level xj had before perturbation, but
since that   s also true of reality that   s really a feature rather than a bug. it   s true
that a set of plots for fjs takes more room than a table of   js, but it   s also nicer
to look at, conveys more information, and imposes fewer systematic distortions
on the data.

of course, none of this would be of any use if we couldn   t actually estimate
these models, but we can, through a clever computational trick which is worth
knowing for its own sake. the use of the trick is also something they share with
linear models, so we   ll start there.

1 to see why we need to do this, imagine the simple case where p = 2. if we add constants c1 to f1
and c2 to f2, but subtract c1 + c2 from   , then nothing observable has changed about the model.
this degeneracy or lack of identi   ability is a little like the way collinearity keeps us from de   ning
true slopes in id75. but it   s less harmful than collinearity because we can    x it with this
convention.

168

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

(8.3)

(8.4)

(8.5)

(8.6)

(8.7)

8.2 partial residuals and back-   tting

169

8.2 partial residuals and back-   tting

8.2.1 back-   tting for linear models

the general form of a id75 model is
=   0 + (cid:126)      (cid:126)x =

y | (cid:126)x = (cid:126)x

e(cid:104)

(cid:105)

p(cid:88)

j=0

  jxj

(8.2)

where x0 is always the constant 1. (adding this    ctitious constant variable lets
us handle the intercept just like any other regression coe   cient.)

suppose we don   t condition on all of (cid:126)x but just one component of it, say xk.

what is the conditional expectation of y ?

e [y |xk = xk] = e [e [y |x1, x2, . . . xk, . . . xp]|xk = xk]

(cid:34) p(cid:88)

(cid:35)

= e

  jxj|xk = xk

j=0

=   kxk + e

  jxj|xk = xk

(cid:35)

(cid:34)(cid:88)

j(cid:54)=k

where the    rst line uses the law of total expectation2, and the second line uses
eq. 8.2. turned around,

(cid:34)(cid:88)
  kxk = e [y |xk = xk]     e
(cid:33)

(cid:34)

(cid:32)(cid:88)

= e

y    

  jxj

(cid:35)

j(cid:54)=k
|xk = xk

  jxj|xk = xk

(cid:35)

j(cid:54)=k

the expression in the expectation is the kth partial residual     the (total)
residual is the di   erence between y and its expectation, the partial residual is
the di   erence between y and what we expect it to be ignoring the contribution
from xk. let   s introduce a symbol for this, say y (k).
y (k)|xk = xk

  kxk = e(cid:104)

(8.8)

(cid:105)

in words, if the over-all model is linear, then the partial residuals are linear. and
notice that xk is the only input feature appearing here     if we could somehow
get hold of the partial residuals, then we can    nd   k by doing a simple regression,
rather than a multiple regression. of course to get the partial residual we need
to know all the other   js. . .

this suggests the following estimation scheme for linear models, known as
the gauss-seidel algorithm, or more commonly and transparently as back-
   tting; the pseudo-code is in example 17.

this is an iterative approximation algorithm. initially, we look at how far each    you

say

2 as you learned in baby prob., this is the fact that e [y |x] = e [e [y |x, z] |x]     that we can always

condition more variables, provided we then average over those extra variables when we   re done.

i
   it-

   vicious
circle   ,
say
erative
improve-
ment   .   

170
additive models
given: n    (p + 1) inputs x (0th column all 1s)

n    1 responses y
small tolerance    > 0

for k     1 : p {

j(cid:54)=k(cid:98)  jxij

center y and each column of x

(cid:98)  j     0 for j     1 : p
until (all |(cid:98)  j       j|       ) {
i = yi    (cid:80)
y(k)
(cid:98)  k       k
  k     regression coe   cient of y(k) on x  k
}(cid:98)  0     (n   1(cid:80)n
i=1 yi)    (cid:80)p
return: ((cid:98)  0,(cid:98)  1, . . .(cid:98)  p)

j=1(cid:98)  jn   1(cid:80)n

i=1 xij

}

code example 17: pseudocode for back-   tting linear models. assume we make at least one
pass through the until loop. recall from chapter 1 that centering the data does not change the
  js; this way the intercept only has to be calculated once, at the end. [[attn: fix horizontal
lines]]

point is from the global mean, and do a simple regression of those deviations on
the    rst input variable. this then gives us a better idea of what the regression
surface really is, and we use the deviations from that surface in a simple regression
on the next variable; this should catch relations between y and x2 that weren   t
already caught by regressing on x1. we then go on to the next variable in turn.
at each step, each coe   cient is adjusted to    t in with what we have already
guessed about the other coe   cients     that   s why it   s called    back-   tting   . it is
not obvious3 that this will ever converge, but it (generally) does, and the    xed
point on which it converges is the usual least-squares estimate of   .

back-   tting is rarely used to    t linear models these days, because with modern
computers and numerical id202 it   s faster to just calculate (xt x)   1xt y.
but the cute thing about back-   tting is that it doesn   t actually rely on linearity.

8.2.2 back   tting additive models

de   ning the partial residuals by analogy with the linear case, as

(cid:33)

(cid:32)

(cid:88)
(cid:105)

j(cid:54)=k

y (k) = y    

   +

fj(xj)

e(cid:104)

y (k)|xk = xk

= fk(xk)

a little algebra along the lines of   8.2.1 shows that

3 unless, i suppose, you   re gauss.

(8.9)

(8.10)

8.2 partial residuals and back-   tting

171

given: n    p inputs x

n    1 responses y
small tolerance    > 0
one-dimensional smoother s

i=1 yi

for k     1 : p {

(cid:98)       n   1(cid:80)n
(cid:98)fj     0 for j     1 : p
until (all |(cid:98)fj     gj|       ) {
i = yi    (cid:80)
gk     gk     n   1(cid:80)n
y(k)
gk     s(y(k)     x  k)
(cid:98)fk     gk
return: ((cid:98)  , (cid:98)f1, . . . (cid:98)fp)

}

}

j(cid:54)=k (cid:98)fj(xij)

i=1 gk(xik)

code example 18: pseudo-code for back-   tting additive models. notice the extra step, as com-
pared to back-   tting linear models, which keeps each partial response function centered.

if we knew how to estimate arbitrary one-dimensional regressions, we could now
use back-   tting to estimate additive models. but we have spent a lot of time
learning how to use smoothers to    t one-dimensional regressions! we could use
nearest neighbors, or splines, or kernels, or local-id75, or anything
else we feel like substituting here.

our new, improved back-   tting algorithm in example 18. once again, while it   s
not obvious that this converges, it does. also, the back-   tting procedure works
well with some complications or re   nements of the additive model. if we know the
function form of one or another of the fj, we can    t those parametrically (rather
than with the smoother) at the appropriate points in the loop. (this would be a
semiparametric model.) if we think that there is an interaction between xj and
xk, rather than their making separate additive contributions for each variable,
we can smooth them together; etc.

there are actually two packages standard packages for    tting additive models
in r: gam and mgcv. both have commands called gam, which    t generalized
additive models     the generalization is to use the additive model for things
like the probabilities of categorical responses, rather than the response variable
itself. if that sounds obscure right now, don   t worry     we   ll come back to this
in chapters 11   12 after we   ve looked at generalized linear models.   8.4 below
illustrates using one of these packages to    t an additive model.

172

additive models

8.3 the curse of dimensionality

before illustrating how additive models work in practice, let   s talk about why
we   d want to use them. so far, we have looked at two extremes for regression
models; additive models are somewhere in between.

on the one hand, we had id75, which is a parametric method (with
p+1 parameters). its weakness is that the true regression function    is hardly ever
linear, so even with in   nite data id75 will always make systematic
mistakes in its predictions     there   s always some approximation bias, bigger or
smaller depending on how non-linear    is. the strength of id75 is
that it converges very quickly as we get more data. generally speaking,

m selinear =   2 + alinear + o(n   1)

(8.11)

where the    rst term is the intrinsic noise around the true regression function,
the second term is the (squared) approximation bias, and the last term is the
estimation variance. notice that the rate at which the estimation variance shrinks
doesn   t depend on p     factors like that are all absorbed into the big o.4 other
parametric models generally converge at the same rate.

at the other extreme, we   ve seen a number of completely nonparametric regres-
sion methods, such as kernel regression, local polynomials, k-nearest neighbors,
etc. here the limiting approximation bias is actually zero, at least for any rea-
sonable regression function   . the problem is that they converge more slowly,
because we need to use the data not just to    gure out the coe   cients of a para-
metric model, but the sheer shape of the regression function. we saw in chapter 4
that the mean-squared error of kernel regression in one dimension is   2+o(n   4/5).
splines, k-nearest-neighbors (with growing k), etc., all attain the same rate. but
in p dimensions, this becomes (wasserman, 2006,   5.12)
m senonpara       2 = o(n   4/(p+4))

there   s no ultimate approximation bias term here. why does the rate depend on

p? well, to hand-wave a bit, think of kernel smoothing, where (cid:98)  ((cid:126)x) is an average

over yi for (cid:126)xi near (cid:126)x. in a p dimensional space, the volume within   of (cid:126)x is o( p),
so the id203 that a training point (cid:126)xi falls in the averaging region around (cid:126)x
gets exponentially smaller as p grows. turned around, to get the same number of
training points per (cid:126)x, we need exponentially larger sample sizes. the appearance
of the 4s is a little more mysterious, but can be resolved from an error analysis
of the kind we did for kernel regression in chapter 45. this slow rate isn   t just

(8.12)

4 see appendix c you are not familiar with    big o    notation.
5 remember that in one dimension, the bias of a kernel smoother with bandwidth h is o(h2), and the

variance is o(1/nh), because only samples falling in an interval about h across contribute to the
prediction at any one point, and when h is small, the number of such samples is proportional to nh.
adding bias squared to variance gives an error of o(h4) + o((nh)   1), solving for the best
bandwidth gives hopt = o(n   1/5), and the total error is then o(n   4/5). suppose for the moment
that in p dimensions we use the same bandwidth along each dimension. (we get the same end result
with more work if we let each dimension have its own bandwidth.) the bias is still o(h2), because
the taylor expansion still goes through. but now only samples falling into a region of volume o(hp)

8.3 the curse of dimensionality

173

curve(x^(-1),from=1,to=1e4,log="x",xlab="n",ylab="excess mse")
curve(x^(-4/5),add=true,lty="dashed")
curve(x^(-1/26),add=true,lty="dotted")
legend("topright",legend=c(expression(n^{-1}),
expression(n^{-4/5}),expression(n^{-1/26})),
lty=c("solid","dashed","dotted"))

figure 8.1 schematic of rates of convergence of mses for parametric
models (o(n   1)), one-dimensional nonparametric regressions or additive
models (o(n   4/5)), and a 100-dimensional nonparametric regression
(o(n   1/26)). note that the horizontal but not the vertical axis is on a
logarithmic scale.

a weakness of kernel smoothers, but turns out to be the best any nonparametric
estimator can do.

for p = 1, the nonparametric rate is o(n   4/5), which is of course slower than
o(n   1), but not all that much, and the improved bias usually more than makes
up for it. but as p grows, the nonparametric rate gets slower and slower, and the
fully nonparametric estimate more and more imprecise, yielding the infamous
curse of dimensionality. for p = 100, say, we get a rate of o(n   1/26), which
is not very good at all. (see figure 8.1.) said another way, to get the same
precision with p inputs that n data points gives us with one input takes n(4+p)/5
data points. for p = 100, this is n20.8, which tells us that matching the error of
n = 100 one-dimensional observations requires o(4    1041) hundred-dimensional
observations.

so completely unstructured nonparametric regressions won   t work very well in
high dimensions, at least not with plausible amounts of data. the trouble is that

around x contribute to the prediction at x, so the variance is o((nhp)   1). the best bandwidth is
now hopt = o(n   1/(p+4)), yielding an error of o(n   4/(p+4)) as promised.

1101001000100000.00.20.40.60.81.0nexcess msen-1n-45n-126174

additive models

[[attn:
more
mathe-
matical
expla-
nation
in
pendix?]]

ap-

there are just too many possible high-dimensional functions, and seeing only a
trillion points from the function doesn   t pin down its shape very well at all.

this is where additive models come in. not every regression function is additive,
so they have, even asymptotically, some approximation bias. but we can estimate
each fj by a simple one-dimensional smoothing, which converges at o(n   4/5),
almost as good as the parametric rate. so overall

m seadditive       2 = aadditive + o(n   4/5)

(8.13)
since linear models are a sub-class of additive models, aadditive     alm. from a
purely predictive point of view, the only time to prefer linear models to additive
models is when n is so small that o(n   4/5)     o(n   1) exceeds this di   erence in
approximation biases; eventually the additive model will be more accurate.6

8.4 example: california house prices revisited

as an example, we   ll look at data on median house prices across census tracts
from the data-analysis assignment in   a.13. this has both california and penn-
sylvania, but it   s hard to visually see patterns with both states; i   ll do california,
and let you replicate this all on pennsylvania, and even on the combined data.

start with getting the data:

housing <- read.csv("http://www.stat.cmu.edu/~cshalizi/adafaepov/data/calif_penn_2011.csv")
housing <- na.omit(housing)
calif <- housing[housing$statefp == 6, ]

(how do i know that the statefp code of 6 corresponds to california?)
we   ll    t a linear model for the log price, on the thought that it makes some
sense for the factors which raise or lower house values to multiply together, rather
than just adding.

calif.lm <- lm(log(median_house_value) ~ median_household_income + mean_household_income +

population + total_units + vacant_units + owners + median_rooms + mean_household_size_owners +
mean_household_size_renters + latitude + longitude, data = calif)

this is very fast     about a    fth of a second on my laptop.
here are the summary statistics7:

print(summary(calif.lm), signif.stars = false, digits = 3)
##
## call:
## lm(formula = log(median_house_value) ~ median_household_income +
##
##
##
##

mean_household_income + population + total_units + vacant_units +
owners + median_rooms + mean_household_size_owners + mean_household_size_renters +
latitude + longitude, data = calif)

6 unless the best additive approximation to    is linear; then the linear model has no more bias and

less variance.

7 i have suppressed the usual stars on    signi   cant    regression coe   cients, because, as discussed in

chapter ??, those aren   t really the most important variables, and i have reined in r   s tendency to
use far too many decimal places.

8.4 example: california house prices revisited

175

predlims <- function(preds, sigma) {

prediction.sd <- sqrt(preds$se.fit^2 + sigma^2)
upper <- preds$fit + 2 * prediction.sd
lower <- preds$fit - 2 * prediction.sd
lims <- cbind(lower = lower, upper = upper)
return(lims)

}

code example 19: calculating quick-and-dirty prediction limits from a prediction object
(preds) containing    tted values and their standard errors, plus an estimate of the noise level.
because those are two (presumably uncorrelated) sources of noise, we combine the standard
deviations by    adding in quadrature   .

min

max
1.214

3q
0.189

1q median
0.034

## residuals:
##
## -3.855 -0.153
##
## coefficients:
##
-5.74e+00
## (intercept)
1.34e-06
## median_household_income
1.07e-05
## mean_household_income
-4.15e-05
## population
8.37e-05
## total_units
8.37e-07
## vacant_units
-3.98e-03
## owners
-1.62e-02
## median_rooms
## mean_household_size_owners
5.60e-02
## mean_household_size_renters -7.47e-02
-2.14e-01
## latitude
## longitude
-2.15e-01
##
## residual standard error: 0.317 on 7469 degrees of freedom
## multiple r-squared:
## f-statistic: 1.2e+03 on 11 and 7469 df,

estimate std. error t value pr(>|t|)
< 2e-16
0.0038
< 2e-16
< 2e-16
6.4e-08
0.9719
< 2e-16
0.0525
5.8e-15
< 2e-16
< 2e-16
< 2e-16

5.28e-01
4.63e-07
3.88e-07
5.03e-06
1.55e-05
2.37e-05
3.21e-04
8.37e-03
7.16e-03
6.38e-03
5.66e-03
5.94e-03

-10.86
2.90
27.71
-8.27
5.41
0.04
-12.41
-1.94
7.83
-11.71
-37.76
-36.15

0.639,adjusted r-squared:

p-value: <2e-16

0.638

figure 8.2 plots the predicted prices,   2 standard errors, against the actual
prices. the predictions are not all that accurate     the rms residual is 0.317 on
the log scale (i.e., 37% on the original scale), but they do have pretty reasonable
coverage; about 96% of actual prices fall within the prediction limits8. on the
other hand, the predictions are quite precise, with the median of the calculated

8 remember from your id75 class that there are two kinds of con   dence intervals we

might want to use for prediction. one is a con   dence interval for the conditional mean at a given
value of x; the other is a con   dence interval for the realized values of y at a given x. earlier
examples have emphasized the former, but since we don   t know the true conditional means here, we
need to use the latter sort of intervals, prediction intervals proper, to evaluate coverage. the
predlims function in code example 19 calculates a rough prediction interval by taking the standard
error of the conditional mean, combining it with the estimated standard deviation, and multiplying
by 2. strictly speaking, we ought to worry about using a t-distribution rather than a gaussian here,
but with 7469 residual degrees of freedom, this isn   t going to matter much. (assuming gaussian
noise is likely to be more of a concern, but this is only meant to be a rough cut anyway.)

176

additive models

standard errors being 0.011 on the log scale (i.e., 1.1% in dollars). this linear
model thinks it knows what   s going on.

next, we   ll    t an additive model, using the gam function from the mgcv package;
this automatically sets the bandwidths using a fast approximation to leave-one-
out cv called generalized cross-validation, or gcv (  3.4.3).

system.time(calif.gam <- gam(log(median_house_value) ~ s(median_household_income) +

s(mean_household_income) + s(population) + s(total_units) + s(vacant_units) +
s(owners) + s(median_rooms) + s(mean_household_size_owners) + s(mean_household_size_renters) +
s(latitude) + s(longitude), data = calif))

##
##

user
3.452

system elapsed
3.614

0.144

(that is, it took about    ve seconds total to run this.) the s() terms in the
gam formula indicate which terms are to be smoothed     if we wanted particular
parametric forms for some variables, we could do that as well. (unfortunately we
can   t just write medianhousevalue     s(.), we have to list all the variables on
the right-hand side.9) the smoothing here is done by splines (hence s()), and
there are lots of options for controlling the splines, or replacing them by other
smoothers, if you know what you   re doing.

figure 8.3 compares the predicted to the actual responses. the rms error
has improved (0.27 on the log scale, or 130%, with 96% of observations falling
with   2 standard errors of their    tted values), at only a fairly modest cost in
the claimed precision (the median standard error of prediction is 0.02, or 2.1%).
figure 8.4 shows the partial response functions.

it makes little sense to have latitude and longitude make separate additive con-
tributions here; presumably they interact. we can just smooth them together10:

calif.gam2 <- gam(log(median_house_value) ~ s(median_household_income) + s(mean_household_income) +

s(population) + s(total_units) + s(vacant_units) + s(owners) + s(median_rooms) +
s(mean_household_size_owners) + s(mean_household_size_renters) + s(longitude,
latitude), data = calif)

this gives an rms error of   0.25 (log-scale) and 96% coverage, with a median
standard error of 0.021, so accuracy is improving (at least in sample), with little
loss of precision.

figures 8.6 and 8.7 show two di   erent views of the joint smoothing of longitude
and latitude. in the perspective plot, it   s quite clear that price increases specif-
ically towards the coast, and even more speci   cally towards the great coastal
cities. in the contour plot, one sees more clearly an inward bulge of a negative,
but not too very negative, contour line (between -122 and -120 longitude) which
embraces napa, sacramento, and some related areas, which are comparatively
more developed and more expensive than the rest of central california, and so

9 alternately, we could use kevin gilbert   s formulatools functions     see

10 if the two variables which interact have very di   erent magnitudes, it   s better to smooth them with a

https://gist.github.com/kgilbert-cmu.
te() term than an s() term, but here they are comparable. see   8.5 for more, and
help(gam.models).

8.4 example: california house prices revisited

177

graymapper <- function(z, x = calif$longitude, y = calif$latitude, n.levels = 10,

breaks = null, break.by = "length", legend.loc = "topright", digits = 3,
...) {
my.greys = grey(((n.levels - 1):0)/n.levels)
if (!is.null(breaks)) {

stopifnot(length(breaks) == (n.levels + 1))

}
else {

if (identical(break.by, "length")) {

breaks = seq(from = min(z), to = max(z), length.out = n.levels +

1)

}
else {

breaks = quantile(z, probs = seq(0, 1, length.out = n.levels + 1))

}

}
z = cut(z, breaks, include.lowest = true)
colors = my.greys[z]
plot(x, y, col = colors, bg = colors, ...)
if (!is.null(legend.loc)) {

breaks.printable <- signif(breaks[1:n.levels], digits)
legend(legend.loc, legend = breaks.printable, fill = my.greys)

}
invisible(breaks)

}

code example 20: map-making code. in its basic use, this takes vectors for x and y coordinates,
and draws gray points whose color depends on a third vector for z, with darker points indicating
higher values of z. options allow for the control of the number of gray levels, setting the breaks
between levels automatically, and using a legend. returning the break-points makes it easier to
use the same scale in multiple maps. see online for commented code.

more expensive than one would expect based on their distance from the coast
and san francisco.

if you worked through problem set a.13, you will recall that one of the big
things wrong with the linear model is that its errors (the residuals) are highly
structured and very far from random. in essence, it totally missed the existence
of cities, and the fact that houses cost more in cities (because land costs more
there). it   s a good idea, therefore, to make some maps, showing the actual values,
and then, by way of contrast, the residuals of the models. rather than do the
plotting by hand over and over, let   s write a function (code example 20).

figures 8.8 and 8.9 show that allowing for the interaction of latitude and longi-
tude (the smoothing term plotted in figures 8.6   8.7) leads to a much more ran-
dom and less systematic clumping of residuals. this is desirable in itself, even if it
does little to improve the mean prediction error. essentially, what that smooth-
ing term is doing is picking out the existence of california   s urban regions, and
their distinction from the rural background. examining the plots of the inter-
action term should suggest to you how inadequate it would be to just put in a
longitude  latitude term in a linear model.

including an interaction between latitude and longitude in a spatial problem is

178

additive models

pretty obvious. there are other potential interactions which might be important
here     for instance, between the two measures of income, or between the total
number of housing units available and the number of vacant units. we could, of
course, just use a completely unrestricted nonparametric regression     going to
the opposite extreme from the linear model. in addition to the possible curse-
of-dimensionality issues, however, getting something like npreg to run with 7000
data points and 11 predictor variables requires a lot of patience. other techniques,
like nearest neighbor regression (  1.5.1) or regression trees (ch. 13), may run
faster, though cross-validation can be demanding even there.

8.5 interaction terms and expansions

one way to think about additive models, and about (possibly) including interac-
tion terms, is to imagine doing a sort of taylor series or power series expansion
of the true regression function. the zero-th order expansion would be a constant:

(8.14)
the best constant to use here would just be e [y ]. (   best    here is in the mean-
square sense, as usual.) a purely additive model would correspond to a    rst-order
expansion:

  (x)       

  (x)        +

fj(xj)

(8.15)

p(cid:88)

  (x)        +

two-way interactions come in when we go to a second-order expansion:

fj(xj) +

fjk(xj, xk)

(8.16)

j=1

j=1

k=j+1

(why do i limit k to run from j + 1 to p?, rather than from 1 to p?) we will,
of course, insist that e [fjk(xj, xk)] = 0 for all j, k. if we want to estimate these
terms in r, using mgcv, we use the syntax s(xj, xk) or te(xj, xk). the former
   ts a thin-plate spline over the (xj, xk) plane, and is appropriate when those
variables are measured on similar scales, so that curvatures along each direction
are comparable. the latter uses a tensor product of smoothing splines along
each coordinate, and is more appropriate when the measurement scales are very
di   erent11.

there is an important ambiguity here: for any j, with additive partial-response
function fj, i could take any of its interactions, set f(cid:48)
jk(xj, xk) = fjk(xj, xk) +
fj(xj) and f(cid:48)
j(xj) = 0, and get exactly the same predictions under all circum-
stances. this is the parallel to being able to add and subtract constants from the
   rst-order functions, provided we made corresponding changes to the intercept
term. we therefore need to similarly    x the two-way interaction functions.

a natural way to do this is to insist that the second-order fjk function should
11 for the distinction between thin-plate and tensor-product splines, see   7.4. if we want to interact a

continuous variable xj with a categorical xk, mgcv   s syntax is s(xj, by=xk) or te(xj, by=xk).

p(cid:88)
p(cid:88)

j=1

p(cid:88)

8.5 interaction terms and expansions

179

be uncorrelated with (   orthogonal to   ) the    rst-order functions fj and fk; this
is the analog to insisting that the    rst-order functions all have expectation zero.
the fjks then represent purely interactive contributions to the response, which
could not be captured by additive terms. if this is what we want to do, the best
syntax to use in mgcv is ti, which speci   cally separates the    rst- and higher-
order terms, e.g., ti(xj) + ti(xk) + ti(xj, xk) will estimate three functions,
for the additive contributions and their interaction.

an alternative is to just pick a particular fjk, and absorb fj into it. the model

then looks like

  (x)        +

p(cid:88)

p(cid:88)

fjk(xj, xk)

(8.17)

j=1

k=j+1

we can also mix these two approaches, if we speci   cally do not want additive or
interactive terms for certain predictor variables. this is what i did above, where i
estimated a single second-order smoothing term for both latitude and longitude,
with no additive components for either.

of course, there is nothing special about two-way interactions. if you   re curious
about what a three-way term would be like, and you   re lucky enough to have data
which amenable to    tting it, you could certainly try

p(cid:88)

          +

p(cid:88)

p(cid:88)

(cid:88)

fj(xj) +

fjk(xj, xk) +

fjkl(xj, xk, xl)

(8.18)

j=1

j=1

k=j+1

j,k,l

(how should the indices for the last term go?) more ambitious combinations are
certainly possible, though they tend to become a confused mass of algebra and
indices.

geometric interpretation

it   s often convenient to think of the regression function as living in a big (in   nite-
dimensional) vector space of functions. within this space, the constant functions
form a linear sub-space12, and we can ask for the projection of the true regression
function on to that sub-space; this would be the best approximation13 to    as
a constant. this is, of course, the expectation value. the additive functions of
all p variables also form a linear sub-space14, so the right-hand side of eq. 8.15
is just the projection of    on to that space, and so forth and so on. when we
insist on having the higher-order interaction functions be uncorrelated with the
additive functions, we   re taking the projection of    on to the space of all functions
orthogonal to the additive functions.

12 because if f and g are two constant functions, af + bg is also a constant, for any real numbers a and

b.

13 remember that projecting a vector on to a linear sub-space    nds the point in the sub-space closest

to the original vector. this is equivalent to minimizing the (squared) bias.

14 by parallel reasoning to the previous footnote.

180

additive models

selecting interactions

there are two issues with interaction terms. first, the curse of dimensionality
returns: an order-q interaction term will converge at the rate o(n   4/(4+q)), so
they can dominate the over-all uncertainty. second, there are lots of possible

(cid:1), in fact), which can make it very demanding in time and data

interactions ((cid:0)p

to    t them all, and hard to interpret. just as with linear models, therefore, it
can make a lot of sense to selective examine interactions based on subject-matter
knowledge, or residuals of additive models.

q

in some contexts, people like to use models of the form

varying-coe   cient models

p(cid:88)

  (x) =    +

xjfj(x   j)

(8.19)

j=1

where fj is a function of the non-j predictor variables, or some subset of them.
these varying-coe   cient functions are obviously a subset of the usual class of
additive models, but there are occasions where they have some scienti   c justi   -
cation15. these are conveniently estimated in mgcv through the by option, e.g.,
s(xk, by=xj) will estimate a term of the form xjf (xk).16

8.6 closing modeling advice

with modern computing power, there are very few situations in which it is ac-
tually better to do id75 than to    t an additive model. in fact, there
seem to be only two good reasons to prefer linear models.

1. our data analysis is guided by a credible scienti   c theory which asserts lin-
ear relationships among the variables we measure (not others, for which our
observables serve as imperfect proxies).

2. our data set is so massive that either the extra processing time, or the extra
computer memory, needed to    t and store an additive rather than a linear
model is prohibitive.

even when the    rst reason applies, and we have good reasons to believe a linear
theory, the truly scienti   c thing to do would be to check linearity, by    tting a
   exible non-linear model and seeing if it looks close to linear. (we will see formal
tests based on this idea in chapter 9.) even when the second reason applies, we
would like to know how much bias we   re introducing by using linear predictors,
which we could do by randomly selecting a subset of the data which is small
enough for us to manage, and    tting an additive model.

in the vast majority of cases when users of statistical software    t linear models,
neither of these justi   cations applies: theory doesn   t tell us to expect linearity,

15 they can also serve as a    transitional object    when giving up the use of purely linear models.
16 as we saw above, by does something slightly di   erent when given a categorical variable. how are

these two uses related?

8.7 further reading

181

and our machines don   t compel us to use it. id75 is then employed
for no better reason than that users know how to type lm but not gam. you now
know better, and can spread the word.

8.7 further reading

simon wood, who wrote the mgcv package, has a nice book about additive models
and their generalizations, wood (2006); at this level it   s your best source for
further information. buja et al. (1989) is a thorough theoretical treatment.
the expansions of   8.5 are sometimes called    functional analysis of variance   
or    functional anova   . making those ideas precise requires exploring some of
the geometry of in   nite-dimensional spaces of functions (   hilbert space   ). see
wahba (1990) for a treatment of the statistical topic, and halmos (1957) for a
classic introduction to hilbert spaces.

historical notes

ezekiel (1924) seems to be the    rst publication advocating the use of additive
models as a general method, which he called    curvilinear multiple correlation   .
his paper was complete with worked examples on simulated data (with known
answers) and real data (from economics)17. he was explicit that any reasonable
smoothing or regression technique could be used to    nd what we   d call the partial
response functions. he also gave a successive-approximation algorithm for esti-
mate the over-all model: start with an initial guess about all the partial responses;
plot all the partial residuals; re   ne the partial responses simultaneously; repeat.
this di   ers from back-   tting in that the partial response functions are updating
in parallel within each cycle, not one after the other. this is a subtle di   erence,
and ezekiel   s method will often work, but can run into trouble with correlated
predictor variables, when back-   tting will not.

the gauss-seidel or back   tting algorithm was invented by gauss in the early
1800s during his work on least squares estimation in linear models; he mentioned
it in letters to students, described it as something one could do    while half asleep   ,
but never published it. seidel gave the    rst published version in 1874. (for all this
history, see benzi 2009.) i am not sure when the connection was made between
additive statistical models and back-   tting.

exercises

8.1 repeat the analyses of california housing prices with pennsylvania housing prices. which
partial response functions might one reasonably hope would stay the same? do they?
(how can you tell?)

17    each of these curves illustrates and substantiates conclusions reached by theoretical economic

analysis. equally important, they provide de   nite quantitative statements of the relationships. the
method of . . . curvilinear multiple correlation enable[s] us to use the favorite tool of the economist,
caeteris paribus, in the analysis of actual happenings equally as well as in the intricacies of
theoretical reasoning    (p. 453). (see also exercise 8.4.)

182
8.2 additive? for general p, let (cid:107)(cid:126)x(cid:107) be the (ordinary, euclidean) length of the vector (cid:126)x. is
this an additive function of the (ordinary, cartesian) coordinates? is (cid:107)(cid:126)x(cid:107)2 an additive
function? (cid:107)(cid:126)x     (cid:126)x0(cid:107) for a    xed (cid:126)x0? (cid:107)(cid:126)x     (cid:126)x0(cid:107)2?

additive models

8.3 additivity vs. parallelism

1. take any additive function f of p arguments x1, x2, . . . xp. fix a coordinate index i and
a real number c. prove that f (x1, x2, . . . xi, . . . xp)    f (x1, x2, . . . xi + c, . . . xp) depends
only on xi and c, and not on the other coordinates.

2. suppose p = 2, and continue to assume f is additive. consider the curve formed by
plotting f (x1, x2) against x1 for a    xed value of x2, and the curved formed by plotting
f (x1, x2) against x1 with x2    xed at a di   erent value, say x(cid:48)
2. prove that the curves
are parallel, i.e., that the vertical distance between them is constant.

3. for general p and additive f , consider the surfaces formed by the f by varying all but

one of the coordinates. prove that these surfaces are always parallel to each other.

4. is the converse true? that is, do parallel regression surfaces imply an additive model?

8.4 additivity vs. partial derivatives

1. suppose that the true regression function    is additive, with partial response functions
= fj (xj ), so that each partial derivative is a function of that

fj . show that      
   xj
coordinate alone.

2. (much harder) suppose that, for each coordinate xj , there is some function fj of xj

8.5

alone such that      
   xj

= fj (xj ). is    necessarily additive?

suppose that an additive model holds, so that y =    +(cid:80)p
e(cid:2)fj (xj )(cid:3) = 0 for each j, and e [ |x = x] = 0 for all x.
(cid:3). show that
1. for each j, let   j (xj ) = e(cid:2)y |xj = xj
(cid:88)

  j (xj ) =    + fj (xj ) +

e(cid:2)fk(xk)|xj = xj

(cid:3)

k(cid:54)=j

j=1 fj (xj ) +  , with    = e [y ],

2. show that if xk is statistically independent of xj , for all k (cid:54)= j, then   j (xj )        =

fj (xj ).

3. does the conclusion of exercise 22 still hold if one or more of the xks is statistically
dependent on xj ? explain why this should be the case, or give a counter-example to
show that it   s not true. hint: all linear models are additive models, so if it is true for
all additive models, it   s true for all linear models. is it true for all linear models?

exercises

183

plot(calif$median_house_value, exp(preds.lm$fit), type = "n", xlab = "actual price ($)",

ylab = "predicted ($)", main = "linear model", ylim = c(0, exp(max(predlims.lm))))

segments(calif$median_house_value, exp(predlims.lm[, "lower"]), calif$median_house_value,

exp(predlims.lm[, "upper"]), col = "grey")

abline(a = 0, b = 1, lty = "dashed")
points(calif$median_house_value, exp(preds.lm$fit), pch = 16, cex = 0.1)

figure 8.2 actual median house values (horizontal axis) versus those
predicted by the linear model (black dots), plus or minus two predictive
standard errors (grey bars). the dashed line shows where actual and
predicted prices are equal. here predict gives both a    tted value for each
point, and a standard error for that prediction. (without a newdata
argument, predict defaults to the data used to estimate calif.lm, which
here is what we want.) predictions are exponentiated so they   re comparable
to the original values (and because it   s easier to grasp dollars than
log-dollars).

0e+002e+054e+056e+058e+051e+060e+001e+062e+063e+064e+06linear modelactual price ($)predicted ($)lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll184

additive models

plot(calif$median_house_value, exp(preds.gam$fit), type = "n", xlab = "actual price ($)",

ylab = "predicted ($)", main = "first additive model", ylim = c(0, exp(max(predlims.gam))))

segments(calif$median_house_value, exp(predlims.gam[, "lower"]), calif$median_house_value,

exp(predlims.gam[, "upper"]), col = "grey")

abline(a = 0, b = 1, lty = "dashed")
points(calif$median_house_value, exp(preds.gam$fit), pch = 16, cex = 0.1)

figure 8.3 actual versus predicted prices for the additive model, as in
figure 8.2. note that the sig2 attribute of a model returned by gam() is the
estimate of the noise variance around the regression surface (  2).

0e+002e+054e+056e+058e+051e+060500000100000015000002000000first additive modelactual price ($)predicted ($)lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllexercises

185

figure 8.4 the estimated partial response functions for the additive
model, with a shaded region showing   2 standard errors. the tick marks
along the horizontal axis show the observed values of the input variables (a
rug plot); note that the error bars are wider where there are fewer
observations. setting pages=0 (the default) would produce eight separate
plots, with the user prompted to cycle through them. setting scale=0 gives
each plot its own vertical scale; the default is to force them to share the
same one. finally, note that here the vertical scales are logarithmic.

50000150000   0.10.10.3median_household_incomes(median_household_income,5.03)50000200000   1.00.01.0mean_household_incomes(mean_household_income,6.52)020000   0.6   0.20.2populations(population,2.27)040008000   0.8   0.40.0total_unitss(total_units,5.37)0200050000.01.02.0vacant_unitss(vacant_units,5.85)04080   0.20.20.4ownerss(owners,3.88)2468   0.20.00.2median_roomss(median_rooms,7.39)246810   0.50.00.5mean_household_size_ownerss(mean_household_size_owners,7.82)246810   0.6   0.20.2mean_household_size_renterss(mean_household_size_renters,3.11)343842   1.00.00.5latitudes(latitude,8.81)   124   120   116   1.5   0.50.5longitudes(longitude,8.85)186

additive models

plot(calif.gam2, scale = 0, se = 2, shade = true, resid = true, pages = 1)

figure 8.5 partial response functions and partial residuals for addfit2, as
in figure 8.4. see subsequent    gures for the joint smoothing of longitude
and latitude, which here is an illegible mess. see help(plot.gam) for the
plotting options used here.

50000150000   4   201median_household_incomes(median_household_income,6.74)50000200000   3   2   101mean_household_incomes(mean_household_income,6.08)020000   3   101populations(population,1)040008000   3   101total_unitss(total_units,2.88)020005000   3   101vacant_unitss(vacant_units,4.62)04080   3   101ownerss(owners,6.12)2468   4   201median_roomss(median_rooms,7.89)246810   3   101mean_household_size_ownerss(mean_household_size_owners,7.95)246810   3   101mean_household_size_renterss(mean_household_size_renters,3.05)    0.8     0.8     0.6     0.4     0.4     0.4     0.2     0.2  0  0  0.2  0.4  0.6  0.6  0.6  0.8 s(longitude,latitude,28.47)   124   122   120   118   116   1143436384042longitudelatitude    0.6     0.6     0.4     0.2     0.2  0  0  0.2  0.2  0.4  0.6  0.6  0.8    1se    0.8     0.8     0.8     0.6     0.6     0.4     0.4     0.2     0.2  0  0  0.2  0.4  0.6 +1seexercises

187

plot(calif.gam2, select = 10, phi = 60, pers = true, ticktype = "detailed",

cex.axis = 0.5)

figure 8.6 the result of the joint smoothing of longitude and latitude.

longitude   124   122   120   118   116latitude34363840s(longitude,latitude,28.47)   0.50.00.5188

additive models

plot(calif.gam2, select = 10, se = false)

figure 8.7 the result of the joint smoothing of longitude and latitude.
setting se=true, the default, adds standard errors for the contour lines in
multiple colors. again, note that these are log units.

s(longitude,latitude,28.47)longitudelatitude    0.8     0.8     0.6     0.4     0.4     0.4     0.2     0.2  0  0  0.2  0.4  0.6  0.6  0.6  0.8    124   122   120   118   116   1143436384042exercises

189

par(mfrow = c(2, 2))
calif.breaks <- graymapper(calif$median_house_value, pch = 16, xlab = "longitude",

ylab = "latitude", main = "data", break.by = "quantiles")

graymapper(exp(preds.lm$fit), breaks = calif.breaks, pch = 16, xlab = "longitude",

ylab = "latitude", legend.loc = null, main = "linear model")

graymapper(exp(preds.gam$fit), breaks = calif.breaks, legend.loc = null, pch = 16,

xlab = "longitude", ylab = "latitude", main = "first additive model")

graymapper(exp(preds.gam2$fit), breaks = calif.breaks, legend.loc = null, pch = 16,

xlab = "longitude", ylab = "latitude", main = "second additive model")

par(mfrow = c(1, 1))

figure 8.8 maps of real prices (top left), and those predicted by the linear
model (top right), the purely additive model (bottom left), and the additive
model with interaction between latitude and longitude (bottom right).
categories are deciles of the actual prices.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042datalongitudelatitude16200181000243000296000342000382000431000493000591000705000lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042linear modellongitudelatitudelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042first additive modellongitudelatitudelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042second additive modellongitudelatitude190

additive models

figure 8.9 actual housing values (top left), and the residuals of the three
models. (the residuals are all plotted with the same color codes.) notice
that both the linear model and the additive model without spatial
interaction systematically mis-price urban areas. the model with spatial
interaction does much better at having randomly-scattered errors, though
hardly perfect.     how would you make a map of the magnitude of
regression errors?

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042datalongitudelatitude16200181000243000296000342000382000431000493000591000705000lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042residuals of linear modellongitudelatitude   3.85   0.352   0.205   0.11   0.03370.03370.09520.1560.2260.329llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042residuals errors of first additive modellongitudelatitudellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042residuals of second additive modellongitudelatitude9

testing parametric regression speci   cations

with nonparametric regression

9.1 testing functional forms

one important, but under-appreciated, use of nonparametric regression is in test-
ing whether parametric regressions are well-speci   ed. the typical parametric re-
gression model is something like

y = f (x;   ) +  

(9.1)

where f is some function which is completely speci   ed except for the adjustable
parameters   , and  , as usual, is uncorrelated noise. usually, but not necessarily,
people use a function f that is linear in the variables in x, or perhaps includes
some interactions between them.

how can we tell if the speci   cation is right? if, for example, it   s a linear model,
how can we check whether there might not be some nonlinearity? one common
approach is to modify the speci   cation by adding in speci   c departures from the
modeling assumptions     say, adding a quadratic term     and seeing whether the
coe   cients that go with those terms are signi   cantly non-zero, or whether the
improvement in    t is signi   cant.1 for example, one might compare the model

to the model

y =   1x1 +   2x2 +  

y =   1x1 +   2x2 +   3x2

1 +  

(9.2)

(9.3)

by checking whether the estimated   3 is signi   cantly di   erent from 0, or whether
the residuals from the second model are signi   cantly smaller than the residuals
from the    rst.

this can work, if you have chosen the right nonlinearity to test. it has the
power to detect certain mis-speci   cations, if they exist, but not others. (what if
the departure from linearity is not quadratic but cubic?) if you have good reasons
to think that when the model is wrong, it can only be wrong in certain ways,
   ne; if not, though, why only check for those errors?

nonparametric regression e   ectively lets you check for all kinds of systematic
errors, rather than singling out a particular one. there are three basic approaches,
which i give in order of increasing sophistication.

1 in my experience, this is second in popularity only to ignoring the issue.

191

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

su   ciently small.

testing regression speci   cations

192
    if the parametric model is right, it should predict as well as, or even better than,

the non-parametric one, and we can check whether m sep((cid:98)  )     m senp((cid:98)  ) is
should be very close to the parametric one. so we can check whether f (x;(cid:98)  )    
(cid:98)  (x) is approximately zero everywhere.

    if the parametric model is right, the non-parametric estimated regression curve

    if the parametric model is right, then its residuals should be patternless and

independent of input features, because

e [y     f (x;   )|x] = e [f (x;   ) +       f (x;   )|x] = e [ |x] = 0

(9.4)
so we can apply non-parametric smoothing to the parametric residuals, y    

f (x;(cid:98)  ), and see if their expectation is approximately zero everywhere.

we   ll stick with the    rst procedure, because it   s simpler for us to implement
computationally. however, it turns out to be easier to develop theory for the
other two, and especially for the third     see li and racine (2007, ch. 12), or
hart (1997).

here is the basic procedure.

1. get data (x1, y1), (x2, y2), . . . (xn, yn).

3. fit your favorite nonparametric regression (using cross-validation to pick con-

2. fit the parametric model, getting an estimate (cid:98)  , and in-sample mean-squared
error m sep((cid:98)  ).
trol settings as necessary), getting curve (cid:98)   and in-sample mean-squared error
m senp((cid:98)  ).
4. calculate (cid:98)d = m sep((cid:98)  )     m senp((cid:98)  ).
5. simulate from the parametric model (cid:98)   to get faked data (x   
1. fit the parametric model to the simulated data, getting estimate (cid:98)      and
m sep((cid:98)     ).
2. fit the nonparametric model to the simulated data, getting estimate (cid:98)     
and m senp((cid:98)     ).
3. calculate d    = m sep((cid:98)     )     m senp((cid:98)     ).

1), . . . (x   

n, y   
n).

1, y   

6. repeat step 5 b times to get an estimate of the distribution of d under the

null hypothesis.

7. the approximate p-value is

1+#{d   >(cid:98)d}

1+b

.

let   s step through the logic. in general, the error of the non-parametric model
will be converging to the smallest level compatible with the intrinsic noise of the
process. what about the parametric model?

suppose on the one hand that the parametric model is correctly speci   ed. then
its error will also be converging to the minimum     by assumption, it   s got the

functional form right so bias will go to zero, and as (cid:98)         0, the variance will also

9.1 testing functional forms

193

go to zero. in fact, with enough data the correctly-speci   ed parametric model
will actually generalize better than the non-parametric model2.

suppose on the other hand that the parametric model is mis-speci   ed. then
its predictions are systematically wrong, even with unlimited amounts of data
    there   s some bias which never goes away, no matter how big the sample.
since the non-parametric smoother does eventually come arbitrarily close to the
true regression function, the smoother will end up predicting better than the
parametric model.

smaller errors for the smoother, then, suggest that the parametric model is
wrong. but since the smoother has higher capacity, it could easily get smaller er-
rors on a particular sample by chance and/or over-   tting, so only big di   erences
in error count as evidence. simulating from the parametric model gives us surro-
gate data which looks just like reality ought to, if the model is true. we then see
how much better we could expect the non-parametric smoother to    t under the
parametric model. if the non-parametric smoother    ts the actual data much bet-
ter than this, we can reject the parametric model with high con   dence: it   s really
unlikely that we   d see that big an improvement from using the nonparametric
model just by luck.3

as usual, we simulate from the parametric model simply because we have
no hope of working out the distribution of the di   erences in mses from    rst
principles. this is an example of our general strategy of id64.

9.1.1 examples of testing a parametric model

let   s see this in action. first, let   s detect a reasonably subtle nonlinearity. take
the non-linear function g(x) = log (1 + x), and say that y = g(x)+ , with   being
iid gaussian noise with mean 0 and standard deviation 0.15. (this is one of the
examples from   4.2.) figure 9.1 shows the regression function and the data. the
nonlinearity is clear with the curve to    guide the eye   , but fairly subtle.

a simple id75 looks pretty good:

glinfit = lm(y ~ x, data = gframe)
print(summary(glinfit), signif.stars = false, digits = 2)
##
## call:
## lm(formula = y ~ x, data = gframe)
##
## residuals:
##
## -0.499 -0.091
##
## coefficients:

1q median
0.002

3q
0.106

max
0.425

min

2 remember that the smoother must, so to speak, use up some of the information in the data to

   gure out the shape of the regression function. the parametric model, on the other hand, takes that
basic shape as given, and uses all the data   s information to tune its parameters.

3 as usual with p-values, this is not symmetric. a high p-value might mean that the true regression

function is very close to   (x;   ), or it might mean that we don   t have enough data to draw
conclusions (or that we were unlucky).

194

testing regression speci   cations

x <- runif(300, 0, 3)
yg <- log(x + 1) + rnorm(length(x), 0, 0.15)
gframe <- data.frame(x = x, y = yg)
plot(x, yg, xlab = "x", ylab = "y", pch = 16, cex = 0.5)
curve(log(1 + x), col = "grey", add = true, lwd = 4)

figure 9.1 true regression curve (grey) and data points (circles). the
curve g(x) = log (1 + x).

estimate std. error t value pr(>|t|)
<2e-16
<2e-16

##
## (intercept)
## x
##
## residual standard error: 0.15 on 298 degrees of freedom
## multiple r-squared:

0.86,adjusted r-squared:

0.182
0.434

0.017
0.010

10
43

0.86

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.00.00.51.01.5xy9.1 testing functional forms

195

figure 9.2 as in previous    gure, but adding the least-squares regression
line (black). line widths exaggerated for clarity.

## f-statistic: 1.8e+03 on 1 and 298 df,

p-value: <2e-16

r2 is ridiculously high     the regression line preserves 86 percent of the variance
in the data. the p-value reported by r is also very, very low, but remember all
this really means is    you   d have to be crazy to think a    at line    t better than
straight line with a slope    (figure 9.2).

the in-sample mse of the linear    t is4

4 if we ask r for the mse, by squaring summary(glinfit)$sigma, we get 0.0234815. this di   ers from
the mean of the squared residuals by a factor of factor of n/(n     2) = 300/298 = 1.0067, because r
is trying to estimate the out-of-sample error by scaling up the in-sample error, the same way the
estimated population variance scales up the sample variance. we want to compare in-sample    ts.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.00.00.51.01.5xy196

testing regression speci   cations

sim.lm <- function(linfit, test.x) {

n <- length(test.x)
sim.frame <- data.frame(x = test.x)
sigma <- summary(linfit)$sigma * (n - 2)/n
y.sim <- predict(linfit, newdata = sim.frame)
y.sim <- y.sim + rnorm(n, 0, sigma)
sim.frame <- data.frame(sim.frame, y = y.sim)
return(sim.frame)

}

code example 21: simulate a new data set from a linear model, assuming homoskedastic
gaussian noise. it also assumes that there is one input variable, x, and that the response variable
is called y. could you modify it to work with multiple regression?

calc.d <- function(data) {

mse.p <- mean((lm(y ~ x, data = data)$residuals)^2)
mse.np.bw <- npregbw(y ~ x, data = data)
mse.np <- npreg(mse.np.bw)$mse
return(mse.p - mse.np)

}

code example 22: calculate the di   erence-in-mses test statistic.

signif(mean(residuals(glinfit)^2), 3)
## [1] 0.0233

the nonparametric regression has a somewhat smaller mse5

library(np)
gnpr <- npreg(y ~ x, data = gframe)
signif(gnpr$mse, 3)
## [1] 0.0204

so (cid:98)d is

signif((d.hat = mean(glinfit$residual^2) - gnpr$mse), 3)
## [1] 0.00294

now we need to simulate from the    tted parametric model, using its estimated
coe   cients and noise level. we have seen several times now how to do this. the
function sim.lm in example 21 does this, along the same lines as the examples in
chapter 6; it assumes homoskedastic gaussian noise. again, as before, we need
a function which will calculate the di   erence in mses between a linear model
and a kernel smoother    t to the same data set     which will do automatically
what we did by hand above. this is calc.d in example 22. note that the kernel
bandwidth has to be re-tuned to each new data set.

if we call calc.d on the output of sim.lm, we get one value of the test statistic

under the null distribution:

5 npreg does not apply the kind of correction mentioned in the previous footnote.

9.1 testing functional forms

197

calc.d(sim.lm(glinfit, x))
## [1] 0.0005368707

now we just repeat this a lot to get a good approximation to the sampling

distribution of d under the null hypothesis:

null.samples.d <- replicate(200, calc.d(sim.lm(glinfit, x)))

this takes some time, because each replication involves not just generating a
new simulation sample, but also cross-validation to pick a bandwidth. this adds
up to about a second per replicate on my laptop, and so a couple of minutes for
200 replicates.

(while the computer is thinking, look at the command a little more closely.
it leaves the x values alone, and only uses simulation to generate new y values.
this is appropriate here because our model doesn   t really say where the x values
came from; it   s just about the conditional distribution of y given x. if the model
we were testing speci   ed a distribution for x, we should generate x each time we
invoke calc.d. if the speci   cation is vague, like    x is iid    but with no particular
distribution, then resample x.)

(cid:98)d is pretty far out along the right tail (figure ??). this tells us that it   s very

when it   s done, we can plot the distribution and see that the observed value

unlikely that npreg would improve so much on the linear model if the latter were
true. in fact, exactly 0 of the simulated values of the test statistic were that big:

sum(null.samples.d > d.hat)
## [1] 0

thus our estimated p-value is     0.00498. we can reject the linear model pretty

con   dently.6

as a second example, let   s suppose that the linear model is right     then the

test should give us a high p-value. so let us stipulate that in reality

y = 0.2 + 0.5x +   

(9.5)
with        n (0, 0.152). figure 9.4 shows data from this, of the same size as before.
repeating the same exercise as before, we get that   d = 7.7    10   4, together
with a slightly di   erent null distribution (figure 9.5). now the p-value is 0.3,
which it would be quite rash to reject.

9.1.2 remarks

other nonparametric regressions

there is nothing especially magical about using kernel regression here. any con-
sistent nonparametric estimator (say, your favorite spline) would work. they may
di   er somewhat in their answers on particular cases.

6 if we wanted a more precise estimate of the p-value, we   d need to use more bootstrap samples.

198

testing regression speci   cations

hist(null.samples.d, n = 31, xlim = c(min(null.samples.d), 1.1 * d.hat), id203 = true)
abline(v = d.hat)

figure 9.3 histogram of the distribution of d = m sep     m senp for data
simulated from the parametric model. the vertical line marks the observed
value. notice that the mode is positive and the distribution is right-skewed;
this is typical.

curse of dimensionality

for multivariate regressions, testing against a fully nonparametric alternative can
be very time-consuming, as well as running up against curse-of-dimensionality

histogram of null.samples.dnull.samples.ddensity0.00000.00050.00100.00150.00200.00250.00300500100015009.1 testing functional forms

199

y2 <- 0.2 + 0.5 * x + rnorm(length(x), 0, 0.15)
y2.frame <- data.frame(x = x, y = y2)
plot(x, y2, xlab = "x", ylab = "y")
abline(0.2, 0.5, col = "grey", lwd = 2)

figure 9.4 data from the linear model (true regression line in grey).

issues7. a compromise is to test the parametric regression against an additive
model. essentially nothing has to change.

7 this curse manifests itself here as a loss of power in the test. said another way, because

unconstrained non-parametric regression must use a lot of data points just to determine the general
shape of the regression function, even more data is needed to tell whether a particular parametric
guess is wrong.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.51.01.52.02.53.00.00.51.01.5xy200

testing regression speci   cations

figure 9.5 as in figure 9.3, but using the data and    ts from figure 9.4.

testing e [(cid:98) |x] = 0

i mentioned at the beginning of the chapter that one way to test whether the
parametric model is correctly speci   ed is to test whether the residuals have expec-
tation zero everywhere. setting r(x; m)     e [y     m(x)|x = x], we know from
chapter ?? that r(x;   ) = 0 everywhere, and that, for any other function m,

r(x; m) (cid:54)= 0 for at least some values of x. thus, if we take the residuals(cid:98)  from our

parametric model and we smooth them, we get an estimated function   r(x) that
should be converging to 0 everywhere if the parametric model is well-speci   ed.
a natural test statistic is therefore some measure of the    size    of   r, such as8

8 if you   ve taken functional analysis or measure theory, you may recognize these as the (squared) l2

and l2(f ) norms of the function   r.

histogram of null.samples.d.y2null.samples.d.y2density0.0000.0010.0020.0030.0040.005020040060080010009.2 why use parametric models at all?

(cid:82)   r2(x)dx, or(cid:82)   r2(x)f (x)dx (where f (x) is the pdf of x). (the latter, in particu-
lar, can be approximated by n   1(cid:80)n

i=1   r2(xi).) our testing procedure would then
amount to (i)    nding the residuals by    tting the parametric model, (ii) smooth-
ing the residuals to get   r, (iii) calculating the size of   r, and (iv) simulating to
get a distribution for how big   r should be, under the null hypothesis that the
parametric model is right.

201

an alternative to measuring the size of the expected-residuals function would
be to try to predict the residuals. we would compare the mses of the    model   
that the residuals have conditional expectation 0 everywhere, to the mse of the
model that predicts the residuals by smoothing against x, and proceed much as
before9.

stabilizing the sampling distribution of the test statistic

i have just looked at the di   erence in mses. the bootstrap principle being in-
voked is that the sampling distribution of the test statistic, under the estimated
parametric model, should be close to the distribution under the true parameter
value. as discussed in chapter 6, sometimes some massaging of the test statistic
helps bring these distributions closer. some modi   cations to consider:
    divide the mse di   erence by an estimate of the noise   .
    divide by an estimate of the noise    times the di   erence in degrees of freedom,
using the e   ective degrees of freedom (  1.5.3.2) of the nonparametric regression.
    use the log of the ratio in mses instead of the mse di   erence.

doing a double bootstrap can help you assess whether these are necessary.

9.2 why use parametric models at all?

it might seem by this point that there is little point to using parametric models
at all. either our favorite parametric model is right, or it isn   t. if it is right, then
a consistent nonparametric estimate will eventually approximate it arbitrarily
closely. if the parametric model is wrong, it will not self-correct, but the non-
parametric estimate will eventually show us that the parametric model doesn   t
work. either way, the parametric model seems super   uous.

there are two things wrong with this line of reasoning     two good reasons to

use parametric models.

1. one use of statistical models, like regression models, is to connect scienti   c
theories to data. the theories are ideas about the mechanisms generating the
data. sometimes these ideas are precise enough to tell us what the functional
form of the regression should be, or even what the distribution of noise terms
should be, but still contain unknown parameters. in this case, the parameters

9 can you write the di   erence in mses for the residuals in terms of either of the measures of the size

of   r?

202

testing regression speci   cations

themselves are substantively meaningful and interesting     we don   t just care
about prediction.10

2. even if all we care about is prediction accuracy, there is still the bias-variance
trade-o    to consider. non-parametric smoothers will have larger variance in
their predictions, at the same sample size, than correctly-speci   ed parametric
models, simply because the former are more    exible. both models are converg-
ing on the true regression function, but the parametric model converges faster,
because it searches over a more con   ned space. in terms of total prediction
error, the parametric model   s low variance plus vanishing bias beats the non-
parametric smoother   s larger variance plus vanishing bias. (remember that
this is part of the logic of testing parametric models in the previous section.)
in the next section, we will see that this argument can actually be pushed
further, to work with not-quite-correctly speci   ed models.

of course, both of these advantages of parametric models only obtain if they
are well-speci   ed. if we want to claim those advantages, we need to check the
speci   cation.

9.2.1 why we sometimes want mis-speci   ed parametric models

low-dimensional parametric models have potentially high bias (if the real re-
gression curve is very di   erent from what the model posits), but low variance
(because there isn   t that much to estimate). non-parametric regression models
have low bias (they   re    exible) but high variance (they   re    exible). if the para-
metric model is true, it can converge faster than the non-parametric one. even if
the parametric model isn   t quite true, a small bias plus low variance can some-
times still beat a non-parametric smoother   s smaller bias and substantial vari-
ance. with enough data the non-parametric smoother will eventually over-take
the mis-speci   ed parametric model, but with small samples we might be better
o    embracing bias.

to illustrate, suppose that the true regression function is

e [y |x = x] = 0.2 +

1
2

1 +

sin x
10

x

(9.6)

this is very nearly linear over small ranges     say x     [0, 3] (figure 9.6).

expected generalization error, by averaging over many samples (example 23).

i will use the fact that i know the true model here to calculate the actual
figure 9.7 shows that, out to a fairly substantial sample size (    500), the
lower bias of the non-parametric regression is systematically beaten by the lower
variance of the linear model     though admittedly not by much.

10 on the other hand, it is not uncommon for scientists to write down theories positing linear

relationships between variables, not because they actually believe that, but because that   s the only
thing they know how to estimate statistically.

(cid:18)

(cid:19)

9.2 why use parametric models at all?

203

h <- function(x) { 0.2 + 0.5*(1+sin(x)/10)*x }
curve(h(x),from=0,to=3)

figure 9.6 graph of h(x) = 0.2 + 1
2

(cid:0)1 + sin x

10

(cid:1) x over [0, 3].

nearly.linear.out.of.sample = function(n) {

x <- seq(from = 0, to = 3, length.out = n)
y <- h(x) + rnorm(n, 0, 0.15)
data <- data.frame(x = x, y = y)
y.new <- h(x) + rnorm(n, 0, 0.15)
sim.lm <- lm(y ~ x, data = data)
lm.mse <- mean((fitted(sim.lm) - y.new)^2)
sim.np.bw <- npregbw(y ~ x, data = data)
sim.np <- npreg(sim.np.bw)
np.mse <- mean((fitted(sim.np) - y.new)^2)
mses <- c(lm.mse, np.mse)
return(mses)

}
nearly.linear.generalization <- function(n, m = 100) {

raw <- replicate(m, nearly.linear.out.of.sample(n))
reduced <- rowmeans(raw)
return(reduced)

}

code example 23: evaluating the out-of-sample error for the nearly-linear problem as a func-
tion of n, and evaluting the generalization error by averaging over many samples.

0.00.51.01.52.02.53.00.51.01.5xh(x)204

testing regression speci   cations

sizes <- c(5, 10, 15, 20, 25, 30, 50, 100, 200, 500, 1000)
generalizations <- sapply(sizes, nearly.linear.generalization)
plot(sizes, sqrt(generalizations[1, ]), type = "l", xlab = "n", ylab = "rms generalization error",

log = "xy", ylim = range(sqrt(generalizations)))

lines(sizes, sqrt(generalizations[2, ]), lty = "dashed")
abline(h = 0.15, col = "grey")

figure 9.7 root-mean-square generalization error for linear model (solid
line) and kernel smoother (dashed line),    t to the same sample of the
indicated size. the true regression curve is as in 9.6, and observations are
corrupted by iid gaussian noise with    = 0.15 (grey horizontal line). the
cross-over after which the nonparametric regressor has better generalization
performance happens shortly before n = 500.

510205010020050010000.160.180.200.22nrms generalization error9.3 further reading

205

9.3 further reading

this chapter has been on speci   cation testing for regression models, focusing on
whether they are correctly speci   ed for the conditional expectation function. i
am not aware of any other treatment of this topic at this level, other than the
not-wholly-independent spain et al. (2012). if you have somewhat more statistical
theory than this book demands, there are very good treatments of related tests in
li and racine (2007), and of tests based on smoothing residuals in hart (1997).
econometrics seems to have more of a tradition of formal speci   cation testing
than many other branches of statistics. godfrey (1988) reviews tests based on
looking for parametric extensions of the model, i.e., re   nements of the idea of
testing whether   3 = 0 in eq. 9.3. white (1994) combines a detailed theory
of speci   cation testing within parametric stochastic models, not presuming any
particular parametric model is correct, with an analysis of when we can and
cannot still draw useful id136s from estimates within a mis-speci   ed model.
because of its generality, it, too, is at a higher theoretical level than this book,
but is strongly recommend. white was also the co-author of a paper (hong and
white, 1995) presenting a theoretical analysis of the di   erence-in-mses test used
in this chapter, albeit for a particular sort of nonparametric regression we   ve not
really touched on.

we will return to speci   cation testing in appendix e and chapter 15, but for

models of distributions, rather than regressions.

10

moving beyond conditional expectations:
weighted least squares, heteroskedasticity,

local polynomial regression

so far, all our estimates have been based on the mean squared error, giving equal
importance to all observations, as is generally appropriate when looking at con-
ditional expectations. in this chapter, we   ll start to work with giving more or
less weight to di   erent observations, through weighted least squares. the oldest
reason to want to use weighted least squares is to deal with non-constant vari-
ance, or heteroskedasticity, by giving more weight to lower-variance observations.
this leads us naturally to estimating the conditional variance function, just as
we   ve been estimating conditional expectations. on the other hand, weighted
least squares lets us general kernel regression to locally polynomial regression.

10.1 weighted least squares

when we use ordinary least squares to estimate id75, we (naturally)
minimize the mean squared error:

n(cid:88)

1
n

m se(  ) =

(yi     (cid:126)xi      )2

i=1

(cid:98)  ols = (xt x)   1xt y

the solution is of course

we could instead minimize the weighted mean squared error,

w m se(  , (cid:126)w) =

1
n

wi(yi     (cid:126)xi      )2

n(cid:88)

i=1

this includes ordinary least squares as the special case where all the weights
wi = 1. we can solve it by the same kind of id202 we used to solve the
ordinary linear least squares problem. if we write w for the matrix with the wi
on the diagonal and zeroes everywhere else, the solution is

(cid:98)  w ls = (xt wx)   1xt wy

but why would we want to minimize eq. 10.3?

1. focusing accuracy. we may care very strongly about predicting the response
for certain values of the input     ones we expect to see often again, ones where
mistakes are especially costly or embarrassing or painful, etc.     than others.

206

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

(10.1)

(10.2)

(10.3)

(10.4)

10.1 weighted least squares

207

if we give the points (cid:126)xi near that region big weights wi, and points elsewhere
smaller weights, the regression will be pulled towards matching the data in
that region.
2. discounting imprecision. ordinary least squares is the maximum likelihood
estimate when the   in y = (cid:126)x       +   is iid gaussian white noise. this means
that the variance of   has to be constant, and we measure the regression curve
with the same precision elsewhere. this situation, of constant noise variance,
is called homoskedasticity. often however the magnitude of the noise is not
constant, and the data are heteroskedastic.

when we have heteroskedasticity, even if each noise term is still gaussian,
ordinary least squares is no longer the maximum likelihood estimate, and so no
longer e   cient. if however we know the noise variance   2
i at each measurement
i, and set wi = 1/  2
i , we get the heteroskedastic id113, and recover e   ciency.
(see below.)

to say the same thing slightly di   erently, there   s just no way that we can
estimate the regression function as accurately where the noise is large as we
can where the noise is small. trying to give equal attention to all parts of the
input space is a waste of time; we should be more concerned about    tting well
where the noise is small, and expect to    t poorly where the noise is big.

3. sampling bias. in many situations, our data comes from a survey, and some
members of the population may be more likely to be included in the sample
than others. when this happens, the sample is a biased representation of the
population. if we want to draw id136s about the population, it can help
to give more weight to the kinds of data points which we   ve under-sampled,
and less to those which were over-sampled. in fact, typically the weight put
on data point i would be inversely proportional to the id203 of i being
included in the sample (exercise 10.1). strictly speaking, if we are willing to
believe that linear model is exactly correct, that there are no omitted variables,
and that the inclusion probabilities pi do not vary with yi, then this sort of
survey weighting is redundant (dumouchel and duncan, 1983). when those
assumptions are not met     when there   re non-linearities, omitted variables,
or    selection on the dependent variable        survey weighting is advisable, if
we know the inclusion probabilities fairly well.

the same trick works under the same conditions when we deal with    co-
variate shift   , a change in the distribution of x. if the old id203 density
function was p(x) and the new one is q(x), the weight we   d want to use is
wi = q(xi)/p(xi) (qui  nonero-candela et al., 2009). this can involve estimat-
ing both densities, or their ratio (chapter 14).

4. doing something else. there are a number of other optimization problems
which can be transformed into, or approximated by, weighted least squares.
the most important of these arises from generalized linear models, where the
mean response is some nonlinear function of a linear predictor; we will look at
them in chapters 11 and 12.

in the    rst case, we decide on the weights to re   ect our priorities. in the

208

weighting and variance

figure 10.1 black line: linear response function (y = 3     2x). grey curve:
standard deviation as a function of x (  (x) = 1 + x2/2). (code deliberately
omitted; can you reproduce this    gure?)

third case, the weights come from the optimization problem we   d really rather be
solving. what about the second case, of heteroskedasticity?

10.2 heteroskedasticity

suppose the noise variance is itself variable. for example, the    gure shows a
simple linear relationship between the input x and the response y , but also a
nonlinear relationship between x and v [y ].
in this particular case, the ordinary least squares estimate of the regression line
is 2.69        1.36x, with r reporting standard errors in the coe   cients of   0.71

   4   2024   15   10   505index010.2 heteroskedasticity

209

plot(x, y)
abline(a = 3, b = -2, col = "grey")
fit.ols = lm(y ~ x)
abline(fit.ols, lty = "dashed")

figure 10.2 scatter-plot of n = 100 data points from the above model.
(here x is gaussian with mean 0 and variance 9.) grey: true regression
line. dashed: ordinary least squares regression line.

and 0.24, respectively. those are however calculated under the assumption that
the noise is homoskedastic, which it isn   t. and in fact we can see, pretty much,
that there is heteroskedasticity     if looking at the scatter-plot didn   t convince
us, we could always plot the residuals against x, which we should do anyway.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505   20   100102030xy210

weighting and variance

par(mfrow = c(1, 2))
plot(x, residuals(fit.ols))
plot(x, (residuals(fit.ols))^2)
par(mfrow = c(1, 1))

figure 10.3 residuals (left) and squared residuals (right) of the ordinary
least squares regression as a function of x. note the much greater range of
the residuals at large absolute values of x than towards the center; this
changing dispersion is a sign of heteroskedasticity.

to see whether that makes a di   erence, let   s re-do this many times with dif-

ferent draws from the same model (example 24).

running ols.heterosked.error.stats(1e4) produces 104 random simulated
data sets, which all have the same x values as the    rst one, but di   erent values

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505   40   30   20   1001020xresiduals(fit.ols)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505050010001500x(residuals(fit.ols))^210.2 heteroskedasticity

211

ols.heterosked.example = function(n) {

y = 3 - 2 * x + rnorm(n, 0, sapply(x, function(x) {

1 + 0.5 * x^2

}))
fit.ols = lm(y ~ x)
return(fit.ols$coefficients - c(3, -2))

}
ols.heterosked.error.stats = function(n, m = 10000) {

ols.errors.raw = t(replicate(m, ols.heterosked.example(n)))
intercept.se = sd(ols.errors.raw[, "(intercept)"])
slope.se = sd(ols.errors.raw[, "x"])
return(c(intercept.se = intercept.se, slope.se = slope.se))

}

code example 24: functions to generate heteroskedastic data and    t ols regression to it, and
to collect error statistics on the results.

of y, generated however from the same model. it then uses those samples to
get the standard error of the ordinary least squares estimates. (bias remains a
non-issue.) what we    nd is the standard error of the intercept is only a little
in   ated (simulation value of 0.81 versus o   cial value of 0.71), but the standard
error of the slope is much larger than what r reports, 0.55 versus 0.24. since the
intercept is    xed by the need to make the regression line go through the center
of the data (chapter 2), the real issue here is that our estimate of the slope is
much less precise than ordinary least squares makes it out to be. our estimate is
still consistent, but not as good as it was when things were homoskedastic. can
we get back some of that e   ciency?

10.2.1 weighted least squares as a solution to heteroskedasticity

suppose we visit the oracle of regression (figure 10.4), who tells us that the
noise has a standard deviation that goes as 1 + x2/2. we can then use this to
improve our regression, by solving the weighted least squares problem rather than
ordinary least squares (figure 10.5).
this not only looks better, it is better: the estimated line is now 2.98     1.84x,
with reported standard errors of 0.3 and 0.18. this checks check out with sim-
ulation (example 25): the standard errors from the simulation are 0.23 for the
intercept and 0.26 for the slope, so r   s internal calculations are working very
well.

why does putting these weights into wls improve things?

10.2.2 some explanations for weighted least squares

qualitatively, the reason wls with inverse variance weights works is the follow-
ing. ols tries equally hard to match observations at each data point.1 weighted

1 less anthropomorphically, the objective function in eq. 10.1 has the same derivative with respect to

the squared error at each point,

   m se

   (yi   (cid:126)xi    )2 = 1
n .

212

weighting and variance

figure 10.4 statistician (right) consulting the oracle of regression (left)
about the proper weights to use to overcome heteroskedasticity. (image from
http://en.wikipedia.org/wiki/image:pythia1.jpg.)

wls.heterosked.example = function(n) {

y = 3 - 2 * x + rnorm(n, 0, sapply(x, function(x) {

1 + 0.5 * x^2

}))
fit.wls = lm(y ~ x, weights = 1/(1 + 0.5 * x^2))
return(fit.wls$coefficients - c(3, -2))

}
wls.heterosked.error.stats = function(n, m = 10000) {

wls.errors.raw = t(replicate(m, wls.heterosked.example(n)))
intercept.se = sd(wls.errors.raw[, "(intercept)"])
slope.se = sd(wls.errors.raw[, "x"])
return(c(intercept.se = intercept.se, slope.se = slope.se))

}

code example 25: id75 of heteroskedastic data, using weighted least-squared re-
gression.

10.2 heteroskedasticity

213

plot(x, y)
abline(a = 3, b = -2, col = "grey")
fit.ols = lm(y ~ x)
abline(fit.ols, lty = "dashed")
fit.wls = lm(y ~ x, weights = 1/(1 + 0.5 * x^2))
abline(fit.wls, lty = "dotted")

figure 10.5 figure 10.2, plus the weighted least squares regression line
(dotted).

least squares, naturally enough, tries harder to match observations where the
weights are big, and less hard to match them where the weights are small. but
each yi contains not only the true regression function   (xi) but also some noise
 i. the noise terms have large magnitudes where the variance is large. so we
should want to have small weights where the noise variance is large, because
there the data tends to be far from the true regression. conversely, we should
put big weights where the noise variance is small, and the data points are close
to the true regression.
the qualitative reasoning in the last paragraph doesn   t explain why the weights
should be inversely proportional to the variances, wi     1/  2
    why not wi    
1/  xi, for instance? seeing why those are the right weights requires investigating
how well di   erent, indeed arbitrary, choices of weights would work.

xi

look at the equation for the wls estimates again:

(cid:98)  w ls = (xt wx)   1xt wy

(10.5)
(10.6)
de   ning the matrix h(w) = (xt wx)   1xt w for brevity. (the notation reminds us
that everything depends on the weights in w.) imagine holding x constant, but
repeating the experiment multiple times, so that we get noisy values of y. in each

= h(w)y

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505   20   100102030xy214
experiment, yi = (cid:126)xi       +  i, where e [ i] = 0 and v [ i] =   2

weighting and variance

xi

(cid:98)  w ls = h(w)x   + h(w) 

=    + h(w) 
since e [ ] = 0, the wls estimator is unbiased:

e(cid:104)(cid:98)  w ls

(cid:105)

=   

in fact, for the jth coe   cient,(cid:98)  j =   j + [h(w) ]j

n(cid:88)

i=1

=   j +

hji(w) i

. so

(10.7)
(10.8)

(10.9)

(10.10)

(10.11)

since the wls estimate is unbiased, it   s natural to want it to also have a small
variance, and

=

hji(w)  2
xi

(10.12)

i=1

it can be shown     the result is called the gauss-markov theorem     that
picking weights to minimize the variance in the wls estimate has the unique
solution wi = 1/  2
. it does not require us to assume the noise is gaussian2, but
the proof is a bit tricky, so i will con   ne it to   10.2.2.1 below.
xi

a less general but easier-to-grasp result comes from adding the assumption

that the noise around the regression line is gaussian     that

(cid:105)

v(cid:104)(cid:98)  j

n(cid:88)

y = (cid:126)x       +  ,       n (0,   2
x)

the log-likelihood is then (exercise 10.2)

    n
2

ln 2       1
2

log   2
xi

    1
2

n(cid:88)

i=1

(yi     (cid:126)xi      )2

  2
xi

if we maximize this with respect to   , everything except the    nal sum is irrelevant,
and so we minimize

n(cid:88)

i=1

n(cid:88)

(yi     (cid:126)xi      )2

i=1

  2
xi

(10.13)

(10.14)

(10.15)

which is just weighted least squares with wi = 1/  2
xi
assumption holds, wls is the e   cient maximum likelihood estimator.

. so, if the probabilistic

2 despite the    rst part of the name! gauss himself was much less committed to assuming gaussian

distributions than many later statisticians.

10.2 heteroskedasticity

215

10.2.2.1 proof of the gauss-markov theorem3

we want to prove that, when we are doing weighted least squares for linear
regression, the best choice of weights wi = 1/  2
. we saw that that wls is
xi
unbiased (eq. 10.9), so    best    here means minimizing the variance. we have also
already seen (eq. 10.6) that

(cid:98)  w ls = h(w)y

where the matrix h(w) is

h(w) = (xt wx)   1xt w

(10.16)

(10.17)

it would be natural to try to write out the variance as a function of the weights
w, set the derivative equal to zero, and solve. this is tricky, partly because we
need to make sure that all the weights are positive and add up to one, but mostly
because of the matrix inversion in the de   nition of h(w). a slightly less direct
approach is actually much cleaner.

write w0 for the inverse-variance weight matrix, and h0 for the hat matrix we
get with those weights. then for any other choice of weights, we have h(w) =
h0 + c. (c is implicitly a function of the weights, but let   s suppress that in the
notation for brevity.) since we know all wls estimates are unbiased, we must
have

(h0 + c)x   =   

(10.18)

but using the inverse-variance weights is a particular wls estimate so

h0x   =   

(10.19)

and so we can deduce that

cx = 0

(10.20)

now consider the covariance matrix of the estimates, v(cid:104)     
(cid:105)

from unbiasedness.

. this will be v [(h0 + c)y],

3 you can skip this section, without loss of continuity.

216

weighting and variance

which we can expand:

v(cid:104)     
(cid:105)

= v [(h0 + c)y]
= (h0 + c)v [y ] (h0 + c)t
   1(h0 + c)t
= (h0 + c)w0
   1h0
   1h0
t + cw0
= h0w0
= (xt w0x)   1xt w0w0
   1w0x(xt w0x)   1

t + h0w0

   1ct + cw0

   1w0x(xt w0x)   1

+cw0
+(xt w0x)   1xt w0w0
+cw0

   1ct

   1ct

= (xt w0x)   1xt w0x(xt w0x)   1

+cx(xt w0x)   1 + (xt w0x)   1xt ct
+cw0

   1ct

= (xt w0x)   1 + cw0

   1ct

   1ct

(10.21)

(10.22)
(10.23)
(10.24)
(10.25)

(10.26)

(10.27)

where in the last step we use the fact that cx = 0 (and so xt ct = 0t = 0). since
   1ct     0, because w0 is a positive-de   nite matrix, we see that the variance
cw0
is minimized by setting c = 0, and using the inverse variance weights.

notes:

1. if all the variances are equal, then we   ve proved the optimality of ols.
2. the proof actually works when comparing the inverse-variance weights to any
other linear, unbiased estimator; wls with di   erent weights is just a special
case.
3. we can write the wls problem as that of minimizing (y     x  )t w(y     x  ).
if we allow w to be a non-diagonal, but still positive-de   nite, matrix, then we
have the generalized least squares problem. this is appropriate when there
are correlations between the noise terms at di   erent observations, i.e., when
cov [ i,  j] (cid:54)= 0 even though i (cid:54)= j. in this case, the proof is easily adapted to
show that the optimal weight matrix w is the inverse of the noise covariance
matrix. (this is why i wrote everything as a function of w.)

10.2.3 finding the variance and weights

all of this was possible because the oracle told us what the variance function
was. what do we do when the oracle is not available (figure 10.6)?

sometimes we can work things out for ourselves, without needing an oracle.

    we know, empirically, the precision of our measurement of the response variable
    we know how precise our instruments are, or the response is really an average
of several measurements with known standard deviations, etc.
    we know how the noise in the response must depend on the input variables.
for example, when taking polls or surveys, the variance of the proportions we

10.3 estimating conditional variance functions

217

figure 10.6 the oracle may be out (left), or too creepy to go visit (right).
what then? (left, the sacred oak of the oracle of dodona, copyright 2006
by flickr user    essayen   ,
http://flickr.com/photos/essayen/245236125/; right, the entrace to the
cave of the sibyl of cum  , copyright 2005 by flickr user    pverdicchio   ,
http://flickr.com/photos/occhio/17923096/. both used under creative
commons license.) [[attn: both are only licensed for non-commercial use,
so    nd substitutes or obtain rights for the for-money version of the book]]

   nd should be inversely proportional to the sample size. so we can make the
weights proportional to the sample size.

both of these outs rely on kinds of background knowledge which are easier to
get in the natural or even the social sciences than in many industrial applications.
however, there are approaches for other situations which try to use the observed
residuals to get estimates of the heteroskedasticity; this is the topic of the next
section.

remember that there are two equivalent ways of de   ning the variance:

10.3 estimating conditional variance functions

v [x] = e(cid:2)x 2(cid:3)     (e [x])2 = e(cid:2)(x     e [x])2(cid:3)

(10.28)

the latter is more useful for us when it comes to estimating variance functions. we
have already    gured out how to estimate means     that   s what all this previous
work on smoothing and regression is for     and the deviation of a random variable
from its mean shows up as a residual.

there are two generic ways to estimate conditional variances, which di   er
slightly in how they use non-parametric smoothing. we can call these the squared
residuals method and the log squared residuals method. here is how the
   rst one goes.

1. estimate   (x) with your favorite regression method, getting (cid:98)  (x).
2. construct the squared residuals, ui = (yi    (cid:98)  (xi))2.

218

weighting and variance

x =(cid:98)q(x).

the log-squared residuals method goes very similarly.

3. use your favorite non-parametric method to estimate the conditional mean of

the ui, call it (cid:98)q(x).
4. predict the variance using (cid:98)  2
1. estimate   (x) with your favorite regression method, getting (cid:98)  (x).
2. construct the log squared residuals, zi = log (yi    (cid:98)  (xi))2.
4. predict the variance using (cid:98)  2
the quantity yi    (cid:98)  (xi) is the ith residual. if (cid:98)         , then the residuals should

3. use your favorite non-parametric method to estimate the conditional mean of

x = exp(cid:98)s(x).

the zi, call it   s(x).

have mean zero. consequently the variance of the residuals (which is what we
want) should equal the expected squared residual. so squaring the residuals makes
sense, and the    rst method just smoothes these values to get at their expectations.
what about the second method     why the log? basically, this is a conve-
nience     squares are necessarily non-negative numbers, but lots of regression
methods don   t easily include constraints like that, and we really don   t want to
predict negative variances.4 taking the log gives us an unbounded range for the
regression.

strictly speaking, we don   t need to use non-parametric smoothing for either
method. if we had a parametric model for   2
x, we could just    t the parametric
model to the squared residuals (or their logs). but even if you think you know
what the variance function should look like it, why not check it?

we came to estimating the variance function because of wanting to do weighted
least squares, but these methods can be used more generally. it   s often important
to understand variance in its own right, and this is a general method for esti-
mating it. our estimate of the variance function depends on    rst having a good
estimate of the regression function

the estimate(cid:98)  2

10.3.1 iterative re   nement of mean and variance: an example

x depends on the initial estimate of the regression function(cid:98)  . but,

as we saw when we looked at weighted least squares, taking heteroskedasticity
into account can change our estimates of the regression function. this suggests an
iterative approach, where we alternate between estimating the regression function
and the variance function, using each to improve the other. that is, we take either
x, we

method above, and then, once we have estimated the variance function (cid:98)  2
re-estimate(cid:98)   using weighted least squares, with weights inversely proportional to

our estimated variance. since this will generally change our estimated regression,
it will change the residuals as well. once the residuals have changed, we should
re-estimate the variance function. we keep going around this cycle until the

4 occasionally people do things like claiming that gene di   erences explains more than 100% of the

variance in some psychological trait, and so environment and up-bringing contribute negative
variance. some of them     like alford et al. (2005)     say this with a straight face.

10.3 estimating conditional variance functions

219

change in the regression function becomes so small that we don   t care about
further modi   cations. it   s hard to give a strict guarantee, but usually this sort of
iterative improvement will converge.

let   s apply this idea to our example. figure 10.3b already plotted the residuals
from ols. figure 10.7 shows those squared residuals again, along with the true
variance function and the estimated variance function.

the ols estimate of the regression line is not especially good ((cid:98)  0 = 2.69
versus   0 = 3, (cid:98)  1 =    1.36 versus   1 =    2), so the residuals are systematically

o   , but it   s clear from the    gure that kernel smoothing of the squared residuals
is picking up on the heteroskedasticity, and getting a pretty reasonable picture
of the variance function.

now we use the estimated variance function to re-estimate the regression line,

with weighted least squares.

fit.wls1 <- lm(y ~ x, weights = 1/fitted(var1))
coefficients(fit.wls1)
## (intercept)
##
2.978753
var2 <- npreg(residuals(fit.wls1)^2 ~ x)

x
-1.905204

the slope has changed substantially, and in the right direction (figure 10.8a).
the residuals have also changed (figure 10.8b), and the new variance function is
closer to the truth than the old one.

since we have a new variance function, we can re-weight the data points and

re-estimate the regression:

fit.wls2 <- lm(y ~ x, weights = 1/fitted(var2))
coefficients(fit.wls2)
## (intercept)
##
2.990366
var3 <- npreg(residuals(fit.wls2)^2 ~ x)

x
-1.928978

since we know that the true coe   cients are 3 and    2, we know that this is
moving in the right direction. if i hadn   t told you what they were, you could
still observe that the di   erence in coe   cients between fit.wls1 and fit.wls2
is smaller than that between fit.ols and fit.wls1, which is a sign that this is
converging.

i will spare you the plot of the new regression and of the new residuals. let   s

iterate a few more times:

x
-1.929818

fit.wls3 <- lm(y ~ x, weights = 1/fitted(var3))
coefficients(fit.wls3)
## (intercept)
##
2.990687
var4 <- npreg(residuals(fit.wls3)^2 ~ x)
fit.wls4 <- lm(y ~ x, weights = 1/fitted(var4))
coefficients(fit.wls4)
## (intercept)
##
2.990697

x
-1.929848

by now, the coe   cients of the regression are changing in the fourth signi   cant

220

weighting and variance

plot(x, residuals(fit.ols)^2, ylab = "squared residuals")
curve((1 + x^2/2)^2, col = "grey", add = true)
require(np)
var1 <- npreg(residuals(fit.ols)^2 ~ x)
grid.x <- seq(from = min(x), to = max(x), length.out = 300)
lines(grid.x, predict(var1, exdat = grid.x))

figure 10.7 points: actual squared residuals from the ols line. grey
curve: true variance function,   2
smoothing of the squared residuals, using npreg.

x = (1 + x2/2)2. black curve: kernel

digit, and we only have 100 data points, so the imprecision from a limited sample
surely swamps the changes we   re making, and we might as well stop.

manually going back and forth between estimating the regression function and

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505050010001500xsquared residuals10.3 estimating conditional variance functions

221

figure 10.8 left: as in figure 10.2, but with the addition of the weighted
least squares regression line (dotted), using the estimated variance from
figure 10.7 for weights. right: as in figure 10.7, but with the addition of
the residuals from the wls regression (black squares), and the new
estimated variance function (dotted curve).

estimating the variance function is tedious. we could automate it with a function,
which would look something like this:

iterative.wls <- function(x, y, tol = 0.01, max.iter = 100) {

iteration <- 1
old.coefs <- na
regression <- lm(y ~ x)
coefs <- coefficients(regression)
while (is.na(old.coefs) || ((max(coefs - old.coefs) > tol) && (iteration <

max.iter))) {

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505   20   100102030xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   505050010001500xsquared residuals222

weighting and variance

variance <- npreg(residuals(regression)^2 ~ x)
old.coefs <- coefs
iteration <- iteration + 1
regression <- lm(y ~ x, weights = 1/fitted(variance))
coefs <- coefficients(regression)

}
return(list(regression = regression, variance = variance, iterations = iteration))

}

this starts by doing an unweighted id75, and then alternates be-
tween wls for the getting the regression and kernel smoothing for getting the
variance. it stops when no parameter of the regression changes by more than tol,
or when it   s gone around the cycle max.iter times.5 this code is a bit too in   ex-
ible to be really    industrial strength    (what if we wanted to use a data frame, or
a more complex regression formula?), but shows the core idea.

10.3.2 real data example: old heteroskedastic

  5.4.2 introduced the geyser data set, which is about predicting the waiting
time between consecutive eruptions of the    old faithful    geyser at yellowstone
national park from the duration of the latest eruption. our exploration there
showed that a simple linear model (of the kind often    t to this data in textbooks
and elementary classes) is not very good, and raised the suspicion that one im-
portant problem was heteroskedasticity. let   s follow up on that, building on the
computational work done in that section.

the estimated variance function geyser.var does not look particularly    at,
but it comes from applying a fairly complicated procedure (kernel smoothing
with data-driven bandwidth selection) to a fairly limited amount of data (299
observations). maybe that   s the amount of wiggliness we should expect to see due
to    nite-sample    uctuations? to rule this out, we can make surrogate data from
the homoskedastic model, treat it the same way as the real data, and plot the
resulting variance functions (figure 10.10). the conditional variance functions
estimated from the homoskedastic model are    at or gently varying, with much
less range than what   s seen in the data.

while that sort of qualitative comparison is genuinely informative, one can also
be more quantitative. one might measure heteroskedasticity by, say, evaluating
the conditional variance at all the data points, and looking at the ratio of the in-
terquartile range to the median. this would be zero for perfect homoskedasticity,
and grow as the dispersion of actual variances around the    typical    variance in-
creased. for the data, this is iqr(fitted(geyser.var))/median(fitted(geyser.var))
= 0.86. simulations from the ols model give values around 10   15.

there is nothing particularly special about this measure of heteroskedasticity
    after all, i just made it up. the broad point it illustrates is the one made in
  5.4.2.1: whenever we have some sort of quantitative summary statistic we can

5 the condition in the while loop is a bit complicated, to ensure that the loop is executed at least

once. some languages have an until control structure which would simplify this.

10.3 estimating conditional variance functions

223

library(mass)
data(geyser)
geyser.ols <- lm(waiting ~ duration, data = geyser)
plot(geyser$duration, residuals(geyser.ols)^2, cex = 0.5, pch = 16, xlab = "duration (minutes)",

ylab = expression(`squared residuals of linear model `(minutes^2)))

geyser.var <- npreg(residuals(geyser.ols)^2 ~ geyser$duration)
duration.order <- order(geyser$duration)
lines(geyser$duration[duration.order], fitted(geyser.var)[duration.order])
abline(h = summary(geyser.ols)$sigma^2, lty = "dashed")
legend("topleft", legend = c("data", "kernel variance", "homoskedastic (ols)"),

lty = c("blank", "solid", "dashed"), pch = c(16, na, na))

figure 10.9 squared residuals from the linear model of figure 5.1, plotted
against duration, along with the unconditional, homoskedastic variance
implicit in ols (dashed), and a kernel-regression estimate of the conditional
variance (solid).

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123450200400600800duration (minutes)squared residuals of linear model (minutes2)ldatakernel variancehomoskedastic (ols)224

weighting and variance

duration.grid <- seq(from = min(geyser$duration), to = max(geyser$duration),

length.out = 300)

plot(duration.grid, predict(geyser.var, exdat = duration.grid), ylim = c(0,

300), type = "l", xlab = "duration (minutes)", ylab = expression(`squared residuals of linear model `(minutes^2)))

abline(h = summary(geyser.ols)$sigma^2, lty = "dashed")
one.var.func <- function() {

fit <- lm(waiting ~ duration, data = rgeyser())
var.func <- npreg(residuals(fit)^2 ~ geyser$duration)
lines(duration.grid, predict(var.func, exdat = duration.grid), col = "grey")

}
invisible(replicate(30, one.var.func()))

figure 10.10 the actual conditional variance function estimated from the
old faithful data (and the id75), in black, plus the results of
applying the same procedure to simulations from the homoskedastic linear
regression model (grey lines; see   5.4.2 for the rgeyser function). the fact
that the estimates from the simulations are mostly    at or gently sloped
suggests that the changes in variance found in the data are likely too large
to just be sampling noise.

12345050100150200250300duration (minutes)squared residuals of linear model (minutes2)10.4 re-sampling residuals with heteroskedasticity

225

calculate on our real data, we can also calculate the same statistic on realizations
of the model, and the di   erence will then tell us something about how close the
simulations, and so the model, come to the data. in this case, we learn that the
linear, homoskedastic model seriously understates the variability of this data.
that leaves open the question of whether the problem is the linearity or the
homoskedasticity; i will leave that question to exercise 10.6.

10.4 re-sampling residuals with heteroskedasticity

re-sampling the residuals of a regression, as described in   6.4, assumes that the
distribution of    uctuations around the regression curve is the same for all values of
the input x. under heteroskedasticity, this is of course not the case. nonetheless,
we can still re-sample residuals to get bootstrap con   dence intervals, standard
errors, and so forth, provided we de   ne and scale them properly. if we have a
conditional variance function     2(x), as well as the estimated regression function

(cid:98)  (x), we can combine them to re-sample heteroskedastic residuals.

1. construct the standardized residuals, by dividing the actual residuals by the

conditional standard deviation:

  i =  i/    (xi)

(10.29)

the   i should now be all the same magnitude (in distribution!), no matter
where xi is in the space of predictors.

2. re-sample the   i with replacement, to get     1, . . .     n.
3. set   xi = xi.

4. set   yi =(cid:98)  (  xi) +     (  xi)    i.

5. analyze the surrogate data (  x1,   y1), . . . (  xn,   yn) like it was real data.

of course, this still assumes that the only di   erence in distribution for the noise

at di   erent values of x is the scale.

10.5 local id75

switching gears, recall from chapter 2 that one reason it can be sensible to use
a linear approximation to the true regression function    is that we can typically
taylor-expand (app. refapp:taylor) the latter around any point x0,

   (cid:88)

(x     x0)k

k!

k=1

(cid:12)(cid:12)(cid:12)(cid:12)x=x0

dk  
dxk

  (x) =   (x0) +

(10.30)

and similarly with all the partial derivatives in higher dimensions. truncating
the series at    rst order,   (x)       (x0) + (x     x0)  (cid:48)(x0), we see the    rst derivative
  (cid:48)(x0) is the best linear prediction coe   cient, at least if x close enough to x0. the
snag in this line of argument is that if   (x) is nonlinear, then   (cid:48) isn   t a constant,
and the optimal linear predictor changes depending on where we want to make
predictions.

226

weighting and variance

however, statisticians are thrifty people, and having assembled all the ma-
chinery for id75, they are loathe to throw it away just because the
fundamental model is wrong. if we can   t    t one line, why not    t many? if each
point has a di   erent best id75, why not estimate them all? thus
the idea of local id75:    t a di   erent id75 everywhere,
weighting the data points by how close they are to the point of interest6.

the simplest approach we could take would be to divide up the range of x into
so many bins, and    t a separate id75 for each bin. this has at least
three drawbacks. first, we get weird discontinuities at the boundaries between
bins. second, we pick up an odd sort of bias, where our predictions near the
boundaries of a bin depend strongly on data from one side of the bin, and not at
all on nearby data points just across the border, which is weird. third, we need
to pick the bins.

the next simplest approach would be to    rst    gure out where we want to make
a prediction (say x), and do a id75 with all the data points which
were su   ciently close, |xi     x|     h for some h. now we are basically using a
uniform-density kernel to weight the data points. this eliminates two problems
from the binning idea     the examples we include are always centered on the x
we   re trying to get a prediction for, and we just need to pick one bandwidth h
rather than placing all the bin boundaries. but still, each example point always
has either weight 0 or weight 1, so our predictions change jerkily as training
points fall into or out of the window. it generally works nicer to have the weights
change more smoothly with the distance, starting o    large and then gradually
trailing to zero.

by now bells may be going o   , as this sounds very similar to the kernel regres-
sion. in fact, kernel regression is what happens when we truncate eq. 10.30 at
zeroth order, getting locally constant regression. we set up the problem

n(cid:88)

i=1

1
n

(cid:98)  (x) = argmin
n(cid:88)
(cid:98)  (x) =

m

yi

i=1

wi(x)(yi     m)2
(cid:80)n

wi(x)
j=1 wj(x)

(10.31)

(10.32)

and get the solution

which just is our kernel regression, when the weights are proportional to the
kernels, wi(x)     k(xi, x). (without loss of generality, we can take the constant
of proportionality to be 1.)

what about locally id75? the optimization problem is

= argmin

(10.33)
where again we can make wi(x) proportional to some id81, wi(x)    
k(xi, x). to solve this, abuse notation slightly to de   ne zi = (1, xi     x), i.e., the

m,  

i=1

wi(x)(yi     m     (xi     x)      )2

1
n

(cid:16)(cid:98)  (x),(cid:98)  (x)
(cid:17)

n(cid:88)

6 some people say    local linear    and some    locally linear   .

10.5 local id75

227

curve((1 - abs(x)^3)^3, from = -1, to = 1, ylab = "tricubic function")

figure 10.11 the tricubic kernel, with broad plateau where |x|     0, and
the smooth fall-o    to zero at |x| = 1.

(cid:16)(cid:98)  (x),(cid:98)  (x)

(cid:17)

displacement from x, with a 1 stuck at the beginning to (as usual) handle the
intercept. now, by the machinery above,

= (zt w(x)z)   1zt w(x)y

and the prediction is just the intercept, (cid:98)  (x). if you need an estimate of the    rst
derivatives, those are the (cid:98)  (x). eq. 10.34 guarantees that the weights given to

each training point change smoothly with x, so the predictions will also change
smoothly.7

(10.34)

using a smooth kernel whose density is positive everywhere, like the gaussian,
ensures that the weights will change smoothly. but we could also use a kernel
which goes to zero outside some    nite range, so long as the kernel rises gradually
from zero inside the range. for locally id75, a common choice of kernel
is therefore the tri-cubic,

(cid:32)

(cid:18)|xi     x0|

(cid:19)3(cid:33)3

(10.35)

k(xi, x) =

1    

h
if |x     xi| < h, and = 0 otherwise (figure 10.11).

7 notice that local linear predictors are still linear smoothers as de   ned in chapter 1, (i.e., the

predictions are linear in the yi), but they are not, strictly speaking, kernel smoothers, since you
can   t re-write the last equation in the form of a kernel average.

   1.0   0.50.00.51.00.00.20.40.60.81.0xtricubic function228

weighting and variance

10.5.1 for and against locally id75

why would we use locally id75, if we already have kernel regression?

1. you may recall that when we worked out the bias of kernel smoothers (eq.
4.10 in chapter 4), we got a contribution that was proportional to   (cid:48)(x). if
we do an analogous analysis for locally id75, the bias is the same,
except that this derivative term goes away.

2. relatedly, that analysis we did of kernel regression tacitly assumed the point
we were looking at was in the middle of the training data (or at least rather
more than h from the border). the bias gets worse near the edges of the
training data. suppose that the true   (x) is decreasing in the vicinity of the
largest xi. (see the grey curve in figure 10.12.) when we make our predictions
there, in kernel regression we can only average values of yi which tend to be
systematically larger than the value we want to predict. this means that our
kernel predictions are systematically biased upwards, and the size of the bias
grows with   (cid:48)(x). (see the black line in figure 10.12 at the lower right.) if we
use a locally linear model, however, it can pick up that there is a trend, and
reduce the edge bias by extrapolating it (dashed line in the    gure).

3. the predictions of locally id75 tend to be smoother than those of
kernel regression, simply because we are locally    tting a smooth line rather
dx tend to

than a    at constant. as a consequence, estimates of the derivative d(cid:98)  
be less noisy when(cid:98)   comes from a locally linear model than a kernel regression.

of course, total prediction error depends not only on the bias but also on the
variance. remarkably enough, the variance for kernel regression and locally linear
regression is the same, at least asymptotically. since locally id75 has
smaller bias, local-linear    ts are often better predictors.

despite all these advantages, local linear models have a real drawback. to make
a prediction with a kernel smoother, we have to calculate a weighted average. to
make a prediction with a local linear model, we have to solve a (weighted) linear
least squares problem for each point, or each prediction. this takes much more
computing time8.

there are several packages which implement locally id75. since
we are already using np, one of the simplest is to set the regtype="ll" in

8 let   s think this through. to    nd(cid:98)  (x) with a kernel smoother, we need to calculate k(xi, x) for each

xi. if we   ve got p predictor variables and use a product kernel, that takes o(pn) computational
steps. we then need to add up the kernels to get the denominator, which we could certainly do in
o(n) more steps. (could you do it faster?) multiplying each weight by its yi is a further o(n), and
the    nal adding up is at most o(n); total, o(pn). to make a prediction with a local linear model,
we need to calculate the right-hand side of eq. 10.34. finding (zt w(x)z) means multiplying
[(p + 1)    n][n    n][n    (p + 1)] matrices, which will take o((p + 1)2n) = o(p2n) steps. inverting a
q    q matrix takes o(q3) steps, so our inversion takes o((p + 1)3) = o(p3) steps. just getting
(zt w(x)z)   1 thus requires o(p3 + p2n). finding the (p + 1)    1 matrix zt w(x)y similarly takes
o((p + 1)n) = o(pn) steps, and the    nal id127 is o((p + 1)(p + 1)) = o(p2). total,
o(p2n) + o(p3). the speed advantage of kernel smoothing thus gets increasingly extreme as the
number of predictor variables p grows.

10.5 local id75

229

x <- runif(30, max = 3)
y <- 9 - x^2 + rnorm(30, sd = 0.1)
plot(x, y)
rug(x, side = 1, col = "grey")
rug(y, side = 2, col = "grey")
curve(9 - x^2, col = "grey", add = true, lwd = 3)
grid.x <- seq(from = 0, to = 3, length.out = 300)
np0 <- npreg(y ~ x)
lines(grid.x, predict(np0, exdat = grid.x))
np1 <- npreg(y ~ x, regtype = "ll")
lines(grid.x, predict(np1, exdat = grid.x), lty = "dashed")

figure 10.12 points are samples from the true, nonid75
function shown in grey. the solid black line is a kernel regression, and the
dashed line is a locally id75. note that the locally linear model is
smoother than the kernel regression, and less biased when the true curve has
a non-zero bias at a boundary of the data (far right).

llllllllllllllllllllllllllllll0.00.51.01.52.02.52468xy230

weighting and variance

npreg.9 there are several other packages which support it, notably kernsmooth
and locpoly.

as the name of the latter suggests, there is no reason we have to stop at
locally linear models, and we could use local polynomials of any order. the main
reason to use a higher-order local polynomial, rather than a locally-linear or
locally-constant model, is to estimate higher derivatives. since this is a somewhat
specialized topic, i will not say more about it.

10.5.2 lowess

there is however one additional topic in locally linear models which is worth
mentioning. this is the variant called lowess or loess.10 the basic idea is to    t
a locally linear model, with a kernel which goes to zero outside a    nite window
and rises gradually inside it, typically the tri-cubic i plotted earlier. the wrin-
kle, however, is that rather than solving a least squares problem, it minimizes a
di   erent and more    robust    id168,

wi(x)(cid:96)(y     (cid:126)xi      (x))

(10.36)

n(cid:88)

i=1

argmin

  (x)

1
n

where (cid:96)(a) doesn   t grow as rapidly for large a as a2. the idea is to make the    tting
less vulnerable to occasional large outliers, which would have very large squared
errors, unless the regression curve went far out of its way to accommodate them.
for instance, we might have (cid:96)(a) = a2 if |a| < 1, and (cid:96)(a) = 2|a|     1 otherwise11.
there is a large theory of robust estimation, largely parallel to the more familiar
least-squares theory. in the interest of space, we won   t pursue it further, but
lowess is worth mentioning because it   s such a common smoothing technique,
especially for sheer visualization.

lowess smoothing is implemented in base r through the function lowess
(rather basic), and through the function loess (more sophisticated), as well as
in the cran package locfit (more sophisticated still). the lowess idea can be
combined with local    tting of higher-order polynomials; the loess and locfit
commands both support this.

10.6 further reading

weighted least squares goes back to the 19th century, almost as far back as
ordinary least squares; see the references in chapter 1 and 2.

i am not sure who invented the use of smoothing to estimate variance functions
comes from; i learned it from i learned it from wasserman (2006, pp. 87   88). i   ve

9 "ll" stands for    locally linear   , of course; the default is regtype="lc", for    locally constant   .
10 i have heard this name explained as an acronym for both    locally weighted scatterplot smoothing   

and    locally weight sum of squares   .

11 this is called the huber loss; it continuously interpolates between looking like squared error and

looking like absolute error. this means that when errors are small, it gives results very like
least-squares, but it is resistant to outliers. see also app. l.6.1.

exercises

231

occasionally seen it done with a linear model for the conditional variance; i don   t
recommend that.

simono    (1996) is a good reference on local linear and local polynomial models,
including actually doing the bias-variance analyses where i   ve just made empty
   it can be shown    promises. fan and gijbels (1996) is more comprehensive, but
also a much harder read. lowess was introduced by cleveland (1979), but the
name evidently came later (since it doesn   t appear in that paper).

exercises

so y = n   1
being included in our sample with a id203 proportional to   i.

(cid:80)n
10.1 imagine we are trying to estimate the mean value of y from a large population of size n0,
j=1 yj . we observe n (cid:28) n0 members of the population, with individual i
1. show that(cid:0)(cid:80)n
2. is the unweighted sample mean n   1(cid:80)n

i(cid:48)=1 1/  i(cid:48) is a consistent estimator of y, by showing that

that it is unbiased and it has a variance that shrinks with n towards 0.

i=1 yi a consistent estimator of y when the   i

(cid:1) /(cid:80)n

i=1 yi/  i

0

are not all equal?

the same minimum?

10.2 show that the model of eq. 10.13 has the log-likelihood given by eq. 10.14
10.3 do the calculus to verify eq. 10.4.
10.4 is wi = 1 a necessary as well as a su   cient condition for eq. 10.3 and eq. 10.1 to have
10.5   10.2.2 showed that wls gives better parameter estimates than ols when there is het-
eroskedasticity, and we know and use the variance. modify the code for to see which one
has better generalization error.

10.6   10.3.2 looked at the residuals of the id75 model for the old faithful geyser
data, and showed that they would imply lots of heteroskedasticity. this might, however, be
an artifact of inappropriately using a linear model. use either kernel regression (cf.   6.4.2)
or local id75 to estimate the conditional mean of waiting given duration, and
see whether the apparent heteroskedasticity goes away.

10.7 should local id75 do better or worse than ordinary least squares under het-

eroskedasticity? what exactly would this mean, and how might you test your ideas?

11

id28

11.1 modeling conditional probabilities

so far, we either looked at estimating the conditional expectations of continu-
ous variables (as in regression), or at estimating distributions. there are many
situations where however we are interested in input-output relationships, as in
regression, but the output variable is discrete rather than continuous. in par-
ticular there are many situations where we have binary outcomes (it snows in
pittsburgh on a given day, or it doesn   t; this squirrel carries plague, or it doesn   t;
this loan will be paid back, or it won   t; this person will get heart disease in the
next    ve years, or they won   t). in addition to the binary outcome, we have some
input variables, which may or may not be continuous. how could we model and
analyze such data?

we could try to come up with a rule which guesses the binary output from
the input variables. this is called classi   cation, and is an important topic in
statistics and machine learning. however, guessing    yes    or    no    is pretty crude
    especially if there is no perfect rule. (why should there be a perfect rule?)
something which takes noise into account, and doesn   t just give a binary answer,
will often be useful. in short, we want probabilities     which means we need to
   t a stochastic model.
what would be nice, in fact, would be to have conditional distribution of the
response y , given the input variables, pr (y |x). this would tell us about how
precise our predictions should be. if our model says that there   s a 51% chance
of snow and it doesn   t snow, that   s better than if it had said there was a 99%
chance of snow (though even a 99% chance is not a sure thing). we will see,
in chapter 14, general approaches to estimating conditional probabilities non-
parametrically, which can use the kernels for discrete variables from chapter 4.
while there are a lot of merits to this approach, it does involve coming up with
a model for the joint distribution of outputs y and inputs x, which can be quite
time-consuming.

let   s pick one of the classes and call it    1    and the other    0   . (it doesn   t matter
which is which.) then y becomes an indicator variable, and you can convince
yourself that pr (y = 1) = e [y ]. similarly, pr (y = 1|x = x) = e [y |x = x]. (in
a phrase,    id155 is the conditional expectation of the indica-
tor   .) this helps us because by this point we know all about estimating condi-
tional expectations. the most straightforward thing for us to do at this point

232

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

11.2 id28

233

would be to pick out our favorite smoother and estimate the regression function
for the indicator variable; this will be an estimate of the id155
function.

there are two reasons not to just plunge ahead with that idea. one is that
probabilities must be between 0 and 1, but our smoothers will not necessarily
respect that, even if all the observed yi they get are either 0 or 1. the other is
that we might be better o    making more use of the fact that we are trying to
estimate probabilities, by more explicitly modeling the id203.
assume that pr (y = 1|x = x) = p(x;   ), for some function p parameterized
by   . parameterized function   , and further assume that observations are inde-
pendent of each other. the the (conditional) likelihood function is

n(cid:89)

n(cid:89)

pr (y = yi|x = xi) =

p(xi;   )yi(1     p(xi;   ))1   yi

(11.1)

i=1

i=1

recall that in a sequence of bernoulli trials y1, . . . yn, where there is a constant

id203 of success p, the likelihood is

pyi(1     p)1   yi

(11.2)

n   1(cid:80)n

comes

as you learned in basic statistics, this likelihood is maximized when p =   p =
i=1 yi. if each trial had its own success id203 pi, this likelihood be-

i (1     pi)1   yi
pyi

(11.3)

i=1

without some constraints, estimating the    inhomogeneous bernoulli    model by
maximum likelihood doesn   t work; we   d get   pi = 1 when yi = 1,   pi = 0 when
yi = 0, and learn nothing. if on the other hand we assume that the pi aren   t just
arbitrary numbers but are linked together, if we model the probabilities, those
constraints give non-trivial parameter estimates, and let us generalize. in the
kind of model we are talking about, the constraint, pi = p(xi;   ), tells us that
pi must be the same whenever xi is the same, and if p is a continuous function,
then similar values of xi must lead to similar values of pi. assuming p is known
(up to parameters), the likelihood is a function of   , and we can estimate    by
maximizing the likelihood. this chapter will be about this approach.

11.2 id28

to sum up: we have a binary output variable y , and we want to model the condi-
tional id203 pr (y = 1|x = x) as a function of x; any unknown parameters
in the function are to be estimated by maximum likelihood. by now, it will not
surprise you to learn that statisticians have approached this problem by asking
themselves    how can we use id75 to solve this?   

n(cid:89)

i=1

n(cid:89)

234

id28

1. the most obvious idea is to let p(x) be a linear function of x. every incre-
ment of a component of x would add or subtract so much to the id203.
the conceptual problem here is that p must be between 0 and 1, and lin-
ear functions are unbounded. moreover, in many situations we empirically see
   diminishing returns        changing p by the same amount requires a bigger
change in x when p is already large (or small) than when p is close to 1/2.
linear models can   t do this.

2. the next most obvious idea is to let log p(x) be a linear function of x, so
that changing an input variable multiplies the id203 by a    xed amount.
the problem is that logarithms of probabilities are unbounded in only one
direction, and linear functions are not.

3. finally, the easiest modi   cation of log p which has an unbounded range is
the logistic transformation (or logit) , log p
1   p . we can make this a linear
function of x without fear of nonsensical results. (of course the results could
still happen to be wrong, but they   re not guaranteed to be wrong.)

this last alternative is id28.

formally, the id28 model is that

log

p(x)
1     p(x)

=   0 + x      

solving for p, this gives

p(x;   ) =

e  0+x    

1 + e  0+x    

=

1

1 + e   (  0+x    )

(11.4)

(11.5)

notice that the overall speci   cation is a lot easier to grasp in terms of the trans-
formed id203 that in terms of the untransformed id203.1
to minimize the mis-classi   cation rate, we should predict y = 1 when p     0.5
and y = 0 when p < 0.5 (exercise 11.1). this means guessing 1 whenever   0+x    
is non-negative, and 0 otherwise. so id28 gives us a linear classi   er.
the decision boundary separating the two predicted classes is the solution of
  0+x     = 0, which is a point if x is one dimensional, a line if it is two dimensional,
etc. one can show (exercise!) that the distance from the decision boundary is
  0/(cid:107)  (cid:107) + x     /(cid:107)  (cid:107). id28 not only says where the boundary between
the classes is, but also says (via eq. 11.5) that the class probabilities depend on
distance from the boundary, in a particular way, and that they go towards the
extremes (0 and 1) more rapidly when (cid:107)  (cid:107) is larger. it   s these statements about
probabilities which make id28 more than just a classi   er. it makes
stronger, more detailed predictions, and can be    t in a di   erent way; but those
strong predictions could be wrong.

using id28 to predict class probabilities is a modeling choice, just
like it   s a modeling choice to predict quantitative variables with id75.
in neither case is the appropriateness of the model guaranteed by the gods, nature,

1 unless you   ve taken thermodynamics or physical chemistry, in which case you recognize that this is

the boltzmann distribution for a system with two states, which di   er in energy by   0 + x      .

11.2 id28

235

x <- matrix(runif(n = 50 * 2, min = -1, max = 1), ncol = 2)
par(mfrow = c(2, 2))
plot.logistic.sim(x, beta.0 = -0.1, beta = c(-0.2, 0.2))
y.1 <- plot.logistic.sim(x, beta.0 = -0.5, beta = c(-1, 1))
plot.logistic.sim(x, beta.0 = -2.5, beta = c(-5, 5))
plot.logistic.sim(x, beta.0 = -250, beta = c(-500, 500))

figure 11.1 e   ects of scaling id28 parameters. values of x1
and x2 are the same in all plots (    unif(   1, 1) for both coordinates), but
labels were generated randomly from id28s with   0 =    0.1,
   = (   0.2, 0.2) (top left); from   0 =    0.5,    = (   1, 1) (top right); from
  0 =    2.5,    = (   5, 5) (bottom left); and from   0 = 2.5    102,
   = (   5    102, 5    102). notice how as the parameters get increased in
constant ratio to each other, we approach a deterministic relation between y
and x, with a linear boundary between the classes. (we save one set of the
random binary responses for use later, as the imaginatively-named y.1.)

x1x2 0.4  0.42  0.44  0.46  0.48  0.5  0.52  0.54  0.56    1.0   0.50.00.51.0   1.0   0.50.00.51.0                  ++      +   +            +++++++   ++   +   +      +   +   ++      ++      +   +++x1x2 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8    1.0   0.50.00.51.0   1.0   0.50.00.51.0   ++++   +                     +         +         ++   ++   +      +   +      ++++      +   ++   +      x1x2 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9    1.0   0.50.00.51.0   1.0   0.50.00.51.0+      ++                           +   +   ++   +   +   +                                    +               ++            x1x2 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1    1.0   0.50.00.51.0   1.0   0.50.00.51.0+      ++                           +   +   ++   +         +                     +            +               ++            236

id28

sim.logistic <- function(x, beta.0, beta, bind = false) {

require(faraway)
linear.parts <- beta.0 + (x %*% beta)
y <- rbinom(nrow(x), size = 1, prob = ilogit(linear.parts))
if (bind) {

return(cbind(x, y))

}
else {

return(y)

}

}
plot.logistic.sim <- function(x, beta.0, beta, n.grid = 50, labcex = 0.3, col = "grey",

...) {
grid.seq <- seq(from = -1, to = 1, length.out = n.grid)
plot.grid <- as.matrix(expand.grid(grid.seq, grid.seq))
require(faraway)
p <- matrix(ilogit(beta.0 + (plot.grid %*% beta)), nrow = n.grid)
contour(x = grid.seq, y = grid.seq, z = p, xlab = expression(x[1]), ylab = expression(x[2]),

main = "", labcex = labcex, col = col)

y <- sim.logistic(x, beta.0, beta, bind = false)
points(x[, 1], x[, 2], pch = ifelse(y == 1, "+", "-"), col = ifelse(y ==

1, "blue", "red"))

invisible(y)

}

code example 26: code to simulate binary responses from a id28 model, and to
plot a 2d id28   s id203 contours and simulated binary values. (how would you
modify this to take the responses from a data frame?

mathematical necessity, etc. we begin by positing the model, to get something
to work with, and we end (if we know what we   re doing) by checking whether it
really does match the data, or whether it has systematic    aws.

id28 is one of the most commonly used tools for applied statistics

and discrete data analysis. there are basically four reasons for this.

1. tradition.
2. in addition to the heuristic approach above, the quantity log p/(1     p) plays
an important role in the analysis of contingency tables (the    log odds   ). clas-
si   cation is a bit like having a contingency table with two columns (classes)
and in   nitely many rows (values of x). with a    nite contingency table, we can
estimate the log-odds for each row empirically, by just taking counts in the
table. with in   nitely many rows, we need some sort of interpolation scheme;
id28 is linear interpolation for the log-odds.

3. it   s closely related to    exponential family    distributions, where the id203
of some vector v is proportional to exp
. if one of the
components of v is binary, and the functions fj are all the identity function,
then we get a id28. exponential families arise in many contexts
in statistical theory (and in physics!), so there are lots of problems which can
be turned into id28.

j=1 fj(v)  j

4. it often works surprisingly well as a classi   er. but, many simple techniques

(cid:110)
  0 +(cid:80)m

(cid:111)

11.3 numerical optimization of the likelihood

237

often work surprisingly well as classi   ers, and this doesn   t really testify to
id28 getting the probabilities right.

11.2.1 likelihood function for id28

because id28 predicts probabilities, rather than just classes, we can
   t it using likelihood. for each training data-point, we have a vector of features,
xi, and an observed class, yi. the id203 of that class was either p, if yi = 1,
or 1     p, if yi = 0. the likelihood is then

l(  0,   ) =

p(xi)yi(1     p(xi))1   yi

(11.6)

n(cid:89)

i=1

(i could substitute in the actual equation for p, but things will be clearer in a
moment if i don   t.) the log-likelihood turns products into sums:
yi log p(xi) + (1     yi) log (1     p(xi))

(cid:96)(  0,   ) =

(11.7)

i=1

n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

=

=

=

log (1     p(xi)) +

n(cid:88)
n(cid:88)
    log(cid:0)1 + e  0+xi    (cid:1) +

log (1     p(xi)) +

i=1

i=1

yi log

p(xi)
1     p(xi)
yi(  0 + xi      )
n(cid:88)

yi(  0 + xi      )

(11.8)

(11.9)

(11.10)

where in the next-to-last step we    nally use equation 11.4.

i=1

i=1

typically, to    nd the maximum likelihood estimates we   d di   erentiate the log
likelihood with respect to the parameters, set the derivatives equal to zero, and
solve. to start that, take the derivative with respect to one component of   , say
  j.

   (cid:96)
     j

=     n(cid:88)
n(cid:88)

i=1

=

n(cid:88)

i=1

1

1 + e  0+xi    

e  0+xi    xij +

(yi     p(xi;   0,   )) xij

yixij

(11.11)

(11.12)

i=1

we are not going to be able to set this to zero and solve exactly. (that   s a
transcendental equation, and there is no closed-form solution.) we can however
approximately solve it numerically.

11.3 numerical optimization of the likelihood

while our likelihood isn   t nice enough that we have an explicit expression for
the maximum (the way we do in ols or wls), it is a pretty well-behaved func-

238

id28

tion, and one which is amenable to lots of the usual numerical methods for op-
timization. in particular, like most log-likelihood functions, it   s suitable for an
application of newton   s method. brie   y (see appendix h.2 for details), new-
ton   s method starts with an initial guess about the optimal parameters, and then
calculates the gradient of the log-likelihood with respect to those parameters. it
then adds an amount proportional to the gradient to the parameters, moving up
the surface of the log-likelihood function. the size of the step in the gradient
direction is dictated by the second derivatives     it takes bigger steps when the
second derivatives are small (so the gradient is a good guide to what the function
looks like), and small steps when the curvature is large.

11.3.1 iteratively re-weighted least squares

this discussion of newton   s method is quite general, and therefore abstract. in
the particular case of id28, we can make everything look much more
like a good, old-fashioned id75 problem.

id28, after all, is a linear model for a transformation of the prob-

ability. let   s call this transformation g:

g(p)     log

p
1     p

(11.13)

so the model is

g(p) =   0 + x      

(11.14)
and y |x = x     binom(1, g   1(  0 + x      )). it seems that what we should want to
do is take g(y) and regress it linearly on x. of course, the variance of y , according
to the model, is going to change depending on x     it will be (g   1(  0 + x     ))(1   
g   1(  0 + x      ))     so we really ought to do a weighted id75, with
weights inversely proportional to that variance. since writing g   1(  0 + x      ) is
getting annoying, let   s abbreviate it by p(x) or just p, and let   s abbreviate that
variance as v (p).
the problem is that y is either 0 or 1, so g(y) is either        or +   . we will

evade this by using taylor expansion.

g(y)     g(p) + (y     p)g(cid:48)(p)     z

(11.15)

the right hand side, z will be our e   ective response variable, which we will regress
on x. to see why this should give us the right coe   cients, substitute for g(p) in
the de   nition of z,

z =   0 + x       + (y     p)g(cid:48)(p)

(11.16)
and notice that, if we   ve got the coe   cients right, e [y |x = x] = p, so (y     p)
should be mean-zero noise. in other words, when we have the right coe   cients,
z is a linear function of x plus mean-zero noise. (this is our excuse for throwing
away the rest of the taylor expansion, even though we know the discarded terms

11.4 generalized linear and additive models

239

are in   nitely large!) that noise doesn   t have constant variance, but we can work
it out,

v [z|x = x] = v [(y     p)g(cid:48)(p)|x = x] = (g(cid:48)(p))2v (p) ,

(11.17)

and so use that variance in weighted least squares to recover   .

notice that z and the weights both involve the parameters of our logistic re-
gression, through p(x). so having done this once, we should really use the new
parameters to update z and the weights, and do it again. eventually, we come
to a    xed point, where the parameter estimates no longer change. this loop    
start with a guess about the parameters, use it to calculate the zi and their
weights, regress on the xi to get new parameters, and repeat     is known as iter-
ative reweighted least squares (irls or irwls), iterative weighted least
squares (iwls), etc.

the treatment above is rather heuristic2, but it turns out to be equivalent
to using newton   s method, only with the expected second derivative of the log
likelihood, instead of its actual value. this takes a reasonable amount of algebra
to show, so we   ll skip it (but see exercise 11.3)3. since, with a large number
of observations, the observed second derivative should be close to the expected
second derivative, this is only a small approximation.

11.4 generalized linear and additive models

id28 is part of a broader family of generalized linear models
(glms), where the conditional distribution of the response falls in some para-
metric family, and the parameters are set by the linear predictor. ordinary, least-
squares regression is the case where response is gaussian, with mean equal to the
linear predictor, and constant variance. id28 is the case where the
response is binomial, with n equal to the number of data-points with the given
x (usually but not always 1), and p is given by equation 11.5. changing the
relationship between the parameters and the linear predictor is called changing
the link function. for computational reasons, the link function is actually the
function you apply to the mean response to get back the linear predictor, rather
than the other way around     (11.4) rather than (11.5). there are thus other
forms of binomial regression besides id28.4 there is also poisson re-

2 that is, mathematically incorrect.
3 the two key points are as follows. first, the gradient of the log-likelihood turns out to be the sum of
the zixi. (cf. eq. 11.12.) second, take a single bernoulli observation with success id203 p. the
log-likelihood is y log p + (1     y ) log 1     p. the    rst derivative with respect to p is
y /p     (1     y )/(1     p), and the second derivative is    y /p2     (1     y )/(1     p)2. taking expectations
of the second derivative gives    1/p     1/(1     p) =    1/p(1     p). in other words, v (p) =    1/e [(cid:96)(cid:48)(cid:48)].
using weights inversely proportional to the variance thus turns out to be equivalent to dividing by
the expected second derivative. but gradient divided by second derivative is the increment we use in
newton   s method, qed.

4 my experience is that these tend to give similar error rates as classi   ers, but have rather di   erent

guesses about the underlying probabilities.

240

id28

gression (appropriate when the data are counts without any upper limit), gamma
regression, etc.; we will say more about these in chapter 12.

in r, any standard glm can be    t using the (base) glm function, whose syntax
is very similar to that of lm. the major wrinkle is that, of course, you need to
specify the family of id203 distributions to use, by the family option    
family=binomial defaults to id28. (see help(glm) for the gory
details on how to do, say, probit regression.) all of these are    t by the same sort
of numerical likelihood maximization.

perfect classi   cation

one caution about using maximum likelihood to    t id28 is that it
can seem to work badly when the training data can be linearly separated. the
reason is that, to make the likelihood large, p(xi) should be large when yi = 1,
and p should be small when yi = 0. if   0,   0 is a set of parameters which perfectly
classi   es the training data, then c  0, c   is too, for any c > 1, but in a logistic
regression the second set of parameters will have more extreme probabilities, and
so a higher likelihood. for linearly separable data, then, there is no parameter
vector which maximizes the likelihood, since (cid:96) can always be increased by making
the vector larger but keeping it pointed in the same direction.
you should, of course, be so lucky as to have this problem.

11.4.1 generalized additive models

a natural step beyond generalized linear models is generalized additive mod-
els (gams), where instead of making the transformed mean response a linear
function of the inputs, we make it an additive function of the inputs. this means
combining a function for    tting additive models with likelihood maximization.
this is actually done in r with the same gam function we used for additive mod-
els (hence the name). we will look at how this works in some detail in chapter 12.
for now, the basic idea is that the iteratively re-weighted least squares procedure
of   11.3.1 doesn   t really require the model for the log odds to be linear. we get
a gam when we    t an additive model to the zi; we could even    t an arbitrary
non-parametric model, like a kernel regression, though that   s not often done.

gams can be used to check glms in much the same way that smoothers can
be used to check parametric regressions:    t a gam and a glm to the same
data, then simulate from the glm, and re-   t both models to the simulated data.
repeated many times, this gives a distribution for how much better the gam
will seem to    t than the glm does, even when the glm is true. you can then
read a p-value o    of this distribution. this is illustrated in   11.6 below.

11.5 model checking

the validity of the id28 model is no more a fact of mathematics or
nature than is the validity of the id75 model. both are sometimes

11.5 model checking

241

convenient assumptions, but neither is guaranteed to be correct, nor even some
sort of generally-correct default. in either case, if we want to use the model, the
proper scienti   c (and statistical) procedure is to check the validity of the modeling
assumptions.

11.5.1 residuals

in your linear models course, you learned a lot of checks based on the residuals of
the model (see chapter 2). many of these ideas translates to id28,
but we need to re-de   ne residuals. sometimes people work with the    response   
residuals,

which should have mean zero (why?), but are heteroskedastic even when the
model is true (why?). others work with standardized or pearson residuals,

(11.18)

(11.19)

yi     p(xi)

yi     p(xi)

(cid:112)v (p(xi))

and there are yet other notions of residuals for logistic models. still, both the
response and the pearson residuals should be unpredictable from the covariates,
and the latter should have constant variance.

11.5.2 non-parametric alternatives

chapter 9 discussed how non-parametric regression models can be used to check
whether parametric regressions are well-speci   ed. the same ideas apply to logistic
regressions, with the minor modi   cation that in place of the di   erence in mses,
one should use the di   erence in log-likelihoods, or (what comes to the same thing,
up to a factor of 2) the di   erence in deviances. the use of generalized additive
models (  11.4.1) as the alternative model class is illustrated in   11.6 below.

11.5.3 calibration

because id28 predicts actual probabilities, we can check its predic-
tions in a more stringent way than an ordinary regression, which just tells us
the mean value of y , but is otherwise silent about its distribution. if we   ve got
a model which tells us that the id203 of rain on a certain class of days is
50%, it had better rain on half of those days, or there model is just wrong about
the id203 of rain. more generally, we   ll say that the model is calibrated
(or well-calibrated) when

pr (y = 1|  p(x) = p) = p

(11.20)

that is, the actual probabilities should match the predicted probabilities. if we
have a large sample, by the law of large numbers, observed relative frequencies
will converge on true probabilities. thus, the observed relative frequencies should

n(cid:88)

i=1

n(cid:88)

242

id28

be close to the predicted probabilities, or else the model is making systematic
mistakes.

in practice, each case often has its own unique predicted id203 p, so
we can   t really accumulate many cases with the same p and check the relative
frequency among those cases. when that happens, one option is to look at all
the cases where the predicted id203 is in some small range [p, p +  ); the
observed relative frequency had then better be in that range too.   11.7 below
illustrates some of the relevant calculations.

a second option is to use what is called a proper scoring rule, which is a
function of the outcome variables and the predicted probabilities that attains its
minimum when, and only when, the predicted are calibrated. for binary out-
comes, one proper scoring rule (historically the oldest) is the brier score,

n   1

(yi     pi)2

(11.21)

another however is simply the (normalized) negative log-likelihood,

    n   1

yi log pi + (1     yi) log (1     pi)

(11.22)

i=1

of course, proper scoring rules are better evaluated out-of-sample, or, failing
that, through cross-validation, than in-sample. even an in-sample evaluation is
better than nothing, however, which is too often what happens.

11.6 a toy example

here   s a worked r example, using the data from the upper right panel of fig-
ure 11.1. the 50    2 matrix x holds the input variables (the coordinates are
independently and uniformly distributed on [   1, 1]), and y.1 the correspond-
ing class labels, themselves generated from a id28 with   0 =    0.5,
   = (   1, 1).

df <- data.frame(y = y.1, x1 = x[, 1], x2 = x[, 2])
logr <- glm(y ~ x1 + x2, data = df, family = "binomial")

the deviance of a model    tted by maximum likelihood is twice the di   erence
between its log likelihood and the maximum log likelihood for a saturated model,
i.e., a model with one parameter per observation. hopefully, the saturated model
can give a perfect    t.5 here the saturated model would assign id203 1 to

the observed outcomes6, and the logarithm of 1 is zero, so d = 2(cid:96)((cid:99)  0,(cid:98)  ). the

5 the factor of two is so that the deviance will have a   2 distribution. speci   cally, if the model with p

parameters is right, the deviance will have a   2 distribution with n     p degrees of freedom. see
appendix i for the connection between log likelihood ratios and   2 distributions.

6 this is not possible when there are multiple observations with the same input features, but di   erent

classes.

11.6 a toy example

243

null deviance is what   s achievable by using just a constant bias   0 and setting
the rest of    to 0. the    tted model de   nitely improves on that.7

if we   re interested in inferential statistics on the estimated model, we can see

those with summary, as with lm:

max
2.1593

3q
1.0914

median
-0.6665

1q
-1.0306

summary(logr, digits = 2, signif.stars = false)
##
## call:
## glm(formula = y ~ x1 + x2, family = "binomial", data = df)
##
## deviance residuals:
##
min
## -1.7313
##
## coefficients:
##
## (intercept)
## x1
## x2
## ---
## signif. codes:
##
## (dispersion parameter for binomial family taken to be 1)
##
##
null deviance: 68.593 on 49
## residual deviance: 60.758 on 47
## aic: 66.758
##
## number of fisher scoring iterations: 4

estimate std. error z value pr(>|z|)
0.3345
0.0493 *
0.0557 .

degrees of freedom
degrees of freedom

-0.3226
-1.0528
1.3493

-0.965
-1.966
1.913

0.3342
0.5356
0.7052

0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

the    tted values of the id28 are the class probabilities; this next

line gives us the (in-sample) mis-classi   cation rate.

mean(ifelse(fitted(logr) < 0.5, 0, 1) != df$y)
## [1] 0.26

an error rate of 26% may sound bad, but notice from the contour lines in
figure 11.1 that lots of the probabilities are near 0.5, meaning that the classes
are just genuinely hard to predict.

to see how well the id28 assumption holds up, let   s compare this
to a gam. we   ll use the same package for estimating the gam, mgcv, that we
used to    t the additive models in chapter 8.

library(mgcv)
(gam.1 <- gam(y ~ s(x1) + s(x2), data = df, family = "binomial"))
##
## family: binomial
## link function: logit
7 aic is of course the akaike information criterion,    2(cid:96) + 2p, with p being the number of parameters

(here, p = 3). (some people divide this through by n.) aic has some truly devoted adherents,
especially among non-statisticians, but i have been deliberately ignoring it and will continue to do
so. basically, to the extent aic succeeds, it works as fast, large-sample approximation to doing
leave-one-out cross-validation. claeskens and hjort (2008) is a thorough, modern treatment of aic
and related model-selection criteria from a statistical viewpoint; see especially   2.9 of that book for
the connection between aic and leave-one-out.

244

id28

simulate.from.logr <- function(df, mdl) {

probs <- predict(mdl, newdata = df, type = "response")
df$y <- rbinom(n = nrow(df), size = 1, prob = probs)
return(df)

}

code example 27: code for simulating from an estimated id28 model. by default
(type="link"), predict for id28s returns predictions for the log odds; changing
the type to "response" returns a id203.

##
## formula:
## y ~ s(x1) + s(x2)
##
## estimated degrees of freedom:
## 1.22 8.70
##
## ubre score: 0.1544972

total = 10.92

this    ts a gam to the same data, using spline smoothing of both input vari-
ables. (figure 11.2 shows the partial response functions.) the (in-sample) de-
viance is

signif(gam.1$deviance, 3)
## [1] 35.9

which is lower than the id28, so the gam gives the data higher
likelihood. we expect this; the question is whether the di   erence is signi   cant, or
within the range of what we should expect when id28 is valid. to
test this, we need to simulate from the id28 model.

now we simulate from our    tted model, and re-   t both the id28

and the gam.

delta.deviance.sim <- function(df, mdl) {
sim.df <- simulate.from.logr(df, mdl)
glm.dev <- glm(y ~ x1 + x2, data = sim.df, family = "binomial")$deviance
gam.dev <- gam(y ~ s(x1) + s(x2), data = sim.df, family = "binomial")$deviance
return(glm.dev - gam.dev)

}

notice that in this simulation we are not generating new (cid:126)x values. the logistic
regression and the gam are both models for the response conditional on the
inputs, and are agnostic about how the inputs are distributed, or even whether
it   s meaningful to talk about their distribution.

finally, we repeat the simulation a bunch of times, and see where the observed

di   erence in deviances falls in the sampling distribution.

(delta.dev.observed <- logr$deviance - gam.1$deviance)
## [1] 24.86973
delta.dev <- replicate(100, delta.deviance.sim(df, logr))
mean(delta.dev.observed <= delta.dev)
## [1] 0.11

11.7 weather forecasting in snoqualmie falls

245

plot(gam.1, residuals = true, pages = 0)

figure 11.2 partial response functions estimated when we    t a gam to
the data simulated from a id28. notice that the vertical axes
are on the logit scale.

in other words, the amount by which a gam    ts the data better than logistic
regression is pretty near the middle of the null distribution. since the example
data really did come from a id28, this is a relief.

11.7 weather forecasting in snoqualmie falls

for our worked data example, we are going to build a simple weather forecaster.
our data consist of daily records, from the start of 1948 to the end of 1983, of

   1.0   0.50.00.51.0   40   20020x1s(x1,1.22)   1.0   0.50.00.51.0   40   20020x2s(x2,8.7)246

id28

hist(delta.dev, main = "", xlab = "amount by which gam fits better than id28")
abline(v = delta.dev.observed, col = "grey", lwd = 4)

figure 11.3 sampling distribution for the di   erence in deviance between a
gam and a id28, on data generated from a id28.
the observed di   erence in deviances is shown by the grey vertical line.

precipitation at snoqualmie falls, washington (figure 11.4)8. each row of the
data    le is a di   erent year; each column records, for that day of the year, the
1
day   s precipitation (rain or snow), in units of
100 inch. because of leap-days, there

8 i learned of this data set from guttorp (1995); the data    le is available from

http://www.stat.washington.edu/peter/stoch.mod.data.html. see code example 28 for the
commands used to read it in, and to reshape it into a form more convenient for r.

amount by which gam fits better than id28frequency01020304050607001020304050607011.7 weather forecasting in snoqualmie falls

247

snoqualmie <- scan("http://www.stat.washington.edu/peter/book.data/set1", skip = 1)
snoq <- data.frame(tomorrow = c(tail(snoqualmie, -1), na), today = snoqualmie)
years <- 1948:1983
days.per.year <- rep(c(366, 365, 365, 365), length.out = length(years))
snoq$year <- rep(years, times = days.per.year)
snoq$day <- rep(c(1:366, 1:365, 1:365, 1:365), times = length(years)/4)
snoq <- snoq[-nrow(snoq), ]

code example 28: read in and re-shape the snoqualmie data set. prof. guttorp, who has
kindly provided the data, formatted it so that each year was a di   erent row, which is rather
inconvenient for r.

are 366 columns, with the last column having an na value for three out of four
years.

what we want to do is predict tomorrow   s weather from today   s. this would
be of interest if we lived in snoqualmie falls, or if we operated one of the local
hydroelectric power plants, or the tourist attraction of the falls themselves. ex-
amining the distribution of the data (figures 11.5 and 11.6) shows that there is a
big spike in the distribution at zero precipitation, and that days of no precipita-
tion can follow days of any amount of precipitation but seem to be less common
after heavy precipitation.

these facts suggest that    no precipitation    is a special sort of event which
would be worth predicting in its own right (as opposed to just being when the
precipitation happens to be zero), so we will attempt to do so with logistic re-
gression. speci   cally, the input variable xi will be the amount of precipitation on
the ith day, and the response yi will be the indicator variable for whether there
was any precipitation on day i + 1     that is, yi = 1 if xi+1 > 0, an yi = 0 if
xi+1 = 0. we expect from figure 11.6, as well as common experience, that the
coe   cient on x should be positive.9
the estimation is straightforward:

snoq.logistic <- glm((tomorrow > 0) ~ today, data = snoq, family = binomial)

to see what came from the    tting, run summary:

print(summary(snoq.logistic), digits = 3, signif.stars = false)
##
## call:
## glm(formula = (tomorrow > 0) ~ today, family = binomial, data = snoq)
##
## deviance residuals:
##
min
## -4.525
##
## coefficients:
##
## (intercept) -0.43520
## today
0.04523

estimate std. error z value pr(>|z|)
<2e-16
<2e-16

0.02163
0.00131

1q
-0.999

median
0.167

max
1.367

-20.1
34.6

3q
1.170

9 this does not attempt to model how much precipitation there will be tomorrow, if there is any. we

could make that a separate model, if we can get this part right.

248

id28

figure 11.4 snoqualmie falls, washington, on a low-precipitation day.
photo by jeannine hall gailey, from http://myblog.webbish6.com/2011/
07/17-years-and-hoping-for-another-17.html. [[todo: get
permission for photo use!]]

##
## (dispersion parameter for binomial family taken to be 1)
##
##
null deviance: 18191
## residual deviance: 15896
## aic: 15900
##
## number of fisher scoring iterations: 5

degrees of freedom
degrees of freedom

on 13147
on 13146

the coe   cient on the amount of precipitation today is indeed positive, and (if
we can trust r   s assumptions) highly signi   cant. there is also an intercept term,

11.7 weather forecasting in snoqualmie falls

249

hist(snoqualmie, n = 50, id203 = true, xlab = "precipitation (1/100 inch)")
rug(snoqualmie, col = "grey")

figure 11.5 histogram of the amount of daily precipitation at snoqualmie
falls

which is slight positive, but not very signi   cant. we can see what the intercept
term means by considering what happens when on days of no precipitation. the
linear predictor is then just the intercept, -0.435, and the predicted id203
of precipitation is 0.393. that is, even when there is no precipitation today, it   s
almost as likely as not that there will be some precipitation tomorrow.10

we can get a more global view of what the model is doing by plotting the data

10 for western washington state, this is plausible     but see below.

histogram of snoqualmieprecipitation (1/100 inch)density01002003004000.000.010.020.030.040.050.06250

id28

plot(tomorrow ~ today, data = snoq, xlab = "precipitation today (1/100 inch)",

ylab = "precipitation tomorrow (1/100 inch)", cex = 0.1)

rug(snoq$today, side = 1, col = "grey")
rug(snoq$tomorrow, side = 2, col = "grey")

figure 11.6 scatterplot showing relationship between amount of
precipitation on successive days. notice that days of no precipitation can
follow days of any amount of precipitation, but seem to be more common
when there is little or no precipitation to start with.

and the predictions (figure 11.7). this shows a steady increase in the id203
of precipitation tomorrow as the precipitation today increases, though with the
leveling o    characteristic of id28. the (approximate) 95% con   dence
limits for the predicted id203 are (on close inspection) asymmetric.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000100200300400precipitation today (1/100 inch)precipitation tomorrow (1/100 inch)11.7 weather forecasting in snoqualmie falls

251

plot((tomorrow > 0) ~ today, data = snoq, xlab = "precipitation today (1/100 inch)",

ylab = "positive precipitation tomorrow?")

rug(snoq$today, side = 1, col = "grey")
data.plot <- data.frame(today = (0:500))
pred.bands <- function(mdl, data, col = "black", mult = 1.96) {

preds <- predict(mdl, newdata = data, se.fit = true)
lines(data[, 1], ilogit(preds$fit), col = col)
lines(data[, 1], ilogit(preds$fit + mult * preds$se.fit), col = col, lty = "dashed")
lines(data[, 1], ilogit(preds$fit - mult * preds$se.fit), col = col, lty = "dashed")

}
pred.bands(snoq.logistic, data.plot)

figure 11.7 data (dots), plus predicted probabilities (solid line) and
approximate 95% con   dence intervals from the id28 model
(dashed lines). note that calculating standard errors for predictions on the
logit scale, and then transforming, is better practice than getting standard
errors directly on the id203 scale.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.20.40.60.81.0precipitation today (1/100 inch)positive precipitation tomorrow?252

id28

how well does this work? we can get a    rst sense of this by comparing it
to a simple nonparametric smoothing of the data. remembering that when y
is binary, pr (y = 1|x = x) = e [y |x = x], we can use a smoothing spline to
estimate e [y |x = x] (figure 11.8). this would not be so great as a model     it
ignores the fact that the response is a binary event and we   re trying to estimate
a id203, the fact that the variance of y therefore depends on its mean, etc.
    but it   s at least suggestive.

the result starts out notably above the id28, then levels out and
climbs much more slowly. it also has a bunch of dubious-looking wiggles, despite
the cross-validation.

we can try to do better by    tting a generalized additive model. in this case,
with only one predictor variable, this means using non-parametric smoothing to
estimate the log odds     we   re still using the logistic transformation, but only
requiring that the log odds change smoothly with x, not that they be linear in
x. the result (figure 11.9) is initially similar to the spline, but has some more
exaggerated undulations, and has con   dence intervals. at the largest values of
x, the latter span nearly the whole range from 0 to 1, which is not unreasonable
considering the sheer lack of data there.

visually, the id28 curve is hardly ever within the con   dence limits
of the non-parametric predictor. what can we say about the di   erence between
the two models more quantiatively?
numerically, the deviance is 1.5895596    104 for the id28, and
1.5121622    104 for the gam. we can go through the testing procedure outlined
in   11.6. we need a simulator (which presumes that the id28 model
is true), and we need to calculate the di   erence in deviance on simulated data
many times.

snoq.sim <- function(model = snoq.logistic) {

fitted.probs = fitted(model)
return(rbinom(n = length(fitted.probs), size = 1, prob = fitted.probs))

}

a quick check of the simulator against the observed values:

0.0000

min. 1st qu.
0.0000

summary(ifelse(snoq[, 1] > 0, 1, 0))
##
##
summary(snoq.sim())
##
##

min. 1st qu.
0.0000

median
1.0000

median
1.0000

0.5262

0.0000

0.5176

mean 3rd qu.
1.0000

mean 3rd qu.
1.0000

max.
1.0000

max.
1.0000

this suggests that the simulator is not acting crazily.
now for the di   erence in deviances:

diff.dev <- function(model = snoq.logistic, x = snoq[, "today"]) {

y.new <- snoq.sim(model)
glm.dev <- glm(y.new ~ x, family = binomial)$deviance
gam.dev <- gam(y.new ~ s(x), family = binomial)$deviance
return(glm.dev - gam.dev)

}

11.7 weather forecasting in snoqualmie falls

253

plot((tomorrow > 0) ~ today, data = snoq, xlab = "precipitation today (1/100 inch)",

ylab = "positive precipitation tomorrow?")

rug(snoq$today, side = 1, col = "grey")
data.plot <- data.frame(today = (0:500))
pred.bands(snoq.logistic, data.plot)
snoq.spline <- smooth.spline(x = snoq$today, y = (snoq$tomorrow > 0))
lines(snoq.spline, col = "red")

figure 11.8 as figure 11.7, plus a smoothing spline (red).

a single run of this takes about 0.6 seconds on my computer.
finally, we calculate the distribution of di   erence in deviances under the null
(that the id28 is properly speci   ed), and the corresponding p-value:

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.20.40.60.81.0precipitation today (1/100 inch)positive precipitation tomorrow?254

id28

library(mgcv)
plot((tomorrow > 0) ~ today, data = snoq, xlab = "precipitation today (1/100 inch)",

ylab = "positive precipitation tomorrow?")

rug(snoq$today, side = 1, col = "grey")
pred.bands(snoq.logistic, data.plot)
lines(snoq.spline, col = "red")
snoq.gam <- gam((tomorrow > 0) ~ s(today), data = snoq, family = binomial)
pred.bands(snoq.gam, data.plot, "blue")

figure 11.9 as figure 11.8, but with the addition of a generalized additive
model (blue line) and its con   dence limits (dashed blue lines).

diff.dev.obs <- snoq.logistic$deviance - snoq.gam$deviance
null.dist.of.diff.dev <- replicate(100, diff.dev())
p.value <- (1 + sum(null.dist.of.diff.dev > diff.dev.obs))/(1 + length(null.dist.of.diff.dev))

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.20.40.60.81.0precipitation today (1/100 inch)positive precipitation tomorrow?11.7 weather forecasting in snoqualmie falls

255

using a thousand replicates takes about 67 seconds, or a bit over a minute; it
gives a p-value of < 1/101. (a longer run of 1000 replicates, not shown, gives a
p-values of < 10   3.)

having detected that there is a problem with the logistic model, we can ask
where it lies. we could just use the gam, but it   s more interesting to try to
diagnose what   s going on.

in this respect figure 11.9 is actually a little misleading, because it leads the
eye to emphasize the disagreement between the models at large x, when actually
there are very few data points there, and so even large di   erences in predicted
probabilities there contribute little to the over-all likelihood di   erence. what is
actually more important is what happens at x = 0, which contains a very large
number of observations (about 47% of all observations), and which we have reason
to think is a special value anyway.

let   s try introducing a dummy variable for x = 0 into the id28,
and see what happens. it will be convenient to augment the data frame with an
extra column, recording 1 whenever x = 0 and 0 otherwise.

snoq2 <- data.frame(snoq, dry = ifelse(snoq$today == 0, 1, 0))
snoq2.logistic <- glm((tomorrow > 0) ~ today + dry, data = snoq2, family = binomial)
snoq2.gam <- gam((tomorrow > 0) ~ s(today) + dry, data = snoq2, family = binomial)

notice that i allow the gam to treat zero as a special value as well, by giving
it access to that dummy variable. in principle, with enough data it can decide
whether or not that is useful on its own, but since we have guessed that it is, we
might as well include it. the new glm has a deviance of 1.4954796    104, lower
than even the gam before, and the new gam has a deviance of 1.4841671   104.
i will leave repeating the speci   cation test as an exercise. figure 11.10 shows
the data and the two new models. these are extremely close to each other at
low percipitation, and diverge thereafter. the new gam is the smoothest model
we   ve seen yet, which suggests that before the it was being under-smoothed to
help capture the special value at zero.

let   s turn now to looking at calibration. the actual fraction of no-precipitation

days which are followed by precipitation is

signif(mean(snoq$tomorrow[snoq$today == 0] > 0), 3)
## [1] 0.287

what does the new logistic model predict?

signif(predict(snoq2.logistic, newdata = data.frame(today = 0, dry = 1), type = "response"),

3)

##
1
## 0.287

this should not be surprising     we   ve given the model a special parameter
dedicated to getting this one id203 exactly right! the hope however is that
this will change the predictions made on days with precipitation so that they are
better.

looking at a histogram of    tted values (hist(fitted(snoq2.logistic)))

256

id28

plot((tomorrow > 0) ~ today, data = snoq, xlab = "precipitation today (1/100 inch)",

ylab = "positive precipitation tomorrow?")

rug(snoq$today, side = 1, col = "grey")
data.plot = data.frame(data.plot, dry = ifelse(data.plot$today == 0, 1, 0))
lines(snoq.spline, col = "red")
pred.bands(snoq2.logistic, data.plot)
pred.bands(snoq2.gam, data.plot, "blue")

figure 11.10 as figure 11.9, but allowing the two models to use a dummy
variable indicating when today is completely dry (x = 0).

shows a gap in the distribution of predicted probabilities below 0.63, so we   ll
look    rst at days where the predicted id203 is between 0.63 and 0.64.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.20.40.60.81.0precipitation today (1/100 inch)positive precipitation tomorrow?11.8 id28 with more than two classes

257

signif(mean(snoq$tomorrow[(fitted(snoq2.logistic) >= 0.63) & (fitted(snoq2.logistic) <

0.64)] > 0), 3)

## [1] 0.526

not bad     but a bit painful to write out. let   s write a function:

frequency.vs.id203 <- function(p.lower, p.upper = p.lower + 0.01, model = snoq2.logistic,

events = (snoq$tomorrow > 0)) {
fitted.probs <- fitted(model)
indices <- (fitted.probs >= p.lower) & (fitted.probs < p.upper)
ave.prob <- mean(fitted.probs[indices])
frequency <- mean(events[indices])
se <- sqrt(ave.prob * (1 - ave.prob)/sum(indices))
return(c(frequency = frequency, ave.prob = ave.prob, se = se))

}

i have added a calculation of the average predicted id203, and a crude
estimate of the standard error we should expect if the observations really are
binomial with the predicted probabilities11. try the function out before doing
anything rash:

frequency.vs.id203(0.63)
##
se
## 0.52603037 0.63414568 0.01586292

frequency

ave.prob

this agrees with our previous calculation.
now we can do this for a lot of id203 brackets:

f.vs.p <- sapply(c(0.28, (63:100)/100), frequency.vs.id203)

this comes with some unfortunate r cruft, removable thus

f.vs.p <- data.frame(frequency = f.vs.p["frequency", ], ave.prob = f.vs.p["ave.prob",

], se = f.vs.p["se", ])

and we   re ready to plot (figure 11.11). the observed frequencies are generally
reasonably near the predicted probabilities. while i wouldn   t want to say this
was the last word in weather forecasting12, it   s surprisingly good for such a simple
model. i will leave calibration checking for the gam as another exercise.

11.8 id28 with more than two classes

if y can take on more than two values, say k of them, we can still use logistic
regression. instead of having one set of parameters   0,   , each class c in 0 :
(k     1) will have its own o   set   (c)
0 and vector   (c), and the predicted conditional
probabilities will be

(cid:16)

y = c| (cid:126)x = x

=

pr

(cid:17)

(cid:80)

0 +x    (c)
e  (c)
c e  (c)

0 +x    (c)

(11.23)

11 this could be improved by averaging predicted variances for each point, but using id203

ranges of 0.01 makes it hardly worth the e   ort.

12 there is an extensive discussion of this data in guttorp (1995, ch. 2), including many signi   cant

re   nements, such as dependence across multiple days.

258

id28

you can check that when there are only two classes (say, 0 and 1), equation
0 and    =   (1)       (0). in fact,
11.23 reduces to equation 11.5, with   0 =   (1)
no matter how many classes there are, we can always pick one of them, say c = 0,
and    x its parameters at exactly zero, without any loss of generality (exercise
11.2)13.

0       (0)

calculation of the likelihood now proceeds as before (only with more book-

keeping), and so does id113.

as for r implementations, for am long time the easiest way to do this was
actually to use the nnet package for neural networks (venables and ripley, 2002).
more recently, the multiclass function from the mgcv package does the same
sort of job, with an interface closer to what you will be familiar with from glm
and gam.

exercises

11.1    we minimize the mis-classi   cation rate by predicting the most likely class   : let (cid:98)  (x)
be our predicted class, either 0 or 1. our error rate is then pr (y (cid:54)=(cid:98)  ). show that
pr (y (cid:54)=(cid:98)  ) = e(cid:2)(y    (cid:98)  )2(cid:3). further show that e(cid:2)(y    (cid:98)  )2 | x = x(cid:3) = pr (y = 1|x = x) (1   
2(cid:98)  (x)) +(cid:98)  2(x). conclude by showing that if pr (y = 1|x = x) > 0.5, the risk of mis-
classi   cation is minimized by taking (cid:98)   = 1, that if pr (y = 1|x = x) < 0.5 the risk is
minimized by taking (cid:98)   = 0, and that when pr (y = 1|x = x) = 0.5 both predictions are

equally risky.

0 and   (c) for each class
0 = 0,   (c) =

11.2 a multiclass id28, as in eq. 11.23, has parameters   (c)

c. show that we can always get the same predicted probabilities by setting   (c)
0 for any one class c, and adjusting the parameters for the other classes appropriately.

11.3 find the    rst and second derivatives of the log-likelihood for id28 with one
predictor variable. explicitly write out the formula for doing one step of newton   s method.
explain how this relates to re-weighted least squares.

13 since we can arbitrarily chose which class   s parameters to    zero out    without a   ecting the predicted
probabilities, strictly speaking the model in eq. 11.23 is unidenti   ed. that is, di   erent parameter
settings lead to exactly the same outcome, so we can   t use the data to tell which one is right. the
usual response here is to deal with this by a convention: we decide to zero out the parameters of the
   rst class, and then estimate the contrasting parameters for the others.

exercises

259

plot(frequency ~ ave.prob, data = f.vs.p, xlim = c(0, 1), ylim = c(0, 1), xlab = "predicted probabilities",

ylab = "observed frequencies")

rug(fitted(snoq2.logistic), col = "grey")
abline(0, 1, col = "grey")
segments(x0 = f.vs.p$ave.prob, y0 = f.vs.p$ave.prob - 1.96 * f.vs.p$se, y1 = f.vs.p$ave.prob +

1.96 * f.vs.p$se)

figure 11.11 calibration plot for the modi   ed id28 model
snoq2.logistic. points show the actual frequency of precipitation for each
level of predicted id203. vertical lines are (approximate) 95% sampling
intervals for the frequency, given the predicted id203 and the number
of observations.

llllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0predicted probabilitiesobserved frequencies12

generalized linear models and generalized

additive models

12.1 generalized linear models and iterative least squares

id28 is a particular instance of a broader kind of model, called
a generalized linear model (glm). you are familiar, of course, from your
regression class with the idea of transforming the response variable, what we   ve
been calling y , and then predicting the transformed variable from x. this was
not what we did in id28. rather, we transformed the conditional
expected value, and made that a linear function of x. this seems odd, because it
is odd, but it turns out to be useful.
let   s be speci   c. our usual focus in regression modeling has been the condi-
tional expectation function,   (x) = e [y |x = x]. in plain id75, we
try to approximate   (x) by   0 +x    . in id28,   (x) = e [y |x = x] =
pr (y = 1|x = x), and it is a transformation of   (x) which is linear. the usual
notation says

  (x) =   0 + x      
  (x)
1       (x)

  (x) = log

(12.1)

(12.2)

= g(  (x))

(12.3)
de   ning the logistic link function by g(m) = log m/(1     m). the function   (x)
is called the linear predictor.
now, the    rst impulse for estimating this model would be to apply the trans-
formation g to the response. but y is always zero or one, so g(y ) =      , and
regression will not be helpful here. the standard strategy is instead to use (what
else?) taylor expansion. speci   cally, we try expanding g(y ) around   (x), and
stop at    rst order:

g(y )     g(  (x)) + (y       (x))g(cid:48)(  (x))
=   (x) + (y       (x))g(cid:48)(  (x))     z

(12.4)
(12.5)

we de   ne this to be our e   ective response after transformation. notice that if
there were no noise, so that y was always equal to its conditional mean   (x),
then regressing z on x would give us back exactly the coe   cients   0,   . what
this suggests is that we can estimate those parameters by regressing z on x.
the term y       (x) has expectation zero, so it acts like the noise, with the
factor of g(cid:48) telling us about how the noise is scaled by the transformation. this

260

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

12.1 generalized linear models and iterative least squares

261

lets us work out the variance of z:

v [z|x = x] = v [  (x)|x = x] + v [(y       (x))g(cid:48)(  (x))|x = x]

(12.6)

= 0 + (g(cid:48)(  (x)))2 v [y |x = x]

(12.7)
for id28, with y binary, v [y |x = x] =   (x)(1     (x)). on the other
hand, with the logistic link function, g(cid:48)(  (x)) =
  (x)(1     (x)) . thus, for logistic
regression, v [z|x = x] = [  (x)(1       (x))]

   1.

1

because the variance of z changes with x, this is a heteroskedastic regression
problem. as we saw in chapter 10, the appropriate way of dealing with such a
problem is to use weighted least squares, with weights inversely proportional to
the variances. this means that, in id28, the weight at x should be
proportional to   (x)(1       (x)). notice two things about this. first, the weights
depend on the current guess about the parameters. second, we give lots of weight
to cases where   (x)     0 or where   (x)     1, and little weight to those where
  (x) = 0.5. this focuses our attention on places where we have a lot of potential
information     the distinction between a id203 of 0.499 and 0.501 is just a
lot easier to discern than that between 0.001 and 0.003!

we can now put all this together into an estimation strategy for logistic regres-

sion.

1. get the data (x1, y1), . . . (xn, yn), and some initial guesses   0,   .
2. until   0,    converge

1. calculate   (xi) =   0 + xi       and the corresponding (cid:98)  (xi)
2. find the e   ective transformed responses zi =   (xi) + yi   (cid:98)  (xi)
3. calculate the weights wi =(cid:98)  (xi)(1    (cid:98)  (xi))
(cid:98)  (xi)(1   (cid:98)  (xi))

4. do a weighted id75 of zi on xi with weights wi, and set   0,   

to the intercept and slopes of this regression

our initial guess about the parameters tells us about the heteroskedastic-
ity, which we use to improve our guess about the parameters, which we use
to improve our guess about the variance, and so on, until the parameters stabi-
lize. this is called iterative reweighted least squares (or    iterative weighted
least squares   ,    iteratively weighted least squares   ,    iteratived reweighted least
squares   , etc.), abbreviated irls, irwls, iwls, etc. as mentioned in the last
chapter, this turns out to be almost equivalent to newton   s method, at least for
this problem.

12.1.1 glms in general

the set-up for an arbitrary glm is a generalization of that for id28.
we need
    a linear predictor,   (x) =   0 + x      
    a link function g, so that   (x) = g(  (x)). for id28, we had

g(  ) = log   /(1       ).

262
    a dispersion scale function v , so that v [y |x = x] =   2v (  (x)). for logis-

glms and gams

tic regression, we had v (  ) =   (1       ), and   2 = 1.

with these, we know the conditional mean and conditional variance of the re-
sponse for each value of the input variables x.

as for estimation, basically everything in the irwls set up carries over un-

changed. in fact, we can go through this algorithm:

1. get the data (x1, y1), . . . (xn, yn),    x link function g(  ) and dispersion scale

2. until   0,    converge:

function v (  ), and make some initial guesses   0,   .

1. calculate   (xi) =   0 + xi       and the corresponding (cid:98)  (xi)
2. find the e   ective transformed responses zi =   (xi) + (yi    (cid:98)  (xi))g(cid:48)((cid:98)  (xi))
3. calculate the weights wi = [(g(cid:48)((cid:98)  (xi))2v ((cid:98)  (xi))]

4. do a weighted id75 of zi on xi with weights wi, and set   0,   

   1

to the intercept and slopes of this regression

notice that even if we don   t know the over-all variance scale   2, that   s ok,
because the weights just have to be proportional to the inverse variance.

12.1.2 examples of glms

12.1.2.1 vanilla linear models

to re-assure ourselves that we are not doing anything crazy, let   s see what
happens when g(  ) =    (the    identity link   ), and v [y |x = x] =   2, so that
v (  ) = 1. then g(cid:48) = 1, all weights wi = 1, and the e   ective transformed re-
sponse zi = yi. so we just end up regressing yi on xi with no weighting at all
    we do ordinary least squares. since neither the weights nor the transformed
response will change, irwls will converge exactly after one step. so if we get
rid of all this nonlinearity and heteroskedasticity and go all the way back to our
very    rst days of doing regression, we get the ols answers we know and love.

12.1.2.2 binomial regression

in many situations, our response variable yi will be an integer count running
between 0 and some pre-determined upper limit ni. (think: number of patients
in a hospital ward with some condition, number of children in a classroom passing
a test, number of widgets produced by a factory which are defective, number of
people in a village with some genetic mutation.) one way to model this would be
as a binomial random variable, with ni trials, and a success id203 pi which
is a logistic function of predictors x. the id28 we have done so far
is the special case where ni = 1 always. i will leave it as an exercise (12.1) for
you to work out the link function and the weights for general binomial regression,
where the ni are treated as known.

one implication of this model is that each of the ni    trials    aggregated together
in yi is independent of all the others, at least once we condition on the predictors

12.1 generalized linear models and iterative least squares

263

x. (so, e.g., whether any student passes the test is independent of whether any
of their classmates pass, once we have conditioned on, say, teacher quality and
average previous knowledge.) this may or may not be a reasonable assumption.
when the successes or failures are dependent, even after conditioning on the
predictors, the binomial model will be mis-speci   ed. we can either try to get
more information, and hope that conditioning on a richer set of predictors makes
the dependence go away, or we can just try to account for the dependence by
modifying the variance (   overdispersion    or    underdispersion   ); we   ll return to
both topics in   12.1.4.

recall that the poisson distribution has id203 mass function

12.1.2.3 poisson regression

p(y) =

e       y

y!

(12.8)

with e [y ] = v [y ] =   . as you remember from basic id203, a poisson
distribution is what we get from a binomial if the id203 of success per trial
shrinks towards zero but the number of trials grows to in   nity, so that we keep
the mean number of successes the same:

binom(n,   /n) (cid:32) pois(  )

(12.9)

this makes the poisson distribution suitable for modeling counts with no    xed
upper limit, but where the id203 that any one of the many individual trials
is a success is fairly low. if    is allowed to change with the predictor variables, we
get poisson regression. since the variance is equal to the mean, poisson regression
is always going to be heteroskedastic.

since    has to be non-negative, a natural link function is g(  ) = log   . this
produces g(cid:48)(  ) = 1/  , and so weights w =   . when the expected count is large,
so is the variance, which normally would reduce the weight put on an observation
in regression, but in this case large expected counts also provide more information
about the coe   cients, so they end up getting increasing weight.

12.1.3 uncertainty

standard errors for coe   cients can be worked out as in the case of weighted
least squares for id75. con   dence intervals for the coe   cients will
be approximately gaussian in large samples, for the usual likelihood-theory rea-
sons, when the model is properly speci   ed. one can, of course, also use either a
parametric bootstrap, or resampling of cases/data-points to assess uncertainty.

resampling of residuals can be trickier, because it is not so clear what counts as
a residual. when the response variable is continuous, we can get    standardized   
, resample them to get    i, and then add

or    pearson    residuals,    i = yi   (cid:98)  (xi)
(cid:113) (cid:92)
    (cid:92)v (  (xi))

v (  (xi)) to the    tted values. this does not really work when the response is

   i
discrete-valued, however.

[[attn:
up
look
anyone
if
has a good
trick
for
this]]

264

glms and gams

12.1.4 modeling dispersion

when we pick a family for the conditional distribution of y , we get a pre-
dicted conditional variance function, v (  (x)). the actual conditional variance
v [y |x = x] may however not track this. when the variances are larger, the
process is over-dispersed; when they are smaller, under-dispersed. over-
dispersion is more common and more worrisome. in many cases, it arises from
some un-modeled aspect of the process     some unobserved heterogeneity, or some
missed dependence. for instance, if we observe count data with an upper limit
and use a binomial model, we   re assuming that each    trial    within a data point
is independent; positive correlation between the trials will give larger variance
around the mean that the mp(1     p) we   d expect1.

the most satisfying solution to over-dispersion is to actually    gure out where
it comes from, and model its origin. failing that, however, we can fall back on
more    phenomenological    modeling. one strategy is to say that

v [y |x = x] =   (x)v (  (x))

(12.10)

and try to estimate the function        a modi   cation of the variance-estimation
idea we saw in   10.3. in doing so, we need a separate estimate of v [y |x = xi].
this can come from repeated measurements at the same value of x, or from the
squared residuals at each data point. once we have some noisy but independent
estimate of v [y |x = xi], the ratio v [y |x = xi] /v (  (xi)) can be regressed on
xi to estimate   . some people recommend doing this step, itself, through a gen-
eralized linear or generalized additive model, with a gamma distribution for the
response, so that the response is guaranteed to be positive.

12.1.5 likelihood and deviance

when dealing with glms, it is conventional to report not the log-likelihood, but
the deviance. the deviance of a model with parameters (  0,   ) is de   ned as

d(  0,   ) = 2[(cid:96)(saturated)     (cid:96)(  0,   )]

(12.11)

here, (cid:96)(  0,   ) is the log-likelihood of our model, and (cid:96)(saturated) is the log-
likelihood of a saturated model which has one parameter per data point. thus,
models with high likelihoods will have low deviances, and vice versa. if our model
is correct and has p + 1 parameters in all (including the intercept), then the
deviance will generally approach a   2 distribution asymptotically, with n   (p+1)
degrees of freedom (appendix i); the factor of 2 in the de   nition is to ensure this.
for discrete response variables, the saturated model can usually ensure that
pr (y = yi|x = xi) = 1, so (cid:96)(saturated) = 0, and deviance is just twice the
negative log-likelihood. if there are multiple data points with the same value of
x but di   erent values of y, then (cid:96)(saturated) < 0. in any case, even for repeated
values of x or even continuous response variables, di   erences in deviance are

1 if (for simplicity) all the trials have the same covariance   , then the variance of their sum is

mp(1     p) + m(m     1)   (why?).

12.1 generalized linear models and iterative least squares

265
just twice di   erences in log-likelihood: d(model1)     d(model2) = 2[(cid:96)(model2)    
(cid:96)(model1)].

12.1.5.1 maximum likelihood and the choice of link function

having chosen a family of conditional distributions, it may happen that when we
write out the log-likelihood, the latter depends on the both the response variables
yi and the coe   cients only through the product of yi with some transformation

of the conditional mean (cid:98)  :

n(cid:88)

(cid:96) =

f (yi, xi) + yig((cid:98)  i) + h(  )

(12.12)

i=1

the log-likelihood can be put in this form with g((cid:98)  i) = log(cid:98)  i/(1    (cid:98)  i). in the
g((cid:98)  i) =(cid:98)  i, and h(  ) =    (cid:98)  2

in the case of id28, examining eq. 11.8 (  11.2.1, p. 237) shows that
case of a gaussian conditional distribution for y , we would have f =    y2
i /2,
i . when the log-likelihood can be written in this form,
g(  ) is the    natural    transformation to apply to the conditional mean, i.e., the
natural link function, and assures us that the solution to iterative least squares
will converge on the maximum likelihood estimate.2 of course we are free to
nonetheless use other transformations of the conditional expectation.

12.1.6 r: glm

as with id28, the workhorse r function for all manner of glms is,
simply, glm. the syntax is strongly parallel to that of lm, with the addition of a
family argument that speci   es the intended distribution of the response variable
(binomial, gaussian, poisson, etc.), and, optionally, a link function appropriate
to the family. (see help(family) for the details.) with family="gaussian" and
an identity link function, its intended behavior is the same as lm.

2 to be more technical, we say that a distribution with parameters    is an exponential family if its

id203 density function at x is exp f (x) + t (x)    g(  )/z(  ), for some vector of statistics t and
some transformation g of the parameters. (to ensure id172,

z(  ) =(cid:82) exp (f (x) + t (x)    g(  ))dx. of course, if the sample space x is discrete, replace this integral

with a sum.) we then say that t (  ) are the    natural    or    canonical    su   cient statistics, and g(  )
are the    natural    parameters. eq. 12.12 is picking out the natural parameters, presuming the
response variable is itself the natural su   cient statistic. many of the familiar families of
distributions, like gaussians, exponentials, gammas, paretos, binomials and poissons are
exponential families. exponential families are very important in classical statistical theory, and have
deep connections to thermodynamics and statistical mechanics (where they   re called    canonical
ensembles   ,    boltzmann distributions    or    gibbs distributions    (mandelbrot, 1962)), and to
id205 (where they   re    maximum id178 distributions   , or    minimax codes   
(gr  unwald, 2007)). despite their coolness, they are a rather peripheral topic for our sort of data
analysis     though see guttorp (1995) for examples of using them in modeling discrete processes.
any good book on statistical theory (e.g., casella and berger 2002) will have a fairly extensive
discussion; barndor   -nielsen (1978) and brown (1986) are comprehensive treatments.

266

glms and gams

12.2 generalized additive models

in the development of generalized linear models, we use the link function g to

relate the conditional mean (cid:98)  (x) to the linear predictor   (x). but really nothing

in what we were doing required    to be linear in x. in particular, it all works
perfectly well if    is an additive function of x. we form the e   ective responses
zi as before, and the weights wi, but now instead of doing a id75 on
xi we do an additive regression, using back   tting (or whatever). this gives us a
generalized additive model (gam).

essentially everything we know about the relationship between linear models
and additive models carries over. gams converge somewhat more slowly as n
grows than do glms, but the former have less bias, and strictly include glms
as special cases. the transformed (mean) response is related to the predictor vari-
ables not just through coe   cients, but through whole partial response functions.
if we want to test whether a glm is well-speci   ed, we can do so by comparing
it to a gam, and so forth.

in fact, one could even make   (x) an arbitrary smooth function of x, to be
estimated through (say) kernel smoothing of zi on xi. this is rarely done, however,
partly because of curse-of-dimensionality issues, but also because, if one is going to
go that far, one might as well just use kernels to estimate conditional distributions,
as we will see in chapter 14.

12.3 further reading

at our level of theory, good references on generalized linear and generalized ad-
ditive models include faraway (2006) and wood (2006), both of which include
extensive examples in r. tutz (2012) o   ers an extensive treatment of glms with
categorical response distributions, along with comparisons to other models for
that task.

overdispersion is the subject of a large literature of its own. all of the refer-
ences just named discuss methods for it. lambert and roeder (1995) is worth
mentioning for introducing some simple-to-calculate ways of detecting and de-
scribing over-dispersion which give some information about why the response is
over-dispersed. one of these (the    relative variance curve   ) is closely related to
the idea sketched above about estimating the dispersion factor.

exercises

12.1 in binomial regression, we have y |x = x     binom(n, p(x)), where p(x) follows a logistic
model. work out the link function g(  ), the variance function v (  ), and the weights w,
assuming that n is known and not random.

12.2 problem set a.15, on predicting the death rate in chicago, is a good candidate for using
poisson regression. repeat the exercises in that problem set with poisson-response gams.
how do the estimated functions change? why is this any di   erent from just taking the
log of the death counts, as suggested in that problem set?

13

classi   cation and regression trees

so far, the models we   ve worked with have been built on the principle of every
point in the data set contributing (at least potentially) to every prediction. an
alternative is to divide up, or partition, the data set, so that each prediction
will only use points from one chunk of the space. if this partition is done in a
recursive or hierarchical manner, we get a prediction tree, which comes in two
varieties, regression trees and classi   cation trees. these may seem too crude
to actually work, but they can, in fact, be both powerful and computationally
e   cient.

[[todo:
notes
taken from
another
course;
integrate]]

13.1 prediction trees

the basic idea is simple. we want to predict a response or class y from inputs
x1, x2, . . . xp. we do this by growing a binary tree. at each internal node in the
tree, we apply a test to one of the inputs, say xi. depending on the outcome of
the test, we go to either the left or the right sub-branch of the tree. eventually
we come to a leaf node, where we make a prediction. this prediction aggregates
or averages all the training data points which reach that leaf. figure 13.1 should
help clarify this.

why do this? predictors like linear or polynomial regression are global mod-
els, where a single predictive formula is supposed to hold over the entire data
space. when the data has lots of features which interact in complicated, nonlin-
ear ways, assembling a single global model can be very di   cult, and hopelessly
confusing when you do succeed. as we   ve seen, non-parametric smoothers try to
   t models locally and then paste them together, but again they can be hard to
interpret. (additive models are at least pretty easy to grasp.)

an alternative approach to nonlinear prediction is to sub-divide, or partition,
the space into smaller regions, where the interactions are more manageable. we
then partition the sub-divisions again     this is recursive partitioning (or
hierarchical partitioning)     until    nally we get to chunks of the space which
are so tame that we can    t simple models to them. the global model thus has
two parts: one is just the recursive partition, the other is a simple model for each
cell of the partition.

now look back at figure 13.1 and the description which came before it. predic-
tion trees use the tree to represent the recursive partition. each of the terminal
nodes, or leaves, of the tree represents a cell of the partition, and has attached

267

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

268

trees

figure 13.1 classi   cation tree for county-level outcomes in the 2008
democratic party primary (as of april 16), by amanada cox for the new york
times. [[todo: get    gure permission!]]

13.1 prediction trees

269

to it a simple model which applies in that cell only. a point x belongs to a leaf
if x falls in the corresponding cell of the partition. to    gure out which cell we
are in, we start at the root node of the tree, and ask a sequence of questions
about the features. the interior nodes are labeled with questions, and the edges
or branches between them labeled by the answers. which question we ask next
depends on the answers to previous questions. in the classic version, each ques-
tion refers to only a single attribute, and has a yes or no answer, e.g.,    is hsgrad
> 0.78?    or    is region == midwest?    the variables can be of any combination
of types (continuous, discrete but ordered, categorical, etc.). you could do more-
than-binary questions, but that can always be accommodated as a larger binary
tree. asking questions about multiple variables at once is, again, equivalent to
asking multiple questions about single variables.

(cid:80)c

stants in the tree)

that   s the recursive partition part; what about the simple local models? for
classic regression trees, the model in each cell is just a constant estimate of y .
that is, suppose the points (xi, yi), (x2, y2), . . . (xc, yc) are all the samples belong-
ing to the leaf-node l. then our model for l is just   y = 1
i=1 yi, the sample
c
mean of the response variable in that cell. this is a piecewise-constant model.1
there are several advantages to this:
    making predictions is fast (no complicated calculations, just looking up con-
    it   s easy to understand what variables are important in making the prediction
    if some data is missing, we might not be able to go all the way down the tree
to a leaf, but we can still make a prediction by averaging all the leaves in the
sub-tree we do reach
    the model gives a jagged response, so it can work when the true regression
surface is not smooth. if it is smooth, though, the piecewise-constant surface
can approximate it arbitrarily closely (with enough leaves)

(look at the tree)

    there are fast, reliable algorithms to learn these trees

a last analogy before we go into some of the mechanics. one of the most
comprehensible non-parametric methods is k-nearest-neighbors:    nd the points
which are most similar to you, and do what, on average, they do. there are
two big drawbacks to it:    rst, you   re de   ning    similar    entirely in terms of the
inputs, not the response; second, k is constant everywhere, when some points
just might have more very-similar neighbors than others. trees get around both
problems: leaves correspond to regions of the input space (a neighborhood), but
one where the responses are similar, as well as the inputs being nearby; and their
size can vary arbitrarily. prediction trees are, in a way, adaptive nearest-neighbor
methods.

1 we could instead    t, say, a di   erent id75 for the response in each leaf node, using only
the data points in that leaf (and using dummy variables for non-quantitative variables). this would
give a piecewise-linear model, rather than a piecewise-constant one. if we   ve built the tree well,
however, all the points in each leaf are pretty similar, so the regression surface would be nearly
constant anyway.

270

trees

let   s start with an example.

13.2 regression trees

13.2.1 example: california real estate again

we   ll revisit the califonia house-price data from chapter 8, where we try to pre-
dict the median house price in each census tract of california from the attributes
of the houses and of the inhabitants of the tract. we   ll try growing a regression
tree for this.

there are several r packages for regression trees; the easiest one is called,

simply, tree (ripley, 2015).

calif <- read.table("http://www.stat.cmu.edu/~cshalizi/350/hw/06/cadata.dat",

header = true)

require(tree)
treefit <- tree(log(medianhousevalue) ~ longitude + latitude, data = calif)

this does a tree regression of the log price on longitude and latitude. what does
this look like? figure 13.2 shows the tree itself; figure 13.3 shows the partition,
overlaid on the actual prices in the state. (the ability to show the partition is
why i picked only two input variables.)

qualitatively, this looks like it does a fair job of capturing the interaction
between longitude and latitude, and the way prices are higher around the coasts
and the big cities. quantitatively, the error isn   t bad:

summary(treefit)
##
## regression tree:
## tree(formula = log(medianhousevalue) ~ longitude + latitude,
##
## number of terminal nodes: 12
## residual mean deviance:
## distribution of residuals:
##
median
## -2.75900 -0.26080 -0.01359

0.1662 = 3429 / 20630

max.
1.84100

mean
0.00000

3rd qu.
0.26310

data = calif)

1st qu.

min.

here    deviance    is just mean squared error; this gives us an rms error of 0.41,
which is higher than the smooth non-linear models in chapter 8, but not shocking
since we   re using only two variables, and have only twelve leaves.

the    exibility of a tree is basically controlled by how many leaves they have,
since that   s how many cells they partition things into. the tree    tting function
has a number of controls settings which limit how much it will grow     each node
has to contain a certain number of points, and adding a node has to reduce the
error by at least a certain amount. the default for the latter, mindev, is 0.01;
let   s turn it down and see what happens.

treefit2 <- tree(log(medianhousevalue) ~ longitude + latitude, data = calif,

mindev = 0.001)

figure 13.4 shows the tree itself; with 68 nodes, the plot is fairly hard to read,

13.2 regression trees

271

plot(treefit)
text(treefit, cex = 0.75)

figure 13.2 regression tree for predicting california housing prices from
geographic coordinates. at each internal node, we ask the associated
question, and go to the left child if the answer is    yes   , to the right child if
the answer is    no   . note that leaves are labeled with log prices; the plotting
function isn   t    exible enough, unfortunately, to apply transformations to the
labels.

but by zooming in on any part of it, you can check what it   s doing. figure 13.5
shows the corresponding partition. it   s obviously much    ner-grained than that
in figure 13.3, and does a better job of matching the actual prices (rms error
0.32). more interestingly, it doesn   t just uniformly divide up the big cells from

|latitude < 38.485longitude <    121.655latitude < 37.925latitude < 34.675longitude <    118.315longitude <    117.545latitude < 33.725latitude < 33.59longitude <    116.33longitude <    120.275latitude < 39.35512.4812.1012.5312.5412.1412.0911.1611.6311.7511.2811.7311.32272

trees

price.deciles <- quantile(calif$medianhousevalue, 0:10/10)
cut.prices <- cut(calif$medianhousevalue, price.deciles, include.lowest = true)
plot(calif$longitude, calif$latitude, col = grey(10:2/11)[cut.prices], pch = 20,

xlab = "longitude", ylab = "latitude")

partition.tree(treefit, ordvars = c("longitude", "latitude"), add = true)

figure 13.3 map of actual median house prices (color-coded by decile,
darker being more expensive), and the partition of the treefit tree.

the    rst partition; some of the new cells are very small, others quite large. the
metropolitan areas get a lot more detail than the mojave.

of course there   s nothing magic about the geographic coordinates, except that

they make for pretty plots. we can include all the input features in our model:

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042longitudelatitude12.512.112.512.512.112.111.211.611.811.311.711.313.2 regression trees

273

figure 13.4 as figure 13.2, but allowing splits for smaller reductions in
error (mindev=0.001 rather than the default mindev=0.01). then fact that
the plot is nearly illegible is deliberate.

treefit3 <- tree(log(medianhousevalue) ~ ., data = calif)

with the result shown in figure 13.6. this model has    fteen leaves, as opposed
to sixty-eight for treefit2, but the rms error is almost as good (0.36). this
is highly interactive: latitude and longitude are only used if the income level
is su   ciently low. (unfortunately, this does mean that we don   t have a spatial
partition to compare to the previous ones, but we can map the predictions; figure
13.7.) many of the features, while they were available to the tree    t, aren   t used
at all.

now let   s turn to how we actually grow these trees.

|latitude < 38.485longitude <    121.655latitude < 37.925longitude <    122.305latitude < 37.585longitude <    122.025latitude < 37.175latitude < 37.465longitude <    122.235longitude <    121.865latitude < 37.315latitude < 37.815longitude <    122.145latitude < 37.715longitude <    122.255latitude < 37.875longitude <    122.38latitude < 38.225longitude <    122.335latitude < 37.985latitude < 34.675longitude <    118.315latitude < 34.165longitude <    118.365latitude < 34.055latitude < 33.885longitude <    118.485longitude <    120.415longitude <    119.365longitude <    117.545latitude < 33.725latitude < 34.105longitude <    118.165latitude < 33.875longitude <    118.285latitude < 34.045longitude <    118.225longitude <    118.285latitude < 33.915longitude <    118.245longitude <    117.755latitude < 33.905longitude <    117.815latitude < 34.525longitude <    118.015longitude <    118.115latitude < 33.59longitude <    116.33longitude <    117.165latitude < 33.125longitude <    117.235latitude < 32.755latitude < 34.055longitude <    116.245longitude <    120.275latitude < 37.155longitude <    120.645latitude < 36.02longitude <    121.335latitude < 36.805latitude < 34.825longitude <    119.935latitude < 39.355longitude <    121.365latitude < 38.86longitude <    121.57longitude <    122.91512.712.412.912.812.412.612.412.311.912.212.611.912.512.712.612.211.612.211.912.912.712.112.711.812.712.412.212.512.512.112.211.711.611.912.311.912.312.712.211.912.412.712.211.912.712.412.111.812.111.211.811.311.512.112.011.111.911.611.711.211.311.612.011.611.811.311.911.3274

trees

plot(calif$longitude, calif$latitude, col = grey(10:2/11)[cut.prices], pch = 20,

xlab = "longitude", ylab = "latitude")

partition.tree(treefit2, ordvars = c("longitude", "latitude"), add = true, cex = 0.3)

figure 13.5 partition for treefit2. note the high level of detail around
the cities, as compared to the much coarser cells covering rural areas where
variations in prices are less extreme.

13.2.2 regression tree fitting

once we    x the tree, the local models are completely determined, and easy to
   nd (we just average), so all the e   ort should go into    nding a good tree, which
is to say into    nding a good partitioning of the data.

ideally, we would like to maximize the information the partition gives us about

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042longitudelatitude12.712.412.912.812.412.612.412.311.912.212.611.912.512.712.612.211.612.211.912.912.712.112.711.812.712.412.212.512.512.112.211.711.611.912.311.912.312.712.211.912.412.712.211.912.712.412.111.812.111.211.811.311.512.112.011.111.911.611.711.211.311.612.011.611.811.311.911.313.2 regression trees

275

plot(treefit3)
text(treefit3, cex = 0.5, digits = 3)

figure 13.6 regression tree for log price when all other features are
included as (potential) inputs. note that many of the features are not used
by the tree.

the response variable. since we are doing regression, what we would really like is
for the conditional mean e [y |x = x] to be nearly constant in x over each cell of
the partition, and for adjoining cells to have distinct expected values. (it   s ok if
two cells of the partition far apart have similar average values.) we can   t do this
directly, so we do a greedy search. we start by    nding the one binary question
we can ask about the predictors which maximizes the information we get about

|medianincome < 3.5471medianincome < 2.51025latitude < 34.465longitude <    117.775longitude <    120.275latitude < 37.905latitude < 37.925longitude <    122.235latitude < 34.455longitude <    117.765longitude <    120.385medianincome < 5.5892medianhouseage < 38.5medianincome < 7.39311.911.511.811.411.112.512.211.812.011.411.712.212.512.613.0276

trees

cut.predictions <- cut(predict(treefit3), log(price.deciles), include.lowest = true)
plot(calif$longitude, calif$latitude, col = grey(10:2/11)[cut.predictions],

pch = 20, xlab = "longitude", ylab = "latitude")

figure 13.7 predicted prices for the treefit3 model. same color scale as
in previous plots (where dots indicated actual prices).

the average value of y ; this gives us our root node and two daughter nodes.2 at
each daughter node, we repeat our initial procedure, asking which question would
give us the maximum information about the average value of y , given where we
already are in the tree. we repeat this recursively.

every recursive algorithm needs to know when it   s done, a stopping criterion.

2 mixing botanical and genealogical metaphors for trees is ugly, but i can   t    nd a way around it.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042longitudelatitude13.2 regression trees

277

here this means when to stop trying to split nodes. obviously nodes which contain
only one data point cannot be split, but giving each observations its own leaf is
unlikely to generalize well. a more typical criterion is something like: halt when
each child would contain less than    ve data points, or when splitting increases
the information by less than some threshold. picking the criterion is important
to get a good tree, so we   ll come back to it presently.

to really make this operational, we need to be more precise about what we
mean by    information about the average value of y    . this can be measured
straightforwardly by the mean squared error. the mse for a tree t is

(yi     mc)2

(13.1)

m se(t ) =

1
n

(cid:88)

(cid:88)

c   leaves(t )

i   c

(cid:80)

where mc = 1
nc

i   c yi, the prediction for leaf c.

the basic regression-tree-growing algorithm then is as follows:

1. start with a single node containing all points. calculate mc and m se.
2. if all the points in the node have the same value for all the input variables,
stop. otherwise, search over all binary splits of all variables for the one which
will reduce m se as much as possible. if the largest decrease in m se would
be less than some threshold   , or one of the resulting nodes would contain less
than q points, stop. otherwise, take that split, creating two new nodes.

3. in each new node, go back to step 1.

trees use only one feature (input variable) at each step. if multiple features are
equally good, which one is chosen is a matter of chance, or arbitrary programming
decisions.

one problem with the straight-forward algorithm i   ve just given is that it can
stop too early, in the following sense. there can be variables which are not very
informative themselves, but which lead to very informative subsequent splits.
this suggests a problem with stopping when the decrease in s becomes less than
some   . similar problems can arise from arbitrarily setting a minimum number
of points q per node.

a more successful approach to    nding regression trees uses the idea of cross-
validation (chapter 3), especially k-fold cross-validation. we initially grow a large
tree, looking only at the error on the training data. (we might even set q = 1
and    = 0 to get the largest tree we can.) this tree is generally too large and will
over-   t the data.

the issue is basically about the number of leaves in the tree. for a given number
of leaves, there is a unique best tree. as we add more leaves, we can only lower
the bias, but we also increase the variance, since we have to estimate more. at
any    nite sample size, then, there is a tree with a certain number of leaves which
will generalize better than any other. we would like to    nd this optimal number
of leaves.

the reason we start with a big (lush? exuberant? spreading?) tree is to make
sure that we   ve got an upper bound on the optimal number of leaves. thereafter,

278

trees

we consider simpler trees, which we obtain by pruning the large tree. at each
pair of leaves with a common parent, we evaluate the error of the tree on the
testing data, and also of the sub-tree, which removes those two leaves and puts a
leaf at the common parent. we then prune that branch of the tree, and so forth
until we come back to the root. starting the pruning from di   erent leaves may give
multiple pruned trees with the same number of leaves; we   ll look at which sub-
tree does best on the testing set. the reason this is superior to arbitrary stopping
criteria, or to rewarding parsimony as such, is that it directly checks whether
the extra capacity (nodes in the tree) pays for itself by improving generalization
error. if it does, great; if not, get rid of the complexity.

there are lots of other cross-validation tricks for trees. one cute one is to
alternate growing and pruning. we divide the data into two parts, as before, and
   rst grow and then prune the tree. we then exchange the role of the training
and testing sets, and try to grow our pruned tree to    t the second half. we then
prune again, on the    rst half. we keep alternating in this manner until the size
of the tree doesn   t change.

13.2.2.1 cross-validation and pruning in r

the tree package contains functions prune.tree and cv.tree for pruning trees
by cross-validation.

the function prune.tree takes a tree you    t by tree, and evaluates the error
of the tree and various prunings of the tree, all the way down to the stump.
the evaluation can be done either on new data, if supplied, or on the training
data (the default). if you ask it for a particular size of tree, it gives you the best
pruning of that size3. if you don   t ask it for the best tree, it gives an object which
shows the number of leaves in the pruned trees, and the error of each one. this
object can be plotted.

my.tree <- tree(y ~ x1 + x2, data = my.data)
prune.tree(my.tree, best = 5)
prune.tree(my.tree, best = 5, newdata = test.set)
my.tree.seq <- prune.tree(my.tree)
plot(my.tree.seq)
my.tree.seq$dev
opt.trees <- which(my.tree.seq$dev == min(my.tree.seq$dev))
min(my.tree.seq$size[opt.trees])

finally, prune.tree has an optional method argument. the default is method="deviance",

which    ts by minimizing the mean squared error (for continuous responses) or
the negative log likelihood (for discrete responses; see below).4

the function cv.tree does k-fold cross-validation (default is k = 10). it re-
quires as an argument a    tted tree, and a function which will take that tree and
new data. by default, this function is prune.tree.
3 or, if there is no tree with that many leaves, the smallest number of leaves     the requested size.
4 with discrete responses, you may get better results by saying method="misclass", which looks at

the misclassi   cation rate.

13.3 classi   cation trees

279

my.tree.cv <- cv.tree(my.tree)

the type of output of cv.tree is the same as the function it   s called on. if i

do

cv.tree(my.tree, best = 19)

i get the best tree (per cross-validation) of no more than 19 leaves. if i do

cv.tree(my.tree)

i get information about the cross-validated performance of the whole sequence
of pruned trees, e.g., plot(cv.tree(my.tree)). optional arguments to cv.tree
can include the number of folds, and any additional arguments for the function
it applies (e.g., any arguments taken by prune).

to illustrate, think back to treefit2, which predicted predicted california
house prices based on geographic coordinates, but had a very large number of
nodes because the tree-growing algorithm was told to split at the least provcation.
figure 13.8 shows the size/performance trade-o   . figures 13.9 and 13.10 show the
result of pruning to the smallest size compatible with minimum cross-validated
error.

13.2.3 uncertainty in regression trees

even when we are making point predictions, we have some uncertainty, because
we   ve only seen a    nite amount of data, and this is not an entirely representative
sample of the underlying id203 distribution. with a regression tree, we
can separate the uncertainty in our predictions into two parts. first, we have
some uncertainty in what our predictions should be, assuming the tree is correct.
second, we may of course be wrong about the tree.

the    rst source of uncertainty     imprecise estimates of the conditional means
within a given partition     is fairly easily dealt with. we can consistently estimate
the standard error of the mean for leaf c just like we would for any other mean
of iid samples. the second source is more troublesome; as the response values
shift, the tree itself changes, and discontinuously so, tree shape being a discrete
variable. what we want is some estimate of how di   erent the tree could have
been, had we just drawn a di   erent sample from the same source distribution.

one way to estimate this, from the data at hand, is to use id64 (ch.
6). it is important that we apply the bootstrap to the predicted values, which
can change smoothly if we make a tiny perturbation to the distribution, and not
to the shape of the tree itself (which can only change abruptly).

13.3 classi   cation trees

classi   cation trees work just like regression trees, only they try to predict a dis-
crete category (the class), rather than a numerical value. the variables which go

280

trees

into the classi   cation     the inputs     can be numerical or categorical themselves,
the same way they can with a regression tree. they are useful for the same reasons
regression trees are     they provide fairly comprehensible predictors in situations
where there are many variables which interact in complicated, nonlinear ways.

we    nd classi   cation trees in almost the same way we found regression trees:
we start with a single node, and then look for the binary distinction which gives
us the most information about the class. we then take each of the resulting new
nodes and repeat the process there, continuing the recursion until we reach some
stopping criterion. the resulting tree will often be too large (i.e., over-   t), so
we prune it back using (say) cross-validation. the di   erences from regression-
tree growing have to do with (1) how we measure information, (2) what kind of
predictions the tree makes, and (3) how we measure predictive error.

13.3.1 measuring information

the response variable y is categorical, so we can use id205 to mea-
sure how much we learn about it from knowing the value of another discrete
variable a:

i[y ; a]    (cid:88)

pr (a = a) i[y ; a = a]

(13.2)

where

i[y ; a = a]     h[y ]     h[y |a = a]

(13.3)

a

and you remember the de   nitions of id178 h[y ] and conditional id178 h[y |a =
a],

   pr (y = y) log2 pr (y = y)

(13.4)

h[y ]    (cid:88)

and

h[y |a = a]    (cid:88)

y

y

   pr (y = y|a = a) log2 pr (y = y|a = a)

(13.5)

i[y ; a = a] is how much our uncertainty about y decreases from knowing that
a = a. (less subjectively: how much less variable y becomes when we go from
the full population to the sub-population where a = a.) i[y ; a] is how much our
uncertainty about y shrinks, on average, from knowing the value of a.

for classi   cation trees, a isn   t (necessarily) one of the predictors, but rather
the answer to some question, generally binary, about one of the predictors x,
i.e., a = 1a(x) for some set a. this doesn   t change any of the math above,
however. so we chose the question in the    rst, root node of the tree so as to
maximize i[y ; a], which we calculate from the formula above, using the relative
frequencies in our data to get the probabilities.

when we want to get good questions at subsequent nodes, we have to take
into account what we know already at each stage. computationally, we do this
by computing the probabilities and informations using only the cases in that

13.3 classi   cation trees

281

node, rather than the complete data set. (remember that we   re doing recursive
partitioning, so at each stage the sub-problem looks just like a smaller version
of the original problem.) mathematically, what this means is that if we reach
the node when a = a and b = b, we look for the question c which maximizes
i[y ; c|a = a, b = b], the information conditional on a = a, b = b. algebraically,
(13.6)

i[y ; c|a = a, b = b] = h[y |a = a, b = b]     h[y |a = a, b = b, c]

computationally, rather than looking at all the cases in our data set, we just look
at the ones where a = a and b = b, and calculate as though that were all the
data. also, notice that the    rst term on the right-hand side, h[y |a = a, b = b],
does not depend on the next question c. so rather than maximizing i[y ; c|a =
a, b = b], we can just minimize h[y |a = a, b = b, c].

13.3.2 making predictions

there are two kinds of predictions which a classi   cation tree can make. one is a
point prediction, a single guess as to the class or category: to say    this is a    ower   
or    this is a tiger    and nothing more. the other, a distributional prediction,
gives a id203 for each class. this is slightly more general, because if we need
to extract a point prediction from a id203 forecast we can always do so, but
we can   t go in the other direction.

for id203 forecasts, each terminal node in the tree gives us a distribution
over the classes. if the terminal node corresponds to the sequence of answers a =
a, b = b, . . . q = q, then ideally this would give us pr (y = y|a = a, b = b, . . . q = q)
for each possible value y of the response. a simple way to get close to this is to
use the empirical relative frequencies of the classes in that node. e.g., if there
are 33 cases at a certain leaf, 22 of which are tigers and 11 of which are    owers,
the leaf should predict    tiger with id203 2/3,    ower with id203 1/3   .
this is the maximum likelihood estimate of the true id203 distribution,

and we   ll write it (cid:99)pr (  ).

incidentally, while the empirical relative frequencies are consistent estimates of
the true probabilities under many circumstances, nothing particularly compells
us to use them. when the number of classes is large relative to the sample size,
we may easily fail to see any samples at all of a particular class. the empirical
relative frequency of that class is then zero. this is good if the actual id203
is zero, not so good otherwise. (in fact, under the negative log-likelihood error
discussed below, it   s in   nitely bad, because we will eventually see that class, but
our model will say it   s impossible.) the empirical relative frequency estimator is
in a sense too reckless in following the data, without allowing for the possibility
that it the data are wrong; it may under-smooth. other id203 estimators
   shrink away    or    back o       from the empirical relative frequencies; exercise 1
involves one such estimator.

for point forecasts, the best strategy depends on the id168. if it is just
the mis-classi   cation rate, then the best prediction at each leaf is the class with
the highest id155 in that leaf. with other id168s, we

282

trees

should make the guess which minimizes the expected loss. but this leads us to
the topic of measuring error.

13.3.3 measuring error

there are three common ways of measuring error for classi   cation trees, or indeed
other classi   cation algorithms: misclassi   cation rate, expected loss, and normal-
ized negative log-likelihood, a.k.a. cross-id178.

we   ve already seen this: it   s the fraction of cases assigned to the wrong class.

13.3.3.1 misclassi   cation rate

13.3.3.2 average loss

the idea of the average loss is that some errors are more costly than others.
for example, we might try classifying cells into    cancerous    or    not cancerous   
based on their gene expression pro   les. if we think a healthy cell from someone   s
biopsy is cancerous, we refer them for further tests, which are frightening and
unpleasant, but not, as the saying goes, the end of the world. if we think a cancer
cell is healthy, th consequences are much more serious! there will be a di   erent
cost for each combination of the real class and the guessed class; write lij for the
cost (   loss   ) we incur by saying that the class is j when it   s really i.
for an observation x, the classi   er gives class probabilities pr (y = i|x = x).

then the expected cost of predicting j is:

loss(y = j|x = x) =

lijpr (y = i|x = x)

(13.7)

(cid:88)

a cost matrix might look as follows

i

prediction

truth    cancer   

   healthy   

   cancer   
   healthy   

0
1

100
0

we run an observation through the tree and wind up with class probabilities
(0.4, 0.6). the most likely class is    healthy   , but it is not the most cost-e   ective
decision. the expected cost of predicting    cancer    is 0.4     0 + 0.6     1 = 0.6, while
the expected cost of predicting    healthy    is 0.4   100+0.6   0 = 40. the id203
of y =    healthy    must be 100 times higher than that of y =    cancer    before
   cancer    is a cost-e   ective prediction.

notice that if our estimate of the class probabilities is very bad, we can go
through the math above correctly, but still come out with the wrong answer. if
our estimates were exact, however, we   d always be doing as well as we could,
given the data.

you can show (exercise 13.6) that if the costs are symmetric, we get the mis-
classi   cation rate back as our error function, and should always predict the most
likely class.

13.3 classi   cation trees

283

13.3.3.3 likelihood and cross-id178

n(cid:88)

i=1

the normalized negative log-likelihood is a way of looking not just at whether the
model made the wrong call, but whether it made the wrong call with con   dence
or tentatively. (   often wrong, never in doubt    is not a good way to go through
life.) more precisely, this id168 for a model q is

l(data, q) =     1
n

log q(y = yi|x = xi)

(13.8)

where q(y = y|x = x) is the id155 the model predicts. if
perfect classi   cation were possible, i.e., if y were a function of x, then the best
classi   er would give the actual value of y a id203 of 1, and l = 0. if there is
some irreducible uncertainty in the classi   cation, then the best possible classi   er
would give l = h[y |x], the conditional id178 of y given the inputs x. less-
than-ideal predictors have l > h[y |x]. to see this, try re-write l so we sum
over values rather than data-points:

l =     1
n

x,y

n (y = y, x = x) log q(y = y|x = x)

(cid:88)
=    (cid:88)
(cid:99)pr (y = y, x = x) log q(y = y|x = x)
=    (cid:88)
(cid:99)pr (x = x)(cid:99)pr (y = y|x = x) log q(y = y|x = x)
=    (cid:88)
(cid:99)pr (x = x)
(cid:99)pr (y = y|x = x) log q(y = y|x = x)

(cid:88)

x,y

x,y

x

y

if the quantity in the log was pr (y = y|x = x), this would be h[y |x]. since
it   s the model   s estimated id203, rather than the real id203, it turns
out that this is always larger than the conditional id178. l is also called the
cross-id178 for this reason.

there is a slightly subtle issue here about the di   erence between the in-sample
loss, and the expected generalization error or risk. n (y = y, x = x)/n =

(cid:99)pr (y = y, x = x), the empirical relative frequency or empirical id203. the

law of large numbers says that this converges to the true id203, n (y =
y, x = x)/n     pr (y = y, x = x) as n        . consequently, the model which
minimizes the cross-id178 in sample may not be the one which minimizes it on
future data, though the two ought to converge. generally, the in-sample cross-
id178 is lower than its expected value.
notice that to compare two models, or the same model on two di   erent data
sets, etc., we do not need to know the true conditional id178 h[y |x]. all we
need to know is that l is smaller the closer we get to the true class probabilities.
if we could get l down to the cross-id178, we would be exactly reproducing
all the class probabilities, and then we could use our model to minimize any loss
function we liked (as we saw above).5

5 technically, if our model gets the class probabilities right, then the model   s predictions are just as

284

trees

13.3.3.4 neyman-pearson approach

using a id168 which assigns di   erent weights to di   erent error types has
two noticeable drawbacks. first of all, we have to pick the weights, and this is
often quite hard to do. second, whether our classi   er will do well in the future
depends on getting the same proportion of cases in the future. suppose that we   re
developing a tree to classify cells as cancerous or not from their gene expression
pro   les. we will probably want to include lots of cancer cells in our training
data, so that we can get a good idea of what cancers look like, biochemically.
but, fortunately, most cells are not cancerous, so if doctors start applying our
test to their patients, they   re going to    nd that it massively over-diagnoses cancer
    it   s been calibrated to a sample where the proportion (cancer):(healthy) is, say,
1:1, rather than, say, 1:20.6

there is an alternative to weighting which deals with both of these issues, and
deserves to be better known and more widely-used than it is. this was introduced
by scott and nowak (2005), under the name of the    neyman-pearson approach   
to statistical learning. the reasoning goes as follows.

when we do a binary classi   cation problem, we   re really doing a hypothesis
test, and the central issue in hypothesis testing, as    rst recognized by neyman
and pearson, is to distinguish between the rates of di   erent kinds of errors: false
positives and false negatives, false alarms and misses, type i and type ii. the
neyman-pearson approach to designing a hypothesis test is to    rst    x a limit on
the false positive id203, the size of the test, canonically   . then, among
all tests of size   , we want to minimize the false negative rate, or equivalently
maximize the power,   .

in the traditional theory of testing, we know the distribution of the data under
the null and alternative hypotheses, and so can (in principle) calculate    and   
for any given test. this is not the case in many applied problems, but then we
often do have large samples generated under both distributions (depending on
the class of the data point). if we    x   , we can ask, for any classi   er     say, a tree
    whether its false alarm rate is       . if so, we keep it for further consideration;
if not, we discard it. among those with acceptable false alarm rates, then, we ask
   which classi   er has the lowest false negative rate, the highest   ?    this is the
one we select.

notice that this solves both problems with weighting. we don   t have to pick a
weight for the two errors; we just have to say what rate of false positives    we   re
willing to accept. there are many situations where this will be easier to do than to
   x on a relative cost. second, the rates    and    are properties of the conditional

informative as the original data. we then say that the predictions are a su   cient statistic for
forecasting the class. in fact, if the model gets the exact probabilities wrong, but has the correct
partition of the feature space, then its prediction is still a su   cient statistic. under any loss
function, the optimal strategy can be implemented using only a su   cient statistic, rather than
needing the full, original data. this is an interesting but much more advanced topic; see, e.g.,
blackwell and girshick (1954) for details.

6 cancer is rarer than that, but realistically doctors aren   t going to run a test like this unless they

have some reason to suspect cancer might be present.

13.4 further reading

285
distributions of the features, pr (x|y ). if those conditional distributions stay
they same but the proportions of the classes change, then the error rates are
una   ected. thus, training the classi   er with a di   erent mix of cases than we   ll
encounter in the future is not an issue.

unfortunately, i don   t know of any r implementation of neyman-pearson
learning; it wouldn   t be hard, i think, but goes beyond one problem set at this
level.

13.4 further reading

the classic book on prediction trees, which basically introduced them into statis-
tics and data mining, is breiman et al. (1984). chapter three in berk (2008) is
clear, easy to follow, and draws heavily on breiman et al. another very good
chapter is the one on trees in ripley (1996), which is especially useful for us be-
cause ripley wrote the tree package. (the whole book is strongly recommended.)
there is another tradition of trying to learn tree-structured models which comes
out of arti   cial intelligence and inductive logic; see mitchell (1997).

the clearest explanation of the neyman-pearson approach to hypothesis test-
ing i have ever read is that in reid (1982), which is one of the books which made
me decide to learn statistics.

exercises

13.1 repeat the analysis of the california house-price data with the pennsylvania data from

13.2 explain why, for a    xed partition, a regression tree is a linear smoother.

estimate of the id203 of the ith class is (cid:98)pi = ni/n. suppose that instead we use the

i=1 ni = n. the maximum likelihood

estimates

problem set a.13.

13.3 suppose that we see each of k classes ni times, with(cid:80)k
(cid:0)nj + 1(cid:1)
1. show that(cid:80)k
2. show that if(cid:98)p     p as n        , then   p     p as well.
4. does   p     p imply(cid:98)p     p?

i   pi = 1, no matter what the sample is.

(cid:80)k

ni + 1

  pi =

j=1

3. using the result of the previous part, show that if we observe an iid sample, that

  p     p, i.e., that   p is a consistent estimator of the true distribution.

this estimator goes back to laplace, who called it the    rule of succession   .

(13.9)

5. which of these properties still hold if the +1s in the numerator and denominator are

replaced by +d for an arbitrary d > 0?

13.4 fun with laplace   s rule of succession: will the sun rise tomorrow? one illustration laplace
gave of the id203 estimator in eq. 13.9 was the following. suppose we know, from
written records, that the sun has risen in the east every day for the last 4000 years.7

7 laplace was thus ignoring people who live above the artic circle, or below the antarctic circle. the

latter seems particularly unfair, because so many of them are scientists.

286

trees

1. calculate the id203 of the event    the sun will rise in the east tomorrow   , using

eq. 13.9. you may take the year as containing 365.256 days.

2. calculate the id203 that the sun will rise in the east every day for the next four
thousand years, assuming this is an iid event each day. is this a reasonable assumption?
3. calculate the id203 of the event    the sun will rise in the east every day for four
thousand years    directly from eq. 13.9, treating that as a single event. why does your
answer here not agree with that of part (b)?

(laplace did not, of course, base his belief that the sun will rise in the morning on such
calculations; besides everything else, he was the world   s expert in celestial mechanics! but
this shows a problem with the    rule of succession   .)

13.5 it   s reasonable to wonder why we should measure the complexity of a tree by just the
number of leaves it has, rather than by the total number of nodes. show that for a binary
tree, with |t| leaves, the total number of nodes (including the leaves) is 2|t|     1. (thus,
controlling the number of leaves is equivalent to controlling the number of nodes.)

13.6 show that, when all the o   -diagonal elements of lij (from   13.3.3.2) are equal (and

positive!), the best class to predict is always the most probable class .

exercises

287

treefit2.cv <- cv.tree(treefit2)
plot(treefit2.cv)

figure 13.8 size (horizontal axis) versus cross-validated sum of squared
errors (vertical axis) for successive prunings of the treefit2 model. (the
upper scale on the horizontal axis refers to the    cost/complexity    penalty.
the idea is that the pruning minimizes (total error) +   (complexity) for a
certain value of   , which is what   s shown on that scale. here complexity is
taken to just be the number of leaves in the tree, i.e., its size (though
sometimes other measures of complexity are used).    then acts as a
lagrange multiplier (  h.3.2) which enforces a constraint on the complexity
of the tree. see ripley (1996,   7.2, pp. 221   226) for details.

sizedeviance3500400045005000550060006500110203040506068670.0100.0 66.0 32.0 26.0 16.0 14.0  9.6  8.4  7.7288

trees

opt.trees <- which(treefit2.cv$dev == min(treefit2.cv$dev))
best.leaves <- min(treefit2.cv$size[opt.trees])
treefit2.pruned <- prune.tree(treefit2, best = best.leaves)
plot(treefit2.pruned)
text(treefit2.pruned, cex = 0.75)

figure 13.9 treefit2, after being pruned by ten-fold cross-validation.

|latitude < 38.485longitude <    121.655latitude < 37.925latitude < 34.675longitude <    118.315latitude < 34.165longitude <    118.365longitude <    117.545latitude < 33.725latitude < 34.105longitude <    118.165latitude < 33.59longitude <    116.33longitude <    117.165longitude <    120.275latitude < 39.35512.4812.1012.8612.3812.3712.5411.9212.2012.4012.3811.9511.1611.6311.7511.2811.7311.32exercises

289

plot(calif$longitude, calif$latitude, col = grey(10:2/11)[cut.prices], pch = 20,

xlab = "longitude", ylab = "latitude")

partition.tree(treefit2.pruned, ordvars = c("longitude", "latitude"), add = true,

cex = 0.3)

figure 13.10 treefit2.pruned   s partition of california. compare to
figure 13.5.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   124   122   120   118   116   1143436384042longitudelatitude12.512.112.912.412.412.511.912.212.412.411.911.211.611.811.311.711.3part ii

distributions and latent structure

291

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

14

estimating distributions and densities

we have spent a lot of time looking at how to estimate expectations (which
is regression). we have also seen how to estimate variances, by turning it into
a problem about expectations. we could extend the same methods to looking
at higher moments     if you need to    nd the conditional skewness or kurtosis
functions1, you can tackle that in the same way as    nding the conditional variance.
but what if we want to look at the whole distribution?

you   ve already seen one solution to this problem in earlier statistics courses:
posit a parametric model for the density (gaussian, student   s t, exponential,
gamma, beta, pareto, . . . ) and estimate the parameters. maximum likelihood
estimates are generally consistent and e   cient for such problems. appendix e
reminded us of how this machinery can be extended to multivariate data. but
suppose you don   t have any particular parametric density family in mind, or
want to check one     how could we estimate a id203 distribution non-
parametrically?

14.1 histograms revisited

for most of you, making a histogram was probably one of the    rst things you
learned how to do in intro stats (if not before). this is a simple way of estimating
a distribution: we split the sample space up into bins, count how many samples
fall into each bin, and then divide the counts by the total number of samples. if
we hold the bins    xed and take more and more data, then by the law of large
numbers we anticipate that the relative frequency for each bin will converge on
the bin   s id203.

so far so good. but one of the things you learned in intro stats was also to work
with id203 density functions, not just id203 mass functions. where do
we get pdfs? well, one thing we could do is to take our histogram estimate, and
then say that the id203 density is uniform within each bin. this gives us a
piecewise-constant estimate of the density.

unfortunately, this isn   t going to work     isn   t going to converge on the true pdf
    unless we can shrink the bins of the histogram as we get more and more data.
to see this, think about estimating the pdf when the data comes from any of the
standard distributions, like an exponential or a gaussian. we can approximate

1 when you    nd out what the kurtosis is good for, be sure to tell the world.

293

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

n(cid:88)

  fn(a)     1
n

294

density estimation

the true pdf f (x) to arbitrary accuracy by a piecewise-constant density (indeed,
that   s what happens every time we plot it on our screens), but, for a    xed set of
bins, we can only come so close to the true, continuous density.

this reminds us of our old friend the bias-variance trade-o   , and rightly so. if
we use a large number of very small bins, the minimum bias in our estimate of
any density becomes small, but the variance in our estimates grows. (why does
variance increase?) to make some use of this insight, though, there are some
things we need to establish    rst.
    is learning the whole distribution non-parametrically even feasible?
    how can we measure error so deal with the bias-variance trade-o   ?

14.2    the fundamental theorem of statistics   

let   s deal with the    rst point    rst. in principle, something even dumber than
shrinking histograms will work to learn the whole distribution. suppose we have
one-dimensional samples x1, x2, . . . xn with a common cumulative distribution
function f . the empirical cumulative distribution function on n samples,
  fn(a) is

(14.1)
in words, this is just the fraction of the samples which are     a. then the
glivenko-cantelli theorem says

1(      ,a])(xi)

i=1

|   fn(a)     f (a)|     0

max

a

(14.2)

so the empirical cdf converges to the true cdf everywhere; the maximum
gap between the two of them goes to zero. pitman (1979) calls this the    fun-
damental theorem of statistics   , because it says we can learn distributions just
by collecting enough data.2 the same kind of result also holds for the cdfs of
higher-dimensional vectors.

2 there are some interesting aspects to the theorem which are tangential to what we   ll need, so i will
stick them in this footnote. these hinge on the max in the statement of the theorem. for any one,
   xed value of a, that |   fn(a)     f (a)|     0 is just an application of the law of large numbers. the
extra work glivenko and cantelli did was to show that this held for in   nitely many values of a at
once, so that even if we focus on the biggest gap between the estimate and the truth, that still
shrinks with n. here   s a sketch, with no details. fix an   > 0;    rst show that there is some    nite set
of points on the line, call them b1, . . . bm( ), such that, for any a, |   fn(a)       fn(bi)| <   and
|f (a)     f (bi)| <   for some bi. next, show that, for large enough n, |f (bi)       fn(bi)| <   for all the
bi simultaneously. (this follows from the law of large numbers and the fact that m( ) is    nite.)
finally, use the triangle inequality to conclude that, for large enough n, maxa |   fn(a)     f (a)| < 3 .
since   can be made arbitrarily small, the glivenko-cantelli theorem follows. this general strategy
    combining pointwise convergence theorems with approximation arguments     forms the core of
what   s called empirical process theory, which underlies the consistency of basically all the
non-parametric procedures we   ve seen. if this line of thought is at all intriguing, the closest thing to
a gentle introduction is pollard (1989). (if you know enough to object that i should have been
writing sup instead of max, you know enough to make the substitution for yourself.)

14.3 error for density estimates

295

if the glivenko-cantelli theorem is so great, why aren   t we just content with
the empirical cdf? sometimes we are, but it inconveniently doesn   t give us a
id203 density. suppose that x1, x2, . . . xn are sorted into increasing order.
what id203 does the empirical cdf put on the interval (xi, xi+1)? clearly,
zero. (whereas the interval [xi, xi+1] gets id203 2/n.) this could be right,
but we have centuries of experience now with id203 distributions, and this
tells us that pretty often we can expect to    nd some new samples between our
old ones. so we   d like to get a non-zero density between our observations.

using a uniform distribution within each bin of a histogram doesn   t have this
issue, but it does leave us with the problem of picking where the bins go and how
many of them we should use. of course, there   s nothing magic about keeping the
bin size the same and letting the number of points in the bins vary; we could
equally well pick bins so they had equal counts.3 so what should we do?

14.3 error for density estimates

2

our    rst step is to get clear on what we mean by a    good    density estimate.
there are three leading ideas:

1. (cid:82) (f (x)       f (x))
2. (cid:82) |f (x)       f (x)|dx should be small: minimize the average absolute, rather than
3. (cid:82) f (x) log f (x)(cid:98)f (x)

dx should be small: the average log-likelihood ratio should be

dx should be small: the squared deviation from the true den-

sity should be small, averaging evenly over all space.

squared, deviation.

kept low.

option (1) is reminiscent of the mse criterion we   ve used in regression. option
(2) looks at what   s called the l1 or total variation distance between the true and
the estimated density. it has the nice property that 1
2
the maximum error in our estimate of the id203 of any set. unfortunately
it   s a bit tricky to work with, so we   ll skip it here. (but see devroye and lugosi
(2001)). finally, minimizing the log-likelihood ratio is intimately connected to
maximizing the likelihood. we will come back to this (  14.6), but, like most texts
on density estimation, we will give more attention to minimizing (1), because it   s
mathematically tractable.

(cid:82) |f (x)       f (x)|dx is exactly

3 a speci   c idea for how to do this is sometimes called a k     d tree. we have d random variables and

want a joint density for all of them. fix an ordering of the variables start with the    rst variable,
and    nd the thresholds which divide it into k parts with equal counts. (usually but not always
k = 2.) then sub-divide each part into k equal-count parts on the second variable, then sub-divide
each of those on the third variable, etc. after splitting on the dth variable, go back to splitting on
the    rst, until no further splits are possible. with n data points, it takes about logk n splits before
coming down to individual data points. each of these will occupy a cell of some volume. estimate
the density on that cell as one over that volume. of course it   s not strictly necessary to keep re   ning
all the way down to single points.

296

(cid:90)

notice that

(cid:90)

density estimation

(f (x)       f (x))

2

dx =

f 2(x)dx     2

  f (x)f (x)dx +

(cid:90)

  f 2(x)dx

(14.3)

the    rst term on the right hand side doesn   t depend on the estimate   f (x) at all,
so we can ignore it for purposes of optimization. the third one only involves   f ,
and is just an integral, which we can do numerically. that leaves the middle term,
which involves both the true and the estimated density; we can approximate it
by

  f (xi)

(14.4)

the reason we can do this is that, by the glivenko-cantelli theorem, integrals over
the true density are approximately equal to sums over the empirical distribution.

(cid:90)

(cid:90)

n(cid:88)

i=1

    2
n

so our    nal error measure is
    2
n

n(cid:88)

i=1

  f (xi) +

  f 2(x)dx

(14.5)

in fact, this error measure does not depend on having one-dimension data; we
can use it in any number of dimensions.4 for purposes of cross-validation (you
knew that was coming, right?), we can estimate   f on the training set, and then
restrict the sum to points in the testing set.

14.3.1 error analysis for histogram density estimates

we now have the tools to do most of the analysis of histogram density estimation.
(we   ll do it in one dimension for simplicity.) choose our favorite location x, which
lies in a bin whose boundaries are x0 and x0 + h. we want to estimate the density
at x, and this is

1(x0,x0+h](xi)

(14.6)

let   s call the sum, the number of points in the bin, b. it   s a random quantity,
b     binomial(n, p), where p is the true id203 of falling into the bin, p =
f (x0 + h)     f (x0). the mean of b is np, and the variance is np(1     p), so

n(cid:88)

i=1

  fn(x) =

1
h

1
n

e(cid:104)   fn(x)

(cid:105)

=

=

=

e [b]

1
nh
n[f (x0 + h)     f (x0)]
f (x0 + h)     f (x0)

nh

h

(14.7)

(14.8)

(14.9)

4 admittedly, in high-dimensional spaces, doing the    nal integral can become numerically challenging.

and the variance is

v(cid:104)   fn(x)

(cid:105)

=

14.3 error for density estimates

297

v [b]

1
n2h2
n[f (x0 + h)     f (x0)][1     f (x0 + h) + f (x0)]

(cid:105) 1     f (x0 + h) + f (x0)

n2h2

(14.10)

(14.11)

(14.12)

=

= e(cid:104)   fn(x)
(cid:105)     lim
e(cid:104)   fh(x)

h   0

nh

f (x0 + h)     f (x0)

h

= f (x0)

(14.13)

if we let h     0 as n        , then

since the pdf is the derivative of the cdf. but since x is between x0 and x0 + h,
f (x0)     f (x). so if we use smaller and smaller bins as we get more data, the
histogram density estimate is unbiased. we   d also like its variance to shrink as
the same grows. since 1     f (x0 + h) + f (x0)     1 as h     0, to get the variance
to go away we need nh        .
to put this together, then, our    rst conclusion is that histogram density esti-
mates will be consistent when h     0 but nh         as n        . the bin-width h
needs to shrink, but slower than n   1.

at what rate should it shrink? small h gives us low bias but (as you can
verify from the algebra above) high variance, so we want to    nd the trade-o   

between the two. one can calculate the bias at x from our formula for e(cid:104)   fh(x)

(cid:105)

through a somewhat lengthy calculus exercise, analogous to what we did for kernel
smoothing in chapter 45; the upshot is that the integrated squared bias is

(cid:90) (cid:16)

f (x)     e(cid:104)   fh(x)
(cid:90)

(cid:90)

dx =

h2
12

(cid:105)(cid:17)2
v(cid:104)   fh(x)
(cid:105)
(cid:90)

dx =

ise =

h2
12

(f(cid:48)(x))2dx +

(f(cid:48)(x))2dx + o(h2)

(14.14)

+ o(n   1)

(14.15)

+ o(h2) + o(n   1)

(14.16)

1
nh

1
nh

we already got the variance at x, and when we integrate that over x we    nd

so the total integrated squared error is

di   erentiating this with respect to h and setting it equal to zero, we get

hopt

(cid:90)
(cid:82) (f(cid:48)(x))2dx

6

6

(cid:32)

(f(cid:48)(x))2dx =

1
nh2

opt

(cid:33)1/3

hopt =

n   1/3 = o(n   1/3)

(14.17)

(14.18)

5 you need to use the intermediate value theorem multiple times; see for instance wasserman (2006,

sec. 6.8).

298

so we need narrow bins if the density changes rapidly ((cid:82) (f(cid:48)(x))2dx is large), and

density estimation

wide bins if the density is relatively    at. no matter how rough the density, the
bin width should shrink like o(n   1/3). plugging that rate back into the equation
for the ise, we see that it is o(n   2/3).

it turns out that if we pick h by cross-validation, then we attain this optimal
rate in the large-sample limit. by contrast, if we knew the correct parametric
form and just had to estimate the parameters, we   d typically get an error decay
of o(n   1). this is substantially faster than histograms, so it would be nice if we
could make up some of the gap, without having to rely on parametric assumptions.

14.4 kernel density estimates

it turns out that one can improve the convergence rate, as well as getting smoother
estimates, by using kernels. the kernel density estimate is

(cid:98)fh(x) =

n(cid:88)

i=1

1
n

(cid:18) x     xi

(cid:19)

h

1
h

k

(14.19)

regression. (the factor of 1/h inside the sum is so that (cid:98)fh will integrate to 1;

where k is a id81 such as we encountered when looking at kernel

we could have included it in both the numerator and denominator of the kernel
regression formulae, but then it would   ve just canceled out.) as before, h is the
bandwdith of the kernel. we   ve seen typical kernels in things like the gaussian.
one advantage of using them is that they give us a smooth density everywhere,
unlike histograms, and in fact we can even use them to estimate the derivatives
of the density, should that be necessary.6

14.4.1 analysis of kernel density estimates

how do we know that kernels will in fact work? well, let   s look at the mean and
variance of the kernel density estimate at a particular point x, and use taylor   s

6 the advantage of histograms is that they   re computationally and mathematically simpler.

theorem on the density.

e(cid:104)(cid:98)fh(x)
(cid:105)

14.4 kernel density estimates

299

(cid:19)(cid:21)

h

h

i=1

e

k

(cid:20) 1
(cid:18) x     xi
(cid:19)(cid:21)
(cid:18) x     x
(cid:18) x     t
(cid:19)
(cid:20)

k
f (t)dt
h
k(u)f (x     hu)du

1
n

n(cid:88)
(cid:20) 1
(cid:90) 1
(cid:90)
(cid:90)

k

h

h

h

=

= e

=

=

=

k(u)

f (x)     huf(cid:48)(x) +
h2f(cid:48)(cid:48)(x)

(cid:90)

(14.20)

(14.21)

(14.22)

(14.23)

du (14.24)

(14.25)

(cid:21)

f(cid:48)(cid:48)(x) + o(h2)

h2u2

2

= f (x) +

2

k(u)u2du + o(h2)

because, by de   nition,(cid:82) k(u)du = 1 and(cid:82) uk(u)du = 0. if we call(cid:82) k(u)u2du =

(14.26)

  2
k, then the bias of the kernel density estimate is
kf(cid:48)(cid:48)(x)
2

h2  2

+ o(h2)

(14.27)

so the bias will go to zero if the bandwidth h shrinks to zero. what about the
variance? use taylor   s theorem again:

v(cid:104)(cid:98)fh(x)

(cid:105)

(cid:19)(cid:21)(cid:19)2(cid:35)

e

   

(cid:18)

(cid:20) 1

(cid:18) x     x
(cid:19)(cid:21)
dt    (cid:2)f (x) + o(h2)(cid:3)2(cid:21)
(cid:21)

k

h

h

h

h2
k 2(u)f (x     hu)du     f 2(x) + o(h2)

k 2(u) (f (x)     huf(cid:48)(x)) du     f 2(x) + o(h)

(14.28)

(14.29)

(14.30)

(14.31)

(14.32)

(cid:21)

(cid:105)     f (x) =
(cid:19)(cid:21)
(cid:18) x     x
(cid:18) x     x
(cid:18) x     t
(cid:19)

k 2

h

h

k 2

k

h

v

1
n

e(cid:104)(cid:98)fh(x)
(cid:20) 1
(cid:34)
(cid:20) 1
(cid:20)(cid:90) 1
(cid:20)(cid:90) 1
(cid:20)(cid:90) 1
(cid:90)

1
n
1
n
1
n
f (x)
hn

1
n

e

h

h

h2

=

=

=

=

=

(cid:90)

h4  4

k(f(cid:48)(cid:48)(x))2

4

+

f (x)
hn

=

(14.33)
this will go to zero if nh         as n        . so the conclusion is the same as for
histograms: h has to go to zero, but slower than 1/n.

k 2(u)du + o(1/n)

since the expected squared error at x is the bias squared plus the variance,

k 2(u)du + small

(14.34)

di   erentiating with respect to h for the optimal bandwidth hopt, we    nd

300

density estimation

the expected integrated squared error is

(cid:90)

(cid:82) k 2(u)du
(cid:82) k 2(u)du

nh

nh2

opt

ise     h4  4
4

k

(f(cid:48)(cid:48)(x))2dx +

(f(cid:48)(cid:48)(x))2dx =

h3
opt  4

(cid:90)
(cid:18) (cid:82) k 2(u)du
(cid:82) (f(cid:48)(cid:48)(x))2dx

k

  4
k

(cid:19)1/5

hopt =

n   1/5 = o(n   1/5)

(14.37)

(14.35)

(14.36)

that is, the best bandwidth goes to zero like one over the    fth root of the number
of sample points. plugging this into eq. 14.35, the best ise = o(n   4/5). this
is better than the o(n   2/3) rate of histograms, but still includes a penalty for
having to    gure out what kind of distribution we   re dealing with. remarkably
enough, using cross-validation to pick the bandwidth gives near-optimal results.7
as an alternative to cross-validation, or at least a starting point, one can use eq.
14.37 to show that the optimal bandwidth for using a gaussian kernel to estimate
a gaussian distribution is 1.06  n   1/5, with    being the standard deviation of the
gaussian. this is sometimes called the gaussian reference rule or the rule-
of-thumb bandwidth. when you call density in r, this is basically what it
does.

yet another technique is the plug-in method. eq. 14.37 calculates the optimal
bandwidth from the second derivative of the true density. this doesn   t help if we
don   t know the density, but it becomes useful if we have an initial density estimate
which isn   t too bad. in the plug-in method, we start with an initial bandwidth
(say from the gaussian reference rule) and use it to get a preliminary estimate of
the density. taking that crude estimate and    plugging it in    to eq. 14.37 gives
us a new bandwidth, and we re-do the kernel estimate with that new bandwidth.
iterating this a few times is optional but not uncommon.

14.4.2 joint density estimates

the discussion and analysis so far has been focused on estimating the distribution
of a one-dimensional variable. just as kernel regression can be done with multiple
input variables (  4.3), we can make kernel density estimates of joint distributions.
we simply need a kernel for the vector:

k((cid:126)x     (cid:126)xi)

(14.38)

(cid:98)f ((cid:126)x) =

n(cid:88)
(cid:0)(cid:82) (f(cid:48)(cid:48)(x))2dx(cid:1)1/5(cid:0)(cid:82) k2(u)du(cid:1)4/5

1
n

i=1

7 substituting eq. 14.37 into eq. 14.35 gives a squared error of

kernel are   k and(cid:82) k2(u)du. this is the source of the (correct) folklore that the choice of kernel is

. the only two parts of this which depend on the

1.25n   4/5  4/5

k

less important than the choice of bandwidth.

14.4 kernel density estimates

301

one could use any multivariate distribution as the kernel (provided it is centered
and has    nite covariance). typically, however, just as in smoothing, one uses a
product kernel, i.e., a product of one-dimensional kernels,

k((cid:126)x     (cid:126)xi) = k1(x1     x1

i ) ,

(14.39)

doing this requires a bandwidth for each coordinate, so the over-all form of the

i )k2(x2     x2
d(cid:89)
n(cid:88)

i ) . . . kp(xp     xp
(cid:33)
(cid:32)

xj     xj

i

kj

i=1

j=1

hj

n(cid:81)p

1
j=1 hj

joint pdf estimate is(cid:98)f ((cid:126)x) =

(14.40)

going through a similar analysis for p-dimensional data shows that the ise
goes to zero like o(n   4/(4+p)), and again, if we use cross-validation to pick the
bandwidths, asymptotically we attain this rate. unfortunately, if p is large, this
rate becomes very slow     for instance, if p = 24, the rate is o(n   1/7). there is
simply no universally good way to learn arbitrary high-dimensional distributions.
this is the same    curse of dimensionality    we saw in regression (  8.3). the fun-
damental problem is that in high dimensions, there are just too di   erent possible
distributions which are too hard to tell apart.

evading the curse of dimensionality for density estimation needs some special
assumptions. parametric models make the very strong assumption that we know
exactly what the distribution function looks like, and we just need to    ll in a few
constants. it   s potentially less drastic to hope the distribution has some sort of
special structure we can exploit, and most of the rest of part ii will be about
searching for various sorts of useful structure8. if none of these options sound
appealing, or plausible, we   ve got little alternative but to accept a very slow
convergence of density estimates.

14.4.3 categorical and ordered variables

estimating id203 mass functions with discrete variables can be straightfor-
ward: there are only a    nite number of values, and so one just counts how often
they occur and takes the relative frequency. if one has a discrete variable x and
a continuous variable y and one wants a joint distribution, one could just get a
separate density for y for each value of x, and tabulate the probabilities for x.

in principle, this will work, but it can be practically awkward if the number
of levels for the discrete variable is large compared to the number of samples.
moreover, for the joint distribution problem, it has us estimating completely sep-
arate distributions for y for every x, without any sharing of information between
them. it would seem more plausible to smooth those distributions towards each
others. to do this, we need kernels for discrete variables.

several sets of such kernels have been proposed. the most straightforward,

8 as wiener (1956), the reason the ability to do nonparametric estimation doesn   t make scienti   c

theories redundant is that good theories usefully constrain the distributions we   re searching for, and
tell us what structures to look for.

302

density estimation

however, are the following. if x is a categorical, unordered variable with c possible
values, then, for 0     h < 1,

k(x1, x2) =

(cid:26) 1     h
x1 = x2
h/(c     1) x1 (cid:54)= x2
(cid:33)
|x1     x2|

c

(cid:32)

k(x1, x2) =

h|x1   x2|(1     h)c   |x1   x2|

(14.41)

(14.42)

is a valid kernel. for an ordered x,

where |x1     x2| should be understood as just how many levels apart x1 and x2
are. as h     0, both of these become indicators, and return us to simple relative
frequency counting. both of these are implemented in np.

14.4.4 practicalities

the standard r function density implements one-dimensional kernel density
estimation, defaulting to gaussian kernels with the rule-of-thumb bandwidth.
there are some options for doing cleverer bandwidth selection, including a plug-
in rule. (see the help    le.)

for more sophisticated methods, and especially for more dimensions, you   ll
need to use other packages. the np package estimates joint densities using the
npudens function. (the u is for    unconditional   .) this has the same sort of
automatic bandwidth selection as npreg, using cross-validation. other packages
which do kernel density estimation include kernsmooth and sm.

14.4.5 kernel density estimation in r: an economic example

the data set oecdpanel, in the np library, contains information about much
the same sort of variables at the penn world tables data you worked with in
the homework, over much the same countries and years, but with some of the
variables pre-transformed, with identifying country information removed, and
slightly di   erent data sources. see help(oecdpanel) for details.

here   s an example of using npudens with variables from the oecdpanel data
set, from problem set a.14. we   ll look at the joint density of popgro (the loga-
rithm of the population growth rate) and inv (the logarithm of the investment
rate). figure 14.1 illustrates how to call the command, and a useful trick where
we get np   s plotting function to do our calculations for us, but then pass the re-
sults to a di   erent graphics routine. (see help(npplot).) the distribution we get
has two big modes, one at a comparatively low population growth rate (       2.9
    remember this is logged so it   s not actually a shrinking population) and high
investment (       1.5), and the other at a lower rate of investment (       2) and
higher population growth (       2.6). there is a third, much smaller mode at high
population growth (       2.7) and very low investment (       4).

14.4 kernel density estimates

303

data(oecdpanel)
popinv <- npudens(~popgro + inv, data = oecdpanel)
fhat <- plot(popinv, plot.behavior = "data")$d1
library(lattice)
contourplot(fhat$dens ~ fhat$eval$var1 * fhat$eval$var2, cuts = 20, xlab = "popgro",

ylab = "inv", labels = list(cex = 0.5))

figure 14.1 gaussian kernel estimate of the joint distribution of logged
population growth rate (popgro) and investment rate (inv). notice that
npudens takes a formula, but that there is no dependent variable on the
left-hand side of the    . with objects produced by the np library, one can
give the plotting function the argument plot.behavior     the default is
plot, but if it   s set to data (as here), it calculates all the information needed
to plot and returns a separate set of objects, which can be plotted in other
functions. (the value plot-data does both.) see help(npplot) for more.

popgroinv   4   3   2   1   3.4   3.2   3.0   2.8   2.60.10.10.10.20.20.30.40.50.60.70.80.91.01.11.21.31.31.41.41.51.51.61.61.71.81.92.02.12.2304

density estimation

14.5 conditional density estimation

in addition to estimating marginal and joint densities, we will often want to get
conditional densities. the most straightforward way to get the density of y given
x, fy |x(y | x), is

(cid:98)fy |x(y | x) =

(cid:98)fx,y (x, y)
(cid:98)fx(x)

(14.43)

(14.44)

(14.45)

i.e., to estimate the joint and marginal densities and divide one by the other.

to be concrete, let   s suppose that we are using a product kernel to estimate

the joint density, and that the marginal density is consistent with it:

(cid:98)fx,y (x, y) =
(cid:98)fx(x) =

(cid:19)

n(cid:88)
n(cid:88)

i=1

(cid:18) x     xi
(cid:18) x     xi
(cid:19)

hx

kx

kx

hx

i=1

1

nhxhy

1

nhx

(cid:18) y     yi

(cid:19)

hy

ky

thus we need to pick two bandwidths, hx and hy , one for each variable.

hx and hy so as to minimize the integrated squared error for (cid:98)fx,y , and then

this might seem like a solved problem     we just use cross-validation to    nd

plug in to equation 14.43. however, this is a bit hasty, because the optimal
bandwidths for the joint density are not necessarily the optimal bandwidths for
the conditional density. an extreme but easy to understand example is when y
is actually independent of x. since the density of y given x is just the density
of y , we   d be best o    just ignoring x by taking hx =    . (in practice, we   d just
use a very big bandwidth.) but if we want to    nd the joint density, we would not
want to smooth x away completely like this.

the appropriate integrated squared error measure for the conditional density

(cid:90)

(cid:90)

(cid:16)
fy |x(y | x)     (cid:98)fy |x(y | x)

(cid:17)2

dxfx(x)

dy

(14.46)

is

and this is what we want to minimize by picking hx and hy . the cross-validation
goes as usual.

one nice, and quite remarkable, property of cross-validation for conditional
density estimation is that it can detect and exploit conditional independence.
say that x = (u, v ), and that y is independent of u given v     symbolically,
y        u | v . then fy |u,v (y | u, v) = fy |v (y | v), and we should just ignore u in
is used to pick bandwidths for conditional density estimation, (cid:99)hu         when
our estimation of the conditional density. it turns out that when cross-validation
y        u | v , but not otherwise (hall et al., 2004). in other words, cross-validation
will automatically detect which variables are irrelevant, and smooth them away.

14.6 more on the expected log-likelihood ratio

305

14.5.1 practicalities and a second example

the np package implements kernel conditional density estimation through the
function npcdens. the syntax is pretty much exactly like that of npreg, and
indeed we can think of estimating the conditional density as a sort of regression,
where the dependent variable is actually a distribution.

to give a concrete example, let   s look at how the distribution of countries   
population growth rates has changed over time, using the oecdpanel data (figure
14.2). the selected bandwidth for year is 10, while that for popgro is 0.048. (note
that year is being treated as a continuous variable.)

you can see from the    gure that the mode for population growth rates is
towards the high end of observed values, but the mode is shrinking and becoming
less pronounced over time. the distribution in fact begins as clearly bimodal, but
the smaller mode at the lower growth rate turns into a continuous    shoulder   .
over time, figure 14.2 population growth rates tend to shrink, and the dispersion
of growth rates narrows.

let   s expand on this point. one of the variables in oecdpanel is oecd, which is
1 for countries which are members of the organization for economic cooperation
and development, and 0 otherwise. the oecd countries are basically the    devel-
oped    ones (stable capitalist democracies). we can include oecd membership
as a conditioning variable for population growth (we need to use a categorical-
variable kernel), and look at the combined e   ect of time and development (figure
14.3).

what the    gure shows is that oecd and non-oecd countries both have
unimodal distributions of growth rates. the mode for the oecd countries has
become sharper, but the value has decreased. the mode for non-oecd countries
has also decreased, while the distribution has become more spread out, mostly
by having more id203 of lower growth rates. (these trends have continued
since 1995.) in words, despite the widespread contrary impression, population
growth has actually been slowing for decades in both rich and poor countries.

14.6 more on the expected log-likelihood ratio

i want to say just a bit more about the expected log-likelihood ratio(cid:82) f (x) log f (x)(cid:98)f (x)
tropy of (cid:98)f from f , and is also written d(f(cid:107)(cid:98)f ). let   s expand the log ratio:

more formally, this is called the id181 or relative en-

dx.

(cid:90)

d(f(cid:107)(cid:98)f ) =    

f (x) log (cid:98)f (x)dx +

(cid:90)

f (x) log f (x)dx

(14.47)

purposes of optimizing over (cid:98)f . (in fact, we   re just subtracting o    the id178 of

the second term does not involve the density estimate, so it   s irrelevant for

the true density.) just as with the squared error, we could try approximating the
integral with a sum:

(cid:90)

f (x) log (cid:98)f (x)dx     1

n(cid:88)

i=1

n

log (cid:98)f (xi)

(14.48)

306

density estimation

pop.cdens <- npcdens(popgro ~ year, data = oecdpanel)
plotting.grid <- expand.grid(year = seq(from = 1965, to = 1995, by = 1), popgro = seq(from = -3.5,

to = -2.4, length.out = 300))

fhat <- predict(pop.cdens, newdata = plotting.grid)
wireframe(fhat ~ plotting.grid$year * plotting.grid$popgro, scales = list(arrows = false),

xlab = "year", ylab = "popgro", zlab = "pdf")

figure 14.2 conditional density of logarithmic population growth rates as
a function of time.

which is just the log-likelihood per observation. since we know and like maximum
likelihood methods, why not just use this?

well, let   s think about what   s going to happen if we plug in the kernel density

1965197019751980198519901995   3.4   3.2   3.0   2.8   2.6   2.40.51.01.52.02.53.0yearpopgropdf14.6 more on the expected log-likelihood ratio

307

pop.cdens.o <- npcdens(popgro ~ year + factor(oecd), data = oecdpanel)
oecd.grid <- expand.grid(year = seq(from = 1965, to = 1995, by = 1), popgro = seq(from = -3.4,

to = -2.4, length.out = 300), oecd = unique(oecdpanel$oecd))

fhat <- predict(pop.cdens.o, newdata = oecd.grid)
wireframe(fhat ~ oecd.grid$year * oecd.grid$popgro | oecd.grid$oecd, scales = list(arrows = false),

xlab = "year", ylab = "popgro", zlab = "pdf")

figure 14.3 conditional density of population growth rates given year and
oecd membership. the left panel is countries not in the oecd, the right
is ones which are.

estimate:

n(cid:88)

i=1

1
n

log

(cid:32)

n(cid:88)

j=1

k

(cid:18) xj     xi

h

1
nh

(cid:19)(cid:33)

=     log nh +

n(cid:88)

i=1

1
n

log

(cid:32) n(cid:88)

j=1

(cid:18) xj     xi

h

k

(cid:19)(cid:33)

(14.49)

1965197019751980198519901995   3.2   3.0   2.8   2.6   2.41234yearpopgropdf01965197019751980198519901995   3.2   3.0   2.8   2.6   2.41234yearpopgropdf1308
if we take h to be very small, k( xj   xi
likelihood becomes

h

density estimation

)     0 unless xj = xi, so the over-all

        log nh + log k(0)

(14.50)
which goes to +    as h     0. so if we want to maximize the likelihood of a kernel
density estimate, we always want to make the bandwidth as small as possible. in
fact, the limit is to say that the density is

n(cid:88)

i=1

  f (x) =

1
n

  (x     xi)

(14.51)

where    is the dirac delta function.9 of course, this is just the same distribution
as the empirical cdf.

why is maximum likelihood failing us here? well, it   s doing exactly what we
asked it to: to    nd the distribution where the observed sample is as probable as
possible. giving any id203 to values of x we didn   t see can only come at
the expense of the id203 of observed values, so eq. 14.51 really is the unre-
stricted maximum likelihood estimate of the distribution. anything else imposes
some restrictions or constraints which don   t, strictly speaking, come from the
data. however, those restrictions are what let us generalize to new data, rather
than just memorizing the training sample.

one way out of this is to use the cross-validated log-likelihood to pick a band-
width, i.e., to restrict the sum in eq. 14.48 to running over the testing set only.
this way, very small bandwidths don   t get an unfair advantage for concentrat-
ing around the training set. (if the test points are in fact all very close to the
training points, then small bandwidths get a fair advantage.) this is in fact the
default procedure in the np package, through the bwmethod option ("cv.ml" vs.
"cv.ls").

14.7 simulating from density estimates

14.7.1 simulating from kernel density estimates

there are times when one wants to draw random values from the estimated
distribution. this is easy with kernel density estimates, because each kernel is
itself a id203 density, generally a very tractable one. the pattern goes like so.
suppose the kernel is gaussian, that we have scalar observations x1, x2, . . . xn, and
the selected bandwidth is h. then we pick an integer i uniformly at random from

(cid:82)   (x)f (x)dx = f (0). you can imagine   (x) as zero everywhere except at the origin, where it has an

9 recall that the delta function is de   ned by how it integrates with other functions:

in   nitely tall, in   nitely narrow spike, the area under the spike being one. if you are suspicious that
this is really a bona    de function, you   re right; strictly speaking it   s just a linear operator on
functions. we can however approximate it as the limit of well-behaved functions. for instance, take
  h(x) = 1/h when x     [   h/2, h/2] with   h(x) = 0 elsewhere, and let h go to zero. but this is where
we came in. . .

14.7 simulating from density estimates

309

1 to n, and invoke rnorm(1,x[i],h).10 using a di   erent kernel, we   d just need
to use the random number generator function for the corresponding distribution.
to see that this gives the right distribution needs just a little math. a kernel
k(x, xi, h) with bandwidth h and center xi is a id203 density function. the
id203 the kde gives to any set a is just an integral:

(cid:98)f (a) =

a

a

(cid:90)
(cid:90)

1
n

1
n

1
n

(cid:98)f (x)dx
n(cid:88)
(cid:90)
n(cid:88)
n(cid:88)

i=1

i=1

a

i=1

=

=

=

k(x, xi, h)dx

k(x, xi, h)dx

c(a, xi, h)

(cid:16)   x     a
(cid:17)

pr

(cid:105)(cid:105)

1a(   x)

(cid:105)
1a(   x) | j
= e [c(a, xj , h)]

= e(cid:104)
= e(cid:104)e(cid:104)
n(cid:88)

=

1
n

i=1

c(a, xi, h)

(14.52)

(14.53)

(14.54)

(14.55)

(14.56)

(14.57)

(14.58)

(14.59)

introducing c to stand for the id203 distribution corresponding to the ker-
nel. the simulation procedure works if the id203 that the simulated value
  x falls into a matches this. to generate   x, we    rst pick a random data point,
which really means picking a random integer j, uniformly from 1 to n. then

the    rst step uses the fact that a id203 is the expectation of an indica-
tor function; the second uses the law of total expectation; the last steps us the
de   nitions of c and j, and the distribution of j.

14.7.1.1 sampling from a joint density

the procedure given above works with only trivial modi   cation for sampling
from a joint, multivariate distribution. if we   re using a product kernel, we pick a
random data point, and then draw each coordinate independently from the kernel
distribution centered on our random point. (see code example 29 below.) the
argument for correctness actually goes exactly as before.

14.7.1.2 sampling from a conditional density

sampling from a conditional density estimate with product kernels is again straight-
forward. the one trick is that one needs to do a weighted sample of data points.
to see why, look at the conditional distribution (not density) function:

10 in fact, if we want to draw a sample of size q, rnorm(q,sample(x,q,replace=true),h) will work in

r     it   s important though that sampling be done with replacement.

310

density estimation

(cid:17)

1

a

hx

=

=

ky

nhx hy

i=1 kx

(cid:98)f (y     a | x = x)
(cid:90)
(cid:98)fy |x(y | x)dy
(cid:16) x   xi
(cid:17)
(cid:16) y   yi
(cid:80)n
(cid:90)
(cid:98)fx(x)
(cid:19)
(cid:18) x     xi
(cid:90)
n(cid:88)
nhxhy (cid:98)fx(x)
(cid:18) x     xi
(cid:19)(cid:90)
n(cid:88)
nhxhy (cid:98)fx(x)
(cid:18) x     xi
(cid:19)
n(cid:88)
nhx(cid:98)fx(x)

kx

kx

kx

hx

hx

hx

=

=

=

i=1

i=1

hy

1

1

1

a

a

a

i=1

cy (a, yi, hy )

(cid:19)
(cid:19)

dy

(cid:18) y     yi
(cid:18) y     yi

hy

hy

ky

ky

dy

(14.60)

(14.61)

(14.62)

dy

(14.63)

(14.64)

(cid:16) x   xi

(cid:17)

if we select the data point i with a weight proportional to kx

, and
then generate   y from the ky distribution centered at yi, then,   y will follow the
appropriate id203 density function.

hx

14.7.2 drawing from histogram estimates

sampling from a histogram estimate is also simple, but in a sense goes in the
opposite order from kernel simulation. we    rst randomly pick a bin by drawing
from a multinomial distribution, with weights proportional to the bin counts.
once we have a bin, we draw from a uniform distribution over its range.

14.7.3 examples of simulating from kernel density estimates

to make all this more concrete, let   s continue working with the oecdpanel data.
section 14.4.5 shows the joint pdf estimate for the variables popgro and inv
in that data set. these are the logarithms of the population growth rate and
investment rate. undoing the logarithms and taking the density gives figure
14.4.

let   s abbreviate the actual (not logged) population growth rate as x and the

actual (not logged) investment rate as y in what follows.

since this is a joint distribution, it implies a certain expected value for y /x,
the ratio of investment rate to population growth rate11. extracting this by direct
calculation from popinv2 would not be easy; we   d need to do the integral

(cid:90) 1

(cid:90) 1

x=0

y=0

(cid:98)fx,y (x, y)dydx

y
x

(14.65)

11 economically, we might want to know this because it would tell us about how quickly the capital

stock per person grows.

14.7 simulating from density estimates

311

popinv2 <- npudens(~exp(popgro) + exp(inv), data = oecdpanel)

figure 14.4 gaussian kernel density estimate for the un-logged population
growth rate and investment rate. (plotting code omitted     can you re-make
the    gure?)

to    nd e [y /x] by simulation, however, we just need to generate samples from
the joint distribution, say (   x1,   y1), (   x2,   y2), . . . (   xt ,   yt ), and average:

t(cid:88)

i=1

1
t

t             e

=   gt

  yi
  xi

(cid:20) y

(cid:21)

x

(14.66)

where the convergence happens because that   s the law of large numbers. if the
number of simulation points t is big, then   gt     e [y /x]. how big do we need to

312

density estimation

rpopinv <- function(n) {

n.train <- length(popinv2$dens)
ndim <- popinv2$ndim
points <- sample(1:n.train, size = n, replace = true)
z <- matrix(0, nrow = n, ncol = ndim)
for (i in 1:ndim) {

coordinates <- popinv2$eval[points, i]
z[, i] <- rnorm(n, coordinates, popinv2$bw[i])

}
colnames(z) <- c("pop.growth.rate", "invest.rate")
return(z)

}

code example 29: simulating from the    tted kernel density estimate popinv2. can you see
how to modify it to draw from other bivariate density estimates produced by npudens? from
higher-dimensional distributions? can you replace the for loop with less iterative code?

make t ? use the central limit theorem:

  gt (cid:32) n (e [y /x] , v [  g1] /

   

t )

(14.67)

how do we    nd the variance v [  g1]? we approximate it by simulating.

code example 29 is a function which draws from the    tted kernel density
estimate. first let   s check that it works, by giving it something easy to do, namely
reproducing the means, which we can work out:

signif(mean(exp(oecdpanel$popgro)), 3)
## [1] 0.0693
signif(mean(exp(oecdpanel$inv)), 3)
## [1] 0.172
signif(colmeans(rpopinv(200)), 3)
## pop.growth.rate
##
0.070

invest.rate
0.161

this is pretty satisfactory for only 200 samples, so the simulator seems to be

working. now we just use it:

z <- rpopinv(2000)
signif(mean(z[, "invest.rate"]/z[, "pop.growth.rate"]), 3)
## [1] 2.66
signif(sd(z[, "invest.rate"]/z[, "pop.growth.rate"])/sqrt(2000), 3)
## [1] 0.0352

this tells us that e [y /x]     2.66    0.035.
suppose we want not the mean of y /x but the median?

signif(median(z[, "invest.rate"]/z[, "pop.growth.rate"]), 3)
## [1] 2.34

getting the whole distribution of y /x is not much harder (figure 14.5). of
course complicated things like distributions converge more slowly than simple
things like means or medians, so we want might want to use more than 2000
simulated values for the distribution. alternately, we could repeat the simulation

14.8 further reading

313

yoverx <- z[, "invest.rate"]/z[, "pop.growth.rate"]
plot(density(yoverx), xlab = "y/x", ylab = "id203 density", main = "")
rug(yoverx, side = 1)

figure 14.5 distribution of y /x implied by the joint density estimate
popinv2.

many times, and look at how much variation there is from one realization to the
next (figure 14.6).

of course, if we are going to do multiple simulations, we could just average
them together. say that   g(1)
t are estimates of our statistic of interest
from s independent realizations of the model, each of size t . we can just combine
them into one grand average:

t , . . .   g(s)

t ,   g(2)

s(cid:88)

i=1

  gs,t =

1
s

  g(1)
t

(14.68)

as an average of iid quantities, the variance of   gs,t is 1/s times the variance of
  g(1)
t .
by this point, we are getting the sampling distribution of the density of a
nonlinear transformation of the variables in our model, with no more e   ort than
calculating a mean.

14.8 further reading

good introductory treatments of density estimation can be found in simono   
(1996) and wasserman (2006). my treatment of conditional density estimation is
based on hall et al. (2004).

the glivenko-cantelli theorem has a more    quantitative    version, the    dvoretzky-

024680.000.050.100.150.200.25y/xid203 density314

density estimation

plot(0, xlab = "y/x", ylab = "id203 density", type = "n", xlim = c(-1,

10), ylim = c(0, 0.3))

one.plot <- function() {

zprime <- rpopinv(2000)
yoverxprime <- zprime[, "invest.rate"]/zprime[, "pop.growth.rate"]
density.prime <- density(yoverxprime)
lines(density.prime, col = "grey")

}
invisible(replicate(50, one.plot()))

figure 14.6 showing the sampling variability in the distribution of y /x
by    over-plotting   . each line is a distribution from an estimated sample of
size 2000, as in figure 14.5; here 50 of them are plotted on top of each other.
the thickness of the bands indicates how much variation there is from
simulation to simulation at any given value of y /x. (setting the type of the
initial plot to n, for    null   , creates the plotting window, axes, legends, etc.,
but doesn   t actually plot anything.)

kiefer-wolfowitz inequality   , which asserts that with iid samples from a one-
dimensional cdf f ,

(cid:18)

(cid:19)
|   fn(x)     f (x)| >  

pr

sup

x

    2e   2n 2

(14.69)

and the constants appearing here are known to be the best that hold over all
distributions (wasserman, 2006,   2.2); this can be inverted to get con   dence
bands for the cdf.

on empirical process theory, see pollard (1989, 1990); van de geer (2000);
pollard (1989) is especially good as an introduction. devroye and lugosi (2001)
applies empirical process theory to density estimation, as well as forcefully advo-

cating measuring error using the l1 distance, (cid:82) |   f (x)     f (x)|dx. in this chapter

02468100.000.050.100.150.200.250.30y/xid203 densityexercises

315

i have stuck to l2, partly out of tradition and partly out of desire to keep the
algebra simple (which in turn helps explain the tradition).

historical notes

i do not know of a good history of the glivenko-cantelli theorem (but would like
to read one).

histogram estimates are very old; the word    histogram    was apparently coined
by karl pearson in the 1890s12, but as a convenient name for an already-common
type of graphic. kernel density estimation seems to have    rst been proposed by
rosenblatt (1956) (see especially section 4of that paper). it was re-introduced,
independently, by parzen (1962), and some of the analysis of the error in kdes
that we saw above goes back to this paper.

exercises

14.1 reproduce figure 14.4?
14.2 qualitatively, is this compatible with figure 14.1?
14.3 how could we use popinv2 to calculate a joint density for popgro and inv (not exp(popgro)

and exp(inv))?

(cid:82) u2k(u)du =   2

14.4 should the density popinv2 implies for those variables be the same as what we   d get from

14.5 you are given a kernel k which satis   es k(u)     0, (cid:82) k(u)du = 1, (cid:82) uk(u)du = 0,

directly estimating their density with kernels?

k <    . you are also given a bandwidth h > 0, and a collection of n
univariate observations x1, x2, . . . xn. assume that the data are independent samples from
some unknown density f .
1. give the formula for   fh, the kernel density estimate corresponding to these data, this

bandwidth, and this kernel.

2. find the expectation of a random variable whose density is   fh, in terms of the sample

moments, h, and the properties of the id81.

3. find the variance of a random variable whose density is   fh, in terms of the sample

moments, h, and the properties of the id81.

4. how must h change as n grows to ensure that the expectation and variance of   fh will

converge on the expectation and variance of f ?

14.6 the transformation method many variables have natural range restrictions, like being non-
negative, or lie in some interval. kernel density estimators don   t respect these restrictions,
so they can give positive id203 density to impossible values. one way around this is
the transformation method (or    trick   ): use an invertible function q to map the limited
range of x to the whole real line,    nd the density of the transformed variable, and then
undo the transformation.
in what follows, x is a random variable with pdf f , y is a random variable with pdf g, and
y = q(x), for a known function q. you may assume that q is continuous, di   erentiable
and monotonically increasing, inverse q   1 exists, and is also continuous, di   erentiable and
monotonically increasing.

12 see je    miller (ed.),    earliest known uses of some of the words of mathematics   , s.v.

   histogram   , http://jeff560.tripod.com/h.html. i have not veri   ed the references cited there, by
have found the site to be generally reliable.

316

density estimation

1. find g(y) in terms of f and q.
2. find f (x) in terms of g and q.
3. suppose x is con   ned to the unit interval [0, 1] and q(x) = log x

1   x . find f (x) in terms

of g and this particular q.

4. the beta distribution is con   ned to [0, 1]. draw 1000 random values from the beta
distribution with both shape parameters equal to 1/2. call this sample x, and plot its
histogram. (hint: ?rbeta.)

5. fit a gaussian kernel density estimate to x , using density, npudens, or any other

existing one-dimensional density estimator you like.

6. find a gaussian kernel density estimate for logit(x).
7. using your previous results, convert the kde for logit(x) into a density estimate for

x .

8. make a plot showing (i) the true beta density, (ii) the    raw    kernel density estimate
from 35, and (iii) the transformed kde from 37. make sure that the plotting region
shows all three curves adequately, and that the three curves are visually distinct.

15

relative distributions and smooth tests of

goodness-of-fit

in   5.2.2.3, we saw how to use the quantile function to turn uniformly-distributed
random numbers into random numbers with basically arbitrary distributions. in
this chapter, we will look at two closely-related data-analysis tools which go
the other way, trying to turn data into uniformly-distributed numbers. one of
these, the smooth test, turns a lot of problems into ones of testing a uniform
distribution. another, the relative distribution, gives us a way of comparing
whole distributions, rather than speci   c statistics (like the expectation or the
variance).

15.1 smooth tests of goodness of fit

15.1.1 from continuous cdfs to uniform distributions

suppose that x has id203 density function f , and that f is continuous. the
corresponding cumulative distribution function f is then continuous and strictly
increasing (on the support of f ). since f is a    xed function, we can ask what the
id203 distribution of f (x) is. clearly,

pr (f (x)     0) = 0
pr (f (x)     1) = 1

(15.1)
(15.2)

since f is continuous and strictly increasing, it has an inverse, the quantile func-
tion q, which is also continuous and strictly increasing. then, for 0     a     1,

pr (f (x)     a) = pr (q(f (x))     q(a))

= pr (x     q(a))
= f (q(a)) = a

(15.3)
(15.4)
(15.5)

thus, when f is continuous and strictly-increasing, f (x) is uniformly distributed
on the unit interval,

f (x)     unif(0, 1)

(15.6)

if the distribution of x is f , but we guess that it has some other distribution,
with cdf f0, then this trick will not work. f0(x) will still be in the unit interval,
but it won   t be uniformly distributed:

this only works if x really is distributed according to f . if instead x were

317

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

318

relative distributions and smooth tests

distributed according, say, f0, then f (x) will still be in the unit interval, but it
will not be uniformly distributed:

pr (f0(x)     a) = pr (x     q0(a))
= f (q0(a)) (cid:54)= a

(15.7)
(15.8)

because f0 (cid:54)= q   1.
putting this together, we see that when x has a continuous distribution,
f (x)     unif(0, 1) if and only if f is the cumulative distribution function for
x. this means that we can reduce the problem of testing whether x     f to that
of testing whether f (x) is uniform. we need to work out one testing problem,
rather than many di   erent testing problems for many di   erent distributions.

15.1.2 testing uniformity

now we have a random variable, say y , which lives on the unit interval [0, 1], and
we want to test whether it is uniformly distributed. there are several di   erent
ways we could do this. one frequently-used strategy is to use the kolmogorov-
smirnov test: calculate the k-s distance,

where (cid:98)fn,y (a) is the empirical cdf of y , and look up the appropriate p-value

dks = max
a   [0,1]

(15.9)

for the k-s test. one could use any other one-sample non-parametric test here,
like cram  er-von mises or anderson-darling1 all of these tests can work quite
well in the right circumstances, and they have the advantage of requiring little
additional work over and above typing ks.test or the like.

15.1.3 neyman   s smooth test

there are however two disadvantages of just applying o   -the-shelf tests to check
uniformity. one is that it turns out that they often do not have very high power.
the other, which is in some ways even more serious, is that rejecting the null
hypothesis of uniformity doesn   t tell you how uniformity fails     it doesn   t suggest
any sort of natural alternative.

as you can guess from my having brought up these points, there is a test which
avoids both di   culties, called neyman   s smooth test. it works by embedding
the uniform distribution on the unit interval in a larger class of alternatives, and
then testing the null of uniformity against those alternatives.

the alternatives all have pdfs of the form

(cid:12)(cid:12)(cid:12)(cid:98)fn,y (a)     a

(cid:12)(cid:12)(cid:12)

g(y;   )    

j=1   j hj (y)

z(  )
0

0     y     1
elsewhere

(15.10)

(cid:40)

(cid:80)d

e

1 you could even use a   2 test, but this would be dumb. because the   2 test requires discrete data,

using it means binning continuous values, thereby destroying information, to no good purpose.

15.1 smooth tests of goodness of fit

319

where the hj are carefully chosen functions (see below), and the normalizing
factor or partition function z(  ) just makes sure the density integrates to 1:

(cid:90) 1

(cid:80)d

z(  )    

e

j=1   j hj (y)dy

no matter what functions we pick for the hj, uniformity corresponds to the choice
   = 0, since then the density is just 1. as we move    slightly away from 0, the
density departs smoothly from uniformity; hence the name of the test.

0

to ensure that everything works out, we need to put some requirements on
the functions hj: they need to be orthogonal to each other and to the constant
function,

(cid:90) 1
and normalized in magnitude, (cid:90) 1

0

(cid:90) 1

0

hj(y)dy = 0

hj(y)hk(y)dy = 0

h2
j (y)dy = 1

0

(15.11)

(15.12)

(15.13)

(15.14)

further details, while practically important, do not matter for the general idea
of the test, so i   ll put them o    to   15.1.3.1.

we can estimate    by maximum likelihood. because uniformity corresponds
to    = 0, we can test the hypothesis that    = 0 against the alternative that
   (cid:54)= 0 with a likelihood ratio test. writing (cid:96)(    ) for the log-likelihood under the
id113, and (cid:96)(0) for the log-likelihood under the null, by general results on the
likelihood-ratio (appendix i), under the null, as n        ,

2((cid:96)(    )     (cid:96)(0)) (cid:32)   2

d

(15.15)

in fact, (cid:96)(0) = 0 (why?), so we only need to calculate the log-likelihood under
the alternative, and reject uniformity when, and only when, that log-likelihood
is large.

alternatively, and this was neyman   s original recommendation and what is
usually meant by his    smooth test   , we can calculate the sample mean of each of
the hj,

hj =

1
n

and form the test statistic

hj(yi)

n(cid:88)
d(cid:88)

i=1

  2 = n

2

hj

(15.16)

(15.17)

which also has a   2
2 to appreciate what   s going on, notice that hj     0 under the null, by the law of large numbers.

d distribution under the null.2

j=1

320

relative distributions and smooth tests

it can be shown that neyman   s smooth test has, in a certain sense, optimal
power against smooth alternatives like this     see rayner and best (1989) or bera
and ghosh (2002) for the gory details. more importantly, for data analysis, when
we reject the null hypothesis of uniformity, we have a ready-made alternative to
fall back on, namely g(y;     ).

to make all this work, we have to pick some    basis functions    hj, and we need

to decide how many of them we want to use, d.

15.1.3.1 choice of function basis

neyman   s original proposal was to use orthonormal polynomials for basis
functions: hj would be a polynomial of degree j, which was orthogonal to all the
ones before it,

(cid:90) 1

0

hj(y)hk(y)dy = 0    k < j
(cid:90) 1

h2
j (y)dy = 1

0

(15.18)

(15.19)

including the constant    polynomial    h0(y) = 1, and normalized to size 1,

since there are j + 1 coe   cients in a polynomial of degree j, and this gives j + 1
equations, the polynomial is uniquely determined. in fact, there are recursively
formulas which let you    nd the coe   cients of hj from those of the previous
polynomials3. figure 15.1 shows the    rst few of these polynomials, and their
exponentiated versions (which are what appear in eq. 15.10).

experience has shown that the speci   c choice of basis functions doesn   t matter
as much as ensuring that they are orthonormal. one could, for instance, use
hj(y) = cj cos 2  jy, where cj is a normalizing constant4.

15.1.3.2 choice of number of basis functions

as we make d in eq. 15.10, we include more and more distributions in the alter-
native to the null hypothesis of uniformity. in fact, since any smooth function on
[0, 1] can be approximated arbitrarily closely by su   ciently-high order polynomi-
als5, as we let d         we eventually get all continuous distributions, other than
uniformity, as part of the alternative. however, using a large value of d means

(this is where being orthogonal to the constant function h0(y) = 1 comes in.) multiplying hj
corresponds to looking at
   
variance of this gaussian is 1. (this is where normalizing each hj comes in.) finally,

nhj , which should, by the central limit theorem, be a gaussian; the
nhj and

nhk are uncorrelated. (this is where the mutual orthogonality of the hj comes in.) thus, the   2

   

   

2

by n

statistic is a sum of d uncorrelated standard gaussians, which has a   2

d distribution.

3 in fact, the polynomials neyman proposed to use are, as he knew, the    legendre polynomials   ,

though many math books (and wikipedia) give the version of those de   ned on [   1, 1], rather than
on [0, 1]. if lj is the polynomial on [   1, 1], then hj (y) = lj (2(y     0.5)).

4 if this makes you think of fourier analysis, you   re right.
5 this may be obvious, but making it precise (what do we mean by    smooth    and    arbitrarily

close   ?) is the    stone-weierstrauss theorem   . there is nothing magic about polynomials here; we
could also use sines and cosines, or many other function bases.

15.1 smooth tests of goodness of fit

321

par(mfrow = c(2, 1))
h1 <- function(y) {

sqrt(12) * (y - 0.5)

}
h2 <- function(y) {

sqrt(5) * (6 * (y - 0.5)^2 - 0.5)

}
h3 <- function(y) {

sqrt(7) * (20 * (y - 0.5)^3 - 3 * (y - 0.5))

}
curve(h1(x), ylab = expression(h[j](y)), xlab = "y")
curve(h2(x), add = true, lty = "dashed")
curve(h3(x), add = true, lty = "dotted")
legend(legend = c(expression(h[1]), expression(h[2]), expression(h[3])), lty = c("solid",

"dashed", "dotted"), x = "bottomright")

curve(exp(h1(x)), ylab = expression(e^h[j](y)), xlab = "y")
curve(exp(h2(x)), add = true, lty = "dashed")
curve(exp(h3(x)), add = true, lty = "dotted")
legend(legend = c(expression(h[1]), expression(h[2]), expression(h[3])), lty = c("solid",

"dashed", "dotted"), x = "bottomright")

par(mfrow = c(1, 1))

figure 15.1 left panel: the    rst three of the basis functions for neyman   s
smooth tests, h1, h2 and h3. each hj is a polynomial of order j which is

orthogonal to the others, in the sense that(cid:82) 1
but normalized in size,(cid:82) 1

0 hj(y)hk(y)dy = 0 when j (cid:54)= k,
j (y)dy = 1. the right panel shows ehj (y), to give

an indication of how the functions contribute to the id203 density in
eq. 15.10.

0 h2

0.00.20.40.60.81.0   1.50.01.0yhj(y)h1h2h30.00.20.40.60.81.0012345yehj(y)h1h2h3322

relative distributions and smooth tests

x <- (1:1e+06)/1e+06
z <- sum(exp(h1(x) + h2(x) - h3(x)))/1e+06
curve(exp(h1(x) + h2(x) - h3(x))/z, xlab = "y", ylab = expression(g(y, theta)))
abline(h = 1, col = "grey")

figure 15.2 illustration of a smooth alternative density: using the same
basis functions as before, with   1 = 1,   2 = 1,   3 =    1. the    rst two lines of
the r calculate the normalizing constant z(  ) by a simple numerical integral.
the grey line shows the uniform density.

estimating a lot of parameters, which means we are at risk of over-   tting. what
to do?

neyman   s original advice was to guess a particular value of d before looking
at the data and stick to it. (he thought d = 4 would usually be enough.) more

0.00.20.40.60.81.00246yg(y, q)15.1 smooth tests of goodness of fit

323

modern approaches try to adaptively pick a good value of d. we could attempt
this through cross-validation based on the log-likelihood, but what   s usually done,
in implemented software, is to pick d to maximize schwarz   s information criterion:

(cid:96)((cid:98)  (d))     d

d    = argmax

d

1
n

log n

n

(15.20)

2

which imposes an extra penalty for each parameter (d), with the size of the
penalty depending on how much data we have, and getting relatively harsher as
n grows6. so in a data-driven smooth test (kallenberg and ledwina, 1997),
we pick d    using eq. 15.20, and then compute the test statistic using d   .

unfortunately, since d    is random (through the data), the nice asymptotic
theory which says that the test statistic is   2
d under the null hypothesis no longer
applies. however, this is why we have id64: by simulating from the null
hypothesis, which remember is just unif(0, 1), and treating the simulation output
like real data we can work out the sampling distribution as accurately as we need.
this sampling distribution then gives us our p-values.

15.1.3.3 application: combining p-values

one useful property of p-values is that they are always uniformly distributed on
[0, 1] under the null hypothesis7. suppose we have conducted a bunch of tests of
the same null hypothesis     these might be di   erent clinical trials of the same
drug, or attempts to replicate some surprising e   ect in separate laboratories8. if
the tests are independent, then the p-values should be iid and uniform. it would
seem like we should be able to combine these into some over-all p-value. this is
precisely what neyman   s smooth test of uniformity lets us do.

15.1.3.4 density estimation by series expansion

as an aside, notice what we have done. by using a large enough d, as i said,
densities which look like eq. 15.10 can come as close as we like to any smooth
density on [0, 1]. and now we have at least two ways of picking d: by cross-
validation, or by the schwarz information criterion (eq. 15.20). if we let d        
as n        , then we have a way of approximating any density on the unit interval,
without knowing what it was to start with, or assuming a particular parametric
form for it. that is, we have a way of doing non-parametric density estimation,
at least on [0, 1], without using kernels.
if you want to estimate a density on (      ,   ) instead of on [0, 1], you can do so
by using a transformation, e.g., the inverse logit. this is the opposite of what you

6 it is common in the literature to see the criterion written out multiplied through by n, or even by

2n. also, it is often called the    bayesian information criterion   , or bic. this is an unfortunate
name, because, despite what schwarz (1978) thought, it really has nothing at all to do with bayes   s
rule or even bayesian statistics. it   s best thought of as a fast, but very crude and not always very
accurate, approximation to cross-validation. if you want to know more, claeskens and hjort (2008)
is probably the best reference.

7 unless someone has messed up a calculation, that is.
8 these are typical examples of meta-analysis, trying to combine the results of many di   erent data

analyses (without just going back to the original data).

relative distributions and smooth tests

324
did in the homework, where you used a transformation to take [0, 1] to (      ,   )
so you could use kernel density estimation.

15.1.4 smooth tests of non-uniform parametric families

remember that we went into all these details about testing uniformity because we
want to test whether x is distributed according to some continuous distribution
with cdf f . from   15.1.1, if we de   ne y = f (x), then x     f is equivalent to
y     unif(0, 1), so we have a two-step procedure for testing whether x     f :

1. use the cdf f to transform the data, yi = f (xi)
2. test whether the transformed data yi are uniform

let   s think about what the alternatives considered in the test look like. for y,

the alternative densities are (to repeat eq. 15.10)

(cid:40)

(cid:80)d

e

g(y;   )    

j=1   j hj (y)

z(  )
0

0     y     1
elsewhere

since x = f    1(y ), this implies a density for x:

(cid:80)d
(cid:80)d

e

e

j=1   j hj (f (x))

j=1   j hj (f (x))

z(  )

z(  )

df
dx

f (x)

gx(x;   ) =

=

(15.21)

(15.22)

(15.23)

(cid:90)    

      

where f is the pdf corresponding to the cdf f . (why do we not have to worry
about setting this to zero outside some range?) just like g(  ;   ) is a modulation
or distortion of the uniform density, gx(  ;   ) is a modulation or distortion of f (  ).
if and when we reject the density f , gx(  ;   ) is available to us as an alternative.
even if hj(y) is a polynomial in y, hj(f (x)) will not (in general) be a polynomial

in x, but it remains true that

hj(f (x))hk(f (x))f (x)dx =   jk

(15.24)

figure 15.3 illustrates what happens to the basis functions, and to particular
alternatives.

when it comes to the actual smooth test, we can either use the likelihood ratio,

or we can calculate

hj =

1
n

n(cid:88)

i=1

hj(yi) =

1
n

n(cid:88)

leading as before to the test statistic

  2 = n

d(cid:88)

j=1

hj(f (xi))

(15.25)

i=1

2

hj

(15.26)

15.1 smooth tests of goodness of fit

325

par(mfrow = c(2, 1))
curve(h1(pnorm(x)), xlab = "x", ylab = expression(h[j](f(x))), from = -5, to = 5,

ylim = c(-3, 3))

curve(h2(pnorm(x)), add = true, lty = "dashed")
curve(h3(pnorm(x)), add = true, lty = "dotted")
legend(legend = c(expression(h[1]), expression(h[2]), expression(h[3])), lty = c("solid",

"dashed", "dotted"), x = "bottomright")

curve(dnorm(x) * exp(h1(pnorm(x)) + h2(pnorm(x)) - h3(pnorm(x)))/z, xlab = "x",

ylab = expression(g[x](x, theta)), from = -5, to = 5)

curve(dnorm(x), add = true, col = "grey")
par(mfrow = c(1, 1))

figure 15.3 left panel: the basis functions from figure 15.1 composed with
the standard gaussian cdf. right panel: the alternative to the standard
gaussian corresponding to the alternative to the uniform distribution
plotted in figure 15.2, i.e.,   1 =   2 = 1,   3 =    1. the grey curve is the
standard gaussian density, corresponding to the    at line in figure 15.2.

   4   2024   3   1123xhj(f(x))h1h2h3   4   20240.00.20.40.6xgx(x, q)326

relative distributions and smooth tests

the distribution of the test statistics is unchanged under the null hypothesis, i.e.,
still   2
d if d is    xed. (there are still d degrees of freedom, because we are still
   xing d parameters from distributions of the form eq. 15.23.) if d is chosen from
the data, we still need to bootstrap, but can do so just as before.

15.1.4.1 estimated parameters

so far, the discussion has assumed that f is    xed and won   t change with the
data. this is often not very realistic. rather, f comes from some parametrized
family of distributions, with parameter (say)   , i.e., f (  ;   ) is a di   erent cdf
for each value of   . for gaussians, for instance,    is a vector consisting of the
mean and variance (or mean and standard deviation). let   s assume that there
are always corresponding densities, f (  ;   ), and these are always continuous.

we don   t know    so we have to estimate it. after estimating, we   d like to test
whether the model really matches the data. it would be convenient if we could
do the following:

1. get estimate (cid:98)   from x1, x2, . . . xn
2. calculate yi = f (xi;(cid:98)  )

3. apply a smooth test of uniformity to y1, y2, . . . yn

that is, it would be convenient if we could just ignore the fact that we had to
estimate   .

we can do this if (cid:98)   is the maximum likelihood estimate. to understand this,

think about the family of alternative distributions we   re now considering in the
test. substituting into eq. 15.23, they are

(cid:80)d

e

j=1   j hj (f (x;  ))

gx(x;   ,   ) =

(15.27)
the null hypothesis that x     f (  ;   ) for some    is thus corresponds to x    
gx(  ;   , 0)     we are still    xing d parameters in the larger family. and, generally
speaking, when we    x d parameters in a parametric model, we get a   2
d distribu-
tion in the log-likelihood ratio test (appendix i). if d is not    xed but data-driven,
then, again, we need to bootstrap.

f (x;   )

z(  )

15.1.5 implementation in r

the main implementation of smooth tests available in r is the ddst package
(biecek and ledwina, 2010), standing for    data-driven smooth tests   . it pro-
vides a ddst.uniform.test, which we could use for any family where we can
calculate the cdf. but it also provides functions for directly testing several
families of distributions, notably gaussians (ddst.norm.test) and exponentials
(ddst.exp.test).

let   s give ddst.norm.test some gaussian data and see what happens.

15.1.5.1 some examples

15.1 smooth tests of goodness of fit

327

r <- rnorm(100)
(r.normality <- ddst.norm.test(r))
##
##
##
## data:
## wt* = 0.73293, n. coord = 1

r,

data driven smooth test for normality

base: ddst.base.legendre,

c: 100

this reminds us what the data was, tells us that the test used legendre polyno-
mials (as opposed to cosines), that d was selected to be 1, and that the value of the
test statistic was 0.733. (the c setting has to do with the order-selection penalty,
and is basically ignorable for most users.) these numbers are all attributes of the
returned object.

what is missing is the p-value, because this is computationally expensive to
calculate. (you can control how many bootstraps it uses, but the default is 1000.)

data driven smooth test for normality

(r.normality <- ddst.norm.test(r, compute.p = true))
##
##
##
## data:
## wt* = 0.73293, n. coord = 1, p-value = 0.4066

base: ddst.base.legendre,

c: 100

r,

so the p-value is 0.407, giving us little reason to reject a gaussian distribution
    which is good, because we   re looking at numbers from the standard gaussian.
if we ignored the fact that d was selected from the data and plugged into the
corresponding   2

d distribution, we   d get a p-value of

pchisq(r.normality$statistic, df = 1, lower.tail = false)
##
wt*
## 0.391936

which to say a relative error of about 4%.
what if we give the procedure some non-gaussian data? say, the same amount

of data from a t distribution with 5 degrees of freedom?

ng <- rt(100, df = 5)
ddst.norm.test(ng, compute.p = true)
##
##
##
## data:
## wt* = 0.028171, n. coord = 1, p-value = 0.8712

data driven smooth test for normality

base: ddst.base.legendre,

ng,

c: 100

of course, it won   t always reject, because the we   re only looking at 100 samples,
and the t distribution isn   t that di   erent from a gaussian. still, when i repeat
this experiment many times, we get quite respectable power at the standard 5%
size:

mean(replicate(100, ddst.norm.test(rt(100, df = 5), compute.p = true)$p.value) <

0.05)

## [1] 0.55

328

relative distributions and smooth tests

par(mfrow = c(2, 1))
plot(hist(r, plot = false), freq = false, main = "")
rug(r)
curve(dnorm(x), add = true, col = "grey")
rf <- pnorm(r, mean = mean(r), sd = sd(r))
plot(hist(rf, plot = false), freq = false, main = "")
rug(rf)
abline(h = 1, col = "grey")
par(mfrow = c(1, 1))

figure 15.4 left panel: histogram of the 100 random values from the
standard gaussian used in the text (exact values marked along the
horizontal axis), plus the true density in grey. right panel: transforming the
data according to the gaussian    tted to the data by maximum likelihood.

rdensity   2   10120.00.20.4rfdensity0.00.20.40.60.81.00.00.40.81.215.1 smooth tests of goodness of fit

329

ngdensity   4   20240.000.050.100.150.200.250.300.35ngfdensity0.00.20.40.60.81.00.00.51.01.5330

relative distributions and smooth tests

see exercise 15.3 for a small project of ddst.exp.test to check a pareto

distribution.

15.1.6 conditional distributions and calibration

suppose that we are not interested in the marginal distribution of x, but rather
its conditional distribution given some other variable or variables c (for    covari-
ates   ). if the conditional density f (x|c = c) is continuous in x for every c, then it
is easy to argue, in parallel with   15.1.1, that f (x|c = c), the conditional cdf,
should     unif(0, 1). so, as long as we use the conditional cdf to transform x,
we can apply smooth tests as before.

one important use of this is regression residuals. suppose x is the target vari-
able of a regression, with c being the predictor variables9, and we have some
parametric distribution in mind for the noise (gaussian, say), with the noise  
being independent of c. then the model is x = r(c) +  , so looking at the con-
ditional cdf of x given z is equivalent to looking at the at unconditional cdf
of the residuals. we can then actually test whether the residuals are gaussian,
rather than just squinting at a q-q plot. we could also do this by applying a k-s
test to the transformed residuals, but everything that was said above in favor of
smooth tests would still apply.

notice, by the way, that by applying the cdf transformation to the residuals,
we are checking whether the model is properly calibrated, i.e., whether events it
says happen with id203 p actually have a frequency close to p. we do need
to impose assumptions about the distribution of the noise to check calibration
for a regression model, since if we just predict expected values, we say nothing
about how often any particular range of values should happen.

later, when we look at id114 and at time series, we will see several
other important situations where a statistical model is really about conditional
distributions, and so can be checked by looking at conditional cdf transforma-
tions. it seems to be somewhat more common to apply k-s tests than smooth
tests after the conditional cdf transformation (e.g., bai 2003), but i think this
is just because smooth tests are not as widely known and used as they should be.

15.2 relative distributions

so far, i have been talking about how we can test whether our data follows some
hypothesized distribution, or family of distributions, by using the fact that f (x)
is uniform if and only if x has cdf f . if the values of f (xi) are close enough to
being uniform, the true cdf has to be pretty close to f (with high con   dence);
if they are far from uniform, the true cdf has to be far from f (again with high
con   dence).

in many situations, however, we already know (or are at least pretty sure) that
x doesn   t have some distribution, say f0, and what we are interested in is how

9 i know you   re used to x being the predictor and y being the target.

15.2 relative distributions

331

x fails to follow it; we want, in other words, to compare the distribution of x to
some reference distribution f0. for instance:

1. we are trying a new medical procedure, and we want to compare the dis-
tribution of outcomes for patients who got the treatment to those who did
not.

2. we want to compare the distribution of some social outcome across two cate-
gories at the same time. (for instance, we might compare income, or lifespan,
for men and for women.)

3. we might want to compare members of the same category at di   erent times, or
in di   erent locations. (we might compare the income distribution of american
men in 1980 to that of 2010, or the lifespans of american men in 2010 to those
of canadian men.)

4. we might compare our actual population to the distribution predicted by a
model we know to be too simple (or just approximate) to try to learn what it
is missing.

you learned how to do comparisons of simple summaries of distributions in baby
stats. (for instance, you learned how to compare group means by doing t-tests.)
while these certainly have their places, they can miss an awful lot. for example,
a few years ago now an anesthesiologist came to the cmu statistics department
for help evaluating a new pain-management procedure, which was supposed to
reduce how many pain-killers patients recovering from surgery needed. under
both the old procedure and the new one, the distribution was strongly bimodal,
with some patients needing very little by way of pain-killers, many needing much
more, and a few needing an awful lot of drugs. simply looking at the change
in the mean amount of drugs taken, or even the changes in the mean and the
variance, would have told us very little about whether things were any better10.
in this example, the reference distribution, f0, is given by the distribu-
tion of drug demand for patients on the old pain-management protocol. the
new or comparison sample, x1, . . . xn, are realizations of a random variable x,
representing the demand for pain-killers under the new protocol. x follows the
comparison distribution f , which is presumably not the same as f0; how does
it di   er?

the idea of the relative distribution is to characterize the change in distri-
butions by using f0 to transform x into [0, 1], and then looking at how it departs
from uniformity. the relative data, or grades, are

ri = f0(xi)

(15.28)

simply put, we take the comparison data points and see where they fall in the
reference distribution.

what is the cumulative distribution function of the relative data? let   s look
at this    rst at the population level, where we have f0 (the reference distribution)

10 i am omitting some details, and not providing a reference because the study is still, so far as i know,

unpublished.

332

relative distributions and smooth tests

and f (the comparison distribution), rather than just samples. let   s call the cdf
of the relative data g:

g(a)     pr (r     a)

= pr (f0(x)     a)
= pr (x     q0(a))
= f (q0(a))

(15.29)
(15.30)
(15.31)
(15.32)

where remember q0 = f    1
this in turn implies a id203 density function of the relative data:

is the quantile function of the reference distribution.

0

(cid:12)(cid:12)(cid:12)(cid:12)a=y
(cid:12)(cid:12)(cid:12)(cid:12)u=q0(y)

g(y)     dg
da

=

df
du

(cid:12)(cid:12)(cid:12)(cid:12)a=y

df    1
0
da

1

= f (q0(y))

f0(q0(y))

=

f (q0(y))
f0(q0(y))

(15.33)

(15.34)

(15.35)

this only applies when y     [0, 1]; elsewhere, g(y) is straightforwardly 0.

when g(y) > 1, we have f (q0(y)) > f0(q0(y))     that is, values around
q0(y) are relatively more probable in the comparison distribution than in the
reference distribution. likewise, when g(y) < 1, the comparison distribution puts
less weight on values around q0(y) than does the reference distribution. if the
comparison distribution was exactly the same as the reference distribution, we
would, of course, get g(y) = 1 everywhere.

one very important property of the relative distribution is that it is invariant
under monotone transformations. that is, suppose instead of looking at x, we
looked at h(x) for some monotonic function h. (an obvious example would be
change of units, but we might also take logs or powers.) summary statistics like
di   erences in means are generally not even equi-variant11. but it is easy to check
(exercise 15.4) that the relative distribution of h(x) is the same as the relative
distribution of x. this expresses the idea that the di   erence between the reference
and comparison distributions is independent of our choice of a coordinate system
for x.

15.2.1 estimating the relative distribution

in some situations, the reference distribution can come from a theoretical model,
but the comparison distribution is unknown, though we have samples. estimating
the relative density g is then extremely similar to what we had to do in the last

11 remember that a statistic, say   , is a function of the data,   (x1, x2, . . . xn). the statistic is invariant
under a transformation h if   (h(x1), h(x2), . . . h(xn)) =   (x1, x2, . . . xn)     the transformation does
not change the statistic. the statistic is equivariant if it    changes along with    the transformation,
  (h(x1), h(x2), . . . h(xn)) = h(  (x1, x2, . . . xn)). maximum likelihood estimates are equivariant.
statistics like the mean are equivariant under linear and a   ne transformations (but not others).

15.2 relative distributions

333

section for hypothesis testing. non-parametric estimation of g can thus proceed
either through    tting series expansions like eq. 15.10 (with a data-driven choice
of d, as above), or through using a    xed, data-independent transformation to
map [0, 1] to (      ,   ) and using kernel density estimation12.

if, on the other hand, neither the reference nor the comparison distribution is
fully known, but we have samples from both, estimating the relative distribution
involves estimating q0, the quantile function of the reference distribution. this
is typically estimated as just the empirical quantile function, but in principle one
could use, say, kernel smoothing to get at q0. once we have an estimate for it,
though, we have reduced the problem of estimating g to the case considered in
the previous paragraph.

uncertainty in the estimate of the relative density g is, as usual, most easily
assessed through the bootstrap. be careful to include the uncertainty in estimates
of q0 as well, if the reference quantiles have to be estimated. one can, however,
also use asymptotic approximations (handcock and morris, 1999,   9.6).

15.2.2 r implementation and examples

relative distribution methods were introduced by handcock and morris (1998,
1999), who also wrote an r package, reldist, which is by far the easiest way to
work with relative distributions. rather than explain abstractly how this works,
we   ll turn immediately to examples.

15.2.2.1 example: conservative versus liberal brains

data analysis problem set a.21 looks at the data from kanai et al. (2011), which
record the volumes of two parts of the brain, the amygdala and the anterior
cingulate cortex (acc), adjusted for body size, sex, etc., and political orienta-
tion on a    ve-point ordinal scale, with 1 being the most conservative and 5 the
most liberal13. the subjects being british university students, the lowest score
for political orientation recorded was 2, and so we will look at relative distribu-
tions between those students and the rest of the sample. that is, we take the
conservatives as the comparison sample, and the rest as the reference sample14.
having loaded the data into the data frame n90, we can look at simple density
estimates for the two classes and the two variables (figure 15.6). this indicates
that conservative subjects tend to have relatively larger amygdalas and relatively
smaller accs, though with very considerable overlap. (we are not looking at the
uncertainty here at all.)

enough preliminaries; let   s    nd the relative distribution (figure 15.7).

acc.rel <- reldist(y = n90$acc[n90$orientation < 3], yo = n90$acc[n90$orientation >

2], ci = true, yolabs = pretty(n90$acc[n90$orientation > 2]), main = "relative density of adjusted acc volume")

12 we saw how to do this in the homework
13 i am grateful to dr. kanai for graciously sharing the data.
14 this implies no value judgment about conservatives being    weird   , but rather re   ects the fact that

there are many fewer of them than of non-conservatives in this data.

334

relative distributions and smooth tests

par(mfrow = c(2, 1))
plot(density(n90$amygdala[n90$orientation > 2]), main = "", xlab = "adjusted amygdala volume")
lines(density(n90$amygdala[n90$orientation < 3]), lty = "dashed")
plot(density(n90$acc[n90$orientation < 3]), lty = "dashed", main = "", xlab = "adjusted acc volume")
lines(density(n90$acc[n90$orientation > 2]))
par(mfrow = c(1, 1))

figure 15.6 estimated densities for the (adjusted) volume of the amygdala
(upper panel) and acc (lower panel) in non-conservative (solid lines) and
conservative (dashed) students.

the    rst argument is the comparison sample; the second is the reference sam-
ple. the labeling of the horizontal axis is in terms of the quantiles of the ref-
erence distribution; i convert this back to the original units with the optional

   0.10   0.050.000.050.100246812adjusted amygdala volumedensity   0.04   0.020.000.020.040102030adjusted acc volumedensity15.2 relative distributions

335

yolabs argument15. the dots show a pointwise 95%-con   dence band, but based
on asymptotic approximations which should not be taken seriously when there
are only 77 reference samples and just 13 comparison samples.

15.2.2.2 example: economic growth rates

for a second example, let   s return to the oecd data on economic growth featured
in chapter 14. we want to know how the economic growth rates of countries which
are already economically developed compares to the growth rates of developing
and undeveloped countries. i approximate    is a developed country    by    is a
membership of the oecd   , as in   14.5.1. i will take the non-developed countries
as the reference distribution and the oecd members as the comparison group,
mostly because there are more of the former and they are more diverse.

the basic commands now go as before (aside from loading the data from a

di   erent library):

examining the resulting plot (figure 15.8), the relative distribution is uni-
modal, peaking around the 60th percentile of the reference distribution, a growth
rate of about 2.5% per year. the relative distribution drops below 1 at both low
(negative) or high (> 0.05%) growth rates     developed countries, at least over
the period of this data, tend to grow steadily and within a fairly narrow band,
without so much of both the positive and negative extremes of non-developed
countries16

it   s also worth illustrating how to use reldist for comparison to a theoretical
cdf. a very primitive, or better yet nihilistic, model of economic growth would
say that the factors causing economies to grow or shrink are so many, and so
various, and so complicated that there is no hope of tracking them systematic,
but rather that we should regard them as e   ectively random. as we know from
introductory id203, the average of many small independent terms has a
nearly gaussian distribution; so we   ll just assume that each country grows (or
shrinks) by some independent gaussian amount every year.

doing this just means applying the cumulative distribution function of the
model   s distribution to the values from our comparison sample, as in figure 15.9.
the result does not look too di   erent from figure 15.8. (this does not mean that
the nihilistic model of economic growth is right.)

15.2.3 adjusting for covariates

another nice use of relative distributions is in adjusting for covariates or pre-
dictors more    exibly than is easy to do with regression. suppose that we have

15 the function pretty() is a built-in routine for coming up with reasonable axis tick-marks from a

vector. see help(pretty).

16 it   s easy to tell a story for why the distribution of growth rates for poor countries is so wide. some

poor countries grow very slowly or even shrink because they su   er from poor institutions,
corruption, war, lack of resources, technological backwardness, etc.; some poor countries grow very
quickly if they over-come or escape these obstacles and can quickly make use of technologies
developed elsewhere. nobody has a particular good story for why the growth rates of all developed
countries are so similar.

336

relative distributions and smooth tests

par(mfrow = c(2, 1))
reldist(y = n90$amygdala[n90$orientation < 3], yo = n90$amygdala[n90$orientation >

2], ci = true, yolabs = pretty(n90$amygdala[n90$orientation > 2]), main = "relative density of adjusted amygdala volume")

reldist(y = n90$acc[n90$orientation < 3], yo = n90$acc[n90$orientation > 2],

ci = true, yolabs = pretty(n90$acc[n90$orientation > 2]), main = "relative density of adjusted acc volume")

par(mfrow = c(1, 1))

figure 15.7 relative distribution of adjusted brain-region volumes,
contrasting conservative subjects (comparison samples) to non-conservative
subjects (reference samples). dots indicate 95% con   dence limits, but these
are based on asymptotic approximations which don   t apply here. (the
supposed lower limit for the relative density of the amygdala is almost
always negative!) the dashed lines mark a relative density of 1, which would
be no di   erence.

relative density of adjusted amygdala volumereference proportionrelative density0.901.001.100.00.20.40.60.81.0   0.1   0.0500.05relative density of adjusted acc volumereference proportionrelative density0.51.52.53.50.00.20.40.60.81.0   0.0400.020.0415.2 relative distributions

337

reldist(y = oecdpanel$growth[in.oecd], yo = oecdpanel$growth[!in.oecd], yolabs = pretty(oecdpanel$growth[!in.oecd]),

ci = true, ylim = c(0, 3))

figure 15.8 relative distribution of the per-capita gdp growth rates of
oecd-member countries compared to those of non-oecd countries.

measurements of two variables, x and z. in general, when we move from the
reference population to the comparison population, both variables will change
their marginal distributions. if the marginal distribution of z changes, and the
conditional distribution of x given z did not, then the marginal distribution of x
would change. it is often informative to know how the change in the distribution
of x compares to what would be anticipated just from the change in z:

reference proportionrelative density0.00.51.01.52.02.53.00.00.20.40.60.81.0   0.1500.050.1338

relative distributions and smooth tests

growth.mean <- mean(oecdpanel$growth[!in.oecd])
growth.sd <- sd(oecdpanel$growth[!in.oecd])
r = pnorm(oecdpanel$growth[in.oecd], growth.mean, growth.sd)
reldist(y = r, ci = true, ylim = c(0, 3))
top.ticks <- (1:9)/10
top.tick.values <- signif(qnorm(top.ticks, growth.mean, growth.sd), 2)
axis(side = 3, at = top.ticks, labels = top.tick.values)

figure 15.9 distribution of the growth rates of developed countries,
relative to a gaussian    tted to all growth rates.

    the two populations might be male and female workers in the same industry,

with x income and z (say) education, or some measure of quali   cations.

    the two populations might be students at two di   erent schools, or taught in

reference proportionrelative density0.00.51.01.52.02.53.00.00.20.40.60.81.0   0.0250.000470.0180.0350.0615.2 relative distributions

339

two di   erent ways, with x their test scores at the end of the year, and z some
measure of prior knowledge.

write the conditional density of x given z in the reference population as
f0(x|z). then, just from the de   nitions of conditional and marginal id203,

f0(x) =

f0(x|z)f0(z)dz

(15.36)

if the distribution of the covariate z is instead taken from the comparison pop-
ulation, we get a di   erent distribution for x,

f0c(x) =

f0(x|z)f (z)dz

(15.37)

(cid:90)

(cid:90)

with the c standing for    covariate    or    compensated   , depending on who you
talk to. this is the distribution we would have seen for x if the distribution of
x shifted but the relation between x and z did not.

before, we looked at the relative distribution of the comparison distribution
f to the reference distribution f0, which had the density (eq. 15.35) g(y) =
f (q0(y))/f0(q0(y)). notice that

f (q0(y))
f0(q0(y))

=

f0c(q0(y))
f0(q0(y))

f (q0(y))
f0c(q0(y))

(15.38)

the    rst ratio on the right-hand side the relative density of f0c compared to f0;
the second ratio is the relative density of f compared to f0c.

i have written everything as though z were just a scalar, but it could be a
vector, so we can adjust for multiple covariates at once. also, it is important to
emphasize that there is no implication that z is in any sense the cause of x here
(though such adjustments are often more interesting when that   s true).

15.2.3.1 example: adjusting growth rates

let   s look at an example of his this works. the oecdpanel data set also includes
a variable called humancap, which is the log of the average number of years of
education of people over the age of    fteen17. how do the growth rates of developed
countries compare to those of undeveloped countries once we adjust for education?
as figure 15.10 shows, after adjusting for education levels, the relative density
shifts somewhat to the left, with its peak peaked closer to the median of the refer-
ence distribution. that is, some of the higher-than-usual growth of the developed
countries can be explained away by their (unusually high: figure 15.11) levels of
education. but the relative density is now even more sharply peaked than it was
before.

again, it would be rash to read too much causality into this. it could be that

17 if you look at help(oecdpanel), it calls this variable    average secondary school enrollment rate   ,

but that   s clearly wrong, and examining the original papers referenced there shows the correct
meaning of the variable. i am not sure why it was logged. (incidentally, humancap stands for    human
capital   . whether education is best thought of in this way, or indeed whether years of schooling are
a good measure of human capital, are hard questions which we fortunately do not have to answer.)

340

relative distributions and smooth tests

reldist(y = oecdpanel$growth[in.oecd], yo = oecdpanel$growth[!in.oecd], yolabs = pretty(oecdpanel$growth[!in.oecd]),

z = oecdpanel$humancap[in.oecd], zo = oecdpanel$humancap[!in.oecd], decomp = "covariate",
ci = true, ylim = c(0, 4))

figure 15.10 relative distribution of per-capita gdp growth rates after
adjusting for education (humancap).

education promotes economic growth18, or it could be that education is a luxury
of rich societies, which grow faster than average for other reasons.

18 certainly it   s convenient for a teacher to think so.

reference proportionrelative density012340.00.20.40.60.81.0   0.1500.050.115.3 further reading

341

reldist(y = exp(oecdpanel$humancap[in.oecd]), yo = exp(oecdpanel$humancap[!in.oecd]),

yolabs = pretty(exp(oecdpanel$humancap[!in.oecd])))

figure 15.11 relative distribution of years of education, comparing oecd
countries to non-oecd countries.

15.3 further reading

on smooth tests of goodness of    t, see bera and ghosh (2002) (a pleasantly
enthusiastic paper) and rayner and best (1989). the ddst package is ultimately
based on kallenberg and ledwina (1997). on relative distributions, see handcock
and morris (1998) (an expository paper aimed at social scientists) and handcock
and morris (1999) (a more comprehensive book with technical details).

reference proportionrelative density0510150.00.20.40.60.81.002468342

relative distributions and smooth tests

exercises

15.1   15.1.3.1 asserts that one could use cosines orthonormal basis functions in a neyman test,
with hj (x) = cj cos 2  jx. find an expression for the normalizing constant cj such that
these functions satisfy eq. 15.18 and eq. 15.19.

15.2 prove eq. 15.24. hint: change of variables. also, prove that

(cid:90)    

(cid:80)d

(cid:90) 1

(cid:80)d

j=1   j hj (f (x)) dx =

      

f (x) exp

(15.39)
15.3 if x     pareto(  , x0), then log x/x0     exp(  )     the log of a power-law distributed
variable has an exponential distribution. using the wealth.dat data from chapter 6 and
ddst.exp.test, test whether net worths over $3    108 follow a pareto distribution.

exp

0

j=1   j hj (y) dy = z(  )

15.4 let t = h(x) for some    xed and strictly monotonic function h. prove that the relative
density of t is the same as the relative density of x. hint:    nd the density of t under
both the reference and comparison distribution in terms of f0, f and h.

16

principal components analysis

in chapter 14, we saw that kernel density estimation gives us, in principle, a
consistent way of nonparametrically estimating joint distributions for arbitrarily
many variables. we also saw (  14.4.2) that, like regression (  8.3), density estima-
tion su   ers from the curse of dimensionality     the amount of data needed grows
exponentially with the number of variables. moreover, this is not a    aw in kernel
methods, but re   ects the intrinsic di   culty of the problem.

accordingly, to go forward in multivariate data analysis, we need to somehow
lift the curse of dimensionality. one approach is to hope that while we have
a large number p of variables, the data is really only q-dimensional, and q (cid:28)
p. the next few chapters will explore various ways of    nding low-dimensional
structure. alternatively, we could hope that while the data really does have lots
of dimensions, it also has lots of independent parts. at an extreme, if it had
p dimensions but we knew they were all statistically independent, we   d just do
p one-dimensional density estimates. chapter 20 and its sequels are concerned
with this second approach, of factoring the joint distribution into independent or
conditionally-independent pieces.

principal components analysis (pca) is one of a family of techniques for
taking high-dimensional data, and using the dependencies between the variables
to represent it in a more tractable, lower-dimensional form, without losing too
much information. pca is one of the simplest and most robust ways of doing
such id84. the hope with pca is that the data lie in, or
close to, a low-dimensional linear subspace.

16.1 mathematics of principal components

we start with p-dimensional vectors, and want to summarize them by projecting
down into a q-dimensional subspace. our summary will be the projection of the
original vectors on to q directions, the principal components, which span the
sub-space.

there are several equivalent ways of deriving the principal components math-
ematically. the simplest one is by    nding the projections which maximize the
variance. the    rst principal component is the direction in space along which pro-
jections have the largest variance. the second principal component is the direction
which maximizes variance among all directions orthogonal to the    rst. the kth

343

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

344
component is the variance-maximizing direction orthogonal to the previous k     1
components. there are p principal components in all.

principal components analysis

rather than maximizing variance, it might sound more plausible to look for the
projection with the smallest average (mean-squared) distance between the origi-
nal vectors and their projections on to the principal components; this turns out
to be equivalent to maximizing the variance (as we   ll see in   16.1.1 immediately
below).

throughout, assume that the data have been    centered   , so that every variable
has mean 0. if we write the centered data in a matrix x, where rows are objects
and columns are variables, then xt x = nv, where v is the covariance matrix of
the data. (you should check that last statement!)

16.1.1 minimizing projection residuals

we   ll start by looking for a one-dimensional projection. that is, we have p-
dimensional vectors, and we want to project them on to a line through the origin.
we can specify the line by a unit vector along it, (cid:126)w, and then the projection of a
data vector (cid:126)xi on to the line is (cid:126)xi    (cid:126)w, which is a scalar. (sanity check: this gives
us the right answer when we project on to one of the coordinate axes.) this is the
distance of the projection from the origin; the actual coordinate in p-dimensional
space is ( (cid:126)xi    (cid:126)w) (cid:126)w. the mean of the projections will be zero, because the mean of
the vectors (cid:126)xi is zero:

n(cid:88)

i=1

1
n

( (cid:126)xi    (cid:126)w) (cid:126)w =

(cid:32)(cid:32)

n(cid:88)

i=1

1
n

(cid:33)

(cid:33)

   (cid:126)w

xi

(cid:126)w

(16.1)

if we try to use our projected or image vectors instead of our original vectors,
there will be some error, because (in general) the images do not coincide with the
original vectors. (when do they coincide?) the di   erence is the error or residual
of the projection. how big is it? for any one vector, say (cid:126)xi, it   s
(cid:107) (cid:126)xi     ( (cid:126)w    (cid:126)xi) (cid:126)w(cid:107)2 = ( (cid:126)xi     ( (cid:126)w    (cid:126)xi) (cid:126)w)    ( (cid:126)xi     ( (cid:126)w    (cid:126)xi) (cid:126)w)

= (cid:126)xi    (cid:126)xi     (cid:126)xi    ( (cid:126)w    (cid:126)xi) (cid:126)w

   ( (cid:126)w    (cid:126)xi) (cid:126)w    (cid:126)xi + ( (cid:126)w    (cid:126)xi) (cid:126)w    ( (cid:126)w    (cid:126)xi) (cid:126)w

= (cid:107) (cid:126)xi(cid:107)2     2( (cid:126)w    (cid:126)xi)2 + ( (cid:126)w    (cid:126)xi)2 (cid:126)w    (cid:126)w
= (cid:126)xi    (cid:126)xi     ( (cid:126)w    (cid:126)xi)2

since (cid:126)w    (cid:126)w = (cid:107) (cid:126)w(cid:107)2 = 1.

add those residuals up across all the vectors:

n(cid:88)
(cid:32) n(cid:88)

i=1

m se( (cid:126)w) =

=

1
n

1
n

(cid:107) (cid:126)xi(cid:107)2     ( (cid:126)w    (cid:126)xi)2

(cid:107) (cid:126)xi(cid:107)2     n(cid:88)

( (cid:126)w    (cid:126)xi)2

(cid:33)

i=1

i=1

the    rst summation doesn   t depend on (cid:126)w, so it doesn   t matter for trying to

(16.2)
(16.3)

(16.4)
(16.5)

(16.6)

(16.7)

16.1 mathematics of principal components

345

demo.theta <- runif(10, min = 0, max = pi/2)
demo.x <- cbind(cos(demo.theta), sin(demo.theta))
demo.x <- scale(demo.x, center = true, scale = false)
plot(demo.x, xlab = expression(x^1), ylab = expression(x^2), xlim = c(-1, 1),

ylim = c(-1, 1))

demo.w <- c(cos(-3 * pi/8), sin(-3 * pi/8))
arrows(0, 0, demo.w[1], demo.w[2], col = "blue")
text(demo.w[1], demo.w[2], pos = 4, labels = expression(w))
abline(0, b = demo.w[2]/demo.w[1], col = "blue", lty = "dashed")
projection.lengths <- demo.x %*% demo.w
projections <- projection.lengths %*% demo.w
points(projections, pch = 16, col = "blue")
segments(x0 = demo.x[, 1], y0 = demo.x[, 2], x1 = projections[, 1], y1 = projections[,

2], col = "grey")

figure 16.1 illustration of projecting data points (cid:126)x (black dots) on to an
arbitrary line through the space (blue, dashed), represented by a unit vector
(cid:126)w along the line (also blue but solid). the blue dots are the projections on
to the blue line, ((cid:126)x    (cid:126)w) (cid:126)w ; the gray lines are the vector residuals,
(cid:126)x     ((cid:126)x    (cid:126)w) (cid:126)w. these are not the residuals from regressing one of the
components of the data vector on the other.

llllllllll   1.0   0.50.00.51.0   1.0   0.50.00.51.0x1x2wlllllllllln(cid:88)

i=1

1
n

(cid:32)

n(cid:88)

i=1

1
n

k(cid:88)

346

principal components analysis

minimize the mean squared residual. to make the mse small, what we must do
is make the second sum big, i.e., we want to maximize

( (cid:126)w    (cid:126)xi)2

(16.8)

which we can see is the sample mean of ( (cid:126)w    (cid:126)xi)2. the (sample) mean of a square
is always equal to the square of the (sample) mean plus the (sample) variance:

( (cid:126)w    (cid:126)xi)2 =

(cid:126)xi    (cid:126)w

(16.9)

(cid:33)2

+(cid:98)v [ (cid:126)w    (cid:126)xi]

n(cid:88)

i=1

1
n

since we   ve just seen that the mean of the projections is zero, minimizing the
residual sum of squares is equivalent to maximizing the variance of the projec-
tions.

(of course in general we don   t want to project on to just one vector, but on to
multiple principal components. if those components are orthogonal and have the
unit vectors (cid:126)w1, (cid:126)w2, . . . (cid:126)wk, then the image of xi is its projection into the space
spanned by these vectors,

( (cid:126)xi    (cid:126)wj) (cid:126)wj

(16.10)

j=1

the mean of the projection on to each component is still zero. if we go through
the same algebra for the mean squared error, it turns [exercise 16.1] out that
the cross-terms between di   erent components all cancel out, and we are left with
trying to maximize the sum of the variances of the projections on to the compo-
nents.)

16.1.2 maximizing variance

accordingly, let   s maximize the variance! writing out all the summations grows
tedious, so let   s do our algebra in matrix form. if we stack our n data vectors
into an n    p matrix, x, then the projections are given by xw, which is an n    1
matrix. the variance is

(cid:98)v [ (cid:126)w    (cid:126)xi] =

(cid:88)

( (cid:126)xi    (cid:126)w)2

i

(xw)t (xw)

=

1
n

1
n
1
n

wt xt xw

=
= wt xt x
n
= wt vw

w

(16.11)

(16.12)

(16.13)

(16.14)

we want to chose a unit vector (cid:126)w so as to maximize (cid:98)v [ (cid:126)w    (cid:126)xi]. to do this, we

(16.15)

16.1 mathematics of principal components

347

need to make sure that we only look at unit vectors     we need to constrain the
maximization. the constraint is that (cid:126)w    (cid:126)w = 1, or wt w = 1. to enforce this
constraint, we introduce a lagrange multiplier    (appendix h.3) and do a larger
unconstrained optimization:

l(w,   )     wt vw       (wt w     1)

   l
     
   l
   w

= wt w     1
= 2vw     2  w

setting the derivatives to zero at the optimum, we get

wt w = 1

vw =   w

(16.16)

(16.17)

(16.18)

(16.19)
(16.20)

thus, the desired vector w is an eigenvector1 of the covariance matrix v, and
the maximizing vector will be the one associated with the largest eigenvalue   .
this is good news, because    nding eigenvectors is something which can be done
comparatively rapidly, and because eigenvectors have many nice mathematical
properties, which we can use as follows.
we know that v is a p  p matrix, so it will have at most p di   erent eigenvectors.
we know that v is a covariance matrix, so it is symmetric, and then id202
tells us that the eigenvectors must be orthogonal to one another. again because
v is a covariance matrix, it is a non-negative-de   nite2 matrix, in the sense
that (cid:126)x    v(cid:126)x     0 for any (cid:126)x. this tells us that the eigenvalues of v must all be     0.
the eigenvectors of v are the principal components of the data. because
we know they are orthogonal, together they span the whole p-dimensional space.
the    rst principal component, i.e. the eigenvector which goes the largest value
of   , is the direction along which the data have the most variance. the second
principal component, i.e. the second eigenvector, is the direction orthogonal to
the    rst component with the most variance. because it is orthogonal to the    rst
eigenvector, their projections will be uncorrelated. in fact, projections on to all
the principal components are uncorrelated with each other. if we use q principal
components, our weight matrix w will be a p   q matrix, where each column will
be a di   erent eigenvector of the covariance matrix v. the eigenvalues will give
variance of the projection on to each component. the variance of the projections

on to the    rst q principal components is then(cid:80)q

i=1   i.

16.1.3 more geometry; back to the residuals

if we use all p principal components, the matrix w is a p    p matrix, where each
column is an eigenvector of v. the product xw is a new n    p matrix, in which
each column (the projection on to an eigenvector) is uncorrelated with every other

1 if your memory of eigenvectors and eigenvalues is hazy, see appendix b.
2 or    positive semi-de   nite   .

348

principal components analysis

column. because eigenvectors are orthogonal and normalized, wt w = i, i.e.,
wt = w   1, so w is itself an orthogonal matrix. since the outstanding examples
of orthogonal matrices are rotation matrices, w is often called the rotation
matrix of the principal components analysis. it tells us how to rotate from the
original coordinate system to a new system of uncorrelated coordinates.
suppose that the data really are q-dimensional. then v will have only q positive
eigenvalues, and p     q zero eigenvalues. if the data fall near a q-dimensional
subspace, then p     q of the eigenvalues will be nearly zero.
if we pick the top q components, we can de   ne a projection operator pq. the
images of the data are then xpq. the projection residuals are x     xpq or
x(i     pq). (notice that the residuals here are vectors, not just magnitudes.) if
the data really are q-dimensional, then the residuals will be zero. if the data are
approximately q-dimensional, then the residuals will be small. in any case, we can
de   ne the r2 of the projection as the fraction of the original variance kept by the
image vectors,

(16.21)

(cid:80)q
(cid:80)p

i=1   i
j=1   j

r2    

just as the r2 of a id75 is the fraction of the original variance of the
dependent variable kept by the    tted values.

the q = 1 case is especially instructive. we know that the residual vectors
are all orthogonal to the projections. suppose we ask for the    rst principal com-
ponent of the residuals. this will be the direction of largest variance which is
perpendicular to the    rst principal component. in other words, it will be the
second principal component of the data. this suggests a recursive algorithm for
   nding all the principal components: the kth principal component is the leading
component of the residuals after subtracting o    the    rst k     1 components. in
practice, it is faster to get all the components at once from v   s eigenvectors, but
this idea is correct in principle.

this is a good place to remark that if the data really fall in a q-dimensional
subspace, then v will have only q positive eigenvalues, because after subtracting
o    those components there will be no residuals. the other p    q eigenvectors will
all have eigenvalue 0. if the data cluster around a q-dimensional subspace, then
p     q of the eigenvalues will be very small, though how small they need to be
before we can neglect them is a tricky question.3

projections on to the    rst two or three principal components can be visualized;
there is no guarantee, however, that only two or three dimensions really matter.
usually, to get an r2 of 1, you need to use all p principal components.4 how many

3 be careful when n < p. any two points de   ne a line, and three points de   ne a plane, etc., so if there

are fewer data points than variables, it is necessarily true that the fall on a low-dimensional
subspace. in   16.4.1, we represent stories in the new york times as vectors with p     440, but
n = 102. finding that only 102 principal components keep all the variance is not an empirical
discovery but a mathematical artifact.

4 the exceptions are when some of your variables are linear combinations of the others, so that you

don   t really have p di   erent variables, or when, as just mentioned, n < p.

16.1 mathematics of principal components

349

principal components you should use depends on your data, and how big an r2
you need. sometimes, you can get better than 80% of the variance described with
just two or three components. sometimes, however, to keep a lot of the original
variance you need to use almost as many components as you had dimensions to
start with.

16.1.3.1 scree plots

people sometimes like to make plots of the eigenvalues, in decreasing order, as
in figure 16.3. ideally, one starts with a few big eigenvalues, and then sees a
clear drop-o    to a remainder of small, comparatively negligible eigenvalues. these
diagrams are called scree plots5. (some people make similar plots, but show
1    r2 versus the number of components, rather than the individual eigenvalues.)
folklore recommends    nd the    base of the cli       or    elbow    in the plot, the place
where the number eigenvalues decrease dramatically and then level o    to the
right, and then retaining that number of components. this folklore appears to be
based on nothing more than intuition, and o   ers no recommendation for what to
do when there is no clear cli    or elbow in the scree plot.

16.1.4 statistical id136, or not

you may have noticed, and even been troubled by, the fact that i have said
nothing at all in this chapter like    assume the data are drawn at random from
some distribution   , or    assume the di   erent rows of the data frame are statisti-
cally independent   . this is because no such assumption is required for principal
components. all it does is say    these data can be summarized using projections
along these directions   . it says nothing about the larger population or stochastic
process the data came from; it doesn   t even suppose there is a larger population
or stochastic process. this is part of why   16.1.3 was so wishy-washy about the
right number of components to use.

however, we could add a statistical assumption and see how pca behaves
under those conditions. the simplest one is to suppose that the data come iidly
from a distribution with covariance matrix v0. then the sample covariance matrix
v     n   1xt x will converge on v0 as n        . since the principal components are
smooth functions of v (namely its eigenvectors), they will tend to converge as
n grows6. so, along with that additional assumption about the data-generating

5 the small loose rocks one    nds at the base of cli   s or mountains are called    scree   ; the metaphor is
that one starts with the big eigenvalues at the top of the hill, goes down some slope, and then    nds
the scree beneath it, which is supposed to be negligible noise. those who have had to cross scree
   elds carrying heavy camping backpacks may disagree about whether it can really be ignored.

6 there is a wrinkle if v0 has    degenerate    eigenvalues, i.e., two or more eigenvectors with the same

eigenvalue. then any linear combination of those vectors is also an eigenvector, with the same
eigenvalue (exercise 16.2.) for instance, if v0 is the identity matrix, then every vector is an
eigenvector, and pca routines will return an essentially arbitrary collection of mutually
perpendicular vectors. generically, however, any arbitrarily small tweak to v0 will break the
degeneracy.

350

principal components analysis

variable

meaning

binary indicator for being a sports car
indicator for sports utility vehicle
indicator
indicator
indicator
indicator for all-wheel drive
indicator for rear-wheel drive
suggested retail price (us$)
price to dealer (us$)
engine size (liters)
number of engine cylinders
engine horsepower
city gas mileage

sports
suv
wagon
minivan
pickup
awd
rwd
retail
dealer
engine
cylinders
horsepower
citympg
highwaympg highway gas mileage
weight
wheelbase
length
width

weight (pounds)
wheelbase (inches)
length (inches)
width (inches)

table 16.1 features for the 2004 cars data.

## error in knitr(head(cars)): could not find function "knitr"

table 16.2 the    rst few lines of the 2004 cars data set.

process, pca does make a prediction: in the future, the principal components
will look like they do now.

we could always add stronger statistical assumptions; in fact, chapter 17 will
look at what happens when our assumptions essentially amount to    the data lie
on a low-dimensional linear subspace, plus noise   . even this, however, turns out
to make pca a not-very-attractive estimate of the statistical structure.

16.2 example 1: cars

enough math; let   s work an example. the data7 consists of 388 cars from the
20048 model year, with 18 features. eight features are binary indicators; the
other 11 features are numerical (table 16.1). table 16.2 shows the    rst few lines
from the data set. pca only works with numerical variables, so we have ten of
them to play with.

cars04 = read.csv("http://www.stat.cmu.edu/~cshalizi/adafaepov/data/cars-fixed04.dat")

7 on the course website; from http://www.amstat.org/publications/jse/datasets/04cars.txt,

with incomplete records removed.

8 i realize this is a bit antiquated by the time you read this. you will    nding it character-building to

track down comparable data from your own time, and repeating the analysis.

16.2 example 1: cars

351

there are two r functions for doing pca, princomp and prcomp, which di   er
in how they do the actual calculation.9 the latter is generally more robust, so
we   ll just use it.

cars04.pca = prcomp(cars04[, 8:18], scale. = true)

the second argument to prcomp tells it to    rst scale all the variables to have
variance 1, i.e., to standardize them. you should experiment with what happens
with this data when we don   t standardize.

we can now extract the loadings or weight matrix from the cars04.pca object.

for comprehensibility i   ll just show the    rst two components.

pc1

round(cars04.pca$rotation[, 1:2], 2)
pc2
##
-0.26 -0.47
## retail
-0.26 -0.47
## dealer
-0.35
0.02
## engine
## cylinders
-0.33 -0.08
## horsepower -0.32 -0.29
0.00
## citympg
0.01
## highwaympg
0.17
## weight
0.42
## wheelbase
## length
0.41
0.31
## width

0.31
0.31
-0.34
-0.27
-0.26
-0.30

this says that all the variables except the gas-mileages have a negative projec-
tion on to the    rst component. this means that there is a negative correlation
between mileage and everything else. the    rst principal component tells us about
whether we are getting a big, expensive gas-guzzling car with a powerful engine,
or whether we are getting a small, cheap, fuel-e   cient car with a wimpy engine.
the second component is a little more interesting. engine size and gas mileage
hardly project on to it at all. instead we have a contrast between the physical
size of the car (positive projection) and the price and horsepower. basically, this
axis separates mini-vans, trucks and suvs (big, not so expensive, not so much
horse-power) from sports-cars (small, expensive, lots of horse-power).

to check this interpretation, we can use a useful tool called a biplot, which
plots the data, along with the projections of the original variables, on to the    rst
two components (figure 16.2). notice that the car with the lowest value of the
second component is a porsche 911, with pick-up trucks and mini-vans at the
other end of the scale. similarly, the highest values of the    rst component all
belong to hybrids.

9 princomp actually calculates the covariance matrix and takes its eigenvalues. prcomp uses a di   erent

technique called    singular value decomposition   .

352

principal components analysis

biplot(cars04.pca, cex = 0.4)

figure 16.2    biplot    of the 2004 cars data. the horizontal axis shows
projections on to the    rst principal component, the vertical axis the second
component. car names are written at their projections on to the
components (using the coordinate scales on the top and the right). red
arrows show the projections of the original variables on to the principal
components (using the coordinate scales on the bottom and on the left).

   0.3   0.2   0.10.00.1   0.3   0.2   0.10.00.1pc1pc2acura 3.5 rlacura 3.5 rl navigationacura mdxacura nsx sacura rsxacura tlacura tsxaudi a4 1.8taudi a4 1.8t convertibleaudi a4 3.0 convertibleaudi a4 3.0audi a4 3.0 quattro manualaudi a4 3.0 quattro autoaudi a4 3.0 quattro convertibleaudi a6 2.7 turbo quattro four   dooraudi a6 3.0audi a6 3.0 avant quattroaudi a6 3.0 quattroaudi a6 4.2 quattroaudi a8 l quattroaudi s4 avant quattroaudi s4 quattroaudi rs 6audi tt 1.8audi tt 1.8 quattroaudi tt 3.2bmw 325ibmw 325cibmw 325ci convertiblebmw 325xibmw 325xi sportbmw 330cibmw 330ci convertiblebmw 330ibmw 330xibmw 525i four   doorbmw 530i four   doorbmw 545ia four   doorbmw 745i four   doorbmw 745li four   doorbmw m3bmw m3 convertiblebmw x3 3.0ibmw x5 4.4ibmw z4 convertible 2.5i two   doorbmw z4 convertible 3.0i two   doorbuick century custombuick lesabre custom four   doorbuick lesabre limitedbuick park avenuebuick park avenue ultrabuick rainierbuick regal gsbuick regal lsbuick rendezvous cxcadillac cts vvtcadillac devillecadillac deville dtscadillac escaladetcadillac sevillecadillac srx v8cadillac xlrchevrolet aveochevrolet astrochevrolet aveo lschevrolet cavalier two   doorchevrolet cavalier four   doorchevrolet cavalier lschevrolet corvettechevrolet corvette convertiblechevrolet impalachevrolet impala lschevrolet impala sschevrolet malibuchevrolet malibu lschevrolet malibu ltchevrolet malibu maxxchevrolet monte carlo lschevrolet monte carlo sschevrolet suburban 1500 ltchevrolet tahoe ltchevrolet trackerchevrolet trailblazer ltchevrolet venture lschrysler 300mchrysler 300m special editionchrysler concorde lxchrysler concorde lxichrysler crossfirechrysler pacificachrysler pt cruiserchrysler pt cruiser gtchrysler pt cruiser limitedchrysler sebringchrysler sebring touringchrysler sebring convertiblechrysler sebring limited convertiblechrysler town and country lxchrysler town and country limiteddodge caravan sedodge durango sltdodge grand caravan sxtdodge intrepid esdodge intrepid sedodge neon sedodge neon sxtdodge stratus sxtdodge stratus seford crown victoriaford crown victoria lxford crown victoria lx sportford escape xlsford expedition 4.6 xltford explorer xlt v6ford focus lxford focus seford focus svtford focus ztwford focus zx5ford focus zx3ford freestar seford mustangford mustang gt premiumford taurus lxford taurus seford taurus ses duratecford thunderbird deluxegmc envoy xuv slegmc safari slegmc yukon 1500 slegmc yukon xl 2500 slthonda accord exhonda accord ex v6honda accord lxhonda accord lx v6honda civic exhonda civic dxhonda civic hxhonda civic lxhonda civic sihonda civic hybridhonda cr   v lxhonda element lxhonda insighthonda odyssey exhonda odyssey lxhonda pilot lxhonda s2000hummer h2hyundai accenthyundai accent glhyundai accent gthyundai elantra glshyundai elantra gthyundai elantra gt hatchhyundai santa fe glshyundai sonata glshyundai sonata lxhyundai tiburon gt v6hyundai xg350hyundai xg350 linfiniti fx35infiniti fx45infiniti g35infiniti g35 sport coupeinfiniti g35 awdinfiniti i35infiniti m45infiniti q45 luxuryisuzu ascender sisuzu rodeo sjaguar s   type 3.0jaguar s   type 4.2jaguar s   type rjaguar vanden plasjaguar x   type 2.5jaguar x   type 3.0jaguar xj8jaguar xjrjaguar xk8 coupejaguar xk8 convertiblejaguar xkr coupejaguar xkr convertiblejeep grand cherokee laredojeep liberty sportjeep wrangler saharakia optima lxkia optima lx v6kia rio autokia rio manualkia rio cincokia sedona lxkia sorento lxkia spectrakia spectra gskia spectra gsxland rover discovery seland rover freelander seland rover range rover hselexus es 330lexus gs 300lexus gs 430lexus gx 470lexus is 300 manuallexus is 300 autolexus is 300 sportcrosslexus ls 430lexus lx 470lexus sc 430lexus rx 330lincoln aviator ultimatelincoln ls v6 luxurylincoln ls v6 premiumlincoln ls v8 sportlincoln ls v8 ultimatelincoln navigator luxurylincoln town car signaturelincoln town car ultimatelincoln town car ultimate lmazda6 imazda mpv esmazda mx   5 miatamazda mx   5 miata lsmazda tribute dx 2.0mercedes   benz c32 amgmercedes   benz c230 sportmercedes   benz c240mercedes   benz c240 rwdmercedes   benz c240 awdmercedes   benz c320mercedes   benz c320 sport two   doormercedes   benz c320 sport four   doormercedes   benz cl500mercedes   benz cl600mercedes   benz clk320mercedes   benz clk500mercedes   benz e320mercedes   benz e320 four   doormercedes   benz e500mercedes   benz e500 four   doormercedes   benz g500mercedes   benz ml500mercedes   benz s430mercedes   benz s500mercedes   benz sl500mercedes   benz sl55 amgmercedes   benz sl600mercedes   benz slk230mercedes   benz slk32 amgmercury grand marquis gsmercury grand marquis ls premiummercury grand marquis ls ultimatemercury maraudermercury monterey luxurymercury mountaineermercury sable gsmercury sable gs four   doormercury sable ls premiummini coopermini cooper smitsubishi diamante lsmitsubishi eclipse gtsmitsubishi eclipse spyder gtmitsubishi endeavormitsubishi galantmitsubishi lancer evolutionmitsubishi monteromitsubishi outlandernissan 350znissan 350z enthusiastnissan altima snissan altima senissan maxima senissan maxima slnissan muranonissan pathfinder senissan pathfinder armada senissan quest snissan quest senissan sentra 1.8nissan sentra 1.8 snissan sentra se   rnissan xterra xeoldsmobile alero glsoldsmobile alero gxoldsmobile silhouette glpontiac aztektporsche cayenne spontiac grand am gtpontiac grand prix gt1pontiac grand prix gt2pontiac montanapontiac montana ewbpontiac sunfire 1sapontiac sunfire 1scpontiac vibeporsche 911 carreraporsche 911 carrera 4sporsche 911 targaporsche 911 gt2porsche boxsterporsche boxster ssaab 9   3 arcsaab 9   3 arc sportsaab 9   3 aerosaab 9   3 aero convertiblesaab 9   5 arcsaab 9   5 aerosaab 9   5 aero four   doorsaturn ion1saturn ion2saturn ion2 quad coupesaturn ion3saturn ion3 quad coupesaturn l300saturn l300   2saturn vuescion xascion xbsubaru forestersubaru impreza 2.5 rssubaru impreza wrxsubaru impreza wrx stisubaru legacy gtsubaru legacy lsubaru outbacksubaru outback limited sedansubaru outback h6subaru outback h   6 vdcsuzuki aeno ssuzuki aerio lxsuzuki aerio sxsuzuki forenza ssuzuki forenza exsuzuki verona lxsuzuki vitara lxsuzuki xl   7 extoyota 4runner sr5 v6toyota avalon xltoyota avalon xlstoyota camry letoyota camry le v6toyota camry xle v6toyota camry solara setoyota camry solara se v6toyota camry solara sle v6 two   doortoyota celicatoyota corolla cetoyota corolla stoyota corolla letoyota echo two   door manualtoyota echo two   door autotoyota echo four   doortoyota highlander v6toyota land cruisertoyota matrixtoyota mr2 spydertoyota priustoyota rav4toyota sequoia sr5toyota sienna cetoyota sienna xlevolkswagen golfvolkswagen gtivolkswagen jetta glvolkswagen jetta glivolkswagen jetta glsvolkswagen new beetle gls 1.8tvolkswagen new beetle gls convertiblevolkswagen passat glsvolkswagen passat gls four   doorvolkswagen passat glx v6 4motion four   doorvolkswagen passat w8volkswagen passat w8 4motionvolkswagen touareg v6volvo c70 lptvolvo c70 hptvolvo s40volvo s60 2.5volvo s60 t5volvo s60 rvolvo s80 2.5tvolvo s80 2.9volvo s80 t6volvo v40volvo xc70volvo xc90 t6   30   20   10010   30   20   10010retaildealerenginecylindershorsepowercitympghighwaympgweightwheelbaselengthwidth16.2 example 1: cars

353

plot(cars04.pca, type = "l", main = "")

figure 16.3 scree plot of the 2004 cars data: the eigenvalues of the
principal components, in decreasing order. each eigenvalue is the variance
along that component. folklore suggests adding components until the plot
levels o   , or goes past an    elbow        here this might be 2 or 3 components.

llllllllllvariances0123456712345678910354

principal components analysis

16.3 example 2: the united states circa 1977

r contains a built-in data    le, state.x77, with facts and    gures for the various
states of the usa as of about 1977: population, per-capita income, the adult
illiteracy rate, life expectancy, the homicide rate, the proportion of adults with
at least a high-school education, the number of days of frost a year, and the state   s
area. while this data set is almost as old as i am10, it still makes a convenient
example, so let   s step through a principal components analysis of it.

since the variables all have di   erent, incomparable scales, it   s not a bad idea

to scale them to unit variance before    nding the components11:

state.pca <- prcomp(state.x77, scale. = true)

the biplot and the scree plot (figure 16.4) look reasonable.
with this reasonable-looking pca, we might try to interpret the components.

pc1
0.130
-0.300
0.470

signif(state.pca$rotation[, 1:2], 2)
##
## population
## income
## illiteracy
## life exp
## murder
## hs grad
## frost
## area

pc2
0.410
0.520
0.053
-0.410 -0.082
0.310
0.440
-0.420
0.300
-0.360 -0.150
-0.033
0.590

the    rst component aligns with illiteracy, murder, and (more weakly) popu-
lation; it   s negatively aligned with high school graduation, life expectancy, cold
weather, income, and (very weakly) the area of the state. the second component
is positively algined with area, income, population, high school graduation and
murder, and negativey aligned, weakly, with cold weather and life expectancy.
the    rst component thus separates short-lived, violent, ill-educated, poor warm
states from those with the opposite qualities. the second component separates
big, rich, educated, violent states from those which are small (in land or people),
poor, less educated, and less violent.

since each data point has a geographic location, we can make a map, where
the sizes of the symbols for each state vary with their projection on to the    rst
principal component. this suggests that the component is something we might
call    southeid56ess        more precisely, the contrast between the south and the
rest of the nation12. i will leave making a map of the second component as an
exercise13.

10 again, readers will    nd it character-building to    nd more modern data on which to repeat the

exercise.

11 you should try re-running all this with scale.=false, and ponder what the experience tells you

about the wisdom of advice like    maximize r2   , or even    minimize the approximation error   .

12 the correlation between the    rst component and an indicator for being in the confederacy is 0.8;

for being a state which permitted slavery when the civil war began, 0.78.

13   17.9.1 has more on this example.

16.3 example 2: the united states circa 1977

355

biplot(state.pca, cex = c(0.5, 0.75))
plot(state.pca, type = "l")

figure 16.4 biplot and scree plot for the pca of state.x77.

   0.20.00.20.40.6   0.20.00.20.40.6pc1pc2alabamaalaskaarizonaarkansascaliforniacoloradoconnecticutdelawarefloridageorgiahawaiiidahoillinoisindianaiowakansaskentuckylouisianamainemarylandmassachusettsmichiganminnesotamississippimissourimontananebraskanevadanew hampshirenew jerseynew mexiconew yorknorth carolinanorth dakotaohiooklahomaoregonpennsylvaniarhode islandsouth carolinasouth dakotatennesseetexasutahvermontvirginiawashingtonwest virginiawisconsinwyoming   5051015   5051015populationincomeilliteracylife expmurderhs gradfrostareallllllllstate.pcavariances0.00.51.01.52.02.53.03.512345678356

principal components analysis

plot.states_scaled <- function(sizes, min.size = 0.4, max.size = 2, ...) {

plot(state.center, type = "n", ...)
out.range = max.size - min.size
in.range = max(sizes) - min(sizes)
scaled.sizes = out.range * ((sizes - min(sizes))/in.range)
text(state.center, state.abb, cex = scaled.sizes + min.size)
invisible(scaled.sizes)

}
plot.states_scaled(state.pca$x[, 1], min.size = 0.3, max.size = 1.5, xlab = "longitude",

ylab = "latitude")

figure 16.5 the us states, plotted in their geographic locations, with
symbol size varying with the projection of the state on to the    rst principal
component. this suggests the component is something we might call
   southeid56ess   .

   120   110   100   90   80   703035404550longitudelatitudealakazarcacoctdeflgahiidiliniakskylamemdmamimnmsmomtnenvnhnjnmnyncndohokorpariscsdtntxutvtvawawvwiwy16.4 latent semantic analysis

357

16.4 latent semantic analysis

information retrieval systems (like search engines) and people doing computa-
tional text analysis often represent documents as what are called bags of words:
documents are represented as vectors, where each component counts how many
times each word in the dictionary appears in the text. this throws away informa-
tion about word order, but gives us something we can work with mathematically.
part of the representation of one document might look like:

a abandoned abc ability able about above abroad absorbed absorbing abstract
1

10

0

0

0

0

0

0

0

0

43

and so on through to    zebra   ,    zoology   ,    zygote   , etc. to the end of the dictio-
nary. these vectors are very, very large! at least in english and similar languages,
these bag-of-word vectors have three outstanding properties:

1. most words do not appear in most documents; the bag-of-words vectors are

very sparse (most entries are zero).

2. a small number of words appear many times in almost all documents; these
words tell us almost nothing about what the document is about. (examples:
   the   ,    is   ,    of   ,    for   ,    at   ,    a   ,    and   ,    here   ,    was   , etc.)

3. apart from those hyper-common words, most words    counts are correlated
with some but not all other words; words tend to come in bunches which
appear together.

taken together, this suggests that we do not really get a lot of value from keeping
around all the words. we would be better o    if we could project down a smaller
number of new variables, which we can think of as combinations of words that
tend to appear together in the documents, or not at all. but this tendency needn   t
be absolute     it can be partial because the words mean slightly di   erent things,
or because of stylistic di   erences, etc. this is exactly what principal components
analysis does.

to see how this can be useful, imagine we have a collection of documents
(a corpus), which we want to search for documents about agriculture. it   s en-
tirely possible that many documents on this topic don   t actually contain the word
   agriculture   , just closely related words like    farming   . a simple search on    agri-
culture    will miss them. but it   s very likely that the occurrence of these related
words is well-correlated with the occurrence of    agriculture   . this means that
all these words will have similar projections on to the principal components, and
it will be easy to    nd documents whose principal components projection is like
that for a query about agriculture. this is called id45.

to see why this is indexing, think about what goes into coming up with an index
for a book by hand. someone draws up a list of topics and then goes through the
book noting all the passages which refer to the topic, and maybe a little bit of
what they say there. for example, here   s the start of the entry for    agriculture   
in the index to adam smith   s the wealth of nations:

agriculture, the labour of, does not admit of such subdivisions as manufactures, 6; this

358

principal components analysis

impossibility of separation, prevents agriculture from improving equally with manufactures,
6; natural state of, in a new colony, 92; requires more knowledge and experience than most
mechanical professions, and yet is carried on without any restrictions, 127; the terms of rent,
how adjusted between landlord and tenant, 144; is extended by good roads and navigable canals,
147; under what circumstances pasture land is more valuable than arable, 149; gardening not a
very gainful employment, 152   3; vines the most pro   table article of culture, 154; estimates of
pro   t from projects, very fallacious, ib.; cattle and tillage mutually improve each other, 220; . . .

and so on. (agriculture is an important topic in the wealth of nations.) it   s
asking a lot to hope for a computer to be able to do something like this, but
we could at least hope for a list of pages like    6, 92, 126, 144, 147, 152   3, 154,
220,. . .    . one could imagine doing this by treating each page as its own document,
forming its bag-of-words vector, and then returning the list of pages with a non-
zero entry for    agriculture   . this will fail: only two of those nine pages actually
contains that word, and this is pretty typical. on the other hand, they are full
of words strongly correlated with    agriculture   , so asking for the pages which
are most similar in their principal components projection to that word will work
great.14

at    rst glance, and maybe even second, this seems like a wonderful trick for
extracting meaning, or semantics, from pure correlations. of course there are
also all sorts of ways it can fail, not least from spurious correlations. if our
training corpus happens to contain lots of documents which mention    farming   
and    kansas   , as well as    farming    and    agriculture   , id45
will not make a big distinction between the relationship between    agriculture   
and    farming    (which is genuinely semantic, about the meaning of the words)
and that between    kansas    and    farming    (which re   ects non-linguistic facts
about the world, and probably wouldn   t show up in, say, a corpus collected from
australia).

despite this susceptibility to spurious correlations, id45
is an extremely useful technique in practice, and the foundational papers (deer-
wester et al., 1990; landauer and dumais, 1997) are worth reading.

16.4.1 principal components of the new york times

to get a more concrete sense of how latent semantic analysis works, and how
it reveals semantic information, let   s apply it to some data. the accompanying
r    le and r workspace contains some news stories taken from the new york
times annotated corpus (sandhaus, 2008), which consists of about 1.8 million
stories from the times, from 1987 to 2007, which have been hand-annotated by
actual human beings with standardized machine-readable information about their
contents. from this corpus, i have randomly selected 57 stories about art and 45
stories about music, and turned them into a bag-of-words data frame, one row
per story, one column per word; plus an indicator in the    rst column of whether

14 or it should anyway; i haven   t actually done the experiment with this book.

16.4 latent semantic analysis

359

the story is one about art or one about music.15 the original data frame thus has
102 rows, and 4432 columns: the categorical label, and 4431 columns with counts
for every distinct word that appears in at least one of the stories.16

the pca is done as it would be for any other data:

load("~/teaching/adafaepov/data/pca-examples.rdata")
nyt.pca <- prcomp(nyt.frame[, -1])
nyt.latent.sem <- nyt.pca$rotation

we need to omit the    rst column in the    rst command because it contains
categorical variables, and pca doesn   t apply to them. the second command just
picks out the matrix of projections of the variables on to the components     this
is called rotation because it can be thought of as rotating the coordinate axes
in feature-vector space.

now that we   ve done this, let   s look at what the leading components are.

[[attn:
why isn   t
loading
from the
course
website
working?]]

orchestra
0.067
east
0.049
sunday
0.045

trio
0.084
m
0.054
players
0.047
organ
0.044
april
0.040

music
0.110
theaters
0.055
jersey
0.047
symphony
0.044
x.d
0.041

theater
0.083
festival
0.051
committee
0.046
matinee
0.043
samuel
0.040

signif(sort(nyt.latent.sem[, 1], decreasing = true)[1:30], 2)
composers
##
0.059
##
program
##
0.048
##
june
##
0.045
##
misstated instruments
##
0.041
##
pianist
##
##
0.038
signif(sort(nyt.latent.sem[, 1], decreasing = false)[1:30], 2)
mother
##
-0.110
##
##
he
##
-0.065
image sculpture
##
-0.056
##
studio
##
##
-0.050
##
##

i
-0.150
process paintings
-0.070
picasso
-0.057
you
-0.051

her
-0.240
painting
-0.088
gagosian
-0.062
work
-0.054
like
-0.049

she
-0.260
my
-0.094
me
-0.063
artists
-0.055
says
-0.050

-0.071
was
-0.058
photos
-0.051

said
-0.130
im
-0.068

-0.056
nature
-0.050

0.042
jazz
0.039

ms
-0.200

opera
0.058
y
0.048
concert
0.045
p
0.041
society
0.038

cooper
-0.100
mrs
-0.065
baby
-0.055
out
-0.050

these are the thirty words with the largest positive and negative projections on
to the    rst component.17 the words with positive projections are mostly associ-
ated with music, those with negative components with the visual arts. the letters
   m    and    p    show up with music because of the combination    p.m   , which our
parsing breaks into two single-letter words, and because stories about music give

15 actually, following standard practice in language processing, i   ve normalized the bag-of-word

vectors so that documents of di   erent lengths are comparable, and used    inverse
document-frequency weighting    to de-emphasize hyper-common words like    the    and emphasize
more informative words. see the lecture notes for data mining if you   re interested.

16 if we were trying to work with the complete corpus, we should expect at least 50000 words, and

perhaps more.

17 which direction is positive and which negative is of course arbitrary; basically it depends on

internal choices in the algorithm.

360

principal components analysis

show-times more often than do stories about art. personal pronouns appear with
art stories because more of those quote people, such as artists or collectors.18

what about the second component?

museums
0.073
sculptures
0.051
service
0.046
computer
0.041
dealers
0.040

museum
0.120
tax
0.070
white
0.050
feet
0.043

artists
0.092
sculpture
0.060
artist
0.047
statue
0.042
diamond
0.041

art
0.150
painting
0.073
painted
0.050
decorative
0.043
paris
0.041

images
0.095
paintings
0.065
patterns
0.047
digital
0.043
war collections
0.041

signif(sort(nyt.latent.sem[, 2], decreasing = true)[1:30], 2)
donations
##
0.075
##
gallery
##
0.055
##
nature
##
0.046
##
color
##
0.042
##
stone
##
##
0.041
signif(sort(nyt.latent.sem[, 2], decreasing = false)[1:30], 2)
##
##
##
##
##
##
##
##
##
##
##
##

opera
ms
-0.130
-0.130
sang
festival
-0.075
-0.074
vocal
orchestra
-0.067
-0.067
performance
band
-0.061
-0.060
my
im
-0.056
-0.056
cooper performances
-0.051
-0.051

theater
-0.160
production
-0.075
songs
-0.068
matinee
-0.061
says
-0.058
singer
-0.052

she
-0.220
hour
-0.081
musical
-0.070
singing
-0.065
composers
-0.058
broadway
-0.055

her
-0.220
i
-0.083
music
-0.070
la
-0.065
awards
-0.058
play
-0.056

0.041

here the positive words are about art, but more focused on acquiring and
trading (   collections   ,    dealers   ,    donations   ,    dealers   ) than on talking with
artists or about them. the negative words are musical, speci   cally about musical
theater and vocal performances.

i could go on, but by this point you get the idea.

16.5 pca for visualization

let   s try displaying the times stories using the principal components (figure
16.6).

notice that even though we have gone from 4431 dimensions to 2, and so
thrown away a lot of information, we could draw a line across this plot and have
most of the art stories on one side of it and all the music stories on the other.
if we let ourselves use the    rst four or    ve principal components, we   d still have
a thousand-fold savings in dimensions, but we   d be able to get almost-perfect
separation between the two classes. this is a sign that pca is really doing a
good job at summarizing the information in the word-count vectors, and in turn
that the bags of words give us a lot of information about the meaning of the
stories.

the    gure also illustrates the idea of multidimensional scaling, which means

18 you should check out these explanations for yourself. the raw stories are part of the r workspace.

16.5 pca for visualization

361

plot(nyt.pca$x[, 1:2], pch = ifelse(nyt.frame[, "class.labels"] == "music",
"m", "a"), col = ifelse(nyt.frame[, "class.labels"] == "music", "blue",
"red"))

figure 16.6 projection of the times stories on to the    rst two principal
components. music stories are marked with a blue    m   , art stories with a
red    a   .

   nding low-dimensional points to represent high-dimensional data by preserving
the distances between the points. if we write the original vectors as (cid:126)x1, (cid:126)x2, . . . (cid:126)xn,
and their images as (cid:126)y1, (cid:126)y2, . . . (cid:126)yn, then the mds problem is to pick the images to
minimize the di   erence in distances:

((cid:107)(cid:126)yi     (cid:126)yj(cid:107)     (cid:107)(cid:126)xi     (cid:126)xj(cid:107))2

(16.22)

(cid:88)

(cid:88)

j(cid:54)=i

i

aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaammmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm   0.4   0.3   0.2   0.10.00.10.2   0.3   0.2   0.10.00.10.2pc1pc2362

principal components analysis

this will be small if distances between the image points are all close to the
distances between the original points. pca accomplishes this precisely because
(cid:126)yi is itself close to (cid:126)xi (on average).

16.6 pca cautions

363

16.6 pca cautions

trying to guess at what the components might mean is a good idea, but like
many good ideas it   s easy to go overboard. speci   cally, once you attach an idea
in your mind to a component, and especially once you attach a name to it, it   s
very easy to forget that those are names and ideas you made up; to reify them,
as you might reify clusters. sometimes the components actually do measure real
variables, but sometimes they just re   ect patterns of covariance which have many
di   erent causes. if i did a pca of the same variables but for, say, european cars,
i might well get a similar    rst component, but the second component would
probably be rather di   erent, since suvs are much less common there than here.
a more important example comes from population genetics. starting in the late
1960s, l. l. cavalli-sforza and collaborators began a huge project of mapping
human genetic variation     of determining the frequencies of di   erent genes in
di   erent populations throughout the world. (cavalli-sforza et al. (1994) is the
main summary; cavalli-sforza has also written several excellent popularizations.)
for each point in space, there are a very large number of variables, which are the
frequencies of the various genes among the people living there. plotted over space,
this gives a map of that gene   s frequency. what they noticed (unsurprisingly) is
that many genes had similar, but not identical, maps. this led them to use pca,
reducing the huge number of variables (genes) to a few components. results look
like figure 16.7. they interpreted these components, very reasonably, as signs of
large population movements. the    rst principal component for europe and the
near east, for example, was supposed to show the expansion of agriculture out
of the fertile crescent. the third, centered in steppes just north of the caucasus,
was supposed to re   ect the expansion of indo-european speakers towards the end
of the bronze age. similar stories were told of other components elsewhere.

unfortunately, as novembre and stephens (2008) showed, spatial patterns like
this are what one should expect to get when doing pca of any kind of spatial
data with local correlations, because that essentially amounts to taking a fourier
transform, and picking out the low-frequency components.19 they simulated ge-
netic di   usion processes, without any migration or population expansion, and
got results that looked very like the real maps (figure 16.8). this doesn   t mean
that the stories of the maps must be wrong, but it does undercut the principal
components as evidence for those stories.

16.7 random projections

pca    nds the optimal projection from p dimensions down to q dimensions, in
the sense of minimizing the mse in eq. 16.6. we have seen how this lead to
maximizing variance, and to an elegant solution in terms of eigenvectors of the

19 remember that pca re-writes the original vectors as a weighted sum of new, orthogonal vectors,

just as fourier transforms do. when there is a lot of spatial correlation, values at nearby points are
similar, so the low-frequency modes will have a lot of amplitude, i.e., carry a lot of the variance. so
   rst principal components will tend to be similar to the low-frequency fourier modes.

364

principal components analysis

covariance matrix. however,    nding eigenvectors is a lot of work. lazy people
may therefore wonder whether it would really be so bad to just use a random
q-dimensional sub-space.

a remarkable answer is given by a geometric result which has come to be called
the johnson-lindenstrauss lemma, which runs as follows. we start with n
points (cid:126)x1, (cid:126)x2, . . . (cid:126)xn in rp, and we want to project them down to rq, while ensuring
that all (squared) distances are preserved to within a factor of 1     , i.e., that,
for all i and j,

(1      )(cid:107)(cid:126)xi     (cid:126)xj(cid:107)     (cid:107)w(cid:126)xi     w(cid:126)xj(cid:107)2     (1 +  )(cid:107)(cid:126)xi     (cid:126)xj(cid:107)

(16.23)

(compare this expression to the objective function for multidimensional scaling,
eq. 16.22.) there is always some w which achieves this, provided that q is at
least o(    2 log n). in fact, proofs of the result are constructive (dasgupta and
gupta, 2002): we can form w as follows:

ln n

 /2    3/3 uniformly-distributed, unit-length vectors20 in rp;

1. draw q = 4

2. orthogonalize the vectors;

3. scale up each vector by a factor of(cid:112)p/q;

4. make the vectors the rows of the q    p matrix w.

the id203 that this w keeps all of the distances between the n points to
within a factor of 1      is at least21 1     1/n. the fact that this id203 is
> 0 shows that there is some distance-preserving projection. since it is easy to
check whether a randomly-generated w does preserve the distances, we can make
the id203 of success as close to 1 as we like by generating multiple w and
checking them all.

the johnson-lindenstrauss procedure has a number of very remarkable prop-
erties. one of them is that the projection is, indeed, completely random, and not
at all a function of the data, a drastic contrast with pca. it is plainly foolish to
give any sort of interpretation to the johnson-lindenstrauss projection. (in fact,
the same random projection will work with most data sets!) another remarkable
property is that the required number of dimensions q needed to approximate the
data does not depend on the original dimension p. rather, q grows, slowly, with
the number of data points n. if, on the other hand, there really is a linear low-
dimensional structure to the data, pca should be able to extract it with a    xed
number of principal components.22

20 to create such a vector (cid:126)u , make p draws yi from an n (0, 1) distribution, and set

u = (y1, y2, . . . yp)/(cid:112)(cid:80)p

i=1 y 2
i .

21 again, see dasgupta and gupta (2002) for the detailed calculations; they are not too di   cult, but

not illuminating here.

22 it would seem like it should be possible to turn this last point into an actual test for whether the

data cluster around a linear sub-space, but, if so, i have not found where it is worked out.

16.8 further reading

365

16.8 further reading

principal components goes back to karl pearson (1901). it was independently
re-invented by harold hotelling (1933a,b), who provided the name    principal
components analysis   . it has been re-re-discovered many times in many    elds,
so it is also known as (among other things) the karhunen-lo`eve transformation
(see lo`eve (1955)), the hotelling transformation, the method of empirical orthog-
onal functions, and singular value decomposition23. many statistical presentations
start with the idea of maximizing the variance; this seems less well-motivated to
me than trying to    nd the best-approximating linear subspace, which was, in
fact, pearson   s original goal in 1901.

the enthusiastic and well-written textbook by eshel (2012) is largely devoted
to pca. it starts with the rudiments of id202, looks into numerical and
computational issues i have glossed over, and gives detailed accounts of its uses
in analyzing spatial and spatio-temporal processes, especially in the earth and
environmental sciences.

as i said above, pca is an example of a data analysis method which in-
volves only approximation, with no statistical id136 or underlying probabilis-
tic model. chapters 17 and 19 describe two (rather-di   erent looking) statistical
models which both imply that the data should lie in a linear subspace plus noise.
alternatively, chapter 18 introduces methods for approximating data with low-
dimensional curved manifolds, rather than linear subspaces.

latent semantic analysis, or id45, goes back to deerwester
et al. (1990). hand et al. (2001) has a good discussion, setting it in the context of
other data-analytic methods, and avoiding some of the more extravagant claims
made on its behalf (landauer and dumais, 1997).
  16.5 just scratches the surface of the vast literature on multidimensional scal-
ing, the general goal of which is to    nd low-dimensional, easily-visualized rep-
resentations which are somehow faithful to the geometry of high-dimensional
spaces. much of this literature, including the name    multidimensional scaling   ,
comes from psychology. for a brief introduction with references, i recommend
hand et al. (2001).

concerns about interpretation and rei   cation24 are rarely very far away when-
ever people start using methods for    nding hidden structure, whether they   re
just approximation methods or they attempt proper statistical id136. we will
touch on them again in chapters 17 and 19. in general, people seem to    nd it
easier to say what   s wrong or dubious about other analysts    interpretations of
their components or latent constructs, than to explain what   s right about their
own interpretations; certainly i do.

for more on random projections, see mahoney (2011), which sets them in

23 strictly speaking, singular value decomposition is a matrix algebra trick which is used in the most

common algorithm for pca.

24 i have not been able to    nd out where the term    rei   cation    comes from; some claim that it is

marxist in origin, but it   s used by the early and decidedly non-marxist thomson (1939), so i doubt
that.

366

principal components analysis

the context of related randomized methods for dealing with large and/or high-
dimensional data.

exercises

16.1 suppose that instead of projecting on to a line, we project on to a q-dimensional subspace,
de   ned by q orthogonal length-one vectors (cid:126)w1, . . . (cid:126)wq. we want to show that minimizing
the mean squared error of the projection is equivalent to maximizing the sum of the
variances of the scores along these q directions.
1. write w for the matrix forms by stacking the (cid:126)wi. prove that wt w = iq.
2. find the matrix of q-dimensional scores in terms of x and w. hint: your answer should

reduce to (cid:126)xi    (cid:126)w1 when q = 1.
and w. hint: your answer should reduce to ( (cid:126)xi    (cid:126)w1) (cid:126)w1 when q = 1.

3. find the matrix of p-dimensional approximations based on these scores in terms of x

4. show that the mse of using the vectors (cid:126)w1, . . . (cid:126)wq is the sum of two terms, one of
which depends only on x and not w, and the other depends only on the scores along
those directions (and not otherwise on what those directions are). hint: look at the
derivation of eq. 16.5, and use exercise 41.

5. explain in what sense minimizing projection residuals is equivalent to maximizing the

sum of variances along the di   erent directions.

16.2 suppose that u has two eigenvectors, (cid:126)w1 and (cid:126)w2, with the same eigenvalue a. prove that
any linear combination of (cid:126)w1 and (cid:126)w2 is also an eigenvector of u, and also has eigenvalue
a.

exercises

367

figure 16.7 principal components of genetic variation in the old world,
according to cavalli-sforza et al. (1994), as re-drawn by novembre and
stephens (2008).

368

principal components analysis

figure 16.8 how the pca patterns can arise as numerical artifacts (far
left column) or through simple genetic di   usion (next column). from
novembre and stephens (2008).

17

factor models

17.1 from pca to factor analysis

let   s sum up pca. we start with n di   erent p-dimensional vectors as our data,
i.e., each observation as p numerical variables. we want to reduce the number
of dimensions to something more manageable, say q. the principal components
of the data are the q orthogonal directions of greatest variance in the original
p-dimensional space; they can be found by taking the top q eigenvectors of the
sample covariance matrix. principal components analysis summarizes the data
vectors by projecting them on to the principal components.

all of this is purely an algebraic undertaking; it involves no probabilistic as-
sumptions whatsoever. it also supports no statistical id136s     saying nothing
about the population or stochastic process which made the data, it just summa-
rizes the data. how can we add some id203, and so some statistics? and
what does that let us do?

start with some notation. x is our data matrix, with n rows for the di   erent
observations and p columns for the di   erent variables, so xij is the value of
variable j in observation i. each principal component is a vector of length p, and
there are p of them, so we can stack them together into a p    p matrix, say w.
finally, each data vector has a projection on to each principal component, which
we collect into an n    p matrix f. then
x = fw

(17.1)

[n    p] = [n    p][p    p]

where i   ve checked the dimensions of the matrices underneath. this is an exact
equation involving no noise, approximation or error, but it   s kind of useless; we   ve
replaced p-dimensional vectors in x with p-dimensional vectors in f. if we keep
only to q < p largest principal components, that corresponds to dropping columns
from f and rows from w. let   s say that the truncated matrices are fq and wq.
then

x     fqwq

[n    p] = [n    q][q    p]

(17.2)

the error of approximation     the di   erence between the left- and right- hand-
sides of eq. 17.2     will get smaller as we increase q. (the line below the equation
is a sanity-check that the matrices are the right size, which they are. also, at this

369

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

370

factor models

point the subscript qs get too annoying, so i   ll drop them.) we can of course make
the two sides match exactly by adding an error or residual term on the right:

x = fw +  

(17.3)

where   has to be an n    p matrix.

now, eq. 17.3 should look more or less familiar to you from regression. on the
left-hand side we have a measured outcome variable (x), and on the right-hand
side we have a systematic prediction term (fw) plus a residual ( ). let   s run
with this analogy, and start treating   as noise, as a random variable which has
got some distribution, rather than whatever arithmetic says is needed to balance
the two sides. this move is where we go from mere data reduction, making no
claims about anything other than these particular data points, to actual statistical
id136, making assertions about the process that generated the data. it is the
di   erence between the di   erence between just drawing a straight line through a
scatter plot, and inferring a id75.

having made that move, x will also be a random variable. when we want to
talk about the random variable which goes in the ith column of x, we   ll call it xi.
what about f? well, in the analogy it corresponds to the independent variables
in the regression, which ordinarily we treat as    xed rather than random, but
that   s because we actually get to observe them; here we don   t, so it can make
sense to treat f, too, as random. now that they are random variables, we say
that we have q factors, rather than components, that f is the matrix of factor
scores and w is the matrix of factor loadings. the variables in x are called
observable or manifest variables, those in f are hidden or latent. (technically
  is also latent.)

before we can actually do much with this model, we need to say more about
the distributions of these random variables. the traditional choices are as follows.

1. all of the observable random variables xi have mean zero and variance 1.
2. all of the latent factors have mean zero and variance 1.
3. the noise terms   all have mean zero.
4. the factors are uncorrelated across individuals (rows of f) and across variables

(columns of f).

5. the noise terms are uncorrelated across individuals, and across observable

variables.

6. the noise terms are uncorrelated with the factor variables.

item (1) isn   t restrictive, because we can always center and standardize our data.
item (2) isn   t restrictive either     we could always center and standardize the
factor variables without really changing anything. item (3) actually follows from
(1) and (2). the substantive assumptions     the ones which will give us predictive
power but could also go wrong, and so really de   ne the factor model     are the
others, about lack of correlation. where do they come from?

remember what the model looks like:

x = fw +  

(17.4)

17.2 the graphical model

371

all of the systematic patterns in the observations x should come from the    rst
term on the right-hand side. the residual term   should, if the model is working,
be unpredictable noise. items (3) through (5) express a very strong form of this
idea. in particular it   s vital that the noise be uncorrelated with the factor scores.

17.1.1 preserving correlations

there is another route from pca to the factor model, which many people like
but which i    nd less compelling; it starts by changing the objectives.

pca aims to minimize the mean-squared distance from the data to their
projects, or what comes to the same thing, to preserve variance. but it doesn   t
preserve correlations. that is, the correlations of the features of the image vectors
are not the same as the correlations among the features of the original vectors
(unless q = p, and we   re not really doing any data reduction). we might value
those correlations, however, and want to preserve them, rather than trying to
approximate the actual data.1 that is, we might ask for a set of vectors whose
image in the feature space will have the same correlation matrix as the original
vectors, or as close to the same correlation matrix as possible while still reducing
the number of dimensions. this leads to the factor model we   ve already reached,
as we   ll see in   17.4.2.

17.2 the graphical model

it   s common to represent factor models visually, as in figure 17.1. this is an
example of a graphical model, in which the nodes or vertices of the graph rep-
resent random variables, and the edges of the graph represent direct statistical
dependencies between the variables. the    gure shows the observables or features
in square boxes, to indicate that they are manifest variables we can actual mea-
sure; above them are the factors, drawn in round bubbles to show that we don   t
get to see them. the fact that there are no direct linkages between the factors
shows that they are independent of one another. from below we have the noise
terms, one to an observable.

notice that not every observable is connected to every factor: this depicts
the fact that some entries in w are zero. in the    gure, for instance, x1 has an
arrow only from f1 and not the other factors; this means that while w11 = 0.87,
w21 = w31 = 0.

drawn this way, one sees how the factor model is generative     how it gives
us a recipe for producing new data. in this case, it   s: draw new, independent
values for the factor scores f1, f2, . . . fq; add these up with weights from w; and
then add on the    nal noises  1,  2, . . .  p. if the model is right, this is a procedure

1 why? well, originally the answer was that the correlation coe   cient had just been invented, and
was about the only way people had of measuring relationships between variables. since then it   s
been propagated by statistics courses where it is the only way people are taught to measure
relationships. the great statistician john tukey once wrote    does anyone know when the correlation
coe   cient is useful, as opposed to when it is used? if so, why not tell us?    (tukey, 1954, p. 721).

372

factor models

figure 17.1 graphical model form of a factor model. circles stand for the
unobserved variables (factors above, noises below), boxes for the observed
features. edges indicate non-zero coe   cients     entries in the factor loading
matrix w, or speci   c variances   i. arrows representing entries in w are
decorated with those entries. note that it is common to omit the noise
variables in such diagrams, with the implicit understanding that every
variable with an incoming arrow also has an incoming noise term.

for generating new, synthetic data with the same characteristics as the real data.
in fact, it   s a story about how the real data came to be     that there really are
some latent variables (the factor scores) which linearly cause the observables to
have the values they do.

17.2.1 observables are correlated through the factors

one of the most important consequences of the factor model is that observable
variables are correlated with each other solely because they are correlated with
the hidden factors. to see how this works, take x1 and x2 from the diagram,
and let   s calculate their covariance. (since they both have variance 1, this is the
same as their correlation.)

f1x10.87x2-0.75f20.34x30.13x40.20f30.730.10x50.15x60.45e1e2e3e4e5e617.2 the graphical model

373

(17.5)
(17.6)
(17.7)

(cid:3)

(17.8)

(17.9)
(17.10)

cov [x1, x2] = e [x1x2]     e [x1] e [x2]

= e [x1x2]
= e [(f1w11 + f2w21 +  1)(f1w12 + f2w22 +  2)]
1 w11w12 + f1f2(w11w22 + w21w12) + f 2

= e(cid:2)f 2

2 w21w22

+e [ 1 2] + e [ 1(f1w12 + f2w22)]
+e [ 2(f1w11 + f2w21)]

q(cid:88)

k=1

since the noise terms are uncorrelated with the factor scores, and the noise terms
for di   erent variables are uncorrelated with each other, all the terms containing
 s have expectation zero. also, f1 and f2 are uncorrelated, so

cov [x1, x2] = e(cid:2)f 2

(cid:3) w11w12 + e(cid:2)f 2

(cid:3) w21w22

1

2

= w11w12 + w21w22

using the fact that the factors are scaled to have variance 1. this says that the
covariance between x1 and x2 is what they have from both correlating with f1,
plus what they have from both correlating with f2; if we had more factors we
would add on w31w32 +w41w42 +. . . out to wq1wq2. and of course this would apply
as well to any other pair of observable variables. so the general form is

cov [xi, xj] =

wkiwkj

(17.11)

so long as i (cid:54)= j.

the jargon says that observable i loads on factor k when wki (cid:54)= 0. if two
observables do not load on to any of the same factors, if they do not share any
common factors, then they will be independent. if we could condition on (   control
for   ) the factors, all of the observables would be conditionally independent.

graphically, we draw an arrow from a factor node to an observable node if
and only if the observable loads on the factor. so then we can just see that two
observables are correlated if they both have in-coming arrows from the same
factors. (to    nd the actual correlation, we multiply the weights on all the edges
connecting the two observable nodes to the common factors; that   s eq. 17.11.)
conversely, even though the factors are marginally independent of each other, if
two factors both send arrows to the same observable, then they are dependent
conditional on that observable.2

17.2.2 geometry: approximation by linear subspaces

each observation we take is a vector in a p-dimensional space; the factor model
says that these vectors have certain geometric relations to each other     that
the data has a certain shape. to see what that is, pretend for right now that

2 to see that this makes sense, suppose that x1 = f1w11 + f2w21 +  1. if we know the value of x1,

we know what f1, f2 and  1 have to add up to, so they are conditionally dependent.

factor models

374
we can turn o    the noise terms  . the loading matrix w is a q    p matrix, so
each row of w is a vector in p-dimensional space; call these vectors (cid:126)w1, (cid:126)w2, . . . (cid:126)wq.
without the noise, our observable vectors would be linear combinations of these
vectors (with the factor scores saying how much each vector contributes to the
combination). since the factors are orthogonal to each other, we know that they
span a q-dimensional sub-space of the p-dimensional space     a line if q = 1, a
plane if q = 2, in general a linear subspace. if the factor model is true and we turn
o    noise, we would    nd all the data lying exactly on this subspace. of course,
with noise we expect that the data vectors will be scattered around the subspace;
how close depends on the variance of the noise. (figure 17.2.) but this is still a
rather speci   c prediction about the shape of the data.

a weaker prediction than    the data lie on a low-dimensional linear subspace
in the high-dimensional space    is    the data lie on some low-dimensional surface,
possibly curved, in the high-dimensional space   ; there are techniques for trying
to recover such surfaces. chapter 18 introduces two such techniques, but this is
a broad and still-growing area.

17.3 roots of factor analysis in causal discovery

the roots of factor analysis go back to work by charles spearman just over a
century ago (spearman, 1904); he was trying to discover the hidden structure of
human intelligence. his observation was that schoolchildren   s grades in di   erent
subjects were all correlated with each other. he went beyond this to observe a
particular pattern of correlations, which he thought he could explain as follows:
the reason grades in math, english, history, etc., are all correlated is performance
in these subjects is all correlated with something else, a general or common
factor, which he named    general intelligence   , for which the natural symbol was
of course g or g.

put in a form like eq. 17.4, spearman   s model becomes

x =   + gw

(17.12)

where g is an n    1 matrix (i.e., a row vector) and w is a 1    p matrix (i.e., a
column vector). the correlation between feature i and g is just wi     w1i, and, if
i (cid:54)= j,

vij     cov [xi, xj] = wiwj

(17.13)

where i have introduced vij as a short-hand for the covariance.

up to this point, this is all so much positing and assertion and hypothesis.
what spearman did next, though, was to observe that this hypothesis carried a
very strong implication about the ratios of correlation coe   cients. pick any four

17.3 roots of factor analysis in causal discovery

375

## warning: package    scatterplot3d    was built under r version 3.4.4

n <- 20; library(scatterplot3d)
f <- matrix(sort(rnorm(n)),ncol=1); w <- matrix(c(0.5,0.2,-0.1),nrow=1)
fw <- f %*% w; x <- fw + matrix(rnorm(n*3,sd=c(.15,.05,.09)),ncol=3,byrow=true)
s3d <- scatterplot3d(x,xlab=expression(x^1),ylab=expression(x^2),

s3d$points3d(matrix(seq(from=min(f)-1,to=max(f)+1,length.out=2),ncol=1)%*%w,

zlab=expression(x^3),pch=16)

col="red",type="l")

s3d$points3d(fw,col="red",pch=16)
for (i in 1:nrow(x)) {

s3d$points3d(x=c(x[i,1],fw[i,1]),y=c(x[i,2],fw[i,2]),z=c(x[i,3],fw[i,3]),

col="grey",type="l") }

figure 17.2 geometry of factor models: black dots are observed vectors (cid:126)x
in p = 3 dimensions. these were generated from the q = 1 dimensional factor
scores (cid:126)f by taking (cid:126)f w (red dots) and adding independent noise (grey lines).
the q-dimensional subspace along which all values of (cid:126)f w must fall is also
shown in red. (see also exercise 17.3.)

   1.0   0.5 0.0 0.5 1.0 1.5   0.4   0.3   0.2   0.1 0.0 0.1 0.2 0.3 0.4   0.4   0.2 0.0 0.2 0.4 0.6x1x2x3llllllllllllllllllllllllllllllllllllllll376

factor models

distinct features, i, j, k, l. then, if the model (17.12) is true,

vij/vkj
vil/vkl

=

=

wiwj/wkwj
wiwl/wkwl
wi/wk
wi/wk

= 1

vijvkl = vilvkj

(17.14)

(17.15)

(17.16)

(17.17)

the relationship

is called the    tetrad equation   , and we will meet it again later when we consider
methods for causal discovery in part iii. in spearman   s model, this is one tetrad
equation for every set of four distinct variables.

spearman found that the tetrad equations held in his data on school grades (to
a good approximation), and concluded that a single general factor of intelligence
must exist3. this was, of course, logically fallacious.

later work, using large batteries of di   erent kinds of intelligence tests, showed
that the tetrad equations do not hold in general, or more exactly that depar-
tures from them are too big to explain away as sampling noise. (recall that the
equations are about the true correlations between the variables, but we only get
to see sample correlations, which are always a little o   .) the response, done in
an ad hoc way by spearman and his followers, and then more systematically by
thurstone, was to introduce multiple factors. this breaks the tetrad equation,
but still accounts for the correlations among features by saying that features are
really directly correlated with factors, and uncorrelated conditional on the factor
scores. thurstone   s form of factor analysis is basically the one people still use    
there have been re   nements, of course, but it   s mostly still his method.

17.4 estimation

the factor model introduces a whole bunch of new variables to explain the observ-
ables: the factor scores f, the factor loadings or weights w, and the observable-
speci   c variances   i. the factor scores are speci   c to each individual, and indi-
viduals by assumption are independent, so we can   t expect them to really gener-
alize. but the loadings w and the variances    are, supposedly, characteristic of
the population. so it would be nice if we could separate estimating the population
parameters from estimating the attributes of individuals; here   s how.

since the variables are centered, we can write the covariance matrix in terms

of the data frames:

v = e

(cid:21)

xt x

(cid:20) 1

n

(17.18)

3 actually, the equations didn   t hold when music was one of the grades, so spearman argued musical

ability did not load on general intelligence.

17.4 estimation

377

(this is the true, population covariance matrix on the left.) but the factor model
tells us that

x = fw +  

(17.19)

this involves the factor scores f, but remember that when we looked at the
correlations between individual variables, those went away, so let   s substitute eq.
17.19 into eq. 17.18 and see what happens:

(cid:21)

(cid:20) 1

e

n
1
n
1
n

=

=

xt x

e(cid:2)( t + wt ft )(fw +  )(cid:3)
(cid:0)e(cid:2) t  (cid:3) + wt e(cid:2)ft  (cid:3) + e(cid:2) t f(cid:3) w + wt e(cid:2)ft f(cid:3) w(cid:1)

=    + 0 + 0 +

=    + wt w

1
n

wt niw

behold:

v =    + wt w

(17.20)

(17.21)

(17.22)

(17.23)

(17.24)

(17.25)

the individual-speci   c variables f have gone away, leaving only population pa-
rameters on both sides of the equation. the left-hand side is clearly something
we can learn reliably from data, so if we can solve this equation for    and w, we
can estimate the factor parameter   s models. can we solve eq. 17.25?

17.4.1 degrees of freedom

it only takes a bit of playing with eq. 17.25 to realize that we are in trouble. like
any matrix equation, it represents a system of equations. how many equations in
how many unknowns? naively, we   d say that we have p2 equations (one for each
element of the matrix v), and p + pq unknowns (one for each diagonal element of
  , plus one for each element of w). if there are more equations than unknowns,
then there is generally no solution; if there are fewer equations than unknowns,
then there are generally in   nitely many solutions. either way, solving for w seems
hopeless (unless q = p     1, in which case it   s not very helpful). what to do?

well,    rst let   s do the book-keeping for degrees of freedom more carefully. the
observables variables are scaled to have standard deviation one, so the diagonal
entries of v are all 1. moreover, any covariance matrix is symmetric, so we are
left with only p(p    1)/2 degrees of freedom in v; there are really only that many
equations. on the other side, scaling to standard deviation 1 means we don   t
really need to solve separately for   , because it   s    xed as soon as we know what
wt w is, which saves us p unknowns. also, the entries in w are not completely
free to vary independently of each other, because each row has to be orthogonal
to every other row. (look back at chapter 16.) since there are q rows, this gives

factor models

378
us q(q     1)/2 constraints on w     we can think of these as either extra equations,
or as reductions in the number of free parameters (unknowns).4
summarizing, we really have p(p    1)/2 degrees of freedom in v, and pq    q(q   
1)/2 degrees of freedom in w. if these two match, then there is (in general) a
unique solution which will give us w. but in general they will not be equal; then
what? let us consider the two cases.

more unknowns (free parameters) than equations (constraints)

this is fairly straightforward: there is no unique solution to eq. 17.25; instead
there are in   nitely many solutions. it   s true that the loading matrix w does have
to satisfy some constraints, that not just any w will work, so the data does give
us some information, but there is a continuum of di   erent parameter settings
which are all match the covariance matrix perfectly. (notice that we are working
with the population parameters here, so this isn   t an issue of having only a
limited sample.) there is just no way to use data to decide between these di   erent
parameters, to identify which one is right, so we say the model is unidenti   able.
most software for factor analysis, include r   s factanal function, will check for
this and just refuse to    t a model with too many factors relative to the number
of observables.

more equations (constraints) than unknowns (free parameters)

this is more interesting. in general, systems of equations like this are overdeter-
mined, meaning that there is no way to satisfy all the constraints at once, and
there isn   t even a single solution. it   s just not possible to write an arbitrary co-
variance matrix v among, say, seven variables in terms of, say, a one-factor model
(as p(p     1)/2 = 7(7     1)/2 = 21 > 7(1)     1(1     1)/2 = 7 = pq     q(q     1)/2).
but it is possible for special covariance matrices. in these situations, the factor
model actually has testable implications for the data     it says that only certain
covariance matrices are possible and not others. for example, we saw above that
the one-fator model implies the tetrad equations must hold among the observable
covariances; the constraints on v for multiple-factor models are similar in kind
but more complicated algebraically. by testing these implications, we can check
whether or not our favorite factor model is right.5

now we don   t know the true, population covariance matrix v, but we can

estimate it from data, getting an estimate (cid:98)v. the natural thing to do then is to

equate this with the parameters and try to solve for the latter:

(cid:98)v = (cid:98)   + (cid:98)wt(cid:98)w

(17.26)

the book-keeping for degrees of freedom here is the same as for eq. 17.25. if q is

4 notice that    + wt w is automatically symmetric, since    is diagonal, so we don   t need to impose

any extra constraints to get symmetry.

5 actually, we need to be a little careful here. if we    nd that the tetrad equations don   t hold, we

know a one-factor model must be wrong. we could only conclude that the one-factor model must be
right if we found that the tetrad equations held, and that there were no other models which implied
those equations; but, as we   ll see, there are.

17.4 estimation

379

too large relative to p, the model is unidenti   able; if it is too small, the matrix

equation can only be solved if (cid:98)v is of the right, restricted form, i.e., if the model

is right. of course even if the model is right, the sample covariances are the true
covariances plus noise, so we shouldn   t expect to get an exact match, but we
can try in various way to minimize the discrepancy between the two sides of the
equation.

17.4.2 a clue from spearman   s one-factor model

remember that in spearman   s model with a single general factor, the covariance
between observables i and j in that model is the product of their factor weightings:

vij = wiwj

(17.27)

the exception is that vii = w2
i . however, if we look at
u = v       , that   s the same as v o    the diagonal, and a little algebra shows that
its diagonal entries are, in fact, just w2
i . so if we look at any two rows of u,
they   re proportional to each other:

i +   i, rather than w2

uij =

wi
wk

ukj

(17.28)

this means that, when spearman   s model holds true, there is actually only one
linearly-independent row in in u.

recall from id202 that the rank of a matrix is how many linearly
independent rows it has.6 ordinarily, the matrix is of full rank, meaning all the
rows are linearly independent. what we have just seen is that when spearman   s
model holds, the matrix u is not of full rank, but rather of rank 1. more generally,
when the factor model holds with q factors, the matrix u = wt w has rank q.
the diagonal entries of u, called the common variances or commonalities,
are no longer automatically 1, but rather show how much of the variance in each
observable is associated with the variances of the latent factors. like v, u is a
positive symmetric matrix.

because u is a positive symmetric matrix, we know from id202 that it

can be written as

u = cdct

(17.29)

where c is the matrix whose columns are the eigenvectors of u, and d is the
diagonal matrix whose entries are the eigenvalues. that is, if we use all p eigen-
vectors, we can reproduce the covariance matrix exactly. suppose we instead use
cq, the p    q matrix whose columns are the eigenvectors going with the q largest
eigenvalues, and likewise make dq the diagonal matrix of those eigenvalues. then
t will be a symmetric positive p  p matrix. this is a matrix of rank q, and
cqdqcq
so can only equal u if the latter also has rank q. otherwise, it   s an approximation
which grows more accurate as we let q grow towards p, and, at any given q, it   s a

6 we could also talk about the columns; it wouldn   t make any di   erence.

380

factor models

better approximation to u than any other rank-q matrix. this,    nally, is the pre-
cise sense in which factor analysis tries to preserve correlations: u just contains
information about the correlations, and we   re going to try to approximate u as
well as possible.
1/2 as the q    q diagonal matrix of the square

to resume our algebra, de   ne dq

roots of the eigenvalues. clearly dq = dq

1/2dq

1/2. so

cqdqcq

t = cqdq

1/2dq

1/2cq

t =

cqdq

cqdq

so we have

u    (cid:16)

cqdq

1/2(cid:17)t

(cid:16)
1/2(cid:17)(cid:16)

1/2(cid:17)(cid:16)
1/2(cid:17)t

cqdq

(17.30)

(17.31)

(cid:16)

cqdq

1/2(cid:17)t

but at the same time we know that u = wt w. so we just identify w with

:

(cid:16)

cqdq

1/2(cid:17)t

w =

(17.32)

and we are done with our algebra.

let   s think a bit more about how well we   re approximating v. the approxima-
tion will always be exact when q = p, so that there is one factor for each feature
(in which case    = 0 always). then all factor analysis does for us is to rotate the
coordinate axes in feature space, so that the new coordinates are uncorrelated.
(this is the same as what pca does with p components.) the approximation can
also be exact with fewer factors than features if the reduced covariance matrix is
of less than full rank, and we use at least as many factors as the rank.

17.4.3 estimating factor loadings and speci   c variances

the classical method for estimating the factor model is now simply to do this
eigenvector approximation on the sample correlation matrix. de   ne the reduced
or adjusted sample correlation matrix as

and common starting-point is to do a id75 of each feature j on all

we can   t actually calculate(cid:98)u until we know, or have a guess as to, (cid:98)  . a reasonable
the other features, and then set(cid:99)  j to the mean squared error for that regression.
eigenvectors, getting matrices(cid:99)cq and (cid:99)dq as above. set the factor loadings accord-

once we have the reduced correlation matrix,    nd its top q eigenvalues and

(we   ll come back to this guess later.)

(17.33)

(cid:98)u =(cid:98)v     (cid:98)  

17.5 id113

381

ingly, and re-calculate the speci   c variances:

(cid:16)

cqdq

1/2(cid:17)t
(cid:98)w =
(cid:99)  j = 1     q(cid:88)
  v     (cid:98)   + (cid:98)wt(cid:98)w

w2
rj

r=1

(17.34)

(17.35)

(17.36)

the    predicted    covariance matrix   v in the last line is exactly right on the diag-
onal (by construction), and should be closer o   -diagonal than anything else we
could do with the same number of factors. however, our guess as to u depended
on our initial guess about   , which has in general changed, so we can try iterating
this (i.e., re-calculating cq and dq), until we converge.

17.5 id113

it has probably not escaped your notice that the estimation procedure above
requires a starting guess as to   . this makes its consistency somewhat shaky.
(if we continually put in ridiculous values for   , why should we expect that

(cid:98)w     w?) on the other hand, we know from our elementary statistics courses

that maximum likelihood estimates are generally consistent, unless we choose a
spectacularly bad model. can we use that here?

(cid:16)

l =     np
2

we can, but at a cost. we have so far got away with just making assump-
tions about the means and covariances of the factor scores f. to get an actual
likelihood, we need to assume something about their distribution as well.
the usual assumption is that fik     n (0, 1), and that the factor scores are
independent across factors k = 1, . . . q and individuals i = 1, . . . n. with this
assumption, the features have a multivariate normal distribution (cid:126)xi     n (0,    +
wt w). this means that the log-likelihood is
log |   + wt w|     n
2

   1(cid:98)v
(cid:98)v     the actual factor scores f are not needed for the likelihood.
are the q leading eigenvectors of   1/2(cid:98)v  1/2. starting from a guess as to w, the
optimal choice of    is given by the diagonal entries of(cid:98)v   wt w. so again one starts

one can either try direct numerical maximization, or use a two-stage procedure.
starting, once again, with a guess as to   , one    nds that the crucial quantity is
actually   1/2wt , the optimal value of which is given by the matrix whose columns

where tr a is the trace of the matrix a, the sum of its diagonal elements. notice
that the likelihood only involves the data through the sample covariance matrix

log 2       n
2

with a guess about the unique variances (e.g., the residuals of the regressions)
and iterates to convergence.7

(   + wt w)

(17.37)

(cid:17)

tr

the di   erences between the maximum likelihood estimates and the    principal

7 the algebra is tedious. see section 3.2 in bartholomew (1987) if you really want it. (note that

bartholomew has a sign error in his equation 3.16.)

382

factor models

factors    approach can be substantial. if the data appear to be normally dis-
tributed (as shown by the usual tests), then the additional e   ciency of maxi-
mum likelihood estimation is highly worthwhile. also, as we   ll see below, it is a
lot easier to test the model assumptions if one uses the id113.

17.5.1 alternative approaches

factor analysis is an example of trying to approximate a full-rank matrix, here
the covariance matrix, with a low-rank matrix, or a low-rank matrix plus some
corrections, here    + wt w. such matrix-approximation problems are currently
the subject of very intense interest in statistics and machine learning, with many
new methods being proposed and re   ned, and it is very plausible that some of
these will prove to work better than older approaches to factor analysis.

in particular, kao and van roy (2013) have recently used these ideas to propose
a new factor-analysis algorithm, which simultaneously estimates the number of
factors and the factor loadings, and does so through a modi   cation of pca,
distinct from the old    principal factors    method. in their examples, it works
better than conventional approaches, but whether this will hold true generally is
not clear. they do not, unfortunately, provide code.

17.5.2 estimating factor scores

given x and (estimates of) the parameters, it   s natural to want to estimate
the factor scores f. one of the best methods for doing so is the    regression    or

   thomson    method, which says (cid:98)fir =
and seeks the weights bjr which will minimize the mean squared error, e(cid:104)

xijbjr

j

(17.38)

((cid:98)fir     fir)2(cid:105)

.

(cid:88)

you can work out the bjr as an exercise (17.6), assuming you know w and   .

17.6 the rotation problem

recall from id202 that a matrix o is orthogonal if its inverse is the
same as its transpose, ot o = i. the classic examples are rotation matrices. for
instance, to rotate a two-dimensional vector through an angle   , we multiply it
by

(cid:20) cos        sin   

(cid:21)

r   =

(17.39)
the inverse to this matrix must be the one which rotates through the angle      ,
r   1
   = r     , but trigonometry tells us that r      = rt
   .
to see why this matters to us, go back to the matrix form of the factor model,

cos   

sin   

17.7 factor analysis as a predictive model

383

and insert an orthogonal q    q matrix and its transpose:

x =   + fw

(17.40)
(17.41)
(17.42)
we   ve changed the factor scores to h     fo, and we   ve changed the factor loadings
to y     ot w, but nothing about the features has changed at all. we can do as
many orthogonal transformations of the factors as we like, with no observable
consequences whatsoever.8

=   + foot w
=   + hy

statistically, the fact that di   erent parameter settings give us the same obser-
vational consequences means that the parameters of the factor model are uniden-
ti   able. the rotation problem is, as it were, the revenant of having an ill-posed
problem: we thought we   d slain it through heroic feats of id202, but it   s
still around and determined to have its revenge.9

mathematically, this should not be surprising at all. the factors live in a q-
dimensional vector space of their own. we should be free to set up any coordinate
system we feel like on that space. changing coordinates in factor space will just
require a compensating change in how factor-space coordinates relate to feature
space (the factor loadings matrix w). that   s all we   ve done here with our orthog-
onal transformation.

substantively, this should be rather troubling. if we can rotate the factors as

much as we like without consequences, how on earth can we interpret them?

17.7 factor analysis as a predictive model

unlike principal components analysis, factor analysis really does give us a pre-
dictive model. its prediction is that if we draw a new member of the population
and look at the vector of observables we get from them,

(cid:126)x     n (0, wt w +   )

(17.43)

if we make the usual distributional assumptions. of course it might seem like it
makes a more re   ned, conditional prediction,

(cid:126)x| (cid:126)f     n (f w,   )

(17.44)

8 notice that the log-likelihood only involves wt w, which is equal to wt oot w = yt y, so even
assuming gaussian distributions doesn   t let us tell the di   erence between the original and the
transformed variables. in fact, if (cid:126)f     n (0, i), then (cid:126)f o     n (0o, ot io) = n (0, i)     in other words,
the rotated factor scores still satisfy our distributional assumptions.

9 remember that we obtained the loading matrix w as a solution to wt w = u, that is we got w as a

u and w =       
kind of matrix square root of the reduced correlation matrix. for a real number u there are two
square roots, i.e., two numbers w such that w    w = u, namely the usual w =
u,
because (   1)    (   1) = 1. similarly, whenever we    nd one solution to wt w = u, ot w is another
solution, because oot = i. so while the usual    square root    of u is w = dq
matrix ot dq

1/2c will always work just as well.

1/2c, for any orthogonal

   

384

factor models

but the problem is that there is no way to guess at or estimate the factor scores
(cid:126)f until after we   ve seen (cid:126)x, at which point anyone can predict x perfectly. so
the actual forecast is given by eq. 17.43.10

now, without going through the trouble of factor analysis, one could always

just postulate that

(cid:126)x     n (0, v)

(17.45)

and estimate v; the maximum likelihood estimate of it is the observed covari-
ance matrix, but really we could use any consistent estimator of the covariance
matrix. the closer ours is to the true v, the better our predictions. one way to
think of factor analysis is that it looks for the maximum likelihood estimate, but
constrained to matrices of the form wt w +   .

on the plus side, the constrained estimate has a faster rate of convergence.
that is, both the constrained and unconstrained estimates are consistent and
will converge on their optimal, population values as we feed in more and more
data, but for the same amount of data the constrained estimate is probably closer

to its limiting value. in other words, the constrained estimate (cid:98)wt(cid:98)w + (cid:98)   has less
variance than the unconstrained estimate (cid:98)v.

on the minus side, maybe the true, population v just can   t be written in the
form wt w +   . then we   re getting biased estimates of the covariance and the
bias will not go away, even with in   nitely many samples. using factor analysis
rather than just    tting a multivariate gaussian means betting that either this
bias is really zero, or that, with the amount of data on hand, the reduction in
variance outweighs the bias.

(i haven   t talked about estimation errors in the parameters of a factor model.
with large samples and maximum-likelihood estimation, one could use the usual
asymptotic theory. for small samples, one bootstraps as usual.)

17.7.1 how many factors?

how many factors should we use? all the tricks people use for the how-many-
principal-components question can be tried here, too, with the obvious modi   -
cations. however, some other answers can also be given, using the fact that the
factor model does make predictions, unlike pca.

1. log-likelihood ratio tests sample covariances will almost never be exactly equal
to population covariances. so even if the data comes from a model with q
factors, we can   t expect the tetrad equations (or their multi-factor analogs)
to hold exactly. the question then becomes whether the observed covariances
are compatible with sampling    uctuations in a q-factor model, or are too big
for that.

10 a subtlety is that we might get to see some but not all of (cid:126)x, and use that to predict the rest. say
(cid:126)x = (x1, x2), and we see x1. then we could, in principle, compute the conditional distribution of
the factors, p(f|x1), and use that to predict x2. of course one could do the same thing using the
correlation matrix, factor model or no factor model.

17.7 factor analysis as a predictive model

385

we can tackle this question by using log likelihood ratio tests. the crucial
observations are that a model with q factors is a special case of a model with
q + 1 factors (just set a row of the weight matrix to zero), and that in the most
general case, q = p, we can get any covariance matrix v into the form wt w.
(set    = 0 and proceed as in the    principal factors    estimation method.)

as explained in appendix i, if (cid:98)   is the maximum likelihood estimate in a
restricted model with s parameters, and (cid:98)   is the id113 in a more general model

with r > s parameters, containing the former as a special case, and    nally (cid:96)
is the log-likelihood function

2[(cid:96)((cid:98)  )     (cid:96)((cid:98)  )] (cid:32)   2

r   s

(17.46)

when the data came from the small model. the general regularity conditions
needed for this to hold apply to gaussian factor models, so we can test whether
one factor is enough, two, etc.

(said another way, adding another factor never reduces the likelihood, but
the equation tells us how much to expect the log-likelihood to go up when the
new factor really adds nothing and is just over-   tting the noise.)

determining q by getting the smallest one without a signi   cant result in a
likelihood ratio test is fairly traditional, but statistically messy.11 to raise a
subject we   ll return to, if the true q > 1 and all goes well, we   ll be doing lots
of hypothesis tests, and making sure this compound procedure works reliably
is harder than controlling any one test. perhaps more worrisomely, calculating
the likelihood relies on distributional assumptions for the factor scores and the
noises, which are hard to check for latent variables.

2. if you are comfortable with the distributional assumptions, use eq. 17.43 to
predict new data, and see which q gives the best predictions     for compara-
bility, the predictions should be compared in terms of the log-likelihood they
assign to the testing data. if genuinely new data is not available, use cross-
validation.

comparative prediction, and especially cross-validation, seems to be some-
what rare with factor analysis. there is no good reason why this should be
so.

17.7.1.1 r2 and goodness of fit

for pca, we saw that r2 depends on the sum of the eigenvalues 16.1.3. for factor
models, the natural notion of r2 is the sum of squared factor loadings:

(cid:80)q

(cid:80)p

r2 =

j=1

k=1 w2
jk
p

(17.47)

(remember that the factors are, by design, uncorrelated with each other, and
that the entries of w are the correlations between factors and observables.) if we

11 suppose q is really 1, but by chance that gets rejected. whether q = 2 gets rejected in turn is not an

independent event!

386
write w in terms of eigenvalues and eigenvectors as in   17.4.2, w =
then you can show that the numerator in r2 is, again, a sum of eigenvalues.

factor models

cqdq

(cid:16)

1/2(cid:17)t

,

people sometimes select the number of factors by looking at how much variance
they    explain        really, how much variance is kept after smoothing on to the
plane. as usual with model selection by r2, there is little good to be said for this,
except that it is fast and simple.

in particular, r2 should not be used to assess the goodness-of-   t of a factor
model. the bluntest way to see this is to simulate data which does not come
from a factor model,    t a small number of factors, and see what r2 one gets.
this was done by peterson (2000), who found that it was easy to get r2 of 0.4
or 0.5, and sometimes even higher12 the same paper surveyed values of r2 from
the published literature on factor models, and found that the typical value was
also somewhere around 0.5; no doubt this was just a coincidence13.

instead of looking at r2, it is much better to check goodness-of-   t by actually
goodness-of-   t tests. we looked at some tests of multivariate goodness-of-   t in
chapter e. in the particular case of factor models with the gaussian assumption,
we can use a log-likelihood ratio test, checking the null hypothesis that the number
of factors = q against the alternative of an arbitrary multivariate gaussian (which
is the same as p factors). this test is automatically performed by factanal in r.
if the gaussian assumption is dubious but we want a factor model and goodness-
of-   t anyway, we can look at the di   erence between the empirical covariance ma-

trix v and the one estimated by the factor model, (cid:98)   + (cid:98)wt(cid:98)w. there are several
of v and those of (cid:98)   + (cid:98)wt(cid:98)w. sampling distributions would have to come from

notions of distance between matrices (matrix norms) which could be used as test
statistics; one could also use the sum of squared di   erences between the entries

id64, where we would want to simulate from the factor model.

17.8 factor models versus pca once more

we began this chapter by seeking to add some noise, and some probabilistic
assumptions, into pca. the factor models we came up with are closely related
to principal components, but are not the same. many of the di   erences have been
mentioned as we went, but it   s worth collecting some of the most important ones
here.

1. factor models assume that the data comes from a certain distribution, iid
across data points. pca assumes nothing about distributions at all. moreover,
factor models can be used generatively, to say how the latent factors cause
the observable variables. pca has nothing to say about the data-generating
process.

12 see also http://bactra.org/weblog/523.html for a similar experiment, with (not very elegant) r

code.

13 peterson (2000) also claims that reported values of r2 for pca are roughly equal to those of factor

analysis, but by this point i hope that none of you take that as an argument in favor of pca.

17.9 examples in r

387

2. factor models can be tested by their predictions on new data points; pca

cannot.
3. factor models assume that the variance matrix of the data takes a special
form, wt w +   , where w is q    p and    is diagonal. that is, the variance
matrix must be    low rank plus noise   . pca works no matter what sample
variance matrix the data might have.

4. if the factor model is true, then the principal components are (or approach
with enough data) the eigenvectors of wt w +   . they do not approach the
eigenvectors of wt w, which would be the principal factors. if the noise is
small, the di   erence may also be small, but the factor model can be correct,
if perhaps not so useful, while    is as big as wt w.

5. factor models are subject to the rotation problem; pca is not. which one

has the advantage here is unclear.

6. similarly, a principal component is just a linear combination of the observable
variables. a latent factor is another, distinct random variable. di   erences in
factor scores imply di   erences in the expected values of observables. di   erences
in projections on to principal components imply di   erences in realized values
of observables. (it   s a little like the distinction between the predicted value for
the response in a id75, which is a combination of the covariates,
and the actual value of the response.)

17.9 examples in r

17.9.1 example 1: back to the us circa 1977

we resume looking at the properties of the us states around 1977. in   16.3, we
did a principal components analysis,    nding a    rst component that seemed to
mark the distinction between the south and the rest of the country, and a second
that seemed to separate big, rich states from smaller, poorer ones. let   s now
subject the data to factor analysis. we begin with one factor, using the base r
function factanal.

income illiteracy
0.235

(state.fa1 <- factanal(state.x77,factors=1,scores="regression"))
##
## call:
## factanal(x = state.x77, factors = 1, scores = "regression")
##
## uniquenesses:
## population
0.957
##
frost
##
##
0.600
##
## loadings:
##
## population -0.208
## income
0.458
## illiteracy -0.875
## life exp
0.750

life exp
0.437

0.791
area
0.998

murder
0.308

factor1

hs grad
0.496

388

factor models

-0.832
0.710
0.632

## murder
## hs grad
## frost
## area
##
##
## ss loadings
## proportion var
##
## test of the hypothesis that 1 factor is sufficient.
## the chi square statistic is 91.97 on 20 degrees of freedom.
## the p-value is 3.34e-11

factor1
3.178
0.397

the output here tells us what fraction of the variance in each observable comes

from its own noise (= the diagonal entries in (cid:98)   =    uniquenesses   ). it also gives
us the factor loadings, i.e., the rows of (cid:98)w. here there   s only one loading vector,

since we set factors = q = 1. as a courtesy, the default printing method for the
loadings leaves blanks where the loadings would be very small (here, for area);
this can be controlled through options (see help(loadings)). the last option
picks between di   erent methods of estimating the factor scores.

for comparison, here is the    rst principal component:

pc7

pc6

pc5

pc4

pc3

pc2

pc1

0.406
-0.638

pc8
-0.219

0.411 -0.656 -0.409
0.519 -0.100

0.126
-0.299
0.468
-0.412
0.307
0.444
-0.425
0.299
-0.357 -0.154

##
## loadings:
##
## population
## income
## illiteracy
## life exp
## murder
## hs grad
## frost
## area
##
pc8
##
## ss loadings
1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000
## proportion var 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125
## cumulative var 0.125 0.250 0.375 0.500 0.625 0.750 0.875 1.000

0.462
0.387 -0.620 -0.339
0.527
0.219 -0.256
0.108 -0.166 -0.128 -0.325 -0.295
0.678
-0.645 -0.393 -0.307

0.232
0.387 -0.619
0.201

0.213 -0.472
0.148
0.286

0.588 0.510

0.353
0.443

0.217
0.499

-0.360

0.327

pc1

pc3

pc4

pc5

pc6

pc7

pc2

the    rst principal component is clearly not the same as the single common
factor we extracted, even after a sign change, but it   s not shockingly dissimilar
either, as a map shows.

of course, why use just one factor? given the number of observables, we can    t
up to four factors before the problem becomes totally unidenti   ed and factanal
refuses to work. that function automatically runs the likelihood ratio test every
time it    ts a model, assuming gaussian distributions for the observables. as re-
marked, this can work reasonably well for non-gaussian distributions if they   re
not too non-gaussian, especially if n is much larger than the number of parame-
ters; of course n = 50 is pretty modest. still, let   s try it:

pvalues <- sapply(1:4,function(q){factanal(state.x77,factors=q)$pval})
signif(pvalues,2)

17.9 examples in r

389

plot.states_scaled(state.fa1$score[,1],min.size=0.3,max.size=1.5,

xlab="longitude",ylab="latitude")

figure 17.3 the us states, plotted in position with symbols scaled by
their factor scores in a one-factor model. compare to figure 16.5, which is
where the plot.states scaled function comes from. (try plotting the
negative of the factor scores to make the maps look more similar.)

## objective objective objective objective
##
4.7e-02

3.3e-11

3.3e-05

4.6e-03

(figure 17.4 plots the results.) none of the models has a p-value crossing the
conventional 0.05 level, meaning all of them show systematic, detectable depar-

   120   110   100   90   80   703035404550longitudelatitudealakazarcacoctdeflgahiidiliniakskylamemdmamimnmsmomtnenvnhnjnmnyncndohokorpariscsdtntxutvtvawawvwiwy390

factor models

plot(1:4,pvalues,xlab="q (number of factors)", ylab="pvalue",

log="y",ylim=c(1e-11,0.04))

abline(h=0.05,lty="dashed")

figure 17.4 gaussian likelihood ratio test p-value for models with various
numbers of latent factors,    t to the us-in-1977 data.

tures from what the data should look like if the factor model were true. still, the
four-factor model comes close.

notice that the    rst factor   s loadings do not stay the same when we add more

factors, unlike the    rst principal component:

0.561

0.636
0.189

factor1 factor2 factor3 factor4

print(factanal(state.x77, factors=4)$loadings)
##
## loadings:
##
## population
## income
0.313
## illiteracy -0.466
## life exp
0.891
-0.792
## murder
0.517
## hs grad
0.128
## frost
## area
-0.174
##
##
## ss loadings
## proportion var
## cumulative var

factor1 factor2 factor3 factor4
0.821
0.103
0.734

0.281
-0.878
0.191
-0.384
0.418
0.679

0.109
0.581
0.105
0.796

1.321
0.165
0.632

2.054
0.257
0.257

1.680
0.210
0.467

-0.460

0.405

llll1.01.52.02.53.03.54.01e   111e   091e   071e   051e   03q (number of factors)pvalue17.10 rei   cation, and alternatives to factor models

391

17.9.2 example 2: stocks

classical    nancial theory suggests that the log-returns of corporate stocks should
be iid gaussian random variables, but allows for the possibility that di   erent
stocks might be correlated with each other. in fact, theory suggests that the
returns to any given stock should be the sum of two components: one which is
speci   c to that    rm, and one which is common to all    rms. (more speci   cally,
the common component is one which couldn   t be eliminated even in a perfectly
diversi   ed portfolio.) this in turn implies that stock returns should match a
one-factor model.

17.10 rei   cation, and alternatives to factor models

a natural impulse, when looking at something like figure 17.1, is to reify the
factors, and to treat the arrows causally: that is, to say that there really is
some variable corresponding to each factor, and that changing the value of that
variable will change the features. for instance, one might want to say that there
is a real, physical variable corresponding to the factor f1, and that increasing
this by one standard deviation will, on average, increase x1 by 0.87 standard
deviations, decrease x2 by 0.75 standard deviations, and do nothing to the other
features. moreover, changing any of the other factors has no e   ect on x1.

sometimes all this is even right. how can we tell when it   s right?

delib-

rest
this

the
of
example
is
erately
omitted,
since
it
will be a
homework
assign-
ment.

17.10.1 the rotation problem again

consider the following matrix, call it r:

       cos 30     sin 30 0

cos 30

sin 30

      

0

0

0
1

(17.48)

applied to a three-dimensional vector, this rotates it thirty degrees counter-
clockwise around the vertical axis. if we apply r to the factor loading matrix
of the model in the    gure, we get the model in figure 17.5. now instead of x1
being correlated with the other variables only through one factor, it   s correlated
through two factors, and x4 has incoming arrows from three factors.

because the transformation is orthogonal, the distribution of the observations
is unchanged. in particular, the    t of the new factor model to the data will be
exactly as good as the    t of the old model. if we try to take this causally, however,
we come up with a very di   erent interpretation. the quality of the    t to the data
does not, therefore, let us distinguish between these two models, and so these two
stories about the causal structure of the data.

the rotation problem does not rule out the idea that checking the    t of a factor

model would let us discover how many hidden causal variables there are.

392

factor models

figure 17.5 the model from figure 17.1, after rotating the    rst two
factors by 30 degrees around the third factor   s axis. the new factor loadings
are rounded to two decimal places.

17.10.2 factors or mixtures?

suppose we have two distributions with id203 densities f0(x) and f1(x).
then we can de   ne a new distribution which is a mixture of them, with density
f  (x) = (1       )f0(x) +   f1(x), 0            1. the same idea works if we combine
more than two distributions, so long as the sum of the mixing weights sum to
one (as do    and 1       ). mixture models are a very    exible and useful way of
representing complicated id203 distributions14, and we will look at them in
detail in chapter 19.

i bring up mixture models here because there is a very remarkable result: any
linear factor model with q factors is equivalent to some mixture model with q + 1
clusters, in the sense that the two models have the same means and covariances
(bartholomew, 1987, pp. 36   38). recall from above that the likelihood of a factor
model depends on the data only through the correlation matrix. if the data really

14 they are also a probabilistic, predictive alternative to the kind of id91 techniques you may
have seen in data mining: each distribution in the mixture is basically a cluster, and the mixing
weights are the probabilities of drawing a new sample from the di   erent clusters.

g1x10.13x2-0.45x3-0.13x4-0.20g20.86-0.690.020.03f30.730.10x50.15x60.45e1e2e3e4e5e617.10 rei   cation, and alternatives to factor models

393

were generated by drawing from q + 1 clusters, then a model with q factors can
match the covariance matrix very well, and so get a very high likelihood. this
means it will, by the usual test, seem like a very good    t. needless to say, however,
the causal interpretations of the mixture model and the factor model are very
di   erent. the two may be distinguishable if the clusters are well-separated (by
looking to see whether the data are unimodal or not), but that   s not exactly
guaranteed.

all of which suggests that factor analysis can   t alone really tell us whether we
have q continuous latent variables, or one discrete hidden variable taking q + 1
values.

17.10.3 the thomson sampling model

we have been working with fewer factors than we have features. suppose that   s
not true. suppose that each of our features is actually a linear combination of a
lot of variables we don   t measure:

xij =   ij +

aiktkj =   ij + (cid:126)ai    (cid:126)tj

(17.49)

q(cid:88)

k=1

where q (cid:29) p. suppose further that the latent variables aik are totally independent
of one another, but they all have mean 0 and variance 1; and that the noises   ij
are independent of each other and of the aik, with variance   j; and the tkj are
independent of everything. what then is the covariance between xia and xib?
well, because e [xia] = e [xib] = 0, it will just be the expectation of the product
of the features:

(cid:105)
( (cid:126)ai    (cid:126)ta)( (cid:126)ai    (cid:126)tb)

(cid:105)

+ e(cid:104)
(cid:33)(cid:35)

(cid:126)ai    (cid:126)ta

(cid:105)
+ e(cid:104)
(  ia + (cid:126)ai    (cid:126)ta)(  ib + (cid:126)ai    (cid:126)tb)
(cid:33)(cid:32) q(cid:88)

(cid:105)

  ib

  ia

(cid:34)(cid:32) q(cid:88)
(cid:126)ai    (cid:126)tb
(cid:35)

k=1

l=1

aiktka

ailtlb

e [xiaxib]

= 0 + 0 + 0 + e

= e(cid:104)
= e [  ia  ib] + e(cid:104)
(cid:34)(cid:88)
(cid:88)
(cid:88)
q(cid:88)

= e

e [tkatkb]

=

=

=

k,l

k,l

k,l

k=1

aikailtkatlb

e [aikail] tkatlb

e [aikail] e [tkatlb]

(17.50)

(17.51)

(17.52)

(17.53)

(17.54)

(17.55)

(17.56)

(17.57)

394
where to get the last line i use the fact that e [aikail] = 1 if k = l and = 0
otherwise. if the coe   cients t are    xed, then the last expectation goes away and
we merely have the same kind of sum we   ve seen before, in the factor model.

factor models

instead, however, let   s say that the coe   cients t are themselves random (but
independent of a and   ). for each feature xia, we    x a proportion za between 0
and 1. we then set tka     bernoulli(za), with tka        tlb unless k = l and a = b.
then

and

e [tkatkb] = e [tka] e [tkb] = zazb

e [xiaxib] = qzazb

of course, in the one-factor model,

e [xiaxib] = wawb

(17.58)

(17.59)

(17.60)

so this random-sampling model looks exactly like the one-factor model with factor
loadings proportional to za. the tetrad equation, in particular, will hold.

(cid:80)q

now, it doesn   t make a lot of sense to imagine that every time we make an
observation we change the coe   cients t randomly. instead, let   s suppose that
they are    rst generated randomly, giving values tkj, and then we generate fea-
ture values according to eq. 17.49. the covariance between xia and xib will be
k=1 tkatkb. but this is a sum of iid random values, so by the law of large
numbers as q gets large this will become very close to qzazb. thus, for nearly all
choices of the coe   cients, the feature covariance matrix should come very close
to satisfying the tetrad equations and looking like there   s a single general factor.
in this model, each feature is a linear combination of a random sample of a
huge pool of completely independent features, plus some extra noise speci   c to the
feature.15 precisely because of this, the features are correlated, and the pattern
of correlations is that of a factor model with one factor. the appearance of a
single common cause actually arises from the fact that the number of causes is
immense, and there is no particular pattern to their in   uence on the features.

code example 30 simulates the thomson model.

tm <- rthomson(50,11,500,50)
factanal(tm$data,1)
##
## call:
## factanal(x = tm$data, factors = 1)

15 when godfrey thomson introduced this model in 1914, he used a slightly di   erent procedure to

generate the coe   cient tkj . for each feature he drew a uniform integer between 1 and q, call it qj ,
and then sampled the integers from 1 to q without replacement until he had qj random numbers;
these were the values of k where tkj = 1. this is basically similar to what i describe, setting
zj = qj /q, but a bit harder to analyze in an elementary way.     thomson (1916), the original paper,
includes what we would now call a simulation study of the model, where thomson stepped through
the procedure to produce simulated data, calculate the empirical correlation matrix of the features,
and check the    t to the tetrad equations. not having a computer, thomson generated the values of
tkj with a deck of cards, and of the aik and   ij by rolling 5220 dice.

17.10 rei   cation, and alternatives to factor models

395

# simulate godfrey thomson's 'sampling model' of mental abilities, and
# perform factor analysis on the resulting test scores.

# simulate the thomson model follow thomson's original
# sampling-without-replacement scheme pick a random number in 1:a for the
# number of shared abilities for each test then draw a
# sample-without-replacement of that size from 1:a; those are the shared
# abilities summed in that test.
specific variance of each test is also
# random; draw a number in 1:q, and sum that many independent normals, with
# the same parameters as the abilities.
# number of tests (d) number of shared abilities (a) number of specific
# abilities per test (q) mean of each ability (mean) sd of each ability (sd)
# depends on: mvrnorm from library mass (multivariate random normal
# generator) output: list, containing: matrix of test loadings on to general
# abilities vector of number of specific abilities per test matrix of
# abilities-by-testees matrix of general+specific scores by testees raw data
# (including measurement noise)
rthomson <- function(n, d, a, q, ability.mean = 0, ability.sd = 1) {

inputs: number of testees (n)

# attn: should really use more intuitive argument names number of testees =
# n number of tests = d number of shared abilities = a max. number of
# specific abilities per test = q

stopifnot(require(mass)) # for multivariate normal generation

# assign abilities to tests
general.per.test <- sample(1:a, size = d, replace = true)
specifics.per.test <- sample(1:q, size = d, replace = true)

# define the matrix assigning abilities to tests
general.to.tests <- matrix(0, a, d)
# exercise to the reader: vectorize this
for (i in 1:d) {

abilities <- sample(1:a, size = general.per.test[i], replace = false)
general.to.tests[abilities, i] <- 1

}

# covariance matrix of the general abilities
sigma <- matrix(0, a, a)
diag(sigma) <- (ability.sd)^2
mu <- rep(ability.mean, a)
x <- mvrnorm(n, mu, sigma)

# person-by-abilities matrix of abilities

# the 'general' part of the tests
general.tests <- x %*% general.to.tests
# now the 'specifics'
specific.tests <- matrix(0, n, d)
noisy.tests <- matrix(0, n, d)
# each test gets its own specific abilities, which are independent for each
# person exercise to the reader: vectorize this, too
for (i in 1:d) {

# each test has noises.per.test disturbances, each of which has the given
# sd; since these are all independent their variances add
j <- specifics.per.test[i]
specifics <- rnorm(n, mean = ability.mean * j, sd = ability.sd * sqrt(j))
specific.tests[, i] <- general.tests[, i] + specifics
# finally, for extra realism, some mean-zero trial-to-trial noise, so that
# if we re-use this combination of general and specific ability scores, we
# won't get the exact same test scores twice
noises <- rnorm(n, mean = 0, sd = ability.sd)
noisy.tests[, i] <- specific.tests[, i] + noises

}

tm <- list(data = noisy.tests, general.ability.pattern = general.to.tests,

numbers.of.specifics = specifics.per.test, ability.matrix = x, specific.tests = specific.tests)

return(tm)

}

code example 30: function for simulating the thomson latent-sampling model.

396

factor models

[1] 0.987 0.770 0.883 0.253 0.126 0.204 0.972 0.624 0.070 0.618 0.880

factor1

##
## uniquenesses:
##
##
## loadings:
##
[1,] 0.115
##
[2,] 0.479
##
[3,] 0.342
##
[4,] 0.865
##
[5,] 0.935
##
[6,] 0.892
##
[7,] 0.167
##
[8,] 0.613
##
##
[9,] 0.965
## [10,] 0.618
## [11,] 0.346
##
##
## ss loadings
## proportion var
##
## test of the hypothesis that 1 factor is sufficient.
## the chi square statistic is 36.44 on 44 degrees of freedom.
## the p-value is 0.784

factor1
4.614
0.419

the    rst command generates data from n = 50 items with p = 11 features
and q = 500 latent variables. (the last argument controls the average size of
the speci   c variances   j.) the result of the factor analysis is of course variable,
depending on the random draws; this attempt gave the proportion of variance as-
sociated with the factor as 0.42, and the p-value as 0.78. repeating the simulation
many times, one sees that the p-value is pretty close to uniformly distributed,
which is what it should be if the null hypothesis is true (figure 17.6). for    xed
n, the distribution becomes closer to uniform the larger we make q. in other
words, the goodness-of-   t test has little or no power against the alternative of
the thomson model.

modifying the thomson model to look like multiple factors grows notationally
cumbersome; the basic idea however is to use multiple pools of independently-
sampled latent variables, and sum them:

q1(cid:88)

q2(cid:88)

xij =   ij +

aiktkj +

bikrkj + . . .

(17.61)

k=1

k=1

where the tkj coe   cients are uncorrelated with the rkj, and so forth. in expec-
tation, if there are r such pools, this exactly matches the factor model with r
factors, and any particular realization is overwhelmingly likely to match if the
q1, q2, . . . qr are large enough.16

it   s not feasible to estimate the t of the thomson model in the same way that

16 a recent paper on the thomson model (bartholomew et al., 2009) proposes just this modi   cation to
multiple factors and to bernoulli sampling. however, i proposed this independently, in the fall 2008
version of these notes, about a year before their paper.

17.10 rei   cation, and alternatives to factor models

397

figure 17.6 mimcry of the one-factor model by the thomson model. the
thomson model was simulated 200 times with the parameters given above;
each time, the simulated data was then    t to a factor model with one factor,
and the p-value of the goodness-of-   t test extracted. the plot shows the
empirical cumulative distribution function of the p-values. if the null
hypothesis were exactly true, then p     unif(0, 1), and the theoretical cdf
would be the diagonal line (dashed).

we estimate factor loadings, because q > p. this is not the point of considering
the model, which is rather to make it clear that we actually learn very little about
where the data come from when we learn that a factor model    ts well. it could
mean that the features arise from combining a small number of factors, or on the
contrary from combining a huge number of factors in a random fashion. a lot of
the time the latter is a more plausible-sounding story.

0.00.20.40.60.81.00.00.20.40.60.81.0sampling distribution of fa p   value under thomson model200 replicates of 50 subjects eachp valueempirical cdfllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll398

factor models

for example, a common application of factor analysis is in marketing: you
survey consumers and ask them to rate a bunch of products on a range of features,
and then do factor analysis to    nd attributes which summarize the features.
that   s    ne, but it may well be that each of the features is in   uenced by lots of
aspects of the product you don   t include in your survey, and the correlations are
really explained by di   erent features being a   ected by many of the same small
aspects of the product. similarly for psychological testing: answering any question
is really a pretty complicated process involving lots of small processes and skills
(of perception, several kinds of memory, problem-solving, attention, motivation,
etc.), which overlap partially from question to question.

17.11 further reading

the classical papers by spearman (1904) and thurstone (1934) are readily avail-
able online, and very much worth reading for getting a sense of the problems
which motivated the introduction of factor analysis, and the skill with which
the founders grappled with them. loehlin (1992) is a decent textbook intended
for psychologists; the presumably mathematical and statistical level is decidedly
lower than that of this book, but it   s still useful. thomson (1939) remains one of
the most insightful books on factor analysis, though obviously there have been a
lot of technical re   nements since he wrote. it   s strongly recommended for anyone
who plans to make much use of the method. while out of print, used copies are
reasonably plentiful and cheap, and at least one edition is free online.

on purely statistical issues related to factor analysis, bartholomew (1987) is
by far the best reference i have found; it quite properly sets it in the broader
context of latent variable models, including the sort of latent class models we will
explore in chapter 19. the computational advice of that edition is, necessarily,
now quite obsolete; there is an updated edition from 2011, which i have not been
able to consult by the time of writing.

the use of factor analysis in psychological testing has given rise to a large con-
troversial literature, full of claims, counter-claims, counter-counter-claims, and so
on ad nauseam. without, here, going into that, i will just note that to the extent
the strongest arguments against (say) reifying the general factor extracted from
intelligence tests as    general intelligence    are good arguments, they do not just
apply to intelligence, but also to personality tests, and indeed to many procedures
outside psychology. in other words, if there   s a problem, it   s not just a problem
for intelligence testing alone, or even for psychology alone. on this, see glymour
(1998) and borsboom (2005, 2006).

exercises

17.1 prove eq. 17.13.
17.2 why is it fallacious to go from    the data have the kind of correlations predicted by a

one-factor model    to    the data were generated by a one-factor model   ?

exercises

399

17.3 consider figure 17.2 and its code. what is w here? what is   ? what is the implied

covariance matrix of (cid:126)x? .

17.4 show that the correlation between the jth feature and g, in the one-factor model, is wj .
17.5 check that eq. 17.11 and eq. 17.25 are compatible.
17.6 find the weights bjr for the thomson estimator of factor scores (eq. 17.38), assuming you

know w. do you need to assume a gaussian distribution?

18

nonlinear id84

[[todo:
fix cross-
references]]
[[todo:
fix
tion]]

nota-

[[pca and factor models examples of linear id84; they   re
adapted to situations where there is low-dimensional structure, but that struc-
ture is a line or plane (or linear subspace). non-linear dimension reduction is
an obvious extension, but, since there are many ways of being non-linear, it   s
nowhere near as settled a subject as the linear special case. this chapter will be-
gin with a stark example of how linear methods can fail, and then go through in
some detail one particular nonlinear dimension reduction method, called    locally
linear embedding   , which directly builds on what we   ve done with pca. the
further reading at the end of the chapter points towards other methods, with
some indication of their virtues and drawbacks.]]

18.1 why we need nonlinear id84

consider the points shown in figure 18.1. even though there are two features,
a.k.a. coordinates, all of the points fall on a one-dimensional curve (as it happens,
a logarithmic spiral). this is exactly the kind of constraint which it would be good
to recognize and exploit     rather than using two separate coordinates, we could
just say how far along the curve a data-point is.

pca will do poorly with data like this. remember that to get a one-dimensional
representation out of it, we need to take the    rst principal component, which is
the straight line along which the data   s projections have the most variance. if
this works for capturing structure along the spiral, then projections on to the
   rst pc should have the same order that the points have along the spiral.1 since,
fortuitously, the data are already in that order, we can just plot the    rst pc
against the row index (figure 18.2). the results are     there is really no other
word for it     screwy.

so, pca with one principal component fails to capture the one-dimensional
structure of the spiral. we could add another principal component, but then
we   ve just rotated our two-dimensional data. in fact, any linear dimensionality-
reduction method is going to fail here, simply because the spiral is not even
approximately a one-dimensional linear subspace.

what then are we to do?

1 it wouldn   t matter if the coordinate increased as we went out along the spiral or decreased, just so

long as it was monotonic.

400

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

18.1 why we need nonlinear id84

401

x = matrix(c(exp(-0.2 * (-(1:300)/10)) * cos(-(1:300)/10), exp(-0.2 * (-(1:300)/10)) *

sin(-(1:300)/10)), ncol = 2)

plot(x)

figure 18.1 two-dimensional data constrained to a smooth
one-dimensional region, namely the logarithmic spiral, r = e   0.2   in polar
coordinates.

1. stick to not-too-nonlinear structures.
2. somehow decompose nonlinear structures into linear subspaces.
3. generalize the eigenvalue problem of minimizing distortion.

there   s not a great deal to be said about (1). some curves can be approximated
by linear subspaces without too much heartbreak. (for instance, see figure 18.4.)
we can use things like pca on them, and so long as we remember that we   re

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   300   200   1000100   200   1000100200300400x[,1]x[,2]402

nonlinear id84

figure 18.2 projections of the spiral points on to their    rst principal
component. [[todo: make code-generated    gure!]]

just seeing an approximation, we won   t go too far wrong. but fundamentally this
is weak. (2) is hoping that we can somehow build a strong method out of this
weak one; as it happens we can, and it   s called locally linear embedding (and its
variants). the last is di   usion maps, which we   ll cover next lecture.

18.2 local linearity and manifolds

let   s look again at figure 18.4. a one-dimensional linear subspace is, in plain
words, a straight line. by doing pca on this part of the data alone, we are
approximating a segment of the spiral curve by a straight line. since the segment
is not very curved, the approximation is reasonably good. (or rather, the segment

050100150200250300-200-1000100200300indexprcomp(x)$x[, 1]18.2 local linearity and manifolds

403

fit.all = prcomp(x)
approx.all = fit.all$x[, 1] %*% t(fit.all$rotation[, 1])
plot(x, xlab = expression(x[1]), ylab = expression(x[2]))
points(approx.all, pch = 4)

figure 18.3 spiral data (circles) replotted with their one-dimensional pca
approximations (crosses).

was chosen so the approximation would be good, consequently it had to have low
curvature.) notice that this error is not a random scatter of points around the
line, but rather a systematic mis-match between the true curve and the line    
a bias which would not go away no matter how much data we had from the
spiral. the size of the bias depends on how big a region we are using, and how
much the tangent direction to the curve changes across that region     the average

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   300   200   1000100   200   1000100200300400x1x2404

nonlinear id84

fit = prcomp(x[270:280, ])
pca.approx = fit$x[, 1] %*% t(fit$rotation[, 1]) + colmeans(x[270:280, ])
plot(rbind(x[270:280, ], pca.approx), type = "n", xlab = expression(x[1]), ylab = expression(x[2]))
points(x[270:280, ])
points(pca.approx, pch = 4)

figure 18.4 portion of the spiral data (circles) together with its
one-dimensional pca approximation (crosses).

curvature. by using small regions when the curvature is high and big regions
when the curvature is low, we can maintain any desired degree of approximation.
if we shifted to a di   erent part of the curve, we could do pca on the data there,
too, getting a di   erent principal component and a di   erent linear approximations

   250   200   150   100   200   150   100x1x2lllllllllll18.2 local linearity and manifolds

405

to the data. generally, as we move around the degree of curvature will change,
so the size of the region we   d use would also need to grow or shrink.

this suggests that we could make some progress towards learning nonlinear
structures in the data by patching together lots of linear structures. we could,
for example, divide up the whole data space into regions, and do a separate
pca in each region. here we   d hope that in each region we needed only a single
principal component. such hopes would generally be dashed, however, because
this is a bit too simple-minded to really work.

1. we   d need to chose the number of regions, introducing a trade-o    between
having many points in each region (so as to deal with noise) and having small
regions (to keep the linear approximation good).

2. ideally, the regions should be of di   erent sizes, depending on average curvature,

but we don   t know the curvature.

3. what happens at the boundaries between regions? the principal components

of adjacent regions could be pointing in totally di   erent directions.

nonetheless, this is the core of a good idea. to make it work, we need to say
just a little about di   erential geometry, speci   cally the idea of a manifold.2
for our purposes, a manifold is a smooth, curved subset of a euclidean space,
in which it is embedded. the spiral curve (not the isolated points i plotted)
is a one-dimensional manifold in the plane, just as are lines, circles, ellipses and
parabolas. the surface of a sphere or a torus is a two-dimensional manifold,
like a plane. the essential fact about a q-dimensional manifold is that it can be
arbitrarily well-approximated by a q-dimensional linear subspace, the tangent
space, by taking a su   ciently small region about any point.3 (this generalizes
the fact any su   ciently small part of a curve about any point looks very much like
a straight line, the tangent line to the curve at that point.) moreover, as we move
from point to point, the local linear approximations change continuously, too.
the more rapid the change in the tangent space, the bigger the curvature of the
manifold. (again, this generalizes the relation between curves and their tangent
lines.) so if our data come from a manifold, we should be able to do a local linear
approximation around every part of the manifold, and then smoothly interpolate
them together into a single global system. to do id84     to
learn the manifold     we want to    nd these global low-dimensional coordinates.4

2 di   erential geometry is a very beautiful and important branch of mathematics, with its roots in the

needs of geographers in the 1800s to understand the curved surface of the earth in detail
(geodesy). the theory of curved spaces they developed for this purpose generalized the ordinary
vector calculus and euclidean geometry, and turned out to provide the mathematical language for
describing space, time and gravity (einstein   s general theory of relativity; lawrie (1990)), the other
fundamental forces of nature (gauge    eld theory; lawrie (1990)), dynamical systems arnol   d (1973);
guckenheimer and holmes (1983), and indeed statistical id136 (information geometry; kass and
vos (1997); amari and nagaoka (1993/2000)). good introductions are spivak (1965) and schutz
(1980) (which con   nes the physics to one (long) chapter on applications).

3 if it makes you happier: every point has an open neighborhood which is homeomorphic to rq, and

the transition from neighborhood to neighborhood is continuous and di   erentiable.

4 there are technicalities here which i am going to gloss over, because this is not a class in di   erential

406

nonlinear id84

18.3 locally linear embedding (lle)

locally linear embedding (or: local linear embedding, you see both) is a clever
scheme for    nding low-dimensional global coordinates when the data lie on (or
very near to) a manifold embedded in a high-dimensional space. the trick is
to do a di   erent linear id84 at each point (because locally
a manifold looks linear) and then combine these with minimal discrepancy. it
was introduced by roweis and saul (2000), though saul and roweis (2003) has
a fuller explanation. i don   t think it uses any elements which were unknown,
mathematically, since the 1950s. rather than diminishing what roweis and saul
did, this should make the rest of us feel humble. . .

the lle procedure has three steps: it builds a neighborhood for each point
in the data;    nds the weights for linearly approximating the data in that neigh-
borhood; and    nally    nds the low-dimensional coordinates best reconstructed by
those weights. this low-dimensional coordinates are then returned.
to be more precise, the lle algorithm is given as inputs an n   p data matrix
x, with rows (cid:126)xi; a desired number of dimensions q < p; and an integer k for
   nding local neighborhoods, where k     q + 1. the output is supposed to be an
n    q matrix y, with rows (cid:126)yi.
1. for each (cid:126)xi,    nd the k nearest neighbors.
2. find the weight matrix w which minimizes the residual sum of squares for

reconstructing each (cid:126)xi from its neighbors,

rss(w)     n(cid:88)

(cid:107)(cid:126)xi    (cid:88)

i=1

j(cid:54)=i

wij(cid:126)xj(cid:107)2

(18.1)

where wij = 0 unless (cid:126)xj is one of (cid:126)xi   s k-nearest neighbors, and for each i,

j wij = 1. (i will come back to this constraint below.)

3. find the coordinates y which minimize the reconstruction error using the

weights,

  (y)     n(cid:88)
subject to the constraints that (cid:80)

i=1

(cid:107)(cid:126)yi    (cid:88)

j(cid:54)=i

will come back to those constraints below, too.)

wij(cid:126)yj(cid:107)2

(18.2)

i yij = 0 for each j, and that yt y = i. (i

(cid:80)

18.3.1 finding neighborhoods

in step 1, we de   ne local neighborhoods for each point. by de   ning these in
terms of the k nearest neighbors, we make them physically large where the data

geometry. (take one, it   s good for you!) the biggest one is that most manifolds don   t admit of a
truly global coordinate system, one which is good everywhere without exception. but the places
where it breaks down are usually isolated point and easily identi   ed. for instance, if you take a
sphere, almost every point can be identi   ed by latitude and longitude     except for the poles, where
longitude becomes ill-de   ned. handling this in a mathematically precise way is tricky, but since
these are id203-zero cases, we can ignore them in a statistics class.

18.3 locally linear embedding (lle)

407

points are widely separated, and physically small when the density of the data is
high. we don   t know that the curvature of the manifold is low when the data are
sparse, but we do know that, whatever is happening out there, we have very little
idea what it is, so it   s safer to approximate it crudely. conversely, if the data
are dense, we can capture both high and low curvature. if the actual curvature is
low, we might have been able to expand the region without loss, but again, this is
playing it safe. so, to summarize, using k-nearest neighborhoods means we take
a    ne-grained view where there is a lot of data, and a coarse-grained view where
there is little data.

it   s not strictly necessary to use k-nearest neighbors here; the important thing
is to establish some neighborhood for each point, and to do so in a way which
conforms or adapts to the data.

18.3.2 finding weights

step 2 can be understood in a number of ways. let   s start with the local linearity
of a manifold. suppose that the manifold was exactly linear around (cid:126)xi, i.e., that it
and its neighbors belonged to a q-dimensional linear subspace. since q + 1 points
in generally de   ne a q-dimensional subspace, there would be some combination
of the neighbors which reconstructed (cid:126)xi exactly, i.e., some set of weights wij such
that

(cid:126)xi =

wij(cid:126)xj

(18.3)

j

conversely, if there are such weights, then (cid:126)xi and (some of) its neighbors do form
a linear subspace. since every manifold is locally linear, by taking a su   ciently
small region around each point we get arbitrarily close to having these equations
hold     n   1rss(w) should shrink to zero as n grows.

vitally, the same weights would work to reconstruct xi both in the high-
dimensional embedding space and the low-dimensional subspace. this means that
it is the weights around a given point which characterize what the manifold looks
like there (provided the neighborhood is small enough compared to the curva-
ture). finding the weights gives us the same information as    nding the tangent
space. this is why, in the last step, we will only need the weights, not the original
vectors.

now, about the constraints that (cid:80)

j wij = 1. this can be understood in two
ways, geometrically and probabilistically. geometrically, what it gives us is in-
variance under translation. that is, if we add any vector (cid:126)c to (cid:126)xi and all of its
neighbors, nothing happens to the function we   re minimizing:

(cid:88)

(cid:126)xi + (cid:126)c    (cid:88)

j

wij((cid:126)xj + (cid:126)c) = (cid:126)xi + (cid:126)c    

= (cid:126)xi    (cid:88)

j

(cid:33)

wij(cid:126)xj

(cid:32)(cid:88)

j

wij(cid:126)xj

    (cid:126)c

(18.4)

(18.5)

408

nonlinear id84

since we are looking at the same shape of manifold no matter how we move it
around in space, translational invariance is a constraint we want to impose.

probabilistically, forcing the weights to sum to one makes w a stochastic tran-
sition matrix.5 this should remind you of page-rank, where we built a markov
chain transition matrix from the graph connecting web-pages. there is a tight
connection here, which we   ll return to next time under the heading of di   usion
maps; for now this is just to tantalize.

we will see below how to actually minimize the squared error computationally;
as you probably expect by now, it reduces to an eigenvalue problem. actually it
reduces to a bunch (n) of eigenvalue problems: because there are no constraints
across the rows of w, we can    nd the optimal weights for each point separately.
naturally, this simpli   es the calculation.

18.3.2.1 k > p

if k > p, then not only is there a solution to (cid:126)xi = (cid:80)

if k, the number of neighbors, is greater than p, the number of features, then (in
general) the space spanned by k distinct vectors is the whole space. then (cid:126)xi can
be written exactly as a linear combination of its k-nearest neighbors.6 in fact,
j wij(cid:126)j, there are generally
in   nitely many solutions, because there are more unknowns (k) than equations
(p). when this happens, we say that the optimization problem is ill-posed, or
irregular. there are many ways of regularizing ill-posed problems. a common
one, for this case, is what is called l2 or tikhonov id173: instead of
minimizing

(cid:107)(cid:126)xi    (cid:88)

j

wij(cid:126)xj(cid:107)2
(cid:88)

wij(cid:126)xj(cid:107)2 +   

(18.6)

(18.7)

w2
ij

pick an    > 0 and minimize

(cid:107)(cid:126)xi    (cid:88)

j

j

this says: pick the weights which minimize a combination of reconstruction error
and the sum of the squared weights. as        0, this gives us back the least-squares
problem. to see what the second, sum-of-squared-weights term does, take the
opposite limit,           : the squared-error term becomes negligible, and we just
want to minimize the euclidean (   l2   ) norm of the weight vector wij. since the
weights are constrained to add up to 1, we can best achieve this by making all
the weights equal     so some of them can   t be vastly larger than the others, and
they stabilize at a de   nite preferred value. typically    is set to be small, but not
zero, so we allow some variation in the weights if it really helps improve the    t.
we will see how to actually implement this id173 later, when we look

5 actually, it really only does that if wij     0. in that case we are approximating (cid:126)xi not just by a
linear combination of its neighbors, but by a convex combination. often one gets all positive
weights anyway, but it can be helpful to impose this extra constraint.

6 this is easiest to see when (cid:126)xi lies inside the body which has its neighbors as vertices, their convex

hull, but is true more generally.

18.3 locally linear embedding (lle)

409

at the eigenvalue problems connected with lle. the l2 term is an example of a
penalty term, used to stabilize a problem where just matching the data gives
irregular results, and there is an art to optimally picking   ; in practice, however,
lle results are often fairly insensitive to it, when it   s needed at all7. remember,
the whole situation only comes up when k > p, and p can easily be very large    
[[6380 for the gene-expression data]], much larger for the [[times corpus]], etc.

18.3.3 finding coordinates

as i said above, if the local neighborhoods are small compared to the curvature
of the manifold, weights in the embedding space and weights on the manifold
should be the same. (more precisely, the two sets of weights are exactly equal for
linear subspaces, and for other manifolds they can be brought arbitrarily close
to each other by shrinking the neighborhood su   ciently.) in the third and last
step of lle, we have just calculated the weights in the embedding space, so we
take them to be approximately equal to the weights on the manifold, and solve
for coordinates on the manifold.

so, taking the weight matrix w as    xed, we ask for the y which minimizes

i

  (y) =

(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:126)yi    (cid:88)
a   ecting the sum of squares, because (cid:80)
(cid:88)

j(cid:54)=i

that is, what should the coordinates (cid:126)yi be on the manifold, that these weights
reconstruct them?

as mentioned, some constraints are going to be needed. remember that we saw
above that we could add any constant vector (cid:126)c to (cid:126)xi and its neighbors without
j wij = 1. we could do the same with
the (cid:126)yi, so the minimization problem, as posed, has an in   nity of equally-good
solutions. to    x this     to    break the degeneracy        we impose the constraint

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(18.8)

(cid:126)yjwij

(18.9)

1
n

i

(cid:126)yi = 0

since if the mean vector was not zero, we could just subtract it from all the (cid:126)yi
without changing the quality of the solution, this is just a book-keeping conve-
nience.

similarly, we also impose the convention that

1
n

yt y = i

(18.10)

i.e., that the covariance matrix of y be the (q-dimensional) identity matrix. this
is not as substantial as it looks. if we found a solution where the covariance
matrix of y was not diagonal, we could use pca to rotate the new coordinates
on the manifold so they were uncorrelated, giving a diagonal covariance matrix.

7 it   s no accident that the scaling factor for the penalty term is written with a greek letter; it can

also be seen as the lagrange multiplier enforcing a constraint on the solution (  h.3.3).

410

nonlinear id84

the only bit of this which is not, again, a book-keeping convenience is assuming
that all the coordinates have the same variance     that the diagonal covariance
matrix is in fact i.

this optimization problem is like [[multi-dimensional scaling]]: we are asking for
low-dimensional vectors which preserve certain relationships (averaging weights)
among high-dimensional vectors. we are also asking to do it under constraints,
which we will impose through lagrange multipliers. once again, it turns into an
eigenvalue problem, though one just a bit more subtle than what we saw with
pca in chapter 168.

unfortunately,    nding the coordinates does not break up into n smaller prob-
lems, the way    nding the weights did, because each row of y appears in    multiple
times, once as the focal vector (cid:126)yi, and then again as one of the neighbors of other
vectors.

18.4 more fun with eigenvalues and eigenvectors

to sum up: for each (cid:126)xi, we want to    nd the weights wij which minimize

wij(cid:126)xj(cid:107)2

(18.11)

straint that(cid:80)

vectors (cid:126)yi which minimize

where wij = 0 unless (cid:126)xj is one of the k nearest neighbors of (cid:126)xi, under the con-
j wij = 1. given those weights, we want to    nd the q-dimensional

rssi(w) = (cid:107)(cid:126)xi    (cid:88)

j

n(cid:88)

(cid:107)(cid:126)yi    (cid:88)

i=1

j

with the constraints that n   1(cid:80)

  (y) =

i (cid:126)yi = 0, n   1yt y = i.

wij(cid:126)yj(cid:107)2

(18.12)

18.4.1 finding the weights

we saw that rssi is invariant if we add an arbitrary (cid:126)c to all the vectors. set

in this subsection, assume that j just runs over the neighbors of (cid:126)xi, so we don   t
have to worry about the weights (including wii) which we know are zero.
(cid:126)c =    (cid:126)xi, centering the vectors on the focal point (cid:126)xi:
wij((cid:126)xj     (cid:126)xi)(cid:107)2
wij(cid:126)zj(cid:107)2

rssi = (cid:107)(cid:88)
= (cid:107)(cid:88)

(18.13)

(18.14)

j

de   ning (cid:126)zj = (cid:126)xj     (cid:126)xi. if we correspondingly de   ne the k    p matrix z, and set wi

j

8 one reason to suspect the appearance of eigenvalues, in addition to my very heavy-handed

foreshadowing, is that eigenvectors are automatically orthogonal to each other and normalized, so
making the columns of y be the eigenvectors of some matrix would automatically satisfy eq. 18.10.

18.4 more fun with eigenvalues and eigenvectors
to be the k    1 matrix, the vector we get from the sum is just wt
magnitude of any vector (cid:126)r, considered as a row matrix r, is rrt , so

411

i z. the squared

rssi = wt

(18.15)
notice that zzt is a k   k matrix consisting of all the inner products of the neigh-
bors. this symmetric matrix is called the gram matrix of the set of vectors,
and accordingly abbreviated g     here i   ll say gi to remind us that it depends
on our choice of focal point (cid:126)xi.

i zzt wi

rssi = wt

i giwi

(18.16)

notice that the data matter only in so far as they determine the gram matrix
gi; the problem is invariant under any transformation which leaves all the inner
products alone (translation, rotation, mirror-reversal, etc.).

we want to minimize rssi, but we have the constraint(cid:80)

j wij = 1. we impose
this via a lagrange multiplier,   .9 to express the constraint in matrix form,
introduce the k    1 matrix of all 1s, call it 1.10 then the constraint has the form
1t wi = 1, or 1t wi     1 = 0. now we can write the lagrangian:

i giwi       (1t w     1)
taking derivatives, and remembering that gi is symmetric,

l(wi,   ) = wt

or

if the gram matrix is invertible,

   l
   wi
   l
     

= 2giwi       1 = 0
= 1t wi     1 = 0

giwi =

  
2

1

wi =

g   1
i 1

  
2

where    can be adjusted to ensure that everything sums to 1.

18.4.1.1 k > p
if k > p, we modify the objective function to be

wt

i giwi +   wt

i wi

where    > 0 determines the degree of id173. proceeding as before to
impose the constraint,

l = wt

i giwi +   wt

i wi       (1t wi     1)

(18.23)

9 this    should not be confused with the penalty-term    used when k > p. see next sub-section.
10 this should not be confused with the identity matrix, i.

(18.17)

(18.18)

(18.19)

(18.20)

(18.21)

(18.22)

412

nonlinear id84

where now    is the lagrange multiplier. taking the derivative with respect to wi
and setting it to zero,

(gi +   i)wi =

2giwi + 2  wi =   1
  
2
  
2

wi =

1

(gi +   i)

   11

(18.24)

(18.25)

(18.26)

where, again, we pick    to properly normalize the right-hand side.

18.4.2 finding the coordinates

as with pca, it   s easier to think about the q = 1 case    rst; the general case
follows similar lines. so (cid:126)yi is just a single scalar number, yi, and y reduces to an
n    1 column of numbers. we   ll revisit q > 1 at the end.

the objective function is

  (y) =

wijyj

n(cid:88)
n(cid:88)

i=1

(cid:32)
yi    (cid:88)
(cid:32)(cid:88)

i     yi
y2

j

(cid:33)2
(cid:33)

(cid:32)(cid:88)

   

(cid:33)

(cid:32)(cid:88)

j

j

i=1

wijyj

wijyj

=
= yt y     yt (wy)     (wy)t y + (wy)t (wy)
= ((i     w)y)t ((i     w)y)
= yt (i     w)t (i     w)y

yi +

j

de   ne the m    m matrix m = (i     w)t (i     w).
  (y) = yt my

(18.27)

(cid:33)2

wijyj

(18.28)

(18.29)
(18.30)
(18.31)

(18.32)

this looks promising     it   s the same sort of quadratic form that we maximized
in doing pca.
now let   s use a lagrange multiplier    to impose the constraint that n   1yt y =
i     but, since q = 1, that   s the 1    1 identity matrix, i.e., the scalar number 1.
(18.33)

l(y,   ) = yt my       (n   1yt y     1)

note that this    is not the same as the    which constrained the weights!

proceeding as we did with pca,

   l
   y

= 2my     2  n   1y = 0

my =

  
n

y

(18.34)

(18.35)

or

so y must be an eigenvector of m. because y is de   ned for each point in the
data set, it is a function of the data-points, and we call it an eigenfunction, to

18.5 calculation

413

avoid confusion with things like the eigenvectors of pca (which are p-dimensional
vectors in feature space). because we are trying to minimize yt my, we want the
eigenfunctions going with the smallest eigenvalues     the bottom eigenfunctions
    unlike the case with pca, where we wanted the top eigenvectors.
m being an n    n matrix, it has, in general, n eigenvalues, and n mutually
orthogonal eigenfunctions. the eigenvalues are real and non-negative; the smallest
of them is always zero, with eigenfunction 1. to see this, notice that w1 = 1.11
then

(i     w)1 = 0
(i     w)t (i     w)1 = 0
m1 = 0

(18.36)
(18.37)
(18.38)

since this eigenfunction is constant, it doesn   t give a useful coordinate on the
manifold. to get our    rst coordinate, then, we need to take the two bottom
eigenfunctions, and discard the constant.

again as with pca, if we want to use q > 1, we just need to take multiple
eigenfunctions of m . to get q coordinates, we take the bottom q + 1 eigenfunc-
tions, discard the constant eigenfunction with eigenvalue 0, and use the others as
our coordinates on the manifold. because the eigenfunctions are orthogonal, the
no-covariance constraint is automatically satis   ed. notice that adding another
coordinate just means taking another eigenfunction of the same matrix m     as
is the case with pca, but not with factor analysis.

(what happened to the mean-zero constraint? well, we can add another la-
grange multiplier    to enforce it, but the constraint is linear in y, it   s ay = 0
for some matrix a [exercise: write out a], so when we take partial derivatives
we get

   l(y,   ,   )

   y

= 2my     2  y       a = 0

(18.39)

and this is the only equation in which    appears. so we are actually free to
pick any    we like, and may as well set it to be zero. geometrically, this is the
translational invariance yet again. in optimization terms, the size of the lagrange
multiplier tells us about how much better the solution could be if we relaxed the
constraint     when it   s zero, as here, it means that the constrained optimum is
also an unconstrained optimum     but we knew that already!)

18.5 calculation

let   s break this down from the top. the nice thing about doing this is that the
over-all function is four lines, one of which is just the return (example 31).

11 each row of w1 is a weighted average of the other rows of 1. but all the rows of 1 are the same.

414

nonlinear id84

lle <- function(x, q, k = q + 1, alpha = 0.01) {

stopifnot(q > 0, q < ncol(x), k > q, alpha > 0)
knns = find.knns(x, k)
w = reconstruction.weights(x, knns, alpha)
coords = coords.from.weights(w, q)
return(coords)

}

code example 31: locally linear embedding in r. notice that this top-level function is very
simple, and mirrors the math exactly.

find.knns <- function(x, k, ...) {

x.distances = dist(x, ...)
x.distances = as.matrix(x.distances)
knns = smallest.by.rows(x.distances, k + 1)
return(knns[, -1])

}

code example 32: finding the k nearest neighbors of all the row-vectors in a data frame. as
the main text says, this is a fairly slow way to do it, and shouldn   t be used on large data sets.

18.5.1 finding the nearest neighbors

the following approach is straightforward (exploiting an r utility function, order),
but not recommended for    industrial strength    uses. a lot of thought has been
given to e   cient algorithms for    nding nearest neighbors, and this isn   t even close
to the state of the art [[cites]]. for large n, the di   erence in e   ciency would be
quite substantial. for the present, however, this will do.

to    nd the k nearest neighbors of each point, we    rst need to calculate the
distances between all pairs of points. the neighborhoods only depend on these
distances, not the actual points themselves. we just need to    nd the k smallest
entries in each row of the distance matrix (example 32).

most of the work is done either by dist, a built-in function optimized for
calculating distance matrices, or by smallest.by.rows (example 33), which we
are about to write. the +1 and    1 in the last two lines come from simplifying
that.

smallest.by.rows uses the utility function order. given a vector, it returns
the permutation that puts the vector into increasing order, i.e., its return is a
vector of integers as long as its input.12 the    rst line of smallest.by.rows
applies order to each row of the input matrix m. the    rst column of row.orders
now gives the column number of the smallest entry in each row of m; the second
column, the second smallest entry, and so forth. by taking the    rst k columns,
we get the set of the smallest entries in each row. find.knns applies this function
to the distance matrix, giving the indices of the closest points. however, every
point is closest to itself, so to get k neighbors, we need the k + 1 closest points;
and we want to discard the    rst column we get back from smallest.by.rows.

12 there is a lot of control over ties, but we don   t care about ties. see help(order), though, it   s a

handy function.

[[attn:
consider
calling
fnn pack-
age?]]

18.5 calculation

415

smallest.by.rows <- function(m, k) {

stopifnot(ncol(m) >= k)
row.orders = t(apply(m, 1, order))
k.smallest = row.orders[, 1:k]
return(k.smallest)

}

code example 33: finding which columns contain the smallest entries in each row.

let   s check that we   re getting sensible results from the parts.

7
3

[,1] [,2]
2
4

(r <- matrix(c(7, 3, 2, 4), nrow = 2))
##
## [1,]
## [2,]
smallest.by.rows(r, 1)
## [1] 2 1
smallest.by.rows(r, 2)
##
## [1,]
## [2,]

[,1] [,2]
1
2

2
1

since 7 > 2 but 3 < 4, this is correct. now try a small distance matrix, from

the    rst    ve points on the spiral:

4

3

2

1

round(as.matrix(dist(x[1:5, ])), 2)
5
##
## 1 0.00 0.11 0.21 0.32 0.43
## 2 0.11 0.00 0.11 0.22 0.33
## 3 0.21 0.11 0.00 0.11 0.22
## 4 0.32 0.22 0.11 0.00 0.11
## 5 0.43 0.33 0.22 0.11 0.00
smallest.by.rows(as.matrix(dist(x[1:5, ])), 3)
##
## 1
## 2
## 3
## 4
## 5

[,1] [,2] [,3]
3
3
4
5
3

1
2
3
4
5

2
1
2
3
4

notice that the    rst column, as asserted above, is saying that every point is

closest to itself. but the two nearest neighbors are right.

find.knns(x[1:5, ], 2)
##
## 1
## 2
## 3
## 4
## 5

[,1] [,2]
3
3
4
5
3

2
1
2
3
4

success!

416

nonlinear id84

reconstruction.weights <- function(x, neighbors, alpha) {

stopifnot(is.matrix(x), is.matrix(neighbors), alpha > 0)
n = nrow(x)
stopifnot(nrow(neighbors) == n)
w = matrix(0, nrow = n, ncol = n)
for (i in 1:n) {

i.neighbors = neighbors[i, ]
w[i, i.neighbors] = local.weights(x[i, ], x[i.neighbors, ], alpha)

}
return(w)

}

code example 34: iterative (and so not really recommended) function to    nd linear least-
squares reconstruction weights.

local.weights <- function(focal, neighbors, alpha) {

stopifnot(nrow(focal) == 1, ncol(focal) == ncol(neighbors))
k = nrow(neighbors)
neighbors = t(t(neighbors) - focal)
gram = neighbors %*% t(neighbors)
weights = try(solve(gram, rep(1, k)))
if (identical(class(weights), "try-error")) {

weights = solve(gram + alpha * diag(k), rep(1, k))

}
weights = weights/sum(weights)
return(weights)

}

code example 35: find the weights for approximating a vector as a linear combination of the
rows of a matrix.

18.5.2 calculating the weights

first, the slow iterative way (example 34). aside from sanity-checking the in-
puts, this just creates a square, n    n weight-matrix w, initially populated
with all zeroes, and then    lls each line of it by calling a to-be-written function,
local.weights (example 35).

for testing, it would really be better to break local.weights up into two
sub-parts     one which    nds the gram matrix, and another which solves for the
weights     but let   s just test it altogether this once.

matrix(mapply("*", local.weights(x[1, ], x[2:3, ], 0.01), x[2:3, ]), nrow = 2)
##
## [1,]
## [2,] -0.989357
colsums(matrix(mapply("*", local.weights(x[1, ], x[2:3, ], 0.01), x[2:3, ]),

[,2]
2.014934 -0.4084473
0.3060440

[,1]

nrow = 2))

## [1]
colsums(matrix(mapply("*", local.weights(x[1, ], x[2:3, ], 0.01), x[2:3, ]),

1.0255769 -0.1024033

nrow = 2)) - x[1, ]

## [1]

0.0104723155 -0.0005531495

the mapply function is another of the lapply family of utility functions. just

18.5 calculation

417

as sapply sweeps a function along a vector, mapply sweeps a multi-argument
function (hence the m) along multiple argument vectors, recycling as necessary.
here the function is multiplication, so we   re getting the products of the recon-
struction weights and the vectors. (i re-organize this into a matrix for compre-
hensibility.) then i add up the weighted vectors, getting something that looks
reasonably close to x[1,]. this is con   rmed by actually subtract the latter from
the approximation, and seeing that the di   erences are small for both coordinates.

this didn   t use the id173; let   s turn it on and see what happens.

colsums(matrix(mapply("*", local.weights(x[1, ], x[2:4, ], 0.01), x[2:4, ]),

nrow = 3)) - x[1, ]

## [1]

0.01091407 -0.06487090

the error message alerts us that the unregularized attempt to solve for the
weights failed, since the determinant of the gram matrix was as close to zero
as makes no di   erence, hence it   s uninvertible. (the error message could be sup-
pressed by adding a silent=true option to try; see help(try).) however, with
just a touch of id173 (   = 0.01) we get quite reasonable accuracy.

let   s test our iterative solution. pick k = 2, each row of the weight matrix
should have two non-zero entries, which should sum to one. (we might expect
some small deviation from 1 due to    nite-precision arithmetic.) first, of course,
the weights should match what the local.weights function says.

1.9753018 -0.9753018

x.2nns <- find.knns(x, 2)
x.2nns[1, ]
## [1] 2 3
local.weights(x[1, ], x[x.2nns[1, ], ], 0.01)
## [1]
wts <- reconstruction.weights(x, x.2nns, 0.01)
sum(wts[1, ] != 0)
## [1] 2
all(rowsums(wts != 0) == 2)
## [1] true
all(rowsums(wts) == 1)
## [1] false
summary(rowsums(wts))
##
##

mean 3rd qu.
1

min. 1st qu.
1

median
1

1

1

max.
1

why does summary say that all the rows sum to 1, when directly testing that
says otherwise? because some rows don   t quite sum to 1, just closer-than-display
tolerance to 1.

sum(wts[1, ]) == 1
## [1] true
sum(wts[1, ])
## [1] 1
sum(wts[1, ]) - 1
## [1] 0
summary(rowsums(wts) - 1)
##
min.
## -2.220e-16

1st qu.
0.000e+00

median

mean
0.000e+00 -1.184e-17

3rd qu.
0.000e+00

max.
2.220e-16

so the constraint is satis   ed to   2  10   16, which is good enough for all practical

418

nonlinear id84

local.weights.for.index <- function(focal, x, nns, alpha) {

n = nrow(x)
stopifnot(n > 0, 0 < focal, focal <= n, nrow(nns) == n)
w = rep(0, n)
neighbors = nns[focal, ]
wts = local.weights(x[focal, ], x[neighbors, ], alpha)
w[neighbors] = wts
return(w)

}

code example 36: finding the weights for the linear approximation of a point given its index,
the data-frame, and the matrix of neighbors.

reconstruction.weights.2 <- function(x, neighbors, alpha) {

n = nrow(x)
w = sapply(1:n, local.weights.for.index, x = x, nns = neighbors, alpha = alpha)
w = t(w)
return(w)

}

code example 37: non-iterative calculation of the weight matrix.

[[attn:
simplify
by
all.equal
function?]]

using

purposes. it does, however, mean that we have to be careful about testing the
constraint!

all(abs(rowsums(wts) - 1) < 1e-07)
## [1] true

of course, iteration is usually not the way we do it in r     especially here,
where there   s no dependence between the rows of the weight matrix.13 what
makes this a bit tricky is that we need to combine information from two matrices
    the data frame and the matrix giving the neighborhood of each point. we
could try using something like mapply or map, but it   s cleaner to just write a
function to do the calculation for each row (example 36), and then apply it to
the rows.

as always, check the new function:

w.1 = local.weights.for.index(1, x, x.2nns, 0.01)
w.1[w.1 != 0]
## [1]
which(w.1 != 0)
## [1] 2 3

1.9753018 -0.9753018

so (at least for the    rst row!) it has the right values in the right positions.
now the    nal function is simple (example 37), and passes the check:

13 remember what makes loops slow in r is that every time we change an object, we actually create a

new copy with the modi   ed values and then destroy the old one. if n is large, then the weight
matrix, with n2 entries, is very large, and we are wasting a lot of time creating and destroying big
matrices to make small changes.

18.5 calculation

419

coords.from.weights <- function(w, q, tol = 1e-07) {

n = nrow(w)
stopifnot(ncol(w) == n)
stopifnot(all(abs(rowsums(w) - 1) < tol))
m = t(diag(n) - w) %*% (diag(n) - w)
soln = eigen(m)
coords = soln$vectors[, ((n - q):(n - 1))]
return(coords)

}

code example 38: getting manifold coordinates from approximation weights by    nding eigen-
functions.

wts.2 = reconstruction.weights.2(x, x.2nns, 0.01)
identical(wts.2, wts)
## [1] true

18.5.3 calculating the coordinates

having gone through all the eigen-manipulation, this is a straightforward calcu-
lation (example 38).

notice that w will in general be a very sparse matrix     it has only k non-
zero entries per row, and typically k (cid:28) n. there are special techniques for rapidly
solving eigenvalue problems for sparse matrices, which are not being used here
    another way in which this is not an industrial-strength version.

let   s try this out: make the coordinate (with q = 1), plot it (figure 18.5), and

check that it really is monotonically increasing, as the    gure suggests.

spiral.lle = coords.from.weights(wts, 1)
plot(spiral.lle, ylab = "coordinate on manifold")

420

nonlinear id84

all(diff(spiral.lle) > 0)
## [1] true

so the coordinate we got through lle increases along the spiral, just as it
should, and we have successfully recovered the underlying structure of the data.
to verify this in a more visually pleasing way, figure 18.6 plots the original data
again, but now with points colored so that their color in the rainbow corresponds
to their inferred coordinate on the manifold.

before celebrating our    nal victory, test that everything works when we put it

together:

all.equal(lle(x, 1, 2), spiral.lle)
## [1] true

2

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001502002503000.000.050.100.150.20indexcoordinate on manifold18.6 example

421

plot(coords.from.weights(wts, 1), ylab = "coordinate on manifold")

figure 18.5 coordinate on the manifold estimated by locally-linear
embedding for the spiral data. notice that it increases monotonically along
the spiral, as it should.

[[todo: actual data example?]]

18.6 example

18.7 further reading

[[sne]]

[[eigenmaps: belkin and niyogi (2003)]]

[[todo:
write,
insert
refs.]]

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001502002503000.000.050.100.150.20indexcoordinate on manifold422

nonlinear id84

plot(x, col = rainbow(300, end = 5/6)[cut(spiral.lle, 300, labels = false)])

figure 18.6 the original spiral data, but with color advancing smoothly
along the spectrum according to the intrinsic coordinate found by lle.

[[di   usion maps: form a similarity graph for the data, and then use as coordi-
nates projections on the eigenvectors of the graph laplacian lee and wasserman
(2010); distinct from lle14]] [[di   usion maps: see also http://www.stat.cmu.
edu/~cshalizi/350/lectures/15/lecture-15.pdf]]

14 in fact, in some cases, it can be shown (belkin and niyogi, 2003,   5) that the matrix in the lle

minimization problem is related to the laplacian, because (i     w)t (i     w)     1
2 l2. since the powers
of l have the same eigenvectors as l, when this holds the coordinates we get from the di   usion map
are approximately the same as the lle coordinates.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   300   200   1000100   200   1000100200300400x[,1]x[,2]exercises

423

[[manifold learning]]
[[lle: note that, like pca, not really    tting a id203 model (so pure
data analysis, rather than statistical id136)     refs. on ideas more like factor
models, with an inferential/probabilistic component]]

exercises

18.1 let b be any n    n matrix. show that if (cid:126)v is an eigenvector of b, then it is also an eigen-
vector of b2, and of any power of b. conclude that b and b2 have the same eigenvectors.
(hint: how many eigenvectors does each matrix have?) what happens to the eigenvalues?
18.2 in local linear embedding, we obtain an n    n matrix w, where wij is the weight on (cid:126)xj
we use to reconstruct (cid:126)xi. each row of w sums to one. we then try to    nd coordinates
y1, y2, . . . yn which minimize

n(cid:88)

      yi    

n(cid:88)

      2

  (y) =

wij yj

(18.40)

where y is the n    1 matrix of yi values (this is the q = 1 case, for simplicity). above
[[todo: eqref]] we showed that this is the same as minimizing

i=1

j=1

where

  (y) = yt my

m = ((i     w)t (i     w))

(18.41)

(18.42)

1. show that m is a symmetric matrix.
2. show that 1 is an eigenvector of m, and that its eigenvalue is zero.
3. show that   (y) =   (y + c1), where c is any constant and 1 is the n   1 matrix whose

entries are all 1s. (hint: one way is to use the previous two parts.)

n   1(cid:80)n

4. show that   (y) is minimized by y = 0.
5. to avoid the trivial solution of setting all the yi to zero, we impose the constraint that
i = 1. we use a lagrange multiplier to enforce this constraint; write down

i=1 y2

the lagrangian for the constrained minimization problem.

6. show that a solution y to the constrained minimization problem must be an eigenvec-

tor of m.

[[todo:
check no-
tation,
opening,
as this was
originally
a separate
assign-
ment]]

19

mixture models

19.1 two routes to mixture models

19.1.1 from factor analysis to mixture models

in factor analysis, the origin myth is that we have a fairly small number, q of real
variables which happen to be unobserved (   latent   ), and the much larger number
p of variables we do observe arise as linear combinations of these factors, plus
noise. the mythology is that it   s possible for us (or for someone) to continuously
adjust the latent variables, and the distribution of observables likewise changes
continuously. what if the latent variables are not continuous but ordinal, or even
categorical? the natural idea would be that each value of the latent variable
would give a di   erent distribution of the observables.

19.1.2 from kernel density estimates to mixture models

we have also previously looked at kernel density estimation, where we approx-
imate the true distribution by sticking a small ( 1
n weight) copy of a kernel pdf
at each observed data point and adding them up. with enough data, this comes
arbitrarily close to any (reasonable) id203 density, but it does have some
drawbacks. statistically, it labors under the curse of dimensionality. computa-
tionally, we have to remember all of the data points, which is a lot. we saw similar
problems when we looked at fully non-parametric regression, and then saw that
both could be ameliorated by using things like additive models, which impose
more constraints than, say, unrestricted kernel smoothing. can we do something
like that with density estimation?

additive modeling for densities is not as common as it is for regression    
it   s harder to think of times when it would be natural and well-de   ned1     but
we can do things to restrict density estimation. for instance, instead of putting
a copy of the kernel at every point, we might pick a small number k (cid:28) n of
points, which we feel are somehow typical or representative of the data, and put
a copy of the kernel at each one (with weight 1
k ). this uses less memory, but it

1 remember that the integral of a id203 density over all space must be 1, while the integral of a

regression function doesn   t have to be anything in particular. if we had an additive density,

f (x) =(cid:80)
(cid:82) fj (xj )dx1dx2 . . . dxp = 1. it would be easier to ensure id172 while making the
(cid:80)

j fj (xj ), ensuring id172 is going to be very tricky; we   d need

j

log-density additive, but that assumes the features are independent of each other.

424

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

k(cid:88)

19.1 two routes to mixture models

425

ignores the other data points, and lots of them are probably very similar to those
points we   re taking as prototypes. the di   erences between prototypes and many
of their neighbors are just matters of chance or noise. rather than remembering
all of those noisy details, why not collapse those data points, and just remember
their common distribution? di   erent regions of the data space will have di   erent
shared distributions, but we can just combine them.

19.1.3 mixture models

more formally, we say that a distribution f is a mixture of k cluster2 distri-
butions f1, f2, . . . fk if

(19.1)

with the   k being the mixing weights,   k > 0,(cid:80)

  kfk(x)

f (x) =

k=1

k   k = 1. eq. 19.1 is a complete
stochastic model, so it gives us a recipe for generating new data points:    rst pick
a distribution, with probabilities given by the mixing weights, and then generate
one observation according to that distribution. symbolically,

z     mult(  1,   2, . . .   k)

x|z     fz

(19.2)
(19.3)

where i   ve introduced the discrete random variable z which says which cluster
x is drawn from.

i haven   t said what kind of distribution the fks are. in principle, we could make
these completely arbitrary, and we   d still have a perfectly good mixture model.
in practice, a lot of e   ort is given over to parametric mixture models, where
the fk are all from the same parametric family, but with di   erent parameters    
for instance they might all be gaussians with di   erent centers and variances, or
all poisson distributions with di   erent means, or all power laws with di   erent
exponents. (it   s not necessary, just customary, that they all be of the same kind.)
we   ll write the parameter, or parameter vector, of the kth cluster as   k, so the
model becomes

k(cid:88)

f (x) =

  kf (x;   k)

(19.4)

k=1

the over-all parameter vector of the mixture model is thus    = (  1,   2, . . .   k,   1,   2, . . .   k).

let   s consider two extremes. when k = 1, we have a simple parametric dis-
tribution, of the usual sort, and density estimation reduces to estimating the
parameters, by maximum likelihood or whatever else we feel like. on the other
hand when k = n, the number of observations, we have gone back towards kernel
density estimation. if k is    xed as n grows, we still have a parametric model,
and avoid the curse of dimensionality, but a mixture of (say) ten gaussians is

2 many people write    components    instead of    clusters   , but i am deliberately avoiding that here so

as not to lead to confusion with the components of pca.

426

mixture models

more    exible than a single gaussian     thought it may still be the case that the
true distribution just can   t be written as a ten-gaussian mixture. so we have our
usual bias-variance or accuracy-precision trade-o        using many clusters in the
mixture lets us    t many distributions very accurately, with low approximation
error or bias, but means we have more parameters and so we can   t    t any one of
them as precisely, and there   s more variance in our estimates.

19.1.4 geometry

in chapter 16, we looked at principal components analysis, which    nds linear
structures with q dimensions (lines, planes, hyper-planes, . . . ) which are good
approximations to our p-dimensional data, q (cid:28) p. in chapter 17, we looked at
factor analysis, which imposes a statistical model for the distribution of the data
around this q-dimensional plane (gaussian noise), and a statistical model of the
distribution of representative points on the plane (also gaussian). this set-up is
implied by the mythology of linear continuous latent variables, but can arise in
other ways.

we know from geometry that it takes q + 1 points to de   ne a q-dimensional
plane, and that in general any q +1 points on the plane will do. this means that if
we use a mixture model with q +1 clusters, we will also get data which lies around
a q-dimensional plane. furthermore, by adjusting the mean of each cluster, and
their relative weights, we can make the global mean of the mixture whatever we
like. and we can even match the covariance matrix of any q-factor model by using
a mixture with q + 1 clusters3. now, this mixture distribution will hardly ever
be exactly the same as the factor model   s distribution     mixtures of gaussians
aren   t gaussian, the mixture will usually (but not always) be multimodal while
the factor distribution is always unimodal     but it will have the same geometry (a
q-dimensional subspace plus noise), and the same mean and the same covariances,
so we will have to look beyond those to tell them apart. which, frankly, people
hardly ever do.

19.1.5 identi   ability

before we set about trying to estimate our id203 models, we need to make
sure that they are identi   able     that if we have distinct representations of the
model, they make distinct observational claims. it is easy to let there be too many
parameters, or the wrong choice of parameters, and lose identi   ability. if there
are distinct representations which are observationally equivalent, we either need
to change our model, change our representation, or    x on a unique representation
by some convention.

    with additive regression, e [y |x = x] =    +(cid:80)

j fj(xj), we can add arbitrary
constants so long as they cancel out. that is, we get the same predictions

3 see bartholomew (1987, pp. 36   38). the proof is tedious algebraically.

from    + c0 +(cid:80)
the same form,   (cid:48) +(cid:80)

19.1 two routes to mixture models

j fj(xj) + cj when c0 =    (cid:80)

j f(cid:48)

j cj. this is another model of
j(xj), so it   s not identi   able. we dealt with this by
imposing the convention that    = e [y ] and e [fj(xj)] = 0     we picked out
a favorite, convenient representation from the in   nite collection of equivalent
representations.
    id75 becomes unidenti   able with collinear features. collinearity is
    factor analysis is unidenti   able because of the rotation problem. some people
respond by trying to    x on a particular representation, others just ignore it.

a good reason to not use id75 (i.e., we change the model.)

427

two kinds of identi   cation problems are common for mixture models; one is
trivial and the other is fundamental. the trivial one is that we can always swap
the labels of any two clusters with no e   ect on anything observable at all     if we
decide that cluster number 1 is now cluster number 7 and vice versa, that doesn   t
change the distribution of x at all. this label degeneracy can be annoying,
especially for some estimation algorithms, but that   s the worst of it.

a more fundamental lack of identi   ability happens when mixing two distribu-
tions from a parametric family just gives us a third distribution from the same
family. for example, suppose we have a single binary feature, say an indicator for
whether someone will pay back a credit card. we might think there are two kinds
of customers, with high- and low- risk of not paying, and try to represent this as
a mixture of bernoulli distribution. if we try this, we   ll see that we   ve gotten a
single bernoulli distribution with an intermediate risk of repayment. a mixture
of bernoulli is always just another bernoulli. more generally, a mixture of discrete
distributions over any    nite number of categories is just another distribution over
those categories4.

19.1.6 probabilistic id91

yet another way to view mixture models, which i hinted at when i talked about
how they are a way of putting similar data points together into    clusters   , where
clusters are represented by the distributions going into the mixture. the idea
is that all data points of the same type, belonging to the same cluster or class,
are more or less equivalent and all come from the same distribution, and any
di   erences between them are matters of chance. this view exactly corresponds to
mixture models like eq. 19.1; the hidden variable z i introduced above in just
the cluster label.

one of the very nice things about probabilistic id91 is that eq. 19.1 ac-
tually claims something about what the data looks like; it says that it follows a

4 that is, a mixture of any two n = 1 multinomials is another n = 1 multinomial. this is not

generally true when n > 1; for instance, a mixture of a binom(2, 0.75) and a binom(2, 0.25) is not a
binom(2, p) for any p (exercise 19.2). however, both of those binomials is a distribution on {0, 1, 2},
and so is their mixture. this apparently trivial point actually leads into very deep topics, since it
turns out that which models can be written as mixtures of others is strongly related to what
properties of the data-generating process can actually be learned from data: see lauritzen (1984).

428

mixture models

certain distribution. we can check whether it does, and we can check whether
new data follows this distribution. if it does, great; if not, if the predictions sys-
tematically fail, then the model is wrong. we can compare di   erent probabilistic
id91s by how well they predict (say under cross-validation).5

in particular, probabilistic id91 gives us a sensible way of answering the
question    how many clusters?    the best number of clusters to use is the number
which will best generalize to future data. if we don   t want to wait around to get
new data, we can approximate generalization performance by cross-validation, or
by any other adaptive model selection procedure.

19.1.7 simulation

simulating from a mixture model works rather like simulating from a kernel
density estimate (  14.7.1). to draw a new value   x,    rst draw a random integer
z from 1 to k, with probabilities   k, then draw from the z th cluster. (that is,
  x|z     fz.) note that if we want multiple draws,   x1,   x2, . . .   xb, each of them
needs an independent z.

19.2 estimating parametric mixture models

from intro stats., we remember that it   s generally a good idea to estimate distri-
butions using maximum likelihood, when we can. how could we do that here?

remember that the likelihood is the id203 (or id203 density) of ob-
serving our data, as a function of the parameters. assuming independent samples,
that would be

f (xi;   )

(19.5)

for observations x1, x2, . . . xn. as always, we   ll use the logarithm to turn multi-
plication into addition:

(cid:96)(  ) =

=

log f (xi;   )

k(cid:88)

log

  kf (xi;   k)

(19.6)

(19.7)

i=1

k=1

5 contrast this with id116 or hierarchical id91, which you may have seen in other classes:

they make no predictions, and so we have no way of telling if they are right or wrong. consequently,
comparing di   erent non-probabilistic id91s is a lot harder!

n(cid:89)

i=1

n(cid:88)
n(cid:88)

i=1

19.2 estimating parametric mixture models

429

let   s try taking the derivative of this with respect to one parameter, say   j.

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

(cid:80)k
(cid:80)k
(cid:80)k

   (cid:96)
     j

=

=

=

1

k=1   kf (xi;   k)
  jf (xi;   j)
k=1   kf (xi;   k)
  jf (xi;   j)
k=1   kf (xi;   k)

  j

   f (xi;   j)

     j

1

   f (xi;   j)

f (xi;   j)

     j

    log f (xi;   j)

     j

(19.8)

(19.9)

(19.10)

if we just had an ordinary parametric model, on the other hand, the derivative
of the log-likelihood would be

so maximizing the likelihood for a mixture model is like doing a weighted likeli-
hood maximization, where the weight of xi depends on cluster, being

n(cid:88)

    log f (xi;   j)

i=1

     j

(cid:80)k

  jf (xi;   j)
k=1   kf (xi;   k)

wij =

(19.11)

(19.12)

the problem is that these weights depend on the parameters we are trying to
estimate!6

let   s look at these weights wij a bit more. remember that   j is the id203
that the hidden class variable z is j, so the numerator in the weights is the
joint id203 of getting z = j and x = xi. the denominator is the marginal
id203 of getting x = xi, so the ratio is the id155 of z = j
given x = xi,

(cid:80)k

  jf (xi;   j)
k=1   kf (xi;   k)

wij =

= p(z = j|x = xi;   )

(19.13)

if we try to estimate the mixture model, then, we   re doing weighted maximum
likelihood, with weights given by the posterior cluster probabilities. these, to
repeat, depend on the parameters we are trying to estimate, so there seems to be
a vicious circle.

but, as the saying goes, one man   s vicious circle is another man   s successive
approximation procedure. a crude way of doing this7 would start with an initial
guess about the cluster distributions;    nd out which cluster each point is most
likely to have come from; re-estimate the clusters using only the points assigned
to it, etc., until things converge. this corresponds to taking all the weights wij to
be either 0 or 1. however, it does not maximize the likelihood, since we   ve seen
that to do so we need fractional weights.

what   s called the em algorithm is simply the obvious re   nement of this    hard   

assignment strategy.

6 matters are no better, but also no worse, for    nding   j ; see exercise 19.3.
7 related to what   s called    id116    id91.

430

mixture models

1. start with guesses about the cluster distributions   1,   2, . . .   k and the mixing

weights   1, . . .   k.

2. until nothing changes very much:

1. using the current parameter guesses, calculate the weights wij (e-step)
2. using the current weights, maximize the weighted likelihood to get new

parameter estimates (m-step)

3. return the    nal parameter estimates (including mixing proportions) and clus-

ter probabilities

the m in    m-step    and    em    stands for    maximization   , which is pretty
transparent. the e stands for    expectation   , because it gives us the conditional
probabilities of di   erent values of z, and probabilities are expectations of indica-
tor functions. (in fact in some early applications, z was binary, so one really was
computing the expectation of z.) the whole thing is also called the    expectation-
maximization    algorithm.

19.2.1 more about the em algorithm

the em algorithm turns out to be a general way of maximizing the likelihood
when some variables are unobserved, and hence useful for other things besides
mixture models (e.g., when some variables are missing some of the time     see
]  k.3.2). so in this section, where i try to explain why it works, i am going to be
a bit more general and abstract. (also, it will actually cut down on notation.) i   ll
pack the whole sequence of observations x1, x2, . . . xn into a single variable d (for
   data   ), and likewise the whole sequence of z1, z2, . . . zn into h (for    hidden   ).
what we want to do is maximize

(cid:96)(  ) = log p(d;   ) = log

p(d, h;   )

(19.14)

(cid:88)

h

this is generally hard, because even if p(d, h;   ) has a nice parametric form, that
is lost when we sum up over all possible values of h (as we saw above). the
essential trick of the em algorithm is to maximize not the log likelihood, but
a lower bound on the log-likelihood, which is more tractable; we   ll see that this
lower bound is sometimes tight, i.e., coincides with the actual log-likelihood, and
in particular does so at the global optimum.

we can introduce an arbitrary8 distribution on h, call it q(h), and we   ll write

(cid:88)
(cid:88)
(cid:88)

h

h

h

(cid:96)(  ) = log

= log

= log

p(d, h;   )

q(h)
q(h)

p(d, h;   )

q(h)

p(d, h;   )

q(h)

(19.15)

(19.16)

(19.17)

8 well, almost arbitrary; if some h has id203 > 0 for all   , then it shouldn   t give that h

id203 zero.

19.2 estimating parametric mixture models

431

curve(log(x),from=0.4,to=2.1)
segments(0.5,log(0.5),2,log(2),lty=2)

figure 19.1 the logarithm is a concave function, i.e., the curve connecting
any two points lies above the straight line doing so. thus the average of
logarithms is less than the logarithm of the average.

so far so trivial.

now we need a geometric fact about the logarithm function, which is that
its curve is concave: if we take any two points on the curve and connect them
by a straight line, the curve lies above the line (figure 19.1 and exercise 19.6).
algebraically, this means that

w log t1 + (1     w) log t2     log wt1 + (1     w)t2

(19.18)
for any 0     w     1, and any points t1, t2 > 0. nor does this just hold for two
(cid:80)r
points: for any r points t1, t2, . . . tr > 0, and any set of non-negative weights

i=1 wr = 1,

r(cid:88)

r(cid:88)

wi log ti     log

i=1

i=1

witi

(19.19)

in words: the log of the average is at least the average of the logs. this is called
jensen   s inequality. so

(cid:88)

h

log

q(h)

p(d, h;   )

q(h)

   (cid:88)

h

    j(q,   )

q(h) log

p(d, h;   )

q(h)

(19.20)

(19.21)

we are bothering with this because we hope that it will be easier to maximize
this lower bound on the likelihood than the actual likelihood, and the lower bound

0.51.01.52.0   0.50.00.5xlog(x)432
is reasonably tight. as to tightness, suppose that we set q(h) = p(h|d;   ). for this
special choice of q, call it   q,

mixture models

p(d, h;   )

  q(h)

=

p(d, h;   )
p(h|d;   )

=

p(d, h;   )

p(h, d;   )/p(d;   )

= p(d;   )

(19.22)

no matter what h is. this implies j(  q,   ) = (cid:96)(  ):

(cid:88)
(cid:88)

h

h

j(  q,   ) =

=

p(d, h;   )

  q(h)

  q(h) log
p(h|d;   ) log p(d;   )
p(h|d;   )

(cid:88)

= log p(d;   )

(19.23)

(19.24)

(19.25)

(19.26)
using eq. 19.22 in the second line. this means that the lower bound j(q,   )     (cid:96)(  )
is tight. moreover, setting q =   q maximizes j(q,   ) for    xed   .

= (cid:96)(  )

here   s how the em algorithm goes in this formulation.

h

1. start with an initial guess   (0) about the clusters and mixing weights.
2. until nothing changes very much

1. e-step: q(t) = argmaxq j(q,   (t)), i.e., set q(t)(h) = p(h|d;   (t)).
2. m-step:   (t+1) = argmax   j(q(t),   )

3. return    nal estimates of    and q

the e and m steps are now nice and symmetric; both are about maximizing j.
it   s easy to see that, after the e step,

j(q(t),   (t))     j(q(t   1),   (t))

and that, after the m step,

j(q(t),   (t+1))     j(q(t),   (t))

putting these two inequalities together,

j(q(t+1),   (t+1))     j(q(t),   (t))

(cid:96)(  (t+1))     (cid:96)(  (t))

(19.27)

(19.28)

(19.29)
(19.30)

so each em iteration can only improve the likelihood, guaranteeing convergence
to a local maximum. since it only guarantees a local maximum, it   s a good idea
to try a few di   erent initial values of   (0) and take the best.
posterior id203 p(h|d;   ). what about the maximization in the m step?

we saw above that the maximization in the e step is just computing the

(cid:88)

h

q(h) log

p(d, h;   )

q(h)

=

(cid:88)

q(h) log p(d, h;   )    (cid:88)

h

h

q(h) log q(h)

(19.31)

the second sum doesn   t depend on    at all, so it   s irrelevant for maximizing,

19.3 non-parametric mixture modeling

433

giving us back the optimization problem from the last section. this con   rms
that using the lower bound from jensen   s inequality hasn   t yielded a di   erent
algorithm! (exercise 19.10)

19.2.2 topic models and probabilistic lsa

mixture models over words provide an alternative to id45
(  16.4) for document analysis. instead of    nding the principal components of the
bag-of-words vectors, the idea is as follows. there are a certain number of topics
which documents in the corpus can be about; each topic corresponds to a distri-
bution over words. the distribution of words in a document is a mixture of the
topic distributions. that is, one can generate a bag of words by    rst picking a
topic according to a multinomial distribution (topic i occurs with id203   i),
and then picking a word from that topic   s distribution. the distribution of topics
varies from document to document, and this is what   s used, rather than projec-
tions on to the principal components, to summarize the document. this idea was,
so far as i can tell, introduced by hofmann (1999), who estimated everything by
em. id44, due to blei and collaborators (blei et al.,
2003) is an important variation which smoothes the topic distributions; there is a
cran package called lda. blei and la   erty (2009) is a good recent review paper
of the area.

19.3 non-parametric mixture modeling

we could replace the m step of em by some other way of estimating the distribu-
tion of each cluster. this could be a fast-but-crude estimate of parameters (say a
method-of-moments estimator if that   s simpler than the id113), or it could even
be a non-parametric density estimator of the type we talked about in chapter
14. (similarly for mixtures of regressions, etc.) issues of dimensionality re-surface
now, as well as convergence: because we   re not, in general, increasing j at each
step, it   s harder to be sure that the algorithm will in fact converge. this is an
active area of research.

19.4 worked computing example: snoqualmie falls revisited

19.4.1 mixture models in r

there are several r packages which implement mixture models. the mclust pack-
age (http://www.stat.washington.edu/mclust/) is pretty much standard for
gaussian mixtures. one of the more recent and powerful is mixtools (benaglia
et al., 2009), which, in addition to classic mixtures of parametric densities, han-
dles mixtures of regressions and some kinds of non-parametric mixtures. the
flexmix package (leisch, 2004) is (as the name implies) very good at    exibly
handling complicated situations, though you have to do some programming to
take advantage of this.

434

mixture models

19.4.2 fitting a mixture of gaussians to real data

let   s go back to the snoqualmie falls data set, last used in   11.79. there we built
a system to forecast whether there would be precipitation on day t, on the basis
of how much precipitation there was on day t     1. let   s look at the distribution
of the amount of precipitation on the wet days.

snoqualmie <- scan("http://www.stat.washington.edu/peter/book.data/set1",skip=1)
snoq <- snoqualmie[snoqualmie > 0]

figure 19.2 shows a histogram (with a fairly large number of bins), together
with a simple kernel density estimate. this suggests that the distribution is rather
skewed to the right, which is reinforced by the simple summary statistics:

summary(snoq)
##
##

min. 1st qu.
1.00
6.00

median
19.00

mean 3rd qu.
44.00

32.28

max.
463.00

notice that the mean is larger than the median, and that the distance from the
   rst quartile to the median is much smaller (13/100 of an inch of precipitation)
than that from the median to the third quartile (25/100 of an inch). one way
this could arise, of course, is if there are multiple types of wet days, each with a
di   erent characteristic distribution of precipitation.

we   ll look at this by trying to    t gaussian mixture models with varying num-
bers of clusters. we   ll start by using a mixture of two gaussians. we could code
up the em algorithm for    tting this mixture model from scratch, but instead
we   ll use the mixtools package.

library(mixtools)
snoq.k2 <- normalmixem(snoq,k=2,maxit=100,epsilon=0.01)

the em algorithm    runs until convergence   , i.e., until things change so little
that we don   t care any more. for the implementation in mixtools, this means
running until the log-likelihood changes by less than epsilon. the default toler-
ance for convergence is not 10   2, as here, but 10   8, which can take a very long
time indeed. the algorithm also stops if we go over a maximum number of iter-
ations, even if it has not converged, which by default is 1000; here i have dialed
it down to 100 for safety   s sake. what happens?

snoq.k2 <- normalmixem(snoq,k=2,maxit=100,epsilon=0.01)

summary(snoq.k2)
## summary of normalmixem object:
##
## lambda
## mu
## sigma
## loglik at estimate:

comp 2
0.442442
10.267203 60.012114
8.511208 44.997969

comp 1
0.557558

-32681.21

9 see that section for explanations of some of the data manipulation done in this section.

19.4 worked computating example

435

plot(hist(snoq,breaks=101),col="grey",border="grey",freq=false,

xlab="precipitation (1/100 inch)",main="precipitation in snoqualmie falls")

lines(density(snoq),lty="dashed")

figure 19.2 histogram (grey) for precipitation on wet days in snoqualmie
falls. the dashed line is a kernel density estimate, which is not completely
satisfactory. (it gives non-trivial id203 to negative precipitation, for
instance.)

there are two clusters, with weights (lambda) of about 0.56 and 0.44, two
means (mu) and two standard deviations (sigma). the over-all log-likelihood,
obtained after 59 iterations, is    3.2681206    104. (demanding convergence to
  10   8 would thus have required the log-likelihood to change by less than one
part in a trillion, which is quite excessive when we only have 6920 observations.)

precipitation in snoqualmie fallsprecipitation (1/100 inch)density01002003004000.000.010.020.030.040.05436

mixture models

we can plot this along with the histogram of the data and the non-parametric

density estimate. i   ll write a little function for it.

# plot the (scaled) density associated with a gaussian cluster
# inputs: mixture object (mixture)

# index number of the cluster (cluster.number)
# optional additional arguments to curve (...)

# outputs: none useful
# side-effects: plot is added to the current display
plot.gaussian.clusters <- function(mixture, cluster.number, ...) {

curve(mixture$lambda[cluster.number] *

dnorm(x,mean=mixture$mu[cluster.number],
sd=mixture$sigma[cluster.number]), add=true, ...)

}

this adds the density of a given cluster to the current plot, but scaled by
the share it has in the mixture, so that it is visually comparable to the over-all
density.

19.4.3 calibration-checking for the mixture

examining the two-cluster mixture, it does not look altogether satisfactory    
it seems to consistently give too much id203 to days with about 1 inch of
precipitation. let   s think about how we could check things like this.

when we looked at id28, we saw how to check id203 forecasts
by checking calibration     events predicted to happen with id203 p should
in fact happen with frequency     p. here we don   t have a binary event, but we
do have lots of probabilities. in particular, we have a cumulative distribution
function f (x), which tells us the id203 that the precipitation is     x on any
given day. when x is continuous and has a continuous distribution, f (x) should
be uniformly distributed.10 the cdf of a two-cluster mixture is

f (x) =   1f1(x) +   2f2(x)

(19.32)

and similarly for more clusters. a little r experimentation gives a function for
computing the cdf of a gaussian mixture:

pnormmix <- function(x,mixture) {

lambda <- mixture$lambda
k <- length(lambda)
pnorm.from.mix <- function(x,cluster) {

lambda[cluster]*pnorm(x,mean=mixture$mu[cluster],

sd=mixture$sigma[cluster])

}
pnorms <- sapply(1:k,pnorm.from.mix,x=x)
return(rowsums(pnorms))

}

we can use this to get a plot like figure 19.4. we do not have the tools to

10 we saw this principle when we looked at generating random variables in chapter 5.

19.4 worked computating example

437

plot(hist(snoq,breaks=101),col="grey",border="grey",freq=false,

xlab="precipitation (1/100 inch)",main="precipitation in snoqualmie falls")

lines(density(snoq),lty=2)
invisible(sapply(1:2,plot.gaussian.clusters,mixture=snoq.k2))

figure 19.3 as in the previous    gure, plus the clusters of a mixture of two
gaussians,    tted to the data by the em algorithm (dashed lines). these are
scaled by the mixing weights of the clusters. could you add the sum of the
two cluster densities to the plot?

assess whether the size of the departure from the main diagonal is signi   cant11,
but the fact that the errors are so very structured is rather suspicious.

11 though we could: the most straight-forward thing to do would be to simulate from the mixture, and

repeat this with simulation output.

precipitation in snoqualmie fallsprecipitation (1/100 inch)density01002003004000.000.010.020.030.040.05438

mixture models

distinct.snoq <- sort(unique(snoq))
tcdfs <- pnormmix(distinct.snoq,mixture=snoq.k2)
ecdfs <- ecdf(snoq)(distinct.snoq)
plot(tcdfs,ecdfs,xlab="theoretical cdf",ylab="empirical cdf",xlim=c(0,1),

ylim=c(0,1))

abline(0,1)

figure 19.4 calibration plot for the two-cluster gaussian mixture. for
each distinct value of precipitation x, we plot the fraction of days predicted
by the mixture model to have     x precipitation on the horizontal axis,
versus the actual fraction of days     x.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0theoretical cdfempirical cdf19.4 worked computating example

439

19.4.4 selecting the number of clusters by cross-validation

since a two-cluster mixture seems i   y, we could consider using more clusters. by
going to three, four, etc., clusters, we improve our in-sample likelihood, but of
course expose ourselves to the danger of over-   tting. some sort of model selection
is called for. we could do cross-validation, or we could do hypothesis testing. let   s
try cross-validation    rst.

we can already do    tting, but we need to calculate the log-likelihood on the

held-out data. as usual, let   s write a function; in fact, let   s write two.

# id203 density corresponding to a gaussian mixture model
# inputs: location for evaluating the pdf (x)

# mixture-model object (mixture)
# whether or not output should be logged (log)

# output: the (possibly logged) pdf at the point(s) x
dnormalmix <- function(x,mixture,log=false) {

lambda <- mixture$lambda
k <- length(lambda)
# calculate share of likelihood for all data for one cluster
like.cluster <- function(x,cluster) {

lambda[cluster]*dnorm(x,mean=mixture$mu[cluster],

sd=mixture$sigma[cluster])

}
# create array with likelihood shares from all clusters over all data
likes <- sapply(1:k,like.cluster,x=x)
# add up contributions from clusters
d <- rowsums(likes)
if (log) {

d <- log(d)

}
return(d)

}

# evaluate the loglikelihood of a mixture model at a vector of points
# inputs: vector of data points (x)

# mixture model object (mixture)

# output: sum of log id203 densities over the points in x
loglike.normalmix <- function(x,mixture) {

loglike <- dnormalmix(x,mixture,log=true)
return(sum(loglike))

}

to check that we haven   t made a big mistake in the coding:

loglike.normalmix(snoq,mixture=snoq.k2)
## [1] -32681.21

which matches the log-likelihood reported by summary(snoq.k2). but our func-

tion can be used on di   erent data!

we could do    ve-fold or ten-fold cv, but just to illustrate the approach we   ll
do simple data-set splitting, where a randomly-selected half of the data is used
to    t the model, and half to test.

n <- length(snoq)
data.points <- 1:n

440

mixture models

data.points <- sample(data.points) # permute randomly
train <- data.points[1:floor(n/2)] # first random half is training
test <- data.points[-(1:floor(n/2))] # 2nd random half is testing
candidate.cluster.numbers <- 2:10
loglikes <- vector(length=1+length(candidate.cluster.numbers))
# k=1 needs special handling
mu<-mean(snoq[train]) # id113 of mean
sigma <- sd(snoq[train])*sqrt((n-1)/n) # id113 of standard deviation
loglikes[1] <- sum(dnorm(snoq[test],mu,sigma,log=true))
for (k in candidate.cluster.numbers) {

mixture <- normalmixem(snoq[train],k=k,maxit=400,epsilon=1e-2)
loglikes[k] <- loglike.normalmix(snoq[test],mixture=mixture)

}

when you run this, you will may see a lot of warning messages saying    one
of the variances is going to zero; trying new starting values.    the issue is that
we can give any one value of x arbitrarily high likelihood by centering a gaus-
sian there and letting its variance shrink towards zero. this is however generally
considered unhelpful     it leads towards the pathologies that keep us from doing
pure id113 in non-parametric problems (chapter 14)
    so when that happens the code recognizes it and starts over.

if we look at the log-likelihoods, we see that there is a dramatic improvement

with the    rst few clusters, and then things slow down a lot12:

loglikes
##
##

[1] -17621.29 -16356.58 -15823.50 -15586.07 -15441.41 -15379.08 -15331.60
[8] -15319.54 -15315.34 -15305.32

(see also figure 19.5). this favors nine clusters to the mixture. it looks like
figure 19.6. the calibration is now nearly perfect, at least on the training data
(figure 19.7).

12 notice that the numbers here are about half of the log-likelihood we calculated for the two-cluster
mixture on the complete data. this is as it should be, because log-likelihood is proportional to the
number of observations. (why?) it   s more like the sum of squared errors than the mean squared
error. if we want something which is directly comparable across data sets of di   erent size, we should
use the log-likelihood per observation.

19.4 worked computating example

441

plot(x=1:10, y=loglikes,xlab="number of mixture clusters",

ylab="log-likelihood on testing data")

figure 19.5 log-likelihoods of di   erent sizes of mixture models,    t to a
random half of the data for training, and evaluated on the other half of the
data for testing.

llllllllll246810   17500   17000   16500   16000   15500number of mixture clusterslog   likelihood on testing data442

mixture models

snoq.k9 <- normalmixem(snoq,k=9,maxit=400,epsilon=1e-2)
plot(hist(snoq,breaks=101),col="grey",border="grey",freq=false,

xlab="precipitation (1/100 inch)",main="precipitation in snoqualmie falls")

lines(density(snoq),lty=2)
invisible(sapply(1:9,plot.gaussian.clusters,mixture=snoq.k9))

figure 19.6 as in figure 19.3, but using the nine-cluster gaussian mixture.

precipitation in snoqualmie fallsprecipitation (1/100 inch)density01002003004000.000.010.020.030.040.0519.4 worked computating example

443

distinct.snoq <- sort(unique(snoq))
tcdfs <- pnormmix(distinct.snoq,mixture=snoq.k9)
ecdfs <- ecdf(snoq)(distinct.snoq)
plot(tcdfs,ecdfs,xlab="theoretical cdf",ylab="empirical cdf",xlim=c(0,1),

ylim=c(0,1))

abline(0,1)

figure 19.7 calibration plot for the nine-cluster gaussian mixture.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0theoretical cdfempirical cdf444

mixture models

19.4.5 interpreting the clusters in the mixture, or not

the clusters of the mixture are far from arbitrary. it appears from figure 19.6
that as the mean increases, so does the variance. this impression is con   rmed
from figure 19.8. now it could be that there really are nine types of rainy days
in snoqualmie falls which just so happen to have this pattern of distributions,
but this seems a bit suspicious     as though the mixture is trying to use gaus-
sians systematically to approximate a fundamentally di   erent distribution, rather
than get at something which really is composed of nine distinct gaussians. this
judgment relies on our scienti   c understanding of the weather, which makes us
surprised by seeing a pattern like this in the parameters. (calling this    scienti   c
knowledge    is a bit excessive, but you get the idea.) of course we are sometimes
wrong about things like this, so it is certainly not conclusive. maybe there really
are nine types of days, each with a gaussian distribution, and some subtle me-
teorological reason why their means and variances should be linked like this. for
that matter, maybe our understanding of meteorology is wrong.

there are two directions to take this: the purely statistical one, and the sub-

stantive one.

on the purely statistical side, if all we care about is being able to describe the
distribution of the data and to predict future precipitation, then it doesn   t really
matter whether the nine-cluster gaussian mixture is true in any ultimate sense.
cross-validation picked nine clusters not because there really are nine types of
days, but because a nine-cluster model had the best trade-o    between approxi-
mation bias and estimation variance. the selected mixture gives a pretty good
account of itself, nearly the same as the kernel density estimate (figure 19.9). it
requires 26 parameters13, which may seem like a lot, but the kernel density es-
timate requires keeping around all 6920 data points plus a bandwidth. on sheer
economy, the mixture then has a lot to recommend it.

on the substantive side, there are various things we could do to check the idea
that wet days really do divide into nine types. these are going to be informed
by our background knowledge about the weather. one of the things we know, for
example, is that weather patterns more or less repeat in an annual cycle, and that
di   erent types of weather are more common in some parts of the year than in
others. if, for example, we consistently    nd type 6 days in august, that suggests
that is at least compatible with these being real, meteorological patterns, and
not just approximation artifacts.
let   s try to look into this visually. snoq.k9$posterior is a 6920    9 array
which gives the id203 for each day to belong to each class. i   ll boil this
down to assigning each day to its most probable class:

day.classes <- apply(snoq.k9$posterior,1,which.max)

we can   t just plot this and hope to see any useful patterns, because we want to
see stu    recurring every year, and we   ve stripped out the dry days, the division

13 a mean and a standard deviation for each of nine clusters (=18 parameters), plus mixing weights

(nine of them, but they have to add up to one).

19.4 worked computating example

445

plot(0,xlim=range(snoq.k9$mu),ylim=range(snoq.k9$sigma),type="n",

xlab="cluster mean", ylab="cluster standard deviation")

points(x=snoq.k9$mu,y=snoq.k9$sigma,pch=as.character(1:9),

cex=sqrt(0.5+5*snoq.k9$lambda))

figure 19.8 characteristics of the clusters of the 9-mode gaussian
mixture. the horizontal axis gives the cluster mean, the vertical axis its
standard deviation. the area of the number representing each cluster is
proportional to the cluster   s mixing weight.

into years, the padding to handle leap-days, etc. thus, we need to do a bit of r
magic. remember we started with a giant vector snoqualmie which had all days,
wet or dry; let   s copy that into a data frame, to which we   ll add the classes and
the days of the year.

050100150200020406080cluster meancluster standard deviation123456789446

mixture models

plot(density(snoq),lty=2,ylim=c(0,0.04),

main=paste("comparison of density estimates\n",

"kernel vs. gaussian mixture"),

xlab="precipitation (1/100 inch)")

curve(dnormalmix(x,snoq.k9),add=true)

figure 19.9 dashed line: kernel density estimate. solid line: the
nine-gaussian mixture. notice that the mixture, unlike the kde, gives
negligible id203 to negative precipitation.

snoqualmie.classes <- data.frame(precip=snoqualmie, class=0)
years <- 1948:1983
snoqualmie.classes$day <- rep(c(1:366,1:365,1:365,1:365),times=length(years)/4)
wet.days <- (snoqualmie > 0)
snoqualmie.classes$class[wet.days] <- day.classes

01002003004000.000.010.020.030.04comparison of density estimates kernel vs. gaussian mixtureprecipitation (1/100 inch)density19.4 worked computating example

447

now, it   s somewhat inconvenient that the index numbers of the clusters do
not really tell us about the mean amount of precipitation. let   s try replacing the
numerical labels in snoqualmie.classes by those means.

snoqualmie.classes$class[wet.days] <- snoq.k9$mu[day.classes]

this leaves alone dry days (still zero) and na days (still na). now we can plot

(figure 19.10).

the result is discouraging if we want to read any deeper meaning into the
classes. the class with the heaviest amounts of precipitation is most common in
the winter, but so is the classes with the second-heaviest amount of precipitation,
the etc. it looks like the weather changes smoothly, rather than really having
discrete classes. in this case, the mixture model seems to be merely a predictive
device, and not a revelation of hidden structure.14

14 a a distribution called a    type ii generalized pareto   , where p(x)     (1 + x/  )        1, provides a

decent    t here. (see shalizi 2007; arnold 1983 on this distribution and its estimation.) with only
two parameters, rather than 26, its log-likelihood is only 1% higher than that of the nine-cluster
mixture, and it is almost but not quite as calibrated. one origin of the type ii pareto is as a
mixture of exponentials (maguire et al., 1952). if x|z     exp(  /z), and z itself has a gamma
distribution, z       (  , 1), then the unconditional distribution of x is type ii pareto with scale    and
shape   . we might therefore investigate    tting a    nite mixture of exponentials, rather than of
gaussians, for the snoqualmie falls data. we might of course still end up concluding that there is a
continuum of di   erent sorts of days, rather than a    nite set of discrete types.

448

mixture models

plot(x=snoqualmie.classes$day, y=snoqualmie.classes$class,

xlim=c(1,366),ylim=range(snoq.k9$mu),xaxt="n",
xlab="day of year",ylab="expected precipiation (1/100 inch)",
pch=16,cex=0.2)

axis(1,at=1+(0:11)*30)

figure 19.10 plot of days classi   ed according to the nine-cluster mixture.
horizontal axis: day of the year, numbered from 1 to 366 (to handle
leap-years). vertical axis: expected amount of precipitation on that day,
according to the most probable class for the day.

19.4 worked computating example

449

19.4.6 hypothesis testing for mixture-model selection

an alternative to using cross-validation to select the number of mixtures is to
use hypothesis testing. the k-cluster gaussian mixture model is nested within
the (k + 1)-cluster model, so the latter must have a strictly higher likelihood on
the training data. if the data really comes from a k-cluster mixture (the null
hypothesis), then this extra increment of likelihood will follow one distribution,
but if the data come from a larger model (the alternative), the distribution will
be di   erent, and stochastically larger.

based on general likelihood theory, we might expect that the null distribution

is, for large sample sizes,

2(log lk+1     log lk)       2

dim(k+1)   dim(k)

(19.33)

where lk is the likelihood under the k-cluster mixture model, and dim(k) is
the number of parameters in that model. (see appendix i.) there are however
several reasons to distrust such an approximation, including the fact that we are
approximating the likelihood through the em algorithm. we can instead just    nd
the null distribution by simulating from the smaller model, which is to say we
can do a parametric bootstrap.

while it is not too hard to program this by hand (exercise 19.7), the mixtools
package contains a function to do this for us, called boot.comp, for    bootstrap
comparison   . let   s try it out (figure 19.11).

the command in the    gure tells boot.comp to consider mixtures of up to 10
clusters (just as we did with cross-validation), increasing the size of the mixture
it uses when the di   erence between k and k + 1 is signi   cant. (the default is
   signi   cant at the 5% level   , as assessed by 100 bootstrap replicates, but that   s
controllable.) the command also tells it what kind of mixture to use, and passes
along control settings to the em algorithm which does the    tting. each individual
   t is fairly time-consuming, and we are requiring 200 at each value of k. this took
about three minutes to run on my laptop.

this selected three clusters (rather than nine), and accompanied this decision
with a rather nice trio of histograms explaining why (figure 19.11). remember
that boot.comp stops expanding the model when there   s even a 5% chance of
that the apparent improvement could be due to mere over-   tting. this is actu-
ally pretty conservative, and so ends up with rather fewer clusters than cross-
validation.

let   s explore the output of boot.comp, conveniently stored in the object snoq.boot.

$ p.values
$ log.lik

str(snoq.boot)
## list of 3
##
##
##
##
##
##
##

: num [1:4] 0 0.01 0.04 0.34
:list of 4

..$ : num [1:100] 0.879 3.222 1.464 3.23 1.463 ...
..$ : num [1:100] 0.325 3.977 1.01 7.147 1.928 ...
..$ : num [1:100] 0.407 0.567 1.265 3.841 1.652 ...
..$ : num [1:100] 0.574 5.561 1227.473 2.319 1105.434 ...

$ obs.log.lik: num [1:4] 5096 2355 920 562

450

mixture models

snoq.boot <- boot.comp(snoq,max.comp=10,mix.type="normalmix",

maxit=400,epsilon=1e-2)

figure 19.11 histograms produced by boot.comp(). the vertical red lines
mark the observed di   erence in log-likelihoods.

this tells us that snoq.boot is a list with three elements, called p.values,
log.lik and obs.log.lik, and tells us a bit about each of them. p.values con-
tains the p-values for testing h1 (one cluster) against h2 (two clusters), testing
h2 against h3, and h3 against h4. since we set a threshold p-value of 0.05, it
stopped at the last test, accepting h3. (under these circumstances, if the di   er-
ence between k = 3 and k = 4 was really important to us, it would probably
be wise to increase the number of bootstrap replicates, to get more accurate
p-values.) log.lik is itself a list containing the bootstrapped log-likelihood ra-

1 versus 2 componentsbootstrap likelihoodratio statisticfrequency0510150102030402 versus 3 componentsbootstrap likelihoodratio statisticfrequency01000300050000204060801003 versus 4 componentsbootstrap likelihoodratio statisticfrequency0500150025000204060804 versus 5 componentsbootstrap likelihoodratio statisticfrequency020060010001400010203040506019.4 worked computating example

451

tios for the three hypothesis tests; obs.log.lik is the vector of corresponding
observed values of the test statistic.

looking back to figure 19.5, there is indeed a dramatic improvement in the
generalization ability of the model going from one cluster to two, and from two
to three, and diminishing returns to complexity thereafter. stopping at k = 3
produces pretty reasonable results, though repeating the exercise of figure 19.10
is no more encouraging for the reality of the latent classes.

452

mixture models

19.5 further reading

my presentation of the em algorithm draws heavily on neal and hinton (1998).
because it   s so general, the em algorithm is applied to lots of problems with
missing data or latent variables. traditional estimation methods for factor analy-
sis, for example, can be replaced with em. (arguably, some of the older methods
were versions of em.) a common problem in time-series analysis and signal pro-
cessing is that of       ltering    or    state estimation   : there   s an unknown signal st,
which we want to know, but all we get to observe is some noisy, corrupted mea-
surement, xt = h(st) +   t. (a historically important example of a    state    to be
estimated from noisy measurements is    where is our rocket and which way is
it headed?        see mcgee and schmidt, 1985.) this is solved by the em algo-
rithm, with the signal as the hidden variable; fraser (2008) gives a really good
introduction to such models and how they use em.

instead of just doing mixtures of densities, one can also do mixtures of pre-
dictive models, say mixtures of regressions, or mixtures of classi   ers. the hidden
variable z here controls which regression function to use. a general form of this
is what   s known as a mixture-of-experts model (jordan and jacobs, 1994; ja-
cobs, 1997)     each predictive model is an    expert   , and there can be a quite
complicated set of hidden variables determining which expert to use when.

the em algorithm is so useful and general that it has in fact been re-invented
multiple times. the name    em algorithm    comes from the statistics of mixture
models in the late 1970s; in the time series literature it   s been known since the
1960s as the    baum-welch    algorithm.

exercises

19.1 write a function to simulate from a gaussian mixture model. check that it works by

comparing a density estimated on its output to the theoretical density.

19.2 show that the mixture of a binom(2, 0.75) and a binom(2, 0.25) is not a binom(2, p) for

19.3 following   19.2, suppose that we want to estimate the   j by maximizing the likelihood.

any p

1. show that

2. explain why we need to add a lagrange multiplier to enforce the constraint(cid:80)k
3. show that, including the lagrange multiplier, the optimal value of   j is(cid:80)n

1, and why it was ok to ignore that in eq. 19.10.

i=1

j=1   j =

i=1 wij /n.

(19.34)

can you    nd a simple expression for the lagrange multiplier?

19.4 work through the e- step and m- step for a mixture of two poisson distributions.
19.5 code up the em algorithm for a mixture of k gaussians. simulate data from k = 3
gaussians. how well does your code assign data-points to clusters if you give it the actual
gaussian parameters as your initial guess? if you give it other initial parameters?

19.6 prove eq. 19.18.

n(cid:88)

   (cid:96)
     j

=

wij

exercises

453

19.7 write a function to    nd the distribution of the log-likelihood ratio for testing the hypoth-
esis that the mixture has k gaussian clusters against the alternative that it has k + 1, by
simulating from the k-cluster model. compare the output to the boot.comp function in
mixtools.

19.8 write a function to    t a mixture of exponential distributions using the em algorithm.

does it do any better at discovering sensible structure in the snoqualmie falls data?

19.9 explain how to use relative distribution plots (chapter 15) to check calibration, along the

lines of figure 19.4.

19.10 abstract vs. concrete the abstract em algorithm of   19.2.1 is very general, much more
general than the concrete algorithm given on the previous pages. nonetheless, the former
reduces to the latter when the latent variable z follows a multinomial distribution.

1. show that the m step of the abstract em algorithm is equivalent to solving

n(cid:88)

wij

    log f (xi;   j )

     j

= 0

(19.35)

for the new   .

i=1

2. show that the maximization in the e step of the abstract em algorithm yields eq.

19.13.

20

id114

we have spent a lot of time looking at ways of    guring out how one variable (or
set of variables) depends on another variable (or set of variables)     this is the
core idea in regression and in conditional density estimation. we have also looked
at how to estimate the joint distribution of variables, both with kernel density
estimation and with models like factor and mixture models. the later two show
an example of how to get the joint distribution by combining a conditional distri-
bution (observables given factors; mixture components) with a marginal distri-
bution (gaussian distribution of factors; the component weights). when dealing
with complex sets of dependent variables, it would be nice to have a general way
of composing conditional distributions together to get joint distributions, and
especially nice if this gave us a way of reasoning about what we could ignore,
of seeing which variables are irrelevant to which other variables. this is what
id114 let us do.

20.1 conditional independence and factor models

the easiest way into this may be to start with the diagrams we drew for factor
analysis. there, we had observables and we had factors, and each observable
depended on, or loaded on, some of the factors. we drew a diagram where we
had nodes, standing for the variables, and arrows running from the factors to the
observables which depended on them. in the factor model, all the observables
were conditionally independent of each other, given all the factors:
p(xi|f1, . . . fq)

p(x1, x2, . . . xp|f1, f2, . . . fq) =

p(cid:89)

(20.1)

i=1

but in fact observables are also independent of the factors they do not load on,
so this is still too complicated. let   s write loads(i) for the set of factors on which
the observable xi loads. then

p(x1, x2, . . . xp|f1, f2, . . . fq) =

p(xi|floads(i))

(20.2)

consider figure 20.1. the conditional distribution of observables given factors

is

p(x1, x2, x3, x4|f1, f2) = p(x1|f1, f2)p(x2|f1, f2)p(x3|f1)p(x4|f 2)

(20.3)

i=1

454

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

p(cid:89)

20.2 directed acyclic graph (dag) models

455

f2

f1

x4

x1

x2

x3

figure 20.1 illustration of a typical model with two latent factors (f1 and
f2, in circles) and four observables (x1 through x4).

x1 loads on f1 and f2, so it is independent of everything else, given those two
variables. x1 is unconditionally dependent on x2, because they load on common
factors, f1 and f2; and x1 and x3 are also dependent, because they both load
on f1. in fact, x1 and x2 are still dependent given f1, because x2 still gives in-
formation about f2. but x1 and x3 are independent given f1, because they have
no other factors in common. finally, x3 and x4 are unconditionally independent
because they have no factors in common. but they become dependent given x1,
which provides information about both the common factors.

none of these assertions rely on the detailed assumptions of the factor model,
like gaussian distributions for the factors, or linear dependence between factors
and observables. what they rely on is that xi is independent of everything else,
given the factors it loads on. the idea of id114 is to generalize this,
by focusing on relations of direct dependence, and the conditional independence
relations implied by them.

20.2 directed acyclic graph (dag) models

we have a collection of variables, which to be generic i   ll write x1, x2, . . . xp.
these may be discrete, continuous, or even vectors; it doesn   t matter. we rep-
resent these visually as nodes in a graph. there are arrows connecting some of
these nodes. if an arrow runs from xi to xj, then xi is a parent of xj. this is,
as the name    parent    suggests, an anti-symmetric relationship, i.e., xj cannot
also be the parent of xi. this is why we use an arrow, and why the graph is
directed1. we write the set of all parents of xj as parents(j); this generalizes
the notion of the factors which an observable loads on to. the joint distribution
   decomposes according to the graph   :

p(x1, x2, . . . xp) =

p(xi|xparents(i))

(20.4)

if xi has no parents, because it has no incoming arrows, take p(xi|xparents(i))
just to be the marginal distribution p(xi). such variables are called exogenous;

i=1

1 see appendix j for a brief review of the ideas and jargon of id207.

p(cid:89)

456

id114

the others, with parents, are endogenous. an unfortunate situation could arise
where x1 is the parent of x2, which is the parent of x3, which is the parent of
x1. perhaps, under some circumstances, we could make sense of this and actually
calculate with eq. 20.4, but the general practice is to rule it out by assuming the
graph is acyclic, i.e., that it has no cycles, i.e., that we cannot, by following a
series of arrows in the graph, go from one node to other nodes and ultimately
back to our starting point. altogether we say that we have a directed acyclic
graph, or dag, which represents the direct dependencies between variables.2

what good is this? the primary virtue is that if we are dealing with a dag
model, the graph tells us all the dependencies we need to know; those are the
conditional distributions of variables on their parents, appearing in the product
on the right hand side of eq. 20.4. (this includes the distribution of the exoge-
neous variables.) this fact has two powerful sets of implications, for probabilistic
reasoning and for statistical id136.
let   s take id136    rst, because it   s more obvious: all that we have to estimate
are the conditional distributions p(xi|xparents(i)). we do not have to estimate the
distribution of xi given all of the other variables, unless of course they are all
parents of xi. since estimating distributions, or even just regressions, conditional
on many variables is hard, it is extremely helpful to be able to read o    from the
graph which variables we can ignore. indeed, if the graph tells us that xi is
exogeneous, we don   t have to estimate it conditional on anything, we just have
to estimate its marginal distribution.

20.2.1 conditional independence and the markov property

the probabilistic implication of eq. 20.4 is perhaps even more important, and
that has to do with conditional independence. pick any two variables xi and xj,
where xj is not a parent of xi. consider the distribution of xi conditional on
its parents and xj. there are two possibilities. (i) xj is not a descendant of xi.
then we can see that xi and xj are conditionally independent. this is true no
matter what the actual conditional distribution functions involved are; it   s just
implied by the joint distribution respecting the graph. (ii) alternatively, xj is
a descendant of xi. then in general they are not independent, even conditional
on the parents of xi. so the graph implies that certain conditional independence
relations will hold, but that others in general will not hold.

as you know from your id203 courses, a sequence of random variables
x1, x2, x3, . . . forms a markov process3 when    the past is independent of the
future given the present   : that is,

xt+1        (xt   1, xt   2, . . . x1)|xt

(20.5)

2 see   20.6 for remarks on undirected id114, and graphs with cycles.
3 after the russian mathematician a. a. markov, who introduced the theory of markov processes in
the course of a mathematical dispute with his arch-nemesis, to show that id203 and statistics
could apply to dependent events, and hence that christianity was not necessarily true (i am not
making this up: basharin et al., 2004).

20.3 conditional independence and d-separation

. . .

xt   1

xt

xt+1

457

. . .

figure 20.2 dag for a discrete-time markov process. at each time t, xt is
the child of xt   1 alone, and in turn the sole parent of xt+1.

from which it follows that

(xt+1, xt+2, xt+3, . . .)        (xt   1, xt   2, . . . x1)|xt

(20.6)

which is called the markov property. dag models have a similar property: if
we take any collection of nodes i, it is independent of its non-descendants, given
its parents:

xi        xnon   descendants(i)|xparents(i)

(20.7)

this is the directed graph markov property. the ordinary markov property
is a special case, when the graph looks like figure 20.24.

on the other hand, if we condition on one of xi   s children, xi will generally be
dependent on any other parent of that child. if we condition on multiple children
of xi, we   ll generally    nd xi is dependent on all its co-parents. it should be
plausible, and is in fact true, that xi is independent of everything else in the
graph if we condition on its parents, its children, and its children   s other parents.
this set of nodes is called xi   s markov blanket.

20.3 conditional independence and d-separation

it is clearly very important to us to be able to deduce when two sets of variables
are conditionally independent of each other given a third. one of the great uses of
dags is that they give us a fairly simple criterion for this, in terms of the graph
itself. all distributions which conform to a given dag share a common set of
conditional independence relations, implied by the markov property, no matter
what their parameters or the form of the distributions.

our starting point is that when we have a single directed edge, we can reason
from the parent to the child, or from the child to the parent. while (as we   ll see
in part iii) it   s reasonable to say that in   uence or causation    ows one way, along
the direction of the arrows, statistical information can    ow in either direction.
since dependence is the presence of such statistical information, if we want to
   gure out which variables are dependent on which, we need to keep track of these
information    ows.

while we can do id136 in either direction across any one edge, we do have
to worry about whether we can propagate this information further. consider the
four graphs in figure 20.3. in every case, we condition on x, which acts as the
source of information. in the    rst three cases, we can (in general) propagate

4 to see this, take the    future    nodes, indexed by t + 1 and up, as the set i. their parent consists just

of xt, and all their non-descendants are the even earlier nodes at times t     1, t     2, etc.

458

id114

a

c

b

d

figure 20.3 four dags for three linked variables. the    rst two (a and b)
are called chains; c is a fork; d is a collider. if these were the whole of the
graph, we would have x (cid:54)       y and x        y |z. for the collider, however, we
would have x        y while x (cid:54)       y |z.

the information from x to z to y     the markov property tells us that y is
independent of its non-descendants given its parents, but in none of those cases
does that make x and y independent. in the last graph, however, what   s called a
collider5, we cannot propagate the information, because y has no parents, and
x is not its descendant, hence they are independent. we learn about z from x,
but this doesn   t tell us anything about z   s other cause, y .

all of this    ips around when we condition on the intermediate variable (z in
figure 20.3). the chains (figures 20.3a and b), conditioning on the intermediate
variable blocks the    ow of information from x to y     we learn nothing more
about y from x and z than from z alone, at least not along this path. this is
also true of the fork (figure 20.3c)     conditional on their common cause, the
two e   ects are uninformative about each other. but in a collider, conditioning
on the common e   ect z makes x and y dependent on each other, as we   ve seen
before. in fact, if we don   t condition on z, but do condition on a descendant of
z, we also create dependence between z   s parents.

we are now in a position to work out conditional independence relations. we
pick our two favorite variables, x and y , and condition them both on some third
set of variables s. if s blocks every undirected path6 from x to y , then they
must be conditionally independent given s. an unblocked path is also called
active. a path is active when every variable along the path is active; if even one
variable is blocked by s, the whole path is blocked. a variable z along a path is
active, conditioning on s, if

1. z is a collider along the path, and in s; or,
2. z is a descendant of a collider, and in s; or
3. z is not a collider, and not in s.

turned around, z is blocked or de-activated by conditioning on s if

5 because two incoming arrows    collide    there.
6 whenever i talk about undirected paths, i mean paths without cycles.

xzyzxyzxyxzy20.3 conditional independence and d-separation

459

1. z is a non-collider and in s; or
2. z is collider, and neither z nor any of its descendants is in s

in words, s blocks a path when it blocks the    ow of information by condi-
tioning on the middle node in a chain or fork, and doesn   t create dependence by
conditioning on the middle node in a collider (or the descendant of a collider).
only one node in a path must be blocked to block the whole path. when s
blocks all the paths between x and y , we say it d-separates them7. a collec-
tion of variables u is d-separated from another collection v by s if every x     u
and y     v are d-separated.

in every distribution which obeys the markov property, d-separation implies
conditional independence8. it is not always the case that the reverse implication,
the one from conditional independence to d-separation, holds good. we will see
in part iii, that when the distribution is    faithful    to a dag, causal id136 is
immensely simpli   ed. but going from d-separation to conditional independence
is true in any dag, whether or not it has a causal interpretation.

20.3.1 d-separation illustrated

the discussion of d-separation has been rather abstract, and perhaps confusing
for that reason. figure 20.4 shows a dag which might make this clearer and
more concrete.

if we make the conditioning set s the empty set, that is, we condition on
nothing, we    block    paths which pass through colliders. for instance, there are
three exogenous variables in the graph, x2, x3 and x5. because they have no
parents, any path from one to another must go over a collider (exercises 20.1 and
20.2). if we do not condition on anything, therefore, we    nd that the exogenous
variables are d-separated and thus independent. since x3 is not on any path
linking x2 and x5, or descended from a node on any such path, if we condition
only on x3, then x2 and x5 are still d-separated, so x2        x5|x3. there are two
paths linking x3 to x5: x3     x1     x2     x4     x5, and x3     x1     y     x5.
conditioning on x2 (and nothing else) blocks the    rst path (since x2 is part of
it, but is a fork), and also blocks the second path (since x2 is not part of it, and
y is a blocked collider). thus, x3        x5|x2. similarly, x3        x2|x5 (exercise
20.4).
for a somewhat more challenging example, let   s look at the relation between
x3 and y . there are, again, two paths here: x3     x1     y , and x3     x1    
x2     x4     x5     y . if we condition on nothing, the    rst path, which is a simple
chain, is open, so x3 and y are d-connected and dependent. if we condition on
x1, we block the    rst path. x1 is a collider on the second path, so conditioning
on x1 opens the path there. however, there is a second collider, x4, along this
path, and just conditioning on x1 does not activate the second collider, so the

7 the    d    stands for    directed   
8 we will not prove this, though i hope i have made it plausible. you can    nd demonstrations in

spirtes et al. (2001); pearl (2000); lauritzen (1996).

460

id114

x5

x2

x4

x3

x6

x1

y

figure 20.4 example dag used to illustrate d-separation.

path as a whole remains blocked.

y (cid:54)       x3
y        x3|x1

(20.8)
(20.9)

to activate the second path, we can condition on x1 and either x4 (a collider

along that path) or on x6 (a descendant of a collider) or on both:

(20.10)
(20.11)
(20.12)
conditioning on x4 and/or x6 does not activate the x3     x1     y path, but
it   s enough for there to be one active path to create dependence.

y (cid:54)       x3|x1, x4
y (cid:54)       x3|x1, x6
y (cid:54)       x3|x1, x4, x6

to block the second path again, after having opened it in one of these ways,
we can condition on x2 (since it is a fork along that path, and conditioning on a
fork blocks it), or on x5 (also a fork), or on both x2 and x5. so

y        x3|x1, x2
y        x3|x1, x5
y        x3|x1, x2, x5
y        x3|x1, x2, x4
y        x3|x1, x2, x6
y        x3|x1, x2, x5, x6

(20.13)
(20.14)
(20.15)
(20.16)
(20.17)
(20.18)

20.3 conditional independence and d-separation

461

etc., etc.

let   s look at the relationship between x4 and y . x4 is not an ancestor of
y , or a descendant of it, but they do share common ancestors, x5 and x2.
unconditionally, y and x4 are dependent, both through the path going x4    
x5     y , and through that going x4     x2     x1     y . along both paths,
the exogenous variables are forks, so not conditioning on them leaves the path
unblocked. x4 and y become d-separated when we condition on x5 and x2.
x6 and x3 have no common ancestors. unconditionally, they should be inde-
pendent, and indeed they are: the two paths are x6     x4     x2     x1     x3,
and x6     x4     x5     y     x1     x3. both paths contain a single collider (x1
and y , respectively), so if we do not condition on them the paths are blocked and
x6 and x3 are independent. if we condition on either y or x1 (or both), however,
we unblock the paths, and x6 and x3 become d-connected, hence dependent. to
get back to d-separation while conditioning on y , we must also condition on x4
or x5, or both. to get d-separation while conditioning on x1, we must also con-
dition on x4, or on x2, or on x4 and x2. if we condition on both x1 and y and
want d-separation, we could just add conditioning on x4, or we could condition
on x2 and x5, or all three.

if the abstract variables are insu   ciently concrete, consider reading them as

follows:

y     grade in this class
x1     e   ort spent on this class
x2     enjoyment of statistics
x3     workload this term
x4     quality of work in id75 class
x5     amount learned in id75 class
x6     grade in id75

pretending, for the sake of illustration, that this is accurate, how heavy your
workload is this semester (x3) would predict, or rather retrodict, your grade in
id75 last semester (x6), once we control for how much e   ort you put
into this class (x1). changing your workload this semester would not, however,
reach backwards in time to raise or lower your grade in regression.

20.3.2 linear id114 and path coe   cients

we began our discussion of id114 with factor analysis as our starting
point. factor models are a special case of linear (directed) id114, a.k.a.
path models9 as with factor models, in the larger class we typically center all the
variables (so they have expectation zero) and scale them (so they have variance
1). in factor models, the variables were split into two sets, the factors and the

9 some people use the phrase    structural equation models    for linear directed id114

exclusively.

462

id114

observables, and all the arrows went from factors to observables. in the more
general case, we do not necessarily have this distinction, but we still assume the
arrows from a directed acyclic graph. the conditional expectation of each variable
is a linear combination of the values of its parents:

e(cid:2)xi | xparents(i)

(cid:3) =

(cid:88)

j   parents(i)

wjixj

(20.19)

just as in a factor model. in a factor model, the coe   cients wji were the factor
loadings. more generally, they are called path coe   cients.

the path coe   cients determine all of the correlations between variables in the
model. if all of the variables have been standardized to mean zero and variance 1,
and the path coe   cients are calculated for these standardized variables, we can
   nd the correlation between xi and xj as follows:
    find all of the undirected paths between xi and xj.
    discard all of the paths which go through colliders.
    for each remaining path, multiply all the path coe   cients along the path.
    sum up these products over paths.
these rules were introduced by the great geneticist and mathematical biologist
sewall wright in the early 20th century (see further reading for details). these
   wright path rules    often seem mysterious, particularly the bit where paths with
colliders are thrown out. but from our perspective, we can see that what wright
is doing is    nding all of the unblocked paths between xi and xj. each path is
a channel along which information (here, correlation) can    ow, and so we add
across channels.

it is frequent, and customary, to assume that all of the variables are gaussian.
(we saw this in factor models as well.) with this extra assumption, the joint
distribution of all the variables is a multivariate gaussian, and the correlation
matrix (which we    nd from the path coe   cients) gives us the joint distribution.
if we want to    nd correlations conditional on a set of variables s, corr(xi, xj|s),
we still sum up over the unblocked paths. if we have avoided conditioning on col-
liders, then this is just a matter of dropping the now-blocked paths from the sum.
if on the other hand we have conditioned on a collider, that path does become
active (unless blocked elsewhere), and we in fact need to modify the path weights.
speci   cally, we need to work out the correlation induced between the two par-
ents of the collider, by conditioning on that collider. this can be calculated from
the path weights, and some fairly tedious algebra10. the important thing is to
remember that the rule of d-separation still applies, and that conditioning on a
collider can create correlations.

path coe   cients and covariances

if the variables have not all been standardized, but eq. 20.19 still applies, it is
often desirable to calculate covariances, rather than correlation coe   cients. this

10 see for instance li et al. (1975).

20.3 conditional independence and d-separation

463

involves a little bit of extra work, by way of keeping track of variances, and in
particular the variances of    source    terms. since many references do not state
the path-tracing rules for covariances, it   s worth going over them here.

to    nd the marginal covariance between xi and xj, the procedure is as follows:

1. find all of the unblocked paths between xi and xj (i.e., discard all paths

which go through colliders).

2. for each remaining path:

1. multiply all the path coe   cients along the path;
2.    nd the node along that path which is the ancestor of all the other nodes

along that path11, and call it the path   s source;

3. multiply the product of the coe   cients by the variance of the source.

3. sum the product of path coe   cients and source variances over all remaining

paths.

(notice that if all variables are standardized to variance 1, we don   t have to worry
about source variances, and these rules reduce to the previous ones.)

to    nd the conditional covariance between xi and xj given a set of variables s,
there are two procedures, depending on whether or not conditioning on s opens
any paths between xi and xj by including colliders. if s does not contain any
colliders or descendants of colliders (on paths between xi and xj),

1. for each unblocked path linking xi and xj:

1. multiply all the path coe   cients along the path;
2.    nd the source of each path12;
3. multiply the product of the coe   cients by the variance of the source.

2. sum the product of path coe   cients and source variances over all remaining

paths.

if, on the other hand, conditioning on s opens paths by conditioning on col-
liders (or their descendants), then we would have to handle the consequences of
conditioning on a collider. this is usually too much of a pain to do graphically,
and one should fall back on algebra. the next sub-section does however say a bit [[todo:
about what qualitatively happens to the correlations.

20.3.3 positive and negative associations

we say that variables x and y are positively associated if increasing x pre-
dicts, on average, an increase in y , and vice versa13; if increasing x predicts a de-
crease in y , then they are negatively associated. if this holds when condition-
ing out other variables, we talk about positive and negative partial associations.
heuristically, positive association means positive correlation in the neighborhood

11 showing that such an ancestor exists is exercise 21.
12 showing that the source of an unblocked, collider-free path cannot be in s is exercise 22.
13 i.e., if de[y |x=x]

    0

dx

   nal

in
revision,
write
out
full graph-
rules
ical
for
com-
pleteness]]

464

id114

of any given x, though the magnitude of the positive correlation need not be
constant. note that not all dependent variables have to have a de   nite sign for
their association.

we can multiply together the signs of positive and negative partial associations
along a path in a graphical model, the same we can multiply together path
coe   cients in a linear graphical model. paths which contain (inactive!) colliders
should be neglected. if all the paths connecting x and y have the same sign,
then we know that over-all association between x and y must have that sign. if
di   erent paths have di   erent signs, however, then signs alone are not enough to
tell us about the over-all association.

if we are interested in conditional associations, we have to consider whether our
conditioning variables block paths or not. paths which are blocked by conditioning
should be dropped from consideration. if a path contains an activated collider,
we need to include it, but we reverse the sign of one arrow into the collider.
that is, if x +    z +    y , and we condition on z, we need to replace one of the
plus signs with a     sign, because the two parents now have an over-all negative
association.14 if on the other hand one of the incoming arrows had a positive
association and the other was negative, we need to    ip one of them so they are
both positive or both negative; it doesn   t matter which, since it creates a positive
association between the parents15.

20.4 independence, conditional independence, and information

theory

out

[[todo:
write
formal
proofs
as
appendix]]

to

[[todo:
move
planned
appendix
on
mation
theory]]

infor-

take two random variables, x and y . they have some joint distribution, which
we can write p(x, y). (if they are both discrete, this is the joint id203 mass
function; if they are both continuous, this is the joint id203 density function;
if one is discrete and the other is continuous, there   s still a distribution, but it
needs more advanced tools.) x and y each have marginal distributions as well,
p(x) and p(y). x        y if and only if the joint distribution is the product of the
marginals:

x        y     p(x, y) = p(x)p(y)

(20.20)

we can use this observation to measure how dependent x and y are. let   s start
with the log-likelihood ratio between the joint distribution and the product of
marginals:

log

p(x, y)
p(x)p(y)

(20.21)

14 if both smoking and asbestos are positively associated with lung cancer, and we know the patient

does not have lung cancer, then high levels of smoking must be compensated for by low levels of
asbestos, and vice versa.

15 if yellow teeth are positively associated with smoking and negatively associated with dental

insurance, and we know the patient does not have yellow teeth, then high levels of smoking must be
compensated for by excellent dental care, and conversely poor dental care must be compensated for
by low levels of smoking.

465
this will always be exactly 0 when x        y . we use its average value as our
measure of dependence:

20.4 independence and information

i[x; y ]    (cid:88)

x,y

p(x, y) log

p(x, y)
p(x)p(y)

(20.22)

(if the variables are continuous, replace the sum with an integral.) clearly, if
x        y , then i[x; y ] = 0. one can show16 that i[x; y ]     0, and that i[x; y ] = 0
implies x        y . the quantity i[x; y ] is clearly symmetric between x and y .
less obviously, i[x; y ] = i[f (x); g(y )] whenever f and g are invertible func-
tions. this coordinate-freedom means that i[x; y ] measures all forms of de-
pendence, not just linear relationships, like the ordinary (pearson) correlation
coe   cient, or monotone dependence, like the rank (spearman) correlation co-
e   cient. in id205, i[x; y ] is called the mutual information, or
shannon information, between x and y . so we have the very natural state-
ment that random variables are independent just when they have no information
about each other.

there are (at least) two ways of giving an operational meaning to i[x; y ]. one,
the original use of the notion, has to do with using knowledge of y to improve
the e   ciency with which x can be encoded into bits (shannon, 1948; cover
and thomas, 2006). while this is very important     it   s literally transformed the
world since 1945     it   s not very statistical. for statisticians, what matters is that
if we test the hypothesis that x and y are independent, with joint distribution
p(x)p(y), against the hypothesis that they dependent, with joint distribution
p(x, y), then the mutual information controls the error probabilities of the test.
to be exact, if we    x any power we like (90%, 95%, 99.9%, . . . ), the size or type
i error rate   n, of the best possible test shrinks exponentially with the number of
iid samples n, and the rate of exponential decay is precisely i[x; y ] (kullback,
1968,   4.3, theorem 4.3.2):

n          1

lim

n

log   n     i[x; y ]

(20.23)

so positive mutual information means dependence, and the magnitude of mutual
information tells us about how detectable the dependence is17.

suppose we conditioned x and y on a third variable (or variables) z. for each

realization z, we can calculate the mutual information,

i[x; y |z = z]    (cid:88)

x,y

p(x, y|z) log

p(x, y|z)
p(x|z)p(y|z)

(20.24)

16 using the same type of convexity argument (   jensen   s inequality   ) we used   19.2.1 for

(cid:80)

17 symmetrically, if we follow the somewhat more usual procedure of    xing a type i error rate   , the

understanding why the em algorithm works.
type ii error rate   n (= 1   power) also goes to zero exponentially, and the exponential rate is

x,y p(x)p(y) log p(x)p(y)

p(x,y) , a quantity called the    lautam information    (palomar and verd  u, 2008).
(for proofs of the exponential rate, see palomar and verd  u (2008, p. 965), following kullback (1968,
  4.3, theorem 4.3.3).)

466

id114

z

x

figure 20.5 dag for a mixture model. the latent class z is exogenous,
and the parent of the observable random vector x. (if the components of x
are conditionally independent given z, they could be represented as separate
boxes on the lower level.

and we can average over z,

i[x; y |z]    (cid:88)

z

p(z)i[x; y |z = z]

(20.25)

this is the conditional mutual information. it will not surprise you at this
point to learn that x        y |z if and only if i[x; y |z] = 0. the magnitude of
the conditional mutual information tells us how easy it is to detect conditional
dependence.

20.5 examples of dag models and their uses

factor models are examples of dag models (as we   ve seen). so are mixture mod-
els (figure 20.5) and markov chains (see above). dag models are considerably
more    exible, however, and can combine observed and unobserved variables in
many ways.

consider, for instance, figure 20.6. here there are two exogeneous variables,
labeled    smoking    and    asbestos   . everything else is endogenous. notice that
   yellow teeth    is a child of    smoking    alone. this does not mean that (in the
model) whether someone   s teeth get yellowed (and, if so, how much) is a function
of smoking alone; it means that whatever other in   uences go into that are inde-
pendent of the rest of the model, and so unsystematic that we can think about
those in   uences, taken together, as noise.

continuing, the idea is that how much someone smokes in   uences how yellow
their teeth become, and also how much tar builds up in their lungs. tar in the
lungs, in turn, leads to cancer, as does by exposure to asbestos.

now notice that, in this model, teeth-yellowing will be unconditionally depen-
dent on, i.e., associated with, the level of tar in the lungs, because they share
a common parent, namely smoking. yellow teeth and tarry lungs will however
be conditionally independent given that parent, so if we control for smoking we
should not be able to predict the state of someone   s teeth from the state of their
lungs or vice versa.

on the other hand, smoking and exposure to asbestos are independent, at least

20.5 examples of dag models and their uses

467

smoking

yellow teeth

tar in lungs

asbestos

figure 20.6 dag model indicating (hypothetical) relationships between
smoking, asbestos, cancer, and covariates.

cancer

in this model, as they are both exogenous18. conditional on whether someone has
cancer, however, smoking and asbestos will become dependent.

to understand the logic of this, suppose (what is in fact true) that both how
much someone smokes and how much they are exposed to asbestos raises the risk
of cancer. conditional on not having cancer, then, one was probably exposed to
little of either tobacco smoke or asbestos. conditional on both not having cancer
and having been exposed to a high level of asbestos, one probably was exposed to
an unusually low level of tobacco smoke. vice versa, no cancer plus high levels of
tobacco tend to imply especially little exposure to asbestos. we thus have created
a negative association between smoking and asbestos by conditioning on cancer.
naively, a regression where we    controlled for    cancer would in fact tell us that
exposure to asbestos keeps tar from building up in the lungs, prevents smoking,
and whitens teeth.

more generally, conditioning on a third variable can create dependence be-
tween otherwise independent variables, when what we are conditioning on is a
common descendant of the variables in question.19 this conditional dependence
is not some kind of    nite-sample artifact or error     it   s really there in the joint
id203 distribution. if all we care about is prediction, then it is perfectly
legitimate to use it. in the world of figure 20.6, it really is true that you can pre-
dict the color of someone   s teeth from whether they have cancer and how much

18 if we had two variables which in some physical sense were exogenous but dependent on each other,
we would represent them in a dag model by either a single vector-valued random variable (which
would get only one node), or as children of a latent unobserved variable, which was truly exogenous.

19 economists, psychologists, and other non-statisticians often repeat the advice that if you want to

know the e   ect of x on y , you should not condition on z when z is endogenous. this is bit of
folklore is a relic of the days of ignorance, when our ancestors groped towards truths they could not
grasp. if we want to know whether asbestos is associated with tar in the lungs, conditioning on the
yellowness of teeth is    ne, even though that is an endogenous variable.

468

id114

asbestos they   ve been exposed to, so if that   s what you want to predict20, why
not use that information? but if you want to do more than just make predictions
without understanding, if you want to understand the structure tying together
these variables, if you want to do science, if you don   t want to go around telling
yourself that asbestos whitens teeth, you really do need to know the graph.21

20.5.1 missing variables

suppose that we do not observe one of the variables, such as the quantity of tar
in the lungs, but we somehow know all of the conditional distributions required
by the graph. (tar build-up in the lungs might indeed be hard to measure for
living people.) because we have a joint distribution for all the variables, we could
estimate the conditional distribution of one of them given the rest, using the
de   nition of id155 and of integration:

(cid:82) p(x1, x2, xi   1, xi, xi+1, xp)dxi

p(x1, x2, xi   1, xi, xi+1, xp)

(20.26)

p(xi|x1, x2, xi   1, xi+1, xp) =

we could in principle do this for any joint distribution. when the joint distribu-
tion comes from a dag model, however, we can simplify this considerably. recall
from   20.2.1 that xi is independent of all the other variables given its markov
blanket, i.e., its parents, its children, and the other parents of its children. we
can therefore drop from the conditioning everything which isn   t in the markov
blanket. actually doing the calculation then boils down to a version of the em
algorithm.22

if we observe only a subset of the other variables, we can still use the dag
to determine which ones actually matter to estimating xi, and which ones are
super   uous. the calculations then however become much more intricate.23

20.6 non-dag id114: undirected graphs and directed

graphs with cycles

this section is optional, as, for various reasons, we will not use these models in
this course.

20.6.1 undirected graphs

there is a lot of work on id203 models which are based on undirected graphs,
in which the relationship between random variables linked by edges is completely

20 maybe you want to guess who   d be interested in buying whitening toothpaste.
21 we return to this example in   21.2.2.
22 id114, especially directed ones, are often called    bayes nets    or    id110s   ,

because this equation is, or can be seen as, a version of bayes   s rule. since of course it follows
directly from the de   nition of id155, there is nothing distinctively bayesian here    
no subjective id203, or assigning probabilities to hypotheses.

23 there is an extensive discussion of relevant methods in jordan (1998).

20.6 non-dag id114

469

symmetric, unlike the case of dags24. since the relationship is symmetric, the
preferred metaphor is not    parent and child   , but    neighbors   . the models are
sometimes called markov networks or markov random    elds, but since dag
models have a markov property of their own, this is not a happy choice of name,
and i   ll just call them    undirected id114   .

the key markov property for undirected id114 is that any set of

nodes i is independent of the rest of the graph given its neighbors:

xi        xnon   neighbors(i)|xneighbors(i)

(20.27)

this corresponds to a factorization of the joint distribution, but a more complex
one than that of eq. 20.4, because a symmetric neighbor-of relation gives us no
way of ordering the variables, and conditioning the later ones on the earlier ones.
the trick turns out to go as follows. first, as a bit of id207, a clique
is a set of nodes which are all neighbors of each other, and which cannot be
expanded without losing that property. we write the collection of all cliques in
a graph g as cliques(g). second, we introduce potential functions   c which
take clique con   gurations and return non-negative numbers. third, we say that
a joint distribution is a gibbs distribution25 when

p(x1, x2, . . . xp)     (cid:89)

c   cliques(g)

  c(xi   c)

(20.28)

that is, the joint distribution is a product of factors, one factor for each clique.
frequently, one introduces what are called potential functions, uc = log   c,
and then one has

p(x1, x2, . . . xp)     e   (cid:80)

c   cliques(g) ui(xi   c)

(20.29)

the key correspondence is what is sometimes called the gibbs-markov the-
orem: a distribution is a gibbs distribution with respect to a graph g if, and
only if, it obeys the markov property with neighbors de   ned according to g.26.
in many practical situations, one combines the assumption of an undirected
graphical model with the further assumption that the joint distribution of all
the random variables is a multivariate gaussian, giving a gaussian graphical

24 i am told that this is more like the idea of causation in buddhism, as something like    co-dependent

origination   , than the asymmetric one which europe and the islamic world inherited from the
greeks (especially aristotle), but you would really have to ask a philosopher about that.

25 after the american physicist and chemist j. w. gibbs, who introduced such distributions as part of

statistical mechanics, the theory of the large-scale patterns produced by huge numbers of
small-scale interactions.

26 this theorem was proved, in slightly di   erent versions, under slightly di   erent conditions, and by

very di   erent methods, more or less simultaneously by (alphabetically) dobrushin, gri   eath,
grimmett, and hammersley and cli   ord, and almost proven by ruelle. in the statistics literature, it
has come to be called the    hammersley-cli   ord    theorem, for no particularly good reason. in my
opinion, the clearest and most interesting version of the theorem is that of gri   eath (1976), an
elementary exposition of which is given by pollard (http:
//www.stat.yale.edu/~pollard/courses/251.spring04/handouts/hammersley-clifford.pdf). (on
the other hand, gri   eath was one of my teachers, so discount accordingly.) calling it the
   gibbs-markov theorem    says more about the content, and is fairer to all concerned.

470

id114

-
+

set point\non thermostat

+

furnace

exterior\ntemperature

+

interior\ntemperature

figure 20.7 directed but cyclic graphical model of a feedback loop. signs
(+,     on arrows are    guides to the mind   . cf. figure 20.8.

model. an important consequence of this assumption is that the graph can be
   read o       from the inverse of the covariance matrix   , sometimes called the
precision matrix. speci   cally, there is an edge linking xi to xj if and only
if (     1)ij (cid:54)= 0. (see lauritzen (1996) for an extensive discussion.) these ideas
sometimes still work for non-gaussian distributions, when there is a natural way
of transforming them to be gaussian (liu et al., 2009), though it is unclear just
how far that goes.

20.6.2 directed but cyclic graphs

much less work has been done on directed graphs with cycles. it is very hard to
give these a causal interpretation, in the fashion described in the next chapter.
feedback processes are of course very common in nature and technology, and one
might think to represent these as cycles in a graph. a model of a thermostat,
for instance, might have variables for the set-point temperature, the temperature
outside, how much the furnace runs, and the actual temperature inside, with a
cycle between the latter two (figure 20.7).

thinking in this way is however simply sloppy. it always takes some time to
traverse a feedback loop, and so the cycle really    unrolls    into an acyclic graph
linking similar variables at di   erent times (figure 20.8). sometimes27, it is clear
that when people draw a diagram like figure 20.7, the incoming arrows really
refer to the change, or rate of change, of the variable in question, so it is merely
a visual short-hand for something like figure 20.8.

directed graphs with cycles are thus primarily useful when measurements are so
slow or otherwise imprecise that feedback loops cannot be unrolled into the actual
dynamical processes which implement them, and one is forced to hope that one
can reason about equilibria instead28. if you insist on dealing with cyclic directed
id114, see richardson (1996); lacerda et al. (2008) and references
therein.

27 as in puccia and levins (1985), and the loopanalyst package based on it (dinno, 2009).
28 economists are fond of doing so, generally without providing any rationale, based in economic

theory, for supposing that equilibrium is a good approximation (fisher, 1983, 2010).

20.6 non-dag id114

471

exterior\ntemperature

set point\non thermostat

+

+

interior\ntemperature\nat time t

furnace\nat time t

+

+

+

+-

interior\ntemperature\nat time t+1

furnace\nat time t+1

figure 20.8 directed, acyclic graph for the situation in figure 20.7, taking
into account the fact that it takes time to traverse a feedback loop. one
should imagine this repeating to times t + 2, t + 3, etc., and extending
backwards to times t     1, t     2, etc., as well. notice that there are no longer
any cycles.

set pointon thermostatfurnaceat time t+furnaceat time t+1+interiortemperatureat time t+1+exteriortemperature+interiortemperatureat time t+-+472

id114

20.7 further reading

the paper collection jordan (1998) is actually extremely good, unlike most col-
lections of edited papers; jordan and sejnowski (2001) is also useful. lauritzen
(1996) is thorough but more mathematically demanding. the books by spirtes
et al. (1993, 2001) and by pearl (1988, 2000, 2009b) are deservedly classics, espe-
cially for their treatment of causality, of which much more in part iii. glymour
(2001) discusses applications to psychology.

while i have presented dag models as an outgrowth of factor analysis, their
historical ancestry is actually closer to the    path analysis    models introduced,
starting around 1918, by the great geneticist and mathematical biologist sewall
wright to analyze processes of development and genetics. wright published his
work in a series of papers which culminated in wright (1934). that paper is
now freely available online, and worth reading. (see also http://www.ssc.wisc.
edu/soc/class/soc952/wright/wright_biblio.htm for references to, and in
some cases copies of, related papers by wright.) path analysis proved extremely
in   uential in psychology and sociology. loehlin (1992) is user-friendly, though
aimed at psychologists who know less math anyone taking this course. li (1975),
while older, is very enthusiastic and has many interesting applications in biology.
moran (1961) is a very clear treatment of the mathematical foundations, extended
by wysocki (1992) to the case where each variable is itself multi-dimensional
vector, so that path    coe   cients    are themselves matrices.

markov random    elds where the graph is a regular lattice are used extensively
in spatial statistics. good introductory-level treatments are provided by kinder-
mann and snell (1980) (the full text of which is free online), and by guttorp
(1995), which also covers the associated statistical methods. winkler (1995) is
also good, but presumes more background in statistical theory. (i would recom-
mend reading it after guttorp.) gri   eath (1976), while presuming more probabil-
ity theory on the part of the reader, is extremely clear and insightful, including
what is simultaneously one of the deepest and most transparent proofs of the
gibbs-markov theorem. lauritzen (1996) is a mathematically rigorous treatment
of id114 from the viewpoint of theoretical statistics, covering both the
directed and undirected cases.

if you are curious about gibbs distributions in their (so to speak) natural
habitat, the book by sethna (2006), also free online, is the best introduction to
statistical mechanics i have seen, and presumes very little knowledge of actual
physics on the part of the reader. honerkamp (2002) is less friendly, but tries
harder to make connections to statistics. if you already know what an exponential
family is, then eq. 20.29 is probably extremely suggestive, and you should read
mandelbrot (1962).

on id205 (  20.4), the best book is cover and thomas (2006) by a
large margin. references speci   cally on the connection between causal graphical
models and id205 are given in chapter 21.

exercises

exercises

473

20.1 find all the paths between the exogenous variables in figure 20.4, and verify that every

such path goes through at least one collider .

20.2 is it true that in any dag, every path between exogenous variables must go through
at least one collider, or descendant of a collider? either prove it or construct a counter-
example in which it is not true. does the answer change we say    go through at least one
collider   , rather than    collider or descendant of a collider   ? .

20.3 1. take any two nodes, say x1 and x2, which are linked in a dag by a path which does
not go over colliders. prove that there is a unique node along the path which is an
ancestor of all other nodes on that path. (note that this shared ancestor may in fact
be x1 or x2.) hint: do exercise 20.2.

2. take any two nodes which are linked in a dag by a path which remains open when
conditioning on a set of variables s containing no colliders. prove that for every open
path between x1 and x2, there is a unique node along the path which is an ancestor
of all other nodes on that path, and that this ancestor is not in s.

20.4 prove that x2        x3|x5 in figure 20.4.

part iii

causal id136

475

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

21

graphical causal models

21.1 causation and counterfactuals

take a piece of cotton, say an old rag. apply    ame to it; the cotton burns. we
say the    re caused the cotton to burn. the    ame is certainly correlated with the
cotton burning, but, as we all know, correlation is not causation (figure 21.1).
perhaps every time we set rags on    re we handle them with heavy protective
gloves; the gloves don   t make the cotton burn, but the statistical dependence is
strong. so what is causation?

we do not have to settle 2500 years (or more) of argument among philosophers
and scientists. for our purposes, it   s enough to realize that the concept has a
counter-factual component: if, contrary to fact, the    ame had not been applied
to the rag, then the rag would not have burned1. on the other hand, the    re
makes the cotton burn whether we are wearing protective gloves or not.

to say it a somewhat di   erent way, the distributions we observe in the world

1 if you immediately start thinking about quibbles, like    what if we hadn   t applied the    ame, but the

rag was struck by lightning?   , then you may have what it takes to be a philosopher.

figure 21.1    correlation doesn   t imply causation, but it does waggle its
eyebrows suggestively and gesture furtively while mouthing    look over
there       (image and text copyright by randall munroe, used here under a
creative commons attribution-noncommercial license; see
http://xkcd.com/552/. [[todo: excise from the commercial version]])

477

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

478

graphical causal models

are the outcome of complicated stochastic processes. the mechanisms which
set the value of one variable inter-lock with those which set other variables.
when we make a probabilistic prediction by conditioning     whether we pre-
dict e [y | x = x] or pr (y | x = x) or something more complicated     we are
just    ltering the output of those mechanisms, picking out the cases where they
happen to have set x to the value x, and looking at what goes along with that.
when we make a causal prediction, we want to know what would happen if the
usual mechanisms controlling x were suspended and it was set to x. how would
this change propagate to the other variables? what distribution would result for
y ? this is often, perhaps even usually, what people really want to know from a
data analysis, and they settle for statistical prediction either because they think
it is causal prediction, or for lack of a better alternative.

causal id136 is the undertaking of trying to answer causal questions from
empirical data. its fundamental di   culty is that we are trying to derive counter-
factual conclusions with only factual premises. as a matter of habit, we come to
expect cotton to burn when we apply    ames. we might even say, on the basis
of purely statistical evidence, that the world has this habit. but as a matter of
pure logic, no amount of evidence about what did happen can compel beliefs
about what would have happened under non-existent circumstances2. (for all my
data shows, all the rags i burn just so happened to be on the verge of sponta-
neously bursting into    ames anyway.) we must supply some counter-factual or
causal premise, linking what we see to what we could have seen, to derive causal
conclusions.

one of our goals, then, in causal id136 will be to make the causal premises

as weak and general as possible, thus limiting what we take on faith.

21.2 causal id114

we will need a formalism for representing causal relations. it will not surprise
you by now to learn that these will be id114. we will in fact use dag
models from last time, with    parent    interpreted to mean    directly causes   . these
will be causal id114, or graphical causal models.3

we make the following assumptions.

1. there is some directed acyclic graph g representing the relations of causation

among the our variables.

2 the    rst person to really recognize this seems to have been the medieval muslim theologian and
anti-philosopher al ghazali (1100/1997). (see kogan (1985) for some of the history.) very similar
arguments were made centuries later by hume (1739); whether there was some line of intellectual
descent linking them     that is, any causal connection     i don   t know.

3 because dag models have joint distributions which factor according to the graph, we can always
write them in the form of a set of equations, as xi = fi(xparents(i)) +  i, with the catch that the
noise  i is not necessarily independent of xi   s parents. this is what is known, in many of the social
sciences, as a structural equation model. so those are, strictly, a sub-class of dag models. they
are also often used to represent causal structure.

21.2 causal id114

479

2. the causal markov condition: the joint distribution of the variables obeys

the markov property on g.

3. faithfulness: the joint distribution has all of the conditional independence
relations implied by the causal markov property, and only those conditional
independence relations.

the point of the faithfulness condition is to rule out    conspiracies among the
parameters   , where, say, two causes of a common e   ect, which would typically
be dependent conditional on that e   ect, have their impact on the joint e   ect and
their own distributions matched just so exactly that they remain conditionally
independent.

21.2.1 calculating the    e   ects of causes   

let   s    x two sub-sets of variables in the graph, xc and xe. (assume they don   t
overlap, and call everything else xn .) if we want to make a probabilistic predic-
tion for xe   s value when xc takes a particular value, xc, that   s the conditional
distribution, pr (xe | xc = xc), and we saw last time how to calculate that using
the graph. conceptually, this amounts to selecting, out of the whole population
or ensemble, the sub-population or sub-ensemble where xc = xc, and accepting
whatever other behavior may go along with that.

now suppose we want to ask what the e   ect would be, causally, of setting xc
to a particular value xc. we represent this by    doing surgery on the graph   : we
(i) eliminate any arrows coming in to nodes in xc, (ii)    x their values to xc, and
(iii) calculate the resulting distribution for xe in the new graph. by steps (i)
and (ii), we imagine suspending or switching o    the mechanisms which ordinarily
set xc. the other mechanisms in the assemblage are left alone, however, and so
step (iii) propagates the    xed values of xc through them. we are not selecting a
sub-population, but producing a new one.

if setting xc to di   erent values, say xc and x(cid:48)

c, leads to di   erent distributions
for xe, then we say that xc has an e   ect on xe     or, slightly redundantly,
has a causal e   ect on xe. sometimes4    the e   ect of switching from xc to x(cid:48)
c   
speci   cally refers to a change in the expected value of xe, but since profoundly
di   erent distributions can have the same mean, this seems needlessly restrictive.5
if one is interested in average e   ects of this sort, they are computed by the same
procedure.

it is convenient to have a short-hand notation for this procedure of causal
conditioning. one more-or-less standard idea, introduced by judea pearl, is to
introduce a do operator which encloses the conditioning variable and its value.
that is,

pr (xe | xc = xc)

(21.1)

4 especially in economics.
5 economists are also fond of the horribly misleading usage of talking about    an x e   ect    or    the

e   ect of x    when they mean the regression coe   cient of x. don   t do this.

480

graphical causal models

is probabilistic conditioning, or selecting a sub-ensemble from the old mecha-
nisms; but

pr (xe | do(xc = xc))

(21.2)

notation and will use it.

is causal conditioning, or producing a new ensemble. sometimes one sees this

written as pr (xe | xc   =xc), or even pr (xe | (cid:98)xc). i am actually fond of the do
suppose that pr (xe | xc = xc) = pr (xe | do(xc = xc)). this would be ex-
tremely convenient for causal id136. the conditional distribution on the right
is the causal, counter-factual distribution which tells us what would happen if
xc was imposed. the distribution on the left is the ordinary probabilistic distri-
bution we have spent years learning how to estimate from data. when do they
coincide?

one situation where they coincide is when xc contains all the parents of xe,
and none of its descendants. then, by the markov property, xe is independent of
all other variables given xc, and removing the arrows into xc will not change that,
or the conditional distribution of xe given its parents. doing causal id136 for
other choices of xc will demand other conditional independence relations implied
by the markov property. this is the subject of chapter 22.

21.2.2 back to teeth

let us return to the example of figure 20.6, and consider the relationship between
exposure to asbestos and the staining of teeth. in the model depicted by that
   gure, the joint distribution factors as

p(yellow teeth, smoking, asbestos, tar in lungs, cancer)
= p(smoking)p(asbestos)

(21.3)

  p(tar in lungs|smoking)
  p(yellow teeth|smoking)
  p(cancer|asbestos, tar in lungs)

as we saw, whether or not someone   s teeth are yellow (in this model) is un-
conditionally independent of asbestos exposure, but conditionally dependent on
asbestos, given whether or not they have cancer. a id28 of tooth
color on asbestos would show a non-zero coe   cient, after    controlling for    cancer.
this coe   cient would become signi   cant with enough data. the usual interpre-
tation of this coe   cient would be to say that the log-odds of yellow teeth increase
by so much for each one unit increase in exposure to asbestos,    other variables
being held equal   .6 but to see the actual causal e   ect of increasing exposure to
asbestos by one unit, we   d want to compare p(yellow teeth|do(asbestos = a)) to
p(yellow teeth|do(asbestos = a + 1)), and it   s easy to check (exercise 21.1) that

6 nothing hinges on this being a id28, similar interpretations are given to all the other

standard models.

21.3 conditional independence and d-separation revisited

481

these two distributions have to be the same. in this case, because asbestos is ex-
ogenous, one will in fact get the same result for p(yellow teeth|do(asbestos = a)
and for p(yellow teeth|asbestos = a).

for a more substantial example, consider figure 21.27 the question of interest
here is whether regular brushing and    ossing actually prevents heart disease. the
mechanism by which it might do so is as follows: brushing is known to make it less
likely for people to get gum disease. gum disease, in turn, means the gums su   er
from constant, low-level in   ammation. persistent in   ammation (which can be
measured through various messenger chemicals of the immune system) is thought
to increase the risk of heart disease. against this, people who are generally health-
conscious are likely to brush regularly, and to take other actions, like regularly
exercising and controlling their diets, which also make them less likely to get
heart disease. in this case, if we were to manipulate whether people brush their
teeth8, we would shift the graph from figure 21.2 to figure 21.3, and we would
have

p(heart disease|brushing = b) (cid:54)= p(heart disease|do(brushing = b))

(21.4)

21.3 conditional independence and d-separation revisited

we saw in   20.3 that all distributions which conform to a common dag share
a common set of conditional independence relations. faithful distributions have
no other conditional independence relations. these are vital facts for causal in-
ference.

the reason is that while causal in   uence    ows one way through the graph,
along the directions of arrows from parents to children, statistical information
can    ow in either direction. we can certainly make id136s about an e   ect
from its causes, but we can equally make id136s about causes from their
e   ects. it might be harder to actually do the calculations9, and we might be left
with more uncertainty, but we could do it. as we saw in   20.3, when conditioning
on a set of variables s blocks all channels of information    ow between x and
y , x        y |s. the faithful distributions are the ones where this implication is
reversed, where x        y |s implies that s blocks all paths between x and y .
in faithful id114, blocking information    ow is exactly the same as
conditional independence.

this turns out to be the single most important fact enabling causal id136.
if we want to estimate the e   ects of causes, within a given dag, we need to
block o    all non-causal channels of information    ow. if we want to check whether
a given dag is correct for the variables we have, we need to be able to compare

7 based on de oliveira et al. (2010), and the discussion of this paper by chris blattman (http:

//chrisblattman.com/2010/06/01/does-brushing-your-teeth-lower-cardiovascular-disease/).

8 hopefully, by ensuring that everyone brushes, rather than keeping people from brushing.
9 janzing (2007) [[todo: update refs]] makes the very interesting suggestion that the direction of

causality can be discovered by using this     roughly speaking, that if x|y is much harder to
compute than is y |x, we should presume that x     y rather than the other way around.

482

graphical causal models

health\nconsciousness

frequency of toohbrushing

gum disease

frequency of exercise

in   ammatory\nimmune response

amount of fat and \nred meat in diet

heart disease

figure 21.2 graphical model illustrating hypothetical pathways linking
brushing your teeth to not getting heart disease.

the conditional independence relations implied by the dag to those supported
by the data. if we want to discover the possible causal structures, we have to see
which ones imply the conditional independencies supported by the data.

21.4 further reading

the two foundational books on graphical causal models are spirtes et al. (2001)
and pearl (2009b). both are excellent and recommended in the strongest possible
terms; but if you had to read just one, i would recommend spirtes et al. (2001).
if on the other hand you do not feel up to reading a book at all, then pearl
(2009a) is much shorter, and covers the high points. (also, it   s free online.) the
textbook by morgan and winship (2007, 2015) is much less demanding mathe-
matically, and therefore also less complete conceptually, but it does explain the
crucial ideas clearly, simply, and with abundant examples.10 lauritzen (1996) has

10 that textbook also discusses an alternative formalism for counterfactuals, due mainly to donald b.

rubin and collaborators. while rubin has done very distinguished work in causal id136, his

21.4 further reading

health\nconsciousness

483

frequency of toohbrushing

gum disease

frequency of exercise

in   ammatory\nimmune response

amount of fat and \nred meat in diet

heart disease

figure 21.3 the previous graphical model,    surgically    altered to re   ect a
manipulation (do) of brushing.

a mathematically rigorous treatment of d-separation (among many other things),
but de-emphasizes causality.

many software packages for linear structural equation models and path analysis
o   er options to search for models; these are not, in general, reliable (spirtes et al.,
2001).

raginsky (2011) provides a fascinating information-theoretic account of graphi-
cal causal models and do(), in terms of the notion of directed (rather than mutual)
information.

formalism is vastly harder to manipulate than are id114, but has no more expressive
power. (pearl (2009a) has a convincing discussion of this point, and richardson and robins (2013)
provides a comprehensive proof that the everything expressible in the counterfactuals formalism can
also be expressed with suitably-augmented id114.) i have thus skipped the rubin
formalism here, but there are good accounts in morgan and winship (2007, ch. 2), in rubin   s
collected papers (rubin, 2006), and in imbens and rubin (2015) (though please read shalizi 2016
before taking any of the real-data examples in the last of these as models to imitate).

484

graphical causal models

21.1 show, for the graphical model in figure 20.6, that p(yellow teeth|do(asbestos = a)) is

always the same as p(yellow teeth|do(asbestos = a + 1)).

exercises

22

identifying causal e   ects from observations

there are two problems which are both known as    causal id136   :

1. given the causal structure of a system, estimate the e   ects the variables have

on each other.

2. given data about a system,    nd its causal structure.

the    rst problem is easier, so we   ll begin with it; we come back to the second in
chapter 24.

22.1 causal e   ects, interventions and experiments

as a reminder, when i talk about the causal e   ect of x on y , which i write

pr (y |do(x = x))

(22.1)

i mean the distribution of y which would be generated, counterfactually, were
x to be set to the particular value x. this is not, in general, the same as the
ordinary conditional distribution

pr (y |x = x)

(22.2)

the reason these are di   erent is that the latter represents taking the original
population, as it is, and just    ltering it to get the sub-population where x = x.
the processes which set x to that value may also have in   uenced y through other
channels, and so this distribution will not, typically, really tell us what would
happen if we reached in and manipulated x. we can sum up the contrast in a
little table (table 22.1). as we saw in chapter 20, if we have the full graph for a
directed acyclic graphical model, it tells us how to calculate the joint distribution
of all the variables, from which of course the conditional distribution of any
one variable given another follows. as we saw in chapter 21, calculations of
pr (y |do(x = x)) use a    surgically    altered graph, in which all arrows into x
are removed, and its value is pinned at x, but the rest of the graph is as before.
if we know the dag, and we know the distribution of each variable given its
parents, we can calculate any causal e   ect we want, by graph-surgery.

485

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

486

identifying causal e   ects

probabilistic conditioning

causal conditioning

pr (y |x = x)
factual
select a sub-population
predicts passive observation
calculate from full dag
always identi   able when x and y
are observable

pr (y |do(x = x))
counter-factual
generate a new population
predicts active manipulation
calculate from surgically-altered dag
not always identi   able even
when x and y are observable

table 22.1 contrasts between ordinary probabilistic conditioning and causal conditioning. (see
below on identi   ability.)

22.1.1 the special role of experiment

if we want to estimate pr (y |do(x = x)), the most reliable procedure is also the
simplest: actually manipulate x to the value x, and see what happens to y . (as
my mother says,    why think, when you can just do the experiment?   ) a causal
or counter-factual assumption is still required here, which is that the next time
we repeat the manipulation, the system will respond similarly, but this is pretty
weak as such assumptions go.

while this seems like obvious common sense to us now, it is worth taking a mo-
ment to re   ect on the fact that systematic experimentation is a very recent thing;
it only goes back to around 1600. since then, the knowledge we have acquired
by combining experiments with mathematical theories have totally transformed
human life, but for the    rst four or    ve thousand years of civilization, philoso-
phers and sages much smarter than (almost?) any scientist now alive would have
dismissed experiment as something    t only for cooks, potters and blacksmiths,
who didn   t really know what they were doing.

the major obstacle the experimentalist must navigate around is to make sure
they the experiment they are doing is the one they think they are doing. symboli-
cally, when we want to know pr (y |do(x = x)), we need to make sure that we are
only manipulating x, and not accidentally doing pr (y |do(x = x), z = z) (be-
cause we are only experimenting on a sub-population), or pr (y |do(x = x, z = z))
(because we are also, inadvertently, manipulating z). there are two big main di-
visions about how to avoid these confusions.

1. the older strategy is to deliberately control or manipulate as many other vari-

ables as possible. if we    nd pr (y |do(x = x, z = z)) and pr (y |do(x = x(cid:48), z = z))
then we know the di   erences between them are indeed just due to changing x.
this strategy, of actually controlling or manipulating whatever we can, is the
traditional one in the physical sciences, and more or less goes back to galileo
and the beginning of the scienti   c revolution1.
2. the younger strategy is to randomize over all the other variables but x. that
is, to examine the contrast between pr (y |do(x = x)) and pr (y |do(x = x(cid:48))),

1 the anguished sound you hear as you read this is every historian of science wailing in protest as the

over-simpli   cation, but this will do as an origin myth for our purposes.

22.2 identi   cation and confounding

487

we use an independent source of random noise to decide which experimental
subjects will get do(x = x) and which will get do(x = x(cid:48)). it is easy to
convince yourself that this makes pr (y |do(x = x)) equal to pr (y |x = x).
the great advantage of the randomization approach is that we can apply it
even when we cannot actually control the other causally relevant variables,
or even are unsure of what they are. unsurprisingly, it has its origins in the
biological sciences, especially agriculture. if we want to credit its invention to
a single culture hero, it would not be too misleading2 to attribute it to r. a.
fisher in the early 1900s.

experimental evidence is compelling, but experiments are often slow, expen-
sive, and di   cult. moreover, experimenting on people is hard, both because there
are many experiments we shouldn   t do, and because there are many experiments
which would just be too hard to organize. we must therefore consider how to do
causal id136 from non-experimental, observational data.

22.2 identi   cation and confounding

for the present purposes, the most important distinction between probabilistic
and causal conditioning has to do with the identi   cation (or identi   ability),
of the conditional distributions. an aspect of a statistical model is identi   able
when it cannot be changed without there also being some change in the distribu-
tion of the observable variables. if we can alter part of a model with no observable
consequences, that part of the model is unidenti   able3. sometimes the lack of
identi   cation is trivial: in a two-cluster mixture model, we get the same observ-
able distribution if we swap the labels of the two clusters (  19.1.5). the rotation
problem for factor models (    17.6, 17.10.1) is a less trivial identi   cation prob-
lem4. if two variables are co-linear, then their coe   cients in a id75
are unidenti   able (  2.1.1)5. note that identi   cation is about the true distribution,
not about what happens with    nite data. a parameter might be identi   able, but
we could have so little information about it in our data that our estimates are
unusable, with immensely wide con   dence intervals; that   s unfortunate, but we
just need more data. an unidenti   able parameter, however, cannot be estimated
even with in   nite data.6
when x and y are both observable variables, pr (y |x = x) can   t help being

2 see previous note.
3 more formally, divide the model   s parameters into two parts, say    and   . the distinction between
  1 and   2 is identi   able if, for all   1,   2, the distribution over observables coming from (  1,   1) is
di   erent from that coming from (  2,   2). if the right choice of   1 and   2 masks the distinction
between   1 and   2, then    is unidenti   able.

4 as this example suggests, what is identi   able depends on what is observed. if we could observe the

factors directly, factor loadings would be identi   able.

5 as that example suggests, whether one aspect of a model is identi   able or not can depend on other

aspects of the model. if the co-linearity was broken, the two regression coe   cients would become
identi   able.

6 for more on identi   ability, and what to do with unidenti   able problems, see the great book by

manski (2007).

488

identifying causal e   ects

u

x

y

figure 22.1 the distribution of y given x, pr (y |x), confounds the
actual causal e   ect of x on y , pr (y |do(x = x)), with the indirect
dependence between x and y created by their unobserved common cause u .
(you may imagine that u is really more than one variable, with some
internal sub-graph.)

identi   able. (changing this conditional distribution just is changing part of the
distribution of observables.) things are very di   erent, however, for pr (y |do(x = x)).
in some models, it   s entirely possible to change this drastically, and always have
the same distribution of observables, by making compensating changes to other
parts of the model. when this is the case, we simply cannot estimate causal e   ects
from observational data. the basic problem is illustrated in figure 22.1.
in figure 22.1, x is a parent of y . but if we analyze the dependence of
y on x, say in the form of the conditional distribution pr (y |x = x), we see
that there are two channels by which information    ows from cause to e   ect.
one is the direct, causal path, represented by pr (y |do(x = x)). the other is
the indirect path, where x gives information about its parent u , and u gives
information about its child y . if we just observe x and y , we cannot sep-
arate the causal e   ect from the indirect id136. the causal e   ect is con-
founded with the indirect id136. more generally, the e   ect of x on y is
confounded whenever pr (y |do(x = x)) (cid:54)= pr (y |x = x). if there is some way
to write pr (y |do(x = x)) in terms of distributions of observables, we say that
the confounding can be removed by an identi   cation strategy, which de-
confounds the e   ect. if there is no way to de-confound, then this causal e   ect
is unidenti   able.

the e   ect of x on y in figure 22.1 is unidenti   able. even if we erased the
arrow from x to y , we could get any joint distribution for x and y we liked
by picking p (x|u ), p (y |u ) and p (u ) appropriately. so we cannot even, in
this situation, use observations to tell whether x is actually a cause of y . no-
tice, however, that even if u was observed, it would still not be the case that
pr (y |x = x) = pr (y |do(x = x)). while the e   ect would be identi   able (via
the back door criterion; see below), we would still need some sort of adjustment
to recover it.

in the next section, we will look at such identi   cation strategies and adjust-

ments.

22.3 identi   cation strategies

489

22.3 identi   cation strategies

(cid:88)

to recap, we want to calculate the causal e   ect of x on y , pr (y |do(x = x)),
but we cannot do an experiment, and must rely on observations. in addition to
x and y , there will generally be some covariates z which we know, and we   ll
assume we know the causal graph, which is a dag. is this enough to determine
pr (y |do(x = x))? that is, does the joint distribution identify the causal e   ect?
the answer is    yes    when the covariates z contain all the other relevant vari-
ables7. the inferential problem is then no worse than any other statistical es-
timation problem. in fact, if we know the causal graph and get to observe all
the variables, then we could (in principle) just use our favorite non-parametric
conditional density estimate at each node in the graph, with its parent variables
as the inputs and its own variable as the response. multiplying conditional dis-
tributions together gives the whole distribution of the graph, and we can get any
causal e   ects we want by surgery. equivalently (exercise 22.2), we have that

pr (y |do(x = x)) =

pr (y |x = x, pa(x) = t) pr (pa(x) = t)

(22.3)

t

where pa(x) is the complete set of parents of x.

if we   re willing to assume more, we can get away with just using non-parametric
regression or even just an additive model at each node. assuming yet more, we
could use parametric models at each node; the linear-gaussian assumption is
(alas) very popular.

if some variables are not observed, then the issue of which causal e   ects are
observationally identi   able is considerably trickier. apparently subtle changes in
which variables are available to us and used can have profound consequences.

the basic principle underlying all considerations is that we would like to condi-
tion on adequate control variables, which will block paths linking x and y other
than those which would exist in the surgically-altered graph where all paths into
x have been removed. if other unblocked paths exist, then there is some con-
founding of the causal e   ect of x on y with their mutual dependence on other
variables.

this is familiar to use from regression as the basic idea behind using additional
variables in our regression, where the idea is that by introducing covariates, we
   control for    other e   ects, until the regression coe   cient for our favorite variable
represents only its causal e   ect. leaving aside the inadequacies of id75
as such (chapter 2), we need to be cautious here. just conditioning on everything

7 this condition is sometimes known as causal su   ciency. strictly speaking, we do not have to

suppose that all causes are included in the model and observable. what we have to assume is that
all of the remaining causes have such an unsystematic relationship to the ones included in the dag
that they can be modeled as noise. (this does not mean that the noise is necessarily small.) in fact,
what we really have to assume is that the relationships between the causes omitted from the dag
and those included is so intricate and convoluted that it might as well be noise, along the lines of
algorithmic id205 (li and vit  anyi, 1997), whose key result might be summed up as
   any determinism distinguishable from randomness is insu   ciently complex   . but here we verge on
philosophy.

490

identifying causal e   ects

x

y

z

figure 22.2    controlling for    additional variables can introduce bias into
estimates of causal e   ects. here the e   ect of x on y is directly identi   able,
pr (y |do(x = x)) = pr (y |x = x). if we also condition on z however,
because it is a common e   ect of x and y , we   d get
pr (y |x = x, z = z) (cid:54)= pr (y |x = x). in fact, even if there were no arrow
from x to y , conditioning on z would make y depend on x.

possible does not give us adequate control, or even necessarily bring us closer to
it. as figure 22.2 illustrates, and as several of the data-analysis problem sets will
drive home, adding an ill-chosen covariate to a regression can create confounding.
there are three main ways we can    nd adequate controls, and so get both

identi   ability and appropriate adjustments:

1. we can condition on an intelligently-chosen set of covariates s, which block
all the indirect paths from x to y , but leave all the direct paths open. (that
is, we can follow the regression strategy, but do it right.) to see whether a
candidate set of controls s is adequate, we apply the back-door criterion.
2. we can    nd a set of variables m which mediate the causal in   uence of x
on y     all of the direct paths from x to y pass through m . if we can
identify the e   ect of m on y , and of x on m , then we can combine these
to get the e   ect of x on y . (that is, we can just study the mechanisms by
which x in   uences y .) the test for whether we can do this combination is
the front-door criterion.

3. we can    nd a variable i which a   ects x, and which only a   ects y by in   u-
encing x. if we can identify the e   ect of i on y , and of i on x, then we can,
sometimes,    factor    them to get the e   ect of x on y . (that is, i gives us
variation in x which is independent of the common causes of x and y .) i is
then an instrumental variable for the e   ect of x on y .

22.3 identi   cation strategies

491

u

x

s2

s1

v

s3

y

b

figure 22.3 illustration of the back-door criterion for identifying the
causal e   ect of x on y . setting s = {s1, s2} satis   es the criterion, but
neither s1 nor s2 on their own would. setting s = {s3}, or s = {s1, s2, s3}
also works. adding b to any of the good sets makes them fail the criterion.

let   s look at these three in turn.

22.3.1 the back-door criterion: identi   cation by conditioning

(cid:88)

when estimating the e   ect of x on y , a back-door path is an undirected path
between x and y with an arrow into x. these are the paths which create con-
founding, by providing an indirect, non-causal channel along which information
can    ow. a set of conditioning variables or controls s satis   es the back-door
criterion when (i) s blocks every back-door path between x and y , and (ii) no
node in s is a descendant of x. (cf. figure 22.3.) when s meets the back-door
criterion,

pr (y |do(x = x)) =

pr (y |x = x, s = s) pr (s = s)

(22.4)

s

notice that all the items on the right-hand side are observational conditional
probabilities, not counterfactuals. thus we have achieved identi   ability, as well
as having an adjustment strategy.

the motive for (i) is plain, but what about (ii)? we don   t want to include
descendants of x which are also ancestors of y , because that blocks o    some of
the causal paths from x to y , and we don   t want to include descendants of x
which are also descendants of y , because they provide non-causal information
about y 8.
more formally, we can proceed as follows (pearl, 2009b,   11.3.3). we know from

8 what about descendants of x which are neither ancestors nor descendants of y ? conditioning on
them is either creates potential colliders, if they are also descended from ancestors of y other than
x, or needlessly complicates the adjustment in eq. 22.4.

identifying causal e   ects

492

eq. 22.3 that

pr (y |do(x = x)) =

(cid:88)

t

pr (pa(x) = t) pr (y |x = x, pa(x) = t)

(22.5)

we can always introduce another set of conditioned variables, if we sum out over
them:

pr (y |do(x = x)) =

pr (pa(x) = t)

pr (y, s = s|x = x, pa(x) = t)

(cid:88)

(cid:88)

t

s

(22.6)
we can do this for any set of variables s, it   s just id203. it   s also just
id203 that

pr (y, s|x = x, pa(x) = t) =
pr (y |x = x, pa(x) = t, s = s) pr (s = s|x = x, pa(x) = t)

(22.7)

(cid:88)

so
pr (y |do(x = x)) =
pr (pa(x) = t)

(cid:88)

t

s

(22.8)
pr (y |x = x, pa(x) = t, s = s) pr (s = s|x = x, pa(x) = t)

now we use the fact that s satis   es the back-door criterion. point (i) of the
criterion, blocking back-door paths, implies that y        pa(x)|x, s. thus

(cid:88)

pr (y |do(x = x)) =
pr (pa(x) = t)

(cid:88)

t

s

pr (y |x = x, s = s) pr (s = s|x = x, pa(x) = t)

(22.9)

point (ii) of the criterion, not containing descendants of x, means (by the markov
property) that x        s|pa(x). therefore

pr (y |do(x = x)) =
pr (pa(x) = t)

(cid:88)
t pr (pa(x) = t) pr (s = s|pa(x) = t) = pr (s = s), we have, at last,
pr (y |do(x = x)) =

pr (y |x = x, s = s) pr (s = s|pa(x) = t)

pr (y |x = x, s = s) pr (s = s)

(cid:88)
(cid:88)

s

t

(22.10)

(22.11)

since(cid:80)

as promised. 2

s

22.3.1.1 the entner rules

using the back-door criterion requires us to know the causal graph. recently,
entner et al. (2013) have given a set of rules which provide su   cient conditions
for deciding that set of variables satisfy the back-door criterion, or that x actually
has no e   ect on y , which can be used without knowing the graph completely.

it makes no sense to control for anything which is a descendant of either y or x;
that   s either blocking a directed path, or activating a collider, or just irrelevant.

22.3 identi   cation strategies

493
so let w be the set of all observed variables which descend neither from x nor
y .
1. if there is a set of controls s such that x        y |s, then x has no causal e   ect

on y .

reasoning: y can   t be a child of x if we can make them independent by
conditioning on anything, and y can   t be a more remote descendant either,
since s doesn   t include any descendants of x. so in this situation all the paths
linking x to y must be back-door paths, and s, blocking them, shows there   s
no e   ect.
2. if there is a w     w and a subset s of the w, not including w , such that (i)
w (cid:54)       y |s, but (ii) w        y |s, x, then x has an e   ect on y , and s satis   es
the back-door criterion for estimating the e   ect.

reasoning: point (i) shows that conditioning on s leaves open path from w
to y . by point (ii), these paths must all pass through x, since conditioning
on x blocks them, hence x has an e   ect on y . s must block all the back-door
paths between x and y , otherwise x would be a collider on paths between
w and y , so conditioning on x would activate those paths.
3. if there is a w     w and a subset s of w, excluding w , such that (i) w (cid:54)       x|s
but (ii) w        y |s, then x has no e   ect on y .

reasoning: point (i) shows that conditioning on s leaves open active paths
from w to x. but by (ii), there cannot be any open paths from w to y , so
there cannot be any open paths from x to y .

if none of these rules apply, whether x has an e   ect on y , and if so what
adequate controls are for    nding it, will depend on the exact graph, and cannot be
determined just from independence relations among the observables. (for proofs
of everything, see the paper.)

22.3.2 the front-door criterion: identi   cation by mechanisms

a set of variables m satis   es the front-door criterion when (i) m blocks all
directed paths from x to y , (ii) there are no unblocked back-door paths from x
to m , and (iii) x blocks all back-door paths from m to y . then

(cid:88)

pr (y |do(x = x)) =

pr (m = m|x = x)

(cid:88)

pr (y |x = x(cid:48), m = m) pr (x = x(cid:48))

(22.12)

m

x(cid:48)

the variables m are sometimes called mediators.

a natural reaction to the front-door criterion is    say what?   , but it becomes
more comprehensible if we take it apart. because, by clause (i), m blocks all
directed paths from x to y , any causal dependence of y on x must be mediated
by a dependence of y on m :

pr (y |do(x = x)) =

pr (y |do(m = m)) pr (m = m|do(x = x))

(22.13)

(cid:88)

m

494

identifying causal e   ects

u

x

m

y

figure 22.4 illustration of the front-door criterion, after pearl (2009b,
figure 3.5). x, y and m are all observed, but u is an unobserved common
cause of both x and y . x     u     y is a back-door path confounding the
e   ect of x on y with their common cause. however, all of the e   ect of x on
y is mediated through x   s e   ect on m . m    s e   ect on y is, in turn,
confounded by the back-door path m     x     u     y , but x blocks this
path. so we can use back-door adjustment to    nd pr (y |do(m = m)), and
directly    nd pr (m|do(x = x)) = pr (m|x = x). putting these together
gives pr (y |do(x = x)).

clause (ii) says that we can get the e   ect of x on m directly,
pr (m = m|do(x = x)) = pr (m = m|x = x) .

(22.14)

clause (iii) say that x satis   es the back-door criterion for identifying the e   ect
of m on y , and the inner sum in eq. 22.12 is just the back-door computation
(eq. 22.4) of pr (y |do(m = m)). so really we are using the back door criterion,
twice. (see figure 22.4.)
for example, in the    does tooth-brushing prevent heart-disease?    example of
  21.2.2, we have x =    frequency of tooth-brushing   , y =    heart disease   , and we
could take as the mediating m either    gum disease    or    in   ammatory immune
response   , according to figure 21.2.

22.3.2.1 the front-door criterion and mechanistic explanation

morgan and winship (2007, ch. 8) give a useful insight into the front-door cri-
terion. each directed path from x to y is, or can be thought of as, a separate
mechanism by which x in   uences y . the requirement that all such paths be
blocked by m , (i), is the requirement that the set of mechanisms included in m
be    exhaustive   . the two back-door conditions, (ii) and (iii), require that the
mechanisms be    isolated   , not interfered with by the rest of the data-generating
process (at least once we condition on x). once we identify an isolated and ex-
haustive set of mechanisms, we know all the ways in which x actually a   ects y ,
and any indirect paths can be discounted, using the front-door adjustment 22.12.
one interesting possibility suggested by this is to elaborate mechanisms into
sub-mechanisms, which could be used in some cases where the plain front-door

22.3 identi   cation strategies

495

u

m

m2

y

x

m1

figure 22.5 the path x     m     y contains all the mechanisms by which
x in   uences y , but is not isolated from the rest of the system (u     m ).
the sub-mechanisms x     m1     m and m     m2     y are isolated, and
the original causal e   ect can be identi   ed by composing them.

criterion won   t apply9, such as figure 22.5. because u is a parent of m , we cannot
use the front-door criterion to identify the e   ect of x on y . (clause (i) holds,
but (ii) and (iii) both fail.) but we can use m1 and the front-door criterion to
   nd pr (m|do(x = x)), and we can use m2 to    nd pr (y |do(m = m)). chaining
those together, as in eq. 22.13, would given pr (y |do(x = x)). so even though
the whole mechanism from x to y is not isolated, we can still identify e   ects
by breaking it into sub-mechanisms which are isolated. this suggests a natural
point at which to stop re   ning our account of the mechanism into sub-sub-sub-
mechanisms: when we can identify the causal e   ects we   re concerned with.

22.3.3 instrumental variables

a variable i is an instrument10 for identifying the e   ect of x on y when there is
a set of controls s such that (i) i (cid:54)       x|s, and (ii) every unblocked path from i to
y has an arrow pointing into x. another way to say (ii) is that i        y |s, do(x).
colloquially, i in   uences y , but only through    rst in   uencing x (at least once
we control for s). (see figure 22.6.)
how is this useful? by making back-door adjustments for s, we can identify
pr (y |do(i = i)) and pr (x|do(i = i)). since all the causal in   uence of i on y
must be channeled through x (by point (ii)), we have

pr (y |do(i = i)) =

pr (y |do(x = x)) pr (x = x|do(i = i))

(22.15)

(cid:88)

x

as in eq. 22.3. we can thus identify the causal e   ect of x on y whenever

9 the ideas in this paragraph come from conversation prof. winship; see morgan and winship (2015,

ch. 10).

10 the term    instrumental variables    comes from econometrics, where they were originally used, in the

1940s, to identify parameters in simultaneous equation models. (the metaphor was that i is a
measuring instrument for the otherwise inaccessible parameters.) de   nitions of instrumental
variables are surprisingly murky and controversial outside of extremely simple linear systems; this
one is taken from galles and pearl (1997), via pearl (2009b,   7.4.5).

496

identifying causal e   ects

b

y

i

x

s

u

figure 22.6 a valid instrumental variable, i, is related to the cause of
interest, x, and in   uences y only through its in   uence on x, at least once
control variables block other paths. here, to use i as an instrument, we
should condition on s, but should not condition on b. (if we could condition
on u , we would not need to use an instrument.)

i

x

u

y

figure 22.7 i acts as an instrument for estimating the e   ect of x on y ,
despite the presence of the confounding, unobserved variable u .

eq. 22.15 can be solved for pr (y |do(x = x)) in terms of pr (y |do(i = i)) and
pr (x|do(i = i)). figuring out when this is possible in general requires an excur-
sion into the theory of integral equations11, which i have bracketed in   22.3.3.3.
the upshot is that while there may not be unique solutions, there often are,
though they can be somewhat hard to calculate. however, in the special case
where the relations between all variables are linear, we can be much more spe-
ci   c, fairly easily.

let   s start with the most basic possible set-up for an instrumental variable,
namely that in figure 22.7, where we just have x, y , the instrument i, and the
unobserved confounders s. if everything is linear, identifying the causal e   ect of
x on y is equivalent to identifying the coe   cient on the x     y arrow. we can
write

x =   0 +   i +   u +  x

11 if x is continuous, then the analog of eq. 22.15 is

pr (y |do(i = i)) =(cid:82) p(y |do(x = x))p(x = x|do(i = i))dx, where the    integral operator   
(cid:82)   p(x = x|do(i = i))dx is known, as is pr (y |do(i = i)).

(22.16)

and

22.3 identi   cation strategies

y =   0 +   x +   u +  y

497

(22.17)

where  x and  y are mean-zero noise terms, independent of each other and of
the other variables, and we can, without loss of generality, assume u has mean
zero as well. we want to    nd   . substituting,

y =   0 +     0 +     i + (     +   )u +    x +  y

(22.18)

since u ,  x and  y are all unobserved, we can re-write this as

y =   0 +     i +   

where   0 =   0 +     0, and    = (     +   )u +    x +  y has mean zero.

now take the covariances:

cov [i, x] =   v [i] + cov [ x, i]
cov [i, y ] =     v [i] + cov [  , i]

=     v [i] + (     +   )cov [u, i]
+  cov [ x, i] + cov [ y , i]

(22.19)

(22.20)
(22.21)
(22.22)

by condition (ii), however, we must have cov [u, i] = 0, and of course cov [ x, i] =
cov [ y , i] = 0. therefore cov [i, y ] =     v [i]. solving,

   =

cov [i, y ]
cov [i, x]

(22.23)

this can be estimated by substituting in the sample covariances, or any other
consistent estimators of these two covariances.

on the other hand, the (true or population-level) coe   cient for linearly re-

gressing y on x is

cov [x, y ]

v [x]

=

  v [x] +   cov [u, x]

v [x]

=    +   

=    +   

cov [u, x]

v [x]

  v [u ]

  2v [i] +   2v [u ] + v [ x]

(22.24)

(22.25)

(22.26)

that is,    ols is biased for the causal e   ect when x is correlated with the noise   .
in other words, simple regression is misleading in the presence of confounding12.
the instrumental variable i provides a source of variation in x which is un-
correlated with the other common ancestors of x and y . by seeing how both x
and y respond to these perturbations, and using the fact that i only in   uences y
through x, we can deduce something about how x in   uences y , though linearity
is very important to our ability to do so.

12 but observe that if we want to make a linear prediction of y and only have x available, i.e., to    nd

the best r1 in e [y |x = x] = r0 + r1x, then eq. 22.26 is exactly the coe   cient we would want to
use. ols is doing its job.

498

identifying causal e   ects

i

i

x

z

x

s

u

y

u

y

figure 22.8 left: i is not a valid instrument for identifying the e   ect of x
on y , because i can in   uence y through a path not going through x. if we
could control for z, however, i would become valid. right: i is not a valid
instrument for identifying the e   ect of x on y , because there is an
unblocked back-door path connecting i and y . if we could control for s,
however, i would become valid.

the simple line of reasoning above runs into trouble if we have multiple instru-
ments, or need to include controls (as the de   nition of an instrument allows). in
  23.2 we   ll look at the more complicated estimation methods which can handle
this, again assuming linearity.

22.3.3.1 some invalid instruments

not everything which looks like an instrument actually works. if y is indeed a
descendant of i, but there is a line of descent that doesn   t go through x, then i is
not a valid instrument for x (figure 22.8, left). if there are unblocked back-door
paths linking i and y , e.g., if i and y have common ancestors, then i is again
not a valid instrument (figure 22.8, right).

economists sometimes refer to both sets of problems with instruments as    vi-
olations of exclusion restrictions   . the second sort of problem, in particular, is a
   failure of exogeneity   .

22.3.3.2 critique of instrumental variables

by this point, you may well be thinking that instrumental variable estimation is
very much like using the front-door criterion. there, the extra variable m came
between x and y ; here, x comes between i and y . it is, perhaps, surprising (if
not annoying) that using an instrument only lets us identify causal e   ects under
extra assumptions, but that   s (mathematical) life. just as the front-door criterion
relies on using our scienti   c knowledge, or rather theories, to    nd isolated and
exhaustive mechanisms,    nding valid instruments relies on theories about the

22.3 identi   cation strategies

499

part of the world under investigation, and one would want to try to check those
theories.

in fact, instrumental variable estimates of causal e   ects are often presented as
more or less unquestionable, and free of theoretical assumptions; economists, and
other social scientists in   uenced by them, are especially apt to do this. as the
economist daniel davies puts it13, devotees of this approach

have a really bad habit of saying:
   whichever way you look at the numbers, x   .
when all they can really justify is:
   whichever way i look at the numbers, x   .
but in fact, i should have said that they could only really support:
   whichever way i look at these numbers, x   .

(emphasis in the original.) it will not surprise you to learn that i think this is
very wrong.

i hope that, by this point in the book, if someone tries to sell you a linear
regression, you should be very skeptical, but let   s leave that to one side. (it   s
possible that the problem at hand really is linear.) the clue that instrumental
variable estimation is a creature of theoretical assumptions is point (ii) in the
de   nition of an instrument: i        y |s, do(x). this says that if we eliminate all
the arrows into x, the control variables s block all the other paths between i and
y . this is exactly as much an assertion about mechanisms as what we have to
do with the front-door criterion. in fact it doesn   t just say that every mechanism
by which i in   uences y is mediated by x, it also says that there are no common
causes of i and y (other than those blocked by s).

this assumption is most easily defended when i is genuinely random, for
instance, if we do a randomized experiment, i might be a coin-toss which assigns
each subject to be in either the treatment or control group, each with a di   erent
value of x. if    compliance    is not perfect (if some of those in the treatment group
don   t actually get the treatment, or some in the control group do), it is nonetheless
often plausible that the only route by which i in   uences the outcome is through
x, so an instrumental variable regression is appropriate. (i here is sometimes
called    intent to treat   .)

even here, we must be careful. if we are evaluating a new medicine, whether
people think they are getting a medicine or not could change how they act, and
medical outcomes. knowing whether they were assigned to the treatment or the
control group would thus create another path from i to y , not going through
x. this is why randomized clinical trials are generally    double-blinded    (neither
patients nor medical personnel know who is in the control group); but whether the
steps taken to double-blind the trial actually worked is itself a causal assumption.
more generally, any argument that a candidate instrument is valid is really an
argument that other channels of information    ow, apart from the favored one
through x, can be ruled out. this generally cannot be done through analyzing

13 in part four of his epic and insightful review of freakonomics; see

http://d-squareddigest.blogspot.com/2007/09/freakiology-yes-folks-its-part-4-of.html.

500

identifying causal e   ects

the same variables used in the instrumental-variable estimation (see below), but
involves theories about the world, and rests on the strength of the evidence for
those theories. as has been pointed out multiple times     e.g., by rosenzweig and
wolpin (2000) and deaton (2010)     the theories needed to support instrumental
variable estimates in particular concrete cases are often not very well-supported,
and plausible rival theories can produce very di   erent conclusions from the same
data.
many people have thought that one can test for the validity of an instrument,
by looking at whether i        y |x     the idea being that, if in   uence    ows from i
through x to y , conditioning on x should block the channel. the problem is that,
in the instrumental-variable set-up, x is a collider on the path i     x     u     y ,
so conditioning on x actually creates an indirect dependence between i and y
even if i is valid. so i (cid:54)       y |x, whether or not the instrument is valid, and the
test (even if done perfectly with in   nite data) tells us nothing14.

a    nal, more or less technical, issue with instrumental variable estimation is
that many instruments are (even if valid) weak     they only have a little in   uence
on x, and a small covariance with it. this means that the denominator in eq.
22.23 is a number close to zero. error in estimating the denominator, then, results
in a much larger error in estimating the ratio. weak instruments lead to noisy
and imprecise estimates of causal e   ects. it is not hard to construct scenarios
where, at reasonable sample sizes, one is actually better o    using the biased ols
estimate than the unbiased but high-variance instrumental estimate15.

22.3.3.3 instrumental variables and integral equations

i said above (p. 22.3.3) that, in general, identifying causal e   ects through in-
strumental variables means solving integral equations. it   s worth exploring that,
because it provides some insight into how instrumental variables works, especially
for non-linear systems. since this is somewhat mathematically involved, however,
you may want to skip this section on    rst reading.

to grasp what it means to identify causal e   ects by solving integral equations,
let   s start with the most basic set up, where the cause x, the e   ect y , and
the instrument i are all binary. there are then really only two numbers that
need to be identi   ed, pr (y = 1|do(x = 0)) and pr (y = 1|do(x = 1)). eq. 22.15
becomes now a system of equations involving these e   ects:
pr (y = 1|do(i = 0)) = pr (y = 1|do(x = 0)) pr (x = 0|do(i = 0)) + pr (y = 1|do(x = 1)) pr (x = 1|do(i = 0))
pr (y = 1|do(i = 0)) = pr (y = 1|do(x = 0)) pr (x = 0|do(i = 1)) + pr (y = 1|do(x = 1)) pr (x = 1|do(i = 1))

the left-hand sides are identi   able (by the assumptions on i), as are the prob-
abilities pr (x|do(i)). so, once we get those, we have a system of two linear
equations with two unknowns, pr (y = 1|do(x = 0)) and pr (y = 1|do(x = 1)).
14 however, see pearl (2009b,   8.4) for a di   erent approach which can    screen out very bad would-be

instruments   .

15 young (2017) re-analyzes hundreds of published papers in economics to argue that this is scenario is

actually rather common.

22.3 identi   cation strategies

501

since there are as many equations as unknowns, there is a unique solution, unless
the equations are redundant (exercise 22.4).

if we put together some vectors and matrices,

(cid:21)
(cid:21)

pr (y = 1|do(i = i))

(cid:20) pr (y = 1|do(i = 0))
(cid:20) pr (y = 1|do(x = 0))
(cid:20) pr (x = 0|do(i = 0)) pr (x = 1|do(i = 0))

pr (y = 1|do(x = 1))

pr (x = 0|do(i = 1)) pr (x = 1|do(i = 1))

(cid:126)fi   y    

(cid:126)fx   y    

fi   x    

(22.28)

(22.29)

(22.30)

(cid:21)

then eq. 22.27 becomes

(cid:126)fi   y = fi   x
and we can make the following observations:
1. the e   ect of the instrument i on the response y , (cid:126)fi   y , is a linear transfor-

(cid:126)fx   y

(22.31)

mation of the desired causal e   ects, (cid:126)fx   y .

2. getting those desired e   ects requires inverting a linear operator, the matrix

fi   x.

3. that inversion is possible if, and only if, all of the eigenvalues of fi   x are

non-zero.

there is nothing too special about the all-binary case, except that we can write
everything out explicitly. if the cause, e   ect and instrument are all categorical,
with the number of levels being cx, cy and ci respectively, then there are (cy   1)cx
parameters to identify, and eq. 22.15 leads to a system of (cy     1)ci equations,
so the e   ects will be identi   able (in general) so long as ci     cx. there will, once
again, be a matrix form of the system of equations, and solving the system means
inverting a matrix in whose entries are the e   ects of i on x, pr (x = x|do(i = i)).
this, in turn, is something we can do so long as all of the eigenvalues are non-zero.
in the continuous case, we will replace our vectors by conditional density func-

tions:

fi   y (y|i)     f (y|do(i = i))
fx   y (y|x)     f (y|do(x = x))
fi   x(x|i)     f (x|do(i = i))

eq. 22.15 now reads

fi   y (y|i) =

fx   y (y|x)fi   x(x|i)dx

this is linear in the desired function fx   y , so we de   ne the linear operator

  h    

h(x)fi   x(x|i)dx

(cid:90)
(cid:90)

and re-write eq. 22.15 one last time as

fi   y =   fi   x

(22.32)
(22.33)
(22.34)

(22.35)

(22.36)

(22.37)

502

identifying causal e   ects

which could be solved by

     1fi   y = fi   x

(22.38)

an operator like    is called an    integral operator   , and equations like eq. 22.35
or 22.37 are    integral equations   .

if we take eq. 22.15, multiply both sides by y, and sum (or integrate) over all

possible y, we get

e [y |do(i = i)] =

=

(cid:88)
(cid:88)

x

y

(cid:88)
(cid:88)
(cid:88)

x

y

ypr (y = y|do(x = x)) pr (x = x|do(i = i))(22.39)

ypr (y = y|do(x = x)) pr (x = x|do(i = i))(22.40)

pr (x = x|do(i = i)) e [y |do(x = x)]

=
=   e [y |do(x)]

x

(22.41)

(22.42)

so, again, the conditional expectations (= average causal e   ects) we   d like to
identify can be obtained by solving a linear integral equation. this doesn   t require
that either the functions e [y |do(i = i)] or e [y |do(x = x)] be linear (in i and x,
respectively), it just follows from the markov property16.

22.3.4 failures of identi   cation

the back-door and front-door criteria, and instrumental variables, are all su   -
cient for estimating causal e   ects from probabilistic distributions, but are not
necessary. a necessary condition for un-identi   ability is the presence of an un-
blockable back-door path from x to y . however, this is not su   cient for lack of
identi   cation     we might, for instance, be able to use the front door criterion, as
in figure 22.4. there are necessary and su   cient conditions for the identi   ability
of causal e   ects in terms of the graph, and so for un-identi   ability, but they are
rather complex and i will not go over them (see shpitser and pearl (2008), and
pearl (2009b,     3.4   3.5) for an overview).

as an example of the unidenti   able case, consider figure 22.9. this dag
depicts the situation analyzed in christakis and fowler (2007), a famous paper
claiming to show that obesity is contagious in social networks (at least in the
suburb of boston where the data was collected). at each observation, participants
in the study get their weight taken, and so their obesity status is known over time.
they also provide the name of a friend. this friend is often in the study. christakis
and fowler were interested in the possibility that obesity is contagious, perhaps
through some process of behavioral in   uence. if this is so, then irene   s obesity
status in year 2 should depend on joey   s obesity status in year one, but only if
irene and joey are friends     not if they are just random, unconnected people. it

16 in fact, one reason the markov property is important in studying dynamics is that it lets us move
from studying non-linear individual trajectories to the linear evolution of id203 distributions
(lasota and mackey, 1994).

22.3 identi   cation strategies

503

irene   s\nlatent\ntraits

joey   s\nlatent\ntraits

was irene\nobese\nlast year?

is joey\n irene   s friend?

was joey\nobese\nlast year?

???

is irene\nobese\nthis year?

is joey\nobese\nthis year?

figure 22.9 social in   uence is confounded with selecting friends with
similar traits, unobserved in the data.

is indeed the case that if joey becomes obese, this predicts a substantial increase
in the odds of joey   s friend irene becoming obese, even controlling for irene   s
previous history of obesity17.

the di   culty arises from the latent variables for irene and joey (the round
nodes in figure 22.9). these include all the traits of either person which (a)
in   uence who they become friends with, and (b) in   uence whether or not they
become obese. a very partial list of these would include: taste for recreational ex-
ercise, opportunity for recreational exercise, taste for alcohol, ability to consume
alcohol, tastes in food, occupation and how physically demanding it is, ethnic
background18, etc. put simply, if irene and joey are friends because they spend
two hours in the same bar every day drinking and eating chicken wings with
ranch dressing, it   s less surprising that both of them have an elevated chance of

17 the actual analysis was a bit more convoluted than that, but this is the general idea.
18 friendships often run within ethnic communities. on the one hand, this means that friends tend to
be more genetically similar than random members of the same town, so they will be usually apt to
share genes which in   uence susceptibility to obesity (in that environment). on the other hand,
ethnic communities transmit, non-genetically, traditions regarding food, alcohol, sports, exercise,
etc., and (again non-genetically: tilly (1998)) in   uence employment and housing opportunities.

504

identifying causal e   ects

becoming obese, and likewise if they became friends because they both belong to
the decathlete   s club, they are both unusually unlikely to become obese. irene   s
status is predictable from joey   s, then, not (or not just) because joey in   uences
irene, but because seeing what kind of person irene   s friends are tells us about
what kind of person irene is. it is not too hard to convince oneself that there
is just no way, in this dag, to get at the causal e   ect of joey   s behavior on
irene   s that isn   t confounded with their latent traits (shalizi and thomas, 2011).
to de-confound, we would need to actual measure those latent traits, which may
not be impossible but is certainly was not done here19.

when identi   cation is not possible     when we can   t de-confound     it may
still be possible to bound causal e   ects. that is, even if we can   t say exactly that
pr (y |do(x = x)) must be, we can still say it has to fall within a certain (non-
trivial!) range of possibilities. the development of bounds for non-identi   able
quantities, what   s sometimes called partial identi   cation, is an active area of
research, which i think is very likely to become more and more important in data
analysis; the best introduction i know is manski (2007).

22.4 summary

of the four techniques i have introduced, instrumental variables are clever, but
fragile and over-sold20. experimentation is ideal, but often unavailable. the back-
door and front-door criteria are, i think, the best observational approaches, when
they can be made to work.

often, nothing can be made to work. many interesting causal e   ects are just not
identi   able from observational data. more exactly, they only become identi   able
under very strong modeling assumptions, typically ones which cannot be tested
from the same data, and sometimes ones which cannot be tested by any sort of
empirical data whatsoever. sometimes, we have good reasons (from other parts
of our scienti   c knowledge) to make such assumptions. sometimes, we make such
assumptions because we have a pressing need for some basis on which to act, and
a wrong guess is better than nothing21. if you do make such assumptions, you
need to make clear that you are doing so, and what they are; explain your reasons
for making those assumptions, and not others22; and indicate how di   erent your
conclusions could be if you made di   erent assumptions.

19 of course, the issue is not really about obesity. studies of    viral marketing   , and of social in   uence

more broadly, all generically have the same problem. predicting someone   s behavior from that of
their friend means conditioning on the existence of a social tie between them, but that social tie is a
collider, and activating the collider creates confounding.

20 i confess that i would probably not be so down on them if others did not push them up so

excessively.

21 as i once heard a distinguished public health expert put it,    this problem is too important to

worry about getting it right.   

22    my boss/textbook says so    and    so i can estimate       are not good reasons

exercises

505

22.4.1 further reading

my presentation of the three major criteria is heavily indebted to morgan and
winship (2007), but i hope not a complete rip-o   . pearl (2009b) is also essential
reading on this topic. berk (2004) provides an excellent critique of naive (that is,
overwhelmingly common) uses of regression for estimating causal e   ects.

most econometrics texts devote considerable space to instrumental variables.
didelez et al. (2010) is a very good discussion of instrumental variable methods,
with less-standard applications. there is some work on non-parametric versions of
instrumental variables (e.g., newey and powell 2003), but the form of the models
must be restricted or they are unidenti   able. on the limitations of instrumen-
tal variables, rosenzweig and wolpin (2000) and deaton (2010) are particularly
recommended; the latter reviews the issue in connection with important recent
work in development economics and the alleviation of extreme poverty, an area
where statistical estimates really do matter.

there is a large literature in the philosophy of science and in methodology on
the notion of    mechanisms   . references i have found useful include, in general,
salmon (1984), and, speci   cally on social processes, elster (1989), hedstr  om and
swedberg (1998) (especially boudon 1998), hedstr  om (2005), tilly (1984, 2008),
and delanda (2006).

exercises

22.1 draw a graphical model representing the situation where a causal variable x is randomized
by an experimenter. verify that pr (y |x = x) is then equal to pr (y |do(x = x)). (hint:
use the back door criterion.)

22.2 prove eq. 22.3, by using the causal markov property of the appropriate surgically-altered

graph.

1. the variable t contains all the parents of x; v contains all variables other than x,

y , and t . explain why

pr(cid:0)y = y, x = x

(cid:48)

, t = t, v = v|do(x = x)(cid:1) =   xx(cid:48)

pr (y = y, x = x, t = t, v = v)

pr (x = x|t = t)

(22.43)

where   ij is the    kronecker delta   , 1 when i = j and 0 when i (cid:54)= j.
hint: the left-hand side of the equation has to factor according to the graph we get
after intervening on x, and the id203 in the numerator on the right-hand side
comes from the graphical model before the intervention. how do they di   er?

2. assuming eq. 22.43 holds, show that

pr(cid:0)y = y, x = x

(cid:48)

, t = t, v = v|do(x = x)(cid:1) =   xx(cid:48) pr (y = y, x = x, t = t, v = v|x = x, t = t) pr (t = t)

hint: pr (a|b) = pr (a, b) /pr (b).

(22.44)

3. assuming eq. 22.44 holds, use the law of total id203 to derive eq. 22.3, i.e., to

derive

pr (y = y|do(x = x)) =

(cid:88)

t

pr (y = y|x = x, t = t) pr (t = t)

(22.45)

506

identifying causal e   ects

class

smoking

job

tar in lungs

asbestos

dental care

cell damage

yellow teeth

cancer

figure 22.10 dag for exercise 22.3.

22.3 refer to figure 22.10. can we use the front door criterion to estimate the e   ect of occu-
pational prestige on cancer? if so, give a set of variables which we would use as mediators.
is there more than one such set? if so, can you    nd them all? are there variables we could
add to this set (or sets) which would violate the front-door criterion?

22.4 solve eq. 22.27 for pr (y = 1|do(x = 0)) and pr (y = 1|do(x = 1)) in terms of the other

conditional probabilities. when is the solution unique?

22.5 (lengthy, conceptual, open-ended) read salmon (1984). when does his    statistical rele-

vance basis    provide enough information to identify causal e   ects?

23

estimating causal e   ects from observations

chapter 22 gave us ways of identifying causal e   ects, that is, of knowing when
quantities like pr (y = y|do(x = x)) are functions of the distribution of observ-
able variables. once we know that something is identi   able, the next question is
how we can actually estimate it from data.

23.1 estimators in the back- and front- door criteria

the back-door and front-door criteria for identi   cation not only show us when
causal e   ects are identi   able, they actually give us formulas for representing the
causal e   ects in terms of ordinary conditional probabilities. when s satis   es the
back-door criterion (chapter ??), we can use parametric density models, we can

model y |x, s = f (x, s) +  y and use regression, etc. if(cid:99)pr (y = y|x = x, s = s)
is a consistent estimator of pr (y = y|x = x, s = s), and (cid:99)pr (s = s) is a consis-

tent estimator of pr (s = s), then

(cid:88)

(cid:99)pr (s = s)(cid:99)pr (y = y|x = x, s = s)

(23.1)

will be a consistent estimator of pr (y |do(x = x)).

s

in principle, i could end this section right here, but there are some special
cases and tricks which are worth knowing about. for simplicity, i will in this
section only work with the back-door criterion, since estimating with the front-
door criterion amounts to doing two rounds of back-door adjustment.

23.1.1 estimating average causal e   ects

because pr(y |do(x = x)) is a id203 distribution, we can ask about e [y |do(x = x)],
when it makes sense for y to have an expectation value; it   s just
y pr(y = y|do(x = x))

e [y |do(x = x)] =

(cid:88)

(23.2)

y

as you   d hope. this is the average e   ect, or sometimes just the e   ect of
do(x = x). while it is certainly not always the case that it summarizes all there
is to know about the e   ect of x on y , it is often useful.

if we identify the e   ect of x on y through the back-door criterion, with control

507

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

508

estimating causal e   ects

variables s, then some algebra shows

y

(cid:88)
(cid:88)
(cid:88)
(cid:88)

y

s

e [y |do(x = x)] =

=

=

=

y pr(y = y|do(x = x))

(cid:88)

y

(cid:88)

s

pr(s = s)

pr(y = y|x = x, s = s) pr(s = s)

y pr(y = y|x = x, s = s)

(23.5)

y

pr(s = s)e [y |x = x, s = s]

(23.6)

(23.3)

(23.4)

s

the inner conditional expectation is just the regression function   (x, s), for when
we try to make a point-prediction of y from x and s, so now all of the regression
methods from part i come into play. we would, however, still need to know the
distribution pr(s), so as to average appropriately. let   s turn to this.

23.1.2 avoiding estimating marginal distributions

we   ll continue to focus on estimating the causal e   ect of x on y using the back-
door criterion, i.e., assuming we   ve found a set of control variables s such that

pr(y = y|do(x = x)) =

pr(y = y|x = x, s = s) pr(s = s)

(23.7)

(cid:88)

s

s will generally contain multiple variables, so we are committed to estimating
two potentially quite high-dimensional distributions, pr(s) and pr(y |x, s). even
assuming that we knew all the distributions, just enumerating possible values s
and summing over them would be computationally demanding. (similarly, if s
is continuous, we would need to do a high-dimensional integral.) can we reduce
these burdens?

one useful short-cut is to use the law of large numbers, rather than exhaustively
enumerating all possible values of s. notice that the left-hand side    xes y and x,
so pr(y = y|x = x, s = s) is just some function of s. if we have an iid sample
of realizations of s, say s1, s2, . . . sn, then the law of large numbers says that, for
all well-behaved function f ,

f (s) pr(s = s)

(23.8)

therefore, with a large sample,

pr(y = y|do(x = x))     1
n

i=1

pr(y = y|x = x, s = si)

(23.9)

and this will still be (approximately) true when we use a consistent estimate of
the id155, rather than its true value.
the same reasoning applies for estimating e [y |do(x = x)]. moreover, we can
use the same reasoning to avoid explicitly summing over all possible s if we

n(cid:88)

i=1

1
n

f (si)    (cid:88)
n(cid:88)

s

23.1 estimators in the back- and front- door criteria

509

do have pr(s), by simulating from it1. even if our sample (or simulation) is
not completely iid, but is statistically stationary, in the sense we will cover in
chapter 25 (strictly speaking:    ergodic   ), then we can still use this trick.
none of this gets us away from having to estimate pr(y |x, s), which is still

going to be a high-dimensional object, if s has many variables.

23.1.3 matching

suppose that our causal variable of interest x is binary, or (almost equivalent)
that we are only interested in comparing the e   ect of two levels, do(x = 1) and
do(x = 0). let   s call these the    treatment    and    control    groups for de   nite-
ness, though nothing really hinges on one of them being in any sense a normal or
default value (as    control    suggests)     for instance, we might want to know not
just whether men get paid more than women, but whether they are paid more
because of their sex2. in situations like this, we are often not so interested in the
full distributions pr (y |do(x = 1)) and pr (y |do(x = 0)), but just in the expec-
tations, e [y |do(x = 1)] and e [y |do(x = 0)]. in fact, we are often interested just
in the di   erence between these expectations, e [y |do(x = 1)]     e [y |do(x = 0)],
what is often called the average treatment e   ect, or ate.

suppose we are the happy possessors of a set of control variables s which satisfy
the back-door criterion. how might we use them to estimate this average causal
e   ect?

pr (s = s) e [y |x = 1, s = s]    (cid:88)

pr (s = s) e [y |x = 0, s = s]
(23.10)

pr (s = s) (e [y |x = 1, s = s]     e [y |x = 0, s = s])

s

(23.11)

(cid:88)
(cid:88)

s

at e =

=

s

1 this is a    monte carlo    approximation to the full expectation value.
2 the example is both imperfect and controversial. it is imperfect because biological sex (never mind
socio-cultural gender) is not quite binary, even in mammals, though the exceptional cases are quite
rare. (see dreger 1998 for a historical perspective.) it is controversial because many statisticians
insist that there is no sense in talking about causal e   ects unless there is some actual manipulation
or intervention one could do to change x for an actually-existing    unit        see, for instance,
holland (1986), which seems to be the source of the slogan    no causation without manipulation   . i
will just note that (i) this is the kind of metaphysical argument which statisticians usually avoid (if
we can   t talk about sex or race as causes, because changing those makes the subject a    di   erent
person   , how about native language? the shape of the nose? hair color? whether they go to college?
age at which they started school? grades in school?); (ii) genetic variables are highly manipulable
with modern experimental techniques, though we don   t use those techniques on people; (iii) real
scientists routinely talk about causal e   ects with no feasible manipulation (e.g.,    continental drift
causes earthquakes   ), or even imaginable manipulation (e.g.,    the solar system formed because of
gravitational attraction   ). it may be merely coincidence that (iv) many of the statisticians who
make such pronouncements work or have worked for the educational testing service, an
organization with an interest in asserting that, strictly speaking, sex and race cannot have any
causal role in the score anyone gets on the sat. (points (i)   (iii) follow glymour (1986); glymour
and glymour (2014); marcellesi (2013).)

510
abbreviate e [y |x = x, s = s] as   (x, s), so that the average treatment e   ect is

estimating causal e   ects

(  (1, s)       (0, s))pr (s = s) = e [  (1, s)       (0, s)]

(23.12)

(cid:88)

s

suppose we got to observe   . then we could use the law of large numbers argu-
ment above to say

  (1, si)       (0, si)

(23.13)

n(cid:88)

i=1

at e     1
n

of course, we don   t get to see either   (1, si) or   (0, si). we don   t even get to see
  (xi, si). at best, we get to see yi =   (xi, si) +  i, with  i being mean-zero noise.
estimator of the regression function, (cid:98)  , would do. if, for some reason, you were
clearly, we need to estimate   (1, si)       (0, si). in principle, any consistent

scared of doing a regression, however, the following scheme might occur to you:
first,    nd all the units in the sample with s = s, and compare the mean y
for those who are treated (x = 1) to the mean y for those who are untreated
(x = 0). writing the the set of units with x = 1 and s = s as ts, and the set of
(cid:32)
units with x = 0 and s = s as cs, then
(cid:88)
1
(cid:32)
|ts|
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
yi     1
|cs|
(cid:88)
  (1, s) +  i     1
|cs|

1
|ts|
j   cs
(  (1, s)       (0, s))pr (s = s) +

(cid:33)
(cid:88)

  (0, s) +  j

(cid:88)

(cid:88)

pr (s = s)

pr (s = s)

pr (s = s)

(23.15)

(23.16)

(23.14)

(cid:33)

(cid:32)

(cid:33)

j   cs

i   ts

i   ts

yj

=

=

s

s

 j

1
|ts|

i   ts

 i     1
|cs|

j   cs

s

s

the    rst part is what we want, and the second part is an average of noise terms,
so it goes to zero as n        . thus we have a consistent estimator of the average
treatment e   ect.

we could however go further. take any unit i where x = 1; it has some value
si for the covariates. suppose we can    nd another unit i    with the same value of
the covariates, but with s = 0. then

yi     yi    =   (1, si) +  i       (0, si)      i   

(23.17)

the comparison between the response of the treated unit and this matched
control unit is an unbiased estimate of   (1, si)       (0, si). if we can    nd a match
i    for every unit i, then

n(cid:88)
yi     yi   
(cid:88)

i=1
1
n

1
n

=

i = 1n  (1, si)       (0, si) +

n(cid:88)

i=1

1
n

 i

(23.18)

(23.19)

the    rst average is, by the law-of-large-numbers argument, approximately the

23.1 estimators in the back- and front- door criteria

511

average treatment e   ect, and the second is the average of noise terms, so it should
be going to zero as n        . thus, matching gives us a consistent estimate of the
average treatment e   ect, without any explicit regression. instead, we rely on a
paired comparison, because members of the treatment group are being compared
to with members of the control group with matching values of the covariates s.
this often works vastly better than estimating    through a linear model.

there are three directions to go from here. one is to deal with all of the
technical problems and variations which can arise. we might match each unit
against multiple other units, to get further noise reduction. if we can   t    nd an
exact match, the usual approach is to match each treated unit against the control-
group unit with the closest values of the covariates. exploring these details is
important to applications, but we won   t follow it up here (see further readings).
a second direction is to remember that matching does not solve the identi   -
cation problem. computing eq. 23.19 only gives us an estimate of the average
treatment e   ect if s satis   es the back-door criterion. if s does not, then even
if matching is done perfectly, eq. 23.19 does nothing of any particular interest.
matching is one way of estimating identi   ed average treatment e   ects; it con-
tributes nothing to solving identi   cation problems.
third, and    nally, matching is really doing nearest neighbor regression (  1.5.1).
to get the di   erence between the responses of treated and controlled units, we   re
comparing each treated unit to the control-group unit with the closest values of
the covariates. when people talk about matching estimates of average treatment
e   ects, they usually mean that the number of nearest neighbors we use for each
treated unit is    xed as n grows.

once we realize that matching is really just nearest-neighbor regression, it may
become less compelling; at the very least many issues should come to mind. as we
saw in   1.5.1, to get consistent estimates of    out of k-nearest neighbors, we need

to let k grow (slowly) with n. if k is    xed, then the bias of (cid:98)  (x, s) is either zero
or goes quickly to zero as n grows (quicker the smaller k is), but v [(cid:98)  x, s] (cid:54)    0

as n        . if all we want to do is estimate the average treatment e   ect, this
remaining asymptotic variance at each s will still average out, but it would be
a problem if we wanted to look at anything more detailed. more generally, the
bias-variance tradeo    is a tradeo   , and it   s not always a good idea to prioritize
low bias over anything else. moreover, it   s not exactly clear that we should use
a    xed k, or for that matter should use nearest neighbors instead of any other
consistent regression method.

nearest neighbor regression, like every other nonparametric method, is subject
to the curse of dimensionality3; therefore, so is matching4. it would be very nice

3 an important caveat: when s is high-dimensional but all the data fall on or very near a

low-dimensional sub-space, nearest neighbor regression will adapt to this low e   ective
dimensionality (kpotufe, 2011). not all regression methods have this nice property.

4 if we can could do matching easily for high-dimensional s, then we could match treated units to
other treated units, and control-group units to control-group units, and do easy high-dimensional
regression. since we know high-dimensional regression is hard, and we just reduced regression to
matching, high-dimensional matching must be at least as hard.

512

estimating causal e   ects

if there was some way of lightening the curse when estimating treatment e   ects.
we   ll turn to that next.

23.1.4 propensity scores

the problems of having to estimate high-dimensional conditional distributions
and of averaging over large sets of control values are both reduced if the set
of control variables has in fact only a few dimensions. if we have two sets of
control variables, s and r, both of which satisfy the back-door criterion for
identifying pr (y |do(x = x)), all else being equal we should use r if it contains
fewer variables than s5

an important special instance of this is when we can set r = f (s), for some

function s, and have

x        s|r

(23.20)

in the jargon, r is a su   cient statistic6 for predicting x from s. to see why
this matters, suppose now that we try to identify pr (y = y|do(x = x)) from a
back-door adjustment for r alone, not for s. we have7

pr (y |x = x, r = r) pr (r = r)

pr (y, s = s|x = x, r = r) pr (r = r)

pr (y |x = x, r = r, s = s) pr (s = s|x = x, r = r) pr (r = r)(23.22)

pr (y |x = x, s = s) pr (s = s|x = x, r = r) pr (r = r)

pr (y |x = x, s = s) pr (s = s|r = r) pr (r = r)

(cid:88)

pr (y |x = x, s = s)
pr (y |x = x, s = s) pr (s = s)

r

pr (s = s, r = r)

=
= pr (y |do(x = x))

s

that is to say, if s satis   es the back-door criterion, then so does r. since r is a
function of s, both the computational and the statistical problems which come
from using r are no worse than those of using s, and possibly much better, if r
has much lower dimension.

5 other things which might not be equal: the completeness of data on r and s; parametric

assumptions might be more plausible for the variables in s, giving a better rate of convergence; we
might be more con   dent that s really does satisfy the back-door criterion.

6 this is not the same sense of the word    su   cient    as in    causal su   ciency   .
7 going from eq. 23.22 to eq. 23.23 uses the fact that r = f (s), so conditioning on both r and s is

the same as just conditioning on s. going from eq. 23.23 uses the fact that s        x|r.

(23.21)

(23.23)

(23.24)

(23.25)

(23.26)

(23.27)

r

r,s

=

=

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

=

=

=

r,s

r,s

r,s

s

23.1 estimators in the back- and front- door criteria

513

it may seem far-fetched that such a summary score should exist, but really all
that   s required is that some combinations of the variables in s carry the same
information about x as the whole of s does. consider for instance, the set-up
where

x     p(cid:88)

vj +  x

j=1

y     f (x, v1, v2, . . . vp) +  y

(23.28)

(23.29)

on all of them. however, if r =(cid:80)p

to identify the e   ect of x on y , we need to block the back-door paths between
them. each one of the vj provides such a back-door path, so we need to condition
j=1 vj, then x        {v1, v2, . . . vp}|r, so we could

reduce a p-dimensional set of control variables to a one-dimensional set.

often, as here,    nding summary scores will depend on the functional form,
and so not be available in the general, non-parametric case. there is, however,
an important special case where, if we can use the back-door criterion at all, we
can use a one-dimensional summary.
this is the case where x is binary. if we set f (s) = pr (x = 1|s = s), and then
take this as our summary r, it is not hard to convince oneself that x        s|r
(exercise 23.1). this f (s) is called the propensity score. it is remarkable, and
remarkably convenient, that an arbitrarily large set of control variables s, perhaps
with very complicated relationships with x and y , can always be boiled down
to a single number between 0 and 1, but there it is.

that said, except in very special circumstances, there is no analytical formula
for f (s). this means that it must be modeled and estimated. the most common
model used is id28, but so far as i can see this is just because many
people know no other way to model a binary outcome. since accurate propensity
scores are needed to make the method work, it would seem to be worthwhile
to model r very carefully, and to consider gam or fully non-parametric esti-
mates. if s contains a lot of variables, then estimating pr (x = 1|s = s) is a
high-dimensional regression problem, and so itself subject to the curse of dimen-
sionality.

23.1.5 propensity score matching

if the number of covariates in s is large, the curse of dimensionality settles upon
us. many values of s will have few or no individuals at all, let alone a large
number in both the treatment and the control groups. even if the real di   erence
e [y |x = 1, s = s]     e [y |x = 0, s = s] is small, with only a few individuals in
either sub-group we could easily get a large di   erence in sample means. and of
course with continuous covariates in s, each individual will generally have no
exact matches at all.

the very clever idea of rosenbaum and rubin (1983) is to solve this by match-
ing not on s, but on the propensity score de   ned in the last section. we have seen
already that when x is binary, adjusting for the propensity score is just as good

514

estimating causal e   ects

as adjusting for the full set of covariates s. it is easy to double-check (exercise

23.2) that(cid:88)
(cid:88)

s

=

pr (s = s) (e [y |x = 1, s = s]     e [y |x = 0, s = s])

pr (r = r) (e [y |x = 1, r = r]     e [y |x = 0, r = r])

(23.30)

r

when r = pr (x = 1|s = s), so we lose no essential information by matching
on the propensity score r rather than on the covariates s. intuitively, we now
compare each treated individual with one who was just as likely to have received
the treatment, but, by chance, did not8. on average, the di   erences between such
matched individuals have to be due to the treatment.

what have we gained by doing this? since r is always a one-dimensional vari-
able, no matter how big s is, it is going to be much easier to    nd matches on r
than on s. this does not actually break the curse of dimensionality, but rather
shifts its focus, from the regression of y on x and s to the regression of x on
s. still, this can be a very real advantage.

it is important to be clear, however, that the gain here is in computational
tractability and (perhaps) statistical e   ciency, not in fundamental identi   cation.
with r = pr (x = 1|s = s), it will always be true that x        s|r, whether or
not the back-door criterion is satis   ed. if the criterion is satis   ed, in principle
there is nothing stopping us from using matching on s to estimate the e   ect,
except our own impatience. if the criterion is not satis   ed, having a compact
one-dimensional summary of the wrong set of control variables is just going to
let us get the wrong answer faster.

some confusion seems to have arisen on this point, because, conditional on
the propensity score, the treated group and the control group have the same
distribution of covariates. (again, recall that x        s|r.) since treatment and
control groups have the same distribution of covariates in a randomized experi-
ment, some people have concluded that propensity score matching is just as good
as randomization9. this is emphatically not the case.

the propensity score matching method has become incredibly popular since
rosenbaum and rubin (1983), and there are a huge number of implementations
of various versions of it. the optmatch package in r is notable for doing the
actual matching in an extremely    exible and e   cient way, but leaves de   ning
matching criteria largely to the user (hansen and klopfer, 2006). the matchit
package (ho et al., 2011) includes more tools for actually calculating propensity
scores or other mesures of similarity, and then doing the matching. see stuart
(2010) for a fairly recent listing of relevant software in r and other languages.

8 methods of approximate matching often work better on propensity scores than on the full set of

covariates, because the former are lower-dimensional.

9 these people do not include rubin and rosenbaum, but it is easy to see how their readers could

come away with this impression. see pearl (2009b,   11.3.5), and especially pearl (2009a).

23.2 instrumental-variables estimates

515

23.2 instrumental-variables estimates

  22.3.3 introduced the idea of using instrumental variables to identify causal
e   ects. roughly speaking, i is an instrument for identifying the e   ect of x on
y when i is a cause of x, but the only way i is associated with y is through
directed paths which go through x. to the extent that variation in i predicts
variation in x and y , this can only be because x has a causal in   uence on y .
more precisely, given some controls s, i is a valid instrument when i (cid:54)       x|s,
and every path from i to y left open by s has an arrow into x.

in the simplest case, of figure 22.7, we saw that when everything is linear, we

can    nd the causal coe   cient of y on x as

   =

cov [i, y ]
cov [i, x]

(23.31)

a one-unit change in i causes (on average) an   -unit change in x, and an     -unit
change in y , so    is, as it were, the gearing ratio or leverage of the mechanism
connecting i to y .

estimating    by plugging in the sample values of the covariances into eq. 23.31
is called the wald estimator of   . in more complex situations, we might have
multiple instruments, and be interested in the causal e   ects of multiple variables,
and we might have to control for some covariates to block undesired paths and
get valid instruments. in such situations, the wald estimator breaks down.

there is however a more general procedure which still works, provided the
linearity assumption holds. this is called two-stage regression, or two-stage
least squares (2sls).

1. regress x on i and s. call the    tted values   x.
2. regress y on   x and s, but not on i. the coe   cient of y on   x is a consistent

estimate of   .

the logic is very much as in the wald estimator: conditional on s, variations in
i are independent of the rest of the system. the only way they can a   ect y is
through their e   ect on x. in the    rst stage, then, we see how much changes in
the instruments a   ect x. in the second stage, we see how much these i-caused
changes in x change y ; and this gives us what we want.

to actually prove that this works, we would need to go through some heroic
id202 to show that the population version of the two-stage estimator is
actually equal to   , and then a straight-forward argument that plugging in the
appropriate sample covariance matrices is consistent. the details can be found
in any econometrics textbook, so i   ll skip them. (but see exercise 23.4.)
as mentioned in   23.2, there are circumstances where it is possible to use in-
strumental variables in nonlinear and even nonparametric models. the technique
becomes far more complicated, however, because    nding pr (y = y|do(x = x))
requires solving eq. 22.15,
pr (y |do(i = i)) =

pr (y |do(x = x)) pr (x = x|do(i = i))

(cid:88)

x

(cid:88)

x

(cid:90)

516
estimating causal e   ects
and likewise    nding e [y |do(x = x)] means solving

e [y |do(i = i)] =

e [y |do(x = x)] pr (x = x|do(i = i))

(23.32)

when, as is generally the case, x is continuous, we have rather an integral equa-
tion,

e [y |do(i = i)] =

e [y |do(x = x)] p(x|do(i = i))dx

(23.33)

solving such integral equations is not (in general) impossible, but it is hard,
and the techniques needed are much more complicated than even two-stage least
squares. i will not go over them here, but see li and racine (2007, chs. 16   17).

23.3 uncertainty and id136

the point of the identi   cation strategies from chapter 22 is to reduce the problem
of causal id136 to that of ordinary statistical id136. having done so, we can
assess our uncertainty about any of our estimates of causal e   ects the same way
we would assess any other statistical id136. if we want con   dence intervals
or standard errors for e [y |do(x = 1)]     e [y |do(x = 0)], for instance, we can
treat our estimate of this like any other point estimate, and proceed accordingly.
in particular, we can use the bootstrap (chapter 6), if analytical formulas are
unavailable or unappealing.

the one wrinkle to the use of analytical formulas comes from two-stage least-
squares. taking standard errors, con   dence intervals, etc., for    from the usual
formulas for the second regression neglects the fact that this estimate of    comes
from regressing y on   x, which is itself an estimate and so uncertain. even if this
is handled with some care, two-stage least squares is extraordinarily vulnerable to
any violations in the usual assumptions about iid gaussian errors. young (2017),
reviewing over 1000 (!) instrumental-variable regressions from top economics jour-
nals, shows that this is not merely a theoretical concern, but undermines a huge
amount of the published literature.

23.4 recommendations

instrumental variables are a very clever idea, but they need to be treated with
caution. they only work if the instruments are valid, and that validity is rests
just as much on assumptions about the underlying dag as any of the other
identi   cation strategies. the crucial point, after all, is that the instrument is
an indirect cause of y , but only through x, with no other (unblocked) paths
connecting i to y . this can only too easily fail, if some indirect path has been
neglected. they also require great care in their statistical id136 (young, 2017).
matching, especially propensity score matching, is just as ingenious, and just
as much at the mercy of the correctness of the dag. whether we match di-
rectly on covariates, or indirectly through the propensity score, what matters is

23.5 further reading

517

whether the covariates really block o    the back-door pathways between x and
y . if the covariates block those pathways, well and good; any consistent form
of regression will work, including one called    matching    because    nonparametric
nearest-neighbor smoothing    sounds too scary. if the covariates do not block the
back-door pathways, then no amount of statistical ingenuity is going to help you.
there is a curious divide, among practitioners, between those who lean mostly
on instrumental variables, and those who lean mostly on matching. the former
tend to suspect that (in our terms) the covariates used in matching are not enough
to block all the back-door paths10, and to think that the business is more or less
over once an exogenous variable has been found. the matchers, for their part,
think the instrumentalists are too quick to discount the possibility that their
instruments are connected to y through unmeasured pathways11, but that if you
match on enough variables, you   ve got to block the back-door paths. (they don   t
often worry that they might be conditioning on colliders, or blocking front-door
paths, as they do so.) as is often the case in science, there is much truth to each
faction   s criticism of the other side. you are now in a position to think more clearly
about these matters, and to act more intelligently, than many practitioners.

throughout these chapters, we have been assuming that we know the correct
dag. without such assumptions, or ones equivalent to them, none of these ideas
can be used. in the next chapter, then, we will look at how to actually begin
discovering causal structure from data.

23.5 further reading

the material in   23.1 is largely    folklore   , though see morgan and winship
(2007), which also treats instrumental variable estimation, and a number of
other, more specialized techniques, like    regression discontinuity designs    and

10 as an example for their side, arceneaux et al. (2010) applied matching methods to an actual

experiment, where the real causal relations could be worked out straightforwardly for comparison.
well-conduced propensity-score    matching suggests that [a] pre-election phone call that encouraged
people to wear their seat belts also generated huge increases in voter turnout   . the paper gives a
convincing explanation of where this illusory e   ect comes from, i.e., of what the unblocked
back-door path is, which i will not spoil for you.

11 for instance, a recent and widely-promoted preprint by three economists argued that watching

television caused autism in children. (i leave tracking down the paper as an exercise for the reader.)
the economists used the variation in how much it rains across di   erent locations in california,
oregon and washington as an instrument to predict average tv-watching (x) and its a   ects on the
prevalence of autism (y ). it is certainly plausible that kids watch more tv when it rains, and that
neither tv-watching nor autism causes rain. but this leaves open the question of whether rain and
the prevalence of autism might not have some common cause, and for the west coast in particular
it is easy to    nd one. it is well-established that the risk of autism is higher among children of older
parents, and that more-educated people tend to have children later in life. all three states have, of
course, a striking contrast between large, rainy cities full of educated people (san francisco,
portland, seattle), and very dry, very rural locations on the other side of the mountains. thus there
is a (potential) uncontrolled common cause of rain and autism, namely geographic location, and the
situation is as in figure 22.8.     for a rather more convincing e   ort to apply ideas about causal
id136 to understanding the changing prevalence of autism, see liu et al. (2010).

518

estimating causal e   ects

   di   erence in di   erences   . it does not, however, consider nonparametric regres-
sion methods.

on matching, stuart (2010) is another good review. for some of the asymptotic
theory, including the connection to nearest neighbor methods, see abadie and
imbens (2006).

rubin and waterman (2006) is an extremely clear and easy-to-follow intro-
duction to propensity score matching as a method of causal id136; imbens
and rubin (2015) is a more comprehensive presentation of the estimation work
done by rubin, imbens and collaborators on estimating causal e   ects by match-
ing, propensity scores, and instrumental variables. (much of the original work is
reprinted in rubin 2006.) while sound on theory, its worked examples cannot be
recommended as examples of statistical craft (shalizi, 2016).

king and nielsen (2016) is an interesting argument against matching on propen-
sity scores, in favor of matching on the full set of covariates, related to the extra
variance of estimating the propensity scores.

exercises

23.1 suppose x is binary, and de   ne r = pr (x = 1|s). show that x        s|r. .
23.2 prove eq. 23.30.
23.3 suppose that x has three levels, say 0, 1, 2. let r be the vector (pr (x = 0|s = s) , pr (x = 1|s = s)).

prove that x        s|r. (this is how to generalize propensity scores to non-binary x.)

23.4 for the situation in figure 22.7, prove that the two-stage least-squares estimate of    is

the same as the wald estimate.

24

discovering causal structure from

observations

the last few chapters have, hopefully, convinced you that when you want to do
causal id136, it would help to know the causal graph. we have seen how the
graph would let us calculate the e   ects of actual or hypothetical manipulations of
the variables in the system. furthermore, the graph tells us about what e   ects we
can and cannot identify, and estimate, from observational data. but everything
has posited that we know the graph somehow. this chapter    nally deals with
where the graph comes from.

there are fundamentally three ways to get the dag:

[[attn:
further
examples]]

    prior knowledge
    guessing-and-testing
    discovery algorithms

there is only a little to say about the    rst, because, while it   s important,
it   s not very statistical. as functioning adult human beings, you have a lot of
everyday causal knowledge, which does not disappear the moment you start doing
data analysis. moreover, you are the inheritor of a vast scienti   c tradition which
has, through patient observation, toilsome experiments, ingenious theorizing and
intricate debate, acquired even more causal knowledge. you can and should use
this. someone   s sex or race or caste at birth might be causes of the job they get or
their income at age 30, but not the other way around. running an electric current
through a wire produces heat at a rate proportional to the square of the current.
malaria is due to a parasite transmitted by mosquitoes, and spraying mosquitoes
with insecticides makes the survivors more resistant to those chemicals. all of
these sorts of ideas can be expressed graphically, or at least as constraints on
graphs.

we can, and should, also use graphs to represent scienti   c ideas which are not
as secure as ohm   s law or the epidemiology of malaria. the ideas people work
with in areas like psychology or economics, are really quite tentative, but they are
ideas about the causal structure of parts of the world, and so id114
are implicit in them.

all of which said, even if we think we know very well what   s going on, we will

often still want to check it, and that brings us the guess-and-test route.

519

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

520

discovering causal structure

figure 24.1 a hypothetical causal model in which smoking is associated
with lung disease, but does not cause it. rather, both smoking and lung
disease are caused by common genetic variants. (this idea was due to r. a.
fisher.) smoking is also caused, in this model, by stress.

24.1 testing dags

a graphical causal model makes two kinds of qualitative claims. one is about
direct causation. if the model says x is a parent of y , then it says that changing
x will change the (distribution of) y . if we experiment on x (alone), moving
it back and forth, and yet y is unaltered, we know the model is wrong and can
throw it out.
the other kind of claim a dag model makes is about probabilistic conditional
independence. if s d-separates x from y , then x        y |s. if we observed x, y
and s, and see that x (cid:54)       y |s, then we know the model is wrong and can throw it
out. (more: we know that there is a path linking x and y which isn   t blocked by
s.) thus in the model of figure 24.1, lungdisease        tar|smoking. if lung disease
and tar turn out to be dependent when conditioning on smoking, the model must
be wrong.

this then is the basis for the guess-and-test approach to getting the dag:

    start with an initial guess about the dag.
    deduce conditional independence relations from d-separation.
    test these, and reject the dag if variables which ought to be conditionally

independent turn out to be dependent.

this is a distillation of primary-school scienti   c method: formulate a hypotheses
(the dag), work out what the hypothesis implies, test those predictions, reject
hypotheses which make wrong predictions.

it may happen that there are only a few competing, scienti   cally-plausible
models, and so only a few, competing dags. then it is usually a good idea to
focus on checking predictions which di   er between them. so in both figure 24.1
and in figure 24.2, stress        tar|smoking. checking that independence thus does
nothing to help us distinguish between the two graphs. in particular, con   rming
that stress and tar are independent given smoking really doesn   t give us evidence

smokingtargenesstresslungdisease24.2 testing conditional independence

521

figure 24.2 as in figure 24.1, but now tar in the lungs does cause lung
disease.

for the model from figure 24.1, since it equally follows from the other model. if
we want such evidence, we have to look for something they disagree about.

in any case, testing a dag means testing conditional independence, so let   s

turn to that next.

24.2 testing conditional independence

recall from   20.4 that conditional independence is equivalent to zero conditional
information: x        y |z if and only if i[x; y |z] = 0. in principle, this solves the
problem. in practice, estimating mutual information is non-trivial, and in par-
ticular the sample mutual information often has a very complicated distribution.
you could always bootstrap it, but often something more tractable is desirable.
completely general conditional independence testing is actually an active area
of research. some of this work is still quite mathematical (sriperumbudur et al.,
2010), but it has already led to practical tests (sz  ekely and rizzo, 2009; gretton
et al., 2012; zhang et al., 2011) and no doubt more are coming soon.

if all the variables are discrete, one just has a big contingency table problem,
and could use a g2 or   2 test. if everything is linear and multivariate gaussian,
x        y |z is equivalent to zero partial correlation1. nonlinearly, if x        y |z,
then e [y | z] = e [y | x, z], so if smoothing y on x and z leads to di   erent
predictions than just smoothing on z, conditional independence fails. to reverse
this, and go from e [y | z] = e [y | x, z] to x        y |z, requires the extra as-
sumption that y doesn   t depend on x through its variance or any other moment.
(this is weaker than the linear-and-gaussian assumption, of course.)
pr (y | z). we could check this using non-parametric density estimation, though
we would have to bootstrap the distribution of the test statistic. a more auto-
matic, if slightly less rigorous, procedure comes from the idea mentioned in   14.5:

the conditional independence relation x        y |z is fully equivalent to pr (y | x, z) =

1 recall that the partial correlation between x and y given z is the correlation between x and y ,

after linearly regressing each of them on z separately. that is, it is the correlation of their residuals.

smokingtargenesstresslungdisease522

discovering causal structure

if x is in fact useless for predicting y given z, then an adaptive bandwidth selec-
tion procedure (like cross-validation) should realize that giving any    nite band-
width to x just leads to over-   tting. the bandwidth given to x should tend
to the maximum allowed, smoothing x away altogether. this argument can be
made more formal, and made into the basis of a test (hall et al., 2004; li and
racine, 2007).

24.3 faithfulness and equivalence

in id114, d-separation implies conditional independence: if s blocks
all paths from u to v , then u        v |s. to reverse this, and conclude that if
u        v |s then s must d-separate u and v , we need an additional assumption,
already referred to in   21.2, called faithfulness. more exactly, if the distribution
is faithful to the graph, then if s does not d-separate u from v , u (cid:54)       v |s. the
combination of faithfulness and the markov property means that u        v |s if and
only if s d-separates u and v .
this seems extremely promising. we can test whether u        v |s for any sets
of variables we like. we could in particular test whether each pair of variables is
independent, given all sorts of conditioning variable sets s. if we assume faith-
fulness, when we    nd that x        y |s, we know that s blocks all paths linking x
and y , so we learn something about the graph. if x (cid:54)       y |s for all s, we would
seem to have little choice but to conclude that x and y are directly connected.
might it not be possible to reconstruct or discover the right dag from knowing
all the conditional independence and dependence relations?

this is on the right track, but too hasty. start with just two variables:

x     y     x (cid:54)       y
x     y     x (cid:54)       y

(24.1)
(24.2)

with only two variables, there is only one independence (or dependence) relation
to worry about, and it   s the same no matter which way the arrow points.

similarly, consider these arrangements of three variables:

x     y     z
x     y     z
x     y     z
x     y     z

(24.3)
(24.4)
(24.5)
(24.6)

the    rst two are chains, the third is a fork, the last is a collider. it is not hard
to check (exercise 24.1) that the    rst three dags all imply exactly the same set
of conditional independence relations, which are di   erent from those implied by
the fourth2.
2 in all of the    rst three, x (cid:54)       z but x        z|y , while in the collider, x        z but x (cid:54)       z|y .

remarkably enough, the work which introduced the notion of forks and colliders, reichenbach
(1956), missed this     he thought that x        z|y in a collider as well as a fork. arguably, this one
mistake delayed the development of causal id136 by thirty years or more.

24.4 causal discovery with known variables

523

these examples illustrate a general problem. there may be multiple graphs
which imply the same independence relations, even when we assume faithfulness.
when this happens, the exact same distribution of observables can factor ac-
cording to, and be faithful to, all of those graphs. the graphs are thus said to
be equivalent, or markov equivalent. observations alone cannot distinguish
between equivalent dags. experiment can, of course     changing y alters both
x and z in a fork, but not a chain     which shows that there really is a di   erence
between the dags, just not one observational data can track.

24.3.1 partial identi   cation of e   ects

chapters 22   23 considered the identi   cation and estimation of causal e   ects un-
der the assumption that there was a single known graph. if there are multiple
equivalent dags, then, as mentioned above, no amount of purely observational
data can select a single graph. background knowledge lets us rule out some equiv-
alent dags3, but it may not narrow the set of possibilities to a single graph. how
then are we to actually do our causal estimation?

we could just pick one of the equivalent graphs, and do all of our calculations
as though it were the only possible graph. this is often what people seem to do.
the kindest thing one can say about it is that it shows con   dence; phrases like
   lying by omission    also come to mind.

a more principled alternative is to admit that the uncertainty about the dag
means that causal e   ects are only partially identi   ed. simply put, one does the
estimation in each of the equivalent graphs, and reports the range of results4. if
each estimate is consistent, then this gives a consistent estimate of the range of
possible e   ects. because the e   ects are not fully identi   ed, this range will not
narrow to a single point, even in the limit of in   nite data, but admitting this,
rather than claiming a non-existent precision, is simple scienti   c honesty.

24.4 causal discovery with known variables

section 24.1 talks about how we can test a dag, once we have it. this lets us
eliminate some dags, but still leaves mysterious where they come from in the
   rst place. while in principle there is nothing wrong which deriving your dag
from a vision of serpents biting each others    tails, so long as you test it, it would
be nice to have a systematic way of    nding good models. this is the problem of
model discovery, and especially of causal discovery.

causal discovery is silly with just one variable, and too hard for us with just

two.5

with three or more variables, we have however a very basic principle. if there

3 if we know that x, y and z have to be in either a chain or a fork, with y in the middle, and we
know that x comes before y in time, then we can rule out the fork and the chain x     y     z.
4 sometimes the di   erent graphs will gave the same estimates of certain e   ects. for example, the

chain x     y     z and the fork x     y     z will agree on the e   ect of y on z.

5 but see janzing (2007); hoyer et al. (2009) for some ideas on how you could do it if you   re willing to

524

discovering causal structure

is no edge between x and y , in either direction, then x is neither y    s parent
nor its child. but any variable is independent of its non-descendants given its
parents. thus, for some set6 of variables s, x        y |s (exercise 24.2). if we
assume faithfulness, then the converse holds: if x        y |s, then there cannot be
an edge between x and y . thus, there is no edge between x and y if and only if
we can make x and y independent by conditioning on some s. said another way,
there is an edge between x and y if and only if we cannot make the dependence
between them go away, no matter what we condition on7.

so let   s start with three variables, x, y and z. by testing for independence and
conditional independence, we could learn that there had to be edges between x
and y and y and z, but not between x and z. but conditional independence is a
symmetric relationship, so how could we orient those edges, give them direction?
well, to rehearse a point from the last section, there are only four possible directed
graphs corresponding to that undirected graph:
    x     y     z (a chain);
    x     y     z (the other chain);
    x     y     z (a fork on y );
    x     y     z ( a collision at y )

with the fork or either chain, we have x        z|y . on the other hand, with
the collider we have x (cid:54)       z|y . thus x (cid:54)       z|y if and only if there is a collision
at y . by testing for this conditional dependence, we can either de   nitely orient
the edges, or rule out an orientation. if x     y     z is just a subgraph of a larger
graph, we can still identify it as a collider if x (cid:54)       z|{y, s} for all collections of
nodes s (not including x and z themselves, of course).

with more nodes and edges, we can induce more orientations of edges by
consistency with orientations we get by identifying colliders. for example, suppose
we know that x, y, z is either a chain or a fork on y . if we learn that x     y ,
then the triple cannot be a fork, and must be the chain x     y     z. so orienting
the x     y edge induces an orientation of the y     z edge. we can also sometimes
orient edges through background knowledge; for instance we might know that y
comes later in time than x, so if there is an edge between them it cannot run
from y to x.8 we can eliminate other edges based on similar sorts of background

make some extra assumptions. the basic idea of these papers is that the distribution of e   ects given
causes should be simpler, in some sense, than the distribution of causes given e   ects.

6 possibly empty: conditioning on the empty set of variables is the same as not conditioning at all.
7    no causation without association   , as it were.
8 some have argued, or at least entertained the idea, that the logic here is backwards: rather than

order in time constraining causal relations, causal order de   nes time order. (versions of this idea are
discussed by, inter alia, russell (1927); wiener (1961); reichenbach (1956); pearl (2009b); janzing
(2007) makes a related suggestion). arguably then using order in time to orient edges in a causal
graph begs the question, or commits the fallacy of petitio principii. but of course every syllogism
does, so this isn   t a distinctively statistical issue. (take the classic:    all men are mortal; socrates is
a man; therefore socrates is mortal.    how can we know that all men are mortal until we know
about the mortality of this particular man, socrates? isn   t this just like asserting that tomatoes and
peppers must be poisonous, because they belong to the nightshade family of plants, all of which are

24.4 causal discovery with known variables

525

knowledge: men tend to be heavier than women, but changing weight does not
change sex, so there can   t be an edge (or even a directed path!) from weight to
sex, though there could be one the other way around.

to sum up, we can rule out an edge between x and y whenever we can
make them independent by conditioning on other variables; and when we have
an x     y     z pattern, we can identify colliders by testing whether x and z are
dependent given y . having oriented the arrows going into colliders, we induce
more orientations of other edges.

putting these three things     edge elimination by testing, collider    nding, and
inducing orientations     gives the most basic causal discovery procedure, the
sgs (spirtes-glymour-scheines) algorithm (spirtes et al., 2001,   5.4.1, p. 82).
this assumes:

1. the data-generating distribution has the causal markov property on a graph

g.

2. the data-generating distribution is faithful to g.
3. every member of the population has the same distribution.
4. all relevant variables are in g.
5. there is only one graph g to which the distribution is faithful.

all nodes.
x        y |s; if so, remove the edge between x and y .

abstractly, the algorithm works as follows:
    start with a complete undirected graph on all p variables, with edges between
    for each pair of variables x and y , and each set of other variables s, see if
    find colliders by checking for conditional dependence; orient the edges of col-
    try to orient undirected edges by consistency with already-oriented edges; do

liders.

this recursively until no more edges can be oriented.

pseudo-code is in   24.7.

and the algorithm is correct in its guesses about when variables are conditionally

call the result of the sgs algorithm (cid:98)g. if all of the assumptions above hold,
independent, then (cid:98)g = g. in practice, of course, conditional independence guesses
(cid:98)gn, to indicate that it is based on only n samples. if the conditional independence

are really statistical tests based on    nite data, so we should write the output as

test is consistent, then

lim
n       pr

(cid:16)(cid:98)gn (cid:54)= g
(cid:17)

= 0

(24.7)

in other words, the sgs algorithm converges in id203 on the correct causal
structure; it is consistent for all graphs g. of course, at    nite n, the id203
of error     of having the wrong structure     is (generally!) not zero, but this just

poisonous?) while these philosophical issues are genuinely fascinating, this footnote has gone on
long enough, and it is time to return to the main text.

526

discovering causal structure

means that, like any statistical procedure, we cannot be absolutely certain that
it   s not making a mistake.

one consequence of the independence tests making errors on    nite data can
be that we fail to orient some edges     perhaps we missed some colliders. these

unoriented edges in (cid:98)gn can be thought of as something like a con   dence region

    they have some orientation, but multiple orientations are all compatible with
the data.9 as more and more edges get oriented, the con   dence region shrinks.

if the    fth assumption above fails to hold, then there are multiple graphs g
to which the distribution is faithful. this is just a more complicated version of
the di   culty of distinguishing between the graphs x     y and x     y . all the
graphs in the equivalence class may have some arrows in common; in that case
the sgs algorithm will identify those arrows. if some edges di   er in orientation
across the equivalence class, sgs will not orient them, even in the limit. in terms
of the previous paragraph, the con   dence region never shrinks to a single point,
just because the data doesn   t provide the information needed to do this. the
graph is only partially identi   ed.

if there are unmeasured relevant variables, we can get not just unoriented
edges, but actually arrows pointing in both directions. this is an excellent sign
that some basic assumption is being violated.

24.4.1 the pc algorithm

the sgs algorithm is statistically consistent, but very computationally ine   cient;
the number of tests it does grows exponentially in the number of variables p. this
is the worst-case complexity for any consistent causal-discovery procedure, but
this algorithm just proceeds immediately to the worst case, not taking advantage
of any possible short-cuts.

since it   s enough to    nd one s making x and y independent to remove their
edge, one obvious short-cut is to do the tests in some order, and skip unnecessary
tests. on the principle of doing the easy work    rst, the revised edge-removal step
would look something like this:
    for each x and y , see if x        y ; if so, remove their edge.
    for each x and y which are still connected, and each third variable z con-
nected to x or y , see if x        y |z; if so, remove the edge between x and
y .
    for each x and y which are still connected, and each third and fourth variables
z1 and z2 both connected to x or both connected to y , see if x        y |z1, z2;
if so, remove the edge between x and y .

    . . .
    for each x and y which are still connected at the kth stage, see if there
are k variables z1, z2, . . . zk all connected to x or all connected to y where
x        y |{z1, . . . zk}; if so, remove, the edge between x and y .

9 i say    multiple orientations    rather than    all orientations   , because picking a direction for one edge

might induce an orientation for others.

24.4 causal discovery with known variables

527

    . . .
    stop when k = p     2.

if all the tests are done correctly, this will give the same result as the sgs proce-
dure (exercise 24.4). and if some of the tests give erroneous results, conditioning
on a small number of variables will tend to be more reliable than conditioning on
more (why?).
we can be even more e   cient, however. if x        y |s for any s at all, then
x        y |s(cid:48), where all the variables in s(cid:48) are adjacent to x or y (or both) (exercise
24.3). to see the sense of this, suppose that there is a single long directed path
running from x to y . if we condition on any of the variables along the chain, we
make x and y independent, but we could always move the point where we block
the chain to be either right next to x or right next to y . so when we are trying
to remove edges and make x and y independent, we only need to condition on
variables which are still connected to x and y , not ones in totally di   erent parts
of the graph.
this then gives us the pc10 algorithm (spirtes et al. 2001,   5.4.2, pp. 84   88; see
also   24.7). it works exactly like the sgs algorithm, except for the edge-removal
step, where it tries to condition on as few variables as possible (as above), and only
conditions on adjacent variables. the pc algorithm has the same assumptions as
the sgs algorithm, and the same consistency properties, but generally runs much
faster, and does many fewer statistical tests. it should be the default algorithm
for attempting causal discovery.

24.4.2 causal discovery with hidden variables

suppose that the set of variables we measure is not causally su   cient. could we at
least discover this? could we possibly get hold of some of the causal relationships?
algorithms which can do this exist (e.g., the ci and fci algorithms of spirtes
et al. (2001, ch. 6)), but they require considerably more graph-fu. (the rfci
algorithm (colombo et al., 2012) is a modern, fast successor to fci.) the results
of these algorithms can succeed in removing some edges between observable vari-
ables, and de   nitely orienting some of the remaining edges. if there are actually
no latent common causes, they end up acting like the sgs or pc algorithms.

partial identi   cation of e   ects

when all relevant variables are observed, all e   ects are identi   ed within one
graph; partial identi   cation happens because multiple graphs are equivalent.
when some variables are not observed, we may have to use the identi   cation
strategies to get at the same e   ect. in fact, the same e   ect may be identi   ed in
one graph and not identi   ed in another, equivalent graph. this is, again, unfor-
tunate, but when it happens it needs to be admitted.

10 peter-clark

528

discovering causal structure

24.4.3 on conditional independence tests

the abstract algorithms for causal discovery assume the existence of consistent
tests for conditional independence. the implementations known to me mostly
assume either that variables are discrete (so that one can basically use the   2
test), or that they are continuous, gaussian, and linearly related (so that one
can test for vanishing partial correlations), though the pcalg package does al-
low users to provide their own conditional independence tests as arguments. it
bears emphasizing that these restrictions are not essential. as soon as you have
a consistent independence test, you are, in principle, in business. in particular,
consistent non-parametric tests of conditional independence would work perfectly
well. an interesting example of this is the paper by chu and glymour (2008),
on    nding causal models for the time series, assuming additive but non-linear
models.

24.5 software and examples

the pc and fci algorithms are implemented in the stand-alone java program
tetrad (http://www.phil.cmu.edu/projects/tetrad/). they are also imple-
mented in the pcalg package on cran (kalisch et al., 2010, 2012). this pack-
age also includes functions for calculating the e   ects of interventions from    tted
graphs, assuming linear models. the documentation for the package is somewhat
confusing; rather see kalisch et al. (2012) for a tutorial introduction.

it   s worth going through how pcalg works11. the code is designed to take ad-
vantage of the modularity and abstraction of the pc algorithm itself; it separates
actually    nding the graph completely from performing the conditional indepen-
dence test, which is rather a function the user supplies. (some common ones
are built in.) for reasons of computational e   ciency, in turn, the conditional in-
dependence tests are set up so that the user can just supply a set of su   cient
statistics, rather than the raw data.

let   s walk through an example12, using the mathmarks data set. this contains
grades (   marks   ) from 88 university students in    ve mathematical subjects, al-
gebra, analysis, mechanics, statistics and vectors. all    ve variables are positively
correlated with each other.

library(pcalg)
library(smpracticals)
data(mathmarks)
suffstat <- list(c=cor(mathmarks),n=nrow(mathmarks))
pc.fit <- pc(suffstat, indeptest=gausscitest, p=ncol(mathmarks),alpha=0.005)

11 a word about installing the package: you   ll need the package rgraphviz for drawing graphs, which
is hosted not on cran (like pcalg) but on bioconductor. try installing it, and its dependencies,
before installing pcalg. see
http://www.bioconductor.org/packages/release/bioc/html/rgraphviz.html for help on installing
rgraphviz.

12 after spirtes et al. (2001,   6.12, pp. 152   154).

[[todo:
cleanup
output
from the
package]]

24.5 software and examples

529

library(rgraphviz)
plot(pc.fit,labels=colnames(mathmarks),main="inferred dag for mathmarks")

figure 24.3 dag inferred by the pc algorithm from the mathmarks data.
two-headed arrows, like undirected edges, indicate that the algorithm was
unable to orient the edge. (it is obscure why pcalg sometimes gives an edge
it cannot orient no heads and sometimes two.)

this uses a gaussian (-and-linear) test for conditional independence, gausscitest,

which is built into the pcalg package. basically, it hopes to test whether x        y |z
by testing whether the partial correlation of x and y given z is close to zero.
these partial correlations can all be calculated from the correlation matrix, so the
line before creates the su   cient statistics needed by gausscitest     the matrix
of correlations and the number of data points. we also have to tell pc how many
variables there are, and what signi   cance level to use in the test (here, 0.5%).

before going on, i encourage you to run pc as above, but with verbose=true,

and to study the output.

figure 24.3 shows the resulting dag. if we take it seriously, it says that grades
in analysis are driven by grades in algebra, while algebra in turn is driven by
statistics and vectors. while one could make up stories for why this would be
so (perhaps something about the curriculum?), it seems safer to regard this as
a warning against blindly trusting any algorithm    - a key assumption of the
pc algorithm, after all, is that there are no unmeasured but causally-relevant
variables, and it is easy to believe these are violated. for instance, while knowledge
of di   erent mathematical    elds may be causally linked (it would indeed be hard
to learn much mechanics without knowing about vectors), test scores are only
imperfect measurements of knowledge.

the size of the test may seem low, but remember we are doing a lot of tests:

inferred dag for mathmarksmechanicsvectorsalgebraanalysisstatistics530

discovering causal structure

3

p = ncol(mathmarks))

summary(pc.fit)
## object of class 'pcalgo', from call:
## pc(suffstat = suffstat, indeptest = gausscitest, alpha = 0.005,
##
##
## nmb. edgetests during skeleton estimation:
## ===========================================
## max. order of algorithm:
## number of edgetests from m = 0 up to m = 3 :
##
## graphical properties of skeleton:
## =================================
## max. number of neighbours:
## avg. number of neighbours:
##
## adjacency matrix g:
##
1 2 3 4 5
## 1 . 1 . . .
## 2 1 . 1 . .
## 3 . . . 1 .
## 4 . . . . .
## 5 . . 1 . .

2 at node(s) 2
1

20 38 10 0

this tells us that it considered going up to conditioning on three variables (the
maximum possible, since there are only    ve variables), that it did twenty tests
of unconditional independence, 31 tests where it conditioned on one variable,
four tests where it conditioned on two, and none where it conditioned on three.
this 55 tests in all, so a simple bonferroni correction suggests the over-all size
is 55    0.005 = 0.275. this is probably pessimistic (the bonferroni correction
typically is). setting    = 0.05 gives a somewhat di   erent graph (figure 24.4).

24.5 software and examples

531

plot(pc(suffstat, indeptest=gausscitest, p=ncol(mathmarks),alpha=0.05),

labels=colnames(mathmarks),main="")

figure 24.4 inferred dag when the size of the test is 0.05.

mechanicsvectorsalgebraanalysisstatistics532

discovering causal structure

for a second example13, let   s use some data on academic productivity among
psychologists. the two variables of ultimate interest were the publication (pubs)
and citation (cites) rates, with possible measured causes including ability
(basically, standardized test scores), graduate program quality grad (basically,
the program   s national rank), the quality of the psychologist   s    rst job, first, a
measure of productivity prod, and sex. there were 162 subjects, and while the
actual data isn   t reported, the correlation matrix is.

psychs
##
## ability
## grad
## prod
## first
## sex
## cites
## pubs

ability grad prod first

1.00 0.62 0.25 0.16 -0.10
0.00
0.62 1.00 0.09 0.28
0.03
0.25 0.09 1.00 0.07
0.16 0.28 0.07 1.00
0.10
1.00
-0.10 0.00 0.03 0.10
0.13
0.29 0.25 0.34 0.37
0.18 0.15 0.19 0.41
0.43

sex cites pubs
0.29 0.18
0.25 0.15
0.34 0.19
0.37 0.41
0.13 0.43
1.00 0.55
0.55 1.00

the model found by pcalg is fairly reasonable-looking (figure 24.5). of course,
the linear-and-gaussian assumption has no particular support here, and there is
at least one variable for which it must be wrong (which?), but unfortunately with
just the correlation matrix we cannot go further.

13 following spirtes et al. (2001,   5.8.1, pp. 98   102).

24.5 software and examples

533

plot(pc(list(c=psychs,n=162),indeptest=gausscitest,p=7,alpha=0.01),

labels=colnames(psychs),main="")

figure 24.5 causes of academic success among psychologists. the arrow
from citations to publications is a bit odd, but not impossible     people who
get cited more might get more opportunities to do research and so to
publish.

abilitygradprodfirstsexcitespubs534

discovering causal structure

24.6 limitations on consistency of causal discovery

there are some important limitations to causal discovery algorithms (spirtes
et al., 2001,   12.4). they are universally consistent: for all causal graphs g,14

(cid:16)(cid:98)gn (cid:54)= g
(cid:17)

lim
n       pr

= 0

(24.8)

the id203 of getting the graph wrong can be made arbitrarily small by using
enough data. however, this says nothing about how much data we need to achieve
a given level of con   dence, i.e., the rate of convergence. uniform consistency would
mean that we could put a bound on the id203 of error as a function of n
which did not depend on the true graph g. robins et al. (2003) proved that no
uniformly-consistent causal discovery algorithm can exist. the issue, basically,
is that the adversary could make the convergence in eq. 24.8 arbitrarily slow
by selecting a distribution which, while faithful to g, came very close to being
unfaithful, making some of the dependencies implied by the graph arbitrarily
small. for any given dependence strength, there   s some amount of data which
will let us recognize it with high con   dence, but the adversary can make the
required data size as large as he likes by weakening the dependence, without ever
setting it to zero15.

the upshot is that so uniform, universal consistency is out of the question; we
can be universally consistent, but without a uniform rate of convergence; or we
can converge uniformly, but only on some less-than-universal class of distribu-
tions. these might be ones where all the dependencies which do exist are not too
weak (and so not too hard to learn reliably from data), or the number of true
edges is not too large (so that if we haven   t seen edges yet they probably don   t
exist; janzing and herrmann, 2003; kalisch and b  uhlmnann, 2007).

it   s worth emphasizing that the robins et al. (2003) no-uniform-consistency
result applies to any method of discovering causal structure from data. invoking
human judgment, bayesian prior distributions over possible causal structures,
etc., etc., won   t get you out of it.

24.7 pseudo-code for the sgs algorithm16

when you see a loop, assume that it gets entered at least once.    replace    in the
sub-functions always refers to the input graph.
sgs = function(set of variables v) {

(cid:98)g = colliders(prune( complete undirected graph on v))
until ((cid:98)g == g(cid:48)) {

(cid:98)g = g(cid:48)

14 if the true distribution is faithful to multiple graphs, then we should read g as their equivalence

class, which has some undirected edges.

15 see   20.4 for a more quantitative statement of how the required sample size relates to

non-parametric measures of the strength of dependence.

16 this section may be omitted on    rst (and maybe even second) reading.

24.8 further reading

535

g(cid:48) = orient((cid:98)g)

}

return((cid:98)g)

}
prune = function(g) {
for each a, b     v {

for each s     v \ {a, b} {
}

if a        b|s { g = g \ (a     b) }

}
return(g)

for each (b     c)     g {

}
collliders = function(g) {
for each (a     b)     g {
if (a     c) (cid:54)    g {
collision = true
for each s     b     v \ {a, c} {
}
if (collision) { replace (a     b) with (a     b), (b     c) with (b     c) }

if a        c|s { collision = false }

}

}

}
return(g)

}
orient = function(g) {

}

if ((a     b)     g & (b     c)     g & (a     c) (cid:54)    g) { replace (b     c) with (b     c) }
if ((directed path from a to b)    g & (a     b)     g) { replace (a     b) with (a     b) }
return(g)

24.8 further reading

the best single reference on causal discovery algorithms remains spirtes et al.
(2001). a lot of work has been done in recent years by the group centered around
eth-z  urich, beginning with kalisch and b  uhlmnann (2007), connecting this to
modern statistical concerns about sparse e   ects and high-dimensional modeling.
as already mentioned, the best reference on partial identi   cation is manski
(2007). partial identi   cation of causal e   ects due to multiple equivalent dags
is considered in maathuis et al. (2009), along with e   cient algorithms for linear

536

discovering causal structure

systems, which are applied in maathuis et al. (2010), and implemented in the
pcalg package as ida.

discovery is possible for directed cyclic graphs, though since it   s harder to
understand what such models mean, it is less well-developed. important papers
on this topic include richardson (1996) and lacerda et al. (2008).

exercises

24.1 prove that, assuming faithfulness, a three-variable chain and a three-variable fork imply
exactly the same set of dependence and independence relations, but that these are dif-
ferent from those implied by a three-variable collider. are any implications common to
chains, forks, and colliders? could colliders be distinguished from chains and forks without
assuming faithfulness?

24.2 prove that if x and y are not parent and child, then either x        y , or there exists a set
of variables s such that x        y |s. hint: start with the markov property, that any x is
independent of all its non-descendants given its parents, and consider separately the cases
where y a descendant of x and those where it is not.
24.3 prove that if x        y |s for some set of variables s, then x        y |s(cid:48), where every variable
in s(cid:48) is a neighbor of x or y .

24.4 prove that the graph produced by the edge-removal step of the pc algorithm is exactly
the same as the graph produced by the edge-removal step of the sgs algorithm. hint:
sgs removes the edge between x and y when x        y |s for even one set s.

24.5 when, exactly, does e [y | x, z] = e [y | z] imply y        x|z?
24.6 would the sgs algorithm work on a non-causal, merely-probabilistic dag? if so, in what

sense is it a causal discovery algorithm? if not, why not?

24.7 describe how to use bandwidth selection as a conditional independence test.
24.8 read kalisch et al. (2012) and write a conditional independence test function based on
bandwidth selection (  14.5). check that your test gives the right size when run on simu-
lated cases where you know the variables are conditionally independent. check that your
test function works with pcalg::pc.

part iv

dependent data

537

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

25

time series

so far, we have assumed that all data points are pretty much independent of each
other. in the chapters on regression, we assumed that each yi was independent
of every other, given its xi, and we often assumed that the xi were themselves
independent. in parts ii and iii, we allowed for arbitrarily complicated depen-
dence between the variables, but each multivariate data-point was assumed to be
generated independently. we will now relax this assumption, and see what sense
we can make of dependent data.

25.1 what time series are

the simplest form of dependent data are time series, which are just what they
sound like: a series of values recorded over time. the most common version of this,
in statistical applications, is to have measurements of a variable or variables x
at equally-spaced time-points starting from t, written say xt, xt+h, xt+2h, . . ., or
x(t), x(t+h), x(t+2h), . . .. here h, the amount of time between observations, is
called the    sampling interval   , and 1/h is the    sampling frequency    or    sampling
rate   .

figure 25.1 shows two fairly typical time series. one of them is actual data
(the number of lynxes trapped each year in a particular region of canada); the
other is the output of a purely arti   cial model. (without the labels, it might
not be obvious which one was which.) the core idea of time series analysis is
one which we   re already familiar with from the rest of statistics: we regard the
actual time series we see as one realization of some underlying, partially-random
(   stochastic   ) process, which generated the data. we use the data to make guesses
(   id136s   ) about the process, and want to make reliable guesses while being
clear about the uncertainty involved. the complication is that each observation
is dependent on all the other observations; in fact it   s usually this dependence
that we want to draw id136s about.

other kinds of time series

one sometimes encounters irregularly-sampled time series, x(t1), x(t2), . . ., where
ti    ti   1 (cid:54)= ti+1    ti. this is mostly an annoyance, unless the observation times are
somehow dependent on the values. continuously-observed processes are rarer    
especially now that digital sampling has replaced analog measurement in so many
applications. (it is more common to model the process as evolving continuously

539

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

540

time series

logistic.map <- function(x, r = 4) {

r * x * (1 - x)

}
logistic.iteration <- function(n, x.init, r = 4) {

x <- vector(length = n)
x[1] <- x.init
for (i in 1:(n - 1)) {

x[i + 1] <- logistic.map(x[i], r = r)

}
return(x)

}
x <- logistic.iteration(1000, x.init = runif(1))
y <- x + rnorm(1000, mean = 0, sd = 0.05)

code example 39: code de   ning our synthetic data set. why is this    logistic   ?

in time, but observe it at discrete times.) we skip both of these in the interest of
space.

regular, irregular or continuous time series all record the same variable at
every moment of time. an alternative is to just record the sequence of times at
which some event happened; this is called a    point process   . more re   ned data
might record the time of each event and its type     a    marked point process   .
point processes include very important kinds of data (e.g., earthquakes), but they
need special techniques, and we   ll skip them (though see   25.12).

notation

for a regularly-sampled time series, it   s convenient not to have to keep writing the
actual time, but just the position in the series, as x1, x2, . . ., or x(1), x(2), . . ..
this leads to a useful short-hand, that xi:j = (xi, xi+1, . . . xj   1, xj), a whole
block of time; some people write x j

i with the same meaning.

25.2 stationarity

in our old iid world, the distribution of each observation is the same as the
distribution of every other data point. it would be nice to have something like
this for time series. the property is called stationarity, which doesn   t mean that
the time series never changes, but that its distribution doesn   t.

more precisely, a time series is strictly stationary or strongly stationary
when x1:k and xt:t+k   1 have the same distribution, for all k and t     the distri-
bution of blocks of length k is time-invariant. again, this doesn   t mean that
every block of length k has the same value, just that it has the same distribution
of values.

if there is strong or strict stationarity, there should be weak or loose (or wide-
sense) stationarity, and there is. all it requires is that e [x1] = e [xt], and that
cov [x1, xk] = cov [xt, xt+k   1]. (notice that it   s not dealing with whole blocks
of time any more, just single time-points.) clearly (exercise!), strong stationarity
implies weak stationarity, but not, in general, the other way around, hence the

25.2 stationarity

541

par(mfrow = c(1, 2))
plot(lynx)
plot(y[1:100], xlab = "t", ylab = expression(y[t]), type = "l")
par(mfrow = c(1, 1))

figure 25.1 left: annual number of trapped lynxes in the mackenzie river
region of canada. right: a toy dynamical model, simulated from code
example 39.

names. it may not surprise you to learn that strong and weak stationarity coincide
when xt is a gaussian process, but not,in general, otherwise.

you should convince yourself that an iid sequence is strongly stationary.

timelynx182018601900010002000300040005000600070000204060801000.00.20.40.60.81.0tyt542

time series

25.2.1 autocorrelation

time series are serially dependent: xt is in general dependent on all earlier
values in time, and on all later ones. typically, however, there is decay of depen-
dence (sometimes called decay of correlations): xt and xt+h become more
and more nearly independent as h        . the oldest way of measuring this is the
autocovariance,

  (h) = cov [xt, xt+h]

(25.1)

which is well-de   ned just when the process is weakly stationary. we could equally
well use the autocorrelation,

  (h) =

cov [xt, xt+h]

v [xt]

=

  (h)
  (0)

(25.2)

again using stationarity to simplify the denominator.

as i said, for most time series   (h)     0 as h grows. of course,   (h) could
be exactly zero while xt and xt+h are strongly dependent. figure 25.2 shows
the autocorrelation functions (acfs) of the lynx data and the simulation model;
the correlation for the latter is basically never distinguishable from zero, which
doesn   t accord at all with the visual impression of the series. indeed, we can
con   rm that something is going on the series by the simple device of plotting
xt+1 against xt (figure 25.3). more general measures of dependence would in-
clude looking at the spearman rank-correlation of xt and xt+h, or quantities like
mutual information.

autocorrelation is important for four reasons, however. first, because it is the
oldest measure of serial dependence, it has a    large installed base   : everybody
knows about it, they use it to communicate, and they   ll ask you about it. second,
in the rather special case of gaussian processes, it really does tell us everything
we need to know. third, in the somewhat less special case of linear prediction,
it tells us everything we need to know. fourth and    nally, it plays an important
role in a crucial theoretical result, which we   ll go over next.

25.2.2 the ergodic theorem

with iid data, the ultimate basis of all our statistical id136 is the law of large
numbers, which told us that

xi     e [x1]

(25.3)

n(cid:88)

i=1

1
n

for complicated historical reasons, the corresponding result for time series is
called the ergodic theorem1. the most general and powerful versions of it are

1 in the late 1800s, the physicist ludwig boltzmann needed a word to express the idea that if you

took an isolated system at constant energy and let it run, any one trajectory, continued long
enough, would be representative of the system as a whole. being a highly-educated nineteenth
century german-speaker, boltzmann knew far too much ancient greek, so he called this the
   ergodic property   , from ergon    energy, work    and hodos    way, path   . the name stuck.

25.2 stationarity

543

quite formidable, and have very subtle proofs, but there is a simple version which
gives the    avor of them all, and is often useful enough.

25.2.2.1 the world   s simplest ergodic theorem

suppose xt is weakly stationary, and that

|  (h)| =   (0)   <    

(25.4)

   (cid:88)

h=0

(remember that   (0) = v [xt].) the quantity    is called the correlation time,
or integrated autocorrelation time.

now consider the average of the    rst n observations,

this time average is a random variable. its expectation value is

because the mean is stationary. what about its variance?

v(cid:2)x n

n(cid:88)

t=1

xt

e [xt] = e [x1]

x n =

1
n

1
n

n(cid:88)
(cid:35)

t=1

xt

t=1

1
n

e(cid:2)x n
(cid:3) =
(cid:34)
n(cid:88)
(cid:3) = v
(cid:34) n(cid:88)
(cid:34)
(cid:34)
(cid:34)
(cid:34)

    1
n2

1
n2

1
n2

=

=

t=1

    1
n2
    1
n2

nv [x1] + 2

n  (0) + 2

n  (0) + 2

n  (0) + 2

=

=

n  (0)(1 + 2   )

n2

  (0)(1 + 2   )

n

(cid:35)

v [xt] + 2

cov [xt, xs]

(cid:35)
(cid:35)

  (s     t)

|  (s     t)|

s=t+1

t=1

t=1

n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)
   (cid:88)
n(cid:88)

s=t+1

s=t+1

h=1

t=1

t=1

|  (h)|

|  (h)|

t=1

h=1

(cid:35)
(cid:35)

(25.5)

(25.6)

(25.7)

(25.8)

(25.9)

(25.10)

(25.11)

(25.12)

(25.13)

(25.14)

eq. 25.9 uses stationarity again, and then eq. 25.13 uses the assumption that the
correlation time    is    nite.

544

since e(cid:2)xn

(cid:3) = e [x1], and v(cid:2)x n

time series

(cid:3)     0, we have that

xn     e [x1]

(25.15)

exactly as in the iid case. (   time averages converge on expected values.   ) in
fact, we can say a bit more. remember chebyshev   s inequality: for any random
variable z,

so

pr (|z     e [z]| >  )     v [z]

pr(cid:0)|x n     e [x1]| >  (cid:1)       (0)(1 + 2   )

 2

(25.16)

(25.17)

you may wonder whether the condition that(cid:80)   

which goes to zero as n grows for any given  .

n 2

sible. it turns out that it can in fact be weakened to just limn       1
n
as indeed the proof above might suggest.

h=0 |  (h)| <     is as weak as pos-
h=0   (h) = 0,

(cid:80)n

the argument above can actually be extended to some non-stationary pro-

cesses; see exercise 25.7.

if the xt were all iid, or even just uncorrelated, we would have v(cid:2)x n

25.2.2.2 rate of convergence

(cid:3) =   (0)/n

exactly. our bound on the variance is larger by a factor of (1 + 2   ), which re   ects
the in   uence of the correlations. said another way, we can more or less pretend
that instead of having n correlated data points, we have n/(1 + 2   ) independent
data points, that n/(1 + 2   ) is our e   ective sample size2

generally speaking, dependence between observations reduces the e   ective
sample size, and the stronger the dependence, the greater the reduction. (for
an extreme example, consider the situation where x1 is randomly drawn, but
thereafter xt+1 = xt.) in more complicated situations,    nding the e   ective sam-
ple size is itself a tricky undertaking, but it   s often got this general    avor.

25.2.2.3 why ergodicity matters

the ergodic theorem is important, because it tells us that a single long time
series becomes representative of the whole data-generating process, just the same
way that a large iid sample becomes representative of the whole population or
distribution. we can therefore actually learn about the process from empirical
data.

strictly speaking, we have established that time-averages converge on expecta-
tions only for xt itself, not even for f (xt) where the function f is non-linear. it
might be that f (xt) doesn   t have a    nite correlation time even though xt does,
or indeed vice versa. this is annoying; we don   t want to have to go through the
analysis of the last section for every di   erent function we might want to calculate.

2 some people like to de   ne the correlation time as, in this notation, 1 + 2   for just this reason.

when people say that the whole process is ergodic, they roughly speaking

25.3 markov models

545

mean that

1
n

n(cid:88)

t=1

f (xt:t+k   1)     e [f (x1:k)]

(25.18)

for any reasonable function f . this is (again very roughly) equivalent to

pr (x1:k     a, xt:t+l   1     b)     pr (x1:k     a) pr (x1:l     b)

(25.19)

n(cid:88)

t=1

1
n

which is a kind of asymptotic independence-on-average3

if our data source is ergodic, then what eq. 25.18 tells us is that sample aver-
ages of any reasonable function are representative of expectation values, which is
what we need to be in business statistically. this in turn is basically implied by
stationarity.4 what does this let us do?

25.3 markov models

for this section, we   ll assume that xt comes from a stationary, ergodic time
series. so for any reasonable function f , the time-average of f (xt) converges on
e [f (x1)]. among the    reasonable    functions are the indicators, so

1a(xt)     pr (x1     a)

(25.20)

n(cid:88)

t=1

1
n

3 it   s worth sketching a less rough statement. instead of working with xt, work with the whole future

trajectory yt = (xt, xt+1, xt+2, . . .). now the dynamics, the rule which moves us into the future,
can be summed up in a very simple, and deterministic, operation t :
yt+1 = t yt = (xt+1, xt+2, xt+3, . . .). a set of trajectories is invariant if it is left unchanged by t :
for every y     a, there is another y(cid:48) in a where t y(cid:48) = y. a process is ergodic if every invariant set
either has id203 0 or id203 1. what this means is that (almost) all trajectories generated
by an ergodic process belong to a single invariant set, and they all wander from every part of that
set to every other part     they are metrically transitive. (because: no smaller set with any
id203 is invariant.) metric transitivity, in turn, is equivalent, assuming stationarity, to

t=0 pr(cid:0)y     a, t ty     b(cid:1)     pr (y     a) pr (y     b). from metric transitivity follows

n   1(cid:80)n   1
birkho      s    individual    ergodic theorem, that n   1(cid:80)n   1

t=0 f (t ty )     e [f (y )], with id203 1.

since a function of the trajectory can be a function of a block of length k, we get eq. 25.18.

4 again, a sketch of a less rough statement. use y again for whole trajectories. every stationary

distribution for y can be written as a mixture of stationary and ergodic distributions, rather as we
wrote complicated distributions as mixtures of simple gaussians in chapter 19. (this is called the
   ergodic decomposition    of the process: see gray 2009.) we can think of this as    rst picking an
ergodic process according to some    xed distribution, and then generating y from that process. time
averages computed along any one trajectory thus converge according to eq. 25.18. if we have only a
single trajectory, it looks just like a stationary and ergodic process. it is thus common to assume
that the data source is not only stationary but also ergodic. this only becomes a problem if we have
multiple trajectories from the same source, each of which one may be converging to a di   erent
ergodic component.

546

time series

since this also applies to functions of blocks,

n(cid:88)

t=1

1
n

1a,b(xt, xt+1)     pr (x1     a, x2     b)

(25.21)

and so on. if we can learn joint and marginal probabilities, and we remember how
to divide, then we can learn conditional probabilities.

it turns out that pretty much any density estimation method which works for
iid data will also work for getting the marginal and conditional distributions
of time series (though, again, the e   ective sample size depends on how quickly
dependence decays). so if we want to know p(xt), or p(xt+1 | xt), we can estimate
it just as we learned how to do in chapter 14. just as in that chapter, much the
same techniques apply whether x is discrete or continuous; for brevity, i   ll speak
as though x is continuous and p(xt+1 | xt) is a conditional pdf.
now, the conditional distribution p(xt+1 | xt) always exists, and we can always
estimate it. but why stop just one step back into the past? why not look at
p(xt+1 | xt, xt   1), or for that matter p(xt+1 | xt   999:t)? there are three reasons, in
decreasing order of pragmatism.
    estimating p(xt+1 | xt   999:t) means estimating a thousand-and-one-dimensional
distribution. the curse of dimensionality will crush us.
    because of the decay of dependence, there shouldn   t be much di   erence, much
of the time, between p(xt+1 | xt   999:t) and p(xt+1 | xt   998:t), etc. even if we could
go very far back into the past, it shouldn   t, usually, change our predictions very
much.
    sometimes, a    nite, short block of the past completely screens o    the remote

past.

you will remember the markov property from your previous id203 classes:

xt+1        x1:t   1 | xt

(25.22)
when the markov property holds, there is simply no point in looking at p(xt+1 |
xt, xt   1), because it   s got to be just the same as p(xt+1 | xt). if the process isn   t
a simple markov chain but has a higher-order markov property,

xt+1        x1:t   k | xt   k+1:t

(25.23)

then we never have to condition on more than the last k steps to learn all that
there is to know. the markov property means that the current state screens o   
the future from the past.

it is always an option to model xt as a markov process, or a higher-order
markov process. if it isn   t exactly markov, if there   s really some dependence be-
tween the past and the future even given the current state, then we   re introducing
some bias, but it can be small, and dominated by the reduced variance of not
having to worry about higher-order dependencies.

25.3 markov models

547

25.3.1 meaning of the markov property

the markov property is a weakening both of being strictly iid and of being
strictly deterministic.

that being markov is weaker than being iid is obvious: an iid sequence sat-
is   es the markov property, because everything is independent of everything else
no matter what we condition on.

in a deterministic dynamical system, on the other hand, we have xt+1 = g(xt)
for some    xed function g. iterating this equation, the current state xt    xes the
whole future trajectory xt+1, xt+2, . . .. in a markov chain, we weaken this to
xt+1 = g(xt, ut), where the ut are iid noise variables (which we can take to be
uniform for simplicity). the current state of a markov chain doesn   t    x the exact
future trajectory, but it does    x the distribution over trajectories.

the real meaning of the markov property, then, is about information    ow: the

current state is the only channel through which the past can a   ect the future.

25.3.2 estimating markov models

once we believe that we   re dealing with a markov process, we really have only
two things to have to estimate: the conditional distribution p(xt+1 | xt), and the
initial distribution p(x1). let   s focus on the    rst, for reasons which will become
apparent shortly. if x is continuous, then, as i said above, pretty much any
method for estimating conditional distributions could be used. this is also true
if x is discrete, but it   s also possible to simplify matters.
suppose that there are m states, so that we can collect all the probabilities
pr (xt+1 = j|xt = i) in an m    m matrix p of transition rates or transition
probabilities. then the conditional likelihood of the time series, given the    rst
observation x1, is

n(cid:89)

(cid:89)

pxt   1xt =

pnij
ij

(25.24)

t=2

i,j

where i   ve introduced the (random) transition counts5 nij, which tell us how
many times the state i was followed by the state j. the log-likelihood is therefore

nij log pij

(25.25)

(cid:88)

i,j

before we can maximize this, we need to impose the constraint that each state is

followed by something: for each i, (cid:88)

pij = 1

(25.26)

j

5 it should be clear from the equation that if the process is markovian, then any two sequences with
the same transition counts are equally probable. it turns out that if the id203 is the same for
all sequences with equal transition counts, then the process must be markovian, though the proof is
intricate (diaconis and freedman, 1980).

548

time series

we need to introduce m lagrange multipliers to enforce these m constraints.
doing so, we get the very natural solution (exercise 25.2):

(cid:98)pij =

nij(cid:80)

j nij

(25.27)

each time the process revisits state i, the next state is (by the markov property)
independent of the previous and subsequent visits, so using the law of large

numbers, we have that (cid:98)pij     pij, provided only that the state i is returned to

in   nitely often as n grows6.

at this point, you may be thinking that this is very much like the maximum
likelihood estimate of the multinomial distribution you   ll have seen in baby stats.
this is because it is exactly like that; each state i gets its own multinomial dis-
tribution for the next state, but those estimates can be done separately from
each other. the maximum-likelihood estimate of p is, like the id113 for multino-
mial distributions, generally consistent and e   cient, with a variance which can
be found from the second derivative of the log-likelihood. of course, if transition
rates are not free to be adjusted independently, because they are all functions of
more basic underlying parameters, we should estimate those parameters, which
complicates the calculus a little.

higher-order and variable-length markov chains

in estimating a kth order markov chain, k > 1, we still just need to estimate
the transition rates, but the matrix of rates now has mk rows (one per length-k
history) and m columns. each row must still sum to one, so the form of the
solution remains unchanged; we just need to count transitions from length-k
histories to the next observation.

since mk grows rapidly with k, it would be nice if we could get away from having
to do that many estimations. it can happen that sometimes we don   t need to keep
track of all of the last k observations to get the next-observation distribution.
for instance, in a second-order markov process, it might happen that the history
xt = 0, xt   1 = 0 has the same predictive distribution as xt = 0, xt   1 = 1,
so we only need to estimate that distribution once, if we can realize this. such
approaches are known as    variable length markov chains    or    context trees   
(b  uhlmann, 2000; b  uhlmann and wyner, 1999).

but what about that    rst observation?

by the markov property, x1 is irrelevant to the rest of the time series once we   ve
seen x2. the advantage of this is that the distribution of x1 can be arbitrary,
and we will still get consistent estimates of the transition rates. the disadvantage
is that if, for some reason, we need to estimate the distribution of x1, sometimes
called the    starting    or    initial    distribution, we   ve got a problem, because we
have only one observation!

6 you might wonder how that last condition could fail, but consider a state which, once left, is never

returned to from any other state. (can you show that this implies a failure of ergodicity?)

25.4 autoregressive models

549

if we see multiple independent realizations of the markov chain and believe
that they share a common starting distribution, we could use that to estimate
x1. on the other hand, if we believe the chain is stationary, we could use the
transition rates to estimate the marginal distribution of all the xt, as follows.
any distribution over the m states could be written as an m-dimensional vector,
i qi = 1. for q to match the marginal
distribution over states in a stationary chain, the id203 of arriving in any
state qi has to match the id203 of starting there:

say q, with the constraints that qi     0, (cid:80)
(cid:88)

qi =

qjpji

in matrix form, this is

j

q = qp

so q is an invariant distribution if it is a left eigenvector of p with eigenvalue

one7. if we have an estimate (cid:98)p,    nding its eigenvector(s) with eigenvalue 1 will

then give an estimate of the invariant distribution, which (assuming stationarity)
would be an estimate of the starting distribution8

(25.28)

(25.29)

25.4 autoregressive models

instead of trying to estimate the whole conditional distribution of xt, we can just
look at its conditional expectation. this is a regression problem, but since we are
regressing xt on earlier values of the series, it   s called an autoregression:

e [xt | xt   p:t   1 = xp

1] = r(x1:p)

(25.30)

if we think the process is markov of order p, then of course there is no point in
conditioning on more than p steps of the past when doing an autoregression. but
even if we don   t think the process is markov, the same reasons which inclined us
towards markov approximations also make limited-order autoregressions attrac-
tive.

since this is a regression problem, we can employ all the tools we know for
regression analysis: linear models, kernel regression, spline smoothing, additive
models, etc., mixtures of regressions, etc. since we are regressing xt on earlier
values from the same series, it is useful to have tools for turning a time series
into a regression-style design matrix (as in figure 25.4); see code example 40.

7 conversely, all left eigenvectors of a transition matrix with eigenvalue one must have non-negative

entries, and so must either be invariant distributions, or proportional to invariant distributions. this
result is a non-trivial piece of id202 called the frobenius-perron (or perron-frobenius)
theorem.

8 you could even set up the problem of jointly maximizing the log-likelihood of the entire sequence,

using the eigenvector of p as the distribution of x1, but i don   t recommend it. the eigenvector of p
is a very nonlinear function of the entries in p, so the maximization becomes a complicated
numerical problem, and in the end it   s only to get at the information about p contained in the

single observation x1. if x1 is really very in   uential on(cid:98)p, it   s hard to imagine you   ve got enough

data to be secure in all the other assumptions!

550

time series

design.matrix.from.ts <- function(ts, order, right.older = true) {

n <- length(ts)
x <- ts[(order + 1):n]
for (lag in 1:order) {
if (right.older) {

x <- cbind(x, ts[(order + 1 - lag):(n - lag)])

}
else {

x <- cbind(ts[(order + 1 - lag):(n - lag)], x)

}

}
lag.names <- c("lag0", paste("lag", 1:order, sep = ""))
if (right.older) {

colnames(x) <- lag.names

}
else {

colnames(x) <- rev(lag.names)

}
return(as.data.frame(x))

}

code example 40: example code for turning a time series into a design matrix, suitable for
regression.

aar <- function(ts, order) {
stopifnot(require(mgcv))
fit <- gam(as.formula(auto.formula(order)), data = design.matrix.from.ts(ts,

order))
return(fit)

}
auto.formula <- function(order) {

inputs <- paste("s(lag", 1:order, ")", sep = "", collapse = "+")
form <- paste("lag0 ~ ", inputs)
return(form)

}

code example 41: fitting an additive autoregression of arbitrary order to a time series. see
online for comments.

suppose p = 1. then we essentially want to draw regression curves through
plots like those in figure 25.3. figure 25.5 shows an example for the arti   cial
series.

25.4.1 autoregressions with covariates

nothing keeps us from adding a variable other than the past of xt to the regres-
sion:

e [xt+1 | xt   k+1:t, z]

or even another time series:

e [xt+1 | xt   k+1:t, zt   l+1:t]

(25.31)

(25.32)

p(cid:88)

25.4 autoregressive models

551

these are perfectly well-de   ned conditional expectations, and quite estimable
in principle. of course, adding more variables to a regression means having to
estimate more, so again the curse of dimensionality comes up, but our methods
are very much the same as in the basic regression analyses.

25.4.2 additive autoregressions

as before, if we want some of the    exibility of non-parametric smoothing, without
the curse of dimensionality, we can try to approximate the conditional expectation
as an additive function:

e [xt | xt   p:t   1]       0 +

gj(xt   j)

(25.33)

j=1

my personal experience with applied projects is that additive autoregressions
tend to work surprisingly well.

example: the lynx

let   s try    tting an additive model for the lynx. code example 41 shows some
code for doing this. (most of the work is re-shaping the time series into a data
frame, and then automatically generating the right formula for gam.) let   s try
out p = 2.

lynx.aar2 <- aar(lynx, 2)

this inherits everything we can do with a gam, so we can do things like plot
the partial response functions (figure 25.6), plot the    tted values against the
actual (figure 25.7), etc. to get a sense of how well it can actually extrapolate,
figure 25.8 re-   ts the model to just the    rst 80 data points, and then predicts
the remaining 34.

25.4.3 linear autoregression

when people talk about autoregressive models, they usually (alas) just mean
linear autoregressions. there is almost never any justi   cation in scienti   c theory
for this preference, but we can always ask for the best linear approximation to
the true autoregression, if only because it   s fast to compute and fast to converge.
the analysis we did in chapter 2 of how to    nd the optimal linear predictor
carries over with no change whatsoever. if we want to predict xt as a linear com-
bination of the last k observations, xt   1, xt   2, . . . xt   p, then the ideal coe   cients
   are

   = (v [xt   p:t   1])

   1cov [xt   p:t   1, xt]

(25.34)
where v [xt   p:t   1] is the variance-covariance matrix of (xt   1, . . . xt   p) and sim-
ilarly cov [xt   p:t   1, xt] is a vector of covariances. assume stationarity, v [xt] is
constant in t, and so the common factor of the over-all variance goes away, and   

552

time series

could be written entirely in terms of the correlation function   . stationarity also
lets us estimate these covariances, by taking time-averages.

a huge amount of e   ort is given over to using linear ar models, which in
my opinion is out of all proportion to their utility     but very re   ective of what
was computationally feasible up to about 1980. my experience is that results like
figure 25.9 is pretty typical.

25.4.3.1    unit roots    and stationary solutions

suppose we really believed a    rst-order linear autoregression,

xt+1 =    +   xt +  t

(25.35)

with  t some iid noise sequence. let   s suppose that the mean is zero for simplicity,
so    = 0. then

xt+2 =   2xt +    t +  t+1
xt+3 =   3xt +   2 t +    t+1 +  t+2 ,

(25.36)
(25.37)

etc. if this is going to be stationary, it   d better be the case that what happened
at time t doesn   t go on to dominate what happens at all later times, but clearly
that will happen if |  | > 1, whereas if |  | < 1, eventually all memory of xt
(and  t) fades away. the linear ar(1) model in fact can only produce stationary
distributions when |  | < 1.

for higher-order linear ar models, with parameters   1,   2, . . .   p, the corre-

sponding condition is that all the roots of the polynomial

  jzj     1

(25.38)

j=1

must be outside the unit circle. when this fails, when there is a    unit root   , the
linear ar model cannot generate a stationary process9.

there is a fairly elaborate machinery for testing for unit roots, which is some-
times also used to test for non-stationarity. it is not clear how much this really
matters. a non-stationary but truly linear ar model can certainly be estimated10;
a linear ar model can be non-stationary even if it has no unit roots11; and if the
linear model is just an approximation to a non-linear one, the unit-root criterion
doesn   t apply to the true model anyway.
see   25.6.1 for an alternative way of checking stationarity, which presumes no

particular parametric form.

9 the same argument applies to arma models (  25.9.3.2) more generally.
10 because the correlation structure stays the same, even as the means and variances can change.

consider xt = xt   1 +  t, with  t iid.

11 start it with x1 very far from the expected value.

p(cid:88)

25.4 autoregressive models

553

25.4.4 conditional variance

having estimated the conditional expectation, we can estimate the conditional
variance v [xt | xt   p:t   1] just as we estimated other conditional variances, in
chapter 10.

example: lynx

the lynx series seems ripe for    tting conditional variance functions     presumably
when there are a few thousand lynxes, the noise is going to be larger than when
there are only a few hundred.

sq.res <- residuals(lynx.aar2)^2
lynx.condvar1 <- gam(sq.res ~ s(lynx[-(1:2)]))
lynx.condvar2 <- gam(sq.res ~ s(lag1) + s(lag2), data = design.matrix.from.ts(lynx,

2))

i have    t two di   erent models for the conditional variance here, just because.
figure 25.10 shows the data, and the predictions of the second-order additive ar
model, but with just the standard deviation bands corresponding to the    rst of
these two models; you can try making the analogous plot for lynx.condvar2.

25.4.5 regression with correlated noise; generalized least squares

suppose we have an old-fashioned regression problem

yt = r(xt) +  t

(25.39)

only now the noise terms  t are themselves a dependent time series. ignoring this
dependence, and trying to estimate m by minimizing the mean squared error, is
very much like ignoring heteroskedasticity. (in fact, heteroskedastic  t are a special
case.) what we saw in chapter 10 is that ignoring heteroskedasticity doesn   t lead
to bias, but it does mess up our understanding of the uncertainty of our estimates,
and is generally ine   cient. the solution was to weight observations, with weights
inversely proportional to the variance of the noise.

with correlated noise, we do something very similar. suppose we knew the
covariance function   (h) of the noise. from this , we could construct the variance-
covariance matrix    of the  t (since   ij =   (i     j), of course).

we can use this as follows. say that our guess about the regression function is
m. stacking y1, y2, . . . yn into a matrix y as usual in regression, and likewise cre-
ating m(x), the gauss-markov theorem (  10.2.2.1) tells us that the most e   cient
estimate is the solution to the generalized least squares problem,

(y     m(x))t      1(y     m(x))

(25.40)

as opposed to just minimizing the mean-squared error,

1
n

(cid:98)mgls = argmin
(cid:98)mols = argmin

m

m

(y     m(x))t (y     m(x))

1
n

(25.41)

554

time series

multiplying by the inverse of    appropriately discounts for observations which
are very noisy, and discounts for correlations between observations introduced by
the noise.12
this raises the question of how to get   (h) in the    rst place. if we knew the true
regression function r, we could use the covariance of yt     r(xt) across di   erent
t. since we don   t know r, but have only an estimate   m, we can try alternating
between using a guess at    to estimate   m, and using   m to improve our guess at
  . we used this sort of iterative approximation for weighted least squares, and it
can work here, too.

25.5 id64 time series

the big picture of id64 doesn   t change: simulate a distribution which is
close to the true one, repeat our estimate (or test or whatever) on the simulation,
and then look at the distribution of this statistic over many simulations. the
catch is that the surrogate data from the simulation has to have the same sort
of dependence as the original time series. this means that simple resampling is
just wrong (unless the data are independent), and our simulations will have to
be more complicated.

25.5.1 parametric or model-based bootstrap

conceptually, the simplest situation is when we    t a full, generative model    
something which we could step through to generate a new time series. if we are
con   dent in the model speci   cation, then we can bootstrap by, in fact, simulating
from the    tted model. this is the parametric bootstrap we saw in chapter 6.

25.5.2 block bootstraps

simple resampling won   t work, because it destroys the dependence between suc-
cessive values in the time series. there is, however, a clever trick which does
work, and is almost as simple. take the full time series x1:n and divide it up into
overlapping blocks of length k, so x1:k, x2:k+1, . . . xn   k+1:n. now draw m = n/k of
these blocks with replacement13, and set them down in order. call the new time
series   x1:n.

within each block, we have preserved all of the dependence between obser-
vations. it   s true that successive observations are now completely independent,
which generally wasn   t true of the original data, so we   re introducing some inac-
curacy, but we   re certainly coming closer than just resampling individual obser-
vations (which would be k = 1). moreover, we can make this inaccuracy smaller
and smaller by letting k grow as n grows. one can show14 that the optimal

12 if you want to use a linear model for m, this can be carried through to an explicit modi   cation of

the usual ordinary-least-squares estimate     exercise 25.3.

13 if n/k isn   t a whole number, round.
14 i.e., i will not show.

25.5 id64 time series

555

rblockboot <- function(ts, block.length, len.out = length(ts)) {

the.blocks <- as.matrix(design.matrix.from.ts(ts, block.length - 1, right.older = false))
blocks.in.ts <- nrow(the.blocks)
stopifnot(blocks.in.ts == length(ts) - block.length + 1)
blocks.needed <- ceiling(len.out/block.length)
picked.blocks <- sample(1:blocks.in.ts, size = blocks.needed, replace = true)
x <- the.blocks[picked.blocks, ]
x.vec <- as.vector(t(x))
return(x.vec[1:len.out])

}

code example 42: the basic block bootstrap for univariate time series. see exercise 25.5 for
variants and extensions.

k = o(n1/3); this gives a growing number (o(n2/3)) of increasingly long blocks,
capturing more and more of the dependence. (we will consider how exactly to
pick k in the next chapter.)

the block bootstrap scheme is extremely clever, and has led to a great many

variants. three in particular are worth mentioning.

1. in the circular block bootstrap (or circular bootstrap), we    wrap the
time series around a circle   , so that it goes x1, x2, . . . xn1, xn, x1, x2, . . .. we
then sample the n blocks of length k this gives us, rather than the merely
n     k blocks of the simple block bootstrap. this makes better use of the
information we have about dependence on distances < k.

2. in the block-of-blocks bootstrap, we    rst divide the series into blocks of
length k2, and then subdivide each of those into sub-blocks of length k1 < k2.
to generate a new series, we sample blocks with replacement, and then sample
sub-blocks within each block with replacement. this gives a somewhat better
idea of longer-range dependence, though we have to pick two block-lengths.

3. in the stationary bootstrap, the length of each block is random, chosen from
a geometric distribution of mean k. once we have chosen a sequence of block
lengths, we sample the appropriate blocks with replacement. the advantage
of this is that the ordinary block bootstrap doesn   t quite give us a stationary
time series. (the distribution of xk   1:k is not the same as the distribution of
xk:k+1.) averaging over the random choices of block lengths, the stationary
bootstrap does. it tends to be slightly slower to converge that the block or
circular bootstrap, but there are some applications where the surrogate data
really needs to be strictly stationary.

25.5.3 sieve bootstrap

a compromise between model-based and non-parametric bootstraps is to use a
sieve bootstrap. this also simulates from models, but we don   t really believe
in them; rather, we just want them to be reasonable easy to    t and simulate, yet
   exible enough that they can capture a wide range of processes if we just give
them enough capacity. we then (slowly) let them get more complicated as we get

556

time series

more data15. one popular choice is to use linear ar(p) models, and let p grow
with n     but there is nothing special about linear ar models, other than that
they are very easy to    t and simulate from. additive autoregressive models, for
instance, would often work at least as well.

25.6 cross-validation

there are actually multiple ways to do cross-validation for time series.

the most straight-forward applies to auto-regressive models. since we have,
at least implicitly, converted the time series to a design matrix (as on p. 549),
we can    hold out    rows from that design matrix at random as our testing set.
concretely, this means that we don   t try to predict some time points t     ttest
when estimating the model. (the set ttest is randomly chosen, and then averaged-
over, in the usual way.) those held-out time-points t are then what we try to
predict during testing, using the previously-estimated model and the predecessor
points t     1, t     2, . . . t     p.

notice that even if t is a held-out time-point, we are still likely to use x(t)
to predict (say) x(t + 1), since it is unlikely that both t and t + 1 are in the
same hold-out set. nothing like this happened when doing cross-validation for
iid data     every observed value was either in the training set or the testing set,
not awkwardly straddling the border between them. if we want something like
this under serial dependence, the natural approach is to remove a bu   er around
each testing point. the points in this bu   er are not used during training, but just
to provide input values when trying to predict the points in the test set.

25.6.1 testing stationarity by cross-prediction

25.7 trends and de-trending

the sad fact is that a lot of important time series are not even approximately
stationary. for instance, figure 25.13 shows us national income per person (ad-
justed for in   ation) over the period from 1952 (when the data series begins) until
now. it is possible that this is sample from a stationary process. but in that
case, the correlation time is evidently much longer than 50 years, on the order of
centuries, and so the theoretical stationarity is irrelevant for anyone but a very
ambitious quantitative historian     living in our distant future.

more sensibly, we should try to treat data like this as a non-stationary time
series. the conventional approach is try separating time series like this into a
persistent trend, and stationary    uctuations (or deviations) around the trend,

yt = xt + zt

series =    uctuations + trend

(25.42)

since we could add or subtract a constant to each xt without changing whether

15 this is where the metaphor of the    sieve    comes in: the idea is that the mesh of the sieve gets    ner

and    ner, catching more and more subtle features of the data.

zt = e [y  ,t]     m(cid:88)

25.7 trends and de-trending

557
they are stationary, we   ll stipulate that e [xt] = 0, so e [yt] = e [zt]. (in other
situations, the decomposition might be multiplicative instead of additive, etc.)
how might we    nd such a decomposition?

if we have multiple independent realizations yi,t of the same process, say m of
them, and they all have the same trend zt, then we can try to    nd the common
trend by averaging the time series:

yi,t

(25.43)

i=1

multiple time series with the same trend do exist, especially in the experimental
sciences. yi,t might be the measurement of some chemical in a reactor at time t
in the ith repetition of the experiment, and then it would make sense to average
the yi,t to get the common zt trend, the average trajectory of the chemical
concentration. one can tell similar stories about experiments in biology or even
psychology, though those are complicated by the tendency of animals to get tired
and to learn16.

for better or for worse, however, we have only one realization of the post-wwii
us economy, so we can   t average multiple runs of the experiment together. if we
have a theoretical model of the trend, we can try to    t that model. for instance,
some (simple) models of economic growth predict that series like the one in figure
25.13 should, on average, grow at a steady exponential rate17. we could then
estimate zt by    tting a model to yt of the form   0e  t, or even by doing a linear
regression of log yt on t. the    uctuations xt are then taken to be the residuals
of this model.

if we only have one time series (no replicates), and we don   t have a good theory
which tells us what the trend should be, we fall back on curve    tting. in other
words, we regress yt on t, call the    tted values zt, and call the residuals xt. this is
frankly rests more on hope than on theorems. the hope is that the characteristic
time-scale for the    uctuations xt (say, their correlation time    ) is short compared
18. then if we average yt over a
to the characteristic time-scale for the trend zt
band-width which is large compared to    , but small compared to the scale of zt,
we should get something which is mostly zt     there won   t be too much bias
from averaging, and the    uctuations should mostly cancel out.

once we have the    uctuations, and are reasonably satis   ed that they   re sta-

16 even if we do have multiple independent experimental runs, it is very important to get them aligned
in time, so that yi,t and yj,t refer to the same point in time relative to the start of the experiment;
otherwise, averaging them is just mush. it can also be important to ensure that the initial state,
before the experiment, is the same for every run. chu et al. (2003) explains how the later problem
can lead to complications in studying gene regulation.

17 this is not quite what is claimed by solow (1970), which you should read anyway if this kind of

question is at all interesting to you.

like n   1(cid:80)n   1

18 i am being deliberately vague about what    the characteristic time scale of zt    means. intuitively,
it   s the amount of time required for zt to change substantially. you might think of it as something
t=1 1/|zt+1     zt|, if you promise not to treat that too seriously. trying to get an exact

statement of what   s involved in identifying trends requires being very precise, and getting into topics
at the intersection of statistics and functional analysis which are beyond the scope of this class.

558

time series

tionary, we can model them like any other stationary time series. of course, to
actually make predictions, we need to extrapolate the trend, which is a harder
business.

25.7.1 forecasting trends

the problem with making predictions when there is a substantial trend is that
it is usually hard to know how to continue or extrapolate the trend beyond the
last data point. if we are in the situation where we have multiple runs of the
same process, we can at least extrapolate up to the limits of the di   erent runs.
if we have an actual model which tells us that the trend should follow a certain
functional form, and we   ve estimated that model, we can use it to extrapolate.
but if we have found the trend purely through curve-   tting, we have a problem.
suppose that we   ve found the trend by spline smoothing, as in figure 25.16.
the    tted spline model will cheerfully make predictions for the what the trend
of gdp per capita will be in, say, 2252, far outside the data. this will be a
simple linear extrapolation, because splines are always linear outside the data
range (chapter 7, p. 153). this is just because of the way splines are set up, not
because linear extrapolation is such a good idea. had we used kernel regression,
or any of many other ways of    tting the curve, we   d get di   erent extrapolations.
people in 2252 could look back and see whether the spline had    t well, or some
other curve would have done better. (but why would they want to?) right now,
if all we have is curve-   tting, we are in a dubious position even as regards next
year, never mind 225219

25.7.2 seasonal components

sometimes we know that time series contain components which repeat, pretty
exactly, over regular periods. these are called seasonal components, after the
obvious example of trends which cycle each year with the season. but they could
cycle over months, weeks, days, etc.

the decomposition of the process is thus

yt = xt + zt + st

(25.44)

where xt handles the stationary    uctuations, zt the long-term trends, and st the
repeating seasonal component.

if zt = 0, or equivalently if we have a good estimate of it and can subtract

19 yet again, we hit a basic philosophical obstacle, which is the problem of induction. we have so far
evaded it, by assuming that we   re dealing with iid or a stationary id203 distribution; these
assumptions let us deductively extrapolate from past data to future observations, with more or less
con   dence. (for more on this line of thought, see hacking (2001); spanos (2011); gelman and
shalizi (2013).) if we assume a certain form or model for the trend, then again we can deduce future
behavior on that basis. but if we have neither probabilistic nor mechanistic assumptions, we are, to
use a technical term, stuck with induction. whether there is some principle which might help    
perhaps a form of occam   s razor (kelly, 2007)?     is a nice question.

25.7 trends and de-trending

559

it out, we can    nd st by averaging over multiple cycles of the seasonal trend.
suppose that we know the period of the cycle is t , and we can observe m = n/t
full cycles. then

m   1(cid:88)

j=0

st     1
m

yt+jt

(25.45)

this works because, with zt out of the picture, yt = xt + st, and st is periodic,
st = st+t . averaging over multiple cycles, the stationary    uctuations tend to
cancel out (by the ergodic theorem), but the seasonal component does not.

for this trick to work, we need to know the period. if the true t = 355, but

we use t = 365 without thinking20, we can get mush.

we also need to know the over-all trend. of course, if there are seasonal com-
ponents, we really ought to subtract them out before trying to    nd zt. so we
have yet another vicious cycle, or, more optimistically, another case for iterative
approximation.

25.7.3 detrending by di   erencing

suppose that yt has a linear time trend:

yt =   0 +   t + xt

(25.46)

with xt stationary. then if we take the di   erence between successive values of
yt, the trend goes away:

(25.47)
since xt is stationary,    + xt     xt   1 is also stationary. taking di   erences has
removed the trend.

yt     yt   1 =    + xt     xt   1

di   erencing will not only get rid of linear time trends. suppose that

where the    innovations    or    shocks     t are iid, and that

zt = zt   1 +  t

yt = zt + xt

(25.48)

(25.49)

with xt stationary, and independent of the  t. it is easy to check that (i) zt is
not stationary (exercise 25.4), but that (ii) the    rst di   erence

yt     yt   1 =  t + xt     xt   1

(25.50)

is stationary. so di   erencing can get rid of trends which are built out of the
summation of persistent random shocks.

di   erencing gives us another way of making a time series stationary: instead of
trying to model the time trend, take the di   erence between successive values, and
see if that is stationary. (the diff() function in r does this; see figure 25.18.) if

20 exercise: come up with an example of a time series where the periodicity should be 355 days.

560

time series

such       rst di   erences    don   t look stationary, take di   erences among di   erences,
third di   erences, etc., until you have something satisfying.

di   erencing is like taking the discrete version of a derivative. repeated di   er-
encing will eventually get rid of trends if they correspond to curves (e.g., polyno-
mials) with only    nitely many non-zero derivatives. it fails for trends which aren   t
like that, like exponentials or sinusoids, though you can hope that eventually the
higher di   erences are small enough that they don   t matter much.
notice that now we can continue to the trend (a little): once we predict yt+1   yt,

we add it on to yt (which we observed) to get yt+1.

25.7.4 cautions with detrending

the fact that i   ve explained multiple di   erent ways of detrending non-stationary
time series may have made you uneasy: how are you to know which one to use?
my unhelpful answer is    it depends   , namely, on what you think is a plausible
about the trend and the    uctuations around it. (e.g., if you think the trend
is linear, then di   erencing should work.) my advice is to try several di   erent
ways of detrending your data, and to examine them very carefully if they give
substantially di   erent results.

finally, it is worth considering how much damage you might do by de-trending
if the process really is stationary. e.g., if the original series is really uncorrelated,
di   erencing will create correlations     see exercise 25.6, and   25.9.2 on the yule-
slutsky e   ect.

25.7.5 id64 with trends

all the bootstraps discussed in   25.5 work primarily for stationary time series.
(parametric bootstraps are an exception, since we could include trends in the
model.) if we have done extensive de-trending, the reasonable thing to do is to
use a bootstrap to generate a series of    uctuations, add it to the estimated trend,
and then repeat the whole analysis on the new, non-stationary surrogate series,
including the de-trending. this works on the same sort of principle as resampling
residuals in regressions (  6.4, especially 6.4.3).

25.8 breaks in time series

figure 25.19 shows the employment to population ratio21 for the us since 1990.
there are fairly periodic oscillations     it   s not seasonally adjusted     but it
seems to be    uctuating within a not-too-wide band, and then 2008 happens, and
the lesser depression begins.

what should we, as time series analysts, do with something like this? it goes

21 that is, the ratio of the total number of employed people to all people. this is not one minus the

unemployment rate, because the denominator in the unemployment rate excludes those who
wouldn   t be looking for paid work anyway, such as retirees.

25.8 breaks in time series

561

against intuition to say that this sort of abrupt and dramatic break is all part
of a single stationary process, but by this point i hope you are all thoroughly
suspicious of that sort of intuition. the two big routes to dealing with series
which look like this are (1) to treat them as stationary, never mind our gut, or
(2) to give up on global stationarity, to say that sometimes things just change
abruptly.

25.8.1 long memory series

  (h) = o(h     ) for some    > 0. if    is big enough, (cid:80)   

the simplest option for dealing with series that look like figure 25.19 is to say
that they are fairly ordinary stationary time series, except that the decay of
dependence is very slow     that the time series has a long memory. a formal
de   nition of a long-memory time series is one where the covariance function
h=0 |  (h)| is still    nite    
but the slow decay of   (h) means that the sum, and so the correlation time, is
quite large. a large correlation time means that we need to wait a very long time
before any one trajectory becomes representative of the whole system     in this
case, perhaps, several centuries.22

25.8.2 change points and structural breaks

we could of course give up on the idea that all the data come from a single
stationary process. the most popular alternative is the idea of a change point or
structural break. up to some time, call it tb, the process followed one stationary
process. after this change point, it follows a di   erent stationary process, perhaps
bearing no relationship at all to what went before.

if we think we   re dealing with a change point, the natural questions are, when

did it change?, and what does the process look like after the change?

25.8.2.1 change points and long memory

series n   1(cid:80)

suppose that the change-point manifests itself by a shift in the expectation value
of xt, say from   1 before the change to   2 after. the global mean of the time
t xt is the somewhere between   1 and   2. if h is not too large, then
for most t, xt and xt+h will be on the same side of the change point. if they are
both before, then xt and xt+h will both be somewhere around   1, and if both
times are after the point, both values will be around   2. therefore, it will tend
to be the case that either both xt and xt+h are above the global mean, or both
of them are below it     and so they   re correlated. this argument applies even if
the xt are really all independent, as in figure 25.20.

this phenomenon makes it very hard to distinguish empirically between time
series which have change points and those which have a slow decay of dependence.

22 see also   25.9.3.3 on    regime switching    models.

562

time series

25.8.3 change point detection

it is often reasonable to set aside such scruples, assume there are change points,
and try to    nd them. a large number of methods have been developed for this
purpose, often under very strong parametric restrictions     say that xt    iid
n (  1,   2) when t < tb, and xt    iid n (  2,   2) when t     tb. many of these have
the    avor of looking for    runs    of values which are cumulatively very unlikely    
for instance, we might look for a long run of values which are far from   1 and
on the same side of it. other procedures boil down to    will dividing this time
series here, and letting the parameters change, work better?    along those lines,
it is natural to try to use cross-validation, and arlot and celisse (2011) propose
a segmentation algorithm on exactly such a basis.

25.9 time series with latent variables

there are many time-series problems where we want to use a model where one or
more of the time series are latent. the best reason for to do this is, of course, that
there really is a latent process related to the observables, or at least that we think
their might be one. it can also make models more interpretable, if the dynamics
of the latent process are somehow simpler than those of the observables. it is
sometimes done just to increase the capacity of the model, and soak up some
mis-speci   cation error; this is not a good practice, but it is common.

while there an almost in   nite variety of time series models with latent vari-
ables are possible, people generally work with schemes that share some common
features. the most important of these is that these models sum up everything we
need from the past of the process into one (possibly-multidimensional variable),
the state. the state evolves according to a markov process, and the current
state completely    xes the distribution of all present and future observables. we
generally do not get to directly observe the state, so it is latent, but it is sup-
posed to drive, or at least summarize, everything which we do observe. that
is, the state at time t, st, obeys the markov property, so st+1:           s      :t   1|st,
and screens o    the future of the observables from the past of the whole process,
xt:           x      :t   1, s      :t   1|st.

models of this sort are called    state-space models   , or    partially-observable
markov models   . there are, however, (at least) two kinds of models which meet
all these requirements, most easily distinguished through their id114.
figures 25.21 and figure 25.22 show the alternatives. in both cases, st has as its
parent st   1 and as its child st+1, and the s   s form a markov process. in both
cases, st is the sole parent of xt, so that conditioning on st makes xt independent
of all that came before. where they di   er is in xt   s children. in figure 25.21, xt
has no children. what we observe at time t might tell us about how the state
evolves, but it doesn   t change how the state evolves. in figure 25.22, however,
xt and st are the two parents of st+1, which indeed is usually taken to be a
deterministic function of its parents. in this alternative, what we observe at time
t directly a   ects the future state. former alternative (figure 25.21) has come to

25.9 time series with latent variables

563

be called a hidden markov model (id48s), while the latter (figure 25.22) is
known as a chain with complete connections (cccs). in both cases, you can
verify that the x   s do not form a markov process.

while these are both logically possible and mathematically interesting, sta-
tistical practice has favored id48s over cccs, to the point where    state space
model   , when used in time series analysis or econometrics, almost always means
an id48. however, cccs hve important uses, and there doesn   t seem to be any
really deep reason why they are not used more in statistics.q

regardless of which kind of state-space model you want to use, there are four

basic problems for them:
    simulation: how to generate new time series from a fully-parameterized model?
    state estimation: how, given a fully-parameterized model and the time series
    id136: how do we estimate the parameters of the model (or    t it nonpara-
    prediction: how, given a sequence of observations x1:t and a parameterized

metrically), or test guesses about the parameters?

of x   s, to    nd estimates of the latent states s?

model, do we come up with a guess for xt+1?

simulation is straightforward, at least in outline. our general strategy for sim-
ulation (  5.2.1) tells us to start with generating s1, then generate x1 from the
conditional distribution of x1|s1, etc., etc. in short, we write out the dag, and
we generate each variable by conditioning on its parents. there can be clever
tricks for doing simulation faster in special situations, but this is the core idea.
prediction is also straightforward in principle. we would, ideally, like the dis-
tribution p(xt+1|x1:t), since we can calculate any other prediction (say, a 90%
prediction interval) from this. but this is

(cid:88)
(cid:88)
(cid:88)
(cid:88)

s1:t+1

s1:t+1

s1:t+1

p(xt+1|x1:t) =

=

=

=

p(xt+1|s1:t+1, x1:t)p(s1:t+1|x1:t)

p(xt+1|st+1)p(st+1|x1:t, s1:t)p(s1:t|x1:t)

p(xt+1|st+1)p(st+1|xt, st)p(s1:t|x1:t)

p(xt+1|st+1)p(st+1|xt, st)p(st|x1:t)

(25.51)

(25.52)

(25.53)

(25.54)

st:t+1

(for an id48, p(st+1|xt, st) itself simpli   es to p(st+1|st).) thus the key part of
prediction turns out to be doing state estimation, speci   cally    nding the distri-
bution p(st|x1:t).

state estimation is also important for id136, since the observable likelihood,

p(x1:n), is a product of predictive distributions:

p(x1:n) = p(x1)

p(xt|x1:t   1)

(25.55)

n(cid:89)

t=2

q(cid:88)

564

time series

though, it must be said, there are ways of doing id136 which don   t rely on
the likelihood, some of which can side-step state estimation.

the bulk of my treatment of time series models with latent variables will,
therefore, be devoted to state estimation, since it   s crucial to the other statistical
problems with these models. before plunging in to these details, however, it is
instructive to    rst consider the simplest, and historically oldest, time series model
with latent variables, which is just    the time series is a moving average of random
noise    (  25.9.1), and then some more complex examples (  25.9.3), before diving
in (  25.9.4).

25.9.1 moving averages and apparent cycles

the basic equation for a moving average (ma) model of order q, or ma(q) is

xt = zt +

  izt   i

(25.56)

i=1

with the zt being iid noise terms. that is, what we observe is a weighted aver-
age23 of the q + 1 most recent noise variables.
figure 25.23 shows the graphical model for an ma(1) model. it   s evident from
it that xt (cid:54)       xt   1, but xt        xt   k, k > 1     observables are only dependent on
each other through the hidden noise variables, and xt and xt   k have no common
parents. in general, in an ma(q), xt        xt   k when k > q.

suppose that we try to predict xt from its past values. we condition xt on
xt   1, and ask whether there is still more information to be had about xt from
xt   2. this is asking whether xt and xt   2 are dependent, given xt   1. the answer
is clearly yes from figure 25.23: there is one path linking xt to xt   2, and xt   1
is a collider on that path, so conditioning on it activates the path.

why does xt   2 give us information about xt, conditional on xt   1? to deter-
mine xt, we   d need to know zt and zt   1. since xt   1 is a child of zt   1 and zt   2,
knowing xt   1 tells us something about zt   1, but we learn even more from also
knowing xt   2.
nothing daunted, we try conditioning xt on xt   2:t   1. is xt        xt   3|xt   2:t   1?
clearly not. there is again only a single path, which goes over two colliders    
and we condition on both of them, activating the path. knowing xt   3 would
tell us more about zt   3, and that, with xt   2, tells us more about zt   2, which,
together with xt   1, helps us pin down zt   1 even better. the chain of id136s
is getting longer and longer, but it   s not breaking, and it   s evident that it will
never break, no matter how many steps back into the past we condition.

to sum up, an ma(1) process, and by extension any ma(q), is not markov, no
matter what order of markov chain we consider. nonetheless, all of the depen-

23 the right-hand side would look more like a weighted average if we wrote it xt =

but since the zt are latent we could just re-scale each of them by the denominator. (likewise, we
can always impose weight 1 on the most recent z.)

zt+(cid:80)q
1+(cid:80)q

i=1   izt   i

,

i=1   i

25.9 time series with latent variables

565

dence of future on the past is carried by a simple, low-dimensional state variable,
(zt   1, zt). conditional on that, xt is independent of all other xs   s24.

25.9.2 yule-slutsky

applying a moving average to independent noise creates a process with compli-
cated dependence. this fact was noticed independently by two pioneers of time
series analysis, g. udny yule and e. slutsky. it is therefore known as the yule-
slutsky e   ect. but yule and slutsky gave very di   erent interpretations to it    
both are valid in their own circumstances, but the contrast is instructive.

25.9.2.1 slutsky

slutsky was primarily interested under the    uctuations of the economy     in the
business cycle. the way he thought of a moving average process was that the
economy is (under capitalism) continually subjected to random, unpredictable
shocks, but it takes time for the economy to respond to them, for them to work
through the system, as it were. the coe   cients    represent how the economy
responds over time to any given shock. that this leads to    uctuations with a
characteristic amplitude and (nearly) duration was a feature, not a bug     it was
how slutsky proposed to explain the business cycle25. it is not at all clear that
any subsequent theory of the business cycle has any more predictive power (cf.
figure ??).

25.9.2.2 yule

moving averages are of course a very common way of smoothing time series. we
can think of them as being rather like kernel smoothing, but with a one-sided
kernel. that is, we start with our original data zt, and then average it together
locally to get a smoother series xt, with some of the noise removed. what yule
recognized is that doing this will, all by itself, create correlations among the xt
(cf. chapter 4), and complicated predictive relationships. indeed, even if the zt
are all independent of each other, the xt will be correlated, and will have non-
zero id75 coe   cients (or other regression functions, if you use them).
part of what we infer on the xt is then just the e   ects of our smoothing.

this yule e   ect is very basic, and very easy to understand as soon as one sees
figure 25.23, but it continues to trip up researchers in a wide range of applied
   elds26 don   t be like that.

24 because zt   1 and zt are the only parents of xt, which has no descendants.
25 the ussr in the 1920s being what it was, slutsky had to do some fast talking to try to reconcile

this with marxism, and was lucky to be allowed to escape into pure id203 theory (klein, 1997,
pp. 276   279).

26 for instance, martindale (1990); see discussion at http://bactra.org/weblog/666.html.

566

time series

25.9.3 examples of state-space models

25.9.3.1 general gaussian-linear state space model

the classical example of a hidden markov model has a state variable s evolving
linearly, subject to noise,

but what we observe being a noisy, linear function of the hidden state,

st = ast   1 +   t

xt = bxt +  t

(25.57)

(25.58)

it is often assumed that   t and  t are each iid series (generally with di   erent
distributions), and independent of each other. this is the general linear state
space model. if one further assumes that both the dynamical noise    and the
observation noise   are gaussian, then one gets a general gaussian-linear state
space model. when the parameters are known, the kalman    lter provides an
exact, closed-form formula for estimating the latent state st and updating it as
new observations are made (kalman, 1960; kalman and bucy, 1961). this in turn
forms a component of estimating the parameters by maximum likelihood. because
this model is very extensively treated elsewhere, we will say almost nothing more
about it, but refer the interested reader to durbin and koopman (2001) or fraser
(2008).

25.9.3.2 autoregressive-moving average (arma) models

an important theoretical result in the theory of stochastic processes says that
basically any stationary process can be represented as an in   nite-order autore-
gression,

xt =  t +

  jxt   j

(25.59)

where the    innovations     t are serially uncorrelated, and uncorrelated with pre-
vious x   s. this is matched by another result which says that basically any sta-
tionary process can be represented as an in   nite-order moving average process,

xt = mt +

  jzt   j

(25.60)

j=0

where the z   s are serially uncorrelated, and mt is a deterministic linear combi-
nation of previous m   s (hence the lower-case letter).
these two results, about representing stationary processes as either ar(   ) or
ma(   ) processes, are called the    wold decomposition   .

since in   nite series of parameters are not very useful to the practicing statisti-
cian, people had the bright idea of trying to combine the ar and the ma parts,

   (cid:88)

j=1

   (cid:88)

25.9 time series with latent variables

567

to get an arma(p, q) model:

xt =    +   1xt   1 + . . . +   pxt   p

+zt +   1zt   1 + . . . +   qzt   q
=    +       xt   p:t   1 +       zt   q:t

(25.61)

(25.62)
(25.63)

where    is an intercept,    is the vector of autoregressive parameters,    is the
vector of moving average parameters (including, by convention,   0 = 1). figure
25.25 illustrates the dependence structure for an arma(1, 1) model.

estimation of arma models is complicated by the fact that the    we want
here is not the    we   d get from just regressing xt on xt   p:t   1. the reason for this
is evident from figure 25.25: if we condition x4 on, say, x2 and x3, there is an
unblocked back-door path, x3     z3     x4. indeed, by conditioning on x3, we
open a back-door path, x2     z2     x3     z3     x4. more algebraically, eq. ??
clearly implies that

where   t has mean zero, but is also serially correlated, and correlated with xt.
but

xt =    +       xt   p:t   1 +   t

xt     e [xt|xt   p:t   1]

(25.64)

(25.65)

is always uncorrelated with xt   p:t   1, by the general properties of expectations. so
whatever we   d get from a pure autoregression can   t be the    of the arma(p, q),
because plugging in that    will give correlated residuals.

there are, however, ingenious ways to get around this issue. the key trick is
that xt is (assumed to be) a deterministic function of xt   p:t   1 and of zt   q:t. if
we knew everything up to time t     1, our prediction for xt would be

q(cid:88)

j=1

(cid:98)  t =    +       xt   p:t   1 +
zt = xt    (cid:98)  t

  jzt   j

(25.66)

(25.67)

and so

one way to do the estimation, then, is to begin with a purely autoregressive
model, and use it, via eqs. 25.66   25.67, to get initial estimates of the z   s. if
we know all the z   s, we can estimate   ,    and    by ordinary regression. plug-
ging those parameters back in to the equations gives updated z   s, and so forth
(durbin, 1960; hannan and rissanen, 1982). this is not the only way to do it,
particularly if you are willing to assume the z   s are gaussian, but the details of
the various schemes are thoroughly covered in standard time series texts, e.g.,
shumway and sto   er (2000), and not worth rehashing here.

the biggest reason i do not give much space to arma models is that, despite
their popularity, i have rarely seen them work well on real data sets. leaving
to one side gaussian-noise assumptions, the validity of the wold decomposition

568

time series

does not really give any mathematical reason to expect that arma(p, q) models
should work.

finally, it   s worth noting that arma models can be seen as a special case
of the general linear state-space model, where the state st keeps track of all the
necessary information, namely previous values of xt and of zt. there are actually
(at least) two ways to do this; thiesson et al. (2004) describes one which is more
e   cient than the most obvious procedure.

25.9.3.3 regime switching

hidden-state models give us another way of dealing with apparent non-stationarity,
in addition to change-points and long memory processes (  25.8), namely regime
switching. the idea is that there observed time series is in some sense driven
or controlled by a discrete latent variable, the regime, and can show very di   er-
ent dynamics in di   erent regimes. the regime itself evolves according to its own
dynamics, often taken to be markovian. if every regime has a high id203
of transitioning to itself, we will see long stretches of time where the observables
seem to follow one stationary process, punctuated by rare but rapid transitions
to what looks like a realization of a di   erent stationary process. if the markov
chain for regimes is stationary, the over-all process will also be stationary, but
one would, so to speak, need to look over very long time scales to see it.

25.9.3.4 noisily-observed dynamical systems

25.9.4 state estimation

if we want to know what the latent states are, we need to estimate them from the
observables. we might be interested in those states for their own sake, or might
need them as part of our statistical analysis. in the jargon, if we estimate sn, the
state at time n, using only observations made up to that time, x1:n, then we are
doing    ltering. if, on the other hand, we estimate the whole sequence of states,
s1:n, from the whole sequence of observations, then we are doing smoothing27.
the key di   erence is that in smoothing, our estimate of st, t < n, is also informed
by later observations, xt+1:n.

whether we are doing    ltering or smoothing, there is a straightforward formal
solution to the state-estimation problem, which arises from basic id203.
we start with the likelihood of the observable sequence given the (hypothetical)

27 these terms got    xed very early, when the best way to do state estimation was, in fact, to apply

linear smoothers (wiener, 1949).

25.9 time series with latent variables

569

sequence of latent states, and then use bayes   s rule:

p(x1:n|s1:n) =

p(x1:n) =

p(s1:n|x1:n) =

=

n(cid:89)
(cid:88)
p(x1:n|s1:n)p(s1:n)

p(xt|st)
p(x1:n|s1:n)p(s1:n)

s1:n

t=1

p(x1:n)

(cid:80) p(x1:n|s1:n)p(s1:n)
p(x1:n|s1:n)p(s1:n)
n(cid:89)

p(st|st   1)

if the states are markovian, we have in addition

p(s1:n) = p(s1)

t=2

(cid:81)n
(cid:81)n
t=1 p(xt|st)p(st|st   1)
t=1 p(xt|st)p(st|st   1)

s(cid:48)

1:n

(cid:80)

putting it all together,

1|xn
1 ) =
if we only care about p(sn|xn

p(sn

1 ), a further summation takes care of that.

this is a bit easier to work with recursively. it should seem reasonable that if
we know the smoothing distribution at time t, we can easily extrapolate the state
one step forward in time:

p(s1:t+1|x1:t) = p(s1:t|x1:t)p(st+1|s1:t)
and get a predictive distribution for the next observation,

p(xt+1|x1:t) =

p(st+1|x1:t)p(xt+1|st+1)

(cid:88)

s1:t+1

and then we can update the distribution over states once we see xt+1:

p(s1:t+1|x1:t+1) = p(s1:t+1|x1:t)

p(xt+1|st+1)
p(xt+1|x1:t)

similarly for    ltering:

p(st+1|x1:t) =

(cid:88)

st

p(st|x1:t)p(st+1|st)

and

p(st+1|x1:t+1) = p(st+1|x1:t)

p(xt+1|st+1)
p(xt+1|x1:t)

(see exercise 25.8.)

i said that this was just a    formal    solution to the problem, i.e., not a real
solution. eq. 25.73 is unpleasant-looking, not least the prospect of calculating
the denominator     and it generally is hard to actually calculate. in the limited

(25.68)

(25.69)

(25.70)

(25.71)

(25.72)

(25.73)

(25.74)

(25.75)

(25.76)

(25.77)

(25.78)

time series

570
special case of linear, gaussian state-space models (  25.9.3.1), there is a closed-
form solution, given by what is called the    kalman    lter   . for hidden markov
models where both x and s are discrete, we can use the em algorithm, which in
fact was    rst developed for just this use (baum et al., 1970), and so is sometimes
called the    baum-welch algorithm    in this context28.

in general, however, there just isn   t any way of exactly applying eq. 25.73, and
one must consider approximations. some of these are deterministic, such as using
local linear approximations to an underlying nonlinear system, or exploiting the
fact that p(sn|xn
1 ) is often very sharply peaked around the most probable value
(koyama et al., 2010).

25.9.4.1 id143ing

25.9.4.2 parameter estimation

much of the e   ort of the em algorithm and of particle    ltering goes into esti-
mating the time-evolution of the latent state. if what we are willing to ignore
that, and just focus on estimating the parameters, we can sometimes save greatly
on time and e   ort by using techniques of simulation-based id136, basically
adjusting the parameters until simulated trajectories of the model look like the
data; see chapter 26 for details. we could then always go back and estimate the
states for one parameter value, or a range that re   ects our uncertainty.

25.9.4.3 prediction

25.10 longitudinal data

25.11 multivariate time series

25.12 further reading

shumway and sto   er (2000) is a good introduction to conventional time series
analysis, covering r practicalities. in particular, it includes both arma models,
and the very important subject of frequency-domain methods, which i have de-
liberately omitted because it relies on fourier analysis, otherwise not needed for
this book. on the history of how that standard machinery came to be standard,
and why it seemed like a good idea, i strongly recommend klein (1997), which is
also just one of the best books i   ve seen on the history of statistics and statistical
reasoning.

returning to textbooks, lindsey (2004) surveys a broader range of situations
in less depth; it is readable, but opinionated, and i don   t always agree with
the opinions. (try to contain your surprise.) fan and yao (2003) is a deservedly-
standard reference on nonparametric time series models. the theoretical portions
would be challenging for most readers of this book, but the methodology isn   t,

28 the origin of the name is curious. you may notice that welch is not an author of baum et al.

(1970). that paper cites a submitted manuscript by baum and welch on    a statistical estimation
procedure for probabilistic functions of    nite markov processes   , which seems never to have been
published. in his shannon lecture, welch (2003) disclaims having done more than the    easy part   
(p. 12) of coming up with the idea, and showing that it worked in some particular cases. this
engaging lecture also gives an excellent overview of the algorithm.

25.12 further reading

571

and it devotes about the right amount of space (i.e., little) to the usual linear-
model theory. douc et al. (2014) plays a similar role for parametric nonlinear
statistical models; part ii in particular is a self-contained treatment of stochastic
process theory, and part iii of particle    lters.

the best introduction to stochastic processes i know of, by a very wide mar-
gin, is grimmett and stirzaker (1992). however, like most textbooks on stochastic
processes, it says next to nothing about how to use them as models of data. a no-
table exception is the excellent guttorp (1995), which both introduces the theory
of a range of highly-applicable stochastic processes, and covers their statistical
id136 with real scienti   c examples. bartlett (1955), while similar in intent, is
old enough that it now makes a better second book than a    rst.
the basic ergodic theorem in   25.2.2.1 follows a continuous-time argument
in frisch (1995), which seems to go back to taylor (1922). exercise 25.7 gives
an extension to non-stationary processes. my general treatment of ergodicity is
heavily shaped by gray (1988) and shields (1996).
the block bootstrap was introduced by k  unsch (1989). davison and hinkley
(1997,   8.2) has a clear treatment of the main    avors of bootstrap for time series;
lahiri (2003) is thorough but theoretical. b  uhlmann (2002) is also useful.

on cross-validation for time series, important references include burman et al.
(1994), racine (2000), and carmack et al. (2009). lunde and shalizi (2017) pro-
poses a bootstrap method for putting con   dence intervals on the prediction error,
as an alternative to cross-validation.

arma models have spawned a huge number of modi   cations, extensions, and
re-interpretations. holan et al. (2010) is a recent survey of this    alphabet soup   
of a lineage.

the notion of    state    used in state-space models ultimately derives from physics
and from the mathematical theory of dynamical systems. the transition to state-
space models in the current, statistical sense seems to have been made by engi-
neers, who had to contend with imperfect measurements of the system and the
possibility that it wads disturbed by noise, rather than evolving deterministically.
see, for instance, mcgee and schmidt (1985) for an account of how state-space
modeling was adopted by the us space program.

in parallel to the treatment of time series by statisticians, physicists and math-
ematicians developed their own tradition of time-series analysis (packard et al.,
1980), where the basic models are not stochastic processes but deterministic, yet
unstable, dynamical systems. the focus of this work is exactly recovering the la-
tent state space from observables, with prediction often made by nearest-neighbor
methods in the state space (sometimes called the    method of analogues    in this
literature). perhaps the best guides to this are abarbanel (1996); kantz and
schreiber (2004). there are in fact very deep connections between this approach
and the question of why id203 theory works in the    rst place (ruelle, 1991),
but that   s not a subject for data analysis.

a natural mathematical question is to ask which stochastic processes have
state-space representations. it turns out that the answer is    basically all of them   ,
and that there is a uniquely optimal representation for each original process. the

572

time series

basic idea is to start by considering the conditional distribution over future events,
st     pr (xt+1:   |x      :t). this is a well-de   ned random object, albeit one whose
value is a distribution over the in   nite sequence xt+1:   . we can then inquire
into the properties of the prediction process . . . , st   1, st, st+1, . . .. one can
show that this is always a markov process, that xt+1:           s      :t   1, x      :t|st,
and that this is, in several precise senses, the simplest possible process with
both those properties. you can also show that there   s a deterministic function
q such that st+1 = q(st, xt+1). thus, every stochastic process has a unique,
optimal representation as a state-space process, where the states are predictive
distributions for the original process, and this process is a chain with complete
connections. this construction, or one mathematically equivalent to it, has been
independent discovered by crutch   eld and young (1989), jaeger (2000), knight
(1975), langford et al. (2009) and littman et al. (2002) (that i know of). of these,
the treatment by knight (1975) is the oldest, and most mathematically general.
shalizi and crutch   eld (2001) proves the information-theoretic optimality of the
prediction process, and shalizi (2003) extends it to spatio-temporal processes29.
throughout this chapter, i assumed that the time series records some variable
(or variables) at regular time points, or perhaps continuously over some interval
of time. another very important kind of temporal data records the instants of
time at which events happen     what are called point processes. (we might
distinguish among several types of events, called marked point processes.)
guttorp (1995, ch. 5) is a good starting point; daley and vere-jones (2003) is a
standard reference on some of the deeper intricacies of the subject.

exercises

25.1 write a function which takes in a time series x and makes a plot of xt+1 versus xt, as

in figure 25.3. hint: use code example 40.

25.2 1. prove that maximizing the log-likelihood in eq. 25.25, under the constraints of eq.
25.26, leads to the id113 in eq. 25.27. hint: use lagrange multipliers, and solve for the
value of the multipliers.

2. transition rates, being probabilities, must be non-negative, pij     0. explain why you

do not need to add yet more lagrange multipliers to enforce this constraint.

3. eq. 25.27 presumes that the elements pij in the p matrix can vary independently
(subject to the constraints). suppose instead that they are all functions of a lower-
dimensional parameter vector   . find an expression for the id113 of   . do you still
need the lagrange multipliers?

25.3 in eq. 25.40, assume that m(x) has to be a linear function, m(x) =       x. solve for the
optimal    in terms of y, x, and   . this    generalized least squares    (gls) solution should
reduce to ordinary least squares when    =   2i.

25.4 if zt = zt   1 +  t, with  t iid, prove that zt is not stationary. hint: consider v [zt].
25.5 start with rblockboot from code example 42.

1. modify the function to perform the circular block bootstrap. (hint: extend ts.)

29 if this paragraph is not more than you ever wanted to know, see

http://bactra.org/notebooks/prediction-process.

exercises

573

2. modify the function to work with multivariate time series, given as an array with time
points as the rows and variables as the columns. ensure that the same blocks are used
for all variables, to preserve dependencies across them.

3. modify the function to work with multivariate time series, given as a collection of
univariate time series. again, make sure the same blocks are used for all series. (hint:
reduce to the previous sub-exercise.)

25.6 suppose that xi are iid, but we di   erence them and so look at yi = xi     xi   1. find

the autocovariance function of the y series, in terms of the moments of the xi.

25.7 a non-stationary ergodic theorem suppose that the xt are non-stationary, but they all
have    nite (not necessarily equal) means e [xt], and    nite covariances cov [xt, xs]. de   ne

and

show that if vn = o(n2), then

t=1

mn     1
n

n(cid:88)
n(cid:88)
n(cid:88)
e(cid:104)(cid:0)mn     x n

s=1

t=1

vn    

e [xt]

cov [xt, xs]

(cid:1)2(cid:105)     0

(25.81)
and so that x n     mn. does this result imply eq. 25.15 under the conditions of   25.2.2.1?
could you deduce this result from eq. 25.15?

25.8 recursive equations for state estimation

1. derive eq. 25.74. hint: first show that st+1        x1:t|st.
2. derive eq. 25.75. hint: first show that xt+1        x1:t|st.
3. derive eq. 25.76.
4. derive eq. 25.77.
5. derive eq. 25.78.

(25.79)

(25.80)

574

time series

par(mfrow = c(1, 2))
acf(lynx)
acf(y)
par(mfrow = c(1, 1))

figure 25.2 autocorrelation functions of the lynx data (above) and the
simulation (below). the acf function plots the autocorrelation function as
an automatic side-e   ect; it actually returns the actual value of the
autocorrelations, which you can capture. the 95% con   dence interval
around zero is computed under gaussian assumptions which shouldn   t be
taken too seriously, unless the sample size is quite large, but are useful as
guides to the eye.

05101520   0.50.00.51.0lagacfseries  lynx0510152025300.00.20.40.60.81.0lagacfseries  yexercises

575

par(mfrow = c(1, 2))
plot(lag0 ~ lag1, data = design.matrix.from.ts(lynx, 1), xlab = expression(lynx[t]),

ylab = expression(lynx[t + 1]), pch = 16)

plot(lag0 ~ lag1, data = design.matrix.from.ts(y, 1), xlab = expression(y[t]),

ylab = expression(y[t + 1]), pch = 16)

par(mfrow = c(1, 1))

figure 25.3 plots of xt+1 versus xt, for the lynx (left) and the simulation
(right); see exercise 25.1. note that even though the correlation between
successive iterates is next to zero for the simulation, there is clearly a lot of
dependence (see appendix e.4).

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll020004000600001000200030004000500060007000lynxtlynxt+1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.40.80.00.20.40.60.81.0ytyt+1576

time series

t

x

269
321
585
871
1475
2821
3928
5943
4950

1821
1822
1823
1824
1825
1826
1827
1828
1829
. . .

lag0

lag1

lag2

lag3

   

871
1475
2821
3928
5943
4950
. . .

585
871
1475
2821
3928
5943

321
585
871
1475
2821
3928

269
321
585
871
1475
2821

figure 25.4 turning a time series (here, the beginning of lynx) into a
regression-suitable matrix.

exercises

577

plot(lag0 ~ lag1, data = design.matrix.from.ts(y, 1), xlab = expression(y[t]),

ylab = expression(y[t + 1]), pch = 16)

abline(lm(lag0 ~ lag1, data = design.matrix.from.ts(y, 1)), col = "red")
yaar1 <- aar(y, order = 1)
points(y[-length(y)], fitted(yaar1), col = "blue")

figure 25.5 plotting successive values of the arti   cial time series against
each other, along with the id75, and a spline curve (see below for
the aar function, which    ts additive autoregressive models; with order=1, it
just    ts a spline.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0ytyt+1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll578

time series

plot(lynx.aar2, pages = 1)

figure 25.6 partial response functions for the second-order additive
autoregression model of the lynx. notice that a high count last year predicts
a higher count this year, but a high count two years ago predicts a lower
count this year. this is the sort of alternation which will tend to drive
oscillations.

0200040006000   4000   20000200040006000lag1s(lag1,7.92)0200040006000   4000   20000200040006000lag2s(lag2,3.25)exercises

579

plot(lynx)
lines(1823:1934, fitted(lynx.aar2), lty = "dashed")

figure 25.7 actual time series (solid line) and predicted values (dashed)
for the second-order additive autoregression model of the lynx. the match is
quite good, but of course every one of these points was used to learn the
model, so it   s not quite as impressive as all that. (also, the occasional
prediction of a negative number of lynxes is less than ideal.)

timelynx18201840186018801900192001000200030004000500060007000580

time series

lynx.aar2b <- aar(lynx[1:80], 2)
out.of.sample <- design.matrix.from.ts(lynx[-(1:78)], 2)
lynx.preds <- predict(lynx.aar2b, newdata = out.of.sample)
plot(lynx)
lines(1823:1900, fitted(lynx.aar2b), lty = "dashed")
lines(1901:1934, lynx.preds, col = "grey")

figure 25.8 out-of-sample forecasting. the same model speci   cation as
before is estimated on the    rst 80 years of the lynx data, then used to
predict the remaining 34 years. solid black line, data; dashed line, the
in-sample prediction on the training data; grey lines, predictions on the
testing data. the rms errors are 723 lynxes/year in-sample, 922
lynxes/year out-of-sample.

timelynx18201840186018801900192001000200030004000500060007000exercises

581

library(tseries)
yar8 <- arma(y, order = c(8, 0))
points(y[-length(y)], fitted(yar8)[-1], col = "red")

figure 25.9 adding the predictions of an eighth-order linear ar model
(red dots) to figure 25.5. we will see the arma function in more detail in
  25.9.3.2; for now, it   s enough to know that when the second component of
its order argument is 0, it estimates and    ts a linear ar model.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.20.40.60.81.0ytyt+1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll582

time series

plot(lynx, ylim = c(-500, 10000))
sd1 <- sqrt(fitted(lynx.condvar1))
lines(1823:1934, fitted(lynx.aar2) + 2 * sd1, col = "grey")
lines(1823:1934, fitted(lynx.aar2) - 2 * sd1, col = "grey")
lines(1823:1934, sd1, lty = "dotted")

figure 25.10 the lynx data (black line), together with the predictions of
the additive autoregression   2 conditional standard deviations. the dotted
line shows how the conditional standard deviation changes over time; notice
how it ticks upwards around the big spikes in population.

timelynx1820184018601880190019200200040006000800010000exercises

583

t

x

1821
1822
1823
1824
1825
1826
1827
1828

269
321
585
871
1475
2821
3928
5943

   

lag2

lag1

lag0

269
321
585
871
1475
2821

321
585
871
1475
2821
3928

585
871
1475
2821
3928
5943

   

lag2

lag1

lag0

269
871
585

321
1475
871

585
2821
1475

   

t

  x

1821
1822
1823
1824
1825
1826
1827
1828

269
321
585
871
1475
2821
585
871

figure 25.11 scheme for block id64: turn the time series (here,
the    rst eight years of lynx) into blocks of consecutive values; randomly
resample enough of these blocks to get a series as long as the original; then
string the blocks together in order. see code example 42 for code.

584

time series

plot(lynx)
lines(1821:1934, rblockboot(lynx, 4), col = "blue")

figure 25.12 the lynx time series, and one run of resampling it with a
block bootstrap, block length = 4.

timelynx18201840186018801900192001000200030004000500060007000exercises

585

## warning: package    xts    was built under r version 3.4.4

## loading required package: zoo

## warning: package    zoo    was built under r version 3.4.4

##

## attaching package:    zoo   

## the following objects are masked from    package:base   :

##

as.date, as.date.numeric

##

library(pdfetch)
gdppc.fred <- pdfetch_fred("a939rx0q048sbea")
library(xts)
library(lubridate)
gdppc <- data.frame(year = decimal_date(index(gdppc.fred)), y = as.numeric(gdppc.fred))
plot(gdppc, log = "y", type = "l", ylab = "gdp per capita (constant 2012 dollars)")

figure 25.13 us gdp per capita, adjusted for in   ation (consumer price
index de   ator), with a log scale on the vertical axis. (the values were
initially recorded in the    le in millions of dollars per person per year, hence
the correction.)

1950196019701980199020002010202020000300004000050000yeargdp per capita (constant 2012 dollars)586

time series

gdppc.exp <- lm(log(y) ~ year, data = gdppc)
beta0 <- exp(coefficients(gdppc.exp)[1])
beta <- coefficients(gdppc.exp)[2]
curve(beta0 * exp(beta * x), lty = "dashed", add = true)

figure 25.14 as in figure 25.13, but with an exponential trend    tted.

1950196019701980199020002010202020000300004000050000yeargdp per capita (constant 2012 dollars)exercises

587

plot(gdppc$year, residuals(gdppc.exp), xlab = "year", ylab = "logged fluctuation around trend",

type = "l", lty = "dashed")

figure 25.15 the hopefully-stationary    uctuations around the exponential
growth trend in figure 25.14. note that these are log yt
    0e     t , and so unitless.

19501960197019801990200020102020   0.10   0.050.000.05yearlogged fluctuation around trend588

time series

gdp.spline <- fitted(gam(y ~ s(year), data = gdppc))
lines(gdppc$year, gdp.spline, lty = "dotted")

figure 25.16 figure 25.14, but with the addition of a spline curve for the
time trend (dotted line). this is, perhaps unsurprisingly, not all that
di   erent from the simple exponential-growth trend.

1950196019701980199020002010202020000300004000050000yeargdp per capita (constant 2012 dollars)exercises

589

lines(gdppc$year, log(gdppc$y/gdp.spline), xlab = "year", ylab = "logged fluctuations around trend",

lty = "dotted")

figure 25.17 adding the logged deviations from the spline trend (dotted)
to figure 25.15.

19501960197019801990200020102020   0.10   0.050.000.05yearlogged fluctuation around trend590

time series

plot(gdppc$year[-1], diff(log(gdppc$y)), type = "l", xlab = "year", ylab = "differenced log gdp per capita")

figure 25.18 first di   erences of log gdp per capita, i.e., the year-to-year
growth rate of gdp per capita.

19501960197019801990200020102020   0.03   0.02   0.010.000.010.020.03yeardifferenced log gdp per capitaexercises

591

epr.fred <- pdfetch_fred("lnu02300000")
epr <- data.frame(year = decimal_date(index(epr.fred)), epr = as.numeric(epr.fred))
epr <- epr[epr$year > 1989, ]
plot(epr, ylab = "percent", ylim = c(50, 70), main = "employment to population ratio",

type = "l")

figure 25.19 monthly employment to population ratio for the us, in
percent, without seasonal adjustment, from 1990 forward. (source: series
lnu02300000 from fred,
https://fred.stlouisfed.org/series/lnu02300000.)

1990199520002005201020155055606570employment to population ratioyearpercent592

time series

par(mfrow = c(2, 2))
before_crash <- (epr$year < 2009)
after_crash <- (epr$year >= 2009)
epr_before <- epr$epr[before_crash]
epr_after <- epr$epr[after_crash]
pre <- rnorm(sum(before_crash), mean(epr_before), sd(epr_before))
post <- rnorm(sum(after_crash), mean(epr_after), sd(epr_after))
change <- data.frame(year = epr$year, epr = c(pre, post))
plot(change, ylab = "", type = "l")
acf(change$epr, lag.max = 50, main = "acf of surrogate series")
acf(epr$epr, lag.max = 50, main = "acf of actual data")
par(mfrow = c(1, 1))

figure 25.20 a time series with a change-point. before and after the
change point, the series is an iid sequence of gaussians, but both the
expected value and the variance switch at the change-point. (these are
matched to the employment-population ratio   s values up to 2008 and after
2008.) the middle panel shows the resulting autocorrelation function. the
bottom panel shows the actual acf of the employment-population ratio.
there is more correlation in the data than the change-point alone can
account for, but it comes close.

19902000201058606264year010203040500.00.20.40.60.81.0lagacfacf of surrogate series010203040500.00.20.40.60.81.0lagacfacf of actual dataexercises

593

st   2

xt   2

st   1

xt   1

st

xt

figure 25.21 dag for id48. the current state st is the
only parent of the current observation xt and the only parent of the next
state st+1. thus the s   s are markovian, the x   s are not,
xt        s      :t   1, x      :t   1|st, and xt+1:   , st+1:           xt|st.

st   2

xt   2

st   1

xt   1

st

xt

figure 25.22 dag for a chain with complete connections. the current
state st is the only parent of the current observation xt, and those two
together are the only parents of the next state st+1. (in fact, they   re usually
assumed to    x st+1 deterministically.) the s   s are markovian, the x   s are
not, xt        s      :t   1, x      :t   1|st, and xt+1:   , st+1:    (cid:54)       xt|st.

. . .

zt   3

xt   3

zt   2

xt   2

zt   1

xt   1

zt

xt

figure 25.23 the dag for a    rst-order moving average model.

594

time series

gdppc.ma4 <- arma(x = residuals(gdppc.exp), order = c(0, 4))
plot(gdppc$year, residuals(gdppc.exp), type = "l", xlab = "year", ylab = "logged fluctuations in real us gdp per capita")
lines(gdppc$year, fitted(gdppc.ma4), col = "grey", lwd = 2)

figure 25.24 logged    uctuations for the united states   s gdp per capita
(with exponential trend removed, as in figure 25.15), versus a fourth-order
moving average model. (since each unit of time is a quarter, four quarters is
a year.) the root-mean-squared error, in sample, is 0.013, corresponding to
an r2 of 0.75. (but you know better than to rely on r2.

19501960197019801990200020102020   0.10   0.050.000.05yearlogged fluctuations in real us gdp per capitaexercises

595

. . .

. . .

zt   3

xt   3

zt   2

xt   2

zt   1

xt   1

zt

xt

figure 25.25 the dag for an arma(1, 1) model.

26

simulation-based id136

checking whether the model   s simulation output looks like the data (  5.4.2) nat-
urally suggests the idea of adjusting the model until it does. this becomes a
way of estimating the model     in the jargon, simulation-based id136. all
forms of this involve tweaking parameters of the model until the simulations do
look like the data, but di   er in what, concretely,    looking like the data    means.

26.1 the method of simulated moments

the most straightforward form of simulation-based id136 is the method of
simulated moments, which builds of the method of moments you   ll have
seen in earlier statistics classes.

26.1.1 the method of moments

we have a model with a parameter vector   , and pick a vector m of moments
to calculate. the moments, like the expectation of any variables, are functions of
the parameters,

m = g(  )

(26.1)

for some function g. if that g is invertible, then we can recover the parameters
from the moments,

   = g   1(m)

(26.2)

the method of moments estimator takes the observed, sample moments   m, and
plugs them into eq. 26.2:

(cid:91)  mm = g   1(   m)

(26.3)
what if g   1 is hard to calculate     if it   s hard to explicitly solve for parameters
from moments? in that case, we can use minimization:
(cid:107)g(  )       m(cid:107)2

(cid:91)  mm = argmin

(26.4)

for the minimization version, we just have to calculate moments from parameters
g(  ), not vice versa. to see that eqs. 26.3 and 26.4 do the same thing, notice that

  

596

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

26.1 the method of simulated moments

597
(i) the squared1 distance (cid:107)g(  )       m(cid:107)2     0, (ii) the distance is only zero when the
moments are matched exactly, and (iii) there is only one    which will match the
moments.

in either version, inversion or minimization, the method of moments works
statistically because the sample moments   m converge on their expectations g(  )
as we get more and more data (appendix h.5). this is, generally, a consequence
of the law of large numbers or ergodic theorem.

it   s worth noting that nothing in this argument says that m has to be a vector
of moments in the strict sense. they could be expectations of any functions of
the random variables, so long as g(  ) is invertible, we can calculate the sample
expectations of these functions from the data, and the sample expectations con-
verge. when m isn   t just a vector of moments, then, we have the generalized
method of moments.

it is also worth noting that there   s a somewhat more general version of the

same method, where we minimize

(g(  )       m)    w (g(  )       m)

(26.5)

with some positive-de   nite weight matrix w. this can help if some of the moments
are much more sensitive to the parameters than others.

26.1.2 adding in the simulation

all of this supposes that we know how to calculate g(  )     that we can    nd the
moments exactly. even if this is too hard, however, we could always simulate to
approximate these expectations, and try to match the simulated moments to the
real ones. rather than eq. 26.4, the estimator would be
(cid:107)  gs,t (  )       m(cid:107)2

(cid:92)  smm = argmin

(26.6)

  

with s being the number of simulation paths and t being their size. now consis-
tency requires that   g     g, either as t grows or s or both, but this is generally
assured by the law of large numbers, as before. simulated method of moments
estimates like this are generally more uncertain than ones which don   t rely on
simulation, since there   s an extra layer of approximation, but this can be reduced
by increasing s.2

1 why squared? basically because it makes the function we   re minimizing smoother, and the

optimization nicer.

2 a common trick is to    x t at the actual sample size n, and then to increase s as much as

computationally feasible. by looking at the variance of   g across di   erent runs of the model with the
same   , one gets an idea of how much uncertainty there is in   m itself, and so of how precisely one
should expect to be able to match it. if the optimizer has gotten |  g(  )       m| down to 0.02, and the
standard deviation of   g at constant    is 0.1, further e   ort at optimization is probably wasted.

598

simulation-based id136

26.1.3 an example: moving average models and the stock market

to give a concrete example, we will try    tting a time series model to the stock
market: it   s a familiar subject which interests most students, and we can check
the method of simulated moments here against other estimation techniques.

our data will consist of about ten year   s worth of daily values for the s& p

500 stock index, previously seen in chapter 7:

sp <- pdfetch_yahoo("spy", fields = "adjclose", from = as.date("1993-02-09"),

to = as.date("2018-02-09"))

sp <- diff(log(sp))
sp <- sp[-1]

professionals in    nance do not care so much about the sequence of prices pt,
as the sequence of returns, pt   pt   1
. this is because making $1000 is a lot better
when you invested $1000 than when you invested $1,000,000, but 10% is 10%. in
fact, it   s often easier to deal with the log returns, xt = log pt
, as we do here.
pt   1
the model we will    t is a    rst-order moving average, or ma(1) model
(  25.9.1):

pt   1

xt = zt +   zt   1
zt     n (0,   2) i.i.d.

(26.7)
(26.8)

the xt sequence of variables are the returns we see; the zt variables are invisible
to us. the interpretation of the model is as follows. prices in the stock market
change in response to news that a   ects the prospects of the companies listed, as
well as news about changes in over-all economic conditions. zt represents this
   ow of news, good and bad. it makes sense that zt is uncorrelated, because
the relevant part of the news is only what everyone hadn   t already worked out
from older information3. however, it does take some time for the news to be
assimilated, and this is why zt   1 contributes to xt. a negative contribution,
   < 0, would seem to indicate a    correction    to the reaction to the previous day   s
news.

mathematically, notice that since zt and   zt   1 are independent gaussians, xt
is a gaussian with mean 0 and variance   2 +   2  2. the marginal distribution of
xt is therefore the same for all t. for technical reasons4, we can really only get
sensible behavior from the model when    1            1.

there are two parameters,    and   2, so we need two moments for estimation.

3 nobody will ever say    what? it   s snowing in pittsburgh in february? call my broker!   
4 think about trying to recover zt, if we knew   . one might try xt       xt   1, which is almost right,
it   s zt +   zt   1       zt   1       2zt   2 = zt       2zt   2. similarly, xt       xt   1 +   2xt   2 = zt +   3zt   2,
and so forth. if |  | < 1, then this sequence of approximations will converge on zt; if not, then not. it
turns out that models which are not    invertible    in this way are very strange     see shumway and
sto   er (2000).

26.1 the method of simulated moments

599

let   s try v [xt] and cov [xt, xt   1].

v [xt] = v [zt] +   2v [zt   1]

cov [xt, xt   1] = e [(zt +   zt   1)(zt   1 +   zt   2)]

=   2 +   2  2
=   2(1 +   2)     v(  ,   )

=   e(cid:2)z 2

t   1

(cid:3)

=     2     c(  ,   )

(26.9)
(26.10)
(26.11)
(26.12)
(26.13)
(26.14)

we can solve the system of equations for the parameters, starting with elimi-

nating   2:

c(  ,   )
v(  ,   )

=

  2  

  2(1 +   2)

this is a quadratic in   ,

   =

1 +   2

       +

  

=
0 =   2 c
v

1   (cid:113)

1     4 c2
2c/v

v2

c
v

(26.15)

(26.16)

(26.17)

(26.18)

and it   s easy to con   rm5 that this has only one solution in the meaningful range,
   1            1. having found   , we solve for   2,
  2 = c/  

(26.19)

leads to     mm =    8.28    10   2, and (cid:99)  2

the method of moments estimator takes the sample values of these moments,
  v and   c, and plugs them in to eqs. 26.18 and 26.19. with the s& p returns, the
sample covariance is    1.61    10   5, and the sample variance 1.96    10   4. this
mm = 1.95    10   4. in terms of the model,
then, each day   s news has a follow-on impact on prices which is about 8% as large
as its impact the    rst day, but with the opposite sign.6

if we did not know how to solve a quadratic equation, we could use the mini-

mization version of the method of moments estimator:
  2         c

= argmin

  ,  2

  2(1 +   2)       v

(cid:35)

(cid:34) (cid:98)  mm(cid:91)

  2

mm

(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)2

(26.20)

computationally, it would go something like code example 43.

5 for example, plot c/v as a function of   , and observe that any horizontal line cuts the graph at only

one point.

6 it would be natural to wonder whether (cid:91)  mm is really signi   cantly di   erent from zero. assuming

gaussian noise, one could, in principle, calculate the id203 that even though    = 0, by chance
  c/  v was so far from zero as to give us our estimate. as you will see in the homework, however,
gaussian assumptions are very bad for this data. this sort of thing is why we have id64.

600

simulation-based id136

ma.mm.est <- function(c, v) {

theta.0 <- c/v
sigma2.0 <- v
fit <- optim(par = c(theta.0, sigma2.0), fn = ma.mm.objective, c = c, v = v)
return(fit)

}
ma.mm.objective <- function(params, c, v) {

theta <- params[1]
sigma2 <- params[2]
c.pred <- theta * sigma2
v.pred <- sigma2 * (1 + theta^2)
return((c - c.pred)^2 + (v - v.pred)^2)

}

code example 43: code for implementing method of moments estimation of a    rst-order
moving average model, as in eq. 26.20. see app. l.9.7 for    design notes   , and the online code
for comments.

rma <- function(n, theta, sigma2, s = 1) {

z <- replicate(s, rnorm(n = n + 1, mean = 0, sd = sqrt(sigma2)))
x <- z[-1, ] + theta * z[-(n + 1), ]
return(x)

}

code example 44: function which simulates s independent runs of a    rst-order moving average
model, each of length n, with given noise variance sigma2 and after-e   ect theta. see online for
comments.

the parameters estimated by minimization agree with those from direct algebra
to four signi   cant    gures, which i hope is good enough to reassure you that this
works.

before we can try out the method of simulated moments, we have to    gure out
how to simulate our model. xt is a deterministic function of zt and zt   1, so our
general strategy (  5.2.1) says to    rst generate the zt, and then compute xt from
that. but here the zt are just a sequence of independent gaussians, which is a
solved problem for us. the one wrinkle is that to get our    rst value x1, we need
a previous value z0. code example 44 shows the solution.

what we need to extract from the simulation are the variance and the co-
variance. it will be more convenient to have functions which calculate these call
rma() themselves (code example 45).

figure 26.1 plots the covariance, the variance, and their ratio as functions of   
with   2 = 1, showing both the values obtained from simulation and the theoretical
ones.7 the agreement is quite good, though of course not quite perfect.8

7 i could also have varied   2 and made 3d plots, but that would have been more work. also, the

variance and covariance are both proportional to   2, so the shapes of the    gures would all be the
same.

8 if you look at those    gures and think    why not do a nonparametric regression of the simulated

moments against the parameters and use the    tted values as   g, it   ll get rid of some of the simulation
noise?   , congratulations, you   ve just discovered the smoothed method of simulated moments.

26.1 the method of simulated moments

601

par(mfrow = c(2, 2))
theta.grid <- seq(from = -1, to = 1, length.out = 300)
cov.grid <- sapply(theta.grid, sim.cov, sigma2 = 1, n = length(sp), s = 10)
plot(theta.grid, cov.grid, xlab = expression(theta), ylab = "covariance")
abline(0, 1, col = "grey", lwd = 3)
var.grid <- sapply(theta.grid, sim.var, sigma2 = 1, n = length(sp), s = 10)
plot(theta.grid, var.grid, xlab = expression(theta), ylab = "variance")
curve((1 + x^2), col = "grey", lwd = 3, add = true)
plot(theta.grid, cov.grid/var.grid, xlab = expression(theta), ylab = "ratio of covariance to variance")
curve(x/(1 + x^2), col = "grey", lwd = 3, add = true)
par(mfrow = c(1, 1))

figure 26.1 plots of the covariance, the variance, and their ratio as a
function of   , with   2 = 1. dots show simulation values (averaging 10
realizations each as long as the data), the grey curves the exact calculations.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   1.0   0.50.00.51.0   1.0   0.50.00.51.0qcovariancellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   1.0   0.50.00.51.01.01.21.41.61.82.0qvariancellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   1.0   0.50.00.51.0   0.40.00.20.4qratio of covariance to variance602

simulation-based id136

sim.var <- function(n, theta, sigma2, s = 1) {

vars <- apply(rma(n, theta, sigma2, s), 2, var)
return(mean(vars))

}
sim.cov <- function(n, theta, sigma2, s = 1) {

x <- rma(n, theta, sigma2, s)
covs <- colmeans(x[-1, ] * x[-n, ])
return(mean(covs))

}

code example 45: functions for calculating the variance and covariance for speci   ed parameter
values from simulations.

ma.msm.est <- function(c, v, n, s) {

theta.0 <- c/v
sigma2.0 <- v
fit <- optim(par = c(theta.0, sigma2.0), fn = ma.msm.objective, c = c, v = v,

n = n, s = s)

return(fit)

}
ma.msm.objective <- function(params, c, v, n, s) {

theta <- params[1]
sigma2 <- params[2]
c.pred <- sim.cov(n, theta, sigma2, s)
v.pred <- sim.var(n, theta, sigma2, s)
return((c - c.pred)^2 + (v - v.pred)^2)

}

code example 46: code for implementing the method of simulated moments estimation of a
   rst-order moving average model.

conceptually, we could estimate    by jut taking the observed value   c/  v, running
a horizontal line across figure 26.1c, and seeing at what    it hit one of the
simulation dots. of course, there might not be one it hits exactly...

the more practical approach is code example 46. the code is practically
identical to that in code example 43, except that the variance and covariance
predicted by given parameter settings now come from simulating those settings,
not an exact calculation. also, we have to say how long a simulation to run, and
when i run this, with s=100, i get (cid:98)  m sm =    8.36    10   2 and (cid:98)  2
how many simulations to average over per parameter value.
m sm = 1.94   
10   4, which is quite close to the non-simulated method of moments estimate.
after the more general class of models including ma models), which claims(cid:98)  m l =
in fact, in this case there is actually a maximum likelihood estimator (arima(),
   9.75    10   2 and (cid:98)  2
m l = 1.94    10   4. since the standard error of the id113 on   
is   0.02, this is working essentially as well as the method of moments, or even
the method of simulated moments. [[todo: replace numbers with r]]

in this case, because there is a tractable maximum likelihood estimator, one
generally wouldn   t use the method of simulated moments. but we can in this case

26.2 indirect id136

603

check whether it works (it does), and so we can use the same technique for other
models, where an id113 is unavailable.

26.2 indirect id136

section 26.1 explained the method of simulated moments, where we try to match
expectations of various functions of the data. expectations of functions are sum-
mary statistics, but they   re not the only kind of summary statistics. we could
try to estimate our model by matching any set of summary statistics, so long as
(i) there   s a unique way of mapping back from summaries to parameters, and (ii)
estimates of the summary statistics converge as we get more data.

a powerful but somewhat paradoxical version of this is what   s called indirect
id136, where the summary statistics are the parameters of a di   erent model.
this second or auxiliary model does not have to be correctly speci   ed, it just
has to be easily    t to the data, and satisfy (i) and (ii) above. say the parameters
of the auxiliary model are   , as opposed to the    of our real model. we calculate

(cid:98)   on the real data. then we simulate from di   erent values of   ,    t the auxiliary

to the simulation outputs, and try to match the auxiliary estimates. speci   cally,
the indirect id136 estimator is

(cid:98)  ii = argmin

(cid:107)     (  )    (cid:98)  (cid:107)2

  

where     (  ) is the value of    we estimate from a simulation of   , of the same size
as the original data. (we might average together a couple of simulation runs for
each   .) if we have a consistent estimator of   , then

(cid:98)         
if in addition b(  ) is invertible, then(cid:98)  ii       

    (  )     b(  )

(26.21)

(26.22)
(26.23)

(26.24)

for this to work, the auxiliary model needs to have at least as many parameters
as the real model, but we can often arrange this by, say, making the auxiliary
model a id75 with a lot of coe   cients.

[[todo: nominal con   dence limits by means of the information matrix]]
a speci   c case, often useful for time series, is to make the auxiliary model a lin-
ear autoregressive model (  25.4), where each observation is linearly regressed
on the previous ones     see the discussion in   25.4.

26.3 further reading

the best general reference on simulation-based id136 i know is (despite its
age) still gouri  eroux and monfort (1989/1995); many of the examples presume
some familiarity with the jargon of econometrics, but the general approaches do

604

simulation-based id136

not. it covers the simulated method of moments, simulated maximum likelihood,
and (unsurprisingly: gouri  eroux et al. 1993) indirect id136.

kendall et al. (2005) is an excellent example of applying indirect id136
to testing substantive scienti   c (not statistical!) hypotheses with real data. (i
learned about indirect id136 from hearing prof. ellner describe this paper.)

the weakest conditions i know of under which indirect id136 is consistent

are given in zhao (2010, ch. 5).

wood (2010) proposes an interesting variant on indirect id136; despite the

title, it applies much more generally than to ecology.

indirect id136 has a bayesian counterpart, or even version, called    approxi-
mate bayesian computation   , which originated in population genetics; beaumont
(2010) is an accessible review by one of the inventors.

26.1 indirect id136

exercises

1. convince yourself that if xt comes from an ma(1) process, it can   t also be written as

an ar(1) model.

2. write a function, ar1.fit, to    t an ar(1) model to a time series, using lm, and to

return the three parameters (intercept, slope, noise variance).

3. apply ar1.fit to the s&p 500 data; what are the auxiliary parameter estimates?
4. combine ar1.fit with the simulator rma, and plot the three auxiliary parameters as

functions of   , holding   2    xed at 1. (this is analogous to figure 26.1.)

5. write functions, analogous to ma.msm.est and ma.msm.objective, for estimating an
ma(1) model, using an ar(1) model as the auxiliary function. does this recover the
right parameter values when given data simulated from an ma(1) model?

6. what values does your estimator give for    and   2 on the s& p 500 data? how do

they compare to the other estimates?

26.2 indirect id136 with a mechanistic model [[todo: lotka-volterra model with errors-

in-variables for the lynx data?]]

appendices

605

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

appendix a

data-analysis problem sets

all of the following problem sets have been used in class at least once. they are
arranged in an order approximately matching the order of the chapters, but many
of them draw on multiple chapters. each one is scored out of 90 points, with an
extra 10 points allocated to clarity of writing,    gures, code, etc. (the exact rubric
is given below.) ; in a typical semester, students would do one problem set a week, [[todo:
12   14 in all. a few provide much less    sca   olding    to guide students through the
analysis; these were assigned as take-home exams.

so
fix
they really
are scored
from 90]]

most of these assignments are based on published papers in the scienti   c or
statistical literature; i have provided citations to the source papers, but urge
students to not read them until after they have attempted the assignment.1

[[todo: add references to the source papers]]

a.1 suggested rubric for writing and formatting

this describes the ideal; the suggested weight is 10 points out of 100.

the text is laid out cleanly, with clear divisions between problems and sub-
problems. the writing itself is well-organized, free of grammatical and other me-
chanical errors, and easy to follow. figures and tables are easy to read, with
informative captions, axis labels and legends, and are placed near the text of
the corresponding problems. all quantitative and mathematical claims are sup-
ported by appropriate derivations, included in the text, or calculations in code.
numerical results are reported to appropriate precision. code is either properly
integrated with a tool like r markdown or knitr, or included as a separate r    le.
in the former case, both the knitted and the source    le are included. in the latter
case, the code is clearly divided into sections referring to particular problems. in
either case, the code is indented, commented, and uses meaningful names. all
code is relevant to the text; there are no dangling or useless commands. all parts
of all problems are answered with actual coherent sentences, and never with raw
computer code or its output. for full credit, all code runs, and the markdown    le
knits (if applicable).

1 some of the source papers would be positive hindrances.

607

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

source:
chetty
et
(2014)

al.

608

data-analysis problem sets

a.2 your daddy   s rich and your momma   s good looking

when the assignment says    make a scatterplot of a against b   , or    plot a against b   , a goes
on the vertical axis and b on the horizontal axis.

agenda: getting back into practice with regression; starting to unlearn some bad habits.

this assignment will look at economic mobility across generations in the con-
temporary usa. the data come from a large study, based on tax records, which
allowed researchers to link the income of adults to the income of their parents
several decades previously. for privacy reasons, we don   t have that individual-
level data, but we do have aggregate statistics about economic mobility for several
hundred communities, containing most of the american population, and covariate
information about those communities. we are interested in predicting economic
mobility from the characteristics of communities.

the data

the data    le mobility.csv has information on 741 communities2. the variable
we want to predict is economic mobility; the rest are predictor variables or co-
variates.

1. mobility: the id203 that a child born in 1980   1982 into the lowest quin-
tile (20%) of household income will be in the top quintile at age 30. individuals
are assigned to the community they grew up in, not the one they were in as
adults.

2. population in 2000.
3. is the community primarily urban or rural?
4. black: percentage of individuals who marked black (and nothing else) on cen-

sus forms.

5. racial segregation: a measure of residential segregation by race.
6. income segregation: similarly but for income.
7. segregation of poverty: speci   cally a measure of residential segregation for

those in the bottom quarter of the national income distribution.

8. segregation of a   uence: residential segregation for those in the top qarter.
9. commute: fraction of workers with a commute of less than 15 minutes.
10. mean income: average income per capita in 2000.
11. gini: a measure of income inequality, which would be 0 if all incomes were
perfectly equal, and tends towards 100 as all the income is concentrated among
the richest individuals (see wikipedia, s.v.    gini coe   cient   ).

12. share 1%: share of the total income of a community going to its richest 1%.
13. gini bottom 99%: gini coe   cient among the lower 99% of that community.
14. fraction middle class: fraction of parents whose income is between the national

25th and 75th percentiles.

15. local tax rate: fraction of all income going to local taxes.
16. local government spending: per capita.

2 technically,    commuting zones   . these include cities and their suburbs and exurbs, but also many

rural areas with integrated economies.

a.2 your daddy   s rich and your momma   s good looking

609

17. progressivity: measure of how much state income tax rates increase with in-

come.

18. eitc: measure of how much the state contributed to the earned income tax

credit (a sort of negative income tax for very low-paid wage earners).

19. school expenditures: average spending per pupil in public schools.
20. student/teacher ratio: number of students in public schools divided by number

of teachers.

21. test scores: residuals from a id75 of mean math and english test

scores on household income per capita.

22. high school dropout rate: also, residuals from a id75 of the dropout

rate on per-capita income.

23. colleges per capita
24. college tuition: in-state, for full-time students
25. college graduation rate: again, residuals from a id75 of the actual

graduation rate on household income per capita.

26. labor force participation: fraction of adults in the workforce.
27. manufacturing: fraction of workers in manufacturing.
28. chinese imports: growth rate in imports from china per worker between 1990

and 2000.

29. teenage labor: fraction of those age 14   16 who were in the labor force.
30. migration in: migration into the community from elsewhere, as a fraction of

2000 population.

31. migration out: ditto for migration into other communities.
32. foreign: fraction of residents born outside the us.
33. social capital: index combining voter turnout, participation in the census, and

participation in community organizations.

34. religious: share of the population claiming to belong to an organized religious

body.

35. violent crime: arrests per person per year for violent crimes.
36. single motherhood: number of single female households with children divided

by the total number of households with children.

37. divorced: fraction of adults who are divorced.
38. married: ditto.
39. longitude: geographic coordinate for the center of the community
40. latitude: ditto
41. id: a numerical code, identifying the community.
42. name: the name of principal city or town.
43. state: the state of the principal city or town of the community.

some of these variables are missing for some communities, and this may make a
di   erence for some questions.

610

data-analysis problem sets

1. (5) draw a map of mobility. that is, make a plot where the x and y coordinates
are longitude and latitude, and mobility is indicated by color (possibly grey
scale), by a third coordinate, or some other suitable device. make sure your
map is legible. describe the geographic pattern in words.

2. (15) make scatter plots of mobility against each of the following variables.
include on each plot a line for the simple or univariate regression, and give a
table of the regression coe   cients. carefully explain the interpretation of each
coe   cient. (2 pts each) do any of the results seem odd? (1 pt)

1. population
2. mean household income per capita
3. racial segregation
4. income share of the top 1%
5. mean school expenditures per pupil
6. violent crime rate
7. fraction of workers with short commutes.

3. run a id75 of mobility against all appropriate covariates.

1. (5) report all regression coe   cients and their standard errors to reasonable
precision; you may use either a table or a    gure as you prefer. do not just
paste in r   s output.

2. (1) explain why the id variable must be excluded.
3. (4) explain which other variables, if any, you excluded from the regression,

and why. (if you think they can all be used, explain why.)

4. (5) compare the coe   cients you found in problem 2 to the coe   cients for
the same variables in this regression. are they signi   cantly di   erent? have
any changed sign?

4. the wrong side of the tracks starts at giant eagle find pittsburgh in the data

set.

1. (1) what its actual mobility? what is its predicted mobility, according to

the model?

2. (3) holding all else    xed, what is the predicted mobility if the violent crime

rate is doubled? if it is halved?

3. (3) holding all else    xed, at what level of income segregation does the model

predict that mobility will exceed 1.0?

4. (3) holding all else    xed, what would the income share of the top 1% have

to be for the model to predict that mobility will fall to 0.0?

(we will see later in the course how to avoid the embarrassment of models
that predict probabilities greater than 1 or less than 0.)

5. free as in beer

1. (1) the national mobility level is the average mobility across all communi-

ties, weighted by population. what is it?

2. (3) suppose college were made free for everyone. calculate the change in
the predicted mobility for each community. report the minimum, median,
mean and maximum changes.

a.2 your daddy   s rich and your momma   s good looking

611

3. (1) find the change to the predicted (not actual) national mobility level
from making college free for everyone. hint: consider a weighted average,
or weighted sum, of your vector of answers from problem 52.

4. (3) give a (rough) 95% con   dence interval for the change in the predicted

national mobility level.

5. (2) explain at least one way in which this calculation is unrealistic.

6. distinctions vs. di   erences

1. (2) make a table ranking the variables by the magnitude of the t statistic

in the regression results (i.e., rank by |t|, not t).

2. (6) for each variable in the model,    nd the expected change in mobility
from a one standard deviation change in that variable (assuming all else is
   xed). provide a table ranking variables by the magnitude of their impact.

3. (2) how similar is the ranking by impact to the ranking by t statistics?

7. (5) make a map of the model   s predicted mobility. how does it compare,

qualitatively, to the map of actual mobility?

8. after making proper allowances

1. (1) make a map of the model   s residuals.
2. (2) what are the    ve communities with the largest positive residuals? the
   ve with the most negative residuals? (can you mark these on the map?)
3. (2) one interpretation of these residuals is that they show communities
where some factor not included in the model leads to higher (or lower)
mobility than in otherwise-similar communities. suggest at least one other
interpretation. could you test these ideas with this data set?

9. expectations and reality

1. (3) make a scatterplot of actual mobility against predicted mobility. is the
relationship linear? should it be, if the model is right? is the relationship
   at? should it be, if the model is right?

2. (2) make a scatterplot of the model   s residuals against predicted mobility. is
the relationship linear? should it be, if the model is right? is the relationship
   at? should it be, if the model is right?

10. model checking will continue until morale improves

1. (5) for each variable in the model, make a scatterplot of the model   s resid-

uals against the predictor variable. (you will have a lot of plots.)

2. (5) explain why, if the linear model is right, all the relationships you just

plotted should be    at.

3. (5) explain why, if the usual assumptions for t tests and their p-values are
right, each plot should have a roughly constant vertical spread of points as
one moves from left to right.

4. (5) which residual plots look like they   re    at with constant width? for the

ones which don   t look like this, describe how they di   er.

extra credit, 5 points: add kernel smoothing lines to each of the residual

plots. comment.

source:
rodrik
(2008)

612

data-analysis problem sets

a.3 . . . but we make it up in volume

   gross domestic product    is a standard measure of the size of an economy; it   s the
total value of all goods and services bought and solid in a country over the course
of a year. it   s not a perfect measure of prosperity3, but it is a very common one,
and many important questions in economics turn on what leads gdp to grow
faster or slower.

one common idea is that poorer economies, those with lower initial gdps,
should grower faster than richer ones. the reasoning behind this    catching up   
is that poor economies can copy technologies and procedures from richer ones,
but already-developed countries can only grow as technology advances. a second,
separate idea is that countries can boost their growth rate by under-valuing their
currency, making the goods and services they export cheaper.

this week   s data set contains the following variables:

3166-1_alpha-3).

    country, in a three-letter code (see http://en.wikipedia.org/wiki/iso_
    year (in    ve-year increments).
    per-capita gdp, in dollars per person per year (   real    or in   ation-adjusted).
    average percentage growth rate in gdp over the next    ve years.
    an index of currency under-valuation4. the index is 0 if the currency is neither
over- nor under- valued, positive if under-valued, negative if it is over-valued.

note that not all countries have data for all years. however, there are no missing

values in the data table.

3 a standard example: if vandals break all the windows on a street, a town, gdp goes up by the cost

of the repairs.

4 the idea is to compare the actual exchange rate with the us dollar to what   s implied by the prices

of internationally traded goods in that country     the exchange rate which would ensure
   purchasing power parity   . the details are in the paper this assignment is based on, which will be
revealed in the solutions.

a.3 . . . but we make it up in volume

613

1. (10) linearly regress the growth rate on the under-valuation index and the
log of gdp. report the coe   cients and their standard errors (to reasonable
precision). do the coe   cients support the idea of    catching up   ? do they
support the idea that under-valuing a currency boosts economic growth?

2. (20) repeat the id75 but add as covariates the country, and the

year. use factor(year), not year, in the regression formula.
1. (5) report the coe   cients for log gdp and undervaluation, and their stan-

dard errors, to reasonable precision.

2. (5) explain why it is more appropriate to use factor(year) in the formula

than just year.

3. (5) plot the coe   cients on year versus time.
4. (5) does this expanded model support the idea of catching up? of under-

valuation boosting growth?

3. (10) does adding in year and country as covariates improve the predictive

ability of a linear model which includes log gdp and under-valuation?
1. (1) what are the r2 and the adjusted r2 of the two models?
2. (5) use leave-one-out cross-validation to    nd the mean squared errors of
the two models. which one actually predicts better, and by how much?
hint: use the code from lecture 3.

3. (4) explain why using 5-fold cross-validation would be hard here. (you

don   t need to    gure out how to do it.)

4. (20) kernel smoothing use kernel regression, as implemented in the np package,
to non-parametrically regress growth on log gdp, under-valuation, country,
and year (treating year as a categorical variable). hint: read chapter four
carefully. in particular, try setting tol to about 10   3 and ftol to about 10   4
in the npreg command, and allow several minutes for it to run. (if you are
using r markdown, trying caching this part of your code.)

1. (5) give the coe   cients of the kernel regression, or explain why you can   t.
2. (5) plot the predicted values of the kernel regression, for each country and

year, against the predicted values of the linear model.

3. (5) plot the residuals of the kernel regression against its predicted values.
should these points be scattered around a    at line, if the model is right?
are they?

4. (5) the npreg function reports a cross-validated estimate of the mean
squared error for the model it    ts. what is that? does the kernel regression
predict better or worse than the linear model with the same variables?

5. (20) time courses and interactions in this question, use the kernel regression

you    t in the previous problem.

1. (6) plot the predicted growth rate, as a function of the year, in    ve year
increments from 1955 to 2000, if the initial gdp (not log gdp!) is $10,000
in each period, the under-valuation index is 0 (i.e., no under- or over- val-
uation), and the country is turkey.

2. (3) re-do the plot but change the under-valuation index to +0.5.

614

data-analysis problem sets

3. (3) re-do the plot but hold the initial gdp at $1,000 and the under-

valuation index at 0.

4. (3) re-do the plot with the initial gdp at $1,000 and the under-valuation

index at +0.5.

5. (5) is there evidence of an interaction between initial gdp and under-

valuation? explain.

6. (20) average predictive comparisons   [[4.5]] explains how to calculate the    av-
erage predictive comparison        the typical rate of change in the response
when a given variable is perturbed, even when the model is nonlinear and has
interactions. see, in particular, equation [[4.31]].

hint: at no point in this problem should you re-   t either model.

1. (5) calculate the average predictive comparison for log gdp in the kernel

regression.

2. (5) calculate the average predictive comparison for under-valuation in the

kernel regression.

3. (5) explain how to calculate the corresponding average predictive compar-
isons from the linear model   s coe   cients. what are the average predictive
comparisons for initial log gdp and for under-valuation in the linear model?
4. (5) do the kernel and the id75 agree, qualitatively, about the
average e   ect of increasing initial gdp on growth? do they agree, qualita-
tively, about the e   ect of undervaluation on growth?

a.4 past performance, future results

agenda: practice with cross-validation and with smoothing; baby steps in using simulation to
see how a model behaves and to do hypothesis testing; reinforcement that    the variable matters   
(cid:54)=    the coe   cient on the variable is statistically signi   cant   .

timing: some parts of this assignment, particularly problem 66, are very computation-

intensive. start early, read the hints, and cache your results.

a corporation   s earnings in a given year is its income minus its expenses5.
the return on an investment over a year is the fractional change in its value,
(vt+1   vt)/vt, and the average rate of return over k years is [(vt+k   vt)/vt]1/k. our
data set this week looks at the relationship between us stock prices, the earnings
of the corporations, and the returns on investment in stocks, with returns counting
both changes in stock price and dividends paid to stock holders.6

speci   cally, our data contains the following variables:

    date, with fractions of a year indicating months
    price of an index of us stocks (in   ation-adjusted)
    earnings per share (also in   ation-adjusted);

5 accountants get into subtle issues about whether to include in expenses taxes, interest paid on

loans, and charges for depreciation of assets and amortization of investments. those of you who get
jobs with certain kinds of tech company will grow only too familiar with these wrinkles. in our data
set, earnings are very de   nitely after all these expenses.

6 nothing in this assignment, or the solutions, should be taken as    nancial advice.

from the current date;

a.4 past performance, future results

615
    earnings_10ma_back, a ten-year moving average of earnings, looking backwards
    return_cumul, cumulative return of investing in the stock index, from the be-
    return_10_fwd, the average rate of return over the next 10 years from the

ginning;

current date.

   returns    will refer to return_10_fwd throughout.

[[todo: link to data set]]

1. inventing a variable

1. (1) add a new column, mape, to the data frame, which is the ratio of price

to earnings_10ma_back. it should have the following summary statistics:

min. 1st qu. median

mean 3rd qu.

max.

na's

4.785 11.710 15.950 16.550

19.960 44.200

120

why are there exactly 120 nas?

2. (1) linearly regress the returns on mape (and nothing else). what is the

coe   cient and its standard error? is it signi   cant?

3. (1) what is the mse of this model, under    ve-fold cv?

2. inverting a variable

1. (3) linearly regress the returns on 1/mape (and nothing else). what is the
coe   cient and its standard error? is it signi   cant? (for full credit, do not
add a new column to the data frame, or create a new vector.)

2. (1) what is the    ve-fold cv mse of this model? how does it compare to

the previous one?

3. employing a variable a simple-minded model7 says that expected returns over

the next ten years should be exactly equal to 1/mape.
1. (1) find the in-sample mse of this model.
2. (2) explain why the in-sample mse is an unbiased estimate of the gener-
3. (2) make a q    q plot for the residuals of this model. hint: try subtraction,

alization error for this particular model.

rather than residuals.

4. (5) estimate a t distribution from the residuals. report the parameters
and their standard errors. plot a histogram of the residuals, and add the
estimated t density. hint: see the function fitdistr in the mass package.
4. (5) use npreg to estimate a kernel regression of the returns on mape. what is

the bandwidth? the cross-validated mse?

5. one big happy plot for this problem, you need to only include one plot, and
one paragraph of writing, but make sure you clearly label, with comments,
which parts of your code are answers to each question. (this does not mean
showing your code in your report.) also, in this problem, take    line    to mean

7 assume that: future earnings get added to the value of an investment in the company   s stock; that
nothing else adds to the value of the investment; and that earnings over the next ten years will be
equal to those over the last ten years. solve for the returns.

616

data-analysis problem sets

   straight or curved line, as appropriate   . plotting disconnected points where
a line is called for will get partial credit.

1. (1) make a scatter-plot of the returns against mape.
2. (6) add two lines, showing the predictions from the models you    t in prob-

lem 1 and 2.

3. (1) add a curve showing the predictions of the simple-minded model from

problem 3.

4. (5) add a line of the predictions of the kernel regression to the plot from
problem 5. which of the previous models does it most resemble? is it just
a slightly wiggly copy of that model, or does it do something qualitatively
di   erent?

6. simulating the simple-minded model

1. (10) write a function which simulates the simple-minded model from prob-
lem 3. the function should take as inputs (i) a vector of mape values, and (ii)
the three parameters of the t distribution. it should return a two-column
data frame, with one column being mape and the other being 1/mape plus t-
distributed noise. the columns should have names which match the names
used in the real data frame. make sure that the output of your function has
the right number of rows and columns, and that the summary statistics for
the two columns are what they should be (at least approximately, in the
case of the second column).

2. (5) write a function which takes as input a data frame, estimates the same
linear model as in problem 2 to that data frame, and returns the coe   cient
on 1/mape. check that it works by running it on the original data. check
that it also works when the input comes from your simulation function from
61.

3. (7) by repeated simulation,    nd the id203, under the simple-minded
model, of the coe   cient on 1/mape being as far from 1.0 (in either direction)
as what you found in the data.

4. (8) you can now report a p-value for testing the hypothesis that this slope
is exactly 1.0. carefully state the null and alternative hypotheses, and give
your p-value.

5. (7) write a function which takes as input a data frame, estimates the same
kernel regression as in problem 4, and returns the vector of    tted values
from that regression. check that it works by running it on your original
data. check that it also works when the input comes from your simulation
function.

6. (8) create a plot of predicted returns versus mape for the simple-minded
model, as in problem 53. add 200 kernel regression curves,    t to 200 simu-
lations of the model. finally, add the kernel regression curve from the true
data, as in problem 54. (you   ll want to manipulate graphics settings.) how
plausible is the simple-minded model? explain your answer by referring to
your plot.
hint/warning: estimating all the kernel regressions might well take a few

a.5 free soil

617

seconds per simulation. write and debug your code here with a smaller
number of curves, then increase it for the    nal version.

7. more fun with star-gazing

1. (1) linearly regress the returns on both mape and 1/mape (without interac-

tion). what are the coe   cients? which ones are signi   cant?

2. (1) linearly regress the returns on mape, 1/mape, and the square of mape.

what are the coe   cients? which ones are signi   cant?

3. (8) explain what is going on.

a.5 free soil

agenda: practice writing, testing, and debugging simple r functions. practice decomposing a
big computational problem into a bunch of small, inter-locking functions. practice estimating a
categorical contrast. practice with weighted least squares. practice with id64. finally,
an early observance of lincoln   s birthday.

recall that equation for the standard error of a proportion, when we observe

a binomial with n trials and success id203 p:

further recall the estimated standard error in an observed proportion   p:

recall,    nally, that the mobility variable from homework 1 was an observed
proportion, the fraction of children born into the bottom    fth of the income
distribution who make their way to the top    fth of the distribution by age 30.

load the data set from homework 1 as a data frame named mobility. we will
only need three columns, mobility, population and state, though you may also
want to keep name for debugging purposes. do not remove any row from the data
frame which has complete values for these variables.

1. (15) write a function, se.prop, to calculate the standard error for proportions.
it should take a vector of proportions, p, and a vector of trial numbers, n, and
return a vector of standard errors.

1. (2) construct a test case to check that se.prop gives the right answer when

p = 0.5, n = 1.

2. (2) construct a test case to check that when se.prop is given a vector of
   
di   erent n   s, all with the same p (not equal to 0 or 1), the answers are
proportional to 1/

n.

3. (2) construct a test case to check that when p = 0, the returned value is

always 0, for multiple n.

4. (2) construct a test case to check that when p = 1, the returned value is

always 0, for multiple n.

(cid:114)
(cid:114)

p(1     p)

n

  p(1       p)

n

source:
chetty
et
(2014)

al.

(a.1)

(a.2)

618

data-analysis problem sets

5. (2) construct a test case to check that when given a vector p of mixed 0s

and 1s, the returned vector has all 0s, for multiple n.

6. (2) construct a test case to check that when given a vector of di   erent, non-
extreme values for p, and a constant n, the entries of the returned vector

are proportional to(cid:112)p(1     p).

7. (2) check that se.prop works properly when p=c(0.3,0.8) and n=c(12,72).

this includes working out what the proper answers should be.

8. (1) explain whether your code implements eq. 1 or eq. 2.

2. (10)

1. (3) use se.prop to calculate the standard error of the mobility for each

community in the data from homework 1; report the summary statistics.

2. (1) plot the histogram of the standard errors.
3. (2) make a scatter-plot of the standard errors vs. population.
4. (2) make a scatter-plot of the standard errors vs. mobility.
5. (2) how reliable were the inferential statistics you calculated in homework

1?

3. (15)

1. (5) write a function, wse, to calculate weighted mean squared error. it
should take as arguments predicted, a vector of predicted values; observed,
a vector of observed values; and weights, a vector of weights. it should re-
turn a single real number, the weighted mean squared error. mathemati-

cally, that is to say, it should    nd(cid:80)n
(cid:80)n
i=1 wi (yi       yi)2

i(cid:48)=1 wi(cid:48)

make the default value for observed the mobility column of the data, and
the default values for weights equal to one over the squares of the standard
errors in mobility from the previous problem. hint: you could write this
using a for loop, or even two of them, but there are more elegant ways.

2. (3) check that wse works properly when predicted is c(0.15,0.05), observed
is c(0.14,0.07), and weights is c(0.01, 0.42). (this includes working out
what the right answer should be.)

3. (2) create three modi   ed versions of this test case, each changing one of
the three arguments, and make sure that your function works correctly on
all three.

4. (2) explain why, for modeling mobility, the weights should be the inverse

square standard errors.

5. (3) check that wse returns the mse when all the weights are equal. (they

will not be equal for those default values.)

4. (10)

1. (5) write a function, dixie, which reads in a vector of state names (in the
form used in the mobility data set), and returns a binary vector, 1 if the
state was part of the confederacy during the us civil war, and 0 otherwise.

a.6 there were giants in the earth in those day

619

2. (5) check that it gives the correct results when applied to a vector of the

50 state names and the district of columbia.

5. (10) write a function, dixie.fit, which takes two arguments: a data frame
with a column named state, and a vector of length two, levels. it should test,
for each row, whether the state was in the confederacy (using dixie), and if so
return the    rst element of levels, and if not, return the second element. check
that it works correctly when levels=c(1,0). explain how you know that is the
correct behavior.

6. (10) write a function, dixie.wse, which takes as input levels, without default,
and a data frame, defaulting to mobility. it should predict the mobility level
for each city based on whether it was in the confederacy or not, using the
function dixie.fit, and return the weighted squared error, using wse, with the
actual values of mobility as the response and weights based on their standard
errors. for full credit, call, do not re-write, the functions from the earlier
problems.

construct a test case using a data frame of four rows to check that is working

properly, when levels=c(0.01,0.15).

7. (5) optimize the weighted squared error for this two-parameter model, starting
from the initial guess that the mobility level for the former confederacy is 0.01,
while that for the rest of the country is 0.15. report the best-   tting values of
levels.

8. (10) turn the optimization from the previous problem into a function, which
takes as arguments a data frame (with default equal to mobility) and an
initial guess at levels (with default equal to c(0.01,0.15)), and returns the
   tted values of levels (and nothing else). check that running it with the
defaults reproduces your answer from the previous problem. check that you
get a di   erent answer if you remove the    rst half of the data frame.

9. (5) use resampling of rows to give standard errors for levels.

extra credit (10): show, mathematically, that the optimal values for levels
are always given by two weighted averages of mobility. show how to    nd them
by two calls to weighted.average, without using wse, dixie.fit, dixie.wse, or
any optimization function. for full extra credit, check that code implementing
this matches the answer you obtained above.

a.6 there were giants in the earth in those day

[[todo: see if there   re any improvements over previous versions; if not, cut]]

agenda: explicitly: splines, bootstrap, simulation, comparing a simulation to data; implicitly:
more practice writing, testing, and debugging simple functions.

some biologists argue that larger animals tend to have advantages over smaller
members of their species, so that natural selection should tend to lead to an

source:
clauset
and erwin
(2008)

620

data-analysis problem sets

increase in size within an evolutionary lineage8. there is also some evidence that
larger species tend to be shorter-lived than smaller ones9. in this assignment, we
will look at the evidence for an increase in species size within lineages, and how
the trade-o    between these two forces might lead to a stable distribution of sizes
across species.

we will use two data sets:

    the north american mammalian paleofauna database (nampd.csv) lists, for
about 2000 living and extinct species, the log of the mass, in grams, of a typical
member of the species; the log mass of the ancestral species (when known); and
the dates of the species       rst and last appearance in the fossil record, in millions
of years ago. if the last appearance date is na, the species is still alive. this
means you should not just throw away all rows containing nas.
    the masses of mammals (mom.txt) gives, for about 4000 living species, their
mass in grams, identifying codes for the species, genus, and other taxonomic
groups, and an indicator for whether the species lives in the land or in the
water.

the model we will work with goes as follows: at any given time t, there is
a collection of nt species, whose masses are x1, x2, . . . xnt. at each time step,
one current species a gets picked, uniformly at random, to evolve into two new
species. the masses of a descendant species xd is related to that of its ancestor,
xa, by the model

xd = exp (r(log xa) + z)

(a.3)
where z     n (0,   2), and r is a function to be learned from the data, subject to
the restriction that xd has to be at least xmin and at most xmax. the ancestor xa
is removed from the current list of species, and its two independent descendants
are added. after this, all species currently in the list have a risk of going extinct,
with the id203 for a species of mass x going extinct being a function of their
mass,

pe(x) =   x  

(a.4)

any species become extinct are removed from the collection. we then iterate the
model again.

in all of the following questions, unless otherwise speci   ed, you may take   2 =
0.63 (what are the units?), xmin = 1.8 grams, xmax = 1015 grams,    = 0.025, and
   = 1/5000.

1. (5) linearly regress the log of the new mass on the log of the ancestral mass.
plot this regression line, along with a scatter-plot of the data, in units of grams,
not log-grams. carefully explain the interpretation of both the slope and the

8 among other things, larger animals may be harder for predators to attack,    nd it easier to

over-come prey or other members of their species, and be more e   cient metabolically. for more, see,
e.g., bonner (1988).

9 this may be because larger animals need more food in total, and possibly more specialized food

sources, so they are more vulnerable to shifts in their environment.

a.6 there were giants in the earth in those day

621

intercept. a rote recitation of    a one unit change   , etc., will not receive full
credit; think about the model, the transformations, and what the transformed
model says about the variables.

2. (10) use a smoothing spline to do a nonparametric regression of log new mass
on log ancestral mass. create a plot showing the data points, the model from
question 1, and the spline, making sure that the axes are in units of grams,
not log-grams.

3. (20)

1. (10) using resampling of residuals, calculate 95% con   dence bands for the

2. (10) using resampling of cases, calculate standard errors for the spline

spline curve, and add them to the plot.
curve, and add bands at   2 standard errors to the plot.

4. (10) write a function, rmass, which takes as inputs a single ancestral mass
xa (not log xa), an estimated spline function r, and any other parameters
required by the model, and returns a single random value for xd, according
to eq. a.3. make sure the returned value is in grams, not log grams. you will
probably    nd it easiest to keep generating candidate values for xd, until you
get one which is between the limits. hint: while

1. (2) what model parameters does your rmass need?
2. (4) check, by repeated simulation, that the output is always between xmin

and max, even when xa is brought near either limit.

3. (4) using the spline curve you estimated in question 2, create 150 evenly
spaced xa values between xmin and max, generate an xd for each of them,
and    t a spline curve to the simulated values. check that it is close to, but
not identical with, the one you found from the data. (why should it not be
identical?)

5. (10) write a function, origin, which takes the same arguments as rmass, except
that instead of one ancestral mass it can take a vector of them. origin should
pick one entry from the vector to be xa, and generate two independent values
of xd from it. one of these should replace the entry for xa, and the other
should be added to the end of the vector.

1. (4) check, by simulating with a length-one vector of ancestral masses, that
neither component of the returned value matches the ancestral mass (why?),
that both components have the same marginal distribution, and that the
two components are uncorrelated with each other.

2. (2) check, by simulating, that if the input vector of masses has length m,
the output vector always has length m + 1. (check at least two values of
m.)
3. (4) check, by simulating, that m    1 entries in the output match the input
exactly. check this for at least two values of m. hint: is.element, or %in%,
or match.

6. (5) write a function, extinct.prob, which takes as inputs a vector of species

622

data-analysis problem sets

masses, and parameters    and   , and returns the extinction probabilities ac-
cording to eq. a.4.

1. (2) check that if the masses are c(100, 1600, 10000) grams,    = 1/2 and

   = 1/200, then extinct.prob returns the right values.

2. (1) check that if    = 0, the output probabilities are all   , no matter what

the masses are.

3. (1) check that if the input masses are all equal, so are the returned prob-
4. (1) check that if    (cid:54)= 0 and    (cid:54)= 0, and the masses are all di   erent, then

abilities, for at least three of di   erent combinations of mass,    and   .

the returned probabilities are all distinct.

7. (5) write a function, extinction, which takes a vector of species masses,   
and   , and returns a possibly-shorter vector which removes the masses of
species which were probabilistically selected for extinction. be sure to handle
the (unfortunate) case where every species goes extinct. hint: explain what
rbinom(n,size=1,prob=p) does when p is a vector of length n.

1. (1) check that if    = 0, the output vector is always the same as the input

vector.

2. (3) create a case where the input masses are all equal, and    and    are set
so that the extinction id203 should be 1/2. check that the output is,
on average, half as long as the input.

3. (1) in the same test cases as the previous part, check that all the values in

the new vector of masses were also in the old vector of masses.

8. (5) write a function, evolve_step, which takes as inputs a vector of species
masses, plus all needed parameters and estimated curves; calls origin and
extinction as appropriate; and returns a new vector of species masses. how
do you know it works?

9. (5) write a function, mass_evolve, which takes the same inputs as evolve_step,
plus an additional number t; iterates evolve_step t times; and returns the
   nal vector of species masses. how do you know it works? hint: there will
almost certainly need to be a for loop inside the function.

10. (5) in this question, use the default parameter values, and the spline you

estimated in question 2.

1. (1) run mass_evolve starting from a single species with a mass of 120 grams

for t = 2    105 steps. save the output as masses.1. plot the histogram.

2. (1) re-run mass_evolve from the same conditions. save as masses.2. plot
3. (1) re-run from the same conditions but for t = 4    105 steps, saving as

the histogram.

masses.3. plot the histogram.
4. (1) change the starting condition to two species, one of 40 grams and one
of 1000 grams. run twice, both times with t = 2    105, saving the results
as masses.4 and masses.5.

5. (1) how do the distributions of the various masses compare to each other?

11. (5)

a.7 the sound of gun   re, o    in the distance

623

1. (1) load the masses of mammals data set, and plot the histogram of masses

for land species.

2. (2) compare, verbally, the distribution for land species to that obtained

from the simulations.

3. (2) compare the distributions using qq plots.

12. (5) does the output of the simulation model match the distribution of masses
we actually observe? are the di   erences between the model and reality bigger
than those between di   erent runs of the simulation? are there qualitative dis-
tinctions between the simulation-to-simulation di   erences, and the simulation-
to-reality di   erences? support your answers by reference to the plots you have
already made, or, if need be, new ones.

note: more advanced techniques for comparing distributions exist (e.g., chapter
15).

extra credit: (10) re-write the code so that z, rather than being drawn
from a gaussian distribution, comes from resampling the residuals of the    tted
spline curve. what do you have to modify? how much do the results change?
which version    ts the observed mass distribution better?

a.7 the sound of gun   re, o    in the distance

agenda: explicitly, logistic models, generalized additive models, and checking regression spec-
i   cations. implicitly, the perils of science by p-value.

na means an on-going civil war, while 0 denotes continuing peace;

our data this week, http://www.stat.cmu.edu/~cshalizi/uada/15/hw/06/
ch.csv, comes from a study of the causes of civil wars. every row of the data
represents a combination of a country and of a    ve year interval     the    rst row is
afghanistan, 1960, really meaning afghanistan, 1960   1965. the variables are:
    the country name;
    the year;
    an indicator for whether a civil war began during that period     the code of
    exports, really a measure of how dependent the country   s economy is on com-
    secondary school enrollment rate for males, as a percentage10;
    annual growth rate in gdp;
    an index of the geographic concentration of the country   s population (which
would be 1 if the entire population lives in one city, and 0 if it evenly spread
across the territory);
    the number of months since the country   s last war or the end of world war
    the natural logarithm of the country   s population;

ii, whichever is more recent11;

modity exports;

10 i have been unable to    nd an explanation anywhere of why this rate is greater than 100 for some

data points.

11 this appears to count only civil and not foreign wars.

sources:
collier and
hoe   er
(2004) and
ward et al.
(2010)

data-analysis problem sets

624
    an index of social    fractionalization   , which tries to measure how much the
    an index of ethnic dominance, which tries to measure how much one ethnic

country is divided along ethnic and/or religious lines;

group runs a   airs in the country.

some of these variables are na for some countries.

1. (10) fit id28 for the start of civil war on all other variables except
country and year; include a quadratic term for exports. report the coe   cients
and their standard errors, together with r   s p-values. which ones does r say
are signi   cant at the 5% level?

2. interpretation (15) all parts of this question refer to the id28

model you just    t.

1. (5) what is the model   s predicted id203 for a civil war in india in the
period beginning 1975? what id203 would it predict for a country
just like india in 1975, except that its male secondary school enrollment
rate was 30 points higher? what id203 would it predict for a country
just like india in 1975, except that the ratio of commodity exports to gdp
was 0.1 higher?

2. (5) what is the model   s predicted id203 for a civil war in nigeria in
the period beginning 1965? what id203 would it predict for a country
just like nigeria in 1965, except that its male secondary school enrollment
rate was 30 points higher? what id203 would it predict for a country
just like nigeria in 1965, except that the ratio of commodity exports to
gdp was 0.1 higher?

3. (5) in parts (a) and (b), you changed the same predictor variables by the
same amounts. if you did your calculations properly, the changes in pre-
dicted probabilities are not equal. explain why not. (the reasons may or
may not be the same for the two variables.)

3. confusion (10) id28 predicts a id203 of civil war for each
country and period. suppose we want to make a de   nite prediction of civil war
or not, that is, to classify each data point. the id203 of mis-classi   cation
is minimized by predicting war if the id203 is     0.5, and peace otherwise.
1. (5) build a 2   2    confusion matrix    (a.k.a.    classi   cation table    or    conti-
gency table   ) which counts: the number of outbreaks of civil war correctly
predicted by the id28; the number of civil wars not predicted
by the model; the number of false predictions of civil wars; and the number
of correctly predicted absences of civil wars. (note that some entries in the
table may be zero.) make sure the rows and columns of the table are clearly
labeled.

2. (3) what fraction of the id28   s predictions are correct? (note
that this is if anything too kind to the model, since it   s an in-sample eval-
uation.)

3. (2) consider a foolish (?) pundit who always predicts    no war   . what
fraction of the pundit   s predictions are correct on the whole data set? what

a.8 the bullet or the ballot?

625

fraction are correct on data points where the id28 model also
makes a prediction?

4. calibration (10) divide the data points into groups where the predicted prob-
ability of a civil war is 0   10%, those where it is 10   20%, etc. calculate the
actual proportion of civil wars for each group of data points. give a plot where
the horizontal axis is the predicted id203, and the vertical is the actual
frequency. does the plot go up the 45-degree diagonal? should it, if the model
is right? if it does not, do observed frequencies at least increase as the pre-
dicted id203 goes up, so that civil war really is more common when the
model says it has higher id203? (again, this is if anything too kind to
the id28, because it   s an in-sample comparison.)

5. (10) fit a gam with the same variables to the same data: smooth all the
continuous predictor variables; do not include an explicit quadratic term for
exports. (the ethnic-dominance variable is binary, and should be included in
the model with as.factor.) provide plots of the partial response functions.
which ones are at least roughly linear, and which are not?

6. (10) calculate the confusion matrix for the gam. what fraction of its pre-
dictions are accurate? how does that compare both to the id28
and the peace-always pundit?

7. (10) repeat the calibration checking plot for the gam. are its probabilities
closer to tracking actual frequencies, or further, than those of the logistic
regression?

8. (15) test whether the id28 is properly speci   ed, using the gam
as the alternative model. (follow the procedure in the notes.) what is the
p-value? explain, based on this test and any other results you have reported,
which model you prefer.

extra credit (15): start with the model which predicts a constant probabil-
ity of civil war for all countries and years. evaluate its log-likelihood out of sample
through    ve-fold cross-validation. now consider all one-variable gams, using all
available predictor variables except country and year. which one variable has the
highest cross-validation log-likelihood, and is it higher than the trivial, intercept-
only model? consider all two-variable gams which extend the one-variable gam
you just picked: report their cross-validated log-likelihoods. are the two variables
you picked the two variables with the smallest p-values in the id28?
should they be?

a.8 the bullet or the ballot?

many people assume that violence, while perhaps dangerous or evil, is more
e   ective politically than non-violence. in this exam, we will examine whether,
in fact, non-violent political movements are more or less likely to achieve their
goals than violent ones. moreover, we will look at the conditions which make
non-violence more or less likely to succeed.

our data set, gathered by political scientists who have studied exactly these

source:
stephan
and
chenowth
(2008);
chenoweth
and
stephan
(2011)

626

data-analysis problem sets

questions, is navc.csv on the class website. the units of analysis here are political
movements or campaigns. for each movement, the data records:
    the name of the movement (campaign);
    the country the movement was in (country);
    the peak year of the movement   s activity (year);
    whether the movement fully achieved its aims (1.0), achieved partial success
    an indicator variable (nonviol), 1 for non-violent movements and 0 for others;
    a quantitative measure of how democratic the government of the country was,
from -10 for very un-democratic governments to a possible maximum of +10
(democracy);

(0.5), or failed (0) (outcome);

to help deal with the movement (aid);

    an indicator for the government being under international sanctions (sanctions);
    an indicator for whether the government received aid from other governments
    an indicator for the movement   s receiving aid from foreign governments (support);
    an indicator for the government   s using violence to repress the movement
    an indicator for whether substantial portions of the security (military and
    the duration of the movement, in days (duration).

police) forces of the government sided with the movement (defect);

(viol.repress);

speci   c analytic issues you must address

in general, are non-violent movements more likely to be successful than violent
ones? does violent repression by the government make movements more or less
likely to be successful, and is there a di   erence in this e   ect between movements
which are themselves violent and non-violent? similarly, what is the e   ect of
foreign aid to the government and to the movement? do non-violent movements
become more likely to succeed as the government becomes more democratic?
does the di   erence in id203 of success between violent and non-violent
movements vary with how democratic the government is? all of these should be
answered with reference to the results in your model (or models).

models

use a generalized additive model with a logistic link function; smooth all contin-
uous predictor variables, and include all categorical variables, except campaign
and country names, as your default. (departures from this should be carefully
justi   ed.) be sure to include the year as a predictor variable, and explain the inter-
pretation of your estimated e   ects for the year. some of the analytic issues above
may be most easily addressed through including interaction terms, or through
   tting di   erent models on subsets of the data; describe any such variations, and
the reasons for your choices.

note 1: before    tting a model with a logistic link function, you will need to
re-code partial successes as either successes or failures. explain which one you
chose, and brie   y justify your decision.

a.8 the bullet or the ballot?

627

note 2: the analysis could also be done with kernel models, and doing so would
receive full credit, but computations may take too long. (this could however avoid
needing to re-code partial successes.)

inferential statistics and model assessment

you may not assume that r   s default standard errors or p-values on estimated
regression coe   cients can be trusted. uncertainty should be assessed using suit-
able bootstrap or simulation procedures. (be sure to explain why you used the
procedure you did.) if you need to compare two models in terms of predictive
accuracy, this should not be done through r   s default signi   cance tests or r2   s,
but through either a suitable bootstrap or cross-validation (again, explain the
reasoning behind your choices). exceptions will be made if you can successfully
argue that the default calculations are reliable for this problem.

model checking

the answers you give to the substantive analytical questions rest on your esti-
mated model, so you need to include some assessment of the model   s goodness of
   t. the exact way in which you do this is left up to your initiative; it may help
to remember that the model is predicting probabilities of success. be sure to de-
scribe your procedure and explain why you chose it, that is, why it is appropriate
to answer the questions at hand.

format

your main report should be a humanly-readable document of at most 10 single-
spaced pages, including    gures. it should have the following sections:

introduction describing the scienti   c problem and the data set, possibly including relevant

summary statistics or exploratory graphs.

models with subsections

    describing the speci   cation of the model (or models) you estimated, and

explaining why you decided to use those speci   cations rather than others;

    giving the relevant estimated coe   cients and/or functions (possibly in visual

form), along with suitable measures of uncertainty;

    checking the goodness of    t of the model, including a description of the test
procedures you used, why you chose those ways of checking the model, what
the results were, and what they told you about the ability of the model to
describe the data set.

results answering the analytical questions quantitatively, and with suitable measures

of uncertainty, with reference to your estimated model or models.

you may assume that the reader has a general familiarity with the contents of
401, and with the models and methods we have covered so far in the course, but
will need to be reminded of any details. the reader should not be assumed to
have any prior familiarity with the data set.

628

data-analysis problem sets

numerical results

numerical quantities should be written out to appropriate precision, i.e., neither
more nor fewer signi   cant digits than appropriate.

code

all statistical results must be supported by appropriate code, or they will re-
ceive no credit. (   show your work.   ) the ideal would be to use r markdown, or
knitr+latex, to embed all computations in a humanly readable document, and
submit both the knitted version and the source12 as a second best, it is accept-
able to submit a pdf document containing all text and    gures, and a separate
.r    le, containing all supporting computations, clearly labeled via the comments
so that it is easy to see which claims or results go with which pieces of code.

as usual, this describes the ideal.

rubric

words

(10) the text is laid out cleanly, with clear divisions and transitions between
sections and sub-sections. the writing itself is well-organized, free of grammatical
and other mechanical errors, divided into complete sentences logically grouped
into paragraphs and sections, and easy to follow from the presumed level of
knowledge.

(5) all numerical results or summaries are reported to suitable precision, and
with appropriate measures of uncertainty attached when applicable.

numbers

(5) figures and tables are easy to read, with informative captions, axis labels and
legends, and are placed near the relevant pieces of text.

pictures

code

(15) the code is formatted and organized so that it is easy for others to read
and understand. it is indented, commented, and uses meaningful names. it only
includes computations which are actually needed to answer the analytical ques-
tions, and avoids redundancy. code borrowed from the notes, from books, or from
resources found online is explicitly acknowledged and sourced in the comments.
functions or procedures not directly taken from the notes have accompanying
tests which check whether the code does what it is supposed to. all code runs,
and the markdown    le knits (if applicable).

12 see examples at http://yihui.name/knitr/demos/, and the useful chunk options like echo at

http://yihui.name/knitr/options/.

a.9 a diversi   ed portfolio

629

modeling

(15) regression model speci   cations are described clearly and in appropriate de-
tail. there are clear explanations of how estimating the model helps to answer the
analytical questions, and rationales for all modeling choices. if multiple models
are compared, they are all clearly described, along with the rationale for consid-
ering multiple models, and the reasons for selecting one model over another, or
for using multiple models simultaneously.

id136

(20) the actual estimation of model parameters or estimated functions is tech-
nically correct. all calculations based on estimates are clearly explained, and
also technically correct. all estimates or derived quantities are accompanied with
appropriate measures of uncertainty.

checking

(15) the goodness-of-   t of the model is actively probed by means of tests suitable
to that class of model. the tests chosen are described, along with the rationale
for using those tests. the execution of the tests is technically correct, and the
results of the checks are clearly described. the extent to which the results of the
model assessment build or undermine con   dence in the conclusions is laid out
clearly, with reference to the results of speci   c tests.

conclusions

(15) the substantive, analytical questions are all answered as precisely as the
data and the model allow. the chain of reasoning from estimation results about
the model, or derived quantities, to substantive conclusions is both clear and
convincing. contingent answers (   if x, then y , but if z, then w    ) are likewise
described as warranted by the model and data. if uncertainties in the data and
model mean the answers to some questions must be imprecise, this too is re   ected
in the conclusions.

extra credit

(10) up to ten points may be awarded for reports which are unusually well-
written, where the code is unusually elegant, where the analytical methods are
unusually insightful, or where the analysis goes beyond the required set of ana-
lytical questions.

warning: some questions require slow computations.

a.9 a diversi   ed portfolio

classical    nancial theory suggests that the log-returns of corporate stocks
should be iid gaussian random variables, but allows for the possibility that
di   erent stocks might be correlated with each other. in fact, theory suggests that
the returns to any given stock should be the sum of two components: one which is

source:
classic
material
for    nance,
but
see
especially
fama and
french
(1993),
and,
for
the history,
mackenzie
(2006)

630

data-analysis problem sets

speci   c to that    rm, and one which is common to all    rms. (more speci   cally, the
common component is one which couldn   t be eliminated even in a perfectly diver-
si   ed portfolio.) this in turn implies that stock returns should match a one-factor
model.

the data    le portfolio.csv consists of the log returns for the stocks of 22 selected
large us corporations, centered to have mean zero and scaled to have standard
deviation 1. each row is labeled by the relevant date.

1. (10)

1. (5) report the weights of the    rst principal component. since this is a
vector of length 22, it will be better to report this visually than as a table
or list of numbers. comment on any notable patterns.

2. (5) plot the projection on to the    rst principal component against date.

comment on any notable patterns.

2. (10) fit a one-factor model.

1. (5) report the vector of factor loadings. (again, this will be most easily
reported visually.) comment on any notable patterns, and compare it to
the    rst principal component.

2. (5) plot the factor score against the date. comment on any notable pat-

terns, and compare to the projection on the    rst principal component.

3. (10) use case id64 to provide 90% con   dence intervals for the factor
loadings of the one-factor model. report the results as a    gure rather than a
table.

4. (5) what is the p-value of a goodness of    t test for the hypothesis that one
factor is adequate? explain carefully just what hypothesis is being tested, and
what is entailed by rejecting or retaining it.

5. (5) download the function charles from the class website. explain carefully
what arguments the function takes, what the function does, and exactly what
its return value is. (an acceptable answer to this question could be a thoroughly-
commented version of the function.)

6. (15) write a function which    nds the cross-validated log-likelihood of a factor
model with a given number of factors. that is, it should take a data set and
a number of factors as inputs, divide the data randomly into folds, calculate
the log-likelihood on a test fold of a model    t on the other folds, and return
the average log-likelihood across folds. you are encouraged to re-use existing
code from the solutions and notes; charles may or may not be useful. report
the    ve-fold cross-validated log-likelihood of factor models with from 1 to 10
factors for this data. what is the favored number of factors?

7. (10) using the mvnormalmixem function from the mixtools package,    t a two-

component gaussian mixture model to the data.

1. (5) report the parameters of the two mixture components, and their rela-

tive weights. avoid excessive precision.

a.9 a diversi   ed portfolio

631

2. (5) use posterior component of the object returned by mvnormalmixem to
classify each day as belonging to one mixture component or the other. plot
the mixture components over time, and comment on any patterns.

8. (15) write a function, loglike.mvnormalmix, which takes in a data set and a
model returned by mvnormalmixem, and returns a log-likelihood. check that it
works by seeing that it gives the correct value of the log-likelihood when a
two-component mixture is    t to the whole data. (hint: read section 21.4.4 of
the notes.)

9. (8) write a function which calculates the log-likelihood of mixture models
through cross-validation, as in problem 6. report the    ve-fold cross-validated
log-likelihood of mixture models with from two to four components for this
data. what is the favored number of mixture components?

warning:    ve-fold cv for four mixture components on the full data might
take several hours. start early, and make sure you debug your code on small
parts of the data rather than the whole thing.

10. (2) can you decide whether factor models or mixture models    t this data

better?

632

data-analysis problem sets

company

abbreviation

altria (formerly philip morris)
amazon
apple
archer daniels midland
automatic data processing
bank of america
corrections corporation of america
dow chemicals
equifax
exxonmobil
ford
halliburton
general electric
goldman sachs
graham holding companies
microsoft
proctor and gamble
time warner
united states steel
walmart
yahoo!
yum! brands

mo

amzn
aapl
adm
adp
bac
cxw
dow
efx
xom

f

hal
ge
gs
ghc
msft

pg

twx

x

wmt
yhoo
yum

table a.1 abbreviations for the companies included in the data set.

a.10 the monkey   s paw

[[todo: need to handle continuous vs. discrete issue here     perhaps make
additional problem of constructing a poissonian factor model? or just too com-
plicated?]]

scientific background: nerve cells (or    neurons   ) communicate and pro-
cess information by transmitting little electrical impulses to each other, called
   spikes   13. many neurons use    rate codes   , where the number of spikes they pro-
duce in a short period of time encodes information either about some aspect of
the world the organism is sensing, or about how the organism is acting or is going
to act.

for example, when very    ne electrodes are inserted into certain motor-control
regions of the brains of monkeys, so that neuroscientists can record from individ-
ual neurons, some cells are found to encode the direction in which the monkey
intends to move its hand. speci   cally, a neuron has a preferred direction vector (cid:126)b,
and the when the monkey intends to move its hand with velocity (cid:126)v, the average
number of spikes over a short interval is a + (cid:126)b    (cid:126)v, plus or minus some amount of
noise. a neuron which behaves like this is said to show    directional tuning   , and
(cid:126)b is its    preferred direction   .14

13 because of how they look in a plot of voltage against time.
14 for more on such models of neural coding, see, for example,   3.3 of p. dayan and l. f. abbott,

theoretical neuroscience (mit press, 2001).

a.11 speci   c problems

633

the data set neur.csv is based on an experiment during which the neuro-
scientists recorded simultaneously from 96 directionally-sensitive neurons in a
monkey   s motor region, each cell having a di   erent preferred direction. that is,
each neuron i will have its own (cid:126)bi and its own intercept ai. during each trial, the
monkey was to move its hand in one of eight directions, spread evenly around
a circle. each row of the data frame represents 100ms, and so the entries in the
data frame are the number of spikes produced by each of the 96 neurons spiked
during each time interval.

in this exam, you will both    t a model which derives from this    directional

tuning    idea, and consider alternative multivariate models.

a.11 speci   c problems

1. explain how this model for spiking is, or is related to, a factor model. your
explanation should indicate how a, (cid:126)b and (cid:126)v are related to the factor loadings
and factor scores, and the number of factors.

2. fit a factor model with the number of factors you determined is appropriate
from problem 1. for each neuron, report its preferred direction. (since there are
a large number of neurons, it would probably be best to report this visually.)
3. based on your    tted factor model, report an estimate of the intended direc-
tion (cid:126)v at each time point. (again, this should probably be reported visually.)
the experiment had distinct breaks between trials where the monkey stopped
moving in one direction and started moving in another, random direction; can
you work out, approximately, where these breaks occurred?

4. suppose that instead of recording intended velocities in the usual (x, y) co-
ordinates, we used coordinate axes which were rotated 30 degrees counter-
clockwise from the usual ones. show that this would amount to multiplying

(cid:20) cos   /6     sin   /6

(cid:21)

the intended-velocity vector (cid:126)v by

. explain what e   ect,

sin   /6

cos   /6

if any, this would have on the preferred-direction vector (cid:126)b of each neuron. ex-
plain how this di   erence in coordinate systems could, or could not, be detected
in your factor analysis of the data. in particular, what would this change of
coordinates imply for the interpretation of your factor score estimates and
factor loadings?

5. try    tting a three-cluster mixture model. why might three clusters, specif-
ically, be reasonable? which model predicts better, the factor model or the
three-cluster mixture model?

note: if using the mixtools package, you might    nd it easier to use the
function npem to    t a non-parametric mixture model than to use mvnormalmixem
to    t a gaussian mixture model, since the observable variables are discrete
counts rather than continuous. fitting such a mixture model to the full data
may take as much as a couple of minutes, so allow plenty of time for debugging
and any computation-intensive procedues.

634

data-analysis problem sets

6. try    tting an eight-cluster mixture model. why might eight clusters be rea-

sonable? which model predicts best? (see previous note.)

you are welcome to consider other models for this data as well, but for full

credit you must answer all these questions about these models.

a.12 formatting instructions and rubric

your main report should be a humanly-readable document of at most 10 single-
spaced pages, including    gures. it should have the following sections:

introduction describing the scienti   c problem and the data set, possibly including relevant
summary statistics or exploratory graphs. (do not include eda just to have
eda.)

specific problems answering the questions set above, but avoiding the check-list, itemized format
in favor of continuous text, with a logical succession of sentences and para-
graphs. (writing coherently is more important than following the order of the
questions.)

conclusions summarizing what you have learned from the data and models about whether
the directional-tuning model is really a good description of how these neurons
encode motion.

you may assume that the reader has a general familiarity with the contents of
401, and with the models and methods we have covered so far in the course, but
will need to be reminded of any details. the reader should not be assumed to
have any prior familiarity with the data set.

numerical quantities should be written out to appropriate precision, i.e., neither
more nor fewer signi   cant digits than appropriate.

numerical results

code

all statistical results must be supported by appropriate code, or they will re-
ceive no credit. (   show your work.   ) the ideal would be to use r markdown, or
knitr+latex, to embed all computations in a humanly readable document, and
submit both the knitted version and the source15 as a second best, it is accept-
able to submit a pdf document containing all text and    gures, and a separate
.r    le, containing all supporting computations, clearly labeled via the comments
so that it is easy to see which claims or results go with which pieces of code.

as usual, this describes the ideal.

rubric

15 see examples at http://yihui.name/knitr/demos/, and the useful chunk options like echo at

http://yihui.name/knitr/options/; also the examples in the solutions to exam 1.

a.12 formatting instructions and rubric

635

words

(10) the text is laid out cleanly, with clear divisions and transitions between
sections and sub-sections. the writing itself is well-organized, free of grammatical
and other mechanical errors, divided into complete sentences logically grouped
into paragraphs and sections, and easy to follow from the presumed level of
knowledge.

(5) all numerical results or summaries are reported to suitable precision, and
with appropriate measures of uncertainty attached when applicable.

numbers

(5) figures and tables are easy to read, with informative captions, axis labels and
legends, and are placed near the relevant pieces of text.

pictures

code

(15) the code is formatted and organized so that it is easy for others to read
and understand. it is indented, commented, and uses meaningful names. it only
includes computations which are actually needed to answer the analytical ques-
tions, and avoids redundancy. code borrowed from the notes, from books, or from
resources found online is explicitly acknowledged and sourced in the comments.
functions or procedures not directly taken from the notes have accompanying
tests which check whether the code does what it is supposed to. all code runs,
and the markdown    le knits (if applicable). the main text of the report is free of
intrusive blocks of code, which are used only when a speci   cally-computational
point is being made, or when code is actually the clearest way of describing a
point.

speci   c problems

(25) all speci   c problems posed in   a.11 receive clear, well-written and correct
answers. the answers show, and convey, a real grasp of the mathematical basis
of the models being manipulated, and how quantities in the model are related to
the underlying scienti   c questions about neural coding of movement.

id136 and uncertainty

(15) the actual estimation of model parameters or estimated functions is tech-
nically correct. all calculations based on estimates are clearly explained, and
also technically correct. all estimates or derived quantities are accompanied with
appropriate measures of uncertainty (such as con   dence intervals or standard
errors).

comparisons

(15) all comparisons between models are done in a statistically valid way: if
in-sample, they are accompanied by an explanation of why this particular in-
sample comparison will not lead to over-   tting; if out-of-sample, there is a clear

636

data-analysis problem sets

description of the generalization process being performed. the execution of com-
parisons is technically correct, and their results clearly described. the extent to
which comparisons provide either clear or ambiguous evidence about which mod-
els    t better is made plain to the reader, and is carried through to the ultimate
conclusions.

conclusions

(15) the substantive questions about neural coding are all answered as precisely
as the data and the model allow. the chain of reasoning from estimation results
about models, or derived quantities, to substantive conclusions is both clear and
convincing. contingent answers (   if x, then y , but if z, then w    ) are likewise
described as warranted by the model and data. if uncertainties in the data and
model mean the answers to some questions must be imprecise, this too is re   ected
in the conclusions.

extra credit

(10) up to ten points may be awarded for reports which are unusually well-
written, where the code is unusually elegant, where the analytical methods are
unusually insightful, or where the analysis goes beyond the required set of ana-
lytical questions.

a.13 what   s that got to do with the price of condos in california?
agenda: as a warm-up and refresher in using id75 to explore relationships between
variables, we will look at a large data set on real estate prices.

the census bureau divides the country up into geographic regions, smaller
than counties, called    tracts    of a few thousand people each, and reports much
of its data at the level of tracts. this data set, drawn from the 2011 american
community survey, contains information on the housing stock and economic
circumstances of every tract in california and pennsylvania. for each tract, the
data    le records a large number of variables (not all of which will be used in this
assignment):
    a geographic id code, a code for the state, a code for the county, and a code
    the population, latitude and longitude of the tract
    its name
    the median value of the housing units in the tract
    the total number of units and the number of vacant units
    the median number of rooms per unit
    the mean number of people per household which owns its home, the mean
    the median and mean income of households (in dollars, from all sources)

number of people per renting household

for the tract

a.13 what   s that got to do with the price of condos in california?

637
    the percentage of housing units built in 2005 or later; built in 2000   2004; built
in the 1990s; in the 1980s; in the 1970s; in the 1960s; in the 1950s; in the 1940s;
and in 1939 or earlier
    the percentage of housing units with 0 bedrooms; with 1 bedroom; with 2;
    the percentage of households which own their home, and the percentage which

with 3; with 4; with 5 or more bedrooms

rent

remember that these are not values for individual houses or families, but sum-
maries of all of the houses and families in the tract.

the basic question here has to do with how the quality of the housing stock,
the income of the people, and the geography of the tract relate to house values
in the tract. we will look at several di   erent linear models, and see if they have
reasonable interpretations, and/or make systematic errors.

1. (3 pts) not all variables are available for all tracts. remove the rows containing
na values. all subsequent problems will be done on this cleaned data set. hint:
recipe 5.27.

1. (1) how many tracts are eliminated?
2. (1) how many people live in those tracts?
3. (1) what happens to the summary statistics for median house value and

median income?

2. (7) house value and income

1. (1) linearly regress median house value on median household income. re-
port the intercept and the coe   cient (to reasonable precision), and explain
what they mean.

2. (2) regress median house value on mean household income. report the
intercept and the coe   cient (to reasonable precision), and explain what
they mean. why are the coe   cients for two di   erent measure of household
income di   erent?

3. (4) regress median house value on both mean and median household in-
come. report the estimates, and interpret the coe   cients, as before. does
this interpretation seem reasonable? explain.

3. (10) regress median house value on median income, mean income, population,
number of housing units, number of vacant units, percentage of owners, median
number of rooms, mean household size of homeowners, and mean household
size of renters. report all the estimated coe   cients and their standard errors
to reasonable precision, and explain what they mean. why are the coe   cients
on income di   erent from in the previous models?

4. (5) which three variables are most important, in this model, for predicting
house values? explain your reasoning for deciding on this. hint: make sure
your answers wouldn   t change if we changed the units of measurement for the
predictor variables.

5. (20) checking residuals for the model from problem 3.

638

data-analysis problem sets

1. (5) make a q     q plot of the regression residuals.
2. (5) make scatter-plots of the regression residuals against each of the predic-
tor variables, and add kernel smoother curves (as in chapter 1). describe
any patterns you see. (a very rough rule of thumb is that the bandwidth
should be about   n   1/5, where    is the standard deviation of the predictor
variable and n is the sample size.)

3. (5) make scatter-plots of the squared residuals against each of the predictor
variables, and add kernel smoother curves. describe any patterns you see.

4. (5) explain, using these plots, whether the residuals appear gaussian and

independent of the predictors.

6. (12) fit the model from 3 to data from california alone, and again to data

from pennsylvania alone.

1. (5) report the two sets of coe   cients and standard errors. explain whether

or not it is plausible that the true coe   cients are really equally.

2. (2) what are the square root of the mean squared error (rmses) of the

pennsylvania and california coe   cients, on their own data?

3. (5) use the california coe   cients to predict the pennsylvania data. what
is the rmse? what is the correlation between the california coe   cients   
predictions for pennsylvania, and the pennsylvania coe   cients    predictions?
hint: recipe 11.18.

7. (10) make a map of median house values. the vertical coordinate should be
latitude, the horizontal coordinate should be longitude, and the house value
should be indicated either by the color of the points (hint: recipe 10.23), or
by using a third dimension in a perspective plot. describe the patterns that
you see.

8. (10) make a map of the regression residuals for the model from problem 3.
are they randomly scattered over space, or are there regions where the model
systematically over- or under- predicts? are there regions where the errors are
unusually large in both directions? (you might also want to make a map of
the absolute value of the residuals.)     if you cannot make a map, you can still
get partial credit for scatter-plots of residuals against latitude and longitude.

9. (8) fit a id75 with all the variables from problem 3, as well as
latitude and longitude. report the new coe   cients and their standard errors.
what do the coe   cients on latitude and longitude mean? how important are
latitude and longitude in this new model?

10. (5) make a map of the regression residuals for the new model from problem
9. compare and contrast it with the map of the residuals from the previous
model. are the new residuals spatially uniform, or are there patterns?

11. (10) regress the log of median house value on the same variables as in problem

9. which model more accurately predicts housing prices? how can you tell?

a.14 the advantages of backwardness

639

a.14 the advantages of backwardness

many theories of economic growth say that it   s easier for poor countries to grow
faster than rich countries        catching up   , or the    advantages of backwardness   .
one argument for this is that poor countries can grow by copying existing, suc-
cessful technologies and ways of doing business from rich ones. but rich countries
are already using those technologies, so they can only grow by    nding new ones,
and copying is faster than innovation. so, all else being equal, poor countries
should grow faster than rich ones. one way to check this is to look at how growth
rates are related to other economic variables.

our data for examining this will be taken from the    penn world table    (http:
//pwt.econ.upenn.edu/php_site/pwt_index.php), for selected countries and
years. the data    le is penn-select.csv on the class website. each row of this
table gives, for a given country and a    ve-year period, the starting year, the initial
population of the country, the initial gross domestic product (gdp)16 per capita
(adjusted for in   ation and local purchasing power), the average annual growth
rate of gdp over that period, the average population growth rate, the average
percentage of gdp devoted to investment, and the average percentage ratio of
trade (imports plus exports) to gdp17.

we will use the np package on cran to do kernel regression.18 install it, and

load the data    le penn-select.csv (link on the class website).

1. (5 points) fit a linear model of gdp.growth on log(gdp). what is the coe   -

cient? what does it suggest about catching-up?

2. (5 points) fit a linear model of gdp.growth on log(gdp), pop.growth, invest
and trade. what is the coe   cient on log(gdp)? what does it suggest about
catching-up?

3. (5 points) it is sometimes suggested that the catching-up e   ect only works for
countries which are open to trade with, and learning from, more-developed
economies. add an interaction between log(gdp) and trade to the model
from problem 2. what are the relevant coe   cients? what do they suggest
about catching-up?

4. (15 points) use data-set splitting, as in chapter 3 of the notes, to decide which
of these three linear models predicts best. (you can adapt the code from that
chapter or write your own.) which one is the winner?

5. (15 points) the npreg function in the np package does kernel regression. by
default, it uses a combination of cross-validation and sophisticated but very
slow optimization to pick the best bandwidth. in this problem, we will force
it to use    xed bandwidths, and do the cross-validation ourselves.

16 annual gross domestic product is the total value of all goods and services produced in the country
in a given year. it has some pathologies     an earthquake which breaks everyone   s windows could
increase gdp by the value of the repairs     but it   s a standard measure of economic output.

17 the penn tables call this variable    openness   . it can be bigger than 100, if, for instance, a country

re-exports lots of its imports.

18 in addition to the examples in chapter ?? of the notes, the package has good help    les, and a

tutorial at http://www.jstatsoft.org/v27/i05.

640

data-analysis problem sets

penn.0.1 <- npreg(gdp.growth~log(gdp),bws=0.1,data=penn)

does a kernel regression of growth on log(gdp), using the default kernel
(which is gaussian) and bandwidth 0.1. (you don   t have to call the data
penn.) you can run fitted, predict, etc., on the output of npreg just as you
can on the output of lm. (there are more examples of using npreg in chapter
4.)

the code at the end of this assignment (also online) uses    ve-fold cross-

validation to estimate the mean-squared error for the six bandwidths 0.05, 0.1, 0.2, 0.3, 0.4, 0.5.
use it to create a plot of cross-validated mse versus bandwidth. add to the
same plot the in-sample mses of those six bandwidths on the whole data.
what bandwidth predicts best?

6. (10 points) make a scatterplot of log(gdp) versus growth. add the line for
the linear model from problem 1. add the    tted values for the kernel curve
with the best bandwidth (according to the previous problem). what does this
suggest about catching up?

(there are at least two ways to get the    tted values for the kernel regression,

using fitted or predict.)

7. (5 points) npreg will also do kernel regressions with multiple input variables.

this time, use the built-in bandwidth selector:

penn.npr <- npreg(gdp.growth ~ log(gdp) + pop.growth + invest

+ trade, data=penn, tol=0.1, ftol=0.1)

(the last two arguments tell the bandwidth selector not to try very hard to
optimize; it may still take several minutes.) what are the selected bandwidths?
(use summary.)

8. (5 points) explain why we cannot add an interaction between log(gdp) and

trade to the nonparametric regression from the previous problem.
9. (15 points) sub-divide the data into points where the initial gdp per capita
is     $700 and those where it is above. for each data point, use the kernel
regression from problem 7 to predict the change in growth-rate from a 10%
decrease in initial gdp (not a 10% decrease in log-gdp). report the averages
over the initially-poorer and the initially-richer data points. describe what
this suggests about catching up.

hints: use predict() with partially-modi   ed data; do not estimate another
regression with arti   cially-lowered initial gdps; make sure you are changing
initial gdp by 10%, and not changing the log of gdp by 10%.

10. (10 points) to chose between the best linear model (as picked by you in prob-
lem 4) and the kernel regression from problem 7, use cross-validation again.
modify the code provided to use    ve-fold cross-validation to get cv mses for
both the id75 and for the nonparametric regression (with auto-
matic bandwidth selection). which predicts better?

11. (10 points) based on your analysis, does the data support the idea of catching
up, undermine it, support its happening under certain conditions, or provide
no evidence either way? (as always, explain your answers.)

a.15 it   s not the heat that gets you

641

# compare predictive ability of different bandwidths using k-fold cv

# inputs: number of folds, vector of bandwidths, dataframe
# presumes: data frame contains variables called "gdp.growth" and "gdp"
# output: vector of cross-validated mses for the different bandwidths
# the default bandwidths here are not good ones for other problems

cv.growth.folds <- function(nfolds=5, bandwidths=c(0.05,(1:5)/10), df=penn) {

require(np)
case.folds <- rep(1:nfolds,length.out=nrow(df))

# divide the cases as evenly as possible

case.folds <- sample(case.folds) # randomly permute the order
fold.mses <- matrix(0,nrow=nfolds,ncol=length(bandwidths))
colnames(fold.mses) = as.character(bandwidths)

# by naming the columns, we'll won't have to keep track of which bandwidth
# is in which position

for (fold in 1:nfolds) {

# what are the training cases and what are the test cases?
train <- df[case.folds!=fold,]
test <- df[case.folds==fold,]
for (bw in bandwidths) {

# fit to the training set
# first create a "bandwidth object" with the fixed bandwidth
current.npr.bw <- npregbw(gdp.growth ~ log(gdp), data=train, bws=bw,

bandwidth.compute=false)

# now actually use it to create the kernel regression
current.npr <- npreg(bws=current.npr.bw)
# predict on the test set
predictions <- predict(current.npr, newdata=test)
# what's the mean-squared error?
fold.mses[fold,paste(bw)] <- mean((test$gdp.growth - predictions)^2)
# using paste() here lets us access the column with the right name...

}

}
# average the mses
bandwidths.cv.mses <- colmeans(fold.mses)
return(bandwidths.cv.mses)

}

a.15 it   s not the heat that gets you, it   s the sustained conjunction

of heat with elevated levels of atmospheric pollutants

agenda: more practice with additive models; more practice with transformed variables; ex-
tending additive models to include interactions; re-shaping data frames; answering    what if?   
questions using models.

timing: problems 1   4 and 6 involve    tting models to data, plotting, and interpretation, but
no coding. problem 5 requires explaining and using some provided code. problem 7 requires

642

data-analysis problem sets

doing some math, and possibly writing some code to do the corresponding calculation. the
solutions for problems 1   7 take a few minutes to knit. the extra credit takes about 40 minutes
to run with streamlined code.

the data set chicago, in the package gamair, contains data on the relationship
between air pollution and the death rate in chicago from 1 january 1987 to 31 de-
cember 2000. the seven variables are: the total number of (non-accidental) deaths
each day (death); the median density over the city of large pollutant particles
(pm10median); the median density of smaller pollutant particles (pm25median);
the median concentration of ozone (o3) in the air (o3median); the median con-
centration of sulfur dioxide (so2) in the air (so2median); the time in days (time);
and the daily mean temperature (tmpd).

we will model how the death rate changes with pollution and temperature.
epidemiologists tell us that risk factors usually multiply together rather than
adding, so we will    t additive models to the logarithm of the number of deaths.
for    tting additive models, please use the mgcv package.

1. load the data set and run summary on it.

1. (1) is temperature given in degrees fahrenheit or degrees celsius?
2. (2) the pollution variables are negative at least half the time. what might

this mean?

3. (2) we will ignore the pm25median variable in the rest of this problem set.

why is this reasonable?

2. fit a spline smoothing of log(death) on time. (you can use either smooth.spline

or gam.)
1. (4) plot the smoothing spline along with the actual values.
2. (3) there should be four large outliers, right next to each other in time.
when are they? for full credit, give calendar dates, not day numbers.
(hints: day 0 was 31 december 1993; the as.date function.)

3. (3) calculate the r2 of the model. in what sense, if any, is this the    pro-

portion of variance explained by the model   ?

3. use gam to    t an additive model for log(death) on pm10median, o3median,
so2median, tmpd and time. use spline smoothing for each of these predictor
variables. hint: because of some missing-data issues, some plots later may be
easier to make if you set the na.action=na.exclude option when estimating
the model.
1. (7) plot the partial response functions, with partial residuals. describe the

partial response functions in words.

2. (4) plot the    tted values as a function of time, along with the actual values

of log(death). hint: you will have to be careful about the na values!

3. (4) are the outliers still there? are they any better? hint: look at the

residuals here.

4. medically, it makes more sense to suppose that deaths on day t are due con-
ditions over the previous few days, and not just on the conditions on day t.
this problem re-shapes the data set to let us model this.

a.15 it   s not the heat that gets you

643

1. (8) suppose that on any given day, we want to know the average value of
some variable over today and the previous k days. explain how the following
code computes that.
lag.mean <- function(x, window) {

n <- length(x)
y <- rep(0,n-window)
for (t in 0:window) {
}
return(y/(window+1))

y <- y + x[(t+1):(n-window+t)]

}

in particular, how is k related to the arguments?

2. (7) create a new data frame with the same column names as chicago,
but where, on each day, the value of the pollution concentrations and tem-
perature is the average of that day   s value with the previous three days.
(hint: you will want to do di   erent things to di   erent columns of chicago.)
how many rows should this data frame have? make sure that the time and
death columns are properly aligned with the new, time-average predictor
variables. how can you check that this is working properly?

5. fit an additive model, as in problem 3, with the time-averaged pollution and

temperature variables. (do not average time or death.)

1. (5) plot the partial response functions and their partial residuals.
2. (5) plot the    tted values as a function of time, and the actual values. what

has happened to the outliers? hint: again, look at the residuals.

6. variable examination

1. (4) find the rows in the data frame (with the time-averaged values) corre-
sponding to the large-death outliers. look at all variables for them, and for
three days on either side. now compare this to the same stretch of time a
year earlier. which two variables, aside from death, are unusually high or
low around the outliers?

2. (7) re-   t the model from problem 5, with an interaction between the two

variables you just picked out. plot the partial response functions.

3. (4) plot the    tted values versus time. what has happened to the outliers?

hint: residuals once more.

7. using the last model you    t, we will consider the predicted impact of a 2   
celsius increase in temperature on log(death), taking the last full year of the
data as a baseline.19.

19 2   c is in the middle range of current projections for the global average e   ect of climate change by

the end of this century
(http://www.ipcc.ch/publications_and_data/ar4/wg1/en/contents.html)q. of course it   s
unrealistic to suppose that would be an even shift throughout the year, or for that matter that
chicago would necessarily warm by the average amount. in fact, some of the models

644

data-analysis problem sets

1. (1) prepare a data frame containing only the last full year of the data.

what is the average predicted value of log(deaths)?

2. (1) modify this data frame to increase all temperatures by 2   c.
3. (3) find the new average change in the predicted values of log(deaths)

associated with a 2   c warming.

4. (5) find a standard error for this average predicted change, using the stan-
dard errors for the prediction on each day, and assuming no correlation
among them. include an explanation of why your calculation is correct.
also give the corresponding gaussian 95% con   dence interval. hint 1: the
se.fit option to predict. hint 2: the appendix to the textbook on    prop-
agation of error   .

5. (5) find the predicted change in the number of deaths (not change in
log(death) from a 2   c warming over the course of a whole year. hint:
remember that ex (cid:54)= ex.

6. (5) find a standard error for the predicted change in the number of deaths
(not the change in log(death)) and the corresponding 95% gaussian con-
   dence interval. hint: propagation of error again.

extra credit 1 (10):

1. (4) explain how you could use id64 to give a 95% con   dence interval
for the average increase in log(death) over the year. explain how your idea
will handle the fact that the model uses multiple variables, and that what
happens on day t is not independent of what happens on day t     1. more
credit will be given for more precise, complete and clear explanations. (you
do not have to implement your solution yet.)

2. (6) implement your id64 scheme and give the con   dence interval.

a.16 nice demo city, but will it scale?

a.16.1 version 1

a.16.1.1 background

it has been known for a long time that larger cities tend to be more economically
productive than smaller ones. that is, the economic output per person of a city or
other settlement (y ) tends to increase with the population (n ). recently, there
has been some controversy over the exact form of the relationship, and over its
explanation.

in particular, it has been claimed20 that urban incomes show    power-law scal-

ing   , meaning that

y     y0n a

(http://www.ipcc.ch/publications_and_data/ar4/wg1/en/ch11s11-5-3.html, figure 11.11) have
4   c of warming in the middle of their prediction intervals for central north america.

20 by geo   rey west and collaborators; there   s a good video online of prof. west giving a talk about

the work at a ted conference, if you   re interested.

just

[[todo:
integrate
the
two
versions of
this prob-
lem
set,
perhaps
by
picking
one]]
ver-
this
sion
was
used as a
take-home
exam,
hence
less
sca   olding;
   ag
as
such
in
guide
the
to
the
problems
[[todo:
yank refer-
ences from
preface to

a.16 nice demo city, but will it scale?

645

for some constant y0 > 0 (the same across cities) and some scaling exponent a > 0
(the same across cities). equivalently21,

log y     c + a log n

the scientists who    rst postulated power law scaling for urban economies thought
that the tendency for bigger cities to be more productive was largely due to
what are called    increasing returns to scale   22, which would be stronger in larger
cities. additionally, having more people around, and more di   erent sorts of people
around, could lead to exchanges of ideas and so to new and better ways of doing
business. according to this view, the primary determinant of a city   s economy is
simply its size, and larger cities are just    scaled up    versions of smaller ones.

an alternative explanation is that di   erent industries have di   erent levels of
income per worker, and that some industries tend to be concentrated in larger
cities and others in smaller towns. large cities tend especially to be the places
where one    nds highly skilled providers of very specialized services, though their
services are used, often indirectly, more or less everywhere23. in this view, the
association between the population of cities and their economic productivity is
due to the kind of industries that go with being big cities, not some e   ect of size
as such. there is no reason, according to this    urban hierarchy    view, why the
relationship between per-capita income y and urban population n should be a
power law. in fact, the urban-hierarchy model doesn   t even specify a particular
functional relationship between how much of a city   s economy comes from high-
value industries and the city   s income, just that the relationship is increasing.

note that neither the power-law nor the urban-hierarchy model predicts gaus-

sian distributions.

in this exam, you will assess the evidence for power law scaling, and whether
the    urban hierarchy    idea can explain the relationship between income and
population.

data

for data-collection purposes, urban regions of the united states are divided into
several hundred    metropolitan statistical areas    based on patterns of residence
and commuting; these cut across the boundaries of legal cities and even states.
in the last decade, the u.s. bureau of economic analysis has begun to estimate
   gross metropolitan products    for these areas     the equivalent of gross national
product, but for each metropolitan area. (see homework 2 for the de   nition of
   gross national product   .) our data set contains the following variables, derived
from the bea:

21 why is it equivalent, and how is c related to y0?
22 this is when the cost of producing the same item, with the same factory, employees, etc., is lower

when the volume being produced is high, perhaps because the system runs more e   ciently, or each
sale has to recover a smaller share of the    xed cost of setting up the factory. a constant sale price
minus lower costs equals higher pro   ts.

23 there are probably few, if any, electrochemical engineers who design liquid crystal displays working
in altoona, pa, but everyone there who buys a cellphone indirectly pays for the time and training
of such engineers who live elsewhere.

data-analysis problem sets

646
    the name of each metropolitan area;
    its per-capita gross metropolitan product, in dollars (y );
    its population (n );
    the share of its economy derived from    nance (as a fraction between 0 and 1);
    the share of    professional and technical services   ;
    the share of    information, communication and technology    (ict);
    and the share of    management of    rms and enterprises   .

note that the last four columns have some missing values (nas), since the bea
does not release those    gures when doing so would disclose sensitive information
about individual companies.

a.16.1.2 tasks and questions

you are to write a report assessing the (1) whether the power-law scaling model
accurately represents the relationship between urban population and urban per-
capita income; (2) whether, as the    urban hierarchy    idea implies, the relationship
can be explained away by controlling for which industries are found in which cities;
and (3) whether the power-law scaling or the urban-hierarchy idea provides a
better model of urban economies.

tainty24;

your report should have the following sections: an introduction, laying out
the questions being investigated and the approach taken; a description of the
data; detailed analyses; and conclusions. your report should deal with at least
the following speci   c points:
    the estimation of the scaling exponent a from the data, including its uncer-
    an estimate of the out-of-sample error of the power-law-scaling model;
    an examination of that model   s residuals;
    a comparison of that model to non-parametric models of the size-income rela-
    whether larger cities tend to have higher shares of the four high-value indus-
tries measured in the data set, and if so, what the size-industry relationship
is;
    whether cities with higher shares for those industries have higher incomes, and
    whether, and in what sense, the income-industry relationships can explain the
    how missing values were handled, and why;
    appropriate quanti   cations of uncertainty for all estimates and hypothesis

tionship (including, but not limited to, out-of-sample errors);

if so, what the industry-income relationship is;

size-income relationship;

tests.

adequately dealing with these points may, of course, lead to others.

24 hint: you should get a value in the range (0, 0.5).

[[this ver-
sion
was
a pair of
homework
assign-
ments,
so
the points
add up to
200]]

a.16 nice demo city, but will it scale?

647

a.16.2 version 2

a.16.2.1

for data-collection purposes, urban areas of the united states are divided into
several hundred    metropolitan statistical areas    based on patterns of residence
and commuting; these cut across the boundaries of legal cities and even states.
in the last decade, the u.s. bureau of economic analysis has begun to estimate
   gross metropolitan products    for these areas     the equivalent of gross national
product, but for each metropolitan area. (see homework 2 for the de   nition of
   gross national product   .) even more recently, it has been claimed that these
gross metropolitan products show a simple quantitative regularity, called    supra-
linear power-law scaling   . if y is the gross metropolitan product in dollars, and
n is the number of people in the city, then, the claim goes,

y     cn b

(a.5)

where the exponent b > 1 and the scale factor c > 0. this homework will use the
tools built so far to test this hypothesis.

1. (15 points) a metropolitan area   s gross per capita product is y = y /n . show

that if eq. a.5 holds, then

log y       0 +   1 log n

how are   0 and   1 related to c and b?

2. (15 points) the data    les gmp 2006.csv and pcgmp 2006.csv on the class
website contain the total gross metropolitan product (y ) in millions of dollars,
and the per capita gross metropolitan product (y) in dollars, for all metropoli-
tan areas in the us in 2006. read them in and use them to calculate the
metropolitan populations (n ). if it   s done correctly, then running summary on
the population    gures should give

min. 1st qu.
135600

54980

median
231500

mean 3rd qu.

max.
530900 18850000

680900

(your exact results may di   er very slightly because of rounding and display
settings.) what is the variance of log y?

3. (20 points) estimating the power-law scaling model. use lm to linearly regress
log per capita product, log y, on log population, log n . how does estimating
this statistical model relate to equation a.5? what are the estimated coe   -
cients? are they compatible with the idea of supra-linear scaling? what is the
mean squared error for log y?

4. (15 points) plot per capita product y against n , along with the    tted power-

law relationship from problem 3. (be careful about logs!)

5. (15 points) fit a non-parametric smoother to log y and log n . (you can use
kernel regression, a spline, or any other non-parametric smoother.) what is
the mean squared error for log y? describe, in words, how this curve compares
to the power-law model from problem 3.

648

data-analysis problem sets

6. (20 points) using the method from [[lecture 10, section 1]], test whether the
power-law relationship is correctly speci   ed. what is the p-value? what do
you conclude about the validity of the power-law model, based not just on
this problem but the previous ones as well?

a.16.2.2

we continue to investigate the relationship between how big cities are, and how
economically productive they are. the scientists who    rst postulated power laws
for urban economies thought that the tendency for bigger cities to be more pro-
ductive was largely due to what are called    increasing returns to scale   25, which
would be bigger in larger cities. additionally, having more people around, and
more di   erent sorts of people around, could lead to exchanges of ideas and so to
new and better ways of doing business.

an alternative explanation is that di   erent industries have di   erent levels of
income per worker, and that some industries tend to be concentrated in larger
cities and others in smaller towns. large cities tend especially to be the places
where one    nds highly skilled providers of very specialized services, though their
services are used, often indirectly, more or less everywhere26. in this view, the
association between the population of cities and their economic productivity is
due to the kind of industries that go with being big cities, not some e   ect of size
as such.

in this exam, you will do a fairly simple test of these two explanations.

data

a data    le has been e-mailed to you at your andrew account. it is a comma-
separated text    le (csv), containing the following columns, in order, for each
metropolitan area:
    the name of the metropolitan area;
    its per-capita gross metropolitan product (in dollars)
    its population;
    the share of its economy derived from    nance (as a fraction between 0 and 1);
    the share of    professional and technical services   ;
    the share of    information, communication and technology    (ict);
    and the share of    management of    rms and enterprises   .
the    rst three columns you saw in the last homework. the last four columns
came from the same source. however, those columns have some missing values
(nas), since the bureau of economic analysis does not release the data when
doing so would disclose sensitive information about individual companies.

25 this is when the cost of producing the same item, with the same factory, employees, etc., is lower

when the volume being produced is high, perhaps because the system runs more e   ciently, or each
sale has to recover a smaller share of the    xed cost of setting up the factory. a constant sale price
minus lower costs equals higher pro   ts.

26 there are probably very few electrochemical engineers who design liquid crystal displays in altoona,
but everyone there who buys a cellphone indirectly pays for the time and training of such engineers
who live elsewhere.

a.16 nice demo city, but will it scale?

649

1. more specialist service industries in bigger cities?

a.16.2.3 problems

1. (2 points) for each of the four industries, create a scatter-plot of the share
of that industries in the economy as a function of population. if a city is
missing a value for an industry, omit it from that plot.

2. (5 points) add a nonparametric smoothing curve to each plot. use kernel
regression, local id75, a smoothing spline, etc., as you wish, but
make sure that you use cross-validation to adapt the amount of smoothing
to the roughness of the data.

3. (3 points) describe the patterns made by these plots. in particular, do

larger cities have more of these industries?

2. higher productivity from specialist service industries?

1. (2 points) for each of the four industries, create a scatter-plot of per-capita
gmp as a function of the share of that industry in the city   s economy. if a
city is missing a value for an industry, omit it from the plot.

2. (5 points) add a nonparametric smoothing curve to each plot. (use the

same smoothing method you did for problem 1.)

3. (3 points) describe the patterns made by these plots. in particular, do cities

which are more dependent on these industries have higher productivity?

3. are bigger cities more productive, controlling for industry shares? using the
gam function from the mgcv package,    t the semi-parametric log-additive model

4(cid:88)

ln y =   0 + b ln n +

fj(xj) +  

j=1

where y is per-capita gmp, n is population, and x1 through x4 are the shares
of the four industries.
1. (5 points) explain how this model is related to, but di   erent than, the
power-law scaling model from the last homework. which terms in the model
are parametric, and which are non-parametric?

2. (2 points point) what r command did you use to    t this?
3. (2 points) report your estimated values for   0, b, and the residual standard

error.

4. (6 points) provide plots of each of the four partial response functions fj.
compare them to the plots from question 2     do they suggest the same
relationships between industry shares and the level of productivity, and if
not, how do they di   er? hint: help(plot.gam,package="mgcv")

5. (5 points) do the residuals seem to have a gaussian distribution? (justify

your answer.)

6. (5 points) running summary on your    tted model will produce output which
includes approximate standard errors and p-values for the parametric terms,
assuming homoskedastic gaussian noise. what standard error and p-value
does it report for b? is that term signi   cant? do you think you can trust
those calculations in this case?

650

data-analysis problem sets

4. predictive comparisons

1. (5 points) take the    tted power-law scaling model from the last homework.
(if you were unable to complete that homework, follow the solutions.) for
each city, compute the predicted change in ln y from increasing that city   s
population by 10%. report the average change over all cities.

2. (5 points) repeat this calculation, for the cities where complete data is
available, for the model you    t in problem 3, assuming that only population
changes.

3. (5 points) do the two models seem to lead to di   erent conclusions about

the e   ect of population on productivity? explain

5. model comparisons

1. (3 points) what is the in-sample mean squared error, for ln y, of the additive
model you    t in problem 3? how much smaller is it than the linear (power
law) model from the last homework? explain why the additive model should
always have a smaller in-sample error than the linear model.

2. (11 points) describe, concisely and in your own words, a technique for
determining whether the additive model from problem 3 is better able
to generalize than the pure power law model. explain why this technique
should be reliable here. (you are free to use a method from 36-401, if you
can explain why it is applicable.)

3. (11 points) implement this comparison and report your results. which

model is favored?

6. evaluation

1. (10 points) based on what you have done so far, does it seem that city size
directly e   ects productivity? speci   cally, if an american city wanted to
increase its per-capita economic output, should it try to increase population,
or change its industries?

2. (5 points) suggest additional data, models or comparisons which could

improve your analysis.

a.17 fair   s a   airs

in 1969, the magazine psychology today did a survey of its readers that included
questions about (among other things) how often the respondents had had extra-
marital sex in the previous twelve months. in 1978 the economist ray c. fair
used this data to develop a    theory of extramarital a   airs    (fair, 1978)27, with
the idea that people optimize a trade-o    between working, spending time with
their spouse, and spending time with a    paramour   . the model and data have
become very well known (there are at least a hundred later papers and books
which reference it), and is available as affairs in the package aer on cran.

the variable affairs records the answer to    how often did you engage in
extramarital sexual intercourse during the past year   , with values of    once a

27 this paper also used a similar survey of readers of redbook in 1974, not part of this data set.

a.17 fair   s a   airs

651

month   , or more frequently, all coded as 12. other variables are sex, age, how
many years the respondent had been married28, whether they had children, how
religious they were (on a scale of 1   5), their level of education, how much prestige
their occupation had (on a scale of 1   7), and how happy they were with their
marriage (on a scale of 1   5).

1. (30 points) two speci   cations

1. (15 points) using id28,    t a model for the number of times
respondents said they had extramarital sex during the previous year. de-
scribe, in words, the predictions of the model. which variables are signi   -
cant predictors?

2. (15 points) repeat (1a), but use id28 to    t a model for whether
respondents said they had extramarital sex at all during the previous year.

2. (10 points) are the same variables signi   cant in both models in problem 1?
do they have the same signs in both models? should the models match in this
way? explain.

3. (20 points) comparing predictions

1. (5 points) for each person in the data set, calculate the predicted proba-

bility, under both models, that they did not have an a   air.

2. (10 points) plot these against each other. describe the plot in words.
3. (5 points) do the models agree with each other in their predictions? should

they? explain.

4. (20 points) calibration

1. (2 points) consider all the people for whom the predicted id203 of an
a   air, according to the model from problem (1a), is less than 10%. what
fraction of them report having a   airs?

2. (3 points) repeat this calculation for predicted probabilities between 10%
and 20%, 20% and 30%, etc. plot the actual frequencies against the pre-
dicted probabilities.

3. (5 points) make a similar plot for the other model. (you can combine the

plots, if the result is clear.)

4. (10 points) for which model do the predictions seem to match the data

best? explain with reference to your plots.

5. (10 points) download fair   s paper and read table i (p. 53). does it make
sense to use a linear response for all of the variables (as in problem 1 above),
or would it be better to treat some variables as categorical? explain.

6. (10 points) evaluation

1. (5 points) do either of these models seem to provide an adequate description

of the data? (explain.) if not, what else could one try?

2. (5 points) is it reasonable to use this data to develop theories about con-

temporary behavior? explain.

28 prof. fair removed respondents who had never married, or had married more than once.

652

data-analysis problem sets

a.18 how the north american paleofauna got a crook in its

regression line

our problem set this week concerns an important question for evolutionary bi-
ology and paleontology. it has been argued that larger organisms tend to have
selective advantage over smaller ones of the same species, but larger bodies de-
mand more specialized internal structure, more    division of labor   , than small
ones, indirectly driving the evolution of increased biological complexity (bonner,
1988). to evaluate this, it is important to know whether species tend to get larger
over evolutionary time, and, if so, to characterize this accurately.

our data set this week is taken from the north american mammalian paleo-
faunal database, which contains information on the typical body mass of about
2000 living and extinct species of mammals native to north america. (you can
   nd it on the website, http://www.stat.cmu.edu/~cshalizi/uada/13/hw/04/
nampd.csv.) speci   cally, the columns of the data give: the scienti   c name of the
species; the natural logarithm of its typical body mass (measured in grams); the
natural logarithm of the mass of its ancestor (in grams); how long ago it    rst
appeared in the fossil record (in millions of years); and how recently it last ap-
peared (in millions of years; an na in this column indicates the species is still
alive). we will model how the change body mass is related to the body mass of
the ancestral species. in particular, paleontologists have suggested that the cor-
rect model relating change in log mass to ancestral log mass should be piece-wise
linear: a downward-sloping line for small ancestral log masses, and    at for larger
ancestral masses. in this problem set, you will    t that model, and examine its
predictions.

1. (10) basics

1. (5) load the data. create a vector which gives each species    change in log
body mass from its ancestor, and add it to the data frame as a new column.
explain, in your own words, what it would mean for a species to have a
value of +0.7 in this column. check that this column has na values in the
correct places. explain how you know that those are the correct places.
remove all the rows with na values for the change in log mass, and use
this cleaned version of the data for all subsequent parts of the assignment.
2. (5) plot the change in log body mass versus ancestral log body mass. de-

scribe the plot brie   y.

2. (10) linear model

1. (2) linearly regress the change in log body mass on the ancestral log body

mass. report the coe   cients to reasonable precision.

2. (3) create a new    gure which is the scatter-plot from problem 12, plus your

   tted regression line.

3. (5) based on the estimates 21 and the plot from 22, does this model sup-
port or undermine the idea that new species tend to be larger than their
ancestors? explain.

3. (15) piecewise linear model

a.18 how the north american paleofauna got a crook in its regression line 653

1. (5) the piece-wise linear model predicts the following mean response as a

function of the input x:

  y(x) =

(cid:26) a + bx if x     d

if x     d

c

assuming that this is continuous at d, solve for a in terms of b, c and d.
explain why, in this application, it is reasonable to assume continuity.

2. (10) write a function in r, called29 deac, that takes in a vector of numbers
x, and three parameters b, c, and d, and returns the prediction of the model
at each value of x.
check that your deac function is working properly by seeing that when
b =    1, c = 0.05 and d = 2, giving x=c(1,1.5,3) outputs
[1] 1.05 0.55 0.05
plot deac, with those parameters, as x goes over the range (0, 4). does it
look right?
hints: ifelse for writing deac, curve for plotting.

4. (15) because deac varies nonlinearly with parameter d, we cannot estimate it
by id75. however, we can still estimate the parameters by least
squares. to do this, we need to write a function, make a starting guess about
the parameters, and use the built-in optimization function optim (see recipe
13.2 in the r cookbook).30. the following function    ts the model to a data
set by numerically minimizing the sum of squared errors:

my.start <- c(b=-1,c=0.2,d=10)
fit.a.deac <- function(data,start=my.start) {

sse <- function(par) {

preds <- deac(data$ln_old_mass,par[1],par[2],par[3])
sum((data$delta_ln_mass - preds)^2)

}
fit <- optim(par=start,fn=sse,method="nelder-mead")
coefficients <- fit$par
fitted <- deac(data$ln_old_mass,coefficients[1],coefficients[2],

coefficients[3])

residuals <- data$delta_ln_mass - fitted
mse <- mean(residuals^2)
return(list(coefficients=coefficients,fitted=fitted,residuals=residuals,

mse=mse,data=data))

}

(see online for the commented version; you   ll want to source that, rather than
typing this in and adding original errors.)

29 from the initials of the scientists who proposed this model; they didn   t give it a name.
30 r has a built-in function, nls, for such    nonlinear least-squares    estimation, working more like lm.
unfortunately, nls can be    aky when the model doesn   t have continuous derivatives, which is the
case here. besides, writing your own code builds character.

654

data-analysis problem sets

1. (7) explain what the inner function, sse, does.
2. (8) what sort of output does fit.a.deac give     a vector, a list, an array,
what? what do the various components of the output represent, in terms
of the statistical problem?

5. (15) starting positions the code given above looks for a vector of initial pa-
rameters called my.start, if no other starting point is supplied. the line before
the function makes up some values for my.start; they are bad ones. we will
see in a later problem set that a reasonable guess for d is about 5.
1. (5) use this more-reasonable value of d to get a rough guess for c by taking
the average change in log mass over all animals whose ancestral log mass
exceeds d. explain why this is a reasonable way to guess at c.

2. (5) get a rough guess for b by linearly regressing the change in log mass on
ancestral log mass for animals where the ancestral log mass is less than d.
explain why this is a reasonable way to guess at b.

3. (5) re-de   ne my.start to contain your improved guesses for b, c and d.
run fit.a.deac to get a    tted model, which you should call nampd.deac.
plot the    tted values as a function of log ancestral mass on a scatter-plot
of change in log mass versus log ancestral mass.

6. (20) id64 will continue until morale improves. use resampling of
residuals, not cases, in both parts. note: you can use the same resampled
data-frames for both parts of this problem, but it needs more clever program-
ming. 1000 bootstrap replicates takes 1   2 minutes on my computer.
1. (10) find bootstrap standard errors, and 95% con   dence intervals, for the

parameters b, c and d. report all these quantities.

2. (10) find 95% bootstrap con   dence bands for the    tted curve, and add

them to your plot from problem 53.

7. (15) linear vs. piecewise linear one way to compare two models is to see
which one can predict the other   s parameter values. we will compare the
simple linear model from problem 21 with the piecewise linear model deac
model from problem 53.
1. (5) simulate the    tted deac model, using resampling of residuals, and esti-
mate the linear model on the simulation. what coe   cients do you estimate?
are they compatible with the ones you estimated from the data?

2. (5) simulate the    tted linear model, using resampling of residuals, and
estimate the deac model on the simulation. what coe   cients do you get?
are they compatible with the ones you estimated from the data?

3. (5) use    ve-fold cross-validation to compare the linear model from prob-
lem to the piecewise-linear deac model. which one predicts mass changes
better?

a.19 how the hyracotherium got its mass

agenda: using nonparametric smoothing to check parametric models; more practice with simple
simulations and function-writing.

a.19 how the hyracotherium got its mass

655
we continue to work with the fossil data set from   a.18. as mentioned there,
some paleontologists have suggested that the right curve relating change in log
mass to ancestral log mass should be piece-wise linear and homoskedastic: a
downward-sloping line for small ancestral log masses,    at for larger ancestral
masses, and constant conditional variance:

(cid:26) a + bx +  

c +  

if x     d
if x     d

y =
e[ |x] = 0
var[ |x] =   2

in the last problem set, you    t that model; in this one, you will see whether the
data support non-linear corrections.

you will    rst need to load the data from the other problem set, and add the

column of change in log mass to the data frame.

the mgcv package is recommended for the additive model in problem 5. earlier
problems call for spline smoothing, and can be done with either the smooth.spline
function or with the gam function.

1. (10) plotting the parametric model

1. (5) make a scatter-plot showing the change in log mass as a function of the

log ancestral mass.

2. (5) add the estimated piecewise linear model from homework 4. you may
refer to the solutions for code and parameter estimates, but must explain,
in your own words, any code you borrow from there.

2. (25) residual inspections

1. (5) calculate the residuals of the estimated piecewise linear model and
plot them against the log ancestral mass. describe any patterns to the plot
in words; you should address whether the model systematically over- or
under- predicts in certain ranges of ancestral mass, but there may be other
important features.

2. (5) the column first_appear_mya lists how many millions of years ago each
species    rst appeared. plot the residuals against this variable; describe any
patterns.

3. (5) plot the squared residuals against the log ancestral mass. add a smooth-
ing spline. explain whether the scatter-plot and the spline show evidence
of heteroskedasticity.

4. (5) plot the squared residuals against date of    rst appearance and add
a smoothing spline. explain whether the scatter-plot and the spline show
evidence of heteroskedasticity.

5. (5) plot the histogram of the residuals (not the squared residuals). are they

gaussian? should they be, under the model?

3. (10) a nonparametric alternative

656

data-analysis problem sets

1. (7) fit a spline regression of the change in log mass against log ancestral
mass. plot this spline on the same graph as the data and the estimated
piece-wise linear model. compare, in words, the shape of the spline to that
of the parametric model.

2. (3) find the in-sample root-mean-square error of both the parametric model

and the smoothing spline. which    ts better?

4. (20) testing parametric forms

1. (3) write a function to    t the smoothing spline to a data set. check that

it works by making sure it gives the right answer on the original data.

2. (2) write a function to calculate the mse of a    tted smoothing spline.
check that it works by making sure it gives the right answer on the original
data.

3. (5) write a function to take in a data set and return the di   erence in mses
between the parametric model and the smoothing spline. check that it
works by making sure it gives the right answer on the original data.

4. (5) write a function to simulate from the estimated piecewise-linear model
by resampling the residuals. you can borrow from the solutions to home-
work 4, but must explain, in your own words, how that code works. how
can you check that the simulation works?

5. (5) combine your functions to draw 1000 samples from the distribution of
this test statistic, under the null hypothesis that the parametric model is
right. what is the p-value of this test of the null hypothesis?

5. (25) additional variables the piecewise linear model implicitly assumes that
the relationship between ancestral mass and change in mass is the same at all
times. an alternative is that this relationship has itself evolved.

1. (5) estimate an additive model which regresses the change in log mass
against the log ancestral mass and the date of    rst appearance. plot the two
partial response functions, and describe, in words, the shape of the curves.
compare the shape of the partial response function for log ancestral mass
to the spline curve from problem 31.

2. (4) does the estimated additive model support or undermine the idea that
the relationship between ancestral mass and descendant mass is invariant
over time? explain.

3. (1) what is the in-sample root-mean-square error of the additive model?
4. (10) explain what you would have to change from your code in problem
4 to test the piecewise-linear model against the additive model, and what
pieces of code could stay the same.

5. (5) write the new code called for by problem 54 and run the test. what is

the p-value?

6. (10) is the piecewise-linear, homoskedastic parametric model an acceptable
representation of the data? justify your answer by referring to your work
above.

a.20 how the recent mammals got their size distribution

657

a.20 how the recent mammals got their size distribution

problem sets a.18 and a.19 used regression to study how the typical mass of
(mammalian) species changes over evolution: on average new species are heavier
than their ancestors, especially if the ancestor was very small, but with a wide
variation. if we combine this with the facts that new species branch o    from old
ones, and that sometimes species go extinct without leaving descendants, we get a
model for how the distribution of body masses changes over time. it   s not feasible
to say much about this model mathematically, but we can simulate it, and check
the simulated distribution against the real distribution of body masses today.

the objects in this model are species, each described by its typical mass. (we
assume that this does not change over the lifespan of the species.) each species
can produce new species, who mass is related to that of its ancestor according
to our previously-learned regression model, or go extinct. as time goes on, the
distribution of body masses will    uctuate randomly, but should do so around a
steady, characteristic distribution.

all of this
is
shame-
less ripped
o   
from
http:
//arxiv.
org/abs/
0901.0251
but aaron
said it was
ok

more speci   cally, each species i has a mass xi, which is required to be between
xmin, the smallest possible mass for a mammal, and xmax, the largest possible
mass. at each point in time, one current species a is uniformly selected to evolve
into exactly two new species. each descendant has a mass xd which depends on
the mass of its ancestor, xa, according to the regression model, plus independent
noise:

(cid:26) a + b log xa if log xa     d

log xd = log xa + z +

(a.6)
where z     n (0,   2). continuity means that a = c     bd; we also need to impose
the constraints that xmin     xd     xmax.

if log xa     d

c

species become extinct with a id203 that depends on their body mass,

pe(x) =   x  

(a.7)

unless otherwise speci   ed, you should use   2 = 0.63; xmin = 1.8 grams and
xmax = 1015 grams;    = 0.025;    = 1/5000; and the values of b, c and d from the
solutions to homework 4.

1. (10) write a function, rdeac.1, which takes as inputs a single ancestral mass
xa (not log xa), the parametersb, c, d and   2, and the limits xmin and xmax.
it should generate a candidate value for xd (not log xd) from eq. a.6 and
return it if it is between the limits, otherwise it should discard the candidate
value and try again.

1. (2) set xa to 40 grams and check, by simulating many times, that the
output is always between xmin and xmax, even when those values are brought
close to 40 grams.

2. (8) simulate a single xd value for 100 values of xa evenly spaced between
1 and 100 grams. treat this as real data and re-estimate the parameters
b, c and d according to the methods of homework 4; are they reasonably
close to those in the simulation?

658

data-analysis problem sets

2. (10) write a function, rdeac, which takes the same inputs as rdeac.1 plus
an integer n, and returns a vector containing n independent draws from this
distribution. we will test this with n = 2, but your code must be more general
for full credit.

1. (4) check, by simulating, that the    rst component of the returned vector

has the same marginal distribution as the output of rdeac.1.

2. (4) check that the second component of the returned vector has the same

marginal distribution as the    rst component.

3. (2) check that the two components are uncorrelated.

3. (10) write a function, speciate, which takes the same arguments as rdeac.1,
except that xa is replaced by a vector of ancestral masses. the function should
select one entry from the vector to be xa, and generate two independent values
of xd from it. one of these should replace the entry for xa, and the other
should be added to the end of the vector.

1. (2) check, by simulating, the output always has one more entry than the
2. (8) if the input has length n, check that n     1 of the entries in the output

input vector of masses, no matter how long the input is.

match the input.

4. (15) write a function, extinct.probs, which takes as inputs a vector of species
masses, an exponent   , and a baseline-rate   , and returns the extinction prob-
ability for each species, according to eq. a.7.

1. (1) check that if the input masses are 2 grams and 2500 grams, with the
default parameters the output probabilities     2.0    10   4 and 2.4    10   4
respectively.

2. (2) check that if    = 0, then the output probabilities are always   , no

matter what the masses are.

3. (2) check that if there input masses are all equal, then the output proba-

bilities are all the same, no matter what    and    are.

4. (10) write a function, extinction, which takes a vector of species masses,
   and   , and returns a possibly-shorter vector which removes the masses of
species which have been selected for extinction. hint: what does rbinom(n,size=1,prob=p)
do when p is a vector of length n?

5. (15) evolve!

1. (5) write a function, evolve.1, which takes as inputs a vector of species
masses, b, c, d,   2, xmin, xmax,    and   , and    rst does one speciation step,
then one round of extinction, and returns the resulting vector of species
masses.

2. (5) write a function, evolve, which takes the same inputs at evolve.1,

plus an integer t, and iterates evolve.1 t times.

3. (5) how do you know that your functions are working properly?

6. (15) re-running history

a.21 red brain, blue brain

659

1. (5) run evolve starting from a single species with a mass of 40 grams for
t = 2    105 steps. save the output vector of species masses as y1. plot the
density of y1.

2. (5) repeat the last step to get a di   erent vector y2. does it have the same

distribution as y1? how can you tell?

3. (5) change the initial mass to 1000 grams and get a vector of    nal masses

y3. how does its distribution di   er from that of y1?

7. (25) the data    le mom data full.txt gives the masses of a large (and represen-
tative) sample of currently-living species of mammals. the column mass gives
the mass in grams; the columns species, genus, family, order and code are
identi   ers for the particular species, which do not matter to us. finally, the
column land is 1 for species which live on land and 0 for those which live in
the water.

1. (5) load the data and plot the density of masses for land species.
2. (10) describe, in words, how the distribution of current species masses

compares to that produced by the simulation model in y1.

3. (10) use the relative distribution method from chapter 15 to compare the
actual distribution to the distribution of y1. describe the results and what
they say about how the data di   er from the model.

a.21 red brain, blue brain

agenda: practice with density estimation, conditional densities, and classi   cation models.

timing: problems 1   ?? and 6 involve    tting models to data, plotting, and interpretation, but
no coding. problem 5 requires doing all that and some id64, for which you will need
to write a little code (along lines you have done before). problem 7 requires    tting a model and
making some plots from it, and you will (probably) need to write a little code, along the lines
of examples, to do so. problem 8 requires comparing models, and you will need to either write
some new code, or tweak some example code, to do 82. the solutions to all problems take about
5 minutes to knit without a cache (and about two seconds with a cache     cache everything!).

the data set n90 pol.csv contains information on 90 university students who
participated in a psychological experiment designed to look for relationships be-
tween the size of di   erent regions of the brain and political views. the variables
amygdala and acc indicate the volume of two particular brain regions known
to be involved in emotions and decision-making, the amygdala and the anterior
cingulate cortex; more exactly, these are residuals from the predicted volume,
after adjusting for height, sex, and similar anatomical variables. the variable
orientation gives the subjects    locations on a    ve-point scale from 1 (very con-
servative) to 5 (very liberal). orientation is an ordinal but not a metric variable,
so scores of 1 and 2 are not necessarily as far apart as scores of 2 and 3.

1. marginal density of brain region volumes

1. (5) using npudens, estimate the id203 density for the volume of the

amygdala. plot it and report the bandwidth.

2. (5) repeat this for the volume of the acc.

kanai
et
(2011)

al.

660

data-analysis problem sets

2. joint density of brain regions

1. (5) using npudens, estimate a joint id203 density for the volumes of
the amygdala and the acc. what are the bandwidths? are they the same
as the bandwidths you got in problem 1? should they be?

2. (5) plot the joint density. does it suggest the two volumes are statistically
independent? should they be? you may use three dimensions, color, con-
tours, etc., for your plot, but you will be graded, in part, on how easy to
read it is.

3. predicting brain sizes from political views

1. (10) using npcdens,    nd the conditional density of the volume of the amyg-
dala as a function of political orientation. (make sure that you are treating
orientation as an ordinal variable.) report the bandwidths. is the band-
width for the amygdala the same as either of the previous two bandwidths
you have found for it? should it be? plot the distribution, and comment on
whether it suggests any relationship between the size of this brain region
and political orientation.

2. (5) repeat this for the conditional density of the acc as a function of

orientation.

4. creating a binary response variable

1. (1) create a vector, conservative, which is 1 when the subject has orientation

    2, and 0 otherwise.

2. (2) explain why the cut-o    was put at an orientation score of 2 (as

opposed to some other cut-o   ).

3. (1) check that your conservative vector has the proper values, without

manually examining all 90 entries.

4. (1) add conservative to your data frame. (creating a new data frame

with a new name will only get you partial credit.)

5. id28

1. (5) fit a id28 of conservative (not orientation) on amygdala

and acc. report the coe   cients to no more than three signi   cant digits.
explain what the coe   cients mean.

2. (5) using case resampling, give bootstrap standard errors and 95% con   -
dence intervals for the coe   cients. was the restriction to three signi   cant
digits reasonable?

6. (10) generalized additive model fit a generalized additive model for conservative

on amygdala and acc. (be sure to smooth both the input variables.) make sure
you are using a logistic link function. report the intercep. plot the partial re-
sponse functions, and explain what they mean (be careful!).

7. kernel id155 estimation

1. (5) using npcdens,    nd the id155 of conservative given
amygdala and acc. make sure npcdens treats conservative as a categor-
ical variable and not a continuous one. report the bandwidths.

a.22 brought to you by the letters d, a and g

661

2. (5) plot the estimated id155 that conservative is 1,
with acc set to its median value and amygdala running over the range
[   0.07, 0.09]. (the plotting range for amygdala exceeds the range of values
found in the data.) hint: your code will need to provide values for acc, for
amygdala and for conservative (why?).

3. (5) plot the estimated id155 that conservative is 1,
with amygdala set to its median value and acc running over the range
[   0.04, 0.06]. (this plotting range also requires extrapolating outside the
data.)

8. classi   cation the models from problems ??   7 predict probabilities for conservative.

if we have to make a de   nite prediction of whether someone is conservative or
not, we should predict 1 if the id203 is     0.5 and 0 otherwise.
1. (7) find such predictions for each subject, under each of the three models.
what fraction of subjects are mis-classi   ed? what fraction would be mis-
classi   ed by    predicting    that none of them are conservative?

2. (8) re-calculate the classi   cation error rates using leave-one-out cross-

validation for each model.

a.22 brought to you by the letters d, a and g

agenda: identifying and estimating causal e   ects; the importance of selecting appropriate con-
trols; estimating e   ects in non-linear models.

timing: problems 1 and 2 are straightforward data manipulation; problem 3 needs you to
   t a linearly model and bootstrap some standard errors; problems 4 and 5 need you to    t
nonparametric models, extract predictions from them, and bootstrap some standard errors;
problem 6 needs you to take the ratio of two covariances, and bootstrap some standard errors.
despite all the id64 and using kernel regressions, the solutions take less than two
minutes to knit (without a cache). problems 2   6 all require you to think about some graphical
models. problem 7 requires you to do some math.

the    le sesame.csv contains data on an experiment which sought to learn
whether regularly watching sesame street caused an increase in cognitive skills,
at least on average. the experiment consisted of randomly selecting some children,
the treated, and encouraging them to watch the show, while others received no
such encouragement. the children were tested before and after the experimental
period on a range of cognitive skills. (table a.2 lists the variables.)

1. before and after (5) for each of the skills variables,    nd the di   erence between
pre-test and post-test scores, and add the corresponding column to the data
frame. name these columns deltabody, deltalet, etc. describe and run a
check that the values in these columns are at least approximately right (with-
out examining them all).

2. naive comparison

1. (2) find the mean deltalet scores for children who were regular watchers,
and for children who were not regular watchers. provide standard errors in
these means as well, and the standard error for the di   erence in means.

662

data-analysis problem sets

2. (3) what must be assumed for the di   erence between these means to be a
sound estimate of the average causal e   ect of switching from not watching
to regularly watching sesame street? is that plausible? suggest a way the
assumption could be tested.

3.    holding all else constant   

1. (5) linearly regress the change in reading scores on regular watching, and
all other variables except id, viewcat, and the post-tests.report the coef-
   cients and bootstrap standard errors to reasonable precision. (be careful
of categorical variables.)

2. (3) explain why id, viewcat, and the post variables had to be left out of

the regression. (the reasons need not all be the same.)

3. (2) what would someone who had only taken id75 report as
the average e   ect of making a child become a regular watcher of sesame
street?

4. (5) what would we have to assume for this to be a valid estimate of the

average causal e   ect? is that plausible?

4. consider the graphical model in figure a.1.

1. (10) find a set of variables which satis   es the back-door criterion for esti-

mating the e   ect of regular watching on deltalet.

2. (5) do a nonparametric regression of deltalet on regular and the vari-
ables you selected in 41. (you can use any nonparametric method you like;
you may need to be careful about which variables are categorical.) find the
corresponding estimate of the average e   ect of causing a child to become a
regular watcher. give a bootstrap standard error for this average treatment
e   ect.

5. consider the graphical model in figure a.2.

1. (5) there is at least one set of variables which meets the back-door criterion
in figure a.2 which did not meet it in figure a.1. find such a set, and
explain why it meets the criterion in the new graph, but did not meet it in
the old one.

2. (5) explain whether or not the set of control variables you found in 41 still

works in the new graph.

3. (5) do a nonparametric regression of deltalet on regular and the vari-
ables you selected in 51. find the corresponding estimate of the average
e   ect of causing a child to become a regular watcher, and a bootstrap stan-
dard error for this average treatment e   ect.

4. (5) find a pair of variables which are conditionally (or marginally!) inde-
pendent in figure a.1 but are not in figure a.2, and vice versa. explain
why. note: both the conditioned and conditioning variables must be ob-
served; the point is to    nd something which could be checked with the
data.

6. instrumental encouragement some children were randomly selected for encour-

agement to watch sesame street. this is encoded in the variable encour.

a.23 teacher, leave those kids alone! (they   re the control group)

663

u

setting

encour

site

preform

prelet

prebody

regular

prerelat

deltaform

deltalet

deltabody

deltarelat

figure a.1 first dag.

1. (3) explain why encour is a valid instrument for the e   ect of regular watch-
ing on deltalet in figure a.1. do you need to control for anything else?
2. (2) explain why encour is a valid instrument in figure a.2. do you need

to control for anything?

3. (5) describe a dag in which encour would not be a valid instrument, even

though it is randomized by the experimenters.

4. (5) estimate the average e   ect on deltalet of causing a child to become a
regular watcher using encour and the wald estimator (see notes). provide
a standard error using id64.

7. (5) do exercise 22.2.

## [1] 80

extra credit (5) test whether either of the two conditional independence

relations from 54 hold in the data.

a.23 teacher, leave those kids alone! (they   re the control group)
agenda: applying causal-id136 ideas in an experimental setting. practice in thinking through
what variables should and should not be controlled for.

timing: none of the problems here should require very elaborate coding or time-consuming

computations.

664

data-analysis problem sets

u

setting

encour

site

preform

prerelat

preclasf

regular

prenumb

deltaform

deltarelat

deltaclasf

deltanumb

figure a.2 second dag.

the tennessee star project was a randomized experiment which sought to
determine whether children learn more in classrooms with fewer students. stu-
dents within participating schools were randomly assigned to small (   18 student)
classrooms, to ordinary-sized classrooms, and to ordinary classrooms where the
teacher had an aide. the study began in kindergarten and continued through
third grade. students initially assigned to the small-class condition for the most
part stayed in it (there were a few unavoidable exceptions for administrative rea-
sons); students assigned to the two large-class conditions were re-randomized in
the second year of the study, and thereafter changed only minimally. new stu-
dents entering the schools in the study were randomized into the three conditions.
teachers were also randomized as to which kind of classroom they got. learning
was assessed (in the initial phase of the project) through annual standardized
tests of reading and math.

a standard version of the data set is available as star in the aer package, which
you may need to install. see help(star) for the de   nitions of the variables named
below.

general: whenever you are asked to give standard errors, you should either
bootstrap or provide an explanation of why, in this particular situation, r   s de-
fault calculations of standard errors should be reliable. unless explicitly called
for, do not report r   s p-values, or any signi   cance stars.

1. causality? reverse causality?

1. (5) linearly reqgress readk and mathk on stark. report the coe   cients and

a.23 teacher, leave those kids alone! (they   re the control group)

665

id
site

sex
age
setting
viewcat

regular
encour
peabody

prelet, postlet
prebody, postbody
preform, postform
prenumb, postnumb
prerelat, postrelat
preclasf, postclasf

subject id number
categorical; social background
1: disadvantaged inner-city children, 3   5 yr old
2: advantaged suburban children, 4 yr old
3: advantaged rural children, various ages
4: disadvantaged rural children
5: disadvantaged spanish-speaking children
male=1, female=2
in months
categorical; whether show was watched at home (1) or school (2)
categorical; frequency of viewing sesame street
1: watched < 1/wk
2: watched 1        2/wk
3: watched 3        5/wk
4: watched > 5/wk
0: watched < 1/wk, 1: watched     1/wk
encouraged to watch = 1, not encouraged=0
mental age, according to the peabody picture vocabulary test
(to measure vocabulary knowledge)
pre-experiment and post-experiment scores on knowledge of letters
pre-test and post-test on body parts
pre-test and post-test on geometric forms
tests on numbers
tests on relational terms
pre-test and post-test on classi   cation skills
(   one of these things is not like the others   )
(   one of these things just doesn   t belong   )

table a.2 variables in the sesame data    le. the pre- and post- experiment test scores are
integers, but can be treated as continuous.

standard errors. explain why a non-parametric regression would be redun-
dant here.

2. (5) linearly regress read3 and math3 on stark. report the coe   cients and

their standard errors as above.

3. (5) explain how a randomized treatment received in kindergarten can pre-

dict test scores three years later.

4. (5) linearly regress readk and mathk on star3. report the coe   cients and

their standard errors as above.

5. (5) explain how a treatment received in the third grade can predict test

scores in kindergarten, three years earlier.

6. (5) to estimate the causal e   ect of the stark on readk and mathk, should

we control for star3? (explain.)

7. (5) to estimate the causal e   ect of the star3 on read3 and math3, should

we control for stark? (again, explain.)

2. (15) for each year from kindergarten through third grade, provide an estimate
of the expected reading and math scores when students are assigned to a
regular classroom, a small classroom, and a regular classroom with a teacher   s
aide. include an estimated standard error for each of these. you may present

666

data-analysis problem sets

your results either as a table or graphically; make sure it   s easy to read and
compare across conditions.

explain how you obtained your estimates, and why that procedure is, for
this data, a valid way of estimating the desired causal e   ect. if you have to
control or adjust for any covariates to get the causal e   ects, explain which
ones you used and why.

3. (15) heterogeneity of e   ects there is considerable interest in knowing whether

the e   ects of smaller classes are di   erent for di   erent groups of students.
1. (10) report estimates of the e   ect of the three classroom sizes on kinder-
garten reading and math scores, for all six ethnic sub-groups in the data.
include standard errors.

2. (5) explain why, to get such estimates from id75, the right mod-

els would be of the form lm(readk~stark*ethnicity), and why lm(readk~stark+ethnicity)
would be uninformative.

4. (25) observational id136 in an experimental study students whose families
are su   ciently poor qualify for free lunches at school. this is recorded in the
variables lunchk through lunch3. we want to know whether being above or
below this threshold level of poverty has a causal e   ect on student   s scores.
1. (5) report the mean scores for reading and for math in each grade for
students who do and do not qualify for free lunches (in that grade). include
standard errors.

2. (5) if we want to estimate the e   ect of lunchk on kindergarten reading and

math scores, does it make sense to control for stark? explain.

3. (10) consider the following variables: gender, ethnicity, schoolk, experiencek,

tethnicityk, systemk, schoolidk, lunch1. when estimating the e   ect of
lunchk on kindergarten test scores, which of these should be controlled for,
which of them should not be controlled for, and which of them do you not
have enough information to say? if you answer    not enough information   
for any variables, what more would you have to know? (be more speci   c
than    the complete causal graph   .)

4. (5) if we want to estimate the e   ect of lunchk on    rst-grade reading and
math scores, under what assumptions should we control for readk and
mathk? under what assumptions should we not control for them?

## [1] 130

a.24 estimating with dags

this homework will illustrate some of the advantages of using a known dag
structure. you will need to read the lectures on id114 carefully in
order to do it.

figure a.3 is an elaboration of the graph used in lectures. all problems refer

to it, unless otherwise speci   ed.

a.24 estimating with dags

667

figure a.3 graphical model for use in all problems, except part of the
last. signs on arrows indicate the sign of the associations (not necessarily
linear) between parents and children.

the    le fake-smoke.csv contains some (synthetic) data, for use in problem 5.

1. parents and children (10 points)

occupationalprestigeamount oid122oking-asbestosexposure-access todental care+amount oftar in lungs+yellowingof teeth+cellulardamage+cancer++-668

data-analysis problem sets

1. (5 points) for each variable in the model, list its parents; or, if it has no

parents, say so.

2. (5 points) for each variable in the model, list its children. (some variables

have no children.)

2. joint distributions and factorization (10 points) using the graph, list the
smallest collection of marginal and conditional distributions which must be
estimated in order to get the joint distribution of all variables.

3. associations (20 points) should there be a positive association, a negative
association, or no association between the following variables? explain with
reference to the graph. (2 points each)

1. yellowing of teeth and cancer?
2. yellowing of teeth and cancer, controlling for smoking?
3. yellowing of teeth and cancer, controlling for occupational prestige?
4. yellowing of teeth and cancer, controlling for smoking and exposure to

asbestos?

5. smoking and cancer, controlling for the amount of tar in the lungs?
6. asbestos and cancer, controlling for cellular damage?
7. smoking and cancer, controlling for asbestos?
8. smoking and asbestos, controlling for cellular damage?
9. tar in lungs and cancer, controlling for asbestos, smoking, and yellowing

of teeth?

10. smoking and cancer, controlling for asbestos and occupational prestige?

4. using conditional independence to specify regressions (40 points)

1. (10 points) we wish to know the conditional risk of cancer given smoking.
what other variables should be controlled for? which other variables do
not need to be controlled for?

2. (10 points) using the fake-smoke.csv data from the class website,    t a
id28 model for the risk of cancer given the level of smoking,
controlling for any appropriate covariates.

3. (10 points) using the same data set,    t another id28 for the
risk of cancer using all the covariates. what does this say about the rela-
tionship between smoking and cancer? why is this di   erent than what is
implied by the model in 4b?

4. (5 points) a medical insurance company needs to predict the risk of cancer
among customers in order to set rates. should it use the model from 4b
or the one from 4c? why? (assume, for the sake of the problem, that the
training data and the insurance customers are both representative samples
of the general population.)

5. (5 points) a doctor wants to advise their patients about what actions to
take to reduce their risk of cancer. should they use the model from 4b or
4c? why?

5. (20 points) consider the alternative graph in figure a.4.

a.24 estimating with dags

669

figure a.4 an alternative dag for the same variables.

1. (10 points) repeat problem 3 with the new graph. clearly indicate in your

response which associations di   er for the two dags.

2. (10 points) suggest an experiment, or an observational analysis, which could

let us check which structure was right; explain, in terms of the graphs.

6. (10 points) extra credit: which dag did the example data come from?

how can you tell?

occupationalprestigeamount oid122oking-asbestosexposure-access todental care+amount oftar in lungs+yellowingof teeth+cellulardamage+cancer+-670

data-analysis problem sets

figure a.5 dag for problem 2.

a.25 use and abuse of conditioning

1. (30 points) refer to    gure [[1]] in problem set a.24.

1. (5 points) using the back door criterion, describe a way to estimate the

causal e   ect of smoking on cancer.

2. (5 points) using the front door criterion, describe a di   erent way to estimate

the causal e   ect of smoking on cancer.

3. (5 points) is there a way to use instrumental variables to estimate the causal

4. (5 points) using your back-door identi   cation strategy and the data    le

e   ect of smoking on cancer in this model? explain.
from last time, estimate pr (cancer = 1|do(smoking = 1.5)).

5. (5 points) repeat this using your front-door identi   cation strategy.
6. (5 points) do your two estimates of the casual e   ect match? explain.

2. (25 points) take the model in figure a.5. suppose that x     n (0, 1), y =
  x +   and z =   1x +   2y +   , where   and    are mean-zero gaussian noise
with common variance   2. set this up in r and regress y twice, once on x
alone and once on x and z. can you    nd any values of the parameters where
the coe   cient of x in the second regression is even approximately equal to   ?
(it   s possible to solve this problem exactly through id202 instead.)
3. (25 points) take the model in figure a.6 and parameterize it as follows: u    
n (0, 1), x =   1u +  , z =   x +   , y =   z +   2u +   , where  ,   ,    are
independent gaussian noises with mean zero and common variance   2. if you
regress y on z, what coe   cient do you get, on average? if you regress y on
z and x? if you do a back-door adjustment for x? (approach this either
analytically or through simulation, as you like.)

4. (20 points) continuing in the set-up of the previous problem, what coe   cient
do you get for x when you regress y on z and x? now compare this to the
front-door adjustment for the e   ect of x on y .

xyza.26 what makes the union strong?

671

figure a.6 dag for problems 3 and 4.

a.26 what makes the union strong?

finding the factors which control the frequency and severity of strikes by orga-
nized workers is an important problem in economics, sociology and political sci-
ence31. our data set, http://www.stat.cmu.edu/~cshalizi/uada/12/hw/06/
strikes.csv, kindly provided by a distinguished specialist in the    eld, contains
information about the incidence of strikes, and several variables which are plau-
sibly related to that, for 18 developed (oecd) countries during 1951   1985:
    country name
    year
    strike volume, de   ned as    days [of work] lost due to industrial disputes per
    unemployment rate (percentage)
    in   ation rate (consumer prices, percentage)
       parliamentary representation of social democratic and labor parties   . (for the
united states, this is the fraction of congressional seats held by the democratic
party.)
    a measure of the centralization of the leadership in that country   s union move-
    union density, the fraction of salary earners belonging to a union (only available

ment, on a scale of 0 to 132.

1000 wage salary earners   

source:
western
(1996)

from 1960).

note that some variables are missing (na) for some cases.

1. estimate a linear model to predict strike volume in terms of all of the other

variables, except country and year.

31 or it used to be, anyway.
32 this measure really should be a constant for each country over the period, but having a variable
with only 8 levels is trouble for the spline smoother used in problem 3, so a very small amount of
arti   cial noise (  0.005 at most) has been added to each value.

point

[[todo:
fix
assign-
ments]]

xzyu672

data-analysis problem sets

1. report the coe   cients, with 90% (not 95%) con   dence intervals calculated

according to

1. (2) the standard formulas
2. (9) resampling of the residuals
3. (9) resampling of the cases

do not use more digits than you can justify.

2. (10) describe the meaning of the coe   cients qualitatively. (i.e., do not write
   a one unit change in foo produces a change of bar units in strike volume   
over and over.)

3. (5) rank the predictor variables from most to least important, with    im-
portance    measured by the magnitude of the predicted change to strike
volume in response to a 1% relative change of the predictor away from its
mean value.

4. (5) rank the predictor variables from most to least important in terms of

predicted response to a 1 standard deviation change in the variable.

5. (5) do the two rankings agree? should they? which one seems more rea-

sonable for this problem?

2. some theories suggest that english-speaking countries have legal and political
institutions which make strikes operate di   erently than in other industrialized
countries. figure out which countries in the data set are primarily english-
speaking, create an indicator (dummy) variable for whether a case belongs to
one of those countries, and add it to the data set.

1. (5) fit a linear model in which the predictors from problem 1 interact
with the english-using variable. report the new coe   cients (to reasonable
precision)

2. (5) explain how (if at all) this model di   ers qualitatively from the model

in problem 1.

3. (5) use    ve-fold cross-validation to compare this model to the model in

problem 1. which one does better?

3. fit an additive model for strike volume as a smooth function of all the variables

except country and year.

1. (5) plot all the partial response functions. do they agree qualitatively with

the conclusions you drew from the model in problem 1?

2. (5) consider increasing each of the predictor variables by 1% from its mean,
leaving the other variables alone. rank the predictors according to the
magnitude of this model   s predicted change in strike volume. would the
ranking be the same for a 1% decrease? hint: use predict and a data
frame with arti   cial data.

3. (5) consider increasing each of the predictor variables by one standard devi-
ation from its mean, leaving the other variables alone. rank the predictors
according to the magnitude of this model   s predicted change in strike vol-
ume.

a.26 what makes the union strong?

673

4. (5) discuss the contrast (if any) between these rankings, and the corre-

sponding ones for the linear model.

4. (10) use the methods of chapter 10 to test whether the linear model from

problem 1 is well-speci   ed against an additive alternative.

5. continuing past the training data

1. (2) what were the values of unemployment, in   ation, union density, and
left.parliament for the united states in 2009? hint: you can get most
of these from the last the statistical abstract of the united states.

2. (4) assuming the union centralization variable for the us in 2009 was 0,
what strike volume was predicted by (i) the model from problem 1, (ii)
the english-is-di   erent model from problem 2, and (iii) the additive model
from problem 3?

3. (4) the actual strike volume for the united states in 2009 was 0.8. is this
plausible under any of the models? hint: how much do you expect actual
values to di   er from predicted values?

6. 1. (5) use pc() from pcalg to obtain a graph, assuming all relations between
variables are linear. report the causal parents (if any) and children (if any)
of every variable. if the algorithm is unable to orient one or more of the
edges, report this, and in later parts of this problem, consider all the graphs
which result from di   erent possible orientations.
note: see http://bactra.org/weblog/914.html for help with installing
pcalg. the most troublesome component is the rgraphviz package. if
you are unable to get rgraphviz to work, you can still extract the in-
formation from the    tted model returned by pc: if that   s pc.fit, then
pc.fit@graph@edgel is the    edge list    of the graph, listing, for each node,
the nodes it has arrows to. with this information, you can make your own
picture of the dag.

2. (10) linearly model each variable as a function of its parents. report the co-
e   cients (to reasonable precision), the standard deviation of the regression
noise (ditto), and 95% con   dence intervals for all of these, as determined
by id64 the residuals.

3. (10 total) you should    nd that strike volume and union density are not
connected, but that there is at least one directed path linking them    
either density is an ancestor of strike volume, or the other way around.

1. (5) find the expected change in the descendant from a one-standard-

deviation increase in the ancestor above its mean value.

2. (5) linearly regress the descendant on all the other variables, including
the ancestor. according to this regression, what is the expected change
in the descendant, when the ancestor increases one sd above its mean
value and all other variables are at their mean values?

4. (15 total) check the linearity assumption for each variable which has a
parent. (putting in interactions and/or quadratic terms is inadequate and
will result in only partial credit at best.)

674

data-analysis problem sets

1. (5) describe your method, and why it should work.
2. (5) report the p-value for each case, to reasonable precision.
3. (5) what is your over-all judgment about whether it is reasonable to
model each endogenous variable as linearly related to its parents? if you
need more information than just p-values to reach a decision, describe
it.

5. (10) discuss the over-all adequacy of the model, on both statistical grounds
(goodness-of-   t, appropriateness of modeling assumptions, etc.) and sub-
stantive, scienti   c ones (whether it makes sense, given what is known about
the processes involved).

as

[[todo:
brad de-
long
(!)
points out
e-mail
by
that
one
should re-
ally de   ne
returns
here
log ((pt+1 + dt+1)/pt),
where d
is the divi-
dend series
    prices
aren   t
the
only thing
that mat-
ters with
s&p!
the
obtain
a
historical
dividend
series, or a
dividend-
adjusted
price
ries.]]

se-

a.27 an insu   ciently random walk down wall street

in this assignment, you will work with a data set of historical values for the s&
p 500 stock index. you will need to download sphistory.short.csv from the
class website. this data set records the actual prices of the index, say pt on day t,
but in    nance we actually care about the returns, pt
, or about the logarithmic
pt   1
returns,

rt = log

pt
pt   1

since we care more about whether we   re making 1% on our investment than $1
per share. in this assignment,    returns    always means    logarithmic returns   .

problems 2 and 3 are about estimating the    rst percentile of the return dis-
tribution, q(0.01), under various assumptions. the returns will be larger than
this 99% of the time, so q(0.01) gives an idea of how bad the bad performance
will be, which is useful for planning. note that a calendar year contains about
250 trading days, and so should average two or three days when returns are even
worse than q(0.01). problems 4 and 5 are about predicting future returns from
historical returns, and the uncertainty in this. doing all the id64 for
problem 5 may be time-consuming, and should not be left to the last minute.

1. (5) load the data    le, take the last column (containing the daily closing price),
and calculate the logarithmic returns. note that the    le is in reverse chrono-
logical order (newest    rst). when you are done, if everything worked right,
running summary on the returns series should give

min.

mean
-0.094700 -0.006440 0.000467 -0.000064

1st qu.

median

3rd qu.
0.006310

max.
0.110000

hint: help(rev) and recipe 14.8 in the r cookbook.

2. in    nance, it is common to model daily returns as independent gaussian vari-

ables.

1. (5) find the mean and standard deviation of the returns. what is q(0.01)

of the corresponding gaussian distribution? hint: qnorm.

a.27 an insu   ciently random walk down wall street

675

2. (5) write an expression which will generate a series of independent gaussian
values of the same length as the returns, with the mean and standard
deviation you found in 21. check that the mean and standard deviation
of the output is approximately right, and that their histogram looks like a
bell-curve.

3. (10) write a function which takes in a data vector, calculates its mean and
standard deviation, and returns q(0.01) according to the corresponding
gaussian distribution. check that it works by seeing that it matches what
the answer you got in 21 when run on the actual returns.

4. (10) using the code you wrote in 22 and 23,    nd a 95% con   dence interval
for q(0.01) from 21. hint: look at the examples of parametric bootstrap-
ping in chapter 6.

5. (5 points) what is the    rst percentile of the data? is it within the con   dence

interval you found in 24? hint: quantile.

3. 1. (5) use hist to plot the histogram of returns. also plot, on the same
graph, the id203 density function of the gaussian distribution you    t
in problem 21. comment on their di   erences.

2. (5) write a function to resample the returns; it should generate a di   erent
random vector of the sample length as the data every time it is run. check
that running summary on these vectors produces results close to those on
the data. hint: look at the examples of resampling in chapter 6.

3. (5) write a function to calculate q(0.01) from an arbitrary vector, without
assuming a gaussian distribution. check that it works by seeing that its
answer, when run on the real data, matches what you found in 25.

4. (10) using the code you wrote in 32 and 33,    nd a 95% con   dence interval
for q(0.01). compare this to your answer in 24. which is more believ-
able, and why? hint: look at the examples in the notes of non-parametric
id64.

4. (10) using npreg,    t a kernel regression of rt+1, tomorrow   s returns, on rt,
today   s returns. (use the automatic bandwidth selector.) report the selected
bandwidth and the in-sample mean-squared error. make a scatter-plot with
rt on the horizontal axis and rt+1 on the vertical axis, and add the estimated
kernel regression function. comment on the shape of the curve. hints: make
a data frame with rt as one column and rt+1 as another column. also, see
examples in chapter 4 of plotting    tted models from npreg.

5. (25) uncertainty in the kernel regression

1. (5) write a function which resamples (rt, rt+1) pairs from the returns
series, and produces a new data frame of the same size as the original.
check that it works by running summary on it, and seeing that both columns
approximately match the summaries of the data. hint: look at the examples
of resampling cases for regression in the notes.

2. (10) write a function which takes a data frame with appropriately-named
columns, and runs a kernel regression of rt+1 on rt. it should return    tted
values at 30 evenly-spaced values of rt which span its observed range.

676

data-analysis problem sets

3. (10) using your code from 51 and 52, add 95% con   dence bands for the
kernel regression to your plot from problem 4. hint: see the examples of
plotting bootstrapped nonparametric regressions in the notes.

1. (5 points) load the data    le, take the last column (containing the daily closing
price), and calculate the logarithmic returns. note that the    le is in reverse
chronological order (newest    rst). when you are done, if everything worked
right, running summary on the returns series should give

[[todo:
integrate
this version
with
the
one above]]

min.

3rd qu.
-0.094700 -0.006440 0.000467 -0.000064 0.006310

1st qu.

median

mean

max.
0.110000

2. in many applications in    nance, it is common to model daily returns as inde-

pendent gaussian variables.

1. (5 points) use maximum likelihood to estimate the mean and standard

deviation of the best-   tting gaussian, and the q(0.01) it implies.

2. (5 points) write a function which simulates a data set of the same size as
the real data, using the independent gaussian model you    t in (2a), and
returns a list or vector, with components named mean and sd, containing
the parameter values estimated from the simulation output.

3. (5 points) write a function which takes as arguments a list or vector, with
components named mean and sd, and returns the    rst percentile of the
corresponding gaussian distribution. check that it works by verifying that
when run with mean 5 and sd 2, it returns 0.347.

4. (10 points) using the code you wrote in (2b) and (2c),    nd a 95% con   dence
interval for q(0.01) from (2a). hint: look at the examples in the notes of
parametric id64.

5. (5 points) what is the    rst percentile of the data? is it within the con   dence

interval you found in (2d)?

3. 1. (5 points) use density(), or any other suitable non-parametric density
estimator, to plot the distribution of returns. also plot, on the same graph,
the gaussian distribution you    t in problem 2. comment on their di   er-
ences.

2. (10 points) write a function to re-sample the returns, and calculate q(0.01)
on each surrogate data set. use this to    nd a 95% con   dence interval for
q(0.01). hint: look at the examples in the notes of non-parametric boot-
strapping.

4. (15 points) in an autoregressive model, the measurement at time t is re-
gressed on the measurement at time t     1, xt =   0 +   1xt   1 +  t. (  25.4 has
much more information.) use lm to    t an autoregressive model to the returns.
also give the reported standard error for(cid:99)  1.
give the estimates of   0,   1 and v [ ], and try to interpret what they mean.

5. hint: look at the examples in the notes of re-sampling regression residuals.

[[todo:
clarify
using
quantile
here.]]

a.28 predicting nine of the last five recessions

677

1. (5 points) write a function which re-samples the residuals of the autore-
gressive model from (4). make sure it returns a vector of values. check
that the mean and standard deviation of its output are close to those of
the residuals.

2. (15 points) write a function which simulates the autoregressive model you
   t in (4), with noise provided by the function you wrote for (5a). the initial
value of x should match the initial value in the data, and it should return
a vector.

3. (5 points) write a function which takes a time series,    ts an autoregressive
model, and returns the estimate of   1. check that it works by seeing that
when it   s give the data, the output matches what you found in (4).

4. (10 points) using the function you wrote in (5c), and the simulator you

wrote in (5b),    nd the bootstrap standard error for(cid:99)  1. does it match what

lm reported in (4)?

note: if you cannot solve (5b), you can get full credit for (5d) using the built-in function
arima.sim instead, but make sure that the distribution of innovations or noise comes from
the function you wrote in (5a). if you cannot solve (5a), you can get full credit for (5b) and
(5d) by providing suitable gaussian noise.

a.28 predicting nine of the last five recessions

the data set http://www.stat.cmu.edu/~cshalizi/uada/13/exams/3/macro.
csv on the class website contains    ve standard macroeconomic time series for the
united states, from the beginning of 1948 to the beginning of 2010: total national
income or gdp; value of goods consumed; investment spending; hours worked;
and output per hour worked for all non-   nancial    rms. (some of these series are
in in   ation-adjusted dollars, some of them are in hours, and some of them are
indexes where a particular date has been set as 100 and others are expressed
relative to that.) all variables are measured    quarterly   , i.e., four times a year.
most macroeconomic forecasting models do not concern themselves directly
with these values, but only with the logged    uctuations around their long-run
trends.

for full credit on the modeling questions, you must use models which go beyond
those available in 401, or you must use appropriate methods to show that linear
model are justi   ed here.

it is    rst necessary to remove trends; macroeconomists traditionally do this

with the following function.

hpfilter <- function(y, w=1600){

eye = diag(length(y))
d = diff(eye,d=2)
ybar = solve(eye + w*crossprod(d), y)
yhat = log(y) - log(ybar)
return(list(fluctuation=yhat,trend=ybar))

}

678

data-analysis problem sets

1. (10) create    ve plots, showing each of the variables and its trend (as returned
by hpfilter) as functions of time. use a logged scale for the vertical axis.
report r2, with and without logging, for each of the    ve trends.

2. (10) plot the logged    uctuations around trend (as returned by hpfilter) for
each of the    ve variables. does it make sense to compare these    uctuations
across variables? do the    uctuations look stationary?     after this problem,
references to the variables always mean their logged    uctuations around their
trends.

3. (10) are the variables gaussian? (you can do better than looking at a his-

togram.)

4. (20) for the    rst four variables (gdp, consumption, investment, hours worked),
   t an additive regression of each variable on the values of all four at the pre-
vious time-step. use only data up to, but not including, 2005 (   the training
period   ). report the mean squared error on the training data (to reasonable
precision), and include plots of the partial response functions. describe, in
words, what the partial response functions say about the relations between
these variables.

5. (20 total) using the circular block bootstrap, with blocks of length 24, generate

new time series which are as long as the training data.

1. (4) write a function to calculate the mean squared errors of the    tted
models from problem 4, on a time series. (each of the four variables should
have its own mse.) check that it works by making sure that it gives the
right answer for the training data.

2. (6) report the mean mses, and the standard error of these means, from
enough bootstrap replicates that the standard errors are no more than 10%
of the means.

3. (10) what do you need to assume for the numbers from 52 to be good

estimates of the generalization error of this model?

6. (20 total)    real    (as opposed to    monetary   ) business cycle theories hold that
   uctuations in macroeconomic variables are ultimately caused by exogenous
   real shocks   , especially changes to productivity. the productivity variable
in macro.csv is a measurement of this variable, which, according to these
theories, should be exogenous. the other variables, in such theories, are en-
dogenous.

1. (10) fit an model for each of the four endogenous variables, as an additive
function of the endogenous variables in the previous quarter, and produc-
tivity for the previous four quarters. report the mses and include plots of
the partial response functions. compare the plots to those in problem 4.

2. (4) describe a method which could be used to decide whether including
productivity in this way really improves predictive performance. discuss
the assumptions of the method, and why you think they apply here.

3. (6) implement your method. for which variables does including productiv-

ity actually help? how con   dent are you of this conclusion?

a.29 debt needs time for what it kills to grow in

679

7. (10 total) now consider the period 2005   2010. what are the mean squared

errors, on this data, of

1. (4) predicting according to the additive model from problem 4?
2. (4) predicting according to the additive model from problem 6?
3. (2) predicting the mean of each variable, as estimated from the training

period?

8. (5, extra credit) explain how what hpfilter does is related to spline smooth-

ing.

a.29 debt needs time for what it kills to grow in

an important and controversial question in macroeconomics and political econ-
omy is whether high levels of government debt causes the economy to grow more
slowly or even shrink. there are several plausible-sounding reasons why it might33;
some economists claim that there is a threshold level of debt, perhaps around 90%
of gdp, above which growth rates plummet.

against this, there are other reasons why high levels of debt might not cause
growth to slow, at least not always34. in particular, since    high levels of govern-
ment debt    are de   ned relative to the size of the economy, as a high ratio of debt
to gdp, slow growth itself might cause higher levels of government debt.

this week   s data set contains information on gdp and government debt for a
selection of countries since world war ii. for each country and year, we should
have the gdp (nominal, i.e., not adjusted for in   ation or di   erences in exchange
rates) and the size of government debt (also nominal). unfortunately, one or both
values may be missing for some countries in some years.

1. (10) the data set contains a variable, growth, which is the annual growth rate
in real (in   ation-adjusted) gdp for each country and year. it also contains a
variable, ratio which is the ratio of government debt to gdp. make a scatter-
plot with growth on the vertical axis and ratio on the horizontal. describe
the patterns you see, if any.

2. (15) run a nonparametric regression of growth on ratio, and plot the result-
ing curve. describe and interpret the curve. does it suggest an abrupt slowing
of growth above some threshold level of debt?

33 high levels of government borrowing might    crowd out    investing in the private sector, by using up
available savings and/or raising the interest rates at which businesses can borrow; capitalists might
anticipate that the debt will either be paid o    through high taxes or discharged through in   ation,
and prefer to spend their money on luxuries now, rather than invest and see the investment go away
later; high levels of debt might lead to lower con   dence that the government generally knows what
it   s doing, making investment seem too risky; etc.

34 a depressed economy has unused resources, so government employment needn   t lead to crowding

out; the things government spends money on (roads, schools, hospitals, basic research, honest
markets) increase the value of private investments; governments which can borrow large sums are
receiving a market endorsement of their willingness and ability to pay their debts; etc.

680

data-analysis problem sets

3. (10) since changes in government debt levels might take some time to a   ect
economic growth, we would like to compare growth in year t+1 to ratio in year
t. create a new variable, growth.lead1, which records for each country/year
the next year   s gdp growth, with nas in the right places when it is not
available. describe, in words, how your code works. add growth.lead1 to the
data frame.

hints: make sure that you do not confuse growth rates from di   erent coun-
tries (so that, e.g., the last year for austria gets a growth rate from belgium).
you may    nd recipes 14.7 (and 6.6) from the r cookbook helpful.

4. (10) plot growth.lead1 against ratio, and do a nonparametric regression of
the former on the latter. describe the results, and compare them to those of
problem 2.

5. (15) economic growth rates tend to be rather persistent over time within
countries. estimate an additive model where growth.lead1 is predicted from
growth and ratio. is the partial response to the previous year   s growth nearly
linear? should it be? compare the partial response function for debt to the
curves from problems 2 and 4.

6. (10) create a new variable, growth.lag1, which represents the previous year   s
growth rate (with nas in appropriate places), and add it to the data set. plot
it against ratio and    t a nonparametric regression. does ratio do a better
job of predicting growth or growth.lag1?

7. (15) estimate an additive model in which the current year   s ratio is predicted
by last year   s ratio, last year   s growth, and the current year   s growth. (you
may have to create a new column.) describe the partial response functions,
and whether any predictor variables could be dropped.

8. (15) explain what we would have to assume for the model in problem 5 to
give us an unconfounded estimate of the causal e   ect of government debt on
future economic growth; be as speci   c as possible. (you may want to draw
some dags, and include them in your write-up.) comment on how plausible
those assumptions are, and on what might go wrong if the assumptions fail.

a.30 how tetracycline came to peoria

now-common ideas like    early adopters    and    viral marketing    grew from soci-
ological studies of the di   usion of innovations. one of the most famous of these
studies tracked how a then-new antibiotic, tetracycline, spread among doctors in
four towns in illionis in the 1950s (coleman et al., 1957). in this exam, we will
go back to that data to look at one of the crucial ideas, that of the innovation
(prescribing tetracycline) spreading from person to person.

for this assignment, you will need two data    les, ckm nodes.csv and ckm network.dat.35

the former has information about each individual doctor in the four towns.

35 slightly modi   ed from http://moreno.ss.uci.edu/data.html to    t r conventions, and collapsing

three distinct, directed social relationships into one undirected social network.

[[todo:
better
urls]]

[[todo:
re-work
points and
instruc-
for
tions
this
not
to be an
exam?]]

a.30 how tetracycline came to peoria

681

adoption_date records the month in which the doctor began prescribing tetra-
cycline, counting from november 1953. if the doctor did not begin prescribing
it by month 17, i.e., february 1955, when the study ended, this is recorded as
inf. if it   s not known when or if a doctor adopted tetracycline, their value is na.
(apparently no doctors gave up tetracycline after adopting it.) other columns
record when the doctor attended medical school, whether they attend medical
conferences (and if so, what kind), how many medical journals they read, and
other information about the individual doctors. note that the covariates in this
   le are a mix of ordinal variables, categorical variables, and numerical variables.
the ckm_network.dat    le contains a binary matrix, which records the social
network among the doctors. there is one row and one column for each doctor;
the i, j entry is 1 if doctor number i and doctor number j knew each other, and
0 if they did not.

1. (5) create a plot of the number of doctors who began prescribing tetracycline
each month versus time. (it is ok for the numbers on the horizontal axis to
just be integers rather than formatted dates.) produce another plot of the
total number of doctors prescribing tetracycline in each month. (the curve for
total adoptions should    rst rise rapidly and then level out around month 6.)
2. estimate the id203 that a doctor who had not yet adopted the drug will
begin to do so in a given month t, as a function of the total number of doctors
nt who had adopted before t. (you may assume that these probabilities are the
same for all t.) you may estimate this function however you like, but be sure
to explain how you are estimating these probabilities, and how you know that
method is reliable in this particular case. (this may involve model checking.)

1. (5) report these probabilities as a curve, with n ranging from 0 to 125.
if you do not think you can estimate the whole range, plot as much as
you can, and explain why you cannot go further. for full credit, your plot
must have more than 17 points. also for full credit, your curve should be
accompanied by some measure of its error.

2. (5) averaging over doctors and months, how much does the predicted prob-
ability of adoption change n increases by 1? give a standard error to this
change in predicted probabilities.

hint: you may    nd it useful to create a new data frame which records, for
each month, the number of doctors who adopted tetracycline that month, and
the number who had previously adopted tetracycline.

3. estimate the id203 that a doctor i who had not yet adopted the drug will
begin to do so in month t, as a function of the number cit of doctors linked to
i who had adopted before t. (again, you may assume that these probabilities
are the same for all t.)

1. (8) make a plot of these probabilities, with cit ranging from 0 to 30. if you
do not think you can estimate the whole range, plot as much as you can, and
explain why you cannot go further. for full credit, your plot must include
at least 29 points, and include a measure of uncertainty in your estimates.

682

data-analysis problem sets

does your curve support the idea that the use of tetracycline is transmitted
from one doctor to another through the social network? explain, including
a description of what curves which did not support this idea would look
like, or why the shape of this curve is actually irrelevant to this issue.

2. (7) averaging over doctors and months, how much does the predicted prob-
ability of adoption change when cit increases by one? what is your standard
error for this change in predicted probabilities?

hint: you may    nd it useful to create a data frame recording, for every com-
bination of doctor and month, whether that doctor began prescribing tetracy-
cline that month, the number of their contacts who began prescribing before
that month. such a data frame should have 2125 rows.

4. 1. (1) are your estimates from problem 22 and 32 consistent with one another?

explain.

2. (4) what would you have to assume for either of these to be estimates of the
causal e   ect on adoption by other doctors of making one extra doctor adopt
the drug? be as speci   c as you can, rather than just repeating de   nitions
from the notes. drawing graphs is encouraged.

5. estimate a model which predicts the id203 that a doctor i who had not
yet adopted the drug by month t will begin to do so in month t, as a function
of cit and of the covariates which indicate when i went to medical school,
whether they attended medical-society meetings (and if so what kind), and
how many medical journals they read.
1. (5) plot the estimated id203 of adoption as a function of cit for doc-
tors who read the minimal number of journals, do not attend conferences,
and graduated from medical school (i) in 1919 or earlier, (ii) in the 1920s,
and (iii) in 1940 or after. for full credit, have all three lines on the same plot
(clearly visually distinct from each other), and some measure of uncertainty
for each line.

2. (5) averaging over doctors and months, how much does increasing cit by
one change the id203 of doctor i adopting tetracycline in month t?
include a standard error for this change in predicted probabilities.

3. (5) under what assumptions does this give a valid estimate of the average

causal e   ect of increasing cit by one?

note if you want to display the social network, the r package igraph is designed

for such things.

a.31 formatting instructions and rubric

your main report should be a humanly-readable document of at most 10 single-
spaced pages, including    gures. it should have the following sections:

introduction describing the scienti   c problem and the data set, possibly including relevant
summary statistics or exploratory graphs. (do not include eda just to have
eda.)

a.31 formatting instructions and rubric

683

specific problems answering the questions set above, but avoiding the check-list, itemized format
in favor of continuous text, with a logical succession of sentences and para-
graphs. (writing coherently is more important than following the order of the
questions.)

conclusions summarizing what you have learned from the data and models about whether
the transmission of an innovation from person to person is really a good de-
scription of how these doctors came to use tetracycline.

you may assume that the reader has a general familiarity with the contents of
401, and with the models and methods we have covered so far in the course, but
will need to be reminded of any details. the reader should not be assumed to
have any prior familiarity with the data set.

code

all statistical results must be supported by appropriate code, or they will receive
no credit. (   show your work.   ) code should only appear in the text of the report
when it is the best way of conveying some point. the ideal would be to use r
markdown, or knitr+latex, to embed all computations in a humanly readable
document, and submit both the knitted version and the source36 as a second
best, it is acceptable to submit a pdf document containing all text and    gures,
and a separate .r    le, containing all supporting computations, clearly labeled
via the comments so that it is easy to see which claims or results go with which
pieces of code.

as usual, this describes the ideal.

rubric

words

(5) the text is laid out cleanly, with clear divisions and transitions between
sections and sub-sections. the writing itself is well-organized, free of grammatical
and other mechanical errors, divided into complete sentences logically grouped
into paragraphs and sections, and easy to follow from the presumed level of
knowledge.

(5) all numerical results or summaries are reported to suitable precision, and
with appropriate measures of uncertainty attached when applicable.

numbers

pictures

(5) figures and tables are easy to read, with informative captions, axis labels and
legends, and are placed near the relevant pieces of text.

36 see examples at http://yihui.name/knitr/demos/, and the useful chunk options like echo at

http://yihui.name/knitr/options/; also the examples in the solutions to exam 1.

684

data-analysis problem sets

code

(15) the code is formatted and organized so that it is easy for others to read
and understand. it is indented, commented, and uses meaningful names. it only
includes computations which are actually needed to answer the analytical ques-
tions, and avoids redundancy. code borrowed from the notes, from books, or from
resources found online is explicitly acknowledged and sourced in the comments.
functions or procedures not directly taken from the notes have accompanying
tests which check whether the code does what it is supposed to. all code runs,
and the markdown    le knits (if applicable). the main text of the report is free of
intrusive blocks of code, which are used only when a speci   cally-computational
point is being made, or when code is actually the clearest way of describing a
point.

id136 and uncertainty

(10) the actual estimation of model parameters or estimated functions is tech-
nically correct. all calculations based on estimates are clearly explained, and
also technically correct. all estimates or derived quantities are accompanied with
appropriate measures of uncertainty (such as con   dence intervals or standard
errors).

conclusions

(10) the substantive questions about di   usion of innovations are all answered as
precisely as the data and the model allow. the chain of reasoning from estimation
results about models, or derived quantities, to substantive conclusions is both
clear and convincing. contingent answers (   if x, then y , but if z, then w    ) are
likewise described as warranted by the model and data. if uncertainties in the
data and model mean the answers to some questions must be imprecise, this too
is re   ected in the conclusions.

extra credit

(10) up to ten points may be awarded for reports which are unusually well-
written, where the code is unusually elegant, where the analytical methods are
unusually insightful, or where the analysis goes beyond the required set of ana-
lytical questions. example: simulating the model estimated in problem 5, taking
the set of doctors who have adopted in month 1 for the initial conditions and
continuing for another 16 months, with a detailed and quantitative comparison
of multiple simulation runs to the actual data, and an informative assessment of
what the comparison says about the strengths and weaknesses of the model.

references

abadie, alberto and guido w. imbens (2006).    large sample properties of matching estimators
for average treatment e   ects.    econometrica, 74: 235   267. url http://www.ksg.harvard.
edu/fs/aabadie/smep.pdf. doi:10.1111/j.1468-0262.2006.00655.x. 518

abarbanel, henry d. i. (1996). analysis of observed chaotic data. berlin: springer-verlag.

571

adler, joseph (2009). r in a nutshell . sebastopol, california: o   reilly. 841
al ghazali, abu hamid muhammad ibn muhammad at-tusi (1100/1997). the incoherence
of the philosophers = tahafut al-falasifah: a parallel english-arabic text. provo, utah:
brigham young university press. translated by michael e. marmura. 478

alford, j. r., c. l. funk and j. r. hibbibng (2005).    are political orientations genetically

transmitted?    american political science review , 99: 153   167. 218

amari, shun-ichi and hiroshi nagaoka (1993/2000). methods of information geometry. provi-
dence, rhode island: american mathematical society. translated by daishi harada. as joho
kika no hoho, tokyo: iwanami shoten publishers. 405

anthony, martin and peter l. bartlett (1999). neural network learning: theoretical founda-

tions. cambridge, england: cambridge university press. 78

arceneaux, kevin, alan s. gerber and donald p. green (2010).    a cautionary note on the
use of matching to estimate causal e   ects: an empirical example comparing matching
estimates to an experimental benchmark.    sociological methods and research, 39: 256   282.
doi:10.1177/0049124110378098. 517

arlot, sylvain and alain celisse (2010).    a survey of cross-validation procedures for model
selection.    statistics surveys, 4: 40   79. url http://projecteuclid.org/euclid.ssu/
1268143839. 78

    (2011).    segmentation of the mean of heteroscedastic data via cross-validation.    statistics

and computing, 21: 613   632. url http://arxiv.org/abs/0902.3977. 562

arnold, barry c. (1983). pareto distributions. fairland, maryland: international cooperative

publishing house. 138, 447

arnol   d, v. i. (1973). ordinary di   erential equations. cambridge, massachusetts: mit press.

trans. richard a. silverman from obyknovennye di   erentsial   nye uravneniya. 405

axler, sheldon (1996). id202 done right. berlin: springer-verlag. 721
bai, jushan (2003).    testing parametric conditional distributions of dynamic models.    the

review of economics and statistics, 85: 531   549. doi:10.1162/003465303322369704. 330

barndor   -nielsen, o. e. (1978). information and exponential families in statistical theory.

new york: john wiley and sons. 265

barndor   -nielsen, o. e. and d. r. cox (1995). id136 and asymptotics. london: chapman

and hall. 772

bartholomew, david j. (1987). latent variable models and factor analysis. new york: oxford

university press. 381, 392, 398, 426

bartholomew, david j., ian j. deary and martin lawn (2009).

   a new lease on
life for thomson   s bonds model of intelligence.    psychological review , 116: 567   579.
doi:10.1037/a0016262. 396

685

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

686

references

bartlett, m. s. (1955). an introduction to stochastic processes, with special reference to meth-

ods and applications. cambridge, england: cambridge university press. 571

basharin, gely p., amy n. langville and valeriy a. naumov (2004).    the life and work of
a. a. markov.    id202 and its applications, 386: 3   26. url http://langvillea.
people.cofc.edu/markovreprint.pdf. 456

baum, leonard e., ted petrie, george soules and norman weiss (1970).    a maximization
technique occurring in the statistical analysis of probabilistic functions of markov chains.   
annals of mathematical statistics, 41: 164   171. url https://projecteuclid.org/euclid.
aoms/1177697196. doi:10.1214/aoms/1177697196. 570

beaumont, mark a. (2010).    approximate bayesian computation in evolution and ecology.   
annual review of ecology, evolution, and systematics, 41: 379   406. doi:10.1146/annurev-
ecolsys-102209-144621. 604

becker, howard s. (2017). evidence. chicago: university of chicago press. 800
belkin, mikhail and partha niyogi (2003).    laplacian eigenmaps for id84
and data representation.    neural computation, 15: 1373   1396. url http://www.cse.
ohio-state.edu/~mbelkin/papers/lem_nc_03.pdf. doi:10.1162/089976603321780317. 421,
422

benaglia, tatiana, didier chauveau, david r. hunter and derek s. young (2009).    mixtools:
an r package for analyzing mixture models.    journal of statistical software, 32. url
http://www.jstatsoft.org/v32/i06. 433, 707

benzi, michele (2009).    the early history of matrix iterations: with a focus on the ital-
ian contribution.    presentation at siam conference on applied id202, monterey
bay, california, 26 october 2009. url https://archive.siam.org/meetings/la09/talks/
benzi.pdf. 181

bera, anil k. and aurobindo ghosh (2002).    neyman   s smooth test and its applications in
econometrics.    in handbook of applied econometrics and statistical id136 (aman ullah
and alan t. k. wan and anoop chaturvedi, eds.), pp. 177   230. new york: marcel dekker.
url http://ssrn.com/abstract=272888. 320, 341

berk, richard, lawrence brown, andreas buja, kai zhang and linda zhao (2013).    valid
post-selection id136.    annals of statistics, 41: 802   837. url http://arxiv.org/abs/
1306.1059. doi:10.1214/12-aos1077. 78

berk, richard a. (2004). regression analysis: a constructive critique. thousand oaks, cali-

fornia: sage. 60, 505

    (2008). statistical learning from a regression perspective. new york: springer-verlag. 285
beygelzimer, alina, sham kakade, john langford, sunil arya, david mount and shengqiao
li (2013). fnn: fast nearest neighbor search algorithms and applications. url http:
//cran.r-project.org/package=fnn. r package version 1.1. 30, 707

biecek, przemyslaw and teresa ledwina (2010). ddst: data driven smooth test. url http:

//cran.r-project.org/package=ddst. r package, version 1.02. 326

blackwell, david and m. a. girshick (1954). theory of games and statistical decisions. new

york: wiley. 284

blei, david m. and john d. la   erty (2009).    topic models.    in id111: theory and
applications (a. srivastava and m. sahami, eds.). london: taylor and francis. url http:
//www.cs.princeton.edu/~blei/papers/bleilafferty2009.pdf. 433

blei, david m., andrew y. ng and michael i. jordan (2003).    id44.   
journal of machine learning research, 3: 993   1022. url http://jmlr.csail.mit.edu/
papers/v3/blei03a.html. 433

boas, mary l. (1983). mathematical methods in the physical sciences. new york: wiley, 2nd

edn. 721, 726

bonner, john tyler (1988). the evolution of complexity, by means of natural selection. prince-

ton, new jersey: princeton university press. 620, 652

borsboom, denny (2005). measuring the mind: conceptual issues in contemporary psychomet-

rics. cambridge, england: cambridge university press. 398

references

687

    (2006).    the attack of the psychometricians.    psychometrika, 71: 425   440. url https:
//sites.google.com/site/borsboomdenny/borsboompm2006.pdf. doi:10.1007/s11336-006-
1447-6. 398

boudon, raymond (1998).    social mechanisms without black boxes.    in hedstr  om and swed-

berg (1998), pp. 172   203. 505

bousquet, olivier, st  ephane boucheron and g  abor lugosi (2004).    introduction to statistical
learning theory.    in advanced lectures in machine learning (olivier bousquet and ulrike
von luxburg and gunnar r  atsch, eds.), pp. 169   207. berlin: springer-verlag. url http:
//www.econ.upf.edu/~lugosi/mlss_slt.pdf. 78

boyd, stephen and lieven vandenberghe (2004). id76. cambridge, england:

cambridge university press. 772

braun, w. john and duncan j. murdoch (2008). a first course in statistical programming

with r. cambridge university press. 841

breiman, leo, jerome friedman, r. olshen and c. stone (1984). classi   cation and regression

trees. belmont, california: wadsworth. 285

breiman, leo and jerome h. friedman (1985).    estimating optimal transformations for mul-
tiple regression and correlation.    journal of the american statistical association, 80: 580   
598. doi:10.1080/01621459.1985.10478157. 55

brown, lawrence d. (1986). fundamentals of statistical exponential families: with applications
in statistical decision theory. hayward, california: institute of mathematical statistics.
url http://projecteuclid.org/euclid.lnms/1215466757. 265

b  uhlmann, peter (2000).    model selection for variable length markov chains and tuning the
context algorithm.    annals of the institute of statistical mathematics, 52: 287   315. url
http://e-collection.ethbib.ethz.ch/show?type=incoll&nr=117. 548

    (2002).

   bootstraps for time series.   

statistical science, 17: 52   72. url http:

//projecteuclid.org/euclid.ss/1023798998. doi:10.1214/ss/1023798998. 571

b  uhlmann, peter and abraham j. wyner (1999).    variable length markov chains.    annals
of statistics, 27: 480   513. url http://projecteuclid.org/euclid.aos/1018031204. 548
buja, andreas, richard berk, lawrence brown, edward george, emil pitkin, mikhail traskin,
linda zhao and kai zhang (2014).    models as approximations, part i: a conspiracy of
nonlinearity and random regressors in id75.    arxiv:1404.1578. url http:
//arxiv.org/abs/1404.1578. 60, 150, 763

buja, andreas, trevor hastie and robert tibshirani (1989).    linear smoothers and additive
models.    annals of statistics, 17: 453   555. url http://projecteuclid.org/euclid.aos/
1176347115. doi:10.1214/aos/1176347115. 39, 181

burman, prabir, edmond chow and deborah nolan (1994).    a cross-validatory method for

dependent data.    biometrika, 81: 351   358. doi:10.1093/biomet/81.2.351. 571

canty, angelo and brian ripley (2013).    boot: bootstrap r (s-plus) functions.    r package

version 1.3-9. url http://cran.r-project.org. 149

canty, angelo j., anthony c. davison, david v. hinkley and val  erie ventura (2006).    bootstrap
diagnostics and remedies.    the canadian journal of statistics, 34: 5   27. url http:
//www.stat.cmu.edu/tr/tr726/tr726.html. doi:10.1002/cjs.5550340103. 149

carmack, patrick s., william r. schucany, je   rey s. spence, richard f. gunst, qihua lin and
robert w. haley (2009).    far casting cross validation.    journal of computational and
graphical statistics, 18: 879   893. doi:10.1198/jcgs.2009.07034. 571

carroll, raymond j., aurore delaigle and peter hall (2009).    nonparametric prediction in
measurement error models.    journal of the american statistical association, 104: 993   
1003. doi:10.1198/jasa.2009.tm07543. 52

casella, george and r. l. berger (2002). statistical id136. belmont, california: duxbury

press, 2nd edn. 265

cavalli-sforza, luigi l., paolo menozzi and alberto piazza (1994). the history and geography

of human genes. princeton: princeton university press. 363, 367

688

references

chambers, john m. (2008). software for data analysis: programming with r. new york:

springer. 841

chenoweth, erica and maria j. stephan (2011). why civil resistance works: the strategic

logic of nonviolent con   ict. new york: columbia university press. 626

chetty, raj, nathaniel hendren, patrick kline and emmanuel saez (2014).    where is the land
of opportunity? the geography of intergenerational mobility in the united states.    quar-
terly journal of economics, 129: 1553   1623. url http://www.equality-of-opportunity.
org/index.php/papers. doi:10.1093/qje/qju022. 608, 617

christakis, nicholas a. and james h. fowler (2007).    the spread of obesity in a large social
network over 32 years.    the new england journal of medicine, 357: 370   379. url http:
//content.nejm.org/cgi/content/abstract/357/4/370. 502

chu, tianjiao and clark glymour (2008).    search for additive nonlinear time series causal
models.    journal of machine learning research, 9: 967   991. url http://jmlr.csail.mit.
edu/papers/v9/chu08a.html. 528

chu, tianjiao, clark glymour, richard scheines and peter spirtes (2003).    a statistical prob-
lem for id136 to regulatory structure from associations of gene expression measure-
ments with microarrays.    bioinformatics, 19: 1147   1152. 557

claeskens, gerda and nils lid hjort (2008). model selection and model averaging. cambridge,

england: cambridge university press. 73, 78, 243, 323

clauset, aaron and douglas h. erwin (2008).

   the evolution and distribution of
science, 321: 399   401. url http://arxiv.org/abs/0901.0251.

species body size.   
doi:10.1126/science.1157534. 619

clauset, aaron, cosma rohilla shalizi and m. e. j. newman (2009).    power-law distributions
in empirical data.    siam review , 51: 661   703. url http://arxiv.org/abs/0706.1062.
135, 136, 137

cleveland, w. s. (1979).    robust locally weighted regression and smoothing scatterplots.   
journal of the american statistical association, 74: 829   836. url https://www.jstor.org/
stable/2286407. doi:10.2307/2286407. 231

coleman, james, elihu katz and herbert menzel (1957).    the di   usion of an innovation among
sociometry, 20: 253   270. url http://www.jstor.org/stable/2785979.

physicians.   
doi:10.2307/2785979. 680

collier, paul and anke hoe   er (2004).    greed and grievance in civil war.    oxford economic
papers, 56: 563   595. url http://economics.ouls.ox.ac.uk/12055/1/2002-01text.pdf.
623

colombo, diego, marloes h. maathuis, markus kalisch and thomas s. richardson (2012).
   learning high-dimensional directed acyclic graphs with latent and selection variables.   
annals of statistics, 40: 249   321. url http://arxiv.org/abs/1104.5617. doi:10.1214/11-
aos940. 527

cormen, thomas h., charles e. leiserson, ronald l. rivest and cli   ord stein (2001). intro-

duction to algorithms. cambridge, massachusetts: mit press, 2nd edn. 842

cover, thomas m. (1968a).    estimation by the nearest neighbor rule.    ieee transactions
on id205, 14: 50   55. url http://www-isl.stanford.edu/~cover/papers/
transit/0050cove.pdf. 39

    (1968b).    rates of convergence for nearest neighbor procedures.    in proceedings of the
hawaii international conference on systems sciences (b. k. kinariwala and f. f. kuo, eds.),
pp. 413   415. honolulu: university of hawaii press. url http://www-isl.stanford.edu/
~cover/papers/paper009.pdf. 39

cover, thomas m. and p. e. hart (1967).    nearest neighbor pattern classi   cation.    ieee
transactions on id205, 13: 21   27. url http://www-isl.stanford.edu/
~cover/papers/transit/0021cove.pdf. 39

cover, thomas m. and joy a. thomas (2006). elements of id205. new york:

john wiley, 2nd edn. 465, 472

references

689

cram  er, harald (1945). mathematical methods of statistics. uppsala: almqvist and wiksells.

772

cristianini, nello and john shawe-taylor (2000). an introduction to support vector machines:
and other kernel-based learning methods. cambridge, england: cambridge university
press. 78

crutch   eld, james p. and karl young (1989).

   inferring statistical complexity.    phys-
ical review letters, 63: 105   108. url http://www.santafe.edu/~cmg/compmech/pubs/
isctitlepage.htm. 572

daley, d. j. and d. vere-jones (2003). elementary theory and methods, vol. 1 of an introduction

to the theory of point processes. new york: springer-verlag, 2nd edn. 572

dasgupta, sanjoy and anupam gupta (2002).    an elementary proof of a theorem of johnson
and lindenstrauss.    random structures and algorithms, 22: 60   65. url http://cseweb.
ucsd.edu/~dasgupta/papers/jl.pdf. doi:10.1002/rsa.10073. 364

davison, a. c. (2013). smpracticals: practicals for use with davison (2003) statistical models.

url https://cran.r-project.org/package=smpracticals. r package version 1.4-2. 707

davison, a. c. and d. v. hinkley (1997). bootstrap methods and their applications. cambridge,

england: cambridge university press. 149, 571

dawes, robyn m. (1975).    graduate admission variables and future success.    science, 187:
721   723. url https://www.jstor.org/stable/1739800. doi:10.1126/science.187.4178.721.
787

de oliveira, cesar, richard watt and mark hamer (2010).    toothbrushing, in   ammation, and
risk of cardiovascular disease: results from scottish health survey.    british medical journal ,
340: c2451. doi:10.1136/bmj.c2451. 481

deaton, angus (2010).    instruments, randomization, and learning about development.    jour-

nal of economic literature, 48: 424   455. doi:10.1257/jel.48.2.424. 500, 505

deerwester, scott, susan t. dumais, george w. furnas, thomas k. landauer and richard
harshman (1990).    indexing by latent semantic analysis.    journal of the american society
for information science, 41: 391   407. url http://lsa.colorado.edu/papers/jasis.lsi.
90.pdf. doi:10.1002/(sici)1097-4571(199009)41:6  391::aid-asi1  3.0.co;2-9. 358, 365

delanda, manuel (2006). a new philosophy of society: assemblage theory and social com-

plexity. london: continuum. 505

devroye, luc and g  abor lugosi (2001). combinatorial methods in density estimation. berlin:

springer-verlag. 295, 314

diaconis, persi and david freedman (1980).    de finetti   s theorem for markov chains.    an-
nals of id203, 8: 115   130. url http://projecteuclid.org/euclid.aop/1176994828.
doi:10.1214/aop/1176994828. 547

didelez, vanessa, sha meng and nuala a. sheehan (2010).    assumptions of iv methods for
observational epidemiology.    statistical science, 25: 22   40. url http://arxiv.org/abs/
1011.0595. 505

ding, peng and fang li (2018).

sta-
tistical science, 33: 214   237. url https://projecteuclid.org/euclid.ss/1525313143.
doi:10.1214/18-sts645. 792

   causal id136: a missing data perspective.   

dinno, alexis (2009). loopanalyst: a collection of tools to conduct levins    loop analysis. url

http://cran.r-project.org/package=loopanalyst. r package version 1.2-2. 470

douc, randal, eric moulines and david s. sto   er (2014). nonlinear time series: theory,
methods, and applications with r examples. boca raton, florida: chapman hall/crc. 571
dreger, alice (1998). hermaphrodites and the medical invention of sex . cambridge, mas-

sachusetts: harvard university press. 509

dumouchel, william h. and greg j. duncan (1983).    using sample survey weights in multiple
regression analyses of strati   ed samples.    journal of the american statistical association,
78: 535   543. url http://www.jstor.org/stable/2288115. 207

690

references

durbin, j. (1960).    the    tting of time series models.    revue de l   institut international de
statistique / review of the international statistical institute, 28: 233   244. url https:
//repository.lib.ncsu.edu/handle/1840.4/2230. doi:10.2307/1401322. 567

durbin, james and siem jam koopman (2001). time series analysis by state space methods.

oxford: oxford university press. 566

efron, bradley (1979).

an-
nals of statistics, 7: 1   26. url http://projecteuclid.org/euclid.aos/1176344552.
doi:10.1214/aos/1176344552. 128, 150

   bootstrap methods: another look at the jackknife.   

    (1982). the jackknife, the bootstrap, and other resampling plans. philadelphia: siam

press. 150

efron, bradley and robert j. tibshirani (1993). an introduction to the bootstrap. new york:

chapman and hall. 149

eliade, mircea (1971). the forge and the crucible: the origin and structure of alchemy. new

york: harper and row. 58

elster, jon (1989). nuts and bolts for the social sciences. cambridge, england: cambridge

university press. 505

entner, doris, patrik o. hoyer and peter spirtes (2013).    data-driven covariate selection for
nonparametric estimation of causal e   ects.    in sixteenth international conference on arti-
   cial intelligence and statistics [aistats 2013] (carlos m. carvalho and pradeep raviku-
mar, eds.), pp. 256   264. url http://jmlr.org/proceedings/papers/v31/entner13a.html.
492

eshel, gidon (2012). spatiotemporal data analysis. princeton, new jersey: princeton university

press. 365

ezekiel, mordecai (1924).    a method of handling curvilinear correlation for any number
of variables.    journal of the american statistical association, 19: 431   453. url http:
//www.jstor.org/stable/2281561. 181

fair, ray c. (1978).    a theory of extramarital a   airs.    journal of political economy, 86:

45   61. url http://fairmodel.econ.yale.edu/rayfair/pdf/1978a200.pdf. 650

fama, eugene f. and kenneth r. french (1993).    common risk factors in the returns on stocks
and bonds.    journal of financial economics, 33: 3   56. doi:10.1016/0304-405x(93)90023-5.
629

fan, jianqing and qiwei yao (2003). nonlinear time series: nonparametric and parametric

methods. berlin: springer-verlag. 570

fan, jiaqing and i. gijbels (1996). local polynomial modelling and its applications. london:

chapman and hall. 231

faraway, julian j. (1992).    on the cost of data analysis.    journal of computational and
graphical statistics, 1: 213   229. url http://people.bath.ac.uk/jjf23/papers/cda.pdf.
doi:10.1080/10618600.1992.10474582. 78

    (2004). linear models with r. boca raton, florida: chapman and hall/crc press. 60
    (2006). extending the linear model with r: generalized linear, mixed e   ects and nonpara-

metric regression models. boca raton, florida: chapman and hall/crc. 39, 266

    (2014). faraway: functions and datasets for books by julian faaway. url https://cran.

r-project.org/package=faraway. r package version 1.0.6. 707

    (2016).    does data splitting improve prediction?    statistics and computing, 26: 49   60.

url http://arxiv.org/abs/1301.2983. doi:10.1007/s11222-014-9522-9. 76, 78

fisher, franklin m. (1983). disequilibrium foundations of equilibrium economics. cambridge,

england: cambridge university press. 470

    (2010).    the stability of general equilibrium     what do we know and why is it im-
portant?    in general equilibrium analysis: a century after walras (pascal bridel, ed.), pp.
34   45. london: routledge. url http://economics.mit.edu/files/6988. 470

fisher, r. a. (1922).    on the mathematical foundations of theoretical statistics.    philosoph-
ical transactions of the royal society a, 222: 309   368. url http://digital.library.
adelaide.edu.au/dspace/handle/2440/15172. 772

references

691

fraser, andrew m. (2008). id48 and dynamical systems. philadelphia: siam

press. url http://www.siam.org/books/ot107/. 452, 566

freedman, david a. (1983).    a note on screening regression equations.    the american

statistician, 37: 152   155. doi:10.1080/00031305.1983.10482792. 79

frisch, uriel (1995). turbulence: the legacy of a. n. kolmogorov . cambridge, england: cam-

bridge university press. 571

galles, david and judea pearl (1997).    axioms of causal relevance.    arti   cial intelligence,

97: 9   43. url http://nexus.cs.usfca.edu/~galles/research/relaxiom.ps. 495

gamow, george (1970). my world-line: an informal autobiography. new york: viking press.

foreword by stanislaw m. ulam. 725

gelman, andrew (2003).

goodness-of-   t testing.   
//www.stat.columbia.edu/~gelman/research/published/isr.pdf.
5823.2003.tb00203.x. 122

   a bayesian formulation of exploratory data analysis and
international statistical review , 71: 369   382. url http:
doi:10.1111/j.1751-

gelman, andrew and iain pardoe (2007).

   average predictive comparisons for models
with nonlinearity, interactions, and variance components.    sociological methodology, 37:
23   51. url http://www.stat.columbia.edu/~gelman/research/published/ape17.pdf.
doi:10.1111/j.1467-9531.2007.00181.x. 96

gelman, andrew and donald b. rubin (1992).    id136 from iterative simulation using

multiple sequences.    statistical science, 7: 457   472. doi:10.1214/ss/1177011136. 849

gelman, andrew and cosma rohilla shalizi (2013).    philosophy and the practice of bayesian
statistics.    british journal of mathematical and statistical psychology, 66: 8   38. url
http://arxiv.org/abs/1006.3868. doi:10.1111/j.2044-8317.2011.02037.x. 122, 558

genz, alan, frank bretz, tetsuhisa miwa, xuefei mi, friedrich leisch, fabian scheipl and
torsten hothorn (2016). mvtnorm: multivariate normal and t distributions. url http:
//cran.r-project.org/package=mvtnorm. r package version 1.0-5. 707

gershenfeld, neil (1999). the nature of mathematical modeling. cambridge, england: cam-

bridge university press. 164

geyer, charles j. (1992).    practical id115.    statistical science, 7: 473   

483. doi:10.1214/ss/1177011137. 849

    (2013).    asymptotics of maximum likelihood without the lln or clt or sample size
going to in   nity.    in advances in modern statistical theory and applications: a festschrift
in honor of morris l. eaton (galin jones and xiaotong shen, eds.), pp. 1   24. beach-
wood, ohio: institute of mathematical statistics. url http://arxiv.org/abs/1206.4762.
doi:10.1214/12-imscoll1001. 772

gilbert, paul and ravi varadhan (2015). numderiv: accurate numerical derivatives. url

https://cran.r-project.org/package=numderiv. r package version 2014.2-1. 707

glymour, clark (1986).    statistics and metaphysics.    journal of the american statistical as-
sociation, 81: 964   966. url http://www.hss.cmu.edu/philosophy/glymour/glymour1986.
pdf. 509

    (1998).    what went wrong? re   ections on science by observation and the bell curve.   
philosophy of science, 65: 1   32. url http://www.hss.cmu.edu/philosophy/glymour/
glymour1998.pdf. 398

    (2001). the mind   s arrows: bayes nets and graphical causal models in psychology. cam-

bridge, massachusetts: mit press. 472

glymour, clark and madelyn r. glymour (2014).    race and sex are causes.    epidemiology,

25: 488   490. doi:10.1097/ede.0000000000000122. 509

gnedenko, b. v. and a. n. kolmogorov (1954). limit distributions for sums of independent
random variables. cambridge, massachusetts: addison-wesley. translated from the russian
and annotated by k. l. chung, with an appendix by j. l. doob. 758

godfrey, l. g. (1988). misspeci   cation tests in econometrics; the lagrange multiplier principle

and other approaches. cambridge, england: cambridge university press. 205

692

references

gouri  eroux, christian and alain monfort (1989/1995). statistics and econometric models.
themes in modern econometrics. cambridge, england: cambridge university press. trans-
lated by quang vuong from statistique et mod`eles   econom  etriques, paris:   economica. 603

gouri  eroux, christian, alain monfort and e. renault (1993).    indirect id136.    journal of

applied econometrics, 8: s85   s118. url http://www.jstor.org/pss/2285076. 604

gray, robert m. (1988). id203, random processes, and ergodic properties. new york:

springer-verlag. url http://ee.stanford.edu/~gray/arp.html. 571

    (2009). id203, random processes, and ergodic properties. new york: springer-verlag,

2nd edn. url http://ee.stanford.edu/~gray/arp.html. 545

gretton, arthur, karsten m. borgwardt, malte j. rasch, bernhard sch  olkopf and alexander
smola (2012).    a kernel two-sample test.    journal of machine learning research, 13:
723   773. url http://jmlr.csail.mit.edu/papers/v13/gretton12a.html. 521

gri   eath, david (1976).    introduction to markov random fields.    in denumerable markov
chains (john g. kemeny and j. laurie snell and anthony w. knapp, eds.), pp. 425   457.
berlin: springer-verlag, 2nd edn. 469, 472

grimmett, g. r. and d. r. stirzaker (1992). id203 and random processes. oxford: oxford

university press, 2nd edn. 571

gr  unwald, peter d. (2007). the minimum description length principle. cambridge, mas-

sachusetts: mit press. 265

guckenheimer, john and philip holmes (1983). nonlinear oscillations, dynamical systems and

bifurcations of vector fields. new york: springer-verlag. 405

guttorp, peter (1995). stochastic modeling of scienti   c data. london: chapman and hall.

125, 246, 257, 265, 472, 571, 572

gy  or   , l  aszl  o, michael kohler, adam krzy  zak and harro walk (2002). a distribution-free

theory of nonparametric regression. new york: springer-verlag. 30, 39, 78

hacking, ian (1990). the taming of chance. cambridge, england: cambridge university press.

17

    (2001). an introduction to id203 and inductive logic. cambridge, england: cambridge

university press. 558

hall, peter and joel horowitz (2013).    a simple bootstrap method for constructing non-
parametric con   dence bands for functions.    annals of statistics, 41: 1892   1921. url
https://projecteuclid.org/euclid.aos/1378386242. doi:10.1214/13-aos1137. 147

hall, peter, je    racine and qi li (2004).    cross-validation and the estimation of conditional
id203 densities.    journal of the american statistical association, 99: 1015   1026. url
http://www.ssc.wisc.edu/~bhansen/workshop/qili.pdf. 304, 313, 522

halmos, paul r. (1957). an introduction to hilbert space and the theory of spectral multiplicity.

new york: chelsea publishing co., 2nd edn. first edition, 1951. 181

hand, david, heikki mannila and padhraic smyth (2001). principles of data mining. cam-

bridge, massachusetts: mit press. 365

handcock, mark s. (2015). relative distribution methods. url http://cran.r-project.org/

package=reldist. version 1.6-4. 707

handcock, mark s. and martina morris (1998).    relative distribution methods.    sociological

methodology, 28: 53   97. url http://www.jstor.org/pss/270964. 333, 341

    (1999). relative distribution methods in the social sciences. berlin: springer-verlag. 333,

341

hannan, e. j. and jorma rissanen (1982).    recursive estimation of mixed autoregressive-
moving average order.    biometrika, 69: 81   94. url https://www.jstor.org/stable/
2335856. doi:10.2307/2335856. 567

hansen, ben b. and stephanie olsen klopfer (2006).

full matching and re-
journal of computational and graphical statis-
lated designs via network    ows.   
tics, 15: 609   627. url http://www.stat.lsa.umich.edu/~bbh/hansenklopfer2006.pdf.
doi:10.1198/106186006x137047. 514

   optimal

references

693

hart, je   rey d. (1997). nonparametric smoothing and lack-of-fit tests. berlin: springer-

verlag. 192, 205

hastie, trevor, robert tibshirani and jerome friedman (2009). the elements of statistical
learning: data mining, id136, and prediction. berlin: springer, 2nd edn. url http:
//www-stat.stanford.edu/~tibs/elemstatlearn/. 60, 78, 165, 754

hay   eld, tristen and je   rey s. racine (2008).    nonparametric econometrics: the np package.   
journal of statistical software, 27(5): 1   32. url http://www.jstatsoft.org/v27/i05. 90,
96, 707

heckman, james j. (1976).    the common structure of statistical models of truncation, sample
selection and limited dependent variables and a simple estimator for such models.    annals
of economic and social measurement, 5: 475   492. url http://www.nber.org/chapters/
c10491. 790, 791

    (1979).    sample bias as a speci   cation error.    econometrica, 47: 153   162. url http:

//www.nber.org/papers/w0172. 791

hedstr  om, peter (2005). dissecting the social: on the principles of analytical sociology. cam-

bridge, england: cambridge university press. 505

hedstr  om, peter and richard swedberg (eds.) (1998). social mechanisms: an analytical ap-
proach to social theory, studies in rationality and social change, cambridge, england.
cambridge university press. 505, 687

ho, daniel e., kosuke imai, gary king and elizabeth a. stuart (2011).    matchit: nonparamet-
ric preprocessing for parametric causal id136.    journal of statistical software, 42(8):
1   28. url http://www.jstatsoft.org/v42/i08/. 514

hoerl, arthur e. and robert w. kennard (1970).    ridge regression: biased estimation for
nonorthogonal problems.    technometrics, 12. url http://www.jstor.org/pss/1267351.
753

hofmann, thomas

(1999).

   probabilistic latent semantic analysis.   

in uncer-
tainty in arti   cial intelligence: proceedings of
the fiftheenth conference [uai 1999]
(kathryn laskey and henri prade, eds.), pp. 289   296. san francisco: morgan kauf-
mann.
url http://uai.sis.pitt.edu/displayarticledetails.jsp?mmnu=1&smnu=2&
article_id=179&proceeding_id=15. 433

holan, scott h., robert lund and ginger davis (2010).    the arma alphabet soup: a tour of
arma model variants.    statistics surveys, 4: 232   274. url http://projecteuclid.org/
euclid.ssu/1291731822. doi:10.1214/09-ss060. 571

holland, paul w. (1986).    statistics and causal id136.    journal of the american statistical

association, 81: 945   970. 509

honerkamp, josef (2002). statistical physics: an advanced approach with applications. berlin:

springer-verlag, 2nd edn. translated by thomas filk. 472

hong, yongmiao and halbert white (1995).    consistent speci   cation testing via nonparamet-
ric series regression.    econometrica, 63: 1133   1159. url http://www.jstor.org/stable/
2171724. doi:10.2307/2171724. 205

hotelling, harold (1933a).    analysis of a complex of statistical variables into principal compo-
nents [part 1 of 2].    journal of educational psychology, 24: 471   441. doi:10.1037/h0071325.
365

    (1933b).    analysis of a complex of statistical variables into principal components [part 2 of

2].    journal of educational psychology, 24: 498   520. doi:10.1037/h0070888. 365

hoyer, patrik o., domink janzing, joris mooij, jonas peters and bernhard sch  olkopf (2009).
   nonlinear causal discovery with additive noise models.    in advances in neural information
processing systems 21 [nips 2008] (daphne koller and d. schuurmans and y. bengio and
l  eon bottou, eds.), pp. 689   696. cambridge, massachusetts: mit press. url http://books.
nips.cc/papers/files/nips21/nips2008_0266.pdf. 523

huber, peter j. (1967).    the behavior of maximum likelihood estimates under nonstandard
conditions.    in proceedings of the fifth berkeley symposium on mathematical statistics
and id203 (lucien m. le cam and jerzy neyman, eds.), vol. 1, pp. 221   233. berkeley:

694

references

university of california press. url http://projecteuclid.org/euclid.bsmsp/1200512988.
762, 772

hume, david (1739). a treatise of human nature: being an attempt to introduce the exper-
imental method of reasoning into moral subjects. london: john noon. reprint (oxford:
clarendon press, 1951) of original edition, with notes and analytical index. 478

hunter, david r., steven m. goodreau and mark s. handcock (2008).    goodness of fit of social
network models.    journal of the american statistical association, 103: 248   258. url http:
//www.csss.washington.edu/papers/wp47.pdf. doi:10.1198/016214507000000446. 122

imbens, guido w. and donald b. rubin (2015). causal id136 for statistics, social, and
biomedical sciences: an introduction. cambridge, england: cambridge university press.
483, 518, 701

iyigun, murat (2008).    luther and suleyman.    quarterly journal of economics, 123: 1465   
1494. url http://www.colorado.edu/economics/courses/iyigun/ottoman081506.pdf.
doi:10.1162/qjec.2008.123.4.1465. 76, 77

jacobs, robert a. (1997).    bias/variance analyses of mixtures-of-experts architectures.    neu-

ral computation, 9: 369   383. 452

jaeger, herbert (2000).    observable operator models for discrete stochastic time series.   
neural computation, 12: 1371   1398. url http://minds.jacobs-university.de/sites/
default/files/uploads/papers/oom_neco00.pdf. doi:10.1162/089976600300015411. 572

janzing, dominik (2007).    on causally asymmetric versions of occam   s razor and their relation
to thermodynamics.    arxiv:0708.3411. url http://arxiv.org/abs/0708.3411. 481, 523,
524

janzing, dominik and daniel herrmann (2003).    reliable and e   cient id136 of bayesian
networks from sparse data by statistical learning theory.    arxiv:cs.lg/0309015. url
http://arxiv.org/abs/cs.lg/0309015. 534

jones, owen, robert maillardet and andrew robinson (2009). introduction to scienti   c pro-

gramming and simulation using r. boca raton, florida: chapman and hall/crc. 125

jordan, michael i. (ed.) (1998). learning in id114, dordrecht. kluwer academic.

468, 472, 698

jordan, michael i. and robert a. jacobs (1994).    hierarchical mixtures of experts and the
em algorithm.    neural computation, 6: 181   214. url ftp://publications.ai.mit.edu/
ai-publications/pdf/aim-1440.pdf. doi:10.1162/neco.1994.6.2.181. 452

jordan, michael i. and terrence j. sejnowski (eds.) (2001). id114: foundations of
neural computation, computational neuroscience, cambridge, massachusetts. mit press.
472

kahn, joan r. and j. richard udry (1986).    marital coital frequency: unnoticed outliers and
unspeci   ed interactions lead to erroneous conclusions.    american sociological review , 51:
734   737. url https://www.jstor.org/stable/2095496. doi:10.2307/2095496. 780

kalisch, markus and peter b  uhlmnann (2007).    estimating high-dimensional directed acyclic
graphs with the pc-algorithm.    journal of machine learning research, 8: 616   636. url
http://jmlr.csail.mit.edu/papers/v8/kalisch07a.html. 534, 535

kalisch, markus, martin m  achler and diego colombo (2010). pcalg: estimation of cpdag/pag
and causal id136 using the ida algorithm. url http://cran.r-project.org/package=
pcalg. r package version 1.1-2. 528, 707

kalisch, markus, martin m  achler, diego colombo, marloes h. maathuis and peter b  uhlmnann
(2012).    causal id136 using id114 with the r package pcalg.    journal of
statistical software, 47(11): 1   26. url http://www.jstatsoft.org/v47/i11. 528, 536, 707
kallenberg, wilbert c. m. and teresa ledwina (1997).    data-driven smooth tests when the
hypothesis is composite.    journal of the american statistical association, 92: 1094   1104.
url http://doc.utwente.nl/62408/. 323, 341

kalman, r. e. and r. s. bucy (1961).    new results in linear filtering and prediction.    asme

transactions, journal of basic engineering, 83d: 95   108. 566

references

695

kalman, rudolf e. (1960).    a new approach to linear filtering and prediction problems.   

asme transactions, journal of basic engineering, 82d: 35   50. 566

kanai, ryota, tom feilden, colin firth and geraint rees (2011).

   political orientations
are correlated with brain structure in young adults.    current biology, 21: 677   680.
doi:10.1016/j.cub.2011.03.017. 333, 659

kantz, holger and thomas schreiber (2004). nonlinear time series analysis. cambridge,

england: cambridge university press, 2nd edn. 571

kao, yi-hao and benjamin van roy (2013).

   learning a factor model via regular-
ized pca.    machine learning, 91: 279   303. url http://arxiv.org/abs/1111.6201.
doi:10.1007/s10994-013-5345-8. 382

kaplan, e. l. and paul meier (1958).

   nonparametric estimation from incomplete ob-
servations.    journal of the american statistical association, 53: 457   481. url https:
//www.jstor.org/stable/2281868. doi:10.2307/2281868. 787

kass, robert e. and paul w. vos (1997). geometrical foundations of asymptotic id136.

new york: wiley. 405

kearns, michael j. and umesh v. vazirani (1994). an introduction to computational learning

theory. cambridge, massachusetts: mit press. 78

kelly, kevin t. (2007).    ockham   s razor, empirical complexity, and truth-   nding e   ciency.   

theoretical computer science, 383: 270   289. doi:10.1016/j.tcs.2007.04.009. 558

kendall, bruce e., stephen p. ellner, edward mccauley, simon n. wood, cheryl j. briggs,
william w. murdoch and peter turchin (2005).    population cycles in the pine looper
moth: dynamical tests of mechanistic hypotheses.    ecological monographs, 75: 259   276.
url https://escholarship.org/uc/item/2tq9h5tq. doi:10.1890/03-4056. 604

kindermann, ross and j. laurie snell (1980). markov random fields and their applications.
providence, rhode island: american mathematical society. url http://www.ams.org/
online_bks/conm1/. 472

king, gary and richard nielsen (2016).    why propensity scores should not be used for

matching.    electronic preprint. url http://j.mp/1fqhysn. 518

king, gary and margaret e roberts (2015).

methodological problems they do not fix, and what
litical analysis, 23:
how-robust-standard-errors-expose-methodological-problems-they-do-not-fix.
doi:10.1093/pan/mpu015. 150

   how robust standard errors expose
po-
url http://gking.harvard.edu/publications/

to do about

159   179.

it.   

klein, judy l. (1997). statistical visions in time: a history of time series analysis, 1662   

1938 . cambridge, england: cambridge university press. 60, 565, 570

knight, frank b. (1975).    a predictive view of continuous time processes.    annals of prob-

ability, 3: 573   596. url http://projecteuclid.org/euclid.aop/1176996302. 572

koenker, roger and kevin f. hallock (2001).    quantile regression.    journal of economic
perspectives, 15: 143   156. url http://www.econ.uiuc.edu/~roger/research/rq/qrjep.
pdf. doi:10.1257/jep.15.4.143. 757

kogan, barry s. (1985). averroes and the metaphysics of causation. albany, new york: state

university of new york press. 478

koyama, shinsuke, lucia castellanos p  erez-bolde, cosma rohilla shalizi and robert e. kass
(2010).    approximate methods for state-space models.    journal of the american statistical
association, 105: 170   180. url http://arxiv.org/abs/1004.3476. 570

kpotufe, samory (2011).    id92 regression adapts to local intrinsic dimension.    in ad-
vances in neural information processing systems 24 [nips 2011] (john shawe-taylor and
richard s. zemel and peter l. bartlett and fernando pereira and kilian q. weinberger,
eds.), pp. 729   737. cambridge, massachusetts: mit press. url http://papers.nips.cc/
paper/4455-id92-regression-adapts-to-local-intrinsic-dimension. 511

kullback, solomon (1968). id205 and statistics. new york: dover books, 2nd

edn. 465

696

references

k  unsch, hans r. (1989).    the jackknife and the bootstrap for general stationary observa-
tions.    annals of statistics, 17: 1217   1241. url http://projecteuclid.org/euclid.aos/
1176347265. doi:10.1214/aos/1176347265. 571

lacerda, gustavo, peter spirtes, joseph ramsey and patrik hoyer (2008).    discovering cyclic
causal models by independent components analysis.    in proceedings of the proceedings of
the twenty-fourth conference annual conference on uncertainty in arti   cial intelligence
(uai-08), pp. 366   374. corvallis, oregon: auai press. url http://uai.sis.pitt.edu/
papers/08/p366-lacerda.pdf. 470, 536

lahiri, s. n. (2003). resampling methods for dependent data. new york: springer-verlag. 571
lambert, diane and kathryn roeder (1995).    overdispersion diagnostics for generalized linear
models.    journal of the american statistical association, 90: 1225   1236. url http://www.
jstor.org/stable/2291513. 266

landauer, thomas k. and susan t. dumais (1997).    a solution to plato   s problem: the la-
tent semantic analysis theory of acquisition, induction, and representation of knowledge.   
psychological review , 104: 211   240. url http://lsa.colorado.edu/papers/plato/plato.
annote.html. 358, 365

lange, kenneth (2013). optimization. new york: springer, 2nd edn. 772
langford, john, ruslan salakhutdinov and tong zhang (2009).    learning nonlinear dynamic
models.    in proceedings of the 26th annual international conference on machine learning
[icml 2009] (andrea danyluk and l  eon bottou and michael littman, eds.), pp. 593   600.
new york: association for computing machinery. url http://arxiv.org/abs/0905.3369.
572

lasota, andrzej and michael c. mackey (1994). chaos, fractals, and noise: stochastic aspects
of dynamics. berlin: springer-verlag. first edition, probabilistic properties of deterministic
systems, cambridge university press, 1985. 502

lauritzen, ste   en l. (1984).    extreme point models in statistics.    scandinavian journal
of statistics, 11: 65   91. url http://www.jstor.org/pss/4615945. with discussion and
response. 427

    (1996). id114. new york: oxford university press. 459, 470, 472, 482
lawrie, ian d. (1990). a uni   ed grand tour of theoretical physics. bristol, england: adam

hilger. 405

lee, ann b. and larry wasserman (2010).    spectral connectivity analysis.    journal of the
american statistical association, 105: 1241   1255. url http://arxiv.org/abs/0811.0121.
doi:10.1198/jasa.2010.tm09754. 422

lehmann, erich l. (2008).    on the history and use of some standard statistical models.   
in id203 and statistics; essays in honor of david a. freedman (deborah nolan and
terry speed, eds.), pp. 114   126. brentwood, ohio: institute of mathematical statistics. url
http://projecteuclid.org/euclid.imsc/1207580081. 60

leisch, friedrich (2004).    flexmix: a general framework for finite mixture models and latent
class regression in r.    journal of statistical software, 11. url http://www.jstatsoft.
org/v11/i08. 433

li, ching chun (1975). path analysis: a primer . paci   c grove, california: the boxwood

press. 472

li, ching chun, sati mazumdar and b. raja rao (1975).    partial correlation in terms of path
coe   cients.    the american statistician, 29: 89   90. url http://www.jstor.org/stable/
2683271. 462

li, ming and paul m. b. vit  anyi (1997). an introduction to kolmogorov complexity and its

applications. new york: springer-verlag, 2nd edn. 489

li, qi and je   rey scott racine (2007). nonparametric econometrics: theory and practice.

princeton, new jersey: princeton university press. 99, 192, 205, 516, 522

ligges, uwe and martin m  achler (2003).    scatterplot3d: an r package for visualizing mul-
tivariate data.    journal of statistical software, 8(11): 1   20. url http://www.jstatsoft.
org/v8/i11/. 707

references

697

lindsey, j. k. (2004). statistical analysis of stochastic processes in time. cambridge, england:

cambridge university press. 570

little, roderick j. a. and donald b. rubin (1987). statistical analysis with missing data. new

york: john wiley and sons. 800

littman, michael l., richard s. sutton and satinder singh (2002).

   predictive repre-
sentations of state.   
in advances in neural information processing systems 14 (nips
2001) (thomas g. dietterich and suzanna becker and zoubin ghahramani, eds.), pp.
1555   1561. cambridge, massachusetts: mit press. url http://papers.nips.cc/paper/
1983-predictive-representations-of-state. 572

liu, han, john la   erty and larry wasserman (2009).    the nonparanormal: semiparametric
estimation of high dimensional undirected graphs.    journal of machine learning research,
10: 2295   2328. url http://jmlr.csail.mit.edu/papers/v10/liu09a.html. 470

liu, ka-yuet, marissa king

and
ence
1387   1434.
social-influence-and-the-autism-epidemic-(2010).pdf. doi:10.1086/651448. 517

in   u-
115:
http://www.understandingautism.columbia.edu/papers/

s. bearman
american

the autism epidemic.   

   social
sociology,

and peter

journal

(2010).

url

of

loehlin, john c. (1992). latent variable models: an introduction to factor, path, and structural

analysis. hillsdale, new jersey: lawrence erlbaum associates, 2nd edn. 398, 472

lo`eve, michel (1955). id203 theory. new york: d. van nostrand company, 1st edn. 365
lunde, robert and cosma rohilla shalizi (2017).    id64 generalization error bounds

for time series.    arxiv:1711.02834. url https://arxiv.org/abs/1711.02834. 571

maathuis, marloes h., diego colombo, markus kalisch and peter b  uhlmann (2010).    pre-
dicting causal e   ects in large-scale systems from observational data.    nature meth-
ods, 7: 247   248. url http://stat.ethz.ch/manuscripts/buhlmann/maathuisetal2010.
pdf. doi:10.1038/nmeth0410-247. see also http://stat.ethz.ch/manuscripts/buhlmann/
maathuisetal2010si.pdf. 536

maathuis, marloes h., markus kalisch and peter b  uhlmann (2009).

   estimating high-
dimensional intervention e   ects from observational data.    annals of statistics, 37: 3133   
3164. url http://arxiv.org/abs/0810.4214. doi:10.1214/09-aos685. 535

macculloch, diarmaid (2004). the reformation: a history. new york: penguin. 77
mackenzie, donald (2006). an engine, not a camera: how financial models shape markets.

cambridge, massachusetts: mit press. 629

maguire, b. a., e. s. pearson and a. h. a. wynn (1952).    the time intervals between industrial

accidents.    biometrika, 39: 168   180. url http://www.jstor.org/pss/2332475. 447

mahoney, michael w. (2011).    randomized algorithms for matrices and data.    foundations
and trends in machine learning, 2: 123   224. url https://arxiv.org/abs/1104.5557.
doi:10.1561/2200000035. 365

mandelbrot, benoit (1962).    the role of su   ciency and of estimation in thermodynam-
ics.    annals of mathematical statistics, 33: 1021   1038. url http://projecteuclid.org/
euclid.aoms/1177704470. 265, 472

manski, charles f. (2003). partial identi   cation of id203 distributions. new york:

springer-verlag. 800

    (2007).

identi   cation for prediction and decision. cambridge, massachusetts: harvard

university press. 487, 504, 535, 790, 800

marcellesi, alexandre (2013).

   is race a cause?    philosophy of science, 80: 650   659.

doi:10.1086/673721. 509

martindale, colin (1990). the clockwork muse: the predictability of artistic change. new

york: basic books. 565

matlo   , norman (2011). the art of r programming: a tour of statistical software design.

san francisco: no starch press. 841

698

references

mcgee, leonard a. and stanley f. schmidt (1985). discovery of the kalman filter as a
practical tool for aerospace and industry. tech. rep. 86847, nasa technical memo-
randum. url http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19860003843_
1986003843.pdf. 452, 571

mclachlan, geo   rey j. and thriyambakam krishnan (2008). the em algorithm and exten-

sions. hoboken, new jersey: john wiley and sons. 800

metropolis, nicholas, arianna w. rosenbluth, marshall n. rosenbluth, augusta h. teller and
edward teller (1953).    equations of state calculations by fast computing machines.   
journal of chemical physics, 21: 1087   1092. doi:10.1063/1.1699114. 120

miller, john h. (1998).    active nonlinear tests (ants) of complex simulation models.    man-
agement science, 44: 820   830. url http://zia.hss.cmu.edu/miller/papers/antabst.
html. 125

mitchell, tom m. (1997). machine learning. new york: mcgraw-hill. 285
mohan, karthika, judea pearl and jin tian (2013).    id114 for id136 with
missing data.    in advances in neural information processing systems 26 [nips 2013]
(c. j. c. burges and l  eon bottou and max welling and zoubin ghahramani and kilian q.
weinberger, eds.), pp. 1277   1285. curran associates. url http://papers.nips.cc/paper/
4899-graphical-models-for-id136-with-missing-data. 793

mohri, mehryar, afshin rostamizadeh and ameet talwalkar (2012). foundations of machine
learning. adaptive computation and machine learning. cambridge, massachusetts: mit
press. 78

moran, p. a. p. (1961).    path coe   cients reconsidered.    australian journal of statistics, 3:

87   93. doi:10.1111/j.1467-842x.1961.tb00314.x. 472

morgan, stephen l. and christopher winship (2007). counterfactuals and causal id136:
methods and principles for social research. cambridge, england: cambridge university
press. 482, 483, 494, 505, 517

    (2015). counterfactuals and causal id136: methods and principles for social research.

cambridge, england: cambridge university press, 2nd edn. 482, 495

nadaraya, e. a. (1964).    on estimating regression.    theory of id203 and its applications,

9: 141   142. doi:10.1137/1109020. 39, 99

neal, radford m. and geo   rey e. hinton (1998).    a view of the em algorithm that justi   es
incremental, sparse, and other variants.    in jordan (1998), pp. 355   368. url http:
//www.cs.toronto.edu/~radford/em.abstract.html. 452

newey, whitney k. and james l. powell (2003).    instrumental variable estimation of non-

parametric models.    econometrica, 71: 1565   1578. doi:10.1111/1468-0262.00459. 505

newman, mark e. j. and g. t. barkema (1999). monte carlo methods in statistical physics.

oxford: clarendon press. 848

novembre, john and matthew stephens (2008).    interpreting principal component analyses
of spatial population genetic variation.    nature genetics, 40: 646   649. doi:10.1038/ng.139.
363, 367, 368

packard, norman h., james p. crutch   eld, j. doyne farmer and robert s. shaw (1980).    ge-

ometry from a time series.    physical review letters, 45: 712   716. 571

paige, robert l. and a. alexandre trindade (2010).    the hodrick-prescott filter: a special
case of penalized spline smoothing.    electronic journal of statistics, 4: 856   874. url
http://projecteuclid.org/euclid.ejs/1284557751. doi:10.1214/10-ejs570. 166

palomar, daniel p. and sergio verd  u (2008).    lautum information.    ieee transactions on
id205, 54: 964   975. url http://www.princeton.edu/~verdu/lautum.info.
pdf. 465

parzen, emanuel (1962).    on estimation of a id203 density function and mode.    annals
of mathematical statistics, 33: 1065   1076. url https://projecteuclid.org/euclid.aoms/
1177704472. doi:10.1214/aoms/1177704472. 315

pearl, judea (1988). probabilistic reasoning in intelligent systems. new york: morgan kauf-

mann. 472

references

699

    (2000). causality: models, reasoning, and id136. cambridge, england: cambridge uni-

versity press. 459, 472

    (2009a).    causal id136 in statistics: an overview.    statistics surveys, 3: 96   146. url

http://projecteuclid.org/euclid.ssu/1255440554. 482, 483, 514

    (2009b). causality: models, reasoning, and id136. cambridge, england: cambridge

university press, 2nd edn. 472, 482, 491, 494, 495, 500, 502, 505, 514, 524

pearson, karl (1901).    on lines and planes of closest    t to systems of points in space.    philo-

sophical magazine, 2 (series 6): 559   572. doi:10.1080/14786440109462720. 365

peterson, robert a. (2000).    a meta-analysis of variance accounted for and factor loadings

in exploratory factor analysis.    marketing letters, 11: 261   275. 386

pitman, e. j. g. (1979). some basic theory for statistical id136. london: chapman and

hall. 294

pollard, david (1989).    asymptotics via empirical processes.    statistical science, 4: 341   354.
url http://projecteuclid.org/euclid.ss/1177012394. doi:10.1214/ss/1177012394. 294,
314

    (1990). empirical processes: theory and applications, vol. 2 of nsf-cbms regional con-
ference series in id203 and statistics. hayward, california: institute of mathematical
statistics. url http://www.stat.yale.edu/~pollard/. 314

porter, theodore m. (1986). the rise of statistical thinking, 1820   1900 . princeton, new

jersey: princeton university press. 17, 60

press, william h., saul a. teukolsky, william t. vetterling and brian p. flannery (1992).
numerical recipes in c: the art of scienti   c computing. cambridge, england: cambridge
university press, 2nd edn. url http://www.nrbook.com/. 125, 848

puccia, charles j. and richard levins (1985). qualitative modeling of complex systems: an
introduction to loop analysis and time averaging. cambridge, massachusetts: harvard
university press. 470

qui  nonero-candela, joaquin, masashi sugiyama, anton schwaighofer and neil d. lawrence
(eds.) (2009). dataset shift in machine learning. cambridge, massachusetts: mit press.
74, 207

r core team (2015a). r: a language and environment for statistical computing. r foundation
isbn 3-

for statistical computing, vienna, austria. url http://www.r-project.org.
900051-07-0. 706

    (2015b). foreign: read data stored by minitab, s, sas, spss, stata, systat, weka, dbase,

.... url https://cran.r-project.org/package=foreign. r package version 0.8-66. 707

racine, je    (2000).    consistent cross-validatory model-selection for dependent data: hv-block
cross-validation.    journal of econometrics, 99: 39   61. doi:10.1016/s0304-4076(00)00030-0.
571

racine, je   rey s. (2008).

   nonparametric econometrics: a primer.    foundations and
trends in econometrics, 3: 1   88. url http://socserv.mcmaster.ca/racine/eco0301.pdf.
doi:10.1561/0800000009. 99

raginsky, maxim (2011).    directed information and pearl   s causal calculus.    in proceed-
ings of the 49th annual allerton conference on communication, control and computing
(s. meyn and b. hajek, eds.), pp. 958   965. ieee. url http://arxiv.org/abs/1110.0718.
doi:10.1109/allerton.2011.6120270. 483

rayner, j. c. w. and d. j. best (1989). smooth tests of goodness of fit. oxford: oxford

university press. 320, 341

reichenbach, hans (1956). the direction of time. berkeley: university of california press.

edited by maria reichenbach. 522, 524

reid, constance (1982). neyman from life. new york: springer-verlag. 285
reinhart, abiel (2014). pdfetch: fetch economic and    nancial time series data from public
sources. url https://cran.r-project.org/package=pdfetch. r package version 0.1.6.
707

700

references

reinsch, christian h. (1967).    smoothing by spline functions.    numerische mathematik , 10:

177   183. 166

richardson, thomas (1996).    a discovery algorithm for directed cyclic graphs.    in pro-
ceedings of the proceedings of the twelfth conference annual conference on uncertainty in
arti   cial intelligence (uai-96), pp. 454   446. san francisco, ca: morgan kaufmann. url
ftp://ftp.andrew.cmu.edu/pub/phil/thomas/tr68.ps. url is for expanded version. 470,
536

richardson, thomas s. and james m. robins (2013). single world intervention graphs
(swigs): a uni   cation of the counterfactual and graphical approaches to causality. tech.
rep. 128, center for statistics and the social sciences, university of washington. url
http://www.csss.washington.edu/papers/wp128.pdf. 483

ripley, brian d. (1996). pattern recognition and neural networks. cambridge, england: cam-

bridge university press. 285, 287

    (2015). tree: classi   cation and regression trees. url https://cran.r-project.org/

package=tree. r package version 1.0-36. 270, 707

robins, james m., richard scheines, peter spirtes and larry wasserman (2003).    uniform
consistency in causal id136.    biometrika, 90: 491   515. url http://www.stat.cmu.
edu/tr/tr725/tr725.html. 534

rodrik, dani (2008).    the real exchange rate and economic growth.    brookings papers
on economic activity, 2008(2): 365   412. url http://www.hks.harvard.edu/fs/drodrik/
research%20papers/rer%20and%20growth.pdf. doi:10.1353/eca.0.0020. 612

rosenbaum, paul and donald rubin (1983).    the central role of the propensity score in
observational studies for causal e   ects.    biometrika, 70: 41   55. url http://www.jstor.
org/stable/2335942. 513, 514

rosenblatt, murray (1956).    remarks on some nonparametric estimates of a density func-
tion.    annals of mathematical statistics, 27: 832   837. url https://projecteuclid.org/
euclid.aoms/1177728190. doi:10.1214/aoms/1177728190. 315

rosenzweig, mark r. and kenneth i. wolpin (2000).    natural    natural experiments    in eco-

nomics.    journal of economic literature, 38: 827   874. doi:10.1257/jel.38.4.827. 500, 505

roweis, sam t. and laurence k. saul (2000).    nonlinear id84 by locally

linear embedding.    science, 290: 2323   2326. doi:10.1126/science.290.5500.2323. 406

rubin, donald b. (1987). multiple imputation for nonresponse in surveys. new york: wiley.

795, 800

    (2006). matched sampling for causal e   ects. cambridge, england: cambridge university

press. 483, 518

rubin, donald b. and richard p. waterman (2006).    estimating the causal e   ects of marketing
interventions using propensity score methodology.    statistical science, 21: 206   222. url
http://arxiv.org/abs/math.st/0609201. 518

ruelle, david (1991). chance and chaos. princeton, new jersey: princeton university press.

571, 848

russell, bertrand (1920).

introduction to mathematical philosophy. london: george allen
and unwin, 2nd edn. url http://people.umass.edu/klement/russell-imp.html. first
edition, 1919. 123

    (1927). the analysis of matter . london: k. paul trench, trubner and co. reprinted new

york: dover books, 1954. 524

salmon, wesley c. (1984). scienti   c explanation and the causal structure of the world . prince-

ton: princeton university press. 505, 506

sandhaus, evan (2008).    the new york times annotated corpus.    electronic database. url

http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2008t19. 358

sarkar, deepayan (2008). lattice: multivariate data visualization with r. new york: springer.

url http://lmdvr.r-forge.r-project.org. 707

references

701

saul, lawrence k. and sam t. roweis (2003).    think globally, fit locally: supervised learning
of low dimensional manifolds.    journal of machine learning research, 4: 119   155. url
http://jmlr.csail.mit.edu/papers/v4/saul03a.html. 406

schoenberg, i. j. (1964).    spline functions and the problem of graduation.    proceedings of the

national academy of sciences (usa), 52: 947   950. 166

schutz, bernard f. (1980). geometrical methods of mathematical physics. cambridge, england:

cambridge university press. 405

schwarz, gideon (1978).    estimating the dimension of a model.    annals of statistics, 6:

461   464. url http://projecteuclid.org/euclid.aos/1176344136. 323

scott, clayton and robert nowak (2005).    a neyman-pearson approach to statistical learn-
ing.    ieee transactions on id205, 51: 3806   3819. url http://www.ece.
wisc.edu/~nowak/np.pdf. doi:10.1109/tit.2005.856955. 284

sethna, james p. (2006). statistical mechanics: id178, order parameters, and complex-
ity. oxford: oxford university press. url http://pages.physics.cornell.edu/sethna/
statmech/. 472

shalizi, cosma rohilla (2003).    optimal nonlinear prediction of random fields on networks.   
discrete mathematics and theoretical computer science, ab(dmcs): 11   30. url http:
//arxiv.org/abs/math.pr/0305160. 572

    (2007).    id113 and model testing for q-exponential distribu-
tions.    physical review e , submitted. url http://arxiv.org/abs/math.st/0701854.
447

    (2016).    review of imbens and rubin (2015).    journal of the american statistical associa-

tion, 111: 1364   1365. doi:10.1080/01621459.2016.1235436. 483, 518

shalizi, cosma rohilla and james p. crutch   eld (2001).    computational mechanics: pattern
and prediction, structure and simplicity.    journal of statistical physics, 104: 817   879. url
http://arxiv.org/abs/cond-mat/9907176. 572

shalizi, cosma rohilla and andrew c. thomas

   homophily and conta-
gion are generically confounded in observational social network studies.   
socio-
logical methods and research, 40: 211   239. url http://arxiv.org/abs/1004.4704.
doi:10.1177/0049124111404820. 504

(2011).

shannon, claude e. (1948).    a mathematical theory of communication.    bell system technical

journal , 27: 379   423. reprinted in shannon and weaver (1963). 465

shannon, claude e. and warren weaver (1963). the mathematical theory of communication.

urbana, illinois: university of illinois press. 701

shields, paul c. (1996). the ergodic theory of discrete sample paths. providence, rhode

island: american mathematical society. 571

shpitser, ilya and judea pearl (2008).    complete identi   cation methods for the causal hierar-
chy.    journal of machine learning research, 9: 1941   1979. url http://jmlr.csail.mit.
edu/papers/v9/shpitser08a.html. 502

shumway, robert h. and david s. sto   er (2000). time series analysis and its applications.

new york: springer-verlag. 567, 570, 598

silverman, b. w. (1984).    spline smoothing: the equivalent variable kernel method.    annals

of statistics, 12: 898   916. doi:10.1214/aos/1176346710. 163

    (1985).    some aspects of the spline smoothing approach to non-parametric regression
curve fitting.    journal of the royal statistical society b , 47: 1   52. url http://www.
jstor.org/stable/2345542. 166

simono   , je   rey s. (1996). smoothing methods in statistics. berlin: springer-verlag. 39, 92,

99, 163, 165, 231, 313

solow, robert m. (1970). growth theory: an exposition. radcli   e lectures, university of
warwick, 1969. oxford: oxford university press. new edition with the 1987 nobel lecture.
557

702

references

spain, seth m., kristin l. sotak, joey (chou-yu) tsai, p. d. harms and sean t. hannah
(2012).    testing the form of theoretical models by relaxing assumptions: comparing
parametric and nonparametric models.    electronic preprint, ssrn/2164297. url http:
//ssrn.com/abstract=2164297. 205

spanos, aris (2011).    a frequentist interpretation of id203 for model-based inductive
id136.    synthese, 190. url http://www.econ.vt.edu/faculty/2008vitas_research/
spanos/1spanos-2011-synthese.pdf. doi:10.1007/s11229-011-9892-x. 558

spearman, charles (1904).       general intelligence,    objectively determined and measured.   
american journal of psychology, 15: 201   293. url http://psychclassics.yorku.ca/
spearman/. 374, 398

spector, phil (2008). data manipulation with r. berlin: springer. 841
spector, phil, jerome friedman, robert tibshirani and thomas luid113y (2013). acepack:
ace() and avas() for selecting regression transformations. url http://cran.r-project.
org/package=acepack. r package version 1.3-3.3. 55

spirtes, peter, clark glymour and richard scheines (1993). causation, prediction, and search.

berlin: springer-verlag, 1st edn. 472

    (2001). causation, prediction, and search. cambridge, massachusetts: mit press, 2nd edn.

459, 472, 482, 483, 525, 527, 528, 532, 534, 535

spivak, michael (1965). calculus on manifolds: a modern approach to classical theorems of

advanced calculus. menlo park, california: benjamin cummings. 405

spu   ord, francis (2010). red plenty. london: faber and faber. 772
sriperumbudur, bharath k., arthur gretton, kenji fukumizu, bernhard sch  olkopf and
gert r.g. lanckriet (2010).    hilbert space embeddings and metrics on id203 mea-
sures.    journal of machine learning research, 11: 1517   1561. url http://jmlr.csail.
mit.edu/papers/v11/sriperumbudur10a.html. 521

stephan, maria j. and erica chenowth (2008).    why civil resistance works; the strategic
logic of nonviolent con   ict.    international security, 33: 7   44. doi:10.1162/isec.2008.33.1.7.
626

stigler, stephen m. (1986). the history of statistics: the measurement of uncertainty before

1900 . cambridge, massachusetts: harvard university press. 17, 60

stone, m. (1974).    cross-validatory choice and assessment of statistical predictions.    journal of
the royal statistical society b , 36: 111   147. url http://www.jstor.org/stable/2984809.
78

stuart, elizabeth a. (2010).

   matching methods for causal id136: a review and a
look forward.    statistical science, 25: 1   21. url http://arxiv.org/abs/1010.5586.
doi:10.1214/09-sts313. 514, 518

sung, yun ju and charles j. geyer (2007).    monte carlo likelihood id136 for missing data

models.    annals of statistics, 35: 990   1011. doi:10.1214/009053606000001389. 800

sz  ekely, g  abor j. and maria l. rizzo (2009).    brownian distance covariance.    annals of
applied statistics, 3: 1236   1265. url http://arxiv.org/abs/1010.0297. doi:10.1214/09-
aoas312. with discussion and reply. 521

taylor, g. i. (1922).    di   usion by continuous movements.    proceedings of the london mathe-

matical society, 20: 196   212. doi:10.1112/plms/s2-20.1.196. 571

taylor, jonathan and robert j. tibshirani (2015).

   statistical learning and selective
id136.    proceedings of the national academy of sciences (usa), 112: 7629   7634.
doi:10.1073/pnas.1507583112. 78

therneau, terry m. (2015). a package for survival analysis in s . url https://cran.

r-project.org/package=survival. version 2.38. 787

thiesson, bo, david maxwell chickering, david heckerman and christopher meek (2004).
   arma time-series modeling with id114.    in uncertainty in arti   cial intelli-
gence: proceedings of the twentieth conference (uai 2004) (max chickering and joseph y.
halpern, eds.), pp. 552   560. arlington, virginia: auai press. url http://arxiv.org/abs/
1207.4162. 568

references

703

thomson, godfrey h. (1916).    a hierarchy without a general factor.    british journal of

psychology, 8: 271   281. 394

    (1939). the factorial analysis of human ability. boston: houghton mi   in company. url

http://www.archive.org/details/factorialanalysi032965mbp. 365, 398

thurstone, l. l. (1934).    the vectors of mind.    psychological review , 41: 1   32. url http:

//psychclassics.yorku.ca/thurstone/. 398

tibshirani, robert (1996).    regression shrinkage and selection via the lasso.    journal of
the royal statistical society b , 58: 267   288. url http://www-stat.stanford.edu/~tibs/
lasso/lasso.pdf. 753

tibshirani, ryan j., alessandro rinaldo, robert tibshirani and larry wasserman (2015). uni-
form asymptotic id136 and the bootstrap after model selection. tech. rep., statistics
department, carnegie mellon university. url http://arxiv.org/abs/1506.06266. 78

tibshirani, ryan j. and robert tibshirani (2009).    a bias correction for the minimum error
rate in cross-validation.    annals of applied statistics, 3: 822   829. url http://arxiv.
org/abs/0908.2904. 72, 91

tilly, charles (1984). big structures, large processes, huge comparisons. new york: russell

sage foundation. 505

    (1998). durable inequality. berkeley: university of california press. 503
    (2008). explaining social processes. boulder, colorado: paradigm publishers. 505
trapletti, adrian and kurt hornik (2015). tseries: time series analysis and computational
finance. url http://cran.r-project.org/package=tseries. r package version 0.10-34.
707

tukey, john w. (1954).    unsolved problems of experimental statistics.    journal of the amer-

ican statistical association, 49: 706   731. url http://www.jstor.org/pss/2281535. 371

tutz, gerhard (2012). regression for categorical data. cambridge, england: cambridge uni-

versity press. 266

van de geer, sara a. (2000). empirical processes in m-estimation. cambridge, england:

cambridge university press. 314

van der vaart, a. w. (1998). asymptotic statistics. cambridge, england: cambridge university

press. 147, 772

vapnik, vladimir n. (2000). the nature of statistical learning theory. berlin: springer-verlag,

2nd edn. 78

varadhan, ravi (2012). alabama: constrained nonlinear optimization. url http://cran.

r-project.org/package=alabama. r package version 2011.9-1. 754

venables, w. n. and b. d. ripley (2002). modern applied statistics with s . berlin: springer-

verlag, 4th edn. url http://www.stats.ox.ac.uk/pub/mass4. 258, 707

vidyasagar, mathukumalli (2003). learning and generalization: with applications to neural

networks. berlin: springer-verlag, 2nd edn. 78

von luxburg, ulrike and bernhard sch  olkopf (2008).    statistical learning theory: models,

concepts, and results.    arxiv:0810.4752. url http://arxiv.org/abs/0810.4752. 78

von plato, jan (1994). creating modern id203: its mathematics, physics and philosophy

in historical perspective. cambridge, england: cambridge university press. 848

vuong, quang h. (1989).

   likelihood ratio tests for model selection and non-nested
hypotheses.    econometrica, 57: 307   333. url http://www.jstor.org/pss/1912557.
doi:10.2307/1912557. 734

wahba, grace (1990). spline models for observational data. philadelphia: society for industrial

and applied mathematics. 165, 181

ward, michael d., brian d. greenhill and kristin m. bakke (2010).

   the perils of pol-
journal of peace research, 47: 363   375.

icy by p-value: predicting civil con   cts.   
doi:10.1177/0022343309356491. 623

wasserman, larry (2003). all of statistics: a concise course in statistical id136. berlin:

springer-verlag. 39, 771

704

references

    (2006). all of nonparametric statistics. berlin: springer-verlag. 39, 92, 99, 165, 172, 230,

297, 313, 314

watson, geo   rey s. (1964).    smooth regression analysis.    sanhkya, 26: 359   372. url

http://www.jstor.org/stable/25049340. 39, 99

weisberg, sanford (1985). applied id75. new york: wiley, 2nd edn. 60
welch, lloyd r. (december 2003).    shannon lecture: id48 and the baum-
welch algorithm.    ieee id205 society newsletter , 53(4): 1 and 10   13. url
http://www-bcf.usc.edu/~lototsky/math508/baum-welch.pdf. 570

western, bruce (1996).

sociological methodology, 26: 165   192.
doi:10.2307/271022. 671

   vague theory and model uncertainty in macrosociology.   
url http://www.jstor.org/stable/271022.

white, halbert (1994). estimation, id136 and speci   cation analysis. cambridge, england:

cambridge university press. 78, 205, 762, 763, 766, 772

whittaker, e. t. (1922).    on a new method of graduation.    proceedings of the edinburgh

mathematical society, 41: 63   75. doi:10.1017/s001309150000359x. 166

wickham, hadley (2011).    the split-apply-combine strategy for data analysis.    journal of

statistical software, 40(1): 1   29. url http://www.jstatsoft.org/v40/i01/. 707

    (2015). advanced r. boca raton, florida: crc press. 841
wiener, norbert (1949). extrapolation, interpolation, and smoothing of stationary time series:
with engineering applications. cambridge, massachusetts: the technology press of the
massachusetts institute of technology.    first published during the war [1942] as a classifed
report to section d2, national defense research council   . 568

    (1956).    nonlinear prediction and dynamics.    in proceedings of the third berkeley sym-
posium on mathematical statistics and id203 (jerzy neyman, ed.), vol. 3, pp. 247   252.
berkeley: university of california press. url http://projecteuclid.org/euclid.bsmsp/
1200502197. 301

    (1961). cybernetics: or, control and communication in the animal and the machine. cam-

bridge, massachusetts: mit press, 2nd edn. first edition new york: wiley, 1948. 524

winkler, gerhard (1995). image analysis, random fields and dynamic monte carlo methods:

a mathematical introduction. berlin: springer-verlag. 472

wood, simon n. (2004).    stable and e   cient multiple smoothing parameter estimation for
generalized additive models.    journal of the american statistical association, 99: 673   686.
url http://www.maths.bath.ac.uk/~sw283/simon/papers/magic.pdf. 707

    (2006). generalized additive models: an introduction with r. boca raton, florida: chapman

and hall/crc. 165, 181, 266

    (2010).    statistical id136 for noisy nonlinear ecological dynamic systems.    nature, 466:

1102   1104. doi:10.1038/nature09319. 604

wright, sewall (1934).    the method of path coe   cients.    annals of mathematical statistics,

5: 161   215. url http://projecteuclid.org/euclid.aoms/1177732676. 472

wysocki, w. (1992).    mathematical foundations of multivariate path analysis.    inventiones

mathematicae, 21: 387   397. url https://eudml.org/doc/263277. 472

xie, yihui (2015). dynamic documents with r and knitr . boca raton, florida: crc press,

2nd edn. url http://yihui.name/knitr/. 707

yates, joanne (1989). control through communication: the rise of system in american man-

agement. baltimore: johns hopkins university press. 795

ye, jianming (1998).

and model selection.   
doi:10.1080/01621459.1998.10474094. 39

   on measuring and correcting the e   ects of data mining
journal of the american statistical association, 93: 120   131.

young, alwyn (2017). consistency without id136: instrumental variables in practical ap-

plication. tech. rep., london school of economics. 500, 516

zeileis, achim (2004).    econometric computing with hc and hac covariance matrix estima-

tors.    journal of statistical software, 11(10): 1   17. doi:10.18637/jss.v011.i10. 763

references

705

    (2006).

   object-oriented computation of sandwich estimators.   

journal of statis-
tical software, 16(9): 1   16. url https://www.jstatsoft.org/article/view/v016i09.
doi:10.18637/jss.v016.i09. 763

zhang, kun, jonas peters, dominik janzing and bernhard sch  olkopf (2011).    kernel-based
conditional independence test and application in causal discovery.    in proceedings of
the twenty-seventh conference annual conference on uncertainty in arti   cial intelligence
(uai-11) (fabio gagliardi cozman and avi pfe   er, eds.), pp. 804   813. corvallis, oregon:
auai press. url http://arxiv.org/abs/1202.3775. 521

zhao, linqiao (2010). a model of limit-order book dynamics and a consistent estimation
procedure. ph.d. thesis, carnegie mellon university. url http://citeseerx.ist.psu.
edu/viewdoc/download?doi=10.1.1.173.2067&rep=rep1&type=pdf. 604

acknowledgments

i am grateful to my students in 36-402 for their feedback, and for their endur-
ing    rst (and second and third. . . ) drafts. martin gold and danny yee made
detailed and valuable comments on early versions. jim crutch   eld, chris gen-
ovese, clark glymour, kristina klinkner, valerie ventura and larry wasserman
all both shaped how i think about statistics, and o   ered vital early encourage-
ment; john miller o   ered vital early encouragement and late advice, all the while
reminding me that this was a bad idea. abigail owen made me actually    nish.

dr. ryota kanai and prof. bruce western kindly shared data sets from their
papers. for speci   c corrections and comments, i thank souhaib ben taieb, burak
bayramli, christopher bradsher, bob carpenter, kathy chen, joe chi, brad de-
long, jeremy drelich, beatriz estefania etchegaray, nj  al foldnes, manuel gar-
ber, max grazier g   sell, ben hansen, anas hoque, maksim horowitz, crystal
hou, rafael izbicki, xi jin, kent johnson, sidharth kapur, kidong    justin    kim,
maksim levental, shubao liu, terra mack, shaina mitchell, sinnott murphy,
spencer nelson, brendan o   connor, oh won-joon, nathan m. palmer, mark
t. patterson, dana peck, ariel polako   , daniel posen, akhil prakash, calvin
price, david pugh, bryn raschke, janet e. rosenbaum, timothy ruel, donald
schoolmaster, jr., howard seltman, sonia shi, navdeep sood, michael stanley,
stephanie stern, nicholas thieme, ryan tibshirani, johan ugander, michelle
wan, jerzy wieczorek, peter windridge and sasha zhang; barry dewitt and
patrick kane deserve particular thanks for suggesting improvements and correc-
tions for almost every chapter. remaining errors and infelicities are, naturally,
my fault.

while i was working on this book, my research was supported by grants from
the national science foundation (dms1207759 and dms1418124), the national
institutes of health (r01 ns047493), and the institute for new economic think-
ing (in01100005 and ino1400020). while not intended to pay for writing a text-
book, money and time are fungible, so they helped. none of those bodies, or
carnegie mellon university, are at all responsible for what i have written.

package acknowledgements

this book was written with latex and emacs; the text is set in urw garamond.
it uses the following latex packages: amsmath, amssymb, caption, datetime,
dot2texi, fancyhdr, float, graphicx, hyperref, latexsym, mathdesign, and
natbib. it also uses r (r core team, 2015a), and the following r packages:

706

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

acknowledgments

707

faraway (faraway, 2014), fnn (beygelzimer et al., 2013), foreign (r core team,
2015b), knitr (xie, 2015), lattice (sarkar, 2008), mass (venables and ripley,
2002), mgcv (wood, 2004), mixtools (benaglia et al., 2009), mvtnorm (genz
et al., 2016), np (hay   eld and racine, 2008), numderiv (gilbert and varad-
han, 2015), pcalg (kalisch et al., 2010, 2012), pdfetch (reinhart, 2014), plyr
(wickham, 2011), scatterplot3d (ligges and m  achler, 2003), smpracticals
(davison, 2013), reldist (handcock, 2015), rgraphviz, tseries (trapletti and
hornik, 2015), and tree (ripley, 2015). the book would have been much worse
without these free resources, if it existed at all, and i am grateful to their authors
for their generosity. it is thus as a small token of reciprocity that i make this
book freely accessible online.

part v

online appendices

709

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

p(cid:88)

appendix b

id202 reminders

this appendix is just for reminders and quick references, with a few exercises
as warm-ups. you need to know what vectors and matrices are, and how to do
arithmetic with them.

de   nition 1. the vectors (cid:126)v1, (cid:126)v2, . . . (cid:126)vp are linearly independent when no non-
trivial linear combination of them is 0, i.e., when

ci(cid:126)vi = 0

(b.1)

if and only if all the ci = 0.

i=1

vectors which aren   t linearly independent are linearly dependent.

   linearly    is often dropped when it   s implied by context.

de   nition 2. a vector space has dimension p when it has at least one set of p
linearly independent vectors, but every set of p + 1 vectors is linearly dependent.

b.1 inner product, norm, orthogonal vectors

de   nition 3 (inner or dot product). the inner product of on a vector space
v is a function from two vectors to the real numbers, written (cid:126)v    (cid:126)u, which satis   es
the following properties:
1. linearity: (a(cid:126)v + b (cid:126)w)    (cid:126)u = a((cid:126)v    (cid:126)u) + b( (cid:126)w    (cid:126)u)
2. symmetry: (cid:126)v    (cid:126)u = (cid:126)u    (cid:126)v
3. positive-de   niteness: (cid:126)v    (cid:126)v     0, and (cid:126)v    (cid:126)v = 0 i    (cid:126)v = (cid:126)0.

the inner product is also called the dot product (because it   s written with

  ), and also sometimes written (cid:104)(cid:126)v, (cid:126)u(cid:105).
example 1. in the vector space of p  1 column matrices, the usual inner product
is

de   nition 4. the norm of a vector is the square root of its inner product with
itself:

(cid:126)v    (cid:126)u = vt u

(cid:107)(cid:126)v(cid:107) =

   
(cid:126)v    (cid:126)v

(b.2)

(b.3)

711

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

712

id202 reminders

example 2. for p-dimensional vectors, the ordinary or euclidean norm is

(cid:118)(cid:117)(cid:117)(cid:116) p(cid:88)

(cid:107)(cid:126)v(cid:107) =

v2
i

(b.4)

this is also called the length of the vector.

i=1

we can actually de   ne multiple norms just on    nite-dimensional vector spaces,
which all obey some very natural properties, which make it sensible to call them
all    norms   . we won   t get into that here, but they are useful in sparse regression.

b.1.1 orthogonal vectors

de   nition 5. vectors are orthogonal (to each other) when their inner product
is zero:

(cid:126)v    (cid:126)u = 0

(b.5)

the reason for the name is that when we   re dealing with    nite-dimensional,

geometric vectors, there is always an angle    between them1, and

(cid:126)v    (cid:126)u = (cid:107)(cid:126)v(cid:107)(cid:107)(cid:126)u(cid:107) cos   

(b.6)

so an inner product of zero means the vectors are at right angles to each other,
i.e., orthogonal.

b.2 bases, orthonormal bases

de   nition 6 (basis, orthogonal basis, normal basis, orthonormal basis). the
vectors (cid:126)b1, . . .(cid:126)bp are a basis for their vector space, given any (cid:126)v

p(cid:88)

(cid:126)v =

(cid:126)bj

vj

j=1

for some coe   cients v1, . . . vp. the basis is orthogonal when (cid:126)bi    (cid:126)bj = 0 unless
i = j, and normal when each (cid:126)bi has norm 1. a basis which is both orthogonal
and normal is orthonormal.

in a    nite-dimensional space, the unit vectors along the di   erent coordinate
axes form an orthonormal basis, so no orthonormal basis has to have more vectors
than the space has dimensions. (a non-orthogonal basis might need more.)

proposition 1. if a basis is orthonormal, then the coe   cients vj in the expansion
are just given by inner products:

p(cid:88)

(cid:126)v =

((cid:126)v    (cid:126)bj)(cid:126)bj

1 for geometric purists: the angle between the line segment running from the origin to the coordinate

(cid:126)v, and the line segment running from the origin to (cid:126)u.

j=1

p(cid:88)

(cid:126)v    (cid:126)u =

b.3 matrices and operators

713

proposition 2. in any p-dimensional vector space, once we choose an orthonor-
mal basis, we can represent any vector by a list of p numbers, and the inner
product between vectors is the sum of the pairwise products of their coordinates,

viui

(b.7)

the choice of basis doesn   t matter:

i=1

proposition 3. the inner product between two vectors, from eq. b.7, is the
same in all bases.

b.3 matrices and operators

de   nition 7. a (linear) operator a is a vector-valued linear function of a
vector, i.e., a function where, for any two vectors (cid:126)v, (cid:126)u     v, and any two scalars
c, d,

a(c(cid:126)v + d(cid:126)u) = ca((cid:126)v) + da((cid:126)u)

(b.8)
proposition 4. in the vector space of p   1 column matrices, left-multiplying by
a p    p matrix a de   nes a linear operator, (cid:126)v (cid:55)    a(cid:126)v.
proposition 5. in any p-dimensional vector space, once we choose an orthonor-
mal basis, any linear operator can be represented by a p    p matrix.

proof: say the basis vectors are (cid:126)b1, . . .(cid:126)bp. de   ne the matrix through

aij = (cid:126)bi    (a((cid:126)bj)

(b.9)
now pick any arbitrary vector (cid:126)v. in this basis, it is represented by a p   1 matrix
j=1 vja((cid:126)bj), which will be rep-
(cid:126)bi    (a((cid:126)bj)vj. but this is the

with entries vj = (cid:126)bj    (cid:126)v. by linearity, a((cid:126)v) = (cid:80)p
resented by a column matrix whose entries are (cid:80)

j

same as a(cid:126)v. 2

b.4 rank

de   nition 8 (rank, full rank, rank de   cient, range space). the column rank
of a p    q matrix a is the number of linearly independent columns of a. if all
the columns are linearly independent, then a has full (column) rank, otherwise it
is (column) rank de   cient. column rank equals the dimension of the (column)
range space or range of a, the linear subspace of vectors p-vectors of the form
a(cid:126)v. the row rank, similarly, is the number of linearly independent rows of a,
and the de   nitions of full row rank and row rank de   ciency are parallel.

proposition 6. the row rank and column rank of a matrix are always equal.

de   nition 9. the (column) null space of a is the set of all vectors such that
a(cid:126)v = 0.

714

id202 reminders

exercise 3 proves that the null space is, in fact, a linear subspace.

proposition 7. for a q-column matrix, the sum of the column rank and the
dimension of the null space is always q; for a p-row matrix, the sum of the row
rank and the dimension of the row null space is p.

b.5 eigenvalues and eigenvectors of matrices

de   nition 10 (eigenvalue, eigenvector, spectrum, leading eigen). a non-zero
vector (cid:126)v is an eigenvector of an operator a, with eigenvalue   , when

a(cid:126)v =   (cid:126)v

(b.10)

an operator   s spectrum is its set of eigenvalues. if one puts the eigenvalues in
order, the largest one is the leading eigenvalue, and the corresponding eigenvec-
tor is also called the leading eigenvector.

since matrices represent operators on    nite-dimensional spaces, we uses these

same de   nitions for the eigenvectors, eigenvalues, and spectra of matrices.

note: in general, eigenvalues may be complex numbers, not just real numbers.
in this book, the places where we need eigenvalues are ones where the eigenvalues
will be real, but complex eigenvalues are important in many other applications.

proposition 8. if (cid:126)v is an eigenvector with eigenvalue   , then so is b(cid:126)v, for any
b.

because of this, it   s usual to normalize the eigenvector to have norm 1. (some-

times we normalize to have its entries sum to 1.)
proposition 9 (number of eigenvalues). an p  p matrix a has at most p distinct
eigenvalues.

if a matrix has fewer than p distinct eigenvalues, the    repeated    ones are
degenerate. if    is k-fold degenerate, or has multiplicity k, then there is
k   -dimensional linear subspace of eigenvectors with eigenvalue    (exercise 8).
a matrix with no repeated eigenvalues is non-degenerate. example: the p    p
identity matrix has the unique eigenvalue 1, which is p-fold degenerate; all vectors
are eigenvectors of the identity matrix.

de   nition 11. the trace of a square matrix, tr a, is the sum of its diagonal
entries:

proposition 10. 1. trace is a linear operator on matrices, tr (ca + db) = c tr a+

d tr b.

i=1

aii

(b.11)

tr a     p(cid:88)

det(a)     |a|     p(cid:89)

b.6 special kinds of matrix

715

2. the trace of matrix equals the sum of its eigenvalues,    counted with mutliplic-

ity   :

tr a =

p(cid:88)

  i

(b.12)

3. the trace of a matrix product doesn   t care about the order of multiplication:

i=1

(b.13)
de   nition 12. the determinant of a, det a or |a|, is the product of its eigen-
values (again,    counted with multiplicity   ):

tr (ab) = tr (ba)

  i

(b.14)

i=1

(there are, of course, other, equivalent ways to de   ne the determinant.)

proposition 11. a square matrix is invertible if and only if |a| (cid:54)= 0, i.e., if and
only if all of its eigenvalues are non-zero.
proposition 12. the rank of an p    p matrix is p minus the eigenvalue multi-
plicity of 0.

proof: exercise 9.

proposition 13. a square matrix is invertible if and only if it is of full rank.

proof: follows from the previous two propositions. 2

proposition 14. the eigenvalues of ak, for positive integer k, are the powers
of the eigenvalues of a; the eigenvectors of ak are the eigenvectors of a. the
eigenvalues of a   1 (when that matrix exists) are the reciprocals of the eigenvalues
of a.

de   nition 13. a matrix a is normal when at a = aat .

b.6 special kinds of matrix

proposition 15. all of the eigenvectors of a normal matrix are (or can be chosen
to be) orthogonal to each other.

de   nition 14. a square matrix is symmetric when it is equal to its own trans-
pose, at = a.

proposition 16. every symmetric matrix is normal (in the sense of de   nition
13).

proposition 17. for a symmetric matrix, all of the eigenvalues are real (not
complex) numbers, and all of the eigenvectors can be chosen to be orthogonal to
each other.

716

id202 reminders

de   nition 15. a square matrix is positive semi-de   nite when, for all non-
zero vectors (cid:126)v, (cid:126)v    a(cid:126)v     0. this is often abbreviated a (cid:23) 0. if (cid:126)v    a(cid:126)v > 0, then the
matrix is positive de   nite, a (cid:31) 0.
proposition 18. all the eigenvalues of a positive semi-de   nite matrix are     0;
all the eigenvalues of a positive de   nite matrix are > 0.

negative de   nite matrices are de   ned similarly, and have negative eigenvalues,

but have fewer uses in statistics.

de   nition 16. a matrix is orthogonal when it is inverse is the same as its
transpose, ot = o   1.

proposition 19. o is orthogonal if and only if its rows are a set of orthonormal
vectors, which happens if and only if its columns are orthonormal as well.2

proposition 20. 1. all eigenvalues of an orthogonal matrix have magnitude 1.
2. since ot o = i, if bt b = a, then (ob)t (ob) = a.

b.7 eigendecomposition

proposition 21 (eigendecomposition or spectral decomposition). suppose a is
a square, p    p matrix, and its eigenvectors are linearly independent. then

(b.15)
where v is the p   p matrix whose columns are the eigenvectors of a, and d is the
p    p diagonal matrix of a   s eigenvalues.

a = vdv   1

proof: exercise ??. 2

corollary 1. if the eigenvectors of a are linearly independent, then

a   1 = vd   1v

proof: a   1 = (vdv   1)

   1, by the proposition. but this is just (v   1)

and the corollary follows. 2

(b.16)
   1d   1v   1,

corollary 2. if the eigenvectors of a are linearly independent, and, in addition,
a is normal (de   nition 13), then v is orthogonal (de   nition 16), and

a = vdvt

(b.17)

remark: remember that every symmetric matrix is normal, so the corollary

applies to symmetric matrices.

2 on the basis of this proposition, it might have been better to call these matrices    orthonormal   

rather than    orthogonal   ; i haven   t been able to work out where the name comes from.

b.8 orthogonal projections, idempotent matrices

717

b.7.1 singular value decomposition

the spectral decomposition (proposition ??) shows how to break up nice, square
matrices into products of their eigenvalues and eigenvectors. it turns out that
any matrix can be broken up into a product of eigenvalues and eigenvectors of
related matrices.
proposition 22 (singular value decomposition). an n    m matrix b has as its
singular value decomposition

b = udvt

(b.18)
where u, the n   n matrix of left singular vectors, contains the eigenvectors of
bbt ; v, the m   m matrix of right singular vectors, contains the eigenvectors
of bt b; and d is an n    m diagonal matrix of singular values, containing the
square roots of the non-zero eigenvalues of bbt (which are also the eigenvalues
of bt b).

in the special case of a square, n    n matrix, all of the matrices in the svd
are also n    n, but don   t much simplify further. however, one can interpret the
terms more easily: multiplying by vt rotates a vector to a new set of coordinate
axes, multiplying by d stretches the vector along the axes (and possibly re   ects
along some of them), and then multiplying by u does a    nal rotation to new
coordinates.

proposition 23. if b is symmetric as well as square, then bbt = bt b, and
the matrices u and v are identical. moreover, d is simply the diagonal matrix of
eigenvalues of b.

b.7.2 square root of a matrix

any matrix b such that bt b = a is a square root of a symmetric matrix a.
there are in   nitely many of them3 but a fairly straightforward one can be de   ned
through the eigendecomposition:

a1/2 = d1/2v

(b.19)

where d1/2 takes the square root of each element of d, the diagonal matrix of
eigenvalues of a.

b.8 orthogonal projections, idempotent matrices

de   nition 17. the projection of a vector (cid:126)v on to a set s is the point in the
set which comes closest to (cid:126)v.

de   nition 18. if s is a q < p dimensional linear sub-space, we can    nd the

3 because, for any orthogonal matrix c, bt b = bt ot ob = (ob)t (ob).

718

id202 reminders

projection by using an orthonormal basis for that subspace:

q(cid:88)

(cid:126)v(cid:107) =

((cid:126)v    (cid:126)sj)(cid:126)sj

(b.20)

this is the orthogonal projection of (cid:126)v on to s.

j=1

proposition 24. any vector has a unique decomposition into its projection on
to the subspace and a residual (cid:126)v = (cid:126)v(cid:107) + (cid:126)v   . the residual (cid:126)v    is orthogonal to the
subspace, i.e., orthogonal to every vector in the subspace.

de   nition 19. an operator (or matrix) p is idempotent when its powers are
equal to itself, i.e., when pk = p for all integer k > 1.

proposition 25. every orthogonal project is idempotent.

geometrically, this means that once a vector has been projected into a sub-

space, projecting into the same space again does nothing.

proposition 26. all eigenvalues of an idempotent matrix are either 0 or 1, hence
its rank is equal to its trace.

proof: exercise 10. 2
this fact is surprisingly useful for linear smoothers (  1.5.3).

b.9 r commands for id202

for id127, use the %*% operator; if one of its arguments is an r
vector, it will (sometimes) try to convert it to an appropriate matrix, but you   re
probably better o    doing that explicitly yourself.

t() transposes, det() and determinant calculate the determinant.
if a is an existing matrix, diag(a) can be used to extract or set the entries on
its diagonal. similarly, lower.tri() and upper.tri can extract or set the lower or
upper triangular parts of a matrix.
integer k creates a k    k identity matrix.

if v is a vector, diag(v) creates the corresponding diagonal matrix. diag(k) for

there is, oddly, no built-in function for calculating a trace, but it   s easy to

write one.

for a matrix a and a vector b, solve(a,b) solves the linear system a(cid:126)x = (cid:126)b.

with just a matrix, a    nds a   1.

eigen(a) returns a list containing the eigenvalues and (normalized) eigenvectors

of a. similarly, svd(a)    nds the singular value decomposition.

                  

                  

   f
   x1
   f

   x2...

   f
   xp

   f =

b.10 vector calculus

719

b.10 vector calculus

de   nition 20. the gradient of a scalar-valued function is the vector-valued
function of its    rst partial derivatives:

(b.21)

   f is a vector-valued function;    f (x) is a vector.

at any point x,    f (x) is the direction in which the function f increases most
rapidly. more exactly, the unit vector    f (x)/(cid:107)   f (x)(cid:107) gives the direction of
   steepest ascent   , and (cid:107)   f (x)(cid:107) is the slope in that direction.       f (x) likewise is
the direction of steepest descent.

the gradient itself changes; these changes show up in the second derivatives of

f .

de   nition 21. the hessian matrix hf of a scalar-valued function f is the
matrix of its second partial derivatives,

hijf =

   2f

   xi   xj

because    2f /   xi   xj =    2f /   xj   xi, the hessian is always a symmetric matrix.
some alternative notations are    2f ,       f , and          f , which all indicate the
idea of taking derivatives of the gradient    f .

note: there is a lot of disagreement about whether    the hessian    means this
matrix, or the determinant of this matrix. i have tried to consistently refer to the
   hessian matrix    to avoid this ambiguity.

in many situations, we do not need all of the second derivatives, but just the

sum of the second derivatives along the coordinates.

de   nition 22. let f be a scalar function of a p-dimensional vector space. its
laplacian is the sum of its second partial derivatives along each coordinate axis:

p(cid:88)

   f =

   2f

   xi   xi

(b.22)

alternate notations are    2f and           f .

i=1

notice that    f = tr hf . while the hessian matrix hf is obviously coordinate-
dependent, one can show that    f is not. the laplacian is centrally important
to di   usion, and used extensively in nonlinear id84 (chapter
18).

when we transform scalars into scalars, we often need to keep track of how
lengths get altered. if we apply a function f , a small region of length dx around
x gets transformed into a region of length |df /dx|dx around f (x). when we

720

id202 reminders

transform vectors into vectors, we need to keep track of how volumes transform,
and this job is done a determinant.

de   nition 23. let f be a mapping from a p-dimensional vector space to a q-
dimensional vector space. the jacobian of f is the determinant of its matrix of
partial derivatives:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

jf =

   f1
   x1
   f2
   x1

...

   f1
   x2
   f2
   x2

...

partialfq

   x1

   fq
   x2

. . .

   f1
   xp
   f2
   xp

...

. . .
. . .
. . .

   fq
   xp

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(b.23)

spaces of functions can often be treated as linear vector spaces. the usual de   -
nition of inner product is

b.11 function spaces

(cid:90)

(cid:104)f, g(cid:105) =

f (x)g(x)dx

(cid:90)

f 2(x)dx

this leads to a norm for functions,
(cid:107)f(cid:107)2

2 =

functions where (cid:107)f(cid:107)2 <     are square-integrable, and the space of square-
integrable functions is called l2 (or sometimes l2). (exercise: suggest a de   nition
of lp, without looking it up.) sometimes it is convenient to introduce a    base   
or    reference    density   , and to modify the de   nition of inner product to

(cid:90)

(cid:104)f, g(cid:105) =

and

(cid:107)f(cid:107)2

2 =

(cid:90)

f (x)g(x)  (x)dx

f 2(x)  (x)dx

the space of function is then called l2(  ). the inner product is linear in both f
and g, with or without the factor of   .

similarly, one can de   ne inner products and square-integrability for functions
0 f 2(x)dx, or combine a domain restriction with a

on a restricted domain, e.g.,(cid:82) 1

non-uniform reference distribution.

b.11.1 bases

because (cid:104)f, g(cid:105) acts like an ordinary inner product, lots of what   s familiar from
vector spaces carries over to l2. in particular, it makes sense to speak of a se-
quence of functions   1,   2, . . . being a basis, and even of its being an orthonormal

b.12 further reading

721

basis, in which case

   (cid:88)

j=1

f =

(cid:104)f,   j(cid:105)  j

notice that the mononomials 1, x, x2, . . . are a basis for l2 on [0, 1] and on
r, but they are neither orthogonal nor normalized. many families of orthogonal
polynomials exist; the most straightforward begin by taking   1(x) = 1, and then
making   k the (k + 1)-degree polynomial which is orthogonal to all previous
functions in the series.

another basis for l2 on [0, 1] are the sines and cosines, which in this notation
we may write as   1 = 1,   2k(x) = sin 2k  x,   2k+1 = cos 2k  x. this is the fourier
basis, and the coe   cients in this expansion are the fourier coe   cients or fourier
transform of the original function.

f into(cid:82) x

b.11.2 eigenvalues and eigenfunctions of operators

an operator o is a a higher-order function, something which maps functions
to other functions. you know two operators from calculus: taking a derivative
transforms one function, f , into another, df /dx. likewise integration transforms

u=       f (u)du, a function of x.

operators can have eigenvalues and eigenfunctions, just as matrices have eigen-

values and eigenvectors:

of =   f

you know examples of this, too, from calculus: eax is an eigenfunction of both
di   erentiation and integration. (what are the eigenvalues?)

as the last example suggests, in general operators on function spaces have

in   nitely many eigenvalues and eigenvectors.

b.12 further reading

there are many, many decent references on id202, often as part of a
more general reference on mathematical methods, e.g., boas (1983). axler (1996)
is notable for presenting the whole subject    from the ground up   , but at the
same time from the abstract perspective characteristic of modern mathematics
(as opposed to sheer calculational procedures).

b.13 exercises

1. prove that for any (cid:126)v, 0(cid:126)v = (cid:126)0.
2. prove that the span of of any set of vectors is a linear subspace.
3. prove that the null space of a matrix a, i.e., the set of all vectors where a(cid:126)v = 0,
4. suppose that a is symmetric, that a(cid:126)u =   1(cid:126)u, a(cid:126)v =   2(cid:126)v, and   1 (cid:54)=   2. show

is in fact a linear subspace, by verifying every component of the de   nition.
that (cid:126)u    (cid:126)v = 0. why does your proof need a to be symmetric?

722

id202 reminders

5. assume the notation of proposition ??
1. show that, for any square matrix a,

(b.24)
2. show that if the eigenvectors of a are all linearly independent, then v   1

av = vd

exists.

3. conclude that, when the eigenvectors are linearly independent, a = vdv   1.
4. can v   1 exist if the eigenvectors of a are linearly dependent?

6. show that (cid:126)v(cid:107), as de   ned in eq. b.20, is also

(cid:107)(cid:126)v     q(cid:88)

j=1

cj(cid:126)sj(cid:107)

argmin
c1,c2,...cq

7. show that (cid:126)v    = (cid:126)v     (cid:126)v(cid:107) is orthogonal to (i.e., has inner product zero with) any

vector of the form(cid:80)q

j=1 bj(cid:126)sj.

8. let    be an eigenvector of a matrix a with multiplicity k. show that the set
of eigenvectors of a with eigenvalue    forms a linear subspace of dimension k.
hints: start with the k = 1 case; use exercise 16.2.
9. let a be an arbitrary p    p matrix. show that its rank equal to p minus the
eigenvalue multiplicity of 0. hint: first, show that the dimension of a   s null
space must be     the multiplicity of 0; then (slightly harder) that the dimension
of the null space must be     the number of zero eigenvalues.

10. show the following:

1. all eigenvalues of an idempotent matrix are either 0 or 1. hint: try proof
by contradiction: suppose there were some other eigenvalue   , and show
that this con   icts with idempotency.

2. the rank of an idempotent matrix equals its trace.
more challenging: is it true that if each eigenvalue of a matrix is either 0 or
1, the matrix must be idempotent? (does it matter whether the eigenvectors
form a basis?)

to show that all its eigenvalues are > 0.

11. let a be a symmetric, positive-de   nite matrix. use the specrral decomposition
12. show that (cid:104)f, g(cid:105), as de   ned in   b.11, is linear in both f and g.
13. 1. show that for any square matrix a, av = dv, where d and v are de   ned as

in the spectral decomposition (proposition ??).

2. show that a = vdv   1, when v   1 exists.
3. show that v   1 exists if, and only if, the eigenvectors of a are all linearly

independent.

appendix c

big o and little o notation

it is often useful to talk about the rate at which some function changes as its
argument grows (or shrinks), without worrying to much about the detailed form.
this is what the o(  ) and o(  ) notation lets us do.

a function f (n) is    of constant order   , or    of order 1    when there exists some

non-zero constant c such that

f (n)

    1

(c.1)
as n        ; equivalently, since c is a constant, f (n)     c as n        . it doesn   t
matter how big or how small c is, just so long as there is some such constant. we
then write

c

f (n) = o(1)

(c.2)

and say that    the proportionality constant c gets absorbed into the big o   .
for example, if f (n) = 37, then f (n) = o(1). but if g(n) = 37(1     2
n ), then
g(n) = o(1) also.

the other orders are de   ned recursively. saying

means

or

g(n) = o(f (n))

g(n)
f (n)

= o(1)

(c.3)

(c.4)

g(n)
f (n)

    c

(c.5)
as n             that is to say, g(n) is    of the same order    as f (n), and they    grow at
the same rate   , or    shrink at the same rate   . for example, a quadratic function
a1n2 + a2n + a3 = o(n2), no matter what the coe   cients are. on the other hand,
b1n   2 + b2n   1 is o(n   1).
ultimately smaller than   : f (n) = o(1) means that f (n)/c     0 for any constant
c. recursively, g(n) = o(f (n)) means g(n)/f (n) = o(1), or g(n)/f (n)     0. we
also read g(n) = o(f (n)) as    g(n) is ultimately negligible compared to f (n)   .

big-o means    is of the same order as   . the corresponding little-o means    is [[attn:

there are some rules for arithmetic with big-o symbols:

723

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

in to
same
as   
   is
most
order

get
   of
order
vs.
at
the
of   ?]]

big o and little o notation

724
    if g(n) = o(f (n)), then cg(n) = o(f (n)) for any constant c.
    if g1(n) and g2(n) are both o(f (n)), then so is g1(n) + g2(n).
    if g1(n) = o(f (n)) but g2(n) = o(f (n)), then g1(n) + g2(n) = o(f (n)).
    if g(n) = o(f (n)), and f (n) = o(h(n)), then g(n) = o(h(n)).
these are not all of the rules, but they   re enough for most purposes.

appendix d

taylor expansions

as you know, the    rst derivative of a function f at a point x0 is the slope of the
line tangent to the curve of f at x0, which is the limit of slopes taken through
the curve at near-by points:

f(cid:48)(x0)     lim
x   x0

f (x)     f (x0)

x     x0

this suggests that if x     x0, we should have

f (x)     f (x0) + (x     x0)f(cid:48)(x0)

(d.1)

(d.2)

the idea of a taylor series is to make this suggestion concrete, and to deal with
higher derivatives.

de   nition 24 (taylor series (one-dimensional)). for a real-valued function of
one real argument, the taylor series (or    expansion   ) of order k at (or
   around   ) x0 approximates f (x) by

f (x)     k(cid:88)

i=0

f (i)(x0)

(x     x0)i

i!

(d.3)

where f (i)(x0) is the ith derivative of f at x0. (we presume derivatives of at least
order k exist at x0.) the complete taylor series is obtained by setting k =    . a
function whose (complete) taylor series converges everywhere is called analytic.
the taylor series approximation will become more and more accurate as x    
x0. intuitively, the magnitude of the error involved should depend both on how
far x is from the point x0 we   re expanding around, and on the magnitude of the
higher-order derivatives we   re ignoring. (a    rst-order taylor series would be exact
for a linear function; if the function is non-linear but curved, it will be better, at
a given from x0, the less curvature f has.) there is in fact a theoretical bound
on the approximation error1:

proposition 27. suppose we do a kth-order taylor series around x0. then there

1 this result is useful when trying to decide to what order a taylor expansion needs to be carried out,
or when one needs to prove one   s scienti   c bona    des to an anarchist cossak militia (gamow, 1970,
pp. 19   20).

725

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

taylor expansions

726
is a point x(cid:48), between x and x0, such that
(x     x0)i

f (x)     k(cid:88)

f (i)(x0)

i=0

f (k+1)(x(cid:48))
(k + 1)!

=

(x     x0)k+1

(d.4)

i!

consequently, if f (k+1)(x) is bounded, then the error of a kth order taylor ap-
proximation is o((x     x0)k+1).

it   s nice to use taylor series for functions of multiple arguments. the second-
order taylor expansion for a real-valued function f of a vector (cid:126)x around the point
(cid:126)x0 is

f ((cid:126)x)     f ((cid:126)x0) + ((cid:126)x     (cid:126)x0)       f ((cid:126)x0) +

((cid:126)x     (cid:126)x0)    h((cid:126)x0)((cid:126)x     (cid:126)x0)

1
2

(d.5)

with h being the hessian matrix, the matrix of second partial derivatives of f .
if the third derivatives are bounded, the approximation error is o((cid:107)vecx    (cid:126)x0(cid:107)3).
higher-order multivariate taylor expansions won   t be needed in this book, but
you can    nd them in good calculus textbooks, if you need them.

references/further reading

because taylor series are such a basic tool, you can    nd extensive treatments in
almost any good book on calculus or on mathematical methods. i recommend
boas (1983), but that   s because it   s what i used as a student.

exercises

1+x     1     x for |x| (cid:28) 1.

1

1. show that
2. show that (1 + x)k     1 + kx for |x| (cid:28) 1.
3. (everything looks quadratic near the optimum) suppose that f has a local
minimum or maximum at x0. find the 2nd order taylor expansion around x0.
(you may assume the curvature at x0 is non-zero.) where is the extremum of
the approximation?

4. (newton   s method) suppose that f has a local minimum or maximum at x0,
but that we taylor-expand f to second order around another point x, which
is not x0 but is close to it. find the extremum of the approximation. can you
say when this will be closer to x0 than the initial expansion point x was?

a

[[todo:
replace
with
copyright-
pic-
free
ture,
or
get permis-
sion]]

taylor expansions

727

figure d.1 sound advice for almost any problem in statistical theory.

appendix e

multivariate distributions

e.1 review of de   nitions

let   s review some de   nitions from basic id203. when we have a random
vector (cid:126)x with p di   erent components, x1, x2, . . . xp, the joint cumulative dis-
tribution function is

f ((cid:126)a) = f (a1, a2, . . . ap) = pr (x1     a1, x2     a2, . . . xp     ap)

thus

f ((cid:126)b)     f ((cid:126)a) = pr (a1 < x1     b1, a2 < x2     b2, . . . ap < xp     bp)

(e.1)

(e.2)

this is the id203 that x is in a (hyper-)rectangle, rather than just in an
interval.

the joint id203 density function is

f ((cid:126)x) = f (x1, x2, . . . xp) =

(e.3)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)a=(cid:126)x

   pf (a1, . . . ap)

   a1 . . .    ap

of course,

(cid:90) a1

(cid:90) a2

      

      

(cid:90) ap

      

. . .

f ((cid:126)a) =

f (x1, x2, . . . xp)dxp . . . dx2dx1

(e.4)

(in this case, the order of integration doesn   t matter. why?)

from these, and especially from the joint pdf, we can recover the marginal

pdf of any group of variables, say those numbered 1 through q,

f (x1, x2, . . . xq) =

f (x1, x2, . . . xp)dxq+1dxq+2 . . . dxp

(e.5)

(what are the limits of integration here?) then the conditional pdf for some
variables given the others     say, use variables 1 through q to condition those
numbered q + 1 through p     just comes from division:
f (xq+1, xq+2, . . . xp|x1 = x1, . . . xq = xq) =

(e.6)

f (x1, x2, . . . xp)
f (x1, x2, . . . xq)

these two tricks can be iterated, so, for instance,

(cid:90)

(cid:90)

f (x3|x1) =

f (x3, x2|x1)dx2

728

(e.7)

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

e.2 multivariate gaussians

729

e.2 multivariate gaussians

the multivariate gaussian is just the generalization of the ordinary gaussian to
vectors. scalar gaussians are parameterized by a mean    and a variance   2, so
we write x     n (  ,   2). multivariate gaussians, likewise, are parameterized by a
mean vector (cid:126)  , and a variance-covariance matrix   , written (cid:126)x     mvn ((cid:126)  ,   ).
the components of (cid:126)   are the means of the di   erent components of (cid:126)x. the i, jth
component of    is the covariance between xi and xj (so the diagonal of    gives
the component variances).

just as the id203 density of scalar gaussian is

(cid:27)

(x       )2

  2

    1
2

(cid:26)

f (x) =(cid:0)2    2(cid:1)   1/2 exp
(cid:26)

   p/2 exp

f ((cid:126)x) = (2   det   )

((cid:126)x     (cid:126)  )         1((cid:126)x     (cid:126)  )

    1
2

(cid:27)

(e.8)

(e.9)

the id203 density of the multivariate gaussian is

finally, remember that the parameters of a gaussian change along with linear
transformations

x     n (  ,   2)     ax + b     n (a   + b, a2  2)

(e.10)

and we can use this to    standardize    any gaussian to having mean 0 and variance
1 (by looking at x     

   ). likewise, if

then

(cid:126)x     mvn ((cid:126)  ,   )

a (cid:126)x + (cid:126)b     mvn (a(cid:126)   + (cid:126)b, a  at )

(e.11)

(e.12)

the analogy between univariate and multivariate gaussians is so complete that
it is common to not really distinguish the two, and write n for both.

the multivariate gaussian density is most easily visualized when p = 2, as in
figure e.1. the id203 contours are ellipses1. the density changes compar-
atively slowly along the major axis, and quickly along the minor axis. the two
points marked + in the    gure have equal geometric distance from (cid:126)  , but the one
to its right lies on a higher id203 contour than the one above it, because of
the directions of their displacements from the mean.

e.2.1 id202 and the covariance matrix

we can use some facts from id202 to understand the general pattern here,
for arbitrary multivariate gaussians in an arbitrary number of dimensions. the

1 recall that a circle consists of all points at a constant distance, the radius, from a single point, the
center. an ellipse likewise de   ned by two points, the foci, and consists of all points where the sum
of the distance from the foci is constant. the major axis of the ellipse is the line linking the foci; the
minor axis is the line perpendicular to the major axis, through the mid-point between the foci.

730

multivariate distributions

library(mvtnorm)
x.points <- seq(-3,3,length.out=100)
y.points <- x.points
z <- matrix(0,nrow=100,ncol=100)
mu <- c(1,1)
sigma <- matrix(c(2,1,1,1),nrow=2)
for (i in 1:100) {

for (j in 1:100) {

z[i,j] <- dmvnorm(c(x.points[i],y.points[j]),mean=mu,sigma=sigma)

}

}
contour(x.points,y.points,z)
points(mu[1], mu[2], pch=16)

figure e.1 id203 density contours for a two-dimensional multivariate

(= bivariate) gaussian, with mean (cid:126)   =

(solid dot), and variance

matrix    =

. using expand.grid, as in chapter 4, would be more

elegant coding than this double for loop.

(cid:18) 2

1

(cid:19)

1
1

(cid:19)

(cid:18) 1

1

 0.02  0.04  0.06  0.08  0.1  0.12  0.14    3   2   10123   3   2   10123le.2 multivariate gaussians

731

covariance matrix    is symmetric and positive-de   nite, so we know from linear
algebra that it can be written in terms of its eigenvalues and eigenvectors:

   = vt dv

(e.13)

where d is the diagonal matrix of the eigenvalues of   , and v is the matrix whose
columns are the eigenvectors of   . because the eigenvectors are all of length 1,
and they are all perpendicular to each other, it is easy to check that vt v = i, so
v   1 = vt and v is an orthogonal matrix. what actually shows up in the equation
for the multivariate gaussian density is      1, which is

(vt dv)   1 = v   1d   1(cid:0)vt(cid:1)   1

= vt d   1v

(e.14)

geometrically, orthogonal matrices represent rotations. multiplying by v ro-
tates the coordinate axes so that they are parallel to the eigenvectors of   .
probabilistically, this tells us that the axes of the id203-contour ellipse are
parallel to those eigenvectors. the radii of those axes are proportional to the
square roots of the eigenvalues. to see that, look carefully at the math. fix a
level for the id203 density whose contour we want, say f0. then we have

(cid:27)

(cid:26)

    1
2

   p/2 exp

((cid:126)x     (cid:126)  )         1((cid:126)x     (cid:126)  )

f0 = (2   det   )
c = ((cid:126)x     (cid:126)  )         1((cid:126)x     (cid:126)  )
= ((cid:126)x     (cid:126)  )t vt d   1v((cid:126)x     (cid:126)  )
= ((cid:126)x     (cid:126)  )t vt d   1/2d   1/2v((cid:126)x     (cid:126)  )

(cid:17)t(cid:16)
(cid:16)
(cid:13)(cid:13)(cid:13)d   1/2v((cid:126)x     (cid:126)  )
(cid:13)(cid:13)(cid:13)2

d   1/2v((cid:126)x     (cid:126)  )

=

d   1/2v((cid:126)x     (cid:126)  )

(cid:17)

(e.15)

(e.16)
(e.17)
(e.18)

(e.19)

=

(e.20)
where c combines f0 and all the other constant factors, and d   1/2 is the diagonal
matrix whose entries are one over the square roots of the eigenvalues of   . the
v((cid:126)x     (cid:126)  ) term takes the displacement of (cid:126)x from the mean, (cid:126)  , and replaces the
components of that vector with its projection on to the eigenvectors. multiplying
by d   1/2 then scales those projections, and so the radii have to be proportional
to the square roots of the eigenvalues.2

e.2.2 conditional distributions and least squares

suppose that (cid:126)x is bivariate gaussian, so p = 2, with mean vector (cid:126)mu = (  1,   2),

and variance matrix

. one can show (exercise 5) that the conditional

(cid:20)   11   12

  21   22

(cid:21)

2 if you know about principal components analysis and think that all this manipulation of

eigenvectors and eigenvalues of the covariance matrix seems familiar, you   re right; this was one of
the ways in which pca was originally discovered. but pca does not require any distributional
assumptions. if you do not know about pca, read chapter 16.

(cid:20)   aa   ab

  ba   bb

(cid:21)

732

multivariate distributions

distribution of x2 given x1 is gaussian, and in fact

x2|x1 = x1     n (  2 +   21     1

11 (x1       1),   22       21     1

11   12)

(e.21)

to understand what is going on here, remember from chapter 1 that the optimal
slope for linearly regressing x2 on x1 would be cov [x2, x1] /v [x1]. this is
precisely the same as   21     1
11 . so in the bivariate gaussian case, the best linear
regression and the optimal regression are exactly the same     there is no need to
consider nonid75s. moreover, we get the same conditional variance for
each value of x1, so the regression of x2 on x1 is homoskedastic, with independent
gaussian noise. this is, in short, exactly the situation which all the standard
regression formulas aim at.

more generally, if x1, x2, . . . xp are multivariate gaussian, then conditioning
on x1, . . . xq gives the remaining variables xq+1, . . . xp a gaussian distribution

as well. if we say that (cid:126)   = ((cid:126)  a, (cid:126)  b) and    =

, where a stands

for the conditioning variables and b for the conditioned, then
(cid:126)xb| (cid:126)xa = (cid:126)xa     mvn ((cid:126)  b +   ba     1
(remember that here   ba =   t
regression of (cid:126)xb on (cid:126)xa.

aa((cid:126)xa     (cid:126)  a),   bb       ba     1

ab [why?].) this, too, is just doing a linear

aa  ab) (e.22)

e.2.3 projections of multivariate gaussians

a useful fact about multivariate gaussians is that all their univariate projections
are also gaussian. that is, if (cid:126)x     mvn ((cid:126)  ,   ), and we    x any unit vector (cid:126)w,
then (cid:126)w    (cid:126)x has a gaussian distribution. this is easy to see if    is diagonal: then
(cid:126)w    (cid:126)x reduces to a sum of independent gaussians, which we know from basic
id203 is also gaussian. but we can use the eigen-decomposition of    to
check that this holds more generally.
one can also show that the converse is true: if (cid:126)w    (cid:126)x is a univariate gaussian
for every choice of (cid:126)w, then (cid:126)x must be multivariate gaussian. this fact is more
useful for id203 theory than for data analysis3, but it   s still worth knowing.

e.2.4 computing with multivariate gaussians

computationally, it is not hard to write functions to calculate the multivariate
gaussian density, or to generate multivariate gaussian random vectors. unfortu-
nately, no one seems to have thought to put a standard set of such functions in
the basic set of r packages, so you have to use a di   erent library. the mass li-
brary contains a function, mvrnorm, for generating multivariate gaussian random
vectors. the mvtnorm contains functions for calculating the density, cumulative

3 it   s a special case of a result called the cram  er-wold theorem, or the cram  er-wold device,

which asserts that two random vectors (cid:126)x and (cid:126)y have the same distribution if and only if (cid:126)w    (cid:126)x and
(cid:126)w    (cid:126)y have the same distribution for every (cid:126)w.

e.3 id136 with multivariate distributions

733

distribution and quantiles of the multivariate gaussian, as well as generating
random vectors4. the package mixtools, which will use in chapter 19 for mix-
ture models, includes functions for the multivariate gaussian density and for
random-vector generation.

e.3 id136 with multivariate distributions

as with univariate distributions, there are several ways of doing statistical in-
ference for multivariate distributions. here i will focus on parametric id136,
since non-parametric id136 is covered in chapter 14.

parameter estimation by maximum likelihood, the sampling distribution of the
id113, and the resulting hypothesis tests and con   dence sets work exactly as they
do for one-dimensional distributions. that is to say, they are special cases of
general results about estimation by minimizing a id168, described in app.
h.5.

e.3.1 model comparison

out of sample, models can be compared on log-likelihood. when a strict out-of-
sample comparison is not possible, we can use cross-validation.

in sample, a likelihood ratio test can be used. this has two forms, depending
on the relationship between the models. suppose that there is a large or wide
model, with parameter   , and a narrow or small model, with parameter   , which
we get by    xing some of the components of   . thus the dimension of    is q and
that of    is r < q. since every distribution we can get from the narrow model
we can also get from the wide model, in-sample the likelihood of the wide model
must always be larger. thus

here we have a clear null hypothesis, which is that the data comes from the
narrower, smaller model. under this null hypothesis, as n        ,

(e.23)

(e.24)

provided that the restriction imposed by the small model doesn   t place it on the
boundary of the parameter space of   . (see appendix i.)

for instance, suppose that (cid:126)x is bivariate, and the larger model is an unre-

stricted gaussian, so    =

. a possible narrow model

(cid:26)

(cid:20)   11

0
0   22

might impose the assumption that the components of (cid:126)x are uncorrelated, so

   =

(  1,   2),

. this is a restriction on the broader model, but

not one which is on the boundary of the parameter space, so the large-sample   2

4 it also has such functions for multivariate t distributions, which are to multivariate gaussians

exactly as ordinary t distributions are to univariate gaussians.

(cid:96)((cid:98)  )     (cid:96)((cid:98)  )     0
2[(cid:96)((cid:98)  )     (cid:96)((cid:98)  )] (cid:32)   2
(cid:20)   11   12
(cid:26)
(cid:21)(cid:27)

  12   22

(  1,   2),

q   r

(cid:21)(cid:27)

734

multivariate distributions

distribution should apply. a restriction which would be on the boundary would
be to insist that x2 was constant, so   22 = 0. (this would also force   12 = 0.)

if, on the other hand, that we have two models, with parameters    and   , and
they are completely non-nested, meaning there are no parameter combinations
where

f (  ;   ) = f (  ;   )

(e.25)

then in many ways things become easier. for    xed parameter values   0,   0, the
mean log-likelihood ratio is just an average of iid terms:

1
n

[(cid:96)(  0)     (cid:96)(  0)]     1
n

=

1
n

  i

log

f (xi;   0)
f (xi;   0)

(e.26)

(e.27)

n(cid:88)
n(cid:88)

i=1

i=1

by the law of large numbers, then, the mean log-likelihood ratio converges to an
expected value e [  ]. this is positive if   0 has a higher expected log-likelihood
than   0, and negative the other way around. furthermore, by the central limit
theorem, as n grows, the    uctuations around this expected value are gaussian,
with variance   2

also ordinarily, (cid:98)  m le and (cid:98)  m le both converge to limits, which we can call   0

ordinarily, we don   t have just a single parameter value for each model, but

   by the sample variance of log f (xi;  0)
f (xi;  0) .

  /n. we can estimate   2

and   0. at the cost of some fancy id203 theory, one can show that, in the
non-nested case,

(cid:32) n (e [  ] , 1)

   by    plugging in    (cid:98)   and (cid:98)   in

(e.28)

and that we can consistently estimate e [  ] and   2
place of   0 and   0. this gives the vuong test for comparing the two models
(vuong, 1989). the null hypothesis in the vuong test is that the two models are
equally good (and neither is exactly true). in this case,

(cid:96)((cid:98)  )     (cid:96)((cid:98)  )

  2
  

   

n
n

(cid:96)((cid:98)  )     (cid:96)((cid:98)  )

(cid:98)    

v =

1   
n

(cid:32) n (0, 1)

(e.29)

if v is signi   cantly positive, we have evidence in favor of the    model being better
(though not necessarily true), while if it is signi   cantly negative we have evidence
in favor of the    model being better.

the cases where two models partially overlap is complicated; see vuong (1989)

for the gory details5.

5 if you are curious about why this central-limit-theorem argument doesn   t work in the nested case,

notice that when we have nested models, and the null hypothesis is true, then (cid:98)      (cid:98)  , so the
numerator in the vuong test statistic, [(cid:96)((cid:98)  )     (cid:96)((cid:98)  )]/n, is converging to zero, but so is the

denominator   2
which gives us back eq. e.24. see, yet again, vuong (1989).

  . since 0/0 is unde   ned, we need to use a stochastic version of l   hoptial   s rule,

e.3 id136 with multivariate distributions

735

e.3.2 goodness-of-fit

for univariate distributions, we often assess goodness-of-   t through the kolmogorov-
smirnov (ks) test6, where the test statistic is

|(cid:98)fn(a)     f (a)|

with (cid:98)fn being the empirical cdf, and f its theoretical counterpart. the null hy-

dks = max

(e.30)

a

pothesis here is that the data were drawn iid from f , and what kolmogorov and
smirnov did was to work out the distribution of dks under this null hypothesis,
and show it was the same for all f (at least for large n). this lets us actually
calculate p values.

we could use such a test statistic for multivariate data, where we   d just take
the maximum over vectors a, rather than scalars. but the problem is that we do
not know its sampling distribution under the null hypothesis in the multivariate
case     kolmogorov and smirnov   s arguments don   t work there     so we don   t
know whether a given value of dks is large or small or what.

there is however a fairly simple approximate way of turning univariate tests
into multivariate ones. suppose our data consists of vectors (cid:126)x1, (cid:126)x2, . . . (cid:126)xn. pick
a unit vector (cid:126)w, and set zi = (cid:126)w    (cid:126)xi. geometrically, this is just the projection
of the data along the direction (cid:126)w, but these projections are univariate random
variables. if the (cid:126)xi were drawn from f , then the zi must have been drawn from
the corresponding projection of f , call it f (cid:126)w. if we can work out the latter
distribution, then we can apply our favorite univariate test to the zi. if the    t
is bad, then we know that the (cid:126)xi can   t have come from f . if the    t is good for
the zi, then the    t is also good for the (cid:126)xi     at least along the direction (cid:126)w. now,
we can either carefully pick (cid:126)w to be a direction which we care about for some
reason, or we can chose it randomly. if the projection of the (cid:126)xi along several
random directions matches that of f , it becomes rather unlikely that they fail to
match overall7.

to summarize:

(cid:126)w = (cid:126)u /(cid:107)(cid:126)u(cid:107).)

1. chose a random unit vector (cid:126)w . (for instance, let (cid:126)u     mvn (0, ip), and
2. calculate zi = (cid:126)w    (cid:126)xi.
3. calculate the corresponding projection of the theoretical distribution f , call

it f (cid:126)w .

4. apply your favorite univariate goodness-of-   t test to (cid:126)zi and f (cid:126)w .
5. repeat (1)   (4) multiple times, with correction for multiple testing.

6 i discuss the ks test here for concreteness. much the same ideas apply to the anderson-darling

test, the cram  er-von mises test, and others which, not being such good ideas, were only invented by
one person.

7 theoretically, we appeal to the cram  er-wold device again: the random vectors (cid:126)x and (cid:126)y have the
same distribution if and only if (cid:126)w    (cid:126)x and (cid:126)w    (cid:126)y have the same distribution for every (cid:126)w. failing to
match for any (cid:126)w implies that (cid:126)x and (cid:126)y have di   erent distributions. conversely, if (cid:126)x and (cid:126)y di   er in
distribution at all, (cid:126)w    (cid:126)x must di   er in distribution from (cid:126)w    (cid:126)y for some choice of (cid:126)w. randomizing
the choice of (cid:126)w gives us power to detect a lot of di   erences in distribution.

[[attn:
multiple
compar-
isons as an
appendix
topic?]]

736

multivariate distributions

e.4 uncorrelated (cid:54)= independent

as you know, two random variables x and y are uncorrelated when their
correlation coe   cient is zero:

since

  (x, y ) = 0

  (x, y ) =

(cid:112)v [x] v [y ]

cov [x, y ]

being uncorrelated is the same as having zero covariance. since

cov [x, y ] = e [xy ]     e [x] e [y ]

(e.31)

(e.32)

(e.33)

having zero covariance, and so being uncorrelated, is the same as

(e.34)
one says that    the expectation of the product factors   . if   (x, y ) (cid:54)= 0, then x
and y are correlated.

e [xy ] = e [x] e [y ]

as you also know, two random variables are statistically independent, or
just independent, when their joint id203 distribution is the product of
their marginal id203 distributions: for all x and y,

equivalently8, the conditional and marginal distributions are the same:

fx,y (x, y) = fx(x)fy (y)

fy |x(y|x) = fy (y)

(e.35)

(e.36)

if x and y are not independent, then they are dependent. if, in particular, y
is a function of x, then they always dependent9.

if x and y are independent, then they are also uncorrelated. to see this, write

the expectation of their product:

(cid:90) (cid:90)
(cid:90) (cid:90)
(cid:90)
(cid:18)(cid:90)

e [xy ] =

=

=

xyfx,y (x, y)dxdy

xyfx(x)fy (y)dxdy

(cid:18)(cid:90)

(cid:19)(cid:18)(cid:90)

(cid:19)

xfx(x)

yfy (y)dy

dx

yfy (y)dy

(cid:19)

(e.37)

(e.38)

(e.39)

(e.40)

(e.41)

xfx(x)dx

=
= e [x] e [y ]

however, if x and y are uncorrelated, then they can still be dependent. to

8 why is this equivalent?
9 for the sake of mathematical quibblers: a non-constant function of x.

e.5 exercises

737

see an extreme example of this, let x be uniformly distributed on the interval
[   1, 1]. if x     0, then y =    x, while if x is positive, then y = x. you can
easily check for yourself that:
    y is uniformly distributed on [0, 1];
   1    x2dx =    1/3;
0 x2dx = +1/3;

    e [xy |x     0] =(cid:82) 0
    e [xy |x > 0] =(cid:82) 1

    e [xy ] = 0 (hint: law of total expectation);
    e [x] = 0;
    cov [x, y ] = e [xy ]     e [x] e [y ] = 0;
    the joint distribution of x and y is not uniform on the rectangle [   1, 1]  [0, 1],

as it would be if x and y were independent (figure e.2).

the only general case when lack of correlation implies independence is when

the joint distribution of x and y is a multivariate gaussian.

e.5 exercises

1. write a function to calculate the density of a multivariate gaussian with a
given mean vector and covariance matrix. check it against an existing function
from one of the packages mentioned in   e.2.4.

2. write a function to generate multivariate gaussian random vectors, using

rnorm.
random vector,    nd the mean and variance of w    x.

3. if (cid:126)x has mean (cid:126)   and variance-covariance matrix   , and (cid:126)w is a    xed, non-
4. if (cid:126)x     mvn ((cid:126)  ,   ), and b and c are two non-random matrices,    nd the

covariance matrix of b (cid:126)x and c (cid:126)x.

5. prove eq. e.21.
6. one multivariate generalization of the pareto distribution is de   ned by

(cid:32) p(cid:88)

(cid:33)   a

pr (x1     x1, x2     x2, . . . xp     xp) =

(xj/sj)     p + 1

(e.42)

when all xj     sj.
1. find the joint pdf of all the variables.
2. show that the marginal distribution of each xj is a univariate pareto dis-

j=1

tribution, and    nd its parameters.

3. show that the conditional distribution of any xj given the other variables
is a univariate pareto distribution, and    nd its parameters. hint: it is not
the same as the marginal distribution.

[[todo: move model-comparison, goodness-of-   t stu    to main text]]

738

multivariate distributions

x <- runif(200,min=-1,max=1)
y <- ifelse(x>0,x,-x)
plot(x,y,pch=16)
rug(x,side=1,col="grey")
rug(y,side=2,col="grey")

figure e.2 an example of two random variables which are uncorrelated
but strongly dependent. the grey    rug plots    on the axes show the marginal
distributions of the samples from x and y .

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   1.0   0.50.00.51.00.00.20.40.60.81.0xyappendix f

algebra with expectations and variances

the following are all straightforward consequences of the de   nitions of expecta-
tion, variance, and covariance, but you will probably end up learning them by
heart, either deliberately, or because you keep using them so often. many of these
have catch-phrases associated with them; i   ve given them in parentheses.
    (   linearity of expectations   ;    expectation is a linear operator   )

   

   

e [ax + by ] = ae [x] + be [y ]

v [x] = e(cid:2)x 2(cid:3)     (e [x])2 = e(cid:2)(x     e [x])2(cid:3)

cov [x, y ] = e [xy ]     e [x] e [y ] = e [(x     e [x])(y     e [y ])]

    (   covariance is symmetric   )

    (   variance is covariance with itself   )

cov [x, y ] = cov [y, x]

cov [x, x] = v [x]

v [ax + b] = a2v [x]

cov [ax + b, y ] = acov [x, y ]

   

   

   

   

(cid:34) n(cid:88)

v

(cid:35)

xi

=

v [x + y ] = v [x] + v [y ] + 2cov [x, y ]

n(cid:88)

n(cid:88)

n(cid:88)

n   1(cid:88)

(cid:88)

cov [xi, xj] =

v [xi] + 2

cov [xi, xj]

(f.9)

i=1

i=1

j=1

    (   law of total expectation   )

i=1

i=1

j>i

remember: e [x|y ] is a function of y ; it   s random.

e [x] = e [e [x|y ]]

(f.10)

739

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

(f.1)

(f.2)

(f.3)

(f.4)

(f.5)

(f.6)

(f.7)

(f.8)

740
    (   law of total variance   )

algebra with expectations and variances

v [x] = v [e [x|y ]] + e [v [y |x]]

(f.11)
    (   independent implies uncorrelated   ) if x and y are independent, cov [x, y ] =
0. the reverse is not true; cov [x, y ] = 0 is even compatible with y being a
function of x.

f.1 variance-covariance matrices of vectors

a random vector (cid:126)x has both an expectation value and a variance, just like a
random scalar.

the expectation value is another vector. its ith coordinate is the expectation

of the ith coordinate of (cid:126)x,

(cid:105)

i

e(cid:104) (cid:126)x
(cid:105)
e(cid:104) (cid:126)x

=

    e [xi]
(cid:90)

(cid:126)xp((cid:126)x)d(cid:126)x

(f.12)

(f.13)

so

this is the obvious generalization of how things work with random scalars.

on the other hand, every coordinate of (cid:126)x has its own variance, and in general
those coordinates have some covariance with each other. therefore the variance
of a p-dimensional random vector is a p    p matrix of variances and covariances:

(cid:105)

v(cid:104) (cid:126)x
= e(cid:104) (cid:126)x     (cid:126)x

ij

v(cid:104) (cid:126)x

(cid:105)

    cov [xi, xj]

(cid:105)     e(cid:104) (cid:126)x

(cid:105)     e(cid:104) (cid:126)x

(cid:105)

therefore

(f.15)
where     is outer product. if you think of a vector as a p    1 matrix, then this
just means e [x t x]     e [x]t e [x].

(f.14)

f.2 quadratic forms

a quadratic form is an expression of the form (cid:126)x    a(cid:126)x, or equivalently (cid:126)xt a(cid:126)x.
the most common quadratic forms we encounter in statistics are ones where the
vector is random but the matrix is not. in that case,

e(cid:104) (cid:126)x t a (cid:126)x

(cid:105)

= tr (av(cid:104) (cid:126)x

(cid:105)
) + e(cid:104) (cid:126)x

(cid:105)t

ae(cid:104) (cid:126)x

(cid:105)

(f.16)

unfortunately, the variance of quadratic forms depends on the distribution of

(cid:126)x, and there usually isn   t a nice closed-form expression.

[[todo: needed?]]

appendix g

propagation of error, and standard errors

for derived quantities

a reminder about how we get approximate standard errors for functions of quantities which are
themselves estimated with error.

suppose we are trying to estimate some quantity   . we compute an estimate(cid:98)  ,
based on our data. since our data is more or less random, so is(cid:98)  . one convenient
way of measuring the purely statistical noise or uncertainty in (cid:98)   is its standard
suppose that our estimate (cid:98)   is a function of some intermediate quantities
(cid:99)  1,(cid:99)  2, . . . ,(cid:99)  p, which are also estimated:

deviation. this is the standard error of our estimate of   .1 standard errors are
not the only way of summarizing this noise, nor a completely su   cient way, but
they are often useful.

(cid:98)   = f ((cid:99)  1,(cid:99)  2, . . .(cid:99)  p)

(g.1)

we start with (what else?) a taylor expansion (app. d). we   ll write      

is in fact a simple if approximate way of doing so, which is called propagation
of error2.

for instance,    might be the di   erence in expected values between two groups,
if we have a standard error for each of the original quantities (cid:98)  i, it would seem
with   1 and   2 the expected values in the two groups, and f (  1,   2) =   1       2.
like we should be able to get a standard error for the derived quantity(cid:98)  . there
true (ensemble or population) value which is estimated by (cid:98)  i.
p(cid:88)
i     (cid:98)  i)
p(cid:88)
((cid:98)  i          
i ((cid:98)  )

p)     f ((cid:99)  1,(cid:99)  2, . . .(cid:99)  p) +

f ((cid:99)  1,(cid:99)  2, . . .(cid:99)  p)     f (     

(cid:12)(cid:12)(cid:12)(cid:12)  =(cid:98)  
(cid:12)(cid:12)(cid:12)(cid:12)  =(cid:98)  

((cid:98)  i          

               +

2, . . .      

2, . . .      

p(cid:88)

i for the

   f
     i

   f
     i

1,      

1,      

f (     

i )f(cid:48)

(g.2)

(g.4)

(g.3)

(     

p) +

i )

i=1

i=1

i=1

i as an abbreviation for    f
     i

introducing f(cid:48)
confused with the standard error of the mean, unless    is the expected value of the data and(cid:98)   is the

1 it is not, of course, to be confused with the standard deviation of the data. it is not even to be

. the left-hand side is now the quantity

sample mean.

2 or, sometimes, the delta method.

741

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

742

propagation of error

whose standard error we want. i have done this manipulation because now      is a
linear function (approximately!) of some random quantities whose variances we
know, and some derivatives which we can calculate.

remember the rules for arithmetic with variances: if x and y are random

variables, and a, b and c are constants,

v [a] = 0

v [a + bx] = b2v [x]

(g.5)
(g.6)
(g.7)

2, . . .      

v [a + bx + cy ] = b2v [x] + c2v [y ] + 2bccov [x, y ]

p), it   s constant, so it has variance 0. similarly,

while we don   t know f (     

(cid:105)
v(cid:104)(cid:98)  i          
= v(cid:104)(cid:98)  i
v(cid:104)(cid:98)  
(cid:105)     p(cid:88)

i

1,      

(cid:105)
i ((cid:98)  ))2v(cid:104)(cid:98)  i

(cid:105)

(f(cid:48)

p(cid:88)

p   1(cid:88)

. repeatedly applying these rules to eq. g.4,

(cid:104)(cid:98)  i,(cid:99)  j
j((cid:98)  )cov
the standard error for (cid:98)   would then be the square root of this.
if we follow this rule for the simple case of group di   erences, f (  1,   2) =   1     2,
(cid:105)
(cid:104)(cid:99)  1,(cid:99)  2

(cid:105)     2cov

= v(cid:104)(cid:99)  1

+ v(cid:104)(cid:99)  2

v(cid:104)(cid:98)  
(cid:105)

i ((cid:98)  )f(cid:48)

we    nd that

(g.9)

(g.8)

(cid:105)

(cid:105)

+ 2

j=i+1

f(cid:48)

i=1

i=1

just as we would    nd from the basic rules for arithmetic with variances. the
approximation in eq. g.8 comes from the nonlinearities in f .

if the estimates of the initial quantities are uncorrelated, eq. g.8 simpli   es to

v(cid:104)(cid:98)  
(cid:105)     p(cid:88)

i ((cid:98)  ))2v(cid:104)(cid:98)  i

(cid:105)

(f(cid:48)

and, again, the standard error of (cid:98)   would be the square root of this. the special

i=1

(g.10)

case of eq. g.10 is sometimes called the propagation of error formula, but i think
it   s better to use that name for the more general eq. g.8.

[[attn:
example of
calculation
with
a
nonlinear
function?]]
[[attn:
example of
accuracy
vs. simula-
tion?]]

appendix h

optimization

[[todo: multiple sections either need to be written from the beginning, or seriously re-written]]

many statistical problems are conveniently cast as optimization problems. this
is particularly true of    nding point estimates. this appendix therefore reviews
some basic ideas of optimization (  h.1), the asymptotics of optimizing noisy ob-
jective functions (  h.5) and its implications for maximum likelihood (  h.5.5), and
the theory of constrained and penalized optimization (    h.3.1   h.3.3), including
constrained id75 (ridge regression and the lasso) as an application
(  h.3.4).

h.1 basic concepts of optimization

we start with some real-valued function m on a domain   , called the objective1
function. a point           is a global minimum if m (  )     m (  (cid:48)) for all   (cid:48) (cid:54)=   ,
and a global maximum if m (  )     m (  (cid:48)). a local minimum is a point   
where m (  )     m (  (cid:48)) for all    which are both in    and su   ciently close to   ;
similarly for local maxima. all global minima are thus also local minima, and
similarly for maxima. the minima and maxima together form the set of extrema
or extremes, local or global.

we minimize a function by making it as small as possible, i.e., by    nding
the global minima, or coming close to (at least) one, and similarly maximizing
means    nding the global maxima. generalically, minimizing and maximizing are
both instances of optimizing, of    nding the    best    values of the function.

an interior extremum is one which is not on the boundary of the domain
  . (if    has no boundaries, all extrema are interior extrema.) if    is an interior
local minimum, then su   ciently small movements away from    in any direction
must increase the function. for smooth functions, therefore, it follows2 that the
gradient at an interior minimum is zero, and the matrix of second derivatives is
positive-de   nite. that is,    m (  ) = 0 and    2m (cid:23) 0, where the latter statement
means, more precisely, that for any vector v, (cid:104)v,   2f v(cid:105)     0. similar statements
apply to local interior maxima, but with the signs reversed.

the solution of an equation m (  ) = c is the value (or values) of    which make
the two sides of the equation balance. similarly, the solution to an optimization

1    objective    here means    goal   , not    factual   .
2 consult appendix d if this doesn   t seem reasonable.

743

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

744

problem

optimization

max
       

m (  )

is the value of    which maximizes the objective function m ; we also write this
solution, or solutions, as

argmax

       

m (  ) ,

i.e., the argument that maximizes the function. the de   nitions for a minimization
problem and the argmin are parallel. it is common to add equality or inequality
constraints to an optimization problem, e.g.,

max
       

m (  )

such that g(  ) = c
r(  )     d

in principle, such constraints cut down the domain from    to        g   1(c)    
{   : r(  )     d}; this is not always the best way of solving such problems (  h.3).

if q is a monotonic-increasing function, then

transforming the objective function

argmax m (  ) = argmax q(m (  ))

while if q is monotonic-decreasing

thus for instance maximizing m (  ) is the same as minimizing     log m (  ).

argmax m (  ) = argmin q(m (  ))

transforming the domain

if r is an invertible function from    to   (cid:48), we can de   ne a new objective function
by m(cid:48) = m     r   1. optimization problems for m and m(cid:48) are equivalent, in the
sense that min m = min m(cid:48) and argmin m = r(argmin m(cid:48)) (and similarly for
maxima). if r is continuous, even local minima and maxima are equivalent.

iterative improvement

suppose we can come up with a sequence of values   1,   2, . . . where m (  n)    
m (  n   1), and we know that f     c. then the sequence of   n must converge to a
local minimum (which may or may not be a global minimum); the same applies,
with signs reversed, for an increasing sequence.

h.2 newton   s method

there are a huge number of methods for numerical optimization, because there
is no magical method which always works better than anything else. however,

h.2 newton   s method

745

there are some methods which work very well on an awful lot of practical problems
which keep coming up, and acquiring some knowledge of them is very useful when
doing practical data analysis. because of its close connection with generalized
linear models, we   ll look at one of the most ancient and important of them,
namely newton   s method (alias    newton-raphson   ).

let   s start with the simplest case of minimizing a function of one scalar variable,
say m (  ). we want to    nd the location of the global minimum,      . we suppose
that f is smooth, and that       is a regular interior minimum, meaning that the
derivative at       is zero and the second derivative is positive. near the minimum
we could make a taylor expansion (app. d) around      :

m (  )     m (     ) +

1
2

(            )2 d2f
d  2

(h.1)

(cid:12)(cid:12)(cid:12)(cid:12)  =     

m (  )     m (  (0)) + (         (0))

df
dw

(we can see here that the second derivative has to be positive to ensure that
m (  ) > m (     ).) in words, m (  ) is close to quadratic near the minimum.

newton   s method uses this fact, and minimizes a quadratic approximation to
the function we are really interested in. (in other words, newton   s method is to
replace the problem we want to solve, with a problem which we can solve.) guess
an initial point   (0). if this is close to the minimum, we can take a second order
taylor expansion around   (0) and it will still be accurate:

(cid:12)(cid:12)(cid:12)(cid:12)  =  (0)

(cid:16)

         (0)(cid:17)2 d2f

dw2

+

1
2

(cid:12)(cid:12)(cid:12)(cid:12)  =  (0)
(cid:12)(cid:12)  =  (0) = f(cid:48)(  (0)),

(h.2)

(cid:12)(cid:12)(cid:12)  =  (0)

now it   s easy to minimize the right-hand side of equation h.2. let   s abbreviate the
derivatives, because they get tiresome to keep writing out: df
dw
d2f
dw2
equal to zero at a point we   ll call   (1):

= f(cid:48)(cid:48)(  (0)). we just take the derivative with respect to   , and set it

f(cid:48)(cid:48)(  (0))2(  (1)       (0))

1
2

0 = f(cid:48)(  (0)) +
  (1) =   (0)     f(cid:48)(  (0))
f(cid:48)(cid:48)(  (0))

(h.3)

(h.4)

the value   (1) should be a better guess at the minimum       than the initial one   (0)
was. so if we use it to make a quadratic approximation to f , we   ll get a better ap-
proximation, and so we can iterate this procedure, minimizing one approximation
and then using that to get a new approximation:

  (n+1) =   (n)     f(cid:48)(  (n))
f(cid:48)(cid:48)(  (n))

(h.5)

notice that the true minimum       is a    xed point of equation h.5: if we happen
to land on it, we   ll stay there (since f(cid:48)(     ) = 0). we won   t show it, but it can
be proved that if   (0) is close enough to      , then   (n)          , and that in general
|  (n)          | = o(n   2), a very rapid rate of convergence. (doubling the number of

746

optimization

iterations we use doesn   t reduce the error by a factor of two, but by a factor of
four.)

let   s put this together in an algorithm.

my.newton = function(f,f.prime,f.prime2,beta0,tolerance=1e-3,max.iter=50) {

beta = beta0
old.f = f(beta)
iterations = 0
made.changes = true
while(made.changes & (iterations < max.iter)) {

iterations <- iterations +1
made.changes <- false
new.beta = beta - f.prime(beta)/f.prime2(beta)
new.f = f(new.beta)
relative.change = abs(new.f - old.f)/old.f -1
made.changes = (relative.changes > tolerance)
beta = new.beta
old.f = new.f

}
if (made.changes) {

warning("newton's method terminated before convergence")

}
return(list(minimum=beta,value=f(beta),deriv=f.prime(beta),

deriv2=f.prime2(beta),iterations=iterations,
converged=!made.changes))

}

the    rst three arguments here have to all be functions. the fourth argument is
our initial guess for the minimum,   (0). the last arguments keep newton   s method
from cycling forever: tolerance tells it to stop when the function stops changing
very much (the relative di   erence between f (  (n)) and f (  (n+1)) is small), and
max.iter tells it to never do more than a certain number of steps no matter what.
the return value includes the estmated minimum, the value of the function there,
and some diagnostics     the derivative should be very small, the second derivative
should be positive, etc.

you may have noticed some potential problems     what if we land on a point
where f(cid:48)(cid:48) is zero? what if f (  (n+1)) > f (  (n))? etc. there are ways of handling
these issues, and more, which are incorporated into real optimization algorithms
from numerical analysis     such as the optim function in r; i strongly recom-
mend you use that, or something like that, rather than trying to roll your own
optimization code (  h.4).

newton   s method in more than one dimension

suppose that the objective f is a function of multiple arguments, f (  1,   2, . . .   p).
let   s bundle the parameters into a single vector, w. then the newton update is

  (n+1) =   (n)     h   1(  (n))   f (  (n))

(h.6)

where    f is the gradient of f , its vector of partial derivatives [   f /     1,    f /     2, . . .    f /     p],
and h is the hessian matrix of f , its matrix of second partial derivatives,
hij =    2f /     i     j.

h.3 constrained and penalized optimization

747
calculating h and    f isn   t usually very time-consuming, but taking the inverse
of h is, unless it happens to be a diagonal matrix. this leads to various quasi-
id77s, which either approximate h by a diagonal matrix, or take a
proper inverse of h only rarely (maybe just once), and then try to update an
estimate of h   1(  (n)) as   (n) changes.

h.3 constrained and penalized optimization

h.3.1 constrained optimization

suppose we want to minimize a function m (u, v) of two variables u and v. (it
could be more, but this will illustrate the pattern.) ordinarily, we know exactly
what to do: we take the derivatives of l with respect to u and to v, and solve
for the u   , v    which makes the derivatives equal to zero, i.e., solve the system of
equations

   l
   u
   l
   v

= 0

= 0

(h.7)

(h.8)

if necessary, we take the second derivative matrix of l and check that it is positive.
suppose however that we want to impose a constraint on u and v, to demand
that they satisfy some condition which we can express as an equation, g(u, v) = c.
the old, unconstrained minimum u   , v    generally will not satisfy the constraint,
so there will be a di   erent, constrained minimum, say   u,   v. how do we    nd it?

we could attempt to use the constraint to eliminate either u or v     take the
equation g(u, v) = c and solve for u as a function of v, say u = h(v, c). then
m (u, v) = m (h(v, c), v), and we can minimize this over v, using the chain rule:

dl
dv

=

   l
   v

+

   l
   u

   h
   v

(h.9)

which we then set to zero and solve for v. except in quite rare cases, this is messy.

h.3.2 lagrange multipliers

when we need to optimize under constraints, we don   t usually explicitly use
the constraint to eliminate variables. rather, we typically employ the method of
lagrange multipliers. this goes as follows.

with one constraint, we introduce one new variable   , the lagrange multiplier,

and a new objective function, the lagrangian,

l(u, v,   ) = m (u, v) +   (g(u, v)     c)

(h.10)

748

optimization

which we minimize with respect to u and v and   . that is, we solve

= 0

= 0

(h.11)

(h.12)

   l
     
   l
   u
   l
   v

= 0

(h.13)
notice that minimize l with respect to    always gives us back the constraint
      = g(u, v)     c. moreover, when the constraint is satis   ed,
equation, because    l
l(u, v,   ) = m (u, v). taken together, these facts mean that the   u,   v we get from
the unconstrained minimization of l is equal to what we would    nd from the con-
strained minimization of l. we have encoded the constraint into the lagrangian.
practically, the value of this is that we know how to solve unconstrained op-
timization problems. the derivative with respect to    yields, as i said, the con-
straint equation. the other derivatives are however yields

   l
   u
   l
   v

=

=

   l
   u
   l
   v

+   

+   

   g
   u
   g
   v

(h.14)

(h.15)

together with the constraint, this gives us as many equations as unknowns, so a
solution exists.
if    = 0, then the constraint doesn   t matter     we could just as well have
ignored it. when    (cid:54)= 0, the size (and sign) of the constraint tells us about
how it a   ects the value of the objective function at the minimum. the value
of the objective function l at the constrained minimum is bigger than at the
unconstrained minimum, m (  u,   v) > m (u   , v   ). changing the level of constraint
c changes how big this gap is. as we saw, l(  u,   v) = m (  u,   v), so we can see how
much in   uence the level of the constraint on the value of the objective function
by taking the derivative of l with respect to c,

=      

   l
   c

(h.16)

that is, at the constrained minimum, increasing the constraint level from c to
c +   , with    very small, would change the value of the objective function by        .
(note that    might be negative.) this makes    the    price   , in units of l, which
we would be willing to pay for a marginal increase in c     what economists would
call the shadow price3.

if there is more than one constraint equation, then we just introduce one multi-
plier per constraint, and add all those terms into the lagrangian. each multiplier
thus corresponds to a di   erent constraint. the size of each multiplier indicates

3 shadow prices are internal to each decision maker, and depend on their values and resources; they
are distinct from market prices, which depend on exchange and are common to all decision makers.
see exercise ??.

h.3 constrained and penalized optimization

749

how much lower the objective function l could be if we relaxed that constraint
    the set of shadow prices.
what about inequality constraints, g(u, v)     c? well, either the unconstrained
minimum exists in that set, in which case we don   t need to worry about it, or it
does not, in which case the constraint is    binding   , and we can treat this as an
equality constraint4.

h.3.3 penalized optimization

so much for constrained optimization; how does this relate to penalties? well,
once we    x   , the (u, v) which minimizes the full lagrangian

m (u, v) +   g(u, v) +   c

has to be the same as the one which minimizes

m (u, v) +   g(u, v)

(h.17)

(h.18)

this is a penalized optimization problem. changing the magnitude of the penalty
   corresponds to changing the level c of the constraint. conversely, if we start
with a penalized problem, it implicitly corresponds to a constraint on the value
of the penalty function g(u, v). so, generally speaking, constrained optimization
corresponds to penalized optimization, and vice versa.

h.3.4 constrained id75

to make this more concrete, let   s tackle a simple one-variable statistical optimiza-
tion problem, namely univariate regression through the origin, with a constraint
on the slope. that is, we have the statistical model

y =   x +  

(h.19)

where   is noise, and x and y are both scalars. we want to estimate the optimal
value of the slope   , but subject to the constraint that it not be too large, say
  2 < c. the unconstrained optimization problem is just least squares, i.e.,

(yi     bxi)2

(h.20)

m (  ) =

1
n

call the unconstrained optimum     :

n(cid:88)

i=1

(h.21)
as was said above in   h.3.3, there are really only two cases. either the uncon-
strained optimum is inside the constraint set, i.e.,,     2 < c, or it isn   t, in which

m (  )

  

     = argmin

4 a full and precise statement of this idea is the karush-kuhn-tucker theorem of optimization, which

you can look up.

750

optimization

case we can treat the inequality constraint like an equality. so we write out the
lagrangian

l(  ,   ) =

1
n

and we optimize:

(yi       xi)2 +   (  2     c)

n(cid:88)

i=1

   l
     
   l
     

= 0

= 0

the    rst of these just gives us the constraint back again,

    2 = c

writing      for the constrained optimum. the second equation is

n(cid:88)

i=1

1
n

2(yi         xi)(   xi) + 2        = 0

(cid:80)n
(cid:80)n

i=1 xiyi
i=1 x2
i

(if it weren   t for the    term, we   d just solve for the slope and get, as usual,
     =
.) now we have two unknowns,      and   , and two equations. let   s
c sgn     , so, plugging
solve for   . the equation     2 = c can also be written      =
in to eq. h.27,

   

n(cid:88)
xiyi        
2
(cid:80)n
c sgn     
n
i=1 xiyi     c sgn      2

i=1

n

n(cid:88)
(cid:80)n

i=1

   

c sgn     

2
n

2
n

0 =

   =

i=1 x2
i

   
c sgn     

x2
i +   

the only thing left to    gure out then is sgn     , but this just has to be the same
as sgn     . (why?)
to illustrate, i generate 100 observations from the model in eq. h.19, with the
true    = 4, x uniformly distributed on [   1, 1], and   having a t distribution with
2 degrees of freedom (figure h.1. figure h.2 shows the mse as a function of   ,
c is smaller than          3.95, then the constraint is
i.e., the m (  ) of eq. h.20. if
active and    is non-zero. figure h.3 plots    against c from eq. h.29. notice how,
as the constraint comes closer and closer to including the unconstrained optimum,
the lagrange multiplier    becomes closer and closer to 0,    nally crossing when
c =     2     15.6.

   

turned around, we could    x    and try to solve the penalized optimization

(h.22)

(h.23)

(h.24)

(h.25)

(h.26)

(h.27)

(h.28)

(h.29)

h.3 constrained and penalized optimization

751

x <- runif(n=100,min=-1,max=1)
beta.true <- 4
y <- beta.true*x + rt(n=100,df=2)
plot(y~x)
abline(0,beta.true,col="grey")
abline(lm(y~x),lty=2)

figure h.1 example for constrained regression. dots are data points, the
grey line is the true regression line, and the dashed line is the ordinary least
squares    t through the origin, without a constraint on the slope.

problem

     = argmin

  

l(  ,   )

n(cid:88)

i=1

= argmin

  

1
n

(yi       xi)2 +     2

(h.30)

(h.31)

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   1.0   0.50.00.51.0   1001020xy752

optimization

demo.mse <- function(b) { return(mean((y-b*x)^2)) }
curve(vectorize(demo.mse)(x),from=0,to=10,xlab=expression(beta),ylab="mse")
rug(x=beta.true,side=1,col="grey")

figure h.2 mean squared error as a function of   . the grey tick marks the
true    = 4; the minimum of the curve is at      = 3.95.

taking the derivative with respect to   ,

0 =

0 =

     =

   l
     
1
n

n(cid:88)
2(yi         xi)(   xi) + 2       
(cid:80)n
(cid:80)n

i=1 xiyi
i=1 x2
i

i=1
1
n

   + 1
n

(h.32)

(h.33)

(h.34)

which is of course just eq. h.27 again. figure h.4 shows how      and     2 change with
  . the fact that the latter plot shows the same curve as figure h.3 only turned
on its side re   ects the general correspondence between penalized and constrained
optimization.

h.3.5 statistical remark:    ridge regression    and    the lasso   

the idea of penalizing or constraining the coe   cients of a id75 model
can be extended to having more than one coe   cient. the general case, with p
covariates, is that one penalizes the sum of the squared coe   cients,   2
1 + . . . +   2
p,
which of course is just the squared length of the coe   cient vector, (cid:107)  (cid:107)2. this is

024681015202530bmseh.3 constrained and penalized optimization

753

lambda.from.c <- function(c) { (2*mean(x*y) - sqrt(c)*2*mean(x^2))/sqrt(c) }
curve(lambda.from.c(x),from=0,to=15.7,xlab="c",ylab=expression(lambda))

figure h.3    as a function of the constraint level c, according to eq. h.29
and the data in figure h.1.

called ridge regression (hoerl and kennard, 1970), and yields the estimates

     = (xt x +   i)   1xt y

(h.35)
where i is the p   p identity matrix. instead of penalizing or constraining the sum
of squared coe   cients, we could penalized or constrain the sum of the absolute
values of the coe   cients, |  1| + |  2| + . . . + |  p|, abbreviated (cid:107)  (cid:107)1. this is called
the lasso (tibshirani, 1996). it doesn   t have a nice formula like eq. h.35, but it
can be computed e   ciently.

051015012345cl754

optimization

examining eq. h.35 should convince you that      is generally smaller than the
unpenalized estimate     . (this may be easier to see from eq. h.34.) the same
is true for the lasso penalty. both are examples of shrinkage estimators, called
that because the usual estimate is    shrunk    towards the null model of an all-0
parameter vector. this introduces a bias, but it also reduces the variance. shrink-
age estimators are rarely very helpful in situations like the simulation example
above, where the number of observations n (here = 100) is large compared to
the number of parameters to estimate p (here = 1), but they can be very handy
when n is close to p, and p > n, ordinary least squares is useless, but shrinkage
estimators can still work. (ridge regression in particular can be handy in the
face of collinearity, even when p (cid:28) n.) while the lasso is a bit harder to deal
with mathematically and computationally than is ridge regression, it has the nice
property of shrinking small coe   cients to zero exactly, so that they drop out of
the problem; this is especially helpful when there are really only a few predictor
variables that matter, but you don   t know which.

for much more on the lasso, ridge regression, shrinkage, etc., see hastie et al.

(2009).

h.4 optimization in r

the basic work-horse function for optimization in r is optim. this is actually a
wrapper for several di   erent optimization methods. the default, method="nelder-mead",
does not use derivatives. method="bfgs" selects a newton-style method, which
is more e   cient about re-calculating the hessian matrix than a pure newton   s
method would be. (bfgs is an acronym for the names of the algorithm   s inven-
tors.) if you can write a function which calculates the gradient, optim will use it;
if not, it will approximate it by    nite di   erences.5

optim includes a method, method="l-bfgs-b", for    box    constraints, where each
parameter has to be above a lower bound and below an upper bound. for more
complicated constraints, including both equality and inequality constraints, i
typically use the alabama package (varadhan, 2012).

beyond this, there are a large variety of packages implementing speci   c meth-
ods, and/or tailored to speci   c types of optimization problems. the cran
webpage on    optimization and mathematical programming   , https://cran.
r-project.org/web/views/optimization.html, is the best starting point.

h.5 small-noise asymptotics for optimization

the basic math of asymptotic estimation theory is pretty simple, and is the math
of optimizing a function which is perturbed by a small amount of noise. many
presentations obscure what   s going on, because they include all the conditions
needed to obtain rigorous results. in the spirit of    to explain,    rst over-simplify,
then exaggerate   , this section tries to convey the basic intuitions by deliberately

5 for a view of optim in action, see chapter 26.

h.5 small-noise asymptotics for optimization

755

par(mfrow=c(2,1))
beta.from.lambda <- function(l) { return(mean(x*y)/(l+mean(x^2))) }
curve(beta.from.lambda(x),from=0,to=6,

xlab=expression(lambda),ylab=expression(tilde(beta)))

curve(beta.from.lambda(x)^2,from=0,to=6,

xlab=expression(lambda),ylab=expression(tilde(beta)^2))

par(mfrow=c(1,1))

figure h.4 left: the penalized estimation of the regression slope, as a
function of the strength of the penalty   . right: square of the penalized
regression slope.

ignoring the quali   cations, regularity conditions, etc., etc. those details are worth
knowing, because they can matter a lot when you are trying to make these ideas

01234560.51.52.5lb~01234560246810lb~2756

optimization

work in new or non-standard circumstances, but    rst you have to grasp the big-
picture basic ideas.

these ideas apply to all of the most common methods of parameter estima-
tion, including the method of maximum likelihood, because they all boil down
to optimizing a quantity which is a function of both the data and of the param-
eters. we treat the data as    xed, and look for the optimal parameter. because
the data are random, we are thus optimizing a random function, and the location
of the optimum will be random too. if we   ve chosen a good objective function,
however, these random functions are converging to a non-random limit, so the
optima will also converge, and ideally converge on the truth. we can thus say a
lot about asymptotic estimates, purely from knowing that we are optimizing a
random function which is converging on a non-random limit.

h.5.1 basic set-up and notation

we observe n data points x1, . . . xn. these might each be multidimensional, and
they may be dependent (a time series, a spatial    eld, etc.). these actually end
up playing little role in the theory and will mostly be suppressed in the notation.
we are trying to infer a parameter   . here, by a    parameter    i mean sim-
ply    some function of the true id203 distribution   . it may or may not be
   nite-dimensional, and it may or may not be used to characterize the id203
distribution generating the xi. examples of parameters, in this sense, include:
expectations, medians, other speci   c quantiles, conditional expectations (e.g.,
if xi = (yi, zi), the parameter might be e [y |z = 5]), conditional expectation
functions (e.g., the function z (cid:55)    e [y |z = z]), variances, etc., the coe   cients of
particular regression models (e.g., the slope of the optimal id75 of
y on z), etc., etc. all that is required is that there is some well-de   ned way of
calculating the parameter from the true id203 distribution.

h.5.1.1 examples of objective functions

we introduce a sequence of objective functions mn, which are functions of both
the data and of the parameter. they are thus strictly written mn(x1:n,   ). how-
ever, i will generally suppress the    rst argument, writing mn(  ). the capital
letter m here is to remind you that this is a random function, and that its ran-
domness comes from the data. here are some examples:
estimating the expectation we take    = e [x], and use

n(cid:88)

i=1

mn(  ) =

1
n

(xi       )2

(h.36)

quantile estimation if we want    to be the    quantile of the distribution of

(univariate) x, we can set

mn(  ) =

1
n

n(cid:88)

i=1

xi(       i(xi < 0))

(h.37)

n(cid:88)

i=1

n(cid:88)

i=1

n   1(cid:88)

t=1

h.5 small-noise asymptotics for optimization

757

(see koenker and hallock 2001, which also covers the extension to esti-
mating conditional quantiles.)

estimating a simple id75 with ordinary least squares we take

   = (  1,   2) to be the coe   cients of the best id75 of yi on
zi, and make mn the corresponding mse:

mn(  ) =

1
n

(yi       1       2zi)2

(h.38)

estimating a parametric nonid75 we assume (or want to ap-
proximate) e [y |z = z] by a parametric family of functions   (z;   ), where
   are the unknown coe   cients in some appropriate nonlinear form. (per-
haps   (z;   ) =   1 +   2e  3(z     4)/(1 + e  3(z     4)).) again, we can make mn
the corresponding mse:

mn(  ) =

1
n

(yi       (zi;   ))2

(h.39)

estimating an autoregression the xi are ordered in time, we want    to be
the function x (cid:55)    e [xt+1|xt = x], and we use the mse of this autore-
gression as mn:

mn(  ) =

1

n     1

(xt+1)       (xt))2

(h.40)

parameters and negative normalized log-likelihood if    is the parameter
vector of a family of id203 densities f (x1:n;   ), we can set mn(  ) to
be the negative log-likelihood per observation:

mn(  ) =

1
n

log f (x1:n;   )

(h.41)

note that while mn is often a sum or average of functions of each data point,
this isn   t required, as in the last two examples. indeed, mn doesn   t even have to
be an average of any kind.

in every case, we obtain our estimate by minimizing mn:

    n     argmin

mn(  )

(h.42)

  

we will assume that there is a unique minimum for mn, since the complications
that arise from multiple minima are both technical and boring.

since the function mn is random (through the data x1:n),     n is also random.
finally, throughout this section, all limits are to be taken as n        .

h.5.2 basic convergence assumption

the basic assumption which is needed to get asymptotic estimation theory to
work is that the random objective functions mn converge on a non-random lim-
iting function m. it doesn   t particularly matter to the argument why this is hap-

758

optimization

pening, though we might have our suspicions6, just that it is. this is an appeal
to the gods of stochastic convergence. we typically further assume that m has a
unique minimum, so

          argmin

m(  )

(h.43)

is well-de   ned. this is an appeal to the gods of optimization.

  

h.5.2.1 examples of limiting objective functions

  h.5.1.1 gave examples of random objective functions mn used for estimation.
here are the corresponding non-random limiting objective functions m:

estimating the expectation the expected squared error of predicting x with

a constant:

m(  ) = e(cid:2)(x       )2(cid:3)

quantile estimation the expected value of the    check-mark    error function:

m(  ) = e [x(       i(x < 0))]

(h.45)

estimating a simple id75 with ordinary least squares the ex-

pected squared error of predicting y from z:

estimating a parametric nonid75 again, the expected squared

error:

estimating an autoregression once more, the expected squared error:

m(  ) = e(cid:2)(y       1       2z)2(cid:3)
m(  ) = e(cid:2)(y       (z;   ))2(cid:3)
m(  ) = e(cid:2)(xt+1)       (xt))2(cid:3)

(h.44)

(h.46)

(h.47)

(h.48)

parameters and negative normalized log-likelihood the limiting expected
negative log-likelihood per observation, a.k.a. the cross-id178 rate
([[cross-ref]]):

m(  ) = lim

n       n   1e [    log f (x1:n;   )]

(h.49)

h.5.2.2 modes of convergence

readers with more training in math, especially in id203 theory, may at
this point start asking some questions. these can be answered, but the rest of
this sub-section can be skipped by readers who were willing to nod along to the
previous paragraph.
in saying that mn     m, i glossed over two issues. first, random variables have
di   erent modes of convergence, and i didn   t say which one i was invoking with

6    in fact, all epistemologic value of the theory of the id203 is based on this: that large-scale

random phenomena in their collective action create strict, nonrandom regularity        gnedenko and
kolmogorov (1954, p. 1).

h.5 small-noise asymptotics for optimization

759
the arrow          . second, mn and m are functions    (cid:55)    r, and functions, too,
have di   erent modes of convergence. so just what, if anything, does    mn     m   
mean?

the two relevant modes of convergence for functions are pointwise convergence

and uniform convergence. pointwise convergence means that, for all   ,

or

mn(  )     m(  )

     , |mn(  )     m(  )|     0

(h.50)

(h.51)

(since, as i just said, random variables have di   erent modes of convergence them-
selves, there is one sense of pointwise convergence for each sense of random-
variable convergence.) the stronger notion of uniform convergence is that

|mn(  )     m(  )|     0

sup
       

(h.52)

both of these can be applied to any notion of convergence for random variables:
almost-sure convergence, convergence in id203, or convergence in lp norm
(e.g., in l2). thus pointwise l2 convergence would be the assertion that

while its uniform counterpart would be

      , e(cid:2)|mn(  )     m(  )|2(cid:3)     0
(cid:19)2(cid:35)
(cid:34)(cid:18)

|mn(  )     m(  )|

    0

e

sup
       

(h.53)

(h.54)

most of what i say below will, taken literally, require uniform convergence, but
it will apply to any mode of convergence of random variables. that is, if you
assume uniform convergence in id203, you   ll get results about convergence-
in-id203 of estimators. in many cases, strict uniform convergence can be
replaced by assuming that convergence is uniform over some domain around the
limiting optimum      , and that     n enters this domain with id203 tending
to 1. details can be pursued through any of the standard references given under
   further reading   .

suppose, in line with our assumptions, that mn     m, and that m has a unique
minimum at      . then it is natural suppose that

h.5.3 consistency

    n          

(h.55)

which is to say that     n is a consistent estimator of      .

it   s plausible, and generally true7, that m(     n)     m(     ), which is sometimes
called risk-consistency. a further condition is usually also needed to get actual
7 notice that mn(     n)     mn(     ) (by de   nition of     n and that m(     )     m(     n) (by de   nition of      .

so m(     n)     m(     )     m(     n)     m(     ) + mn(     n)     mn(     )    

760

optimization

consistency. a nice su   cient condition for this that m (not mn!) should have a
well-separated minimum: for any   > 0,

m(     ) <

inf

   : (cid:107)          (cid:107)    

m(  )

(h.56)

in words, this just says that you can   t get (arbitrarily) close to the value of the
minimum without also coming close the location of the minimum: for each    > 0,
m(  )     m(     ) +    only if (cid:107)            (cid:107) <   for some  , and       0 as        0. so this
plus risk-consistency means that     n          .
to sum up: if mn     m, and this convergence is well-behaved, and m has a
nice, well-separated minimum at      , then     n          .

h.5.4 asymptotic variance

we can say more about the distribution of     n if we strengthen our assumptions
about mn and m. speci   cally, assume that mn has a smooth, interior minimum
at     n; likewise that m has a smooth, interior minimum at      ; and that derivatives
converge:

   mn(     n) = 0
   m(     ) = 0
      mn(     n) (cid:23) 0
      m(  ) (cid:23) 0

(h.57)
(h.58)

   mn        m
      mn           m

(h.59)
(h.60)
(h.61)
(h.62)
here    mn is the gradient of mn (= vector of partial derivatives with respect to   )
and       mn is its hessian matrix (= square matrix of second partial derivatives),
and similarly for    m and       m. it   s convenient to not have to write out the       
over and over, so de   ne hn(  )           mn(  ) and h(  )           m(  ).
to see how these assumptions help us get at the distribution of     n, start with
eq. h.57, and then do a taylor series for the gradient    mn around the limiting
optimum      :

0 =    mn(     n)
0        mn(     ) + (     n          )      mn(     )
    n               (hn(     )   1   mn(     )

(h.63)

(h.64)

(h.65)

(if this reminds you of newton   s method [[cross-ref]], that   s no coincidence!) by
assumption, hn(     )     h(     ), i.e., the hessian matrix of mn converges on the
hessian matrix of m. also by assumption,    mn        m, so     n               h(     )0 =
|m(     n)     m(     ) + mn(     n)     mn(     )|     2 sup   |mn(  )     m(  )|. assuming uniform convergence,
this     0. something weaker than uniform convergence can also work, e.g., uniform convergence on a
sub-domain containing      .

h.5 small-noise asymptotics for optimization

761
     , i.e., we get consistency again. we are however interested in the    uctuations
of     n around      , so we should step back just a little.
    n               h(     )   1   mn(     )

(h.66)

e(cid:104)     n
v(cid:104)     n

(cid:105)               h(  )   1e [mn(     )]          
(cid:105)     h(     )   1v [   mn(     )] h(     )   1

(h.67)

(h.68)

eq. h.67 asserts an (asymptotic) lack of bias in     n. this is reassuring but usually
of less interest than variance, eq. h.68, which gives us standard errors. that
equation calls for some special comment.

h.5.4.1 the sandwich covariance matrix

eq. h.68 is the famous sandwich covariance matrix for estimators obtained by
minimization. the    bread    of the sandwich are the two copies of h(     )   1. these
will be small when m is very sharply curved around its minimum      , telling us
that that when the limiting objective function has a lot of curvature, and hence a
very sharp optimum, it   s easy to    nd the optimum, and there   s little uncertainty
about its location. it is for this reason that statisticians, and statistical programs,
care so much about the hessian of the objective function.
the       lling    of the sandwich, v [   mn(     )], will be small when there is little
variance to    mn(     ). that is, there will be little uncertainty in the location of the
optimum when there is little noise in the gradient. the variance of the gradient
is important enough that we should give it a symbol, jn     v [   mn(     )], so we
can write the sandwich covariance as

this makes it clear that the rate at which v(cid:104)     n

(cid:105)     0 will depend on the rate at

(h.69)

which jn     0. in many situations, mn, and hence    mn, will be an average over
data-points, with a variance     n   1. (this will be true if mn is an average over
uncorrelated data points, and even for many correlated data sources, if there is a
   nite    correlation time    or    correlation length    [[cross-ref time series]]).
if that   s true, and we write njn     j, we have

v(cid:104)     n

(cid:105)     h(     )   1jnh(     )   1

nv(cid:104)     n

(cid:105)     h(     )   1jh(     )   1

(h.70)

which is another common expression for the sandwich covariance. note, however,
that eqns. h.68 and h.69 hold more broadly than eq. h.70.

practical estimation of the sandwich covariance matrix

if we want to actually get numbers out of eq. h.69, we need to be able to plug
something in for h(     ) and v [   mn(     )]. the obvious di   culty is that these
involve      , which we don   t know. the obvious solution is to substitute in     n
for      , since     n          . (this is just like substituting in the sample mean when

n(cid:88)

i=1

n(cid:88)
n(cid:88)

i=1

   mni(  )
(cid:88)
n(cid:88)

j(cid:54)=i

762

optimization

calculating the sample variance.) we probably also don   t know h, but we can use
hn as a substitute. so we get

v(cid:104)     n

(cid:105)     hn(     n)   1v(cid:104)   mn(     n)

(cid:105)

merical di   erentiation (if we   re not). this leaves getting the    lling v(cid:104)   mn(     n)
(cid:105)
we can    nd the hessians either by doing some calculus (if we   re lucky), or nu-
.
this is generally trickier, because we only have one    observation    of    mn(     n),
which is zero. . .

(h.71)

hn(     n)   1

if mn is a sum or average over data-points,

mn(  ) =

mni(  )

(h.72)

then we can do a bit more. (notice that all the examples of objective functions
mn in   [[cross-ref]] above were of this form.) eq. h.72 implies

   mn(  ) =

(h.73)

i=1

i=1

=

and so

v [   mn(     )] =

v [   mni(     )] +

n(cid:88)
n(cid:88)
(cid:107)e(cid:104)   mni(     n)
v [   mn(     )]     n(cid:88)
we can in turn approximate the    rst, sum-of-the-variances term by(cid:80)

(cid:107)e [   mni(     )](cid:107)2 +
since e [   mni(     )] = 0. substituting     n          ,
(cid:88)
n(cid:88)

cov [   mni(     ),   mnj(     )](h.74)
(cid:88)
e [   mni(     )        mnj(     )](h.75)
e(cid:104)   mni(   psin)        mnj(     n)
(cid:105)

(cid:105)(cid:107)2 +

(h.76)

(h.77)

j(cid:54)=i

j(cid:54)=i

i=1

i=1

i=1

i=1

(this is analogous to the way we estimate conditional variance functions in chap-
ter 10. the second, sum-of-covariances term would go away if the data points were
independent. if they   re not, we need to estimate it somehow, and this is typically
done using

i (cid:107)   mni(     n)(cid:107)2.

   mni(   psin)        mnj(     n)w(|i     j|)

(h.78)

n(cid:88)

(cid:88)

i=1

j(cid:54)=i

where the weights w(h)     0 as h        , to help keep the sum stable. (in practice,
w is often a kernel, with a bandwidth that needs to be tuned.) the resulting vari-
ance estimate is variously called heteroskedastic-autocorrelation consistent
(hac), robust, or huber-white (after huber 1967 and white 1994).

while there are, clearly, a lot of moving parts here, the general framework is

h.5 small-noise asymptotics for optimization

763

very repetitive, across di   erent choices of parameter, model, objective function,
etc. this suggests makes it an excellent candidate for automation via software.
this is most nicely done in r with the sandwich package (zeileis, 2004, 2006).

use simulation to get an estimate of j = v(cid:104)   mn(     n)
(cid:105)
also use simulation to directly get an estimate of v(cid:104)     n
(cid:105)

    finally, it   s worth noting that, with id64 (chapter ??), one can
. of course, you could

, avoiding the sandwich
step. under some circumstances, the bootstrap and the sandwich estimates of
the variance are known to coincide (buja et al., 2014).

go back to eq. h.66:

h.5.4.2 asymptotic gaussianity

    n               h(     )   1   mn(     )

if    mn(     ) is approximately gaussian, then     n must be approximately gaussian
as well. since we   ve worked out the expectation and variance of    mn(     ), we   d
have

    n (cid:32) n (     , h(     )   1j(     )h(     )   1)

(h.79)

for this conclusion, it doesn   t matter why the gradient    mn(     ) becomes gaus-
sian as n grows, just that it does. if mn is an average of iid terms, then we   d
anticipate mn converging in distribution to a gaussian as n grows (by the cen-
tral limit theorem), and for the same behavior to generally carry over to its
derivatives. we can also anticipate things like this under weak dependence and
heterogeneous distributions, provided that all of the terms in the average have
only weak in   uence on the over-all value of the average, diminishing as n grows.
su   cient conditions for this tend to involve some rather intricate id203
theory, but are discussed in the literature on asymptotics, e.g., white (1994).

h.5.4.3    optimism   

typically (as in the examples in     [[crossref]] and [[crossref]]), mn is
some measure of in-sample performance of a model, while m measures expected
performance on new data. while it is generally the case that mn(     n)     m(     ),
there is generally a gap between mn(     n) and m(     n), i.e., between how well the
estimated parameter performs on the data used to estimate it, and how well it
will do in the future. this measure of over-   tting is sometimes (especially for
regression) referred to as the optimism of the estimator. we can get at the
optimism by doing some more taylor expansions. first, we taylor-expand m(     n)

(h.80)

(h.81)

(h.82)

(h.83)

764
around      , since    m(     ) = 0:

optimization

e(cid:104)

m(     n)     m(     ) +

(cid:105)     m(     ) +

m(     n)

= m(     ) +
    m(     ) +
= m(     ) +

(cid:68)     n          |h(     )|     n          (cid:69)
tr (h(     )v(cid:104)     n          (cid:105)
tr (h(     )v(cid:104)     n

(cid:105)

)

)

tr (h(     )h   1(     )jh   1(     ))
tr (jh   1(     ))

1
2
1
2
1
2
1
2
1
2

(h.84)
using     n          , and eq. f.16 for the expectation of a quadratic form. this
involves the unknown limiting function m at the unknown limiting optimum      ,
so it   s not that useful as an estimate, but we can    x that by doing a parallel
expansion of mn(     ) around     n:

mn(     )     mn(     n) +
    mn(     n) +

e [mn(     )]     e(cid:104)
= e(cid:104)
= e(cid:104)
m(     )     e(cid:104)
tr (jh   1(     ))     e(cid:104)
(cid:105)     e(cid:104)
e(cid:104)

mn(     n)

mn(     n)

mn(     n)

mn(     n)

mn(     n)

(cid:68)
(cid:68)

+

+

+

+

tr

(cid:69)
(cid:69)
              n|h(     n)|              n
(cid:105)(cid:17)
              n|h(     )|              n
(cid:105)(cid:17)
              n
1
2
1
2
1
2
1
2
1
2

h(     )v(cid:104)
(cid:16)
h(     )v(cid:104)     n
(cid:16)
tr(cid:0)jh   1(     )(cid:1)
tr(cid:0)jh   1(     )(cid:1)

tr (jh   1(     ))

tr

+
+ tr (jh   1(     ))

(h.85)

(h.86)

(h.87)

(h.88)

(h.89)

(h.90)

(h.91)

1
2
1
2

(cid:105)
(cid:105)
(cid:105)
(cid:105)
(cid:105)
(cid:105)

e [m(hat  n)]     1
2

m(     n)

mn(     n)

since mn(     n) is something we can observe, and its average equals e(cid:104)
as an unbiased estimator of e(cid:104)

(h.93)
, and tr (jh   1(     )) as our estimate of the

mn(     n) + tr (jh   1(     ))

mn(     n)

we get

(h.92)

(cid:105)

(cid:105)

m(     n)

,

optimism.

h.5.4.4 application to maximum likelihood

all of the theory above applies to id113.

as indicated in     [[crossref]] above, mn(  ) is the negative normalized log-
likelihood for a parametric model with parameter   , and     n is the maximum
likelihood estimate, and the limiting objective function m(  ) is the expected

h.5 small-noise asymptotics for optimization

765

log-likelihood per observation, known in id205 [[cross-ref]] as
the cross-id178 rate. (the convergence of mn(  )     m(  ) is then called the
   asymptotic equipartition    or    shannon-mcmillan-breiman    property.) the gra-
dient    mn(  ) is called the score vector. the expected value of the hessian is
the fisher information8,

f (  )     e [      mn(  )]

which simpli   es, for iid observations, to

f (  ) =       m(  )

with non-iid observations, many authors will use eq. h.95 as the de   nition of
f (  ), which thus takes the role of h in our general estimation theory.

correctly-speci   ed parametric models: fisher   s identity

if the parametric model is correctly speci   ed, then one can show that fisher   s
identity holds (exercise h.1),

e [      mn(  )] = v [   mn(  )]

or, in terms of the symbols used above,

f (  ) = h(  ) = j

because this involves the fisher information matrix, eq. h.96 is also often called
the information identity.
when fisher   s identity holds, the sandwich variance matrix simpli   es from
h   1jh   1 to just h   1(     ) = j   1 = f   1(     ). plugging in     n          , one gets three
approximations for the variance of the id113:

(h.94)

(h.95)

(h.96)

(h.97)

(h.98)

(h.99)
(h.100)

v(cid:104)    n

(cid:105)     f   1(    n)

n (    n)

    h   1
    j   1

n

the    rst of these relies on being able to calculate the fisher information ma-
trix. the second approximates the fisher information matrix by the the hessian
matrix of the (normalized, negative) log-likelihood at the id113     what   s called
the observed information matrix. the third, rare form approximates it with
the covariance matrix of the score vector9. note, however, that this simpli   cation
relies on fisher   s identity holding, and it generally does not hold unless the model
is well-speci   ed.

this    nal thought suggest the possibility of testing whether the model is well-

speci   ed, by testing whether

n (    n)     v(cid:104)   mn(    n)

(cid:105)

h   1

(h.101)

8 it   s more usual to write i(  ) or even i(  ), but i have found that this leads to confusion with the

identity matrix i, so i am using a slightly non-standard letter, while insisting that (i) matrices stay
in bold type, and (ii) random variables are upper case and non-random ones lower-case.

9 see above on ways to calculate j from data.

766

optimization

since this is a matrix equation, we need to think carefully about how to measure
discrepancy between the two sides, and about what distribution it should have
under the null hypothesis of correct speci   cation     white (1994) is a good source
for such details.

akaike   s information criterion

[[todo:
very rough
re-
draft,
work
for
notational
consis-
tency,
tone, etc.]]

when

the basic ideas underlying asymptotic estimation theory are very simple; most
presentations rather cloud the basics, because they include lots of detailed con-
ditions needed to show rigorously that everything works.
we have a statistical model, which tells us, for each sample size n, the probabil-
ity that the observations x1, x2, . . . xn     x1:n will take on any particular value
x1:n, or the id203 density if the observables are continuous. this model
contains some unknown parameters, bundled up into a single object   , which
we need to calculate those probabilities. that is, the model   s probabilities are
m(x1:n;   ), not just m(x1:n). because this is the elementary setting, we   ll say that
there are only a    nite number of unknown parameters, which don   t change with
n, so          . finally, we have a id168, which tells us how badly the model   s
predictions fail to align with the data:

(h.102)

(h.103)

(h.104)

for instance, each xi might really be a (ui, vi) pair, and we try to predict vi

  n(x1:n, m(  ;   ))
n(cid:88)

from ui, with loss being mean-weighted-squared error:
(vi     e   [vi|ui = ui])2

  n =

1
n

i=1

v   [vi|ui = ui]

(if i don   t subscript expectations e [  ] and variances v [  ] with   , i mean that
they should be taken under the true, data-generating distribution, whatever that
might be. with the subscript, calculate assuming that the m(  ;   ) distribution is
right.)

or we might look at the negative mean log likelihood,
log m(xi|x1:i   1;   )

  n =     1
n

being simple folk, we try to estimate    by minimizing the loss:

n(cid:88)
(cid:98)  n = argmin

i=1

  

  n

(h.105)

we would like to know what happens to this estimate as n grows. to do this,

we will make two (fallible) assumptions.

the    rst assumption is about what happens to the id168s.   n depends
both on the parameter we plug in and on the data we happen to see. the later is
random, so the loss at any one    is really a random quantity,   n(  ) =   n(x1:n,   ).
our    rst assumption is that these random losses tend towards non-random limits:

h.5 small-noise asymptotics for optimization

767

for each   ,

  n(  )     (cid:96)(  )

(h.106)

where (cid:96) is a deterministic function of    (and nothing else).

our second assumption is that we always have a unique interior minimum with

a positive-de   nite hessian matrix: with id203 1,

     n((cid:98)  n) = 0
        n((cid:98)  n) > 0

(h.107)

(h.108)

(all gradients and other derviatives will be with respect to   ; the dimensionality
of x is irrelevant.)

moreover, we assume that the limiting id168 (cid:96) also has a nice, unique
interior minimum at some point      , the minimizer of the limiting, noise-free loss:

      = argmin

(cid:96)

(h.109)

  

   (cid:96)(     ) = 0
      (cid:96)(     ) > 0

since the hessian matrices will be important, i will abbreviate         n((cid:98)  n) by
hn (notice the capital letter, because it   s a random variable), and       (cid:96)(     ) by j
(notice that it   s not random).

(h.110)
(h.111)

not always true. to see that they are sometimes true, here   s an example.

1)x     . then   n(  ) =   log xn     log (       1), where log xn = n   1(cid:80)n

these assumptions about the minima, and the derivatives at the minima, are
suppose that our models are pareto distributions for x     1, m(x;   ) = (      
sample mean of the log values. from the law of large numbers, (cid:96)(  ) =   e [log x]   
log (       1). to show the convergence, figure h.5 plots   10,   1000 and   105 for a
particular random sample, and the corresponding (cid:96). i chose this example in part
because the pareto distribution is heavy tailed, and i actually generated data from
a parameter value where the variance of x is in   nite10. the objective functions,
however, converge just    ne.

i=1 log xi, the [[todo:
cross-refs
to pareto
distribu-
tions]]

having made assumptions h.107 and h.109, we want to see how the minimizers
of   n converge on the minimizers of (cid:96). to do so, we use (all together now) a taylor
expansion. speci   cally, we expand the gradient      n around      :

     n((cid:98)  n) = 0

         n(     ) + hn((cid:98)  n          )
(cid:98)  n =           h   1

n      n(     )

(h.112)

(h.113)

(h.114)
the    rst term on the right hand side,      , is the population/ensemble/true
minimizer of the loss. if we had (cid:96) rather than just   n, we   d get that for the location
of the minimum. but since we see (cid:96) corrupted by noise, we need to include the
extra term    h   1
n      n(     ). the hessian matrix hn tells us how sharply curved   n

10 for purists: unde   ned.

768

optimization

figure h.5 convergence of objective functions, here, negative average
log-likelihoods. note that the limiting, n =     objective function (solid blue
line) is extremely close to what we see at n = 105 already. see code
example 47 for code.

is near its minimum; the bigger this is, the easier, all else being equal, to    nd the
location of the minimum. the other factor,      n(     ), indicates how much noise
after all    (cid:96)(     ) = 0. we would like (cid:98)  n          , so we have to convince ourselves
there is     not so much in the function being minimized, as in its gradient, since

that the rest is asymptotically negligible, that h   1

n      n(     ) = o(1).

start with the hessian matrix. hn is the matrix of second derivatives of a

random function:

hn((cid:98)  n) =         n((cid:98)  n)

(h.115)

2468105101520253035qn=1e1n=1e3n=1e5limith.5 small-noise asymptotics for optimization

769

source("~/things-to-work-on/pli-r/pareto.r")

# for pareto.r, which includes rpareto() and pareto.loglike(), see
# http://www.santafe.edu/~aaronc/powerlaws/pli-r-v0.0.3-2007-07-25.tgz

# generate 10^6 samples from a pareto distribution with exponent 1.5

# note that var(x) = infinity because exponent < 2

x <- rpareto(1e6,threshold=1,exponent=1.5)
# define the negative mean log-likelihood function on the first n samples
lambda.once <- function(n,theta) {

-(1/n)*pareto.loglike(x[1:n],threshold=1,exponent=theta)

}
# vectorize over theta for plotting
lambda <- vectorize(lambda.once)
# start with curve based on first ten samples
curve(lambda(n=10,theta=x),from=1,to=10,xlab=expression(theta),ylab="")
# add curved based on first 10^3
curve(lambda(n=1000,theta=x),add=true,lty="dashed")
# add curved based on first 10^5
curve(lambda(n=1e5,theta=x),add=true,lty="dotted")
# add curve for the infinite-sample limit

# uses e[log(x)]=2 from knowledge of the pareto

curve(x*2 - log(x-1),add=true,col="blue")
# decorate with a legend
legend("bottomright",legend=c("n=1e1","n=1e3","n=1e5","limit"),

lty=c("solid","dashed","dotted","solid"),
col=c("black","black","black","blue"))

code example 47: code for figure h.5.

since   n     (cid:96), we may hope that
di   erentiation and limits    in this way. since hn((cid:98)  n) is tending to j((cid:98)  n), it follows

hn((cid:98)  n)           (cid:96)((cid:98)  n) = j((cid:98)  n)

we   ll assume that everything is nice (   regular   ) enough to let us    exchange

that hn = o(1), and consequently h   1
n = o(1) by continuity. with more words:
since   n is tending towards (cid:96), the curvature of the former is tending towards
the curvature of the latter. but this means that the inverse curvature is also
stabilizing.
our hope then has to be the noise-in-the-gradient factor,      n(     ). remember

(h.116)

again that

(h.117)
and that   n     (cid:96). so if, again, we can exchange taking derivatives and taking
limits, we do indeed have

   (cid:96)(     ) = 0

and we   re done. more precisely, we   ve established consistency:

     n(     )     0
(cid:98)  n          

(h.118)

(h.119)

of course it   s not enough just to know that an estimate will converge, one also
wants to know something about the uncertainty in the estimate. here things are

(h.120)

(h.121)

(h.122)

(h.123)
(h.124)
(h.125)

770

optimization

to posit more.

mostly driven by the    uctuations in the noise-in-the-gradient term. we   ve said

that      n(     ) = o(1); to say anything more about the uncertainty in (cid:98)  n, we need
it is very common to be able to establish that      n(     ) = o(1/

n), often
because   n is some sort of sample- or time- average, as in my example above,
and so an ergodic theorem [[todo: cross-ref]] applies. in that case, because
h   1

n = o(1), we have

   

if we can go further, and    nd

then we can get a variance for (cid:98)  n, using propagation of error (g):

   

(cid:98)  n           = o(1/
v [     n(     )] = kn

n)

= v(cid:104)(cid:98)  n          (cid:105)
= v(cid:2)   h   1

n      n(     )(cid:3)

(cid:105)

v(cid:104)(cid:98)  n

    j   1(     )v [     n(     )] j   1(     )
= j   1knj   1

should have nkn     k, for a limiting variance k. then nv(cid:104)(cid:98)  n
   
the infamous sandwich covariance matrix. if      n(     ) = o(1/
to    nd, but we can estimate it by j((cid:98)  n), or even by h   1

of course we don   t know j(     ), because that involves the parameter we   re trying

n ((cid:98)  n). that still leaves

(cid:105)     j   1kj   1.

n), then we

getting an estimate of kn. if   n is an average over the xi, as in my initial examples,
then we can often use the variance of the gradients at each data point as an
estimate of kn. in other circumstances, we might actually have to think.
tral limit theorem, then so is(cid:98)  n, and we can use gaussians for hypothesis testing,
finally, if      n(     ) is asymptotically gaussian, because it   s governed by a cen-

con   dence regions, etc.

now, there are a lot of steps here where i am being very loose. (to begin with:
in what sense is the random function   n converging on (cid:96), and does it have to
converge everywhere, or just in a neighborhood of      ?) turning this sketch into a
rigorous argument is the burden of good books on asymptotic statistics. but this
is the core. it does not require the use of likelihood, or correctly speci   ed models,
or independent data, just that the id168 we minimize be converging, in a
well-behaved way, to a function which is nicely behaved around its minimum.

h.5.5 application to maximum likelihood

a case where we can short-circuit a lot of thinking is when the model is correctly
speci   ed, so that the data-generating distribution is m(  ;      ), and the id168
is the negative mean log-likelihood. (that is, we are maximizing the likelihood.)
then the negative of the limiting hessian matrix j is the fisher information.
more importantly, under reasonable conditions, one can show that j = k, that the

h.5 small-noise asymptotics for optimization

771

variance of the gradient is exactly the limiting negative hessian matrix. then the
variance of the estimate simpli   es to just j   1. this turns out to actually be the
best variance you can hope for, at least with unbiased estimators (the cram  er-
rao inequality). but the bulk of the analysis doesn   t depend on the fact that
we   re estimating by maximum likelihood; it goes the same way for minimizing
any well-behaved objective function.

[[todo: smooth over break, following text taken from old multivariate chap-

ter]]

the approach which has generally replaced the method of moments is simply
the method of maximum likelihood. the likelihood is de   ned in exactly the same
way for multivariate distributions as for univariate ones. if the observations (cid:126)xi are
assumed to be independent, and    stands for all the parameters bundled together,
then

m (  ) =

p((cid:126)xi;   )

(h.126)

and the maximum likelihood estimate (id113) is

m (  )

(h.127)

it is usually simpler and more stable to use the log-likelihood:

(cid:96)(  ) =

log p((cid:126)xi;   )

(h.128)

since

i=1

argmax

m (  ) = argmax

(cid:96)(  )

(h.129)

(cid:98)  m le, we need some idea of uncertainty. we can get that pretty generically from

of course, for id136, we generally need more than just a point estimate like

  

  

maximum likelihood. very informally, since we are maximizing the log-likelihood,
the precision with which we estimate the parameter depends on how sharp that
maximum is     the bigger the second derivative, the more precise our estimate.
in fact, one can show (wasserman, 2003,   9.7 and 9.10) that

n(cid:89)

i=1

n(cid:88)

(cid:98)  m le = argmax

  

where   0 is the true parameter value, and h is the hessian matrix of the
log-likelihood, its matrix of second partial derivatives,

(cid:98)  m le (cid:32) mvn (  0,   h   1(  0))
(cid:12)(cid:12)(cid:12)(cid:12)  
(cid:21)
(cid:20)    2 log p(x;   0)

cjk(  ) =

     j     k

   2(cid:96)

in turn,

1
n

hjk(  0)     e

       jjk(  0)

     j     k

(h.130)

(h.131)

(h.132)

772

optimization

(cid:105)

which de   nes the fisher information matrix j. one can therefore get (ap-

proximate) con   dence regions by assuming that (cid:98)  m le has a gaussian distribu-
tion with covariance matrix n   1j   1((cid:98)  m le) or    h   1((cid:98)  m le). we thus get that
v(cid:104)(cid:98)  m le

= o(n   1), and (cid:98)  m le       0 = o(n   1/2).

note that eq. h.130 is only valid as n        , and further assumes that (i) the
model is well-speci   ed, (ii) the true parameter value   0 is in the interior of the
parameter space, and (iii) the hessian matrix is strictly positive. if these condi-
tions fail, then the distribution of the id113 need not be gaussian, or controlled
by the fisher information matrix, etc.

an alternative to the asymptotic formula, eq. h.130, is simply parametric or

non-parametric id64.

h.5.6 the akaike information criterion

[[todo: take from networks notes]]

[[todo: explain connection to leave-one-out cv]]
[[todo: explain how it   sin- consistent for model selection]]

h.6 further reading

my usual reference on optimization methods is lange (2013), but there are many
other good ones. boyd and vandenberghe (2004) is, deservedly, the standard
reference on theory and algorithms for optimizing convex functions.

spu   ord (2010) is the best historical novel about attempting to achieve utopia
through the power of optimization and computers, and should be read by anyone
trying to use data to change the world.

the material on optimizing noisy functions, and in particular on the behavior of
maximum likelihood, is a simpli   ed view of classical parts of theoretical statistics,
much of it dating back to fisher (1922); cram  er (1945) gives a detailed treatment,
with extensive references to the earlier literature. most presentations require more
advanced math (measure theory) than i use here; barndor   -nielsen and cox
(1995) can be recommended as a thorough, modern account. for the importance
of considering what happens when the model is false, see huber (1967) and white
(1994). the simple approach to consistency of   [[ref]] comes from van der vaart
(1998,   5.2). the best single paper to read on this whole subject is geyer (2013).

exercises

h.1 let p(x1:n;   ) be a family of id203 densities parameterized by a vector   . (you can
make them gaussians for concreteness if you want, but it doesn   t matter.) de   ne mn(  ) =
   n   1 log p(x1:n;   ). suppose that x1:n really are generated from this distribution, with
parameter      . prove fisher   s identity:

e(cid:2)      mn(  

   

)(cid:3) = v(cid:2)   mn(  

   

)(cid:3)

(h.133)

exercises

773

taking all derivatives with respect to   . assume you can interchange di   erentiation and
integration whenever it is convenient to do so. hints: integration by parts; all id203
densities integrate to 1;     log f (x)/   x = 1

f (x)    f /   x.

appendix i

where the   2 null distribution for the

likelihood-ratio test comes from

here is a very hand-wavy explanation for eq. 2.35. we   re assuming that the true
parameter value, call it   , lies in the restricted class of models   . so there are q
components to    which matter, and the other p     q are    xed by the constraints
de   ning   . to simplify the book-keeping, let   s say those constraints are all that
the extra parameters are zero, so    = (  1,   2, . . .   q, 0, . . . 0), with p     q zeroes at
the restricted id113 (cid:98)   obeys the constraints, so
the end.
(cid:98)   = ((cid:98)  1,(cid:98)  2, . . .(cid:98)  q, 0, . . . 0)
(cid:98)   = ((cid:98)  1,(cid:98)  2, . . .(cid:98)  q,(cid:98)  q+1, . . .(cid:98)  p)

because both id113s are consistent, we know that (cid:98)  i       i, (cid:98)  i       i if 1     i     q,
and that (cid:98)  i     0 if q + 1     i     p.
very roughly speaking, it   s the last extra terms which end up making l((cid:98)  )
larger than l((cid:98)  ). each of them tends towards a mean-zero gaussian whose vari-

the unrestricted id113 does not have to obey the constraints, so it   s

(i.2)

(i.1)

second order.

p   q distribution.

1 variables, which has a   2

a maximum over a larger space than the latter. let   s try to see how big the

ance is o(1/n), but their impact on the log-likelihood depends on the square of
their sizes, and the square of a mean-zero gaussian has a   2 distribution with
one degree of freedom. a whole bunch of factors cancel out, leaving us with a
sum of p     q independent   2

in slightly more detail, we know that l((cid:98)  )     l((cid:98)  ), because the former is
di   erence is by doing a taylor expansion around (cid:98)  , which we   ll take out to
((cid:98)  j    (cid:98)  j)
l((cid:98)  )     l((cid:98)  ) +
= l((cid:98)  ) +

p(cid:88)
((cid:98)  i    (cid:98)  i)
p(cid:88)
p(cid:88)

(cid:18)    l
((cid:98)  i    (cid:98)  i)

p(cid:88)
p(cid:88)
((cid:98)  i    (cid:98)  i)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)  
(cid:19)
((cid:98)  j    (cid:98)  j)

all the    rst-order terms go away, because (cid:98)   is a maximum of the likelihood,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)  
(cid:19)
(cid:18)    2l

(cid:18)    2l

(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)  

     i     j

     i     j

(cid:19)

(i.3)

     i

1
2

1
2

j=1

j=1

+

i=1

i=1

i=1

and so the    rst derivatves are all zero there. now we   re left with the second-
order terms. writing all the partials out repeatedly gets tiresome, so abbreviate
   2l/     i     j as l,ij.

774

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

  2 and likelihood ratios

775

to simplify the book-keeping, suppose that the second-derivative matrix, or
hessian, is diagonal. (this should seem like a swindle, but we get the same
conclusion without this supposition, only we need to use a lot more algebra    
we diagonalize the hessian by an orthogonal transformation.) that is, suppose [[todo:
l,ij = 0 unless i = j. now we can write

p(cid:88)
((cid:98)  i    (cid:98)  i)2l,ii
l((cid:98)  )     l((cid:98)  )         1
(cid:105)         q(cid:88)
((cid:98)  i    (cid:98)  i)2l,ii     p(cid:88)
l((cid:98)  )     l((cid:98)  )

i=1

2

(cid:104)

2

i=1

i=q+1

((cid:98)  i)2l,ii

(i.4)

(i.5)

out
less-

write
the
swindly
version]]

at this point, we need a fact about the asymptotic distribution of maximum
likelihood estimates: they   re generally gaussian, centered around the true value,
and with a shrinking variance that depends on the hessian evaluated at the true
parameter value; this is called the fisher information, f or i. (call it f .) if
the hessian is diagonal, then we can say that

(cid:98)  i ; n (  i,   1/nfii)
(cid:98)  i ; n (  1,   1/nfii) 1     i     q
(cid:98)  i = 0 q + 1     i     p

(i.6)

(i.7)

(i.8)

also, (1/n)l,ii        fii.

putting all this together, we see that each term in the second summation in

eq. i.5 is (to abuse notation a little)

   1
nfii

(n (0, 1))2l,ii       2

1

(i.9)

so the whole second summation has a   2

meanwhile, goes to zero because (cid:98)  i and (cid:98)  i are actually strongly correlated, so

p   q distribution1. the    rst summation,

their di   erence is o(1/n), and their di   erence squared is o(1/n2). since l,ii is
only o(n), that summation drops out.

a somewhat less hand-wavy version of the argument uses the fact that the
id113 is really a vector, with a multivariate normal distribution which depends
on the inverse of the fisher information matrix:

(cid:98)   ; n (  , (1/n)f    1)

(i.10)

then, at the cost of more id202, we don   t have to assume that the hessian
is diagonal.

1 thanks to xiaoran yan for catching a typo in a previous version here.

appendix j

rudimentary id207

a graph g is built out of a set of nodes or vertices, and edges or links
connecting them. the edges can either be directed or undirected. a graph with
undirected edges, or an undirected graph, represents a symmetric binary relation
among the nodes. for instance, in a social network, the nodes might be people,
and the relationship might be    spends time with   . a graph with directed edges,
or arrows, is called a directed graph or digraph1, and represents an asymmetric
relation among the nodes. to continue the social example, the arrows might
mean    admires   , pointing from the admirer to the object of admiration. if the
relationship is reciprocal, that is indicated by drawing a pair of arrows between
the nodes, one in each direction (as between a and b in figure j.1).

a directed path from node v1 to node v2 is a sequence of edges, beginning
at v1 and ending at v2, which is connected and which follows the orientation of
the edges at each step. an undirected path is a sequence of connected edges
ignoring orientation. (every path in an undirected graph is undirected.) if there
is a directed path from v1 to v2 and from v2 to v1, then those two nodes are
strongly connected. (in figure j.1, a and c are strongly connected, but a
and d are not.) if there are undirected paths in both directions, they are weakly
connected. (a and d are weakly connected.) strong connection implies weak
connection (exercise 1). we also stipulate that every node is strongly connected
to itself.

strong connection is an equivalence relation, i.e., it is re   ective, symmetric and
transitive (exercise 2). weak connection is also an equivalence relation (exercise
3). therefore, a graph can be divided into non-overlapping strongly connected
components, consisting of maximal sets of nodes which are all strongly con-
nected to each other. (in figure j.1, a, b and c form one strongly connected
component, and d and e form components with just one node; exercise ??.)
it can also be divided into weakly connected components, maximal sets of
nodes which are all weakly connected to each other. (there is only one weakly
connected component in the graph. if either of the edges into d were removed,
there would be two weakly connected components; exercise ??.)

a cycle is a directed path from a node to itself. the existence of two distinct
nodes which are strongly connected to each other implies the existence of a cycle,
and vice versa (exercise 6). a directed graph without cycles is called acyclic.

1 or, more rarely, a guthrie diagram.

776

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

j.1 exercises

777

a

b

c

e

d

figure j.1 example for illustrating the concepts of id207.

said another way, an acyclic graph is one where all the strongly connected compo-
nents consist of individual nodes. the weakly connected components can however
contain an unlimited number of nodes.

in a directed acyclic graph, or dag, it is common to refer to the nodes
connected by an edge as    parent    and    child    (so that the arrow runs from the
parent to the child). if there is a directed path (of any length) from v1 to v2,
then v1 is the ancestor of v2, which is the descendant of v1. in the jargon,
the ancestor/descendant relation is the transitive closure of the parent/child
relation.

j.1 exercises

1. prove that if two nodes are strongly connected, they are also weakly connected.
draw a graph in which two nodes are weakly connected but not strongly
connected.

2. prove that strong connection between nodes is an equivalence relation.

1. re   exive prove that every node is strongly connected to itself.
2. symmetric prove that if a is strongly connected to b, then b is strongly

connected to a.

3. transitive prove that if a is strongly connected to b, and b is strongly

connected to c, then a is strongly connected to c.

3. prove that weak connection between nodes is an equivalence relation. divide

the proof into parts as in exercise 2.

778

rudimentary id207

4. verify that the graph in figure j.1 has three strongly connected components,

{a, b, c}, {d} and {e}.

5. verify that the graph in figure j.1 has only one weakly connected component.
check that if either of the edges into d were removed, there would be two
weakly connected components     what would they be?
6. prove that if a is strongly connected to even one other node b (cid:54)= a, then

there is a cycle in the graph.

appendix k

missing data

779

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

780

missing data

there was a point to the blank page, beyond the obvious joke. tautologously,
missing data is data we do not have. we don   t know what it would have been.
anything we say about it is guesswork, based on assumptions. all statistical
id136 rests on assumptions, but they are especially hard to take for granted,
and especially hard to check or to justify, when they   re assumptions about things
we know we don   t have evidence about.

to be a bit more formal,    missing data    refers to situations where we are able
to record some variable for some but not all of the units of analysis in our data
set. conventionally, in r, those are na values in our data frame1 variables we
never measure are not missing, but latent or hidden, or indeed ignored. if we
do a face-to-face survey about people   s    nances, our survey-takers would typically
be able to record the interviewees    eye colors and the number of times they say
   um   , but wouldn   t do so: those variables are ignored. if some but not all of
those surveyed refuse to say how much they spend on housing every month, that
is missing data. missing data is very, very common in the real world.

we would (in general) draw di   erent id136s if only we had the complete
data, the data we actually got plus the data that is, in fact, missing. but (to
hammer home the point), precisely because it   s missing, we don   t have direct
evidence about how our id136s would change. any e   ort to take missing data
into account will rely on assumptions about what we would have seen. any anal-
ysis which tries to just ignore missing-ness is only going to make sense under
assumptions of its own.

to give an initial feel for the kinds of problems which arise, start with figure
k.12. it   s a scatter-plot of a kind you   ve seen many times already, where you may
suppose we   re interested in how y depends on x.

looking carefully at figure k.1, you may notice that there are more tick-marks
on the horizontal axis than there are points in the scatter-plot. these tick-marks
come from data points where i am treating y as missing but x as observed. (in
fact, to keep things simple, i have made it so that x is never missing.) in the
observed data, higher values of x clearly predict higher values of y . whether
that is also true over-all depends on what the missing values are like. figure
k.2 illustrates a few di   erent possibilities, all of which are equally compatible,
logically, with the observations in figure k.1.

more concretely, figure k.2 illustrates four di   erent data-generating processes,
   lling in the missing values. the relationship between x and y implied by the
   lled circles in figure k.2b is clearly very di   erent from the one implied by

1 r, as a modern computer language, is capable of doing arithmetic with na values, and (correctly)

      propagates    them, so that anything plus na is also na, etc. in earlier times, however, many
computing systems lacked an na value, and so particular numbers were sometimes used to    code   
missing values. common choices included    1, 0, and (very insidiously) 99 and 999. (for a case
where 99 was the missing value code, but some 99s were apparently mis-entered as 88s, leading to
very surprising conclusions, see kahn and udry (1986).) i myself once worked with a data-set which
coded missing values of worker   s ages 66, because everyone was supposed to retire at 65.

2 this is a simulation, not real data, but it   s inspired by a project i participated in on predicting

which people who had been arrested could be safely released while awaiting trial. the code
generating it appears on p. 804, but please don   t peek ahead.

missing data

781

figure k.1 running example for illustrating missing-data issues. the
rug-plots along the axes indicate the marginal distributions of the observed
(not missing) values. the generating code is deliberately hidden here, but
will be given at the end of the chapter.

the empty diamonds in figure k.2d. but all four processes, and in   nitely many
others, are equally compatible with the fully-observed data points. this makes it
vivid that there are two big tasks when dealing with missing data:

1. almost all of our computational procedures presume complete data sets. how,
as a technical matter, can we use data sets with missing values in such proce-
dures, and under which assumptions are di   erent techniques good ideas?

2. how, if it all, can we check assumptions about missing-ness?

since the right techniques for handling missing data depends on assumptions,

llllllllllllllllllllllllllllll0204060800.00.20.40.60.81.0xy782

missing data

figure k.2 some possible complete data sets, all compatible with the
observations in figure k.1. the four point shapes indicate four distinct
data-generating processes. in all four, the hollow circles are the
fully-observed data points.

you would be forgiven for thinking that assumption-checking is more important
than techniques. unfortunately, as we   ll see in   ??, there are good reasons why
statisticians have given much more attention to techniques.

k.0.1 notation and preliminaries

we will often need a concise way to refer to whether or not a variable is missing.
my will be the indicator variable which is 1 when y is missing and 0 when y is
observed. if we need to refer to its value in a particular observation i, we   ll write

llllllllllllllllllllllllllllll0204060800.00.20.40.60.81.0axyllllllllllllllllllllllllllllllllllllllllllllllllll0204060800.00.20.40.60.81.0bxyllllllllllllllllllllllllllllllllllllllllllll0204060800.00.20.40.60.81.0cxyllllllllllllllllllllllllllllll0204060800.00.20.40.60.81.0dxyk.1 deletion, and missing-at-random assumptions

783

that myi. we will also often refer to the id203 that an observation is not
missing, pr (my = 0), the inclusion or capture id2033,    = 1     e [my ].
when this is conditioned on x = x, we   ll write   (x). when we need to collectively
refer to all the fully-observed values of y , they will be yobs (realization, yobs), and,
likewise, the collection of all missing values will be ymiss (realization, ymiss).

we would like to know about the complete-data distribution, what generated
the data before some of it went missing. depending on how ambitious we are,
this might be pr (y = y), or pr (y = y|x = x), or pr (y = y, x = x). (as usual,
everything works out just the same for continuous id203 densities as for
discrete id203 mass functions, so i   ll just write things out for the latter.)
basic id203 tells us that

pr (y = y, x = x)
(k.1)
= pr (y = y, x = x, my = 0) + pr (y = y, x = x, my = 0)
= pr (y = y, x = x|my = 0) pr (my = 0) + pr (y = y, x = x|my = 1) pr (my = 1)
(k.2)
= pr (y = y, x = x|my = 0)    + pr (y = y, x = x|my = 1) (1       ) ,
(k.3)
that
pr (y = y|x = x)
= pr (y = y, my = 0|x = x) + pr (y = y, my = 1|x = x)
(k.4)
= pr (y = y|x = x, my = 0) pr (my = 0|x = x) + pr (y = y|x = x, my = 1) pr (my = 1|x = x)
(k.5)
= pr (y = y|x = x, my = 0)   (x) + pr (y = y|x = x, my = 1) (1       (x)) ,
(k.6)
and that

pr (y = y) = pr (y = y, my = 0) + pr (y = y, my = 1)

= pr (y = y|my = 0)    + pr (y = y|my = 1) (1       ) .

(k.7)
(k.8)

in every case, what we want is the complete-data expression on the left-hand
side. but what we might be able to identify from the data are just the    rst parts
of the sums on the right, where we   re conditioning on my = 0, i.e., condition-
ing on y being observed. the complete-data expressions will be identi   ed from
observations only under assumptions which somehow tie the missing-data terms,
where my = 1, to things we observe.

k.1 deletion, and missing-at-random assumptions

the simplest way to handle missing data is to not use incomplete records     to
delete them4. this can be done in multiple ways, depending on how aggressive
one wants to be about using the variables that aren   t missing.

one extreme of deletion is to drop all records which are incomplete in any

3 it may seem perverse to have a missing indicator and an inclusion id203, but these choices will

simplify formulae later, and anyway are conventional.

4 despite the name, it is generally a bad idea to actually delete them from your main data    le!

instead, drop rows from working copies of the data-frame in your code.

784

missing data

variable5 this has come to be called listwise deletion. its greatest advantage is
stark simplicity. with a data-frame in r, this amounts to taking the columns you
need, and then dropping any row with an na, as performed by na.omit(). this
is the default behavior of most model-   tting functions, once they   ve determined
which columns of the data-frame they will actually use.

just because something is r   s default behavior does not, however, mean it is
a good idea6. while no good might come of throwing away some of our data
at random, random deletion at least wouldn   t lead to systematic mistakes. but
the one thing we know about the rows with missing values is that they are
systematically di   erent from the complete rows in an important way     namely,
some variables are missing! listwise deletion would seem, on the face of it, to be
a recipe for creating biased samples and introducing systematic errors. if it   s to
make any statistical sense, very strong assumptions will be required.

k.1.1 practicalities of data analysis after deletion

if we make such assumptions, though, our life can be very straightforward. figure
k.3 shows what our running example data set looks like after deletion, with a
simple smoothing spline run through it.

the only even slightly subtle thing to remember is that our sample size has
shrunk: it   s not the original number of data points, but just the number of fully-
observed data points.

k.1.2 assumptions justifying deletion

k.1.2.1 missing completely at random

one strong assumption which would justify deletion is that of    missingness com-
pletely at random    (mcar), that my        y, x. the idea is that, in e   ect, some-
body went down the rows of the data frame, tossed a coin for each row, and
erased y whenever the coin came up heads. in a situation like this, the com-
plete rows really are a representative sample of the complete data. formally, the
independence assumed by mcar means that

pr (y = y, x = x) = pr (y = y, x = x|my = 0)

(k.9)

so anything we could identify from the complete data (because it   s a function
of the joint distribution) is also something we can identify from the distribution
of the data after deletion. (can you prove this, using eq. k.3?) similarly, since
our estimates are built using an e   ectively-random subset of the total sample,
it   s as though we just had a somewhat smaller random sample to begin with, or
at least one that   s no more biased than the complete data. anything procedure

5 that is, any variable you are actually using in a given analysis.
6 by now, you   ve learned this lesson when it comes to the precision with which numerical results

should be reported.

k.1 deletion, and missing-at-random assumptions

785

which should be consistent with the complete data should also be consistent after
deletion, if we assume mcar.

mcar is, obviously, a very strong assumption, and, honestly, hard to believe
in lots of real-world situations. it   s not logically impossible that the people who
won   t tell interviewers how much they spend on housing are just like those who
do say, both in terms of their income and their housing expenses, but it   s not
very plausible. in fact, because my        x, y implies my        x, this is one of
the easiest assumptions about missing-ness to dis-prove (see   k.4). in the case of
figure k.1, it   s clear by inspection that large and small values of x predict very
di   erent rates of missing-ness for y , so mcar is very implausible here7.

k.1.2.2 missing at random

because mcar is so strong, and usually so implausible, but deletion is so tempt-
ing, people have looked for weaker assumptions which would still justify ignoring
incomplete records. one favorite has come to be called missing-at-random
(mar), or ignorable missingness or uninformative missingness. it is sim-
ply that

my        y |x

(k.10)

by the de   nition of conditional independence, this is equivalent to saying that,
for all x, y,

pr (y = y|x = x, my = 0) = pr (y = y|x = x, my = 1)

(k.11)

in words: given x, the missing values of y follow exactly the same id203
distribution as the observed values8. from this, and eq. k.6, it follows that

pr (y = y|x = x) = pr (y = y|x = x, my = 0)

(k.12)

thus, under mar, any function of the complete-data conditional distribution
can be calculated directly from the observed conditional distribution. in fact, the
complete-data joint distribution is identi   ed under mar, though it   s not equal
to the observed joint distribution (exercise 1).

because mar is all about conditional independence, it   s very natural to want
to apply the graphical modeling ideas introduced in chapter 20. thus figure k.4
shows the simplest, though by no means the only, graphical model in which mar
holds, but mcar (in general) does not.

it   s natural to wonder if anything weaker than mar could still justify dele-
tion. the answer is pretty much    no   . if we want eq. k.12 to hold, so that
the complete-data and observed conditional distributions are equal, then the
assumption-free eq. k.6 implies eq. k.11, and we   re back to mar.

7 how might you conduct a formal test, if you   re not willing to believe me? hint: can you apply
8 the id203 of being missing can change with x, so the marginal distributions y |my = 0 and

chapter 9?
y |my = 1 need not be equal; see exercise 2.

786

missing data

k.1.3 partial deletion

the disadvantage of listwise deletion is that records which are missing one variable
might still have perfectly good values for other variables. if we assume missingness
is uninformative, not using those records won   t introduce biases, but it will reduce
the precision of some statistics. in partial deletion (a.k.a. pairwise deletion,
or    available pairs   ), we use incomplete data points to calculate whatever
statistics they can help with. for example, in id75, a point which
is missing y can still contribute to calculating xt x. in fact, a point which also
missing some columns of x could still contribute to some entries in xt x. thus, in
partial deletion, we try to stretch out the data as much as possible. the ability
to do this why i do not recommend just running na.omit on your data, even if
you   re willing to assume mar (see also exercises 1 and 2).

as usual, these advantages come at a price. because di   erent calculations are
done with more-or-less di   erent data sizes, it is possible to get mutually inconsis-
tent results. thus if you estimate v [x], v [y ] and cov [x, y ] when y is missing
using partial deletion, you might get a correlation coe   cient bigger than 1 or
smaller than    1.

k.2 informative missingness, or missing-not-at-random

the opposite of missing-at-random is, naturally, missing-not-at-random, or
mnar:

my (cid:54)       y |x

(k.13)

this also has the more comprehensible names of non-ignorable missingness
or informative missingness (see exercise 3).

when missingness is informative, arguments parallel to the ones we used to

justify deletion under mar tell us that deletion is a bad idea. speci   cally,

pr (y = y|x = x) (cid:54)= pr (y = y|x = x, my = 0)

(k.14)

so the observed data doesn   t follow the same conditional distribution as the com-
plete data, and deletion will give us a biased, systematically-distorted idea of that
conditional distribution.

there is very little more which can be said at this level of generality at mnar.
in particular, while missingness is informative, it might or might not be very
informative. it can also take many di   erent forms. two of the most common are
censoring and selection.

k.2.1 censoring

when y values in certain ranges are just not observed, y is censored. the most
common examples of this are right censoring, when we never see y > ymax,
and left censoring, when we never see y < ymin. the classic setting for right-
censored data is when y is the time to some event     how long patients live, or

k.2 informative missingness, or missing-not-at-random

787

how long a machine lasts before breakdown, etc. any patient who is still alive at
the end of the study that collected the data will die eventually, so they have some
survival time, but it   s missing, and the missingness is directly a function of how
long they   ve lived. as a classical problem, it has a classical solution, the    product-
limit    or kaplan and meier (1958) estimator (implemented in, e.g., the survival
package (therneau, 2015)). this, however, rests on the assumption that the time
at which each observation is censored is deterministic, or at least independent
of the actual lifespan9. lifespans or durations can also be left-censored, if events
which happen too quickly don   t show up in our data10.

k.2.2 selection

in many situations, we only get to observe y for individuals (cases, etc.), which
are somehow selected into one condition or another, but the process of selection
is itself (supposed to be) sensitive to what y would be. that   s a rather vague,
abstract statement, but concrete cases are very common. admissions processes at
many schools deliberately try to select students who will do well at the school, but
we only get to see academic outcomes for students who were selected. if y is any
measure of academic outcomes, then, there   s potentially informative missingness.
ignoring such selection can be seriously misleading. as a little simulation to
prove the point, i    rst generate test scores uniformly distributed between 200
and 1600, and then a subsequent grade in the range of 0 to 4, which is a linear
and increasing, but noisy, function of the test scores:

n <- 1000
test.scores <- runif(n, min=200, max=1600)
gpas <- 4*(test.scores-200)/1400 + rnorm(n, sd=0.5)

the correlation between scores and outcomes is unsurprisingly high, 0.91. but if
i only look at those who scored above, say, 1300, the correlation drops immensely,
to 0.42. people who dislike using test-scores in admissions decisions sometimes
point to evidence that such scores are poor predictors of success in academic
programs among those admitted, which is often true, but is exactly what we
would expect if such scores were good predictors and used for selection11

this is not just a literally-academic issue. perhaps the most consequential place
such issues arise is in courts and prisons, which make decisions about who will be
arrested, who will be released on bail between arrest and trial, who will be released
on parole, etc. increasingly, such decisions are made using predictive statistical

9 that is, it   s assumed that the real life-time of unit i is yi, that it is lost to observation at a time li,

that we see yi if yi     li, and that li is either constant for all i, or that yi        li.

10 this can be an issue when studying the lifespans of social or political movements, online fads, etc.
11 this is a very old point     see dawes (1975)     but still a valid one. (dawes raises the additional,
more subtle point that when those admitted are selected on multiple variables, say x1 and x2, we
are conditioning on a collider (see ch. 21), which creates negative correlations among the predictors,
and tends to make each predictor only weakly related to the outcome y .)     i say all this as
someone who generally doesn   t like emphasizing standardized tests in admissions decisions, but for
other reasons.

788

missing data

models12. in developing these models, there is information about whether (for
example) those released on bail show up for their trial, commit another crime13,
etc., but this information is missing for those who are kept in jail while awaiting
trial. any y which measures those outcomes is potentially subject to informative
missingness14

in these situations, it   s imaginable that y is really missing at random, con-
ditional on the right set of x variables. after all, the people currently making
decisions about bail (mostly judges) can   t see the future to know what y would
be, but rather have to rely on cues and signals that actually exist when they make
their decisions15. one can imagine bundling all those cues and signals up into an
x variable, and then it would be positively unreasonable to think anything other
than y        my |x (see figure k.5). but if we as data-analysts don   t have access
to exactly the x used by the decision-makers, but instead some other variable
x(cid:48), it could very easily be that y (cid:54)       my |x(cid:48), so that missingness is informative
for us (see figure k.5 again).

in e   orts to develop models for deciding on pre-trial release, for example, one
commonly has data about the demographic characteristics of the arrestee (age,
sex, etc.), what they have been arrested for, and their prior history of arrests
and convictions16. but a judge might consider other factors, such as the repu-
tation of the arresting o   cer, or testimony about the character of the arrestee,
or the arrestee   s appearance17 the decisions made by the selectors would then
re   ect some information about y which is not available to us in x(cid:48). this can,
in fact, completely change the apparent implications of the variables in x(cid:48). if,
overall, people who have been people who have been convicted of many violent
crimes are especially likely to re-o   end before trial if granted bail, they might
appear unusually safe, because judges only grant them bail when (say) there
is abundant and credible testimony that they have reformed. that is, when
these decisions are made accurately, using cues and signs that really are in-
formative, pr (re-o   end|history of violence, m=0) can be very low, even though

12 i have been myself involved in an e   ort to evaluate pre-trial release models for a non-pro   t

organization.

13 more precisely: about whether they are arrested for another crime.
14 the same issue arises for credit risk: lenders try to select loan applicants who will repay, but we

don   t see whether those denied loans would have repaid. any y re   ecting repayment is, then,
potentially subject to informative missingness. but let   s stick with crime and punishment, rather
than banking, for right now.

15 this is why econometricians sometimes refer to mar as    selection on observables   . it is also one

way selection di   ers, conceptually, from censoring, where my depends directly on y .

16 though not always. the prior legal history is itself missing more often than anyone should like. in

large part this is because di   erent organizations (e.g., police vs. courts vs. prisons vs. parole o   ces),
even within the same legal jurisdiction, do a very bad job at sharing and linking up their records.
even the same organization may not have a good way to keep track of whether the joe smith now
on trial for theft is the same as the j. e. smith previously convicted of fraud.

17 as this last indicates, nothing says that everything in x(cid:48) has to be either ethically legitimate or
rationally linked to y . (devising situations where it is legitimate, rational and legal for a judge to
be in   uenced by how an arrestee looks when deciding whether or not to grant bail is left as an
exercise for your ingenuity.)

k.2 informative missingness, or missing-not-at-random

789
p robre-o   end|history of violence is very high. under less extreme circumstances,
a history of violent crime might seem to pose less of a risk than a history of
non-violent crime, for the same reason.

censoring and selection are not the only two kinds of informative missingness,
but it   s almost impossible to give a complete catalog. in every event, for further
work, it   s important to investigate the precise mechanisms leading to missingness.
this needs to be done at two levels, the statistical or probabilistic, and the
substantive.

k.2.3 the    missingness mechanism   , statistically considered

in the statistics of missing data, the    missingness mechanism    has come to refer
to the conditional distribution

pr (my = 1|x = x, y = y) = 1       (x, y)

(k.15)

(of course, it works just as well to know the id155 that my = 0.)
recall that when we   re trying to    nd the conditional distribution of y given x,
the data lets us identify

pr (y = y|x = x, my = 0) pr (my = 0|x = x)

but (eq. k.6) we need that plus

pr (y = y|x = x, my = 1) (1       (x))

(k.16)

(k.17)

to get pr (y = y|x = x). but basic id203 (exercise 5) tells us that

pr (y = y|my = 1, x = x) =

pr (my = 1|x = x, y = y) pr (y = y|x = x)

pr (my = 1|x = x)

so (exercise 6)

pr (y = y|x = x) = pr (y = y|x = x, my = 0)

  (x)
  (x, y)

(k.18)

(k.19)

so long as   (x, y) > 0. we thus have an expression for the complete-data condi-
tional distribution, in terms of the observable conditional distribution, and the
missingness mechanism, or the inclusion probabilities. knowing the latter allows
us to, so to speak, undo the distortions due to missingness. and, because we   ve
assumed nothing but basic id203 to get eq. k.19, any assumption which
is strong enough to let us identify pr (y |x) under mnar has to either be an
assumption about   (x)/  (x, y), or has to imply the form of that ratio18.

18 in fact, mar is the assumption that   (x)/  (x, y) = 1 for all x, y, so that the exact inclusion

probabilities can be ignored (exercise 7).

790

missing data

k.2.3.1 example: heckman selectivity19

an example of modeling assumptions which de   ne a missingness mechanism
comes from a situation of selection that originally arose in studying the dis-
tribution of wages. a basic model of wage labor is that every individual has a
   reservation wage   , an amount that they would have to be paid to accept em-
ployment at all20. people may refuse o   ers of employment above their reservation
wage, but they will de   nitely refuse o   ers below their reservation wage. some
people may therefore not be employees (at any given time) because they haven   t
received any o   er exceeding their reservation wage. if we want to know how the
wages people can command in the labor-market vary with their characteristics,
it   s important to have some idea of what those missing wages would have been.
stated at this level of generality, the problem is unsolvable. the distribution
of wages below the reservation wage could, logically speaking, be absolutely any-
thing, without altering the observable distribution at all. economic theory does
not, in this case, provide any real constraints either. as they have so often done
when faced with an unsolvable problem and unguided by substantive economics,
econometricians have responded by trying to make id75 work. specif-
ically, in the heckman (1976) model, we assume that (log) wages y are linear in
a covariate x,

as are (log) reservation wages,

y =   1x +  

r =   2x +   

(k.20)

(k.21)

with   and    being independent of x and sharing a gaussian distribution with
mean 0 and variance matrix   . we further assume that we see the wages of
person i if, and only if, yi > ri,

my = (cid:49)(y < r)

(k.22)

under these assumptions, you can show that

  (x) =   (x(  1       2)/  )

(k.23)
where    is the standard gaussian cdf, and   2 is the variance of          (exercise
??). since the inclusion probabilities are observable, the composite parameter
(  1       2)/   can be identi   ed from observations (under all these assumptions). a
similar but longer calculation (exercise 84) shows that

e [y |x = x, my = 0] =   1x     c

  (x(  1       2)/  )
  (x(  1       2)/  )

(k.24)

where    is the standard gaussian pdf, and c is the covariance between   and
19 my treatment of this classic topic in econometrics is heavily indebted to manski (2007,     2.6 and

4.1   4.2).

20 as opposed to continuing to look for work in hopes of better jobs, going into business for themselves,

dealing with family responsibilities, sleeping under bridges and stealing bread, or whatever.

k.2 informative missingness, or missing-not-at-random

791
(        )/  . once we know (  1       2)/  , this lets us identify   1, which is usually
what   s of interest.

there are several points worth making about this example.

1. you can extend the same logic to many situations of selection beyond wages.
for instance, if we   re studying life-spans, we can have yi be the life of unit i,
and ri be the amount of time between when unit i appears and the end of the
study period. then we only get to observe a completed lifespan if yi     ri. it
is thus unsurprising that heckman (1976) has been cited over 5500 times21.

2. extending the model to multiple predictor variables is straightforward, if te-

dious, provided both y and r remain linear in the coordinates of x.

3. we can identify   1 without ever having to record the reservation wage r. this

is handy, because usually we can   t measure r at all.
4. the identifying power of the assumptions breaks down if we don   t assume that
( ,   )        x     for instance, heteroskedasticity in   or    is in general enough to
make things unidenti   able again.

5. economic theory provides absolutely no reason to think that (log) wages and
(log) reservation wages should be jointly gaussian, with conditional means
which are linear in the observed features, and homoskedastic disturbances.
many of those thousands of citations come from other areas of social science,
like sociology and education, where theory is even less de   nite, and no more
supportive of these assumptions.

  

6. we can use data to check whether eq. k.23 holds, because it concerns purely
observable quantities, with one unknown parameter. (in terms of the model,
that   s a composite,   1     2
, but that   s still only one adjustable parameter.) we
can also use data to check whether eq. k.24 holds, since that   s a parameteric
form with three unknown parameters (  1, c and   1     2
), albeit one which is
non-linear in x. one could even, in principle, test whether the data imply
the same value for   1     2
in the two equations. but (and this is crucial) we
cannot test the full model. there are many di   erent models which would also
imply both eqns. k.23 and k.24. (the simplest of these, though perhaps not
the most plausible, would say that inclusion probabilities follow eq. ??, while
y        my |z, and e [y |x = x] also follows the form given by eq. k.24.)

  

  

k.2.4 mechanisms of missingness, substantively

i have said that the    missingness mechanism   , in the jargon, is just the function
  (x, y) (or its complement). mathematically, if we know this, we know how to
deal with our missing data. this immediately raises the question of how we might
learn it. the only real way is to study how and why, exactly, some, but only some,
of our data comes to be missing. that is, we must study the actual mechanisms

21 see https://scholar.google.com/scholar?cites=16798156444849893273 (as of july 2018). a

related paper, heckman (1979), is claimed to have over 27000 citations
(https://scholar.google.com/scholar?cites=4067958607302478696), but i am not sure that isn   t
a record-linkage error in the database.

792

missing data

which lead to missingness. once we understand them, they will often suggest
one, or more, plausible statistical models, which in turn will get us to the   (x, y)
function we need for our calculations.

studying this part of the data-generating, or perhaps data-collecting or data-
creating, process is a key part of applied statistics, but it   s not a statistical task in
the same way that, say, estimating a regression surface is. rather, it is something
that requires a good deal of substantive, domain-speci   c knowledge, because the
actual causes of missingness are very di   erent in di   erent areas of inquiry. if we
are dealing with surveys, for instance, we have to investigate why people22 don   t
answer some questions on surveys (but not others). these causes will be very
di   erent from those which lead meteorological measuring stations to sometimes
fail to record the concentration of certain pollutants in the air. this in turn will
have nothing to do with gaps in credit records when making loan decisions.

k.2.5 causal id136 as a missing data problem, and vice versa

the most basic sort of causal id13623 is asking about the average e   ect of
changing the value of one cause, the average treatment e   ect (ate):

at e     e [y |do(x = 1)]     e [y |do(x = 0)]

(k.25)

if we have many units we can observe, say i = 1, . . . n, we might hope to approx-
imate this by the average of the e   ects for each unit:

yi|do(x = 1)     yi|do(x = 0)

(k.26)

i=1

unfortunately, we cannot simultaneously give unit i both treatments. thus one
or the other of the two values we want there is un-observed, or, in a word, missing.
this is why a lot of work on estimating treatment e   ects re-uses tools developed
for handling missing data, to the point where donald rubin, an eminent authority
in both areas, has been known to say that    causal id136 is a missing data
problem    (ding and li, 2018).

on the other hand, whether or not some variables are missing has, clearly, some
sort of probabilistic connection to those variables. if i have convinced you that
you should really investigate in detail why variables are missing, you will be led
to build models of the missing-data mechanism, and that in turn will lead you
to things like id114, which make assessing conditional independence
relations very straightforward. for instance, the simplest graphical model which
would support mar is given in figure k.4. from this perspective, whether or not
the relationship between y and x can be identi   ed when y is sometimes missing
turns on what features of the joint distribution are left alone when conditioning

22 that is, people who can be reached by the survey at all; why some people are easier to survey than

others is another, though related, problem.

23 if you have skipped around, it would be a good idea to read part iii, or at least chapter 21 before

going further in this section.

at e     n(cid:88)

k.3 further methods: imputation, em, weighting

793

on my , i.e., on what paths such conditioning opens or closes. one can, therefore,
completely reverse the perspective, and treat    missing data as a causal id136
problem    (mohan et al., 2013).

k.3 further methods: imputation, em, weighting

some methods for dealing with missing data can be used whether we assume
mar or informative missingness (though they will give di   erent answers in the
two cases). the most important ones are:
    making up, or imputing, values for the missing variables, and analyzing the
    averaging the log-likelihood function over all possible values of the missing
    weighting the complete observations, so that the point we see appropriate

variables, using the em algorithm;

completed data set;

represent the ones we missed.

we will deal with these in turn.

k.3.1 imputation

an alternative to any form of deletion is to make something up for the missing
values, and then analyze the data set, with both real and imputed values, as
though it were complete. the best reason to do such imputation is so that
the partial cases can be used in procedures (statistical or computational) which
require complete data. imputation never creates more information; at best, it
uses the available information e   ciently, and it can easily lead to systematic
distortions. it is thus a tool to be applied carefully. indeed, i rather suspect half
the reason we call this    imputation    is that calling it    making stu    up for the
missing values so we can use the data    is so blunt and explicit about what we   re
doing that it makes people nervous24.

k.3.1.1 imputation under mar

1. imputation by hopefully-representative constants the simplest and oldest sort
of imputation replaces every missing value of a variable by the same value,
derived from the observed cases, usually the mean, median or mode. this is
rarely a good idea, because it will distort the relationship between the imputed
variable and everything else. in particular, imputing a constant value for a
missing response variable will tend to attenuate any regression relationship.

2. imputation from the marginal distribution rather than using a single constant
value, one can also impute randomly, using the marginal distribution of ob-
served cases for the missing variable. the oldest form of this replaces each
missing value of y by independently sampling from the observed values of y .

24 the other half of the reason is that    making stu    up for the missing values so we can use the data   

is a mouthful.

794

missing data

one could also attempt to learn the distribution of y (by a parametric model,
nonparametric density estimation, etc.), and then draw from that.

sampling from the marginal distribution has the advantage over the typical-
value method of not arti   cially reducing the variance of y , or otherwise dis-
torting its (marginal) distribution. but, because the draws are independent of
other variables, it will tend to attenuate the dependence of y on those other
variables, and so will still introduce distortions into, say, regressions. this
would be true even if we were sampling from the true marginal distribution of
y . you should be able to convince yourself that if y is missing at random, but
not missing completely at random, then sampling from the observed marginal
distribution of y will introduce distortions. on the other hand, if y really is
missing completely at random, such imputation only a   ects the relationship
to other variables.

figure k.2b is our running example, with marginal-distribution imputation.

3. conditional imputation by regression a somewhat more    exible strategy is to
use the complete cases to learn a regression function for the sometimes-missing

variable, i.e., to estimate the function (cid:98)   = e [y |x]. in a record where y is
missing and we see x = x, then, we impute the value(cid:98)  (x) for y . this tries to

preserve something of the relationship between the missing variable and the
others, but obviously relies very strongly on missing-ness being uninformative
about y .

(an additional complication for this strategy is that other variables, besides
y , might be missing for some records but not for others. one thus might have
to end up estimating, and using, many di   erent regression functions.)

figure k.2c is our running example, with imputation from having smoothed

the observed data points.

4. conditional imputation from the conditional distribution of course, nothing
says that we need to only use regression. once we can estimate pr (y |x), we
can draw samples from it, and impute those samples to the missing observa-
tions. the easiest form of this is to predict the missing values using regression,
and then add noise, but all the techniques used to learn conditional distribu-
tions (  14.5) are potentially in play.

one version of this idea is to impute by matching, that is, to search for
an observation with the same value of x as the one where y is missing,
and to copy its value of y ; if there are multiple matches, chose among them
randomly25. the exact performance here will depend on a lot of details     what
if there is no match? since exact equality is implausible if x is continuous, how
close does it need to be to declare a match? what if x is multi-dimensional?

25 this is, of course, similar to the matching methods sometimes used in causal id136 (  23.1.3).

k.3 further methods: imputation, em, weighting

795

    which can lead to very complicated algorithms26. the point, however, is to
sample from pr (y |x).

k.3.1.2 imputation under informative missingness

basically every idea about imputation under mar has its counter-part for im-
putation under informative missingness. the most natural starting point is im-
putation from the conditional distribution. if we   re working under mnar, and
we think we know pr (y |x, my = 1), then we should (in principle) be able to
sample from it, and use those sample to    ll in the missing values. figure k.2d
shows an example of such imputation, where the trend for the missing values is
assumed to run opposite to the one we get from the fully-observed data.

in this context, it   s worth putting in a word for imputation constants. this
can make a fair amount of sense in situations where we know the missing values
arose by censoring. if, for instance, we   re measuring how long it takes a chemical
reaction to run, but each experiment only lasts for a certain amount of time t0,
it can be a good idea to impute t0 +    to all the reactions which didn   t    nish in
time     and to vary    to see how much that a   ects our conclusions.

k.3.1.3 multiple imputation and uncertainty

once we have completed the data set, we can run our usual analyses on it,
whatever those might be. when we look at how uncertain those analyses are,
however, we really ought to take into account the fact that we did imputation,
and we might well have imputed di   erent values than the ones we did. (this is
especially true when we impute randomly-sampled values!) one way to get at
this is through multiple imputation, where we do our stochastic imputation
many times, re-running the analysis on each.

at this point, we need to somehow synthesize the multiple results from multi-
ple imputations into a single measure of uncertainty. this is fairly simple if we
bootstrap:    rst draw a bootstrap sample, then do imputation using that sample,
and then combine the results as we would ordinarily for id64.

if we   re just reporting means and variances, though, we need to be just a little
bit more careful. in these situations, we often calculate a estimate      from each
imputation run, and an associated variance   2. say that we get     1, . . .     m from our
m imputations, and   2
m for the variances. then our best over-all estimate
is clearly

1, . . .   2

m(cid:88)

j=1

   =

1
m

    j

(k.27)

26 a historically important one, developed by the us census bureau, was called    hot-deck

imputation   , because of the way it re-used the punched paper cards then used for data storage.
(using punch cards to store census data actually goes back to the late 1800s, and the tabulating
machines were important precursors of digital computers     see, e.g., yates 1989.) cards with a
similar value of x had just been processed, hence were warm from the card-reading machinery,
hence    hot   . by contrast, sampling from the marginal distribution by picking a totally random card
was    cold-deck imputation   . for more on this, see rubin (1987), and the references therein.

796

missing data

but the associated variance is

m(cid:88)

j=1

m(cid:88)

j=1

(    j       )2

  2
j +

1

m     1

(k.28)

1
m

(this is just our old friend, the law of total variance.)

k.3.2 the em algorithm

in   19.2, we looked at the em algorithm as a way of dealing with latent variables,
ones which are never observed, but we nonetheless postulate, and imagine are
linked to the observable variables in systematic ways. this is very close to the
problem we have with missing data, and the em algorithm can also be used here.

what we would like to do is to maximize the observed-data likelihood

by elementary id203,

p(x, yobs;   ) =

p(x, yobs, ymiss;   )

p(x, yobs;   )

(cid:88)

(k.29)

(k.30)

y

i.e., a sum over all possible complete-data likelihoods. (if the missing observations
are continuous, make the sum into an integral.) appealing to the theory in   19.2.1,
the em procedure for doing this is as follows:
1. start with a guess     (0) for   
2. until things stop improving:

1. e-step: find the conditional distribution of ymiss given x = x and yobs =

yobs,

q(ymiss) = p(ymiss|x, yobs, my ;     (t))

(k.31)
note that if data points are iid, then ymiss        yobs|x, my , so this simpli   es
to just p(ymiss|x, my ;     (t)). in fact, with the iid assumption this simpli   es
even further, since each missing y value needs to be conditioned only on the
corresponding x and its own missing-ness indicator, and the whole joint
distribution over missing y values is a product of the distributions for each
value.

2. m-step: find the    which maximizes the approximation to the complete-

data log-likelihood:

    (t+1) = argmax

(cid:88)

  

ymiss

q(ymiss) log p(x, yobs, ymiss, my ; theta)

(k.32)

again, if ymiss is continuous, replace the sum with an integral.
if (as in regression problems) we only care about the conditional-on-x
likelihood, we use that here, so

    (t+1) = argmax

q(ymiss) log p(yobs, ymiss, my |x;   )

(k.33)

(cid:88)

  

ymiss

k.3 further methods: imputation, em, weighting

797

finally, under the iid assumption, both the numerator and the denomina-
tor inside the log are products of independent terms for each data point,
simplifying the calculation.

3. return the    nal     (t) and (optionally) the    nal distribution q over the missing

observations.

note that since we are conditioning ymiss not just on x but also my , the em
procedure can work when the y su   ers from informative missingness. we just
need to guess (correctly) the conditional distribution of missing y .

it is very likely that this all feels rather abstract. exercise 9 guides you through
implementing the em algorithm for missing data in a classic problem, both in a
version where data are missing at random, and one subject to censoring.

k.3.2.1 monte carlo em

in many situations, the e step of the em algorithm is hard to implement exactly,
because it   s di   cult to get a closed-form expression for the conditional distribu-
tion of the missing observations. in these situations, it is often possible to draw a
random sample from the distribution instead (see appendix m). instead of aver-
aging over averaging over q(ymiss) in eq. k.32, then, we average over the sample
of di   erent possible ymiss values.

k.3.3 inverse id203 weighting

a    nal approach to dealing with missing data is worth mentioning. this is the
simple trick of giving more or less weight to the complete observations. suppose
that we a certain data point was fully observed, but we think that its inclusion
id203 was only 0.1. then there should have been about nine other data
points just like it, in order to get one successful observation.

this logic leads to the idea of inverse id203 weighting (ipw). if we
know the inclusion probabilities as a function of x and y ,   (x, y), then we give
data point i a weight of

wi =

1

  (xi, yi)

(k.34)

when we compute things like mse or log-likelihood. this, of course, will simplify
if we assume mar, to just

wi =

1

  (xi)

(k.35)

after that, the analysis proceeds as in ch. 10. exercise 10 covers derives from
properties for estimating expectation values in this way.

it is worth noting that ipw leans very heavily on knowing the inclusion prob-
abilities. this is particularly true when we believe that some of the inclusion
probabilities are very small, and so imply very large weights. if we have to es-

798

missing data

timate those probabilities from data27, then we really ought to propagate the
uncertainty from the inclusion probabilities to the sampling weights to our ulti-
mate conclusions. the bootstrap provides a convenient way to do this: in each
bootstrap replicate, re-learn the inclusion probabilities, then use those re-learnt
weights; pool over replicates as usual.

k.4 checking assumptions about missingness

the sad truth is that most assumptions about missingness cannot be checked in
a purely    statistical    way, by running tests on the observed data. this is because
most assumptions about missingness are about how y related to my , and (once
more, with feeling) we don   t know what y is when my = 1.

the only important exception to this negative conclusion is the assumption of

   missing complete at random   , mcar. this is that

this implies that

my        x, y

my        x

(k.36)

(k.37)

which only involves observable variables, so we can check it. if we    nd, in our
data, that my (cid:54)       x, we can reject mcar. of course, mcar can be false even
if my        x, because then missingness might still be very informative about y ,
and we cannot test this without knowing y .

let   s turn now to the contrast between missing-at-random, mar, and informa-
tive missingness or missing-not-at-random, mnar. this is the contrast between
my        y |x, and my (cid:54)       y |x. basic id203 tells us that mar is equivalent
to the equation

pr (y = y|x = x, my = 1) = pr (y = y|x = x, my = 0)

(k.38)

holding for all x and y. unfortunately, our data tells us, quite literally, nothing
about pr (y = y|x = x, my = 1). that distribution could be anything at all, and
the distribution of what we observe would not change28. so there is no way to do
a formal, statistical test of whether the missing-ness in y is informative about
y . whether you take this to mean that the data can never support mar, or to
mean that the data can never undermine mar, is to some extent a matter of
temperament.

however, the fact that there is no formal, statistical test to appeal to does not
mean that there is no work for a statistician to do. it is often possible to inves-
tigate why some data are missing, by a detailed study of the data-collection pro-
cess. particular assumptions may also be made more or less plausible by means of
analogies with other situations where the missing-data mechanisms are (thought
to be) well-understood.

27 as opposed to, say, knowing them because they re   ect properties of the data-collection process

under our control.

28 this doesn   t mean that pr (y = y|x = x) can be anything at all, however; see   k.5 below.

k.5 bounds

k.5 bounds

799

so far, all our techniques for handling missing data have focused on somehow
working out what the missing values were, or might have been. we might instead
frankly accept that we have no idea what they were, but try to place bounds on
their impact. think back to eq. refeqn:complete-data-marginal-prob-in-terms-of-
missing-data-marginal-prob:

pr (y = y) = pr (y = y|my = 0)    + pr (y = y|my = 1) (1       )

(k.39)
we can, in principle, learn pr (y = y|my = 0) and    from data     they   re iden-
ti   ed. pr (y = y|my = 1) is not identi   ed, without further assumptions, but of
course, being a id203, it   s between 0 and 1. so we can say, without any
assumptions, that

pr (y = y|my = 0)        pr (y = y)     pr (y = y|my = 0)    + 1       

(k.40)

this doesn   t tell us everything, but it does rule out some possibilities for pr (y = y)
    for instance, it can   t be 1
data doesn   t uniquely determine some quantity, but does put restrictions on it,
we say that the quantity is partially identi   ed or set-identi   ed (as opposed
to the usual point-identi   ed).

2 pr (y = y|my = 0)   . when the distribution of the

a similar argument gives partial-identi   cation bounds for the conditional prob-

ability:
pr (y = y|my = 0, x = x)   (x)     pr (y = y|x = x)     1+(pr (y = y|my = 0, x = x)   1)  (x)

(k.41)
going further than these bounds typically requires either some detailed con-
sideration of what we   re really trying to estimate (when do we care about a
single id203?), possibly combined with additional assumptions, e.g., that
pr (y = y|x = x) is monotone in y for each x. this sort of elaboration inolves
too many special cases to be treated here, but see the references under further
reading.

k.6 closing modeling advice

i cannot, unfortunately, provide any hard and fast rules about how to deal with
missing data. i can, however, provide some advice.

1. the best way to deal with missing data is not to have any. to the extent
that you can in   uence how the data are collected, try to make sure that
missing data does not arise; failing that, try to make sure that missingness is
uninformative and rare. it   s usually better to spend your e   orts securing good
data to begin with, than trying to compensate for bad data collection later
with fancy techniques29.

29 statisticians get rewarded, professionally, for developing new techniques, so that   s the focus of most

of our scienti   c literature on missing data. but i don   t think any experienced applied statistician
would disagree with this point.

800

missing data

2. understand, concretely, why some of this data might go missing. if you can
change how the data is collected, this will help you make sure less does go
missing (as in the    rst point). if the data are just handed to you and you have
to make the best of them, then this understand is crucial to creating stochastic
models of missingness, and deciding whether mar is plausible or not.

3. consider more than one model for missingness. even if you think mar is
plausible (or at least defensible), it   s usually a good idea to consider at least
one or two models of informative missingness. if small departures from your
initial or most-favorite model don   t change the conclusions of your analysis
very much, well and good. (though, in that case, you might want to ask how
big a departure would be needed to seriously alter a conclusion.) if, on the
other hand, your conclusions are very sensitive to exactly how you deal with
missing values, you either need a strong justi   cation for preferring one model
over another, or you need to be up-front about just how much your conclusions
rest on assumptions about missing data.

k.7 further reading

the classic reference on missing data, which standardized the mcar/mar/mnar
terminology, is little and rubin (1987). from the same school, rubin (1987) is
a classic reference on multiple imputation.

on the em algorithm, in addition to the references given in ch. 19, mclachlan
and krishnan (2008) includes an extensive treatment of its uses in missing data
problems. the monte carlo em approach, where in the e step we sample from the
distribution of missing values instead of calculating that distribution, still requires
us to iterate the e and m steps many times. sung and geyer (2007) provides a
truly ingenious procedure where we need only a single, data-independent sample
of potential missing values. the basic idea of the paper is well within the grasp
of readers of this book, though the proofs of its validity are much more technical.
the bounding approach, as an alternative to identifying assumptions, is most
closely associated with the work of charles manski. manski (2007) is the easiest
introduction to his thought; see especially chapter 2 of that book. manski (2003)
is a more thoroughly technical treatment.

becker (2017) provides a rich discussion of many of the ways data-gathering on
social phenomena can go wrong, including highly-informative missingness, and
case studies of how to investigate such problems. much of his discussion applies
equally well to measurement in other domains.

k.8 exercises

1. in this exercise, assume that y is missing at random, but not missing com-

pletely at random, and that x is not missing at all.

1. show that the complete-data joint distribution, pr (x = x, y = y), is not

equal to the observed joint distribution, pr (x = x, y = y|my = 0).

k.8 exercises

801

2. show that the complete-data joint distribution is still observationally iden-

ti   ed.

3. can the complete-data joint distribution be estimated after listwise dele-

tion?

2. in this exercise, assume that y is missing at random, but not missing com-

pletely at random, and that x is not missing at all.
1. show that pr (y = y|my = 0) (cid:54)= pr (y = y), so that the observed marginal
distribution of y is not equal to the complete-data marginal distribution.

2. is pr (y = y) identi   ed?

3. read appendix ?? on id205.

1. show that mcar holds if and only if i[y ; my ] = 0
2. show that mar holds if and only if i[y ; my |x] = 0.
3. show that mnar holds if and only if i[y ; my |x] > 0.
4. explain why mnar is also called    informative missingness   .

4. refer to figure k.5 (and to the concept of d-separation in chapter 20).

1. use the d-separation rules to explain why

but

y        my |{v, w, z, r}

y (cid:54)       my |{v, w, q}

(k.42)

(k.43)

2. decision-makers often rely on proxies for the variables they really wish
they had. in the diagram, r could be such a proxy, since it   s not a causal
ancestor of y . does this mean that we could still have y        my |{v, w, z}?
explain.
3. can you    nd a set of variables s such that s does not include all the parents
of my , but y        my |s? if so, give it, and explain why the independence
holds; if not, explain why such a set is impossible.

5. derive eq. k.18.
6. derive eq. k.19.
7. show that y is mar if and only if   (x, y) =   (x). explain how eq. k.19
can still apply when y is missing-at-random, and why mar is also known as
   ignorable missingness   .
8. in both parts of this exercise, but only in this exercise, make all the assump-
tions of   k.2.3.1. note: the    rst part is (for most people!) much easier than
the second.
1. find the variance of          in terms of the elements of the variance matrix

  .

2. derive eq. ??.
3. find the covariance of   and       
4. derive eq. k.24.

  

in terms of the elements of   .

802

missing data

9. detailed example of em for missing data in this problem, we work through the
em algorithm for missing data, in the classic (i.e., simple!) case of exponentially-
distributed random variables. each unit i has a lifetime yi, and these follow
an exponential distribution, so the pdf is   e     y. assume throughout that the
yi are independent and identically distributed.
1. assume all the yi are observed. write out the log-likelihood and    nd the

id113 of   .

2. assume that some of the yi are missing completely at random, with prob-

ability   .
1. write out the log-likelihood for the observed values of y and the miss-

ingness indicators.

2. find the id113 for    and   , based on this log-likelihood for observables

alone.

3. (e-step) find the conditional distribution for the unobserved values of
y , given the observed values and the missingness indicators. this should
be a function of   . (should it also be a function of   ?)

4. (m-step) write out the complete-data log-likelihood.
5. (m-step, continued) write out the expected value of the complete data
log-likelihood, averaging over the distribution of missing values. hint:
what is the expected value of an exponential distribution?
6. find an expression for     (t+1) in terms of     (t) and the data.
7. find the    xed point of this expression. does it match the id113 for    you

found earlier?

3. assume that the yi are censored: there is a time t0 such that if yi     t0, we
get to see yi, but if yi > t0, yi is missing (and we know that it   s missing).
assume we know t0.
1. write out the log-likelihood function based on the observed yi and the
missingness indicators. hint: the id203 of an observation being
censored is a function of    and t0.

2. find the maximum likelihood estimator of    based on this observed-data

log-likelihood.
3. (e-step) find the conditional distribution of the missing yi, given the
observed yi and the missingness indicators. hint: if y     exp(  ), then
y |y > y0 follows what distribution?

4. (m-step) write out the complete-data log-likelihood.
5. (m-step) write out the expected value of the complete-data log-likelihood,

averaging over the distribution of missing values.

6. find an expression for     (t+1) in terms of     (t) and the data.
7. find the    xed point of this expression. does it match the id113 for    you

found earlier?

8. explain what assumption this procedure makes about the unobserved
values of y , and why this assumption cannot be tested (with this data).
9. can you extend the procedure to handle the case where each unit i has

its own (known) censoring time ti?

k.8 exercises

803

10. a classic example of inverse id203 weighting concerns estimating the
mean (or total) of a population from samples. suppose that there are n mem-
bers of the population, each with some value of y , say yi. the population
mean is therefore

n(cid:88)

1
n

(cid:88)

i   o

1
n

yi
  i

  yht =

y =

(k.44)
we actually observe n < n members of the population, say the ones where i    
o. the id203 of observing yi is   i. the horvitz-thompson estimate
of the population mean is

yi

i=1

note that the denominator is the total population size, not the sample size!

1. show that   yht is an unbiased estimate of y. hint: first show that(cid:80)
(cid:80)n
i=1 yi(1     myi).

i   o yi =

2. find the variance of   yht . you will need the joint id203 that both i
3. show that the variance     0 as n grows.

and j are observed,   ij.

(k.45)

804

missing data

the running example

here is the code for the simulation that provides this chapter   s running example:

library(faraway) # for ilogit
n <- 50
# x is uniform (put it in order for easy plotting)
x <- sort(runif(n, min=0, max=100))
# y increases with x, though non-linearly
y <- ilogit(0.05*(x-50)+rnorm(n, sd=1))
# missing-ness depends on the value of y, high values => more missing
prob.y.missing <- ilogit(50*logit(y))
missing.y <- (rbinom(n=n, size=1, prob=prob.y.missing) == 1) # to make it boolean
y.obs <- y[!missing.y]
x.obs <- x[!missing.y]
the.df <- data.frame(x=x, y=ifelse(missing.y, na, y), missing.y=missing.y)
plot(y~x, data=the.df, xlab="x", ylab="y", ylim=c(0,1))
rug(side=1, x=the.df$x)
rug(side=2, x=the.df$y)

as you may have worked out by a process of elimination, this is shown in
figure k.2a. missingness is here directly based on y , and so informative. notice,
by the way, that when we run a regression on the fully-observed data points (as
in figure k.2c, or figure k.3), we get a very di   erent regression curve than is
implied by the actual generative process.

k.8 exercises

805

the.df.post.deletion <- na.omit(the.df)
plot(y ~ x, data=the.df.post.deletion, xlab="x", ylab="y", type="p",

xlim=c(min(x), max(x)), ylim=c(0,1))

rug(side=1, x=the.df.post.deletion$x)
rug(side=2, x=the.df.post.deletion$y)
require(mgcv)
a.spline <- gam(y ~ s(x), data=the.df.post.deletion)
lines(the.df.post.deletion$x, fitted(a.spline), col="grey")

figure k.3 what the running-example data looks like, after deleting
incomplete cases. the grey line is a spline run through the fully-observed
points.

llllllllllllllllllllllllllllll0204060800.00.20.40.60.81.0xy806

missing data

x

y

my

figure k.4 the simplest (but not the only!) graphical model in which y
might be missing at random (mar), but not missing completely at random
(mcar).

u

w

q

v

z

r

y

my

figure k.5 whether y is missing-at-random or not can depend on the
variables used for conditioning. suppose that whether or not a student is
admitted (or a loan approved, or an arrestee released) depends on
x = {v, w, z, r}, and y is some measure of academic success (or loan
repayment, or subsequent trouble with the law). then y        my |x, and y is
missing-at-random for the decision-makers. but we as data-analysts might
only have access to x(cid:48) = {v, w, q}, and then y (cid:54)       my |x(cid:48). see exercise 4
for proofs and extensions.

appendix l

writing r functions

the ability to read, understand, modify and write simple pieces of code is an
essential skill for modern data analysis. lots of high-quality software already
exists for speci   c purposes, which you can and should use, but statisticians need
to grasp how such software works, tweak it to suit their needs, recombine existing
pieces of code, and, when needed, build their own tools. someone who just knows
how to run canned routines is not a data analyst but a human interface to a
machine they do not understand.

fortunately, writing code is not actually very hard, especially not in r. all it
demands is the discipline to think logically, and the patience to practice. this
appendix tries to illustrate what   s involved, starting from the very beginning. it
is redundant for many students, but included through popular demand.

l.1 functions

programming in r is organized around functions. you all know what a mathe-
matical function is, like log x or   (z) or sin   : it is a rule which takes some inputs
and delivers a de   nite output. a function in r, like a mathematical function,
takes zero or more inputs, also called arguments, and returns an output. the
output is arrived at by going through a series of calculations, based on the in-
put, which we specify in the body of the function. as the computer follows our
instructions, it may do other things to the system; these are called side-e   ects.
(the most common sort of side-e   ect, in r, is probably making or updating a
plot on the screen.) the basic declaration or de   nition of a function looks like
so:

my.function <- function(argument.1, argument.2, ...) {

# clever manipulations of arguments
return(the.return.value)

}

strictly speaking, we often don   t need the return() command; without it, the
function will return the last thing it evaluated. but it   s usually clearer, and never
hurts, to be explicit.

we write functions because we often    nd ourselves going through the same
sequence of steps at the command line, perhaps with small variations. it saves
mental e   ort on our part to take that sequence and bind it together into an

807

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

808

programming

integrated procedure, the function, so that then we can think about the function
as a whole, rather than the individual steps. it also reduces error, because, by
invoking the same function every time, we don   t have to worry about missing a
step, or wondering whether we forgot to change the third step to be consistent
with the second, and so on.

l.2 first example: pareto quantiles

(cid:40)      1

let me give a really concrete example. in chapter 6, i mentioned the pareto
(cid:17)     
(cid:16) x
distribution, which has the id203 density function
x     x0
x < x0
(cid:18) x
(cid:19)     +1

consequently, the cdf is

f (x;   , x0) =

(l.1)

x0
0

x0

(l.2)

f (x;   , x0) = 1    

x0

and the quantile function is

q(p;   , x0) = x0(1     p)

    1
     1

(l.3)

say i want to    nd the median of a pareto distribution with    = 2.33 and

x0 = 6    108. i can do that in r:

6e8 * (1-0.5)^(-1/(2.33-1))
## [1] 1010391288

if i decide i want the 40th percentile of the same distribution, i can do that:

6e8 * (1-0.4)^(-1/(2.33-1))
## [1] 880957225

if i decide to raise the exponent to 2.5, lower the threshold to 1    106, and ask

about the 92nd percentile, i can do that, too:

1e6 * (1-0.92)^(-1/(2.5-1))
## [1] 5386087

but doing this all by hand gets quite tiresome, and at some point i   m going to
mess up and (say) type when i meant ^. i   ll write a function to do this for me,
and so that there is only one place for me to make a mistake:

# calculate quantiles of the pareto distribution
# inputs: desired quantile (p)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)

# outputs: the pth quantile
qpareto.1 <- function(p, exponent, threshold) {

q <- threshold*((1-p)^(-1/(exponent-1)))
return(q)

}

l.3 functions which call functions

809

the name of the function is what goes on the left of the assignment <-, with
the declaration (beginning function) on the right. (i called this qpareto.1 to
distinguish it from later modi   cations.) the three terms in the parenthesis after
function are the arguments to qpareto     the inputs it has to work with. the
body of the function is just like some r code we would type into the command
line, after assigning values to the arguments. the very last line tells the function,
explicitly, what its output or return value should be. here, of course, the body
of the function calculates the pth quantile of the pareto distribution with the
exponent and threshold we ask for.

when i enter the code above, de   ning qpareto.1, into the command line, r
just accepts it without outputting anything. it thinks of this as assigning certain
value to the name qpareto.1, and it doesn   t produce outputs for assignments
when they succeed, just as if i   d said alpha <- 2.5.

all that successfully creating a function means, however, is that we didn   t make
a huge error in the syntax. we should still check that it works, by invoking the
function with values of the arguments where we know, by other means, what the
output should be. i just calculated three quantiles of pareto distributions above,
so let   s see if we can reproduce them.

qpareto.1(p=0.5,exponent=2.33,threshold=6e8)
## [1] 1010391288
qpareto.1(p=0.4,exponent=2.33,threshold=6e8)
## [1] 880957225
qpareto.1(p=0.92,exponent=2.5,threshold=1e6)
## [1] 5386087

so, our    rst function seems to work successfully.

l.3 functions which call functions

if we examine other quantile functions (e.g., qnorm), we see that most of them
take an argument called lower.tail, which controls whether p is a id203
from the lower tail or the upper tail. qpareto.1 implicitly assumes that it   s the
lower tail, but let   s add the ability to change this.

# calculate quantiles of the pareto distribution
# inputs: desired quantile (p)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)
# flag for whether to give lower or upper quantiles (lower.tail)

# outputs: the pth quantile
qpareto.2 <- function(p, exponent, threshold, lower.tail=true) {

if(lower.tail==false) {

p <- 1-p

}
q <- threshold*((1-p)^(-1/(exponent-1)))
return(q)

}

810

programming

when, in a function declaration, an argument is followed by = and an expres-
sion, the expression sets the default value of the argument, the one which will
be used unless explicitly over-ridden. the default value of lower.tail is true,
so, unless it is explicitly set to false, we will assume p is a id203 counted
from        on up.

the if command is a control structure     if the condition in parenthesis
is true, then the commands in the following braces will be executed; if not, not.
since lower tail probabilities plus upper tail probabilities must add to one, if we
are given an upper tail id203, we just    nd the lower tail id203 and
proceed as before.

let   s try it:

qpareto.2(p=0.5,exponent=2.33,threshold=6e8,lower.tail=true)
## [1] 1010391288
qpareto.2(p=0.5,exponent=2.33,threshold=6e8)
## [1] 1010391288
qpareto.2(p=0.92,exponent=2.5,threshold=1e6)
## [1] 5386087
qpareto.2(p=0.5,exponent=2.33,threshold=6e8,lower.tail=false)
## [1] 1010391288
qpareto.2(p=0.92,exponent=2.5,threshold=1e6,lower.tail=false)
## [1] 1057162

first: the answer qpareto.2 gives with lower.tail explicitly set to true
matches what we already got from qpareto.1. second and third: the default
value for lower.tail works, and it works for two di   erent values of the other
arguments. fourth and    fth: setting lower.tail to false works properly (since
the 50th percentile is the same from above or from below, but the 92nd percentile
is di   erent, and smaller from above than from below).

the function qpareto.2 is equivalent to this:

# calculate quantiles of the pareto distribution
# inputs: desired quantile (p)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)
# flag for whether to give lower or upper quantiles (lower.tail)

# outputs: the pth quantile
qpareto.3 <- function(p, exponent, threshold, lower.tail=true) {

if(lower.tail==false) {

p <- 1-p

}
q <- qpareto.1(p, exponent, threshold)
return(q)

}

when r tries to execute this, it will look for a function named qpareto.1 in
the workspace. if we have already de   ned such a function, then r will execute it,
with the arguments we have provided, and q will become whatever is returned by
qpareto.1. when we give r the above function de   nition for qpareto.3, it does
not check whether qpareto.1 exists     it only has to be there at run time. if

l.3 functions which call functions

811

qpareto.1 changes, then the behavior of qpareto.3 will change with it, without
our having to rede   ne qpareto.3.

this is extremely useful. it means that we can take our programming problem
and sub-divide it into smaller tasks e   ciently. if i made a mistake in writing
qpareto.1, when i    x it, qpareto.3 automatically gets    xed as well     along
with any other function which calls qpareto.1, or qpareto.3 for that matter. if
i discover a more e   cient way to calculate the quantiles and modify qpareto.1,
the improvements are likewise passed along to everything else. but when i write
qpareto.3, i don   t have to worry about how qpareto.1 works, i can just assume
it does what i need somehow.

l.3.1 sanity-checking arguments

it is good practice, though not strictly necessary, to write functions which check
that their arguments make sense before going through possibly long and compli-
cated calculations. for the pareto quantile function, for instance, p must be in
[0, 1], the exponent    must be at least 1, and the threshold x0 must be positive,
or else the mathematical function just doesn   t make sense.

here is how to check all these requirements:

# calculate quantiles of the pareto distribution
# inputs: desired quantile (p)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)
# flag for whether to give lower or upper quantiles (lower.tail)

# outputs: the pth quantile
qpareto.4 <- function(p, exponent, threshold, lower.tail=true) {

stopifnot(p >= 0, p <= 1, exponent > 1, threshold > 0)
q <- qpareto.3(p,exponent,threshold,lower.tail)
return(q)

}

the function stopifnot halts the execution of the code, with an error message,
if all of its arguments do not evaluate to true. if all those conditions are met,
however, r just goes on to the next command, which here happens to be running
qpareto.3. of course, i could have written the checks on the arguments directly
into the latter.

let   s see this in action:

qpareto.4(p=0.5,exponent=2.33,threshold=6e8,lower.tail=true)
## [1] 1010391288
qpareto.4(p=0.92,exponent=2.5,threshold=1e6,lower.tail=false)
## [1] 1057162
qpareto.4(p=1.92,exponent=2.5,threshold=1e6,lower.tail=false)

## error: p <= 1 is not true

qpareto.4(p=-0.02,exponent=2.5,threshold=1e6,lower.tail=false)

## error: p >= 0 is not true

qpareto.4(p=0.92,exponent=0.5,threshold=1e6,lower.tail=false)

812

programming

## error: exponent > 1 is not true

qpareto.4(p=0.92,exponent=2.5,threshold=-1,lower.tail=false)

## error: threshold > 0 is not true

qpareto.4(p=-0.92,exponent=2.5,threshold=-1,lower.tail=false)

## error: p >= 0 is not true

the    rst two lines give the same results as our earlier functions     as they
should, because all the arguments are in the valid range. the third, fourth,    fth
and sixth lines all show that qpareto.4 stops with an error message when one
of the conditions in the stopifnot is violated. notice that the error message
says which condition was violated. the seventh line shows one limitation of this:
the arguments violate two conditions, but stopifnot   s error message will only
mention the    rst one. (what is the other violation?)

l.4 layering functions and debugging

functions can call functions which call functions, and so on inde   nitely. to il-
lustrate, i   ll write a function which generates pareto-distributed random num-
bers, using the    quantile transform    method from   5.2.2.3. this    rst generates
a uniform random number u on [0, 1], and then returns q(u ), with q being the
quantile function of the desired distribution.

the    rst version contains a deliberate bug, which i will show how to

track down and    x.

# generate random numbers from the pareto distribution
# inputs: number of random draws (n)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)

# outputs: vector of random numbers
rpareto <- function(n,exponent,threshold) {

x <- vector(length=n)
for (i in 1:n) {

x[i] <- qpareto.4(p=rnorm(1),exponent=exponent,threshold=threshold)

}
return(x)

}

notice that this calls qpareto.4, which calls qpareto.3, which calls qpareto.1.
it doesn   t work:

rpareto(10)

## error in qpareto.4(p = rnorm(1), exponent = exponent, threshold = threshold): argument
"exponent" is missing, with no default

this is a puzzling error message     the expression exponent > 1 never appears
in rpareto! the error is coming from further down the chain of execution. we

l.4 layering functions and debugging

813

can see where it happens by using the traceback() function, which gives the
chain of function calls leading to the latest error1:

<<wrapper=true, eval=false>>=

traceback()
## 3: stopifnot(p >= 0, p <= 1, exponent > 1, threshold > 0) at #2
## 2: qpareto.4(p = rnorm(1), exponent = exponent, threshold = threshold) at #4
## 1: rpareto(10)

@

traceback() outputs the sequence of function calls leading up to the error in
reverse order, so that the last line, numbered 1, is what we actually entered on
the command line. this tells us that the error is happening when qpareto.4 tries
to check the arguments to the quantile function. and the reason it is happening
is that we are not providing qpareto.4 with any value of exponent. and the
reason that is happening is that we didn   t give rpareto any value of exponent
as an explicit argument when we called it, and our de   nition didn   t set a default.

let   s try this again.

rpareto(n=10,exponent=2.5,threshold=1)

## error: p <= 1 is not true

<<wrapper=true, eval=false>>=

traceback()
## 3: stopifnot(p >= 0, p <= 1, exponent > 1, threshold > 0) at #2
## 2: qpareto.4(p = rnorm(1), exponent = exponent, threshold = threshold) at #4
## 1: rpareto(n = 10, exponent = 2.5, threshold = 1)

@

this is progress! the stopifnot in qpareto.4 is at least able to evaluate all
the conditions     it just happens that one of them is false. the problem, then,
is that qpareto.4 is being passed a negative value of p. this tells us that the
problem is coming from the part of rpareto.1 which sets p. looking at that,

p = rnorm(1)

the culprit is obvious: i stupidly wrote rnorm, which generates a gaussian
random number, when i meant to write runif, which generates a uniform random
number.2

the obvious    x is just to replace rnorm with runif:

1 for users of knitr/r markdown: traceback is one of a number of highly-interactive commands
which don   t work properly in knitr. this is not much of a loss, since it   s for debugging, and you
shouldn   t be doing your debugging in your report.

2 i actually made this exact mistake the    rst time i wrote the function, in 2004.

814

programming

# generate random numbers from the pareto distribution
# inputs: number of random draws (n)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)

# outputs: vector of random numbers
rpareto <- function(n,exponent,threshold) {

x <- vector(length=n)
for (i in 1:n) {

x[i] <- qpareto.4(p=runif(1),exponent=exponent,threshold=threshold)

}
return(x)

}

let   s see if this is enough to    x things, or if i have any other errors:

rpareto(n=10,exponent=2.5,threshold=1)
##
##

[1] 1.875049 3.463652 1.009382 2.231161 1.285460 1.746962 3.126330
[8] 1.134979 1.818675 1.475200

this function at least produces numerical return values rather than errors! are

they the right values?

we can   t expect a random number generator to always give the same results, so
i can   t cross-check this function against direct calculation, the way i could check
qpareto.1. (actually, one way to check a random number generator is to make
sure it doesn   t give identical results when run twice!) it   s at least encouraging that
all the numbers are above threshold, but that   s not much of a test. however,
since this is a random number generator, if i use it to produce a lot of random
numbers, the quantiles of the output should be close to the theoretical quantiles,
which i do know how to calculate.

r <- rpareto(n=1e4,exponent=2.5,threshold=1)
qpareto.4(p=0.5,exponent=2.5,threshold=1)
## [1] 1.587401
quantile(r,0.5)
##
50%
## 1.585473
qpareto.4(p=0.1,exponent=2.5,threshold=1)
## [1] 1.072766
quantile(r,0.1)
##
10%
## 1.073453
qpareto.4(p=0.9,exponent=2.5,threshold=1)
## [1] 4.641589
quantile(r,0.9)
##
90%
## 4.539353

this looks pretty good. figure l.1 shows a plot comparing all the theoretical
percentiles to the simulated ones, con   rming that we didn   t just get lucky with
choosing particular percentiles above.

l.4 layering functions and debugging

815

simulated.percentiles <- quantile(r,(0:99)/100)
theoretical.percentiles <- qpareto.4((0:99)/100,exponent=2.5,threshold=1)
plot(theoretical.percentiles,simulated.percentiles)
abline(0,1)

figure l.1 theoretical percentiles of the pareto distribution with    = 2.5,
x0 = 1, and empirical percentiles from a sample of 104 values simulated from
it with the rpareto function. (the solid line is the x = y diagonal, for visual
reference.)

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll51015205101520theoretical.percentilessimulated.percentiles816

programming

l.4.1 more on debugging

everyone who writes their own code spends a lot of time debugging3. there are
some guidelines for making it easier and less painful.

characterize the bug

we   ve got a bug when the code we   ve written won   t do what we want. to    x this,
it helps a lot to know exactly what error we   re seeing. the    rst step to this is to
make the error reproducible. can we always get the error when re-running the
same code and values? if we start the same code in a clean copy of r, does the
same thing happen? once we can reproduce the error, we map its boundaries.
how much can we change the inputs and get the same error? a di   erent error?
for what inputs (if any) does the bug go away? how big is the error?

localize the bug

the problem may be a di   use all-pervading wrongness, but often it   s a lot more
localized, to a few lines or even just one line of code; it helps to know where! we
have seen some tools for localizing the bug above: traceback() and stopifnot().
another very helpful one is to add print statements, so that our function gives
us messages about the progress of its calculations, selected variables, etc., as it
goes; the warning command can be used to much the same e   ect4.

fix the bug

once you know what   s going wrong and where it   s going wrong, it   s often not too
hard to spot the error, either one of syntax (say = vs. ==) or logic. try a    x and
see if it makes it better. do the inputs which gave you the bugs before now work
properly? are you getting di   erent errors?

l.5 automating repetition and passing arguments

the match between the theoretical quantiles and the simulated ones in figure
l.1 is close, but it   s not perfect. on the one hand, this might indicate some subtle
mistake. on the other hand, it might just be random sampling noise     rpareto
is supposed to be a random number generator, after all. we could check this by
seeing whether we get di   erent deviations around the line with di   erent runs of
rpareto, or if on the contrary they all pull in the same direction. we could just
make many plots by hand, the way we made that plot by hand, but since we   re
doing almost exactly the same thing many times, let   s write a function.

# compare random draws from pareto distribution to theoretical quantiles
# inputs: none
# outputs: none

3 those who don   t write their own code but use computers anyway spend a lot of time putting up

with other people   s bugs.

4 real software engineers look down on this, in favor of more sophisticated tools, like interactive

debuggers. they have a point, but that   s usually over-kill for the purposes of this class.

l.5 automating repetition and passing arguments

817

# side-effects: adds points showing random draws vs. theoretical quantiles

# to current plot

pareto.sim.vs.theory <- function() {

r <- rpareto(n=1e4,exponent=2.5,threshold=1)
simulated.percentiles <- quantile(r,(0:99)/100)
points(theoretical.percentiles,simulated.percentiles)

}

this doesn   t return anything. all it does is draw a new sample from the same
pareto distribution as before, re-calculate the simulated percentiles, and add them
to an existing plot     this is an example of a side-e   ect. notice also that the func-
tion presumes that theoretical.percentiles already exists. (the theoretical
percentiles won   t need to change from one simulation draw to the next, so it
makes sense to only calculate them once.)

figure l.2 shows how we can use it to produce multiple simulation runs. we
can see that, looking over many simulation runs, the quantiles seem to be too
large about as often, and as much, as they are too low, which is reassuring.

one thing which that    gure doesn   t do is let us trace the connections between
points from the same simulation. more generally, we can   t modify the plotting
properties, which is kind of annoying. this is easily    xed modifying the function
to pass along arguments:

# compare random draws from pareto distribution to theoretical quantiles
# inputs: graphical arguments, passed to points()
# outputs: none
# side-effects: adds points showing random draws vs. theoretical quantiles

# to current plot

pareto.sim.vs.theory <- function(...) {

r <- rpareto(n=1e4,exponent=2.5,threshold=1)
simulated.percentiles <- quantile(r,(0:99)/100)
points(theoretical.percentiles,simulated.percentiles,...)

}

putting the ellipses (...) in the argument list means that we can give pareto.sim.vs.theory.2

an arbitrary collection of arguments, but with the expectation that it will pass
them along unchanged to some other function that it will call with ...     here,
that   s the points function. figure l.3 shows how we can use this, by passing
along graphical arguments to points     in particular, telling it to connect the
points by lines (type="b"), varying the shape of the points (pch=i) and the line
style (lty=i).

these    gures are reasonably convincing that nothing is going seriously wrong
with the simulation for these parameter values. to check other parameter settings,
again, i could repeat all these steps by hand, or i could write another function:

# check pareto random number generator, by repeatedly generating random draws

# and comparing them to theoretical quantiles

# inputs: number of random points to generate per replication (n)

# exponent of distribution (exponent)
# lower threshold of distribution (threshold)
# number of replications to create (b)

# outputs: none

818

programming

simulated.percentiles <- quantile(r,(0:99)/100)
theoretical.percentiles <- qpareto.4((0:99)/100,exponent=2.5,threshold=1)
plot(theoretical.percentiles,simulated.percentiles)
abline(0,1)
for (i in 1:10) { pareto.sim.vs.theory() }

figure l.2 comparing multiple simulated quantile values to the theoretical
quantiles.

# side-effects: creates new plot, plots simulated points vs. theory
check.rpareto <- function(n=1e4, exponent=2.5, threshold=1, b=10) {

# one set of percentiles for everything
theoretical.percentiles <- qpareto.4((0:99)/100, exponent=exponent,

# set up plotting window, but don't put anything in it:

threshold=threshold)

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll51015205101520theoretical.percentilessimulated.percentileslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll.5 automating repetition and passing arguments

819

simulated.percentiles <- quantile(r,(0:99)/100)
theoretical.percentiles <- qpareto.4((0:99)/100,exponent=2.5,threshold=1)
plot(theoretical.percentiles,simulated.percentiles)
abline(0,1)
for (i in 1:10) {

pareto.sim.vs.theory(pch=i,type="b",lty=i)

}

figure l.3 as figure l.2, but using the ability to pass along arguments to
a subsidiary function to distinguish separate simulation runs.

plot(0,type="n", xlim=c(0, max(theoretical.percentiles)),

# no more horizontal room than we need
ylim=c(0,1.1*max(theoretical.percentiles)),
# allow some extra vertical room for noise

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll51015205101520theoretical.percentilessimulated.percentilesllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll820

programming

xlab="theoretical percentiles", ylab="simulated percentiles",
main = paste("exponent = ", exponent, ", threshold = ", threshold))

# diagonal, for visual reference
abline(0,1)
for (i in 1:b) {

pareto.sim.vs.theory(n=n, exponent=exponent, threshold=threshold,

pch=i, type="b", lty=i)

}

}

r will accept this de   nition, but it won   t run properly until we re-de   ned

pareto.sim.vs.theory to take the arguments n, exponent and threshold.5
it seems like a simple modi   cation of the old de   nition should do the trick:

# compare random draws from pareto distribution to theoretical quantiles
# inputs: graphical arguments, passed to points()
# outputs: none
# side-effects: adds points showing random draws vs. theoretical quantiles

# to current plot

pareto.sim.vs.theory <- function(n, exponent, threshold,...) {

r <- rpareto(n=n, exponent=exponent, threshold=threshold)
simulated.percentiles <- quantile(r, (0:99)/100)
points(theoretical.percentiles, simulated.percentiles, ...)

}

after de   ning this, the checker function seems to work    ne. the following
commands produce the plot in figure l.4, which looks very like the manually-
created one. (random noise means it won   t be exactly the same.) putting in the
default arguments explicitly gives the same results (not shown).

check.rpareto()
check.rpareto(n=1e4, exponent=2.5, threshold=1)

unfortunately, changing the arguments reveals a bug (figure l.5). notice that
the vertical coordinates of the points, coming from the simulation, look like they
have about the same range as the theoretical quantiles, used to lay out the plotting
window. but the horizontal coordinates are all pretty much the same (on a scale
of tens of billions, anyway). what   s going on?

the horizontal coordinates for the points being plotted are set in pareto.sim.vs.theory.3:

points(theoretical.percentiles, simulated.percentiles, ...)

where does this function get theoretical.percentiles from? since the vari-
able isn   t assigned inside the function, r tries to    gure it out from context. since
pareto.sim.vs.theory was de   ned on the command line, the context r uses to
interpret it is the global workspace     where there is, in fact, a variable called
theoretical.percentiles, which i set by hand for the previous plots. so the
plotted theoretical quantiles are all too small in figure l.5, because they   re for a
distribution with a much lower threshold.

5 try running check.rpareto(), followed by warnings().

l.5 automating repetition and passing arguments

821

check.rpareto()

figure l.4 automating the checking of rpareto.

didn   t check.rpareto assign is own value to theoretical.percentiles, which
it used to set the plot boundaries? yes, but that assignment only applied in the
context of the function. assignments inside a function have limited scope, they
leave values in the broader context alone. try this:

x <- 7
x
## [1] 7
square <- function(y) { x <- y^2; return(x) }
square(7)
## [1] 49

0510152005101520exponent =  2.5 , threshold =  1theoretical percentilessimulated percentilesllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll822

programming

check.rpareto(n=1e4, exponent=2.33, threshold=9e8)

figure l.5 a bug in check.rpareto.

x
## [1] 7

the function square assigns x to be the square of its argument. this assignment
holds within the scope of the function, as we can see from the fact that the
returned value is always the square of the argument, and not what we assigned
x to be in the global, command-line context. however, this does not over-write
that global value, as the last line shows.6

6 there are techniques by which functions can change assignments outside of their scope. they are

0.0e+005.0e+091.0e+101.5e+102.0e+102.5e+100.0e+005.0e+091.0e+101.5e+102.0e+102.5e+103.0e+10exponent =  2.33 , threshold =  9e+08theoretical percentilessimulated percentileslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll.5 automating repetition and passing arguments

823

there are two ways to    x this problem. one is to re-de   ne pareto.sim.vs.theory

to calculate the theoretical quantiles:

# compare random draws from pareto distribution to theoretical quantiles
# inputs: number of random points to generate (n)

# exponent of distribution (exponent)
# lower threshold of distribution (threshold)
# graphical arguments, passed to points() (...)

# outputs: none
# side-effects: adds points showing random draws vs. theoretical quantiles

# to current plot

pareto.sim.vs.theory <- function(n, exponent, threshold,...) {

r <- rpareto(n=n, exponent=exponent, threshold=threshold)
theoretical.percentiles <- qpareto.4((0:99)/100, exponent=exponent,

simulated.percentiles <- quantile(r,(0:99)/100)
points(theoretical.percentiles, simulated.percentiles, ...)

threshold=threshold)

}

this will work (try running check.rpareto(1e4,2.33,9e8) now), but it   s very
redundant     every time we call this, we   re recalculating the same percentiles,
which we already calculated in check.rpareto. a cleaner solution is to make the
vector of theoretical percentiles an argument to pareto.sim.vs.theory, and
change check.rpareto to provide it.

# compare random draws from pareto distribution to theoretical quantiles
# inputs: graphical arguments, passed to points()
# outputs: none
# side-effects: adds points showing random draws vs. theoretical quantiles

# to current plot

check.rpareto <- function(n=1e4,exponent=2.5,threshold=1,b=10) {

# one set of percentiles for everything
theoretical.percentiles <- qpareto.4((0:99)/100,exponent=exponent,

threshold=threshold)

# set up plotting window, but don't put anything in it:
plot(0,type="n", xlim=c(0,max(theoretical.percentiles)),

# no more horizontal room than we need
ylim=c(0,1.1*max(theoretical.percentiles)),
# allow some extra vertical room for noise
xlab="theoretical percentiles", ylab="simulated percentiles",
main = paste("exponent = ", exponent, ", threshold = ", threshold))

# diagonal, for visual reference
abline(0,1)
for (i in 1:b) {

pareto.sim.vs.theory(n=n,exponent=exponent,threshold=threshold,

theoretical.percentiles=theoretical.percentiles,
pch=i,type="b",lty=i)

}

}

# compare random draws from pareto distribution to theoretical quantiles
# inputs: number of random points to generate (n)

# exponent of distribution (exponent)

tricky, rare, and best avoided except by those who really know what they are doing. (if you think
you do, you are probably wrong.)

824

programming

# lower threshold of distribution (threshold)
# vector of theoretical percentiles (theoretical.percentiles)
# graphical arguments, passed to points()

# outputs: none
# side-effects: adds points showing random draws vs. theoretical quantiles

# to current plot

pareto.sim.vs.theory <- function(n,exponent,threshold,

theoretical.percentiles,...) {
r <- rpareto(n=n,exponent=exponent,threshold=threshold)
simulated.percentiles <- quantile(r,(0:99)/100)
points(theoretical.percentiles,simulated.percentiles,...)

}

figure l.6 shows that this succeeds.

l.5 automating repetition and passing arguments

825

check.rpareto(1e4,2.33,9e8)

figure l.6 using the corrected simulation checker.

0.0e+005.0e+091.0e+101.5e+102.0e+102.5e+100.0e+005.0e+091.0e+101.5e+102.0e+102.5e+103.0e+10exponent =  2.33 , threshold =  9e+08theoretical percentilessimulated percentilesllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll826

programming

l.6 avoiding iteration: manipulating objects

let   s go back to the declaration of rpareto, which i repeat here, unchanged, for
convenience:

# generate random numbers from the pareto distribution
# inputs: number of random draws (n)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)

# outputs: vector of random numbers
rpareto <- function(n,exponent,threshold) {

x <- vector(length=n)
for (i in 1:n) {

x[i] <- qpareto.4(p=runif(1),exponent=exponent,threshold=threshold)

}
return(x)

}

we   ve con   rmed that this works, but it involves explicit iteration in the form
of the for loop. because of the way r carries out iteration7, it is slow, and better
avoided when possible. many of the utility functions in r, like replicate, are
designed to avoid explicit iteration. we could re-write rpareto using replicate,
for example:

# generate random numbers from the pareto distribution
# inputs: number of random draws (n)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)

# outputs: vector of random numbers
rpareto <- function(n,exponent,threshold) {

x <- replicate(n,qpareto.4(p=runif(1),exponent=exponent,threshold=threshold))
return(x)

}

(the outstanding use of replicate is when we want to repeat the same random
experiment many times     there are examples in the notes for chapters 5 and 6.)
an even clearer alternative makes use of the way r automatically vectorizes

arithmetic:

# generate random numbers from the pareto distribution
# inputs: number of random draws (n)

# exponent of the distribution (exponent)
# lower threshold of the distribution (threshold)

# outputs: vector of random numbers
rpareto <- function(n,exponent,threshold) {

x <- qpareto.4(p=runif(n),exponent=exponent,threshold=threshold)
return(x)

}

this feeds qpareto.4 a vector of quantiles p, of length n, which in turn gets

passed along to qpareto.1, which    nally tries to evaluate

7 roughly speaking, it ends up having to create and destroy a whole copy of everything which gets

changed in the course of one pass around the iteration loop, which can involve lots of memory and
time.

l.6 avoiding iteration: manipulating objects

827

threshold*((1-p)^(-1/(exponent-1)))

with p being a vector, r hopes that threshold and exponent are also vectors,
and of the same length, so that it evaluates this arithmetic expression component-
wise. if exponent and threshold are shorter, it will    recycle    their values, in
order, until it has vectors equal in length to p. in particular, if exponent and
threshold have length 1, it will repeat both of them length(p) times, and
then evaluate everything component by component. (see the    introduction to
r    manual for more on this    recycling rule   .) the quantile functions we have
de   ned inherit this ability to recycle, without any special work on our part. the
   nal version of rpareto we have written is not only faster, it is clearer and easier
to read. it focuses our attention on what is being done, and not on the mechanics
of doing it.

l.6.1 ifelse and which

sometimes we want to do di   erent things to di   erent parts of a vector (or larger
structure) depending on its values. for instance, in robust regression one often
replaces the squared error loss with what   s called the huber loss8,

  (x) =

x2

if |x|     1
2|x|     1 if |x| > 1

(l.4)

(cid:26)

which isn   t so vulnerable to outliers, as in figure l.7.

we might code this up like so:

# calculate huber's id168
# input: vector of numbers x
# return: x^2 for |x|<1, 2|x|-1 otherwise
huber <- function(x) {

n <- length(x)
y <- vector(n)
for (i in 1:n) {

if (abs(x) <= 1) {

y[i] <- x[i]^2

} else {

y[i] <- 2*abs(x[i])-1

}

}
return(y)

}

this is not very easy to follow. r provides a very useful function, ifelse, which
lets us apply a binary test to each element in a vector, and then draw from either
of two calculations. using it, we re-write huber like so:

8 one applies this not to the residuals directly, but to residuals divided by some robust measure of

dispersion.

828

programming

curve(x^2,col="grey",from=-5,to=5,ylab="")
curve(huber,add=true)

figure l.7 the huber id168    from eq. l.4 (black) versus the
squared error loss (grey).

# calculate huber's id168
# input: vector of numbers x
# return: x^2 for |x|<1, 2|x|-1 otherwise
huber <- function(x) {

return(ifelse(abs(x) <= 1, x^2, 2*abs(x)-1))

}

the    rst argument needs to produce a vector of true/false values; the sec-
ond argument provides the outputs for the true positions, the third outputs for
the false positions. here all three are expressions involving the same variable,
but that   s not essential.

another useful device is the which function, whose argument is a vector of
true/false values, returning a vector of the indices where the argument is
true, e.g.,

incomplete.cases <- which(is.na(cholesterol))

would give us the positions at which the vector cholesterol had na values.

this is equivalent to

incomplete.cases <- c()
for (i in 1:length(cholesterol)) {

if (is.na(cholesterol[i])) {

incomplete.cases <- c(incomplete.cases,i)

}

   4   20240510152025xl.6 avoiding iteration: manipulating objects

829

}

l.6.2 apply and its variants

particularly useful ways of avoiding iteration come from the function apply, and
the closely related sapply and lapply functions. (it particularly shows up apply
in chapter 6.)

x <- replicate(10,rpareto(100,2.5,1))
apply(x,2,quantile,probs=0.9)

each call to rpareto inside the replicate creates a vector of length 100.
replicate then stacks these, as columns, into an array. the apply function applies
the same function to each row or column of the array, depending on whether its
second argument is 1 (rows) or 2 (columns). so this will    nd the 90th percentile
of each of the 10 random-number draws, and give that back to us as a vector.

array only works for arrays, matrices and data frames (and works on them
by treating them as arrays). if we want to apply the same function to every
element of a vector or list, we use lapply. this gives us back a list, which can
be inconvenient:

y <- c(0.9,0.99,0.999,0.99999)
lapply(y,qpareto.4,exponent=2.5,threshold=1)
## [[1]]
## [1] 4.641589
##
## [[2]]
## [1] 21.54435
##
## [[3]]
## [1] 100
##
## [[4]]
## [1] 2154.435

the function sapply works like lapply, but tries to simplify its output down

to a vector or array:

sapply(y,qpareto.4,exponent=2.5,threshold=1)
## [1]

21.544347

4.641589

100.000000 2154.434690

that last line just did the equivalent of qpareto.4(y,exponent=2.5,threshold=1),

but sapply can take considerably more complicated functions:

# suppose we have models lm.1 and lm.2 hanging around
some.models <- list(model.1=lm.1, model.2=lm.2)
# extract all the coefficients from all the models
sapply(some.models,coefficients)

sapply has a simplify argument, which defaults to true; setting it to false

830

programming

turns o    the simpli   cation. replicate has the same argument. usually, simpli-
fying the output of sapply or replicate is a good thing, but it can lead to
weirdness when what   s being repeated is a complicated value itself.

for instance, let   s revisit the data set about economic growth and currency
undervaluation across countries and times (problem set a.3), and try    tting a
di   erent model for each    ve-year period.

uv <- read.csv("http://www.stat.cmu.edu/~cshalizi/uada/16/hw/02/uv.csv")
uv.lm.fiveyear <- function(fiveyear) {

lm(growth ~ log(gdp) + underval,data=uv[uv$year==fiveyear,])

}
# what are all the five-year periods in the data?
fiveyears <- sort(unique(uv$year))
fiveyear.models.1 <- sapply(fiveyears, uv.lm.fiveyear)

working with fiveyear.models.1 is going to be very hard, because it wants
to be an array, but isn   t quite, and is generally very confused. (try it!) instead
do it this way:

fiveyear.models.2 <- sapply(fiveyears, uv.lm.fiveyear, simplify=false)

fiveyear.models.2 is simply a list with 10 elements, each one of which is an
lm-style model. now it   s easy extract information about any particular one, or
use sapply:

[,3]

[,1]

0.00843245
-0.00738292

sapply(fiveyear.models.2, coefficients)
##
[,4]
[,2]
## (intercept) -0.04635778 -0.045134479 -0.040404844 -0.045820302
0.008659137
## log(gdp)
0.009419719
0.003926747 -0.007497302 -0.007846092
## underval
##
[,8]
## (intercept) -0.022554554 -0.011886137 -0.028066634 -0.10547596
0.002667598
## log(gdp)
0.01358393
0.013164665
## underval
0.01808939
##
[,10]
## (intercept) -0.038967138 -0.054008775
0.008512894
## log(gdp)
## underval
0.019044209

0.005690720
0.004461034
[,9]

0.006042791
-0.011033117

0.004361408
0.007724422

0.008534740

[,5]

[,6]

[,7]

l.7 more complicated return values

so far, all the functions we have written have returned either a single value,
or a simple vector, or nothing at all. the built-in functions return much more
complicated things, like matrices, data frames, or lists, and we can too.

to illustrate, let   s switch gears away from the pareto distribution, and think
about the gaussian for a change. as you know, if we have data x1, x2, . . . xn and
we want to    t a gaussian distribution to them by maximizing the likelihood, the
best-   tting gaussian has mean

n(cid:88)

i=1

     =

1
n

xi

(l.5)

l.7 more complicated return values

831

which is just the sample mean, and variance

n(cid:88)

i=1

    2 =

1
n

(xi         )2

(l.6)

which di   ers from the usual way of de   ning the sample variance by having a
factor of n in the denominator, instead of n     1. let   s write a function which
takes in a vector of data points and returns the maximum-likelihood parameter
estimates for a gaussian.

gaussian.id113 <- function(x) {

n <- length(x)
mean.est <- mean(x)
var.est <- var(x)*(n-1)/n
est <- list(mean=mean.est, sd=sqrt(var.est))
return(est)

}

there is one argument, which is the vector of data. to be cautious, i should
probably check that it is a vector of numbers, but skip that to be clear here.
the    rst line    gures out how many data points we have. the second takes the
mean. the third    nds the estimated variance     the de   nition of the built-in var
function uses n   1 in its denominator, so i scale it down by the appropriate factor9.
the fourth line creates a list, called est, with two components, named mean and
sd, since those are the names r likes to use for the parameters of gaussians. the
   rst component is our estimated mean, and the second is the standard deviation
corresponding to our estimated variance10. finally, the function returns the list.
as always, it   s a good idea to check the function on a case where we know the

answer.

x <- 1:10
mean(x)
## [1] 5.5
var(x) * (length(x)-1)/length(x)
## [1] 8.25
sqrt(var(x) * (length(x)-1)/length(x))
## [1] 2.872281
gaussian.id113(x)
## $mean
## [1] 5.5
##
## $sd
## [1] 2.872281

9 clearly, if n is large, n   1
10 if n is large,

n =

(cid:113) n   1

(cid:113)
n = 1     1/n will be very close to one, but why not be precise?

2n (using the binomial theorem in the last step). for

1     1

n     1     1

reasonable data sets, the error of just using sd(x) would have been small     but why have it at all?

832

programming

l.8 re-writing your code: an extended example

suppose we want to    nd a standard error for the median of a gaussian distri-
bution. we know, somehow, that the mean of the gaussian is 3, the standard
deviation is 2, and the sample size is one hundred. if we do

x <- rnorm(n=100,mean=3,sd=2)

we   ll get a draw from that distribution in x. if we do

x <- rnorm(n=100,mean=3,sd=2)
median(x)
## [1] 2.862481

we   ll calculate the median on one random draw. following the general idea of
monte carlo (  5.4.1) we can approximate the standard error of the median by
repeating this calculation many times, on many random draws, and taking the
standard deviation. we   ll do this by explicitly iterating, so we need to set up a
vector to store our intermediate results    rst.

medians <- vector(length=100)
for (i in 1:100) {

x <- rnorm(n=100,mean=3,sd=2)
medians[i] <- median(x)

}
se.in.median <- sd(medians)

well, how do we know that 100 replicates is enough to get a good approxima-
tion? we   d need to run this a couple of times, typing it in or at least pasting it in
many times. instead, we can write a function which just gives everything we   ve
done a single name. (i   ll add comments as i go on.)

# inputs: none (everything is hard-coded)
# output: the standard error in the median
find.se.in.median <- function() {

# set up a vector to store the simulated medians
medians <- vector(length=100)
# do the simulation 100 times
for (i in 1:100) {

x <- rnorm(n=100,mean=3,sd=2) # simulate
medians[i] <- median(x) # calculate the median of the simulation

}
se.in.median <- sd(medians) # take standard deviation
return(se.in.median)

}

if we decide that 100 replicates isn   t enough and we want 1000, we need to
change this function. we could just change the    rst two appearances of    100    to
   1000   , but we have to catch all of them; we have to remember that the 100 in
rnorm is there for a di   erent reason and leave it alone; and if we later decide that
actually 500 replicates would be enough, we have to do everything all over again.
it is easier, safer, clearer and more    exible to abstract a little and add an

l.8 re-writing your code: an extended example

833

argument to the function, which is the number of replicates. i   ll add comments
as i go.

# inputs: number of replicates (b)
# output: the standard error in the median
find.se.in.median <- function(b) {

# set up a vector to store the simulated medians
medians <- vector(length=b)
# do the simulation b times
for (i in 1:b) {

x <- rnorm(n=100,mean=3,sd=2) # simulate
medians[i] <- median(x) # calculate median of the simulation

}
se.in.median <- sd(medians) # take standard deviation
return(se.in.median)

}

now suppose we want to    nd the standard error of the median for an ex-
ponential distribution with rate 2 and sample size 37. we could write another
function,

find.se.in.median.exp <- function(b) {

# set up a vector to store the simulated medians
medians <- vector(length=b)
# do the simulation b times
for (i in 1:b) {

x <- rexp(n=37,rate=2) # simulate
medians[i] <- median(x) # calculate median of the simulation

}
se.in.median <- sd(medians) # take standard deviation
return(se.in.median)

}

but it is wasteful to de   ne two functions which do almost the same job. it   s
not just inelegant; it invites mistakes, it   s harder to read (imagine coming back
to this in two weeks     was there a big reason why we had two separate functions
here?), and it   s harder to improve. we need to abstract a bit more.

we could put in some kind of switch which would simulate from either of these

two distributions, maybe like this:

# inputs: number of replicates (b)

# flag for whether to use a normal or an exponential (use.norm)

# output: the standard error in the median
find.se.in.median <- function(b,use.norm=true) {

medians <- vector(length=b)
for (i in 1:b) {

if (use.norm) {

x <- rnorm(100,3,2)

} else {

x <- rexp(37,2)

}
medians[i] <- median(x)

}
se.in.median <- sd(medians)
return(se.in.median)

834

}

programming

but why just these two? if we wanted any other distribution whatsoever, plainly
all we   d have to do is change how x is simulated. so we really want to be able to
give a simulator to the median-   nding function as an argument.

fortunately, in r you can give one function as an argument to another, so we   d

do something like this.

# inputs: number of replicates (b)
# simulator function (simulator)

# presumes: simulator is a no-argument function which produce a vector of

# numbers

# output: the standard error in the media
find.se.in.median <- function(b,simulator) {

median <- vector(length=b)
for (i in 1:b) {

x <- simulator()
medians[i] <- median(x)

}
se.in.medians <- sd(medians)
return(se.in.medians)

}

now to repeat our original calculations, we de   ne a simulator function:

# inputs: none
# output: ten draws from the mean 3, s.d. 2 gaussian
simulator.1 <- function() {

return(rnorm(100,3,2))

}

if we now call this function, then every time find.se.in.median goes through
the for loop, it will call simulator.1, which in turn will produce the right
random numbers.

find.se.in.median(b=100,simulator=simulator.1)
## [1] 0.2551885

if we also de   ne

# inputs: none
# output: 37 draws from the rate 2 exponential
simulator.2 <- function() {

return(rexp(37,2))

}

then to    nd the standard error in the median of this, we just call

find.se.in.median(b=100, simulator=simulator.2)
## [1] 0.09738438

this same approach works if we want to sample from a much more complicated
distribution. if we    t a kernel regression to the data on economic growth and
currency undervaluation (problem set a.3), and want a standard error in the

l.8 re-writing your code: an extended example

835

median of the predicted growth rate, with noise coming from resampling cases,
we would do something like this for the simulator

# perturb the currency-undervaluation data by re-sampling and fit a kernel

# regression for growht on initial gdp and undervaluation

# inputs: none
# output: the fitted growth rates from a new kernel regression
simulator.3 <- function() {

# make sure the np library is loaded
require(np)
# if we haven't already loaded the data, load it
if (!exists("uv")) {

uv <- read.csv("http://www.stat.cmu.edu/~cshalizi/uada/16/hw/02/uv.csv")

}
# how big is the data set?
n <- nrow(uv)
# treat the data set like a population and draw a sample
resampled.rows <- sample(1:n,size=n,replace=true)
uv.r <- uv[resampled.rows,]
# see the chapter on smoothing for the following incantation
fit <- npreg(growth~log(gdp)+underval, data=uv.r, tol=1e-2, ftol=1e-2)
growth.rates <- fitted(fit)
return(growth.rates)

}

and then this to    nd the standard error in the median:

find.se.in.median(b=10, simulator=simulator.3)
## [1] 0.9280185

by breaking up the task this way, if we encounter errors or just general trouble
when we run that last command, it is easier to localize the problem. we can check
whether find.se.in.median seems to work properly with other simulator func-
tions. (for instance, we might write a    simulator    that either does rep(10,1) or
rep(10,-1) with equal id203, since then we can work out what the stan-
dard error of the median ought to be.) we can also check whether simulator.3
is working properly, and    nally whether there is some issue with putting them
together, say that the output from the simulator is not quite in a format that
find.se.in.median can handle. if we just have one big ball of code, it is much
harder to read, to understand, to debug, and to improve.
to turn to that last point, one of the things r does poorly is explicit iteration
with for loops. as mentioned in   l.6, it   s generally better to replace such loops
with    vectorized    functions, which do the iteration using fast code outside of r.
one of these, especially for this situation, is the function replicate. we can
re-write find.se.in.median using it:

# inputs: number of replicates (b)
# simulator function (simulator)

# presumes: simulator is a no-argument function which produces a vector of

# numbers

# outputs: standard error in the median of the output of simulator
find.se.in.median <- function(b,simulator) {

medians <- replicate(b, median(simulator()))

[[todo:
increase
number
repli-
of
cates
for
production
draft]]

836

programming

se.in.median <- sd(medians)
return(se.in.median)

}

again: shorter, faster, and easier to understand (if you know what replicate
does). also, because we are telling this what simulation function to use, and
writing those functions separately, we do not have to change any of our simulators.
they don   t care how find.se.in.median works. in fact, they don   t care that
there is any such function     they could be used as components in many other
functions which can also process their outputs. so long as these interfaces are
maintained, the inner workings of the functions are irrelevant to each other.

suppose for instance that we want not the standard error of the median, but
the interquartile range of the median     the median is after all a    robust   , outlier-
resistant measure of the central tendency, and the iqr is likewise a robust mea-
sure of dispersion. this is now easy:

# inputs: number of replicates (b)
# simulator function (simulator)

# presumes: simulator is a no-argument function which produces a vector of

# numbers

# outputs: interquartile range of the median of the output of simulator
find.iqr.of.median <- function(b,simulator) {
medians <- replicate(b,median(simulator()))
iqr.of.median <- iqr(medians)
return(iqr.of.median)

}

or for that matter the good old standard error of the mean:

# inputs: number of replicates (b)
# simulator function (simulator)

# presumes: simulator is a no-argument function which produces a vector of

# numbers

# outputs: standard error of the mean of the output of simulator
find.se.of.mean <- function(b,simulator) {

means <- replicate(b,mean(simulator()))
se.of.mean <- sd(means)
return(se.of.mean)

}

these last few examples suggest that we could abstract even further, by swap-
ping in and out di   erent estimators (like median and mean) and di   erent sum-
marizing functions (like se or iqr).

# inputs: number of replicates (b)
# simulator function (simulator)
# estimator function (estimator)
# sample summarizer function (summarizer)

# presumes: simulator is a no-argument function which produces a vector of

# numbers
# estimator is a function that takes a vector of numbers and produces one
# output
# summarizer takes a vector of outputs from estimator

l.9 general advice on programming

837

# outputs: summary of the simulated distribution of estimates
summarize.sampling.dist.of.estimates <- function(b,simulator,estimator,

summarizer) {

estimates <- replicate(b,estimator(simulator()))
return(summarizer(estimates))

}

the name is too long, of course, so we should replace it with something catchier

(chapter 6):

bootstrap <- function(b,simulator,estimator,summarizer) {

estimates <- replicate(b,estimator(simulator()))
return(summarizer(estimates))

}

our very    rst example in this section is equivalent to

bootstrap(b=100,simulator=simulator.1, estimator=median, summarizer=sd)
## [1] 0.2142745

bootstrap is just two lines: one simulates and re-estimates, the other summa-
rizes the re-estimates. this is the essence of what we are trying to do, and is
logically distinct from the details of particular simulators, estimators and sum-
maries.

we started with a particular special case and generalized it. the alternative
route is to start with a very general framework     here, writing bootstrap    
and then    gure out what lower-level functions we would need to make it work in
a the case at hand, writing them if necessary. (we need to write a simulator, but
someone   s already written median for us.) getting the    rst stage right involves a
certain amount of re   ection on how to solve the problem     it   s rather like doing
a    show that    math problem by starting from the desired conclusion and working
backwards.

it is still somewhat clunky to have to write a new function every time we want

to change the settings in the simulation, but this has gone on long enough.

l.9 general advice on programming

programming is an act of communication: with the computer, of course, but
also with your co-workers, and with yourself in the future11. clear and e   ective
communication is a valuable skill in itself; it also tends to make it easier to do the
job, and to make debugging easier. this section, then, gives some general advice
about making your programs clearer and more e   ective, closing (in   l.9.7) by
going over how i used these principles when writing code to implement simulation-
based estimation for a time-series model in chapter 26.

11 and, in a class, with your graders.

838

programming

l.9.1 comment your code

comments lengthen your    le, but they make it immensely easier for other people
to understand. (   other people    includes your future self; there are few experiences
more frustrating than coming back to a program after a break only to wonder
what you were thinking.) comments should say what each part of the code does,
and how it does it. the    what    is more important; you can change the    how   
more often and more easily.

every function (or subroutine, etc.) should have comments at the beginning

saying:
    what it does;
    what all its inputs are (in order);
    what it requires of the inputs and the state of the system (   presumes   );
    what side-e   ects it may have (e.g.,    plots histogram of residuals   );
    what all its outputs are (in order)

listing what other functions or routines the function calls (   dependencies   ) is
optional; this can be useful, but it   s easy to let it get out of date.

you should treat    thou shalt comment thy code    as a commandment which

moses brought down from mt. sinai, written on stone by a    ery hand.

l.9.2 use meaningful names

unlike some older languages, r lets you give variables and functions names of
essentially arbitrary length and form. so give them meaningful names. writing
loglikelihood, or even loglike, instead of l makes your code a little longer,
but generally a lot clearer, and it runs just the same.

this rule is lower down in the list because there are exceptions and quali   ca-
tions. if your code is tightly associated to a mathematical paper, or to a    eld
where certain symbols are conventionally bound to certain variables, you may as
well use those names (e.g., call the id203 of success in a binomial p). you
should, however, explain what those symbols are in your comments. in fact, since
what you regard as a meaningful name may be obscure to others (e.g., those
grading your work), you should use comments to explain variables in any case.
finally, it   s ok to use single-letter variable names for counters in loops (but see
the advice on iteration in   l.6).

l.9.3 check whether your program works

it   s not enough     in fact it   s very little     to have a program which runs and
gives you some output. it needs to be the right output. you should therefore
construct tests, which are things that the correct program should be able to do,
but an incorrect program should not. this means that:
    you need to be able to check whether the output is right;

gram to pass them;

l.9 general advice on programming

839
    your tests should be reasonably severe, so that it   s hard for an incorrect pro-
    your tests should help you    gure out what isn   t working;
    you should think hard about programming the test, so it checks whether the
output is right, and you can easily repeat the test as many times as you need.

try to write tests for the component functions, as well as the program as a
whole. that way you can see where failures are. also, it   s easier to    gure out
what the right answers should be for small parts of the problem than the whole.
try to write tests as very small functions which call the component you   re
testing with controlled input values. for instance, we tested qpareto by looking
at what it returned for selected arguments with manually carrying out the com-
putation. with statistical procedures, tests can look at average or distributional
results     we saw an example of this with checking rpareto.

of course, unless you are very clever, or the problem is very simple, a program
could pass all your tests and still be wrong, but a program which fails your tests
is de   nitely not right.

(some people would actually advise writing your tests before writing any actual

functions. they have a point, but i think that   s overkill for this class.)

l.9.4 avoid writing the same thing twice

many data-analysis tasks involve doing the same thing multiple times, either as
iteration, or to slightly di   erent pieces of data, or with some parameters adjusted,
etc. try to avoid writing two pieces of code to do the same job. if you    nd yourself
copying the same piece of code into two places in your program, look into writing
one function, and calling it twice.

doing this means that there is only one place to make a mistake, rather than
many. it also means that when you    x your mistake, you only have one piece of
code to correct, rather than many. (even if you don   t make a mistake, you can
always make improvements, and then there   s only one piece of code you have to
work on.) it also leads to shorter, more comprehensible and more adaptable code.

l.9.5 start from the beginning and break it down

when you have a big problem, start by thinking about what you want your
program to do. then    gure out a set of slightly smaller steps which, put together,
would accomplish that. then take each of those steps and break them down into
yet smaller ones. keep going until the pieces you   re left with are so small that
you can see how to do each of them with only a few lines of code. then write
the code for the smallest bits, check it, once it works write the code for the next
larger bits, and so on.

in slogan form:

    think before you write.

840
    what    rst, then how.
    design from the top down, code from the bottom up.

programming

(not everyone likes to design code this way, and it   s not in the written-in-stone-

atop-sinai category, but there are many much worse ways to start.)

l.9.6 break your code into many short, meaningful functions

since you have broken your programming problem into many small pieces, try
to make each piece a short function. (in other languages you might make them
subroutines or methods, but in r they should be functions.)

each function should achieve a single coherent task     its function, if you will.
the division of code into functions should respect this division of the problem
into sub-problems. more exactly, the way you break your code into functions is
how you have divided your problem.

each function should be short, generally less than a page of print-out. the
function should do one single meaningful thing. (do not just break the calculation
into arbitrary thirty-line chunks and call each one a function.) these functions
should generally be separate, not nested one inside the other.

using functions has many advantages:

program or in other programs

    you can re-use the same code many times, either at di   erent places in this
    the rest of your code only has to care about the inputs and outputs to the
function (its interfaces), not about the internal machinery that turns inputs
into outputs. this makes it easier to design the rest of the program, and it
means you can change that machinery without having to re-design the rest of
the program.

    it makes your code easier to test (see below), to debug, and to understand.
of course, every function should be commented, as described above.

l.9.7 illustration: the method of moments code from   26.1.3

this section goes over the code for the method of moments in   26.1.3 as an
example of how to write code in r, using the principles above.

the    rst function, ma.mm.est, estimates the parameters taking as inputs two
numbers, representing the covariance and the variance. the real work is done by
the built-in optim function12, which itself takes two major arguments. one, fn, is
the function to optimize. another, par, is an initial guess about the parameters
at which to begin the search for the optimum.13

the fn argument to optim must be a function, here ma.mm.objective. the
   rst argument to that function has to be a vector, containing all the parameters
12 see   h.4.
13 here par is a very rough guess based on c and v     it   ll actually be right when c=0, but otherwise
it   s not much good. fortunately, it doesn   t have to be! anyway, let   s return to designing the code

l.10 further reading

841

to be optimized over. (otherwise, optim will quit and complain.) there can be
other arguments, not being optimized over, to that function, which optim will
pass along, as you see here. optim will also accept a lot of optional arguments to
control the search for the optimum     see help(optim).

all ma.mm.objective has to do is calculate the objective function. the    rst
two lines peel out    and   2 from the parameter vector, just to make it more
readable. the next two lines calculate what the moments should be. the last
line calculates the distance between the model predicted moments and the actual
ones, and returns it. the whole thing could be turned into a one-liner, like

return(t(params-c(c,v)) %*% (params-c(c,v)))

or perhaps even more obscure, but that is usually a bad idea.
notice that i could write these two functions independently of one another,
at least to some degree. when writing ma.mm.est, i knew i would need the
objective function, but all i needed to know about it was its name, and the
promise that it would take a parameter vector and give back a real number.
when writing ma.mm.objective, all i had to remember about the other function
was the promise this one needed to ful   ll. in my experience, it is usually easiest to
do any substantial coding in this    top-down    fashion14. start with the high-level
goal you are trying to achieve, break it down into a few steps, write something
which will put those steps together, presuming other functions or programs can
do them. now go and write the functions to do each of those steps.

the code for the method of simulated moments is entirely parallel to these.
writing it as two separate pairs of functions is therefore somewhat wasteful. if i
   nd a mistake in one pair, or thing of a way to improve it, i need to remember to
make corresponding changes in the other pair (and not introduce a new mistake).
in the long run, when you    nd yourself writing parallel pieces of code over and
over, it is better to try to pull together the common parts and write them once.
here, that would mean something like one pair of functions, with the inner one
having an argument which controlled whether to calculate the predicted moments
by simulation or by a formula. you may try your hand at writing this.

l.10 further reading

matlo    (2011) is a good introduction to programming for total novices using r.
braun and murdoch (2008) has more on statistical calculations and related topics,
but can also work as an introduction for absolute beginners. adler (2009) is an
introduction to r for those with some prior knowledge of other programming
languages. for sheer data manipulation, see spector (2008). chambers (2008)
and wickham (2015) are both essential for anyone who wants to be serious about
programming in r.

if you are going to do a lot of computational work, it is worthwhile learning
some of what programmers are taught. the    software carpentry    website (http:
//software-carpentry.org) provides good introduction to key tools, like the

14 what quali   es as    substantial coding    depends on how much experience you have

842

programming

unix shell and version control. it is also worth learning about common data
structures and the algorithms for working with them, since the right choices
there can make dramatic di   erences; i like cormen et al. (2001), but there are
many    ne alternatives.

appendix m

generating random variables

m.1 rejection method

the quantile method of   5.2.2.3 is a very general approach to drawing from a
given distribution, but presumes we can compute the quantile function e   ciently.
another general alternative, which avoids needing quantiles, is the rejection
method. suppose that we want to generate z, with id203 density function
fz, and we have a method to generate r, with p.d.f.   , called the proposal
distribution. also suppose that fz(x)       (x)m , for some constant m > 1.
for instance, if fz has a limited range [a, b], we could take    to be the uniform
distribution on [a, b], and m the maximum density of fz.

the rejection method algorithm then goes as follows.

[[todo:
opening/back-
ref to ch.
5.]]
[[attn:
this
make
purely
a
online sup-
plement?
needed at
all?]]

1. generate a proposal r from   .
2. generate a uniform u , independently of r.
3. is m u   (r) < fz(r)?

    if yes,    accept the proposal    by returning r and stopping.
    if no,    reject the proposal   , discard r and u , and go back to (1)

if    is uniform, this just amounts to checking whether m u < fz(r), with m the
maximum density of z.

computationally, the idea looks like example 48.
one way to understand the rejection method is as follows. imagine drawing

the curve of fz(x). the total area under this curve is 1, because(cid:82) dxfz(x) = 1.
the area between any two points a and b on the horizontal axis is(cid:82) b

a dxfz(x) =
fz(b)     fz(a). it follows that if we could uniformly sample points from the area
between the curve and the horizontal axis, their x coordinates would have exactly
the distribution function we are looking for. if    is a uniform distribution, then
we are drawing a rectangle which just encloses the curve of fz, sampling points
uniformly from the rectangle (with x coordinates r and y coordinates m u ), and
only keeping the ones which fall under the curve. when    is not uniform, but we
can sample from it nonetheless, then we are uniformly sampling from the area
under m   , and keeping only the points which are also below fz.

example. the beta distribution, f (x; a, b) =   (a+b)

  (a)  (b) xa   1(1     x)b   1, is de   ned
on the unit interval1. while its quantile function can be calculated and so we
0 dxe   xxa   1. it is not obvious, but for integer a,   (a) = (a     1)!. the distribution

1 here   (a) =(cid:82)    

843

11:17 monday 1st april, 2019
copyright c(cid:13)cosma rohilla shalizi; do not distribute without permission
updates at http://www.stat.cmu.edu/~cshalizi/adafaepov/

844

generating random variables

rrejection.1 <- function(dtarget,dproposal,rproposal,m) {

rejected <- true
while(rejected) {

r <- rproposal(1)
u <- runif(1)
rejected <- (m*u*dproposal(r) < dtarget(r))

}
return(r)

}

rrejection <- function(n,dtarget,dproposal,rproposal,m) {

replicate(n,rrejection.1(dtarget,dproposal,rproposal,m))

}

code example 48: an example of how the rejection method would be used. the arguments
dtarget, dproposal and rproposal would all be functions. this is not quite industrial-strength
code, because it does not let us pass arguments to those functions    exibly. see online code for
comments. [[todo:    nd those comments!]]

could use the quantile method, we could also use the reject method, taking the
uniform distribution for the proposals. figure m.1 illustrates how it would go for
the beta(5,10) distribution

the rejection method   s main drawback is speed. the id203 of accepting
on any given pass through the algorithm is 1/m (exercise 11). thus produce n
random variables from it takes, on average, nm cycles (exercise 12). clearly, we
want m to be as small, which means that we want the proposal distribution   
to be close to the target distribution fz. of course if we   re using the rejection
method because it   s hard to draw from the target distribution, and the proposal
distribution is close to the target distribution, it may be hard to draw from the
proposal.

m.2 the metropolis algorithm and id115

one very important, but tricky, way of getting past the limitations of the rejection
method is what   s called the metropolis algorithm. once again, we have a
density fz from which we wish to sample. once again, we introduce a distribution
for    proposals   , and accept or reject proposals depending on the density fz. the
twist now is that instead of making independent proposals each time, the next
proposal depends on the last accepted value     the proposal distribution is a
conditional pdf   (r|z).
assume for simplicity that   (r|z) = rho(z|r). (for instance, we could have a
gaussian proposal distribution centered on z.) then the metropolis algorithm
goes as follows.

[[todo:
rejection
method
when
proposal
distri-
bution
is
uniform]]

non-

1. start with value z0 (   xed or random).

gets its name because   (a+b)

generalization of(cid:0)a+b

(cid:1). the beta distribution arises in connection with problems about minima and

  (a)  (b) is called the beta function of a and b, a kind of continuous

a

maxima, and id136 for binomial distributions.

m.2 the metropolis algorithm and id115

845

m <- 3.3
curve(dbeta(x,5,10),from=0,to=1,ylim=c(0,m))
r <- runif(300,min=0,max=1)
u <- runif(300,min=0,max=1)
below <- (m*u*dunif(r,min=0,max=1) <= dbeta(r,5,10))
points(r,m*u,pch=ifelse(below,"+","-"))

figure m.1 illustration of the rejection method for generating random
numbera from a beta(5,10) distribution. the proposal distribution is
uniform on the range of the beta, which is [0, 1]. points are thus sampled
uniformly from the rectangle which runs over [0, 1] on the horizontal axis
and [0, 3.3] on the vertical axis, i.e., m = 3.3, because the density of the
beta is < 3.3 everywhere. (this is not the lowest possible m but it is close.)
proposed points which fall below the beta   s pdf are marked + and are
accepted; those above the pdf curve are marked     and are rejected. in this
case, exactly 70% of proposals are rejected.

0.00.20.40.60.81.00.00.51.01.52.02.53.0xdbeta(x, 5, 10)                  +               +   +   +                  ++      ++         ++            ++            +         +      ++         +            +      ++   +                  ++++                  ++                                    +         +++++   +   +   +         ++   +                     +               ++   +            ++         +   +      +         +   +   ++         ++   +   +      +      +                     +         +   +            ++                        ++   +         ++   +               +            +   +            ++                                       +   ++               ++         +                  +            +      +   ++               +            +                     ++generating random variables

846
2. generate r from the conditional distribution   (  |zt).
3. generate a uniform u , independent of r.
4. is u     fz(r)/fz(zt)?

    if yes, set zt+1 = r and go to (2)
    if not, set zt+1 = zt and go to (2)

mostly simply, the algorithm is run until t = n, at which point it returns
z1, z2, . . . zn. in practice, better results are obtained if it   s run for n + n0 steps,
and the    rst n0 values of z are discarded     this is called    burn-in   .

notice that if fz(r) > fz(zt), then r is always accepted. the algorithm always
accepts proposals which move it towards places where the density is higher than
where it currently is. if fz(r) < fz(zt), then the algorithm accepts the move
with some id203, which shrinks as the density at r gets lower. it should not
be hard to persuade yourself that the algorithm will spend more time in places
where fz is high.
it   s possible to say a bit more. successive values of zt are dependent on each
other, but zt+1        zt   1|zt     this is a markov process. the target distribution fz
is in fact exactly the stationary distribution of the markov process. if the proposal
distributions have broad enough support that the algorithm can get from any z
to any z(cid:48) in a    nite number of steps, then the process will    mix   . (in fact we only
need to be able to visit points where fz > 0.) this means that if we start with an
arbitrary distribution for z0, the distribution of zt approaches fz and stays there
    the point of burn-in is to give this convergence time to happen. the fraction
of time zt is close to x is in fact proportional to fz(x), so we can use the output
of the algorithm as, approximately, so many draws from that distribution.2

it would seem that the metropolis algorithm should be superior to the rejection
method, since to produce n random values we need only n steps, or n + n0 to
handle burn-in, not nm steps. however, this is deceptive, because if the proposal
distribution is not well-chosen, the algorithm ends up staying stuck in the same
spot for, perhaps, a very long time. suppose, for instance, that the distribution is
bimodal. if z0 starts out in between the modes, it   s easy for it to move rapidly to
one peak or the other, and spend a lot of time there. but to go from one mode to
the other, the algorithm has to make a series of moves, all in the same direction,
which all reduce fz, which happens but is unlikely. it thus takes a very long
time to explore the whole distribution. the    best    optimal proposal distribution
is make   (r|z) = fz(r), i.e., to just sample from the target distribution. if we
could do that, of course, we wouldn   t need the metropolis algorithm, but trying
to make    close to fz is generally a good idea.

the original metropolis algorithm was invented in the 1950s to facilitate
designing the hydrogen bomb. it relies on the assumption that the proposal distri-
bution is symmetric,   (r|z) =   (z|r). it is sometimes convenient to allow an asym-
  (zt|r)     fz (r)
metric proposal distribution, in which case one accepts r if u   (r|zt)
fz (zt) .

2 and if the dependence between zt and zt+1 bothers us, we can always randomly permute them,

once we have them.

m.3 generating uniform random numbers

847

this is called metropolis-hastings. both are examples of the broader class of
id115 algorithms, where we give up on getting inde-
pendent samples from the target distribution, and instead make the target the
invariant distribution of a markov process.

m.3 generating uniform random numbers

everything previously to this rested on being able to generate uniform random
numbers, so how do we do that? well, really that   s a problem for computer
scientists. . . but it   s good to understand a little bit about the basic ideas.

first of all, the numbers we get will be produced by some deterministic algo-
rithm, and so will be merely pseudorandom rather than truly random. but we
would like the deterministic algorithm to produce extremely convoluted results,
so that its output looks random in as many ways that we can test as possible.
dependencies should be complicated, and correlations between easily-calculated
functions of successive pseudorandom numbers should be small and decay quickly.
(in fact,    truly random    can be de   ned, more or less, as the limit of the algo-
rithm becoming in   nitely complicated.) typically, pseudorandom number gener- [[todo:
ators are constructed to produce a sequence of uniform values, starting with an
initial value or seed. in normal operation, the seed is set from the computer   s
clock; when debugging, the seed can be held    xed, to ensure that results can be
reproduced exactly.

kol-

cites
on
mogorov
complex-
ity]]

probably the simplest example is incommensurable rotations. imagine a
watch which fails very slightly, but deterministically, to keep proper time, so that
its second hand advances    (cid:54)= 1 seconds in every real second of time. the position
of the watch after t seconds is

  t =   0 + t   mod 60

(m.1)

if    is commensurable with 60, meaning   /60 = k/m for some integers k, m,
then the positions would just repeat every 60k seconds. if    is incommensu-
rable, because it is an irrational number, then   t never repeats. in this case, not
only does   t never repeat, but it is uniformly distributed between 0 and 60, in the
sense that the fraction of time it spends in any sub-interval is just proportional
to the length of the interval (exercise 2).

you could use this as a pseudo-random number generator, with   0 as the seed,
but it would not be a very good one, for two reasons. first, exactly representing an
irrational number    on a digital computer is impossible, so at best you could use
a rational number such that the period 60k is large. second, and more pointedly,
the successive   t are really too close to each other, and too similar. even if we
only took, say, every 50th value, they   d still be quite correlated with each other.
one way this has been improved is to use multiple incommensurable rotations.
say we have a second inaccurate watch,   t =   0 +   t mod 60, where    is incom-
mensurable with both 60 and with   . we record   t when   t is within some small
window of 0.3
3 the core idea here actually dates back to a medieval astronomer named nicholas oresme in the

848

generating random variables

arnold.map <- function(v) {

theta <- v[1]
phi <- v[2]
theta.new <- (theta+phi)%%1
phi.new <- (theta+2*phi)%%1
return(c(theta.new,phi.new))

}

rarnold <- function(n,seed) {

z <- vector(length=n)
for (i in 1:n) {

seed <- arnold.map(seed)
z[i] <- seed[1]

}
return(z)

}

code example 49: a function implementing the arnold cat map (eq. m.3), and a second
function which uses it as a pseudo-random number generator. see online version for comments.
[[todo:    nd those comments]]

another approach is to use more aggressively complicated deterministic map-

pings. take the system

  t+1 =   t +   t mod 1
  t+1 =   t + 2  t mod 1

(m.2)

this is known as    arnold   s cat map   , after the great soviet mathematician v. i.
arnold, and figure m.2. we can think of this as the second-hand   t advancing
not by a    xed amount    every second, but by a varying amount   t. the variable
  t, meanwhile, advances by the amount   t +  t. the e   ect of this is that if we look
at only one of the two coordinates, say   t, we get a sequence of numbers which,
while deterministic, is uniformly distributed, and very hard to predict (figure
m.3).

[[todo:
mersenne
twisters]]

m.4 further reading

[[todo:
clean up]] there are good discussions of methods of random variable generation in press
et al. (1992) [[monahan]] [[others]]. at a higher level of technicality, [[devroye]]
is authoritative. press et al. (1992) also covers techniques for generating uniform
random numbers. also, [[refs. on discrepancy theory/equidistribution.]] ruelle
(1991) provides a thought-provoking and inspiring guide to the idea that a de-
terministic system can look random, among many other topics.

on monte carlo: [[robert and casella]] is a standard authority on applications
and techniques common in statistics. newman and barkema (1999) is excellent
if you know some physics, especially thermodynamics.

on id115, the books already named, together with the

1300s, as part of an argument that the universe would not repeat exactly (von plato, 1994, pp.
279   284).

m.5 exercises

849

[[gilks book]], are worth consulting. anyone attempting to use the technique
should read both geyer (1992), attacking such practices as    burn-in   ,    thinning   ,
the use of convergence diagnostics, etc., and their defense by gelman and rubin
(1992). geyer has, to my mind, the better of the argument, but people i respect
strongly disagree.

m.5 exercises

1. 1. prove that a draw in the rejection method of   m.1 is accepted is accepted
with id203 1/m . that is, the rejection id203 of the rejection
method is 1     1/m .

2. prove that generating n random values with the rejection method requires

on average nm proposals.

2. prove that when    in eq. m.1 is irrational, then the fraction of times at which

  t falls into any given interval [a, b] is proportional to its length:

n(cid:88)

t=1

1
n

1[a,b]  t             
n      

b     a
60

(m.3)

850

generating random variables

figure m.2 e   ect of the arnold cat map. the original image is 300    300,
and mapped into the unit square. the cat map is then applied to the
coordinates of each pixel separately, giving a new pixel which inherits the
old color. (this can most easily seen in the transition from the original to
time 1.) the original image re-assembles itself at time 300 because all the
original coordinates we multiples of 1/300. if we had sampled every, say, 32
time-steps, it would have taken much longer to see a repetition. in the
meanwhile, following the x coordinate of a single pixel from the original
image would provide a very creditable sequence of pseudo-random values.
(figure from wikipedia, s.v.    arnold   s cat map   . see also
http://math.gmu.edu/~sander/movies/arnold.html.)

m.5 exercises

851

par(mfrow=c(2,1))
z <- rarnold(1000,c(0.11124,0.42111))
hist(z,id203=true)
plot(z[-1000],z[-1],xlab=expression(z[t]),ylab=expression(z[t+1]))
par(mfrow=c(1,1))

figure m.3 left: histogram from 1000 samples of the    coordinate of the
arnold cat map, started from (0.11124, 0.42111). right: scatter-plot of
successive values from the sample, showing that the dependence is very
subtle.

histogram of zzdensity0.00.20.40.60.81.00.00.40.81.2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.40.8ztzt+1