id29 with 
id35s
yoav artzi, nicholas fitzgerald and luke zettlemoyer!
university of washington

acl 2013 tutorial!
so   a, bulgaria

website http://yoavartzi.com/tutorial

language to meaning

more informative

language to meaning

information 
extraction

recover information 
about pre-speci   ed 
relations and entities

example task
id36

more informative

is a(obam a, p residen t )

language to meaning

broad-coverage 

semantics

focus on speci   c 

phenomena (e.g., verb-
argument matching)

example task
summarization

more informative

obama wins 
election. big party 
in chicago. 
romney a bit 
down, asks for 
some tea.

language to meaning

semantic 
parsing

recover complete 

meaning 

representation

example task
database query

what states 
border texas?

more informative

oklahoma!
new mexico!

arkansas!
louisiana

language to meaning

semantic 
parsing

recover complete 

meaning 

representation

more informative

example task
instructing a robot

at the chair, 
turn right

language to meaning

semantic 
parsing

recover complete 

meaning 

representation

more informative

complete meaning is suf   cient to 

complete the task

    convert to database query to get the answer!
    allow a robot to do planning

language to meaning

semantic 
parsing

recover complete 

meaning 

representation

more informative

at the chair, move forward three steps past the sofa

 a.pre(a,    x.chair(x)) ^ move(a) ^ len(a, 3)^

dir(a, f orward) ^ past(a,    y.sof a(y))

language to meaning

semantic 
parsing

recover complete 

meaning 

representation

more informative

at the chair, move forward three steps past the sofa

 a.pre(a,    x.chair(x)) ^ move(a) ^ len(a, 3)^

dir(a, f orward) ^ past(a,    y.sof a(y))

language to meaning

at the chair, move forward three steps past the sofa
 a.pre(a,    x.chair(x)) ^ move(a) ^ len(a, 3)^

dir(a, f orward) ^ past(a,    y.sof a(y))

learn

f : sentence ! logical form

language to meaning

at the chair, move forward three steps past the sofa

learn

f : sentence ! logical form

central problems

parsing

learning

modeling

parsing choices

    grammar formalism!
    id136 procedure
inductive logic programming [zelle and mooney 1996]!
sid18 [wong and mooney 2006]!
id35 + cky [zettlemoyer and collins 2005]!
constrained optimization + ilp [clarke et al. 2010]!
dcs + projective id33 [liang et al. 2011]

learning

    what kind of supervision is available?!
    mostly using latent variable methods
annotated parse trees [miller et al. 1994]!
sentence-lf pairs [zettlemoyer and collins 2005]!
question-answer pairs [clarke et al. 2010]!
instruction-demonstration pairs [chen and mooney 2011]!
conversation logs [artzi and zettlemoyer 2011]!
visual sensors [matuszek et al. 2012a]

semantic modeling

    what logical language to use?!
    how to model meaning?
variable free logic [zelle and mooney 1996; wong and mooney 2006]!
high-order logic [zettlemoyer and collins 2005]!
relational algebra [liang et al. 2011]!
id114 [tellex et al. 2011]

today

parsing

id35s

learning

uni   ed learning algorithm

modeling

best practices for semantics design

parsing

learning

modeling

parsing

learning

modeling

!

    id198!
    parsing with combinatory categorial 

grammars !

    linear id35s!
    factored lexicons

parsing

learning

modeling

!

    structured id88!
    a uni   ed learning algorithm!
    supervised learning!
    weak supervision

parsing

!

learning

modeling

    semantic modeling for:!
- querying databases!
- referring to physical objects!
- executing instructions!

uw spf

open source id29 framework!

http://yoavartzi.com/spf

semantic 
parser

flexible high-order 
logic representation

learning 
algorithms

includes ready-to-run examples

[artzi and zettlemoyer 2013a]

parsing

learning

modeling

!

    id198!
    parsing with combinatory categorial 

grammars !

    linear id35s!
    factored lexicons

id198

    formal system to express computation!
    allows high-order functions

 a.move(a) ^ dir(a, lef t ) ^ to(a,    y.chair(y))^

pass(a,ay.sof a(y) ^ intersect(az.intersection(z), y))

[church 1932]

id198 

base cases

    logical constant!
    variable!
    literal!
    lambda term

id198 

logical constants

    represent objects in the world

n y c, ca, rain ier, lef t, . . .
located in, depart date, . . .

id198 

variables

    abstract over objects in the world!
    exact value not pre-determined
x, y, z, . . .

id198 

literals

    represent function application
city(au st in )

located in(au st in, t exas)

id198 

literals

    represent function application
city(au st in )

located in(au st in, t exas)

predicate

arguments

logical expression

list of logical expressions

id198 

lambda terms

    bind/scope a variable!
    repeat to bind multiple variables

 x.city(x)

 x. y.located in(x, y)

id198 

lambda terms

    bind/scope a variable!
    repeat to bind multiple variables

 x.city(x)

 x. y.located in(x, y)

body

lambda 
operator

variable

id198 

quanti   ers?

    higher order constants!
    no need for any special mechanics!
    can represent all of    rst order logic

8( x.big(x) ^ apple(x))
  (9( x.lovely(x))
   ( x.beautif ul(x) ^ grammar(x))

id198 

syntactic sugar

^ (a,^(b, c)) , a ^ b ^ c
_ (a,_(b, c)) , a _ b _ c
  (a) ,   a
q( x.f (x)) , qx.f (x)
for q 2 {   ,a,9,8}

 x.f light(x) ^ to(x, move)
 x.f light(x) ^ to(x, n y c)
 x.n y c(x) ^ x(to, move)

 x.f light(x) ^ to(x, move)
 x.f light(x) ^ to(x, n y c)
 x.n y c(x) ^ x(to, move)

simply typed id198

    like id198!
    but, typed

 x.f light(x) ^ to(x, move)
 x.f light(x) ^ to(x, n y c)
 x.n y c(x) ^ x(to, move)

[church 1940]

id198 

typing
    simple types!
    complex types

< e, t >
<< e, t >, e >

value

t truth-
e

entity

id198 

typing
    simple types!
    complex types

type 

constructor

< e, t >
<< e, t >, e >

domain

range

value

t truth-
e

entity

t
e

tr

loc

id198 

typing
    simple types!
    complex types

type 

constructor

< e, t >
<< e, t >, e >

domain

range

    hierarchical typing system

id198 

typing
    simple types!
    complex types

type 

constructor

< e, t >
<< e, t >, e >

domain

range

    hierarchical typing system

t
e

tr

f la

f l

gt

ap
ci

ti

loc

i

simply typed id198

 a.move(a) ^ dir(a, lef t ) ^ to(a,    y.chair(y))^

pass(a,ay.sof a(y) ^ intersect(az.intersection(z), y))

type information usually omitted

capturing meaning with 

id198

state

border

mountains

show me mountains in states 

bordering texas

[zettlemoyer and collins 2005]

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazcapturing meaning with 

id198

[artzi and zettlemoyer 2011]

systemhow can i help you ?useri     d like to    y to new yorksystem   ying to new york . leaving what city ?userfrom boston on june seven with american airlinessystem   ying to new york . what date would you like to depart boston ?userjune seventhsystemdo you have a preferred airline ?useramerican airlinessystemo . k . leaving boston to new york on june seventh    ying with american airlines . where would you like to go to next ?userback to boston on june tenth[conversation continues]capturing meaning with 

id198

go to the chair 
and turn right

 a.move(a)
^ to(a, . . .

[artzi and zettlemoyer 2013b]

capturing meaning with 

id198

    flexible representation!
    can capture full complexity of natural 

language

more on modeling meaning later

constructing lambda 
calculus expressions

at the chair, move forward three steps past the sofa

?

 a.pre(a,    x.chair(x)) ^ move(a) ^ len(a, 3)^

dir(a, f orward) ^ past(a,    y.sof a(y))

combinatory categorial 

grammars

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

<

[steedman 1996, 2000]

combinatory categorial 

grammars

semantics!

    categorial formalism!
    transparent interface between syntax and 
    designed with computation in mind!
    part of a class of mildly context sensitive 
formalisms (e.g., tag, hg, lig) [joshi et al. 1990]

id35 categories

adj :  x.f un(x)

    basic building block!
    capture syntactic and semantic information 

jointly

id35 categories

syntax

adj :  x.f un(x)

semantics

    basic building block!
    capture syntactic and semantic information 

jointly

id35 categories

syntax adj :  x.f un(x)
(s\n p )/adj :  f. x.f (x)

n p : id35

    primitive symbols: n, s, np, adj and pp!
    syntactic combination operator (/,\)!
    slashes specify argument order and direction

id35 categories

adj :  x.f un(x)

semantics

(s\n p )/adj :  f. x.f (x)

n p : id35

      -calculus expression!
    syntactic type maps to semantic type

id35 lexical entries
fun ` adj :  x.f un(x)

    pair words and phrases with meaning!
    meaning captured by a id35 category

id35 lexical entries
fun ` adj :  x.f un(x)

id35 category

natural!
language

    pair words and phrases with meaning!
    meaning captured by a id35 category

id35 lexicons

fun ` adj :  x.f un(x)
is ` (s\n p )/adj :  f. x.f (x)
id35 ` n p : id35

    pair words and phrases with meaning!
    meaning captured by a id35 category

between id35s and id18s

id18sid35scombination operationsmanyfewparse tree nodesnon-terminalscategoriessyntactic symbolsfew dozenhandful, but can combinepaired with wordspos tagscategoriesparsing with id35s

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

<

use lexicon to match words and 

phrases with their categories

id35 operations

    small set of operators!
    input: 1-2 id35 categories!
    output: a single id35 category!
    operate on syntax semantics together!
    mirror natural logic operations

id35 operations 

application

(<)

(>)

b : g a\b : f ) a : f (g)
a/b : f b : g ) a : f (g)
    equivalent to function application!
    two directions: forward and backward!
- determined by slash direction

id35 operations 

application

argument

function

result

(<)

(>)

b : g a\b : f ) a : f (g)
a/b : f b : g ) a : f (g)
    equivalent to function application!
    two directions: forward and backward!
- determined by slash direction

parsing with id35s

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

<

use lexicon to match words and 

phrases with their categories

parsing with id35s

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

<

combine categories using operators

a/b : f b : g ) a : f (g)

(>)

parsing with id35s

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

<

combine categories using operators

b : g a\b : f ) a : f (g)

(<)

parsing with id35s

composed 
adjectives

square blue or round yellow pillow

non-standard 
coordination

id35 operations 

composition

a/b : f b/c : g ) a/c :  x.f (g(x))
b\c : g a\b : f ) a\c :  x.f (g(x))
    equivalent to function composition*!
    two directions: forward and backward

(> b)

(< b)

* formal de   nition of logical composition in supplementary slides

id35 operations 

composition

f

g

f   g

a/b : f b/c : g ) a/c :  x.f (g(x))
b\c : g a\b : f ) a\c :  x.f (g(x))
    equivalent to function composition*!
    two directions: forward and backward

(> b)

(< b)

* formal de   nition of logical composition in supplementary slides

id35 operations 

type shifting

adj :  x.g(x) ) n/n :  f. x.f (x) ^ g(x)
p p :  x.g(x) ) n\n :  f. x.f (x) ^ g(x)
ap :  e.g(e) ) s\s :  f. e.f (e) ^ g(e)
ap :  e.g(e) ) s/s :  f. e.f (e) ^ g(e)

    category-speci   c unary operations!
    modify category type to take an argument!
    helps in keeping a compact lexicon

id35 operations 

type shifting

input

output

adj :  x.g(x) ) n/n :  f. x.f (x) ^ g(x)
p p :  x.g(x) ) n\n :  f. x.f (x) ^ g(x)
ap :  e.g(e) ) s\s :  f. e.f (e) ^ g(e)
ap :  e.g(e) ) s/s :  f. e.f (e) ^ g(e)

    category-speci   c unary operations!
    modify category type to take an argument!
    helps in keeping a compact lexicon

id35 operations 

type shifting

input

output

topicalization

adj :  x.g(x) ) n/n :  f. x.f (x) ^ g(x)
p p :  x.g(x) ) n\n :  f. x.f (x) ^ g(x)
ap :  e.g(e) ) s\s :  f. e.f (e) ^ g(e)
ap :  e.g(e) ) s/s :  f. e.f (e) ^ g(e)

    category-speci   c unary operations!
    modify category type to take an argument!
    helps in keeping a compact lexicon

id35 operations 

coordination
and ` c : conj
or ` c : disj
    coordination is special cased!
- speci   c rules perform coordination!
- coordinating operators are marked with 

special lexical entries

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

<

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

<

use lexicon to match words and 

phrases with their categories

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

<

shift adjectives to combine

adj :  x.g(x) ) n/n :  f. x.f (x) ^ g(x)

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

<

shift adjectives to combine

adj :  x.g(x) ) n/n :  f. x.f (x) ^ g(x)

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

>

compose pairs of adjectives

a/b : f b/c : g ) a/c :  x.f (g(x))

(> b)

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

>

coordinate composed adjectives

parsing with id35s

square
adj

 x.square(x)

blue
adj

 x.blue(x)

or
c
disj

round
adj

 x.round(x)

yellow
adj

 x.yellow(x)

pillow

n

 x.pillow(x)

n/n

n/n

n/n

n/n

 f. x.f (x) ^ square(x)  f. x.f (x) ^ blue(x)

>b

 f. x.f (x) ^ round(x)  f. x.f (x) ^ yellow(x)

>b

n/n

 f. x.f (x) ^ square(x) ^ blue(x)

n/n

 f. x.f (x) ^ round(x) ^ yellow(x)

 f. x.f (x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

n/n

n

 x.pillow(x) ^ ((square(x) ^ blue(x)) _ (round(x) ^ yellow(x)))

< >

>

apply coordinated adjectives to noun
a/b : f b : g ) a : f (g)

(>)

x 
y

parsing with id35s

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

z

<

lexical 
ambiguity

+

many parsing 

decisions

many potential 
trees and lfs

weighted linear id35s

    given a weighted linear model:!
- id35 lexicon   !
- feature function !
- weights !
    the best parse is:!

w 2 rm

f : x     y ! rm

!

y    = arg max

w    f (x, y)

y

    we consider all possible parses y for sentence x given 

the lexicon   

parsing algorithms

time cky-style algorithms!

    syntax-only id35 parsing has polynomial 
    parsing with semantics requires entire 
category as chart signature!
- e.g., !
    in practice, prune to top-n for each span!
- approximate, but polynomial time

adj :  x.f un(x)

more on id35s

serial dependencies!

    generalized type-raising operations!
    cross composition operations for cross 
    compositional approaches to english 
    and a lot more ... even jazz

intonation!

[steedman 1996; 2000; 2011; granroth and steedman 2012]

the lexicon problem

    key component of id35!
    same words often paired with many 
    dif   cult to learn with limited data

different categories!

factored lexicons

the house dog

the dog of the house

   x.dog(x) ^ of (x,    y.house(y))

the garden dog

   x.dog(x) ^ of (x,    y.garden(y))
    lexical entries share information!
    decomposition of entries can lead to more 

compact lexicons

[kwiatkowski et al. 2011]

factored lexicons

the house dog

the dog of the house

house ` adj :  x.of (x,    y.house(y))
house ` n :  x.house(x)

   x.dog(x) ^ of (x,    y.house(y))

the garden dog

garden ` adj :  x.of (x,    y.garden(y))

   x.dog(x) ^ of (x,    y.garden(y))
    lexical entries share information!
    decomposition of entries can lead to more 

compact lexicons

factored lexicons

the house dog

the dog of the house

house ` adj :  x.of (x,    y.house(y))
house ` n :  x.house(x)

   x.dog(x) ^ of (x,    y.house(y))

the garden dog

garden ` adj :  x.of (x,    y.garden(y))

   x.dog(x) ^ of (x,    y.garden(y))
    lexical entries share information!
    decomposition of entries can lead to more 

compact lexicons

factored lexicons

the house dog

the dog of the house

house ` adj :  x.of (x,    y.house(y))
house ` n :  x.house(x)

   x.dog(x) ^ of (x,    y.house(y))

the garden dog

garden ` adj :  x.of (x,    y.garden(y))

   x.dog(x) ^ of (x,    y.garden(y))
    lexical entries share information!
    decomposition of entries can lead to more 

compact lexicons

factored lexicons

lexemes

house ` adj :  x.of (x,    y.house(y))
house ` n :  x.house(x)
garden ` adj :  x.of (x,    y.garden(y))

(garden,{garden})
(house,{house})

templates
 (!,{vi}n
1 ).

[! ` adj :  x.of (x,    y.v1(y))]
 (!,{vi}n
1 ).

[! ` n :  x.v1(x)]

factored lexicons

templates
 (!,{vi}n
1 ).

[! ` adj :  x.of (x,    y.v1(y))]
 (!,{vi}n
1 ).

[! ` n :  x.v1(x)]
    capture systematic variations 

in word usage!

    each variation can then be 
applied to compact units of 
lexical meaning

lexemes

(garden,{garden})
(house,{house})

    model word meaning!
    abstracts the compositional 

nature of the word!

factored lexicons

(garden,{garden})

words

constants

 (!,{vi}n
1 ).

[! ` n :  x.v1(x)]

!   garden
v1   garden

garden ` n :  x.garden(x)

factored lexicons

original!
lexicon

   ight ` s|n p :  x.f light(x)
   ight ` s|n p/(s|n p ) :  f. x.f light(x) ^ f (x)
   ight ` s|n p\(s|n p ) :  f. x.f light(x) ^ f (x)
ground transport ` s|n p :  x.trans(x)
ground transport ` s|n p/(s|n p ) :  f. x.trans(x) ^ f (x)
ground transport ` s|n p\(s|n p ) :  f. x.trans(x) ^ f (x)

factored!
lexicon

(   ight,{f light})
(ground transport,{trans})
 (!,{vi}n
 (!,{vi}n
 (!,{vi}n

1 ).[! ` s|n p :  x.v1(x)]
1 ).[! ` s|n p/(s|n p ) :  f. x.v1(x) ^ f (x)]
1 ).[! ` s|n p\(s|n p ) :  f. x.v1(x) ^ f (x)]

factoring a lexical entry

house ` adj :  x.of (x,    y.house(y))

partial 
factoring

partial 
factoring

maximal 
factoring

(house,{house})
 (!,{vi}n
(house,{of})
 (!,{vi}n
(house,{of, house})
 (!,{vi}n

1 ).[! ` adj :  x.of (x,    y.v1(y))]

1 ).[! ` adj :  x.v1(x,    y.house(y))]

1 ).[! ` adj :  x.v1(x,    y.v2(y))]

parsing

learning

modeling

!

    id198!
    parsing with combinatory categorial 

grammars !

    linear id35s!
    factored lexicons

learning

data

learning 
algorithm

id35

    what kind of data/supervision we can use?!
    what do we need to learn?

parsing as structure 

prediction

show me

   ights

to

s/n
 f.f

n

p p/n p

 x.f light(x)  y. x.to(x, y)

boston

n p

bost on

>

 x.to(x, bost on )

p p

n\n

 f. x.f (x) ^ to(x, bost on )

<

n

 x.f light(x) ^ to(x, bost on )

>

s

 x.f light(x) ^ to(x, bost on )

learning id35

show me

   ights

to

s/n
 f.f

n

p p/n p

 x.f light(x)  y. x.to(x, y)

boston

n p

bost on

>

 x.to(x, bost on )

p p

n\n

 f. x.f (x) ^ to(x, bost on )

<

n

 x.f light(x) ^ to(x, bost on )

>

s

 x.f light(x) ^ to(x, bost on )

lexicon

combinators

learning id35

show me

   ights

to

s/n
 f.f

n

p p/n p

 x.f light(x)  y. x.to(x, y)

boston

n p

bost on

>

 x.to(x, bost on )

p p

n\n

 f. x.f (x) ^ to(x, bost on )

<

n

 x.f light(x) ^ to(x, bost on )

>

s

 x.f light(x) ^ to(x, bost on )

lexicon

combinators

prede   ned

learning id35

show me

   ights

to

s/n
 f.f

n

p p/n p

 x.f light(x)  y. x.to(x, y)

boston

n p

bost on

>

 x.to(x, bost on )

p p

n\n

 f. x.f (x) ^ to(x, bost on )

<

n

 x.f light(x) ^ to(x, bost on )

s

 x.f light(x) ^ to(x, bost on )

lexicon

combinators

prede   ned

>

w

supervised data

show me

   ights

to

s/n
 f.f

n

p p/n p

 x.f light(x)  y. x.to(x, y)

boston

n p

bost on

>

 x.to(x, bost on )

p p

n\n

 f. x.f (x) ^ to(x, bost on )

<

n

 x.f light(x) ^ to(x, bost on )

>

s

 x.f light(x) ^ to(x, bost on )

supervised data

show me

   ights

to

s/n
 f.f

n

p p/n p

 x.f light(x)  y. x.to(x, y)

boston

n p

bost on

>

n t

p p

n\n

 x.to(x, bost on )

 f. x.f (x) ^ to(x, bost on )

<

n

 x.f light(x) ^ to(x, bost on )

>

 x.f light(x) ^ to(x, bost on )

a t e

s

l

supervised data

supervised learning is done from pairs 

of sentences and logical forms

show me    ights to boston
 x.f light(x) ^ to(x, bost on )
i need a    ight from baltimore to seattle
 x.f light(x) ^ f rom(x, balt im ore) ^ to(x, seat t le)
what ground transportation is available in san francisco
 x.ground transport(x) ^ to city(x, sf )

[zettlemoyer and collins 2005; 2007]

weak supervision

    logical form is latent!
       labeling    requires less expertise!
    labels don   t uniquely determine correct 
    learning requires executing logical forms 
within a system and evaluating the result

logical forms!

weak supervision 

learning from query answers

what is the largest state that borders texas?

n ew m exico

[clarke et al. 2010; liang et al. 2011]

weak supervision 

learning from query answers

what is the largest state that borders texas?

n ew m exico

argmax( x.state(x)

^ border(x, t x),  y.size(y))

argmax( x.river(x)

^ in(x, t x),  y.size(y))

[clarke et al. 2010; liang et al. 2011]

weak supervision 

learning from query answers

what is the largest state that borders texas?

n ew m exico

argmax( x.state(x)

^ border(x, t x),  y.size(y))

argmax( x.river(x)

^ in(x, t x),  y.size(y))

n ew m exico

rio grande

[clarke et al. 2010; liang et al. 2011]

weak supervision 

learning from query answers

what is the largest state that borders texas?

n ew m exico

argmax( x.state(x)

^ border(x, t x),  y.size(y))

argmax( x.river(x)

^ in(x, t x),  y.size(y))

n ew m exico

rio grande

[clarke et al. 2010; liang et al. 2011]

weak supervision 

learning from demonstrations

at the chair, move forward three steps past the sofa

[chen and mooney 2011; kim and mooney 2012; artzi and zettlemoyer 2013b]

weak supervision 

learning from demonstrations

at the chair, move forward three steps past the sofa

some examples from other domains:!
    sentences and labeled game states [goldwasser and roth 2011]!
    sentences and sets of physical objects [matuszek et al. 2012]

[chen and mooney 2011; kim and mooney 2012; artzi and zettlemoyer 2013b]

weak supervision 

learning from conversation logs

[artzi and zettlemoyer 2011]

systemhow can i help you ? (open_task)useri     d like to    y to new yorksystem   ying to new york . (confirm: from(   , atl)) leaving what city ? (ask:   x.from(fl,x))userfrom boston on june seven with american airlinessystem   ying to new york . (confirm: to(   , nyc)) what date would you like to depart boston ? (ask:   x.date(fl,x)   to(fl, bos))userjune seventh[conversation continues]parsing

learning

modeling

!

    structured id88!
    a uni   ed learning algorithm!
    supervised learning!
    weak supervision

structured id88

    simple additive updates !
- only requires ef   cient decoding (argmax)!
- closely related to maxent and other 
feature rich models!
- provably    nds linear separator in    nite 
updates, if one exists!

    challenge: learning with hidden variables

structured id88

data: {(xi, yi) : i = 1 . . . n}
for t = 1 . . . t :

for i = 1 . . . n:

y      arg maxyh   ,  (xi, y)i
if y    6= yi:

          +  (xi, yi)    (xi, y   )

[iterate epochs]!

[iterate examples]
[predict]!
[check]!
[update]

[collins 2002]

one derivation of the id88

ew  f (x,y)

py0 ew  f (x,y0)

log-linear model:

p(y|x) =

step 1: differentiate, to maximize data log-likelihood

update =xi

f (xi, yi)   ep(y|xi)f (xi, y)
step 2: use online, stochastic gradient updates, for example i:
updatei = f (xi, yi)   ep(y|xi)f (xi, y)
step 3: replace expectations with maxes (viterbi approx.)
updatei = f (xi, yi)   f (xi, y   )

y    = arg max

where

y

w    f (xi, y)

the id88 with hidden variables

log-linear 
model:

p(y|x) =xh

p(y, h|x)

p(y, h|x) =

ew  f (x,h,y)

py0,h0 ew  f (x,h0,y0)

step 1: differentiate marginal, to maximize data log-likelihood

update =xi

ep(h|yi,xi)[f (xi, h, yi)]   ep(y,h|xi)[f (xi, h, y)]

step 2: use online, stochastic gradient updates, for example i:

updatei = ep(yi,h|xi)[f (xi, h, yi)]   ep(y,h|xi)[f (xi, h, y)]

step 3: replace expectations with maxes (viterbi approx.)

updatei = f (xi, h0, yi)   f (xi, h   , y   )
w    f (xi, h, y) and h0 = arg max

y   , h    = arg max
y,h

h

where

w    f (xi, h, yi)

hidden variable id88

data: {(xi, yi) : i = 1 . . . n}
for t = 1 . . . t :
for i = 1 . . . n:

y   , h      arg maxy,hh   ,  (xi, h, y)i
if y    6= yi:

h0   arg maxhh   ,  (xi, h, yi)
          +  (xi, h0, yi)    (xi, h   , y   )

[iterate epochs]!

[iterate examples]
[predict]!
[check]!
[predict hidden]!
[update]

[liang et al. 2006; zettlemoyer and collins 2007]

hidden variable id88
    no known convergence guarantees!
- log-linear version is non-convex!
    simple and easy to implement!
- works well with careful initialization!
    modi   cations for id29!
- lots of different hidden information!
- can add a margin constraint, do 
probabilistic version, etc.

uni   ed learning algorithm

    handle various learning signals!
    estimate parsing parameters!
    induce lexicon structure!
    related to loss-sensitive structured 

id88 [singh-miller and collins 2007]

learning choices

validation function

v : y ! {t, f}
    indicates correctness 
of a parse y!
    varying     allows for 
v
differing forms of 
supervision!

lexical generation 

procedure

gen lex(x,v;    ,    )
    given:!

x

sentence!
v
validation function!
lexicon !
   
parameters!

   

    produce a overly general 
set of lexical entries

uni   ed learning algorithm

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)

output: parameters     and lexicon    

    online!
    input:!

    2 steps:!

!
{(xi,vi) : i = 1 . . . n}

- lexical generation!
- parameter update

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)

output: parameters     and lexicon    

initialize parameters and 

lexicon

    weights
   0 initial lexicon

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)

output: parameters     and lexicon    

iterate over data

t # iterations
n # samples

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

output: parameters     and lexicon    

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

output: parameters     and lexicon    

generate a large set of 
potential lexical entries

    weights
xi sentence
vi validation function
gen lex(xi,vi;    ,    )

lexical generation function

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

generate a large set of 
potential lexical entries

    weights
xi sentence
vi validation function
gen lex(xi,vi;    ,    )

lexical generation function

output: parameters     and lexicon    

procedure to propose 
potential new lexical 
entries for a sentence

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

generate a large set of 
potential lexical entries

    weights
xi sentence
vi validation function
gen lex(xi,vi;    ,    )

lexical generation function

output: parameters     and lexicon    

v : y ! {t, f}
y all parses

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

output: parameters     and lexicon    

get top parses

xi sentence
k beam size
gen (xi;  ) set of all parses

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

output: parameters     and lexicon    

get lexical entries from 

highest scoring valid 

parses

    weights
v validation function
lex(y) set of lexical entries
 i(y) =  (xi, y)
m axvi(y ;    ) = {y|y 2 y ^ vi(y)^

8y0 2 y.vi(y) =)

h   ,  i(y0)i     h   ,  i(y)i}

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
a. set  g   gen lex(xi,vi;    ,    ),
b. let y be the k highest scoring parses from

        [  g

gen (xi;  )

c. select lexical entries from the highest scor-

ing valid parses:

 i  sy2m axvi(y ;   ) lex(y)

d. update lexicon:           [  i
step 2: (update parameters)

output: parameters     and lexicon    

update model   s lexicon

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)
a. set gi   m axvi(gen (xi;    );    )
and bi   {e|e 2 gen (xi;    ) ^   vi(y)}
b. construct sets of margin violating good and
bad parses:
ri   {g|g 2 gi ^ 9b 2 bi
ei   {b|b 2 bi ^ 9g 2 gi
c. apply the additive update:
 i(r)
 i(e)

s.t. h   ,  i(g)   i(b)i <   i(g, b)}
s.t. h   ,  i(g)   i(b)i <   i(g, b)}

|ri|pr2ri
          + 1
|ei|pe2ei
  1

output: parameters     and lexicon    

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)
a. set gi   m axvi(gen (xi;    );    )
and bi   {e|e 2 gen (xi;    ) ^   vi(y)}
b. construct sets of margin violating good and
bad parses:
ri   {g|g 2 gi ^ 9b 2 bi
ei   {b|b 2 bi ^ 9g 2 gi
c. apply the additive update:
 i(r)
 i(e)

s.t. h   ,  i(g)   i(b)i <   i(g, b)}
s.t. h   ,  i(g)   i(b)i <   i(g, b)}

|ri|pr2ri
          + 1
|ei|pe2ei
  1

output: parameters     and lexicon    

re-parse and group all 
parses into    good    and 

   bad    sets

    weights
xi sentence
vi validation function
gen (xi;  ) set of all parses
 i(y) =  (xi, y)
m axvi(y ;    ) = {y|y 2 y ^ vi(y)^

8y0 2 y.vi(y) =)

h   ,  i(y0)i     h   ,  i(y)i}

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)
a. set gi   m axvi(gen (xi;    );    )
and bi   {e|e 2 gen (xi;    ) ^   vi(y)}
b. construct sets of margin violating good and
bad parses:
ri   {g|g 2 gi ^ 9b 2 bi
ei   {b|b 2 bi ^ 9g 2 gi
c. apply the additive update:
 i(r)
 i(e)

s.t. h   ,  i(g)   i(b)i <   i(g, b)}
s.t. h   ,  i(g)   i(b)i <   i(g, b)}

|ri|pr2ri
          + 1
|ei|pe2ei
  1

output: parameters     and lexicon    

for all pairs of    good    
and    bad    parses, if their 

scores violate the 
margin, add each to 
   right    and    error    sets 

respectively

    weights
  margin
 i(y) =  (xi, y)
 i(y, y0) = | i(y)    i(y0)|1

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)
a. set gi   m axvi(gen (xi;    );    )
and bi   {e|e 2 gen (xi;    ) ^   vi(y)}
b. construct sets of margin violating good and
bad parses:
ri   {g|g 2 gi ^ 9b 2 bi
ei   {b|b 2 bi ^ 9g 2 gi
c. apply the additive update:
 i(r)
 i(e)

s.t. h   ,  i(g)   i(b)i <   i(g, b)}
s.t. h   ,  i(g)   i(b)i <   i(g, b)}

|ri|pr2ri
          + 1
|ei|pe2ei
  1

output: parameters     and lexicon    

update towards 

violating    good    parses 

and against violating    bad    

parses

    weights
 i(y) =  (xi, y)

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)

output: parameters     and lexicon    

return grammar

    weights
    lexicon

features and initialization

feature 
classes

    parse: indicate lexical entry and combinator use!
    logical form: indicate local properties of logical 

forms, such as constant co-occurrence

lexicon 
initialization

    often use an np list!
    sometimes include additional, domain 
independent entries for function words

initial 
weights

    positive weight for initial lexical indicator 

features

uni   ed learning algorithm

vv
v validation function
gen lex(x,v;  ,    )
gen lex
gen lex
gen lex

lexical generation function

    two parts of the algorithm we still need to de   ne!
    depend on the task and supervision signal

uni   ed learning algorithm

supervised

supervised
v
template-based

gen lex

uni   cation-based

gen lex

weakly supervised
v
template-based

gen lex

supervised learning

show me the afternoon    ights from la to boston

 x.f light(x) ^ during(x, af t ern oon ) ^ f rom(x, la) ^ to(x, bos)

supervised learning

show me the afternoon    ights from la to boston

 x.f light(x) ^ during(x, af t ern oon ) ^ f rom(x, la) ^ to(x, bos)

parse structure is latent

supervised learning

supervised

supervised
v
template-based

gen lex

uni   cation-based

gen lex

supervised validation 

function

    validate logical form against gold label
if lf (y) = zi
else

vi(y) =(true

f alse

y parse
zi labeled logical form
lf (y) logical form at the root of y

supervised template-based

gen lex(x, z;    ,    )

sentence

logical 
form

lexicon weights

small notation abuse: 
take labeled logical 

form instead of 
validation function

supervised template-based

gen lex(x, z;    ,    )

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)

supervised template-based 

genlex

structure!

    use templates to constrain lexical entries 
    for example: from a small annotated dataset
 (!,{vi}n
 (!,{vi}n
 (!,{vi}n
 (!,{vi}n
. . .

1 ).[! ` adj :  x.v1(x)]
1 ).[! ` p p :  x. y.v1(y, x)]
1 ).[! ` n :  x.v1(x)]
1 ).[! ` s\n p/n p :  x. y.v1(x, y)]

[zettlemoyer and collins 2005]

supervised template-based 

genlex

need lexemes to instantiate templates

 (!,{vi}n
 (!,{vi}n
 (!,{vi}n
 (!,{vi}n
. . .

1 ).[! ` adj :  x.v1(x)]
1 ).[! ` p p :  x. y.v1(y, x)]
1 ).[! ` n :  x.v1(x)]
1 ).[! ` s\n p/n p :  x. y.v1(x, y)]

supervised template-based

gen lex(x, z;    ,    )

all possible 
sub-strings

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)

i want
a    ight
   ight
   ight to new
. . .

supervised template-based

gen lex(x, z;    ,    )

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)

all logical 

constants from 

labeled logical form

i want
a    ight
   ight
   ight to new
. . .

f light
to
n y c

supervised template-based

gen lex(x, z;    ,    )

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)

i want
a    ight
   ight
   ight to new
. . .

f light
to
n y c

create 
lexemes

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

supervised template-based

gen lex(x, z;    ,    )

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)

i want
a    ight
   ight
   ight to new
. . .

f light
to
n y c

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

initialize 
templates

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

fast parsing with pruning

    genlex outputs a large number of entries!
    for fast parsing: use the labeled logical form 
    prune partial logical forms that can   t lead to 

to prune!

labeled form

i want a    ight from new york to boston on delta
 x.f rom(x, n y c) ^ to(x, bos) ^ carrier(x, dl)

fast parsing with pruning

i want a    ight from new york to boston on delta
 x.f rom(x, n y c) ^ to(x, bos) ^ carrier(x, dl)
boston

new york

from

to

. . .

. . .

p p/n p

 x. y.to(y, x)
p p

n p
n y c

>

p p/n p

 x. y.to(y, x)
p p

n p
bos

>

 y.to(y, n y c)

 y.to(y, bos)

n\n

 f. y.f (y) ^ to(y, bos)

fast parsing with pruning

i want a    ight from new york to boston on delta
 x.f rom(x, n y c) ^ to(x, bos) ^ carrier(x, dl)
boston

new york

from

to

. . .

. . .

p p/n p

 x. y.to(y, x)
p p

n p
n y c

>

p p/n p

 x. y.to(y, x)
p p

n p
bos

>

 y.to(y, n y c)

 y.to(y, bos)

n\n

 f. y.f (y) ^ to(y, bos)

fast parsing with pruning

i want a    ight from new york to boston on delta
 x.f rom(x, n y c) ^ to(x, bos) ^ carrier(x, dl)
boston

new york

from

to

. . .

. . .

p p/n p

 x. y.to(y, x)
p p

n p
n y c

>

p p/n p

 x. y.to(y, x)
p p

n p
bos

>

 y.to(y, n y c)

 y.to(y, bos)

n\n

 f. y.f (y) ^ to(y, bos)

fast parsing with pruning

i want a    ight from new york to boston on delta
 x.f rom(x, n y c) ^ to(x, bos) ^ carrier(x, dl)
boston

new york

from

to

. . .

. . .

p p/n p

 x. y.to(y, x)
p p

n p
n y c

>

p p/n p

 x. y.to(y, x)
p p

n p
bos

>

 y.to(y, n y c)

 y.to(y, bos)

n\n

 f. y.f (y) ^ to(y, bos)

supervised template-based 

genlex
summary

no initial expert knowledgecreates compact lexicons   language independentrepresentation independenteasily inject linguistic knowledge   weakly supervised learning   uni   cation-based genlex

approaches for semantic modeling!

    automatically learns the templates!
- can be applied to any language and many different 
    two step process!
- initialize lexicon with labeled logical forms!
-    reverse    parsing operations to split lexical 

entries 

[kwiatkowski et al. 2010]

uni   cation-based genlex

    initialize lexicon with labeled logical forms

for every labeled training example:

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

initialize the lexicon with:
i want a    ight to boston ` s :  x.f light(x) ^ to(x, bos)

uni   cation-based genlex

    splitting lexical entries
i want a    ight to boston ` s :  x.f light(x) ^ to(x, bos)

i want a    ight ` s/(s|n p ) :  f. x.f light(x) ^ f (x)

to boston ` s|n p :  x.to(x, bos)

uni   cation-based genlex

    splitting lexical entries
i want a    ight to boston ` s :  x.f light(x) ^ to(x, bos)

i want a    ight ` s/(s|n p ) :  f. x.f light(x) ^ f (x)

to boston ` s|n p :  x.to(x, bos)

many possible 
phrase pairs

many possible 
category pairs

uni   cation-based genlex
    splitting id35 categories:!

1. split logical form h to f and g s.t.!

f (g) = h

or!

 x.f (g(x)) = h

2. infer syntax from logical form type

s :  x.f light(x) ^ to(x, bos)

 f. x.f light(x) ^ f (x)
 x.to(x, bos)

 y. x.f light(x) ^ f (x, y)
bos
. . .

uni   cation-based genlex
    splitting id35 categories:!

1. split logical form h to f and g s.t.!

f (g) = h

or!

 x.f (g(x)) = h

2. infer syntax from logical form type

s :  x.f light(x) ^ to(x, bos)

s/(s|n p ) :
s|n p :

 f. x.f light(x) ^ f (x)
 x.to(x, bos)

s/n p :
n p :

 y. x.f light(x) ^ f (x, y)
bos

. . .

 f. x.flight(x)^f(x) x.to(x,bos)s/(s|np):s|np:uni   cation-based genlex
    split text and create all pairs
i want a    ight to boston ` s :  x.f light(x) ^ to(x, bos)

i want
a    ight to boston

s/(s|n p ) :
s|n p :

 f. x.f light(x) ^ f (x)
 x.to(x, bos)

i want a    ight
to boston
. . .

s/(s|n p ) :
s|n p :

 f. x.f light(x) ^ f (x)
 x.to(x, bos)

s: x.flight(x)^to(x,bos)uni   cation-based
gen lex(x, z;    ,    )

sentence

logical 
form

lexicon weights

1. find highest scoring correct parse!
2. find split that most increases score!
3. return new lexical entries

parameter initialization

compute co-occurrence (ibm model 1) 
between words and logical constants

i want a    ight to boston ` s :  x.f light(x) ^ to(x, bos)

i want a    ight to boston ` s :  x.f light(x) ^ to(x, bos)

initial score for new lexical entries: average 
over pairwise weights

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

1. find highest scoring 

correct parse!

2. find splits that most 

increases score!

3. return new lexical 

entries

i want a    ight to boston

s

 x.f light(x) ^ to(x, bos)

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

1. find highest scoring 

correct parse!

2. find splits that most 

increases score!

3. return new lexical 

entries

i want a    ight
s/(s|n p )

to boston
s|n p

 f. x.f light(x) ^ f (x)  x.to(x, bos)

i want a    ight to boston

s

 x.f light(x) ^ to(x, bos)

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

1. find highest scoring 

correct parse!

2. find splits that most 

increases score!

3. return new lexical 

entries

i want a    ight
s/(s|n p )

to boston
s|n p

 f. x.f light(x) ^ f (x)  x.to(x, bos)

i want a    ight to boston

s

 x.f light(x) ^ to(x, bos)

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

1. find highest scoring 

correct parse!

2. find splits that most 

increases score!

3. return new lexical 

entries

iteration 2

i want a    ight
s/(s|n p )

to boston
s|n p

 f. x.f light(x) ^ f (x)  x.to(x, bos)

>

s

 x.f light(x) ^ to(x, bos)

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

1. find highest scoring 

correct parse!

2. find splits that most 

increases score!

3. return new lexical 

entries

iteration 2

to

boston

(s|n p )/n p
n p
 y. x.to(x, y) bos

i want a    ight
s/(s|n p )

to boston
s|n p

 f. x.f light(x) ^ f (x)  x.to(x, bos)

>

s

 x.f light(x) ^ to(x, bos)

uni   cation-based
gen lex(x, z;    ,    )

i want a    ight to boston
 x.f light(x) ^ to(x, bos)

1. find highest scoring 

correct parse!

2. find splits that most 

increases score!

3. return new lexical 

entries

iteration 2

to

boston

(s|n p )/n p
n p
 y. x.to(x, y) bos

i want a    ight
s/(s|n p )

to boston
s|n p

 f. x.f light(x) ^ f (x)  x.to(x, bos)

>

s

 x.f light(x) ^ to(x, bos)

experiments

    two database corpora:!
- geo880/geo250 [zelle and mooney 1996; tang and mooney 2001]!
- atis [dahl et al. 1994]!
    learning from sentences paired with logical 
    comparing template-based and uni   cation-

forms!

based genlex methods

[zettlemoyer and collins 2007; kwiatkowski et al. 2010; 2011]

results

template-based 

uni   cation-based

uni   cation-based + factored lexicon

90

67.5

45

22.5

0

geo880

atis geo250 english

geo250 spanish

geo250 japanese

geo250 turkish

[zettlemoyer and collins 2007; kwiatkowski et al. 2010; 2011]

genlex comparison

templatesuni   cationno initial expert knowledge   creates compact lexicons   language independent   representation independent   easily inject linguistic knowledge   weakly supervised learning   genlex comparison

templatesuni   cationno initial expert knowledge   creates compact lexicons   language independent   representation independent   easily inject linguistic knowledge   weakly supervised learning   ?recap 

id35s

is

id35
s\n p/adj
n p
id35  f. x.f (x)

fun
adj

 x.f un(x)
>

s\n p
 x.f un(x)
s

f un(id35)

<

[steedman 1996, 2000]

recap 

uni   ed learning algorithm

initialize     using    0 ,          0
for t = 1 . . . t, i = 1 . . . n :

step 1: (lexical generation)
step 2: (update parameters)

output: parameters     and lexicon    

    online!
    2 steps:!

- lexical generation!
- parameter update

recap 

learning choices

validation function

v : y ! {t, f}
    indicates correctness 
of a parse y!
    varying     allows for 
v
differing forms of 
supervision!

lexical generation 

procedure

gen lex(x,v;    ,    )
    given:!

x

sentence!
v
validation function!
lexicon !
   
parameters!

   

    produce a overly general 
set of lexical entries

uni   ed learning algorithm

supervised

supervised
v
template-based

gen lex

uni   cation-based

gen lex

weakly supervised
v
template-based

gen lex

weak supervision

what is the largest state that borders texas?

new mexico

[clarke et al. 2010; liang et al. 2011]

weak supervision

what is the largest state that borders texas?

new mexico

at the chair, move forward three steps past the sofa

[clarke et al. 2010; liang et al. 2011; chen and mooney 2011; artzi and zettlemoyer 2013b]

weak supervision

what is the largest state that borders texas?

new mexico

at the chair, move forward three steps past the sofa

execute the logical form and observe the result

weakly supervised 
validation function
vi(y) =(true

if exec(y) t ei
else

f alse

y 2 y parse
ei 2 e available execution result
exec(y) : y ! e

logical form at the root of y

[artzi and zettlemoyer 2013b]

weakly supervised 
validation function
vi(y) =(true

if exec(y) t ei
else

f alse

domain-speci   c 

execution function: 
sql query engine, 
navigation robot 

y 2 y parse
ei 2 e available execution result
exec(y) : y ! e

logical form at the root of y

weakly supervised 
validation function
vi(y) =(true

if exec(y) t ei
else

f alse

depends on 
supervision

domain-speci   c 

execution function: 
sql query engine, 
navigation robot 

y 2 y parse
ei 2 e available execution result
exec(y) : y ! e

logical form at the root of y

weakly supervised 
validation function
vi(y) =(true

if exec(y) t ei
else

f alse

depends on 
supervision

domain-speci   c 

execution function: 
sql query engine, 
navigation robot 

y 2 y parse
ei 2 e available execution result
exec(y) : y ! e

logical form at the root of y

in general: execution function is a natural 

part of a complete system

weakly supervised 
validation function

example exec(y):

robot moving in an environment

weakly supervised 
validation function

example exec(y):

robot moving in an environment

example supervision:

complete 

demonstration

weakly supervised 
validation function

example exec(y):

robot moving in an environment

example supervision:

complete 

demonstration

validate all steps

weakly supervised 
validation function

example exec(y):

robot moving in an environment

example supervision:

final state

validate only last 

position

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)

i want
a    ight
   ight
   ight to new
. . .

f light
to
n y c

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

initialize 
templates

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

[artzi and zettlemoyer 2013b]

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york
 x.f light(x) ^ to(x, n y c)
labeled logical form

no access to 

i want
a    ight
   ight
   ight to new
. . .

f light
to
n y c

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

initialize 
templates

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

f light, f rom, to,
ground transport, dtime, atime,
n y c, bos, la, sea, . . .

use all logical 
constants in the 
system instead

initialize 
templates

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

f light, f rom, to,
ground transport, dtime, atime,
n y c, bos, la, sea, . . .

use all logical 
constants in the 
system instead

initialize 
templates

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
many more 
. . .

lexemes

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

huge number of 
lexical entries

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

f light, f rom, to,
ground transport, dtime, atime,
n y c, bos, la, sea, . . .

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

parse to prune 
generated lexicon

model

huge number of 
lexical entries

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

f light, f rom, to,
ground transport, dtime, atime,
n y c, bos, la, sea, . . .

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

parse to prune 
generated lexicon

intractable

model

huge number of 
lexical entries

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

?

(   ight,{f light})
(i want,{})
(   ight to new,{to, n y c})
. . .

initialize 
templates

   ight ` n :  x.f light(x)
i want ` s/n p :  x.x
   ight to new : s\n p/n p :  x. y.to(x, y)
. . .

weakly supervised
gen lex(x,v;    ,    )

to-   ne id29 algorithm!

    gradually prune lexical entries using a coarse-
    transition from coarse to    ne de   ned by 

typing system

coarse ontology

f light<f l,t>, f rom<f l,<loc,t>>, to<f l,<loc,t>>,
ground transport<gt,t>, dtime<tr,<ti,t>>, atime<tr,<ti,t>>,
n y cci, bosci, jf kap, lasap, . . .

t
e

tr

f la

f l

gt

coarse ontology

f light<f l,t>, f rom<f l,<loc,t>>, to<f l,<loc,t>>,
ground transport<gt,t>, dtime<tr,<ti,t>>, atime<tr,<ti,t>>,
n y cci, bosci, jf kap, lasap, . . .

generalize types

f light<e,t>, f rom<e,<e,t>>, to<e,<e,t>>,
ground transport<e,t>, dtime<e,<e,t>>, atime<e,<e,t>>,
n y ce, bose, lae, seae, . . .

t
e

tr

f la

f l

gt

f light<f l,t>

f l ! e
t ! t

f light<e,t>

t
e

tr

f la

f l

gt

coarse ontology

f light<f l,t>, f rom<f l,<loc,t>>, to<f l,<loc,t>>,
ground transport<gt,t>, dtime<tr,<ti,t>>, atime<tr,<ti,t>>,
n y cci, bosci, jf kap, lasap, . . .

generalize types

f light<e,t>, f rom<e,<e,t>>, to<e,<e,t>>,
ground transport<e,t>, dtime<e,<e,t>>, atime<e,<e,t>>,
n y ce, bose, lae, seae, . . .

merge identically 
typed constants 

c1<e,t>, c2<e,<e,t>>, c3e, . . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

all possible 
sub-strings

i want
a    ight
   ight
   ight to new
. . .

c1<e,t>
c2<e,<e,t>>
c3e
. . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

all possible 
sub-strings

i want
a    ight
   ight
   ight to new
. . .

c1<e,t>
c2<e,<e,t>>
c3e
. . .

create 
lexemes

(   ight,{c1})
(i want,{})
(   ight to new,{c2})
. . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

c1<e,t>
c2<e,<e,t>>
c3e
. . .

(   ight,{c1})
(i want,{})
(   ight to new,{c2})
. . .

initialize 
templates

   ight ` n :  x.c1(x)
i want ` s/n p :  x.x
   ight to new ` s\n p/n p :  x. y.c2(x, y)
. . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

i want
a    ight
   ight
   ight to new
. . .

c1<e,t>
c2<e,<e,t>>
c3e
. . .

coarse 
constants

(   ight,{c1})
(i want,{})
(   ight to new,{c2})
. . .

initialize 
templates

   ight ` n :  x.c1(x)
i want ` s/n p :  x.x
   ight to new ` s\n p/n p :  x. y.c2(x, y)
. . .

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

prune by 
parsing

   ight ` n :  x.c1(x)
i want ` s/n p :  x.x
   ight to new ` s\n p/n p :  x. y.c2(x, y)
. . .

keep only lexical entries that participate in 

complete parses, which score higher than the 

current best valid parse by a margin

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

prune by 
parsing

   ight ` n :  x.c1(x)
i want ` s/n p :  x.x
   ight to new ` s\n p/n p :  x. y.c2(x, y)
. . .

keep only lexical entries that participate in 

complete parses, which score higher than the 

current best valid parse by a margin

weakly supervised
gen lex(x,v;    ,    )

i want a    ight to new york

   ight ` n :  x.c1(x)
. . .

replace all coarse constants with 

all similarly typed constants

   ight ` n :  x.f light(x)
   ight ` n :  x.ground transport(x)
   ight ` n :  x.nonstop(x)
   ight ` n :  x.connecting(x)
. . .

weak supervision 

requirements

    know how to act given a logical form!
    a validation function !
    templates for lexical induction

experiments

instruction:
at the chair, move forward three steps past the sofa
demonstration:

    situated learning with joint id136!
    two forms of validation!
    template-based

gen lex(x,v;    ,    )

[artzi and zettlemoyer 2013b]

results

final state validation
trace validation

80

64

48

32

16

0

single sentence

sequence

logical form

51.0558.0578.634454.6377.6uni   ed learning algorithm 

extensions

    loss-sensitive learning!
- applied to learning from conversations!
    stochastic id119!
- approximate expectation computation

[artzi and zettlemoyer 2011; zettlemoyer and collins 2005]

parsing

learning

modeling

!

    structured id88!
    a uni   ed learning algorithm!
    supervised learning!
    weak supervision

modeling

show me all papers about id29

parsing with id35

 x.paper(x) ^ topic(x, sem p ar)

modeling

show me all papers about id29

parsing with id35

 x.paper(x) ^ topic(x, sem p ar)

what should these logical forms look like?

but why should we care?

modeling considerations

modeling is key to learning compact 
lexicons and high performing models

    capture language complexity!
    satisfy system requirements!
    align with language units of meaning

parsing

!

learning

modeling

    semantic modeling for:!
- querying databases!
- referring to physical objects!
- executing instructions!

querying databases

state

border

mountains

[zettlemoyer and collins 2005]

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

noun phrases

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

verbs

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

nouns

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

prepositions

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

superlatives

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

determiners

what is the largest state?

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazquerying databases

state

border

mountains

what is the capital of arizona?!

how many states border california?!

questions

what is the largest state?

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaaznamestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoreferring to db entities

noun phrases

select single db entities

prepositions!

verbs

relations between entities

nouns

typing (i.e., column headers)

superlatives

ordering queries

noun phrases

state

mountains

noun phrases name 
speci   c entities
washington
wa

florida
the sunshine state
fl

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertconoun phrases

state

mountains

e-typed 
entities

noun phrases name 
speci   c entities
washington
wa
wa
florida
the sunshine state
fl
fl

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertconoun phrases

state

mountains

noun phrases name 
speci   c entities
washington

n p
w a

the sunshine state

n p
f l

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcostate

verb relations
border

verbs express relations 
between entities

nevada borders california
border(n v, ca)

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazstate

verb relations
border

verbs express relations 
between entities

nevada borders california
border(n v, ca)
true

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazverb relations

state

nevada

n p
n v

borders
s\n p/n p

california

n p
ca

 x. y.border(y, x)
s\n p

 y.border(y, ca)

>

<

s

border(n v, ca)

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4nouns

state

mountains

nouns are functions 
that de   ne entity type
state
 x.state(x)

mountain
 x.mountain(x)

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertconouns

state

mountains

!

e ! t
functions 
de   ne sets

nouns are functions 
that de   ne entity type
state
 x.state(x)

,

,

wa

al

ak

,...
}

mountain
 x.mountain(x)

bianca

antero

,

},...

{

{

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertconouns

state

mountains

nouns are functions 
that de   ne entity type
state

n

 x.state(x)

mountain

n

 x.mountain(x)

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcostate

prepositions
mountains

prepositional phrases are 
conjunctive modi   ers

mountain in colorado

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcostate

prepositions
mountains

prepositional phrases are 
conjunctive modi   ers

mountain
 x.mountain(x)

{

bianca

,

antero

rainier

,
...,
}

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcostate

prepositions
mountains

prepositional phrases are 
conjunctive modi   ers

mountain in colorado
 x.mountain(x)^
{

in(x, co)

}

antero

bianca

,

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoprepositions

state

mountain

in

colorado

n

p p/n p

 x.mountain(x)  y. x.in(x, y)
p p

n p
co

>

 x.in(x, co)

n\n

 f. x.f (x) ^ in(x, co)
n

<

 x.mountain(x) ^ in(x, co)

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4function words

state

border

certain words are used to 
modify syntactic roles

state that borders california
 x.state(x) ^ border(x, ca)
{

}

or

nv

az

,

,

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazfunction words

state

state

n
n v

that

p p/(s\n p )

 f.f

california

n p
ca

borders
s\n p/n p

 x. y.border(y, x)
s\n p

 y.border(y, ca)
p p

 y.border(y, ca)

>

>

<

n\n

 f. y.f (y) ^ border(y, ca)
 x.state(x) ^ (x, ca)

n

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4function words

state

border

certain words are used to 
modify syntactic roles

    may have other senses 
with semantic meaning!

    may carry content in 

other domains

other common function 
words: which, of, for, are, is, 
does, please

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazde   nite determiners

state

mountains

de   nite determiner 
selects the single members 
of a set when such exists

    : (e ! t) ! e

the mountain in washington

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcode   nite determiners

state

mountains

de   nite determiner 
selects the single members 
of a set when such exists

    : (e ! t) ! e

mountain in washington
 x.mountain(x) ^ in(x, w a)
{

}

rainier

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcode   nite determiners

state

mountains

de   nite determiner 
selects the single members 
of a set when such exists

    : (e ! t) ! e

the mountain in washington
   x.mountain(x) ^ in(x, w a)
{

}

rainier

rainier

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcode   nite determiners

state

mountains

de   nite determiner 
selects the single members 
of a set when such exists

    : (e ! t) ! e

the mountain in colorado
   x.mountain(x) ^ in(x, co)
{
{
?

}

antero

bianca

,

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcode   nite determiners

state

mountains

de   nite determiner 
selects the single members 
of a set when such exists

    : (e ! t) ! e

the mountain in colorado
   x.mountain(x) ^ in(x, co)
{
{

}

antero

bianca

,

no information to disambiguate

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcode   nite determiners

state

mountain in colorado

the

n p/n

 f.   x.f (x)

  
  
  
n

 x.mountain(x) ^ in(x, co)

>

n p

   x.mountain(x) ^ in(x, co)

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4inde   nite determiners

state

mountains

inde   nite determiners are  
select any entity from a 
set without a preference

a : (e ! t) ! e

state with a mountain
 x.state(x) ^ in(ay.mountain(y), x)

[steedman 2011; artzi and zettlemoyer 2013b]

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoinde   nite determiners

state

mountains

inde   nite determiners are  
select any entity from a 
set without a preference

a : (e ! t) ! e

state with a mountain
 x.state(x) ^ in(ay.mountain(y), x)

 x.state(x) ^ 9y.mountain(y) ^ in(y, x)

m
exists

[steedman 2011; artzi and zettlemoyer 2013b]

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoinde   nite determiners

state

with

a

mountain

n

p p/n p

n p/n

n

 x.state(x)  x. y.in(x, y)  f.ax.f (x)  x.mountain(x)

>

n p

ax.mountain(x)
p p

 y.(ax.mountain(x), y)

 f. y.f (y) ^ (ax.mountain(x), y)

n\n

n

 y.state(y) ^ (ax.mountain(x), y)

>

<

inde   nite determiners

a

p p\(p p/n p )/n

 f. g. y.9x.g(x, y) ^ f (x)

a

s\n p\(s\n p/n p )/n
 f. g. y.9x.g(x, y) ^ f (x)

a

s\(s\n p )/n

 f. g. y.9x.g(x, y) ^ f (x)

a

n p/n

 f.ax.f (x)

using the inde   nite quanti   er simpli   es id35 

handling of the inde   nite determiner

superlatives

state

superlatives select optimal 
entities according to a measure

the largest state
argmax( x.state(x),  y.pop(y))
... according to 
min or max
this measure

... over this 

set

{

wa,

al

,
,
...
}

ak

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4al3.9ak0.4seattle2.7san francisco4.1ny17.5il11.4superlatives

state

superlatives select optimal 
entities according to a measure

the largest state
argmax( x.state(x),  y.pop(y))
... according to 
min or max
this measure

... over this 

set

ca

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4al3.9ak0.4seattle2.7san francisco4.1ny17.5il11.4state

superlatives

the largest

n p/n

 f.argmax( x.f (x),  y.pop(y))  x.state(x)
>

argmax( x.state(x),  y.pop(y))

n p

state
n

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4superlatives

state

the most
n p/n/n

populated

n

state
n

 g. f.argmax( x.f (x),  y.g(y))  x.pop(x)  x.state(x)

n p/n

 f.argmax( x.f (x),  y.pop(y))

>

argmax( x.state(x),  y.pop(y))

n p

>

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4representing questions
mountains

border

state

which mountains are in arizona?

represent questions as 
the queries that generate 
their answers

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazrepresenting questions
mountains

border

state

which mountains are in arizona?

select name from mountains

where state == az

represent questions as 
the queries that generate 
their answers

re   ects the query sql

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazrepresenting questions
mountains

border

state

which mountains are in arizona?

 x.mountain(x) ^ in(x, az)

represent questions as 
the queries that generate 
their answers

re   ects the query sql

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazrepresenting questions
mountains

border

state

how many states border california?
count( x.state(x) ^ border(x, ca))

represent questions as 
the queries that generate 
their answers

re   ects the query sql

namestatebiancacoanterocorainierwashastacawrangelaksillcabonaakelbertcoabbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazdb queries

    refer to entities in a database!
    query over type of entities, order and other 

database properties

    how does this approach hold for physical 

objects?!

    what do we need to change? add?

so far

next

referring to real world 

objects

[matuszek et al. 2012a]

referring to real world 

objects

all the arches except the green arch

referring to real world 

objects

all the arches except the green arch

referring to real world 

objects

the blue triangle and the green arch

referring to real world 

objects

the blue triangle and the green arch

plurality

arches
 x.arch(x)

{

,

,

,

}

plurality

arches
 x.arch(x)

{

,

the arches
   x.arch(x)

,

,

}

plurality

blue blocks
 x.blue(x) ^ block(x)
{

}

,

brown block
 x.brown(x) ^ block(x)
{ }

plurality

    all entities are sets!
    space of entities includes 

singletons and sets of 
multiple objects

plurality

    all entities are sets!
    space of entities includes 

singletons and sets of 
multiple objects

cognitive evidence 

for sets being a 
primitive type

[scontras et al. 2012]

plurality

plurality is a modi   er and 
entities are de   ned to be 
sets.

plurality

plurality is a modi   er and 
entities are de   ned to be 
sets.

arch
 x.arch(x) ^ sg(x)

plurality

plurality is a modi   er and 
entities are de   ned to be 
sets.

arch
 x.arch(x) ^ sg(x)

{
,
{ }{ },
{ },

}
{ }

plurality

plurality is a modi   er and 
entities are de   ned to be 
sets.

arches
 x.arch(x) ^ plu(x)

{{

,

,

{

,

,
,
}
,...}
}

plurality and determiners

de   nite determiner must 
select a single set. e.g., 
heuristically select the 
maximal set.

the arches
   x.arch(x) ^ plu(x)
{
{

,

,

,

}
}

adjectives

adjectives are conjunctive 
modi   ers

blue objects
 x.blue(x) ^ obj(x) ^ plu(x)

adjectives

adjectives are conjunctive 
modi   ers

blue objects
 x.blue(x) ^ obj(x) ^ plu(x)

{{

,

}}

dbs and physical objects

    describe and refer to entities!
    ask about objects and relations between 
    next: move into more dynamic scenarios

them!

states

borders

abbr.capitalpop.almontgomery3.9akjuneau0.4azphoenix2.7waolympia4.1nyalbany17.5ilspring   eld11.4state1state2waorwaidcaorcanvcaazbeyond queries

noun phrases

nouns

prepositional phrases!
adjectives

speci   c entities

sets of entities

constrain sets

questions

queries to generate response

beyond queries

noun phrases

nouns

prepositional phrases!
adjectives

speci   c entities

sets of entities

constrain sets

questions

queries to generate response

works well for id139 for dbs

how can we use this approach for other domains?

procedural representations

instructional language!

    common approach to represent 
    natural for executing commands

go forward along the stone hall to the 
intersection with a bare concrete hall

v erif y(f ront : grav el hall)
t ravel()
v erif y(side : con cret e hall)

[chen and mooney 2011]

procedural representations

instructional language!

    common approach to represent 
    natural for executing commands

leave the room and go right

do seq(verif y(room(current loc)),

move to(unique thing( x.equals(distance(x), 1))),
move to(right loc))

[matuszek et al. 2012b]

procedural representations

instructional language!

    common approach to represent 
    natural for executing commands

click start, point to search, and the click for files and 
folders. in the search for box, type    msdownld.tmp   .

lef t click(start)
lef t click(search)
. . .
t y p e in f o(search for:,    msdownld.tmp   )

[branavan et al. 2009, branavan et al. 2010]

procedural representations

dissonance between structure of 

semantics and language

    poor generalization of learned models!
    dif   cult to capture complex language

spatial and instructional language

name objects

noun phrases

nouns

prepositional phrases!
adjectives

speci   c entities

sets of entities

constrain sets

instructions to execute 
verbs

davidsonian events

imperatives

sets of events

modeling instructions

describing an 
environment

executing 
instructions

[artzi and zettlemoyer 2013b]

modeling instructions

describing an 
environment

executing 
instructions

agent

modeling instructions

describing an 
environment

executing 
instructions

agent

    model actions and imperatives!
    consider how the state of the agent in   uences its 

understanding of language

modeling instructions

place your back against the 
wall of the t intersection!

turn left!

go forward along the pink 
   owered carpet hall two 
segments to the 
intersection with the brick 
hall

instructional environment 

    maps are graphs of 
connected positions!

    positions have properties and 

contain objects

1234512345instructional environment 

agent

    agent can move forward, 

turn right and turn left!

    agent perceives clusters of 

positions !

    clusters capture objects

1234512345instructional environment 

    agent can move forward, 

turn right and turn left!

    agent perceives clusters of 

positions !

    clusters capture objects

1234512345instructional environment 

    agent can move forward, 

turn right and turn left!

    agent perceives clusters of 

positions !

    clusters capture objects

1234512345instructional environment 

    agent can move forward, 

turn right and turn left!

    agent perceives clusters of 

positions !

    clusters capture objects

1234512345instructional environment 

    agent can move forward, 

turn right and turn left!

    agent perceives clusters of 

positions !

    clusters capture objects

1234512345instructional environment 

    refer to objects similarly to 

our previous domains!

       query    the world

1234512345grounded resolution of 

determiners

nouns denote sets of 
objects

chair

 x.chair(x)

{ }

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

the chair
   x.chair(x)

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

the chair
   x.chair(x)

    : (e ! t) ! e
{ }

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

the chair
   x.chair(x)

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

the chair
   x.chair(x)

fail?

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

the chair
   x.chair(x)

must disambiguate to 
select a single entity

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

agent

the chair
   x.chair(x)

de   nite determiner 
depends on agent state

1234512345grounded resolution of 

determiners

de   nite determiner 
selects a single entity

agent

the chair
   x.chair(x)

de   nite determiner 
depends on agent state

},

{

1234512345modeling instructions

events taking 
place in the 

world

events refer to 
environment

implicit 
requests

modeling instructions

events taking 
place in the 

world

events refer to 
environment

implicit 
requests

walk forward twice

modeling instructions

events taking 
place in the 

world

events refer to 
environment

implicit 
requests

move twice to the chair

modeling instructions

events taking 
place in the 

world

events refer to 
environment

implicit 
requests

need to 
move    rst

at the chair, turn right

davidsonian event semantics

adverbial modi   ers!

    actions in the world are constrained by 
    the number of such modi   ers is    exible

adverbial modi   cation is thus seen to be logically on a par 
with adjectival modi   cation: what adverbial clauses modify is 

not verbs, but the events that certain verbs introduce.

davidson 1969 (quoted in maienborn et al. 2010)

[davidson 1967]

davidsonian event semantics

    use event variable to represent events !
    verbs describe events like nouns describe entities!
    adverbials are conjunctive modi   ers

vincent shot marvin in the car accidentally

9a.shot(a, v in cen t, m arv in )^
in(a,    x.car(x)) ^   intentional(a)

[davidson 1967]

neo-davidsonian event 

semantics
vincent shot marvin

9a.shot(a, v in cen t, m arv in )

active

[parsons 1990]

neo-davidsonian event 

semantics
vincent shot marvin

active

9a.shot(a, v in cen t, m arv in )

passive

marvin was shot by vincent

[parsons 1990]

neo-davidsonian event 

semantics
vincent shot marvin

active

9a.shot(a, v in cen t, m arv in )

passive

marvin was shot (by vincent)

agent 

optional in 

passive

[parsons 1990]

neo-davidsonian event 

semantics
vincent shot marvin

active

9a.shot(a, v in cen t, m arv in )

passive

marvin was shot (by vincent)

9a.shot(a, m arv in )

agent 

optional in 

passive

[parsons 1990]

neo-davidsonian event 

semantics
vincent shot marvin

active

9a.shot(a, v in cen t, m arv in )

passive

marvin was shot (by vincent)

9a.shot(a, m arv in )

agent 

optional in 

passive

can we represent such distinctions without 

requiring different arity predicates?

[parsons 1990]

neo-davidsonian event 

semantics

    separation between semantic and syntactic roles!
    thematic roles captured by conjunctive predicates

vincent shot marvin

9a.shot(a, v in cen t, m arv in )

9a.shot(a) ^ agent(a, v in cen t ) ^ patient(a, m arv in )

[parsons 1990]

neo-davidsonian event 

semantics

vincent shot marvin in the car accidentally

9a.shot(a) ^ agent(a, v in cen t )^
patient(a, m arv in ) ^ in(a,    x.car(x)) ^   intentional(a)

    decomposition to conjunctive modi   ers 
makes incremental interpretation simpler!
    shallow semantic structures: no need to 

modify deeply embedded variables

[parsons 1990]

neo-davidsonian event 

semantics

9a.shot(a) ^ agent(a, v in cen t )^
patient(a, m arv in ) ^ in(a,    x.car(x)) ^   intentional(a)
without events:

shot(v in cen t, m arv in,    x.car(x), in t en t ion al)

    decomposition to conjunctive modi   ers 
makes incremental interpretation simpler!
    shallow semantic structures: no need to 

modify deeply embedded variables

[parsons 1990]

representing imperatives

move forward past the sofa to the chair

representing imperatives

move forward past the sofa to the chair

representing imperatives

move forward past the sofa to the chair

type

direction

intermediate 

position

final position

representing imperatives

move forward past the sofa to the chair

type

direction

intermediate 

position

final position

    imperatives de   ne actions to be executed!
    constrained by adverbials!
    similar to how nouns are de   ned

representing imperatives

move forward past the sofa to the chair

type

direction

intermediate 

position

final position

    imperatives are sets of actions!
    just like nouns: functions from events to truth

f : ev ! t

representing imperatives

move forward past the sofa to the chair

type

direction

intermediate 

position

final position

given a set, what do we actually execute?

representing imperatives

move forward past the sofa to the chair

type

direction

intermediate 

position

final position

given a set, what do we actually execute?
    need to select a single action and execute it!
    reasonable solution: select simplest/shortest

modeling instructions

    imperatives are sets of 
events!
    events are sequences of 
identical actions

move
 a.move(a)

{

,

,

}

1234512345modeling instructions

    imperatives are sets of 
events!
    events are sequences of 
identical actions

move
 a.move(a)

{

,

,

}

disambiguate by preferring 

shorter sequences

1234512345modeling instructions

events can be modi   ed 
by adverbials

move twice
 a.move(a) ^ len(a, 2)

{ }

1234512345modeling instructions

events can be modi   ed 
by adverbials

go to the chair
 a.move(a)^
to(a,    x.chair(x))

{ }

1234512345modeling instructions

go
s

to

the

chair

ap/n p

n p/n

n

 a.move(a)  x. a.to(a, x)  f.   x.f (x)  x.chair(x)
>

n p

   x.chair(x)

ap

 a.to(a,    x.chair(x))

s\s

 f. a.f (a) ^ to(a,    x.chair(x))

s

 a.move(a) ^ to(a,    x.chair(x))

<

<

treatment of events and their adverbials is similar 

to nouns and prepositional phrases

modeling instructions

dynamic models

implicit actions

1234512345dynamic models

world model changes 
during execution

move until you reach the chair
 a.move(a)^
post(a, intersect(   x.chair(x), you))

1234512345dynamic models

world model changes 
during execution

move until you reach the chair
 a.move(a)^
post(a, intersect(   x.chair(x), you))

123451234543dynamic models

world model changes 
during execution

move until you reach the chair
 a.move(a)^
post(a, intersect(   x.chair(x), you))

e r 
v
e
n
i n t e r s e

c ts

123451234543dynamic models

world model changes 
during execution

move until you reach the chair
 a.move(a)^
post(a, intersect(   x.chair(x), you))

update model to re   ect state change

1234512345dynamic models

world model changes 
during execution

move until you reach the chair
 a.move(a)^
post(a, intersect(   x.chair(x), you))

update

update model to re   ect state change

123451234523implicit actions

consider action assignments 
with pre   xed implicit actions

at the chair, turn left
 a.turn(a) ^ dir(a, lef t)^
pre(a, intersect(   x.chair(x), you))

1234512345implicit actions

consider action assignments 
with pre   xed implicit actions

at the chair, turn left
 a.turn(a) ^ dir(a, lef t)^
pre(a, intersect(   x.chair(x), you))

1234512345implicit actions

consider action assignments 
with pre   xed implicit actions

at the chair, turn left
 a.turn(a) ^ dir(a, lef t)^
pre(a, intersect(   x.chair(x), you))

implicit actions

1234512345experiments

instruction:
at the chair, move forward three steps past the sofa
demonstration:

    situated learning with joint id136!
    two forms of validation!
    template-based

gen lex(x,v;    ,    )

[artzi and zettlemoyer 2013b]

results 

sail corpus - cross validation

chen and mooney 2011
chen 2012
kim and mooney 2012
final state validation
trace validation
kim and mooney 2013

single sentence

sequence

[artzi and zettlemoyer 2013b]

70

56

42

28

14

0

31.9365.2830.964.25more reading about 

modeling

type-logical semantics!

by bob carpenter

[carpenter 1997]

today

parsing

id35s

learning

uni   ed learning algorithm

modeling

best practices for semantics design

looking forward

looking forward: scale

challenges

answer any question 
goal
the    nal
a new variable:  p x y.p(x, y).
posed to large, community 
semantics for a lexical entry is then constructed
authored databases !
by substituting rd for p, or more formally, by a
function application sem(rd). the event space
- large domains!
for syn consists of all syntactic categories in
ubl   s output lexicon, and w ranges over r.
- scalable algorithms!
- unseen words and 

lextender   s model for sem and syn are
na    ve bayes classi   ers (nbc), with features for
the part-of-speech for rt (taken from a pos tag-
ger), the suf   x of rt , the number of arguments of
rd, and the argument types of rd. for syn, we
see
add a feature for the predicted value of sem. for
w , we use a id75 model whose fea-
tures are the score from matcher, the probabili-
ties from the syn and sem nbc models, and the
average weight of all lexical entries in ubl with
matching syntax and semantics. using the pre-

cai and yates 2013a, 2013b

concepts!

examples

1. what are the neighborhoods in new

york city?
 x . neighborhoods(new york, x)
2. how many countries use the rupee?

count(x) . countries used(rupee, x)

3. how many peabody award winners are

there?
count(x) . 9y . award honor(y) ^

award winner(y, x) ^
award(y, peabody award)

figure 2: example questions with their logical
forms. the logical forms make use of freebase
symbols as logical constants, as well as a few ad-
ditional symbols such as count and argmin, to

tao lei, fan long, regina barzilay, and martin rinard
computer science and arti   cial intelligence laboratory

looking forward: code

{taolei, fanl, regina, rinard}@csail.mit.edu

massachusetts institute of technology

goal

challenges

see

abstract

computer science and arti   cial intelligence laboratory

id157 from natural language

nate kushman regina barzilay

program using natural 
language !
- data !
- complex intent!
- complex output!
kushman and barzilay 
2013; lei et al. 2013

we present a method for automatically
generating input parsers from english
speci   cations of input    le formats. we
use a bayesian generative model to cap-
using semantic uni   cation to generate
ture relevant natural language phenomena
and translate the english speci   cation into
a speci   cation tree, which is then trans-
lated into a c++ input parser. we model
the problem as a joint dependency pars-
ing and id14 task. our
method is based on two sources of infor-
mation:
(1) the correlation between the
massachusetts institute of technology
text and the speci   cation tree and (2) noisy
supervision as determined by the success
of the generated c++ parser in reading in-
put examples. our results show that our
approach achieves 80.0% f-score accu-
racy compared to an f-score of 66.7%
produced by a state-of-the-art semantic
parser on a dataset of input format speci-
   cations from the acm international col-
legiate programming contest (which were
written in english for humans with no in-
tention of providing support for automated
processing).1

{nkushman, regina}@csail.mit.edu

we consider the problem of translating natu-
ral language text queries into regular expres-
sions which represent their meaning. the mis-
match in the level of abstraction between the
natural language representation and the regu-

abstract

text description
three letter word starting with    x   

figure 1: an example of (a) one natural language
speci   cation describing program input data; (b)
the corresponding speci   cation tree representing
the program input structure; and (c) two input ex-
amples

regular expression
\bx[a-za-z]{2}\b

figure 1: an example text description and its associated
regular expression.3

however, researchers have had success address-
ing speci   c aspects of this problem. recent ad-
vances in this area include the successful transla-

ducing such an alignment during learning is partic-

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:9)(cid:4)(cid:10)(cid:5)(cid:6)(cid:11)(cid:12)(cid:3)(cid:4)(cid:5)(cid:6)(cid:1)(cid:3)(cid:11)(cid:3)(cid:13)(cid:4)(cid:14)(cid:1)(cid:3)(cid:10)(cid:1)(cid:4)(cid:15)(cid:9)(cid:10)(cid:3)(cid:10)(cid:9)(cid:6)(cid:4)(cid:5)(cid:6)(cid:1)(cid:3)(cid:11)(cid:3)(cid:13)(cid:4)(cid:16)(cid:1)(cid:2)(cid:3)(cid:4)(cid:6)(cid:3)(cid:17)(cid:1)(cid:4)(cid:16)(cid:4)(cid:12)(cid:5)(cid:6)(cid:3)(cid:10)(cid:16)(cid:4)(cid:15)(cid:2)(cid:9)(cid:13)(cid:9)(cid:15)(cid:1)(cid:3)(cid:13)(cid:10)(cid:14)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:4)(cid:15)(cid:18)(cid:6)(cid:1)(cid:9)(cid:5)(cid:6)(cid:10)(cid:4)(cid:9)(cid:4)(cid:10)(cid:5)(cid:6)(cid:11)(cid:12)(cid:3)(cid:4)(cid:5)(cid:6)(cid:1)(cid:3)(cid:11)(cid:3)(cid:13)(cid:4)(cid:14)(cid:4)(cid:1)(cid:2)(cid:9)(cid:1)(cid:4)(cid:5)(cid:6)(cid:19)(cid:5)(cid:15)(cid:9)(cid:1)(cid:3)(cid:10)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:6)(cid:8)(cid:20)(cid:21)(cid:3)(cid:13)(cid:4)(cid:18)(cid:22)(cid:4)(cid:1)(cid:3)(cid:10)(cid:1)(cid:4)(cid:15)(cid:9)(cid:10)(cid:3)(cid:10)(cid:23)(cid:4)(cid:14)(cid:2)(cid:3)(cid:6)(cid:4)(cid:22)(cid:18)(cid:12)(cid:12)(cid:18)(cid:24)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:14)(cid:4)(cid:15)(cid:9)(cid:10)(cid:3)(cid:10)(cid:23)(cid:4)(cid:25)(cid:9)(cid:15)(cid:2)(cid:4)(cid:1)(cid:3)(cid:10)(cid:1)(cid:4)(cid:15)(cid:9)(cid:10)(cid:3)(cid:4)(cid:21)(cid:3)(cid:11)(cid:5)(cid:6)(cid:10)(cid:4)(cid:24)(cid:5)(cid:1)(cid:2)(cid:4)(cid:9)(cid:4)(cid:12)(cid:5)(cid:6)(cid:3)(cid:4)(cid:15)(cid:18)(cid:6)(cid:1)(cid:9)(cid:5)(cid:6)(cid:10)(cid:4)(cid:9)(cid:6)(cid:4)(cid:5)(cid:6)(cid:1)(cid:3)(cid:11)(cid:3)(cid:13)(cid:4)(cid:16)(cid:26)(cid:4)(cid:13)(cid:3)(cid:7)(cid:13)(cid:3)(cid:10)(cid:3)(cid:6)(cid:1)(cid:5)(cid:6)(cid:11)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:10)(cid:5)(cid:27)(cid:3)(cid:4)(cid:18)(cid:22)(cid:4)(cid:24)(cid:9)(cid:12)(cid:12)(cid:23)(cid:4)(cid:14)(cid:2)(cid:3)(cid:4)(cid:6)(cid:3)(cid:17)(cid:1)(cid:4)(cid:16)(cid:4)(cid:12)(cid:5)(cid:6)(cid:3)(cid:10)(cid:4)(cid:13)(cid:3)(cid:7)(cid:13)(cid:3)(cid:10)(cid:3)(cid:6)(cid:1)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:18)(cid:13)(cid:5)(cid:11)(cid:5)(cid:6)(cid:9)(cid:12)(cid:4)(cid:24)(cid:9)(cid:12)(cid:12)(cid:23)(cid:4)(cid:25)(cid:9)(cid:15)(cid:2)(cid:4)(cid:12)(cid:5)(cid:6)(cid:3)(cid:4)(cid:15)(cid:18)(cid:6)(cid:1)(cid:9)(cid:5)(cid:6)(cid:10)(cid:4)(cid:16)(cid:4)(cid:15)(cid:2)(cid:9)(cid:13)(cid:9)(cid:15)(cid:1)(cid:3)(cid:13)(cid:10)(cid:23)(cid:4)(cid:14)(cid:2)(cid:3)(cid:4)(cid:28)(cid:29)(cid:1)(cid:2)(cid:4)(cid:15)(cid:2)(cid:9)(cid:13)(cid:9)(cid:15)(cid:1)(cid:3)(cid:13)(cid:4)(cid:18)(cid:22)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:29)(cid:1)(cid:2)(cid:4)(cid:12)(cid:5)(cid:6)(cid:3)(cid:4)(cid:30)(cid:11)(cid:8)(cid:13)(cid:3)(cid:10)(cid:4)(cid:18)(cid:8)(cid:1)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:15)(cid:18)(cid:12)(cid:18)(cid:13)(cid:4)(cid:23)(cid:23)(cid:23)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:6)(cid:11)(cid:12)(cid:13)(cid:11)(cid:2)(cid:8)(cid:12)(cid:14)(cid:15)(cid:16)(cid:1)(cid:17)(cid:3)(cid:4)(cid:9)(cid:10)(cid:6)(cid:11)(cid:12)(cid:13)(cid:11)(cid:2)(cid:8)(cid:12)(cid:14)(cid:15)(cid:4)(cid:5)(cid:18)(cid:6)(cid:6)(cid:16)(cid:1)(cid:11)(cid:3)(cid:4)(cid:5)(cid:19)(cid:14)(cid:4)(cid:20)(cid:18)(cid:14)(cid:21)(cid:18)(cid:2)(cid:22)(cid:4)(cid:23)(cid:15)(cid:10)(cid:24)(cid:8)(cid:4)(cid:25)(cid:7)(cid:2)(cid:22)(cid:10)(cid:26)(cid:6)(cid:27)(cid:16)110yywyywwwwwywwwywwwwwyywyywwwww...wwwwwwwwww21y5ywyww...wwyyylooking forward: context

goal

challenges

see

understanding how 
sentence meaning varies 
with context !
- data!
- linguistics: co-ref, 

ellipsis, etc. !

miller et al. 1996; 
zettlemoyer and collins 
2009;  artzi and 
zettlemoyer 2013 

example #1:
(a) show me the    ights from boston to philly

 x.f light(x) ^ f rom(x, bos) ^ to(x, phi)
(b) show me the ones that leave in the morning
 x.f light(x) ^ f rom(x, bos) ^ to(x, phi)

^ during(x, morning)

(c) what kind of plane is used on these    ights

 y.9x.f light(x) ^ f rom(x, bos) ^ to(x, phi)
^ during(x, morning) ^ aircraf t(x) = y

example #2:
(a) show me    ights from milwaukee to orlando
 x.f light(x) ^ f rom(x, mil) ^ to(x, orl)
argmin( x.f light(x) ^ f rom(x, mil) ^ to(x, orl),

(b) cheapest

 y.f are(y))

(c) departing wednesday after 5 o   clock

argmin( x.f light(x) ^ f rom(x, mil) ^ to(x, orl)
^ day(x, wed) ^ depart(x) > 1700 ,

 y.f are(y))

example #3:
(a) show me    ights from pittsburgh to la thursday evening

(b) thursday afternoon

 x.f light(x) ^ f rom(x, pit) ^ to(x, la)
^ day(x, thur) ^ during(x, evening)
 x.f light(x) ^ f rom(x, pit) ^ to(x, la)

toward information theoretic human-robot dialog

a joint model of language and perception for grounded attribute learning

stefanie tellex1   , pratiksha thaker1   , robin deits   , dimitar simeonov   , thomas kollar  , nicholas roy   

looking forward: sensors

   mit computer science and arti   cial intelligence laboratory, cambridge, ma

email: {ste   e10, prthaker, mitko, nickroy}@csail.mit.edu

   battelle memorial institute, columbus, ohio

email: robin.deits@gmail.com

we evaluate this approach on data gathered on ama-
zon mechanical turk, in which people describe sets of
objects on a table. experiments demonstrate that the
joint learning approach can e   ectively extend the set
of grounded concepts in an incomplete model initial-
ized with supervised training on a small dataset. this
provides a simple mechanism for learning vocabulary
in a physical environment.

  carnegie mellon university, pittsburgh, pa

email: tkollar@cmu.edu

toward information theoretic human-robot dialog

stefanie tellex1   , pratiksha thaker1   , robin deits   , dimitar simeonov   , thomas kollar  , nicholas roy   

   mit computer science and arti   cial intelligence laboratory, cambridge, ma

email: {ste   e10, prthaker, mitko, nickroy}@csail.mit.edu

   battelle memorial institute, columbus, ohio

(a)

email: robin.deits@gmail.com

  carnegie mellon university, pittsburgh, pa

figure 1. an example of an rgb-d object identi   cation
move the pallet from the truck.
scene. columns on the right show example segments, iden-
ti   ed as positive (far right) and negative (center).
remove the pallet from the back of the truck.

email: tkollar@cmu.edu

challenges

goal

see

abstract   our goal

is to build robots that can robustly
interact with humans using natural language. this problem is
challenging because human language is    lled with ambiguity,
and furthermore, due to limitations in sensing, the robot   s
perception of its environment might be much more limited than
that of its human partner. to enable a robot to recover from a
failure to understand a natural language utterance, this paper
describes an information-theoretic strategy for asking targeted
clarifying questions and using information from the answer to
disambiguate the language. to identify good questions, we derive
an estimate of the robot   s uncertainty about the mapping between
speci   c phrases in the language and aspects of the external
world. this metric enables the robot to ask a targeted question
about the parts of the language for which it is most uncertain.
after receiving an answer, the robot fuses information from the
command, the question, and the answer in a joint probabilistic
graphical model in the g3 framework. when using answers to
questions, we show the robot is able to infer mappings between
parts of the language and concrete object groundings in the
external world with higher accuracy than by using information
from the command alone. furthermore, we demonstrate that by
effectively selecting which questions to ask, the robot is able to
achieve signi   cant performance gains while asking many fewer
questions than baseline metrics.

integrate id29 
with rich sensing on real 
robots!
- data !
- managing uncertainty!
- interactive learning !
matuszek et al. 2012; tellex 
et al. 2013; krishnamurthy 
and kollar 2013

is to build robots that can robustly
interact with humans using natural language. this problem is
challenging because human language is    lled with ambiguity,
and furthermore, due to limitations in sensing, the robot   s
perception of its environment might be much more limited than
that of its human partner. to enable a robot to recover from a
failure to understand a natural language utterance, this paper
describes an information-theoretic strategy for asking targeted
clarifying questions and using information from the answer to
disambiguate the language. to identify good questions, we derive
an estimate of the robot   s uncertainty about the mapping between
speci   c phrases in the language and aspects of the external
world. this metric enables the robot to ask a targeted question
about the parts of the language for which it is most uncertain.
after receiving an answer, the robot fuses information from the
command, the question, and the answer in a joint probabilistic

our aim is to make robots that can naturally and    exibly
interact with a human partner via natural language. an espe-
cially challenging aspect of natural language communication is
the use of ambiguous referring expressions that do not map to
a unique object in the external world. for instance, figure 1
shows a robotic forklift in a real-world environment paired
with instructions created by untrained users to manipulate
one of the objects in the scene. these instructions contain
ambiguous phrases such as    the pallet    which could refer
equally well to multiple objects in the environment. even
if the person gives a command that would be unambiguous

abstract   our goal

i. introduction

(b)

offload the metal crate from the truck.
2. overview of the approach
problem we wish to learn a joint language and per-
fig. 1: sample natural language commands collected from
ception model for the object selection task. the goal
untrained users, commanding the forklift to pick up a pallet
is to automatically map a natural language sentence
in (a).
x and a set of scene objects o to the subset g     o
of objects described by x. the left panel of fig. 1
shows an example scene. here, o is the set of objects
language. the robot    rst identi   es the most ambiguous parts
present in this scene. the individual objects o 2 o are
of a command, then asks a targeted question to try to reduce
extracted from the scene via segmentation (the right
its uncertainty about which aspects of the external world
panel of fig. 1 shows example segments). given the
correspond to the language. for example, when faced with
a command such as    pick up the pallet on the truck    in the
sentence x =   here are the yellow ones,    the goal is to
situation shown in figure 1, the robot can infer that because
select the    ve yellow objects for the named set g.
there is only one truck in the scene, but two pallets, the
model components given a sentence and seg-
phrase    the pallets    is the most ambiguous and ask a question
mented scene objects, we learn a distribution p (g |
like,    what do you mean by    the pallet   ?    then it can use

(a)

classi   ers on color and shape features extracted from
object segments recorded using a kinect depth camera.
joint model we combine these language and vision
models in two ways. first, we introduce an explicit
model of alignment between the logical constants in
the logical form z and classi   ers in the set c. this
alignment would, for example, enable us to learn that
the logical constant yellow should be paired with a
classi   er c 2 c that    res on yellow objects.
next, we introduce an execution model that allows
us to determine what scene objects in o would be
selected by a logical expression z, given the classi-
   ers in c. this allows us to, for example, execute
 x.color(x, green)^shape(x, triangle) by testing all of
the objects with the appropriate classi   ers (for green
and triangle), then selecting objects on which both
classi   ers return true. this execution model includes
uncertainty from the semantic parser p (z|x), classi   er
con   dences p (c = true|o), and a deterministic ground-
truth constraint that encodes what objects are actually
intended to be selected. full details are in sec. 5.
model learning we present an approach that
learns the meaning of new words from a dataset d =
{(xi, oi, gi) | i = 1 . . . n}, where each example i con-
tains a sentence xi, the objects oi, and the selected
set gi. this setup is an abstraction of the situa-
tion where a teacher mentions xi while pointing to
the objects gi     oi she describes. as described in
detail in sec. 6, learning proceeds in an online, em-
like fashion by repeatedly estimating expectations over
the latent logical forms zi and the outputs of the clas-
si   ers c 2 c, then using these expectations to update
the parameters of the component models for language
p (z|x) and visual classi   cation p (c|o). to bootstrap
the learning approach, we    rst train a limited language
and perception system in a fully supervised way:
this stage, each example additionally contains labeled

uw spf

open source id29 framework!

http://yoavartzi.com/spf

semantic 
parser

flexible high-order 
logic representation

learning 
algorithms

includes ready-to-run examples

[artzi and zettlemoyer 2013a]

[   n]

supplementary material

function composition
gh   , i =  x.g
fh , i =  y.f
g(a) = ( x.g)(a) = g[x := a]
f (g(a)) = ( y.f )(g[x := a]) =

f [y := g[x := a]]

 x.f (g(a))[a := x] =

 x.f [y := g[x := a]][a := x] =
 x.f [y := g] = (f    g)h   , i

references

artzi, y. and zettlemoyer, l. (2011). id64 semantic parsers from
in proceedings of the conference on empirical methods in

conversations.
natural language processing.

artzi, y. and zettlemoyer, l. (2013a). uw spf: the university of washington

id29 framework.

artzi, y. and zettlemoyer, l. (2013b). weakly supervised learning of semantic
parsers for mapping instructions to actions. transactions of the association
for computational linguistics, 1(1):49   62.

branavan, s., chen, h., zettlemoyer, l., and barzilay, r. (2009). reinforce-
ment learning for mapping instructions to actions. in proceedings of the joint
conference of the association for computational linguistics and the inter-
national joint conference on natural language processing.

branavan, s., zettlemoyer, l., and barzilay, r. (2010). reading between the
lines: learning to map high-level instructions to commands. in proceedings of
the conference of the association for computational linguistics.

cai, q. and yates, a. (2013a). large-scale id29 via schema match-
ing and lexicon extension. in proceedings of the annual meeting of the asso-
ciation for computational linguistics.

cai, q. and yates, a. (2013b). id29 freebase: towards open-domain
id29. in joint conference on lexical and computational seman-
tics: proceedings of the main conference and the shared task: semantic
textual similarity.

carpenter, b. (1997). type-logical semantics. the mit press.

chen, d. and mooney, r. (2011). learning to interpret natural language naviga-
tion instructions from observations. in proceedings of the national conference
on arti   cial intelligence.

church, a. (1932). a set of postulates for the foundation of logic. the annals

of mathematics, 33:346   366.

church, a. (1940). a formulation of the simple theory of types. the journal of

symbolic logic, 5:56   68.

clark, s. and curran, j. (2007). wide-coverage e cient statistical parsing with

id35 and id148. computational linguistics, 33(4):493   552.

clarke, j., goldwasser, d., chang, m., and roth, d. (2010). driving seman-
tic parsing from the world   s response. in proceedings of the conference on
computational natural language learning.

collins, m. (2002). discriminative training methods for id48:
theory and experiments with id88 algorithms. in proceedings of the
conference on empirical methods in natural language processing.

dahl, d. a., bates, m., brown, m., fisher, w., hunicke-smith, k., pallett, d.,
pao, c., rudnicky, a., and shriberg, e. (1994). expanding the scope of the
atis task: the atis-3 corpus. in proceedings of the workshop on human
language technology.

davidson, d. (1967). the logical form of action sentences. essays on actions

and events, pages 105   148.

davidson, d. (1969). the individuation of events. in essays in honor of carl

g. hempel, pages 216   234. springer.

granroth-wilding, m. and steedman, m. (2012). statistical parsing for har-
monic analysis of jazz chord sequences. ann arbor, mi: mpublishing, uni-
versity of michigan library.

joshi, a. k., shanker, k. v., and weir, d. (1990). the convergence of mildly

context-sensitive grammar formalisms. technical report.

kim, j. and mooney, r. j. (2012). unsupervised pid18 induction for grounded
language learning with highly ambiguous supervision. in proceedings of the
conference on empirical methods in natural language processing.

krishnamurthy, j. and kollar, t. (2013). jointly learning to parse and per-
ceive: connecting natural language to the physical world. transactions of
the association for computational linguistics, 1(1):193   206.

kushman, n. and barzilay, r. (2013). using semantic uni   cation to generate
id157 from natural language. in proceedings of the human lan-
guage technology conference of the north american association for compu-
tational linguistics.

kwiatkowski, t., zettlemoyer, l., goldwater, s., and steedman, m. (2010).
inducing probabilistic id35 grammars from logical form with higher-order
uni   cation. in proceedings of the conference on empirical methods in natural
language processing.

kwiatkowski, t., zettlemoyer, l., goldwater, s., and steedman, m. (2011).
lexical generalization in id35 grammar induction for id29.
in proceedings of the conference on empirical methods in natural language
processing.

lei, t., long, f., barzilay, r., and rinard, m. (2013). from natural language
in proceedings of the the annual

speci   cations to program input parsers.
meeting of the association for computational linguistics.

liang, p., bouchard-c  ot  e, a., klein, d., and taskar, b. (2006). an end-to-
in proceedings of the

end discriminative approach to machine translation.
conference of the association of computational linguistics.

liang, p., jordan, m., and klein, d. (2011). learning dependency-based com-
positional semantics. in proceedings of the conference of the association for
computational linguistics.

maienborn, c., von heusinger, k., and portner, p. (2011). semantics: an
international handbook of natural language and meaning. walter de gruyter.

matuszek, c., fitzgerald, n., zettlemoyer, l., bo, l., and fox, d. (2012a).
a joint model of language and perception for grounded attribute learning.
proceedings of the international conference on machine learning.

matuszek, c., herbst, e., zettlemoyer, l. s., and fox, d. (2012b). learning to
parse natural language commands to a robot control system. in proceedings
of the international symposium on experimental robotics.

miller, s., bobrow, r., ingria, r., and schwartz, r. (1994). hidden under-
standing models of natural language. in proceedings of the conference of the
association of computational linguistics.

parsons, t. (1990). events in the semantics of english. the mit press.

scontras, g., gra   , p., and goodman, n. d. (2012). comparing pluralities.

cognition, 123(1):190   197.

singh-miller, n. and collins, m. (2007). trigger-based id38 using
a loss-sensitive id88 algorithm. in ieee international conference on
acoustics, speech and signal processing.

steedman, m. (1996). surface structure and interpretation. the mit press.

steedman, m. (2000). the syntactic process. the mit press.

steedman, m. (2011). taking scope. the mit press.

tang, l. r. and mooney, r. j. (2001). using multiple clause constructors
in inductive logic programming for id29. in proceedings of the
european conference on machine learning.

tellex, s., kollar, t., dickerson, s., walter, m., banerjee, a., teller, s., and
roy, n. (2011). understanding natural language commands for robotic nav-
igation and mobile manipulation. in proceedings of the national conference
on arti   cial intelligence.

tellex, s., thaker, p., joseph, j., and roy, n. (2013). learning perceptually
grounded word meanings from unaligned parallel data. machine learning,
pages 1   17.

wong, y. and mooney, r. (2006). learning for id29 with statisti-
cal machine translation. in proceedings of the human language technology
conference of the north american association for computational linguistics.

zelle, j. and mooney, r. (1996). learning to parse database queries using
inductive logic programming. in proceedings of the national conference on
arti   cial intelligence.

zettlemoyer, l. and collins, m. (2005). learning to map sentences to logical
in

form: structured classi   cation with probabilistic categorial grammars.
proceedings of the conference on uncertainty in arti   cial intelligence.

zettlemoyer, l. and collins, m. (2007). online learning of relaxed id35 gram-
mars for parsing to logical form. in proceedings of the joint conference on
empirical methods in natural language processing and computational nat-
ural language learning.

zettlemoyer, l. and collins, m. (2009). learning context-dependent mappings
from sentences to logical form. in proceedings of the joint conference of the
association for computational linguistics and international joint confer-
ence on natural language processing.

