downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

natural language processing: an introduction
prakash m nadkarni,1 lucila ohno-machado,2 wendy w chapman2

1yale university school of
medicine, new haven,
connecticut, usa
2university of california, san
diego school of medicine,
division of biomedical
informatics, la jolla, california,
usa

correspondence to
dr prakash m nadkarni, yale
center for medical informatics,
300 george st, new haven, ct
06511, usa;
prakash.nadkarni@yale.edu

received 4 july 2011
accepted 6 july 2011

abstract
objectives to provide an overview and tutorial of natural
language processing (nlp) and modern nlp-system
design.
target audience this tutorial targets the medical
informatics generalist who has limited acquaintance with
the principles behind nlp and/or limited knowledge of
the current state of the art.
scope we describe the historical evolution of nlp, and
summarize common nlp sub-problems in this extensive
   eld. we then provide a synopsis of selected highlights
of medical nlp efforts. after providing a brief description
of common machine-learning approaches that are being
used for diverse nlp sub-problems, we discuss how
modern nlp architectures are designed, with a summary
of the apache foundation   s unstructured information
management architecture. we    nally consider possible
future directions for nlp, and re   ect on the possible
impact of ibm watson on the medical    eld.

introduction
this tutorial provides an overview of natural
language processing (nlp) and lays a foundation
for the jamia reader to better appreciate the arti-
cles in this issue.

nlp began in the 1950s as the intersection of
arti   cial intelligence and linguistics. nlp was orig-
inally distinct from text information retrieval (ir),
which employs highly scalable statistics-based
techniques to index and search large volumes of
text ef   ciently: manning et al1 provide an excellent
introduction to ir. with time, however, nlp and
ir have converged somewhat. currently, nlp
borrows from several, very diverse    elds, requiring
today   s nlp researchers and developers to broaden
their mental knowledge-base signi   cantly.

early simplistic approaches, for example, word-
for-word russian-to-english machine translation,2
were defeated by homographsdidentically spelled
words with multiple meaningsdand metaphor,
leading to the apocryphal story of the biblical,    the
spirit is willing, but the    esh is weak    being trans-
lated to    the vodka is agreeable, but the meat is
spoiled.   

chomsky   s 1956 theoretical analysis of language
grammars3 provided an estimate of the problem   s
dif   culty, in   uencing the creation (1963) of backus-
naur form (bnf) notation.4 bnf is used to specify
a    context-free grammar    5 (id18), and is commonly
used to represent programming-language syntax. a
language   s bnf speci   cation is a set of derivation
rules
collectively validate program code
syntactically. (   rules    here are absolute constraints,
not id109    heuristics.) chomsky also
identi   ed still more restrictive    regular     grammars,
the basis of the id1576 used to specify
text-search patterns. regular expression syntax,

that

de   ned by kleene7 (1956), was    rst supported by
ken thompson   s grep utility8 on unix.

lexical-analyzer

subsequently (1970s),

(lexer)
generators and parser generators such as the lex/yacc
combination9 utilized grammars. a lexer transforms
text into tokens; a parser validates a token sequence.
lexer/parser
simplify programming-
language implementation greatly by taking regular-
expression and bnf speci   cations, respectively, as
input, and generating code and lookup tables that
determine lexing/parsing decisions.

generators

(ie,

while id18s are theoretically inadequate for
language,10 they are often employed for
natural
nlp in practice. programming languages are typi-
cally designed deliberately with a restrictive id18
variant, an lalr(1) grammar (lalr, look-ahead
parser with left-to-right processing and rightmost
(bottom-up) derivation),4 to simplify implementa-
tion. an lalr(1) parser scans text left-to-right,
operates
compound
constructs from simpler ones), and uses a look-ahead
of a single token to make parsing decisions.

bottom-up

builds

the prolog language11 was originally invented
(1970) for nlp applications. its syntax is especially
suited for writing grammars, although,
in the
easiest implementation mode (top-down parsing),
rules must be phrased differently (ie, right-recur-
sively12) from those intended for a yacc-style parser.
top-down parsers are easier to implement than
bottom-up parsers (they don   t need generators),
but are much slower.

it

that

the limitations of hand-written rules: the rise of
statistical nlp
natural language   s vastly large size, unrestrictive
nature, and ambiguity led to two problems when
using standard parsing approaches
relied
purely on symbolic, hand-crafted rules:
< nlp must ultimately extract meaning (   seman-
tics   ) from text: formal grammars that specify
relationship between text unitsdparts of speech
such as nouns, verbs, and adjectivesdaddress
syntax primarily. one can extend grammars to
address natural-language semantics by greatly
expanding sub-categorization, with additional
rules/constraints (eg,    eat    applies only to ingest-
ible-item nouns). unfortunately, the rules may
now become unmanageably numerous, often
interacting unpredictably, with more frequent
ambiguous parses
(multiple interpretations of
a word sequence are possible). (punsdambig-
uous parses used for humorous effectdantedate
nlp.)

< handwritten rules handle

   ungrammatical   
spoken prose and (in medical contexts) the
highly telegraphic prose of in-hospital progress
notes very poorly, although such prose is
human-comprehensible.

544

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

the 1980s resulted in a fundamental reorientation, summa-

rized by klein13:
< simple, robust approximations replaced deep analysis.
< evaluation became more rigorous.
< machine-learning methods that used probabilities became
prominent. (chomsky   s book, syntactic structures14 (1959),
had been skeptical about the usefulness of probabilistic
language models).

< large, annotated bodies of text (corpora) were employed to
train machine-learning algorithmsdthe annotation contains
the correct answersdand provided gold standards
for
evaluation.
this reorientation resulted in the birth of statistical nlp. for
example, statistical parsing addresses parsing-rule proliferation
through probabilistic id18s15: individual rules have associated
probabilities, determined through machine-learning on anno-
tated corpora. thus, fewer, broader rules replace numerous
detailed rules, with statistical-frequency information looked up
to disambiguate. other approaches build probabilistic    rules   
from annotated data similar to machine-learning algorithms like
c4.5,16 which build id90 from feature-vector data. in
any case, a statistical parser determines the most likely parse of
a sentence/phrase.
for
example, the stanford statistical parser,17 trained with the penn
treebank18dannotated wall street journal articles, plus tele-
phone-operator conversationsdmay be unreliable for clinical
text. manning and scheutze   s text provides an excellent intro-
duction to statistical nlp.19

is context-dependent:

   most likely   

statistical approaches give good results in practice simply
because, by learning with copious real data, they utilize the
most common cases: the more abundant and representative the
data, the better they get. they also degrade more gracefully with
unfamiliar/erroneous input. this issue   s articles make clear,
however, that handwritten-rule-based and statistical approaches
are complementary.

nlp sub-problems: application to clinical text
we enumerate common sub-problems in nlp: jurafksy and
martin   s text20 provides additional details. the solutions to
some sub-problems have become workable and affordable, if
imperfectdfor example, id133
(desktop operating
systems    accessibility features) and connected-id103
(several commercial systems). others, such as id53,
remain dif   cult.

in the account below, we mention clinical-context issues that
complicate certain sub-problems, citing recent biomedical nlp
work against each where appropriate. (we do not cover the
history of medical nlp, which has been applied rather than
basic/theoretical; spyns21 reviews pre-1996 medical nlp efforts.)

low-level nlp tasks include:

1. sentence boundary detection: abbreviations and titles (   m.g.,   
   dr.   ) complicate this task, as do items in a list or templated
utterances (eg,    mi [x], sob[]   ).

2. id121: identifying individual tokens (word, punctua-
tion) within a sentence. a lexer plays a core role for this task
and the previous one. in biomedical text, tokens often
contain characters typically used as token boundaries, for
example, hyphens, forward slashes (   10 mg/day,       n-acetyl-
cysteine   ).

3. part-of-speech assignment to individual words (   id52   ): in
english, homographs (   set   ) and gerunds (verbs ending in    ing   
that are used as nouns) complicate this task.

4. morphological decomposition of

compound words: many
medical terms, for example,    nasogastric,    need decomposition

sub-task is

lemmatiza-
to comprehend them. a useful
tiondconversion of a word to a root by removing suf   xes.
non-english clinical nlp emphasizes decomposition;
in
highly synthetic languages (eg, german, hungarian), newly
coined compound words may replace entire phrases.22 spell-
checking applications and preparation of text for indexing/
searching (in ir) also employ morphological analysis.

5. id66 (chunking): identifying phrases from constit-
uent part-of-speech tagged tokens. for example, a noun
phrase may comprise an adjective sequence followed by
a noun.

6. problem-speci   c segmentation: segmenting text into meaningful
groups, such as sections, including chief complaint, past
medical history, heent, etc.23
haas24 lists publicly available nlp modules for such tasks:
most modules, with the exception of ctakes (clinical text
analysis and knowledge extraction system),25 have been
developed for non-clinical text and often work less well for
clinical narrative.

higher-level tasks build on low-level tasks and are usually

problem-speci   c. they include:
1. spelling/grammatical error identi   cation and recovery: this task is
mostly interactive because, as word-processing users know, it is
far from perfect. highly synthetic phrases predispose to false
positives (correct words    agged as errors), and incorrectly used
homophones (identically sounding, differently spelled words, eg,
sole/soul, their/there) to false negatives.
2. id39 (ner)26 27: identifying speci   c words
or phrases (   entities   ) and categorizing themdfor example, as
persons, locations, diseases, genes, or medication. an common
ner task is mapping named entities to concepts in a vocabulary. this
task often leverages id66 for candidate entities (eg,
the noun phrase    chest tenderness   ); however, sometimes the
concept is divided across multiple phrases (eg,    chest wall shows
slight tenderness on pressure .   ).

the following issues make ner challenging:

< word/phrase order variation: for example, perforated duodenal

ulcer versus duodenal ulcer, perforated.

< derivation: for example, suf   xes transform one part of speech
(noun) /    mediastinal   

   mediastinum   

(eg,

to another
(adjective)).

< in   ection: for example, changes in number (eg,

   opacity/
   cough(ed)   ), comparative/superlative

opacities)   , tense (eg,
forms (eg,    bigger/biggest)   ).

< synonymy is abundant in biomedicine, for example,
hepatic, addison   s disease/adrenocortical insuf   ciency.

liver/

for example,

< homographs: polysemy refers to homographs with related
to
meanings,
laboratory procedure, or result. homographic
a substance,
abbreviations are increasingly numerous28:
   apc    has 12
expansions, including    activated protein c    and    adenomatous
polyposis coli.   

   direct bilirubin    can refer

3. id51 (wsd)29e31: determining a homo-
graph   s correct meaning.
4. negation and uncertainty identi   cation32e34: inferring whether
a named entity is present or absent, and quantifying that
id136   s uncertainty. around half of all symptoms, diagnoses,
and    ndings in clinical reports are estimated to be negated.35
negation can be explicit, for example,    patient denies chest pain   
or implieddfor example,
   lungs are clear upon auscultation   
implies absence of abnormal
lung sounds. negated/af   rmed
concepts can be expressed with uncertainty (   hedging   ), as in
ill-de   ned density suggests pneumonia.    uncertainty
   the
that represents reasoning processes is hard to capture:
   the

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

545

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

patient probably has a left-sided cerebrovascular accident; post-
convulsive state is less likely.    negation, uncertainty, and
af   rmation form a continuum. uncertainty detection was the
focus of a recent nlp competition.36
5. relationship extraction: determining relationships between
entities or events, such as    treats,       causes,    and    occurs with.   
lookup of problem-speci   c informationdfor example, thesauri,
databasesdfacilitates relationship extraction.

anaphora reference resolution37 is a sub-task that determines
relationships between    hierarchically related    entities: such rela-
tionships include:
< identity: one entitydfor example, a pronoun like    s/he,       hers/
his,    or an abbreviationdrefers to a previously mentioned
named entity;

< part/whole: for example, city within state;
< superset/subset: for example, antibiotic/penicillin.
id136s/relationship extraction38 39: making infer-
6. temporal
ences from temporal expressions and temporal relationsdfor
example, inferring that something has occurred in the past or
may occur in the future, and ordering events within a narrative
(eg, medication x was prescribed after symptoms began).
7. information extraction (ie): the identi   cation of problem-
speci   c information and its transformation into (problem-
speci   c) structured form. tasks 1e6 are often part of the
larger ie task. for example, extracting a patient   s current
diagnoses involves ner, wsd, negation detection, temporal
id136, and anaphoric resolution. numerous modern clinical
ie systems exist,40e44 with some available as open-source.25 44 45
ie and relationship extraction have been themes of several i2b2/
va nlp challenges.46e49 other problem areas include phenotype
characterization,50e52 biosurveillance,53
54 and adverse-drug
reaction recognition.55

the national library of medicine (nlm) provides several
well-known    knowledge infrastructure    resources that apply to
multiple nlp and ir tasks. the umls metathesaurus,56 which
records synonyms and categories of biomedical concepts from
numerous biomedical terminologies, is useful in clinical ner.
the nlm   s specialist lexicon57 is a database of common english
and medical terms that includes part-of-speech and in   ection
data; it is accompanied by a set of nlp tools.58 the nlm also
provides a test collection for word disambiguation.59

some data driven approaches: an overview
statistical and machine learning involve development (or use) of
algorithms that allow a program to infer patterns about example
(   training   ) data, that in turn allows it to    generalize   dmake
predictions about new data. during the learning phase, numer-
ical parameters that characterize a given algorithm   s underlying
model are computed by optimizing a numerical measure,
typically through an iterative process.

in general,

learning can be superviseddeach item in the
training data is labeled with the correct answerdor unsupervised,
where it is not, and the learning process tries to recognize
patterns automatically (as in cluster and factor analysis). one
pitfall in any learning approach is the potential for over-   tting:
the model may    t the example data almost perfectly, but makes
poor predictions for new, previously unseen cases. this is
because it may learn the random noise in the training data rather
than only its essential, desired features. over-   tting risk is
minimized by techniques such as cross-validation, which parti-
tion the example data randomly into training and test sets to
internally validate the model   s predictions. this process of data
partitioning, training, and validation is repeated over several

rounds, and the validation results are then averaged across
rounds.

machine-learning models can be broadly classi   ed as either
generative or discriminative. generative methods seek to create
rich models of id203 distributions, and are so called
because, with such models, one can    generate    synthetic data.
discriminative methods are more utilitarian, directly estimating
posterior probabilities based on observations. srihari60 explains
the difference with an analogy: to identify an unknown
speaker   s language, generative approaches would apply deep
knowledge of numerous languages to perform the match;
discriminative methods would rely on a less knowledge-inten-
sive approach of using differences between languages to    nd the
closest match. compared to generative models, which can
become intractable when many features are used, discriminative
models typically allow use of more features.61 id28
and conditional random    elds (crfs) are examples of discrimi-
native methods, while naive bayes classi   ers and hidden
markov models (id48s) are examples of generative methods.
some common machine-learning methods used in nlp tasks,
and utilized by several articles in this issue, are summarized
below.

support vector machines (id166s)
id166s, a discriminative learning approach, classify inputs (eg,
words) into categories (eg, parts of speech) based on a feature
set. the input may be transformed mathematically using
a    id81    to allow linear separation of the data points
from different categories. that is, in the simplest two-feature
case, a straight line would separate them in an xey plot: in the
general n-feature case, the separator will be an (n 1) hyper-
plane. the commonest id81 used is a gaussian
(the basis of the    normal distribution    in statistics). the sepa-
ration process selects a subset of the training data (the    support
vectors   ddata points closest to the hyperplane) that best
differentiates the categories. the separating hyperplane maxi-
mizes the distance to support vectors from each category
(see    gure 1).

figure 1 support vector machines: a simple 2-d case is illustrated.
the data points, shown as categories a (circles) and b (diamonds), can
be separated by a straight line xey. the algorithm that determines xey
identi   es the data points (   support vectors   ) from each category that are
closest to the other category (a1, a2, a3 and b1, b2, b3) and computes
xey such that the margin that separates the categories on either side is
maximized. in the general n-dimensional case, the separator will be an
(n 1) hyperplane, and the raw data will sometimes need to be
mathematically transformed so that linear separation is achievable.

546

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

a tutorial by hearst et al62 and the dtreg online docu-
mentation63 provide approachable introductions to id166s.
fradkin and muchnik64 provide a more technical overview.

id48 (id48s)
an id48 is a system where a variable can switch (with varying
probabilities) between several states, generating one of several
possible output symbols with each switch (also with varying
probabilities). the sets of possible states and unique symbols
may be large, but    nite and known (see    gure 2). we can observe
the outputs, but the system   s internals (ie, state-switch proba-
bilities and output probabilities) are    hidden.    the problems to
be solved are:
a. id136: given a particular sequence of output symbols,
compute the probabilities of one or more candidate state-
switch sequences.

b. pattern matching:    nd the state-switch sequence most likely to

have generated a particular output-symbol sequence.

c. training:

given examples of output-symbol

sequence
(training) data, compute the state-switch/output probabili-
ties (ie, system internals) that    t this data best.
b and c are actually naive bayesian reasoning extended to
sequences; therefore, id48s use a generative model. to solve
these problems, an id48 uses two simplifying assumptions
(which are true of numerous real-life phenomena):
1. the id203 of switching to a new state (or back to the
same state) depends on the previous n states. in the simplest
      rst-order     case (n  1), this id203 is determined by the

current state alone. (first-order id48s are thus useful to
model events whose likelihood depends on what happened
last.)

2. the id203 of generating a particular output

in

(and

a particular state depends only on that state.
these assumptions allow the id203 of a given state-
switch sequence
observed-output
sequence) to be computed by simple multiplication of the
individual probabilities. several algorithms exist to solve these
problems.65 66 the highly ef   cient viterbi algorithm, which
addresses problem b,    nds applications in signal processing, for
example, cell-phone technology.

corresponding

a

theoretically, id48s could be extended to a multivariate
scenario,67 but the training problem can now become intrac-
table. in practice, multiple-variable applications of id48s (eg,
ner68) use single, arti   cial variables that are uniquely deter-
mined composites of
such
approaches require much more training data.

existing categorical variables:

id48s are widely used for id103, where
a spoken word   s waveform (the output sequence) is matched to
the sequence of individual phonemes (the    states   ) that most
likely produced it. (frederick jelinek, a statistical-nlp advocate
who pioneered id48s at ibm   s id103 group,
reportedly joked,    every time a linguist leaves my group, the
speech recognizer    s performance improves.   20) id48s also
address several bioinformatics problems, for example, multiple
sequence alignment69 and gene prediction.70 eddy71 provides
a lucid bioinformatics-oriented introduction to id48s, while
rabiner72 (id103) provides a more detailed intro-
duction.

commercial id48-based speech-to-text

is now robust
enough to have essentially killed off academic research efforts,
with dictation systems for specialized areasdeg, radiology and
pathologydproviding structured data entry. phrase recognition
is paradoxically more reliable for polysyllabic medical terms than
for ordinary english: few word sequences sound like    angina
pectoris,    while common english has numerous homophones
(eg, two/too/to).

conditional random    elds (crfs)
crfs are a family of discriminative models    rst proposed by
lafferty et al.73 an accessible reference is culotta et al74; sutton
and mccallum75 is more mathematical. the commonest (linear-
chain) crfs resemble id48s in that the next state depends on
the current state (hence the    linear chain    of dependency).

crfs generalize id28 to sequential data in the
same way that id48s generalize naive bayes (see    gure 3).
crfs are used to predict the state variables (   ys   ) based on the
observed variables (   xs   ). for example, when applied to ner, the
state variables are the categories of the named entities: we want
to predict a sequence of named-entity categories within
a passage. the observed variables might be the word itself,
pre   xes/suf   xes, capitalization, embedded numbers, hyphen-
ation, and so on. the linear-chain paradigm    ts ner well: for
example, if the previous entity is    salutation    (eg,    mr/ms   ), the
succeeding entity must be a person.

crfs are better suited to sequential multivariate data than
id48s: the training problem, while requiring more example
data than a univariate id48, is still tractable.

id165s
an    id165   19 is a sequence of n itemsdletters, words, or
phonemes. we know that certain item pairs (or triplets,
quadruplets, etc) are likely to occur much more frequently than

figure 2 id48. the small circles s1, s2 and s3
represent states. boxes o1 and o2 represent output values. (in practical
cases, hundreds of states/output values may occur.) the solid lines/arcs
connecting states represent state switches; the arrow represents the
switch   s direction. (a state may switch back to itself.) each line/arc label
(not shown) is the switch id203, a decimal number. a dashed line/
arc connecting a state to an output value indicates    output id203   :
the id203 of that output value being generated from the particular
state. if a particular switch/output id203 is zero, the line/arc is not
drawn. the sum of the switch probabilities leaving a given state (and the
similar sum of output probabilities) is equal to 1. the sequential or
temporal aspect of an id48 is shown in    gure 3.

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

547

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

figure 3
the relationship between naive bayes, id28,
id48 (id48s) and conditional random    elds (crfs).
id28 is the discriminative-model counterpart of naive
bayes, which is a generative model. id48s and crfs extend naive
bayes and id28, respectively, to sequential data (adapted
from sutton and mccallum73). in the generative models, the arrows
indicate the direction of dependency. thus, for the id48, the state y2
depends on the previous state y1, while the output x1 depends on y1.

others. for example, in english words, u always follows q, and
an initial t is never followed by k (though it may be in
ukrainian). in portuguese, a    is always followed by a vowel
(except e and i). given suf   cient data, we can compute
frequency-distribution data for all id165s occurring in that
data. because the permutations increase dramatically with
ndfor example, english has 26^2 possible letter pairs, 26^3
triplets, and so ondn is restricted to a modest number. google
has computed word id165 data (n#5) from its web data and
from the google books project, and made it available freely.76

id165s are a kind of multi-order markov model: the proba-
bility of a particular item at the nth position depends on the
previous n 1 items, and can be computed from data. once
computed, id165 data can be used for several purposes:
< suggested auto-completion of words and phrases to the user

during search, as seen in google   s own interface.

< id147: a misspelled word in a phrase may be
   agged and a correct spelling suggested based on the correctly
spelled neighboring words, as google does.

< id103: homophones (   two    vs    too   ) can be
disambiguated probabilistically based on correctly recognized
neighboring words.

< word disambiguation:

if we build    word-meaning    id165s
from an annotated corpus where homographs are tagged with
their correct meanings, we can use the non-ambiguous
neighboring words to guess the correct meaning of a homo-
graph in a test document.
id165 data are voluminousdgoogle   s id165 database
requires 28 gbdbut this has become less of an issue as storage
becomes cheap. special data structures, called id165 indexes,
speed up search of such data. id165-based classi   ers leverage
raw training text without explicit linguistic/domain knowledge;
while yielding good performance,
room for
improvement, and are therefore complemented with other
approaches.

they leave

chaining nlp analytical tasks: pipelines
any practical nlp task must perform several sub-tasks. for
example, all of nlp sub-problems section9s low-level tasks must
execute sequentially, before higher-level tasks can commence.
since different algorithms may be used for a given task,
a modular, pipelined system designdthe output of one analytical
module becomes the input to the nextdallows    mixing-and-
matching.    thus, a crf-based pos tagger could be combined
with rule-based medical named-entity recognition. this design
improves system robustness: one could replace one module with
another (possibly superior) module, with minimal changes to
the rest of the system.

this is the intention behind pipelined nlp frameworks, such
as gate77 and ibm (now apache) unstructured information
management architecture (uima).78 uima   s scope goes beyond
nlp: one could integrate structured-format databases, images,
and multi-media, and any arbitrary technology. in uima, each
analytical task transforms (a copy of) its input by adding xml-
based markup and/or reading/writing external data. a task
operates on common analysis structure (cas), which contains
the data (possibly in multiple formats, eg, audio, html),
a schema describing the analysis structure (ie, the details of the
markup/external
results, and links
(indexes) to the portions of the source data that they refer to.
uima does not dictate the design of the analytical tasks
themselves: they interact with the uima pipeline only through
the cas, and can be treated as black boxes: thus, different tasks
could be written in different programming languages.

the analysis

formats),

the schema for a particular cas is developer-de   ned because
it is usually problem-speci   c. (currently, no standard schemas
exist for tasks such as id52, although this may change.)
de   nition is performed using xmi (xml metadata inter-
change),
the uni   ed
modeling language (uml). xmi, however,
is    programmer-
hostile   : it is easier to use a commercial uml tool to design
a uml model visually and then generate xmi from it.79

the xml-interchange equivalent of

in practice, a pure pipeline design may not be optimal for all
solutions. in many cases, a higher-level process needs to provide
feedback to a lower-level process to improve the latter    s accu-
racy. (all supervised machine- learning algorithms, for example,
ultimately rely on feedback.) implementing feedback across
analytical tasks is complicated: it involves modifying the code of
communicating tasksdone outputting data that constitutes the
feedback, the other checking for the existence of such data, and
accepting them if available (see    gure 4). new approaches based
on active learning may help select cases for manual labeling for
construction of training sets.80 81

also, given that no nlp task achieves perfect accuracy, errors
in any one process in a pipeline will propagate to the next, and
so on, with accuracy degrading at each step. this problem,

figure 4 a uima pipeline. an input task is sequentially put through
a series of tasks, with intermediate results at each step and    nal output at
the end. generally, the output of a task is the input of its successor, but
exceptionally, a particular task may provide feedback to a previous one (as
in task 4 providing input to task 1). intermediate results (eg, successive
transformations of the original bus) are read from/written to the cas,
which contains metadata de   ning the formats of the data required at every
step, the intermediate results, and annotations that link to these results.

548

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

however, applies to nlp in general: it would occur even if the
individual tasks were all combined into a single body of code.
one way to address it (adopted in some commercial systems) is
to use alternative algorithms (in multiple or branching pipelines)
and contrast the    nal results obtained. this allows tuning the
output to trade-offs (high precision versus high recall, etc).

a look into the future
recent advances in arti   cial intelligence (eg, computer chess)
have shown that effective approaches utilize the strengths of
electronic circuitrydhigh speed and large memory/disk capacity,
problem-speci   c data-compression techniques and evaluation
functions, highly ef   cient searchdrather than trying to mimic
human neural
function. similarly, statistical-nlp methods
correspond minimally to human thought processes.

by comparison with ir, we now consider what it may take
for multi-purpose nlp technology to become mainstream.
while always important to library science, ir achieved major
prominence with the web, notably after google   s scienti   c and
   nancial success: the limelight also caused a corresponding ir
research and toolset boom. the question is whether nlp has
a similar breakthrough application in the wings. one candidate
is ibm watson, which attracted much attention within the
biomedical informatics community (eg, the acmi discussion
newsgroup and the amia nlp working group discussion list)
after its    jeopardy    performance. watson appears to address the
admittedly hard problem of question-answering successfully.
although the watson effort is impressive in many ways, its
discernible limitations highlight ongoing nlp challenges.

ibm watson: a wait-and-see viewpoint
watson, which employs uima,82
is a system-engineering
triumph, using highly parallel hardware with 2880 cpus+16 tb
ram. all its lookup of reference content (encyclopedias, dictio-
naries, etc) and analytical operations use structures optimized for
in-memory manipulation. (by contrast, most pipelined nlp
architectures on ordinary hardware are disk-i/o-bound.) it inte-
grates several software technologies: ir, nlp, parallel database
search, ontologies, and id99.

a prolog parser extracts key elements such as the relationships
between entities and task-speci   c answers. in a recent public
display, the task was to compete for the fastest correct answer in
a series of questions against two human contestants in the
popular us-based television show,    jeopardy.    during training
with a jeopardy question-databank, nlp is also used to pre-
process online reference text (eg, encyclopedia, dictionaries) into
a structure that provides evidence for candidate answers,
including whether the relationships between entities in the
question match those in the evidence.83 the search, and ranking
of candidate answers, use ir approaches.

a challenge in porting watson   s technology to other domains,
such as medical id53, will be the degree to which
watson   s design is generalizable.
< watson built its lead in the contest with straightforward
direct questions whose answers many of the audience (and
the skilled human contestants) clearly knewdand which
a non-expert human armed with google may have been able
to retrieve using keywords alone (albeit slower). as pointed
out by libresco84 and jennings,85 watson was merely faster
with the buzzerdelectronics beats human reaction time. for
non-game-playing, real-world id53 scenarios,
however, split-second reaction time may not constitute
a competitive advantage.

< for harder questions, watson   s limitations became clearer.
computing the correct response to the question about which
us city (chicago) has two airports, one named after a world
war ii battle (midway), the other after a world war ii hero
(o   hare),
involves three set intersections (eg, the    rst
operation would cross names of airports in us cities against
a list of world war ii battles). watson lacked a higher-level
strategy to answer such complex questions.

< watson   s prolog parser and search, and especially the entire
reference content, were tuned/structured for playing jeop-
ardy, in which the questions and answers are one sentence
long (and the answer is of the form    what/who is/are x?   ).
such an approach runs the risk of    over-   tting    the system to
a particular problem, so that it may require signi   cant effort
to modify it for even a slightly different problem.
ibm recently conducted a medical diagnosis demonstration of
watson, which is reported in an associated press article.86
demonstrations eventually need to be followed by evaluations.
earlier medical diagnosis advice software underwent evaluations
that were rigorous for their time, for example, berner et al87 and
friedman et al,88 and today   s evaluations would need to be even
more stringent. the articles from miller and masarie89 and
miller90 are excellent starting points for learning about the
numerous pitfalls in the automated medical diagnosis domain,
and ibm may rediscover these:
< medico-legal liability: ultimately the provider, not software, is

responsible for the patient.

< reference-content reliability: determining the reliability of a given
unit of evidence is challenging. even some recent recommen-
dations by    authorities    have become tainted (eg, in psychiatry)
with subsequent revelations of undisclosed con   ict of interest.
< the limited role of nlp and unstructured text in medical diagnosis:
it is unclear that accurate medical diagnosis/advice mandates
front-end nlp technology:
structured data entry with
thesaurus/id165 assisted pick-lists or word/phrase comple-
tion might suf   ce. similarly, diagnostic systems have used
structured, curated information rather than unstructured
text for prioritizing diagnoses. even this information requires
tailoring for local prevalence rates, and continual mainte-
nance. unstructured text, in the form of citations, is used
mainly to support the structured information.
to be fair to ibm, nlp technology may conceivably augment
web crawler technologies that search for speci   c information
and alert curators about new information that may require them
to update their database. electronic ie technologies might save
curation time, but given the medico-legal consequences, and the
lack of 100% accuracy, such information would need to be
veri   ed by humans.

from an optimistic perspective, the watson phenomenon may
have the bene   cial side effect of focusing attention not only on
nlp, but also on the need to integrate it effectively with other
technologies.

will nlp software become a commodity?
the post-google interest in ir has led to ir commoditization:
a proliferation of ir tools and incorporation of ir technology
into relational database engines. earlier, statistical packages and,
subsequently, data mining tools also became commoditized.
commodity analytical software is characterized by:
< availability of several tools within a package: the user can
often set up a pipeline without programming using a graph-
ical metaphor.

< high user friendliness and ease of learning: online documen-
tation/tutorials are highly approachable for the non-specialist,

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

549

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

review

focusing on when and how to use a particular tool rather than
its underlying mathematical principles.

< high value in relation to price: some offerings may even be

freeware.
by contrast, nlp toolkits and uima are still oriented toward
the advanced programmer, and commercial offerings are expen-
sive. general purpose nlp is possibly overdue for commoditiza-
tion: if this happens, best-of-breed solutions are more likely to rise
to the top. again, analytics vendors are likely to lead the way,
following the steps of biomedical informatics researchers to devise
innovative solutions to the challenge of processing complex
biomedical language in the diverse settings where it is employed.
funding this work is funded in part by grants from the national institutes of health
(r01lm009520, u54hl108460, and ul1rr031980).

competing interests none.

provenance and peer review commissioned; internally peer reviewed.

references
1. manning c, raghavan p, schuetze h. introduction to information retrieval.

2.

3.

cambridge, uk: cambridge university press, 2008.
hutchins w. the first public demonstration of machine translation: the
georgetown-ibm system, 7th january 1954. 2005. http://www.hutchinsweb.me.uk/
gu-ibm-2005.pdf (accessed 4 jun 2011).
chomsky n. three models for the description of language. ire trans inf theory
1956;2:113e24.

4. aho av, sethi r, ullman jd. compilers: principles, techniques, tools. reading, ma:

5.
6.

7.

8.

9.

10.

11.

addison-wesley, 1988.
chomsky n. on certain formal properties of grammars. inform contr 1959;2:137e67.
friedl jef. mastering id157. sebastopol, ca: o   reilly & associates,
inc., 1997.
kleene sc. representation of events in nerve nets and    nite automata. in: shannon
c, mccarthy j, eds. automata studies. princeton, nj: princeton university press, 1956.
kernighan b, pike r. the unix programming environment. englewood cliffs, nj:
prentice-hall, 1989.
levine jr, mason t, brown d. lex & yacc. sebastopol, ca: o   reilly & associates,
inc., 1992.
joshi a, vijay-shanker k, weir d. the convergence of mildly context-sensitive
grammar formalisms. in: sells p, shieber s, wasow t, eds. foundational issues in
natural language processing. cambridge, ma: mit press, 1991:31e81.
clocksin wf, mellish cs. programming in prolog: using the iso standard. 5th edn.
new york: springer, 2003.

12. warren ds. programming in tabled prolog. 1999. http://www.cs.sunysb.edu/

13.

14.
15.

16.

17.

wwarren/xsbbook/node10.html (accessed 1 jun 2011).
klein d. cs 294e5: statistical natural language processing. 2005. http://www.cs.
berkeley.edu/wklein/cs294-5 (accessed 2 jun 2011).
chomsky n. syntactic structures. the hague, netherlands: mouton and co, 1957.
klein d, manning c. accurate unlexicalized parsing. proceedings of the 41st meeting
of the association for computational linguistics; 2003. 2003:423e30. http://nlp.
stanford.edu/wmanning/papers/unlexicalized-parsing.pdf (accessed 20 jul 2011).
quinlan jr. c4.5: programs for machine learning. san mateo, ca: morgan
kaufmann publishers, 1993.
klein d, manning c. stanford statistical parser. 2003. http://nlp.stanford.edu/
software/lex-parser.shtml (accessed 4 jun 2011).

18. university of pennsylvania. id32 project. 2011. http://www.cis.upenn.

edu/wtreebank/ (accessed 21 may 2011).

19. manning c, schuetze h. foundations of statistical natural language processing.

20.

21.

cambridge, ma: mit press, 1999.
jurafsky d, martin jh. speech and language processing. 2nd edn. englewood
cliffs, nj: prentice-hall, 2008.
spyns p. natural language processing in medicine: an overview. methods inf med
1996;5:285e301.

22. deleger l, namer f, zweigenbaum p. morphoid29 of medical

compound words: transferring a french analyzer to english. int j med inform 2009;78
(suppl 1):s48e55.

28.

29.

30.

liu h, aronson a, friedman c. a study of abbreviations in medline abstracts. proc
amia symp 2002:464e8.
rind   esch tc, aronson ar. ambiguity resolution while mapping free text to
the umls metathesaurus. proc annu symp comput appl med care 1994:
240e4.
pedersen t. free nlp software. 2011. http://www.d.umn.edu/wtpederse/code.
html (accessed 3 jun 2011).

31. weeber m, mork jg, aronson ar. developing a test collection for biomedical word

32.

sense disambiguation. proc amia symp 2001:746e50.
chapman w, bridewell w, hanbury p, et al. a simple algorithm for identifying
negated    ndings and diseases in discharge summaries. j biomed inform
2001;34:301e10.

33. mutalik p, deshpande a, nadkarni p. use of general-purpose negation detection to

34.

35.

augment concept indexing of medical documents: a quantitative study using the
umls. j am med inform assoc 2001;8:598e609.
huang y, lowe hj. a novel hybrid approach to automated negation detection in
clinical radiology reports. j am med inform assoc 2007;14:304e11.
chapman w, bridewell w, hanbury p, et al. evaluation of negation phrases in
narrative clinical reports. proceedings of the amia fall symposium; 2001.
washington dc: hanley & belfus, philadelphia, 2001:105e9.

38.

37.

40.

39.

36. university of szeged (hungary). conference on computational natural language
learning (conll 2010): learning to detect hedges and their scope in natural text.
2010. http://www.inf.u-szeged.hu/rgai/conll2010st/program.html (accessed 4 may
2010).
savova gk, chapman ww, zheng j, et al. anaphoric relations in the clinical
narrative: corpus creation. j am med inform assoc 2011;18:459e65.
tao c, solbrig h, deepak s, et al. time-oriented id53 from clinical
narratives using semantic-web techniques. springer, berlin: lecture note on
computer science, 2011:6496. http://www.springerlink.com/content/
67623p256743wv4u/ (accessed 20 jul 2011).
hripcsak g, elhadad n, chen yh, et al. using empiric semantic correlation to interpret
temporal assertions in clinical texts. j am med inform assoc 2009;16:220e7.
taira rk, johnson db, bhushan v, et al. a concept-based retrieval system for
thoracic radiology. j digit imaging 1996;9:25e36.
sager n, lyman m, nhan n, et al. medical language processing: applications to
patient data representation and automatic encoding. meth inform med
1995;34:140e6.
haug pj, ranum dl, frederick pr. computerized extraction of coded    ndings from
free-text radiologic reports. radiology 1990;174:543e8.
christensen l, haug pj, fiszman m. mplus: a probabilistic medical language
understanding system. philadelphia, pa: ieee. proceedings workshop on natural
language processing in the biomedical domain 2002:29e36. http://acl.ldc.upenn.
edu/w/w02/w02-0305.pdf (accessed 20 jul 2011).
xu h, friedman c, stetson pd. methods for building sense inventories of
abbreviations in clinical notes. amia annu symp proc 2008:819.
christensen l, harkema h, irwin j, et al. onyx: a system for the semantic
analysis of clinical text. philadelphia, pa: ieee. proceedings of the bionlp2009
workshop of the acl conference 2009. http://www.aclweb.org/anthology/w/w09/
w09-1303.pdf (accessed 20 jul 2011).

41.

42.

43.

44.

45.

46. uzuner o, south b, shen s, et al. 2010 i2b2/va challenge on concepts, assertions,

and relations in clinical text. j am med inform assoc 2011;18:552e6.

47. uzuner o, goldstein i, luo y, et al. identifying patient smoking status from medical

discharge records. j am med inform assoc 2008;15:14e24.

48. uzuner o. recognizing obesity and comorbidities in sparse data. j am med inform

assoc 2009;16:561e70.

49. uzuner o, solti i, cadag e. extracting medication information from clinical text. j am

50.

51.

med inform assoc 2010;17:514e18.
chute c. the horizontal and vertical nature of patient phenotype retrieval: new
directions for clinical text processing. proc amia symposium; 2002. washington dc:
american medical informatics association, 2002:165e9.
chen l, friedman c. extracting phenotypic information from the literature via natural
language processing. stud health technol inform 2004;107:758e62.

52. wang x, hripcsak g, friedman c. characterizing environmental and phenotypic

associations using id205 and electronic health records. bmc
bioinformatics 2009;10:s13.
chapman ww, fiszman m, dowling jn, et al. identifying respiratory    ndings in
emergency department reports for biosurveillance using metamap. stud health
technol inform 2004;107:487e91.
chapman ww, dowling jn, wagner mm. fever detection from free-text clinical
records for biosurveillance. j biomed inform 2004;37:120e7.

53.

54.

23. denny jc, spickard a 3rd, johnson kb, et al. evaluation of a method to identify and

55. wang x, hripcsak g, markatou m, et al. active computerized pharmacovigilance

24.

25.

categorize section headers in clinical documents. j am med inform assoc
2009;16:806e15.
haas s. tools for natural language processing. 2011. http://ils.unc.edu/wstephani/
nlpsp08/resources.html#tools (accessed 1 jun 2011).
savova gk, masanz jj, ogren pv, et al. mayo clinical text analysis and knowledge
extraction system (ctakes): architecture, component evaluation and applications.
j am med inform assoc 2010;17:507e13.

26. aronson a. effective mapping of biomedical text to the umls metathesaurus: the

metamap program. proc amia symp 2001;2001:17e21.
zou q, chu ww, morioka c, et al. indexfinder: a method of extracting key concepts
from clinical texts for indexing. amia annu symp proc 2003:763e7.

27.

550

56.

57.

using natural language processing, statistics, and electronic health records:
a feasibility study. j am med inform assoc 2009;16:328e37.
lindberg dab, humphreys bl, mccray at. the uni   ed medical language system.
meth inform med 1993;32:281e91.
browne ac, mccray at, srinivasan s. the specialist lexicon 2000. http://lexsrv3.
nlm.nih.gov/lexsysgroup/projects/lexicon/2003/release/lexicon/docs/techrpt.pdf
(accessed 20 jul 2011)..

58. divita g, browne ac, rind   esch tc. evaluating lexical variant generation to improve

information retrieval. proc amia symp 1998:775e9.

59. national library of medicine. id51 collection. 2011.

http://wsd.nlm.nih.gov (accessed 2 jun 2011).

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

60.

61.

62.

srihari s. machine learning: generative and discriminative models. 2010. http://
www.cedar.buffalo.edu/wsrihari/cse574/discriminative-generative.pdf (accessed
31 may 2011).
elkan c. id148 and id49. 2008. http://cseweb.
ucsd.edu/welkan/250b/cikmtutorial.pdf (accessed 28 jun 2011).
hearst ma, dumais st, osman e, et al. support vector machines. ieee intel sys
appl 1998;13:18e28.

76.

franz a, brants t. google id165 database (all our id165s belong to you). 2011.
http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-you.html
(accessed 1 jun 2011).

77. university of shef   eld natural language group. information extraction: the

gate pipeline. 2011. http://www.gate.ac.uk/ie (accessed 1 jun 2011).

78. apache foundation. the unstructured information management architecture. 2011.

http://uima.apache.org (accessed 3 jun 2011).

63. dtreg inc. an introduction to support vector machines. 2011. http://www.dtreg.

79. apache foundation. apache uima documentation version 2.3.1. 2011. http://uima.

review

64.

65.

com/id166.htm (accessed 4 jun 2011).
fradkin d, muchnik i. support vector machines for classi   cation. in: abello j,
carmode g, eds. discrete methods in epidemiology. piscataway, nj: rutgers state
university of new jersey, 2006:13e20.
viterbi a. error bounds for convolutional codes and an asymptotically optimum
decoding algorithm. ieee trans inform theor 1967;13:260e9.

66. dempster ap, laird nm, rubin db. maximum likelihood from incomplete data via

67.

68.

69.

70.

71.
72.

73.

74.

75.

the em algorithm. j roy stat soc 1977;39:1e38.
hasegawa-johnson m. multivariate-state id48 for simultaneous
transcription of phones and formants. ieee international conference on acoustics,
speech, and signal processing (icassp); 2000. istanbul, turkey: 2000:1323e26.
http://www.isle.illinois.edu/pubs/2000/hasegawa-johnson00icassp.pdf (accessed 20
jul 2011).
zhang j, shen d, zhou g, et al. tan c-l. exploring deep knowledge resources in
biomedical name recognition. j biomed inform 2004;37:411e22.
sonnhammer ell, eddy sr, birney e, et al. pfam: multiple sequence alignments
and id48-pro   les of protein domains. nucleic acids res 1998;26:320e2.
lukashin a, borodovsky m. genemark.id48: new solutions for gene    nding. nucleic
acids res 1998;26:1107e15.
eddy sr. what is a hidden markov model? nat biotechnol 2004;22:1315e16.
rabiner lr. a tutorial on id48 and selected applications in
id103. proc ieee 1989;77:257e86.
lafferty j, mccallum a, pereira f. conditional random    elds: probabilistic models for
segmenting and labeling sequence data. proc 18th international conf on machine
learning; 2001. 2001:282e9. http://www.cis.upenn.edu/wpereira/papers/crf.pdf
(accessed 20 jul 2011).
culotta a, kulp d, mccallum a. gene prediction with id49. 2005.
http://www.cs.umass.edu/wculotta/pubs/culotta05gene.pdf (accessed 1 jun 2011).
sutton c, mccallum a. an introduction to id49 for relational
learning. amherst: university of massachusetts, 2004.

80.

apache.org/documentation.html (accessed 3 jun 2011).
thompson c, califf m, mooney r. active learning for natural language parsing and
information extraction. in: conference potsiml, ed. 1999. http://www.aidanf.net/
publications/   nn03active.pdf (accessed 20 july).

81. north american chapter of the association for computational linguistics. active

82.

83.

84.

85.

86.

87.

88.

learning for nlp. 2020. http://active-learning.net/alnlp2010. (accessed 7 jun 2011).
fodor p, lally a, ferrucci d. the prolog interface to the unstructured information
management architecture. 2011. http://arxiv.org/ftp/arxiv/papers/0809/0809.0680.
pdf (accessed 4 jun 2011).
lally a, fodor p. natural language processing with prolog in the ibm watson
system. association for logic programming (alp) newsletter, 2011. http://www.cs.
nmsu.edu/alp/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-
system/ (accessed 20 jul 2011).
libresco la. a non-trivial advantage for watson. huf   ngton post (tech). 2011.
http://www.huf   ngtonpost.com/leah-anthony-libresco/a-nontrivial-advantage-
fo_b_825837.html (accessed 20 jul 2011).
jennings k. conversation: jeopardy! champ ken jennings washington post. 2011.
http://live.washingtonpost.com/jeopardy-ken-jennings.html (accessed 20 jul 2011).
fitzgerald j.    jeopardy!   -winning computer delving into medicine. 2011. http://
www.physorg.com/news/2011-05-jeopardy-winning-delving-medicine.html
(accessed 4 jun 2011).
berner es, webster gd, shugerman aa, et al. performance of four computer-based
diagnostic systems. n engl j med. 1994;330:1792e6.
friedman cp, elstein as, wolf fm, et al. enhancement of clinicians    diagnostic
reasoning by computer-based consultation: a multisite study of 2 systems. jama
1999;282:1851e6.

89. miller r, masarie f. the demise of the greek oracle model of diagnostic decision

support systems. meth inform med 1990;29:1e8.

90. miller r. medical diagnostic decision support systemsepast, present, and future:

a threaded bibliography and brief commentary. j am med inform assoc 1994;1:8e27.

page fraction trail=7.5

j am med inform assoc 2011;18:544e551. doi:10.1136/amiajnl-2011-000464

551

downloaded from 

jamia.bmj.com

 on october 5, 2011 - published by 

group.bmj.com
 

natural language processing: an
introduction
 
prakash m nadkarni, lucila ohno-machado and wendy w chapman
 
jamia
doi: 10.1136/amiajnl-2011-000464

 2011 18: 544-551

updated information and services can be found at: 
 
http://jamia.bmj.com/content/18/5/544.full.html

these include:

references

this article cites 37 articles, 16 of which can be accessed free at:
 
http://jamia.bmj.com/content/18/5/544.full.html#ref-list-1
 
article cited in: 
 
http://jamia.bmj.com/content/18/5/544.full.html#related-urls

email alerting
service

receive free email alerts when new articles cite this article. sign up in
the box at the top right corner of the online article.

notes

to request permissions go to:
 
http://group.bmj.com/group/rights-licensing/permissions

to order reprints go to:
 
http://journals.bmj.com/cgi/reprintform

to subscribe to bmj go to:
 
http://group.bmj.com/subscribe/

