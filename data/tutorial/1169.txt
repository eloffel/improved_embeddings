spectral algorithms for latent variable models 

part 2: dynamical systems

byron boots

http://www.cs.cmu.edu/~beb/

machine learning department

carnegie mellon university

tuesday, june 26, 2012

  sense
  learn
act

sequences of observations

o1

o2

o3

. . .

o  

byron boots     spectral algorithms for latent variable models: dynamical systems

2

tuesday, june 26, 2012

  sense
  learn
act

latent state

x1

o1

x2

o2

. . .

. . .

x3

o3

x  

o  

byron boots     spectral algorithms for latent variable models: dynamical systems

3

tuesday, june 26, 2012

  sense
  learn
act

dynamics

x1

o1

x2

o2

x3

o3

. . .

x  

o  

byron boots     spectral algorithms for latent variable models: dynamical systems

4

tuesday, june 26, 2012

  sense
  learn
act

dynamical systems

x1

o1

x2

o2

x3

o3

. . .

x  

o  

    lti systems (kalman filter)
    (i-o) id48
    predictive state representations

byron boots     spectral algorithms for latent variable models: dynamical systems

5

tuesday, june 26, 2012

  sense
  learn
act

learning a dynamical system

?

?

x2

o2

?

?

x3

o3

?

x1

o1

?
. . .

?

x  

o  

byron boots     spectral algorithms for latent variable models: dynamical systems

6

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for dynamical systems

    lti systems (kalman filter)

subspace
identification
for linear systems

theory - implementation - applic ations

peter van overschee
bart de mo or
katholieke universiteitleuven
belgium

kluwer academic publishers

boston/london/dordrecht

    id48
    spectral learning of id48s [andersson, ryden, 2008]
    spectral learning of id48s [hsu, kakade, zhang, 2009]
    spectral learning of rr-id48s [siddiqi, boots, gordon, 2009]
    predictive state representations 
    spectral learning of psrs [boots, siddiqi, gordon, 2010]
    online spectral learning of psrs [boots, gordon, 2011]

byron boots     spectral algorithms for latent variable models: dynamical systems

7

tuesday, june 26, 2012

  sense
  learn
act

why id106?

there are many ways to learn a dynamical system

    maximum likelihood via expectation maximization, id119, ...
    bayesian id136 via gibbs, metropolis hastings, ...

byron boots     spectral algorithms for latent variable models: dynamical systems

8

tuesday, june 26, 2012

  sense
  learn
act

why id106?

there are many ways to learn a dynamical system

    maximum likelihood via expectation maximization, id119, ...
    bayesian id136 via gibbs, metropolis hastings, ...

in contrast to these methods, spectral learning algorithms give

    no local optima:
    huge gain in computational efficiency
    slight loss in statistical efficiency

byron boots     spectral algorithms for latent variable models: dynamical systems

8

tuesday, june 26, 2012

  sense
  learn
act

the focus of this part of the tutorial

    a spectral learning algorithm for kalman filters
    a spectral learning algorithm for id48s
    relation to psrs

byron boots     spectral algorithms for latent variable models: dynamical systems

9

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

xt+1 = axt + noise
ot = cxt + noise

x1

o1

x2

o2

x3

o3

. . .

x  

o  

observation matrix:

n

m

c

transition matrix:

n

a

n

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

we can relax both assumptions in practice

byron boots     spectral algorithms for latent variable models: dynamical systems

10

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

for          ,
k     1

=  k

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

  k

=

c

ak

p

ct

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

byron boots     spectral algorithms for latent variable models: dynamical systems

11

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

for          ,
k     1

=  k

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

  k

=

c

ak

p

ct

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

byron boots     spectral algorithms for latent variable models: dynamical systems

12

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

for          ,
k     1

=  k

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

  k

=

c

ak

p

ct

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

byron boots     spectral algorithms for latent variable models: dynamical systems

13

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

for          ,
k     1

=  k

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

  k

=

c

ak

p

ct

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

byron boots     spectral algorithms for latent variable models: dynamical systems

14

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

for          ,
k     1

=  k

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

  k

=

c

ak

p

ct

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

byron boots     spectral algorithms for latent variable models: dynamical systems

15

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

for          ,
k     1

=  k

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

  k

=

c

ak

p

ct

    assume for simplicity that
  and that     and     are full rank

m     n

a

c

byron boots     spectral algorithms for latent variable models: dynamical systems

16

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= (u   ca)ap c   (u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ap c   (u   ca)   1
= sas   1

   c := u (sas   1)   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

17

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

   c := u (sas   1)   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

18

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

   c := u (sas   1)   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

19

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

   c := u (sas   1)   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

20

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

   c := u (sas   1)   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

21

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

similarity transform of a

   c := u (sas   1)   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

22

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

similarity transform of a

   c := u (sas   1)   1

u   a   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

23

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

similarity transform of a

   c := u (sas   1)   1

u   a   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

24

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

similarity transform of a

   c := u (sas   1)   1

u   a   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

byron boots     spectral algorithms for latent variable models: dynamical systems

25

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

  k = e   ot+kot

t    = cakp ct

let    be the left    singular vectors of     , 
  1

u

n

   a := u     2(u     1)   

= u   ca2p c   (u   cap c   )   
= u   ca2(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)a(u   ca)   1
= sas   1

similarity transform of a

   c := u (sas   1)   1

u   a   1

= u sa   1s   1
= u (u   ca)a   1s   1
= cs   1

linear transform of c

byron boots     spectral algorithms for latent variable models: dynamical systems

26

tuesday, june 26, 2012

  sense
  learn
act

kalman filters

spectral learning algorithm:

  2

  1

    estimate     and     from data
    find    by svd
    plug in for    and 

   u

   a

   c

learning is consistent:

    law of large numbers for     and 
    continuity of formulas for    and

  1

  2

   a

   c

byron boots     spectral algorithms for latent variable models: dynamical systems

27

tuesday, june 26, 2012

  sense
  learn
act

variations on spectral learning for

kalman filters

    use arbitrary features of past and future observations
    work from covariance of past, future features
    good features make a big difference in practice

    use different spectral decompositions to find state space: cca, rrr
   
    learn kalman filters with control inputs

impose constraints on learned model (e.g., stability)

byron boots     spectral algorithms for latent variable models: dynamical systems

28

tuesday, june 26, 2012

  sense
  learn
act

example: video textures

works well for learning models of video textures

observations = raw pixels (vector of reals over time)

simulations from learned models
[siddiqi, boots, gordon, 2007]

(40 dimensions)

(40 dimensions)

byron boots     spectral algorithms for latent variable models: dynamical systems

29

tuesday, june 26, 2012

  sense
  learn
act

additional examples

    glass oven modeling [backx, 1987]
    aircraft wing flutter [peloubet et al., 1990]
    control of air temperature and flow [ljung, 1991]
    mechanical construction of cd player arms [van den hof et al., 1993]
    heat flow through walls [bloem, 1994]
    chemical processes [van overschee, de moor, 1996]
    economic forecasting [aoki, havenner, 1997]
   

...

byron boots     spectral algorithms for latent variable models: dynamical systems

30

tuesday, june 26, 2012

  sense
  learn
act

kalman filter spectral learning: failure

given a short video

learn a model

byron boots     spectral algorithms for latent variable models: dynamical systems

31

tuesday, june 26, 2012

  sense
  learn
act

kalman filter spectral learning: failure

given a short video

learn a model

byron boots     spectral algorithms for latent variable models: dynamical systems

31

tuesday, june 26, 2012

  sense
  learn
act

kalman filter spectral learning: failure

simulations from models trained on clock data

?

kalman filter (spectral)

10 dimensions

id48 (baum-welch)

10 states

something better...

10 dimensions

byron boots     spectral algorithms for latent variable models: dynamical systems

32

tuesday, june 26, 2012

  sense
  learn
act

kalman filter spectral learning: failure

simulations from models trained on clock data

?

kalman filter (spectral)

10 dimensions

id48 (baum-welch)

10 states

something better...

10 dimensions

byron boots     spectral algorithms for latent variable models: dynamical systems

32

tuesday, june 26, 2012

  sense
  learn
act

kalman filter spectral learning: failure

simulations from models trained on clock data

?

kalman filter (spectral)

10 dimensions

id48 (baum-welch)

10 states

something better...

10 dimensions

byron boots     spectral algorithms for latent variable models: dynamical systems

32

tuesday, june 26, 2012

  sense
  learn
act

can we generalize spectral learning? 

id48s

xt+1 = axt + noise
ot = cxt + noise

x1

o1

x2

o2

x3

o3

    get rid of gaussian noise assumption
    hidden markov model: same form as kalman 
filter but,

        noise ~ multinomial distribution
a     0, a1 = 1, c     0, c1 = 1
       and   are indicators: e.g. 
x

o

   4    = [ 0 0 0 1 0 ]t

. . .

x  

o  

observation matrix:

n

m

c

transition matrix:

n

a

n

byron boots     spectral algorithms for latent variable models: dynamical systems

33

tuesday, june 26, 2012

  sense
  learn
act

spectral learning: gaussian vs. multinomial

kalman filter

hidden markov model

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

    assume for simplicity that             and that     and     are full rank

a

c

m     n

byron boots     spectral algorithms for latent variable models: dynamical systems

34

tuesday, june 26, 2012

  sense
  learn
act

spectral learning: gaussian vs. multinomial

kalman filter

hidden markov model

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

e   ot+kot

t    = e   e   ot+kot
t | xt      
= e   e   ot+k | xt]e[ot
= e      cakxt    (cxt)t   

t ]ct

= cake[xtxt
= cakp ct

t | xt      

exactly the same!

    assume for simplicity that             and that     and     are full rank

a

c

m     n

byron boots     spectral algorithms for latent variable models: dynamical systems

34

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

  k

=

c

ak

p

ct

   a := u     2   u     1      

= (u   ca)a(u   ca)   1
= sas   1

    as before, recover     and     from      and       
   a
  2
    does not satisfy 
    is this a problem?

a     0, a1 = 1, c     0, c1 = 1

   c

  1

byron boots     spectral algorithms for latent variable models: dynamical systems

35

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

  k

=

c

ak

p

ct

   a := u     2   u     1      

= (u   ca)a(u   ca)   1
= sas   1

    as before, recover     and     from      and       
   a
  2
    does not satisfy 
    is this a problem?

a     0, a1 = 1, c     0, c1 = 1

   c

  1

yes. id136 is different in an id48.

byron boots     spectral algorithms for latent variable models: dynamical systems

35

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

byron boots     spectral algorithms for latent variable models: dynamical systems

36

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

factor by chain rule

marginalizing out latent state

p [x2 | x1] p [o1 | x1] p [x1]

byron boots     spectral algorithms for latent variable models: dynamical systems

37

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

transition id203

observation likelihood

byron boots     spectral algorithms for latent variable models: dynamical systems

38

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

likelihood of 

o1o2...

c

byron boots     spectral algorithms for latent variable models: dynamical systems

39

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

= 

1   n sas   1 sdiag(co  ,:)s   1 . . . sas   1 sdiag(co2,:)s   1 sas   1 sdiag(co1,:)s   1sp[x1]

byron boots     spectral algorithms for latent variable models: dynamical systems

40

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

1   n sas   1 sdiag(co  ,:)s   1 . . . sas   1 sdiag(co2,:)s   1 sas   1 sdiag(co1,:)s   1sp[x1]

we have access to:

no good way of finding the observation likelihoods

   a = sas   1

but    c = cs   1

(e.g.                             )

sdiag(co1,:)s   1

byron boots     spectral algorithms for latent variable models: dynamical systems

41

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

byron boots     spectral algorithms for latent variable models: dynamical systems

42

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

combine into a single observable operator, one for each observation

standard id48
parameterization

a

c

observable operator id48

parameterization
[jaeger, 1998]

..
o4
o3
o2
o1

ao

byron boots     spectral algorithms for latent variable models: dynamical systems

43

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

1t
nao   . . . ao2ao1p [x1]

byron boots     spectral algorithms for latent variable models: dynamical systems

44

tuesday, june 26, 2012

  sense
  learn
act

id136 for id48s

p [o1, o2, . . . , o   ]

   x   +1   x  

p [x   +1 | x   ] p [o   | x   ] . . .   x2

p [x3 | x2] p [o2 | x2]   x1

p [x2 | x1] p [o1 | x1] p [x1]

1t
na diag(co  ,:) . . . a diag(co2,:)a diag(co1,:)p [x1]

1t
nao   . . . ao2ao1p [x1]

1s   1sao   s   1 . . . sao2s   1sao1s   1sp[x1]

in fact, only need to estimate similarity transforms of parameters saos   1

the    s cancel

s

byron boots     spectral algorithms for latent variable models: dynamical systems

45

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

goal is to find similarity transforms of      sao

ao

..
o4
o3
o2
o1

byron boots     spectral algorithms for latent variable models: dynamical systems

46

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

goal is to find similarity transforms of      sao

ao

..
o4
o3
o2
o1

  1

:= e   ot+1o   t   

= cap c   

  2

:= e   ot+2o   t   

= ca2p c   

  1

  2

byron boots     spectral algorithms for latent variable models: dynamical systems

46

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

goal is to find similarity transforms of      sao

ao

..
o4
o3
o2
o1

  1

:= e   ot+1o   t   

= cap c   

  2

:= e   ot+2o   t   

= ca2p c   

  o
2

:= e[ot+2(     o ot+1)o   t ]
= caaop ct

  1

  2

  o
2

.
..
o4
o3
o2
o1

a tensor

byron boots     spectral algorithms for latent variable models: dynamical systems

46

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

  o
2

  o
2

.
..
o4
o3
o2
o1

a tensor

:= e[ot+2(     o ot+1)o   t ]
= e[e[ot+2(     o ot+1)o   t
| xt]]
= e[e[ot+2(     o ot+1) | xt]e[o   t
= e[e[ot+2 | xt, ot+1 = o]p[ot+1 = o | xt](cxt)   ]
= e[e[ot+2 | xt, ot+1 = o](1   aoxt)(cxt)   ]
1   aoxt    (1   aoxt)(cxt)      
= e   ca    aoxt

| xt]]

= e[caaoxt(cxt)   ]
= caaoe[xtx   t ]c   
= caaop c   

byron boots     spectral algorithms for latent variable models: dynamical systems

47

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

  o
2

  o
2

.
..
o4
o3
o2
o1

a tensor

:= e[ot+2(     o ot+1)o   t ]
= e[e[ot+2(     o ot+1)o   t
| xt]]
= e[e[ot+2(     o ot+1) | xt]e[o   t
= e[e[ot+2 | xt, ot+1 = o]p[ot+1 = o | xt](cxt)   ]
= e[e[ot+2 | xt, ot+1 = o](1   aoxt)(cxt)   ]
1   aoxt    (1   aoxt)(cxt)      
= e   ca    aoxt

| xt]]

= e[caaoxt(cxt)   ]
= caaoe[xtx   t ]c   
= caaop c   

byron boots     spectral algorithms for latent variable models: dynamical systems

47

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

  o
2

  o
2

.
..
o4
o3
o2
o1

a tensor

:= e[ot+2(     o ot+1)o   t ]
= e[e[ot+2(     o ot+1)o   t
| xt]]
= e[e[ot+2(     o ot+1) | xt]e[o   t
= e[e[ot+2 | xt, ot+1 = o]p[ot+1 = o | xt](cxt)   ]
= e[e[ot+2 | xt, ot+1 = o](1   aoxt)(cxt)   ]
1   aoxt    (1   aoxt)(cxt)      
= e   ca    aoxt

... and then a miracle occurs!

| xt]]

= e[caaoxt(cxt)   ]
= caaoe[xtx   t ]c   
= caaop c   

byron boots     spectral algorithms for latent variable models: dynamical systems

47

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

goal is to find similarity transforms of      sao

ao

..
o4
o3
o2
o1

  1

:= e   ot+1o   t   

= cap c   

  2

:= e   ot+2o   t   

= ca2p c   

  o
2

:= e[ot+2(     o ot+1)o   t ]
= caaop ct

  1

  2

  o
2

.
..
o4
o3
o2
o1

a tensor

byron boots     spectral algorithms for latent variable models: dynamical systems

48

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

    assume for simplicity that             and that     and     are full rank
    let    be the left    singular vectors of     ,
  1

m     n

a

c

u

n

  ao

2(u     1)   

:= u     o
= u   caaop c   (u   cap c   )   
= (u   ca)ao(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ao(u   ca)   1
= saos   1

byron boots     spectral algorithms for latent variable models: dynamical systems

49

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

    assume for simplicity that             and that     and     are full rank
    let    be the left    singular vectors of     ,
  1

m     n

a

c

u

n

  ao

2(u     1)   

:= u     o
= u   caaop c   (u   cap c   )   
= (u   ca)ao(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ao(u   ca)   1
= saos   1

byron boots     spectral algorithms for latent variable models: dynamical systems

50

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

    assume for simplicity that             and that     and     are full rank
    let    be the left    singular vectors of     ,
  1

m     n

a

c

u

n

  ao

2(u     1)   

:= u     o
= u   caaop c   (u   cap c   )   
= (u   ca)ao(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ao(u   ca)   1
= saos   1

byron boots     spectral algorithms for latent variable models: dynamical systems

51

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

    assume for simplicity that             and that     and     are full rank
    let    be the left    singular vectors of     ,
  1

m     n

a

c

u

n

  ao

2(u     1)   

:= u     o
= u   caaop c   (u   cap c   )   
= (u   ca)ao(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ao(u   ca)   1
= saos   1

byron boots     spectral algorithms for latent variable models: dynamical systems

52

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

    assume for simplicity that             and that     and     are full rank
    let    be the left    singular vectors of     ,
  1

m     n

a

c

u

n

  ao

2(u     1)   

:= u     o
= u   caaop c   (u   cap c   )   
= (u   ca)ao(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ao(u   ca)   1
= saos   1

similarity transform of ao

byron boots     spectral algorithms for latent variable models: dynamical systems

53

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

    assume for simplicity that             and that     and     are full rank
    let    be the left    singular vectors of     ,
  1

m     n

a

c

u

n

  ao

2(u     1)   

:= u     o
= u   caaop c   (u   cap c   )   
= (u   ca)ao(u   ca)   1(u   ca)p c   (u   cap c   )   
= (u   ca)ao(u   ca)   1
= saos   1

similarity transform of ao

    additional parameters, like normalizer and initial state can be
  found in a similar manner
       always cancels when predicting, filtering, simulating: e.g.
s

1s   1sao   s   1 . . . sao2s   1sao1s   1sp[x1]

byron boots     spectral algorithms for latent variable models: dynamical systems

54

tuesday, june 26, 2012

  sense
  learn
act

spectral learning for id48s

spectral learning algorithm:

  1

  o
2

    estimate     and     from data
    find    by svd
    plug in for     s     
learning is consistent:

   u

   ao

    law of large numbers for     and 
  1
    continuity of formulas for     s

  o
2

   ao

byron boots     spectral algorithms for latent variable models: dynamical systems

55

tuesday, june 26, 2012

  sense
  learn
act

example: clock (revisited)

byron boots     spectral algorithms for latent variable models: dynamical systems

56

tuesday, june 26, 2012

  sense
  learn
act

example: clock pendulum

simulations from models trained on clock data

kalman filter (spectral)

10 dimensions

id48 (baum-welch)

10 states

id48? (spectral)
10 dimensions

byron boots     spectral algorithms for latent variable models: dynamical systems

57

tuesday, june 26, 2012

  sense
  learn
act

example: clock pendulum

simulations from models trained on clock data

kalman filter (spectral)

10 dimensions

id48 (baum-welch)

10 states

id48? (spectral)
10 dimensions

byron boots     spectral algorithms for latent variable models: dynamical systems

57

tuesday, june 26, 2012

  sense
  learn
act

can we generalize? 

byron boots     spectral algorithms for latent variable models: dynamical systems

58

tuesday, june 26, 2012

  sense
  learn
act

can we generalize? 

byron boots     spectral algorithms for latent variable models: dynamical systems

58

tuesday, june 26, 2012

  sense
  learn
act

can we generalize? 

lots of states: not a problem in itself, but means we need lots
                         of data to learn transition & observation models

byron boots     spectral algorithms for latent variable models: dynamical systems

58

tuesday, june 26, 2012

  sense
  learn
act

generalizing id48s 

x        

id48 state space:
    id48s had    
    intuition: number of discrete states = number of dimensions
    we now have
    essentially equally restrictive
    can we allow a more general state space?
    e.g. # states > # dimensions
    discretize more finely while keeping dimensionality the same

x     s   

byron boots     spectral algorithms for latent variable models: dynamical systems

59

tuesday, june 26, 2012

  sense
  learn
act

predictive state representations

    ooms, multiplicity automata, etc   

ao

like id48, but lift restriction of 

    psr: defined by transition matrices     , and a id172 vector
   
   
   

lift restrictions on     s, top eigenvalue of            must be       

instead of a set of discrete states, can think of state space as a possibly 
infinite-dimensional simplex projected onto a finite dimensional space

   o

x = s   

ao

ao

1

   

includes id48s (and pomdps) as special case

byron boots     spectral algorithms for latent variable models: dynamical systems

60

tuesday, june 26, 2012

  sense
  learn
act

ssid for psrs

psrs are more expressive than id48s & pomdps ... and as easy to learn!

predictive state representations

id48s & pomdps

byron boots     spectral algorithms for latent variable models: dynamical systems

61

tuesday, june 26, 2012

for fixed latent dimension n

  sense
  learn
act

example: clock pendulum

simulations from models trained on clock data

kalman filter (spectral)

10 states

id48 (baum-welch)

10 dimensions

psr (spectral)
10 dimensions

byron boots     spectral algorithms for latent variable models: dynamical systems

62

tuesday, june 26, 2012

  sense
  learn
act

variations on spectral learning for psrs

    use arbitrary features of past and future observations

    work from covariance of past, future features
    good features make a big difference in practice
    but still need a discrete set of transition matrices  
ao

    use different spectral decompositions to find state space: cca, rrr
    can extend to learn models with actions

byron boots     spectral algorithms for latent variable models: dynamical systems

63

tuesday, june 26, 2012

  sense
  learn
act

can we generalize? features!

    so far: allowed finer discretization of state space
    can we improve? allow continuous observations?
    yes: featurize!
    let         be a feature function
    
:= e[ot+2  (ot+1)o   t ]
2

  (o)

  (o)e[ot+2(     o ot+1)o   t ]

  (o)  o
2

  a  

:= u       

2 (u     1)   
  (o)   ao

=    o
=    o
=    o
   a  

byron boots     spectral algorithms for latent variable models: dynamical systems

64

tuesday, june 26, 2012

store      for many different    , recover      as needed

  

   ao

  sense
  learn
act

can we generalize? infinite features!

    if some features are good, more must be better!   
    kernels
    everything that we have seen is id202
    works just fine in an arbitrary rkhs
    can rewrite all of the formulas in terms of gram matrices

result: hilbert space embeddings of predictive state representations

    handles near arbitrary observation distributions
    good prediction performance

byron boots     spectral algorithms for latent variable models: dynamical systems

65

tuesday, june 26, 2012

example: prediction (slot car domain)

hilbert space embeddings of id48

r
a
c

 
t
o
l
s

  sense
  learn
act
a.

r
a
c

 
t
o
l

s

imu

imu
x 106

b.

 

.
r
r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

8
7
6
5
4
3
2
1
0

racetrack

 

r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

7
6
5
4
3
2
1
0

mean
last
lds

id48
rr-id48
embedded

joint work with dieter fox   s lab
10

0

20

50

40

30
prediction horizon
80

90 100

mean
last
lds

)

%

(
 

y
c
a
r
u
c
c
a

90

85

80

75

70
60
65
60

0

10

20

50

40

70
30
prediction horizon

60

figure 4. slot car inertial measurement data. (a) the slot
racetrack
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 4. slot car inertial measurement data. (a) the slot
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 5. accuracies and 95% con   dence intervals for hu-
man vs. non-human audio event classi   cation, comparing
embedded id48s to other common sequential models at
di   erent latent state space sizes.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was
to learn a model of the noisy imu data, and, after
   ltering, to predict future imu readings.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was

11 seconds of human and 28 minutes 43 seconds of
non-human audio data. to reduce noise and training
time we averaged the data every 100 timesteps (corre-
sponding to 1 second) and downsampled.

byron boots     spectral algorithms for latent variable models: dynamical systems

66

tuesday, june 26, 2012

example: prediction (slot car domain)

hilbert space embeddings of id48

r
a
c

 
t
o
l
s

  sense
  learn
act
a.

r
a
c

 
t
o
l

s

imu

imu
x 106

b.

 

.
r
r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

8
7
6
5
4
3
2
1
0

racetrack

 

r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

7
6
5
4
3
2
1
0

mean
last
lds

id48
rr-id48
embedded

joint work with dieter fox   s lab
10

0

20

50

40

30
prediction horizon
80

90 100

mean
last
lds

)

%

(
 

y
c
a
r
u
c
c
a

90

85

80

75

70
60
65
60

0

10

20

50

40

70
30
prediction horizon

60

figure 4. slot car inertial measurement data. (a) the slot
racetrack
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 4. slot car inertial measurement data. (a) the slot
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 5. accuracies and 95% con   dence intervals for hu-
man vs. non-human audio event classi   cation, comparing
embedded id48s to other common sequential models at
di   erent latent state space sizes.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was
to learn a model of the noisy imu data, and, after
   ltering, to predict future imu readings.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was

11 seconds of human and 28 minutes 43 seconds of
non-human audio data. to reduce noise and training
time we averaged the data every 100 timesteps (corre-
sponding to 1 second) and downsampled.

byron boots     spectral algorithms for latent variable models: dynamical systems

66

tuesday, june 26, 2012

example: prediction (slot car domain)

hilbert space embeddings of id48

r
a
c

 
t
o
l
s

  sense
  learn
act
a.

r
a
c

 
t
o
l

s

imu

imu
x 106

b.

 

.
r
r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

8
7
6
5
4
3
2
1
0

racetrack

 

r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

7
6
5
4
3
2
1
0

mean
last
lds

id48
rr-id48
embedded

joint work with dieter fox   s lab
10

0

20

50

40

30
prediction horizon
80

90 100

mean
last
lds

)

%

(
 

y
c
a
r
u
c
c
a

90

85

80

75

70
60
65
60

0

10

20

40

70
30
prediction horizon

60

50
3 x10 3
2

figure 4. slot car inertial measurement data. (a) the slot
racetrack
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 4. slot car inertial measurement data. (a) the slot
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 5. accuracies and 95% con   dence intervals for hu-
man vs. non-human audio event classi   cation, comparing
embedded id48s to other common sequential models at
di   erent latent state space sizes.

(cid:239)1

0

1

(cid:239)2

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was
to learn a model of the noisy imu data, and, after
   ltering, to predict future imu readings.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was

11 seconds of human and 28 minutes 43 seconds of
non-human audio data. to reduce noise and training
time we averaged the data every 100 timesteps (corre-
sponding to 1 second) and downsampled.

byron boots     spectral algorithms for latent variable models: dynamical systems

(cid:239)3
0

100

150

50

66

tuesday, june 26, 2012

example: prediction (slot car domain)

hilbert space embeddings of id48

r
a
c

 
t
o
l
s

  sense
  learn
act
a.

r
a
c

 
t
o
l

s

imu

 

r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

7
6
5
4
3
2
1
0

imu
x 106

b.

 

.
r
r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

8
7
6
5
4
3
2
1
0

mean
last
lds

id48
rr-id48
embedded

mean
last
lds

)

%

(
 

y
c
a
r
u
c
c
a

90

85

80

75

70
60
65
60

joint work with dieter fox   s lab
10

0

20

50

40

30
prediction horizon
80

90 100

racetrack

5

40

10

feature-psr
0
20
kernel-psr

6 x 106
mean obs.
figure 4. slot car inertial measurement data. (a) the slot
lds
racetrack
gplvm
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 4. slot car inertial measurement data. (a) the slot
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

70
30
prediction horizon

50
3 x10 3
2

figure 5. accuracies and 95% con   dence intervals for hu-
man vs. non-human audio event classi   cation, comparing
embedded id48s to other common sequential models at
di   erent latent state space sizes.

4
e
s
3
m

(cid:239)1

60

0

1

2

1

(cid:239)2

20

10

40

50

0
0

30
70
prediction horizon

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was
to learn a model of the noisy imu data, and, after
   ltering, to predict future imu readings.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was

11 seconds of human and 28 minutes 43 seconds of
non-human audio data. to reduce noise and training
time we averaged the data every 100 timesteps (corre-
sponding to 1 second) and downsampled.

byron boots     spectral algorithms for latent variable models: dynamical systems

90 100

(cid:239)3
0

100

150

50

60

80

66

tuesday, june 26, 2012

example: prediction (slot car domain)

hilbert space embeddings of id48

r
a
c

 
t
o
l
s

  sense
  learn
act
a.

r
a
c

 
t
o
l

s

imu

 

r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

7
6
5
4
3
2
1
0

imu
x 106

b.

 

.
r
r
e
n
o
i
t
c
i
d
e
r
p

 
.

g
v
a

8
7
6
5
4
3
2
1
0

mean
last
lds

id48
rr-id48
embedded

mean
last
lds

)

%

(
 

y
c
a
r
u
c
c
a

90

85

80

75

70
60
65
60

joint work with dieter fox   s lab
10

0

20

50

40

30
prediction horizon
80

racetrack

5

60

40

10

4
e
s
3
m

feature-psr
0
20
kernel-psr

[ko & fox, 2010]

70
30
prediction horizon

gaussian process latent variable models

6 x 106
mean obs.
figure 4. slot car inertial measurement data. (a) the slot
lds
racetrack
gplvm
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

figure 4. slot car inertial measurement data. (a) the slot
car platform and the imu (top) and the racetrack (bot-
tom). (b) squared error for prediction with di   erent esti-
mated models and baselines.

nonparametric models win
50
90 100
3 x10 3
2
~1 day on 8-core i7 workstation 
1

11.6 seconds to learn model 
on my laptop in matlab
this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was
to learn a model of the noisy imu data, and, after
   ltering, to predict future imu readings.

this data while the slot car circled the track controlled
by a constant policy. the goal of this experiment was

byron boots     spectral algorithms for latent variable models: dynamical systems

30
70
prediction horizon

in matlab/c++

kernel psrs:

90 100

(cid:239)3
0

100

150

0
0

(cid:239)2

(cid:239)1

50

50

60

80

40

10

20

66

0

2

1

tuesday, june 26, 2012

figure 5. accuracies and 95% con   dence intervals for hu-
man vs. non-human audio event classi   cation, comparing
embedded id48s to other common sequential models at
di   erent latent state space sizes.

11 seconds of human and 28 minutes 43 seconds of
non-human audio data. to reduce noise and training
time we averaged the data every 100 timesteps (corre-
sponding to 1 second) and downsampled.

  sense
  learn
act

making it all fast:

online updates to spectral learning

    with each new observation, rank-1 update of:

    svd (brand)
   

inverse (sherman-morrison)

    n features;  latent dimension d;  t steps

    space = o(nd): may fit in cache!
   

time = o(nd2t): bounded time per example

    small loss in statistical efficiency (estimated subspace 

rotates), but can deal with it

    problem: no rank-1 update of k-svd 

    can use random projections

byron boots     spectral algorithms for latent variable models: dynamical systems

67

tuesday, june 26, 2012

  sense
  learn
act

summary

    learn dynamical system models with no local optima, fast online computation
   
in contrast with many other methods, learning and id136 is extremely fast 
and robust

    nonparametric (kernel-based) version handles near-arbitrary observation 

distributions

    one general principle yields algorithms for kalman system id, id48s, psrs
    good results from a general-purpose algorithm on problems typically tackled 

by lots of engineering

byron boots     spectral algorithms for latent variable models: dynamical systems

68

tuesday, june 26, 2012

  sense
  learn
act

thank you!

byron boots     spectral algorithms for latent variable models: dynamical systems

69

tuesday, june 26, 2012

