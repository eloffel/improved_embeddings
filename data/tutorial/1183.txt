1 an introduction to conditional random

fields for relational learning

charles sutton
department of computer science
university of massachusetts, usa
casutton@cs.umass.edu
http://www.cs.umass.edu/   casutton

andrew mccallum
department of computer science
university of massachusetts, usa
mccallum@cs.umass.edu
http://www.cs.umass.edu/   mccallum

1.1 introduction

relational data has two characteristics:    rst, statistical dependencies exist between
the entities we wish to model, and second, each entity often has a rich set of features
that can aid classi   cation. for example, when classifying web documents, the
page   s text provides much information about the class label, but hyperlinks de   ne
a relationship between pages that can improve classi   cation [taskar et al., 2002].
id114 are a natural formalism for exploiting the dependence structure
among entities. traditionally, id114 have been used to represent the
joint id203 distribution p(y, x), where the variables y represent the attributes
of the entities that we wish to predict, and the input variables x represent our
observed knowledge about the entities. but modeling the joint distribution can
lead to di   culties when using the rich local features that can occur in relational
data, because it requires modeling the distribution p(x), which can include complex
dependencies. modeling these dependencies among inputs can lead to intractable
models, but ignoring them can lead to reduced performance.
a solution to this problem is to directly model the conditional distribution p(y|x),
which is su   cient for classi   cation. this is the approach taken by conditional ran-
dom    elds [la   erty et al., 2001]. a conditional random    eld is simply a conditional
distribution p(y|x) with an associated graphical structure. because the model is

2

an introduction to id49 for relational learning

conditional, dependencies among the input variables x do not need to be explicitly
represented, a   ording the use of rich, global features of the input. for example,
in natural language tasks, useful features include neighboring words and word bi-
grams, pre   xes and su   xes, capitalization, membership in domain-speci   c lexicons,
and semantic information from sources such as id138. recently there has been
an explosion of interest in crfs, with successful applications including text process-
ing [taskar et al., 2002, peng and mccallum, 2004, settles, 2005, sha and pereira,
2003], bioinformatics [sato and sakakibara, 2005, liu et al., 2005], and computer
vision [he et al., 2004, kumar and hebert, 2003].
this chapter is divided into two parts. first, we present a tutorial on current
training and id136 techniques for conditional random    elds. we discuss the
important special case of linear-chain crfs, and then we generalize these to
arbitrary graphical structures. we include a brief discussion of techniques for
practical crf implementations.
second, we present an example of applying a general crf to a practical relational
learning problem. in particular, we discuss the problem of information extraction,
that is, automatically building a relational database from information contained
in unstructured text. unlike linear-chain models, general crfs can capture long
distance dependencies between labels. for example, if the same name is mentioned
more than once in a document, all mentions probably have the same label, and it
is useful to extract them all, because each mention may contain di   erent comple-
mentary information about the underlying entity. to represent these long-distance
dependencies, we propose a skip-chain crf, a model that jointly performs seg-
mentation and collective labeling of extracted mentions. on a standard problem
of extracting speaker names from seminar announcements, the skip-chain crf has
better performance than a linear-chain crf.

1.2 id114

1.2.1 de   nitions
we consider id203 distributions over sets of random variables v = x     y ,
where x is a set of input variables that we assume are observed, and y is a set of
output variables that we wish to predict. every variable v     v takes outcomes from
a set v, which can be either continuous or discrete, although we discuss only the
discrete case in this chapter. we denote an assignment to x by x, and we denote
an assignment to a set a     x by xa, and similarly for y . we use the notation
1{x=x0} to denote an indicator function of x which takes the value 1 when x = x0
and 0 otherwise.
a graphical model is a family of id203 distributions that factorize according
to an underlying graph. the main idea is to represent a distribution over a large
number of random variables by a product of local functions that each depend on
only a small number of variables. given a collection of subsets a     v , we de   ne

1.2 id114

3

an undirected graphical model as the set of all distributions that can be written in
the form

  a(xa, ya),

(1.1)

y

a

p(x, y) =

1
z

for any choice of factors f = {  a}, where   a : v n     <+. (these functions are
also called local functions or compatibility functions.) we will occasionally use the
term random    eld to refer to a particular distribution among those de   ned by an
undirected model. to reiterate, we will consistently use the term model to refer to a
family of distributions, and random    eld (or more commonly, distribution) to refer
to a single one.
the constant z is a id172 factor de   ned as

  a(xa, ya),

(1.2)

z =x

y

x,y

a

which ensures that the distribution sums to 1. the quantity z, considered as a
function of the set f of factors, is called the partition function in the statistical
physics and id114 communities. computing z is intractable in general,
but much work exists on how to approximate it.
graphically, we represent the factorization (1.1) by a factor graph [kschischang
et al., 2001]. a factor graph is a bipartite graph g = (v, f, e) in which a variable
node vs     v is connected to a factor node   a     f if vs is an argument to   a. an
example of a factor graph is shown graphically in figure 1.1 (right). in that    gure,
the circles are variable nodes, and the shaded boxes are factor nodes.
in this chapter, we will assume that each local function has the form

(x

k

)

  a(xa, ya) = exp

  akfak(xa, ya)

,

(1.3)

for some real-valued parameter vector   a, and for some set of feature functions or
su   cient statistics {fak}. this form ensures that the family of distributions over v
parameterized by    is an exponential family. much of the discussion in this chapter
actually applies to exponential families in general.
a directed graphical model, also known as a id110, is based on a directed
graph g = (v, e). a directed model is a family of distributions that factorize as:

p(v|  (v)),

(1.4)

p(y, x) = y

v   v

where   (v) are the parents of v in g. an example of a directed model is shown in
figure 1.1 (left).
we use the term generative model to refer to a directed graphical model in which
the outputs topologically precede the inputs, that is, no x     x can be a parent of
an output y     y . essentially, a generative model is one that directly describes how
the outputs probabilistically    generate    the inputs.

4

an introduction to id49 for relational learning

figure 1.1 the naive bayes classi   er, as a directed model (left), and as a factor
graph (right).

1.2.2 applications of id114

in this section we discuss a few applications of id114 to natural language
processing. although these examples are well-known, they serve both to clarify the
de   nitions in the previous section, and to illustrate some ideas that will arise again
in our discussion of conditional random    elds. we devote special attention to the
hidden markov model (id48), because it is closely related to the linear-chain crf.

1.2.2.1 classi   cation

first we discuss the problem of classi   cation, that is, predicting a single class
variable y given a vector of features x = (x1, x2, . . . , xk). one simple way to
accomplish this is to assume that once the class label is known, all the features
are independent. the resulting classi   er is called the naive bayes classi   er. it is
based on a joint id203 model of the form:

p(y, x) = p(y)

p(xk|y).

(1.5)

ky

k=1

this model can be described by the directed model shown in figure 1.1 (left). we
can also write this model as a factor graph, by de   ning a factor   (y) = p(y), and
a factor   k(y, xk) = p(xk|y) for each feature xk. this factor graph is shown in
figure 1.1 (right).
another well-known classi   er that is naturally represented as a graphical model is
id28 (sometimes known as the maximum id178 classi   er in the nlp
community). in statistics, this classi   er is motivated by the assumption that the log
id203, log p(y|x), of each class is a linear function of x, plus a id172
constant. this leads to the conditional distribution:

exp

  y,jxj

(1.6)

where z(x) = p

1

z(x)

p(y|x) =

y exp{  y +pk

j=1   y,jxj} is a normalizing constant, and   y is a
bias weight that acts like log p(y) in naive bayes. rather than using one vector per
class, as in (1.6), we can use a di   erent notation in which a single set of weights is
shared across all the classes. the trick is to de   ne a set of feature functions that are

           y +

kx

j=1

          ,

xyxy1.2 id114

5

nonzero only for a single class. to do this, the feature functions can be de   ned as
fy0,j(y, x) = 1{y0=y}xj for the feature weights and fy0(y, x) = 1{y0=y} for the bias
weights. now we can use fk to index each feature function fy0,j, and   k to index
its corresponding weight   y0,j. using this notational trick, the id28
model becomes:

p(y|x) =

1

z(x)

exp

  kfk(y, x)

.

(1.7)

)

( kx

k=1

we introduce this notation because it mirrors the usual notation for conditional
random    elds.

1.2.2.2 sequence models

classi   ers predict only a single class variable, but the true power of graphical
models lies in their ability to model many variables that are interdependent. in this
section, we discuss perhaps the simplest form of dependency, in which the output
variables are arranged in a sequence. to motivate this kind of model, we discuss an
application from natural language processing, the task of named-entity recognition
(ner). ner is the problem of identifying and classifying proper names in text,
including locations, such as china; people, such as george bush; and organizations,
such as the united nations. the named-entity recognition task is, given a sentence,
   rst to segment which words are part of entities, and then to classify each entity
by type (person, organization, location, and so on). the challenge of this problem
is that many named entities are too rare to appear even in a large training set, and
therefore the system must identify them based only on context.
one approach to ner is to classify each word independently as one of either
person, location, organization, or other (meaning not an entity). the
problem with this approach is that it assumes that given the input, all of the named-
entity labels are independent. in fact, the named-entity labels of neighboring words
are dependent; for example, while new york is a location, new york times is an
organization.
this independence assumption can be relaxed by arranging the output variables in
a linear chain. this is the approach taken by the hidden markov model (id48)
[rabiner, 1989]. an id48 models a sequence of observations x = {xt}t
t=1 by
assuming that there is an underlying sequence of states y = {yt}t
t=1 drawn from a
   nite state set s. in the named-entity example, each observation xt is the identity
of the word at position t, and each state yt is the named-entity label, that is, one
of the entity types person, location, organization, and other.
to model the joint distribution p(y, x) tractably, an id48 makes two independence
assumptions. first, it assumes that each state depends only on its immediate
predecessor, that is, each state yt is independent of all its ancestors y1, y2, . . . , yt   2
given its previous state yt   1. second, an id48 assumes that each observation
variable xt depends only on the current state yt. with these assumptions, we can

6

an introduction to id49 for relational learning

ty

specify an id48 using three id203 distributions:    rst, the distribution p(y1)
over initial states; second, the transition distribution p(yt|yt   1); and    nally, the
observation distribution p(xt|yt). that is, the joint id203 of a state sequence
y and an observation sequence x factorizes as

p(y, x) =

p(yt|yt   1)p(xt|yt),

(1.8)

t=1

where, to simplify notation, we write the initial state distribution p(y1) as p(y1|y0).
in natural language processing, id48s have been used for sequence labeling tasks
such as part-of-speech tagging, named-entity recognition, and information extrac-
tion.

1.2.3 discriminative and generative models

an important di   erence between naive bayes and id28 is that naive
bayes is generative, meaning that it is based on a model of the joint distribution
p(y, x), while id28 is discriminative, meaning that it is based on
a model of the conditional distribution p(y|x). in this section, we discuss the
di   erences between generative and discriminative modeling, and the advantages of
discriminative modeling for many tasks. for concreteness, we focus on the examples
of naive bayes and id28, but the discussion in this section actually
applies in general to the di   erences between generative models and conditional
random    elds.
the main di   erence is that a conditional distribution p(y|x) does not include a
model of p(x), which is not needed for classi   cation anyway. the di   culty in
modeling p(x) is that it often contains many highly dependent features, which
are di   cult to model. for example, in named-entity recognition, an id48 relies on
only one feature, the word   s identity. but many words, especially proper names, will
not have occurred in the training set, so the word-identity feature is uninformative.
to label unseen words, we would like to exploit other features of a word, such as
its capitalization, its neighboring words, its pre   xes and su   xes, its membership in
predetermined lists of people and locations, and so on.
to include interdependent features in a generative model, we have two choices: en-
hance the model to represent dependencies among the inputs, or make simplifying
independence assumptions, such as the naive bayes assumption. the    rst approach,
enhancing the model, is often di   cult to do while retaining tractability. for exam-
ple, it is hard to imagine how to model the dependence between the capitalization of
a word and its su   xes, nor do we particularly wish to do so, since we always observe
the test sentences anyway. the second approach, adding independence assumptions
among the inputs, is problematic because it can hurt performance. for example,
although the naive bayes classi   er performs surprisingly well in document classi-
   cation, it performs worse on average across a range of applications than logistic
regression [caruana and niculescu-mizil, 2005].

1.2 id114

7

figure 1.2 diagram of the relationship between naive bayes, id28,
id48s, linear-chain crfs, generative models, and general crfs.

furthermore, even when naive bayes has good classi   cation accuracy, its prob-
ability estimates tend to be poor. to understand why, imagine training naive
bayes on a data set in which all the features are repeated, that is, x =
(x1, x1, x2, x2, . . . , xk, xk). this will increase the con   dence of the naive bayes
id203 estimates, even though no new information has been added to the data.
assumptions like naive bayes can be especially problematic when we generalize
to sequence models, because id136 essentially combines evidence from di   erent
parts of the model. if id203 estimates at a local level are overcon   dent, it
might be di   cult to combine them sensibly.
actually, the di   erence in performance between naive bayes and id28
is due only to the fact that the    rst is generative and the second discriminative;
the two classi   ers are, for discrete input, identical in all other respects. naive bayes
and id28 consider the same hypothesis space, in the sense that any
id28 classi   er can be converted into a naive bayes classi   er with the
same decision boundary, and vice versa. another way of saying this is that the naive
bayes model (1.5) de   nes the same family of distributions as the id28
model (1.7), if we interpret it generatively as

exp{p
p
  y,  x exp{p

k   kfk(y, x)}

k   kfk(  y,   x)} .

p(y, x) =

(1.9)

this means that if the naive bayes model (1.5) is trained to maximize the con-
ditional likelihood, we recover the same classi   er as from id28. con-
versely, if the id28 model is interpreted generatively, as in (1.9), and is
trained to maximize the joint likelihood p(y, x), then we recover the same classi   er
as from naive bayes. in the terminology of ng and jordan [2002], naive bayes and
id28 form a generative-discriminative pair.
the principal advantage of discriminative modeling is that it is better suited to

id28id48slinear-chain crfsnaive bayessequencesequenceconditionalconditionalgenerative directed modelsgeneral crfsconditionalgeneralgraphsgeneralgraphs8

an introduction to id49 for relational learning

including rich, overlapping features. to understand this, consider the family of naive
bayes distributions (1.5). this is a family of joint distributions whose conditionals
all take the    id28 form    (1.7). but there are many other joint models,
some with complex dependencies among x, whose conditional distributions also
have the form (1.7). by modeling the conditional distribution directly, we can
remain agnostic about the form of p(x). this may explain why it has been observed
that conditional random    elds tend to be more robust than generative models to
violations of their independence assumptions [la   erty et al., 2001]. simply put,
crfs make independence assumptions among y, but not among x.
another way to make the same point is due to minka [2005]. suppose we have a
generative model pg with parameters   . by de   nition, this takes the form

(1.10)

(1.11)

pg(y, x;   ) = pg(y;   )pg(x|y;   ).

but we could also rewrite pg using bayes rule as

where pg(x;   ) and pg(y|x;   ) are computed by id136, i.e., pg(x;   ) =p

pg(y, x;   ) = pg(x;   )pg(y|x;   ),

and pg(y|x;   ) = pg(y, x;   )/pg(x;   ).
now, compare this generative model to a discriminative model over the same family
p
of joint distributions. to do this, we de   ne a prior p(x) over inputs, such that p(x)
could have arisen from pg with some parameter setting. that is, p(x) = pc(x;   0) =
y pg(y, x|  0). we combine this with a conditional distribution pc(y|x;   ) that
could also have arisen from pg, that is, pc(y|x;   ) = pg(y, x;   )/pg(x;   ). then the
resulting distribution is

y pg(y, x;   )

pc(y, x) = pc(x;   0)pc(y|x;   ).

(1.12)

by comparing (1.11) with (1.12), it can be seen that the conditional approach has
more freedom to    t the data, because it does not require that    =   0. intuitively,
because the parameters    in (1.11) are used in both the input distribution and the
conditional, a good set of parameters must represent both well, potentially at the
cost of trading o    accuracy on p(y|x), the distribution we care about, for accuracy
on p(x), which we care less about.
in this section, we have discussed the relationship between naive bayes and lo-
gistic regression in detail because it mirrors the relationship between id48s and
linear-chain crfs. just as naive bayes and id28 are a generative-
discriminative pair, there is a discriminative analog to id48, and
this analog is a particular type of conditional random    eld, as we explain next. the
analogy between naive bayes, id28, generative models, and conditional
random    elds is depicted in figure 1.2.

1.3 linear-chain id49

9

figure 1.3 graphical model of an id48-like linear-chain crf.

figure 1.4 graphical model of a linear-chain crf in which the transition score
depends on the current observation.

1.3 linear-chain id49

in the previous section, we have seen advantages both to discriminative modeling
and sequence modeling. so it makes sense to combine the two. this yields a linear-
chain crf, which we describe in this section. first, in section 1.3.1, we de   ne linear-
chain crfs, motivating them from id48s. then, we discuss parameter estimation
(section 1.3.2) and id136 (section 1.3.3) in linear-chain crfs.

1.3.1 from id48s to crfs

to motivate our introduction of linear-chain conditional random    elds, we begin
by considering the conditional distribution p(y|x) that follows from the joint
distribution p(y, x) of an id48. the key point is that this conditional distribution
is in fact a conditional random    eld with a particular choice of feature functions.
first, we rewrite the id48 joint (1.8) in a form that is more amenable to general-
ization. this is

         x

t

x

i,j   s

  ij1{yt=i}1{yt   1=j} +x

x

x

i   s

o   o

t

p(y, x) =

1
z

exp

  oi1{yt=i}1{xt=o}

          ,

(1.13)
where    = {  ij,   oi} are the parameters of the distribution, and can be any real
numbers. every id48 can be written in this form, as can be seen simply by setting
  ij = log p(y0 = i|y = j) and so on. because we do not require the parameters to
be log probabilities, we are no longer guaranteed that the distribution sums to 1,
unless we explicitly enforce this by using a id172 constant z. despite this
added    exibility, it can be shown that (1.13) describes exactly the class of id48s
in (1.8); we have added    exibility to the parameterization, but we have not added
any distributions to the family.

. . .. . .yx. . .. . .yx10

an introduction to id49 for relational learning

( kx

k=1

p(y, x) =

1
z

exp

we can write (1.13) more compactly by introducing the concept of feature functions,
just as we did for id28 in (1.7). each feature function has the
form fk(yt, yt   1, xt). in order to duplicate (1.13), there needs to be one feature
fij(y, y0, x) = 1{y=i}1{y0=j} for each transition (i, j) and one feature fio(y, y0, x) =
1{y=i}1{x=o} for each state-observation pair (i, o). then we can write an id48 as:

  kfk(yt, yt   1, xt)

.

(1.14)

again, equation (1.14) de   nes exactly the same family of distributions as (1.13),
and therefore as the original id48 equation (1.8).
the last step is to write the conditional distribution p(y|x) that results from the
id48 (1.14). this is

p

p(y|x) = p(y, x)

y0 p(y0, x)

=

p

exp

k=1   kfk(yt, yt   1, xt)

y0 exp

k=1   kfk(y0

t, y0

t   1, xt)

(1.15)

npk
npk

o

o .

this conditional distribution (1.15) is a linear-chain crf, in particular one that
includes features only for the current word   s identity. but many other linear-chain
crfs use richer features of the input, such as pre   xes and su   xes of the current
word, the identity of surrounding words, and so on. fortunately, this extension
requires little change to our existing notation. we simply allow the feature functions
fk(yt, yt   1, xt) to be more general than indicator functions. this leads to the general
de   nition of linear-chain crfs, which we present now.

de   nition 1.1
let y, x be random vectors,    = {  k}     <k be a parameter vector, and
{fk(y, y0, xt)}k
k=1 be a set of real-valued feature functions. then a linear-chain
conditional random    eld is a distribution p(y|x) that takes the form

( kx
( kx

k=1

p(y|x) =

1

z(x)

exp

z(x) =x

  kfk(yt, yt   1, xt)

,

(1.16)

where z(x) is an instance-speci   c id172 function

exp

  kfk(yt, yt   1, xt)

.

(1.17)

y

k=1

we have just seen that if the joint p(y, x) factorizes as an id48, then the associated
conditional distribution p(y|x) is a linear-chain crf. this id48-like crf is
pictured in figure 1.3. other types of linear-chain crfs are also useful, however.
for example, in an id48, a transition from state i to state j receives the same
score, log p(yt = j|yt   1 = i), regardless of the input. in a crf, we can allow the
score of the transition (i, j) to depend on the current observation vector, simply

)

)

)

1.3 linear-chain id49

11

by adding a feature 1{yt=j}1{yt   1=1}1{xt=o}. a crf with this kind of transition
feature, which is commonly used in text applications, is pictured in figure 1.4.
to indicate in the de   nition of linear-chain crf that each feature function can
depend on observations from any time step, we have written the observation
argument to fk as a vector xt, which should be understood as containing all the
components of the global observations x that are needed for computing features
at time t. for example, if the crf uses the next word xt+1 as a feature, then the
feature vector xt is assumed to include the identity of word xt+1.
finally, note that the id172 constant z(x) sums over all possible state
sequences, an exponentially large number of terms. nevertheless, it can be computed
e   ciently by forward-backward, as we explain in section 1.3.3.

nx

1 , x(i)

2 , . . . x(i)

t } is a sequence of inputs, and each y(i) = {y(i)

1.3.2 parameter estimation
in this section we discuss how to estimate the parameters    = {  k} of a linear-
chain crf. we are given iid training data d = {x(i), y(i)}n
i=1, where each x(i) =
t } is
{x(i)
a sequence of the desired predictions. thus, we have relaxed the iid assumption
within each sequence, but we still assume that distinct sequences are independent.
(in section 1.4, we will see how to relax this assumption as well.)
parameter estimation is typically performed by penalized maximum likelihood.
because we are modeling the conditional distribution, the following log likelihood,
sometimes called the conditional log likelihood, is appropriate:

2 , . . . y(i)

1 , y(i)

   (  ) =

log p(y(i)|x(i)).

(1.18)

one way to understand the conditional likelihood p(y|x;   ) is to imagine combining
it with some arbitrary prior p(x;   0) to form a joint p(y, x). then when we optimize
the joint log likelihood

i=1

log p(y, x) = log p(y|x;   ) + log p(x;   0),

(1.19)
the two terms on the right-hand side are decoupled, that is, the value of   0 does
not a   ect the optimization over   . if we do not need to estimate p(x), then we can
simply drop the second term, which leaves (1.18).
after substituting in the crf model (1.16) into the likelihood (1.18), we get the
following expression:

nx

tx

kx

t )     nx

   (  ) =

  kfk(y(i)
t

, y(i)

t   1, x(i)

log z(x(i)),

(1.20)

i=1

t=1

k=1

i=1

before we discuss how to optimize this, we mention id173. it is often the
case that we have a large number of parameters. as a measure to avoid over   tting,
we use id173, which is a penalty on weight vectors whose norm is too

12

an introduction to id49 for relational learning

large. a common choice of penalty is based on the euclidean norm of    and on a
id173 parameter 1/2  2 that determines the strength of the penalty. then
the regularized log likelihood is

nx

tx

kx

t )     nx

log z(x(i))     kx

   (  ) =

  kfk(y(i)
t

, y(i)

t   1, x(i)

i=1

t=1

k=1

i=1

k=1

  2
k
2  2 .

(1.21)

the notation for the regularizer is intended to suggest that id173 can also
be viewed as performing maximum a posteriori estimation of   , if    is assigned
a gaussian prior with mean 0 and covariance   2i. the parameter   2 is a free
parameter which determines how much to penalize large weights. determining the
best id173 parameter can require a computationally-intensive parameter
sweep. fortunately, often the accuracy of the    nal model does not appear to
be sensitive to changes in   2, even when   2 is varied up to a factor of 10. an
alternative choice of id173 is to use the    1 norm instead of the euclidean
norm, which corresponds to an exponential prior on parameters [goodman, 2004].
this regularizer tends to encourage sparsity in the learned parameters.
in general, the function    (  ) cannot be maximized in closed form, so numerical
optimization is used. the partial derivatives of (1.21) are

nx

tx

i=1

t=1

      
     k

=

fk(y(i)
t

, y(i)

t   1, x(i)

tx

x

t=1

y,y0

t )p(y, y0|x(i))     kx

fk(y, y0, x(i)

  k
  2 .
(1.22)

k=1

the    rst term is the expected value of fk under the empirical distribution:

  p(y, x) =

1{y=y(i)}1{x=x(i)}.

(1.23)

from the convexity of functions of the form g(x) = logp

the second term, which arises from the derivative of log z(x), is the expectation
of fk under the model distribution p(y|x;   )  p(x). therefore, at the unregularized
maximum likelihood solution, when the gradient is zero, these two expectations are
equal. this pleasing interpretation is a standard result about maximum likelihood
estimation in exponential families.
now we discuss how to optimize    (  ). the function    (  ) is concave, which follows
i exp xi. convexity is
extremely helpful for parameter estimation, because it means that every local
optimum is also a global optimum. adding id173 ensures that     is strictly
concave, which implies that it has exactly one global optimum.
perhaps the simplest approach to optimize     is steepest ascent along the gradient
(1.22), but this requires too many iterations to be practical. newton   s method
converges much faster because it takes into account the curvature of the likelihood,
but it requires computing the hessian, the matrix of all second derivatives. the size
of the hessian is quadratic in the number of parameters. since practical applications
often use tens of thousands or even millions of parameters, even storing the full
hessian is not practical.

t )     nx
nx

i=1

1
n

i=1

1.3 linear-chain id49

13

instead, current techniques for optimizing (1.21) make approximate use of second-
order information. particularly successful have been quasi-id77s such
as bfgs [bertsekas, 1999], which compute an approximation to the hessian from
only the    rst derivative of the objective function. a full k    k approximation to
the hessian still requires quadratic size, however, so a limited-memory version of
bfgs is used, due to byrd et al. [1994]. as an alternative to limited-memory bfgs,
conjugate gradient is another optimization technique that also makes approximate
use of second-order information and has been used successfully with crfs. either
can be thought of as a black-box optimization routine that is a drop-in replacement
for vanilla gradient ascent. when such second-order methods are used, gradient-
based optimization is much faster than the original approaches based on iterative
scaling in la   erty et al. [2001], as shown experimentally by several authors [sha
and pereira, 2003, wallach, 2002, malouf, 2002, minka, 2003].
finally, it is important to remark on the computational cost of training. both the
partition function z(x) in the likelihood and the marginal distributions p(yt, yt   1|x)
in the gradient can be computed by forward-backward, which uses computational
complexity o(t m 2). however, each training instance will have a di   erent partition
function and marginals, so we need to run forward-backward for each training
instance for each gradient computation, for a total training cost of o(t m 2n g),
where n is the number of training examples, and g the number of gradient
computations required by the optimization procedure. for many data sets, this
cost is reasonable, but if the number of states is large, or the number of training
sequences is very large, then this can become expensive. for example, on a standard
named-entity data set, with 11 labels and 200,000 words of training data, crf
training    nishes in under two hours on current hardware. however, on a part-of-
speech tagging data set, with 45 labels and one million words of training data, crf
training requires over a week.

1.3.3

id136

there are two common id136 problems for crfs. first, during training, com-
puting the gradient requires marginal distributions for each edge p(yt, yt   1|x), and
computing the likelihood requires z(x). second, to label an unseen instance, we
compute the most likely (viterbi) labeling y    = arg maxy p(y|x). in linear-chain
crfs, both id136 tasks can be performed e   ciently and exactly by variants
of the standard dynamic-programming algorithms for id48s. in this section, we
brie   y review the id48 algorithms, and extend them to linear-chain crfs. these
standard id136 algorithms are described in more detail by rabiner [1989].
first, we introduce notation which will simplify the forward-backward recursions.
t   t(yt, yt   1, xt) where z = 1,

an id48 can be viewed as a factor graph p(y, x) =q

and the factors are de   ned as:

  t(j, i, x) def= p(yt = j|yt   1 = i)p(xt = x|yt = j).

(1.24)

14

an introduction to id49 for relational learning

if the id48 is viewed as a weighted    nite state machine, then   t(j, i, x) is the
weight on the transition from state i to state j when the current observation is x.
now, we review the id48 forward algorithm, which is used to compute the
id203 p(x) of the observations. the idea behind forward-backward is to    rst

y p(x, y) using the distributive law:

rewrite the naive summation p(x) =p
p(x) =x
  t(yt, yt   1, xt)x
=x

ty
x

  t(yt, yt   1, xt)

t=1

y

  t   1(yt   1, yt   2, xt   1)x

      

(1.25)

(1.26)

yt

yt   1

yt   2

yt   3

now we observe that each of the intermediate sums is reused many times during
the computation of the outer sum, and so we can save an exponential amount of
work by caching the inner sums.
this leads to de   ning a set of forward variables   t, each of which is a vector of size
m (where m is the number of states) which stores one of the intermediate sums.
these are de   ned as:

  t(j) def= p(xh1...ti, yt = j)

= x

yh1...t   1i

  t(j, yt   1, xt)

t   1y

t0=1

  t0(yt0 , yt0   1, xt0),

(1.27)

(1.28)

where the summation over yh1...t   1i ranges over all assignments to the sequence
of random variables y1, y2, . . . , yt   1. the alpha values can be computed by the
recursion

  t(j) =x
the id48.) it is easy to see that p(x) =p

i   s

with initialization   1(j) =   1(j, y0, x1). (recall that y0 is the    xed initial state of
yt   t(yt) by repeatedly substituting the

recursion (1.29) to obtain (1.26). a formal proof would use induction.
the backward recursion is exactly the same, except that in (1.26), we push in the
summations in reverse order. this results in the de   nition

  t(j, i, xt)  t   1(i),

(1.29)

and the recursion

  t(i) def= p(xht+1...ti|yt = i)

ty

  t0(yt0 , yt0   1, xt0),

yht+1...ti

t0=t+1

= x
  t(i) =x

j   s

  t+1(j, i, xt+1)  t+1(j),

(1.30)

(1.31)

(1.32)

which is initialized   t(i) = 1. analogously to the forward case, we can compute

p(x) using the backward variables as p(x) =   0(y0) def= p

y1   1(y1, y0, x1)  1(y1).

1.3 linear-chain id49

15

by combining results from the forward and backward recursions, we can compute
the marginal distributions needed for the gradient (1.22). applying the distributive
law again, we see that

p(yt   1, yt|x) =   t(yt, yt   1, xt)

      

       x

yh1...t   2i

t   1y
       x

t0=1

  t0(yt0 , yt0   1, xt0)

ty

yht+1...ti

t0=t+1

       ,

  t0(yt0 , yt0   1, xt0)

(1.33)

which can be computed from the forward and backward recursions as

p(yt   1, yt|x)       t   1(yt   1)  t(yt, yt   1, xt)  t(yt).

(1.34)
finally, to compute the globally most probable assignment y    = arg maxy p(y|x),
we observe that the trick in (1.26) still works if all the summations are replaced by
maximization. this yields the viterbi recursion:

  t(j) = max
i   s

  t(j, i, xt)  t   1(i)

(1.35)

now that we have described the forward-backward and viterbi algorithms for
id48s, the generalization to linear-chain crfs is fairly straightforward. the
forward-backward algorithm for linear-chain crfs is identical to the id48 version,
except that the transition weights   t(j, i, xt) are de   ned di   erently. we observe that
the crf model (1.16) can be rewritten as:

where we de   ne

p(y|x) =

1

z(x)

  t(yt, yt   1, xt) = exp

  t(yt, yt   1, xt),

(1.36)

)

  kfk(yt, yt   1, xt)

.

(1.37)

k

with that de   nition, the forward recursion (1.29), the backward recursion (1.32),
and the viterbi recursion (1.35) can be used unchanged for linear-chain crfs.
instead of computing p(x) as in an id48, in a crf the forward and backward
recursions compute z(x).
a    nal id136 task that is useful in some applications is to compute a marginal
id203 p(yt, yt+1, . . . yt+k|x) over a range of nodes. for example, this is useful
for measuring the model   s con   dence in its predicted labeling over a segment of
input. this marginal id203 can be computed e   ciently using constrained
forward-backward, as described by culotta and mccallum [2004].

ty
(x

t=1

16

an introduction to id49 for relational learning

1.4 crfs in general

in this section, we de   ne crfs with general graphical structure, as they were
introduced originally [la   erty et al., 2001]. although initial applications of crfs
used linear chains, there have been many later applications of crfs with more
general graphical structures. such structures are especially useful for relational
learning, because they allow relaxing the iid assumption among entities. also,
although crfs have typically been used for across-network classi   cation, in which
the training and testing data are assumed to be independent, we will see that crfs
can be used for within-network classi   cation as well, in which we model probabilistic
dependencies between the training and testing data.
the generalization from linear-chain crfs to general crfs is fairly straightfor-
ward. we simply move from using a linear-chain factor graph to a more general
factor graph, and from forward-backward to more general (perhaps approximate)
id136 algorithms.

1.4.1 model

first we present the general de   nition of a conditional random    eld.

de   nition 1.2
let g be a factor graph over y . then p(y|x) is a conditional random    eld if for
any    xed x, the distribution p(y|x) factorizes according to g.
thus, every conditional distribution p(y|x) is a crf for some, perhaps trivial,
factor graph. if f = {  a} is the set of factors in g, and each factor takes the
exponential family form (1.3), then the conditional distribution can be written as

y

  a   g

         k(a)x

k=1

p(y|x) =

1

z(x)

          .

exp

  akfak(ya, xa)

(1.38)

in addition, practical models rely extensively on parameter tying. for exam-
ple,
in the linear-chain case, often the same weights are used for the factors
  t(yt, yt   1, xt) at each time step. to denote this, we partition the factors of g
into c = {c1, c2, . . . cp}, where each cp is a clique template whose parameters are
tied. this notion of clique template generalizes that in taskar et al. [2002], sutton
et al. [2004], and richardson and domingos [2005]. each clique template cp is a
set of factors which has a corresponding set of su   cient statistics {fpk(xp, yp)} and
parameters   p     <k(p). then the crf can be written as

p(y|x) =

1

z(x)

y

y

cp   c

  c   cp

  c(xc, yc;   p),

(1.39)

1.4 crfs in general

where each factor is parameterized as

  c(xc, yc;   p) = exp

          ,

  pkfpk(xc, yc)

         k(p)x
y

k=1

17

(1.40)

(1.41)

and the id172 function is

z(x) =x

y

y

cp   c

  c   cp

  c(xc, yc;   p).

t=1 is used for the entire network.

for example, in a linear-chain conditional random    eld, typically one clique tem-
plate c = {  t(yt, yt   1, xt)}t
several special cases of conditional random    elds are of particular interest. first,
dynamic conditional random    elds [sutton et al., 2004] are sequence models which
allow multiple labels at each time step, rather than single labels as in linear-chain
crfs. second, relational markov networks [taskar et al., 2002] are a type of general
crf in which the graphical structure and parameter tying are determined by an
sql-like syntax. finally, markov logic networks [richardson and domingos, 2005,
singla and domingos, 2005] are a type of probabilistic logic in which there are
parameters for each    rst-order rule in a knowledge base.

1.4.2 applications of crfs

crfs have been applied to a variety of domains, including text processing, com-
puter vision, and bioinformatics. in this section, we discuss several applications,
highlighting the di   erent graphical structures that occur in the literature.
one of the    rst large-scale applications of crfs was by sha and pereira [2003], who
matched state-of-the-art performance on segmenting noun phrases in text. since
then, linear-chain crfs have been applied to many problems in natural language
processing, including named-entity recognition [mccallum and li, 2003], feature
induction for ner [mccallum, 2003], identifying protein names in biology abstracts
[settles, 2005], segmenting addresses in web pages [culotta et al., 2004],    nding
semantic roles in text [roth and yih, 2005], identifying the sources of opinions [choi
et al., 2005], chinese id40 [peng et al., 2004], japanese morphological
analysis [kudo et al., 2004], and many others.
in bioinformatics, crfs have been applied to rna structural alignment [sato and
sakakibara, 2005] and protein structure prediction [liu et al., 2005]. semi-markov
crfs [sarawagi and cohen, 2005] add somewhat more    exibility in choosing
features, which may be useful for certain tasks in information extraction and
especially bioinformatics.
general crfs have also been applied to several tasks in nlp. one promising
application is to performing multiple labeling tasks simultaneously. for example,
sutton et al. [2004] show that a two-level dynamic crf for part-of-speech tagging
and noun-phrase chunking performs better than solving the tasks one at a time.
another application is to multi-label classi   cation, in which each instance can

18

an introduction to id49 for relational learning

have multiple class labels. rather than learning an independent classi   er for each
category, ghamrawi and mccallum [2005] present a crf that learns dependencies
between the categories, resulting in improved classi   cation performance. finally, the
skip-chain crf, which we present in section 1.5, is a general crf that represents
long-distance dependencies in information extraction.
an interesting graphical crf structure has been applied to the problem of proper-
noun coreference, that is, of determining which mentions in a document, such as
mr. president and he, refer to the same underlying entity. mccallum and wellner
[2005] learn a distance metric between mentions using a fully-connected conditional
random    eld in which id136 corresponds to graph partitioning. a similar model
has been used to segment handwritten characters and diagrams [cowans and
szummer, 2005, qi et al., 2005].
in some applications of crfs, e   cient dynamic programs exist even though the
graphical model is di   cult to specify. for example, mccallum et al. [2005] learn
the parameters of a string-edit model in order to discriminate between matching
and nonmatching pairs of strings. also, there is work on using crfs to learn
distributions over the derivations of a grammar [riezler et al., 2002, clark and
curran, 2004, sutton, 2004, viola and narasimhan, 2005]. a potentially useful
unifying framework for this type of model is provided by case-factor diagrams
[mcallester et al., 2004].
in copmputer vision, several authors have used grid-shaped crfs [he et al., 2004,
kumar and hebert, 2003] for labeling and segmenting images. also, for recognizing
objects, quattoni et al. [2005] use a tree-shaped crf in which latent variables are
designed to recognize characteristic parts of an object.

1.4.3 parameter estimation

parameter estimation for general crfs is essentially the same as for linear-chains,
except that computing the model expectations requires more general id136
algorithms. first, we discuss the fully-observed case, in which the training and
testing data are independent, and the training data is fully observed. in this case
the conditional log likelihood is given by

   (  ) = x

x

k(p)x

cp   c

  c   cp

k=1

  pkfpk(xc, yc)     log z(x).

(1.42)

it is worth noting that the equations in this section do not explicitly sum over
training instances, because if a particular application happens to have iid training
instances, they can be represented by disconnected components in the graph g.
the partial derivative of the log likelihood with respect to a parameter   pk associ-
ated with a clique template cp is

= x

  c   cp

fpk(xc, yc)     x

x

  c   cp

y0

c

      
     pk

fpk(xc, y0

c)p(y0

c|x).

(1.43)

1.4 crfs in general

19

  c

   cient statistics fpk(x, y) = p

the function    (  ) has many of the same properties as in the linear-chain case.
first, the zero-gradient conditions can be interpreted as requiring that the suf-
fpk(xc, yc) have the same expectations under
the empirical distribution and under the model distribution. second, the function
   (  ) is concave, and can be e   ciently maximized by second-order techniques such
as conjugate gradient and l-bfgs. finally, id173 is used just as in the
linear-chain case.
now, we discuss the case of within-network classi   cation, where there are depen-
dencies between the training and testing data. that is, the random variables y are
partitioned into a set ytr that is observed during training and a set ytst that is
unobserved during training. it is assumed that the graph g contains connections
between ytr and ytst.
within-network classi   cation can be viewed as a kind of latent variable problem,
in which certain variables, in this case ytst, are not observed in the training data.
it is more di   cult to train crfs with latent variables, because optimizing the
likelihood p(ytr|x) requires marginalizing out the latent variables ytst. because of
this di   cultly, the original work on crfs focused on fully-observed training data,
but recently there has been increasing interest in training latent-variable crfs
[quattoni et al., 2005, mccallum et al., 2005].
suppose we have a conditional random    eld with inputs x in which the output
variables y are observed in the training data, but we have additional variables w
that are latent, so that the crf has the form

  c(xc, wc, yc;   p).

(1.44)

p(y, w|x) =

1

y

y
   (  ) = log p(y|x) = logx

  c   cp

z(x)

cp   c

the objective function to maximize during training is the marginal likelihood

p(y, w|x).

(1.45)

w

realize that we need to compute logp

the    rst question is how even to compute the marginal likelihood    (  ), because if
there are many variables w, the sum cannot be computed directly. the key is to
w p(y, w|x) not for any possible assignment
y, but only for the particular assignment that occurs in the training data. this
motivates taking the original crf (1.44), and clamping the variables y to their
y
observed values in the training data, yielding a distribution over w:
y

where the id172 factor is

  c(xc, wc, yc;   p),

p(w|y, x) =

z(y, x) =x

  c(xc, wc, yc;   p).

y

y

z(y, x)

  c   cp

(1.46)

(1.47)

cp   c

1

w

cp   c

  c   cp

this new id172 constant z(y, x) can be computed by the same id136

20

an introduction to id49 for relational learning

algorithm that we use to compute z(x). in fact, z(y, x) is easier to compute,
because it sums only over w, while z(x) sums over both w and y. graphically, this
amounts to saying that clamping the variables y in the graph g can simplify the
structure among w.
once we have z(y, x), the marginal likelihood can be computed as

p(y|x) =

1

z(x)

  c(xc, wc, yc;   p) = z(y, x)
z(x) .

(1.48)

x

y

y

w

cp   c

  c   cp

now that we have a way to compute    , we discuss how to maximize it with respect
to   . maximizing    (  ) can be di   cult because     is no longer convex in general
(intuitively, log-sum-exp is convex, but the di   erence of two log-sum-exp functions
might not be), so optimization procedures are typically guaranteed to    nd only local
maxima. whatever optimization technique is used, the model parameters must be
carefully initialized in order to reach a good local maximum.
we discuss two di   erent ways to maximize    : directly using the gradient, as in
quattoni et al. [2005]; and using em, as in mccallum et al. [2005]. to maximize    
directly, we need to calculate its gradient. the simplest way to do this is to use the
following fact. for any function f(  ), we have

df
d  

= f(  ) d log f
d  

,

which can be seen by applying the chain rule to log f and rearranging. applying

this to the marginal likelihood    (  ) = logp
w p(y, w|x) yields
x

   

      
     pk

=

1p
=x
w p(y, w|x)
p(w|y, x)    
     pk

w

w

(cid:2)p(y, w|x)(cid:3)
(cid:2) log p(y, w|x)(cid:3).

     pk

(1.49)

(1.50)

(1.51)

this is the expectation of the fully-observed gradient, where the expectation is
taken over w. this expression simpli   es to
c|y, x)fk(yc, xc, w0

c)     x

= x

c|xc)fk(y0

c, xc, w0
c).

x

x

p(w0

p(w0

c, y0

      
     pk

  c   cp

w0

c

  c   cp

w0

c,y0

c

(1.52)

this gradient requires computing two di   erent kinds of marginal probabilities.
c|y, x), which is exactly a
the    rst term contains a marginal id203 p(w0
marginal distribution of the clamped crf (1.46). the second term contains a
c|xc), which is the same marginal id203 required
di   erent marginal p(w0
in a fully-observed crf. once we have computed the gradient,     can be maximized
by standard techniques such as conjugate gradient. in our experience, conjugate
gradient tolerates violations of convexity better than limited-memory bfgs, so it
may be a better choice for latent-variable crfs.
alternatively,     can be optimized using expectation maximization (em). at each

c, y0

1.4 crfs in general

21

iteration j in the em algorithm, the current parameter vector   (j) is updated as
follows. first, in the e-step, an auxiliary function q(w) is computed as q(w) =
p(w|y, x;   (j)). second, in the m-step, a new parameter vector   (j+1) is chosen as

  (j+1) = arg max

  0

q(w0) log p(y, w0|x;   0).

(1.53)

x

w0

the direct maximization algorithm and the em algorithm are strikingly similar.
this can be seen by substituting the de   nition of q into (1.53) and taking deriva-
tives. the gradient is almost identical to the direct gradient (1.52). the only dif-
ference is that in em, the distribution p(w|y, x) is obtained from a previous,    xed
parameter setting rather than from the argument of the maximization. we are un-
aware of any empirical comparison of em to direct optimization for latent-variable
crfs.

1.4.4

id136

in general crfs, just as in the linear-chain case, gradient-based training requires
computing marginal distributions p(yc|x), and testing requires computing the most
likely assignment y    = arg maxy p(y|x). this can be accomplished using any
id136 algorithm for id114. if the graph has small treewidth, then the
junction tree algorithm can be used to exactly compute the marginals, but because
both id136 problems are np-hard for general graphs, this is not always possible.
in such cases, approximate id136 must be used to compute the gradient. in this
section, we mention various approximate id136 algorithms that have been used
successfully with crfs. detailed discussion of these are beyond the scope of this
tutorial.
when choosing an id136 algorithm to use within crf training, the important
thing to understand is that it will be invoked repeatedly, once for each time that
the gradient is computed. for this reason, sampling-based approaches which may
take many iterations to converge, such as id115, have not
been popular, although they might be appropriate in some circumstances. indeed,
contrastive divergence [hinton, 2000], in which an mcmc sampler is run for only
a few samples, has been successfully applied to crfs in vision [he et al., 2004].
because of their computational e   ciency, variational approaches have been most
popular for crfs. several authors [taskar et al., 2002, sutton et al., 2004] have
used loopy belief propagation. belief propagation is an exact id136 algorithm for
trees which generalizes the forward-backward. although the generalization of the
forward-backward recursions, which are called message updates, are neither exact
nor even guaranteed to converge if the model is not a tree, they are still well-de   ned,
and they have been empirically successful in a wide variety of domains, including
text processing, vision, and error-correcting codes. in the past    ve years, there has
been much theoretical analysis of the algorithm as well. we refer the reader to
yedidia et al. [2004] for more information.

22

an introduction to id49 for relational learning

1.4.5 discussion

this section contains miscellaneous remarks about crfs. first, it is easily seen
that id28 model (1.7) is a conditional random    eld with a single
output variable. thus, crfs can be viewed as an extension of id28
to arbitrary graphical structures.
although we have emphasized the view of a crf as a model of the conditional
distribution, one could view it as an objective function for parameter estimation of
joint distributions. as such, it is one objective among many, including generative
likelihood, pseudolikelihood [besag, 1977], and the maximum-margin objective
[taskar et al., 2004, altun et al., 2003]. another related discriminative technique for
structured models is the averaged id88, which has been especially popular in
the natural language community [collins, 2002], in large part because of its ease of
implementation. to date, there has been little careful comparison of these, especially
crfs and max-margin approaches, across di   erent structures and domains.
given this view, it is natural to imagine training directed models by conditional
likelihood, and in fact this is commonly done in the speech community, where it is
called maximum mutual information training. however, it is no easier to maximize
the conditional likelihood in a directed model than an undirected model, because in
a directed model the conditional likelihood requires computing log p(x), which plays
the same role as z(x) in the crf likelihood. in fact, training is more complex in a
directed model, because the model parameters are constrained to be probabilities   
constraints which can make the optimization problem more di   cult. this is in stark
contrast to the joint likelihood, which is much easier to compute for directed models
than undirected models (although recently several e   cient parameter estimation
techniques have been proposed for undirected factor graphs, such as abbeel et al.
[2005] and wainwright et al. [2003]).

1.4.6

implementation concerns

there are a few implementation techniques that can help both training time and
accuracy of crfs, but are not always fully discussed in the literature. although
these apply especially to language applications, they are also useful more generally.
first, when the predicted variables are discrete, the features fpk are ordinarily
chosen to have a particular form:

fpk(yc, xc) = 1{yc=  yc}qpk(xc).

(1.54)

in other words, each feature is nonzero only for a single output con   guration   yc, but
as long as that constraint is met, then the feature value depends only on the input
observation. essentially, this means that we can think of our features as depending
only on the input xc, but that we have a separate set of weights for each output
con   guration. this feature representation is also computationally e   cient, because
computing each qpk may involve nontrivial text or image processing, and it need be

1.4 crfs in general

23

evaluated only once for every feature that uses it. to avoid confusion, we refer to
the functions qpk(xc) as observation functions rather than as features. examples of
observation functions are    word xt is capitalized    and    word xt ends in ing   .
this representation can lead to a large number of features, which can have signi   -
cant memory and time requirements. for example, to match state-of-the-art results
on a standard natural language task, sha and pereira [2003] use 3.8 million features.
not all of these features are ever nonzero in the training data. in particular, some
observation functions qpk are nonzero only for certain output con   gurations. this
point can be confusing: one might think that such features can have no e   ect on
the likelihood, but actually they do a   ect z(x), so putting a negative weight on
them can improve the likelihood by making wrong answers less likely. in order to
save memory, however, sometimes these unsupported features, that is, those which
never occur in the training data, are removed from the model. in practice, however,
including unsupported features typically results in better accuracy.
in order to get the bene   ts of unsupported features with less memory, we have had
success with an ad hoc technique for selecting only a few unsupported features. the
main idea is to add unsupported features only for likely paths, as follows:    rst train
a crf without any unsupported features, stopping after only a few iterations; then
add unsupported features fpk(yc, xc) for cases where xc occurs in the training data,
and p(yc|x) >  . mccallum [2003] presents a more principled method of feature
selection for crfs.
second, if the observations are categorical rather than ordinal, that is, if they are
discrete but have no intrinsic order, it is important to convert them to binary
features. for example, it makes sense to learn a linear weight on fk(y, xt) when fk
is 1 if xt is the word dog and 0 otherwise, but not when fk is the integer index
of word xt in the text   s vocabulary. thus, in text applications, crf features are
typically binary; in other application areas, such as vision and speech, they are
more commonly real-valued.
third, in language applications, it is sometimes helpful to include redundant factors
in the model. for example, in a linear-chain crf, one may choose to include both
edge factors   t(yt, yt   1, xt) and variable factors   t(yt, xt). although one could
de   ne the same family of distributions using only edge factors, the redundant node
factors provide a kind of backo   , which is useful when there is too little data.
in language applications, there is always too little data, even when hundreds of
thousands of words are available.
finally, often the probabilities involved in forward-backward and belief propagation
become too small to be represented within numerical precision. there are two
standard approaches to this common problem. one approach is to normalize each
of the vectors   t and   t to sum to 1, thereby magnifying small values. a second
approach is to perform computations in the logarithmic domain, e.g., the forward
recursion becomes

log   t(j) =m

(cid:0) log   t(j, i, xt) + log   t   1(i)(cid:1),

(1.55)

i   s

24

an introduction to id49 for relational learning

where     is the operator a     b = log(ea + eb). at    rst, this does not seem much of
an improvement, since numerical precision is lost when computing ea and eb. but
    can be computed as

a     b = a + log(1 + eb   a) = b + log(1 + ea   b),

(1.56)

which can be much more numerically stable, particularly if we pick the version of
the identity with the smaller exponent. crf implementations often use the log-
space approach because it makes computing z(x) more convenient, but in some
applications, the computational expense of taking logarithms is an issue, making
id172 preferable.

1.5 skip-chain crfs

in this section, we present a case study of applying a general crf to a practical
natural language problem. in particular, we consider a problem in information
extraction, the task of building a database automatically from unstructured text.
recent work in extraction has often used sequence models, such as id48s and
linear-chain crfs, which model dependencies only between neighboring labels, on
the assumption that those dependencies are the strongest.
but sometimes it is important to model certain kinds of long-range dependencies
between entities. one important kind of dependency within information extraction
occurs on repeated mentions of the same    eld. when the same entity is mentioned
more than once in a document, such as robert booth, in many cases all mentions
have the same label, such as seminar-speaker. we can take advantage of this
fact by favoring labelings that treat repeated words identically, and by combining
features from all occurrences so that the extraction decision can be made based on
global information. furthermore, identifying all mentions of an entity can be useful
in itself, because each mention might contain di   erent useful information. however,
most extraction systems, whether probabilistic or not, do not take advantage of
this dependency, instead treating the separate mentions independently.
to perform collective labeling, we need to represent dependencies between distant
terms in the input. but this reveals a general limitation of sequence models,
whether generatively or discriminatively trained. sequence models make a markov
assumption among labels, that is, that any label yt is independent of all previous
labels given its immediate predecessors yt   k . . . yt   1. this represents dependence
only between nearby nodes   for example, between bigrams and trigrams   and
cannot represent the higher-order dependencies that arise when identical words
occur throughout a document.
to relax this assumption, we introduce the skip-chain crf, a conditional model
that collectively segments a document into mentions and classi   es the mentions by
entity type, while taking into account probabilistic dependencies between distant
mentions. these dependencies are represented in a skip-chain model by augmenting

1.5 skip-chain crfs

25

figure 1.5 graphical representation of a skip-chain crf. identical words are
connected because they are likely to have the same label.

wt = w
wt matches [a-z][a-z]+
wt matches [a-z][a-z]+
wt matches [a-z]
wt matches [a-z]+
wt matches [a-z]+[a-z]+[a-z]+[a-z]
wt appears in list of    rst names,

last names, honori   cs, etc.

wt appears to be part of a time followed by a dash
wt appears to be part of a time preceded by a dash
wt appears to be part of a date
tt = t
qk(x, t +   ) for all k and        [   4, 4]

table 1.1 input features qk(x, t) for the seminars data. in the above wt is the word
at position t, tt is the pos tag at position t, w ranges over all words in the training
data, and t ranges over all part-of-speech tags returned by the brill tagger. the
   appears to be    features are based on hand-designed id157 that can
span several tokens.

a linear-chain crf with factors that depend on the labels of distant but similar
words. this is shown graphically in figure 1.5.
even though the limitations of id165 models have been widely recognized within
natural language processing, long-distance dependencies are di   cult to represent
in generative models, because full id165 models have too many parameters if n
is large. we avoid this problem by selecting which skip edges to include based on
the input string. this kind of input-speci   c dependence is di   cult to represent in
a generative model, because it makes generating the input more complicated. in
other words, conditional models have been popular because of their    exibility in
allowing overlapping features; skip-chain crfs take advantage of their    exibility
in allowing input-speci   c model structure.

xtxt+1xt-1ytyt+1yt-1johngreensenatorxt+101yt+101xt+100yt+100......green.xt+101yt+101ran......26

an introduction to id49 for relational learning

1.5.1 model

the skip-chain crf is essentially a linear-chain crf with additional long-distance
edges between similar words. we call these additional edges skip edges. the features
on skip edges can incorporate information from the context of both endpoints, so
that strong evidence at one endpoint can in   uence the label at the other endpoint.
when applying the skip-chain model, we must choose which skip edges to include.
the simplest choice is to connect all pairs of identical words, but more generally we
can connect any pair of words that we believe to be similar, for example, pairs of
words that belong to the same stem class, or have small id153. in addition,
we must be careful not to include too many skip edges, because this could result
in a graph that makes approximate id136 di   cult. so we need to use similarity
metrics that result in a su   ciently sparse graph. in the experiments below, we focus
on named-entity recognition, so we connect pairs of identical capitalized words.
formally, the skip-chain crf is de   ned as a general crf with two clique templates:
one for the linear-chain portion, and one for the skip edges. for an sentence x, let
i = {(u, v)} be the set of all pairs of sequence positions for which there are skip
edges. for example, in the experiments reported here, i is the set of indices of all
pairs of identical capitalized words. then the id203 of a label sequence y given
an input x is modeled as

p  (y|x) =

1

z(x)

  uv(yu, yv, x),

(1.57)

where   t are the factors for linear-chain edges, and   uv are the factors over skip
edges. these factors are de   ned as

ty

t=1

(u,v)   i

  t(yt, yt   1, x) y
(x
(x

k

)
)

(1.58)

(1.59)

,

  uv(yu, yv, x) = exp

  2kf2k(yu, yv, x, u, v)

k

k=1 are the parameters of the linear-chain template, and   2 =
k=1 are the parameters of the skip template. the full set of model parameters

where   1 = {  1k}k1
{  2k}k2
are    = {  1,   2}.
as described in section 1.4.6, both the linear-chain features and skip-chain features
are factorized into indicator functions of the outputs and observation functions,
as in (1.54). in general the observation functions qk(x, t) can depend on arbitrary
positions of the input string. for example, a useful feature for ner is qk(x, t) = 1
if and only if xt+1 is a capitalized word.

  t(yt, yt   1, x) = exp

  1kf1k(yt, yt   1, x, t)

1.5 skip-chain crfs

27

system
bien peshkin and pfe   er [2003]
linear-chain crf
skip-chain crf

stime

etime

location

speaker

overall

96.0
97.5
96.7

98.8
97.5
97.2

87.1
88.3
88.1

76.9
77.3
80.4

89.7
90.2
90.6

table 1.2 comparison of f1 performance on the seminars data. the top line gives
a dynamic bayes net that has been previously used on this data set. the skip-chain
crf beats the previous systems in overall f1 and on the speaker    eld, which has
proved to be the hardest    eld of the four. overall f1 is the average of the f1 scores
for the four    elds.

the observation functions for the skip edges are chosen to combine the observations
from each endpoint. formally, we de   ne the feature functions for the skip edges to
factorize as:

f0
k(yu, yv, x, u, v) = 1{yu=  yu}1{yv=  yv}q0

k(x, u, v)

(1.60)

this choice allows the observation functions q0
k(x, u, v) to combine information from
the neighborhood of yu and yv. for example, one useful feature is q0
k(x, u, v) = 1
if and only if xu = xv =    booth    and xv   1 =    speaker:   . this can be a useful
feature if the context around xu, such as    robert booth is manager of control
engineering. . . ,    may not make clear whether or not robert booth is presenting a
talk, but the context around xv is clear, such as    speaker: robert booth.    1
because the loops in a skip-chain crf can be long and overlapping, exact id136
is intractable for the data we consider. the running time required by exact id136
is exponential in the size of the largest clique in the graph   s junction tree. in junction
trees created from the seminars data, 29 of the 485 instances have a maximum
clique size of 10 or greater, and 11 have a maximum clique size of 14 or greater.
(the worst instance has a clique with 61 nodes.) these cliques are far too large to
perform id136 exactly. for reference, representing a single factor that depends on
14 variables requires more memory than can be addressed in a 32-bit architecture.
instead, we perform approximate id136 using loopy belief propagation, which
was mentioned in section 1.4.4. we use an asynchronous tree-based schedule known
as trp [wainwright et al., 2001].

1.5.2 results

we evaluate skip-chain crfs on a collection of 485 e-mail messages announcing
seminars at carnegie mellon university. the messages are annotated with the
seminar   s starting time, ending time, location, and speaker. this data set is due to

1. this example is taken from an actual error made by a linear-chain crf on the seminars
data set. we present results from this data set in section 1.5.2.

28

an introduction to id49 for relational learning

field

stime

etime

location

speaker

linear-chain

skip-chain

12.6

3.2

6.4

30.2

17

5.2

0.6

4.8

table 1.3 number of inconsistently mislabeled tokens, that is, tokens that are
mislabeled even though the same token is labeled correctly elsewhere in the docu-
ment. learning long-distance dependencies reduces this kind of error in the speaker
and location    elds. numbers are averaged over 5 folds.

freitag [1998], and has been used in much previous work.
often the    elds are listed multiple times in the message. for example, the speaker
name might be included both near the beginning and later on, in a sentence like    if
you would like to meet with professor smith. . .     as mentioned earlier, it can be
useful to    nd both such mentions, because di   erent information can occur in the
surrounding context of each mention: for example, the    rst mention might be near
an institutional a   liation, while the second mentions that smith is a professor.
we evaluate a skip-chain crf with skip edges between identical capitalized words.
the motivation for this is that the hardest aspect of this data set is identifying
speakers and locations, and capitalized words that occur multiple times in a seminar
announcement are likely to be either speakers or locations.
table 1.1 shows the list of input features we used. for a skip edge (u, v), the input
features we used were the disjunction of the input features at u and v, that is,

k(x, u, v) = qk(x, u)     qk(x, v)
q0

(1.61)
where     is binary or. all of our results are averaged over 5-fold cross-validation
with an 80/20 split of the data. we report results from both a linear-chain crf
and a skip-chain crf with the same set of input features.
we calculate precision and recall as2

p =

r =

# tokens extracted correctly

# tokens extracted

# tokens extracted correctly

# true tokens of    eld

2. previous work on this data set has traditionally measured precision and recall per
document, that is, from each document the system extracts only one    eld of each type.
because the goal of the skip-chain crf is to extract all mentions in a document, these
metrics are inappropriate, so we cannot compare with this previous work. peshkin and
pfe   er [2003] do use the per-token metric (personal communication), so our comparison
is fair in that respect.

1.5 skip-chain crfs

29

as usual, we report f1 = (2p r)/(p + r).
table 1.2 compares a skip-chain crf to a linear-chain crf and to a dynamic
bayes net used in previous work [peshkin and pfe   er, 2003]. the skip-chain crf
performs much better than all the other systems on the speaker    eld, which is
the    eld for which the skip edges would be expected to make the most di   erence.
on the other    elds, however, the skip-chain crf does slightly worse (less than 1%
absolute f1).
we expected that the skip-chain crf would do especially well on the speaker    eld,
because speaker names tend to appear multiple times in a document, and a skip-
chain crf can learn to label the multiple occurrences consistently. to test this
hypothesis, we measure the number of inconsistently mislabeled tokens, that is, to-
kens that are mislabeled even though the same token is classi   ed correctly elsewhere
in the document. table 1.3 compares the number of inconsistently mislabeled to-
kens in the test set between linear-chain and skip-chain crfs. for the linear-chain
crf, on average 30.2 true speaker tokens are inconsistently mislabeled. because
the linear-chain crf mislabels 121.6 true speaker tokens, this situation includes
24.7% of the missed speaker tokens.
the skip-chain crf shows a dramatic decrease in inconsistently mislabeled tokens
on the speaker    eld, from 30.2 tokens to 4.8. consequently, the skip-chain crf also
has much better recall on speaker tokens than the linear-chain crf (70.0 r linear
chain, 76.8 r skip chain). this explains the increase in f1 from linear-chain to
skip-chain crfs, because the two have similar precision (86.5 p linear chain, 85.1
skip chain). these results support the original hypothesis that treating repeated
tokens consistently especially bene   ts recall on the speaker    eld.
on the location    eld, on the other hand, where we might also expect skip-
chain crfs to perform better, there is no bene   t. we explain this by observing
in table 1.3 that inconsistent misclassi   cation occurs much less frequently in this
   eld.

1.5.3 related work

recently, bunescu and mooney [2004] have used a relational markov network to
collectively classify the mentions in a document, achieving increased accuracy by
learning dependencies between similar mentions. in their work, however, candidate
phrases are extracted heuristically, which can introduce errors if a true entity is
not selected as a candidate phrase. our model performs collective segmentation
and labeling simultaneously, so that the system can take into account dependencies
between the two tasks.
as an extension to our work, finkel et al. [2005] augment the skip-chain model with
richer kinds of long-distance factors than just over pairs of words. these factors
are useful for modeling exceptions to the assumption that similar words tend to
have similar labels. for example, in named-entity recognition, the word china is
as a place name when it appears alone, but when it occurs within the phrase the
china daily, it should be labeled as a organization. because this model is more

30

an introduction to id49 for relational learning

complex than the original skip-chain model, finkel et al. estimate its parameters in
two stages,    rst training the linear-chain component as a separate crf, and then
heuristically selecting parameters for the long-distance factors. finkel et al. report
improved results both on the seminars data set that we consider in this chapter,
and on several other standard information extraction data sets.
finally, the skip-chain crf can also be viewed as performing extraction while
taking into account a simple form of coreference information, since the reason that
identical words are likely to have similar tags is that they are likely to be coreferent.
thus, this model is a step toward joint probabilistic models for extraction and data
mining as advocated by mccallum and jensen [2003]. an example of such a joint
model is the one of wellner et al. [2004], which jointly segments citations in research
papers and predicts which citations refer to the same paper.

1.6 conclusion

conditional random    elds are a natural choice for many relational problems be-
cause they allow both graphically representing dependencies between entities, and
including rich observed features of entities. in this chapter, we have presented a
tutorial on crfs, covering both linear-chain models and general graphical struc-
tures. also, as a case study in crfs for collective classi   cation, we have presented
the skip-chain crf, a type of general crf that performs joint segmentation and
collective labeling on a practical language understanding task.
the main disadvantage of crfs is the computational expense of training. although
crf training is feasible for many real-world problems, the need to perform id136
repeatedly during training becomes a computational burden when there are a large
number of training instances, when the graphical structure is complex, when there
are latent variables, or when the output variables have many outcomes. one focus
of current research [abbeel et al., 2005, sutton and mccallum, 2005, wainwright
et al., 2003] is on more e   cient parameter estimation techniques.

acknowledgments

we thank tom minka and jerod weinman for helpful conversations, and we thank
francine chen and benson limketkai for useful comments. this work was supported
in part by the center for intelligent information retrieval; in part by the defense
advanced research projects agency (darpa), the department of the interior,
nbc, acquisition services division, under contract number nbchd030010; and
in part by the central intelligence agency, the national security agency and
national science foundation under nsf grants #iis-0427594 and #iis-0326249.
any opinions,    ndings and conclusions or recommendations expressed in this
material are the author(s) and do not necessarily re   ect those of the sponsors.

references

pieter abbeel, daphne koller, and andrew y. ng. learning factor graphs in poly-
nomial time and sample complexity. in twenty-   rst conference on uncertainty
in arti   cial intelligence (uai05), 2005.

yasemin altun, ioannis tsochantaridis, and thomas hofmann. hidden markov
support vector machines. in 20th international conference on machine learning
(icml), 2003.

dimitri p. bertsekas. nonid135. athena scienti   c, 2nd edition, 1999.
julian besag. e   ciency of pseudolikelihood estimation for simple gaussian    elds.

biometrika, 64(3):616   618, 1977.

razvan bunescu and raymond j. mooney. collective information extraction with
relational markov networks. in proceedings of the 42nd annual meeting of the
association for computational linguistics, 2004.

richard h. byrd, jorge nocedal, and robert b. schnabel. representations of quasi-
newton matrices and their use in limited memory methods. math. program., 63
(2):129   156, 1994.

rich caruana and alexandru niculescu-mizil.

compari-
son of supervised learning algorithms using di   erent performance metrics.
technical report tr2005-1973, cornell university, 2005.
available at
http://www.cs.cornell.edu/   alexn/.

an empirical

yejin choi, claire cardie, ellen rilo   , and siddharth patwardhan.

identifying
sources of opinions with conditional random    elds and extraction patterns.
in proceedings of the human language technology conference/conference on
empirical methods in natural language processing (hlt-emnlp), 2005.

stephen clark and james r. curran. parsing the wsj using id35 and log-linear
models. in proceedings of the 42nd meeting of the association for computational
linguistics (acl   04), main volume, pages 103   110, barcelona, spain, july 2004.
michael collins. discriminative training methods for id48:
theory and experiments with id88 algorithms. in conference on empirical
methods in natural language processing (emnlp), 2002.

philip j. cowans and martin szummer. a graphical model for simultaneous parti-
tioning and labeling. in tenth international workshop on arti   cial intelligence
and statistics, 2005.

32

references

aron culotta, ron bekkerman, and andrew mccallum. extracting social networks
and contact information from email and the web. in first conference on email
and anti-spam (ceas), mountain view, ca, 2004.

aron culotta and andrew mccallum. con   dence estimation for information

extraction. in human language technology conference (hlt), 2004.

jenny finkel, trond grenager, and christopher d. manning. incorporating non-
local information into information extraction systems by id150.
in
proceedings of the 43rd annual meeting of the association for computational
linguistics (acl), 2005.

dayne freitag. machine learning for information extraction in informal domains.

phd thesis, carnegie mellon university, 1998.

nadia ghamrawi and andrew mccallum. collective multi-label classi   cation. in

conference on information and knowledge management (cikm), 2005.

joshua goodman. exponential priors for maximum id178 models. in proceedings
of the human language technology conference/north american chapter of the
association for computational linguistics (hlt/naacl), 2004.

xuming he, richard s. zemel, and miguel   a. carreira-perpi  ni  an. multiscale con-
ditional random    elds for image labelling. in ieee computer society conference
on id161 and pattern recognition, 2004.

g.e. hinton. training products of experts by minimizing contrastive divergence.

technical report 2000-004, gatsby computational neuroscience unit, 2000.

f. r. kschischang, b. j. frey, and h. a. loeliger. factor graphs and the sum-
product algorithm. ieee transactions on id205, 47(2):498   519,
2001.

taku kudo, kaoru yamamoto, and yuji matsumoto. applying conditional random
   elds to japanese morphological analysis. in proceedings of the conference on
empirical methods in natural language processing (emnlp), 2004.

sanjiv kumar and martial hebert. discriminative    elds for modeling spatial de-
pendencies in natural images. in sebastian thrun, lawrence saul, and bernhard
sch  olkopf, editors, advances in neural information processing systems 16. mit
press, cambridge, ma, 2003.

j. la   erty, a. mccallum, and f. pereira. conditional random    elds: probabilistic
models for segmenting and labeling sequence data. proc. 18th international conf.
on machine learning, 2001.

yan liu, jaime carbonell, peter weigele, and vanathi gopalakrishnan. segmenta-
tion conditional random    elds (scrfs): a new approach for protein fold recogni-
tion. in acm international conference on research in computational molecular
biology (recomb05), 2005.

r. malouf. a comparison of algorithms for maximum id178 parameter estima-
tion. in dan roth and antal van den bosch, editors, proceedings of the sixth
conference on natural language learning (conll-2002), pages 49   55, 2002.

references

33

david mcallester, michael collins, and fernando pereira. case-factor diagrams
for structured probabilistic modeling. in conference on uncertainty in arti   cial
intelligence (uai), 2004.

andrew mccallum. e   ciently inducing features of conditional random    elds. in

conference on uncertainty in ai (uai), 2003.

andrew mccallum, kedar bellare, and fernando pereira. a conditional random
   eld for discriminatively-trained    nite-state string id153. in conference
on uncertainty in ai (uai), 2005.

andrew mccallum and david jensen. a note on the uni   cation of information
extraction and data mining using conditional-id203, relational models. in
ijcai   03 workshop on learning statistical models from relational data, 2003.
andrew mccallum and wei li. early results for id39 with
in

conditional random    elds, feature induction and web-enhanced lexicons.
seventh conference on natural language learning (conll), 2003.

andrew mccallum and ben wellner. conditional models of identity uncertainty
with application to noun coreference. in lawrence k. saul, yair weiss, and l  eon
bottou, editors, advances in neural information processing systems 17, pages
905   912. mit press, cambridge, ma, 2005.

thomas p. minka.

regression.
   minka/papers/logreg/.

technical

report, 2003.

a comparsion of numerical optimizers

logistic
http://research.microsoft.com/

for

tom minka.

discriminative models, not discriminative training.

nical report msr-tr-2005-144, microsoft research, october
ftp://ftp.research.microsoft.com/ pub/tr/tr-2005-144.pdf .

tech-
2005.

a. y. ng and m. i. jordan. on discriminative vs. generative classi   ers: a compar-
ison of id28 and naive bayes. in t. g. dietterich, s. becker, and
z. ghahramani, editors, advances in neural information processing systems 14,
pages 841   848, cambridge, ma, 2002. mit press.

fuchun peng, fangfang feng, and andrew mccallum. chinese segmentation and
new word detection using conditional random    elds. in proceedings of the 20th
international conference on computational linguistics (coling), pages 562   
568, 2004.

fuchun peng and andrew mccallum. accurate information extraction from re-
search papers using conditional random    elds. in proceedings of human lan-
guage technology conference and north american chapter of the association
for computational linguistics (hlt-naacl   04), 2004.

leonid peshkin and avi pfe   er. bayesian information extraction network.

in

international joint conference on arti   cial intelligence (ijcai), 2003.

yuan qi, martin szummer, and thomas p. minka. diagram structure recognition
by bayesian conditional random    elds. in international conference on computer
vision and pattern recognition, 2005.

34

references

ariadna quattoni, michael collins, and trevor darrell. conditional random    elds
for object recognition. in lawrence k. saul, yair weiss, and l  eon bottou, editors,
advances in neural information processing systems 17, pages 1097   1104. mit
press, cambridge, ma, 2005.

l.r. rabiner. a tutorial on id48 and selected applications in

id103. proceedings of the ieee, 77(2):257     286, 1989.

matthew richardson and pedro domingos. markov logic networks. machine

learning, 2005. to appear.

s. riezler, t. king, r. kaplan, r. crouch, j. maxwell, and m. johnson. parsing the
wall street journal using a lexical-functional grammar and discriminative esti-
mation techniques. in proceedings of the 40th annual meeting of the association
for computational linguistics, 2002.

d. roth and w. yih. integer id135 id136 for conditional random
   elds. in proc. of the international conference on machine learning (icml),
pages 737   744, 2005.

sunita sarawagi and william w. cohen. semi-markov conditional random    elds
for information extraction. in lawrence k. saul, yair weiss, and l  eon bottou,
editors, advances in neural information processing systems 17, pages 1185   1192.
mit press, cambridge, ma, 2005.

kengo sato and yasubumi sakakibara. rna secondary structural alignment with

conditional random    elds. bioinformatics, 21:ii237   242, 2005.

burr settles. abner: an open source tool for automatically tagging genes, proteins,

and other entity names in text. bioinformatics, 21(14):3191   3192, 2005.

fei sha and fernando pereira. id66 with conditional random    elds. in

proceedings of hlt-naacl, pages 213   220, 2003.

p. singla and p. domingos. discriminative training of markov logic networks. in
proceedings of the twentieth national conference on arti   cial intelligence, pages
868   873, pittsburgh, pa, 2005. aaai press.

charles sutton.

mas-
ter   s thesis, university of massachusetts, 2004. http://www.cs.umass.edu/
   casutton/publications.html.

conditional id140.

charles sutton and andrew mccallum. piecewise training of undirected models.

in 21st conference on uncertainty in arti   cial intelligence, 2005.

charles sutton, khashayar rohanimanesh, and andrew mccallum. dynamic con-
ditional random    elds: factorized probabilistic models for labeling and segment-
ing sequence data. in proceedings of the twenty-first international conference
on machine learning (icml), 2004.

ben taskar, pieter abbeel, and daphne koller. discriminative probabilistic mod-
in eighteenth conference on uncertainty in arti   cial

els for relational data.
intelligence (uai02), 2002.

ben taskar, carlos guestrin, and daphne koller. max-margin markov networks.

references

35

in sebastian thrun, lawrence saul, and bernhard sch  olkopf, editors, advances
in neural information processing systems 16. mit press, cambridge, ma, 2004.
paul viola and mukund narasimhan. learning to extract information from semi-
structured text using a discriminative id18. in proceedings of
the acm sigir, 2005.

m. wainwright, t. jaakkola, and a. willsky. tree-based reparameterization for
approximate estimation on graphs with cycles. advances in neural information
processing systems (nips), 2001.

m. j. wainwright, t. jaakkola, and a. s. willsky. tree-reweighted belief propa-
gation and approximate ml estimation by pseudo-moment matching. in ninth
workshop on arti   cial intelligence and statistics, 2003.

hanna wallach. e   cient training of conditional random    elds. m.sc. thesis,

university of edinburgh, 2002.

ben wellner, andrew mccallum, fuchun peng, and michael hay. an integrated,
conditional model of information extraction and coreference with application to
citation graph construction.
in 20th conference on uncertainty in arti   cial
intelligence (uai), 2004.

j.s. yedidia, w.t. freeman, and y. weiss. constructing free energy approximations
and generalized belief propagation algorithms. technical report tr2004-040,
mitsubishi electric research laboratories, 2004.

