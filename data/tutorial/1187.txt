deep learning for natural language processing and

machine translation

kevin duh

nara institute of science and technology, japan

2014/11/04

what is deep learning?

a family of methods that uses deep architectures to learn high-level
feature representations

(p.2)

example of trainable features [lee et al., 2009]

input: images (raw pixels)
    output: features representing edges, body parts, full faces

(p.3)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.4)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.5)

problem setup

training data: a set of (x (m), y (m))m={1,2,..m} pairs

(cid:73) input x (m)     r d
(cid:73) output y (m) = {0, 1}

goal: learn function f : x     y to predict correctly on new inputs x.

(cid:73) step 1: choose a function model family:

(cid:70) e.g. id28, support vector machines, neural networks

(cid:73) step 2: optimize parameters w on the training data

(cid:80)m
m=1(fw (x (m))     y (m))2

(cid:70) e.g. minimize id168 minw

(p.6)

id28 (1-layer net)

function model: f (x) =   (w t    x)

(cid:73) parameters: vector w     r d
(cid:73)    is a non-linearity, e.g. sigmoid:   (z) = 1/(1 + exp(   z))

non-linearity will be important in expressiveness multi-layer nets.
other non-linearities, e.g., tanh(z) = (ez     e   z )/(ez + e   z )

(p.7)

id119 for id28

(cid:80)

(cid:2)  (w t x (m))     y (m)(cid:3)  (cid:48)(w t x (m))x (m)

m(  (w t x (m))     y (m))2

assume squared-error    loss(w ) = 1

gradient:    w loss =(cid:80)
(cid:73) general form of gradient: (cid:80)

(cid:73) de   ne input into non-linearity in(m) = w t x (m)
(cid:73) derivative of sigmoid   (cid:48)(z) =   (z)(1       (z))

m error (m)       (cid:48)(in(m))     x (m)

m

2

id119 algorithm ():

1

initialize w randomly

2 update until convergence: w     w       (   w loss)

stochastic id119 (sgd) algorithm:

1

initialize w randomly

2 update until convergence: w     w       (error (m)       (cid:48)(in(m))     x (m))

(cid:80)
*an alternative is cross-id178 loss:
m y (m) log(  (w t x (m))) + (1     y (m)) log(1       (w t x (m)))

(p.8)

sgd pictorial view

(cid:80)

loss objective contour plot: 1
2

m(  (w t x (m))     y (m))2 + ||w||

(cid:73) id119 goes in steepest descent direction
(cid:73) sgd is noisy descent (but faster per iteration)

(p.9)

2-layer neural networks

y

w1

w2

w3

h1

h2

h3

w11

w12

wj

hj

wij

x1

xi

f (x) =   ((cid:80)

x3

x2

j wj    hj ) =   ((cid:80)

j wj      ((cid:80)

x4

i wij xi ))

hidden units hj    s can be viewed as new    features    from combining xi    s

called multilayer id88 (mlp), but more like multilayer id28

(p.10)

expressive power of non-linearity

a deeper architecture is more expressive than a shallow one given
same number of nodes [bishop, 1995]

(cid:73) 1-layer nets only model linear hyperplanes
(cid:73) 2-layer nets can model any continuous function (given su   cient nodes)
(cid:73) >3-layer nets can do so with fewer nodes

(p.11)

training neural nets: back-propagation

y

w1

w2

w3

h1

h2

h3

adjust weights

predict f (x (m))

w11

w12

wj

hj

wij

xi

x1

1. for each sample, compute f (x (m)) =   ((cid:80)

x2

x3

x4

j wj      ((cid:80)

i wij x (m)

i

))

2. if f (x (m)) (cid:54)= y (m), back-propagate error and adjust weights {wij , wj}.

(p.12)

derivatives of the weights

and loss per sample: loss =(cid:80)

assume two outputs (y1, y2) per input x,

1

2 [  (ink )     yk ]2

k

y1

y2

h1

h2

h3

yk

wjk

hj

wij

x1

xi

=   k

   ((cid:80)
   ((cid:80)
2 [  (ink )     yk ]2(cid:17)
=(cid:80)

j wjk hj )
   wjk
j wij xi )
   wij

k   k       

=   j

   inj

   ink
   wjk
   inj
   wij
1

k

   ink
   inj

x2

x3

x4

=   k hj

=   j xi
= [  (ink )     yk ]   (cid:48)(ink )

(cid:17)

= [(cid:80)

(cid:16)(cid:80)

j wjk   (inj )

=    loss
   ink
=    loss
   inj

(cid:16)(cid:80)

   loss
   wjk
   loss
   wij
  k =    
   ink

  j =(cid:80)

   loss
   ink

k

k   k wjk ]   (cid:48)(inj )

(p.13)

training neural nets: back-propagation

all updates involve some scaled error from output     input feature:

=   k hj where   k = [  (ink )     yk ]   (cid:48)(ink )

=   j xi where   j = [(cid:80)

k   k wjk ]   (cid:48)(inj )

   loss
   wjk
   loss
   wij

first compute   k from    nal layer, then   j for previous layer and iterate.

yk

wjk

hj

wij

xi

y1

  k=y1

y2

  k=y2

w31

w32
  j=h3 = [  k=y1w31 +   k=y2w32]  (cid:48)(inh3)
h3

h1

h2

x1

x2

x3

   loss
   wij

x4

(p.14)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.15)

potential of deep architecture

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

(p.16)

*figure from [bengio, 2009]

di   culties of deep architecture

vanishing gradient problem in
id26
=    loss
   inj

   loss
   wij

=   j xi

   inj
   wij

  j =

j+1   j+1wj(j+1)

  (cid:48)(inj )

(cid:104)(cid:80)

(cid:105)

  j may vanish after repeated
multiplication

also, exploding gradient
problem!

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

wj(j+1)

h3

wij

x3

(p.17)

analysis of training di   culties [erhan et al., 2009]

mnist digit classi   cation task
train neural net by id26 (random initialization of wij )

(p.18)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.19)

layer-wise pre-training [hinton et al., 2006]

first, train one layer at a time, optimizing data-likelihood objective p(x)

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer1

(p.20)

layer-wise pre-training [hinton et al., 2006]

first, train one layer at a time, optimizing data-likelihood objective p(x)

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer2

keep layer1    xed

(p.21)

layer-wise pre-training [hinton et al., 2006]
finally,    ne-tune labeled objective p(y|x) by id26

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

adjust weights

predict f(x)

(p.22)

layer-wise pre-training [hinton et al., 2006]

key idea:
focus on modeling the input p(x ) better with each successive layer.
worry about optimizing the task p(y|x ) later.

   if you want to do id161,    rst learn computer
graphics.        geo    hinton

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer2

train layer1

extra advantage:
can exploit large
amounts of unlabeled
data!

(p.23)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.24)

deep learning paradigm

recall problem setup: learn function f : x     y
first learn hidden features h that model input, i.e. x     h     y
how do we discover useful latent features h from data x?

(cid:73) e.g. use restricted id82s (rbms)

(p.25)

restricted id82 (rbm)

rbm is a probabilistic model on binary variables hj and xi :

p(x, h) =

=

1
z  
1
z  

exp (   e  (x, h))

exp (x t wh + bt x + d t h)

(cid:73) w is a matrix; elements wij models correlation between xi and hj
(cid:73) b and d are bias terms; we   ll assume b = d = 0 here.

(cid:73) normalizer (partition function): z   =(cid:80)

(x,h) exp(   e  (x, h))

h1

wij

x1

h2

x2

h3

x3

(p.26)

restricted id82 (rbm): example

h1

x1

h2

x2

h3

x3

let weights wij on edges (h1, x1), (h1, x3) be positive, others be near 0.

x1
1
1
1
0
0
0

x2
0
0
0
0
1
0

x3
0
0
0
0
0
1

h1
1
0
1
1
0
0

h2
0
0
0
0
0
0

h3
1
1
0
1
0
0
etc

p(x, h) = 1
z  

exp (x t wh)

highest

high
high
low
low
low

(p.27)

rbm posterior distribution (easy!)

computing p(h|x) is easy due to factorization:

p(h|x) =

=

=

=

=

=

p(x, h)
h p(x, h)
exp(x t wh + bt x + d t h)
h exp(x t wh + bt x + d t h)

(cid:80)
1/z   exp(    e(x, h))
h 1/z   exp(    e(x, h))
(cid:81)
h2   {0,1}       (cid:80)

(cid:81)

hj
j exp(x t wj hj + dj hj )
hj   {0,1} exp(x t wj hj + dj hj )

(cid:80)
(cid:80)
(cid:80)
h1   {0,1}(cid:80)
(cid:81)
(cid:80)
(cid:81)
(cid:89)
(cid:80)

j

exp(x t wj hj + dj hj )

hj   {0,1} exp(x t wj hj + dj hj )

j

(cid:89)

j

=

p(hj|x)

j exp(x t wj hj + dj hj )    exp(bt x)

j exp(x t wj hj + dj hj )    exp(bt x)

p(hj = 1|x) = exp(x t wj + dj )/z =   (x t wj + dj ) is logistic
regression!

similarly, computing p(x|h) =(cid:81)

i p(xi|h) is easy

(p.28)

rbm max-likelihood training (hard!)

derivative of the log-likelihood:    wij log pw (x = x (m))

(cid:88)
(cid:88)

h

=    wij log

pw (x = x (m), h)

h

1
zw

exp (    ew(x(m), h))

(cid:88)
=    wij log
=        wij log zw +    wij log
(cid:88)
e(    ew(x,h))    wij ew(x, h)   
(cid:88)

(cid:80)
pw (x, h)[   wij ew(x, h)]    (cid:88)

=

h,x

h

=

1
zw

exp (    ew(x(m), h))

1

h e(    ew(x(m),h))

(cid:88)

h

h,x

h

=    ep(x,h)[xi    hj ] + e

p(h|x=x (m))[x (m)

i

   hj ]

(1)

(2)

(3)

e(    ew(x(m),h))    wij ew(x(m), h)

pw (x (m), h)[   wij ew(x(m), h)]

(4)

(5)

second term (positive phase) increases id203 of x (m); first term
(negative phase) decreases id203 of samples generated by the model

(p.29)

contrastive divergence algorithm

h1

x1

h2

x2

h3

x3

the negative phase term (ep(x,h)[xi    hj ]) is expensive because it
requires sampling (x,h) from the model
id150 (sample x then h iteratively) works, but waiting for
convergence at each gradient step is slow.
contrastive divergence is a faster but biased method: initialize with
training point and wait only a few (usu. 1) sampling steps

1 let x (m) be training point, w = [wij ] be current model weights
i + dj )    j.

2 sample   hj     {0, 1} from p(hj|x = x (m)) =   ((cid:80)
3 sample   xi     {0, 1} from p(xi|h =   h) =   ((cid:80)
4 sample   hj     {0, 1} from p(hj|x =   x) =   ((cid:80)

j wij   hj + bi )    i.
i wij   xi + dj )    j.

i wij x (m)

5 wij     wij +   (x (m)

     hj       xi      hj )

i

(p.30)

contrastive divergence pictorial view

goal: make rbm p(x, h) have high id203 on training samples
to do so, we   ll    steal    id203 mass from nearby samples that
incorrectly preferred by the model
for detailed analysis, see [carreira-perpinan and hinton, 2005]

(p.31)

distributed representations learned by rbm

vector h act as distributed representation of data

(cid:73) multiple hj may be active simultaneously for a given x.
(cid:73) 2|h| possible representations with |h|    |x| parameters.

(multi-id91)

h1

h2

h3

x3

an equivalent mixture model p(x) =(cid:80)

x1

x2

parameters:

h p(c)p(x|c) needs 2|h|    |x|

c

x2

x3

x1

(p.32)

deep belief nets (dbn) = stacked rbm

h(cid:48)(cid:48)

1

h(cid:48)

1

h1

x1

h(cid:48)(cid:48)

2

h(cid:48)

2

h2

x2

h(cid:48)(cid:48)

3

layer3 rbm

h(cid:48)

3

layer2 rbm

h3

layer1 rbm

x3

dbn de   nes a probabilistic
generative model p(x) =

(cid:80)
h,h(cid:48),h(cid:48)(cid:48) p(x|h)p(h|h(cid:48))p(h(cid:48), h(cid:48)(cid:48))
(top 2 layers is interpreted as a
rbm; lower layers are directed
sigmoids)

stacked rbms can also be used
to initialize a deep neural
network (dnn)

(p.33)

example of what a deep generative model can do

after training on 20k images, the generative model of
[salakhutdinov and hinton, 2009]* can generate random images
(dimension=8976) that are amazingly realistic!

this model is a deep id82 (dbm), di   erent from deep belief nets

(dbn) but also built by stacking rbms.

(p.34)

deep belief nets (dbn) summary

1 layer-wise pre-training is the innovation that rekindled interest in

deep architectures.

2 pre-training focuses on optimizing likelihood on the data, not the

target label. first model p(x) to do better p(y|x).

3 why rbm? p(h|x) is tractable, so it   s easy to stack.
4 rbm training can be expensive. solution: contrastive divergence

5 we can stack rbms to form a deep probabilistic generative model

(dbn), or to initialize deep neural network (dnn)

(p.35)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.36)

auto-encoders: e   cient replacement for rbm

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (wx + b)

x1

x2

x3

e.g. loss =(cid:80)

encourage h to give small reconstruction error:

m ||x (m)     decoder(encoder(x (m)))||2

reconstruction: x(cid:48) =   (w (cid:48)  (wx + b) + d)
|h| is small to enforce    compression    of data
training by id26 for 2-layer nets, with x (m) as both input
and output

(p.37)

stacked auto-encoders (sae)

the encoder/decoder gives same form p(h|x), p(x|h) as rbms, so
can be stacked in the same way to form deep architectures

y

h(cid:48)

1

h(cid:48)

2

h1

h2

h3

output layer

layer2 encoder

layer1 encoder

x1

x2

x3

x4

unlike rbms, auto-encoders are deterministic.

(cid:73) h =   (wx + b), not p(h = {0, 1}) =   (wx + b)
(cid:73) disadvantage: can   t form deep generative model
(cid:73) advantage: fast to train, and useful still for deep neural nets

(p.38)

auto-encoder variants: e.g. denoising auto-encoder

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (w   x + b)

  x1

  x2

  x3

  x = x+ noise

1 perturb input data x to   x using invariance from domain knowledge.

2 train weights to reduce reconstruction error with respect to original

input: ||x     x(cid:48)||

(p.39)

denoising auto-encoders

example: randomly shift, rotate, and scale input image

an image of    2    is a    2    no matter how you add noise, so the
auto-encoder will try to cancel the variations that are not important.

figure from geo    hinton   s 2012 coursera course, lecture 1:

https://www.coursera.org/course/neuralnets

(p.40)

stacked auto-encoders (sae): summary

1 auto-encoders are cheaper alternatives to rbms.

(cid:73) not probabilistic, but fast to train using id26
(cid:73) achieves similar accuracies as rbm [bengio et al., 2006]

2 auto-encoders learn to    compress    and    re-construct    input data.

again, the focus is on modeling p(x)    rst.

3 many variants, some provide ways to incorporate domain knowledge.

(p.41)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.42)

why does pre-training work? [erhan et al., 2010]

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

a deep net can    t the training data in
many ways (non-convex):

1 by optimizing upper-layers really hard
2 by optimizing lower-layers really hard

top-down vs. bottom-up information

1 even if lower-layers are random

weights, upper-layer may still    t well.
but may not generalize to new data
2 pre-training with objective on p(x)

learns more generalizable features

pre-training seems to help put weights
at a better local optimum

(p.43)

h(cid:48)

3

h3

x3

is pre-training really necessary?

answer in 2006: yes!
answer in 2014: no!

1

2

if initialization is done well by design (e.g. sparse connections and
convolutional nets), maybe won   t have vanishing gradient problem

if you have an extremely large datasets, maybe won   t over   t. (but
maybe that also means you want an ever deeper net)

3 new architectures are emerging: e.g. stacked id166   s with random

projections [vinyals et al., 2012]

(p.44)

success in id103

hybrid dnn-id48 system: (typically 3-8 layers, 2000 units/layer, 15
frames of input, 6000 output)

(p.45)

success in id103

word error rate results [hinton et al., 2012a]:

(p.46)

success in id161 [le et al., 2012]

id163 test accuracy (22k categories):

method
random
previous state-of-the-art
   9   -layer net, back-propagation without pre-training
+ pre-training on 10 million youtube images

accuracy
0.005%
9.3%
13.6%
15.8%

deep network has 1 billion parameters, trained on 16k cores for a week

(p.47)

cat neuron

*graphics from [le et al., 2012]

(p.48)

face neuron

*graphics from [le et al., 2012]

(p.49)

further enhancements worth knowing about

sgd alternative, e.g. 2nd order methods [martens, 2010], accelerated
gradient [sutskever et al., 2013]

better id173, e.g. dropout [hinton et al., 2012b]

scaling to large data, e.g. [dean et al., 2012, coates et al., 2013]

hyper-parameter search, e.g. [bergstra et al., 2011]

recent analyses on why things work or fail, e.g. [szeged et al., 2014]

(p.50)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.51)

recent papers with keywords: deep or neural

sequence labeling

id52 & name entity recognition [turian et al., 2010, collobert et al., 2011, wang and manning, 2013,
ma et al., 2014, tsuboi, 2014, guo et al., 2014, qi et al., 2014]

id40 [zheng et al., 2013, pei et al., 2014]

syntax & morphology

id33 [stenetorp, 2013, chen et al., 2014a, levy and goldberg, 2014, bansal et al., 2014,
chen and manning, 2014, le and zuidema, 2014]

constituency parsing [billingsley and curran, 2012, socher et al., 2013a, andreas and klein, 2014]

id35 [hermann and blunsom, 2013], selectional preference [van de cruys, 2014], morphology [luong et al., 2013]

semantics

word representations [tsubaki et al., 2013, srivastava et al., 2013, rockt  aschel et al., 2014, baroni et al., 2014,
hashimoto et al., 2014, pennington et al., 2014, neelakantan et al., 2014, chen et al., 2014b, milajevs et al., 2014]

id14: [hermann et al., 2014, roth and woodsend, 2014]

paraphrase [socher et al., 2011]

grounding/multi-modal [fyshe et al., 2014, kiela and bottou, 2014]

discourse

[ji and eisenstein, 2014, li et al., 2014a]

id53, knowledge bases, & id36

[hashimoto et al., 2013, fu et al., 2014, chang et al., 2014, yih et al., 2014, bordes et al., 2014, iyyer et al., 2014,
yang et al., 2014, gardner et al., 2014]

id31

[glorot et al., 2011, socher et al., 2013b, irsoy and cardie, 2014]

summarization

[liu et al., 2012]

novel applications

poetry [zhang and lapata, 2014], interestingness [gao et al., 2014b], hashtags [weston et al., 2014]

(p.52)

disclaimer!

there   s no consensus yet on how best to apply deep learning in nlp

(cid:73) good results have been reported, but not yet revolutionary
(cid:73) compared to vision/speech, methods for nlp seem to have

less emphasis on    deep.    perhaps a single word is already an extremely
informative feature, worth a thousand pixels?

what we   ll do: summarize 2 ways deep/neural ideas can be used

1 as non-linear classi   er
2 as distributed representation

show one successful and one unsuccessful case study each, to
emphasise that everything is work-in-progress!
    what you believe is good today may be bad tomorrow.

(p.53)

use as non-linear classi   er

idea: directly replace a linear classifer used in nlp with a deep network.

expected to work if:

1 di   cult to engineer e   ective features

2 linear classi   er under   ts (e.g. high training error)

(p.54)

case study 1: id20 for large-scale
sentiment classi   cation [glorot et al., 2011]

amazon review dataset [blitzer et al., 2007]

(cid:73) 4 domains: electronics, books, dvds, kitchen.
(cid:73) pre-train on unlabeled data to get    good features    across domains
(cid:73) then, train id166 on target labeled data per domain

y

h(cid:48)

1

h(cid:48)

2

output layer: id166

denoising auto-encoder: noise=gaussian; recti   er non-linearity

h1

h2

h3

denoising auto-encoder: noise=randomly set xi =0;    non-linearity

x1

x2

x3

x4

input: 5000 most frequent 1gram/2gram words

hyperparameters: masking noise (0.8), gaussian noise var, number of hidden
layers (1-3), no. hidden layers (1000, 2500, 5000), id173, learning rate   

(p.55)

experiment results [glorot et al., 2011]

evaluation metric:

error (a, b) = error of classi   er trained on domaina, test on domainb
transfer loss = error (source, target)     error (target, target)

(p.56)

case study 2: machine translation n-best list re-ranking

hypothesis: dense features in current systems are not su   ciently
expressive [duh and kirchho   , 2008]

(cid:73) translation model, language model scores are too coarse-grained
(cid:73) linear re-ranker attains only convex hull of n-best candidates

(p.57)

experiment results

(p.58)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.59)

distributed (vector) representation of words

embed word in vector space, such that nearby words are syntactically
or semantically similar

neural nets can be used to learn these vectors from raw text
[collobert et al., 2011, chen et al., 2013]

(p.60)

use as distributed representation

idea: replace/append original features with distributed representation.

expected to work if:

1 original features are too sparse (e.g. small training data)

2 distributed representation enables more    exible model of language

(p.61)

case study 1: id52 of web text

motivation: id52 performs well on wsj (news) but poorly on web
text. one main reason is unknown/rare words.

distributed representation ensures unknown words can be tagged
because similar words occur in wsj treebank

*id56lm = recurrent neural net language model [mikolov et al., 2011]

(p.62)

id52 accuracy (sancl2012 shared task data)

conclusion:

distributed representation helps. but brown id91 is just as good.
what to improve: representation? crf integration? joint learning?

(p.63)

case study 2: parsing with compositional vector
grammar [socher et al., 2013a]

background: parsing results can be improved by splitting coarse
categories (e.g. np, vp).

here, rather than splitting, directly learn distributed representation of
phrases

(p.64)

compositional vector grammar [socher et al., 2013a]

begin with word representation. phrase vector is output of 1 layer
net, compute recursively bottom-up.
the score of e.g. node p(1) is v (b,c )p(1) + log p(p1     bc )
predicted parse tree is the one that achieves max sum of node scores.

weight matrix (for each node type) is trained by structured
max-margin objective

(p.65)

wsj section 23 experiment results

system
stanford parser (pid18)
stanford parser (factored)
berkeley parser
compositional vector grammar

f1
85.5
86.6
90.1
90.4

end-to-end distributed representation outperforms both manually factored
and automatically split state systems.

(p.66)

case study summary

exploiting non-linear classi   ers

it   s possible to directly apply deep learning to text problems with
little modi   cation, as evidenced by [glorot et al., 2011]

but sometimes nlp-speci   c modi   cations are needed, e.g. training
objective mismatch in machine translation n-best experiment

exploiting distributed representation

distributed representation is a simple way to improve robustness of
nlp, but it   s not the only way (id52 experiment)

promising direction: distributed representations beyond words,
considering e.g. compositionality [socher et al., 2013a]

note: the above uses are really two sides of the same coin, not
mutually-exclusive.

(p.67)

outline

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.68)

a taxonomy of neural nets in machine translation

core engine: what is being modeled?

target word id203:

(cid:73) language model: p(target wordt | target wordt   1)
(cid:73) language model with source information:
p(target wordt | target wordt   1, source)

translation/reordering probabilities under phrase-based mt:
(cid:73) translation model: p(target phrase | source phrase)
(cid:73) reordering model: p(orientation | target phrase, source phrase)

probabilities under tuple-based mt:
p([target phrase, source phrase]t | [target phrase, source phrase]t   1)
inversion transduction grammar (itg) model

related components:

word alignment

adaptation / topic context

multilingual embeddings

(p.69)

a taxonomy of neural nets in machine translation

core engine: what is being modeled?

target word id203:

(cid:73) language model: [schwenk et al., 2012, vaswani et al., 2013,

niehues and waibel, 2013, auli and gao, 2014]

(cid:73) lm w/ source: [kalchbrenner and blunsom, 2013, auli et al., 2013,

devlin et al., 2014, cho et al., 2014, bahdanau et al., 2014,
sundermeyer et al., 2014, sutskever et al., 2014]

translation/reordering probabilities under phrase-based mt:

(cid:73) translation: [maskey and zhou, 2012, schwenk, 2012, liu et al., 2013,
gao et al., 2014a, lu et al., 2014, tran et al., 2014, wu et al., 2014a]

(cid:73) reordering: [li et al., 2014b]

tuple-based mt: [son et al., 2012, wu et al., 2014b, hu et al., 2014]
itg model: [li et al., 2013, zhang et al., 2014, liu et al., 2014]

related components:

word align: [yang et al., 2013, tamura et al., 2014, songyot and chiang, 2014]
adaptation / topic context: [duh et al., 2013, cui et al., 2014]
multilingual embeddings:
[klementiev et al., 2012, lauly et al., 2013, zou et al., 2013, ko  cisk  y et al., 2014,
faruqui and dyer, 2014, hermann and blunsom, 2014, chandar et al., 2014]

(p.70)

next, we   ll discuss...

core engine: what is being modeled?

target word id203:

(cid:73) language model: [vaswani et al., 2013, auli and gao, 2014]
(cid:73) lm w/ source:

[kalchbrenner and blunsom, 2013, devlin et al., 2014, sutskever et al., 2014]

translation/reordering probabilities under phrase-based mt:

(cid:73) translation: [gao et al., 2014a]
(cid:73) reordering

tuple-based mt: [son et al., 2012]

itg model: [zhang et al., 2014]

related components:

word align

adaptation / topic context

multilingual embeddings: [klementiev et al., 2012]

(obviously there   s no time to discuss everything. these papers are chosen to
pedagogically demonstrate the diverse ways in which neural nets are used in mt.)

(p.71)

language models (lm) using neural nets

model p(current word | previous words) using neural nets.

(cid:73) motivation: continuous distributed representations of words learned by

neural nets reduce sparsity problems

example rare word:    bar-ba-loots   

(cid:73) p(wt = fruits | wt   2 = like, wt   2 = bar -ba-loots) =?
| wt   2 = like, wt   2 = bar -ba-loots) =?
(cid:73) p(wt = bars
(cid:73) which has higher id203?
(cid:73) what if i tell you vector(bar-ba-loots) is similar to vector(bears)?

feed-forward neural net language model:

(cid:73) (1) map wt   1, wt   2, . . . to vectors. (2) compress. (3) predict wt

recurrent neural net language model:

(cid:73) (1) map wt   1 to vector.

(2) combine with previous state & compress.
(3) predict wt

(p.72)

(feed-forward) neural language models [bengio et al., 2003]

p(current word = k) = yk =

(cid:80)

exp(w t
jk h)
k(cid:48) exp(w t

jk(cid:48) h)

y1

h1

v2

wjk

m

v1

y2

h2

y3

h3

v3

compression: h =   (m t v )

v4

distributed representation

distributed representation

wij

x1

x2

x3

x4

x5

x6

word at t-2, [x1, x2, x3] = [0, 1, 0]

word at t-1, [x4, x5, x6] = [1, 0, 0]

(p.73)

training feed-forward neural lms

training data = sets of id165

(cid:73) supervised task: given previous n-1 words, predict current word
(cid:73) standard id26 works
(cid:73) deeper nets are possible [arisoy et al., 2012] (minor gains?)

by-product: [wij ]i can be used as    id27s   . useful for
many applications [zhila et al., 2013, turian et al., 2010]
in practice:

(cid:80)

(cid:73) yk =
(cid:73) many speed-up techniques proposed, e.g. class-based vocabulary,

exp(w t
jk h)
jk(cid:48) h) requires expensive summation k over vocabulary size
k(cid:48) exp(w t

noise-contrastive estimation, approximate id172

(cid:73) if we only need embeddings, alternative models are recommended, esp.

[collobert et al., 2011], id97 [mikolov et al., 2013]

(p.74)

recurrent neural net language models [mikolov et al., 2010]

model p(current word|previous words) with a recurrent hidden layer

current word (assume 3-word vocabulary)

y1

y2

wjk

wij

x1

x2

y3

h1

x3

h2

x4

x5

previous word

previous h

(cid:80)

id203 of word k:

exp(w t
jk h)
k(cid:48) exp(w t

yk =

jk(cid:48) h)
[x4, x5] is a copy of
[h1, h2] from the
previous time-step
hj =   (w t
ij xi ) is hidden
state of partial sentence

arbitrarily-long history is
(theoretically) kept
through recurrence

(p.75)

training recurrent nets: id26 through time

unroll the hidden states for certain time-steps.
given error at y , update weights by id26
example: bar-ba-loots like | fruits
y1

y2

y3

wjk

wij

h1

x3
x1
   like    [x1, x2, x3] = [1, 0, 0]

x2

wij

h2

h(cid:48)

1

h(cid:48)

2

previous h

x1

x2

x3

h(cid:48)(cid:48)

1

h(cid:48)(cid:48)

2

   bar-ba-loots    [x1, x2, x3] = [0, 1, 0]

initial h

(p.76)

neural language model decoder integration

feed-forward neural lms have same form as id165s, so
straightforward to integrate into mt decoder

(cid:73) caveat: for calculation speed-up (esp. id172 constant), resort

to approximations and caching.

recurrent neural lms require history going back to start-of-sentence.
harder to do id145.

(cid:73) [auli and gao, 2014]: to score new words, each decoder state needs to

maintain h. for recombination, merge hypotheses by traditional
id165 context but keep a beam of h   s.

(p.77)

neural language models generally improve mt

feed-forward neural lm [vaswani et al., 2013]

baseline (id165)
1000-best rescoring

decoding

nist06
zh-en
34.3
34.7
34.9

wmt06

fr-en de-en es-en
32.0
25.5
26.0
32.2
32.1
26.1

21.5
21.5
21.9

recurrent neural lm [auli and gao, 2014]

wmt12 fr-en wmt12 de-en

baseline (id165)
100-best rescoring
lattice rescoring

decoding

24.85
25.74
26.43
26.86

19.80
20.54
20.63
20.93

(p.78)

next, we   ll discuss...

core engine: what is being modeled?

target word id203:

(cid:73) language model: [vaswani et al., 2013, auli and gao, 2014]
(cid:73) lm w/ source:

[kalchbrenner and blunsom, 2013, devlin et al., 2014, sutskever et al., 2014]

translation/reordering probabilities under phrase-based mt:

(cid:73) translation: [gao et al., 2014a]
(cid:73) reordering

tuple-based mt: [son et al., 2012]

itg model: [zhang et al., 2014]

related components:

word align

adaptation / topic context

multilingual embeddings: [klementiev et al., 2012]

(p.79)

language model with source

model p(target wordt | target wordt   1, source)
main question is how to model source:

(cid:73) entire source sentence or aligned source words only?
(cid:73) vector representation or traditional words?
(cid:73) if vector representation, how to compute it?

(p.80)

model of [devlin et al., 2014]

extend feed-forward neural lm to include window around aligned
source word.

(cid:73) heuristic: if align to multiple source words, choose middle. if

unaligned, inherit alignment from closest target word

train on bitext with alignment; optimize target likelihood.

(p.81)

model of [kalchbrenner and blunsom, 2013]

(f=target, e=source for our purposes here)

extend recurrent lm with vector representation of source sentence

(cid:73) a matrix k convolves arbitrary-length source sentence into vector(s) of

predetermined dimension

train on bitext without alignment; optimize target likelihood.

(p.82)

model of [sutskever et al., 2014]

(   a b c    is source sentence;    w x y z    is target sentence)

treats mt as general sequence-to-sequence transduction

(cid:73) (1) read source (2) accumulate hidden state, (3) generate target.
(cid:73) end-of-sentence    (cid:104)eos(cid:105)    token stops the recurrent process.
(cid:73) in practice, read input sentence in reverse gave better mt results.

used long short-term memory (lstm); better modeling of
long-range dependencies than basic recurrent nets.

train on bitext; optimize target likelihood. (common in all lm w/ source models)

(p.83)

next, we   ll discuss...

core engine: what is being modeled?

target word id203:

(cid:73) language model: [vaswani et al., 2013, auli and gao, 2014]
(cid:73) lm w/ source:

[kalchbrenner and blunsom, 2013, devlin et al., 2014, sutskever et al., 2014]

translation/reordering probabilities under phrase-based mt:

(cid:73) translation: [gao et al., 2014a]
(cid:73) reordering

tuple-based mt: [son et al., 2012]

itg model: [zhang et al., 2014]

related components:

word align

adaptation / topic context

multilingual embeddings: [klementiev et al., 2012]

(p.84)

neural net translation model under phrase-based mt

(cid:80)

k   k   k (target, source, align) where   k are language

recall log-linear mt formulation:
arg maxtarget
model, translation model scores, etc.
translation model score p(target phrase | source phrase) is
conventionally based on counts (max likelihood estimate)
potential advantages of replacing this score with neural net score:

(cid:73) alleviate data sparsity
(cid:73) enable complex scoring functions
(cid:73) incorporate more source side context e.g. [tran et al., 2014]

easy to add as feature, with no decoder modi   cation.

(p.85)

model of [gao et al., 2014a]

two neural nets (one for source side, one for target side)

(cid:73) input: bag-of-words representation of source/target phrase
(cid:73) output: vectors yfi for source phrase, yej for target phrase
t yej

score of phrase pair = dot product of these vectors yfi

(p.86)

training procedure of [gao et al., 2014a]

1 baseline mt generates n-best list for training data.

(cid:73) key assumption: oracle in n-best is much better than 1-best, so it   s

worthwhile to train the neural nets.

2 optimize neural net parameters:

(cid:73) use    expected id7   1 objective to enable smooth gradients:

  expid7(w )
  scorew (yfi ,yej )

  scorew (yfi ,yej )

  w

ei ,fj

3 optimize (mert) feature weights    in log-linear model

k   k   k (target, source, align). [loop if desired]

(cid:80)

(cid:80)

note: alternative models / training procedures are possible, e.g.

pairwise ranking (pro) objective on dev set [liu et al., 2013]

direct training on extracted phrase table [schwenk, 2012]

rbms/autoencoders on top of conventional phrase features
[maskey and zhou, 2012, lu et al., 2014]

1expected id7: (cid:80)

e   nbest p(e|f )sentid7(e , eref )

(p.87)

next, we   ll discuss...

core engine: what is being modeled?

target word id203:

(cid:73) language model: [vaswani et al., 2013, auli and gao, 2014]
(cid:73) lm w/ source:

[kalchbrenner and blunsom, 2013, devlin et al., 2014, sutskever et al., 2014]

translation/reordering probabilities under phrase-based mt:

(cid:73) translation: [gao et al., 2014a]
(cid:73) reordering

tuple-based mt: [son et al., 2012]

itg model: [zhang et al., 2014]

related components:

word align

adaptation / topic context

multilingual embeddings: [klementiev et al., 2012]

(p.88)

tuple-based mt [son et al., 2012]

p([target phrase, source phrase]t | [target phrase, source phrase]t   1)

a target and source phrase tuple forms a single unit (u)
apply standard neural language model to score sequences of u

(cid:73) challenge: large output space using tuple.
(cid:73) one solution: factorize!

(p.89)

inversion transduction grammar (itg) model

itg views translation as bilingual
parsing, allows phrase blocks to
combine hierarchically.

basic component:
p(straight/inverted | block1, block2)
can use neural net [li et al., 2013]

[zhang et al., 2014] extension:
constrain phrase embeddings of
translations to be similar.

advantage: elegantly models
monolingual composition and
bilingual equivalence in uni   ed
framework.

(p.90)

next, we   ll discuss...

core engine: what is being modeled?

target word id203:

(cid:73) language model: [vaswani et al., 2013, auli and gao, 2014]
(cid:73) lm w/ source:

[kalchbrenner and blunsom, 2013, devlin et al., 2014, sutskever et al., 2014]

translation/reordering probabilities under phrase-based mt:

(cid:73) translation: [gao et al., 2014a]
(cid:73) reordering

tuple-based mt: [son et al., 2012]

itg model: [zhang et al., 2014]

related components:

word align

adaptation / topic context

multilingual embeddings: [klementiev et al., 2012]

(p.91)

multilingual embeddings

what: train word representations such that words in di   erent
languages map to same space
why: useful for many cross-lingual tasks as well as mt

(cid:73) train classi   er on large labeled english data; test on xhosa

main questions:

(cid:73) amount of multilingual info: parallel bitext? comparable corpora?

word alignment table?

(cid:73) how to multilingual info incorporated?

(p.92)

multilingual embeddings from [klementiev et al., 2012]

optimize independent neural language models,
with regularizer     = vec(wordi )t    aij    vec(wordj )
enforcing word vectors to be similar if alignment score aij is high:

log p(en wordt | en wordt   1) + log p(zh wordt | zh wordt   1) +    

example embeddings & nearby words:

[zou et al., 2013] proposed similar model (di   erent language model/regularizer),
show mt gains by adding embedding similarity as translation model score

(p.93)

discussion: outlook on neural nets for mt

active    eld! still lots to try. e.g.

model tree/forest-based machine translation

even better decoder integration

more synergy with id152

move beyond parallel bitext; exploit comparable corpora

improve existing work; experiments on more tasks by more researchers

three main questions to consider if you want to start:

1 what to model? i.e. what is input/output of neural net?

2 how to setup training data? (input/ouput is often not explicit in mt)

3 what kind of network and training algorithm? what are reasonable

hyper-parameters to try? details matter.

but also be humble! lots of ideas hidden in older work, e.g.
[castano et al., 1997, jain et al., 1991]

(p.94)

summary of entire talk

1 deep learning background

neural networks (1-layer, 2-layer)
potentials and di   culties of deep architecture
the breakthrough in 2006

2 two main types of deep architectures

deep belief nets (dbn) [hinton et al., 2006]
stacked auto-encoders (sae) [bengio et al., 2006]
current status of deep learning

3 applications in natural language processing and machine translation

use as non-linear classi   er
use as distributed representation
survey of machine translation research

(p.95)

final thoughts

is deep learning just a fad?

what ideas will stand the test of time?

should i jump on the bandwagon?

(p.96)

to learn more

survey paper:

(cid:73) yoshua bengio   s [bengio, 2009] short book: learning deep

architectures for ai2

courses & in-depth lecture notes:

(cid:73) my course @ naist:

http://cl.naist.jp/~kevinduh/a/deep2014/

(cid:73) hugo larochelle   s course @ sherbrooke3
(cid:73) geo    hinton   s coursera course4

tutorials for nlpers:

(cid:73) richard socher et. al.   s naacl2013 tutorial5
(cid:73) ed grefenstette et.al. acl2014 tutorial6

to learn even more:

(cid:73) theano code samples: http://deeplearning.net/tutorial/
(cid:73) blog at http://deeplearning.net

2http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf
3http://tinyurl.com/qccl66y
4https://www.coursera.org/course/neuralnets
5http://www.socher.org/index.php/deeplearningtutorial/
6https://www.youtube.com/watch?v=_asoqxiwbvo

(p.97)

thanks for your attention! questions?

(p.98)

references:

andreas, j. and klein, d. (2014).
how much do id27s encode about syntax?
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: short papers), pages
822   827, baltimore, maryland. association for computational linguistics.

arisoy, e., sainath, t. n., kingsbury, b., and ramabhadran, b. (2012).
deep neural network language models.
in proceedings of the naacl-hlt 2012 workshop: will we ever really replace the id165 model? on the future of
id38 for hlt, pages 20   28, montr  eal, canada. association for computational linguistics.

auli, m., galley, m., quirk, c., and zweig, g. (2013).
joint language and translation modeling with recurrent neural networks.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1044   1054, seattle,
washington, usa. association for computational linguistics.

auli, m. and gao, j. (2014).
decoder integration and expected id7 training for recurrent neural network language models.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: short papers), pages
136   142, baltimore, maryland. association for computational linguistics.

bahdanau, d., cho, k., and bengio, y. (2014).
id4 by jointly learning to align and translate.
corr, abs/1409.0473.

bansal, m., gimpel, k., and livescu, k. (2014).
tailoring continuous word representations for id33.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: short papers), pages
809   815, baltimore, maryland. association for computational linguistics.

baroni, m., dinu, g., and kruszewski, g. (2014).
don   t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
238   247, baltimore, maryland. association for computational linguistics.

(p.99)

bengio, y. (2009).
learning deep architectures for ai, volume foundations and trends in machine learning.
now publishers.

bengio, y., ducharme, r., vincent, p., and jauvin, c. (2003).
a neural probabilistic language model.
jmlr.

bengio, y., lamblin, p., popovici, d., and larochelle, h. (2006).
greedy layer-wise training of deep networks.
in nips   06, pages 153   160.

bergstra, j., bardenet, r., bengio, y., and k  egel, b. (2011).
algorithms for hyper-parameter optimization.
in proc. neural information processing systems 24 (nips2011).

billingsley, r. and curran, j. (2012).
improvements to training an id56 parser.
in proceedings of coling 2012, pages 279   294, mumbai, india. the coling 2012 organizing committee.

bishop, c. (1995).
neural networks for pattern recognition.
oxford university press.

blitzer, j., dredze, m., and pereira, f. (2007).
biographies, bollywood, boom-boxes and blenders: id20 for sentiment classi   cation.
in acl.

bordes, a., chopra, s., and weston, j. (2014).
id53 with subgraph embeddings.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 615   620,
doha, qatar. association for computational linguistics.

carreira-perpinan, m. a. and hinton, g. e. (2005).
on contrastive divergence learning.
in aistats.

(p.100)

castano, m. a., casacuberta, f., and vidal, e. (1997).
machine translation using neural networks and    nite-state models.
in 7ths interantional conference on theoretical and methodological issues in machine translation (tmi).

chandar, a. p. s., lauly, s., larochelle, h., khapra, m. m., ravindran, b., raykar, v., and saha, a. (2014).
an autoencoder approach to learning bilingual word representations.
in nips.

chang, k.-w., yih, w.-t., yang, b., and meek, c. (2014).
typed tensor decomposition of knowledge bases for id36.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1568   1579,
doha, qatar. association for computational linguistics.

chen, d. and manning, c. (2014).
a fast and accurate dependency parser using neural networks.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 740   750,
doha, qatar. association for computational linguistics.

chen, w., zhang, y., and zhang, m. (2014a).
feature embeddings for id33.
in proceedings of coling.

chen, x., liu, z., and sun, m. (2014b).
a uni   ed model for word sense representation and disambiguation.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1025   1035,
doha, qatar. association for computational linguistics.

chen, y., perozzi, b., al-rfou, r., and skiena, s. (2013).
the expressive power of id27s.
in icml 2013 workshop on deep learning for audio, speech, and language processing.

cho, k., van merrienboer, b., gulcehre, c., bougares, f., schwenk, h., and bengio, y. (2014).
learning phrase representations using id56 encoder-decoder for id151s.
in conference on empirical methods in natural language processing (emnlp) 2014, number 1406.1078 in cs.cl.

(p.101)

coates, a., huval, b., wang, t., wu, d. j., catanzaro, b., and ng, a. y. (2013).
deep learning with cots hpc systems.
in proceedings of the international conference on machine learning (icml).

collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p. (2011).
natural language processing (almost) from scratch.
journal of machine learning research, 12:2493   2537.

cui, l., zhang, d., liu, s., chen, q., li, m., zhou, m., and yang, m. (2014).
learning topic representation for smt with neural networks.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
133   143, baltimore, maryland. association for computational linguistics.

dean, j., corrado, g. s., monga, r., chen, k., devin, m., le, q. v., mao, m. z., ranzato, m., senior, a., tucker, p., yang,
k., and ng, a. y. (2012).
large scale distributed deep networks.
in neural information processing systems (nips).

devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j. (2014).
fast and robust neural network joint models for id151.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1370   1380, baltimore, maryland. association for computational linguistics.

duh, k. and kirchho   , k. (2008).
beyond id148: boosted minimum error rate training for n-best re-ranking.
in proceedings of acl-08: hlt, short papers, pages 37   40, columbus, ohio. association for computational linguistics.

duh, k., neubig, g., sudoh, k., and tsukada, h. (2013).
adaptation data selection using neural language models: experiments in machine translation.
in proceedings of the 51th annual meeting of the association for computational linguistics (volume 2: short papers), so   a,
bulgaria. association for computational linguistics.

erhan, d., bengio, y., courville, a., manzagol, p., vincent, p., and bengio, s. (2010).
why does unsupervised pre-training help deep learning?
journal of machine learning research, 11:625   660.

(p.102)

erhan, d., manzagol, p., bengio, y., bengio, s., and vincent, p. (2009).
the di   culty of training deep architectures and the e   ect of unsupervised pre-training.
in aistats.

faruqui, m. and dyer, c. (2014).
improving vector space word representations using multilingual correlation.
in proceedings of the 14th conference of the european chapter of the association for computational linguistics, pages
462   471, gothenburg, sweden. association for computational linguistics.

fu, r., guo, j., qin, b., che, w., wang, h., and liu, t. (2014).
learning semantic hierarchies via id27s.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1199   1209, baltimore, maryland. association for computational linguistics.

fyshe, a., talukdar, p. p., murphy, b., and mitchell, t. m. (2014).
interpretable semantic vectors from a joint model of brain- and text- based meaning.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
489   499, baltimore, maryland. association for computational linguistics.

gao, j., he, x., yih, w.-t., and deng, l. (2014a).
learning continuous phrase representations for translation modeling.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
699   709, baltimore, maryland. association for computational linguistics.

gao, j., pantel, p., gamon, m., he, x., and deng, l. (2014b).
modeling interestingness with deep neural networks.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 2   13, doha,
qatar. association for computational linguistics.

gardner, m., talukdar, p., krishnamurthy, j., and mitchell, t. (2014).
incorporating vector space similarity in random walk id136 over knowledge bases.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 397   406,
doha, qatar. association for computational linguistics.

glorot, x., bordes, a., and bengio, y. (2011).
id20 for large-scale sentiment classication: a deep learning approach.
in icml.

(p.103)

guo, j., che, w., wang, h., and liu, t. (2014).
revisiting embedding features for simple semi-supervised learning.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 110   120,
doha, qatar. association for computational linguistics.

hashimoto, k., miwa, m., tsuruoka, y., and chikayama, t. (2013).
simple customization of id56s for semantic relation classi   cation.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1372   1376, seattle,
washington, usa. association for computational linguistics.

hashimoto, k., stenetorp, p., miwa, m., and tsuruoka, y. (2014).
jointly learning word representations and composition functions using predicate-argument structures.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1544   1555,
doha, qatar. association for computational linguistics.

hermann, k. m. and blunsom, p. (2013).
the role of syntax in vector space models of id152.
in proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: long papers), pages
894   904, so   a, bulgaria. association for computational linguistics.

hermann, k. m. and blunsom, p. (2014).
multilingual models for compositional distributed semantics.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
58   68, baltimore, maryland. association for computational linguistics.

hermann, k. m., das, d., weston, j., and ganchev, k. (2014).
semantic frame identi   cation with distributed word representations.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1448   1458, baltimore, maryland. association for computational linguistics.

hinton, g., deng, l., yu, d., dahl, g., a.mohamed, jaitly, n., senior, a., vanhoucke, v., nguyen, p., sainath, t., and
kingsbury, b. (2012a).
deep neural networks for acoustic modeling in id103.
ieee signal processing magazine, 29.

(p.104)

hinton, g., osindero, s., and teh, y.-w. (2006).
a fast learning algorithm for deep belief nets.
neural computation, 18:1527   1554.

hinton, g. e., srivastava, n., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2012b).
improving neural networks by preventing co-adaptation of feature detectors.
corr, abs/1207.0580.

hu, y., auli, m., gao, q., and gao, j. (2014).
minimum translation modeling with recurrent neural networks.
in proceedings of the 14th conference of the european chapter of the association for computational linguistics, pages
20   29, gothenburg, sweden. association for computational linguistics.

irsoy, o. and cardie, c. (2014).
opinion mining with deep recurrent neural networks.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 720   728,
doha, qatar. association for computational linguistics.

iyyer, m., boyd-graber, j., claudino, l., socher, r., and daum  e iii, h. (2014).
a neural network for factoid id53 over paragraphs.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 633   644,
doha, qatar. association for computational linguistics.

jain, a. n., mcnair, a. e., waibel, a., saito, h., hauptmann, a. g., and tebelskis, j. (1991).
connectionist and symbolic processing in speech-to-speech translation: the janus systems.
in mt summit, volume 3, pages 113   117.

ji, y. and eisenstein, j. (2014).
representation learning for text-level discourse parsing.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
13   24, baltimore, maryland. association for computational linguistics.

kalchbrenner, n. and blunsom, p. (2013).
recurrent continuous translation models.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1700   1709, seattle,
washington, usa. association for computational linguistics.

(p.105)

kiela, d. and bottou, l. (2014).
learning image embeddings using convolutional neural networks for improved multi-modal semantics.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 36   45, doha,
qatar. association for computational linguistics.

klementiev, a., titov, i., and bhattarai, b. (2012).
inducing crosslingual distributed representations of words.
in proceedings of coling 2012, pages 1459   1474, mumbai, india. the coling 2012 organizing committee.

ko  cisk  y, t., hermann, k. m., and blunsom, p. (2014).
learning bilingual word representations by marginalizing alignments.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: short papers), pages
224   229, baltimore, maryland. association for computational linguistics.

lauly, s., boulanger, a., and larochelle, h. (2013).
learning id73 representations using a bag-of-words autoencoder.
in nips 2013 deep learning workshop.

le, p. and zuidema, w. (2014).
the inside-outside id56 model for id33.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 729   739,
doha, qatar. association for computational linguistics.

le, q. v., ranzato, m., monga, r., devin, m., chen, k., corrado, g. s., dean, j., and ng, a. y. (2012).
building high-level features using large scale unsupervised learning.
in icml.

lee, h., grosse, r., ranganath, r., and ng, a. (2009).
convolutional id50 for scalable unsupervised learning of hierarchical representations.
in icml.

levy, o. and goldberg, y. (2014).
dependency-based id27s.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: short papers), pages
302   308, baltimore, maryland. association for computational linguistics.

(p.106)

li, j., li, r., and hovy, e. (2014a).
recursive deep models for discourse parsing.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 2061   2069,
doha, qatar. association for computational linguistics.

li, p., liu, y., and sun, m. (2013).
recursive autoencoders for itg-based translation.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 567   577, seattle,
washington, usa. association for computational linguistics.

li, p., liu, y., sun, m., izuha, t., and zhang, d. (2014b).
a neural reordering model for phrase-based translation.
in proceedings of coling 2014, the 25th international conference on computational linguistics: technical papers, pages
1897   1907, dublin, ireland. dublin city university and association for computational linguistics.

liu, l., watanabe, t., sumita, e., and zhao, t. (2013).
additive neural networks for id151.
in proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: long papers), pages
791   801, so   a, bulgaria. association for computational linguistics.

liu, s., yang, n., li, m., and zhou, m. (2014).
a recursive recurrent neural network for id151.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1491   1500, baltimore, maryland. association for computational linguistics.

liu, y., hua zhong, s., and li, w. (2012).
query-oriented id57 via unsupervised deep learning.
in aaai conference on arti   cial intelligence.

lu, s., chen, z., and xu, b. (2014).
learning new semi-supervised deep auto-encoder features for id151.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
122   132, baltimore, maryland. association for computational linguistics.

(p.107)

luong, t., socher, r., and manning, c. (2013).
better word representations with id56s for morphology.
in proceedings of the seventeenth conference on computational natural language learning, pages 104   113, so   a, bulgaria.
association for computational linguistics.

ma, j., zhang, y., and zhu, j. (2014).
tagging the web: building a robust web tagger with neural network.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
144   154, baltimore, maryland. association for computational linguistics.

martens, j. (2010).
deep learning via hessian-free optimization.
in proceedings of the 27th international conference on machine learning (icml).

maskey, s. and zhou, b. (2012).
unsupervised deep belief features for speech translation.
in icassp.
mikolov, t., deoras, a., povey, d., burget, l., and   cernock  y, j. (2011).
strategies for training large scale neural network language model.
in asru.
mikolov, t., kara   at, s., burget, l.,   cernock  y, j., and khudanpur, s. (2010).
recurrent neural network based language models.
in proceedings of the 11th annual conference of the international speech communication association (interspeech 2010).

mikolov, t., sutskever, i., chen, k., corrado, g., and dean, j. (2013).
distributed representations of words and phrases and their compositionality.
in nips.

milajevs, d., kartsaklis, d., sadrzadeh, m., and purver, m. (2014).
evaluating neural word representations in tensor-based compositional settings.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 708   719,
doha, qatar. association for computational linguistics.

(p.108)

neelakantan, a., shankar, j., passos, a., and mccallum, a. (2014).
e   cient non-parametric estimation of multiple embeddings per word in vector space.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1059   1069,
doha, qatar. association for computational linguistics.

niehues, j. and waibel, a. (2013).
continuous space language models using restricted id82s.
in iwlt.

pei, w., ge, t., and chang, b. (2014).
max-margin tensor neural network for chinese id40.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
293   303, baltimore, maryland. association for computational linguistics.

pennington, j., socher, r., and manning, c. (2014).
glove: global vectors for word representation.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1532   1543,
doha, qatar. association for computational linguistics.

qi, y., das, s., weston, j., and collobort, r. (2014).
a deep learning framework for character-based information extraction.
in proceedings of the european conference on information retrieval (ecir).

rockt  aschel, t., bo  snjak, m., singh, s., and riedel, s. (2014).
low-dimensional embeddings of logic.
in proceedings of the acl 2014 workshop on id29, pages 45   49, baltimore, md. association for computational
linguistics.

roth, m. and woodsend, k. (2014).
composition of word representations improves semantic role labelling.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 407   413,
doha, qatar. association for computational linguistics.

salakhutdinov, r. and hinton, g. (2009).
deep id82s.
in proceedings of the international conference on arti   cial intelligence and statistics, volume 5, pages 448   455.

(p.109)

schwenk, h. (2012).
continuous space translation models for phrase-based id151.
in coling (posters).

schwenk, h., rousseau, a., and attik, m. (2012).
large, pruned or continuous space language models on a gpu for id151.
in proceedings of the naacl-hlt 2012 workshop: will we ever really replace the id165 model? on the future of
id38 for hlt, pages 11   19, montr  eal, canada. association for computational linguistics.

socher, r., bauer, j., manning, c. d., and andrew y., n. (2013a).
parsing with compositional vector grammars.
in proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: long papers), pages
455   465, so   a, bulgaria. association for computational linguistics.

socher, r., huang, e. h., pennin, j., ng, a. y., and manning, c. d. (2011).
dynamic pooling and unfolding recursive autoencoders for paraphrase detection.
in nips.

socher, r., perelygin, a., wu, j., chuang, j., manning, c. d., ng, a., and potts, c. (2013b).
recursive deep models for semantic compositionality over a sentiment treebank.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631   1642, seattle,
washington, usa. association for computational linguistics.

son, l. h., allauzen, a., and yvon, f. (2012).
continuous space translation models with neural networks.
in naacl.

songyot, t. and chiang, d. (2014).
improving word alignment using word similarity.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1840   1845,
doha, qatar. association for computational linguistics.

srivastava, s., hovy, d., and hovy, e. (2013).
a walk-based semantically enriched tree kernel over distributed word representations.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1411   1416, seattle,
washington, usa. association for computational linguistics.

(p.110)

stenetorp, p. (2013).
transition-based id33 using id56s.
in nips 2013 deep learning workhop.

sundermeyer, m., alkhouli, t., wuebker, j., and ney, h. (2014).
translation modeling with id182.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 14   25, doha,
qatar. association for computational linguistics.

sutskever, i., martens, j., dahl, g., and hinton, g. (2013).
on the importance of initialization and momentum in deep learning.
in proceedings of the international conference on machine learning (icml).

sutskever, i., vinyals, o., and le, q. (2014).
sequence to sequence learning with neural networks.
in nips.

szeged, c., zaremba, w., sutskever, i., bruna, j., erhan, d., goodfellow, i., and fergus, r. (2014).
intriguing properties of neural networks.
in international conference on learning representations (iclr).

tamura, a., watanabe, t., and sumita, e. (2014).
recurrent neural networks for word alignment model.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1470   1480, baltimore, maryland. association for computational linguistics.

tran, k. m., bisazza, a., and monz, c. (2014).
word translation prediction for morphologically rich languages with bilingual neural networks.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1676   1688,
doha, qatar. association for computational linguistics.

tsubaki, m., duh, k., shimbo, m., and matsumoto, y. (2013).
modeling and learning semantic co-compositionality through prototype projections and neural networks.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 130   140, seattle,
washington, usa. association for computational linguistics.

(p.111)

tsuboi, y. (2014).
neural networks leverage corpus-wide information for part-of-speech tagging.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 938   950,
doha, qatar. association for computational linguistics.

turian, j., ratinov, l.-a., and bengio, y. (2010).
word representations: a simple and general method for semi-supervised learning.
in proceedings of the 48th annual meeting of the association for computational linguistics, pages 384   394, uppsala,
sweden. association for computational linguistics.

van de cruys, t. (2014).
a neural network approach to selectional preference acquisition.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 26   35, doha,
qatar. association for computational linguistics.

vaswani, a., zhao, y., fossum, v., and chiang, d. (2013).
decoding with large-scale neural language models improves translation.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1387   1392, seattle,
washington, usa. association for computational linguistics.

vinyals, o., jia, y., deng, l., and darrell, t. (2012).
learning with recursive perceptual representations.
in nips.

wang, m. and manning, c. d. (2013).
e   ect of non-linear deep architecture in sequence labeling.
in proceedings of the sixth international joint conference on natural language processing, pages 1285   1291, nagoya, japan.
asian federation of natural language processing.

weston, j., chopra, s., and adams, k. (2014).
#tagspace: semantic embeddings from hashtags.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1822   1827,
doha, qatar. association for computational linguistics.

(p.112)

wu, h., dong, d., hu, x., yu, d., he, w., wu, h., wang, h., and liu, t. (2014a).
improve id151 with context-sensitive bilingual semantic embedding model.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 142   146,
doha, qatar. association for computational linguistics.

wu, y., watanabe, t., and hori, c. (2014b).
recurrent neural network-based tuple sequence model for machine translation.
in proceedings of coling 2014, the 25th international conference on computational linguistics: technical papers, pages
1908   1917, dublin, ireland. dublin city university and association for computational linguistics.

yang, m.-c., duan, n., zhou, m., and rim, h.-c. (2014).
joint relational embeddings for knowledge-based id53.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 645   650,
doha, qatar. association for computational linguistics.

yang, n., liu, s., li, m., zhou, m., and yu, n. (2013).
word alignment modeling with context dependent deep neural network.
in proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: long papers), pages
166   175, so   a, bulgaria. association for computational linguistics.

yih, w.-t., he, x., and meek, c. (2014).
id29 for single-relation id53.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: short papers), pages
643   648, baltimore, maryland. association for computational linguistics.

zhang, j., liu, s., li, m., zhou, m., and zong, c. (2014).
bilingually-constrained phrase embeddings for machine translation.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
111   121, baltimore, maryland. association for computational linguistics.

zhang, x. and lapata, m. (2014).
chinese poetry generation with recurrent neural networks.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 670   680,
doha, qatar. association for computational linguistics.

(p.113)

zheng, x., chen, h., and xu, t. (2013).
deep learning for chinese id40 and id52.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 647   657, seattle,
washington, usa. association for computational linguistics.

zhila, a., yih, w.-t., meek, c., zweig, g., and mikolov, t. (2013).
combining heterogeneous models for measuring relational similarity.
in proceedings of the 2013 conference of the north american chapter of the association for computational linguistics:
human language technologies, pages 1000   1009, atlanta, georgia. association for computational linguistics.

zou, w. y., socher, r., cer, d., and manning, c. d. (2013).
bilingual id27s for phrase-based machine translation.
in proceedings of the 2013 conference on empirical methods in natural language processing, pages 1393   1398, seattle,
washington, usa. association for computational linguistics.

(p.114)

