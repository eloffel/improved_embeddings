deep learning for natural language processing 

tianchuan du 

department of computer and information 

sciences 

university of delaware 

newark, de 19711 

tdu@udel.edu 

 

 

 

 

 

 
 

vijay k. shanker 

department of computer and information 

sciences 

university of delaware 

newark, de 19711 

vijay@cis.udel.edu 

abstract 

2  deep learning 

learning  from 

deep  learning  has  emerged  as  a  new  area 
of  machine  learning  research.  it  tries  to 
mimic  the  human  brain,  which  is  capable 
of  processing  and 
the 
complex  input  data  and  solving  different 
kinds of complicated tasks well. it has been 
successfully  applied  to  several  fields  such 
as  images,  sounds,  text  and  motion.  the 
techniques  developed  from  deep  learning 
research  have  already  been  impacting  the 
research  of  natural  language  process.  this 
paper  reviews  the  recent  research  on  deep 
learning, 
recent 
development 
language 
processing. 

its  applications  and 

natural 

in 

1 

introduction 

deep  learning  has  emerged  as  a  new  area  of 
machine learning research since  2006 (hinton and 
salakhutdinov  2006;  bengio  2009;  arel,  rose  et 
al.  2010;  yoshua  2013).  deep 
learning  (or 
sometimes called id171 or representation 
learning)  is  a  set  of  machine  learning  algorithms 
which attempt  to  learn  multiple-layered  models  of 
inputs,  commonly  neural  networks.  the  deep 
neural networks are composed of multiple levels of 
non-linear  operations.  before  2006,  searching  the 
is  a 
parameter  space  of  deep  architectures 
nontrivial 
learning 
algorithms  have  been  proposed  to  resolve  this 
problem with notable success, beating the state-of-
the-art in certain areas (bengio 2009). 

recently  deep 

task,  but 

can  be 

learning  process 

a  central  idea  (bengio,  courville  et  al.  2013)  of 
deep  learning  is  referred  to  as  greedy  layerwise 
unsupervised  pre-training,  which  is  to  learn  a 
hierarchy  of  features  one  level  at  a  time.  the 
features 
purely 
unsupervised,  which  can 
take  advantage  of 
massive  unlabeled  data.  the  feature  learning  is 
trying  to  learn  a  new  transformation  of  the 
previously  learned  features  at  each level,  which  is 
able  to  reconstruct  the  original  data.  the  greedy 
layerwise  unsupervised  pre-training 
(hinton, 
osindero et al. 2006; bengio, lamblin et al. 2007; 
bengio 2009) is based on training each layer with 
an  unsupervised  learning  algorithm,  taking  the 
features produced at the previous level as input for 
the  next  level.  it  is  then  straightforward  to 
extracted  features  either  as  input  to  a  standard 
supervised  machine  learning  predictor  (such  as  an 
support  vector  machines  or  conditional  random 
field)  or  as  initialization  for  a  deep  supervised 
neural  network.  for  example,  each  iteration  of 
unsupervised  feature  learning  adds  one  layer  of 
weights  to  a  deep  neural  network.  finally,  the  set 
of layers with learned weights could be stacked to 
initialize  a  deep  supervised  predictor,  such  as  a 
neural  network  classifier,  or  a  deep  generative 
model,  such  as  a  deep  boltzmann  machine 
(salakhutdinov and hinton 2009).   
 

2.1  stacked auto-encoder 

one  good  illustration  of  the  idea  of  greedy 
layerwise  unsupervised  pre-training  is  the  stacked 
auto-encoder.  an  auto-encoder  is  an  artificial 

neural  network  used  for  learning  efficient  coding 
(liou,  huang  et  al.  2008).  the  aim  of  an  auto-
encoder  is  to  learn  a  compressed  representation 
(encoding)  for  a  set  of  data,  which  means  that  it 
was  being  used  for  dimensionality  reduction  or 
data compression. as shown in figure 1, the auto-
encoder is consisted of an input layer, a number of 
considerably  smaller  hidden  layers,  which  will 
form the encoding, and an output layer, which will 
try to reconstruct the input layer. it was shown that 
if linear neurons are used, or only a single sigmoid 
hidden layer, then the optimal solution to an auto-
encoder  is  strongly  related  to  pca  (bourlard  and 
kamp 1988). then use the learned feature to train 
another  layer  of  auto-encoder.  finally,  use  the 
learned weights to initialize a deep neural network 
as shown in figure 2. 

figure  1.  structure  of  auto-encoder.  set  the  output 
layer  same  as  the  input  layer  to  train  the  network.  the 
hidden layer is the learned feature of the input. 

 

figure  2.  stacked  auto-encoder.  use  the  weights  of 
auto-encoders to initialize the deep neural network. then 
fine-tune the whole network by back propagation. 

 

2.2  deep id82s 

input  data.  the 

another  way  to  implement  the  pre-training  is 
through restricted id82s (rbms) as 
explained  in  hinton   s  science  paper  (hinton  and 
salakhutdinov 2006). it uses the learned restricted 
boltzmann  machines  (rbms)  to  try  to  regenerate 
the  original 
learned  feature 
activations of one rbm are used as the  input data 
for training the next layer rbm in the stack. after 
the pre-training, the rbms are    unrolled    to create 
a  deep  network,  which  is  then  fine-tuned  using 
back-propagation  of  error  derivatives  as  shown  in 
figure  3.  the  stacks  of  rbms  will  create  deep 
boltzmann  machines  (salakhutdinov  and  hinton 
2009). then use the pre-trained dbm to initialize a 
deep  neural  network  and 
train  with  back 
propagation as the stacked auto-encoder explained 
in the previous section. 

results. 

it  was 

time,  exploiting  an  unsupervised 

layers),  but  training  deeper  networks  consistently 
yielded  poorer 
sometimes 
considered  a  breakthrough  happened  in  2006: 
hinton  and  collaborators  at  university  of  toronto 
introduced  deep  belief  networks  or  dbns  for 
short  (hinton,  osindero  et  al.  2006),  with  a 
learning algorithm that greedily trains one layer at 
a 
learning 
algorithm  for  each  layer,  a  restricted  boltzmann 
machine  (rbm)(freund  and  haussler  1994). 
shortly  after,  related  algorithms  based  on  auto-
encoders  were  proposed  (poultney,  chopra  et  al. 
2006;  bengio,  lamblin  et  al.  2007),  which 
apparently follows the  same principle: guiding the 
training  of  intermediate  levels  of  representation 
using  unsupervised 
learning,  which  can  be 
performed  locally  at  each  level.  more  other 
algorithms  for  deep  architectures  were  proposed 
that  exploit  neither  rbms  nor  auto-encoders,  but 
they 
(mobahi, 
collobert et al. 2009; weston, ratle et al. 2012). 

the  same  principle 

followed 

2.4  multi-task  and  transfer  learning, 

id20 

tasks.  as  discussed  below, 

another  advantage  of  deep  learning  is  transfer 
learning.  transfer  learning  is  the  ability  of  a 
learning  algorithm 
to  exploit  commonalities 
between  different  learning  tasks  in  order  to  share 
statistical  strength,  and  transfer  knowledge  across 
different 
is 
hypothesized that id171 algorithms have 
an  advantage  for  such  tasks  because  they  learn 
features that capture underlying factors, a subset of 
features  which  may  be  relevant  for  a  particular 
task,  as  illustrated  in  figure  4.  this  hypothesis 
seems confirmed by a number of empirical results 
showing the advantages of id171 or deep 
learning algorithms in id20 and mult-
task (bengio, courville et al. 2013). 

it 

figure  3.  restricted  boltzmann  machines 

to 

 

compress images. 

2.3  why deep? 

one  of  the  main reasons  to  go  deep  is  that  a  non-
linear function can be more efficiently represented 
by  deep  architecture  with  fewer  parameters.  the 
most  formal  arguments  about  the  power  of  deep 
architectures  come 
into 
computational 
circuits.  the  
investigations suggests that when a function can be 
compactly  represented  by  a  deep  architecture,  it 
might  need  a  very 
to  be 
represented  by  an  insufficiently  deep  one  (bengio 
2009).  

large  architecture 

complexity  of 

investigations 

from 

in  another  word,  a  number  of  computational 
complexity  results  strongly  suggest  that  functions 
that  can  be  compactly  represented  with  a  deeper 
architecture  could  require  a  very  large  number  of 
elements in order to be represented by a shallower 
architecture.  because  each  parameter  of 
the 
architecture  might  have  to  be  selected  or  learned, 
using examples, these results suggest that depth of 
architecture  can  be  very  important  from  the  point 
of  view  of  statistical  efficiency.  another  reason  is 
that  deep  representations  might  allow  for  a 
hierarchical  representation.  and  multiple  levels  of 
latent  variables  allow  combinatorial  sharing  of 
statistical strength (bengio 2009). 
inspired  by  the  architectural  depth  of  the  brain, 
neural network researchers had wanted for decades 
to  train  deep  multi-layer  neural  networks  (utgoff 
and  stracuzzi  2002;  bengio  and  lecun  2007),  but 
it  was  not  successful  before  2006:  researchers 
reported  positive  experimental 
results  with 
typically two or three levels (i.e. one or two hidden 

3.1  object recognition 

object recognition is thought to be a nontrivial task 
for  computer.  mnist  digit  image  classification 
problem  has  been  used  as  benchmark  for  many 
machine  learning  algorithms,  deep  learning  was 
focused  on  the  problem  since  2006  (hinton, 
osindero et al. 2006; bengio, lamblin et al. 2007), 
outperforming  the  supremacy  of  id166s  (1.4% 
error)  on  this  dataset.  the  latest  records  are  still 
held  by  deep  networks:  ciresan  et  al.  (ciresan, 
meier  et  al.  2012)  currently  claims  the  title  of 
state-of-the-art for the unconstrained version of the 
task (e.g., using a convolutional architecture), with 
0.27% error, and rifai et al. (rifai, dauphin et al. 
2011)  is  state-of-the-art  for  the  knowledge  free 
version of mnist, with 0.81% error. 

in the last few years, deep learning has extended 
from digits to object recognition in natural images. 
the  latest  breakthrough  has  been  achieved  on  the 
id163  dataset,  which improve  the  state-of-the-
art  error  rate  from  26.1%  to  15.3%  (krizhevsky, 
sutskever et al. 2012). 

 

3.2  id103 and signal processing  

recognition  was  one  of 

speech 
the  early 
applications  of  neural  networks,  in  particular 
convolutional  (or  time-delay)  neural  networks.  
the  recent  revival  of  interest  in  neural  networks, 
deep learning, and representation learning has had 
a  strong  impact  in  the  area  of  speech  recognition. 
deep  learning  was  thought  to  yield  breakthrough 
results (dahl, mohamed et al. 2010; seide, li et al. 
2011; dahl, yu et al. 2012; mohamed, dahl et al. 
2012),  obtained  by  several  academics  as  well  as 
researchers  at  industrial  labs,  even  bringing  these 
algorithms to a larger scale and into products. for 
example,  microsoft  has  released  a  new  version  of 
their  mavis  (microsoft  audio  video  indexing 
service)  speech  system  based  on  deep  learning  in 
2012  (seide,  li  et  al.  2011).  in  this  paper,  the 
author  reduce  the  word  error  rate  on  four  major 
benchmarks  by  about  30%  (from  27.4%  to  18.5% 
on  rt03s)  compared  to  state-of-the-art  models 
based  on  gaussian  mixtures  for  the  acoustic 
modeling  and  trained  on  the  same  amount  of  data 
(309 hours of speech). similarly dahl (dahl, yu et 
al.  2012)    managed  to  decrease  the    relative  error 
rate by between 16% and 23% on a smaller large-
vocabulary  speech  recognition  benchmark  (bing 

fig. 4. illustration of a id171 model which 
discovers explanatory factors (the middle hidden layer in 
red).  the  shared  features  are  learned  unsupervised  or 
supervised.  because  these  subsets  overlap,  sharing  of 
statistical strength allows gains in generalization. 

 
the  illustrative  empirical  examples  are  the  two 
transfer  learning  challenges  held  in  2011  and  won 
by feature leaning or deep learning algorithms. the 
first  one  was  the  transfer  learning  challenge, 
which  held  at  an  icml  2011  workshop.  it  was 
won  using  unsupervised  layer-wise  pre-training 
(bengio  ;  mesnil,  dauphin  et  al.  2012).  a  second 
transfer  learning  challenge  was  held  at  nips 
2011   s  challenges 
in  learning  hierarchical 
models  workshop  and  also  won  by  deep  learning 
(goodfellow,  courville  et  al.  2012).  there  more  
examples  of  the  successful  application  of  feature 
learning  in  fields  related  to  transfer  learning 
include  domain  adaptation  (glorot,  bordes  et  al. 
2011; chen, xu et al. 2012).  

3  the applications of deep learning  

and 

learning 

during  the  past  several  years,  the  deep  learning 
techniques  have  already  been  impacting  a  wide 
range  of  machine 
artificial 
intelligence.  it  is  thought  that  moving  machine 
learning  closer  to  one  of  its  original  goals: 
artificial  intelligence.  it  has  been  successfully 
applied  to  several  fields  such  as  images,  sounds, 
text  and  motion.  the  rapid  increase  in  scientific 
activity on deep learning has been motivated by the 
empirical  successes  both  in  academia  and  in 
industry. 
 

mobile  business  search  dataset,  with  40  hours  of 
speech. 

has  been  successfully  used  in  speech  recognition 
experiments as reported in (dahl et al., 2012). 

the  standard  deep  neural  network  is  a  static 
classifier  with 
input  vectors  having  a  fixed 
dimensionality.  however,  many  practical  pattern 
recognition  and  information  processing  problems, 
including  speech  recognition,  machine  translation, 
natural  language  understanding,  video  processing 
and  bio-information  processing,  require  sequence 
recognition.  in  sequence  recognition,  sometimes 
called  classification  with  structured  input/output, 
the  dimensionality  of  both  inputs  and  outputs  are 
variable. one way to solve this problem is through 
the id48.  

other approaches to tackle the problem that  the 
dimensionality  of  both  inputs  and  outputs  are 
variable are based on recurrent neural networks or 
convolutional  network  (collobert  and  weston 
2008;  socher,  huang  et  al.  2011;  socher, 
pennington  et  al.  2011).  they  have  also  been 
applied to music, substantially beating the state-of-
the-art  in  polyphonic  transcription  (boulanger-
lewandowski, bengio et al. 2012), with a relative 
error  improvement  of  between  5%  and  30%  on  a 
standard benchmark of four different datasets. 

3.3  natural language processing  

besides id103, deep learning has been 
applied 
to  many  other  natural  language 
processing applications. one important application 
is  word  embedding.  the  idea  that  symbolic  data 
can  be  represented  via  distributed  representation 
for  was  introduced  by  hinton  (hinton  1986).  it 
was  first  developed  in  the  context  of  statistical 
language  modeling  by  bengio  et  al.  (bengio, 
ducharme  et  al.  2003).  the 
learning  of  a 
distributed  representation  for  each  word,  also 
called a id27.  

collobert  et  al.  (collobert  and  weston  2008; 
collobert,  weston  et  al.  2011)  applied  deep 
convolutional  network  to  implement  the  word 
embedding.  he  further  developed  the  senna 
system  that  shares  representations  across  different 
nlp  tasks.  this  is  also  strong  evidence  that  deep 
learning  has  the  transfer  learning  potential.  the 
result in this paper illustrated that the deep learning 
approaches  surpasses  the  state-of-the-art  on  most 
of  the  tasks  but  is  much  faster  than  traditional 
predictors.  
 

one major contribution of collobert   s work is to 
avoid 
feature 
engineering,  and  to  learn  versatility  and  unified 
features  automatically  from  deep  learning.  those 
learned  features  can  be  shared  by  all  natural 
language processing tasks. the system described in 
(collobert and weston 2008; collobert, weston et 
al. 
internal 
representation  from  vast  amounts  of  mostly 
unlabeled  training  data  (deng  and  yu  ;  bengio, 
courville  et  al.  2013).  it  de   nes  a  uni   ed 
architecture  for  natural  language  processing  that 
learns  features  that  are  relevant  to  the  many  well-
known  nlp 
including  part-of-speech 

automatically 

task-specific, 

   man-made    

learns 

2011) 

tasks 

figure 5: interface between dbn/dnn and id48 to 
form  a  dbn-id48  or  dnn-id48.  this  architecture 
has  been  successfully  used 
in  speech  recognition 
experiments reported in (dahl et al., 2012). 

 
the  id48  is  a  convenient  tool  to  model  the 
sequence data with variable length, which based on 
dynamic  programing  operations.  by  integrating 
static  classifiers  and  id48,  it  is  able  to  handle 
dynamic  or  sequential  patterns.  thus,  it  is  natural 
to  combine  deep  neural  network  and  id48  to 
bridge  the  gap  between  the  static  and  sequence 
pattern  recognition.  a  popular  architecture  to 
fulfill  this  is  shown  in  figure  5.  this  architecture 

recognition, 
tagging,  chunking,  named-entity 
learning a language model and the task of semantic 
role-labeling  given  very  limited  prior  knowledge. 
all  of  these  tasks  are  integrated  into  a  single 
system  which  is  trained  jointly.  all  the  tasks 
except  the  language  model  are  supervised  tasks 
with  labeled  training  data.  the  language  model  is 
trained  in  an  unsupervised  fashion  on  the  entire 
wikipedia website.  

is 

the  feature  selection 

in  an  end-to-end  fashion.  the 

the theme behind the deep learning approach is 
different from the traditional nlp approach, which 
is:  extract  from  the  sentence  a  rich  set  of  hand-
designed features which are then fed to a classical 
shallow  classi   cation  algorithm,  e.g.  a  support 
vector machine (id166), often with a linear kernel. 
in this way, the choice of features is a completely 
empirical process, mainly based on trial and error, 
and 
task  dependent, 
implying  additional  research  for  each  new  nlp 
task.  it  has  some  success  for  simple  nlp  tasks 
such  as  pos.  but  complex  tasks  like  srl  then 
require  a  large  number  of  possibly  complex 
features  (e.g.,  extracted  from  a  parse  tree)  which 
makes such systems slow and intractable for large-
scale applications. instead in this paper, the author 
proposed a deep neural network (nn) architecture, 
trained 
input 
sentence  is  processed  by  several  layers  of  feature 
extraction.  the  features  in  deep  layers  of  the 
network 
by 
id26  to  be  relevant  to  the  task.  the 
structure  is  summarized  in  figure  6.  in  this 
structure,  the     rst  layer  extracts  features  for  each 
word.  the  second  layer  extracts  features  from  the 
sentence  treating  it  as  a  sequence  with  local  and 
global structure. the following layers are classical 
nn  layers.  the  semi-supervised  training  of  srl 
using  the  language  model  performs  better  than 
other  combinations.  the  result  reported  in  this 
paper was as low as 14.30% in per-word error rate, 
which  beats  the  state-of-the-art,  16.54%,  based  on 
parse  trees  (pradhan  et  al.,  2004).  besides,  this 
system is the only one not to use pos tags or parse 
tree  features.  with  the  multiple  task  learning,  the 
author managed to obtain 2.91% for pos and 3.8% 
for chunking. pos error rates in the 3% range are 
state-of-the-art. 

automatically 

trained 

are 

figure  6.  a  general  deep  nn  architecture  for  nlp 
reported  in  collobert   s  work  (collobert  and  weston 
2008).  given  an  input  sentence,  the  nn  outputs  class 
probabilities for one chosen word. 

 

(id56) 

 
beside  the  standard  deep  neural  networks, 
recurrence  neutral  network 
is 
successfully  applied  to  many  aspects  of  natural 
language processing. stanford nlp group recently 
applied  id56  to  sentiment  analysis  for  semantic 
compositionality  (socher,  perelygin  et  al.).  it 
improves  the  state  of  the  art  in  single  sentence 
positive/negative  classification  from  80%  up  to 
85.4%.  id56  was  also  applied  to  parsing,  which 
improves the pid18 of the stanford parser by 3.8% 
to  obtain  an  f1  score  of  90.4%  (socher,  bauer  et 
al. 2013). the neural network language model was 
also  improved  by  adding  recurrence  to  the  hidden 
layers (mikolov, deoras et al. 2011), allowing it to 
surpass 
the  state-of-the-art  (smoothed  id165 
models)  not  only 
terms  of  perplexity 
(exponential of the average negative log-likelihood 
of predicting the right next word, going down from 
140 to 102) but also in terms of word error rate in 
speech  recognition,  decreasing  it  from  17.2% 
(kn5 baseline) or 16.9% (discriminative language 

in 

model)  to  14.4%  on  the  wall  street  journal 
benchmark  task.  it  have  also  been  applied  to 
id151 (schwenk, rousseau 
et  al.  2012),  which  improves  the  id7  score  by 
almost  2  points.  similar  structure,  recursive  auto-
encoders  (which  generalize  recurrent  networks) 
have  also  been  used  to  beat  the  state-of-the-art  in 
full  sentence  paraphrase  detection  (socher,  huang 
et  al.  2011)  almost  doubling  the  f1  score  for 
paraphrase  detection.  deep  learning  can  also  be 
to  perform  word  sense  disambiguation 
used 
(bordes,  glorot  et  al.  2012), 
the 
accuracy from 67.8% to 70.2%.  

improving 

4  conclusions 

investigations  are 

in sum, deep learning has becoming a new field of 
machine 
learning,  and  has  gained  extensive 
interests  in  different  research  area.  it  has  shown 
some  advantages  over  the  traditional  machine 
learning  methods  in  some  fields.  although  deep 
learning  works  well  in  many  machine  learning 
tasks, it works equally poorly in some areas as the 
other  learning  methods.  besides  most  of  the  deep 
learning 
solid 
theoretical foundations of deep learning need to be 
established.  deep  learning  has  been  applied  to 
natural  language  processing  with  some  success. 
the result from deep learning looks promising, but 
the  results  are preliminary  from  some  subfields  of 
nlp, and from a few research groups. besides, the 
result  for  nlp  is  still  far  from  satisfying,  letting 
the  computers  understand  human 
languages. 
further  investigations  are  needed  for  both  deep 
learning and nlp. 

empirical, 

references  

bengio,  y.,  p.  lamblin,  et  al.  (2007).  "greedy  layer-
wise training of deep networks."  advances in  neural 
information processing systems 19: 153. 

bengio,  y.  and  y.  lecun  (2007).  scaling  learning 
towards  ai.  large-scale  kernel 
algorithms 
machines. l. bottou, o. chapelle, d. decoste and j. 
weston, mit press. 

bordes,  a.,  x.  glorot,  et  al.  (2012).  joint  learning  of 
words  and  meaning  representations  for  open-text 
semantic  parsing. 
international  conference  on 
artificial intelligence and statistics. 

temporal 

boulanger-lewandowski,  n.,  y.  bengio,  et  al.  (2012). 
"modeling 
high-
dimensional  sequences:  application  to  polyphonic 
music  generation  and  transcription."  arxiv  preprint 
arxiv:1206.6392. 

dependencies 

in 

bourlard, h. and y. kamp (1988). "auto-association by 
multilayer 
value 
decomposition." biological cybernetics 59(4-5): 291-
294. 

id88s 

singular 

and 

chen, m., z. xu, et al. (2012). "marginalized denoising 
autoencoders  for  domain  adaptation."  arxiv  preprint 
arxiv:1206.4683. 

ciresan, d., u. meier, et al. (2012).  multi-column deep 
neural  networks  for  image  classification.  computer 
vision and pattern recognition (cvpr), 2012 ieee 
conference on, ieee. 

collobert,  r.  and  j.  weston  (2008).  a  unified 
architecture  for  natural  language  processing:  deep 
neural networks with multitask learning. proceedings 
of  the  25th  international  conference  on  machine 
learning, acm. 

collobert,  r.,  j.  weston,  et  al.  (2011).  "natural 
language  processing  (almost)  from  scratch."  the 
journal  of  machine  learning  research  12:  2493-
2537. 

arel,  i.,  d.  c.  rose,  et  al.  (2010).  "deep  machine 
learning  -  a  new  frontier  in  artificial  intelligence 
research 
[research  frontier]."  computational 
intelligence magazine, ieee 5(4): 13-18. 

dahl,  g.,  a.-r.  mohamed,  et  al.  (2010).  phone 
recognition  with 
the  mean-covariance  restricted 
id82. advances in neural information 
processing systems. 

bengio,  y.  "deep  learning  of  representations  for 

unsupervised and id21." 

bengio,  y.  (2009).  "learning  deep  architectures  for 

ai." found. trends mach. learn. 2(1): 1-127. 

bengio, y., a. courville, et al. (2013). "representation 

learning: a review and new perspectives." 

bengio,  y.,  r.  ducharme,  et  al.  (2003).  "a  neural 
probabilistic  language  model."  journal  of  machine 
learning research 3: 1137-1155. 

dahl,  g.  e.,  d.  yu,  et  al.  (2012).  "context-dependent 
pre-trained  deep  neural  networks 
large-
vocabulary  speech  recognition."  audio,  speech,  and 
language  processing,  ieee  transactions  on  20(1): 
30-42. 

for 

deng, l. and d. yu "deep learning for signal 

and information processing." 

freund,  y.  and  d.  haussler  (1994).  unsupervised 
learning  of  distributions  of  binary  vectors  using  two 

salakhutdinov,  r.  and  g.  e.  hinton  (2009).  deep 
boltzmann  machines.  international  conference  on 
artificial intelligence and statistics. 

schwenk, h., a. rousseau, et al. (2012).  large, pruned 
or  continuous  space  language  models  on  a  gpu  for 
statistical  machine  translation.  proceedings  of  the 
naacl-hlt 2012 workshop: will we ever really 
replace  the  id165  model?  on  the  future  of 
language  modeling  for  hlt,  association  for 
computational linguistics. 

seide,  f.,  g.  li,  et  al.  (2011).  conversational  speech 
transcription using context-dependent deep neural 
networks. interspeech. 

socher,  r.,  j.  bauer,  et  al.  (2013).  parsing  with 
compositional  vector  grammars.  proceedings  of  the 
acl conference (to appear). 

socher, r., e. h. huang, et al. (2011). dynamic pooling 
and  unfolding  recursive  autoencoders  for  paraphrase 
detection. advances in neural information processing 
systems. 

socher, r., j. pennington, et al. (2011). semi-supervised 
recursive  autoencoders  for  predicting  sentiment 
distributions.  proceedings  of  the  conference  on 
empirical  methods  in  natural  language  processing, 
association for computational linguistics. 

socher, r., a. perelygin, et al. "recursive deep models 
for  semantic  compositionality  over  a  sentiment 
treebank." 

utgoff, p. e. and d. j. stracuzzi (2002). "many-layered 

learning." neural comput. 14(10): 2497-2529. 

weston,  j.,  f.  ratle,  et  al.  (2012).  deep  learning  via 
embedding.  neural  networks: 

semi-supervised 
tricks of the trade, springer: 639-655. 

yoshua,  b.  (2013).  "representation  learning:  a 
review  and  new  perspectives."  ieee  transactions 
on  pattern  analysis  and  machine  intelligence  35(8): 
1798-1828. 

 

 

layer  networks,  computer  research  laboratory 
[university of california, santa cruz. 

glorot, x., a. bordes, et al. (2011).  id20 
large-scale  sentiment  classification:  a  deep 
for 
the  28th 
learning  approach.  proceedings  of 
international  conference  on  machine  learning 
(icml-11). 

goodfellow,  i.  j.,  a.  courville,  et  al.  (2012).  "spike-
and-slab  sparse  coding  for  unsupervised  feature 
discovery." arxiv preprint arxiv:1201.3382. 

hinton,  g.  e. 

(1986).  learning 

distributed 
representations  of  concepts.  proceedings  of  the 
eighth  annual  conference  of  the  cognitive  science 
society, amherst, ma. 

hinton,  g.  e.,  s.  osindero,  et  al.  (2006).  "a  fast 
learning  algorithm  for  deep  belief  nets."  neural 
computation 18(7): 1527-1554. 

hinton,  g.  e.  and  r.  r.  salakhutdinov  (2006). 
"reducing  the  dimensionality  of  data  with  neural 
networks." science 313(5786): 504-507. 

krizhevsky,  a.,  i.  sutskever,  et  al.  (2012).  id163 
convolutional  neural 
information 

classification  with  deep 
networks.  advances 
processing systems 25. 

in  neural 

liou, c.-y., j.-c. huang, et al. (2008). "modeling word 
network." 

perception 
neurocomputing 71(16): 3150-3157. 

elman 

using 

the 

mesnil,  g.,  y.  dauphin,  et  al.  (2012).  "unsupervised 
and  transfer  learning  challenge:  a  deep  learning 
approach."  journal  of  machine  learning  research-
proceedings track 27: 97-110. 

mikolov,  t.,  a.  deoras,  et  al.  (2011).  empirical 
evaluation  and  combination  of  advanced  language 
modeling techniques. interspeech. 

mobahi,  h.,  r.  collobert,  et  al.  (2009).  deep  learning 
from temporal coherence in video. proceedings of the 
26th  annual  international  conference  on  machine 
learning, acm. 

mohamed,  a.-r.,  g.  e.  dahl,  et  al.  (2012).  "acoustic 
modeling  using  deep  belief  networks."  audio, 
speech, 
language 
ieee 
transactions on 20(1): 14-22. 

processing, 

and 

poultney, c., s. chopra, et al. (2006). efficient learning 
of  sparse  representations  with  an  energy-based 
model.  advances  in  neural  information  processing 
systems. 

rifai,  s.,  y.  n.  dauphin,  et  al.  (2011).  the  manifold 
tangent  classifier.  advances  in  neural  information 
processing systems. 

