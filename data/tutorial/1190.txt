natural	
   language	
   processing	
   
tools	
   for	
   the	
   digital	
   humanities	
   

christopher	
   manning	
   
stanford	
   university	
   

digital	
   humanities	
   2011	
   

http://nlp.stanford.edu/~manning/courses/digitalhumanities/	
   	
   

commencement	
   2010	
   

my	
   humanities	
   quali   cations	
   

       b.a.	
   (hons),	
   australian	
   national	
   university	
   
       ph.d.	
   linguistics,	
   stanford	
   university	
   

       but:	
   

      i   m	
   not	
   sure	
   i   ve	
   ever	
   taken	
   a	
   real	
   humanities	
   class	
   
(if	
   you	
   discount	
   linguistics	
   classes	
   and	
   high	
   school	
   
english   )	
   

so,	
   feel	
   free	
   to	
   ask	
   

questions!	
   

text	
   

the	
   promise	
   

phrase	
   net	
   visualization	
   of	
   	
   
pride	
   &	
   prejudice	
   (*	
   (in|at)	
   *)	
   
http://www-958.ibm.com/software/data/cognos/manyeyes/ 

   how	
   i	
   write   	
   [code]	
   

       i	
   think	
   you	
   tend	
   to	
   get	
   too	
   much	
   of	
   people	
   

showing	
   the	
   glitzy	
   output	
   of	
   something	
   

       so,	
   for	
   this	
   tutorial,	
   at	
   least	
   in	
   the	
   slides	
   i   m	
   
trying	
   to	
   include	
   the	
   low-     level	
   hacking	
   and	
   
plumbing	
   

       it   s	
   a	
   standard	
   truism	
   of	
   data	
   mining	
   that	
   more	
   
time	
   goes	
   into	
      data	
   preparation   	
   than	
   anything	
   
else.	
   de   nitely	
   goes	
   for	
   text	
   processing.	
   

outline	
   

introduction	
   

1.   
2.    getting	
   some	
   text	
   
3.    words	
   
4.    collocations,	
   etc.	
   
5.    nlp	
   frameworks	
   and	
   tools	
   
6.    part-     of-     speech	
   tagging	
   
7.    named	
   entity	
   recognition	
   
8.    parsing	
   
9.    coreference	
   resolution	
   
10.    the	
   rest	
   of	
   the	
   languages	
   of	
   the	
   world	
   
11.    parting	
   words	
   

2.	
   getting	
   some	
   text	
   

first	
   step:	
   text	
   
       to	
   do	
   anything,	
   you	
   need	
   some	
   texts!	
   

display	
   interfaces	
   

       many	
   sites	
   give	
   you	
   various	
   sorts	
   of	
   search-     and-     
       but,	
   normally	
   you	
   just	
   can   t	
   do	
   what	
   you	
   want	
   in	
   nlp	
   
for	
   the	
   digital	
   humanities	
   unless	
   you	
   have	
   a	
   copy	
   of	
   
the	
   texts	
   sitting	
   on	
   your	
   computer	
   

       this	
   may	
   well	
   change	
   in	
   the	
   future:	
   there	
   is	
   

increasing	
   use	
   of	
   cloud	
   computing	
   models	
   where	
   you	
   
might	
   be	
   able	
   to	
   upload	
   code	
   to	
   run	
   it	
   on	
   data	
   on	
   a	
   
server	
   
       or,	
   conversely,	
   upload	
   data	
   to	
   be	
   processed	
   by	
   code	
   on	
   a	
   server	
   	
   	
   

first	
   step:	
   text	
   

       people	
   in	
   the	
   audience	
   are	
   probably	
   more	
   familiar	
   

with	
   the	
   state	
   of	
   play	
   here	
   than	
   me,	
   but	
   my	
   
impression	
   is:	
   
1.    there	
   are	
   increasingly	
   good	
   supplies	
   of	
   critical	
   texts	
   
in	
   well-     marked-     up	
   xml	
   available	
   commercially	
   for	
   
license	
   to	
   university	
   libraries	
   

2.    there	
   are	
   various,	
   more	
   community	
   e   orts	
   to	
   
produce	
   good	
   digitized	
   collections,	
   but	
   most	
   of	
   
those	
   seem	
   to	
   be	
   available	
   to	
      friends   	
   rather	
   than	
   
to	
   anybody	
   with	
   a	
   web	
   browser	
   

3.    there   s	
   project	
   gutenberg	
      	
   

      
      

plain	
   text,	
   or	
   very	
   simple	
   html,	
   which	
   may	
   or	
   may	
   not	
   be	
   
automatically	
   generated	
   
unicode	
   utf-     8	
   if	
   you   re	
   lucky,	
   us-     ascii	
   if	
   you   re	
   not	
   

1.	
   early	
   english	
   books	
   online	
   

       tei-     compliant	
   xml	
   texts	
   
       http://eebo.chadwyck.com/	
   

2.	
   old	
   bailey	
   online	
   

3.	
   project	
   gutenberg	
   

running	
   example:	
   h.	
   rider	
   haggard	
   

       the	
   hugely	
   popular	
   king	
   solomon's	
   mines	
   (1885)	
   by	
   h.	
   
rider	
   haggard	
   is	
   sometimes	
   considered	
   the	
      rst	
   of	
   the	
   
   lost	
   world   	
   or	
      imperialist	
   romance   	
   genres	
   

       allan	
   quatermain	
   (1887)	
   
       she	
   (1887)	
   
       nada	
   the	
   lily	
   (1892)	
   
       ayesha:	
   the	
   return	
   of	
   she	
   

(1905)	
   

       she	
   and	
   allan	
   (1921)	
   
       zip	
      le	
   at:	
   

http://nlp.stanford.edu/~manning/courses/digitalhumanities/	
   	
   

interfaces	
   to	
   tools	
   

web	
   

applications	
   

programming	
   

apis	
   

gui	
   

applications	
   

command-     

line	
   

applications	
   

you   ll	
   need	
   to	
   program	
   
       lisa	
   spiro,	
   tamu	
   digital	
   scholarship	
   2009:	
   

i   m a digital humanist with only limited programming 
skills (perl & xslt). enhancing my programming 
skills would allow me to: 

       avoid so much tedious, manual work 
       do citation analysis 
       pre-process texts (remove the junk) 
       automatically download web pages 
       and much more    

you   ll	
   need	
   to	
   program	
   

       program	
   in	
   what?	
   

       perl	
   

       traditional	
   seat-     of-     the-     pants	
   scripting	
   language	
   for	
   	
   text	
   
processing	
   (it	
   nailed	
      exible	
   regex).	
   	
   i	
   use	
   it	
   some	
   below   .	
   

       cleaner,	
   more	
   modern	
   scripting	
   language	
   with	
   a	
   lot	
   of	
   
energy,	
   and	
   the	
   best-     documented	
   nlp	
   framework,	
   nltk.	
   

       python	
   

       java	
   

       there	
   are	
   more	
   nlp	
   tools	
   for	
   java	
   than	
   any	
   other	
   language.	
   
and	
   it   s	
   one	
   of	
   those	
   most	
   popular	
   languages	
   in	
   general.	
   
good	
   regular	
   expressions,	
   unicode,	
   etc.	
   

you   ll	
   need	
   to	
   program	
   

       program	
   with	
   what?	
   

      there	
   are	
   some	
   general	
   skills	
   that	
   you   ll	
   want	
   the	
   
cut	
   across	
   programming	
   languages	
   
       regular	
   expressions	
   
       xml,	
   especially	
   xpath	
   and	
   xslt	
   
       unicode	
   

       but	
   i   m	
   wisely	
   not	
   going	
   to	
   try	
   to	
   teach	
   

programming	
   or	
   these	
   skills	
   in	
   this	
   tutorial	
      	
   

grabbing	
      les	
   from	
   websites	
   

       wget	
   (linux)	
   or	
   curl	
   (mac	
   os	
   x,	
   bsd)	
   

       wget	
   http://www.gutenberg.org/browse/authors/h	
   
       curl	
   -     o	
   http://www.gutenberg.org/browse/authors/h	
   

       if	
   you	
   really	
   want	
   to	
   use	
   your	
   browser,	
   there	
   are	
   

things	
   you	
   can	
   get	
   like	
   this	
   firefox	
   plug-     in	
   

       downthemall	
   	
   http://www.downthemall.net/	
   

	
   	
   	
   	
   	
   but	
   then	
   you	
   just	
   can   t	
   do	
   things	
   as	
      exibly	
   

grabbing	
      les	
   from	
   websites	
   

#!/usr/bin/perl	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
while	
   (<>)	
   {	
   last	
   if	
   (m/haggard/);	
   }	
   
while	
   (<>)	
   {	
   
	
   	
   	
   	
   last	
   if	
   (m/hague/);	
   
	
   	
   	
   	
   if	
   (m!pgdbetext\"><a	
   href="/ebooks/(\d+)">(.*)</a>	
   \(english\)!)	
   {	
   
	
   	
   	
   	
   	
   	
   	
   	
   $title	
   =	
   $2;	
   
	
   	
   	
   	
   	
   	
   	
   	
   $num	
   =	
   $1;	
   
	
   	
   	
   	
   	
   	
   	
   	
   $title	
   =~	
   s/<br>/	
   /g;	
   
	
   	
   	
   	
   	
   	
   	
   	
   $title	
   =~	
   s/\r//g;	
   
	
   	
   	
   	
   	
   	
   	
   	
   print	
   "curl	
   -     o	
   \"$title	
   $num.txt\"	
   http://www.gutenberg.org/cache/epub/$num/pg$num.txt\n";	
   
	
   	
   	
   	
   	
   	
   	
   	
   #	
   expect	
   only	
   one	
   of	
   the	
   html	
   to	
   exist	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
	
   	
   	
   	
   	
   	
   	
   	
   print	
   "curl	
   -     o	
   \"$title	
   $num.html\"	
   http://www.gutenberg.org/   les/$num/$num-     h/$num-     h.htm\n";	
   
	
   	
   	
   	
   	
   	
   	
   	
   print	
   "curl	
   -     o	
   \"$title	
   $num-     g.html\"	
   http://www.gutenberg.org/cache/epub/$num/pg$num.html\n";	
   
	
   	
   	
   	
   }	
   
}	
   
	
   

grabbing	
      les	
   from	
   websites	
   

wget	
   http://www.gutenberg.org/browse/authors/h	
   
perl	
   gethaggard.pl	
   <	
   h	
   >	
   h.sh	
   
chmod	
   755	
   h.sh	
   
./h.sh	
   
#	
   and	
   a	
   bit	
   of	
   futzing	
   by	
   hand	
   that	
   i	
   will	
   leave	
   out   .	
   
	
   
       often	
   you	
   want	
   the	
   90%	
   solution:	
   automating	
   

nothing	
   would	
   be	
   slow	
   and	
   painful,	
   but	
   automating	
   
everything	
   is	
   more	
   trouble	
   than	
   it   s	
   worth	
   for	
   a	
   one-     
o   	
   process	
   

typical	
   text	
   problems	
   

"devilish	
   strange!"	
   thought	
   he,	
   chuckling	
   to	
   himself;	
   "queer	
   business!	
   capital	
   trick	
   of	
   the	
   cull	
   in	
   the	
   cloak	
   to	
   make	
   another	
   person's	
   brat	
   stand	
   the	
   brunt	
   
for	
   his	
   own-     -     -     capital!	
   ha!	
   ha!	
   won't	
   do,	
   though.	
   he	
   must	
   be	
   a	
   sly	
   fox	
   to	
   get	
   out	
   of	
   the	
   mint	
   without	
   my	
   	
   

[page	
   59	
   ]	
   	
   

knowledge.	
   i've	
   a	
   shrewd	
   guess	
   where	
   he's	
   taken	
   refuge;	
   but	
   i'll	
   ferret	
   him	
   out.	
   these	
   bloods	
   will	
   pay	
   well	
   for	
   his	
   capture;	
   if	
   not,	
   he'll	
   pay	
   well	
   to	
   get	
   out	
   
of	
   their	
   hands;	
   so	
   i'm	
   safe	
   either	
   way-     -     -     ha!	
   ha!	
   blueskin,"	
   he	
   added	
   aloud,	
   and	
   motioning	
   that	
   worthy,	
   "follow	
   me."	
   

upon	
   which,	
   he	
   set	
   o   	
   in	
   the	
   direction	
   of	
   the	
   entry.	
   his	
   progress,	
   however,	
   was	
   checked	
   by	
   loud	
   acclamations,	
   announcing	
   the	
   arrival	
   of	
   the	
   master	
   of	
   
the	
   mint	
   and	
   his	
   train.	
   

baptist	
   kettleby	
   (for	
   so	
   was	
   the	
   master	
   named)	
   was	
   a	
   "goodly	
   portly	
   man,	
   and	
   a	
   corpulent,"	
   whose	
   fair	
   round	
   paunch	
   bespoke	
   the	
   a   ection	
   he	
   
entertained	
   for	
   good	
   liquor	
   and	
   good	
   living.	
   he	
   had	
   a	
   quick,	
   shrewd,	
   merry	
   eye,	
   and	
   a	
   look	
   in	
   which	
   duplicity	
   was	
   agreeably	
   veiled	
   by	
   good	
   humour.	
   it	
   
was	
   easy	
   to	
   discover	
   that	
   he	
   was	
   a	
   knave,	
   but	
   equally	
   easy	
   to	
   perceive	
   that	
   he	
   was	
   a	
   pleasant	
   fellow;	
   a	
   combination	
   of	
   qualities	
   by	
   no	
   means	
   of	
   rare	
   
occurrence.	
   so	
   far	
   as	
   regards	
   his	
   attire,	
   baptist	
   was	
   not	
   seen	
   to	
   advantage.	
   no	
   great	
   lover	
   of	
   state	
   or	
   state	
   costume	
   at	
   any	
   time,	
   he	
   was	
   	
   

[page	
   60	
   ]	
   	
   

generally,	
   towards	
   the	
   close	
   of	
   an	
   evening,	
   completely	
   in	
   dishabille,	
   and	
   in	
   this	
   condition	
   he	
   now	
   presented	
   himself	
   to	
   his	
   subjects.	
   his	
   shirt	
   was	
   
unfastened,	
   his	
   vest	
   unbuttoned,	
   his	
   hose	
   ungartered;	
   his	
   feet	
   were	
   stuck	
   into	
   a	
   pair	
   of	
   pantou   es,	
   his	
   arms	
   into	
   a	
   greasy	
      annel	
   dressing-     gown,	
   his	
   
head	
   into	
   a	
   thrum-     cap,	
   the	
   cap	
   into	
   a	
   tie-     periwig,	
   and	
   the	
   wig	
   into	
   a	
   gold-     edged	
   hat.	
   a	
   white	
   apron	
   was	
   tied	
   round	
   his	
   waist,	
   and	
   into	
   the	
   apron	
   was	
   
thrust	
   a	
   short	
   thick	
   truncheon,	
   which	
   looked	
   very	
   much	
   like	
   a	
   rolling-     pin.	
   

the	
   master	
   of	
   the	
   mint	
   was	
   accompanied	
   by	
   another	
   gentleman	
   almost	
   as	
   portly	
   as	
   himself,	
   and	
   quite	
   as	
   deliberate	
   in	
   his	
   movements.	
   the	
   costume	
   of	
   
this	
   personage	
   was	
   somewhat	
   singular,	
   and	
   might	
   have	
   passed	
   for	
   a	
   masquerading	
   habit,	
   had	
   not	
   the	
   imperturbable	
   gravity	
   of	
   his	
   demeanour	
   
forbidden	
   any	
   such	
   supposition.	
   it	
   consisted	
   of	
   a	
   close	
   jerkin	
   of	
   brown	
   frieze,	
   ornamented	
   with	
   a	
   triple	
   row	
   of	
   brass	
   buttons;	
   loose	
   dutch	
   slops,	
   made	
   
very	
   wide	
   in	
   the	
   seat	
   and	
   very	
   tight	
   at	
   the	
   knees;	
   red	
   stockings	
   with	
   black	
   clocks,	
   and	
   	
   

[page	
   61	
   ]	
   	
   

a	
   fur	
   cap.	
   the	
   owner	
   of	
   this	
   dress	
   had	
   a	
   broad	
   weather-     beaten	
   face,	
   small	
   twinkling	
   eyes,	
   and	
   a	
   bushy,	
   grizzled	
   beard.	
   though	
   he	
   walked	
   by	
   the	
   side	
   of	
   
the	
   governor,	
   he	
   seldom	
   exchanged	
   a	
   word	
   with	
   him,	
   but	
   appeared	
   wholly	
   absorbed	
   in	
   the	
   contemplations	
   inspired	
   by	
   a	
   broad-     bowled	
   dutch	
   pipe.	
   

there	
   are	
   always	
   text-     processing	
   

gotchas	
      	
   

          	
   and	
   not	
   dealing	
   with	
   them	
   can	
   badly	
   degrade	
   

the	
   quality	
   of	
   subsequent	
   nlp	
   processing.	
   

1.    the	
   gutenberg	
   *.txt	
      les	
   frequently	
   represent	
   

italics	
   with	
   _underscores_.	
   

2.    there	
   may	
   be	
      le	
   headers	
   and	
   footers	
   
3.    elements	
   like	
   headings	
   may	
   be	
   run	
   together	
   
with	
   following	
   sentences	
   if	
   not	
   demarcated	
   or	
   
eliminated	
   (example	
   later).	
   

there	
   are	
   always	
   text-     processing	
   

gotchas	
      	
   

#!/usr/bin/perl	
   
$   nishedheader	
   =	
   0;	
   
$startedfooter	
   =	
   0;	
   
while	
   ($line	
   =	
   <>)	
   {	
   
	
   	
   if	
   ($line	
   =~	
   /^\*\*\*\s*end/	
   &&	
   $   nishedheader)	
   {	
   
	
   	
   	
   	
   $startedfooter	
   =	
   1;	
   
	
   	
   }	
   
	
   	
   if	
   ($   nishedheader	
   &&	
   !	
   $startedfooter)	
   {	
   
	
   	
   	
   	
   $line	
   =~	
   s/_//g;	
   	
   #	
   minor	
   cleanup	
   of	
   italics	
   
	
   	
   	
   	
   print	
   $line;	
   
	
   	
   }	
   
	
   	
   if	
   ($line	
   =~	
   /^\*\*\*\s*start/	
   &&	
   !	
   $   nishedheader)	
   {	
   
	
   	
   	
   	
   $   nishedheader	
   =	
   1;	
   
	
   	
   }	
   
}	
   
if	
   (	
   !	
   ($   nishedheader	
   &&	
   $startedfooter))	
   {	
   
	
   	
   print	
   stderr	
   "****	
   probable	
   book	
   format	
   problem!\n";	
   
}	
   

3.	
   words	
   

in	
   the	
   beginning	
   was	
   the	
   word	
   

       word	
   counts	
   

       word	
   counts	
   are	
   the	
   basis	
   of	
   all	
   the	
   simple,	
      rst	
   

order	
   methods	
   of	
   text	
   analysis	
   
      tag	
   clouds,	
   collocations,	
   topic	
   models	
   

       sometimes	
   you	
   can	
   get	
   a	
   fair	
   distance	
   with	
   word	
   

counts	
   

she	
   (1887)	
   

http://wordle.net/	
   	
   jonathan	
   feinberg	
   

ayesha:	
   the	
   return	
   of	
   she	
   (1905)	
   

she	
   and	
   allan	
   (1921)	
   

wisdom's	
   daughter:	
   the	
   life	
   and	
   love	
   story	
   of	
   she-     who-     must-     be-     obeyed	
   (1923)	
   

wisdom's	
   daughter:	
   the	
   life	
   and	
   love	
   story	
   of	
   she-     who-     must-     be-     obeyed	
   (1923)	
   

google	
   books	
   ngram	
   viewer	
   

http://ngrams.googlelabs.com/	
   

google	
   books	
   ngram	
   viewer	
   

          	
   you	
   have	
   to	
   be	
   the	
   most	
   jaded	
   or	
   cynical	
   scholar	
   

not	
   to	
   be	
   excited	
   by	
   the	
   release	
   of	
   the	
   
google	
   books	
   ngram	
   viewer	
      	
   digital	
   humanities	
   
needs	
   gateway	
   drugs.	
      	
      culturomics   	
   
sounds	
   like	
   an	
   80s	
   new	
   wave	
   band.	
   if	
   we   re	
   going	
   to	
   
coin	
   neologisms,	
   let   s	
   at	
   least	
   go	
   with	
   sean	
   gillies   	
   
satirical	
   alternative:	
   freakumanities.   	
   for	
   me,	
   the	
   
biggest	
   problem	
   with	
   the	
   viewer	
   and	
   the	
   data	
   is	
   that	
   
you	
   cannot	
   seaid113ssly	
   move	
   from	
   distant	
   reading	
   to	
   
close	
   reading	
   

language	
   change:	
   as	
   least	
   as	
   

c.	
   d.	
   manning.	
   2003.	
   probabilistic	
   syntax	
   	
   
       i	
   found	
   this	
   example	
   in	
   russo	
   r.,	
   2001,	
   empire	
   

falls	
   (on	
   p.3!):	
   
      by	
   the	
   time	
   their	
   son	
   was	
   born,	
   though,	
   honus	
   
whiting	
   was	
   beginning	
   to	
   understand	
   and	
   
privately	
   share	
   his	
   wife   s	
   opinion,	
   as	
   least	
   as	
   it	
   
pertained	
   to	
   empire	
   falls.	
   
       what   s	
   interesting	
   about	
   it?	
   

language	
   change:	
   as	
   least	
   as	
   

       a	
   language	
   change	
   in	
   progress?	
   i	
   found	
   a	
   bunch	
   of	
   other	
   

examples:	
   
       indeed,	
   the	
   will	
   and	
   the	
   means	
   to	
   follow	
   through	
   are	
   as	
   
least	
   as	
   important	
   as	
   the	
   initial	
   commitment	
   to	
   de   cit	
   
reduction.	
   
       as	
   many	
   of	
   you	
   know	
   he	
   had	
   his	
   boat	
   built	
   at	
   the	
   same	
   

time	
   as	
   mine	
   and	
   it   s	
   as	
   least	
   as	
   well	
   maintained	
   and	
   
equipped.	
   

       apparently	
   not	
   a	
      dialect   	
   

       second,	
   if	
   the	
   required	
   disclosures	
   are	
   made	
   by	
   on-     screen	
   

notice,	
   the	
   disclosure	
   of	
   the	
   vendor   s	
   legal	
   name	
   and	
   address	
   
must	
   appear	
   on	
   one	
   of	
   several	
   speci   ed	
   screens	
   on	
   the	
   vendor   s	
   
electronic	
   site	
   and	
   must	
   be	
   at	
   least	
   as	
   legible	
   and	
   set	
   in	
   a	
   font	
   
as	
   least	
   as	
   large	
   as	
   the	
   text	
   of	
   the	
   o   er	
   itself.	
   

language	
   change:	
   as	
   least	
   as	
   

language	
   change:	
   as	
   least	
   as	
   

4.	
   collocations,	
   etc.	
   

using	
   a	
   text	
   editor	
   

       you	
   can	
   get	
   a	
   fair	
   distance	
   with	
   a	
   text	
   editor	
   that	
   

allows	
   multi-        le	
   searches,	
   regular	
   expressions,	
   
etc.	
   
      it   s	
   like	
   a	
   little	
   concordancer	
   that   s	
   good	
   for	
   close	
   
reading	
   
       jedit	
   	
   	
   	
   http://www.jedit.org/	
   	
   	
   	
   	
   	
   	
   

       bbedit	
   on	
   windows	
   

traditional	
   concordancers	
   

       wordsmith	
   tools	
   	
   	
   	
   commercial;	
   windows	
   

       http://www.lexically.net/wordsmith/	
   

       concordance	
   	
   	
   	
   	
   commercial;	
   windows	
   
       http://www.concordancesoftware.co.uk/	
   

       antconc	
   	
   	
   free;	
   windows,	
   mac	
   os	
   x	
   (only	
   under	
   x11);	
   linux	
   
       http://www.antlab.sci.waseda.ac.jp/antconc_index.html	
   

       casualconc	
   	
   	
   free;	
   mac	
   os	
   x	
   

       http://sites.google.com/site/casualconc/	
   

       by	
   yasu	
   imao	
   

the	
   decline	
   of	
   honour	
   

5.	
   nlp	
   frameworks	
   

and	
   tools	
   

the	
   big	
   3	
   nlp	
   frameworks	
   

       gate	
      	
   general	
   architecture	
   for	
   text	
   engineering	
   (u.	
   she   eld)	
   

       http://gate.ac.uk/	
   
       java,	
   quite	
   well	
   maintained	
   (now)	
   
       includes	
   tons	
   of	
   components	
   

       uima	
      	
   unstructured	
   information	
   management	
   architecture.	
   

originally	
   ibm;	
   now	
   apache	
   project	
   

       http://uima.apache.org/	
   
       professional,	
   scalable,	
   etc.	
   
       but,	
   unless	
   you   re	
   comfortable	
   with	
   xml,	
   eclipse,	
   java	
   or	
   c++,	
   etc.,	
   i	
   

think	
   it   s	
   a	
   non-     starter	
   

       nltk	
      	
   natural	
   language	
   to0lkit	
   (started	
   by	
   steven	
   bird)	
   

       http://www.nltk.org/	
   
       big	
   community;	
   large	
   python	
   package;	
   corpora	
   and	
   books	
   about	
   it	
   
       but	
   it   s	
   code	
   modules	
   and	
   api,	
   no	
   gui	
   or	
   command-     line	
   tools	
   
       like	
   r	
   for	
   nlp.	
   	
   but,	
   hey,	
   r   s	
   becoming	
   very	
   successful   .	
   

the	
   main	
   nlp	
   packages	
   

       http://www.nltk.org/	
   

       http://incubator.apache.org/opennlp/	
   

       nltk	
   	
   	
   python	
   
       opennlp	
   
       stanford	
   nlp	
   
       lingpipe	
   
       more	
   one-     o   	
   packages	
   than	
   i	
   can	
      t	
   on	
   this	
   slide	
   

       http://nlp.stanford.edu/software/	
   

       http://alias-     i.com/lingpipe/	
   	
   

       http://nlp.stanford.edu/links/statnlp.html	
   

nlp	
   tools:	
   rules	
   of	
   thumb	
   for	
   2011	
   
1.    unless	
   you   re	
   unlucky,	
   the	
   tool	
   you	
   want	
   to	
   use	
   
will	
   work	
   with	
   unicode	
   (at	
   least	
   bmp),	
   so	
   most	
   
any	
   characters	
   are	
   okay	
   

2.    unless	
   you   re	
   lucky,	
   the	
   tool	
   you	
   want	
   to	
   use	
   

will	
   work	
   only	
   on	
   completely	
   plain	
   text,	
   or	
   
extremely	
   simple	
   xml-     style	
   mark-     up	
   (e.g.,	
   <s>	
   
   	
   </s>	
   around	
   sentences,	
   recognized	
   by	
   regexp)	
   
3.    by	
   default,	
   you	
   should	
   assume	
   that	
   any	
   tool	
   for	
   

english	
   was	
   trained	
   on	
   american	
   newswire	
   

gate	
   

rule-     based	
   nlp	
   and	
   statistical/

machine	
   learning	
   nlp	
   

       most	
   work	
   on	
   nlp	
   in	
   the	
   1960s,	
   70s	
   and	
   80s	
   was	
   

with	
   hand-     built	
   grammars	
   and	
   morphological	
   
analyzers	
   (   nite	
   state	
   transducers),	
   etc.	
   
      annie	
   in	
   gate	
   is	
   still	
   in	
   this	
   space	
   

       most	
   academic	
   research	
   work	
   in	
   nlp	
   in	
   the	
   
1990s	
   and	
   2000s	
   use	
   probabilistic	
   or	
   more	
   
generally	
   machine	
   learning	
   methods	
   (   statistical	
   
nlp   )	
   
      the	
   stanford	
   nlp	
   tools	
   and	
   morphadorner,	
   
which	
   we	
   will	
   come	
   to	
   soon,	
   are	
   in	
   this	
   space	
   

rule-     based	
   nlp	
   and	
   statistical/

machine	
   learning	
   nlp	
   

       hand-     built	
   grammars	
   are	
      ne	
   for	
   tasks	
   in	
   a	
   closed	
   

space	
   which	
   do	
   not	
   involve	
   reasoning	
   about	
   
contexts	
   
       e.g.,	
      nding	
   the	
   possible	
   morphological	
   parses	
   of	
   a	
   

       in	
   the	
   old	
   days	
   they	
   worked	
   really	
   badly	
   on	
      real	
   

text   	
   	
   
       they	
   were	
   always	
   insu   ciently	
   tolerant	
   of	
   the	
   
       but,	
   built	
   with	
   modern,	
   empirical	
   approaches,	
   they	
   

variability	
   of	
   real	
   language	
   

word	
   

can	
   do	
   reasonably	
   well	
   
       annie	
   is	
   an	
   example	
   of	
   this	
   

rule-     based	
   nlp	
   and	
   statistical/

machine	
   learning	
   nlp	
   

      

in	
   statistical	
   nlp:	
   
       you	
   gather	
   corpus	
   data,	
   and	
   usually	
   hand-     annotate	
   it	
   with	
   the	
   
kind	
   of	
   information	
   you	
   want	
   to	
   provide,	
   such	
   as	
   part-     of-     speech	
   
       you	
   then	
   train	
   (or	
      learn   )	
   a	
   model	
   that	
   learns	
   to	
   try	
   to	
   predict	
   
annotations	
   based	
   on	
   features	
   of	
   words	
   and	
   their	
   contexts	
   via	
   
numeric	
   feature	
   weights	
   

       you	
   then	
   apply	
   the	
   trained	
   model	
   to	
   new	
   text	
   

       this	
   tends	
   to	
   work	
   much	
   better	
   on	
   real	
   text	
   

       it	
   more	
      exibly	
   handles	
   contextual	
   and	
   other	
   evidence	
   

       but	
   the	
   technology	
   is	
   still	
   far	
   from	
   perfect,	
   it	
   requires	
   annotated	
   

data,	
   and	
   degrades	
   (sometimes	
   very	
   badly)	
   when	
   there	
   are	
   
mismatches	
   between	
   the	
   training	
   data	
   and	
   the	
   runtime	
   data	
   

how	
   much	
   hardware	
   do	
   you	
   need?	
   
       nlp	
   software	
   often	
   needs	
   plenty	
   of	
   ram	
   (especially)	
   
       but	
   these	
   days	
   we	
   have	
   really	
   powerful	
   laptops!	
   
       some	
   of	
   the	
   software	
   i	
   show	
   you	
   could	
   run	
   on	
   a	
   

and	
   processing	
   power	
   

machine	
   with	
   256	
   mb	
   of	
   ram	
   (e.g.,	
   stanford	
   
parser),	
   but	
   much	
   of	
   it	
   requires	
   more	
   

ram	
   

       stanford	
   corenlp	
   requires	
   a	
   machine	
   with	
   4gb	
   of	
   
       i	
   ran	
   everything	
   in	
   this	
   tutorial	
   on	
   the	
   laptop	
   i   m	
   
presenting	
   on	
      	
   4gb	
   ram,	
   2.8	
   ghz	
   core	
   2	
   duo	
   
       but	
   it	
   wasn   t	
   always	
   pleasant	
   writing	
   the	
   slides	
   while	
   

software	
   was	
   running   .	
   

how	
   much	
   hardware	
   do	
   you	
   need?	
   
       why	
   do	
   you	
   need	
   more	
   hardware?	
   

      more	
   speed	
   

      more	
   scale	
   

       it	
   took	
   me	
   95	
   minutes	
   to	
   run	
   ayesha,	
   the	
   return	
   of	
   she	
   
through	
   stanford	
   corenlp	
   on	
   my	
   laptop   .	
   

       you   d	
   like	
   to	
   be	
   able	
   to	
   analyze	
   1	
   million	
   books	
   

       order	
   of	
   magnitude	
   rules	
   of	
   thumb:	
   

      pos	
   tagging,	
   ner,	
   etc:	
   5   10,000	
   words/second	
   
      parsing:	
   1   10	
   sentences	
   per	
   second	
   

how	
   much	
   hardware	
   do	
   you	
   need?	
   
       luckily,	
   most	
   of	
   our	
   problems	
   are	
   trivially	
   

parallelizable	
   
      each	
   book/chapter	
   can	
   be	
   run	
   separately,	
   perhaps	
   
on	
   a	
   separate	
   machine	
   

       what	
   do	
   we	
   actually	
   use?	
   

      we	
   do	
   most	
   of	
   our	
   computing	
   on	
   rack	
   mounted	
   
linux	
   servers	
   
       currently	
   4	
   x	
   quad	
   core	
   xeon	
   processors	
   with	
   24	
   gb	
   of	
   
ram	
   seem	
   about	
   the	
   sweet	
   spot	
   
       about	
   $3500	
   per	
   machine	
      	
   not	
   like	
   the	
   old	
   days	
   

6.	
   part-     of-     speech	
   

tagging	
   

part-     of-     speech	
   tagging	
   

       part-     of-     speech	
   tagging	
   is	
   normally	
   done	
   by	
   a	
   sequence	
   

model	
   (acronyms:	
   id48,	
   crm,	
   memm/cmm)	
   
       a	
   pos	
   tag	
   is	
   to	
   be	
   placed	
   above	
   each	
   word	
   
       the	
   model	
   considers	
   a	
   local	
   context	
   of	
   possible	
   previous	
   
and	
   following	
   pos	
   tags,	
   the	
   current	
   word,	
   neighboring	
   
words,	
   and	
   features	
   of	
   them	
   (capitalized?,	
   ends	
   in	
   -     ing?)	
   

       each	
   such	
   feature	
   has	
   a	
   weight,	
   and	
   the	
   evidence	
   is	
   

combined,	
   and	
   the	
   most	
   likely	
   sequence	
   of	
   tags	
   
(according	
   to	
   the	
   model)	
   is	
   chosen	
   

rb	
   

nnp	
   

nnp	
   

rb	
   

vbd	
   

when	
   

mr.	
   

holly	
   

last	
   

wrote	
   

,	
   

,	
   

jj	
   

nns	
   

many	
   

years	
   

stanford	
   pos	
   tagger	
   
http://nlp.stanford.edu/software/tagger.shtml	
   

$	
   java	
   -     mx1g	
   -     cp	
   ../software/stanford-     postagger-     full-     2011-     06-     19/
stanford-     postagger.jar	
   edu.stanford.nlp.tagger.maxent.maxenttagger	
   -     
model	
   ../software/stanford-     postagger-     full-     2011-     06-     19/models/
left3words-     distsim-     wsj-     0-     18.tagger	
   -     outputformat	
   tsv	
   -     tokenizeroptions	
   
untokenizable=allkeep	
   -     textfile	
   she\	
   3155.txt	
   >	
   she\	
   3155.tsv	
   
loading	
   default	
   properties	
   from	
   trained	
   tagger	
   ../software/stanford-     
postagger-     full-     2011-     06-     19/models/left3words-     distsim-     wsj-     0-     18.tagger	
   
reading	
   pos	
   tagger	
   model	
   from	
   ../software/stanford-     postagger-     
full-     2011-     06-     19/models/left3words-     distsim-     wsj-     0-     18.tagger	
   ...	
   done	
   [2.2	
   
sec].	
   
jun	
   15,	
   2011	
   8:17:15	
   pm	
   edu.stanford.nlp.process.ptblexer	
   next	
   
warning:	
   untokenizable:	
   ?	
   (u+1fbd,	
   decimal:	
   8125)	
   
tagged	
   132377	
   words	
   at	
   5559.72	
   words	
   per	
   second.	
   

little	
   

alone	
   
koronis	
   

greek	
   stand-     

character	
   (a	
   

obscure?)	
   

stanford	
   pos	
   tagger	
   

       for	
   the	
   second	
   time	
   you	
   do	
   it   	
   
$	
   alias	
   stanfordtag	
   "java	
   -     mx1g	
   -     cp	
   /users/manning/software/
stanford-     postagger-     full-     2011-     06-     19/stanford-     postagger.jar	
   
edu.stanford.nlp.tagger.maxent.maxenttagger	
   -     model	
   /users/
manning/software/stanford-     postagger-     full-     2011-     06-     19/models/
left3words-     distsim-     wsj-     0-     18.tagger	
   -     outputformat	
   tsv	
   -     
tokenizeroptions	
   untokenizable=allkeep	
   -     textfile"	
   
$	
   stanfordtag	
   riderhaggard/king\	
   solomon\'s\	
   mines\	
   2166.txt	
   >	
   
tagged/king\	
   solomon\'s\	
   mines\	
   2166.tsv	
   
reading	
   pos	
   tagger	
   model	
   from	
   /users/manning/software/
stanford-     postagger-     full-     2011-     06-     19/models/left3words-     distsim-     
wsj-     0-     18.tagger	
   ...	
   done	
   [2.1	
   sec].	
   
tagged	
   98178	
   words	
   at	
   9807.99	
   words	
   per	
   second.	
   

morphadorner	
   

http://morphadorner.northwestern.edu/	
   

       morphadorner	
   is	
   a	
   set	
   of	
   nlp	
   tools	
   developed	
   at	
   
northwestern	
   by	
   martin	
   mueller	
   and	
   colleagues	
   
speci   cally	
   for	
   english	
   language	
      ction,	
   over	
   a	
   
long	
   historical	
   period	
   from	
   eme	
   onwards	
   
      lemmatizer,	
   named	
   entity	
   recognizer,	
   pos	
   
tagger,	
   spelling	
   standardizer,	
   etc.	
   

       aims	
   to	
   deal	
   with	
   variation	
   in	
   word	
   breaking	
   and	
   

spelling	
   over	
   this	
   period	
   

       includes	
   its	
   own	
   pos	
   tag	
   set:	
   nupos	
   

morphadorner	
   

$	
   ./adornplaintext	
   temp	
   temp/3155.txt	
   
2011-     06-     15	
   20:30:52,111	
   info	
   	
   -     	
   morphadorner	
   version	
   1.0	
   
2011-     06-     15	
   20:30:52,111	
   info	
   	
   -     	
   initializing,	
   please	
   wait...	
   
2011-     06-     15	
   20:30:52,318	
   info	
   	
   -     	
   using	
   trigram	
   tagger.	
   
2011-     06-     15	
   20:30:52,319	
   info	
   	
   -     	
   using	
   i	
   retagger.	
   
2011-     06-     15	
   20:30:53,578	
   info	
   	
   -     	
   loaded	
   word	
   lexicon	
   with	
   151,922	
   entries	
   in	
   2	
   seconds.	
   
2011-     06-     15	
   20:30:55,920	
   info	
   	
   -     	
   loaded	
   su   x	
   lexicon	
   with	
   214,503	
   entries	
   in	
   3	
   seconds.	
   
2011-     06-     15	
   20:30:57,927	
   info	
   	
   -     	
   loaded	
   transition	
   matrix	
   in	
   3	
   seconds.	
   
2011-     06-     15	
   20:30:58,137	
   info	
   	
   -     	
   loaded	
   162,248	
   standard	
   spellings	
   in	
   1	
   second.	
   
2011-     06-     15	
   20:30:58,697	
   info	
   	
   -     	
   loaded	
   5,434	
   alternative	
   spellings	
   in	
   1	
   second.	
   
2011-     06-     15	
   20:30:58,703	
   info	
   	
   -     	
   loaded	
   349	
   more	
   alternative	
   spellings	
   in	
   14	
   word	
   classes	
   in	
   1	
   second.	
   
2011-     06-     15	
   20:30:58,713	
   info	
   	
   -     	
   loaded	
   0	
   names	
   into	
   name	
   standardizer	
   in	
   <	
   1	
   second.	
   
2011-     06-     15	
   20:30:58,779	
   info	
   	
   -     	
   1	
      le	
   to	
   process.	
   
2011-     06-     15	
   20:30:58,789	
   info	
   	
   -     	
   before	
   processing	
   input	
   texts:	
   free	
   memory:	
   105,741,696,	
   total	
   memory:	
   480,694,272	
   
2011-     06-     15	
   20:30:58,789	
   info	
   	
   -     	
   processing	
      le	
   'temp/3155.txt'	
   .	
   
2011-     06-     15	
   20:30:58,789	
   info	
   	
   -     	
   adorning	
   temp/3155.txt	
   with	
   parts	
   of	
   speech.	
   
2011-     06-     15	
   20:30:58,832	
   info	
   	
   -     	
   loaded	
   text	
   from	
   temp/3155.txt	
   in	
   1	
   second.	
   
2011-     06-     15	
   20:31:01,498	
   info	
   	
   -     	
   	
   	
   	
   extracted	
   131,875	
   words	
   in	
   4,556	
   sentences	
   in	
   3	
   seconds.	
   
2011-     06-     15	
   20:31:03,860	
   info	
   	
   -     	
   	
   	
   	
   	
   	
   	
   lines:	
   1,000;	
   words:	
   27,756	
   
2011-     06-     15	
   20:31:04,364	
   info	
   	
   -     	
   	
   	
   	
   	
   	
   	
   lines:	
   2,000;	
   words:	
   58,728	
   
2011-     06-     15	
   20:31:04,676	
   info	
   	
   -     	
   	
   	
   	
   	
   	
   	
   lines:	
   3,000;	
   words:	
   84,735	
   
2011-     06-     15	
   20:31:04,990	
   info	
   	
   -     	
   	
   	
   	
   	
   	
   	
   lines:	
   4,000;	
   words:	
   115,396	
   
2011-     06-     15	
   20:31:05,152	
   info	
   	
   -     	
   	
   	
   	
   	
   	
   	
   lines:	
   4,556;	
   words:	
   131,875	
   
2011-     06-     15	
   20:31:05,152	
   info	
   	
   -     	
   	
   	
   	
   part	
   of	
   speech	
   adornment	
   completed	
   in	
   4	
   seconds.	
   36,100	
   words	
   adorned	
   per	
   second.	
   
2011-     06-     15	
   20:31:05,152	
   info	
   	
   -     	
   	
   	
   	
   generating	
   other	
   adornments.	
   
2011-     06-     15	
   20:31:13,840	
   info	
   	
   -     	
   	
   	
   	
   adornments	
   written	
   to	
   temp/3155-     005.txt	
   in	
   9	
   seconds.	
   
2011-     06-     15	
   20:31:13,840	
   info	
   	
   -     	
   all	
      les	
   adorned	
   in	
   16	
   seconds.	
   
	
   

ah,	
   the	
   old	
   days!	
   

$	
   ./adornplaintext	
   temp	
   temp/hunter\	
   quartermain.txt	
   	
   
2011-     06-     15	
   17:18:15,551	
   info	
   	
   -     	
   morphadorner	
   version	
   1.0	
   
2011-     06-     15	
   17:18:15,552	
   info	
   	
   -     	
   initializing,	
   please	
   wait...	
   
2011-     06-     15	
   17:18:15,730	
   info	
   	
   -     	
   using	
   trigram	
   tagger.	
   
2011-     06-     15	
   17:18:15,731	
   info	
   	
   -     	
   using	
   i	
   retagger.	
   
2011-     06-     15	
   17:18:16,972	
   info	
   	
   -     	
   loaded	
   word	
   lexicon	
   with	
   151,922	
   entries	
   in	
   2	
   
seconds.	
   
2011-     06-     15	
   17:18:18,684	
   info	
   	
   -     	
   loaded	
   su   x	
   lexicon	
   with	
   214,503	
   entries	
   in	
   2	
   
seconds.	
   
2011-     06-     15	
   17:18:20,662	
   info	
   	
   -     	
   loaded	
   transition	
   matrix	
   in	
   2	
   seconds.	
   
2011-     06-     15	
   17:18:20,887	
   info	
   	
   -     	
   loaded	
   162,248	
   standard	
   spellings	
   in	
   1	
   second.	
   
2011-     06-     15	
   17:18:21,300	
   info	
   	
   -     	
   loaded	
   5,434	
   alternative	
   spellings	
   in	
   1	
   second.	
   
2011-     06-     15	
   17:18:21,303	
   info	
   	
   -     	
   loaded	
   349	
   more	
   alternative	
   spellings	
   in	
   14	
   word	
   
classes	
   in	
   1	
   second.	
   
2011-     06-     15	
   17:18:21,312	
   info	
   	
   -     	
   loaded	
   0	
   names	
   into	
   name	
   standardizer	
   in	
   1	
   second.	
   
2011-     06-     15	
   17:18:21,381	
   info	
   	
   -     	
   no	
      les	
   found	
   to	
   process.	
   

       but	
   it	
   works	
   better	
   if	
   you	
   make	
   sure	
   the	
      lename	
   has	
   

no	
   spaces	
   in	
   it	
      	
   

comparing	
   taggers:	
   penn	
   treebank	
   vs.	
   

nupos	
   

	
   ,	
   
	
   cs	
   
	
   pn22	
   
	
   vmb	
   

holly 	
   nnp 	
   holly 	
   n1	
   
	
   ,
	
   
,
	
   
	
   in 	
   
if
	
   
	
   prp
you	
   
will	
   
	
   md	
   
accept 	
   vb 	
   
	
   dt 	
   
the	
   
trust
	
   nn	
   
	
   
	
   ,
	
   
,
	
   prp
i
	
   
am	
   
	
   vbp
	
   

	
   ,
	
   
	
   if
	
   
	
   you	
   
	
   will	
   
	
   accept 	
   vvi	
   
	
   dt	
   
	
   the	
   
	
   trust
	
   n1	
   
	
   ,	
   
	
   
	
   ,
	
   pns11	
   
	
   i
	
   
	
   am	
   
	
   vbm	
   

	
   pc-     acp	
   

	
   pn22	
   
	
   d	
   

	
   to 	
   
	
   leave 	
   vvi	
   

going 	
   vbg	
   	
   	
   	
   	
   going 	
   vvg	
   
to 	
   
	
   to	
   
leave 	
   vb 	
   
you
that
boy
's 	
   
sole
guardian	
   nn 	
   guardian	
   n1	
   
.
	
   

	
   prp	
   	
   	
   	
   	
   you	
   
	
   that
	
   in 	
   
	
   nn	
   
	
   boy's 	
   ng1
	
   pos	
   
	
   jj 	
   

	
   sole

	
   .	
   

	
   j	
   

	
   .

	
   .

	
   

	
   

	
   

	
   	
   

comparing	
   taggers:	
   penn	
   treebank	
   vs.	
   

nupos	
   

	
   ,	
   
	
   cs	
   
	
   pn22	
   
	
   vmb	
   

holly 	
   nnp 	
   holly 	
   n1	
   
	
   ,
	
   
,
	
   
	
   in 	
   
if
	
   
	
   prp
you	
   
will	
   
	
   md	
   
accept 	
   vb 	
   
	
   dt 	
   
the	
   
trust
	
   nn	
   
	
   
	
   ,
	
   
,
	
   prp
i
	
   
am	
   
	
   vbp
	
   

	
   ,
	
   
	
   if
	
   
	
   you	
   
	
   will	
   
	
   accept 	
   vvi	
   
	
   dt	
   
	
   the	
   
	
   trust
	
   n1	
   
	
   ,	
   
	
   
	
   ,
	
   pns11	
   
	
   i
	
   
	
   am	
   
	
   vbm	
   

	
   pc-     acp	
   

	
   pn22	
   
	
   d	
   

	
   to 	
   
	
   leave 	
   vvi	
   

going 	
   vbg	
   	
   	
   	
   	
   going 	
   vvg	
   
to 	
   
	
   to	
   
leave 	
   vb 	
   
you
that
boy
's 	
   
sole
guardian	
   nn 	
   guardian	
   n1	
   
.
	
   

	
   prp	
   	
   	
   	
   	
   you	
   
	
   that
	
   in 	
   
	
   nn	
   
	
   boy's 	
   ng1
	
   pos	
   
	
   jj 	
   

	
   sole

	
   .	
   

	
   j	
   

	
   .

	
   .

	
   

	
   

	
   

	
   	
   

stylistic	
   factors	
   from	
   pos	
   

14000	
   

12000	
   

10000	
   

8000	
   

6000	
   

4000	
   

2000	
   

0	
   

jj	
   
md	
   
dt	
   

she	
   

ayesha	
   

she	
   and	
   allan	
    wisdom's	
   
daughter	
   

7.	
   named	
   entity	
   

recognition	
   

(ner)	
   

named	
   entity	
   recognition	
   	
   

   	
      the	
   chad	
   problem   	
   
 germany     s representative to the 
european union     s veterinary 
committee werner zwingman said on 
wednesday consumers should     

 il-2 gene expression and nf-kappa b 
activation through cd28 requires 
reactive oxygen production by            
5-lipoxygenase. 

conditional	
   random	
   fields	
   (crfs)	
   

o	
   

per	
   

per	
   

o	
   

o	
   

when	
   

mr.	
   

holly	
   

last	
   

wrote	
   

o	
   

,	
   

o	
   

o	
   

many	
   

years	
   

       we	
   again	
   use	
   a	
   sequence	
   model	
      	
   di   erent	
   

problem,	
   but	
   same	
   technology	
   
       indeed,	
   sequence	
   models	
   are	
   used	
   for	
   lots	
   of	
   tasks	
   

that	
   can	
   be	
   construed	
   as	
   labeling	
   tasks	
   that	
   
require	
   only	
   local	
   context	
   (to	
   do	
   quite	
   well)	
   

       there	
   is	
   a	
   background	
   label	
      	
   o	
      	
   and	
   labels	
   for	
   
       entities	
   are	
   both	
   segmented	
   and	
   categorized	
   

each	
   class	
   

stanford	
   ner	
   features	
   

       word	
   features:	
   current	
   word,	
   previous	
   word,	
   next	
   
word,	
   a	
   word	
   is	
   anywhere	
   in	
   a	
   +/   	
   4	
   word	
   window	
   

       orthographic	
   features:	
   	
   

       jenny	
   	
      	
   	
   xxxx	
   
       il-     2	
   	
   	
   	
   	
   	
      	
   	
   xx-     #	
   

       pre   xes	
   and	
   su   xes:	
   

       jenny	
   	
      	
   	
   <j,	
   <je,	
   <jen,	
      ,	
   nny>,	
   ny>,	
   y>	
   

       label	
   sequences	
   
       lots	
   of	
   feature	
   conjunctions	
   

stanford	
   ner	
   

http://nlp.stanford.edu/software/crf-     ner.shtml	
   

$	
   java	
   -     mx500m	
   -     d   le.encoding=utf-     8	
   -     cp	
   software/stanford-     
ner-     2011-     06-     19/stanford-     ner.jar	
   edu.stanford.nlp.ie.crf.crfclassi   er	
   -     
loadclassi   er	
   software/stanford-     ner-     2011-     06-     19/classi   ers/all.
3class.distsim.crf.ser.gz	
   -     textfile	
   riderhaggard/she\	
   3155.txt	
   >	
   ner/she\	
   
3155.ner	
   
	
   
for	
   thou	
   shalt	
   rule	
   this	
   <location>england</location>-     -     -     -        	
   
"but	
   we	
   have	
   a	
   queen	
   already,"	
   broke	
   in	
   <location>leo</location>,	
   
hastily.	
   
"it	
   is	
   naught,	
   it	
   is	
   naught,"	
   said	
   <person>ayesha</person>;	
   "she	
   can	
   
be	
   overthrown.   	
   
at	
   this	
   we	
   both	
   broke	
   out	
   into	
   an	
   exclamation	
   of	
   dismay,	
   and	
   explained	
   
that	
   we	
   should	
   as	
   soon	
   think	
   of	
   overthrowing	
   ourselves.	
   
"but	
   here	
   is	
   a	
   strange	
   thing,"	
   said	
   <person>ayesha</person>,	
   in	
   
astonishment;	
   "a	
   queen	
   whom	
   her	
   people	
   love!	
   surely	
   the	
   world	
   must	
   
have	
   changed	
   since	
   i	
   dwelt	
   in	
   <location>k  r</location>."	
   

8.	
   parsing	
   

statistical	
   parsing	
   

       one	
   of	
   the	
   big	
   successes	
   of	
   1990s	
   statistical	
   nlp	
   

was	
   the	
   development	
   of	
   statistical	
   parsers	
   

       these	
   are	
   trained	
   from	
   hand-     parsed	
   sentences	
   
(   treebanks   ),	
   and	
   know	
   statistics	
   about	
   phrase	
   
structure	
   and	
   word	
   relationships,	
   and	
   use	
   them	
   to	
   
assign	
   the	
   most	
   likely	
   structure	
   to	
   a	
   new	
   sentence	
   
       they	
   will	
   return	
   a	
   sentence	
   parse	
   for	
   any	
   sequence	
   

of	
   words.	
   and	
   it	
   will	
   usually	
   be	
   mostly	
   right	
   

       there	
   are	
   many	
   opportunities	
   for	
   exploiting	
   this	
   

richer	
   level	
   of	
   analysis,	
   which	
   have	
   only	
   been	
   partly	
   
realized.	
   

phrase	
   structure	
   parsing	
   
       phrase	
   structure	
   representations	
   have	
   dominated	
   
       they	
   focus	
   on	
   showing	
   words	
   that	
   go	
   together	
   to	
   form	
   
       they	
   are	
   good	
   for	
   showing	
   and	
   querying	
   details	
   of	
   

natural	
   groups	
   (constituents)	
   that	
   behave	
   alike	
   

american	
   linguistics	
   since	
   the	
   1930s	
   

sentence	
   structure	
   and	
   embedding	
   

s 

np 

np 

pp 

in 

np 

nns 

nns 

cc 

nn 

vp 

vbd 

vp 

vbn 

pp 

in 

np 

nnp 

nnp 

bills     on     ports     and   immigration         were     submitted     by     senator     brownback 

dependency	
   parsing	
   

       a	
   dependency	
   parse	
   shows	
   which	
   words	
   in	
   a	
   sentence	
   modify	
   other	
   words	
   
       the	
   key	
   notion	
   are	
   governors	
   with	
   dependents	
   
       widespread	
   use:	
   p     ini,	
   early	
   arabic	
   grammarians,	
   diagramming	
   sentences,	
      	
   

submitted 
auxpass 

were 

prep 
by 

pobj 

nsubjpass 
bills 

prep 

on 

pobj 

ports 

cc 
and 

conj 
immigration 

brownback 
appos 
nn 
 senator 
republican 
prep 

of 

pobj 
kansas 

stanford	
   dependencies	
   

       sd	
   is	
   a	
   particular	
   dependency	
   representation	
   designed	
   for	
   easy	
   

extraction	
   of	
   meaning	
   relationships	
   	
   [de	
   marne   e	
   &	
   manning,	
   2008]	
   
       it   s	
   basic	
   form	
   in	
   the	
   last	
   slide	
   has	
   each	
   word	
   as	
   is	
   
       a	
      collapsed   	
   form	
   focuses	
   on	
   relations	
   between	
   main	
   words	
   

nsubjpass 
bills 

prep_on 

ports 

submitted 
auxpass 

were 

agent 

brownback 
appos 
nn 
 senator 
republican 

conj_and 

prep_on 

prep_of 

immigration 

kansas 

statistical	
   parsers	
   	
   

       there	
   are	
   now	
   many	
   good	
   statistical	
   parsers	
   that	
   

are	
   freely	
   downloadable	
   
      constituency	
   parsers	
   
       collins/bikel	
   parser	
   
       berkeley	
   parser	
   
       bllip	
   parser	
   =	
   charniak/johnson	
   parser	
   

      dependency	
   parsers	
   

       maltparser	
   
       mst	
   parser	
   

       but	
   i   ll	
   show	
   the	
   stanford	
   parser	
      	
   

tregex/tgrep2	
      	
   tools	
   for	
   searching	
   

over	
   syntax	
   	
   

dreadful	
   things	
   

ayesha	
   
amod(clouds-     5,	
   dreadful-     2)	
   
amod(debt-     26,	
   dreadful-     25)	
   
amod(doom-     21,	
   dreadful-     20)	
   
amod(fashion-     50,	
   dreadful-     47)	
   
amod(form-     10,	
   dreadful-     7)	
   
amod(oath-     42,	
   dreadful-     41)	
   
amod(road-     23,	
   dreadful-     22)	
   
amod(silence-     5,	
   dreadful-     4)	
   
amod(threat-     19,	
   dreadful-     18)	
   

she	
   
amod(day-     18,	
   dreadful-     17)	
   
amod(day-     45,	
   dreadful-     44)	
   
amod(feast-     33,	
   dreadful-     32)	
   
amod(   ts-     51,	
   dreadful-     50)	
   
amod(form-     59,	
   dreadful-     58)	
   
amod(laugh-     9,	
   dreadful-     8)	
   
amod(manifestation-     9,	
   dreadful-     8)	
   
amod(manner-     29,	
   dreadful-     28)	
   
amod(marshes-     17,	
   dreadful-     16)	
   
amod(people-     12,	
   dreadful-     11)	
   
amod(people-     46,	
   dreadful-     45)	
   
amod(place-     16,	
   dreadful-     15)	
   
amod(place-     6,	
   dreadful-     5)	
   
amod(sight-     5,	
   dreadful-     4)	
   
amod(spot-     13,	
   dreadful-     12)	
   
amod(thing-     41,	
   dreadful-     40)	
   
amod(thing-     5,	
   dreadful-     4)	
   
amod(tragedy-     22,	
   dreadful-     21)	
   
amod(wilderness-     43,	
   dreadful-     42)	
   

making	
   use	
   of	
   dependency	
   structure	
   

j.	
   engelberg	
   costly	
   information	
   processing	
   (afa,	
   2009):	
   	
   
       an	
   e   cient	
   market	
   should	
   immediately	
   incorporate	
   all	
   

publicly	
   available	
   information.	
   

       but	
   many	
   studies	
   have	
   shown	
   there	
   is	
   a	
   lag	
   

       and	
   the	
   lag	
   is	
   greater	
   on	
   fridays	
   (!)	
   

       an	
   explanation	
   for	
   this	
   is	
   that	
   there	
   is	
   a	
   cost	
   to	
   information	
   

processing	
   

       engelberg	
   tests	
   and	
   shows	
   that	
      soft   	
   (textual)	
   information	
   

takes	
   longer	
   to	
   be	
   absorbed	
   than	
      hard   	
   (numeric)	
   
information	
      	
   it  s	
   higher	
   cost	
   information	
   processing	
   

       but	
      soft   	
   information	
   has	
   value	
   beyond	
      hard   	
   information	
   

       it   s	
   especially	
   valuable	
   for	
   predicting	
   further	
   out	
   in	
   time	
   

	
   	
   

evidence from earnings announcements 

[engelberg afa 2009] 

       but	
   how	
   do	
   you	
   use	
   the	
      soft   	
   information?	
   
       simply	
   using	
   proportion	
   of	
      negative   	
   words	
   (from	
   the	
   

harvard	
   general	
   inquirer	
   lexicon)	
   is	
   a	
   useful	
   predictive	
   feature	
   
of	
   future	
   stock	
   behavior	
   
	
   	
   	
   although	
   sales	
   remained	
   steady,	
   the	
      rm	
   continues	
   to	
   
          but	
   this	
   [or	
   text	
   categorization]	
   is	
   not	
   enough.	
   in	
   order	
   to	
   

su   er	
   from	
   rising	
   oil	
   prices.	
   

re   ne	
   my	
   analysis,	
   i	
   need	
   to	
   know	
   that	
   the	
   negative	
   
sentiment	
   is	
   about	
   oil	
   prices.   	
   

       he	
   thus	
   turns	
   to	
   use	
   of	
   the	
   typed	
   dependencies	
   

representation	
   of	
   the	
   stanford	
   parser.	
   
       words	
   that	
   negative	
   words	
   relate	
   to	
   are	
   grouped	
   into	
   1	
   of	
   

6	
   categories	
   [5	
   word	
   lists	
   or	
      other   ]	
   

evidence from earnings announcements 

[engelberg 2009] 

      

in	
   a	
   regression	
   model	
   with	
   many	
   standard	
   quantitative	
   
predictors   	
   
       just	
   the	
   negative	
   word	
   fraction	
   is	
   a	
   signi   cant	
   predictor	
   of	
   3	
   

day	
   or	
   80	
   day	
   post	
   earnings	
   announcement	
   abnormal	
   
returns	
   (car)	
   
       coe   cient	
      0.173,	
   p	
   <	
   0.05	
   for	
   80	
   day	
   car	
   

       negative	
   sentiment	
   about	
   di   erent	
   things	
   has	
   di   erential	
   

e   ects	
   
       fundamentals:	
      0.198,	
   p	
   <	
   0.01	
   for	
   80	
   day	
   car	
   
       future:	
      0.356,	
   p	
   <	
   0.05	
   for	
   80	
   day	
   car	
   
       other:	
      0.023,	
   p	
   <	
   0.01	
   for	
   80	
   day	
   car	
   

       only	
   some	
   of	
   which	
   analysts	
   pay	
   attention	
   to	
   

       analyst	
   forecast-     for-     quarter-     ahead	
   earnings	
   is	
   predicted	
   by	
   

negative	
   sentiment	
   on	
   environment	
   and	
   other	
   but	
   not	
   
fundamentals	
   or	
   future!	
   

syntactic packaging and implicit sentiment 

[greene 2007; greene and resnik 2009] 

       positive	
   or	
   negative	
   sentiment	
   can	
   be	
   carried	
   by	
   words	
   (e.g.,	
   

adjectives),	
   but	
   often	
   it	
   isn   t   .	
   
       these	
   sentences	
   di   er	
   in	
   sentiment,	
   even	
   though	
   the	
   

words	
   aren   t	
   so	
   di   erent:	
   
       a	
   soldier	
   veered	
   his	
   jeep	
   into	
   a	
   crowded	
   market	
   and	
   killed	
   
three	
   civilians	
   
       a	
   soldier  s	
   jeep	
   veered	
   into	
   a	
   crowded	
   market	
   and	
   three	
   
civilians	
   were	
   killed	
   

       as	
   a	
   measurable	
   version	
   of	
   such	
   issues	
   of	
   linguistic	
   perspective,	
   

they	
   de   ne	
   opus	
   features	
   
       for	
   domain	
   relevant	
   terms,	
   opus	
   features	
   pair	
   the	
   word	
   with	
   a	
   

syntactic	
   stanford	
   dependency:	
   
	
   nsubj:soldier 	
   

       killed:dobj 	
   

	
   killed:nsubj	
   

predicting opinions of the death penalty 

[greene 2007; greene and resnik 2009] 

       collected	
   pro-     	
   and	
   anti-     	
   death	
   penalty	
   texts	
   from	
   websites	
   with	
   

manual	
   checking	
   

       training	
   is	
   cross-     validation	
   of	
   training	
   on	
   some	
   pro-     	
   and	
   anti-     	
   sites	
   

and	
   testing	
   on	
   documents	
   from	
   others	
   	
   	
   	
   	
   	
   	
   	
   [can  t	
   use	
   site-     speci   c	
   
nuances]	
   
       baseline	
   is	
   word	
   and	
   word	
   bigram	
   features	
   in	
   a	
   support	
   vector	
   

machine	
   	
   	
   	
   	
   [id166	
   =	
   good	
   classi   er]	
   

condition 
baseline 
with opus features 

id166 accuracy 
72.0% 
88.1% 

       58%	
   error	
   reduction!	
   

9.	
   coreference	
   

resolution	
   

coreference	
   resolution	
   

       the	
   goal	
   is	
   to	
   work	
   out	
   which	
   (noun)	
   phrases	
   

refer	
   to	
   the	
   same	
   entities	
   in	
   the	
   world	
   
      sarah	
   asked	
   her	
   father	
   to	
   look	
   at	
   her.	
   he	
   
appreciated	
   that	
   his	
   eldest	
   daughter	
   wanted	
   to	
   
speak	
   frankly.	
   

          	
   anaphora	
   resolution	
      	
   pronoun	
   resolution	
      	
   

entity	
   resolution	
   

coreference	
   resolution	
   warnings	
   
       warning:	
   the	
   tools	
   we	
   have	
   looked	
   at	
   so	
   far	
   work	
   

one	
   sentence	
   at	
   a	
   time	
      	
   or	
   use	
   the	
   whole	
   
document	
   but	
   ignore	
   all	
   structure	
   and	
   just	
   count	
   
   	
   but	
   coreference	
   uses	
   the	
   whole	
   document	
   

       the	
   resources	
   used	
   will	
   grow	
   with	
   the	
   document	
   
size	
      	
   you	
   might	
   want	
   to	
   try	
   a	
   chapter	
   not	
   a	
   novel	
   

       coreference	
   systems	
   normally	
   require	
   

processing	
   with	
   parsers,	
   ner,	
   etc.	
      rst,	
   and	
   use	
   
of	
   lexicons	
   

coreference	
   resolution	
   warnings	
   
       english-     only	
   for	
   the	
   moment   .	
   
       while	
   there	
   are	
   some	
   papers	
   on	
   coreference	
   

resolution	
   in	
   other	
   languages,	
   i	
   am	
   aware	
   of	
   no	
   
downloadable	
   coreference	
   systems	
   for	
   any	
   
language	
   other	
   than	
   english	
   

       for	
   english,	
   there	
   are	
   a	
   good	
   number	
   of	
   

downloadable	
   systems,	
   but	
   their	
   performance	
   
remains	
   modest.	
   	
   it   s	
   just	
   not	
   like	
   pos	
   tagging,	
   
ner	
   or	
   parsing	
   

coreference	
   resolution	
   warnings	
   

nevertheless,	
   it   s	
   not	
   yet	
   known	
   to	
   the	
   state	
   of	
   
california	
   to	
   cause	
   cancer,	
   so	
   let   s	
   continue   .	
   

stanford	
   corenlp	
   

http://nlp.stanford.edu/software/corenlp.shtml	
   

       stanford	
   corenlp	
   is	
   our	
   new	
   package	
   that	
   ties	
   

together	
   a	
   bunch	
   of	
   nlp	
   tools	
   
      pos	
   tagging	
   
      named	
   entity	
   recognition	
   
      parsing	
   
      and	
   coreference	
   resolution	
   

       output	
   is	
   an	
   xml	
   representation	
   [only	
   choice	
   at	
   present]	
   
       contains	
   a	
   state-     of-     the-     art	
   coreference	
   system!	
   

stanford	
   corenlp	
   

$	
   java	
   -     mx3g	
   -     d   le.encoding=utf-     8	
   -     cp	
   "software/
stanford-     corenlp-     2011-     06-     08/stanford-     
corenlp-     2011-     06-     08.jar:software/stanford-     
corenlp-     2011-     06-     08/stanford-     corenlp-     
models-     2011-     06-     08.jar:software/stanford-     
corenlp-     2011-     06-     08/xom.jar:software/stanford-     
corenlp-     2011-     06-     08/jgrapht.jar"	
   
edu.stanford.nlp.pipeline.stanfordcorenlp	
   -        le	
   
riderhaggard/hunter\	
   quatermain\'s\	
   story\	
   
2728.txt	
   -     outputdirectory	
   corenlp	
   
	
   

what	
   stanford	
   corenlp	
   gives	
   
      sarah	
   asked	
   her	
   father	
   to	
   look	
   at	
   her	
   .	
   	
   
      he	
   appreciated	
   that	
   his	
   eldest	
   daughter	
   wanted	
   
to	
   speak	
   frankly	
   .	
   

       coreference	
   resolution	
   graph	
   
      sentence	
   1,	
   headword	
   1	
   (gov)	
   	
   
      sentence	
   1,	
   headword	
   3	
   
      sentence	
   1,	
   headword	
   4	
   (gov)	
   	
   
      sentence	
   2,	
   headword	
   1	
   
      sentence	
   2,	
   headword	
   4	
   

what	
   stanford	
   corenlp	
   gives	
   
      sarah	
   asked	
   her	
   father	
   to	
   look	
   at	
   her	
   .	
   	
   
      he	
   appreciated	
   that	
   his	
   eldest	
   daughter	
   wanted	
   
to	
   speak	
   frankly	
   .	
   

       coreference	
   resolution	
   graph	
   
      sentence	
   1,	
   headword	
   1	
   (gov)	
   	
   
      sentence	
   1,	
   headword	
   3	
   

      sentence	
   1,	
   headword	
   4	
   (gov)	
   	
   
      sentence	
   2,	
   headword	
   1	
   
      sentence	
   2,	
   headword	
   4	
   

the	
   rest	
   of	
   the	
   
languages	
   of	
   the	
   

world	
   

	
   

english-     only?	
   

       there	
   are	
   a	
   lot	
   of	
   languages	
   out	
   there	
   in	
   the	
   world!	
   
       but	
   there	
   are	
   a	
   lot	
   more	
   nlp	
   tools	
   for	
   english	
   than	
   

anything	
   else	
   

       however,	
   there	
   is	
   starting	
   to	
   be	
   fairly	
   reasonable	
   

support	
   (or	
   the	
   ability	
   to	
   build	
   it)	
   for	
   most	
   of	
   the	
   top	
   
50	
   or	
   so	
   languages   	
   

       i   ll	
   say	
   a	
   little	
   about	
   that,	
   since	
   some	
   people	
   are	
   
de   nitely	
   interested,	
   even	
   if	
   i   ve	
   covered	
   mainly	
   
english	
   

pos	
   taggers	
   for	
   many	
   languages?	
   
       two	
   choices:	
   

1.    find	
   a	
   tagger	
   with	
   an	
   existing	
   model	
   for	
   the	
   

language	
   (and	
   period)	
   of	
   interest	
   

2.    find	
   pos-     tagged	
   training	
   data	
   for	
   the	
   language	
   

(and	
   period)	
   of	
   interest	
   and	
   train	
   your	
   own	
   
tagger	
   
       most	
   downloadable	
   taggers	
   allow	
   you	
   to	
   train	
   new	
   
models	
      	
   e.g.,	
   the	
   stanford	
   pos	
   tagger	
   	
   

       but	
   it	
   may	
   involve	
   considerable	
   data	
   preparation	
   work	
   and	
   

understanding	
   and	
   not	
   be	
   for	
   the	
   faint-     hearted	
   

pos	
   taggers	
   for	
   many	
   languages?	
   
       one	
   tagger	
   with	
   good	
   existing	
   multi-     lingual	
   support	
   

       treetagger	
   (helmut	
   schmid)	
   

       http://www.ims.uni-     stuttgart.de/projekte/corplex/
treetagger/	
   
       bulgarian,	
   chinese,	
   dutch,	
   english,	
   estonian,	
   french,	
   old	
   
french,	
   galician,	
   german,	
   greek,	
   italian,	
   latin,	
   portuguese,	
   
russian,	
   spanish,	
   swahili	
   
       free	
   for	
   non-     commercial,	
   not	
   open	
   source;	
   linux,	
   mac,	
   
sparc	
   (not	
   windows)	
   

       stanford	
   pos	
   tagger	
   presently	
   comes	
   with:	
   

       english,	
   arabic,	
   chinese,	
   german	
   

       one	
   place	
   to	
   look	
   for	
   more	
   resources:	
   

       http://nlp.stanford.edu/links/statnlp.html	
   

       but	
   it   s	
   always	
   out	
   of	
   date,	
   so	
   also	
   try	
   a	
   google	
   search	
      	
   

chinese	
   example	
   

       chinese	
   doesn   t	
   put	
   spaces	
   between	
   words	
   

      nor	
   did	
   ancient	
   greek	
   

       so	
   almost	
   all	
   tools	
      rst	
   require	
   word	
   

segmentation	
   

       i	
   demonstrate	
   the	
   stanford	
   chinese	
   word	
   segmenter	
   	
   
       http://nlp.stanford.edu/software/segmenter.shtml	
   	
   

       even	
   in	
   english,	
   words	
   need	
   some	
   segmentation	
   

   	
   often	
   called	
   id121	
   

       it	
   was	
   being	
   implicitly	
   done	
   before	
   further	
   processing	
   
in	
   the	
   examples	
   till	
   now:	
   	
      i   ll	
   go.   	
   	
   	
      	
   	
   	
      	
   	
   	
   i	
   	
   	
      ll	
   	
   	
   go	
   	
   	
   .	
   	
   	
      	
   	
   

chinese	
   example	
   

       $	
   ../software/stanford-     chinese-     

segmenter-     2010-     03-     08/segment.sh	
   ctb	
   
xinhua.txt	
   utf-     8	
   0	
   >	
   xinhua.seg	
   

       $	
   java	
   -     mx300m	
   -     cp	
   ../software/stanford-     

postagger-     full-     2011-     05-     18/stanford-     postagger.jar	
   
edu.stanford.nlp.tagger.maxent.maxenttagger	
   -     
model	
   ../software/stanford-     postagger-     
full-     2011-     05-     18/models/chinese.tagger	
   -     textfile	
   
xinhua.seg	
   >	
   xinhua.tag	
   

chinese	
   example	
   

#	
   space	
   before	
      	
   below!	
   
$	
   perl	
   -     pe	
   'if	
   (	
   !	
   m/^\s*$/	
   &&	
   !	
   m/^.{100}/)	
   {	
   s/$/	
      /;	
   }'	
   <	
   xinhua.seg	
   >	
   
xinhua.seg.   xed	
   
$	
   java	
   -     mx600m	
   -     cp	
   ../software/stanford-     parser-     2011-     06-     15/stford-     
parser.jar	
   edu.stanford.nlp.parser.lexparser.lexicalizedparser	
   -     
encoding	
   utf-     8	
   ../software/stanford-     parser-     2011-     04-     17/
chinesefactored.ser.gz	
   xinhua.seg.   xed	
   >	
   xinhua.parsed	
   
$	
   java	
   -     mx1g	
   -     cp	
   ../software/stanford-     parser-     2011-     06-     15/stanford-     
parser.jar	
   edu.stanford.nlp.parser.lexparser.lexicalizedparser	
   -     
encoding	
   utf-     8	
   -     outputformat	
   typeddependencies	
   ../software/
stanford-     parser-     2011-     04-     17/chinesefactored.ser.gz	
   
xinhua.seg.   xed	
   >	
   xinhua.sd	
   

other	
   tools	
   

       dependency	
   parsers	
   are	
   now	
   available	
   for	
   many	
   

languages,	
   especially	
   via	
   maltparser:	
   
       http://maltparser.org/	
   

       for	
   instance,	
   it   s	
   used	
   to	
   provide	
   a	
   russian	
   parser	
   

among	
   the	
   resources	
   here:	
   
       http://corpus.leeds.ac.uk/mocky/	
   	
   

       the	
   opus	
   (open	
   parallel	
   corpus)	
   collects	
   tools	
   for	
   
various	
   languages:	
   
       http://opus.ling   l.uu.se/trac/wiki/tagging%20and

%20parsing	
   
       look	
   around!	
   

data	
   sources	
   

       parsers	
   depend	
   on	
   annotated	
   data	
   (treebanks)	
   
       you	
   can	
   use	
   a	
   parser	
   trained	
   on	
   news	
   articles,	
   but	
   

better	
   resources	
   for	
   humanities	
   scholars	
   will	
   
depend	
   on	
   community	
   e   orts	
   to	
   produce	
   better	
   
data	
   

       one	
   e   ort	
   is	
   the	
   construction	
   of	
   greek	
   and	
   latin	
   
dependency	
   treebanks	
   by	
   the	
   perseus	
   projecti:	
   
      http://nlp.perseus.tufts.edu/syntax/treebank/	
   	
   

parting	
   words	
   

applications?	
   (beyond	
   word	
   counts)	
   

       there	
   are	
   starting	
   to	
   be	
   a	
   few	
   applications	
   in	
   the	
   

humanities	
   using	
   richer	
   nlp	
   methods:	
   

       but	
   only	
   a	
   few   .	
   

applications?	
   (beyond	
   word	
   counts)	
   

      cameron	
   blevins.	
   2011.	
   topic	
   modeling	
   historical	
   
sources:	
   analyzing	
   the	
   diary	
   of	
   martha	
   ballard.	
   
dh	
   2011.	
   
       uses	
   (latent	
   variable)	
   topic	
   models	
   (lda	
   and	
   friends)	
   

running	
   through	
   a	
   group	
   of	
   texts	
   

       topic	
   model	
   are	
   primarily	
   used	
   to	
      nd	
   themes	
   or	
   topics	
   
       but,	
   here,	
   also	
   helpful	
   for	
   dealing	
   with	
   spelling	
   variation	
   (!)	
   
       uses	
   mallet	
   (http://mallet.cs.umass.edu/),	
   a	
   toolkit	
   with	
   a	
   
fair	
   amount	
   of	
   stu   	
   for	
   text	
   classi   cation,	
   sequence	
   tagging	
   
and	
   topic	
   models	
   

      we	
   also	
   have	
   the	
   stanford	
   topic	
   modeling	
   toolbox	
   
       http://nlp.stanford.edu/software/tmt/tmt-     0.3/	
   
       examines	
   change	
   in	
   diary	
   entry	
   topics	
   over	
   time	
   

applications?	
   (beyond	
   word	
   counts)	
   
      david	
   k.	
   elson,	
   nicholas	
   dames,	
   kathleen	
   r.	
   
mckeown.	
   2010.	
   extracting	
   social	
   networks	
   from	
   
literary	
   fiction.	
   acl	
   2010.	
   
       how	
   size	
   of	
   community	
   in	
   novel	
   or	
   world	
   relates	
   to	
   
amount	
   of	
   conversation	
   

       (stanford)	
   ner	
   tagger	
   to	
   identify	
   people	
   and	
   organizations	
   
       heuristically	
   matching	
   to	
   name	
   variants/shortenings	
   
       system	
   for	
   speech	
   attribution	
   (elson	
   &	
   mckeown	
   2010)	
   
       social	
   network	
   construction	
   

       results	
   showing	
   that	
   urban	
   novel	
   social	
   networks	
   are	
   
not	
   richer	
   than	
   those	
   in	
   rural	
   settings,	
   etc.	
   

applications?	
   (beyond	
   word	
   counts)	
   
      aditi	
   muralidharan.	
   2011.	
   a	
   visual	
   interface	
   for	
   
exploring	
   language	
   use	
   in	
   slave	
   narratives	
   dh	
   
2011.	
   http://bebop.berkeley.edu/wordseer	
   	
   
       a	
   visualization	
   and	
   reading	
   interface	
   to	
   american	
   slae	
   
narratives	
   

       (stanford)	
   parser	
   used	
   to	
   allow	
   searching	
   of	
   particular	
   

grammatical	
   relationships:	
   grammatical	
   search	
   

       visualization	
   tools	
   to	
   show	
   a	
   word   s	
   distribution	
   in	
   text	
   and	
   to	
   

provide	
   a	
      collapsed	
   concordance   	
   view	
      	
   and	
   for	
   close	
   
reading	
   

       	
   example	
   application	
   is	
   exploring	
   relationship	
   with	
   god	
   

parting	
   words	
   

	
   

this	
   talk	
   has	
   been	
   about	
   tools	
      	
   	
   

they   re	
   what	
   i	
   know	
   

	
   

but	
   you	
   should	
   focus	
   on	
   disciplinary	
   insight	
      	
   
not	
   on	
   building	
   corpora	
   and	
   tools,	
   but	
   on	
   using	
   
	
   them	
   as	
   tools	
   for	
   producing	
   disciplinary	
   research	
   

	
   

