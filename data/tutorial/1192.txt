deep	learning	

	nips   2015	tutorial	

	

	

geo   	hinton,	yoshua	bengio	&	yann	lecun	

breakthrough 

deep	learning:	machine	
learning	algorithms	based	on	
learning	mulhple	levels	of	
representahon	/	abstrachon.	

amazing	improvements	in	error	rate	in	object	recogni4on,	object	
detec4on,	speech	recogni4on,	and	more	recently,	in	natural	language	
processing	/	understanding	

	

2	

machine learning, 
ai & no free lunch 

       four	key	ingredients	for	ml	towards	ai	

1.    lots	&	lots	of	data	
2.    very	   exible	models	
3.    enough	compu4ng	power	
4.    powerful	priors	that	can	defeat	the	curse	of	

dimensionality	

3	

bypassing the curse of 
dimensionality 

we	need	to	build	composi4onality	into	our	ml	models		

just	as	human	languages	exploit	composi4onality	to	give	
representa4ons	and	meanings	to	complex	ideas	

exploi4ng	composi4onality	gives	an	exponen4al	gain	in	
representa4onal	power	

(1)	distributed	representa4ons	/	embeddings:	feature	learning	
(2)	deep	architecture:	mul4ple	levels	of	feature	learning	
addi4onal	prior:	composi4onality	is	useful	to	
describe	the	world	around	us	e   ciently	

4	

	

classical symbolic ai vs  
learning distributed representations 
       two	symbols	are	equally	far	from	each	other	
       concepts	are	not	represented	by	symbols	in	our	

brain,	but	by	pawerns	of	ac4va4on		

	(connec   onism,	1980   s)	

geo   rey	hinton	

output	units	

hidden	units	

input	
units	

5	

cat		

dog		

person		

david	rumelhart	

exponential advantage of distributed 
representations 

learning	a	set	of	parametric	features	that	are	not	
mutually	exclusive	can	be	exponen4ally	more	sta4s4cally	
e   cient	than	having	nearest-neighbor-like	or	id91-
like	models	

hidden units discover semantically 
meaningful concepts 
       zhou	et	al	&	torralba,	arxiv1412.6856	submiwed	to	iclr	2015	
       network	trained	to	recognize	places,	not	objects	

under review as a conference paper at iclr 2015

people

lighting

tables

figure 10: interpretation of a picture by different layers of the places-id98 using the tags provided
by amt workers. the    rst shows the    nal layer output of places-id98. the other three show
detection results along with the con   dence based on the units    activation and the semantic tags.

object counts in sun

fireplace (j=5.3%, ap=22.9%)

15000

bed (j=24.6%, ap=81.1%)

10000

wardrobe (j=4.2%, ap=12.7%)

5000

mountain (j=11.3%, ap=47.6%)

b)

0

l
l

a
w

 
 
 
 

r
i
a
h
c

 
 
 
 

w
o
d
n
w

i

 
 
 
 

g
n
d

i

l
i

u
b

 
 
 
 

r
o
o
fl

 
 
 
 

e
e
r
t
 
 
 
 

p
m
a

l
 

g
n

i

t
e
n
b
a
c

 
 
 
 

g
n

i
l
i

e
c

 
 
 
 

n
o
s
r
e
p

 
 
 
 

t
n
a
p

l

 
 
 
 

y
k
s
 
 
 
 

i

n
o
h
s
u
c

 
 
 
 

r
o
o
d

 
 
 
 

e
r
u
t
c
i
p

 
 
 
 

i

n
a
t
r
u
c

 
 
 
 

g
n
i
t
n
a
p

i

 
 
 
 

p
m
a

l
 
k
s
e
d

 
 
 
 

l

e
b
a
t
 
e
d
i
s
 
 
 
 

d
e
b

 
 
 
 

l

e
b
a
t
 
 
 
 

s
k
o
o
b

 
 
 
 

w
o

l
l
i

p

 
 
 
 

i

n
a
t
n
u
o
m

 
 
 
 

r
a
c

 
 
 
 

t
o
p

 
 
 
 

x
o
b

 
 
 
 

e
s
a
v

 
 
 
 

r
i
a
h
c
m
r
a

s
r
e
w
o
fl

 
 
 
 

d
a
o
r
 
 
 
 

s
s
a
r
g

 
 
 
 

e
l
t
t
o
b

 
 
 
 

s
e
o
h
s
 
 
 
 

a
f
o
s
 
 
 
 

t
e
l
t
u
o

 
 
 
 

p
o
t
k
r
o
w

n
g
i
s
 
 
 
 

k
o
o
b

 
 
 
 

 
 
 
 

sofa (j=10.8%, ap=36.2%)

 
 
 
 

e
t
a
p

l

 
 
 
 

e
c
n
o
c
s
 
 
 
 

r
o
r
r
i

m

 
 
 
 

n
m
u
o
c

l

 
 
 
 

g
u
r
 
 
 
 

k
s
e
d

 
 
 
 

t
e
k
s
a
b

 
 
 
 

d
n
u
o
r
g

 
 
 
 

k
c
o
l
c

 
 
 
 

l

s
e
v
e
h
s
 
 
 
 

l

e
b
a
t
 
e
e
ff
o
c

 
 
 
 

billiard table (j=3.2%, ap=42.6%)

e
c

i
l
i

 
 
 
 

counts of id98 units discovering each object class.

20

15

animals

seating

building (j=14.6%, ap=47.2%)

10

washing machine (j=3.2%, ap=34.4%)

5

0

30

a)

object counts of most informative objects for scene recognition

c)
figure 11: (a) segmentation of images from the sun database using pool5 of places-id98 (j =
jaccard segmentation index, ap = average precision-recall.) (b) precision-recall curves for some
discovered objects. (c) histogram of ap for all discovered object classes.

10

20

a)

7	

figure 9: (a) segmentations from pool5 in places-id98. many classes are encoded by several units
covering different object appearances. each row shows the 3 top most con   dent images for each

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

i
l
i

l
l

0

a
w

note that there are 115 units in pool5 of places-id98 not detecting objects. this could be due to
incomplete learning or a complementary texture-based or part-based representation of the scenes.
d)

e
b
a
t
 
e
e
ff
o
c

e
b
a
t
 
e
d
i
s
 
 
 
 

n
a
t
n
u
o
m

s
e
v
e
h
s
 
 
 
 

e
c
n
o
c
s
 
 
 
 

r
i
a
h
c
m
r
a

p
o
t
k
r
o
w

g
n
i
t
n
a
p

w
o
d
n
w

n
o
h
s
u
c

s
e
o
h
s
 
 
 
 

n
m
u
o
c

t
e
n
b
a
c

d
n
u
o
r
g

s
r
e
w
o
fl

e
b
a
t
 
 
 
 

e
r
u
t
c
i
p

n
a
t
r
u
c

n
o
s
r
e
p

d
a
o
r
 
 
 
 

t
e
k
s
a
b

n
g
i
s
 
 
 
 

a
f
o
s
 
 
 
 

s
k
o
o
b

e
l
t
t
o
b

t
e
l
t
u
o

e
e
r
t
 
 
 
 

l
 
k
s
e
d

g
u
r
 
 
 
 

y
k
s
 
 
 
 

t
n
a
p

k
c
o
l
c

s
s
a
r
g

k
o
o
b

e
t
a
p

r
i
a
h
c

r
o
o
d

r
o
o
fl

k
s
e
d

g
n
d

p
m
a

p
m
a

e
s
a
v

r
o
r
r
i

d
e
b

x
o
b

t
o
p

w
o

r
a
c

u
b

g
n

g
n

e
c

e
c

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

m

l
l
i

i
l
i

p

l
i

l
 

l

l

l

l

l

l

l

i

i

i

i

i

i

i

each feature can be discovered 
without the need for seeing the 
exponentially large number of 
configurations of the other features 
       consider	a	network	whose	hidden	units	discover	the	following	

features:	
       person	wears	glasses	
       person	is	female	
       person	is	a	child	
       etc.	

if	each	of	n	feature	requires	o(k)	parameters,	need	o(nk)	examples	
	
non-parametric	methods	would	require	o(nd)	examples	

8	

exponential advantage of distributed 
representations 
       bengio	2009	(learning	deep	architectures	for	ai,	f	&	t	in	ml)	
       montufar	&	morton	2014	(when	does	a	mixture	of	products	

contain	a	product	of	mixtures?	siam	j.	discr.	math)	

       longer	discussion	and	rela4ons	to	the	no4on	of	priors:	deep	

learning,	to	appear,	mit	press.	

       prop.	2	of	pascanu,	montufar	&	bengio	iclr   2014:	number	of	
pieces	dis4nguished	by	1-hidden-layer	rec4   er	net	with	n	units	
and	d	inputs	(i.e.	o(nd)	parameters)	is	

9	

deep learning: 
automating 
feature discovery 

output

mapping 
from 
features

output

output

output

mapping 
from 
features

mapping 
from 
features

most 
complex 
features

hand-
designed 
program

hand-
designed 
features

features

simplest 
features

input

input

input

input

10	

rule-based
systems

classic
machine
learning

representation

learning

deep
learning

fig:	i.	goodfellow	

exponential advantage of depth 

theore4cal	arguments:	

2 layers of 

logic gates 
formal neurons 
rbf units 

= universal approximator 

rbms & auto-encoders = universal approximator 

theorems on advantage of depth: 
(hastad et al 86 & 91, bengio et al 2007, bengio 
& delalleau 2011, martens et al 2013, pascanu 
et al 2014, montufar et al nips 2014) 

1	 2	 3	

   	

some functions compactly 
represented with k layers may 
require exponential size with 2 
layers 

   	

1	 2	 3	

n	

2n 

why does it work? no free lunch 
      

it	only	works	because	we	are	making	some	assump4ons	about	
the	data	genera4ng	distribu4on	

       worse-case	distribu4ons	s4ll	require	exponen4al	data	

       but	the	world	has	structure	and	we	can	get	an	exponen4al	gain	

by	exploi4ng	some	of	it	

12	

exponential advantage of depth 

       expressiveness	of	deep	networks	with	piecewise	linear	ac4va4on	

func4ons:	exponen4al	advantage	for	depth 		(montufar	et	al,	
nips	2014)	

       number	of	pieces	dis4nguished	for	a	network	with	depth	l	and	ni	

units	per	layer	is	at	least	

					or,	if	hidden	layers	have	width	n	and	input	has	size	n0	

13	

y lecun 

 
 

backprop 

(modular approach) 

typical multilayer neural net architecture 

y lecun 

c(x,y,  ) 

squared distance 

w3, b3 

linear 

relu 

w2, b2 

linear 

relu 

w1, b1 

linear 

l    complex learning machines can be 

built by assembling modules into 
networks 

l    linear module 

l    out = w.in+b 

l    relu module (rectified linear unit) 

l    outi = 0  if ini<0 
l    outi = ini  otherwise 
l    cost module: squared distance 

l    c = ||in1 - in2||2 

l    objective function 

l    l(  )=1/p   k c(xk,yk,  ) 
l       = (w1,b1,w2,b2,w3,b3) 

x (input) 

y (desired output) 

building a network by assembling modules 

y lecun 

l    all major deep learning frameworks use modules (inspired by sn/lush, 1991) 

l    torch7, theano, tensorflow   . 

 

c(x,y,  ) 

negativeloglikelihood 

logsoftmax 

w2,b2 

linear 

relu 

w1,b1 

linear 

x 

input 

y 

label  

computing gradients by back-propagation 

y lecun 

c(x,y,  ) 

cost 

wn 
dc/dwn 

wi 
dc/dwi 

fn(xn-1,wn) 

dc/dxi 

xi 

fi(xi-1,wi) 

dc/dxi-1 

xi-1 

f1(x0,w1) 

l    a practical application of chain rule 
 
l    backprop for the state gradients: 
l    dc/dxi-1 = dc/dxi . dxi/dxi-1  
l    dc/dxi-1 = dc/dxi . dfi(xi-1,wi)/dxi-1  
 
l    backprop for the weight gradients: 
l    dc/dwi = dc/dxi . dxi/dwi  
l    dc/dwi = dc/dxi . dfi(xi-1,wi)/dwi  

x (input) 

y (desired output) 

y lecun 

running backprop 

l    torch7 example 
l    gradtheta contains the gradient 
 

c(x,y,  ) 

negativeloglikelihood 

   

logsoftmax 

w2,b2 

linear 

relu 

w1,b1 

linear 

x 

input 

y 

label  

module classes 

y lecun 

linear 

relu 

duplicate 

add 

max 

logsoftmax 

l    y = w.x   ;   dc/dx = wt . dc/dy  ;  dc/dw = dc/dy . (dc/dx)t 
 
l    y = relu(x)  ;   if (x<0)  dc/dx = 0  else  dc/dx = dc/dy 
 
l    y1 = x, y2 = x   ;  dc/dx = dc/dy1 + dc/dy2 
 
l    y = x1 + x2    ;   dc/dx1 = dc/dy   ;   dc/dx2 = dc/dy 
 
l    y = max(x1,x2) ; if (x1>x2) dc/dx1 = dc/dy else dc/dx1=0 
 
l    yi = xi     log[   j exp(xj)] ;     .. 

module classes 

l    many more basic module classes 
l    cost functions: 

l    squared error 
l    hinge loss 
l    ranking loss 

y lecun 

l    non-linearities and operators 

l    relu,    leaky    relu, abs,   . 
l    tanh, logistic 
l    just about any simple function (log, exp, add, mul,   .) 

l    specialized modules 

l    multiple convolutions (1d, 2d, 3d) 
l    pooling/subsampling: max, average, lp, log(sum(exp())), maxout 
l    long short-term memory, attention, 3-way multiplicative interactions. 
l    switches 
l    id172s: batch norm, contrast norm, feature norm... 
l    inception 

any architecture works 

y lecun 

" any connection graph is permissible 
"    directed acyclic graphs (dag) 
"    networks with loops must be 

   unfolded in time   . 
" any module is permissible 

"    as long as it is continuous and 

differentiable almost everywhere with 
respect to the parameters, and with 
respect to non-terminal inputs. 

" most frameworks provide automatic 
differentiation  
"    theano, torch7+autograd,    
"     programs are turned into 

computation dags and automatically 
differentiated. 

backprop in practice 

y lecun 

" use relu non-linearities 
" use cross-id178 loss for classification 
" use stochastic id119 on minibatches 
" shuffle the training samples (    very important) 
" normalize the input variables (zero mean, unit variance) 
" schedule to decrease the learning rate 
" use a bit of l1 or l2 id173 on the weights (or a combination) 
" use    dropout    for id173 
" lots more in [lecun et al.    efficient backprop    1998] 
" lots, lots more in    neural networks, tricks of the trade    (2012 edition) 
edited by g. montavon, g. b. orr, and k-r m  ller (springer) 
" more recent: deep learning (mit press book in preparation) 

"    but it's best to turn it on after a couple of epochs 

y lecun 

 

convolutional  

networks 

deep learning = training multistage machines 

y lecun 

" traditional pattern recognition: fixed/handcrafted feature extractor 

feature  
extractor 

" mainstream pattern recognition 9until recently) 

feature  
extractor 

mid-level 
features 

" deep learning: multiple stages/layers trained end to end 

trainable  
classifier 

trainable  
classifier 

low-level 
features 

mid-level 
features 

high-level 
features 

trainable  
classifier 

overall architecture: multiple stages of  
id172     filter bank     non-linearity     pooling 

y lecun 

norm 

filter 
bank  

non- 
linear 

feature 
pooling  

norm 

filter 
bank  

non- 
linear 

feature 
pooling  

classifier 

" id172: variation on whitening (optional) 

       subtractive: average removal, high pass filtering 
       divisive: local contrast id172, variance id172 

" filter bank: dimension expansion, projection on overcomplete basis 
" non-linearity: sparsification, saturation, lateral inhibition.... 

       rectification (relu), component-wise shrinkage, tanh,.. 
 
 

" pooling: aggregation over space or feature type 

       max, lp norm, log prob.  

convnet architecture 

y lecun 

filter bank +non-linearity 

pooling 

filter bank +non-linearity 

pooling 

filter bank +non-linearity 

" lenet1  [lecun et al. nips 1989] 

multiple convolutions 

y lecun 

animation: andrej karpathy http://cs231n.github.io/convolutional-networks/ 

convolutional networks (vintage 1990)  

y lecun 

"

filters     tanh     average-tanh     filters     tanh     average-tanh     filters     tanh 

example: 1d (temporal) convolutional net 

" 1d (temporal) convnet, aka timed-delay neural nets 
" groups of units are replicated at each time step. 
" replicas have identical (shared) weights. 

y lecun 

lenet5  

" simple convnet  
" for mnist 
" [lecun 1998] 

y lecun 

input 
1@32x32 

layer 1 
6@28x28 

layer 2 
6@14x14 

layer 3 
12@10x10 

layer 4 
12@5x5 

layer 5 
100@1x1 

layer 6: 10 

10 

5x5 
convolution 

2x2 
pooling/ 
subsampling 

5x5 
convolution 
 

5x5 
convolution 
 

2x2 
pooling/ 
subsampling 

applying a convnet with a sliding window 

" every layer is a convolution  
" sometimes called    fully convolutional nets    
" there is no such thing as a    fully connected layer    
 

y lecun 

sliding window convnet + weighted id122 (fixed post-proc) 

y lecun 

[matan,	burges,	lecun,	denker	nips	1991]	[lecun,	bowou,	bengio,	ha   ner,	proc	ieee	1998]	

sliding window convnet + weighted id122 

y lecun 

why multiple layers? the world is compositional 
" hierarchy of representations with increasing level of abstraction 
" each stage is a kind of trainable feature transform 
"
image recognition: pixel     edge     texton     motif     part     object 
" text: character     word     word group     clause     sentence     story 
" speech: sample     spectral band     sound             phone     phoneme     word 
 
 

high-level 

low-level 
feature 

mid-level 
feature 

trainable  
classifier 

feature 

y lecun 

yes, convnets are somewhat inspired by the visual cortex 

y lecun 

" the ventral (recognition) pathway in the visual cortex has multiple stages 
" retina - lgn - v1 - v2 - v4 - pit - ait .... 
 

[picture from simon thorpe] 

[gallant & van essen]  

what are convnets good for 

y lecun 

" signals that comes to you in the form of (multidimensional) arrays. 
" signals that have strong local correlations 
" signals where features can appear anywhere 
" signals in which objects are invariant to translations and distortions. 
 
" 1d convnets: sequential signals, text 

       text classification 
       musical genre recognition 
       acoustic modeling for id103 
       time-series prediction 

" 2d convnets: images, time-frequency representations (speech and audio) 

       id164, localization, recognition 

" 3d convnets: video, volumetric images, tomography images 

       video recognition / understanding 
       biomedical image analysis 
       hyperspectral image analysis 

recurrent neural networks 

37	

recurrent neural networks 
       selec4vely	summarize	an	input	sequence	in	a	   xed-size	state	

vector	via	a	recursive	update	

f   

unfold

s

x

38	

st 1

st

st+1

f   

f   

f   

xt 1

xt

xt+1

recurrent neural networks 
       can	produce	an	output	at	each	4me	step:	unfolding	the	graph	

tells	us	how	to	back-prop	through	4me.	
o
ot 1

v w
s

u
x

39	

unfold

v

w

st 1
w

u
xt 1

ot

v

st

u
xt

ot+1

v

st+1

w

w

u
xt+1

generative id56s 
       an	id56	can	represent	a	fully-connected	directed	generahve	

model:	every	variable	predicted	from	all	previous	ones.	

lt 1

lt

lt+1

v

w

ot 1
st 1
w

ot

v

st

ot+1

v

st+1

w

w

u
xt 1

u
xt

40	

u
xt+1

xt+2

maximum likelihood =  
teacher forcing 
       during	training,	past	y	
in	input	is	from	training	
data	

  yt     p (yt | ht)

p (yt | ht)

       at	genera4on	4me,	

past	y	in	input	is	
generated	

       mismatch	can	cause		
   compounding	error   		

ht

xt

yt

41	

(xt, yt) : next input/output training pair

increasing the expressive power of 
id56s with more depth 
      
	

iclr	2014,	how	to	construct	deep	recurrent	neural	networks	

(cid:1)(cid:2)(cid:7)(cid:4)

(cid:1)(cid:2)(cid:3)(cid:4)

(cid:1)(cid:2)

(cid:6)(cid:2)(cid:3)(cid:4)

(cid:6)(cid:2)

(cid:6)(cid:2)(cid:7)(cid:4)

(cid:5)(cid:2)

(cid:5)(cid:2)(cid:3)(cid:4)
ordinary	id56s	

(cid:5)(cid:2)(cid:7)(cid:4)

(cid:6)(cid:2)

+	stacking	

(cid:7) (cid:2)(cid:4)(cid:5)

(cid:3)(cid:2)(cid:4)(cid:5)

42	

(cid:7) (cid:2)

(cid:3)(cid:2)

(cid:1)(cid:2)

(cid:3)(cid:2)(cid:4)(cid:5)

+	deep	hid-to-out	
+	deep	hid-to-hid	
+deep	in-to-hid	

(cid:6)(cid:2)

(cid:3)(cid:2)
(cid:1)(cid:2)

(cid:3)(cid:2)(cid:4)(cid:5)

(cid:6)(cid:2)

(cid:3)(cid:2)
(cid:1)(cid:2)

+	skip	connec4ons	for	
crea4ng	shorter	paths	

long-term dependencies 
       the	id56	gradient	is	a	product	of	jacobian	matrices,	each	

  

associated	with	a	step	in	the	forward	computa4on.	to	store	
informa4on	robustly	in	a	   nite-dimensional	state,	the	dynamics	
must	be	contrac4ve	[bengio	et	al	1994].		

	
       problems:		

	
43	

       sing.	values	of	jacobians	>	1	  	gradients	explode		
       or	sing.	values	<	1	  	gradients	shrink	&	vanish	
       or	random	  	variance	grows	exponen4ally	

storing	bits	
robustly	requires	
sing.	values<1	

gradient	
clipping	
(hochreiter	1991)	

gradient norm clipping 

(mikolov	thesis	2012;	
pascanu,	mikolov,	bengio,	icml	2013)	

44	

id56 tricks  
(pascanu,	mikolov,	bengio,	icml	2013;	bengio,	boulanger	&	pascanu,	icassp	2013)	
       clipping	gradients	(avoid	exploding	gradients)	
       leaky	integra4on	(propagate	long-term	dependencies)	
       momentum	(cheap	2nd	order)	
      
       sparse	gradients	(symmetry	breaking)	
       gradient	propaga4on	regularizer	(avoid	vanishing	gradient)	
       lstm	self-loops	(avoid	vanishing	gradient)	

ini4aliza4on	(start	in	right	ballpark	avoids	exploding/vanishing)	

error

45	

   

   

id149 & lstm 
       create	a	path	where	
gradients	can	   ow	for	
longer	with	self-loop	

output

  

       corresponds	to	an	

eigenvalue	of	jacobian	
slightly	less	than	1	

       lstm	is	heavily	used	

(hochreiter	&	schmidhuber	
1997)	

       gru	light-weight	version	

(cho	et	al	2014)	

46	

self-loop

  

state

+

  

input

input gate

forget gate

output gate

id56 tricks 
       delays	and	mul4ple	4me	scales,	elhihi	&	bengio	nips	1996	

o

ot 1

ot

w3

st 2
w1

w3

st 1
w1

st

w3

w1

ot+1

w3

st+1
w1

xt 1

xt

xt+1

w1

s

x

w3

unfold

47	

backprop in practice 

other	tricks:	see	deep	learning	book	(in	prepara4on,	online)	

48	

the convergence of id119 

y 
lecun 

"    batch gradient 
"    there is an optimal learning 
rate 
nd
"    equal to inverse 2

 derivative 

let's look at a single linear unit 

"    single unit, 2 inputs 
"    quadratic loss 

"    e(w) = 1/p    p (y     w   xp)2 

"    dataset: classification: y=-1 for blue, +1 for red. 
"    hessian is covariance matrix of input vectors 

"    to avoid ill conditioning: normalize the inputs 

"    h = 1/p     xp xpt 
"    zero mean  
"    unit variance for all variable  

y 
lecun 

w2 

x2 

w0 

w1 

x1 

convergence is slow when hessian has different eigenvalues 

"    batch gradient, small learning rate            batch gradient, large learning rate 

y 
lecun 

convergence is slow when hessian has different eigenvalues 

"    batch gradient, small learning rate            "    stochastic gradient: much faster 
"    batch gradient, small learning rate            
"    batch gradient, small learning rate            
"    but fluctuates near the minimum 

y 
lecun 

multilayer nets have non-convex objective functions 

"    1-1-1 network  
"     trained to compute the identity function with quadratic loss 
"     solution: w2 = 1/w2  hyperbola. 

"    y = w1*w2*x  
"    single sample x=1, y=1  l(w) = (1-w1*w2)^2 

y 
lecun 
y 
w2 
z 
w1 
x 

solution  saddle point  solution 

deep nets with relus and max pooling 

"    stack of linear transforms interspersed with max operators 
"    point-wise relus: 

"    max pooling 
"    input-output function 

"       switches    from one layer to the next 
"    sum over active paths 
"    product of all weights along the path 
"    solutions are hyperbolas 

"    objective function is full of saddle points 

y 
lecun 

31 

22 

w31,22 

w22,14 

14 

w14,3 

3 
z3 

a myth has been debunked: local 
minima in neural nets  
! convexity is not needed 

(pascanu,	dauphin,	ganguli,	bengio,	arxiv	may	2014):	on	the	
saddle	point	problem	for	non-convex	op   miza   on	
(dauphin,	pascanu,	gulcehre,	cho,	ganguli,	bengio,	nips   	2014):	
iden   fying	and	a[acking	the	saddle	point	problem	in	high-
dimensional	non-convex	op   miza   on		
(choromanska,	hena   ,	mathieu,	ben	arous	&	lecun,	
aistats   2015):	the	loss	surface	of	mul   layer	nets	

      

      

      

55	

saddle points 

       local	minima	dominate	in	low-d,	but	

saddle	points	dominate	in	high-d	
       most	local	minima	are	close	to	the	

bowom	(global	minimum	error)	

56	

saddle points during training 
       oscilla4ng	between	two	behaviors:	
       slowly	approaching	a	saddle	point	
       escaping	it	

57	

low index critical points 

choromanska	et	al	&	lecun	2014,	   the	loss	surface	of	mul   layer	nets   	
shows	that	deep	rec4   er	nets	are	analogous	to	spherical	spin-glass	models	
the	low-index	cri4cal	points	of	large	models	concentrate	in	a	band	just	
above	the	global	minimum	

58	

piecewise linear nonlinearity 

	

      

jarreth,		kavukcuoglu,	ranzato	&	lecun	iccv	2009:	absolute	value	
rec4   ca4on	works	bewer	than	tanh	in	lower	layers	of	convnet	

       nair	&	hinton	icml	2010:	duplica4ng	sigmoid	units	with	same	
weights	but	di   erent	bias	in	an	rbm	approximates	a	rec4   ed	
linear	unit	(relu)	

sotplus	

f(x)=log(1+exp(x))	

       glorot,	bordes	and	bengio	aistats	2011:	using	a	rec4   er	non-

linearity	(relu)	instead	of	tanh	of	sotplus	allows	for	the	   rst	4me	
to	train	very	deep	supervised	networks	without	the	need	for	
unsupervised	pre-training;	was	biologically	mohvated	

f(x)=max(0,x)	

neuroscience motivations 
leaky integrate-and-fire model 

       krizhevsky,	sutskever	&	hinton	nips	2012:	
						rec4   ers	one	of	the	crucial	ingredients	in	
						id163	breakthrough	

leaky integrate-and-fire model 

improving	neural	networks	by	prevenhng	co-adaptahon	

stochastic neurons as regularizer: 
of	feature	detectors	(hinton et al 2012, arxiv) 
       dropouts	trick:	during	training	mul4ply	neuron	output	by	random	

bit	(p=0.5),	during	test	by	0.5	

       used	in	deep	supervised	networks	
       similar	to	denoising	auto-encoder,		but	corrup4ng	every	layer	
       works	bewer	with	some	non-lineari4es	(rec4   ers,	maxout)						
       equivalent	to	averaging	over	exponen4ally	many	architectures	

(goodfellow	et	al.	icml	2013)	

       used	by	krizhevsky	et	al	to	break	through	id163	sota	
       also	improves	sota	on	cifar-10	(18  16%	err)	
       knowledge-free	mnist	with	dbms	(.95  .79%	err)	
       timit	phoneme	classi   ca4on	(22.7  19.7%	err)	

60	

dropout regularizer: super-efficient 
id112 

*	

   	

   	

61	

(3)

  xk =

where m is the size of the mini-batch. using these statistics,
we can standardize each feature as follows
xi,k,

bn (xk) =  k   xk +  k.

batch id172 
p 2

mxi=1
1
m
xk     xk
(io   e	&	szegedy	icml	2015)		
mxi=1
1
k +    
m

,
(xi,k     xk)2,

  xk =
by setting  k to  k and  k to   xk, the network can recover the
       standardize	ac4va4ons	(before	nonlinearity)	across	minibatch	
 2
k =
original layer representation. so, for a standard feedforward
       backprop	through	this	operahon	
layer in a neural network
       regularizes	&	helps	to	train	

where     is a small positive constant to improve numerical sta-
bility.

where m is the size of the mini-batch. using these statistics,
we can standardize each feature as follows

however, standardizing the intermediate activations re-
duces the representational power of the layer. to account for
this, batch id172 introduces additional learnable pa-
rameters   and  , which respectively scale and shift the data,
leading to a layer of the form

if we have access to the whole input sequence, we can use
y =  (wx + b),
information not only from the past time steps, but also from
xk     xk
the future ones, allowing for bidirectional id56s [12]
k +    

where w is the weights matrix, b is the bias vector, x is the
 !h t =  ( !wh !h t 1 +  !wxxt),
  h t =  (  wh  h t+1 +   wxxt),
input of the layer and   is an arbitrary activation function,
ht = [ !h t :   h t],
batch id172 is applied as follows
(4)
where [x : y] denotes the concatenation of x and y. finally,
by setting  k to  k and  k to   xk, the network can recover the
we can stack id56s by using h as the input to another id56,
creating deeper architectures [13]
original layer representation. so, for a standard feedforward
layer in a neural network

however, standardizing the intermediate activations re-
duces the representational power of the layer. to account for
this, batch id172 introduces additional learnable pa-
rameters   and  , which respectively scale and shift the data,
where     is a small positive constant to improve numerical sta-
leading to a layer of the form
t 1 + wxhl 1
note that the bias vector has been removed, since its effect
bility.
however, standardizing the intermediate activations re-

where     is a small positive constant to improve numerical sta-
bility.

where m is the size of the mini-batch. using these statistics,
we can standardize each feature as follows

y =  (bn (wx)).

bn (xk) =  k   xk +  k.

p 2

(xi,k     xk)2,

mxi=1
mxi=1

xk     xk
k +    

hl
t =  (whhl

p 2

(1)
  xk =

 2
k =

  xk =

  xk =

xi,k,

1
m

1
m

(2)

(3)

62	

,

,

early stopping 
       beau4ful	free	lunch	(no	need	to	launch	many	di   erent	
training	runs	for	each	value	of	hyper-parameter	for	#itera4ons)	

       monitor	valida4on	error	during	training	(ater	visi4ng	#	of	

training	examples	=	a	mul4ple	of	valida4on	set	size)	

       keep	track	of	parameters	with	best	valida4on	error	and	report	

them	at	the	end	

if	error	does	not	improve	enough	(with	some	pa4ence),	stop.	

      

63	

random sampling of hyperparameters 
(bergstra	&	bengio	2012)	
       common	approach:	manual	+	grid	search	
       grid	search	over	hyperparameters:	simple	&	wasteful	
       random	search:	simple	&	e   cient	

       independently	sample	each	hp,	e.g.	l.rate~exp(u[log(.1),log(.0001)])	
       each	training	trial	is	iid	
       if	a	hp	is	irrelevant	grid	search	is	wasteful	
       more	convenient:	ok	to	early-stop,	con4nue	further,	etc.	

64	

sequential model-based optimization 
of hyper-parameters 
      

(huwer	et	al	jair	2009;	bergstra	et	al	nips	2011;	thornton	et	al	
arxiv	2012;	snoek	et	al	nips	2012)	
iterate	
			es4mate	p(valid.	err	|	hyper-params	con   g	x,	d)	
			choose	op4mis4c	x,	e.g.	maxx	p(valid.	err	<	current	min.	err	|	x)	
			train	with	con   g	x,	observe	valid.	err.	v,	d	  	d	u	{(x,v)}	

      
      
      
      

65	

distributed training 
       minibatches		
       large	minibatches	+	2nd	order	&	natural	gradient	methods	
       asynchronous	sgd	(bengio	et	al	2003,	le	et	al	icml	2012,	dean	et	al	nips	2012)	

       data	parallelism	vs	model	parallelism	
       bowleneck:	sharing	weights/updates	among	nodes,	to	avoid	
node-models	to	move	too	far	from	each	other	
       easgd	(zhang	et	al	nips	2015)	works	well	in	prac4ce	
       e   ciently	exploi4ng	more	than	a	few	gpus	remains	a	challenge	

66	

vision 

67	

((switch	laptops)	

id103 

68	

the dramatic impact of deep 
learning on id103 
 (according to microsoft) 

 

100% 

 

d
r
a
o
b
h
c
t
i

using dl 

 

w
s
n
o
e

 

t

a
r
 
r
o
r
r
e

 

d
r
o
w

69 

10% 

4% 

2% 

1% 

1990 

2000 

2010 

id103 with convolutional nets (nyu/ibm) 

y lecun 

"    multilingual recognizer 
"    multiscale input 

"    large context window 

id103 with convolutional nets (nyu/ibm) 

y lecun 

"    acoustic model: convnet with 7 layers. 54.4 million parameters. 
"    classifies acoustic signal into 3000 context-dependent subphones categories 
"    relu units + dropout for last layers 
"    trained on gpu. 4 days of training 

id103 with convolutional nets (nyu/ibm) 

y lecun 

"    training samples.  

"    40 mel-frequency cepstral coefficients 
"    window: 40 frames, 10ms each 

id103 with convolutional nets (nyu/ibm) 

y lecun 

"    convolution kernels at layer 1: 

"    64 kernels of size 9x9 

0.4

"c"

end-to-end training 
with search 
       hybrid	systems,	neural	nets	+	
id48s	(bengio	1991,	bo[ou	1991)	
       neural	net	outputs	scores	for	
each	arc,	recognized	output	=	
labels	along	best	path;	trained	
discrimina4vely	(lecun	et	al	1998)	
       connec4onist	temporal	
classi   ca4on	(graves	2006)	
       deepspeech	and	awen4on-
based	end-to-end	id56s	
(hannun	et	al	2014;	graves	&	
jaitly	2014;	chorowski	et	al	
nips	2015)	

n
o
i
t
i
s
o
p
m
o
c
h
p
a
r

match
& add

"o"

"d"

"c"

g

 

0.4

1.0

1.8

interpretations:
cut  (2.0)
cap (0.8)
cat  (1.4)

interpretation graph
0.8

"t"

0.8

"u"

"a"

0.2

"p"

0.2

"t"

0.8

match
& add

match
& add

"b"

"c"

grammar graph

"a"

"u"

"u"

"a"

"t"

"r"

"n"

"t"

"t"

"r"
"p"
"r"

"e"

"e"

"d"

"x"

0.1

"a"

0.2

"u"

0.8

"p"

0.2

"t"

0.8

recognition
graph

74	

natural language 
representations 

75	

neural language models: fighting one 
exponential by another one! 
      

(bengio	et	al	nips   2000)	

i   th output = p(w(t)  = i | context)

output

. . .

softmax

. . .

exponen4ally	large	set	of	
generaliza4ons:	seman4cally	close	
sequences	

most  computation here

. . .

tanh

. . .

r(w1)

r(w2)

r(w3)

r(w4)

r(w5)

r(w6)

c(w(t   n+1))
. . .

c(w(t   2)) c(w(t   1))

. . .

. . .

. . .

table
look   up
in

c

matrix

c

shared parameters
across words

w1 w2 w3 w4 w5 w6

index for w(t   n+1)

index for w(t   2)

index for w(t   1)

input sequence

76	

exponen4ally	large	set	of	possible	contexts	

neural id27s: visualization 
directions = learned attributes 

77	

analogical representations for free 

(mikolov	et	al,	iclr	2013)	
       seman4c	rela4ons	appear	as	linear	rela4onships	in	the	space	of	

learned	representa4ons	

       king	   	queen	   		man	   	woman	
       paris	   	france	+	italy	   	rome	

france	

italy	

paris	

rome	

78	

	

handling large output spaces 
       sampling	   nega4ve   	examples:	increase	score	of	
correct	word	and	stochas4cally	decrease	all	the	
others		
       uniform	sampling	(collobert	&	weston,	icml	2008)		
      

importance	sampling,	(bengio	&	senecal	aistats	2003;	dauphin	et	al	icml	
2011)	;	gpu	friendly	implementa4on	(jean	et	al	acl	2015)	

       decompose	output	probabili4es	hierarchically		(morin	&	

bengio	2005;	blitzer	et	al	2005;	mnih	&	hinton	2007,2009;	mikolov	et	al	2011)	

79	

categories	

words	within	each	category	

encoder-decoder framework 
      

intermediate	representa4on	of	meaning		
=	   universal	representa4on   	

       encoder:	from	word	sequence	to	sentence	representa4on	
       decoder:	from	representa4on	to	word	sequence	distribu4on	

english	sentence	

	

a
t
a
d
	
t
x
e
t
i
b
	
r
o
f

english	
decoder	

french	
encoder	

french	sentence	

	

a
t
a
d

	
l

a
u
g
n

i
l
i

n
u
	
r
o
f

english	sentence	

english	
decoder	

english	
encoder	

english	sentence	

(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)

(cid:5)(cid:4)(cid:6)

(cid:5)(cid:3)

(cid:5)(cid:2)

(cid:7)

(cid:1)(cid:2)

(cid:1)(cid:3)

(cid:1)(cid:4)

(cid:13)(cid:14)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)

80	

(cho	et	al	emnlp	2014;	sutskever	et	al	nips	2014)		

attention mechanism for deep 
learning 
       consider	an	input	(or	intermediate)	sequence	or	image	
       consider	an	upper	level	representa4on,	which	can	choose	

  	where	to	look	  ,	by	assigning	a	weight	or	id203	to	each	
input	posi4on,	as	produced	by	an	mlp,	applied	at	each	posi4on	

sotmax	over	lower		
loca4ons	condi4oned	
on	context	at	lower	and	
higher	loca4ons		

higher-level	

       sot	awen4on	(backprop)	vs	
       stochas4c	hard	awen4on	(rl)	

lower-level	

81	

(bahdanau,	cho	&	bengio,	arxiv	sept.	2014)	following	up	on	(graves	2013)	and	
(larochelle	&	hinton	nips	2010)	
	

end-to-end machine translation with 
recurrent nets and attention mechanism 
(bahdanau	et	al	2014,	jean	et	al	2014,	gulcehre	et	al	2015,	jean	et	al	2015)			
       reached	the	state-of-the-art	in	one	year,	from	scratch	

(cid:62)(cid:81)(cid:114) (cid:55)(cid:28)(cid:96) (cid:43)(cid:28)(cid:77) (cid:114)(cid:50) (cid:59)(cid:81) (cid:114)(cid:66)(cid:105)(cid:63) (cid:28) (cid:112)(cid:50)(cid:96)(cid:118) (cid:72)(cid:28)(cid:96)(cid:59)(cid:50) (cid:105)(cid:28)(cid:96)(cid:59)(cid:50)(cid:105) (cid:112)(cid:81)(cid:43)(cid:28)(cid:35)(cid:109)(cid:72)(cid:28)(cid:96)(cid:118)(cid:92) (cid:85)(cid:82)(cid:86)

(cid:85)(cid:28)(cid:86) (cid:49)(cid:77)(cid:59)(cid:72)(cid:66)(cid:98)(cid:63)   (cid:54)(cid:96)(cid:50)(cid:77)(cid:43)(cid:63) (cid:85)(cid:113)(cid:74)(cid:104)(cid:64)(cid:82)(cid:57)(cid:86)

(cid:76)(cid:74)(cid:104)(cid:85)(cid:27)(cid:86) (cid:58)(cid:81)(cid:81)(cid:59)(cid:72)(cid:50)
(cid:106)(cid:121)(cid:88)(cid:101)   

(cid:83)(cid:64)(cid:97)(cid:74)(cid:104)

(cid:76)(cid:74)(cid:104)
(cid:89)(cid:42)(cid:28)(cid:77)(cid:47)
(cid:89)(cid:108)(cid:76)(cid:69)
(cid:89)(cid:49)(cid:77)(cid:98)

(cid:106)(cid:107)(cid:88)(cid:101)(cid:51)
(cid:106)(cid:106)(cid:88)(cid:107)(cid:51)
(cid:106)(cid:106)(cid:88)(cid:78)(cid:78)
(cid:106)(cid:101)(cid:88)(cid:100)(cid:82)
(cid:85)(cid:35)(cid:86) (cid:49)(cid:77)(cid:59)(cid:72)(cid:66)(cid:98)(cid:63)   (cid:58)(cid:50)(cid:96)(cid:75)(cid:28)(cid:77) (cid:85)(cid:113)(cid:74)(cid:104)(cid:64)(cid:82)(cid:56)(cid:86)
(cid:74)(cid:81)(cid:47)(cid:50)(cid:72)
(cid:107)(cid:57)(cid:88)(cid:51)
(cid:107)(cid:57)(cid:88)(cid:121)
(cid:107)(cid:106)(cid:88)(cid:101)
(cid:107)(cid:107)(cid:88)(cid:51)
(cid:107)(cid:107)(cid:88)(cid:100)

(cid:108)(cid:88)(cid:49)(cid:47)(cid:66)(cid:77)(cid:35)(cid:109)(cid:96)(cid:59)(cid:63)(cid:45) (cid:97)(cid:118)(cid:77)(cid:105)(cid:28)(cid:43)(cid:105)(cid:66)(cid:43) (cid:97)(cid:74)(cid:104)

(cid:108)(cid:88)(cid:49)(cid:47)(cid:66)(cid:77)(cid:35)(cid:109)(cid:96)(cid:59)(cid:63)(cid:45) (cid:83)(cid:63)(cid:96)(cid:28)(cid:98)(cid:50) (cid:97)(cid:74)(cid:104)

(cid:69)(cid:65)(cid:104)(cid:45) (cid:83)(cid:63)(cid:96)(cid:28)(cid:98)(cid:50) (cid:97)(cid:74)(cid:104)

(cid:71)(cid:65)(cid:74)(cid:97)(cid:65)(cid:102)(cid:69)(cid:65)(cid:104)

(cid:76)(cid:50)(cid:109)(cid:96)(cid:28)(cid:72) (cid:74)(cid:104)

(cid:76)(cid:81)(cid:105)(cid:50)

(cid:284)

(cid:106)(cid:100)(cid:88)(cid:121)(cid:106)   

(cid:106)(cid:107)(cid:88)(cid:100)   
(cid:106)(cid:101)(cid:88)(cid:78)   
(cid:85)(cid:43)(cid:86) (cid:49)(cid:77)(cid:59)(cid:72)(cid:66)(cid:98)(cid:63)   (cid:42)(cid:120)(cid:50)(cid:43)(cid:63) (cid:85)(cid:113)(cid:74)(cid:104)(cid:64)(cid:82)(cid:56)(cid:86)
(cid:74)(cid:81)(cid:47)(cid:50)(cid:72)
(cid:82)(cid:51)(cid:88)(cid:106)
(cid:82)(cid:51)(cid:88)(cid:107)
(cid:82)(cid:100)(cid:88)(cid:101)
(cid:82)(cid:100)(cid:88)(cid:57)
(cid:82)(cid:101)(cid:88)(cid:82)

(cid:108)(cid:88)(cid:49)(cid:47)(cid:66)(cid:77)(cid:35)(cid:109)(cid:96)(cid:59)(cid:63)(cid:45) (cid:83)(cid:63)(cid:96)(cid:28)(cid:98)(cid:50) (cid:97)(cid:74)(cid:104)
(cid:108)(cid:88)(cid:49)(cid:47)(cid:66)(cid:77)(cid:35)(cid:109)(cid:96)(cid:59)(cid:63)(cid:45) (cid:97)(cid:118)(cid:77)(cid:105)(cid:28)(cid:43)(cid:105)(cid:66)(cid:43) (cid:97)(cid:74)(cid:104)

(cid:67)(cid:62)(cid:108)(cid:45) (cid:97)(cid:74)(cid:104)(cid:89)(cid:71)(cid:74)(cid:89)(cid:80)(cid:97)(cid:74)(cid:89)(cid:97)(cid:84)(cid:28)(cid:96)(cid:98)(cid:50)

(cid:42)(cid:108)(cid:45) (cid:83)(cid:63)(cid:96)(cid:28)(cid:98)(cid:50) (cid:97)(cid:74)(cid:104)

(cid:76)(cid:50)(cid:109)(cid:96)(cid:28)(cid:72) (cid:74)(cid:104)

(cid:76)(cid:81)(cid:105)(cid:50)

82	

iwslt 2015     luong & manning (2015) 
ted talk mt, english-german 

id7	(cased)	

hter	(he	set)	

30.85	

26.18	

26.02	

24.96	

22.51	

20.08	

stanford	

karlsruhe	 edinburgh	 heidelberg	

pjait	

baseline	

30	

25	

20	

15	

10	

5	

0	

28.18	

22.67	

23.42	

-26%	

21.84	

16.16	

stanford	

edinburgh	 karlsruhe	 heidelberg	

pjait	

35	

30	

25	

20	

15	

10	

5	

0	

83	

image-to-text: id134 
with attention 

(xu	et	al,	icml	2015)	

f = (a,   man,   is,   jumping,   into,   a,   lake,   .)

following	many	papers	
on	cap4on	genera4on,	
including	(kiros	et	al	
2014;	mao	et	al	2014;	
vinyals	et	al	2014;	
donahue	et	al	2014;	
karpathy	&	li	2014;	
fang	et	al	2014)	

e ui

l
p
m
a
s
s

 

d
r
o
w

t
n
e
r
r
u
c
e
r

e zi

t
a
t
s

m
s
i
n
a
h
c
e

m

n
o
i
t
n
e
t
t

a

a

j

attention 
       weight

aj(cid:1) =1

+

k
r
o
w
t
e
n

 
l
a
r
u
e
n

 
l
a
n
o
i
t
u
l
o
v
n
o
c

annotation
vectors

hj

(cid:85)(cid:115)(cid:109) (cid:50)(cid:105) (cid:28)(cid:72)(cid:88)(cid:45) (cid:107)(cid:121)(cid:82)(cid:56)(cid:86)(cid:45) (cid:85)(cid:117)(cid:28)(cid:81) (cid:50)(cid:105) (cid:28)(cid:72)(cid:88)(cid:45) (cid:107)(cid:121)(cid:82)(cid:56)(cid:86)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

84	

paying 
attention to 
selected parts 
of the image 
while uttering 
words 

85	

the good 

86	

and the bad 

87	

but how can neural nets remember things? 

y 
lecun 

"    recurrent networks cannot remember things for very long 
"    the cortex only remember things for 20 seconds 
"    we need a    hippocampus    (a separate memory module) 

"    lstm [hochreiter 1997], registers 
"    memory networks [weston et 2014] (fair), associative memory 
"    ntm [graves et al. 2014],    tape   . 

attention 
mechanism 

recurrent net 

memory 

memory networks enable reasoning 

y 
lecun 

"    add a short-term memory to a network 

http://arxiv.org/abs/1410.3916 

results on  
id53 
task 

(weston, chopra, 
bordes 2014) 
 

end-to-end memory network 

"    [sukhbataar, szlam, weston, fergus nips 2015, arxiv:1503.08895] 
"    weakly-supervised memnn: no need to tell which memory location to use. 

y 
lecun 

stack-augmented id56: learning    algorithmic    sequences 

"    [joulin & mikolov, arxiv:1503.01007] 

y 
lecun 

sparse access memory for long-term 
dependencies 
       a	mental	state	stored	in	an	external	memory	can	stay	for	
arbitrarily	long	dura4ons,	un4l	evoked	for	read	or	write	

       forge(cid:133)ng	=	vanishing	gradient.	
       memory	=	larger	state,	reducing	the	need	for	forge(cid:133)ng/vanishing	

passive	copy	

access	

92	

how do humans generalize 
from very few examples? 
       they	transfer	knowledge	from	previous	learning:	

       representa4ons	
      

explanatory	factors	

       previous	learning	from:	unlabeled	data		

	

	

					

	+	labels	for	other	tasks	

       prior:	shared	underlying	explanatory	factors,	in	

parhcular	between	p(x)	and	p(y|x)		

93	

	

unsupervised and id21 challenge 
+ id21 challenge: won by 
unsupervised deep learning 

raw	data	

icml   2011	
workshop	on	
unsup.	&	
transfer	learning	

1	layer	

2	layers	

3	layers	

4	layers	

nips   2011	
transfer	
learning	
challenge		
paper:	
icml   2012	

id72 

       generalizing	bewer	to	new	tasks	(tens	
of	thousands!)	is	crucial	to	approach	ai	
       example:	speech	recogni4on,	sharing	

across	mul4ple	languages	

task 1  
output y1 
task	a	

task 2 
output y2 
task	b	

task 3  
output y3 
task	c	

       deep	architectures	learn	good	

intermediate	representa4ons	that	can	
be	shared	across	tasks	

					(collobert	&	weston	icml	2008,	
					bengio	et	al	aistats	2011)	
       good	representa4ons	that	disentangle	

underlying	factors	of	varia4on	make	
sense	for	many	tasks	because	each	
task	concerns	a	subset	of	the	
factors	
prior:	shared	underlying	explanatory	factors	between	tasks		
	

raw input x 

e.g.	dic4onary,	with	intermediate	
concepts	re-used	across	many	de   ni4ons	

95	

google image search 
joint embedding: different 
object types represented in same space 

google:	
s.	bengio,	j.	
weston	&	n.	
usunier	
(ijcai	2011,	
nips   2010,	
jmlr	2010,	
ml	j	2010)	

wsabie	objec4ve	func4on:	

combining multiple sources of evidence 
with shared representations 
       tradi4onal	ml:	data	=	matrix	
       rela4onal	learning:	mul4ple	sources,	

person	 url	

history	

words	

event	

url	

di   erent	tuples	of	variables	

       share	representa4ons	of	same	types	

across	data	sources	

       shared	learned	representa4ons	help	
propagate	informa4on	among	data	
sources:	e.g.,	id138,	xwn,	
wikipedia,	freebase,	id163   
(bordes	et	al	aistats	2012,	ml	j.	2013)	

       facts	=	data	
       deduchon	=	generalizahon	
97	

event	

url	

person	

history	

words	

url	

p(person,url,event)	

p(url,words,history)	

multi-task / multimodal learning 
with different inputs for different 
tasks 

y

e.g.	speaker	adapta4on,	
mul4modal	input   	

unsupervised	mul4modal	case:	
		(srivastava	&	salakhutdinov	nips	2012)	

selection switch

h1

h2

h3

98	

x1

x2

x3

maps between 
representations 

hx = fx(x)

fx

-space
x

xtest

x	and	y	represent	
di   erent	modali4es,	e.g.,	
image,	text,	sound   	
	
can	provide	0-shot	
generaliza4on	to	new	
categories	(values	of	y)	

99	

hy = fy(y)

fy

y-space

ytest

pairs in the training set

(x, y)
-representation (encoder) function
x
fx
-representation (encoder) function fy
y
relationship between embedded points 
within one of the domains
maps between representation spaces 

unsupervised representation 
learning 

100	

why unsupervised learning? 

       recent	progress	mostly	in	supervised	dl	
       real	challenges	for	unsupervised	dl	
       poten4al	bene   ts:	

       exploit	tons	of	unlabeled	data	
       answer	new	ques4ons	about	the	variables	observed	
       regularizer	   	transfer	learning	   	domain	adapta4on	
       easier	op4miza4on	(divide	and	conquer)	
       joint	(structured)	outputs	

101	

why latent factors & unsupervised 
representation learning? because of 
causality. 

on	causal	and	an   causal	learning,	(janzing	et	al	icml	2012)			

      

if	ys	of	interest	are	among	the	causal	factors	of	x,	then	

p (y |x) =

p (x|y )p (y )

p (x)

is	4ed	to	p(x)	and	p(x|y),	and	p(x)	is	de   ned	in	terms	of	p(x|y),	i.e.	
       the	best	possible	model	of	x	(unsupervised	learning)	must	

involve	y	as	a	latent	factor,	implicitly	or	explicitly.	

       representa4on	learning	seeks	the	latent	variables	h	that	explain	

the	varia4ons	of	x,	making	it	likely	to	also	uncover	y.	

		

102	

if y is a cause of x, semi-supervised 
learning works 
      
       ater	learning	p(x)	as	a	mixture,	a	single	labeled	example	per	class	

just	observing	the	x-density	reveals	the	causes	y	(cluster	id)	

su   ces	to	learn	p(y|x)	

(cid:77)(cid:105)(cid:120)(cid:116)(cid:117)(cid:114)(cid:101)(cid:32)(cid:109)(cid:111)(cid:100)(cid:101)(cid:108)

(cid:121)(cid:61)(cid:49)

(cid:121)(cid:61)(cid:50)

(cid:121)(cid:61)(cid:51)

(cid:48)(cid:46)(cid:53)

(cid:48)(cid:46)(cid:52)

(cid:48)(cid:46)(cid:51)

(cid:48)(cid:46)(cid:50)

(cid:48)(cid:46)(cid:49)

)
x
(
p

103	

(cid:48)(cid:46)(cid:48)

(cid:48)

(cid:53)

(cid:49)(cid:48)
x

(cid:49)(cid:53)

(cid:50)(cid:48)

invariance & disentangling 
underlying factors 
       invariant	features	
       which	invariances?	
       alterna4ve:	learning	to	disentangle	factors,	i.e.	
keep	all	the	explanatory	factors	in	the	
representa4on	
       good	disentangling	  		
       emerges	from	representa4on	learning						

	avoid	the	curse	of	dimensionality	

(goodfellow	et	al.	2009,	glorot	et	al.	2011)	

104	

id82s / 
undirected id114 

       boltzmann	machines:	
			(hinton	84)	
	
      

itera4ve	sampling	scheme	=	
stochas4c	relaxa4on,	
monte-carlo	markov	chain	

       training	requires	sampling:	
might	take	a	lot	of	4me	to	
converge	if	there	are	well-
separated	modes	

restricted id82 
(rbm) 

(smolensky	1986,	hinton		et	al	2006)	

       a	building	block	
(single-layer)	for	
deep	architectures	
	
       biparhte	undirected	

graphical	model	

                                                       

 

 
 

 h ~ p(h|x) 
 

 
     

 h   ~ p(h|x  ) 

 x   ~ p(x | h) 

         x 

                                                        
 
 

hidden 

observed 

block	
gibbs	
sampling	

capturing the shape of the 
distribution: positive & negative 
samples 

boltzmann	machines,	undirected	graphical	models,	
p r(x) =
rbms,	energy-based	models	
z
       observed (+) examples push the energy down 
       generated / dream / fantasy (-) samples / particles push 

the energy up 

e energy(x)

x+ 

x- 

yann 
lecun 
lecun 

eight strategies to shape the energy function 

"     1. build the machine so that the volume of low energy stuff is constant 

"    pca, id116, gmm, square ica 

"     2. push down of the energy of data points, push up everywhere else 

"    max likelihood (needs tractable partition function) 

"     3. push down of the energy of data points, push up on chosen locations 

"     contrastive divergence, ratio matching, noise contrastive estimation, 
minimum id203 flow 

"     4. minimize the gradient and maximize the curvature around data points  

"    score matching 

"     5. train a dynamical system so that the dynamics goes to the manifold 

" denoising auto-encoder, diffusion inversion (nonequilibrium dynamics) 

"     6. use a regularizer that limits the volume of space that has low energy 

"    sparse coding, sparse auto-encoder, psd 

"     7. if e(y) = ||y - g(y)||^2, make g(y) as "constant" as possible. 

"    contracting auto-encoder, saturating auto-encoder 

"     8. adversarial training: generator tries to fool real/synthetic classifier. 

auto-encoders 

p(x|h)	

decoder.g!

p(h)	

q(h|x)	

encoder.f!

x	

109	

reconstruc,on!r!

code!h!

input!x!

itera4ve	sampling	/	undirected	models:	
		rbm,	denoising	auto-encoder	

      
	
       ancestral	sampling	/	directed	models	

	helmholtz	machine,	vae,	etc.	
(hinton	et	al	1995)	

probabilishc	reconstruchon	criterion:	
reconstruc4on	log-likelihood	=	
	-	log	p(x	|	h)	

denoising	auto-encoder:	
during	training,	input	is	corrupted	
stochas4cally,	and	auto-encoder	must	
learn	to	guess	the	distribu4on	of	the	
missing	informa4on.	

predictive sparse decomposition (psd) 

yann 
lecun 
lecun 

[kavukcuoglu, ranzato, lecun, rejected by every conference, 2008-2009] 
"    train a    simple    feed-forward function to predict the result of a complex 
optimization on the data points of interest 

generative model 
factor a 

distance 

decoder 

factor b 

input 

y 

fast feed-forward model 

factor a' 

encoder 

distance 

z 

latent 
variable 

1. find optimal zi for all yi; 
2. train encoder to predict 
zi from yi 

energy  = reconstruction_error + code_prediction_error + code_sparsity 

probabilistic interpretation of auto-
encoders 
       manifold	&	probabilis4c	interpreta4ons	of	auto-encoders	
       denoising	score	matching	as	induc4ve	principle	

       es4ma4ng	the	gradient	of	the	energy	func4on	

(vincent	2011)	

       sampling	via	markov	chain	

(alain	&	bengio	iclr	2013)	

       varia4onal	auto-encoders	

(bengio	et	al	nips	2013;	sohl-dickstein	et	al	icml	2015)	

(kingma	&	welling	iclr	2014)	
(gregor	et	al	arxiv	2015)	

111	

denoising auto-encoder 
       learns	a	vector	   eld	poin4ng	towards	higher	

id203	direc4on	(alain	&	bengio	2013)	
reconstruction(x)   x !  2 @ log p(x)
       some	daes	correspond	to	a	kind	of	gaussian	
rbm	with	regularized	score	matching	(vincent	
2011)	

@x

					[equivalent	when	noise  0]	

corrupted input 

prior:	examples	
concentrate	near	a	
lower	dimensional	
   manifold   		

corrupted input 

regularized auto-encoders learn a 
vector field that estimates a 
gradient field  (alain	&	bengio	iclr	2013)	

113	

denoising auto-encoder markov chain 

corrupt	

~	
xt	

denoise	

~	
xt+1	

~	
xt+2	

xt	

xt+1	

xt+2	

the	corrupt-encode-decode-sample	markov	chain	associated	with	a	dae	
samples	from	a	consistent	es4mator	of	the	data	genera4ng	distribu4on	

114	

preference for locally constant features 
       denoising	or	contrac4ve	auto-encoder	on	1-d	input:	

@e(x)

r(x)"

    x  

@x

e(x)

x1"

x"
e[||r(x +  z)   x||2]     e[||r(x)   x||2] +  2||

x3"

x2"

@r(x)
@x ||2

f

115	

helmholtz machines (hinton	et	al	1995)		and 
variational auto-encoders (vaes) 

(kingma	&	welling	2013,	iclr	2014)	
(gregor	et	al	icml	2014;	rezende	et	al	icml	2014)	
(mnih	&	gregor	icml	2014;	kingma	et	al,	nips	2014)	

	

e
c
n
e
r
e
f
n

i
	

=
	
r
e
d
o
c
n
e

q(h3|h2)

q(h2|h1)

q(h1|x)

	
       parametric	approximate	

id136	

       successors	of	helmholtz	
machine	(hinton	et	al	   95)	

       maximize	varia4onal	lower	

bound	on	log-likelihood:	

	
min kl(q(x, h)||p (x, h))
where														=	data	distr.		
or	equivalently	

q(x)

maxxx

116	

q(h|x) log

p (x, h)
q(h|x)

= maxxx

h3

h2

h1

p (h3)

p (h2|h3)

p (h1|h2)

p (x|h1)

	
r
o
t
a
r
e
n
e
g
=
	
r
e
d
o
c
e
d

	

x

q(x)

q(h|x) log p (x|h) + kl(q(h|x)||p (h))

geometric interpretation 
       encoder:	map	input	to	a	new	space	

where	the	data	has	a	simpler	
distribu4on	

       add	noise	between	encoder	output	

and	decoder	input:	train	the	
decoder	to	be	robust	to	mismatch	
between	encoder	output	and	prior	
output.	

q(h|x)	

f(x)	

p(h)	

contrac4ve	

f	

g	

x	

117	

draw: sequential variational auto-
encoder with attention 

draw: a recurrent neural network for image generation

(gregor	et	al	of	google	deepmind,	arxiv	1502.04623,	2015)		

karolg@google.com
danihelka@google.com
gravesa@google.com
wierstra@google.com

       even	for	a	sta4c	input,	the	encoder	and	decoder	are	now	

recurrent	nets,	which	gradually	add	elements	to	the	answer,	
and	use	an	awen4on	mechanism	to	choose	where	to	do	so.	

draw: a recurrent neural network for image generation

quence of partial glimpses, or foveations, than by a sin-
gle sweep through the entire image (larochelle & hinton,
2010; denil et al., 2012; tang et al., 2013; ranzato, 2014;
zheng et al., 2014; mnih et al., 2014; ba et al., 2014; ser-
manet et al., 2014). the main challenge faced by sequential
id12 is learning where to look, which can be
addressed with id23 techniques such as
policy gradients (mnih et al., 2014). the attention model in
draw, however, is fully differentiable, making it possible
to train with standard id26. in this sense it re-
sembles the selective read and write operations developed
for the id63 (graves et al., 2014).
the following section de   nes the draw architecture,
along with the id168 used for training and the pro-
cedure for image generation. section 3 presents the selec-

118	

figure 1. a trained draw network generating mnist dig-
its. each row shows successive stages in the generation of a sin-
gle digit. note how the lines composing the digits appear to be

p (x|z)
decoder
fnn
z

sample

encoder
fnn

ct 1
hdec
t 1

henc
t 1

write

ct

write

. . .

ct  

decoder
id56

sample

encoder
id56

read

decoder
id56

sample
q(zt+1|x, z1:t)
encoder
id56

read

decoding
(generative model)
encoding
(id136)

figure 2. left: conventional variational auto-encoder. dur-
ing generation, a sample z is drawn from a prior p (z) and passed
through the feedforward decoder network to compute the proba-

timexztzt+1p(x|z1:t)xq(zt|x,z1:t 1)xq(z|x)draw: a recurrent neural network for image generation

draw samples of svhn images: 
generated samples vs training nearest 
neighbor 

nearest	training	
example	for	last	
column	of	samples	

119	

gan: id3 
adversarial nets framework 

goodfellow	et	al	nips	2014	

random	
vector	

generator	
network	

fake	
image	

discriminator	

network	

random	
index	

training	

set	

real	
image	

1
2
0

lapgan: laplacian pyramid of 
id3 
laplacian pyramid 

(denton	+	chintala,	et	al	2015)	

http://soumith.ch/eyescream/	
	

1
2
1

lapgan: visual turing test 
lapgan results 

40%	of	samples	mistaken	by	humans	for	real	photos	

      

(denton + chintala, et al 2015)	

sharper	images	than	max.	lik.	proxys	(which	min.	kl(data|model)):		

      
       gan	objec4ve	=	compromise	between	kl(data|model)	and	kl(model|data)	

122	

convolutional gans 

figure 2: generated bedrooms after one training pass through the dataset. theoretically, the model
could learn to memorize training examples, but this is experimentally unlikely as we train with a
small learning rate and minibatch sgd. we are aware of no prior empirical evidence demonstrating
memorization with sgd and a small learning rate in only one epoch.
strided	convolu4ons,	batch	normaliza4on,	only	convolu4onal	layers,	
relu	and	leaky	relu	

(radford	et	al,	arxiv		1511.06343)	

123	

space-filling in representation-space 

(bengio	et	al	icml	2013)	

deeper	representahons	"	abstrachons	"	disentangling	
manifolds	are	expanded	and	   abened	

x-space	

h-space	

linear	interpola4on	at	layer	2	

3   s	manifold	

9   s	manifold	

linear	interpola4on	at	layer	1	

linear	interpola4on	in	pixel	space	

under review as a conference paper at iclr 2016

gan: interpolating in latent space 

if	the	model	is	good	(unfolds	the	manifold),	interpola4ng	between	
latent	values	yields	plausible	images.	

125	

figure 4: top rows: interpolation between a series of 9 random points in z show that the space
learned has smooth transitions, with every image in the space plausibly looking like a bedroom. in

supervised and unsupervised in one learning rule?	

"    id82s have all the right properties [hinton 1831] [ok, ok 1983 ;-]	

"    sup & unsup, generative & discriminative in one simple/local learning rule	
"    feedback circuit reconstructs and propagates virtual hidden targets	
"    but they don't really work (or at least they don't scale).	
"    problem: the feedforward path eliminates information	
"    if the feedforward path is invariant, then	
"    the reconstruction path is a one-to-many mapping	

"    usual solution: sampling. but i'm allergic.	

predicted what	

many	
to 	
one	

what	

one	
to	

many	

predicted what	

cost	

many	
to 	
one	

what	

one	
to	

many	

input	

cost	

reconstruction	

input	

cost	

reconstruction	

deep semi-supervised learning 
       unlike	unsupervised	pre-training,	modern	approaches	op4mize	

jointly	the	supervised	and	unsupervised	objec4ve	

       discrimina4ve	rbms	(larochelle	&	bengio,	icml	2008)	

       semi-supervised	vae	(kingma	et	al,	nips	2014)	

       ladder	network	(rasmus	et	al,	nips	2015)	

127	

  h(0)     z(0)   x(n) + noise
  h(0)     z(0)   x(n) + noise
for l = l to 0 do
for l = l to 0 do
for l = 1 to l do
for l = 1 to l do
semisupervised learning with ladder 
if l = l then
  z(l)
pre   w(l)   h(l 1)
if l = l then
  z(l)
pre   w(l)   h(l 1)
network 
u(l)   batchnorm(  h(l))
u(l)   batchnorm(  h(l))
    (l)   batchmean(  z(l)
    (l)   batchmean(  z(l)
pre)
pre)
else
else
   (l)   batchstd(  z(l)
   (l)   batchstd(  z(l)
pre)
(rasmus	et	al,	nips	2015)		
pre)
u(l)   batchnorm(v(l)  z(l+1))
u(l)   batchnorm(v(l)  z(l+1))
  z(l)   batchnorm(  z(l)
end if
  z(l)   batchnorm(  z(l)
pre) + noise
end if
pre) + noise
      
, u(l)
8i :   z(l)
i   g(  z(l)
i ) # eq. (1)
  h(l)   activation( (l)   (  z(l) +  (l)))
i   g(  z(l)
, u(l)
8i :   z(l)
i ) # eq. (1)
jointly	trained	stack	of	denoising	auto-encoders	with	gated	
  h(l)   activation( (l)   (  z(l) +  (l)))
i
i
end for
i      (l)
i,bn     z(l)
end for
8i :   z(l)
i      (l)
i,bn     z(l)
lateral	connec4ons	and	semi-supervised	objec4ve	
8i :   z(l)
p (  y | x)     h(l)
   (l)
p (  y | x)     h(l)
   (l)
i
end for
i
# clean encoder (for denoising targets)
end for
# clean encoder (for denoising targets)
# cost function c for training:
h(0)   z(0)   x(n)
# cost function c for training:
h(0)   z(0)   x(n)
c   0
for l = 1 to l do
c   0
for l = 1 to l do
if t(n) then
z(2)
z(l)   batchnorm(w(l)h(l 1))
if t(n) then
z(l)   batchnorm(w(l)h(l 1))
c     log p (  y = t(n) | x)
n (0,  2)
h(l)   activation( (l)   (z(l) +  (l)))
c     log p (  y = t(n) | x)
h(l)   activation( (l)   (z(l) +  (l)))
end if
end for
end if
f (2)(  )
end for
z(1)

semi-supervised	objec4ve:	
		

g(2)(  ,  )

f (2)(  )

c(2)

  z(2)

  z(2)

2
2

y

  y

i
i

d

  z(1)

  z(1)

g(1)(  ,  )

c(1)

d

c   c +pl
c   c +pl

l=1  l   z(l)     z(l)
bn   
l=1  l   z(l)     z(l)
bn   

they	also	use	
batch	normaliza4on	

# eq. (2)
# eq. (2)

n (0,  2)

n (0,  2)

	

f (1)(  )

x

f (1)(  )

  x

g(0)(  ,  )

  x

c(0)

d

x

4
4

x

1%	error	on	pi-mnist	with	100	labeled	examples	(pezeshki	et	al	arxiv	1511.06430)	
128	

figure 2: a conceptual illustration of the ladder network when l = 2. the feedforward path
x ! z(1) ! z(2) ! y) shares the mappings f (l) with the corrupted feedforward path, or encoder
(x !   z(1) !   z(2) !   y). the decoder (  z(l) !   z(l) !   x) consists of denoising functions g(l) and
has costs functions c(l)
d on each layer trying to minimize the difference between   z(l) and z(l). the

stacked what-where 
auto-encoder (swwae) 

predicted 
output 

loss 

[zhao, mathieu, lecun arxiv:1506.02351] 
stacked what-where auto-encoder 
 

desired 
output 
yann 
lecun 
lecun 

a bit like a convnet paired with a deconvnet 

inpu
t 

recons
- 
tructio
n 

conclusions & challenges 

130	

learning     how the world ticks     
       so	long	as	our	machine	learning	models	  	cheat	  	by	relying	only	
on	surface	sta4s4cal	regulari4es,	they	remain	vulnerable	to	out-
of-distribu4on	examples	

       humans	generalize	bewer	than	other	animals	by	implicitly	having	

a	more	accurate	internal	model	of	the	underlying	causal	
rela4onships	

       this	allows	one	to	predict	future	situa4ons	(e.g.,	the	e   ect	of	
planned	ac4ons)	that	are	far	from	anything	seen	before,	an	
essen4al	component	of	reasoning,	intelligence	and	science	

131	

learning multiple levels of 
abstraction 
       the	big	payo   	of	deep	learning	is	to	allow	learning	

higher	levels	of	abstrac4on	

       higher-level	abstrac4ons	disentangle	the	factors	of	

varia4on,	which	allows	much	easier	generaliza4on	and	
transfer	

132	

challenges & open problems 

a	more	scienh   c	approach	is	needed,	not	just	building	beber	systems	

       unsupervised	learning	
       how	to	evaluate?	

       long-term	dependencies		
       natural	language	understanding	&	reasoning	
       more	robust	op4miza4on	(or	easier	to	train	architectures)	
       distributed	training	(that	scales)	&	specialized	hardware	
       bridging	the	gap	to	biology	
       deep	reinforcement	learning	

133	

