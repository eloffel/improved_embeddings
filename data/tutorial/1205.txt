1

learning deep architectures for ai

yoshua bengio

dept. iro, universit  e de montr  eal

c.p. 6128, montreal, qc, h3c 3j7, canada

yoshua.bengio@umontreal.ca

http://www.iro.umontreal.ca/   bengioy

to appear in foundations and trends in machine learning

abstract

theoretical results suggest that in order to learn the kind of complicated functions that can represent high-
level abstractions (e.g. in vision, language, and other ai-level tasks), one may need deep architectures.
deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with
many hidden layers or in complicated propositional formulae re-using many sub-formulae. searching the
parameter space of deep architectures is a dif   cult task, but learning algorithms such as those for deep
belief networks have recently been proposed to tackle this problem with notable success, beating the
state-of-the-art in certain areas. this paper discusses the motivations and principles regarding learning
algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning
of single-layer models such as restricted id82s, used to construct deeper models such as
id50.

1

introduction

allowing computers to model our world well enough to exhibit what we call intelligence has been the focus
of more than half a century of research. to achieve this, it is clear that a large quantity of information
about our world should somehow be stored, explicitly or implicitly, in the computer. because it seems
daunting to formalize manually all that information in a form that computers can use to answer questions
and generalize to new contexts, many researchers have turned to learning algorithms to capture a large
fraction of that information. much progress has been made to understand and improve learning algorithms,
but the challenge of arti   cial intelligence (ai) remains. do we have algorithms that can understand scenes
and describe them in natural language? not really, except in very limited settings. do we have algorithms
that can infer enough semantic concepts to be able to interact with most humans using these concepts? no.
if we consider image understanding, one of the best speci   ed of the ai tasks, we realize that we do not yet
have learning algorithms that can discover the many visual and semantic concepts that would seem to be
necessary to interpret most images on the web. the situation is similar for other ai tasks.

consider for example the task of interpreting an input image such as the one in figure 1. when humans
try to solve a particular ai task (such as machine vision or natural language processing), they often exploit
their intuition about how to decompose the problem into sub-problems and multiple levels of representation,
e.g., in object parts and constellation models (weber, welling, & perona, 2000; niebles & fei-fei, 2007;
sudderth, torralba, freeman, & willsky, 2007) where models for parts can be re-used in different object in-
stances. for example, the current state-of-the-art in machine vision involves a sequence of modules starting
from pixels and ending in a linear or kernel classi   er (pinto, dicarlo, & cox, 2008; mutch & lowe, 2008),
with intermediate modules mixing engineered transformations and learning, e.g.    rst extracting low-level

features that are invariant to small geometric variations (such as edge detectors from gabor    lters), trans-
forming them gradually (e.g. to make them invariant to contrast changes and contrast inversion, sometimes
by pooling and sub-sampling), and then detecting the most frequent patterns. a plausible and common way
to extract useful information from a natural image involves transforming the raw pixel representation into
gradually more abstract representations, e.g., starting from the presence of edges, the detection of more com-
plex but local shapes, up to the identi   cation of abstract categories associated with sub-objects and objects
which are parts of the image, and putting all these together to capture enough understanding of the scene to
answer questions about it.

here, we assume that the computational machinery necessary to express complex behaviors (which one
might label    intelligent   ) requires highly varying mathematical functions, i.e. mathematical functions that
are highly non-linear in terms of raw sensory inputs, and display a very large number of variations (ups and
downs) across the domain of interest. we view the raw input to the learning system as a high dimensional
entity, made of many observed variables, which are related by unknown intricate statistical relationships. for
example, using knowledge of the 3d geometry of solid objects and lighting, we can relate small variations in
underlying physical and geometric factors (such as position, orientation, lighting of an object) with changes
in pixel intensities for all the pixels in an image. we call these factors of variation because they are different
aspects of the data that can vary separately and often independently. in this case, explicit knowledge of
the physical factors involved allows one to get a picture of the mathematical form of these dependencies,
and of the shape of the set of images (as points in a high-dimensional space of pixel intensities) associated
with the same 3d object. if a machine captured the factors that explain the statistical variations in the data,
and how they interact to generate the kind of data we observe, we would be able to say that the machine
understands those aspects of the world covered by these factors of variation. unfortunately, in general and
for most factors of variation underlying natural images, we do not have an analytical understanding of these
factors of variation. we do not have enough formalized prior knowledge about the world to explain the
observed variety of images, even for such an apparently simple abstraction as man, illustrated in figure 1.
a high-level abstraction such as man has the property that it corresponds to a very large set of possible
images, which might be very different from each other from the point of view of simple euclidean distance
in the space of pixel intensities. the set of images for which that label could be appropriate forms a highly
convoluted region in pixel space that is not even necessarily a connected region. the man category can be
seen as a high-level abstraction with respect to the space of images. what we call abstraction here can be a
category (such as the man category) or a feature, a function of sensory data, which can be discrete (e.g., the
input sentence is at the past tense) or continuous (e.g., the input video shows an object moving at
2 meter/second). many lower-level and intermediate-level concepts (which we also call abstractions here)
would be useful to construct a man-detector. lower level abstractions are more directly tied to particular
percepts, whereas higher level ones are what we call    more abstract    because their connection to actual
percepts is more remote, and through other, intermediate-level abstractions.

in addition to the dif   culty of coming up with the appropriate intermediate abstractions, the number of
visual and semantic categories (such as man) that we would like an    intelligent    machine to capture is
rather large. the focus of deep architecture learning is to automatically discover such abstractions, from the
lowest level features to the highest level concepts. ideally, we would like learning algorithms that enable
this discovery with as little human effort as possible, i.e., without having to manually de   ne all necessary
abstractions or having to provide a huge set of relevant hand-labeled examples. if these algorithms could
tap into the huge resource of text and images on the web, it would certainly help to transfer much of human
knowledge into machine-interpretable form.

1.1 how do we train deep architectures?

deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy
formed by the composition of lower level features. automatically learning features at multiple levels of
abstraction allows a system to learn complex functions mapping the input to the output directly from data,

2

figure 1: we would like the raw input image to be transformed into gradually higher levels of representation,
representing more and more abstract functions of the raw input, e.g., edges, local shapes, object parts,
etc. in practice, we do not know in advance what the    right    representation should be for all these levels
of abstractions, although linguistic concepts might help guessing what the higher levels should implicitly
represent.

3

without depending completely on human-crafted features. this is especially important for higher-level ab-
stractions, which humans often do not know how to specify explicitly in terms of raw sensory input. the
ability to automatically learn powerful features will become increasingly important as the amount of data
and range of applications to machine learning methods continues to grow.

depth of architecture refers to the number of levels of composition of non-linear operations in the func-
tion learned. whereas most current learning algorithms correspond to shallow architectures (1, 2 or 3 levels),
the mammal brain is organized in a deep architecture (serre, kreiman, kouh, cadieu, knoblich, & poggio,
2007) with a given input percept represented at multiple levels of abstraction, each level corresponding to
a different area of cortex. humans often describe such concepts in hierarchical ways, with multiple levels
of abstraction. the brain also appears to process information through multiple stages of transformation and
representation. this is particularly clear in the primate visual system (serre et al., 2007), with its sequence
of processing stages: detection of edges, primitive shapes, and moving up to gradually more complex visual
shapes.

inspired by the architectural depth of the brain, neural network researchers had wanted for decades to
train deep multi-layer neural networks (utgoff & stracuzzi, 2002; bengio & lecun, 2007), but no success-
ful attempts were reported before 20061: researchers reported positive experimental results with typically
two or three levels (i.e. one or two hidden layers), but training deeper networks consistently yielded poorer
results. something that can be considered a breakthrough happened in 2006: hinton and collaborators at
u. of toronto introduced id50 or dbns for short (hinton, osindero, & teh, 2006), with
a learning algorithm that greedily trains one layer at a time, exploiting an unsupervised learning algorithm
for each layer, a restricted id82 (rbm) (freund & haussler, 1994). shortly after, related
algorithms based on auto-encoders were proposed (bengio, lamblin, popovici, & larochelle, 2007; ran-
zato, poultney, chopra, & lecun, 2007), apparently exploiting the same principle: guiding the training of
intermediate levels of representation using unsupervised learning, which can be performed locally at each
level. other algorithms for deep architectures were proposed more recently that exploit neither rbms nor
auto-encoders and that exploit the same principle (weston, ratle, & collobert, 2008; mobahi, collobert, &
weston, 2009) (see section 4).

since 2006, deep networks have been applied with success not only in classi   cation tasks (bengio et al.,
2007; ranzato et al., 2007; larochelle, erhan, courville, bergstra, & bengio, 2007; ranzato, boureau, &
lecun, 2008; vincent, larochelle, bengio, & manzagol, 2008; ahmed, yu, xu, gong, & xing, 2008; lee,
grosse, ranganath, & ng, 2009), but also in regression (salakhutdinov & hinton, 2008), dimensionality re-
duction (hinton & salakhutdinov, 2006a; salakhutdinov & hinton, 2007a), modeling textures (osindero &
hinton, 2008), modeling motion (taylor, hinton, & roweis, 2007; taylor & hinton, 2009), object segmen-
tation (levner, 2008), information retrieval (salakhutdinov & hinton, 2007b; ranzato & szummer, 2008;
torralba, fergus, & weiss, 2008), robotics (hadsell, erkan, sermanet, scof   er, muller, & lecun, 2008),
natural language processing (collobert & weston, 2008; weston et al., 2008; mnih & hinton, 2009), and
collaborative    ltering (salakhutdinov, mnih, & hinton, 2007). although auto-encoders, rbms and dbns
can be trained with unlabeled data, in many of the above applications, they have been successfully used to
initialize deep supervised feedforward neural networks applied to a speci   c task.

1.2

intermediate representations: sharing features and abstractions across tasks

since a deep architecture can be seen as the composition of a series of processing stages, the immediate
question that deep architectures raise is: what kind of representation of the data should be found as the out-
put of each stage (i.e., the input of another)? what kind of interface should there be between these stages? a
hallmark of recent research on deep architectures is the focus on these intermediate representations: the suc-
cess of deep architectures belongs to the representations learned in an unsupervised way by rbms (hinton
et al., 2006), ordinary auto-encoders (bengio et al., 2007), sparse auto-encoders (ranzato et al., 2007, 2008),
or denoising auto-encoders (vincent et al., 2008). these algorithms (described in more detail in section 7.2)

1except for neural networks with a special structure called convolutional networks, discussed in section 4.5.

4

can be seen as learning to transform one representation (the output of the previous stage) into another, at
each step maybe disentangling better the factors of variations underlying the data. as we discuss at length
in section 4, it has been observed again and again that once a good representation has been found at each
level, it can be used to initialize and successfully train a deep neural network by supervised gradient-based
optimization.

each level of abstraction found in the brain consists of the    activation    (neural excitation) of a small
subset of a large number of features that are, in general, not mutually exclusive. because these features
are not mutually exclusive, they form what is called a distributed representation (hinton, 1986; rumelhart,
hinton, & williams, 1986b): the information is not localized in a particular neuron but distributed across
many. in addition to being distributed, it appears that the brain uses a representation that is sparse: only
around 1-4% of the neurons are active together at a given time (attwell & laughlin, 2001; lennie, 2003).
section 3.2 introduces the notion of sparse distributed representation and 7.1 describes in more detail the
machine learning approaches, some inspired by the observations of the sparse representations in the brain,
that have been used to build deep architectures with sparse representations.

whereas dense distributed representations are one extreme of a spectrum, and sparse representations are
in the middle of that spectrum, purely local representations are the other extreme. locality of representation
is intimately connected with the notion of local generalization. many existing machine learning methods are
local in input space: to obtain a learned function that behaves differently in different regions of data-space,
they require different tunable parameters for each of these regions (see more in section 3.1). even though
statistical ef   ciency is not necessarily poor when the number of tunable parameters is large, good general-
ization can be obtained only when adding some form of prior (e.g. that smaller values of the parameters are
preferred). when that prior is not task-speci   c, it is often one that forces the solution to be very smooth, as
discussed at the end of section 3.1. in contrast to learning methods based on local generalization, the total
number of patterns that can be distinguished using a distributed representation scales possibly exponentially
with the dimension of the representation (i.e. the number of learned features).

in many machine vision systems, learning algorithms have been limited to speci   c parts of such a pro-
cessing chain. the rest of the design remains labor-intensive, which might limit the scale of such systems.
on the other hand, a hallmark of what we would consider intelligent machines includes a large enough reper-
toire of concepts. recognizing man is not enough. we need algorithms that can tackle a very large set of
such tasks and concepts. it seems daunting to manually de   ne that many tasks, and learning becomes essen-
tial in this context. furthermore, it would seem foolish not to exploit the underlying commonalities between
these tasks and between the concepts they require. this has been the focus of research on multi-task learn-
ing (caruana, 1993; baxter, 1995; intrator & edelman, 1996; thrun, 1996; baxter, 1997). architectures
with multiple levels naturally provide such sharing and re-use of components: the low-level visual features
(like edge detectors) and intermediate-level visual features (like object parts) that are useful to detect man
are also useful for a large group of other visual tasks. deep learning algorithms are based on learning inter-
mediate representations which can be shared across tasks. hence they can leverage unsupervised data and
data from similar tasks (raina, battle, lee, packer, & ng, 2007) to boost performance on large and chal-
lenging problems that routinely suffer from a poverty of labelled data, as has been shown by collobert and
weston (2008), beating the state-of-the-art in several natural language processing tasks. a similar multi-task
approach for deep architectures was applied in vision tasks by ahmed et al. (2008). consider a multi-task
setting in which there are different outputs for different tasks, all obtained from a shared pool of high-level
features. the fact that many of these learned features are shared among m tasks provides sharing of sta-
tistical strength in proportion to m. now consider that these learned high-level features can themselves be
represented by combining lower-level intermediate features from a common pool. again statistical strength
can be gained in a similar way, and this strategy can be exploited for every level of a deep architecture.

in addition, learning about a large set of interrelated concepts might provide a key to the kind of broad
generalizations that humans appear able to do, which we would not expect from separately trained object
detectors, with one detector per visual category. if each high-level category is itself represented through
a particular distributed con   guration of abstract features from a common pool, generalization to unseen

5

categories could follow naturally from new con   gurations of these features. even though only some con   g-
urations of these features would be present in the training examples, if they represent different aspects of the
data, new examples could meaningfully be represented by new con   gurations of these features.

1.3 desiderata for learning ai

summarizing some of the above issues, and trying to put them in the broader perspective of ai, we put
forward a number of requirements we believe to be important for learning algorithms to approach ai, many
of which motivate the research described here:

    ability to learn complex, highly-varying functions, i.e., with a number of variations much greater than

the number of training examples.

    ability to learn with little human input the low-level, intermediate, and high-level abstractions that

would be useful to represent the kind of complex functions needed for ai tasks.

    ability to learn from a very large set of examples: computation time for training should scale well

with the number of examples, i.e. close to linearly.

    ability to learn from mostly unlabeled data, i.e. to work in the semi-supervised setting, where not all

the examples come with complete and correct semantic labels.

    ability to exploit the synergies present across a large number of tasks, i.e. id72. these

synergies exist because all the ai tasks provide different views on the same underlying reality.

    strong unsupervised learning (i.e. capturing most of the statistical structure in the observed data),
which seems essential in the limit of a large number of tasks and when future tasks are not known
ahead of time.

other elements are equally important but are not directly connected to the material in this paper. they
include the ability to learn to represent context of varying length and structure (pollack, 1990), so as to
allow machines to operate in a context-dependent stream of observations and produce a stream of actions,
the ability to make decisions when actions in   uence the future observations and future rewards (sutton &
barto, 1998), and the ability to in   uence future observations so as to collect more relevant information about
the world, i.e. a form of active learning (cohn, ghahramani, & jordan, 1995).

1.4 outline of the paper

section 2 reviews theoretical results (which can be skipped without hurting the understanding of the remain-
der) showing that an architecture with insuf   cient depth can require many more computational elements,
potentially exponentially more (with respect to input size), than architectures whose depth is matched to the
task. we claim that insuf   cient depth can be detrimental for learning. indeed, if a solution to the task is
represented with a very large but shallow architecture (with many computational elements), a lot of training
examples might be needed to tune each of these elements and capture a highly-varying function. section 3.1
is also meant to motivate the reader, this time to highlight the limitations of local generalization and local
estimation, which we expect to avoid using deep architectures with a distributed representation (section 3.2).
in later sections, the paper describes and analyzes some of the algorithms that have been proposed to train
deep architectures. section 4 introduces concepts from the neural networks literature relevant to the task of
training deep architectures. we    rst consider the previous dif   culties in training neural networks with many
layers, and then introduce unsupervised learning algorithms that could be exploited to initialize deep neural
networks. many of these algorithms (including those for the rbm) are related to the auto-encoder: a simple
unsupervised algorithm for learning a one-layer model that computes a distributed representation for its
input (rumelhart et al., 1986b; bourlard & kamp, 1988; hinton & zemel, 1994). to fully understand rbms

6

and many related unsupervised learning algorithms, section 5 introduces the class of energy-based models,
including those used to build generative models with hidden variables such as the id82.
section 6 focus on the greedy layer-wise training algorithms for id50 (dbns) (hinton
et al., 2006) and stacked auto-encoders (bengio et al., 2007; ranzato et al., 2007; vincent et al., 2008).
section 7 discusses variants of rbms and auto-encoders that have been recently proposed to extend and
improve them, including the use of sparsity, and the modeling of temporal dependencies. section 8 discusses
algorithms for jointly training all the layers of a deep belief network using variational bounds. finally, we
consider in section 9 forward looking questions such as the hypothesized dif   cult optimization problem
involved in training deep architectures. in particular, we follow up on the hypothesis that part of the success
of current learning strategies for deep architectures is connected to the optimization of lower layers. we
discuss the principle of continuation methods, which minimize gradually less smooth versions of the desired
cost function, to make a dent in the optimization of deep architectures.

2 theoretical advantages of deep architectures

in this section, we present a motivating argument for the study of learning algorithms for deep architectures,
by way of theoretical results revealing potential limitations of architectures with insuf   cient depth. this part
of the paper (this section and the next) motivates the algorithms described in the later sections, and can be
skipped without making the remainder dif   cult to follow.

the main point of this section is that some functions cannot be ef   ciently represented (in terms of number
of tunable elements) by architectures that are too shallow. these results suggest that it would be worthwhile
to explore learning algorithms for deep architectures, which might be able to represent some functions
otherwise not ef   ciently representable. where simpler and shallower architectures fail to ef   ciently represent
(and hence to learn) a task of interest, we can hope for learning algorithms that could set the parameters of a
deep architecture for this task.

we say that the expression of a function is compact when it has few computational elements, i.e. few
degrees of freedom that need to be tuned by learning. so for a    xed number of training examples, and short of
other sources of knowledge injected in the learning algorithm, we would expect that compact representations
of the target function2 would yield better generalization.

more precisely, functions that can be compactly represented by a depth k architecture might require an
exponential number of computational elements to be represented by a depth k     1 architecture. since the
number of computational elements one can afford depends on the number of training examples available to
tune or select them, the consequences are not just computational but also statistical: poor generalization may
be expected when using an insuf   ciently deep architecture for representing some functions.

we consider the case of    xed-dimension inputs, where the computation performed by the machine can
be represented by a directed acyclic graph where each node performs a computation that is the application
of a function on its inputs, each of which is the output of another node in the graph or one of the external
inputs to the graph. the whole graph can be viewed as a circuit that computes a function applied to the
external inputs. when the set of functions allowed for the computation nodes is limited to logic gates, such
as { and, or, not }, this is a boolean circuit, or logic circuit.

to formalize the notion of depth of architecture, one must introduce the notion of a set of computational
elements. an example of such a set is the set of computations that can be performed logic gates. another is
the set of computations that can be performed by an arti   cial neuron (depending on the values of its synaptic
weights). a function can be expressed by the composition of computational elements from a given set. it
is de   ned by a graph which formalizes this composition, with one node per computational element. depth
of architecture refers to the depth of that graph, i.e. the longest path from an input node to an output node.
when the set of computational elements is the set of computations an arti   cial neuron can perform, depth
corresponds to the number of layers in a neural network. let us explore the notion of depth with examples

2the target function is the function that we would like the learner to discover.

7

element 
   set

*
sin
+
   

output
*

sin

+

*

x

a
inputs

b

element 
   set

neuron

neuron
...

neuron

output

neuron

neuron

neuron

neuron neuron neuron

inputs

figure 2: examples of functions represented by a graph of computations, where each node is taken in some
   element set    of allowed computations. left: the elements are {   , +,    , sin}   r. the architecture computes
x   sin(a   x+b) and has depth 4. right: the elements are arti   cial neurons computing f (x) = tanh(b+w   x);
each element in the set has a different (w, b) parameter. the architecture is a multi-layer neural network of
depth 3.

of architectures of different depths. consider the function f (x) = x     sin(a     x + b). it can be expressed
as the composition of simple operations such as addition, subtraction, multiplication, and the sin operation,
as illustrated in figure 2. in the example, there would be a different node for the multiplication a     x and
for the    nal multiplication by x. each node in the graph is associated with an output value obtained by
applying some function on input values that are the outputs of other nodes of the graph. for example, in a
logic circuit each node can compute a boolean function taken from a small set of boolean functions. the
graph as a whole has input nodes and output nodes and computes a function from input to output. the depth
of an architecture is the maximum length of a path from any input of the graph to any output of the graph,
i.e. 4 in the case of x     sin(a     x + b) in figure 2.

    if we include af   ne operations and their possible composition with sigmoids in the set of computa-

tional elements, id75 and id28 have depth 1, i.e., have a single level.

    when we put a    xed kernel computation k(u, v) in the set of allowed operations, along with af   ne
operations, kernel machines (sch  olkopf, burges, & smola, 1999a) with a    xed kernel can be consid-
ered to have two levels. the    rst level has one element computing k(x, xi) for each prototype xi (a
selected representative training example) and matches the input vector x with the prototypes xi. the

second level performs an af   ne combination b +pi   ik(x, xi) to associate the matching prototypes

xi with the expected response.

    when we put arti   cial neurons (af   ne transformation followed by a non-linearity) in our set of el-
ements, we obtain ordinary multi-layer neural networks (rumelhart et al., 1986b). with the most
common choice of one hidden layer, they also have depth two (the hidden layer and the output layer).

    id90 can also be seen as having two levels, as discussed in section 3.1.

    boosting (freund & schapire, 1996) usually adds one level to its base learners: that level computes a

vote or linear combination of the outputs of the base learners.

    stacking (wolpert, 1992) is another meta-learning algorithm that adds one level.

    based on current knowledge of brain anatomy (serre et al., 2007), it appears that the cortex can be

seen as a deep architecture, with 5 to 10 levels just for the visual system.

8

although depth depends on the choice of the set of allowed computations for each element, graphs
associated with one set can often be converted to graphs associated with another by an graph transformation
in a way that multiplies depth. theoretical results suggest that it is not the absolute number of levels that
matters, but the number of levels relative to how many are required to represent ef   ciently the target function
(with some choice of set of computational elements).

2.1 computational complexity

the most formal arguments about the power of deep architectures come from investigations into computa-
tional complexity of circuits. the basic conclusion that these results suggest is that when a function can be
compactly represented by a deep architecture, it might need a very large architecture to be represented by
an insuf   ciently deep one.

a two-layer circuit of logic gates can represent any boolean function (mendelson, 1997). any boolean
function can be written as a sum of products (disjunctive normal form: and gates on the    rst layer with
optional negation of inputs, and or gate on the second layer) or a product of sums (conjunctive normal
form: or gates on the    rst layer with optional negation of inputs, and and gate on the second layer).
to understand the limitations of shallow architectures, the    rst result to consider is that with depth-two
logical circuits, most boolean functions require an exponential (with respect to input size) number of logic
gates (wegener, 1987) to be represented.

more interestingly, there are functions computable with a polynomial-size logic gates circuit of depth k
that require exponential size when restricted to depth k     1 (h  astad, 1986). the proof of this theorem relies
on earlier results (yao, 1985) showing that d-bit parity circuits of depth 2 have exponential size. the d-bit
parity function is de   ned as usual:

parity : (b1, . . . , bd)     {0, 1}d 7   (cid:26) 1 if pd

0 otherwise.

i=1 bi is even

one might wonder whether these computational complexity results for boolean circuits are relevant to
machine learning. see orponen (1994) for an early survey of theoretical results in computational complexity
relevant to learning algorithms. interestingly, many of the results for boolean circuits can be generalized to
architectures whose computational elements are linear threshold units (also known as arti   cial neurons (mc-
culloch & pitts, 1943)), which compute

f (x) = 1w    x+b   0

(1)

with parameters w and b. the fan-in of a circuit is the maximum number of inputs of a particular element.
circuits are often organized in layers, like multi-layer neural networks, where elements in a layer only take
their input from elements in the previous layer(s), and the    rst layer is the neural network input. the size of
a circuit is the number of its computational elements (excluding input elements, which do not perform any
computation).

of particular interest is the following theorem, which applies to monotone weighted threshold circuits
(i.e. multi-layer neural networks with linear threshold units and positive weights) when trying to represent a
function compactly representable with a depth k circuit:

theorem 2.1. a monotone weighted threshold circuit of depth k     1 computing a function fk     fk,n has
size at least 2cn for some constant c > 0 and n > n0 (h  astad & goldmann, 1991).

the class of functions fk,n is de   ned as follows. it contains functions with n 2k   2 inputs, de   ned by a
depth k circuit that is a tree. at the leaves of the tree there are unnegated input variables, and the function
value is at the root. the i-th level from the bottom consists of and gates when i is even and or gates when
i is odd. the fan-in at the top and bottom level is n and at all other levels it is n 2.

the above results do not prove that other classes of functions (such as those we want to learn to perform
ai tasks) require deep architectures, nor that these demonstrated limitations apply to other types of circuits.

9

however, these theoretical results beg the question: are the depth 1, 2 and 3 architectures (typically found
in most machine learning algorithms) too shallow to represent ef   ciently more complicated functions of the
kind needed for ai tasks? results such as the above theorem also suggest that there might be no universally
right depth: each function (i.e. each task) might require a particular minimum depth (for a given set of
computational elements). we should therefore strive to develop learning algorithms that use the data to
determine the depth of the    nal architecture. note also that recursive computation de   nes a computation
graph whose depth increases linearly with the number of iterations.

(x1x2)(x2x3) + (x1x2)(x3x4) + (x2x3)2 + (x2x3)(x3x4)

  

(x1x2) + (x2x3)

+

(x2x3) + (x3x4)
+

x1x2

x2x3

  
  

  

x3x4
  

x1

x2

x3

x4

figure 3: example of polynomial circuit (with products on odd layers and sums on even ones) illustrating
the factorization enjoyed by a deep architecture. for example the level-1 product x2x3 would occur many
times (exponential in depth) in a depth 2 (sum of product) expansion of the above polynomial.

2.2

informal arguments

depth of architecture is connected to the notion of highly-varying functions. we argue that, in general, deep
architectures can compactly represent highly-varying functions which would otherwise require a very large
size to be represented with an inappropriate architecture. we say that a function is highly-varying when
a piecewise approximation (e.g., piecewise-constant or piecewise-linear) of that function would require a
large number of pieces. a deep architecture is a composition of many operations, and it could in any case
be represented by a possibly very large depth-2 architecture. the composition of computational units in
a small but deep circuit can actually be seen as an ef   cient    factorization    of a large but shallow circuit.
reorganizing the way in which computational units are composed can have a drastic effect on the ef   ciency
of representation size. for example, imagine a depth 2k representation of polynomials where odd layers
implement products and even layers implement sums. this architecture can be seen as a particularly ef   cient
factorization, which when expanded into a depth 2 architecture such as a sum of products, might require a
huge number of terms in the sum: consider a level 1 product (like x2x3 in figure 3) from the depth 2k
architecture. it could occur many times as a factor in many terms of the depth 2 architecture. one can see
in this example that deep architectures can be advantageous if some computations (e.g. at one level) can
be shared (when considering the expanded depth 2 expression): in that case, the overall expression to be
represented can be factored out, i.e., represented more compactly with a deep architecture.

further examples suggesting greater expressive power of deep architectures and their potential for ai
and machine learning are also discussed by bengio and lecun (2007). an earlier discussion of the ex-
pected advantages of deeper architectures in a more cognitive perspective is found in utgoff and stracuzzi
(2002). note that connectionist cognitive psychologists have been studying for long time the idea of neu-
ral computation organized with a hierarchy of levels of representation corresponding to different levels of

10

abstraction, with a distributed representation at each level (mcclelland & rumelhart, 1981; hinton & an-
derson, 1981; rumelhart, mcclelland, & the pdp research group, 1986a; mcclelland, rumelhart, & the
pdp research group, 1986; hinton, 1986; mcclelland & rumelhart, 1988). the modern deep architecture
approaches discussed here owe a lot to these early developments. these concepts were introduced in cogni-
tive psychology (and then in computer science / ai) in order to explain phenomena that were not as naturally
captured by earlier cognitive models, and also to connect the cognitive explanation with the computational
characteristics of the neural substrate.

to conclude, a number of computational complexity results strongly suggest that functions that can be
compactly represented with a depth k architecture could require a very large number of elements in order to
be represented by a shallower architecture. since each element of the architecture might have to be selected,
i.e., learned, using examples, these results suggest that depth of architecture can be very important from
the point of view of statistical ef   ciency. this notion is developed further in the next section, discussing a
related weakness of many shallow architectures associated with non-parametric learning algorithms: locality
in input space of the estimator.

3 local vs non-local generalization

3.1 the limits of matching local templates

how can a learning algorithm compactly represent a    complicated    function of the input, i.e., one that has
many more variations than the number of available training examples? this question is both connected to the
depth question and to the question of locality of estimators. we argue that local estimators are inappropriate
to learn highly-varying functions, even though they can potentially be represented ef   ciently with deep
architectures. an estimator that is local in input space obtains good generalization for a new input x by
mostly exploiting training examples in the neighborhood of x. for example, the k nearest neighbors of
the test point x, among the training examples, vote for the prediction at x. local estimators implicitly or
explicitly partition the input space in regions (possibly in a soft rather than hard way) and require different
parameters or degrees of freedom to account for the possible shape of the target function in each of the
regions. when many regions are necessary because the function is highly varying, the number of required
parameters will also be large, and thus the number of examples needed to achieve good generalization.

the local generalization issue is directly connected to the literature on the curse of dimensionality, but
the results we cite show that what matters for generalization is not dimensionality, but instead the number
of    variations    of the function we wish to obtain after learning. for example, if the function represented
by the model is piecewise-constant (e.g. id90), then the question that matters is the number of
pieces required to approximate properly the target function. there are connections between the number of
variations and the input dimension: one can readily design families of target functions for which the number
of variations is exponential in the input dimension, such as the parity function with d inputs.

architectures based on matching local templates can be thought of as having two levels. the    rst level
is made of a set of templates which can be matched to the input. a template unit will output a value that
indicates the degree of matching. the second level combines these values, typically with a simple linear
combination (an or-like operation), in order to estimate the desired output. one can think of this linear
combination as performing a kind of interpolation in order to produce an answer in the region of input space
that is between the templates.

the prototypical example of architectures based on matching local templates is the kernel ma-

chine (sch  olkopf et al., 1999a)

  ik(x, xi),

(2)

where b and   i form the second level, while on the    rst level, the id81 k(x, xi) matches the
input x to the training example xi (the sum runs over some or all of the input patterns in the training set).

f (x) = b +xi

11

in the above equation, f (x) could be for example the discriminant function of a classi   er, or the output of a
regression predictor.

a kernel is local when k(x, xi) >    is true only for x in some connected region around xi (for some
threshold   ). the size of that region can usually be controlled by a hyper-parameter of the id81.
an example of local kernel is the gaussian kernel k(x, xi) = e   ||x   xi||2/  2, where    controls the size of
the region around xi. we can see the gaussian kernel as computing a soft conjunction, because it can be

written as a product of one-dimensional conditions: k(u, v) =qj e   (uj   vj )2/  2. if |uj     vj|/   is small

for all dimensions j, then the pattern matches and k(u, v) is large. if |uj     vj |/   is large for a single j,
then there is no match and k(u, v) is small.

well-known examples of kernel machines include support vector machines (id166s) (boser, guyon, &
vapnik, 1992; cortes & vapnik, 1995) and gaussian processes (williams & rasmussen, 1996) 3 for classi   -
cation and regression, but also classical non-parametric learning algorithms for classi   cation, regression and
density estimation, such as the k-nearest neighbor algorithm, nadaraya-watson or parzen windows density
and regression estimators, etc. below, we discuss manifold learning algorithms such as isomap and lle that
can also be seen as local kernel machines, as well as related semi-supervised learning algorithms also based
on the construction of a neighborhood graph (with one node per example and arcs between neighboring
examples).

kernel machines with a local kernel yield generalization by exploiting what could be called the smooth-
ness prior: the assumption that the target function is smooth or can be well approximated with a smooth
function. for example, in supervised learning, if we have the training example (xi, yi), then it makes sense
to construct a predictor f (x) which will output something close to yi when x is close to xi. note how this
prior requires de   ning a notion of proximity in input space. this is a useful prior, but one of the claims
made in bengio, delalleau, and le roux (2006) and bengio and lecun (2007) is that such a prior is often
insuf   cient to generalize when the target function is highly-varying in input space.

the limitations of a    xed generic kernel such as the gaussian kernel have motivated a lot of research in
designing kernels based on prior knowledge about the task (jaakkola & haussler, 1998; sch  olkopf, mika,
burges, knirsch, m  uller, r  atsch, & smola, 1999b; g  artner, 2003; cortes, haffner, & mohri, 2004). how-
ever, if we lack suf   cient prior knowledge for designing an appropriate kernel, can we learn it? this question
also motivated much research (lanckriet, cristianini, bartlett, el gahoui, & jordan, 2002; wang & chan,
2002; cristianini, shawe-taylor, elisseeff, & kandola, 2002), and deep architectures can be viewed as a
promising development in this direction. it has been shown that a gaussian process kernel machine can
be improved using a deep belief network to learn a feature space (salakhutdinov & hinton, 2008): after
training the deep belief network, its parameters are used to initialize a deterministic non-linear transfor-
mation (a multi-layer neural network) that computes a feature vector (a new feature space for the data), and
that transformation can be tuned to minimize the prediction error made by the gaussian process, using a
gradient-based optimization. the feature space can be seen as a learned representation of the data. good
representations bring close to each other examples which share abstract characteristics that are relevant fac-
tors of variation of the data distribution. learning algorithms for deep architectures can be seen as ways to
learn a good feature space for kernel machines.

consider one direction v in which a target function f (what the learner should ideally capture) goes
up and down (i.e. as    increases, f (x +   v)     b crosses 0, becomes positive, then negative, positive,
then negative, etc.), in a series of    bumps   . following schmitt (2002), bengio et al. (2006), bengio and
lecun (2007) show that for kernel machines with a gaussian kernel, the required number of examples
grows linearly with the number of bumps in the target function to be learned. they also show that for a
maximally varying function such as the parity function, the number of examples necessary to achieve some
error rate with a gaussian kernel machine is exponential in the input dimension. for a learner that only relies
on the prior that the target function is locally smooth (e.g. gaussian kernel machines), learning a function
with many sign changes in one direction is fundamentally dif   cult (requiring a large vc-dimension, and a

3in the gaussian process case, as in kernel regression, f (x) in eq. 2 is the conditional expectation of the target variable y to predict,

given the input x.

12

correspondingly large number of examples). however, learning could work with other classes of functions
in which the pattern of variations is captured compactly (a trivial example is when the variations are periodic
and the class of functions includes periodic functions that approximately match).

for complex tasks in high dimension, the complexity of the decision surface could quickly make learning
impractical when using a local kernel method. it could also be argued that if the curve has many variations
and these variations are not related to each other through an underlying regularity, then no learning algorithm
will do much better than estimators that are local in input space. however, it might be worth looking for
more compact representations of these variations, because if one could be found, it would be likely to lead to
better generalization, especially for variations not seen in the training set. of course this could only happen
if there were underlying regularities to be captured in the target function; we expect this property to hold in
ai tasks.

estimators that are local in input space are found not only in supervised learning algorithms such as those
discussed above, but also in unsupervised and semi-supervised learning algorithms, e.g. locally linear
embedding (roweis & saul, 2000), isomap (tenenbaum, de silva, & langford, 2000), kernel principal
component analysis (sch  olkopf, smola, & m  uller, 1998) (or kernel pca) laplacian eigenmaps (belkin &
niyogi, 2003), manifold charting (brand, 2003), spectral id91 algorithms (weiss, 1999), and kernel-
based non-parametric semi-supervised algorithms (zhu, ghahramani, & lafferty, 2003; zhou, bousquet,
navin lal, weston, & sch  olkopf, 2004; belkin, matveeva, & niyogi, 2004; delalleau, bengio, & le roux,
2005). most of these unsupervised and semi-supervised algorithms rely on the neighborhood graph: a graph
with one node per example and arcs between near neighbors. with these algorithms, one can get a geometric
intuition of what they are doing, as well as how being local estimators can hinder them. this is illustrated
with the example in figure 4 in the case of manifold learning. here again, it was found that in order to cover
the many possible variations in the function to be learned, one needs a number of examples proportional to
the number of variations to be covered (bengio, monperrus, & larochelle, 2006).

figure 4: the set of images associated with the same object class forms a manifold or a set of disjoint
manifolds, i.e. regions of lower dimension than the original space of images. by rotating or shrinking, e.g.,
a digit 4, we get other images of the same class, i.e. on the same manifold. since the manifold is locally
smooth, it can in principle be approximated locally by linear patches, each being tangent to the manifold.
unfortunately, if the manifold is highly curved, the patches are required to be small, and exponentially many
might be needed with respect to manifold dimension. graph graciously provided by pascal vincent.

finally let us consider the case of semi-supervised learning algorithms based on the neighborhood
graph (zhu et al., 2003; zhou et al., 2004; belkin et al., 2004; delalleau et al., 2005). these algorithms
partition the neighborhood graph in regions of constant label. it can be shown that the number of regions
with constant label cannot be greater than the number of labeled examples (bengio et al., 2006). hence one
needs at least as many labeled examples as there are variations of interest for the classi   cation. this can be

13

prohibitive if the decision surface of interest has a very large number of variations.

id90 (breiman, friedman, olshen, & stone, 1984) are among the best studied learning algo-
rithms. because they can focus on speci   c subsets of input variables, at    rst blush they seem non-local.
however, they are also local estimators in the sense of relying on a partition of the input space and using
separate parameters for each region (bengio, delalleau, & simard, 2009), with each region associated with
a leaf of the decision tree. this means that they also suffer from the limitation discussed above for other
non-parametric learning algorithms: they need at least as many training examples as there are variations
of interest in the target function, and they cannot generalize to new variations not covered in the training
set. theoretical analysis (bengio et al., 2009) shows speci   c classes of functions for which the number of
training examples necessary to achieve a given error rate is exponential in the input dimension. this analysis
is built along lines similar to ideas exploited previously in the computational complexity literature (cucker
& grigoriev, 1999). these results are also in line with previous empirical results (p  erez & rendell, 1996;
vilalta, blix, & rendell, 1997) showing that the generalization performance of id90 degrades when
the number of variations in the target function increases.

ensembles of trees (like boosted trees (freund & schapire, 1996), and forests (ho, 1995; breiman,
2001)) are more powerful than a single tree. they add a third level to the architecture which allows the
model to discriminate among a number of regions exponential in the number of parameters (bengio et al.,
2009). as illustrated in figure 5, they implicitly form a distributed representation (a notion discussed further
in section 3.2) with the output of all the trees in the forest. each tree in an ensemble can be associated with
a discrete symbol identifying the leaf/region in which the input example falls for that tree. the identity
of the leaf node in which the input pattern is associated for each tree forms a tuple that is a very rich
description of the input pattern: it can represent a very large number of possible patterns, because the number
of intersections of the leaf regions associated with the n trees can be exponential in n.

3.2 learning distributed representations

in section 1.2, we argued that deep architectures call for making choices about the kind of representation
at the interface between levels of the system, and we introduced the basic notion of local representation
(discussed further in the previous section), of distributed representation, and of sparse distributed repre-
sentation. the idea of distributed representation is an old idea in machine learning and neural networks
research (hinton, 1986; rumelhart et al., 1986a; miikkulainen & dyer, 1991; bengio, ducharme, & vin-
cent, 2001; schwenk & gauvain, 2002), and it may be of help in dealing with the curse of dimensionality
and the limitations of local generalization. a cartoon local representation for integers i     {1, 2, . . . , n } is a
vector r(i) of n bits with a single 1 and n     1 zeros, i.e. with j-th element rj(i) = 1i=j, called the one-hot
representation of i. a distributed representation for the same integer could be a vector of log2 n bits, which
is a much more compact way to represent i. for the same number of possible con   gurations, a distributed
representation can potentially be exponentially more compact than a very local one. introducing the notion
of sparsity (e.g. encouraging many units to take the value 0) allows for representations that are in between
being fully local (i.e. maximally sparse) and non-sparse (i.e. dense) distributed representations. neurons
in the cortex are believed to have a distributed and sparse representation (olshausen & field, 1997), with
around 1-4% of the neurons active at any one time (attwell & laughlin, 2001; lennie, 2003). in practice,
we often take advantage of representations which are continuous-valued, which increases their expressive
power. an example of continuous-valued local representation is one where the i-th element varies according
to some distance between the input and a prototype or region center, as with the gaussian kernel discussed
in section 3.1. in a distributed representation the input pattern is represented by a set of features that are not
mutually exclusive, and might even be statistically independent. for example, id91 algorithms do not
build a distributed representation since the clusters are essentially mutually exclusive, whereas independent
component analysis (ica) (bell & sejnowski, 1995; pearlmutter & parra, 1996) and principal component
analysis (pca) (hotelling, 1933) build a distributed representation.

consider a discrete distributed representation r(x) for an input pattern x, where ri(x)     {1, . . . m },

14

partition 3

c1=1
c2=0
c3=1

c1=1
c2=1
c3=0

partition 2

c1=1
c2=1
c3=1

c1=1
c2=0
c3=0

c1=0
c2=0
c3=0

c1=0
c2=1
c3=0

partition 1

c1=0
c2=1
c3=1

figure 5: whereas a single decision tree (here just a 2-way partition) can discriminate among a number of
regions linear in the number of parameters (leaves), an ensemble of trees (left) can discriminate among a
number of regions exponential in the number of trees, i.e. exponential in the total number of parameters (at
least as long as the number of trees does not exceed the number of inputs, which is not quite the case here).
each distinguishable region is associated with one of the leaves of each tree (here there are 3 2-way trees,
each de   ning 2 regions, for a total of 7 regions). this is equivalent to a multi-id91, here 3 id91s
each associated with 2 regions. a binomial rbm with 3 hidden units (right) is a multi-id91 with 2
linearly separated regions per partition (each associated with one of the three binomial hidden units). a
multi-id91 is therefore a distributed representation of the input pattern.

i     {1, . . . , n }. each ri(x) can be seen as a classi   cation of x into m classes. as illustrated in figure 5
(with m = 2), each ri(x) partitions the x-space in m regions, but the different partitions can be combined
to give rise to a potentially exponential number of possible intersection regions in x-space, corresponding
to different con   gurations of r(x). note that when representing a particular input distribution, some con-
   gurations may be impossible because they are incompatible. for example, in id38, a local
representation of a word could directly encode its identity by an index in the vocabulary table, or equivalently
a one-hot code with as many entries as the vocabulary size. on the other hand, a distributed representation
could represent the word by concatenating in one vector indicators for syntactic features (e.g., distribution
over parts of speech it can have), morphological features (which suf   x or pre   x does it have?), and semantic
features (is it the name of a kind of animal? etc). like in id91, we construct discrete classes, but the
potential number of combined classes is huge: we obtain what we call a multi-id91 and that is similar to
the idea of overlapping clusters and partial memberships (heller & ghahramani, 2007; heller, williamson,
& ghahramani, 2008) in the sense that cluster memberships are not mutually exclusive. whereas id91
forms a single partition and generally involves a heavy loss of information about the input, a multi-id91
provides a set of separate partitions of the input space. identifying which region of each partition the input
example belongs to forms a description of the input pattern which might be very rich, possibly not losing
any information. the tuple of symbols specifying which region of each partition the input belongs to can
be seen as a transformation of the input into a new space, where the statistical structure of the data and the
factors of variation in it could be disentangled. this corresponds to the kind of partition of x-space that an
ensemble of trees can represent, as discussed in the previous section. this is also what we would like a deep
architecture to capture, but with multiple levels of representation, the higher levels being more abstract and
representing more complex regions of input space.

in the realm of supervised learning, multi-layer neural networks (rumelhart et al., 1986a, 1986b) and in
the realm of unsupervised learning, id82s (ackley, hinton, & sejnowski, 1985) have been
introduced with the goal of learning distributed internal representations in the hidden layers. unlike in
the linguistic example above, the objective is to let learning algorithms discover the features that compose
the distributed representation. in a multi-layer neural network with more than one hidden layer, there are

15

h4

...

...

...

...

h
3

h
2

h
1

x

figure 6: multi-layer neural network, typically used in supervised learning to make a prediction or classi   ca-
tion, through a series of layers, each of which combines an af   ne operation and a non-linearity. deterministic
transformations are computed in a feedforward way from the input x, through the hidden layers hk, to the
network output h   , which gets compared with a label y to obtain the loss l(h   , y) to be minimized.

several representations, one at each layer. learning multiple levels of distributed representations involves a
challenging training problem, which we discuss next.

4 neural networks for deep architectures

4.1 multi-layer neural networks

a typical set of equations for multi-layer neural networks (rumelhart et al., 1986b) is the following. as
illustrated in figure 6, layer k computes an output vector hk using the output hk   1 of the previous layer,
starting with the input x = h0,

hk = tanh(bk + w k hk   1)

(3)

with parameters bk (a vector of offsets) and w k (a matrix of weights). the tanh is applied element-wise
and can be replaced by sigm(u) = 1/(1 + e   u) = 1
2 (tanh(u) + 1) or other saturating non-linearities. the
top layer output h    is used for making a prediction and is combined with a supervised target y into a loss
function l(h   , y), typically convex in b    + w    h      1. the output layer might have a non-linearity different
from the one used in other layers, e.g., the softmax

h      1

(4)

eb   

i +w    

i

h      1

j+w    

j

h   

i =

pj eb   
i is positive andpi

is the i-th row of w    , h   

i can be used as
where w    
i
estimator of p (y = i|x), with the interpretation that y is the class associated with input pattern x. in this
case one often uses the negative conditional log-likelihood l(h   , y) =     log p (y = y|x) =     log h   
y as a
loss, whose expected value over (x, y) pairs is to be minimized.

i = 1. the softmax output h   
h   

4.2 the challenge of training deep neural networks

after having motivated the need for deep architectures that are non-local estimators, we now turn to the
dif   cult problem of training them. experimental evidence suggests that training deep architectures is more
dif   cult than training shallow architectures (bengio et al., 2007; erhan, manzagol, bengio, bengio, & vin-
cent, 2009).

16

until 2006, deep architectures have not been discussed much in the machine learning literature, because
of poor training and generalization errors generally obtained (bengio et al., 2007) using the standard random
initialization of the parameters. note that deep convolutional neural networks (lecun, boser, denker, hen-
derson, howard, hubbard, & jackel, 1989; le cun, bottou, bengio, & haffner, 1998; simard, steinkraus,
& platt, 2003; ranzato et al., 2007) were found easier to train, as discussed in section 4.5, for reasons that
have yet to be really clari   ed.

many unreported negative observations as well as the experimental results in bengio et al. (2007), erhan
et al. (2009) suggest that gradient-based training of deep supervised multi-layer neural networks (starting
from random initialization) gets stuck in    apparent local minima or plateaus   4, and that as the architecture
gets deeper, it becomes more dif   cult to obtain good generalization. when starting from random initializa-
tion, the solutions obtained with deeper neural networks appear to correspond to poor solutions that perform
worse than the solutions obtained for networks with 1 or 2 hidden layers (bengio et al., 2007; larochelle,
bengio, louradour, & lamblin, 2009). this happens even though k + 1-layer nets can easily represent
what a k-layer net can represent (without much added capacity), whereas the converse is not true. how-
ever, it was discovered (hinton et al., 2006) that much better results could be achieved when pre-training
each layer with an unsupervised learning algorithm, one layer after the other, starting with the    rst layer
(that directly takes in input the observed x). the initial experiments used the rbm generative model for
each layer (hinton et al., 2006), and were followed by experiments yielding similar results using variations
of auto-encoders for training each layer (bengio et al., 2007; ranzato et al., 2007; vincent et al., 2008).
most of these papers exploit the idea of greedy layer-wise unsupervised learning (developed in more de-
tail in the next section):    rst train the lower layer with an unsupervised learning algorithm (such as one
for the rbm or some auto-encoder), giving rise to an initial set of parameter values for the    rst layer of
a neural network. then use the output of the    rst layer (a new representation for the raw input) as input
for another layer, and similarly initialize that layer with an unsupervised learning algorithm. after having
thus initialized a number of layers, the whole neural network can be    ne-tuned with respect to a supervised
training criterion as usual. the advantage of unsupervised pre-training versus random initialization was
clearly demonstrated in several statistical comparisons (bengio et al., 2007; larochelle et al., 2007, 2009;
erhan et al., 2009). what principles might explain the improvement in classi   cation error observed in the
literature when using unsupervised pre-training? one clue may help to identify the principles behind the
success of some training algorithms for deep architectures, and it comes from algorithms that exploit neither
rbms nor auto-encoders (weston et al., 2008; mobahi et al., 2009). what these algorithms have in common
with the training algorithms based on rbms and auto-encoders is layer-local unsupervised criteria, i.e., the
idea that injecting an unsupervised training signal at each layer may help to guide the parameters of that
layer towards better regions in parameter space. in weston et al. (2008), the neural networks are trained
using pairs of examples (x,   x), which are either supposed to be    neighbors    (or of the same class) or not.
consider hk(x) the level-k representation of x in the model. a local training criterion is de   ned at each
layer that pushes the intermediate representations hk(x) and hk(  x) either towards each other or away from
each other, according to whether x and   x are supposed to be neighbors or not (e.g., k-nearest neighbors in
input space). the same criterion had already been used successfully to learn a low-dimensional embedding
with an unsupervised manifold learning algorithm (hadsell, chopra, & lecun, 2006) but is here (weston
et al., 2008) applied at one or more intermediate layer of the neural network. following the idea of slow
feature analysis (wiskott & sejnowski, 2002), mobahi et al. (2009), bergstra and bengio (2010) exploit
the temporal constancy of high-level abstraction to provide an unsupervised guide to intermediate layers:
successive frames are likely to contain the same object.

clearly, test errors can be signi   cantly improved with these techniques, at least for the types of tasks stud-
ied, but why? one basic question to ask is whether the improvement is basically due to better optimization
or to better id173. as discussed below, the answer may not    t the usual de   nition of optimization
and id173.

4we call them apparent local minima in the sense that the id119 learning trajectory is stuck there, which does not com-

pletely rule out that more powerful optimizers could not    nd signi   cantly better solutions far from these.

17

in some experiments (bengio et al., 2007; larochelle et al., 2009) it is clear that one can get training
classi   cation error down to zero even with a deep neural network that has no unsupervised pre-training,
pointing more in the direction of a id173 effect than an optimization effect. experiments in erhan
et al. (2009) also give evidence in the same direction: for the same training error (at different points during
training), test error is systematically lower with unsupervised pre-training. as discussed in erhan et al.
(2009), unsupervised pre-training can be seen as a form of regularizer (and prior): unsupervised pre-training
amounts to a constraint on the region in parameter space where a solution is allowed. the constraint forces
solutions    near   5 ones that correspond to the unsupervised training, i.e., hopefully corresponding to solutions
capturing signi   cant statistical structure in the input. on the other hand, other experiments (bengio et al.,
2007; larochelle et al., 2009) suggest that poor tuning of the lower layers might be responsible for the worse
results without pre-training: when the top hidden layer is constrained (forced to be small) the deep networks
with random initialization (no unsupervised pre-training) do poorly on both training and test sets, and much
worse than pre-trained networks. in the experiments mentioned earlier where training error goes to zero, it
was always the case that the number of hidden units in each layer (a hyper-parameter) was allowed to be as
large as necessary (to minimize error on a validation set). the explanatory hypothesis proposed in bengio
et al. (2007), larochelle et al. (2009) is that when the top hidden layer is unconstrained, the top two layers
(corresponding to a regular 1-hidden-layer neural net) are suf   cient to    t the training set, using as input the
representation computed by the lower layers, even if that representation is poor. on the other hand, with
unsupervised pre-training, the lower layers are    better optimized   , and a smaller top layer suf   ces to get a
low training error but also yields better generalization. other experiments described in erhan et al. (2009)
are also consistent with the explanation that with random parameter initialization, the lower layers (closer to
the input layer) are poorly trained. these experiments show that the effect of unsupervised pre-training is
most marked for the lower layers of a deep architecture.

we know from experience that a two-layer network (one hidden layer) can be well trained in general, and
that from the point of view of the top two layers in a deep network, they form a shallow network whose input
is the output of the lower layers. optimizing the last layer of a deep neural network is a id76
problem for the training criteria commonly used. optimizing the last two layers, although not convex, is
known to be much easier than optimizing a deep network (in fact when the number of hidden units goes
to in   nity, the training criterion of a two-layer network can be cast as convex (bengio, le roux, vincent,
delalleau, & marcotte, 2006)).

if there are enough hidden units (i.e. enough capacity) in the top hidden layer, training error can be
brought very low even when the lower layers are not properly trained (as long as they preserve most of the
information about the raw input), but this may bring worse generalization than shallow neural networks.
when training error is low and test error is high, we usually call the phenomenon over   tting. since unsuper-
vised pre-training brings test error down, that would point to it as a kind of data-dependent regularizer. other
strong evidence has been presented suggesting that unsupervised pre-training acts like a regularizer (erhan
et al., 2009): in particular, when there is not enough capacity, unsupervised pre-training tends to hurt gener-
alization, and when the training set size is    small    (e.g., mnist, with less than hundred thousand examples),
although unsupervised pre-training brings improved test error, it tends to produce larger training error.

on the other hand, for much larger training sets, with better initialization of the lower hidden layers, both
training and generalization error can be made signi   cantly lower when using unsupervised pre-training (see
figure 7 and discussion below). we hypothesize that in a well-trained deep neural network, the hidden layers
form a    good    representation of the data, which helps to make good predictions. when the lower layers are
poorly initialized, these deterministic and continuous representations generally keep most of the information
about the input, but these representations might scramble the input and hurt rather than help the top layers to
perform classi   cations that generalize well.

according to this hypothesis, although replacing the top two layers of a deep neural network by convex
machinery such as a gaussian process or an id166 can yield some improvements (bengio & lecun, 2007),
especially on the training error, it would not help much in terms of generalization if the lower layers have

5in the same basin of attraction of the id119 procedure

18

not been suf   ciently optimized, i.e., if a good representation of the raw input has not been discovered.

hence, one hypothesis is that unsupervised pre-training helps generalization by allowing for a    better   
tuning of lower layers of a deep architecture. although training error can be reduced either by exploiting
only the top layers ability to    t the training examples, better generalization is achieved when all the layers are
tuned appropriately. another source of better generalization could come from a form of id173: with
unsupervised pre-training, the lower layers are constrained to capture regularities of the input distribution.
consider random input-output pairs (x, y ). such id173 is similar to the hypothesized effect of
unlabeled examples in semi-supervised learning (lasserre, bishop, & minka, 2006) or the id173
effect achieved by maximizing the likelihood of p (x, y ) (generative models) vs p (y |x) (discriminant
models) (ng & jordan, 2002; liang & jordan, 2008).
if the true p (x) and p (y |x) are unrelated as
functions of x (e.g., chosen independently, so that learning about one does not inform us of the other), then
unsupervised learning of p (x) is not going to help learning p (y |x). but if they are related 6, and if the
same parameters are involved in estimating p (x) and p (y |x)7, then each (x, y ) pair brings information
on p (y |x) not only in the usual way but also through p (x). for example, in a deep belief net, both
distributions share essentially the same parameters, so the parameters involved in estimating p (y |x) bene   t
from a form of data-dependent id173: they have to agree to some extent with p (y |x) as well as
with p (x).

let us return to the optimization versus id173 explanation of the better results obtained with
unsupervised pre-training. note how one should be careful when using the word    optimization    here. we
do not have an optimization dif   culty in the usual sense of the word. indeed, from the point of view of
the whole network, there is no dif   culty since one can drive training error very low, by relying mostly
on the top two layers. however, if one considers the problem of tuning the lower layers (while keeping
small either the number of hidden units of the penultimate layer (i.e. top hidden layer) or the magnitude of
the weights of the top two layers), then one can maybe talk about an optimization dif   culty. one way to
reconcile the optimization and id173 viewpoints might be to consider the truly online setting (where
examples come from an in   nite stream and one does not cycle back through a training set). in that case,
online id119 is performing a stochastic optimization of the generalization error. if the effect of
unsupervised pre-training was purely one of id173, one would expect that with a virtually in   nite
training set, online error with or without pre-training would converge to the same level. on the other hand, if
the explanatory hypothesis presented here is correct, we would expect that unsupervised pre-training would
bring clear bene   ts even in the online setting. to explore that question, we have used the    in   nite mnist   
dataset (loosli, canu, & bottou, 2007) i.e. a virtually in   nite stream of mnist-like digit images (obtained
by random translations, rotations, scaling, etc. de   ned in simard, lecun, and denker (1993)). as illustrated
in figure 7, a 3-hidden layer neural network trained online converges to signi   cantly lower error when it
is pre-trained (as a stacked denoising auto-encoder, see section 7.2). the    gure shows progress with the
online error (on the next 1000 examples), an unbiased monte-carlo estimate of generalization error. the    rst
2.5 million updates are used for unsupervised pre-training. the    gure strongly suggests that unsupervised
pre-training converges to a lower error, i.e., that it acts not only as a regularizer but also to    nd better minima
of the optimized criterion. in spite of appearances, this does not contradict the id173 hypothesis:
because of local minima, the id173 effect persists even as the number of examples goes to in   nity.
the    ip side of this interpretation is that once the dynamics are trapped near some apparent local minimum,
more labeled examples do not provide a lot more new information.

to explain that lower layers would be more dif   cult to optimize, the above clues suggest that the gradient
propagated backwards into the lower layer might not be suf   cient to move the parameters into regions cor-
responding to good solutions. according to that hypothesis, the optimization with respect to the lower level
parameters gets stuck in a poor apparent local minimum or plateau (i.e. small gradient). since gradient-based

6for example, the mnist digit images form rather well-separated clusters, especially when learning good representations, even
unsupervised (van der maaten & hinton, 2008), so that the decision surfaces can be guessed reasonably well even before seeing any
label.

7for example, all the lower layers of a multi-layer neural net estimating p (y |x) can be initialized with the parameters from a deep

belief net estimating p (x).

19

3   layer net, budget of 10000000 iterations

 

0 unsupervised + 10000000 supervised
2500000 unsupervised + 7500000 supervised

101

100

10   1

10   2

10   3

r
o
r
r
e

 

n
o

i
t

a
c
i
f
i
s
s
a
c
 
e
n

l

i
l

n
o

10   4

 
0

1

2

3

4

5

6

number of examples seen

7

8

9

10
x 106

figure 7: deep architecture trained online with 10 million examples of digit images, either with pre-training
(triangles) or without (circles). the classi   cation error shown (vertical axis, log-scale) is computed online
on the next 1000 examples, plotted against the number of examples seen from the beginning. the    rst
2.5 million examples are used for unsupervised pre-training (of a stack of denoising auto-encoders). the
oscillations near the end are because the error rate is too close to zero, making the sampling variations
appear large on the log-scale. whereas with a very large training set id173 effects should dissipate,
one can see that without pre-training, training converges to a poorer apparent local minimum: unsupervised
pre-training helps to    nd a better minimum of the online error. experiments performed by dumitru erhan.

20

training of the top layers works reasonably well, it would mean that the gradient becomes less informative
about the required changes in the parameters as we move back towards the lower layers, or that the error
function becomes too ill-conditioned for id119 to escape these apparent local minima. as argued
in section 4.5, this might be connected with the observation that deep convolutional neural networks are eas-
ier to train, maybe because they have a very special sparse connectivity in each layer. there might also be
a link between this dif   culty in exploiting the gradient in deep networks and the dif   culty in training recur-
rent neural networks through long sequences, analyzed in hochreiter (1991), bengio, simard, and frasconi
(1994), lin, horne, tino, and giles (1995). a recurrent neural network can be    unfolded in time    by con-
sidering the output of each neuron at different time steps as different variables, making the unfolded network
over a long input sequence a very deep architecture. in recurrent neural networks, the training dif   culty can
be traced to a vanishing (or sometimes exploding) gradient propagated through many non-linearities. there
is an additional dif   culty in the case of recurrent neural networks, due to a mismatch between short-term
(i.e., shorter paths in unfolded graph of computations) and long-term components of the gradient (associated
with longer paths in that graph).

4.3 unsupervised learning for deep architectures

as we have seen above, layer-wise unsupervised learning has been a crucial component of all the successful
learning algorithms for deep architectures up to now. if gradients of a criterion de   ned at the output layer
become less useful as they are propagated backwards to lower layers, it is reasonable to believe that an
unsupervised learning criterion de   ned at the level of a single layer could be used to move its parameters in
a favorable direction. it would be reasonable to expect this if the single-layer learning algorithm discovered a
representation that captures statistical regularities of the layer   s input. pca and the standard variants of ica
requiring as many causes as signals seem inappropriate because they generally do not make sense in the so-
called overcomplete case, where the number of outputs of the layer is greater than the number of its inputs.
this suggests looking in the direction of extensions of ica to deal with the overcomplete case (lewicki
& sejnowski, 1998; hyv  arinen, karhunen, & oja, 2001; hinton, welling, teh, & osindero, 2001; teh,
welling, osindero, & hinton, 2003), as well as algorithms related to pca and ica, such as auto-encoders
and rbms, which can be applied in the overcomplete case. indeed, experiments performed with these one-
layer unsupervised learning algorithms in the context of a multi-layer system con   rm this idea (hinton et al.,
2006; bengio et al., 2007; ranzato et al., 2007). furthermore, stacking linear projections (e.g. two layers of
pca) is still a linear transformation, i.e., not building deeper architectures.

in addition to the motivation that unsupervised learning could help reduce the dependency on the unre-
liable update direction given by the gradient of a supervised criterion, we have already introduced another
motivation for using unsupervised learning at each level of a deep architecture. it could be a way to naturally
decompose the problem into sub-problems associated with different levels of abstraction. we know that
unsupervised learning algorithms can extract salient information about the input distribution. this informa-
tion can be captured in a distributed representation, i.e., a set of features which encode the salient factors of
variation in the input. a one-layer unsupervised learning algorithm could extract such salient features, but
because of the limited capacity of that layer, the features extracted on the    rst level of the architecture can
be seen as low-level features. it is conceivable that learning a second layer based on the same principle but
taking as input the features learned with the    rst layer could extract slightly higher-level features. in this
way, one could imagine that higher-level abstractions that characterize the input could emerge. note how
in this process all learning could remain local to each layer, therefore side-stepping the issue of gradient
diffusion that might be hurting gradient-based learning of deep neural networks, when we try to optimize a
single global criterion. this motivates the next section, where we discuss deep generative architectures and
introduce id50 formally.

21

...

...

...

...

h
3

h
2

h1

x

figure 8: example of a generative multi-layer neural network, here a sigmoid belief network, represented as
a directed graphical model (with one node per random variable, and directed arcs indicating direct depen-
dence). the observed data is x and the hidden factors at level k are the elements of vector hk. the top layer
h3 has a factorized prior.

4.4 deep generative architectures

besides being useful for pre-training a supervised predictor, unsupervised learning in deep architectures
can be of interest to learn a distribution and generate samples from it. generative models can often be
represented as id114 (jordan, 1998): these are visualized as graphs in which nodes represent ran-
dom variables and arcs say something about the type of dependency existing between the random variables.
the joint distribution of all the variables can be written in terms of products involving only a node and its
neighbors in the graph. with directed arcs (de   ning parenthood), a node is conditionally independent of its
ancestors, given its parents. some of the random variables in a graphical model can be observed, and others
cannot (called hidden variables). sigmoid belief networks are generative multi-layer neural networks that
were proposed and studied before 2006, and trained using variational approximations (dayan, hinton, neal,
& zemel, 1995; hinton, dayan, frey, & neal, 1995; saul, jaakkola, & jordan, 1996; titov & henderson,
2007). in a sigmoid belief network, the units (typically binary random variables) in each layer are indepen-
dent given the values of the units in the layer above, as illustrated in figure 8. the typical parametrization
of these conditional distributions (going downwards instead of upwards in ordinary neural nets) is similar to
the neuron activation equation of eq. 3:

p (hk

i = 1|hk+1) = sigm(bk

i +xj

w k+1

i,j

hk+1

j

)

(5)

i is the binary activation of hidden node i in layer k, hk is the vector (hk

2, . . .), and we denote the
where hk
input vector x = h0. note how the notation p (. . .) always represents a id203 distribution associated
with our model, whereas   p is the training distribution (the empirical distribution of the training set, or the
generating distribution for our training examples). the bottom layer generates a vector x in the input space,
and we would like the model to give high id203 to the training data. considering multiple levels, the
generative model is thus decomposed as follows:

1, hk

p (x, h1, . . . , h   ) = p (h   )       1yk=1

p (hk|hk+1)! p (x|h1)

(6)

and marginalization yields p (x), but this is intractable in practice except for tiny models. in a sigmoid belief
i ),

network, the top level prior p (h   ) is generally chosen to be factorized, i.e., very simple: p (h   ) =qi p (h   

22

p(   ,    ) ~ rbm

3h h
2

...

...

...

...

h
3

h2

h1

x

figure 9: graphical model of a deep belief network with observed vector x and hidden layers h1, h2 and
h3. notation is as in figure 8. the structure is similar to a sigmoid belief network, except for the top
two layers. instead of having a factorized prior for p (h3), the joint of the top two layers, p (h2, h3), is a
restricted id82. the model is mixed, with double arrows on the arcs between the top two
layers because an rbm is an undirected graphical model rather than a directed one.

and a single bernoulli parameter is required for each p (h   

i = 1) in the case of binary units.

id50 are similar to sigmoid belief networks, but with a slightly different parametrization

for the top two layers, as illustrated in figure 9:

the joint distribution of the top two layers is a restricted id82 (rbm),

p (x, h1, . . . , h   ) = p (h      1, h   )       2yk=1
...

   
   
   
   
   
   
   
   

  
  
  
  
  
  
  
  

   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   

h

p (hk|hk+1)! p (x|h1).

(7)

   
   
   
   
   
   
   
   
   
   

  
  
  
  
  
  
  
  

   
   
   
   
   
   
   
   

...

   
   
   
   
   
   
   
   

x

figure 10: undirected graphical model of a restricted id82 (rbm). there are no links
between units of the same layer, only between input (or visible) units xj and hidden units hi, making the
conditionals p (h|x) and p (x|h) factorize conveniently.

p (h      1, h   )     eb   h      1+c    h   +h       

w h      1

(8)

23

illustrated in figure 10, and whose id136 and training algorithms are described in more detail in sec-
tions 5.3 and 5.4 respectively. this apparently slight change from sigmoidal belief networks to dbns comes
with a different learning algorithm, which exploits the notion of training greedily one layer at a time, building
up gradually more abstract representations of the raw input into the posteriors p (hk|x). a detailed descrip-
tion of rbms and of the greedy layer-wise training algorithms for deep architectures follows in sections 5
and 6.

4.5 convolutional neural networks

although deep supervised neural networks were generally found too dif   cult to train before the use of
unsupervised pre-training, there is one notable exception: convolutional neural networks. convolutional nets
were inspired by the visual system   s structure, and in particular by the models of it proposed by hubel and
wiesel (1962). the    rst computational models based on these local connectivities between neurons and on
hierarchically organized transformations of the image are found in fukushima   s neocognitron (fukushima,
1980). as he recognized, when neurons with the same parameters are applied on patches of the previous
layer at different locations, a form of translational invariance is obtained. later, lecun and collaborators,
following up on this idea, designed and trained convolutional networks using the error gradient, obtaining
state-of-the-art performance (lecun et al., 1989; le cun et al., 1998) on several pattern recognition tasks.
modern understanding of the physiology of the visual system is consistent with the processing style found
in convolutional networks (serre et al., 2007), at least for the quick recognition of objects, i.e., without the
bene   t of attention and top-down feedback connections. to this day, pattern recognition systems based on
convolutional neural networks are among the best performing systems. this has been shown clearly for
handwritten character recognition (le cun et al., 1998), which has served as a machine learning benchmark
for many years.8

concerning our discussion of training deep architectures, the example of convolutional neural net-
works (lecun et al., 1989; le cun et al., 1998; simard et al., 2003; ranzato et al., 2007) is interesting
because they typically have    ve, six or seven layers, a number of layers which makes fully-connected neural
networks almost impossible to train properly when initialized randomly. what is particular in their architec-
ture that might explain their good generalization performance in vision tasks?

lecun   s convolutional neural networks are organized in layers of two types: convolutional layers and
subsampling layers. each layer has a topographic structure, i.e., each neuron is associated with a    xed
two-dimensional position that corresponds to a location in the input image, along with a receptive    eld (the
region of the input image that in   uences the response of the neuron). at each location of each layer, there
are a number of different neurons, each with its set of input weights, associated with neurons in a rectangular
patch in the previous layer. the same set of weights, but a different input rectangular patch, are associated
with neurons at different locations.

one untested hypothesis is that the small fan-in of these neurons (few inputs per neuron) helps gradients
to propagate through so many layers without diffusing so much as to become useless. note that this alone
would not suf   ce to explain the success of convolutional networks, since random sparse connectivity is not
enough to yield good results in deep neural networks. however, an effect of the fan-in would be consistent
with the idea that gradients propagated through many paths gradually become too diffuse, i.e., the credit
or blame for the output error is distributed too widely and thinly. another hypothesis (which does not
necessarily exclude the    rst) is that the hierarchical local connectivity structure is a very strong prior that is
particularly appropriate for vision tasks, and sets the parameters of the whole network in a favorable region
(with all non-connections corresponding to zero weight) from which gradient-based optimization works
well. the fact is that even with random weights in the    rst layers, a convolutional neural network performs
well (ranzato, huang, boureau, & lecun, 2007), i.e., better than a trained fully connected neural network
but worse than a fully optimized convolutional neural network.

8maybe too many years? it is good that the    eld is moving towards more ambitious benchmarks, such as those introduced by lecun,

huang, and bottou (2004), larochelle et al. (2007).

24

very recently, the convolutional structure has been imported into rbms (desjardins & bengio, 2008)
and dbns (lee et al., 2009). an important innovation in lee et al. (2009) is the design of a generative
version of the pooling / subsampling units, which worked beautifully in the experiments reported, yielding
state-of-the-art results not only on mnist digits but also on the caltech-101 object classi   cation benchmark.
in addition, visualizing the features obtained at each level (the patterns most liked by hidden units) clearly
con   rms the notion of multiple levels of composition which motivated deep architectures in the    rst place,
moving up from edges to object parts to objects in a natural way.

4.6 auto-encoders

some of the deep architectures discussed below (deep belief nets and stacked auto-encoders) exploit as
component or monitoring device a particular type of neural network: the auto-encoder, also called auto-
associator, or diabolo network (rumelhart et al., 1986b; bourlard & kamp, 1988; hinton & zemel, 1994;
schwenk & milgram, 1995; japkowicz, hanson, & gluck, 2000). there are also connections between the
auto-encoder and rbms discussed in section 5.4.3, showing that auto-encoder training approximates rbm
training by contrastive divergence. because training an auto-encoder seems easier than training an rbm,
they have been used as building blocks to train deep networks, where each level is associated with an auto-
encoder that can be trained separately (bengio et al., 2007; ranzato et al., 2007; larochelle et al., 2007;
vincent et al., 2008).

an auto-encoder is trained to encode the input x into some representation c(x) so that the input can be
reconstructed from that representation. hence the target output of the auto-encoder is the auto-encoder input
itself. if there is one linear hidden layer and the mean squared error criterion is used to train the network,
then the k hidden units learn to project the input in the span of the    rst k principal components of the
data (bourlard & kamp, 1988). if the hidden layer is non-linear, the auto-encoder behaves differently from
pca, with the ability to capture multi-modal aspects of the input distribution (japkowicz et al., 2000). the
formulation that we prefer generalizes the mean squared error criterion to the minimization of the negative
log-likelihood of the reconstruction, given the encoding c(x):

if x|c(x) is gaussian, we recover the familiar squared error. if the inputs xi are either binary or considered
to be binomial probabilities, then the id168 would be

re =     log p (x|c(x)).

(9)

    log p (x|c(x)) =    xi

xi log fi(c(x)) + (1     xi) log(1     fi(c(x)))

(10)

where f (  ) is called the decoder, and f (c(x)) is the reconstruction produced by the network, and in this case
should be a vector of numbers in (0, 1), e.g., obtained with a sigmoid. the hope is that the code c(x) is a
distributed representation that captures the main factors of variation in the data: because c(x) is viewed as a
lossy compression of x, it cannot be a good compression (with small loss) for all x, so learning drives it to
be one that is a good compression in particular for training examples, and hopefully for others as well (and
that is the sense in which an auto-encoder generalizes), but not for arbitrary inputs.

one serious issue with this approach is that if there is no other constraint, then an auto-encoder with
n-dimensional input and an encoding of dimension at least n could potentially just learn the identity func-
tion, for which many encodings would be useless (e.g., just copying the input). surprisingly, experiments
reported in bengio et al. (2007) suggest that in practice, when trained with stochastic id119, non-
linear auto-encoders with more hidden units than inputs (called overcomplete) yield useful representations
(in the sense of classi   cation error measured on a network taking this representation in input). a simple
explanation is based on the observation that stochastic id119 with early stopping is similar to an
   2 id173 of the parameters (zinkevich, 2003; collobert & bengio, 2004). to achieve perfect re-
construction of continuous inputs, a one-hidden layer auto-encoder with non-linear hidden units needs very
small weights in the    rst layer (to bring the non-linearity of the hidden units in their linear regime) and very

25

large weights in the second layer. with binary inputs, very large weights are also needed to completely
minimize the reconstruction error. since the implicit or explicit id173 makes it dif   cult to reach
large-weight solutions, the optimization algorithm    nds encodings which only work well for examples simi-
lar to those in the training set, which is what we want. it means that the representation is exploiting statistical
regularities present in the training set, rather than learning to replicate the identity function.

there are different ways that an auto-encoder with more hidden units than inputs could be prevented from
learning the identity, and still capture something useful about the input in its hidden representation. instead
or in addition to constraining the encoder by explicit or implicit id173 of the weights, one strategy is
to add noise in the encoding. this is essentially what rbms do, as we will see later. another strategy, which
was found very successful (olshausen & field, 1997; doi, balcan, & lewicki, 2006; ranzato et al., 2007;
ranzato & lecun, 2007; ranzato et al., 2008; mairal, bach, ponce, sapiro, & zisserman, 2009), is based
on a sparsity constraint on the code. interestingly, these approaches give rise to weight vectors that match
well qualitatively the observed receptive    elds of neurons in v1 and v2 (lee, ekanadham, & ng, 2008),
major areas of the mammal visual system. the question of sparsity is discussed further in section 7.1.

whereas sparsity and id173 reduce representational capacity in order to avoid learning the iden-
tity, rbms can have a very large capacity and still not learn the identity, because they are not (only) trying
to encode the input but also to capture the statistical structure in the input, by approximately maximizing the
likelihood of a generative model. there is a variant of auto-encoder which shares that property with rbms,
called denoising auto-encoder (vincent et al., 2008). the denoising auto-encoder minimizes the error in
reconstructing the input from a stochastically corrupted transformation of the input. it can be shown that it
maximizes a lower bound on the log-likelihood of a generative model. see section 7.2 for more details.

5 energy-based models and id82s

because id50 (dbns) are based on restricted id82s (rbms), which are
particular energy-based models, we introduce here the main mathematical concepts helpful to understand
them, including contrastive divergence (cd).

5.1 energy-based models and products of experts

energy-based models associate a scalar energy to each con   guration of the variables of interest (lecun
& huang, 2005; lecun, chopra, hadsell, ranzato, & huang, 2006; ranzato, boureau, chopra, & lecun,
2007). learning corresponds to modifying that energy function so that its shape has desirable properties. for
example, we would like plausible or desirable con   gurations to have low energy. energy-based probabilistic
models may de   ne a id203 distribution through an energy function, as follows:

p (x) =

e   energy(x)

z

,

(11)

i.e., energies operate in the log-id203 domain. th above generalizes exponential family models (brown,
1986), for which the energy function energy(x) has the form   (  )      (x). we will see below that the
conditional distribution of one layer given another, in the rbm, can be taken from any of the exponential
family distributions (welling, rosen-zvi, & hinton, 2005). whereas any id203 distribution can be
cast as an energy-based models, many more specialized distribution families, such as the exponential family,
can bene   t from particular id136 and learning procedures. some instead have explored rather general-
purpose approaches to learning in energy-based models (hyv  arinen, 2005; lecun et al., 2006; ranzato et al.,
2007).

the normalizing factor z is called the partition function by analogy with physical systems,

z =xx

e   energy(x)

26

(12)

with a sum running over the input space, or an appropriate integral when x is continuous. some energy-based
models can be de   ned even when the sum or integral for z does not exist (see sec.5.1.2).

in the product of experts formulation (hinton, 1999, 2002), the energy function is a sum of terms, each

one associated with an    expert    fi:

i.e.

fi(x),

energy(x) =xi
pi(x)    yi
p (x)    yi

e   fi(x).

(13)

(14)

each expert pi(x) can thus be seen as a detector of implausible con   gurations of x, or equivalently, as
enforcing constraints on x. this is clearer if we consider the special case where fi(x) can only take two
values, one (small) corresponding to the case where the constraint is satis   ed, and one (large) corresponding
to the case where it is not. hinton (1999) explains the advantages of a product of experts by opposition to
a mixture of experts where the product of probabilities is replaced by a weighted sum of probabilities. to
simplify, assume that each expert corresponds to a constraint that can either be satis   ed or not. in a mixture
model, the constraint associated with an expert is an indication of belonging to a region which excludes the
other regions. one advantage of the product of experts formulation is therefore that the set of fi(x) forms
a distributed representation: instead of trying to partition the space with one region per expert as in mixture
models, they partition the space according to all the possible con   gurations (where each expert can have its
constraint violated or not). hinton (1999) proposed an algorithm for estimating the gradient of log p (x) in
eq. 14 with respect to parameters associated with each expert, using the    rst instantiation (hinton, 2002) of
the contrastive divergence algorithm (section 5.4).

5.1.1 introducing hidden variables

in many cases of interest, x has many component variables xi, and we do not observe of these components
simultaneously, or we want to introduce some non-observed variables to increase the expressive power of
the model. so we consider an observed part (still denoted x here) and a hidden part h

and because only x is observed, we care about the marginal

p (x, h) =

e   energy(x,h)

z

p (x) =xh

e   energy(x,h)

z

.

(15)

(16)

in such cases, to map this formulation to one similar to eq. 11, we introduce the notation (inspired from
physics) of free energy, de   ned as follows:

p (x) =

e   freeenergy(x)

z

,

with z =px e   freeenergy(x), i.e.

freeenergy(x) =     logxh

e   energy(x,h).

(17)

(18)

so the free energy is just a marginalization of energies in the log-domain. the data log-likelihood gradient
then has a particularly interesting form. let us introduce    to represent parameters of the model. starting

27

from eq. 17, we obtain

    log p (x)

     

=    

=    

1

+

     

     

   freeenergy(x)

   freeenergy(x)

zx  x
+x  x
(cid:21) =    e   p(cid:20)    freeenergy(x)

     

e   p(cid:20)     log p (x)

     

e   freeenergy(  x)    freeenergy(  x)

     

p (  x)

   freeenergy(  x)

     

.

(cid:21) + ep(cid:20)    freeenergy(x)

     

(cid:21)

hence the average log-likelihood gradient over the training set is

(19)

(20)

where expectations are over x, with   p the training set empirical distribution and ep the expectation under
the model   s distribution p . therefore, if we could sample from p and compute the free energy tractably, we
would have a monte-carlo method to obtain a stochastic estimator of the log-likelihood gradient.

if the energy can be written as a sum of terms associated with at most one hidden unit

energy(x, h) =      (x) +xi

  i(x, hi),

(21)

a condition satis   ed in the case of the rbm, then the free energy and numerator of the likelihood can be
computed tractably (even though it involves a sum with an exponential number of terms):

p (x) =

1
z

e   freeenergy(x) =

e   energy(x,h)

1

zxh

e  (x)   pi   i(x,hi) =

=

=

=

. . .xhk
e     1(x,h1)xh2

1

e  (x)

zxh1 xh2
z xh1
z yi xhi

e  (x)

1

zxh1 xh2

. . .xhk

e  (x)yi

e     i(x,hi)

e     2(x,h2) . . .xhk

e     k(x,hk)

e     i(x,hi)

(22)

is a sum over all the values that hi can take (e.g. 2 values in the usual binomial units case);

be replaced by integrals if h is continuous, and the same principles apply. in many cases of interest, the sum
or integral (over a single hidden unit   s values) is easy to compute. the numerator of the likelihood (i.e. also

in the above,phi
note how that sum is much easier to carry out than the sumph over all values of h. note that all sums can
the free energy) can be computed exactly in the above case, where energy(x, h) =      (x) +pi   i(x, hi),

and we have

freeenergy(x) =     log p (x)     log z =      (x)    xi

logxhi

e     i(x,hi).

(23)

5.1.2 conditional energy-based models

whereas computing the partition function is dif   cult in general, if our ultimate goal is to make a decision
concerning a variable y given a variable x, instead of considering all con   gurations (x, y), it is enough to
consider the con   gurations of y for each given x. a common case is one where y can only take values in a
small discrete set, i.e.,

p (y|x) =

e   energy(x,y)

py e   energy(x,y) .

28

(24)

in this case the gradient of the conditional log-likelihood with respect to parameters of the energy function
can be computed ef   ciently. this formulation applies to a discriminant variant of the rbm called discrimi-
native rbm (larochelle & bengio, 2008). such conditional energy-based models have also been exploited
in a series of probabilistic language models based on neural networks (bengio et al., 2001; schwenk &
gauvain, 2002; bengio, ducharme, vincent, & jauvin, 2003; xu, emami, & jelinek, 2003; schwenk, 2004;
schwenk & gauvain, 2005; mnih & hinton, 2009). that formulation (or generally when it is easy to sum
or maximize over the set of values of the terms of the partition function) has been explored at length (lecun
& huang, 2005; lecun et al., 2006; ranzato et al., 2007, 2007; collobert & weston, 2008). an important
and interesting element in the latter work is that it shows that such energy-based models can be optimized
not just with respect to log-likelihood but with respect to more general criteria whose gradient has the prop-
erty of making the energy of    correct    responses decrease while making the energy of competing responses
increase. these energy functions do not necessarily give rise to a probabilistic model (because the expo-
nential of the negated energy function is not required to be integrable), but they may nonetheless give rise
to a function that can be used to choose y given x, which is often the ultimate goal in applications. indeed
when y takes a    nite number of values, p (y|x) can always be computed since the energy function needs to
be normalized only over the possible values of y.

5.2 id82s

the id82 is a particular type of energy-based model with hidden variables, and rbms are
special forms of id82s in which p (h|x) and p (x|h) are both tractable because they factorize.
in a id82 (hinton, sejnowski, & ackley, 1984; ackley et al., 1985; hinton & sejnowski,
1986), the energy function is a general second-order polynomial:

energy(x, h) =    b   x     c   h     h   w x     x   u x     h   v h.

(25)

there are two types of parameters, which we collectively denote by   : the offsets bi and ci (each associated
with a single element of the vector x or of the vector h), and the weights wij, uij and vij (each associated
with a pair of units). matrices u and v are assumed to be symmetric9, and in most models with zeros in
the diagonal. non-zeros in the diagonal can be used to obtain other variants, e.g., with gaussian instead of
binomial units (welling et al., 2005).

because of the quadratic interaction terms in h, the trick to analytically compute the free energy (eq. 22)
cannot be applied here. however, an mcmc (monte carlo markov chain (andrieu, de freitas, doucet, &
jordan, 2003)) sampling procedure can be applied in order to obtain a stochastic estimator of the gradient.
the gradient of the log-likelihood can be written as follows, starting from eq. 16:

    log p (x)

     

=

=    

    logp  x,h e   energy(  x,h)

     

e   energy(x,h)    energy(x, h)

     

e   energy(  x,h)    energy(  x, h)

     

   

     
1

    logph e   energy(x,h)
ph e   energy(x,h)xh
p  x,h e   energy(  x,h)x  x,h

1

+

p (h|x)

     

   energy(x, h)

=    xh

+x  x,h

p (  x, h)

   energy(  x, h)

     

.

(26)

note that    energy(x,h)
is easy to compute. hence if we have a procedure to sample from p (h|x) and one to
sample from p (x, h), we can obtain an unbiased stochastic estimator of the log-likelihood gradient. hinton

     

9e.g. if u was not symmetric, the extra degrees of freedom would be wasted since xiuij xj + xj uji xi can be rewritten xi(uij +

uji)xj = 1

2

xi(uij + uji)xj + 1

2

xj(uij + uji)xi, i.e., in a symmetric-matrix form.

29

et al. (1984), ackley et al. (1985), hinton and sejnowski (1986) introduced the following terminology: in
the positive phase, x is clamped to the observed input vector, and we sample h given x; in the negative
phase both x and h are sampled, ideally from the model itself. in general, only approximate sampling can
be achieved tractably, e.g., using an iterative procedure that constructs an mcmc. the mcmc sampling
approach introduced in hinton et al. (1984), ackley et al. (1985), hinton and sejnowski (1986) is based on
id150 (geman & geman, 1984; andrieu et al., 2003). id150 of the joint of n random
variables s = (s1 . . . sn ) is done through a sequence of n sampling sub-steps of the form

si     p (si|s   i = s   i)

(27)

where s   i contains the n     1 other random variables in s, excluding si. after these n samples have
been obtained, a step of the chain is completed, yielding a sample of s whose distribution converges to
p (s) as the number of steps goes to    , under some conditions. a suf   cient condition for convergence of a
   nite-state markov chain is that it is aperiodic10 and irreducible11.

how can we perform id150 in a id82? let s = (x, h) denote all the units in the
id82, and s   i the set of values associated with all units except the i-th one. the boltzmann
machine energy function can be rewritten by putting all the parameters in a vector d and a symmetric matrix
a,

energy(s) =    d   s     s   as.

(28)

let d   i denote the vector d without the element di, a   i the matrix a without the i-th row and column,
and a   i the vector that is the i-th row (or column) of a, without the i-th element. using this notation, we
obtain that p (si|s   i) can be computed and sampled from easily in a id82. for example, if
si     {0, 1} and the diagonal of a is null:

p (si = 1|s   i) =

exp(di + d   

   i

exp(di + d   
s   i + 2a   

   i

   i
s   i + s   

s   i + 2a   

s   i + s   

   ia   is   i)

   i

   ia   is   i) + exp(d   

   i

exp(di + 2a   

=

exp(di + 2a   
   i
= sigm(di + 2a   

   i

s   i)

   i
s   i) + 1
s   i)

=

1

1 + exp(   di     2a   

   i

s   i)

s   i + s   

   ia   is   i)

(29)

which is essentially the usual equation for computing a neuron   s output in terms of other neurons s   i, in
arti   cial neural networks.

since two mcmc chains (one for the positive phase and one for the negative phase) are needed for each
example x, the computation of the gradient can be very expensive, and training time very long. this is
essentially why the id82 was replaced in the late 80   s by the back-propagation algorithm for
multi-layer neural networks as the dominant learning approach. however, recent work has shown that short
chains can sometimes be used successfully, and this is the principle of contrastive divergence, discussed
below (section 5.4) to train rbms. note also that the negative phase chain does not have to be restarted for
each new example x (since it does not depend on the training data), and this observation has been exploited
in persistent mcmc estimators (tieleman, 2008; salakhutdinov & hinton, 2009) discussed in section 5.4.2.

5.3 restricted id82s

the restricted id82 (rbm) is the building block of a deep belief network (dbn) because
it shares parametrization with individual layers of a dbn, and because ef   cient learning algorithms were
found to train it. the undirected graphical model of an rbm is illustrated in figure 10, showing that
the hi are independent of each other when conditioning on x and the xj are independent of each other
when conditioning on h. in an rbm, u = 0 and v = 0 in eq. 25, i.e., the only interaction terms are

10aperiodic: no state is periodic with period k > 1; a state has period k if one can only return to it at times t + k, t + 2k, etc.
11irreducible: one can reach any state from any state in    nite time with non-zero id203.

30

between a hidden unit and a visible unit, but not between units of the same layer. this form of model was
   rst introduced under the name of harmonium (smolensky, 1986), and learning algorithms (beyond the
ones for id82s) were discussed in freund and haussler (1994). empirically demonstrated
and ef   cient learning algorithms and variants were proposed more recently (hinton, 2002; welling et al.,
2005; carreira-perpi  nan & hinton, 2005). as a consequence of the lack of input-input and hidden-hidden
interactions, the energy function is bilinear,

energy(x, h) =    b   x     c   h     h   w x

(30)

and the factorization of the free energy of the input, introduced with eq. 21 and 23 can be applied with
  (x) = b   x and   i(x, hi) =    hi(ci + wix), where wi is the row vector corresponding to the i-th row of
w . therefore the free energy of the input (i.e. its unnormalized log-id203) can be computed ef   ciently:

freeenergy(x) =    b   x    xi

logxhi

ehi(ci+wi x).

(31)

using the same factorization trick (in eq. 22) due to the af   ne form of energy(x, h) with respect to h,

we readily obtain a tractable expression for the id155 p (h|x):

p (h|x) =

=

exp(b   x + c   h + h   w x)

exp(ci   hi +   hiwix)

p  h exp(b   x + c      h +   h   w x)
qi exp(cihi + hiwix)
qip  hi
p  hi

exp(  hi(ci + wix))

exp(hi(ci + wix))

p (hi|x).

= yi
= yi

in the commonly studied case where hi     {0, 1}, we obtain the usual neuron equation for a neuron   s output
given its input:

p (hi = 1|x) =

eci+wi x

1 + eci+wix = sigm(ci + wix).

(32)

since x and h play a symmetric role in the energy function, a similar derivation allows to ef   ciently compute
and sample p (x|h):

and in the binary case

p (x|h) =yi

p (xi|h)

p (xj = 1|h) = sigm(bj + w    
  j

h)

(33)

(34)

where w  j is the j-th column of w .

in hinton et al. (2006), binomial input units are used to encode pixel gray levels in input images as if
they were the id203 of a binary event. in the case of handwritten character images this approximation
works well, but in other cases it does not. experiments showing the advantage of using gaussian input
units rather than binomial units when the inputs are continuous-valued are described in bengio et al. (2007).
see welling et al. (2005) for a general formulation where x and h (given the other) can be in any of the
exponential family distributions (discrete and continuous).

although rbms might not be able to represent ef   ciently some distributions that could be represented
compactly with an unrestricted id82, rbms can represent any discrete distribution (freund
& haussler, 1994; le roux & bengio, 2008), if enough hidden units are used. in addition, it can be shown
that unless the rbm already perfectly models the training distribution, adding a hidden unit (and properly
choosing its weights and offset) can always improve the log-likelihood (le roux & bengio, 2008).

31

an rbm can also be seen as forming a multi-id91 (see section 3.2), as illustrated in figure 5. each
hidden unit creates a 2-region partition of the input space (with a linear separation). when we consider the
con   gurations of say three hidden units, there are 8 corresponding possible intersections of 3 half-planes
(by choosing each half-plane among the two half-planes associated with the linear separation performed by
a hidden unit). each of these 8 intersections corresponds to a region in input space associated with the same
hidden con   guration (i.e. code). the binary setting of the hidden units thus identi   es one region in input
space. for all x in one of these regions, p (h|x) is maximal for the corresponding h con   guration. note that
not all con   gurations of the hidden units correspond to a non-empty region in input space. as illustrated in
figure 5, this representation is similar to what an ensemble of 2-leaf trees would create.

the sum over the exponential number of possible hidden-layer con   gurations of an rbm can also be
seen as a particularly interesting form of mixture, with an exponential number of components (with respect
to the number of hidden units and of parameters):

p (x) =xh

p (x|h)p (h)

(35)

where p (x|h) is the model associated with the component indexed by con   guration h. for example, if
p (x|h) is chosen to be gaussian (see welling et al. (2005), bengio et al. (2007)), this is a gaussian mixture
with 2n components when h has n bits. of course, these 2n components cannot be tuned independently
because they depend on shared parameters (the rbm parameters), and that is also the strength of the model,
since it can generalize to con   gurations (regions of input space) for which no training example was seen. we
can see that the gaussian mean (in the gaussian case) associated with component h is obtained as a linear
combination b + w    h, i.e., each hidden unit bit hi contributes (or not) a vector wi in the mean.

5.3.1 id150 in rbms

sampling from an rbm is useful for several reasons. first of all it is useful in learning algorithms, to obtain
an estimator of the log-likelihood gradient. second, inspection of examples generated from the model is
useful to get an idea of what the model has captured or not captured about the data distribution. since the
joint distribution of the top two layers of a dbn is an rbm, sampling from an rbm enables us to sample
from a dbn, as elaborated in section 6.1.

id150 in fully connected id82s is slow because there are as many sub-steps in
the gibbs chain as there are units in the network. on the other hand, the factorization enjoyed by rbms
brings two bene   ts:    rst we do not need to sample in the positive phase because the free energy (and therefore
its gradient) is computed analytically; second, the set of variables in (x, h) can be sampled in two sub-steps
in each step of the gibbs chain. first we sample h given x, and then a new x given h. in general product
of experts models, an alternative to id150 is hybrid monte-carlo (duane, kennedy, pendleton,
& roweth, 1987; neal, 1994), an mcmc method involving a number of free-energy gradient computation
sub-steps for each step of the markov chain. the rbm structure is therefore a special case of product of
e(ci+wi x)hi in eq. 31 corresponds to an expert, i.e., there is one expert
per hidden neuron and one for the input offset. with that special structure, a very ef   cient id150
can be performed. for k gibbs steps, starting from a training example (i.e. sampling from   p ):

experts model: the i-th term logphi

x1       p (x)
h1     p (h|x1)
x2     p (x|h1)
h2     p (h|x2)

. . .

xk+1     p (x|hk).

(36)

it makes sense to start the chain from a training example because as the model becomes better at capturing
the structure in the training data, the model distribution p and the training distribution   p become more

32

similar (having similar statistics). note that if we started the chain from p itself, it would have converged in
one step, so starting from   p is a good way to ensure that only a few steps are necessary for convergence.

algorithm 1
rbmupdate(x1,   , w, b, c)
this is the rbm update procedure for binomial units. it can easily adapted to other types of units.
x1 is a sample from the training distribution for the rbm
   is a learning rate for the stochastic id119 in contrastive divergence
w is the rbm weight matrix, of dimension (number of hidden units, number of inputs)
b is the rbm offset vector for input units
c is the rbm offset vector for hidden units
notation: q(h2   = 1|x2) is the vector with elements q(h2i = 1|x2)

for all hidden units i do

end for
for all visible units j do

    sample h1i     {0, 1} from q(h1i|x1)

    compute q(h1i = 1|x1) (for binomial units, sigm(ci +pj wij x1j))
    compute p (x2j = 1|h1) (for binomial units, sigm(bj +pi wij h1i))
    compute q(h2i = 1|x2) (for binomial units, sigm(ci +pj wij x2j))

    sample x2j     {0, 1} from p (x2j = 1|h1)

end for
for all hidden units i do

end for
    w     w +   (h1x   
    b     b +   (x1     x2)
    c     c +   (h1     q(h2   = 1|x2))

1     q(h2   = 1|x2)x   
2)

5.4 contrastive divergence

contrastive divergence is an approximation of the log-likelihood gradient that has been found to be a suc-
cessful update rule for training rbms (carreira-perpi  nan & hinton, 2005). a pseudo-code is shown in
algorithm 1, with the particular equations for the conditional distributions for the case of binary input and
hidden units.

5.4.1 justifying contrastive divergence

to obtain this algorithm, the    rst approximation we are going to make is replace the average over all
possible inputs (in the second term of eq. 20) by a single sample. since we update the parameters often (e.g.,
with stochastic or mini-batch gradient updates after one or a few training examples), there is already some
averaging going on across updates (which we know to work well (lecun, bottou, orr, & m  uller, 1998)),
and the extra variance introduced by taking one or a few mcmc samples instead of doing the complete sum
might be partially canceled in the process of online gradient updates, over consecutive parameter updates.
we introduce additional variance with this approximation of the gradient, but it does not hurt much if it is
comparable or smaller than the variance due to online id119.

running a long mcmc chain is still very expensive. the idea of k-step contrastive divergence (cd-
k) (hinton, 1999, 2002) is simple, and involves a second approximation, which introduces some bias in the
gradient: run the mcmc chain x1, x2, . . . xk+1 for only k steps starting from the observed example x1 = x.

33

the cd-k update (i.e., not the log-likelihood gradient) after seeing example x is therefore

         

   freeenergy(x)

     

   

   freeenergy(  x)

     

(37)

where   x = xk+1 is the last sample from our markov chain, obtained after k steps. we know that when
k        , the bias goes away. we also know that when the model distribution is very close to the empirical
distribution, i.e., p       p , then when we start the chain from x (a sample from   p ) the mcmc has already
converged, and we need only one step to obtain an unbiased sample from p (although it would still be
correlated with x).

the surprising empirical result is that even k = 1 (cd-1) often gives good results. an extensive numer-
ical comparison of training with cd-k versus exact log-likelihood gradient has been presented in carreira-
perpi  nan and hinton (2005). in these experiments, taking k larger than 1 gives more precise results, although
very good approximations of the solution can be obtained even with k = 1. theoretical results (bengio &
delalleau, 2009) discussed below in section 5.4.3 help to understand why small values of k can work: cd-k
corresponds to keeping the    rst k terms of a series that converges to the log-likelihood gradient.

one way to interpret contrastive divergence is that it is approximating the log-likelihood gradient locally
around the training point x1. the stochastic reconstruction   x = xk+1 (for cd-k) has a distribution (given
x1) which is in some sense centered around x1 and becomes more spread out around it as k increases, until
it becomes the model distribution. the cd-k update will decrease the free energy of the training point x1
(which would increase its likelihood if all the other free energies were kept constant), and increase the free
energy of   x, which is in the neighborhood of x1. note that   x is in the neighborhood of x1, but at the same
time more likely to be in regions of high id203 under the model (especially for k larger). as argued
by lecun et al. (2006), what is mostly needed from the training algorithm for an energy-based model is that
it makes the energy (free energy, here, to marginalize hidden variables) of observed inputs smaller, shoveling
   energy    elsewhere, and most importantly in areas of low energy. the contrastive divergence algorithm is
fueled by the contrast between the statistics collected when the input is a real training example and when
the input is a chain sample. as further argued in the next section, one can think of the unsupervised learning
problem as discovering a decision surface that can roughly separate the regions of high id203 (where
there are many observed training examples) from the rest. therefore we want to penalize the model when it
generates examples on the wrong side of that divide, and a good way to identify where that divide should be
moved is to compare training examples with samples from the model.

5.4.2 alternatives to contrastive divergence

an exciting recent development in the research on learning algorithms for rbms is use of a so-called
persistent mcmc for the negative phase (tieleman, 2008; salakhutdinov & hinton, 2009), following
an approach already introduced in neal (1992). the idea is simple: keep a background mcmc chain
. . . xt     ht     xt+1     ht+1 . . . to obtain the negative phase samples (which should be from the model).
instead of running a short chain as in cd-k, the approximation made is that we ignore the fact that param-
eters are changing as we move along the chain, i.e., we do not run a separate chain for each value of the
parameters (as in the traditional id82 learning algorithm). maybe because the parameters
move slowly, the approximation works very well, usually giving rise to better log-likelihood than cd-k (ex-
periments were against k = 1 and k = 10). the trade-off with cd-1 is that the variance is larger but the bias
is smaller. something interesting also happens (tieleman & hinton, 2009): the model systematically moves
away from the samples obtained in the negative phase, and this interacts with the chain itself, preventing it
from staying in the same region very long, substantially improving the mixing rate of the chain. this is a
very desirable and unforeseen effect, which helps to explore more quickly the space of rbm con   gurations.
another alternative to contrastive divergence is score matching (hyv  arinen, 2005, 2007b, 2007a), a
general approach to train energy-based models in which the energy can be computed tractably, but not
the id172 constant z. the score function of a density p(x) = q(x)/z is    =     log p(x)
, and we

   x

34

   x

exploit the fact that the score function of our model does not depend on its id172 constant, i.e.,
   =     log q(x)
. the basic idea is to match the score function of the model with the score function of the
empirical density. the average (under the empirical density) of the squared norm of the difference between
the two score functions can be written in terms of squares of the model score function and second derivatives
    2 log q(x)
. score matching has been shown to be locally consistent (hyv  arinen, 2005), i.e. converging if the
model family matches the data generating process, and it has been used for unsupervised models of image
and audio data (k  oster & hyv  arinen, 2007).

   x2

5.4.3 truncations of the log-likelihood gradient in gibbs-chain models

here we approach the contrastive divergence update rule from a different perspective, which gives rise to
possible generalizations of it and links it to the reconstruction error often used to monitor its performance
and that is used to optimize auto-encoders (eq. 9). the inspiration for this derivation comes from hinton
et al. (2006):    rst from the idea (explained in section 8.1) that the gibbs chain can be associated with an
in   nite directed graphical model (which here we associate with an expansion of the log-likelihood gradient),
and second that the convergence of the chain justi   es contrastive divergence (since the expected value of
eq. 37 becomes equivalent to eq. 19 when the chain sample   x comes from the model). in particular we are
interested in clarifying and understanding the bias in the contrastive divergence update rule, compared to
using the true (intractable) gradient of the log-likelihood.

consider a converging markov chain xt     ht     xt+1     . . . de   ned by conditional distributions
p (ht|xt) and p (xt+1|ht), with x1 sampled from the training data empirical distribution. the following
theorem, demonstrated by bengio and delalleau (2009), shows how one can expand the log-likelihood
gradient for any t     1.

theorem 5.1. consider the converging gibbs chain x1     h1     x2     h2 . . . starting at data point x1.
the log-likelihood gradient can be written

    log p (x1)

     

=    

   freeenergy(x1)

     

+ e(cid:20)    freeenergy(xt)

     

(cid:21) + e(cid:20)     log p (xt)

     

(cid:21)

(38)

and the    nal term converges to zero as t goes to in   nity.

since the    nal term becomes small as t increases, that justi   es truncating the chain to k steps in the

markov chain, using the approximation

    log p (x1)

     

       

   freeenergy(x1)

     

+ e(cid:20)    freeenergy(xk+1)

     

(cid:21)

which is exactly the cd-k update (eq. 37) when we replace the expectation with a single sample   x = xk+1.

this tells us that the bias of cd-k is eh     log p (xk+1)

     

i. experiments and theory support the idea that cd-k

yields better and faster convergence (in terms of number of iterations) than cd-(k     1), due to smaller bias
(though the computational overhead might not always be worth it). however, although experiments show
that the cd-k bias can indeed be large when k is small, empirically the update rule of cd-k still mostly
moves the model   s parameters in the same quadrant as log-likelihood gradient (bengio & delalleau, 2009).
this is in agreement with the good results can be obtained even with k = 1. an intuitive picture that may
help to understand the phenomenon is the following: when the input example x1 is used to initialize the
chain, even the    rst markov chain step (to x2) tends to be in the right direction compared to x1, i.e. roughly
going down the energy landscape from x1. since the gradient depends on the change between x2 and x1,
we tend to get the direction of the gradient right.

so cd-1 corresponds to truncating the chain after two samples (one from h1|x1, and one from x2|h1).
what about stopping after the    rst one (i.e. h1|x1)? it can be analyzed from the following log-likelihood

35

gradient expansion (bengio & delalleau, 2009):

    log p (x1)

     

= e(cid:20)     log p (x1|h1)

     

(cid:21)     e(cid:20)     log p (h1)

     

(cid:21) .

(39)

let us consider a mean-   eld approximation of the    rst expectation, in which instead of the average over

yielding:

all h1 con   gurations according to p (h1|x1) one replaces h1 by its average con   gurationbh1 = e[h1|x1],

(40)

e(cid:20)     log p (x1|h1)

     

(cid:21)    

    log p (x1|bh1)

     

.

if, as in cd, we then ignore the second expectation in eq. 39 (incurring an additional bias in the estimation
of the log-likelihood gradient), we then obtain the right-hand side of eq. 40 as an update direction, which is
minus the gradient of the reconstruction error,

    log p (x1|bh1)

typically used to train auto-encoders (see eq. 9 with c(x) = e[h|x])12.

so we have found that the truncation of the gibbs chain gives rise to    rst approximation (one sample) to
roughly reconstruction error (through a biased mean-   eld approximation), with slightly better approximation
(two samples) to cd-1 (approximating the expectation by a sample), and with more terms to cd-k (still
approximating expectations by samples). note that reconstruction error is deterministically computed and
is correlated with log-likelihood, which is why it has been used to track progress when training rbms with
cd.

5.4.4 model samples are negative examples

here we argue that training an energy-based model can be achieved by solving a series of classi   cation
problems in which one tries to discriminate training examples from samples generated by the model. in
the id82 learning algorithms, as well as in contrastive divergence, an important element
is the ability to sample from the model, maybe approximately. an elegant way to understand the value of
these samples in improving the log-likelihood was introduced in welling, zemel, and hinton (2003), using
a connection with boosting. we start by explaining the idea informally and then formalize it, justifying
algorithms based on training the generative model with a classi   cation criterion separating model samples
from training examples. the maximum likelihood criterion wants the likelihood to be high on the training
examples and low elsewhere. if we already have a model and we want to increase its likelihood, the contrast
between where the model puts high id203 (represented by samples) and where the training examples
are indicates how to change the model. if we were able to approximately separate training examples from
model samples with a decision surface, we could increase likelihood by reducing the value of the energy
function on one side of the decision surface (the side where there are more training examples) and increasing
it on the other side (the side where there are more samples from the model). mathematically, consider
the gradient of the log-likelihood with respect to the parameters of the freeenergy(x) (or energy(x) if
we do not introduce explicit hidden variables), given in eq. 20. now consider a highly regularized two-
class probabilistic classi   er that will attempt to separate training samples of   p (x) from model samples of
p (x), and which is only able to produce an output id203 q(x) = p (y = 1|x) barely different from 1
2
(hopefully on the right side more often than not). let q(x) = sigm(   a(x)), i.e.,    a(x) is the discriminant
function or an unnormalized conditional log-id203, just like the free energy. let   p denote the empirical
distribution over (x, y) pairs, and   pi the distribution over x when y = i. assume that   p (y = 1) =   p (y =
0) = 1
[f (x, 1)] +

[f (x, 0)]   p (y = 0) = 1

2 , so that    f, e   p [f (x, y)] = e   p1

[f (x, 1)]   p (y = 1) + e   p0

2 (e   p1

12it is debatable whether or not one would take into account the fact that bh1 depends on    when computing the gradient in the

mean-   eld approximation of eq. 40, but it must be the case to draw a direct link with auto-encoders.

36

[f (x, 0)]). using this, the average conditional log-likelihood gradient for this probabilistic classi   er is

e   p0
written

e   p(cid:20)     log p (y|x)

     

1

(cid:21) = e   p(cid:20)    (y log q(x) + (1     y) log(1     q(x)))
      (cid:21) + e   p0(cid:20)q(x)
      (cid:21) + e   p0(cid:20)    a(x)
      (cid:21)(cid:19)

2(cid:18)e   p1(cid:20)(q(x)     1)
4(cid:18)   e   p1(cid:20)    a(x)

   a(x)

     

=

   

1

(cid:21)
      (cid:21)(cid:19)

   a(x)

(41)

where the last equality is when the classi   er is highly regularized: when the output weights are small, a(x)
is close to 0 and q(x)     1
2 , so that (1     q(x))     q(x). this expression for the log-likelihood gradient
corresponds exactly to the one obtained for energy-based models where the likelihood is expressed in terms
of a free energy (eq. 20), when we interpret training examples from   p1 as positive examples (y = 1) (i.e.
  p1 =   p ) and model samples as negative examples (y = 0, i.e.   p0 = p ). the gradient is also similar in
structure to the contrastive divergence gradient estimator (eq. 37). one way to interpret this result is that if
we could improve a classi   er that separated training samples from model samples, we could improve the log-
likelihood of the model, by putting more id203 mass on the side of training samples. practically, this
could be achieved with a classi   er whose discriminant function was de   ned as the free energy of a generative
model (up to a multiplicative factor), and assuming one could obtain samples (possibly approximate) from
the model. a particular variant of this idea has been used to justify a boosting-like incremental algorithm for
adding experts in products of experts (welling et al., 2003).

6 greedy layer-wise training of deep architectures

6.1 layer-wise training of id50

a deep belief network (hinton et al., 2006) with     layers models the joint distribution between observed
vector x and     hidden layers hk as follows:

p (x, h1, . . . , h   ) =       2yk=0

p (hk|hk+1)! p (h      1, h   )

(42)

where x = h0, p (hk   1|hk) is a visible-given-hidden conditional distribution in an rbm associated with
level k of the dbn, and p (h      1, h   ) is the joint distribution in the top-level rbm. this is illustrated in
figure 11.

the conditional distributions p (hk|hk+1) and the top-level joint (an rbm) p (h      1, h   ) de   ne the gen-
erative model. in the following we introduce the letter q for exact or approximate posteriors of that model,
which are used for id136 and training. the q posteriors are all approximate except for the top level
q(h   |h      1) which is equal to the true p (h   |h      1) because (h   , h      1) form an rbm, where exact id136
is possible.

when we train the dbn in a greedy layer-wise fashion, as illustrated with the pseudo-code of algo-
rithm 2, each layer is initialized as an rbm, and we denote q(hk, hk   1) the k-th rbm trained in this way,
whereas p (. . .) denotes probabilities according to the dbn. we will use q(hk|hk   1) as an approximation
of p (hk|hk   1), because it is easy to compute and sample from q(hk|hk   1) (which factorizes), and not
from p (hk|hk   1) (which does not). these q(hk|hk   1) can also be used to construct a representation of
the input vector x. to obtain an approximate posterior or representation for all the levels, we use the fol-
lowing procedure. first sample h1     q(h1|x) from the    rst-level rbm, or alternatively with a mean-   eld

q(h1|x). this is just the vector of output probabilities of the hidden units, in the common case where they

approach usebh1 = e[h1|x] instead of a sample of h1, where the expectation is over the rbm distribution

37

algorithm 2

train a dbn in a purely unsupervised way, with the greedy layer-wise procedure in which each added layer
is trained as an rbm (e.g. by contrastive divergence).

trainunsuperviseddbn(bp,   ,    , w, b, c, mean field computation)
bp is the input training distribution for the network

   is a learning rate for the rbm training
    is the number of layers to train
w k is the weight matrix for level k, for k from 1 to    
bk is the visible units offset vector for rbm at level k, for k from 1 to    
ck is the hidden units offset vector for rbm at level k, for k from 1 to    
mean field computation is a boolean that is true iff training data at each additional level is obtained
by a mean-   eld approximation instead of stochastic sampling

for k = 1 to     do

    initialize w k = 0, bk = 0, ck = 0
while not stopping criterion do

for i = 1 to k     1 do

    sample h0 = x from bp

if mean field computation then

    assign hi

j to q(hi

j = 1|hi   1), for all elements j of hi

else

    sample hi

j from q(hi

j|hi   1), for all elements j of hi

end if

end for
    rbmupdate(hk   1,   , w k, bk, ck) {thus providing q(hk|hk   1) for future use}

end while

end for

38

2

3

p(h , h  ) ~ rbm

...

...

2

q(h | h  )1

1

2
p(h | h  )

1

q(h | x  )

...

p(x | h  )1
...

3

h

h2

h1

x

figure 11: deep belief network as a generative model (generative path with p distributions, full arcs) and a
means to extract multiple levels of representation of the input (recognition path with q distributions, dashed
arcs). the top two layers h2 and h3 form an rbm (for their joint distribution). the lower layers form a
directed graphical model (sigmoid belief net h2     h1     x) and the prior for the penultimate layer h2 is
provided by the top-level rbm. q(hk+1|hk) approximates p (hk+1|hk) but can be computed easily.

i = sigm(b1 + w 1
i

are binomial units: bh1
x). taking either the mean-   eld vectorbh1 or the sample h1 as
input for the second-level rbm, computebh2 or a sample h2, etc. until the last layer. once a dbn is trained

as per algorithm 2, the parameters w i (rbm weights) and ci (rbm hidden unit offsets) for each layer can
be used to initialize a deep multi-layer neural network. these parameters can then be    ne-tuned with respect
to another criterion (typically a supervised learning criterion).

a sample of the dbn generative model for x can be obtained as follows:

1. sample a visible vector h      1 from the top-level rbm. this can be achieved approximately by running
a gibbs chain in that rbm alternating between h        p (h   |h      1) and h      1     p (h      1|h   ), as
outlined in section 5.3.1. by starting the chain from a representation h      1 obtained from a training
set example (through the q   s as above), fewer gibbs steps might be required.

2. for k =         1 down to 1, sample hk   1 given hk according to the level-k hidden-to-visible conditional

distribution p (hk   1|hk).

3. x = h0 is the dbn sample.

6.2 training stacked auto-encoders

auto-encoders have been used as building blocks to build and initialize a deep multi-layer neural net-
work (bengio et al., 2007; ranzato et al., 2007; larochelle et al., 2007; vincent et al., 2008). the training
procedure is similar to the one for id50:

1. train the    rst layer as an auto-encoder to minimize some form of reconstruction error of the raw input.

this is purely unsupervised.

2. the hidden units    outputs (i.e. the codes) of the auto-encoder are now used as input for another layer,

also trained to be an auto-encoder. again, we only need unlabeled examples.

39

3. iterate as in (2) to initialize the desired number of additional layers.

4. take the last hidden layer output as input to a supervised layer and initialize its parameters (either

randomly or by supervised training, keeping the rest of the network    xed).

5. fine-tune all the parameters of this deep architecture with respect to the supervised criterion. alter-
nately, unfold all the auto-encoders into a very deep auto-encoder and    ne-tune the global reconstruc-
tion error, as in hinton and salakhutdinov (2006b).

the hope is that the unsupervised pre-training in this greedy layer-wise fashion has put the parameters of
all the layers in a region of parameter space from which a good13 local optimum can be reached by local
descent. this indeed appears to happen in a number of tasks (bengio et al., 2007; ranzato et al., 2007;
larochelle et al., 2007; vincent et al., 2008).

the principle is exactly the same as the one previously proposed for training dbns, but using auto-
encoders instead of rbms. comparative experimental results suggest that id50 typically
have an edge over stacked auto-encoders (bengio et al., 2007; larochelle et al., 2007; vincent et al.,
2008). this may be because cd-k is closer to the log-likelihood gradient than the reconstruction error
gradient. however, since the reconstruction error gradient has less variance than cd-k (because no sampling
is involved), it might be interesting to combine the two criteria, at least in the initial phases of learning. note
also that the dbn advantage disappeared in experiments where the ordinary auto-encoder was replaced by
a denoising auto-encoder (vincent et al., 2008), which is stochastic (see section 7.2).

an advantage of using auto-encoders instead of rbms as the unsupervised building block of a deep
architecture is that almost any parametrization of the layers is possible, as long as the training criterion
is continuous in the parameters. on the other hand, the class of probabilistic models for which cd or
other known tractable estimators of the log-likelihood gradient can be applied is currently more limited. a
disadvantage of stacked auto-encoders is that they do not correspond to a generative model: with generative
models such as rbms and dbns, samples can be drawn to check qualitatively what has been learned, e.g.,
by visualizing the images or word sequences that the model sees as plausible.

6.3 semi-supervised and partially supervised training

with dbns and stacked auto-encoders two kinds of training signals are available, and can be combined:
the local layer-wise unsupervised training signal (from the rbm or auto-encoder associated with the layer),
and a global supervised training signal (from the deep multi-layer network sharing the same parameters
as the dbn or stacked auto-encoder). in the algorithms presented above, the two training signals are
used in sequence:    rst an unsupervised training phase, and second a supervised    ne-tuning phase. other
combinations are possible.

one possibility is to combine both signals during training, and this is called partially supervised training
in bengio et al. (2007). it has been found useful (bengio et al., 2007) when the true input distribution p (x)
is believed to be not strongly related to p (y |x). to make sure that an rbm preserves information relevant
to y in its hidden representation, the cd update is combined with the classi   cation log-id203 gradient,
and for some distributions better predictions are thus obtained.

an appealing generalization of semi-supervised learning, especially in the context of deep architectures,
is self-taught learning (lee, battle, raina, & ng, 2007; raina et al., 2007), in which the unlabeled examples
potentially come from classes other than the labeled classes. this is more realistic than the standard semi-
supervised setting, e.g., even if we are only interested in some speci   c object classes, one can much more
easily obtain unlabeled examples of arbitrary objects from the web (whereas it would be expensive to select
only those pertaining to those selected classes of interest).

13good at least in the sense of generalization.

40

7 variants of rbms and auto-encoders

we review here some of the variations that have been proposed on the basic rbm and auto-encoder models
to extend and improve them.

we have already mentioned that it is straightforward to generalize the conditional distributions associated
with visible or hidden units in rbms, e.g., to any member of the exponential family (welling et al., 2005).
gaussian units and exponential or truncated exponential units have been proposed or used in freund and
haussler (1994), welling et al. (2003), bengio et al. (2007), larochelle et al. (2007). with respect to the
analysis presented here, the equations can be easily adapted by simply changing the domain of the sum
(or integral) for the hi and xi. diagonal quadratic terms (e.g., to yield gaussian or truncated gaussian
distributions) can also be added in the energy function without losing the property that the free energy
factorizes.

7.1 sparse representations in auto-encoders and rbms

sparsity has become a concept of great interest recently, not only in machine learning but also in statistics and
signal processing, in particular with the work on compressed sensing (candes & tao, 2005; donoho, 2006),
but it was introduced earlier in computational neuroscience in the context of sparse coding in the visual
system (olshausen & field, 1997), and has been a key element deep convolutional networks exploiting
of a variant of auto-encoders (ranzato et al., 2007, 2007; ranzato & lecun, 2007; ranzato et al., 2008;
mairal et al., 2009) with a sparse distributed representation, and has become a key ingredient in deep belief
networks (lee et al., 2008).

7.1.1 why a sparse representation?

we argue here that if one is going to have    xed-size representations, then sparse representations are more
ef   cient (than non-sparse ones) in an information-theoretic sense, allowing for varying the effective number
of bits per example. according to learning theory (vapnik, 1995; li & vitanyi, 1997), to obtain good
generalization it is enough that the total number of bits needed to encode the whole training set be small,
compared to the size of the training set. in many domains of interest different examples require different
number of bits when compressed.

on the other hand, id84 algorithms, whether linear such as pca and ica, or non-
linear such as lle and isomap, map each example to the same low-dimensional space. in light of the above
argument, it would be more ef   cient to map each example to a variable-length representation. to simplify
the argument, assume this representation is a binary vector. if we are required to map each example to a
   xed-length representation, a good solution would be to choose that representation to have enough degrees
of freedom to represent the vast majority of the examples, while at the same allowing to compress that    xed-
length bit vector to a smaller variable-size code for most of the examples. we now have two representations:
the    xed-length one, which we might use as input to make predictions and make decisions, and a smaller,
variable-size one, which can in principle be obtained from the    xed-length one through a compression step.
for example, if the bits in our    xed-length representation vector have a high id203 of being 0 (i.e. a
sparsity condition), then for most examples it is easy to compress the    xed-length vector (in average by the
amount of sparsity). for a given level of sparsity, the number of con   gurations of sparse vectors is much
smaller than when less sparsity (or none at all) is imposed, so the id178 of sparser codes is smaller.

another argument in favor of sparsity is that the    xed-length representation is going to be used as input
for further processing, so that it should be easy to interpret. a highly compressed encoding is usually highly
entangled, so that no subset of bits in the code can really be interpreted unless all the other bits are taken into
account. instead, we would like our    xed-length sparse representation to have the property that individual
bits or small subsets of these bits can be interpreted, i.e., correspond to meaningful aspects of the input, and
capture factors of variation in the data. for example, with a speech signal as input, if some bits encode the
speaker characteristics and other bits encode generic features of the phoneme being pronounced, we have

41

disentangled some of the factors of variation in the data, and some subset of the factors might be suf   cient
for some particular prediction tasks.

another way to justify sparsity of the representation was proposed in ranzato et al. (2008), in the con-
text of models based on auto-encoders. this view actually explains how one might get good models even
though the partition function is not explicitly minimized, or only minimized approximately, as long as other
constraints (such as sparsity) are used on the learned representation. suppose that the representation learned
by an auto-encoder is sparse, then the auto-encoder cannot reconstruct well every possible input pattern, be-
cause the number of sparse con   gurations is necessarily smaller than the number of dense con   gurations. to
minimize the average reconstruction error on the training set, the auto-encoder then has to    nd a representa-
tion which captures statistical regularities of the data distribution. first of all, ranzato et al. (2008) connect
the free energy with a form of reconstruction error (when one replaces summing over hidden unit con   gu-
rations by maximizing over them). minimizing reconstruction error on the training set therefore amounts to
minimizing free energy, i.e., maximizing the numerator of an energy-based model likelihood (eq. 17). since
the denominator (the partition function) is just a sum of the numerator over all possible input con   gurations,
maximizing likelihood roughly amounts to making reconstruction error high for most possible input con   g-
urations, while making it low for those in the training set. this can be achieved if the encoder (which maps
an input to its representation) is constrained in such a way that it cannot represent well most of the possible
input patterns (i.e., the reconstruction error must be high for most of the possible input con   gurations). note
how this is already achieved when the code is much smaller than the input. another approach is to impose a
sparsity penalty on the representation (ranzato et al., 2008), which can be incorporated in the training crite-
rion. in this way, the term of the log-likelihood gradient associated with the partition function is completely
avoided, and replaced by a sparsity penalty on the hidden unit code. interestingly, this idea could potentially
be used to improve cd-k rbm training, which only uses an approximate estimator of the gradient of the
log of the partition function. if we add a sparsity penalty to the hidden representation, we may compensate
for the weaknesses of that approximation, by making sure we increase the free energy of most possible input
con   gurations, and not only of the reconstructed neighbors of the input example that are obtained in the
negative phase of contrastive divergence.

7.1.2 sparse auto-encoders and sparse coding

there are many ways to enforce some form of sparsity on the hidden layer representation. the    rst success-
ful deep architectures exploiting sparsity of representation involved auto-encoders (ranzato et al., 2007).
sparsity was achieved with a so-called sparsifying logistic, by which the codes are obtained with a nearly
saturating logistic whose offset is adapted to maintain a low average number of times the code is signi   cantly
non-zero. one year later the same group introduced a somewhat simpler variant (ranzato et al., 2008) based
on a student-t prior on the codes. the student-t prior has been used in the past to obtain sparsity of the map
estimates of the codes generating an input (olshausen & field, 1997) in computational neuroscience models
of the v1 visual cortex area. another approach also connected to computational neuroscience involves two
levels of sparse rbms (lee et al., 2008). sparsity is achieved with a id173 term that penalizes a
deviation of the expected activation of the hidden units from a    xed low level. whereas olshausen and field
(1997) had already shown that one level of sparse coding of images led to    lters very similar to those seen
in v1, lee et al. (2008)    nd that when training a sparse deep belief network (i.e. two sparse rbms on top
of each other), the second level appears to learn to detect visual features similar to those observed in area
v2 of visual cortex (i.e., the area that follows area v1 in the main chain of processing of the visual cortex of
primates).

in the compressed sensing literature sparsity is achieved with the    1 penalty on the codes, i.e., given
bases in matrix w (each column of w is a basis) we typically look for codes h such that the input signal x
is reconstructed with low    2 reconstruction error while h is sparse:

min

h

||x     w h||2

2 +   ||h||1

(43)

42

where ||h||1 =pi |hi|. the actual number of non-zero components of h would be given by the    0 norm,

but minimizing with it is combinatorially dif   cult, and the    1 norm is the closest p-norm that is also convex,
making the overall minimization in eq. 43 convex. as is now well understood (candes & tao, 2005; donoho,
2006), the    1 norm is a very good proxy for the    0 norm and naturally induces sparse results, and it can even
be shown to recover exactly the true sparse code (if there is one), under mild conditions. note that the    1
penalty corresponds to a laplace prior, and that the posterior does not have a point mass at 0, but because
of the above properties, the mode of the posterior (which is recovered when minimizing eq. 43) is often at
0. although minimizing eq. 43 is convex, minimizing jointly the codes and the decoder bases w is not
convex, but has been done successfully with many different algorithms (olshausen & field, 1997; lewicki
& sejnowski, 2000; doi et al., 2006; grosse, raina, kwong, & ng, 2007; raina et al., 2007; mairal et al.,
2009).

like directed id114 (such as the sigmoid belief networks discussed in section 4.4), sparse
coding performs a kind of explaining away: it chooses one con   guration (among many) of the hidden codes
that could explain the input. these different con   gurations compete, and when one is selected, the others
are completely turned off. this can be seen both as an advantage and as a disadvantage. the advantage
is that if a cause is much more probable than the other, than it is the one that we want to highlight. the
disadvantage is that it makes the resulting codes somewhat unstable, in the sense that small perturbations of
the input x could give rise to very different values of the optimal code h. this instability could spell trouble
for higher levels of learned transformations or a trained classi   er that would take h as input. indeed it could
make generalization more dif   cult if very similar inputs can end up being represented very differently in
the sparse code layer. there is also a computational weakness of these approaches that some authors have
tried to address. even though optimizing eq. 43 is ef   cient it can be hundreds of time slower than the kind
of computation involved in computing the codes in ordinary auto-encoders or rbms, making both training
and recognition very slow. another issue connected to the stability question is the joint optimization of the
bases w with higher levels of a deep architecture. this is particularly important in view of the objective of
   ne-tuning the encoding so that it focuses on the most discriminant aspects of the signal. as discussed in
section 9.1.2, signi   cant classi   cation error improvements were obtained when    ne-tuning all the levels of a
deep architecture with respect to a discriminant criterion of interest. in principle one can compute gradients
through the optimization of the codes, but if the result of the optimization is unstable, the gradient may not
exist or be numerically unreliable. to address both the stability issue and the above    ne-tuning issue, bagnell
and bradley (2009) propose to replace the    1 penalty by a softer approximation which only gives rise to
approximately sparse coef   cients (i.e., many very small coef   cients, without actually converging to 0).

keep in mind that sparse auto-encoders and sparse rbms do not suffer from any of these sparse coding
issues: computational complexity (of inferring the codes), stability of the inferred codes, and numerical
stability and computational cost of computing gradients on the    rst layer in the context of global    ne-
tuning of a deep architecture. sparse coding systems only parametrize the decoder: the encoder is de   ned
implicitly as the solution of an optimization. instead, an ordinary auto-encoder or an rbm has an encoder
part (computing p (h|x)) and a decoder part (computing p (x|h)). a middle ground between ordinary auto-
encoders and sparse coding is proposed in a series of papers on sparse auto-encoders (ranzato et al., 2007,
2007; ranzato & lecun, 2007; ranzato et al., 2008) applied in pattern recognition and machine vision tasks.
they propose to let the codes h be free (as in sparse coding algorithms), but include a parametric encoder (as
in ordinary auto-encoders and rbms) and a penalty for the difference between the free non-parametric codes
h and the outputs of the parametric encoder. in this way, the optimized codes h try to satisfy two objectives:
reconstruct well the input (like in sparse coding), while not being too far from the output of the encoder
(which is stable by construction, because of the simple parametrization of the encoder). in the experiments
performed, the encoder is just an af   ne transformation followed by a non-linearity like the sigmoid, and
the decoder is linear as in sparse coding. experiments show that the resulting codes work very well in the
context of a deep architecture (with supervised    ne-tuning) (ranzato et al., 2008), and are more stable (e.g.
with respect to slight perturbations of input images) than codes obtained by sparse coding (kavukcuoglu,
ranzato, & lecun, 2008).

43

7.2 denoising auto-encoders

the denoising auto-encoder (vincent et al., 2008) is a stochastic version of the auto-encoder where the input
is stochastically corrupted, but the uncorrupted input is still used as target for the reconstruction. intuitively,
a denoising auto-encoder does two things: try to encode the input (preserve the information about the input),
and try to undo the effect of a corruption process stochastically applied to the input of the auto-encoder. the
latter can only be done by capturing the statistical dependencies between the inputs. in fact, in vincent et al.
(2008), the stochastic corruption process consists in randomly setting some of the inputs (as many as half of
them) to zero. hence the denoising auto-encoder is trying to predict the missing values from the non-missing
values, for randomly selected subsets of missing patterns. the training criterion for denoising auto-encoders
is expressed as a reconstruction log-likelihood,

    log p (x|c(  x))

(44)

where x is the uncorrupted input,   x is the stochastically corrupted input, and c(  x) is the code obtained from
  x. hence the output of the decoder is viewed as the parameter for the above distribution (over the uncorrupted
input). in the experiments performed (vincent et al., 2008), this distribution is factorized and binomial (one
bit per pixel), and input pixel intensities are interpreted as probabilities. note that a recurrent version of the
denoising auto-encoder had been proposed earlier by seung (1998), with corruption also corresponding to a
form of occlusion (setting a rectangular region of the input image to 0). using auto-encoders for denoising
was actually introduced much earlier (lecun, 1987; gallinari, lecun, thiria, & fogelman-soulie, 1987).
the main innovation in vincent et al. (2008) is therefore to show how this strategy is highly successful as
unsupervised pre-training for a deep architecture, and to link the denoising auto-encoder to a generative
model.

consider a random d-dimensional vector x, s a set of k indices, xs = (xs1 , . . . xsk ) the sub-elements
selected by s, and let x   s all the sub-elements except those in s. note that the set of conditional distribu-
tions p (xs|x   s) for some choices of s fully characterize the joint distribution p (x), and this is exploited
for example in id150. note that bad things can happen when |s| = 1 and some pairs of input
are perfectly correlated: the predictions can be perfect even though the joint has not really been captured,
and this would correspond to a gibbs chain that does not mix, i.e., does not converge. by considering
random-size subsets and also insisting on reconstructing everything (like ordinary auto-encoders), this type
of problem may be avoided in denoising auto-encoders.

interestingly, in a series of experimental comparisons over 8 vision tasks, stacking denoising auto-
encoders into a deep architecture    ne-tuned with respect to a supervised criterion yielded generalization
performance that was systematically better than stacking ordinary auto-encoders, and comparable or supe-
rior to id50 (vincent et al., 2008).

an interesting property of the denoising auto-encoder is that it can be shown to correspond to a genera-
tive model. its training criterion is a bound on the log-likelihood of that generative model. several possible
generative models are discussed in (vincent et al., 2008). a simple generative model is semi-parametric:
sample a training example, corrupt it stochastically, apply the encoder function to obtain the hidden repre-
sentation, apply the decoder function to it (obtaining parameters for a distribution over inputs), and sample
an input. this is not very satisfying because it requires to keep the training set around (like non-parametric
density models). other possible generative models are explored in vincent et al. (2008).

another interesting property of the denoising auto-encoder is that it naturally lends itself to data with
missing values or multi-modal data (when a subset of the modalities may be available for any particular
example). this is because it is trained with inputs that have    missing    parts (when corruption consists in
randomly hiding some of the input values).

7.3 lateral connections

the rbm can be made slightly less restricted by introducing interaction terms or    lateral connections    be-
tween visible units. sampling h from p (h|x) is still easy but sampling x from p (x|h) is now generally

44

more dif   cult, and amounts to sampling from a markov random field which is also a fully observed boltz-
mann machine, in which the offsets are dependent on the value of h. osindero and hinton (2008) propose
such a model for capturing image statistics and their results suggest that deep belief nets (dbns) based on
such modules generate more realistic image patches than dbns based on ordinary rbms. their results also
show that the resulting distribution has marginal and pairwise statistics for pixel intensities that are similar
to those observed on real image patches.

these lateral connections capture pairwise dependencies that can be more easily captured this way than
using hidden units, saving the hidden units for capturing higher-order dependencies. in the case of the    rst
layer, it can be seen that this amounts to a form of whitening, which has been found useful as a preprocessing
step in image processing systems (olshausen & field, 1997). the idea proposed by osindero and hinton
(2008) is to use lateral connections at all levels of a dbn (which can now be seen as a hierarchy of markov
random    elds). the generic advantage of this type of approach would be that the higher level factors rep-
resented by the hidden units do not have to encode all the local    details    that the lateral connections at the
levels below can capture. for example, when generating an image of a face, the approximate locations of the
mouth and nose might be speci   ed at a high level whereas their precise location could be selected in order
to satisfy the pairwise preferences encoded in the lateral connections at a lower level. this appears to yield
generated images with sharper edges and generally more accuracy in the relative locations of parts, without
having to expand a large number of higher-level units.

in order to sample from p (x|h), we can start a markov chain at the current example (which presumably
already has pixel co-dependencies similar to those represented by the model, so that convergence should be
quick) and only run a short chain on the x   s (keeping h    xed). denote u the square matrix of visible-to-
visible connections, as per the general id82 energy function in eq. 25. to reduce sampling
variance in cd for this model, osindero and hinton (2008) used    ve damped mean-   eld steps instead of an
ordinary gibbs chain on the x   s: xt =   xt   1 + (1       )sigm(b + u xt   1 + w    h), with        (0, 1).

7.4 conditional rbms and temporal rbms

a conditional rbm is an rbm where some of the parameters are not free but are instead parametrized func-
tions of a conditioning random variable. for example, consider an rbm for the joint distribution p (x, h)
between observed vector x and hidden vector h, with parameters (b, c, w ) as per eq. 25, respectively for in-
put offsets b, hidden units offsets c, and the weight matrix w . this idea has been introduced by taylor et al.
(2007), taylor and hinton (2009) for context-dependent rbms in which the hidden units offsets c are af   ne
functions of a context variable z. hence the rbm represents p (x, h|z) or, marginalizing over h, p (x|z). in
general the parameters    = (b, c, w ) of an rbm can be written as a parametrized function    = f (z;   ), i.e.,
the actual free parameters of the conditional rbm with conditioning variable z are denoted   . generalizing
rbms to conditional rbms allows building deep architectures in which the hidden variables at each level
can be conditioned on the value of other variables (typically representing some form of context).

the contrastive divergence algorithm for rbms can be easily generalized to the case of conditional
rbms. the cd gradient estimator       on a parameter    can be simply back-propagated to obtain a gradient
estimator on   :

      =      

.

(45)

     
     

in the af   ne case c =    + m z (with c,    and z column vectors and m a matrix) studied by taylor et al.
(2007), the cd update on the conditional parameters is simply

      =    c
   m =    c z   

(46)

where the last multiplication is an outer product (applying the chain rule on derivatives), and    c is the update
given by cd-k on hidden units offsets.

45

ht   2

ht   1

ht

xt   2

xt   1

xt

figure 12: example of temporal rbm for modeling sequential data, including dependencies between the
hidden states. the double-arrow full arcs indicate an undirected connection, i.e. an rbm. the single-arrow
dotted arcs indicate conditional dependency: the (xt, ht) rbm is conditioned by the values of the past inputs
and past hidden state vectors.

this idea has been successfully applied to model conditional distributions p (xt|xt   1, xt   2, xt   3) in
sequential data of human motion (taylor et al., 2007), where xt is a vector of joint angles and other ge-
ometric features computed from motion capture data of human movements such as walking and running.
interestingly, this allows generating realistic human motion sequences, by successively sampling the t-th
frame given the previously sampled k frames, i.e. approximating

p (x1, x2, . . . , xt )     p (x1, . . . xk)

tyt=k+1

p (xt|xt   1, . . . xt   k).

(47)

the initial frames can be generated by using special null values as context or using a separate model for
p (x1, . . . xk).

as demonstrated by memisevic and hinton (2007), it can be useful to make not just the offsets but also
the weights conditional on a context variable. in that case we greatly increase the number of degrees of
freedom, introducing the capability to model three-way interactions between an input unit xi, a hidden unit
hj, and a context unit zk through interaction parameters   ijk. this approach has been used with x an image
and z the previous image in a video, and the model learns to capture    ow    elds (memisevic & hinton, 2007).
probabilistic models of sequential data with hidden variables ht (called state) can gain a lot by capturing
the temporal dependencies between the hidden states at different times t in the sequence. this is what
allows id48 (id48s) (rabiner & juang, 1986) to capture dependencies in a long observed
sequence x1, x2, . . . even if the model only considers the hidden state sequence h1, h2, . . . to be a markov
chain of order 1 (where the direct dependence is only between ht and ht+1). whereas the hidden state
representation ht in id48s is local (all the possible values of ht are enumerated and speci   c parameters
associated with each of these values), temporal rbms have been proposed (sutskever & hinton, 2007) to
construct a distributed representation of the state. the idea is an extension of the conditional rbm presented
above, but where the context includes not only past inputs but also past values of the state, e.g., we build a
model of

p (ht, xt|ht   1, xt   1, . . . , ht   k, xt   k)

(48)

where the context is zt = (ht   1, xt   1, . . . , ht   k, xt   k), as illustrated in figure 12. although sampling
of sequences generated by temporal rbms can be done as in conditional rbms (with the same mcmc
approximation used to sample from rbms, at each time step), exact id136 of the hidden state sequence
given an input sequence is no longer tractable. instead, sutskever and hinton (2007) propose to use a
mean-   eld    ltering approximation of the hidden sequence posterior.

46

7.5 factored rbms

in several probabilistic language models, it has been proposed to learn a distributed representation of each
word (deerwester, dumais, furnas, landauer, & harshman, 1990; miikkulainen & dyer, 1991; bengio
et al., 2001, 2003; schwenk & gauvain, 2002; xu et al., 2003; schwenk, 2004; schwenk & gauvain, 2005;
collobert & weston, 2008; mnih & hinton, 2009). for an rbm that models a sequence of words, it would
be convenient to have a parametrization that leads to automatically learning a distributed representation
for each word in the vocabulary. this is essentially what mnih and hinton (2007) propose. consider an
rbm input x that is the concatenation of one-hot vectors vt for each word wt in a    xed-size sequence
k)   . mnih
(w1, w2, . . . , wk), i.e., vt contains all 0   s except for a 1 at position wt, and x = (v   
and hinton (2007) use a factorization of the rbm weight matrix w into two factors, one that depends on
the location t in the input subsequence, and one that does not. consider the computation of the hidden
units    probabilities given the input subsequence (v1, v2, . . . , vk). instead of applying directly a matrix w
to x, do the following. first, each word symbol wt is mapped through a matrix r to a d-dimensional vector
.,wk )    are multiplied
r.,wt = rvt, for t     {1 . . . k}; second, the concatenated vectors (r   
by a matrix b. hence w = bdiag(r), where diag(r) is a block-diagonal matrix    lled with r on the
diagonal. this model has produced id165s with better log-likelihood (mnih & hinton, 2007, 2009), with
further improvements in generalization performance when averaging predictions with state-of-the-art id165
models (mnih & hinton, 2007).

.,w2, . . . , r   

2, . . . , v   

.,w1, r   

1, v   

7.6 generalizing rbms and contrastive divergence

let us try to generalize the de   nition of rbm so as to include a large class of parametrizations for which
essentially the same ideas and learning algorithms (such as contrastive divergence) that we have discussed
above can be applied in a straightforward way. we generalize rbms as follows: a generalized rbm is an
energy-based probabilistic model with input vector x and hidden vector h whose energy function is such
that p (h|x) and p (x|h) both factorize. this de   nition can be formalized in terms of the parametrization of
the energy function, which is also proposed by hinton et al. (2006):

proposition 7.1. the energy function associated with a model of the form of eq. 15 such that p (h|x) =

qi p (hi|x) and p (x|h) =qj p (xj|h) must have the form
  j(xj ) +xi

energy(x, h) =xj

  i(hi) +xi,j

  i,j(hi, xj).

(49)

this is a direct application of the hammersley-clifford theorem (hammersley & clifford, 1971; clifford,
1990). hinton et al. (2006) also showed that the above form is a necessary and suf   cient condition to obtain
complementary priors. complementary priors allow the posterior distribution p (h|x) to factorize by a
proper choice of p (h).

in the case where the hidden and input values are binary, this new formulation does not actually bring
any additional power of representation. indeed,   i,j(hi, xj), which can take at most four different values
according to the 2    2 con   gurations of (hi, xj) could always be rewritten as a second order polynomial in
(hi, xj ): a + bxj + chi + dhixj. however, b and c can be folded into the offset terms and a into a global
additive constant which does not matter (because it gets cancelled by the partition function).

on the other hand, when x or h are real vectors, one could imagine higher-capacity modeling of the
(hi, xj ) interaction, possibly non-parametric, e.g., gradually adding terms to   i,j so as to better model the
interaction. furthermore, sampling from the conditional densities p (xj|h) or p (hi|x) would be tractable
even if the   i,j are complicated functions, simply because these are 1-dimensional densities from which
ef   cient approximate sampling and numerical integration are easy (e.g., by computing cumulative sums of
the density over nested sub-intervals or bins).

this analysis also highlights the basic limitation of rbms, which is that its parametrization only con-
siders pairwise interactions between variables. it is because the h are hidden and because we can choose

47

the number of hidden units, that we still have full expressive power over possible marginal distributions in
x (in fact we can represent any discrete distribution (le roux & bengio, 2008)). other variants of rbms
discussed in section 7.4 allow three-way interactions (memisevic & hinton, 2007).

what would be a contrastive divergence update in this generalized rbm formulation? to simplify
notations we note that the   j   s and   i   s in eq. 49 can be incorporated within the   i,j   s, so we ignore them in
the following. theorem 5.1 can still be applied with

the gradient of the free energy of a sample x is thus

   freeenergy(x)

     

     i,j(hi, xj)

     

freeenergy(x) =     logxh

  i,j(hi, xj )       .
exp         xi,j
exp(cid:16)   pi,j   i,j (hi, xj)(cid:17)
= xh
p  h exp(cid:16)   pi,j   i,j(  hi, xj )(cid:17)xi,j
= xh
p (h|x)xi,j
= eh      xi,j

x       .

     i,j(hi, xj)

     i,j(hi, xj)

     

     

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

thanks to proposition 7.1, a gibbs chain can still be run easily. truncating the log-likelihood gradient
expansion (eq. 38) after k steps of the gibbs chain, and approximating expectations with samples from this
chain, one obtains an approximation of the log-likelihood gradient at training point x1 that depends only on
gibbs samples h1, hk+1 and xk+1:

    log p (x1)

     

   freeenergy(x1)

     

+

   freeenergy(xk+1)

     

       

             xi,j

     i,j (h1,i, x1,j)

     

+xi,j

     i,j(hk+1,i, xk+1,j )

     

                

with       the update rule for parameters    of the model, corresponding to cd-k in such a generalized rbm.
note that in most parametrizations we would have a particular element of    depend on   i,j   s in such a way
that no explicit sum is needed. for instance (taking expectation over hk+1 instead of sampling) we recover
algorithm 1 when

  i,j(hi, xj) =    wij hixj    

bj xj
nh

   

cihi
nx

where nh and nx are respectively the numbers of hidden and visible units, and we also recover the other
variants described by welling et al. (2005), bengio et al. (2007) for different forms of the energy and allowed
set of values for hidden and input units.

8 stochastic variational bounds for joint optimization of dbn lay-

ers

in this section we discuss mathematical underpinnings of algorithms for training a dbn as a whole. the log-
likelihood of a dbn can be lower bounded using jensen   s inequality, and as we discuss below, this can justify
the greedy layer-wise training strategy introduced in (hinton et al., 2006) and described in section 6.1. we

48

will use eq. 42 for a dbn joint distribution, writing h for h1 (the    rst level hidden vector) to lighten notation,

and introducing an arbitrary conditional distribution q(h|x). first multiply log p (x) by 1 =ph q(h|x),

p (h|x), and multiply by 1 = q(h|x)

q(h|x) and expand the terms:

then use p (x) = p (x,h)

p (x, h)
p (h|x)

q(h|x) log

q(h|x) log

q(h|x)! log p (x) =xh

log p (x) =  xh
= xh
= hq(h|x) +xh
q(h|x) log p (x, h) +xh
= kl(q(h|x)||p (h|x)) + hq(h|x) +xh

p (x, h)
p (h|x)

q(h|x)
q(h|x)

q(h|x) log

q(h|x)
p (h|x)

q(h|x) (log p (h) + log p (x|h))

(50)

where hq(h|x) is the id178 of the distribution q(h|x). non-negativity of the kl divergence gives the
inequality

q(h|x) (log p (h) + log p (x|h)) ,

(51)

log p (x)     hq(h|x) +xh

which becomes an equality when p and q are identical, e.g. in the single-layer case (i.e., an rbm). whereas
we have chosen to use p to denote probabilities under the dbn, we use q to denote probabilities under an
rbm (the    rst level rbm), and in the equations choose q(h|x) to be the hidden-given-visible conditional
distribution of that    rst level rbm. we de   ne that    rst level rbm such that q(x|h) = p (x|h). in general
p (h|x) 6= q(h|x). this is because although the marginal p (h) on the    rst layer hidden vector h1 = h is
determined by the upper layers in the dbn, the rbm marginal q(h) only depends on the parameters of the
rbm.

8.1 unfolding rbms into in   nite directed belief networks

before using the above decomposition of the likelihood to justify the greedy training procedure for dbns,
we need to establish a connection between p (h1) in a dbn and the corresponding marginal q(h1) given by
the    rst level rbm. the interesting observation is that there exists a dbn whose h1 marginal equals the    rst
rbm   s h1 marginal, i.e. p (h1) = q(h1), as long the dimension of h2 equals the dimension of h0 = x. to
see this, consider a second-level rbm whose weight matrix is the transpose of the    rst-level rbm (that is
why we need the matching dimensions). hence, by symmetry of the roles of visible and hidden in an rbm
joint distribution (when transposing the weight matrix), the marginal distribution over the visible vector of
the second rbm is equal to the marginal distribution q(h1) of the hidden vector of the    rst rbm.

another interesting way to see this is given by hinton et al. (2006): consider the in   nite id150
markov chain starting at t =        and terminating at t = 0, alternating between x and h1 for the    rst rbm,
with visible vectors sampled on even t and hidden vectors on odd t. this chain can be seen as an in   nite
directed belief network with tied parameters (all even steps use weight matrix w     while all odd ones use
weight matrix w ). alternatively, we can summarize any sub-chain from t =        to t =    by an rbm with
weight matrix w or w     according to the parity of   , and obtain a dbn with 1        layers (not counting the
input layer), as illustrated in figure 13. this argument also shows that a 2-layer dbn in which the second
level has weights equal to the transpose of the    rst level weights is equivalent to a single rbm.

8.2 variational justi   cation of greedy layer-wise training

here we discuss the argument made by hinton et al. (2006) that adding one rbm layer improves the like-
lihood of a dbn. let us suppose we have trained an rbm to model x, which provides us with a model

49

rbm

rbm

dbn

...

w    
3

w3

w    
3

w2

w    
1

h2

h1

x

...

w

ht   2

w   

xt   1

w

ht   1

w   

xt

figure 13: an rbm can be unfolded as an in   nite directed belief network with tied weights (see text). left:
the weight matrix w or its transpose are used depending on the parity of the layer index. this sequence
of random variables corresponds to a gibbs markov chain to generate xt (for t large). on the right, the
top-level rbm in a dbn can also be unfolded in the same way, showing that a dbn is an in   nite directed
graphical model in which some of the layers are tied (all except the bottom few ones).

50

q(x) expressed through two conditionals q(h1|x) and q(x|h1). exploiting the argument in the previ-
ous subsection, let us now initialize an equivalent 2-layer dbn, i.e., generating p (x) = q(x), by taking
p (x|h1) = q(x|h1) and p (h1, h2) given by a second-level rbm whose weights are the transpose of the
   rst-level rbm. now let us come back to eq. 50 above, and the objective of improving the dbn likelihood by
changing p (h1), i.e., keeping p (x|h1) and q(h1|x)    xed but allowing the second level rbm to change. in-
terestingly, increasing the kl divergence term increases the likelihood. starting from p (h1|x) = q(h1|x),
the kl term is zero (i.e., can only increase) and the id178 term in eq. 50 does not depend on the dbn
p (h1), so small improvements to the term with p (h1) guarantee an increase in log p (x). we are also
guaranteed that further improvements of the p (h1) term (i.e. further training of the second rbm, detailed
below) cannot bring the log-likelihood lower than it was before the second rbm was added. this is simply
because of the positivity of the kl and id178 terms: further training of the second rbm increases a lower
bound on the log-likelihood (eq. 51), as argued by hinton et al. (2006). this justi   es training the second

rbm to maximize the second term, i.e., the expectation over the training set ofph1 q(h1|x) log p (h1).

the second-level rbm is thus trained to maximize

  p (x)q(h1|x) log p (h1)

(52)

xx,h1

with respect to p (h1). this is the maximum-likelihood criterion for a model that sees examples h1 obtained
as marginal samples from the joint distribution   p (x)q(h1|x). if we keep the    rst-level rbm    xed, then
the second-level rbm could therefore be trained as follows: sample x from the training set, then sample
h1     q(h1|x), and consider that h1 as a training sample for the second-level rbm (i.e. as an observation
for its    visible    vector). if there was no constraint on p (h1), the maximizer of the above training criterion
would be its    empirical    or target distribution

p    (h1) =xx

  p (x)q(h1|x).

(53)

the same argument can be made to justify adding a third layer, etc. we obtain the greedy layer-wise
training procedure outlined in section 6.1. in practice the requirement that layer sizes alternate is not satis-
   ed, and consequently neither is it common practice to initialize the newly added rbm with the transpose of
the weights at the previous layer (hinton et al., 2006; bengio et al., 2007), although it would be interesting
to verify experimentally (in the case where the size constraint is imposed) whether the initialization with the
transpose of the previous layer helps to speed up training.

note that as we continue training the top part of the model (and this includes adding extra layers),
there is no guarantee that log p (x) (in average over the training set) will monotonically increase. as our
lower bound continues to increase, the actual log-likelihood could start decreasing. let us examine more
closely how this could happen. it would require the kl(q(h1|x)||p (h1|x)) term to decrease as the second
rbm continues to be trained. however, this is unlikely in general: as the dbn   s p (h1) deviates more and
more from the    rst rbm   s marginal q(h1) on h1, it is likely that the posteriors p (h1|x) (from the dbn)
and q(h1|x) (from the rbm) deviate more and more (since p (h1|x)     q(x|h1)p (h1) and q(h1|x)    
q(x|h1)q(h1)), making the kl term in eq. 50 increase. as the training likelihood for the second rbm
increases, p (h1) moves smoothly from q(h1) towards p    (h1). consequently, it seems very plausible that
continued training of the second rbm is going to increase the dbn   s likelihood (not just initially) and by
transitivity, adding more layers will also likely increase the dbn   s likelihood. however, it is not true that
increasing the training likelihood for the second rbm starting from any parameter con   guration guarantees
that the dbn likelihood will increase, since at least one pathological counter-example can be found (i.
sutskever, personal communication). consider the case where the    rst rbm has very large hidden biases, so
that q(h1|x) = q(h1) = 1h1=  h = p    (h1), but large weights and small visible offsets so that p (xi|h) =
1xi=hi, i.e., the hidden vector is copied to the visible units. when initializing the second rbm with the
transpose of the weights of the    rst rbm, the training likelihood of the second rbm cannot be improved,
nor can the dbn likelihood. however, if the second rbm was started from a    worse    con   guration (worse

51

in the sense of its training likelihood, and also worse in the sense of the dbn likelihood), then p (h1) would
move towards p    (h1) = q(h1), making the second rbm likelihood improve while the kl term would
decrease and the dbn likelihood would decrease. these conditions could not happen when initializing the
second rbm properly (with a copy of the    rst rbm). so it remains an open question whether we can    nd
conditions (excluding the above) which guarantee that while the likelihood of the second rbm increases,
the dbn likelihood also increases.

another argument to explain why the greedy procedure works is the following (hinton, nips   2007
tutorial). the training distribution for the second rbm (samples h1 from p    (h1)) looks more like data
generated by an rbm than the original training distribution   p (x). this is because p    (h1) was obtained by
applying one sub-step of an rbm gibbs chain on examples from   p (x), and we know that applying many
gibbs steps would yield data from that rbm.

unfortunately, when we train within this greedy layer-wise procedure an rbm that will not be the top-
level level of a dbn, we are not taking into account the fact that more capacity will be added later to
improve the prior on the hidden units. le roux and bengio (2008) have proposed considering alternatives to
contrastive divergence for training rbms destined to initialize intermediate layers of a dbn. the idea is to
consider that p (h) will be modeled with a very high capacity model (the higher levels of the dbn). in the
limit case of in   nite capacity, one can write down what that optimal p (h) will be: it is simply the stochastic
transformation of the empirical distribution through the stochastic mapping q(h|x) of the    rst rbm (or
previous rbms), i.e. p     of eq. 53 in the case of the second level. plugging this back into the expression
for log p (x), one    nds that a good criterion for training the    rst rbm is the kl divergence between the
data distribution and the distribution of the stochastic reconstruction vectors after one step of the gibbs
chain. experiments (le roux & bengio, 2008) con   rm that this criterion yields better optimization of the
dbn (initialized with this rbm). unfortunately, this criterion is not tractable since it involves summing
over all con   gurations of the hidden vector h. tractable approximations of it might be considered, since
this criterion looks like a form of reconstruction error on a stochastic auto-encoder (with a generative model
similar to one proposed for denoising auto-encoders (vincent et al., 2008)). another interesting alternative,
explored in the next section, is to directly work on joint optimization of all the layers of a dbn.

8.3

joint unsupervised training of all the layers

we discuss here how one could train a whole deep architecture such as a dbn in an unsupervised way, i.e.
to represent well the input distribution.

8.3.1 the wake-sleep algorithm

the wake-sleep algorithm (hinton et al., 1995) was introduced to train sigmoidal belief networks (i.e. where
the distribution of the top layer units factorizes). it is based on a    recognition    model q(h|x) (along with
q(x) set to be the training set distribution) that acts as a variational approximation to the generative model
p (h, x). here we denote with h all the hidden layers together. in a dbn, q(h|x) is as de   ned above
(sec. 6.1), obtained by stochastically propagating samples upward (from input to higher layers) at each layer.
in the wake-sleep algorithm, we decouple the recognition parameters (upward weights, used to compute
q(h|x)) from the generative parameters (downward weights, used to compute p (x|h)). the basic idea of
the algorithm is simple:

1. wake phase: sample x from the training set, generate h     q(h|x) and use this (h, x) as fully
observed data for training p (x|h) and p (h). this corresponds to doing one stochastic gradient step
with respect to

xh

q(h|x) log p (x, h).

(54)

52

2. sleep phase: sample (h, x) from the model p (x, h), and use that pair as fully observed data for

training q(h|x). this corresponds to doing one stochastic gradient step with respect to

p (h, x) log q(h|x).

(55)

xh,x

the wake-sleep algorithm has been used for dbns in hinton et al. (2006), after the weights associated
with each layer have been trained as rbms as discussed earlier. for a dbn with layers (h1, . . . , h   ), the
wake phase updates for the weights of the top rbm (between h      1 and h   ) is done by considering the h      1
sample (obtained from q(h|x)) as training data for the top rbm.

a variational approximation can be used to justify the wake-sleep algorithm. the log-likelihood decom-

position in eq. 50

log p (x) = kl(q(h|x)||p (h|x)) + hq(h|x) +xh

q(h|x) (log p (h) + log p (x|h))

(56)

shows that the log-likelihood can be bounded from below by the opposite of the helmholtz free energy (hin-
ton et al., 1995; frey, hinton, & dayan, 1996) f :

where

log p (x) = kl(q(h|x)||p (h|x))     f (x)        f (x)

f (x) =    hq(h|x)    xh

q(h|x) (log p (h) + log p (x|h))

and the inequality is tight when q = p . the variational approach is based on maximizing the lower bound
   f while trying to make the bound tight, i.e., minimizing kl(q(h|x)||p (h|x)). when the bound is tight,
an increase of    f (x) is more likely to yield an increase of log p (x). since we decouple the parameters of
q and of p , we can now see what the two phases are doing. in the wake phase we consider q    xed and
do a stochastic gradient step towards maximizing the expected value of f (x) over samples x of the training
set, with respect to parameters of p (i.e. we do not care about the id178 of q). in the sleep phase we
would ideally like to make q as close to p as possible in the sense of minimizing kl(q(h|x)||p (h|x))
(i.e. taking q as the reference), but instead we minimize kl(p (h, x)||q(h, x)), taking p as the reference,
because kl(q(h|x)||p (h|x)) is intractable.

(57)

(58)

8.3.2 transforming the dbn into a id82

another approach was recently proposed, yielding in the evaluated cases results superior to the use of the
wake-sleep algorithm (salakhutdinov & hinton, 2009). after initializing each layer as an rbm as already
discussed in section 6.1, the dbn is transformed into a corresponding deep id82. because
in a id82 each unit receives input from above as well as from below, it is proposed to halve
the rbm weights when initializing the deep id82 from the layer-wise rbms. it is very in-
teresting to note that the rbm initialization of the deep id82 was crucial to obtain the good
results reported. the authors then propose approximations for the positive phase and negative phase gra-
dients of the id82 (see section 5.2 and eq. 26). for the positive phase (which in principle
requires holding x    xed and sampling from p (h|x)), they propose a variational approximation correspond-
ing to a mean-   eld relaxation (propagating probabilities associated with each unit given the others, rather
than samples, and iterating a few dozen times to let them settle). for the negative phase (which in principle
requires sampling from the joint p (h, x)) they propose to use the idea of a persistent mcmc chain already
discussed in section 5.4.1 and introduced in tieleman (2008). the idea is to keep a set of (h, x) states (or
particles) that are updated by one gibbs step according to the current model (i.e. sample each unit according
to its id203 given all the others at the previous step). even though the parameters keep changing (very
slowly), we continue the same markov chain instead of starting a new one (as in the old id82

53

algorithm (hinton et al., 1984; ackley et al., 1985; hinton & sejnowski, 1986)). this strategy seems to
work very well, and salakhutdinov and hinton (2009) report an improvement over dbns on the mnist
dataset, both in terms of data log-likelihood (estimated using annealed importance sampling (salakhutdinov
& murray, 2008)) and in terms of classi   cation error (after supervised    ne-tuning), bringing down the er-
ror rate from 1.2% to 0.95%. more recently, lee et al. (2009) also transform the trained dbn into a deep
id82 in order to generate samples from it, and here the dbn has a convolutional structure.

9 looking forward

9.1 global optimization strategies

as discussed section 4.2, part of the explanation for the better generalization observed with layer-local
unsupervised pre-training in deep architectures could well be that they help to better optimize the lower
layers (near the input), by initializing supervised training in regions of parameter space associated with
better unsupervised models. similarly, initializing each layer of a deep id82 as an rbm
was important to achieve the good results reported (salakhutdinov & hinton, 2009). in both settings, we
optimize a proxy criterion that is layer-local before    ne-tuning with respect to the whole deep architecture.
here, we draw connections between existing work and approaches that could help to deal with dif   cult
optimization problems, based on the principle of continuation methods (allgower & georg, 1980). al-
though they provide no guarantee to obtain the global optimum, these methods have been particularly useful
in computational chemistry to    nd approximate solutions to dif   cult optimization problems involving the
con   gurations of molecules (coleman & wu, 1994; more & wu, 1996; wu, 1997). the basic idea is to
   rst solve an easier and smoothed version of the problem and gradually consider less smoothing, with the
intuition that a smooth version of the problem reveals the global picture, just like with simulated anneal-
ing (kirkpatrick, jr., , & vecchi, 1983). one de   nes a single-parameter family of cost functions c  (  ) such
that c0 can be optimized more easily (maybe convex in   ), while c1 is the criterion that we actually wish
to minimize. one    rst minimizes c0(  ) and then gradually increases    while keeping    at a local minimum
of c  (  ). typically c0 is a highly smoothed version of c1, so that    gradually moves into the basin of
attraction of a dominant (if not global) minimum of c1.

9.1.1 greedy layer-wise training of dbns as a continuation method

the greedy layer-wise training algorithm for dbns described in section 6.1 can be viewed as an approximate
continuation method, as follows. first of all recall (section 8.1) that the top-level rbm of a dbn can be
unfolded into an in   nite directed graphical model with tied parameters. at each step of the greedy layer-
wise procedure, we untie the parameters of the top-level rbm from the parameters of the penultimate level.
so one can view the layer-wise procedure as follows. the model structure remains the same, an in   nite
chain of sigmoid belief layers, but we change the constraint on the parameters at each step of the layer-
wise procedure. initially all the layers are tied. after training the    rst rbm (i.e. optimizing under this
constraint), we untie the    rst level parameters from the rest. after training the second rbm (i.e. optimizing
under this slightly relaxed constraint), we untie the second level parameters from the rest, etc.
instead
of a continuum of training criteria, we have a discrete sequence of (presumably) gradually more dif   cult
optimization problems. by making the process greedy we    x the parameters of the    rst k levels after they
have been trained and only optimize the (k + 1)-th, i.e. train an rbm. for this analogy to be strict we would
need to initialize the weights of the newly added rbm with the transpose of the previous one. note also
that instead of optimizing all the parameters, the greedy layer-wise approach only optimizes the new ones.
but even with these approximations, this analysis suggests an explanation for the good performance of the
layer-wise training approach in terms of reaching better solutions.

54

9.1.2 unsupervised to supervised transition

the experiments in many papers clearly show that an unsupervised pre-training followed by a supervised
   ne-tuning works very well for deep architectures. whereas previous work on combining supervised and
unsupervised criteria (lasserre et al., 2006) focus on the id173 effect of an unsupervised criterion
(and unlabeled examples, in semi-supervised learning), the discussion of section 4.2 suggests that part of
the gain observed with unsupervised pre-training of deep networks may arise out of better optimization of
the lower layers of the deep architecture.

much recent work has focused on starting from an unsupervised representation learning algorithm (such
as sparse coding) and    ne-tuning the representation with a discriminant criterion or combining the discrimi-
nant and unsupervised criteria (larochelle & bengio, 2008; mairal et al., 2009; bagnell & bradley, 2009).
in larochelle and bengio (2008), an rbm is trained with a two-part visible vector that includes both
the input x and the target class y. such an rbm can either be trained to model the joint p (x, y) (e.g.
by contrastive divergence) or to model the conditional p (y|x) (the exact gradient of the conditional log-
likelihood is tractable). the best results reported (larochelle & bengio, 2008) combine both criteria, but the
model is initialized using the non-discriminant criterion.

in mairal et al. (2009), bagnell and bradley (2009) the task of training the decoder bases in a sparse
coding system is coupled with a task of training a classi   er on to of the sparse codes. after initializing the
decoder bases using non-discriminant learning, they can be    ne-tuned using a discriminant criterion that
is applied jointly on the representation parameters (i.e., the    rst layer bases, that gives rise to the sparse
codes) and a set of classi   er parameters (e.g., a linear classi   er that takes the representation codes as input).
according to mairal et al. (2009), trying to directly optimize the supervised criterion without    rst initializing
with non-discriminant training yielded very poor results. in fact, they propose a smooth transition from
the non-discriminant criterion to the discriminant one, hence performing a kind of continuation method to
optimize the discriminant criterion.

9.1.3 controlling temperature

even optimizing the log-likelihood of a single rbm might be a dif   cult optimization problem. it turns out
that the use of stochastic gradient (such as the one obtained from cd-k) and small initial weights is again
close to a continuation method, and could easily be turned into one. consider the family of optimization
problems corresponding to the id173 path (hastie, rosset, tibshirani, & zhu, 2004) for an rbm,
e.g., with    2 id173 of the parameters, the family of training criteria parametrized by        (0, 1]:

c  (  ) =    xi

log p  (xi)     ||  ||2 log   .

(59)

when        0, we have        0, and it can be shown that the rbm log-likelihood becomes convex in   .
when        1, there is no id173 (note that some intermediate value of    might be better in terms
of generalization, if the training set is small). controlling the magnitude of the offsets and weights in an
rbm is equivalent to controlling the temperature in a id82 (a scaling coef   cient for the
energy function). high temperature corresponds to a highly stochastic system, and at the limit a factorial
and uniform distribution over the input. low temperature corresponds to a more deterministic system where
only a small subset of possible con   gurations are plausible.

interestingly, one observes routinely that stochastic id119 starting from small weights grad-
ually allows the weights to increase in magnitude, thus approximately following the id173 path.
early stopping is a well-known and ef   cient capacity control technique based on monitoring performance
on a validation set during training and keeping the best parameters in terms of validation set error. the
mathematical connection between early stopping and    2 id173 (along with margin) has already been
established (zinkevich, 2003; collobert & bengio, 2004): starting from small parameters and doing gradient
descent yields gradually larger parameters, corresponding to a gradually less regularized training criterion.

55

however, with ordinary stochastic id119 (with no explicit id173 term), there is no guar-
antee that we would be tracking the sequence of local minima associated with a sequence of values of   
in eq. 59. it might be possible to slightly change the stochastic gradient algorithm to make it track better
the id173 path, (i.e. make it closer to a continuation method), by controlling    explicitly, gradually
increasing    when the optimization is near enough a local minimum for the current value of   . note that
the same technique might be extended for other dif   cult non-linear optimization problems found in machine
learning, such as training a deep supervised neural network. we want to start from a globally optimal so-
lution and gradually track local minima, starting from heavy id173 and moving slowly to little or
none.

9.1.4 shaping: training with a curriculum

another continuation method may be obtained by gradually transforming the training task, from an easy
one (maybe convex) where examples illustrate the simpler concepts, to the target one (with more dif   cult
examples). humans need about two decades to be trained as fully functional adults of our society. that
training is highly organized, based on an education system and a curriculum which introduces different
concepts at different times, exploiting previously learned concepts to ease the learning of new abstractions.
the idea of training a learning machine with a curriculum can be traced back at least to elman (1993). the
basic idea is to start small, learn easier aspects of the task or easier sub-tasks, and then gradually increase
the dif   culty level. from the point of view of building representations, advocated here, the idea is to learn
representations that capture low-level abstractions    rst, and then exploit them and compose them to learn
slightly higher-level abstractions necessary to explain more complex structure in the data. by choosing
which examples to present and in which order to present them to the learning system, one can guide training
and remarkably increase the speed at which learning can occur. this idea is routinely exploited in animal
training and is called shaping (skinner, 1958; peterson, 2004; krueger & dayan, 2009).

shaping and the use of a curriculum can also be seen as continuation methods. for this purpose, consider
the learning problem of modeling the data coming from a training distribution   p . the idea is to reweigh the
id203 of sampling the examples from the training distribution, according to a given schedule that starts
from the    easiest    examples and moves gradually towards examples illustrating more abstract concepts. at
point t in the schedule, we train from distribution   pt, with   p1 =   p and   p0 chosen to be easy to learn. like
in any continuation method, we move along the schedule when the learner has reached a local minimum at
the current point t in the schedule, i.e., when it has suf   ciently mastered the previously presented examples
(sampled from   pt). making small changes in t corresponds to smooth changes in the id203 of sampling
examples in the training distribution, so we can construct a continuous path starting from an easy learning
problem and ending in the desired training distribution. this idea is developed further in bengio, louradour,
collobert, and weston (2009), with experiments showing better generalization obtained when training with
a curriculum leading to a target distribution, compared to training only with the target distribution, on both
vision and language tasks.

there is a connection between the shaping/curriculum idea and the greedy layer-wise idea. in both cases
we want to exploit the notion that a high level abstraction can more conveniently be learned once appropriate
lower-level abstractions have been learned.
in the case of the layer-wise approach, this is achieved by
gradually adding more capacity in a way that builds upon previously learned concepts. in the case of the
curriculum, we control the training examples so as to make sure that the simpler concepts have actually been
learned before showing many examples of the more advanced concepts. showing complicated illustrations
of the more advanced concepts is likely to be generally a waste of time, as suggested by the dif   culty for
humans to grasp a new idea if they do not    rst understand the concepts necessary to express that new idea
compactly.

with the curriculum idea we introduce a teacher, in addition to the learner and the training distribution or
environment. the teacher can use two sources of information to decide on the schedule: (a) prior knowledge
about a sequence of concepts that can more easily be learned when presented in that order, and (b) monitoring
of the learner   s progress to decide when to move on to new material from the curriculum. the teacher has

56

to select a level of dif   culty for new examples which is a compromise between    too easy    (the learner will
not need to change its model to account for these examples) and    too hard    (the learner cannot make an
incremental change that can account for these examples so they will most likely be treated as outliers or
special cases, i.e. not helping generalization).

9.2 why unsupervised learning is important

one of the claims of this paper is that powerful unsupervised or semi-supervised (or self-taught) learning is
a crucial component in building successful learning algorithms for deep architectures aimed at approaching
ai. we brie   y cover the arguments in favor of this hypothesis here:

    scarcity of labeled examples and availability of many unlabeled examples (possibly not only of the

classes of interest, as in self-taught learning (raina et al., 2007)).

    unknown future tasks: if a learning agent does not know what future learning tasks it will have to
deal with in the future, but it knows that the task will be de   ned with respect to a world (i.e. random
variables) that it can observe now, it would appear very rational to collect and integrate as much
information as possible about this world so as to learn what makes it tick.

    once a good high-level representation is learned, other learning tasks (e.g., supervised or reinforce-
ment learning) could be much easier. we know for example that kernel machines can be very powerful
if using an appropriate kernel, i.e. an appropriate feature space. similarly, we know powerful rein-
forcement learning algorithms which have guarantees in the case where the actions are essentially
obtained through linear combination of appropriate features. we do not know what the appropriate
representation should be, but one would be reassured if it captured the salient factors of variation in
the input data, and disentangled them.

    layer-wise unsupervised learning: this was argued in section 4.3. much of the learning could be done
using information available locally in one layer or sub-layer of the architecture, thus avoiding the
hypothesized problems with supervised gradients propagating through long chains with large fan-in
elements.

    connected to the two previous points is the idea that unsupervised learning could put the parameters
of a supervised or id23 machine in a region from which id119 (local
optimization) would yield good solutions. this has been veri   ed empirically in several settings, in
particular in the experiment of figure 7 and in bengio et al. (2007), larochelle et al. (2009), erhan
et al. (2009).

    the extra constraints imposed on the optimization by requiring the model to capture not only the
input-to-target dependency but also the statistical regularities of the input distribution might be helpful
in avoiding some poorly generalizing apparent local minima (those that do not correspond to good
modeling of the input distribution). note that in general extra constraints may also create more local
minima, but we observe experimentally (bengio et al., 2007) that both training and test error can
be reduced by unsupervised pre-training, suggesting that the unsupervised pre-training moves the
parameters in a region of space closer to local minima corresponding to learning better representations
(in the lower layers). it has been argued (hinton, 2006) (but is debatable) that unsupervised learning
is less prone to over   tting than supervised learning. deep architectures have typically been used to
construct a supervised classi   er, and in that case the unsupervised learning component can clearly be
seen as a regularizer or a prior (ng & jordan, 2002; lasserre et al., 2006; liang & jordan, 2008; erhan
et al., 2009) that forces the resulting parameters to make sense not only to model classes given inputs
but also to capture the structure of the input distribution.

57

9.3 open questions

research on deep architectures is still young and many questions remain unanswered. the following are
potentially interesting.

1. can the results pertaining to the role of computational depth in circuits be generalized beyond logic

gates and linear threshold units?

2. is there a depth that is mostly suf   cient for the computations necessary to approach human-level

performance of ai tasks?

3. how can the theoretical results on depth of circuits with a    xed size input be generalized to dynamical

circuits operating in time, with context and the possibility of recursive computation?

4. why is gradient-based training of deep neural networks from random initialization often unsuccessful?

5. are rbms trained by cd doing a good job of preserving the information in their input (since they
are not trained as auto-encoders they might lose information about the input that may turn out to be
important later), and if not how can that be    xed?

6. is the supervised training criterion for deep architectures (and maybe the log-likelihood in deep boltz-
mann machines and dbns) really fraught with actual poor local minima or is it just that the criterion is
too intricate for the optimization algorithms tried (such as id119 and conjugate gradients)?

7. is the presence of local minima an important issue in training rbms?

8. could we replace rbms and auto-encoders by algorithms that would be pro   cient at extracting good

representations but involving an easier optimization problem, perhaps even a convex one?

9. current training algorithms for deep architectures involves many phases (one per layer, plus a global
   ne-tuning). this is not very practical in the purely online setting since once we have moved into    ne-
tuning, we might be trapped in an apparent local minimum. is it possible to come up with a completely
online procedure for training deep architectures that preserves an unsupervised component all along?
note that (weston et al., 2008) is appealing for this reason.

10. should the number of gibbs steps in contrastive divergence be adjusted during training?

11. can we signi   cantly improve upon contrastive divergence, taking computation time into account?
new alternatives have recently been proposed which deserve further investigation (tieleman, 2008;
tieleman & hinton, 2009).

12. besides reconstruction error, are there other more appropriate ways to monitor progress during train-
ing of rbms and dbns? equivalently, are there tractable approximations of the partition function in
rbms and dbns? recent work in this direction (salakhutdinov & murray, 2008; murray & salakhut-
dinov, 2009) using annealed importance sampling is encouraging.

13. could rbms and auto-encoders be improved by imposing some form of sparsity penalty on the rep-

resentations they learn, and what are the best ways to do so?

14. without increasing the number of hidden units, can the capacity of an rbm be increased using non-

parametric forms of its energy function?

15. since we only have a generative model for single denoising auto-encoders, is there a probabilistic

interpretation to models learned in stacked auto-encoders or stacked denoising auto-encoders?

16. how ef   cient is the greedy layer-wise algorithm for training id50 (in terms of max-

imizing the training data likelihood)? is it too greedy?

58

17. can we obtain low variance and low bias estimators of the log-likelihood gradient in deep belief
networks and related deep generative models, i.e., can we jointly train all the layers (with respect to
the unsupervised objective)?

18. unsupervised layer-level training procedures discussed here help training deep architectures, but ex-
periments suggest that training still gets stuck in apparent local minima and cannot exploit all the
information in very large datasets. is it true? can we go beyond these limitations by developing more
powerful optimization strategies for deep architectures?

19. can optimization strategies based on continuation methods deliver signi   cantly improved training of

deep architectures?

20. are there other ef   ciently trainable deep architectures besides id50, stacked auto-

encoders, and deep id82s?

21. is a curriculum needed to learn the kinds of high-level abstractions that humans take years or decades

to learn?

22. can the principles discovered to train deep architectures be applied or generalized to train recurrent
networks or dynamical belief networks, which learn to represent context and long-term dependencies?

23. how can deep architectures be generalized to represent information that, by its nature, might seem not

easily representable by vectors, because of its variable size and structure (e.g. trees, graphs)?

24. although id50 are in principle well suited for the semi-supervised and self-taught
learning settings, what are the best ways to adapt the current deep learning algorithms to these setting
and how would they fare compared to existing semi-supervised algorithms?

25. when labeled examples are available, how should supervised and unsupervised criteria be combined

to learn the model   s representations of the input?

26. can we    nd analogs of the computations necessary for contrastive divergence and deep belief net

learning in the brain?

27. the cortex is not at all like a feedforward neural network in that there are signi   cant feedback connec-
tions (e.g. going back from later stages of visual processing to earlier ones) and these may serve a role
not only in learning (as in rbms) but also in integrating contextual priors with visual evidence (lee
& mumford, 2003). what kind of models can give rise to such interactions in deep architectures, and
learn properly with such interactions?

10 conclusion

this paper started with a number of motivations:    rst to use learning to approach ai, then on the intuitive
plausibility of decomposing a problem into multiple levels of computation and representation, followed by
theoretical results showing that a computational architecture that does not have enough of these levels can
require a huge number of computational elements, and the observation that a learning algorithm that relies
only on local generalization is unlikely to generalize well when trying to learn highly-varying functions.

turning to architectures and algorithms, we    rst motivated distributed representations of the data, in
which a huge number of possible con   gurations of abstract features of the input are possible, allowing a
system to compactly represent each example, while opening the door to a rich form of generalization. the
discussion then focused on the dif   culty of successfully training deep architectures for learning multiple
levels of distributed representations. although the reasons for the failure of standard gradient-based methods
in this case remain to be clari   ed, several algorithms have been introduced in recent years that demonstrate

59

much better performance than was previously possible with simple gradient-based optimization, and we have
tried to focus on the underlying principles behind their success.

although much of this paper has focused on deep neural net and deep graphical model architectures, the
idea of exploring learning algorithms for deep architectures should be explored beyond the neural net frame-
work. for example, it would be interesting to consider extensions of decision tree and boosting algorithms
to multiple levels.

kernel-learning algorithms suggest another path which should be explored, since a feature space that
captures the abstractions relevant to the distribution of interest would be just the right space in which to apply
the kernel machinery. research in this direction should consider ways in which the learned kernel would
have the ability to generalize non-locally, to avoid the curse of dimensionality issues raised in section 3.1
when trying to learn a highly-varying function.

the paper focused on a particular family of algorithms, the id50, and their component
elements, the restricted id82, and very near neighbors: different kinds of auto-encoders,
which can also be stacked successfully to form a deep architecture. we studied and connected together
estimators of the log-likelihood gradient in restricted id82s, helping to justify the use of the
contrastive divergence update for training restricted id82s. we highlighted an optimization
principle that has worked well for id50 and related algorithms such as stacked auto-
encoders, based on a greedy, layer-wise, unsupervised initialization of each level of the model. we found that
this optimization principle is actually an approximation of a more general optimization principle, exploited
in so-called continuation methods, in which a series of gradually more dif   cult optimization problems are
solved. this suggested new avenues for optimizing deep architectures, either by tracking solutions along a
id173 path, or by presenting the system with a sequence of selected examples illustrating gradually
more complicated concepts, in a way analogous to the way students or animals are trained.

acknowledgements

the author is particularly grateful for the inspiration from and constructive input from yann lecun,
aaron courville, olivier delalleau, dumitru erhan, pascal vincent, geoffrey hinton, joseph turian, hugo
larochelle, nicolas le roux, j  er  ome louradour, pascal lamblin, james bergstra, pierre-antoine manzagol
and xavier glorot. this research was performed thanks to funding from nserc, mitacs, and the canada
research chairs.

references

ackley, d. h., hinton, g. e., & sejnowski, t. j. (1985). a learning algorithm for id82s.

cognitive science, 9, 147   169.

ahmed, a., yu, k., xu, w., gong, y., & xing, e. p. (2008). training hierarchical feed-forward visual
recognition models using id21 from pseudo tasks. in proceedings of the 10th european
conference on id161 (eccv   08), pp. 69   82.

allgower, e. l., & georg, k. (1980). numerical continuation methods. an introduction. no. 13 in springer

series in computational mathematics. springer-verlag.

andrieu, c., de freitas, n., doucet, a., & jordan, m. (2003). an introduction to mcmc for machine

learning. machine learning, 50, 5   43.

attwell, d., & laughlin, s. b. (2001). an energy budget for signaling in the grey matter of the brain. journal

of cerebral blood flow and metabolism, 21, 1133   1145.

bagnell, j. a., & bradley, d. m. (2009). differentiable sparse coding.

in koller, d., schuurmans, d.,
bengio, y., & bottou, l. (eds.), advances in neural information processing systems 21 (nips   08).
nips foundation.

60

baxter, j. (1995). learning internal representations. in proceedings of the 8th international conference on

computational learning theory (colt   95), pp. 311   320 santa cruz, california. acm press.

baxter, j. (1997). a bayesian/information theoretic model of learning via multiple task sampling. machine

learning, 28, 7   40.

belkin, m., & niyogi, p. (2003). using manifold structure for partially labeled classi   cation. in becker, s.,
thrun, s., & obermayer, k. (eds.), advances in neural information processing systems 15 (nips   02)
cambridge, ma. mit press.

belkin, m., matveeva, i., & niyogi, p. (2004). id173 and semi-supervised learning on large graphs.
in shawe-taylor, j., & singer, y. (eds.), proceedings of the 17th international conference on com-
putational learning theory (colt   04), pp. 624   638. springer.

bell, a. j., & sejnowski, t. j. (1995). an information maximisation approach to blind separation and blind

deconvolution. neural computation, 7(6), 1129   1159.

bengio, y., simard, p., & frasconi, p. (1994). learning long-term dependencies with id119 is

dif   cult. ieee transactions on neural networks, 5(2), 157   166.

bengio, y., & delalleau, o. (2009). justifying and generalizing contrastive divergence. neural computation,

21(6), 1601   1621.

bengio, y., delalleau, o., & le roux, n. (2006). the curse of highly variable functions for local kernel
machines. in weiss, y., sch  olkopf, b., & platt, j. (eds.), advances in neural information processing
systems 18 (nips   05), pp. 107   114. mit press, cambridge, ma.

bengio, y., delalleau, o., & simard, c. (2009). id90 do not generalize to new variations. compu-

tational intelligence. to appear.

bengio, y., ducharme, r., & vincent, p. (2001). a neural probabilistic language model.

in leen, t.,
dietterich, t., & tresp, v. (eds.), advances in neural information processing systems 13 (nips   00),
pp. 933   938. mit press.

bengio, y., ducharme, r., vincent, p., & jauvin, c. (2003). a neural probabilistic language model. journal

of machine learning research, 3, 1137   1155.

bengio, y., lamblin, p., popovici, d., & larochelle, h. (2007). greedy layer-wise training of deep networks.
in sch  olkopf, b., platt, j., & hoffman, t. (eds.), advances in neural information processing systems
19 (nips   06), pp. 153   160. mit press.

bengio, y., le roux, n., vincent, p., delalleau, o., & marcotte, p. (2006). convex neural networks. in
weiss, y., sch  olkopf, b., & platt, j. (eds.), advances in neural information processing systems 18
(nips   05), pp. 123   130. mit press, cambridge, ma.

bengio, y., & lecun, y. (2007). scaling learning algorithms towards ai.

in bottou, l., chapelle, o.,

decoste, d., & weston, j. (eds.), large scale kernel machines. mit press.

bengio, y., louradour, j., collobert, r., & weston, j. (2009). curriculum learning. in international con-

ference on machine learning proceedings.

bengio, y., monperrus, m., & larochelle, h. (2006). non-local estimation of manifold structure. neural

computation, 18(10), 2509   2528.

bergstra, j., & bengio, y. (2010). slow, decorrelated features for pretraining complex cell-like networks.
in schuurmans, d., bengio, y., williams, c., lafferty, j., & culotta, a. (eds.), advances in neural
information processing systems 22 (nips   09). accepted, in preparation.

boser, b. e., guyon, i. m., & vapnik, v. n. (1992). a training algorithm for optimal margin classi   ers. in

fifth annual workshop on computational learning theory, pp. 144   152 pittsburgh. acm.

bourlard, h., & kamp, y. (1988). auto-association by multilayer id88s and singular value decompo-

sition. biological cybernetics, 59, 291   294.

61

brand, m. (2003). charting a manifold. in becker, s., thrun, s., & obermayer, k. (eds.), advances in

neural information processing systems 15 (nips   02), pp. 961   968. mit press.

breiman, l., friedman, j. h., olshen, r. a., & stone, c. j. (1984). classi   cation and regression trees.

wadsworth international group, belmont, ca.

breiman, l. (2001). id79s. machine learning, 45(1), 5   32.
brown, l. d. (1986). fundamentals of statistical exponential families, vol. 9. inst. of math. statist. lecture

notes monograph series.

candes, e., & tao, t. (2005). decoding by id135. ieee transactions on id205,

51(12), 4203   4215.

carreira-perpi  nan, m. a., & hinton, g. e. (2005). on contrastive divergence learning. in cowell, r. g.,
& ghahramani, z. (eds.), proceedings of the tenth international workshop on arti   cial intelligence
and statistics (aistats   05), pp. 33   40. society for arti   cial intelligence and statistics.

caruana, r. (1993). multitask connectionist learning. in proceedings of the 1993 connectionist models

summer school, pp. 372   379.

clifford, p. (1990). markov random    elds in statistics. in grimmett, g., & welsh, d. (eds.), disorder in
physical systems: a volume in honour of john m. hammersley, pp. 19   32. oxford university press.
cohn, d., ghahramani, z., & jordan, m. i. (1995). active learning with statistical models. in tesauro, g.,
touretzky, d., & leen, t. (eds.), advances in neural information processing systems 7 (nips   94),
pp. 705   712. cambridge ma: mit press.

coleman, t. f., & wu, z. (1994). parallel continuation-based global optimization for molecular conforma-

tion and protein folding. tech. rep., cornell university, dept. of computer science.

collobert, r., & bengio, s. (2004). links between id88s, mlps and id166s. in brodley, c. e. (ed.),
proceedings of the twenty-   rst international conference on machine learning (icml   04), p. 23 new
york, ny, usa. acm.

collobert, r., & weston, j. (2008). a uni   ed architecture for natural language processing: deep neural net-
works with multitask learning. in cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings
of the twenty-   fth international conference on machine learning (icml   08), pp. 160   167. acm.

cortes, c., haffner, p., & mohri, m. (2004). rational kernels: theory and algorithms. journal of machine

learning research, 5, 1035   1062.

cortes, c., & vapnik, v. (1995). support vector networks. machine learning, 20, 273   297.
cristianini, n., shawe-taylor, j., elisseeff, a., & kandola, j. (2002). on kernel-target alignment. in diet-
terich, t., becker, s., & ghahramani, z. (eds.), advances in neural information processing systems
14 (nips   01), vol. 14, pp. 367   373.

cucker, f., & grigoriev, d. (1999). complexity lower bounds for approximation algebraic computation

trees. journal of complexity, 15(4), 499   512.

dayan, p., hinton, g. e., neal, r., & zemel, r. (1995). the helmholtz machine. neural computation, 7,

889   904.

deerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., & harshman, r. (1990). indexing by latent

semantic analysis. journal of the american society for information science, 41(6), 391   407.

delalleau, o., bengio, y., & le roux, n. (2005). ef   cient non-parametric function induction in semi-
supervised learning. in cowell, r. g., & ghahramani, z. (eds.), proceedings of the tenth international
workshop on arti   cial intelligence and statistics, pp. 96   103. society for arti   cial intelligence and
statistics.

desjardins, g., & bengio, y. (2008). empirical evaluation of convolutional rbms for vision. tech. rep. 1327,

d  epartement d   informatique et de recherche op  erationnelle, universit  e de montr  eal.

62

doi, e., balcan, d. c., & lewicki, m. s. (2006). a theoretical analysis of robust coding over noisy over-
complete channels. in weiss, y., sch  olkopf, b., & platt, j. (eds.), advances in neural information
processing systems 18 (nips   05), pp. 307   314. mit press, cambridge, ma.

donoho, d. (2006). compressed sensing. ieee transactions on id205, 52(4), 1289   1306.
duane, s., kennedy, a., pendleton, b., & roweth, d. (1987). hybrid monte carlo. phys. lett. b, 195,

216   222.

elman, j. l. (1993). learning and development in neural networks: the importance of starting small..

cognition, 48, 781   799.

erhan, d., manzagol, p.-a., bengio, y., bengio, s., & vincent, p. (2009). the dif   culty of training deep
architectures and the effect of unsupervised pre-training. in proceedings of the twelfth international
conference on arti   cial intelligence and statistics (aistats   09), pp. 153   160.

freund, y., & haussler, d. (1994). unsupervised learning of distributions on binary vectors using two layer

networks. tech. rep. ucsc-crl-94-25, university of california, santa cruz.

freund, y., & schapire, r. e. (1996). experiments with a new boosting algorithm. in machine learning:

proceedings of thirteenth international conference, pp. 148   156 usa. acm.

frey, b. j., hinton, g. e., & dayan, p. (1996). does the wake-sleep algorithm learn good density estimators?.
in touretzky, d., mozer, m., & hasselmo, m. (eds.), advances in neural information processing
systems 8 (nips   95), pp. 661   670. mit press, cambridge, ma.

fukushima, k. (1980). neocognitron: a self-organizing neural network model for a mechanism of pattern

recognition unaffected by shift in position. biological cybernetics, 36, 193   202.

gallinari, p., lecun, y., thiria, s., & fogelman-soulie, f. (1987). memoires associatives distribuees. in

proceedings of cognitiva 87 paris, la villette.

g  artner, t. (2003). a survey of kernels for structured data. acm sigkdd explorations newsletter, 5(1),

49   58.

geman, s., & geman, d. (1984). stochastic relaxation, gibbs distributions, and the bayesian restoration of

images. ieee transactions on pattern analysis and machine intelligence, 6, 721   741.

grosse, r., raina, r., kwong, h., & ng, a. y. (2007). shift-invariant sparse coding for audio classi   cation.

in proceedings of the 23th conference in uncertainty in arti   cial intelligence (uai   07).

hadsell, r., chopra, s., & lecun, y. (2006). id84 by learning an invariant mapping. in
proceedings of the id161 and pattern recognition conference (cvpr   06), pp. 1735   1742.
ieee press.

hadsell, r., erkan, a., sermanet, p., scof   er, m., muller, u., & lecun, y. (2008). deep belief net learning in
a long-range vision system for autonomous off-road driving. in proc. intelligent robots and systems
(iros   08), pp. 628   633.

hammersley, j. m., & clifford, p. (1971). markov    eld on    nite graphs and lattices. unpublished

manuscript.

h  astad, j. (1986). almost optimal lower bounds for small depth circuits. in proceedings of the 18th annual

acm symposium on theory of computing, pp. 6   20 berkeley, california. acm press.

h  astad, j., & goldmann, m. (1991). on the power of small-depth threshold circuits. computational com-

plexity, 1, 113   129.

hastie, t., rosset, s., tibshirani, r., & zhu, j. (2004). the entire id173 path for the support vector

machine. journal of machine learning research, 5, 1391   1415.

heller, k. a., & ghahramani, z. (2007). a nonparametric bayesian approach to modeling overlapping clus-
ters. in proceedings of the eleventh international conference on arti   cial intelligence and statistics
(aistats   07), pp. 187   194 san juan, porto rico. omnipress.

63

heller, k. a., williamson, s., & ghahramani, z. (2008). statistical models for partial membership.

in
cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-   fth international
conference on machine learning (icml   08), pp. 392   399. acm.

hinton, g. e., & sejnowski, t. j. (1986). learning and relearning in id82s. in rumelhart,
d. e., & mcclelland, j. l. (eds.), parallel distributed processing: explorations in the microstructure
of cognition. volume 1: foundations, pp. 282   317. mit press, cambridge, ma.

hinton, g. e., sejnowski, t. j., & ackley, d. h. (1984). id82s: id124
networks that learn. tech. rep. tr-cmu-cs-84-119, carnegie-mellon university, dept. of computer
science.

hinton, g. e., welling, m., teh, y. w., & osindero, s. (2001). a new view of ica. in proceedings of 3rd
international conference on independent component analysis and blind signal separation (ica   01),
pp. 746   751 san diego, ca.

hinton, g., & anderson, j. (1981). parallel models of associative memory. lawrence erlbaum assoc.,

hillsdale, nj.

hinton, g. e. (1986). learning distributed representations of concepts. in proceedings of the eighth annual
conference of the cognitive science society, pp. 1   12 amherst 1986. lawrence erlbaum, hillsdale.
hinton, g. e. (1999). products of experts. in proceedings of the ninth international conference on arti   cial

neural networks (icann), vol. 1, pp. 1   6 edinburgh, scotland. iee.

hinton, g. e. (2002). training products of experts by minimizing contrastive divergence. neural computa-

tion, 14, 1771   1800.

hinton, g. e. (2006). to recognize shapes,    rst learn to generate images. tech. rep. utml tr 2006-003,

university of toronto.

hinton, g. e., dayan, p., frey, b. j., & neal, r. m. (1995). the wake-sleep algorithm for unsupervised

neural networks. science, 268, 1558   1161.

hinton, g. e., & salakhutdinov, r. (2006a). reducing the dimensionality of data with neural networks.

science, 313(5786), 504   507.

hinton, g. e., & salakhutdinov, r. (2006b). reducing the dimensionality of data with neural networks.

science, 313, 504   507.

hinton, g. e., & zemel, r. s. (1994). autoencoders, minimum description length, and helmholtz free
energy. in cowan, d., tesauro, g., & alspector, j. (eds.), advances in neural information processing
systems 6 (nips   93), pp. 3   10. morgan kaufmann publishers, inc.

hinton, g. e., osindero, s., & teh, y. (2006). a fast learning algorithm for deep belief nets. neural

computation, 18, 1527   1554.

ho, t. k. (1995). random decision forest.

in 3rd international conference on document analysis and

recognition (icdar   95), pp. 278   282 montreal, canada.

hochreiter, s. (1991). untersuchungen zu dynamischen neuronalen netzen. diploma thesis, institut f  ur

informatik, lehrstuhl prof. brauer, technische universit  at m  unchen..

hotelling, h. (1933). analysis of a complex of statistical variables into principal components. journal of

educational psychology, 24, 417   441, 498   520.

hubel, d. h., & wiesel, t. n. (1962). receptive    elds, binocular interaction, and functional architecture in

the cat   s visual cortex. journal of physiology (london), 160, 106   154.

hyv  arinen, a. (2005). estimation of non-normalized statistical models using score matching. journal of

machine learning research, 6, 695   709.

hyv  arinen, a. (2007a). connections between score matching, contrastive divergence, and pseudolikelihood

for continuous-valued variables. ieee transactions on neural networks, 18, 1529   1531.

64

hyv  arinen, a. (2007b). some extensions of score matching. computational statistics and data analysis,

51, 2499   2512.

hyv  arinen, a., karhunen, j., & oja, e. (2001). independent component analysis. wiley-interscience.
intrator, n., & edelman, s. (1996). how to make a low-dimensional representation suitable for diverse tasks.

connection science, special issue on transfer in neural networks, 8, 205   224.

jaakkola, t., & haussler, d. (1998). exploiting generative models in discriminative classi   ers. available
from http://www.cse.ucsc.edu/ haussler/pubs.html. preprint, dept.of computer science, univ. of cal-
ifornia. a shorter version is in advances in neural information processing systems 11.

japkowicz, n., hanson, s. j., & gluck, m. a. (2000). nonlinear autoassociation is not equivalent to pca.

neural computation, 12(3), 531   545.

jordan, m. i. (1998). learning in id114. kluwer, dordrecht, netherlands.
kavukcuoglu, k., ranzato, m., & lecun, y. (2008). fast id136 in sparse coding algorithms with applica-
tions to object recognition. tech. rep., computational and biological learning lab, courant institute,
nyu. tech report cbll-tr-2008-12-01.

kirkpatrick, s., jr., c. d. g., , & vecchi, m. p. (1983). optimization by simulated annealing. science, 220,

671   680.

k  oster, u., & hyv  arinen, a. (2007). a two-layer ica-like model estimated by score matching. in int. conf.

arti   cial neural networks (icann   2007), pp. 798   807.

krueger, k. a., & dayan, p. (2009). flexible shaping: how learning in small steps helps. cognition, 110,

380   394.

lanckriet, g., cristianini, n., bartlett, p., el gahoui, l., & jordan, m. (2002). learning the kernel matrix
with semi-de   nite programming. in sammut, c., & hoffmann, a. g. (eds.), proceedings of the nine-
teenth international conference on machine learning (icml   02), pp. 323   330. morgan kaufmann.
larochelle, h., & bengio, y. (2008). classi   cation using discriminative restricted id82s. in
cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-   fth international
conference on machine learning (icml   08), pp. 536   543. acm.

larochelle, h., bengio, y., louradour, j., & lamblin, p. (2009). exploring strategies for training deep neural

networks. journal of machine learning research, 10, 1   40.

larochelle, h., erhan, d., courville, a., bergstra, j., & bengio, y. (2007). an empirical evaluation of deep
architectures on problems with many factors of variation. in ghahramani, z. (ed.), proceedings of the
twenty-fourth international conference on machine learning (icml   07), pp. 473   480. acm.

lasserre, j. a., bishop, c. m., & minka, t. p. (2006). principled hybrids of generative and discriminative
models. in proceedings of the id161 and pattern recognition conference (cvpr   06), pp.
87   94 washington, dc, usa. ieee computer society.

le cun, y., bottou, l., bengio, y., & haffner, p. (1998). gradient-based learning applied to document

recognition. proceedings of the ieee, 86(11), 2278   2324.

le roux, n., & bengio, y. (2008). representational power of restricted id82s and deep belief

networks. neural computation, 20(6), 1631   1649.

lecun, y., bottou, l., orr, g. b., & m  uller, k.-r. (1998). ef   cient backprop. in orr, g. b., & m  uller,

k.-r. (eds.), neural networks: tricks of the trade, pp. 9   50. springer.

lecun, y. (1987). mod`eles connexionistes de l   apprentissage. ph.d. thesis, universit  e de paris vi.
lecun, y., boser, b., denker, j. s., henderson, d., howard, r. e., hubbard, w., & jackel, l. d. (1989).

id26 applied to handwritten zip code recognition. neural computation, 1(4), 541   551.

65

lecun, y., chopra, s., hadsell, r., ranzato, m.-a., & huang, f.-j. (2006). a tutorial on energy-based learn-
ing. in bakir, g., hofman, t., scholkopf, b., smola, a., & taskar, b. (eds.), predicting structured
data, pp. 191   246. mit press.

lecun, y., & huang, f. (2005). id168s for discriminative training of energy-based models.

in
cowell, r. g., & ghahramani, z. (eds.), proceedings of the tenth international workshop on arti   cial
intelligence and statistics (aistats   05).

lecun, y., huang, f.-j., & bottou, l. (2004). learning methods for generic object recognition with invari-
ance to pose and lighting. in proceedings of the id161 and pattern recognition conference
(cvpr   04), vol. 2, pp. 97   104 los alamitos, ca, usa. ieee computer society.
lee, h., battle, a., raina, r., & ng, a. (2007). ef   cient sparse coding algorithms.

in sch  olkopf, b.,
platt, j., & hoffman, t. (eds.), advances in neural information processing systems 19 (nips   06), pp.
801   808. mit press.

lee, h., ekanadham, c., & ng, a. (2008). sparse deep belief net model for visual area v2. in platt, j.,
koller, d., singer, y., & roweis, s. (eds.), advances in neural information processing systems 20
(nips   07). mit press, cambridge, ma.

lee, h., grosse, r., ranganath, r., & ng, a. y. (2009). convolutional id50 for scalable un-
supervised learning of hierarchical representations. in bottou, l., & littman, m. (eds.), proceedings
of the twenty-sixth international conference on machine learning (icml   09). acm, montreal (qc),
canada.

lee, t.-s., & mumford, d. (2003). hierarchical bayesian id136 in the visual cortex. journal of optical

society of america, a, 20(7), 1434   1448.

lennie, p. (2003). the cost of cortical computation. current biology, 13(6), 493   497.
levner, i. (2008). data driven object segmentation. ph.d. thesis, department of computer science, uni-

versity of alberta.

lewicki, m., & sejnowski, t. (1998). learning nonlinear overcomplete representations for ef   cient coding.
in jordan, m., kearns, m., & solla, s. (eds.), advances in neural information processing systems 10
(nips   97), pp. 556   562 cambridge, ma, usa. mit press.

lewicki, m. s., & sejnowski, t. j. (2000). learning overcomplete representations. neural computation,

12(2), 337   365.

li, m., & vitanyi, p. (1997). an introduction to kolmogorov complexity and its applications. second

edition, springer, new york, ny.

liang, p., & jordan, m. i. (2008). an asymptotic analysis of generative, discriminative, and pseudolikelihood
estimators. in cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-   fth
international conference on machine learning (icml   08), pp. 584   591 new york, ny, usa. acm.
lin, t., horne, b. g., tino, p., & giles, c. l. (1995). learning long-term dependencies is not as dif   cult with
narx recurrent neural networks. tech. rep. umicas-tr-95-78, institute for advanced computer
studies, university of mariland.

loosli, g., canu, s., & bottou, l. (2007). training invariant support vector machines using selective sam-
pling. in bottou, l., chapelle, o., decoste, d., & weston, j. (eds.), large scale kernel machines,
pp. 301   320. mit press, cambridge, ma.

mairal, j., bach, f., ponce, j., sapiro, g., & zisserman, a. (2009). supervised dictionary learning. in koller,
d., schuurmans, d., bengio, y., & bottou, l. (eds.), advances in neural information processing
systems 21 (nips   08), pp. 1033   1040. nips foundation.

mcclelland, j. l., & rumelhart, d. e. (1988). explorations in parallel distributed processing. mit press,

cambridge.

66

mcclelland, j. l., & rumelhart, d. e. (1981). an interactive activation model of context effects in letter

perception. psychological review, 88, 375   407.

mcclelland, j. l., rumelhart, d. e., & the pdp research group (1986). parallel distributed processing:

explorations in the microstructure of cognition, vol. 2. mit press, cambridge.

mcculloch, w. s., & pitts, w. (1943). a logical calculus of ideas immanent in nervous activity. bulletin of

mathematical biophysics, 5, 115   133.

memisevic, r., & hinton, g. e. (2007). unsupervised learning of image transformations. in proceedings of

the id161 and pattern recognition conference (cvpr   07).

mendelson, e. (1997). introduction to mathematical logic, 4th ed. chapman & hall.
miikkulainen, r., & dyer, m. g. (1991). natural language processing with modular pdp networks and

distributed lexicon. cognitive science, 15, 343   399.

mnih, a., & hinton, g. e. (2007). three new id114 for statistical language modelling.

in
ghahramani, z. (ed.), proceedings of the twenty-fourth international conference on machine learn-
ing (icml   07), pp. 641   648. acm.

mnih, a., & hinton, g. e. (2009). a scalable hierarchical distributed language model.

in koller, d.,
schuurmans, d., bengio, y., & bottou, l. (eds.), advances in neural information processing systems
21 (nips   08), pp. 1081   1088.

mobahi, h., collobert, r., & weston, j. (2009). deep learning from temporal coherence in video. in bottou,
l., & littman, m. (eds.), proceedings of the 26th international conference on machine learning, pp.
737   744 montreal. omnipress.

more, j., & wu, z. (1996). smoothing techniques for macromolecular global optimization. in pillo, g. d.,

& giannessi, f. (eds.), nonlinear optimization and applications. plenum press.

murray, i., & salakhutdinov, r. (2009). evaluating probabilities under high-dimensional latent variable
models. in koller, d., schuurmans, d., bengio, y., & bottou, l. (eds.), advances in neural informa-
tion processing systems 21 (nips   08), vol. 21, pp. 1137   1144.

mutch, j., & lowe, d. g. (2008). object class recognition and localization using sparse features with limited

receptive    elds. international journal of id161, 80(1), 45   57.

neal, r. m. (1992). connectionist learning of belief networks. arti   cial intelligence, 56, 71   113.
neal, r. m. (1994). bayesian learning for neural networks. ph.d. thesis, dept. of computer science,

university of toronto.

ng, a. y., & jordan, m. i. (2002). on discriminative vs. generative classi   ers: a comparison of logistic
regression and naive bayes. in dietterich, t., becker, s., & ghahramani, z. (eds.), advances in neural
information processing systems 14 (nips   01), pp. 841   848.

niebles, j., & fei-fei, l. (2007). a hierarchical model of shape and appearance for human action classi   -

cation.. in proceedings of the id161 and pattern recognition conference (cvpr   07).

olshausen, b. a., & field, d. j. (1997). sparse coding with an overcomplete basis set: a strategy employed

by v1?. vision research, 37, 3311   3325.

orponen, p. (1994). computational complexity of neural networks: a survey. nordic journal of computing,

1(1), 94   110.

osindero, s., & hinton, g. e. (2008). modeling image patches with a directed hierarchy of markov ran-
dom    eld. in platt, j., koller, d., singer, y., & roweis, s. (eds.), advances in neural information
processing systems 20 (nips   07), pp. 1121   1128 cambridge, ma. mit press.

pearlmutter, b., & parra, l. c. (1996). a context-sensitive generalization of ica. in xu, l. (ed.), interna-

tional conference on neural information processing, pp. 151   157 hong-kong.

67

p  erez, e., & rendell, l. a. (1996). learning despite concept variation by    nding structure in attribute-based
data. in saitta, l. (ed.), proceedings of the thirteenth international conference on machine learning
(icml   96), pp. 391   399. morgan kaufmann.

peterson, g. b. (2004). a day of great illumination: b. f. skinner   s discovery of shaping. journal of the

experimental analysis of behavior, 82(3), 317   328.

pinto, n., dicarlo, j., & cox, d. (2008). establishing good benchmarks and baselines for face recognition. in
eccv 2008 faces in    real-life    images workshop marseille france. erik learned-miller and andras
ferencz and fr  ed  eric jurie.

pollack, j. b. (1990). recursive distributed representations. arti   cial intelligence, 46(1), 77   105.
rabiner, l. r., & juang, b. h. (1986). an introduction to id48. ieee assp magazine,

257   285.

raina, r., battle, a., lee, h., packer, b., & ng, a. y. (2007). self-taught learning: id21 from
unlabeled data. in ghahramani, z. (ed.), proceedings of the twenty-fourth international conference
on machine learning (icml   07), pp. 759   766. acm.

ranzato, m., boureau, y., chopra, s., & lecun, y. (2007). a uni   ed energy-based framework for unsuper-
vised learning. in proceedings of the eleventh international conference on arti   cial intelligence and
statistics (aistats   07) san juan, porto rico. omnipress.

ranzato, m., boureau, y.-l., & lecun, y. (2008). sparse id171 for id50. in platt,
j., koller, d., singer, y., & roweis, s. (eds.), advances in neural information processing systems 20
(nips   07), pp. 1185   1192 cambridge, ma. mit press.

ranzato, m., huang, f., boureau, y., & lecun, y. (2007). unsupervised learning of invariant feature hier-
archies with applications to object recognition. in proceedings of the id161 and pattern
recognition conference (cvpr   07). ieee press.

ranzato, m., & lecun, y. (2007). a sparse and locally shift invariant feature extractor applied to document
images. in international conference on document analysis and recognition (icdar   07), pp. 1213   
1217 washington, dc, usa. ieee computer society.

ranzato, m., poultney, c., chopra, s., & lecun, y. (2007). ef   cient learning of sparse representations
with an energy-based model. in sch  olkopf, b., platt, j., & hoffman, t. (eds.), advances in neural
information processing systems 19 (nips   06), pp. 1137   1144. mit press.

ranzato, m., & szummer, m. (2008). semi-supervised learning of compact id194s with
deep networks. in cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-
   fth international conference on machine learning (icml   08), vol. 307 of acm international con-
ference proceeding series, pp. 792   799. acm.

roweis, s., & saul, l. k. (2000). nonlinear id84 by locally linear embedding. science,

290(5500), 2323   2326.

rumelhart, d. e., mcclelland, j. l., & the pdp research group (1986a). parallel distributed processing:

explorations in the microstructure of cognition, vol. 1. mit press, cambridge.

rumelhart, d. e., hinton, g. e., & williams, r. j. (1986b). learning representations by back-propagating

errors. nature, 323, 533   536.

salakhutdinov, r., & hinton, g. e. (2007a). learning a nonlinear embedding by preserving class neighbour-
hood structure. in proceedings of the eleventh international conference on arti   cial intelligence and
statistics (aistats   07) san juan, porto rico. omnipress.

salakhutdinov, r., & hinton, g. e. (2007b). semantic hashing. in proceedings of the 2007 workshop on

information retrieval and applications of id114 (sigir 2007) amsterdam. elsevier.

68

salakhutdinov, r., & hinton, g. e. (2008). using deep belief nets to learn covariance kernels for gaussian
processes. in platt, j., koller, d., singer, y., & roweis, s. (eds.), advances in neural information
processing systems 20 (nips   07), pp. 1249   1256 cambridge, ma. mit press.

salakhutdinov, r., & hinton, g. e. (2009). deep id82s.

in proceedings of the twelfth
international conference on arti   cial intelligence and statistics (aistats   09), vol. 5, pp. 448   455.
salakhutdinov, r., mnih, a., & hinton, g. e. (2007). restricted id82s for collaborative
in ghahramani, z. (ed.), proceedings of the twenty-fourth international conference on

   ltering.
machine learning (icml   07), pp. 791   798 new york, ny, usa. acm.

salakhutdinov, r., & murray, i. (2008). on the quantitative analysis of id50. in cohen,
w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-   fth international confer-
ence on machine learning (icml   08), vol. 25, pp. 872   879. acm.

saul, l. k., jaakkola, t., & jordan, m. i. (1996). mean    eld theory for sigmoid belief networks. journal of

arti   cial intelligence research, 4, 61   76.

schmitt, m. (2002). descartes    rule of signs for radial basis function neural networks. neural computation,

14(12), 2997   3011.

sch  olkopf, b., burges, c. j. c., & smola, a. j. (1999a). advances in kernel methods     support vector

learning. mit press, cambridge, ma.

sch  olkopf, b., mika, s., burges, c., knirsch, p., m  uller, k.-r., r  atsch, g., & smola, a. (1999b). input space

versus feature space in kernel-based methods. ieee trans. neural networks, 10(5), 1000   1017.

sch  olkopf, b., smola, a., & m  uller, k.-r. (1998). nonlinear component analysis as a kernel eigenvalue

problem. neural computation, 10, 1299   1319.

schwenk, h., & gauvain, j.-l. (2002). connectionist id38 for large vocabulary continu-
in international conference on acoustics, speech and signal processing

ous id103.
(icassp), pp. 765   768 orlando, florida.

schwenk, h., & milgram, m. (1995). transformation invariant autoassociation with application to hand-
written character recognition. in tesauro, g., touretzky, d., & leen, t. (eds.), advances in neural
information processing systems 7 (nips   94), pp. 991   998. mit press.

schwenk, h. (2004). ef   cient training of large neural networks for id38. in international

joint conference on neural networks (ijid98), vol. 4, pp. 3050   3064.

schwenk, h., & gauvain, j.-l. (2005). building continuous space language models for transcribing european

languages. in interspeech, pp. 737   740.

serre, t., kreiman, g., kouh, m., cadieu, c., knoblich, u., & poggio, t. (2007). a quantitative theory of
immediate visual recognition. progress in brain research, computational neuroscience: theoretical
insights into brain function, 165, 33   56.

seung, s. h. (1998). learning continuous attractors in recurrent networks. in jordan, m., kearns, m., &
solla, s. (eds.), advances in neural information processing systems 10 (nips   97), pp. 654   660. mit
press.

simard, d., steinkraus, p. y., & platt, j. c. (2003). best practices for convolutional neural networks. in
international conference on document analysis and recognition (icdar   03), p. 958 washington,
dc, usa. ieee computer society.

simard, p. y., lecun, y., & denker, j. (1993). ef   cient pattern recognition using a new transformation
distance. in giles, c., hanson, s., & cowan, j. (eds.), advances in neural information processing
systems 5 (nips   92), pp. 50   58. morgan kaufmann, san mateo.

skinner, b. f. (1958). reinforcement today. american psychologist, 13, 94   99.

69

smolensky, p. (1986). information processing in dynamical systems: foundations of harmony theory. in
rumelhart, d. e., & mcclelland, j. l. (eds.), parallel distributed processing, vol. 1, chap. 6, pp.
194   281. mit press, cambridge.

sudderth, e. b., torralba, a., freeman, w. t., & willsky, a. s. (2007). describing visual scenes using

transformed objects and parts. int. journal of id161, 77, 291   330.

sutskever, i., & hinton, g. e. (2007). learning multilevel distributed representations for high-dimensional
in proceedings of the eleventh international conference on arti   cial intelligence and

sequences.
statistics (aistats   07) san juan, porto rico. omnipress.

sutton, r., & barto, a. (1998). id23: an introduction. mit press.
taylor, g., & hinton, g. (2009). factored conditional restricted id82s for modeling motion
in bottou, l., & littman, m. (eds.), proceedings of the 26th international conference on

style.
machine learning (icml   09), pp. 1025   1032 montreal. omnipress.

taylor, g., hinton, g. e., & roweis, s. (2007). modeling human motion using binary latent variables. in
sch  olkopf, b., platt, j., & hoffman, t. (eds.), advances in neural information processing systems 19
(nips   06), pp. 1345   1352. mit press, cambridge, ma.

teh, y., welling, m., osindero, s., & hinton, g. e. (2003). energy-based models for sparse overcomplete

representations. journal of machine learning research, 4, 1235   1260.

tenenbaum, j., de silva, v., & langford, j. c. (2000). a global geometric framework for nonlinear dimen-

sionality reduction. science, 290(5500), 2319   2323.

thrun, s. (1996). is learning the n-th thing any easier than learning the    rst?. in touretzky, d., mozer, m., &
hasselmo, m. (eds.), advances in neural information processing systems 8 (nips   95), pp. 640   646
cambridge, ma. mit press.

tieleman, t. (2008). training restricted id82s using approximations to the likelihood gradi-
ent. in cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-   fth interna-
tional conference on machine learning (icml   08), pp. 1064   1071. acm.

tieleman, t., & hinton, g. (2009). using fast weights to improve persistent contrastive divergence. in bot-
tou, l., & littman, m. (eds.), proceedings of the twenty-sixth international conference on machine
learning (icml   09), pp. 1033   1040 new york, ny, usa. acm.

titov, i., & henderson, j. (2007). constituent parsing with incremental sigmoid belief networks. in proc.
45th meeting of association for computational linguistics (acl   07), pp. 632   639 prague, czech
republic.

torralba, a., fergus, r., & weiss, y. (2008). small codes and large databases for recognition. in proceedings

of the id161 and pattern recognition conference (cvpr   08), pp. 1   8.

utgoff, p. e., & stracuzzi, d. j. (2002). many-layered learning. neural computation, 14, 2497   2539.
van der maaten, l., & hinton, g. e. (2008). visualizing data using id167. journal of machine learning

research, 9, 2579   2605.

vapnik, v. n. (1995). the nature of statistical learning theory. springer, new york.
vilalta, r., blix, g., & rendell, l. (1997). global data analysis and the fragmentation problem in decision
tree induction. in proceedings of the 9th european conference on machine learning (ecml   97), pp.
312   327. springer-verlag.

vincent, p., larochelle, h., bengio, y., & manzagol, p.-a. (2008). extracting and composing robust features
with denoising autoencoders. in cohen, w. w., mccallum, a., & roweis, s. t. (eds.), proceedings
of the twenty-   fth international conference on machine learning (icml   08), pp. 1096   1103. acm.
wang, l., & chan, k. l. (2002). learning kernel parameters by using class separability measure. 6th kernel

machines workshop, in conjunction with neural information processing systems (nips).

70

weber, m., welling, m., & perona, p. (2000). unsupervised learning of models for recognition. in proc. 6th

europ. conf. comp. vis., eccv2000, pp. 18   32 dublin.

wegener, i. (1987). the complexity of boolean functions. john wiley & sons.
weiss, y. (1999). segmentation using eigenvectors: a unifying view. in proceedings ieee international

conference on id161 (iccv   99), pp. 975   982.

welling, m., rosen-zvi, m., & hinton, g. e. (2005). exponential family harmoniums with an application
to information retrieval. in saul, l., weiss, y., & bottou, l. (eds.), advances in neural information
processing systems 17 (nips   04), pp. 1481   1488 cambridge, ma. mit press.

welling, m., zemel, r., & hinton, g. e. (2003). self-supervised boosting. in becker, s., thrun, s., &
obermayer, k. (eds.), advances in neural information processing systems 15 (nips   02), pp. 665   
672. mit press.

weston, j., ratle, f., & collobert, r. (2008). deep learning via semi-supervised embedding. in cohen,
w. w., mccallum, a., & roweis, s. t. (eds.), proceedings of the twenty-   fth international confer-
ence on machine learning (icml   08), pp. 1168   1175 new york, ny, usa. acm.

williams, c. k. i., & rasmussen, c. e. (1996). gaussian processes for regression. in touretzky, d., mozer,
m., & hasselmo, m. (eds.), advances in neural information processing systems 8 (nips   95), pp.
514   520. mit press, cambridge, ma.

wiskott, l., & sejnowski, t. j. (2002). slow feature analysis: unsupervised learning of invariances. neural

computation, 14(4), 715   770.

wolpert, d. h. (1992). stacked generalization. neural networks, 5, 241   249.
wu, z. (1997). global continuation for distance geometry problems. siam journal of optimization, 7,

814   836.

xu, p., emami, a., & jelinek, f. (2003). training connectionist models for the structured language model.
in proceedings of the 2003 conference on empirical methods in natural language processing
(emnlp   2003), vol. 10, pp. 160   167.

yao, a. (1985). separating the polynomial-time hierarchy by oracles. in proceedings of the 26th annual

ieee symposium on foundations of computer science, pp. 1   10.

zhou, d., bousquet, o., navin lal, t., weston, j., & sch  olkopf, b. (2004). learning with local and global
consistency. in thrun, s., saul, l., & sch  olkopf, b. (eds.), advances in neural information processing
systems 16 (nips   03), pp. 321   328 cambridge, ma. mit press.

zhu, x., ghahramani, z., & lafferty, j. (2003). semi-supervised learning using gaussian    elds and harmonic
functions. in fawcett, t., & mishra, n. (eds.), proceedings of the twenty international conference
on machine learning (icml   03), pp. 912   919. aaai press.

zinkevich, m. (2003). online convex programming and generalized in   nitesimal gradient ascent. in fawcett,
t., & mishra, n. (eds.), proceedings of the twenty international conference on machine learning
(icml   03), pp. 928   936. aaai press.

71

