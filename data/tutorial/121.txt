id31 in practice 

yongzheng (tiger) zhang 

dan shen* 

catherine baudin 

 

december 12, 2011@icdm   11 

* dianping.com 

outline 

1.

introduction 

2. sentiment identification & classification 

3. key applications 

4. examples 

5. conclusions 

2 

roadmap 

1.

introduction 

2. sentiment identification & classification 

3. key applications 

4. examples 

5. conclusions 

3 

example 1     a wikipedia entry 

4 

example 2     a milo product review 

5 

facts vs. opinions 

    facts: objective expressions about entities, events and 

their attributes, e.g.    i bought an iphone yesterday    

    opinions: subjective expressions of sentiments, 

attitudes, emotions, appraisals or feelings toward 
entities, events and their attributes, e.g.    i really love this 
new camera    

6 

facts vs. opinions     an article from nyt 

   

http://www.nytimes.com/2009/08/24/technology/internet/24emotion.html?_r=2&pagewanted=all 

 

7 

some exceptions! 

    not all subjective sentences contain opinions, e.g.  

       i want a phone with good voice quality    

    not all objective sentences contain no opinions, e.g. 

       the earphone broke in just two days!    

8 

history of id31 

    individuals relied on a circle of  

    family 

    friends 

    colleagues 

    organizations used 

    polls 

    surveys 

    focus groups 

    id31 is growing rapidly 

9 

why now? 

    www has facilitated user-generated content 

    e-commerce sites 

    forums 

    discussion groups 

    blogs 

    twitter, etc. 

    advances in nlp and text-processing 

    distributed computing: e.g. hadoop, cloud 

    our focus: automatically mining opinions, challenging but useful 

10 

opinions and market research 

http://www.fastcompany.com

/blog/kevin-
randall/integrated-
branding/market-research-
30-here-attitudes-meet-
algorithms-sentiment-
a?partner=rss is from a 
market-researcher   s 
perspective 

practical applications     business & 
organizations 

    brand analysis 

    marketing 

    customer voice: e.g. products, tourism 

    event monitoring, e.g. site outage 

    commercial examples 

    radian6 

    lexalytics 

12 

practical applications     individuals 

    shopping research, e.g. product reviews 

    http://www1.epinions.com/prices/canon_powershot_sd1300_is_ixus_105_digital_camera 

 

13 

practical applications     individuals (cont.) 

    seeking out opinions on topics such as finance 

   

http://community.nytimes.com/comments/bucks.blogs.nytimes.com/2011/03/10/where-all-the-mortgage-documents-
go/?scp=2&sq=refinance%20a%20loan&st=cse 

14 

practical applications     advertising  

    placing ads in user-generated content 

    http://www.zurich-hotels-booker.com/why-get-a-canon-powershot-a1200-98757.html 

 

15 

practical applications     opinion search  

    providing general search for opinions, not facts 

    tons of social media applications 

    ted 

    twecan 

    socialmention 

    tweetsentiments 

16 

opinion search     one more example 

17 

the problem 

    a review example: (1) i bought an iphone a few days ago. (2) it 

was such a nice phone. (3) the touch screen was really cool. (4) 
the voice quality was clear too. (5) although the battery life was not 
long, that is ok for me. (6) however, my mother was mad with me as 
i did not tell her before i bought it. (7) she also thought the phone 
was too expensive, and wanted me to return it to the shop. . . . 

    facts vs. opinions 

    opinions have targets (objects and their attributes) on 

which opinions are expressed 

18 

definitions 

    object: an entity that can be a product, service, individual, 

organization, event, or topic, e.g. iphone 

    attribute: an object usually has two types of attributes 

    components, e.g. battery, keypad/touch screen 
    properties, e.g. size, weight, color, voice quality 

    explicit and implicit attributes 

    explicit attributes: those appearing in the opinion, e.g.    the battery 

life of this phone is too short    

    implicit attributes: those not appearing in the opinion, e.g.    this 

phone is too large    (on attribute size) 

    opinion holder: the person or organization that expresses the opinion 

    opinion orientation (polarity): positive, negative, or neutral 

    opinion strength: level/scale/intensity of opinion indicating how 

strong it is, e.g. contented     happy     joyous     ecstatic 

19 

key elements of an opinion 

    opinion: a person or organization that expresses a positive or 

negative sentiment on a particular attribute of an object at a certain 
time 

    quintuple: <object, attribute, orientation, opinion holder, 

time> 

    some information may be implied due to pronouns, context, or 

language conventions 

    some information available from document attributes 

    in practice, not all five elements are needed 

20 

direct vs. comparative opinions 

    direct opinion: sentiment expressions on one or more attributes 

of an object, e.g. products, services, events 

       the voice quality of this phone is fantastic    

       after taking this medicine, my left knee feels worse    

    comparative opinion: relations expressing similarities or 

differences between two or more objects based on some of the 
shared attributes of the objects, e.g. 

       the voice quality of camera x is better than that of camera y    

    there are some difficult cases which are not covered, 

e.g.    the view finder and the lens of the camera are too 
close to each other    

21 

explicit vs. implicit opinions 

    sentence subjectivity: an objective sentence expresses some 

factual information about the world, while a subjective sentence 
expresses some personal feelings or beliefs 

    explicit opinion: an opinion on an attribute explicitly expressed in 

a subjective sentence, e.g. 

       the voice quality of this phone is amazing    

       this camera is too heavy    

    implicit opinion: an opinion on an attribute implied in an objective 

sentence, e.g. 

       the headset broke in two days    

       please bring back the old search    

22 

outline 

1.

introduction 

2. sentiment identification & classification 

3. key applications 

4. examples 

5. conclusions 

23 

a product review example 

    an epinions.com product review for canon powershot 

sd1300 is / digital ixus 105 digital camera 

     my canon powershot takes great pictures!     my friend had 

gotten one about a year ago and she loves it. so, after seeing her 
enthusiasm about it i decided to get one and i will never go back 
to any other camera. i absolutely love this camera. i believe that 
every person on earth should own one of these.     it is amazing! 
... there is not one thing i hate about this product, which is 
strange because i am a very picky person!        

    what do we see in this example? 

24 

id31 tasks 

    goal: identify and classify opinions 

    task 1. sentiment identification (subjectivity 

identification): identify whether a piece of text expresses 
opinions 

    task 2. sentiment orientation classification: determine 

the orientation of an opinionated text 

25 

id31 levels 

    document-level: identify if the document (e.g. product 
reviews, blogs, forum posts) expresses opinions and 
whether the opinions are positive, negative, or neutral 

    sentence-level: identify if a sentence is opinionated and 

whether the opinion is positive, negative, or neutral 

    attribute-level: extract the object attributes (e.g. image 

quality, zoom size) that are the subject of an opinion and 
the opinion orientations 

    as the object becomes more granular, the 

intensity/difficulty increases 

26 

document-level id31 

    tasks: identify if the document expresses opinions and if 

yes classify the document into positive, negative, or 
neutral based on the overall sentiments expressed by 
opinion holders 

    assumptions:  

    the document is opinionated on a single object 
    the opinions are from a single opinion holder 

    similar to but different from topic-based text 

classification 
    in topic-based text classification, topic words are important 
    in sentiment classification, opinion words are more important, 

e.g. wonderful, fabulous, terrible 

27 

opinion words 

    also known as polarity words, sentiment words, opinion 

lexicon, or opinion-bearing words, e.g. 

    positive: wonderful, elegant, amazing 

    negative: horrible, disgusting, poor 

    base type (examples above) and comparative type (e.g. 

better, worse) 

    how to generate them: more on this later 

28 

a simple method     counting opinion words 

    opinion/polarity words: dominating indicators of 

sentiments, especially adjectives, adverbs, and verbs, 
e.g.    i absolutely love this camera. it is amazing!   . 

    pre-defined opinion words: good, terrible,     (more on 

this later) 

    assign orientation score (+1, -1) to all words 

    positive opinion words (+1): great, amazing, love 
    negative opinion words (-1): horrible, hate 
    strength value [0, 1] can be used too 

    the orientation score of the document is the sum of 

orientation scores of all opinion words found 
    the previous review has an orientation score of 4     1 = 3 

 

29 

rule-based method 

    is simply counting opinion words good enough? no! 
       there is not one thing i hate about this product        wrong 

    we need to handle negation:    not     hate    implies like 

    simple rules can be manually created 

       not     negative        positive 

       never     negative        positive 

    the previous review has a score of 4 + 1 = 5 

    note: negation needs to be handled with care, e.g.    not    in    not 

only     but also    does not change the orientation 

30 

terminology 

    pattern/rule: a sequence of tokens 

    token: an abstraction of a word, represented using 

lemma of a word, polarity tag, or part of speech (pos) 
[brill, canlp1992] tag. two special tokens: 

    topic: an attribute, e.g. size, weight 

    gap_digit_digit: how many words can be skipping between two 

tokens to allow more tolerant matching, e.g. gap_1_2 

    polarity tag: positive, negative, neutral, not (negation) 

    pos tag: nn (noun), vb (verb), jj (adjective), rb 

(adverb), in (preposition)     

31 

basic opinion rules     label sequential 
pattern (lsp) matching 

    subject {like | adore | want | work} topic     positive, e.g. 

       i like the old camera    

    subject {is | are} {great | fantastic | simple | easy}     positive, e.g. 

       this camera is fantastic    

    topic gap_0_3 not work     negative, e.g. 

       the new search still does not work    

    please do not vb     negative, e.g. 
       please do not roll out this new search!    

    not gap_0_3 {want | think | believe | need | get}     negative, e.g. 

       i do not want large size pictures in the gallery window    

    {get | bring | give | put | change} gap_0_3 topic gap_0_3 back     

positive 
       please put the old search and browse back!    

32 

limitation of rule-based system 

    an expensive task: 

    only a limited number of opinion words can be found 

    only a limited number of patterns can be created 

    can we automate the task with limited manual work? 

e.g. find opinion words and their orientations 
automatically 

 

33 

automatically finding opinion words [turney, acl2002] 

    data: reviews from epinions.com on automobiles, banks, 

movies and travel destinations 

    step 1. perform part of speech (pos) tagging and extract 

phrases containing adjectives and adverbs based on 
manually specified patterns 
table 1. patterns of pos tags for extracting two-word phrases 
 
 

 
 
 
 
 

    e.g. extract two consecutive words if the first word is adjective, the 
second is a noun and the third (which is not extracted) is anything: 
   this camera produces beautiful pictures    

34 

pointwise mutual information (pmi) 

    step 2. estimate the orientation of each extracted phrase 

using the pmi measure 

    pmi is the amount of information that we acquire about the 

presence of one of the words when we observe the other 

 

 

 

 

    the opinion orientation (oo) of a phrase is computed based on 

its association with the positive reference word    excellent    and its 
association with the negative reference word    poor    

35 

pmi (cont.) 

    estimate probabilities with number of hits of search query 

    for each search query, search engine returns the number of 
relevant documents to the query, which is the number of hits 

    turney used altavista which had a near operator, 

which constrains the search to documents that contain 
the words within 10 words of one another, in either order 

 

    examples: 

       low fees   ,    jj nns   , 0.333 

       unethical practices   ,    jj nns   , -8.484 

       low funds   ,    jj nns   , -6.843 

36 

sentiment orientation classification 

    step 3. compute the average semantic orientation of all 

phrases in the review 

    classify as positive (recommended) or negative (not 

recommended) based on the sign of the average 

    final classification accuracy: 

    automobiles     84% 

    banks     80% 

    movies     66% 

    travel destinations     71% 

  note: recent variations use more than two words to determine the 

orientation 

37 

polarity words generation 

    manual: effective but expensive 

    dictionary-based: use a seed list and grow the list, e.g. 

    sentiid138 

    corpus-based: rely on syntactic or co-occurrence 

patterns in large text corpora [hazivassiloglou & mckeown, acl1997; turney, 

acl2002; yu & hazivassiloglou, emnlp2003, kanayama & nasukawa, emnlp2006; ding & liu, sigir2007] 

38 

anew: affective norms for english words 

http://csea.phhp.ufl.edu/media/anewmessage.html is specific to english 

dictionary-based approach     id64 

    step 1. manually collect a small set of opinion words with 

known orientations 

    {glad} 

    step 2. search an online dictionary (e.g. id138 [fellbaumc1998]) 
for their synonyms and antonyms to grow the word set [hu & liu, 
kdd2004; kim & hovy, coling2004] 

    {glad, happy, joyful, delighted}; {sad, unhappy, sorry, heart-broken} 

    repeat steps 1 & 2 until no more new words are found 

    finally, manual inspection may be done for correction 

    additional information (e.g. glosses) from id138 [andreevskaia & 

bergler, eacl2006] can be used 

 

 

40 

limitations of dictionary-based approach 

    cannot identify context-dependent opinion words 

    example 1: small 

       the lcd screen is too small    vs. 

       the camera is very small and easy to carry    

    example 2: long 

       it takes a long time to focus    vs.  

       the battery life is long    

 

 

 

41 

corpus-based approach 

    rely on syntactic or co-occurrence patterns in large text 
corpora [hazivassiloglou & mckeown, acl1997; turney, acl2002; yu & hazivassiloglou, emnlp2003, 

kanayama & nasukawa, emnlp2006; ding & liu, sigir2007] 

    can find domain dependent orientations and/or context 

dependent ones! 

42 

an example     predicting the sentiment 
orientations of adjectives [hazivassiloglou & mckeown, acl1997] 

    start with a list of seed opinion adjective words 

    use linguistic constraints on connectives to identify additional 

adjective opinion words and their orientations 
    sentiment consistency: conjoined adjectives usually have the same 

orientations     this car is beautiful and spacious. (if beautiful is 
positive, then spacious is positive too) 

    rules can be designed for different connectives: and, or, but, 

either-or, neither-nor 

    use log-linear model to determine if two conjoined adjectives 

are of the same or different orientations 

    use id91 to produce two sets of words, i.e. positive and 

negative 

    data corpus: 21 million words from 1987 wall street journal 

43 

another example     handling context 
dependency [ding & liu, 2007, lu et al, www2011] 

    finding domain opinion words is not sufficient 

    one word may indicate different opinions in the same domain, 
e.g.    the battery life is long    vs.    it takes a long time to focus   . 

    opinion context idea: use pairs of (object_attribute, 

opinion_word) 

    determining opinion words and their orientations 

together with the object attributes 

    it can be used without a large corpus 

 

44 

manual     automated: can we learn from 
examples? 

45 

supervised learning 

training documents 

machine learning 

46 

new/test 
documents 

 
classifier 

supervised learning (cont.) 

    a machine learning technique: find patterns in known 

examples and apply to new documents 

    training and testing examples 

    a set of data features to represent documents 

    learning goal: target classes (e.g. positive vs. negative) 

    product reviews domain is very common 

    positive: 4-5 stars (thumbs up) 

    negative: 1-2 stars (thumbs down) 

47 

supervised learning     feature extraction 

    terms and their frequency 

    unigram and more general id165s 
    word position information 
    term frequency     inverse  document frequency (tfidf) weighting 

    part of speech tags: adjectives are usually important indicators of 

subjectivities and opinions 

    syntactic dependency, e.g. 

    syntactic parsing tree 

 

 

 

 

    for a more comprehensive survey of attributes, see [pang and lee, fatir2008] 

48 

popular supervised learning methods 

    na  ve bayes (nb): 

    a simple probabilistic classifier based on applying bayes' 

theorem with strong (naive) independence assumptions 

    maximum id178 (me) 

    a probabilistic model that estimates the conditional distribution of 

the class label 

    support vector machines (id166) [pang et al, emnlp2002] 

    a representation of the examples as points in space in which 

support vectors are computed to provide a best division of 
points/examples into categories 

    id28 (lr) model [pang & lee, acl 2005] 

    a lr model predicts the classes from a set of variables that may 

be continuous, discrete, or a mixture 

49 

domain dependency issue 

    a classifier trained using opinionated documents from 

domain a often performs poorly when tested on documents 
from domain b 

    reason1: words used in different domains can be 

substantially different, e.g. 

    cars vs. movies; 

    cameras vs. strollers 

    reason 2: some words mean opposite in two domains, e.g.  

       unpredictable    may be negative in a car review, but positive in a 

movie review [turney, acl2002] 

       cheap    may be positive in a travel/lodging review, but negative in a 

toys review 

50 

id20 

    a well studied problem [aue & gamon, ranlp2005; blitzer et al, acl 2007; yang et al, 

trec2006] 

    step 1. use labeled data from one domain and unlabeled data 

from both source the target domain and general opinion words as 
features 

    step 2. choose a set of pivot features which occur frequently in 

both domains 

    step 3. model correlations between the pivot features and all 

other features by training linear pivot predictors to predict 
occurrences of each pivot in the unlabeled data from both 
domains 

51 

sentence-level id31 

    document level id31 is too coarse for most 

applications,     or    ? e.g. 

       i bought a new x phone yesterday. the voice quality is super 
and i really like it. however, it is a little bit heavy. plus, the key 
pad is too soft and it doesn   t feel comfortable. i think the image 
quality is good enough but i am not sure about the battery life       

    task: determine whether a sentence s is subjective or objective, 

and if s is subjective, determine whether its orientation is positive or 
negative 

    assumptions:  

    the sentence is opinionated on a single object 

    the opinion is from a single opinion holder 

52 

learning syntactic patterns [riloff & wiebe, emnlp2003] 

    first, use high prevision but low recall classifiers to 
automatically identify some subjective and objective 
sentences 
    a subjective classifier: the sentence contains two or more strong 

subjective clues 

    an objective classifier: the sentence contains no strong subjective 

clues 

    based on manually collected single words and id165s, which are 

good subjective clues 

    second, learn a set of patterns from subjective and objective 

sentences identified above 
    syntactic templates are used to restrict the kinds of patterns to be 

discovered, e.g. <subject> active-verb     the customer complained 

    third, the learned patterns are used to extract more 

subjective and objective sentences (the process can be 
repeated) 

53 

attribute-level id31 

    a positive/negative document does not mean the author 

likes/dislikes all attributes of the object [from bing liu   s chapter on sentiment 
analysis] 

 

 

 

 

 

 

    attributes can be product properties, important topics, etc. 

 

54 

topic identification     pre-defined 

    obtain topics from database, e.g. product properties 

(image size, quality, weight) 

    identify related topics, e.g. 

       the pictures are very clear        explicit attribute: picture 
       it is small enough to fit easily in a coat pocket or purse        

implicit topic: size 

    group related topics, e.g. 

    size <-> {large, small} 
    weight <-> {heavy, light} 

    require domain knowledge (metadata) 

55 

automated topic extraction     examples 

    term frequency (tf): favor frequently appearing words 

    term frequency     inverse document frequency 

(tfidf): favor words that appear frequently in one 
document but relatively rarely in the corpus 

    id44 (lda): identify topics that are 

characterized by a distribution of words [blei et al, jmlr2003] 

56 

how lda works 

    traditional model: document-term matrix (each document is a vector in 

the keyword space) 

 

 

 

 

    new model: 

    a new set of vectors to span the keyword space, which can better 

represent each document 

    from matrix theory, we know singular vectors of documents are such ones, 
and the related singular values represent the importance of each singular 
vector 

    these singular vectors can be thought as unit vectors to define a new 

coordinate system 

    keep the most important k singular vectors only 
    id84: project the documents to the new k-d space. 

    key: we combine the useful information from multiple keyword vectors and put 

it in one singular vector, so we need less vectors to represent the important 
information in the original collection 

57 

examples of extracted topics 

    a topic is characterized by a distribution over words 

orientation classification for each topic 

    determine orientation of the opinion on each topic in a 

sentence using any of the following: 

    counting polarity words 

    rule-based 

    supervise learning 

    aggregate all the opinions on each topic 

 

 

 

59 

comparative id31 

    motivation: opinions on multiple objects 

    sentence-based analysis 

    comparatives, e.g. 

       iphone4 is better than nexus    

       movie-x is less emotional than movie-y    

       bank-1's service is as good as bank-2's service    

       laptop-1 has a webcam, laptop-2 does not    

    superlatives 

       the battery life of this phone is the longest    

       this chair is the least stable    

 

60 

detection of comparisons 

    comparative adjectives, adverbs 

    more, less, words ending in -er, etc 

    superlative adjectives, adverbs 

    most, least, words ending in -est, etc 

    keywords 

    same, similar, etc 

    learning patterns around keywords 

    specialized sequence classification algorithms used [jindal & liu, 2006]  

61 

preferred object identification 

    objective of comparison 

    lexicon-based approach 

    lists of positive/negative words, comparatives 

    rules for combining them, e.g. decreasing comparative + negative 

word     positive opinion 

    domain knowledge, e.g. car-x gives higher mileage than car-y 

    areas of active research [ganapathibhotla & liu, 2008] 

 

62 

outline 

1.

introduction 

2. sentiment identification & classification 

3. key applications 

4. examples 

5. conclusions 

63 

key id31 applications 

    opinion summarization 

    opinion search & 

retrieval 

 

 

 

 

 

    opinion spam & opinion 

quality 

64 

opinion summarization 

    main objective: convert natural language text to 

structured summary to gain insights and consumer 
opinions 

    little academic research available but crucial to 

applications 

    e.g. movie review mining and summarization [zhuang et al, cikm2006] 

    visualization using bar chart or pie chart based on 

quintuples 

 

 

 

 

65 

structured opinion summary 

    an attribute-based summary of opinions on an object or 

multiple competing objects [hu & liu, kdd2004; liu et al, www2005; zhai et al, 

wsdm2011] 

 

 

 

66 

visualization of an attribute-based summary 

    more straightforward via visualization 

 

 

 

 

 

67 

comparing opinion summaries of multiple 
products 

    more interesting to see a comparison between two or 

more competing products 

 

 

 

 

 

 

    many other visualization approaches are available [pang & 

lee, fatir2008] 

 

68 

summaries without opinions 

    many types of summaries without opinions are also very useful and 

informative 

    attribute buzz summary: shows the relative frequency of attribute 

mentions, e.g. transaction security in an online banking study 

    object buzz summary: shows the frequency of mentions of different 

competing products 

    sentiment trending: reports the trend of sentiments over time 

69 

sentiment summarization using traditional 
techniques 

    research in traditional text summarization is rich, see 

document understanding conference (duc) 

    produce a textual summary for a single document or 

multiple documents based on 

    abstraction: generate natural language sentences using 

predefined templates, e.g.    most people are positive about cell 
phone 1 and negative about cell phone 2   . 

    extraction: keyword and key sentence extraction 

    limitation: qualitative but not quantitative 

70 

opinion search and retrieval 

    find public opinions on a particular object or an attribute 
of the object, e.g. customer opinions on ipad2 or japan 
nuclear crisis 

    find opinions of a person or organization (i.e. opinion 

holder) on a particular object or an attribute of the object, 
e.g. find barack obama   s opinion on japan nuclear crisis 

71 

main tasks 

    data acquisition: crawling and indexing web documents 

    document retrieval: retrieving relevant 

documents/sentences to the query (same as 
conventional search engines) 

    id31: determining whether the 

documents/sentences express opinions and whether the 
opinions are positive or negative (i.e. id31 

    opinion ranking: ranking opinionated documents 

according to certain criteria 

 

72 

data acquisition and cleansing 

    web page crawling 

    product reviews are usually easier to collect 

    forum posts and blogs require more careful handling 

    web page parsing 

    text extraction from web pages 

    document indexing 

73 

opinion ranking 

    traditional web search engines rank web pages based on 

authority and relevance scores [liu, wdm2006] 

    main objectives of opinion ranking: 

    rank opinionated documents/sentences with high utilities or 

information contents at the top 

    reflect the natural distribution of positive and negative opinions 

(same as in traditional opinion surveys) 

    simple solution: one ranking for positive opinions and one for 

negative ones, ratio of positive vs. negative as distribution 

    attribute-based summary for each opinion search is even 

better, unfortunately this is very difficult to achieve 

    comparison search will be useful too, e.g. opinions on yahoo! 

mail vs. gmail vs. hotmail 

74 

an opinion ranking example [zhang & yu, trec2007] 

    document retrieval 

    in addition to keywords, certain concepts (named entities and 
various types of phrases, e.g. wikipedia entries) are also used 

    id183 using synonyms of the search query and 

relevant words from top-ranked documents 

    similarity/relevance score of each document with the expanded 

query is calculated using both keywords and concepts 

    opinion classification: id166 

    classifying each document into opinionated or not: subjective 

reviews from rateitall.com and epinions.com as well as objective 
information from wikipedia are used as training data 

    classifying each opinionated document into positive, negative, or 

mixed: reviews with star ratings from rateitall.com are used as 
training examples 
 

75 

 

sentiment and search 

    http://www.nytimes.com/2010/11/28/business/28borker.html?_r=2&pagewa

nted=all shows how bad sentiments can improve search ranking! 

    http://googleblog.blogspot.com/2010/12/being-bad-to-your-customers-is-

bad-for.html gives google's response! 

opinion search and retrieval     summary 

    being a trec task since 2006, many approaches 

available [macdonald et al, trec2007] 

    main addition is the step of id31 

    opinion ranking needs additional handling 

    id166 classifiers have been popular among the best 

performers 

    research on comparison search is still limited 

77 

opinion spam 

    opinion spam: activities that try to deliberately mislead 

readers or automated opinion mining systems 

    hype spam: giving undeserving positive opinions to some target 

objects in order to promote the objects, or 

    defaming spam: giving unjust or false negative opinions to some 

other objects in order to damage their reputations 

    review spam is becoming a major issue, e.g. see 

http://travel.nytimes.com/2006/02/07/business/07guides.
html 

    automatically detecting spam opinions becomes more 

and more critical 

 

78 

different types of opinion spam 

    bogus/fake opinions (hype, defaming reviews) 

   

   it's very expensive, however, i found www.yourfreeconsole.co.uk 
and got one for free! as you probably know, it's an expensive item 
for what it is... but, i didn't exactly pay for mine.    

    generic reviews: comment on brands, manufacturers or 

buyers/sellers, e.g. 
   

   canon digital cameras are the best on earth       

    non-opinions (advertisements, transactional text, random 

texts) 
   

   

79 

   get one for free!!! have a look at this video first:   
http://www.youtube.com/watch?v=dfkyve__mug just take 2 
minutes and read this. this is very easy to do!    
   item was exactly the same as what was promised on sellers web 
page. delivery was quick and professional. i am a very happy 
camper and would buy again in the future.    
 

opinion spam detection 

    a binary classification problem [jindal & liu, www2007; jindal & liu, 

wsdm2008] 

    for types 2 & 3, three sets of features are used 

    review centric features: review text, #times that brands are 

mentioned, percentage of opinion words, review length, #helpful 
feedback, etc. 

    reviewer centric features: average ratings given by a reviewer, 

standard deviation in rating, etc. 

    product centric features: product price, sales rank, average 

rating, etc. 

    experimental results on product reviews seem promising 

 

 

80 

opinion spam detection (cont.) 

    for type 1, it is very difficult because 

    manually labeling training data is very expensive 

    spammers can craft a spam review that is just like any innocent 

review 

    research in this area is limited and in early stage, see 

[jindal & liu, wsdm2008] for more details 

 

 

 

81 

opinion quality assessment   

    use consumer reviews of products as an example 

    usually posed as a regression task 

    many review aggregation sites have buttons to collect user 

helpfulness feedback, which can be used for training and testing 
    many types of data features are used [kim et al, emnlp2006; ghose & ipeirotis, 

icec2007; zhang & varadarajan, cikm2006] 

    review length 

    review ratings (number of stars) 

    counts of some specific pos tags 

    opinion words and phrases 

    tf-idf weighting scores 

    product attribute mentions and product brands 

    comparison with product specifications    

    main application is in opinion search (ranking) 

 

82 

useful review vs. spam 

    utility classification is different from spam detection 
    not-helpful or low quality reviews are not necessarily fake 

reviews or spam 

    helpful reviews may not be non-spam 

    users often determine the usefulness of a review based 
on whether it expresses opinions on many attributes of a 
product 
    spammer can carefully craft a review that satisfies this 

requirement 

    feedback spam: user feedbacks can be spammed too! 

    a low quality review is still a valid review and should not 

be discarded, but a spam review is untruthful and/or 
malicious and should be removed once detected 

83 

id31 is everywhere! 

http://venturebeat.com/2011/0

3/20/why-sentiment-
analysis-is-the-future-of-
ad-optimization/ discusses 
ways to embed the 
analysis in an application!! 

roadmap 

1.

introduction 

2. sentiment identification & classification 

3. key applications 

4. examples 

5. conclusions 

85 

examples     two ebay applications 

    application 1: mining discussion forums 

vox populi: opinion retrieval & classification engine 

    application 2: enhancing ebay product reviews 

    product review miner: feature-based opinion mining 

    product reviews spam filtering 

86 

mining ebay user discussion boards 

87   

product related  

community & technology boards 

 
88  

technology related  

vox populi: opinion retrieval and classification 

example: health & beauty forum 

    search: mineral makeup 

    ~ 700 posts     202 opinions 

    related terms: mineral makeup, bare mineral, estee lauder, bishmuth 

oxychloride, bare escentuals, everday minerals, mineral veil, titanium 
dioxide 

    search results: positive, negative, neutral 

 many of  the newer mm companies are improving on the mineral makeup concept , and leaving 
out the bismuth  
 
mineral makeup is so daunting because what works for one might not work for another 
 
aromaleigh is the mineral makeup that i have liked best .  
 
everything i 've read about meow mineral makeup is positive so i 'm really looking forward to 
trying them - i 'll update as soon as i get them !  
 

89  

example: opinion retrieval in health & beauty 

search: mineral makeup 

related terms 

90  

trends 

sentiment classification: experiment 

    data set 

    human evaluation: 887 sentences - two annotators  

    annotations are consistent on 776 sentences (67 positive; 389 

negative; 320 neutral) 

    vox populi opinion classifier - 2008 lab results 

positive 

negative  neutral 

precision 

59.6% 

recall 

50.7% 

77.2% 

86.1% 

78.2% 

69.7% 

sarcasms in technology discussion boards -> false positives 

91  

product review miner 

 feature-based id31 of product reviews 

    document level: 

unstructured product reviews: free text + user ratings (1 to 5 

stars) 

 

    topic/feature level 

 

 

  

- ex:  the powershot sd1100 is light and compact 
                  camera                         weight          size 

 

 

92  

product review miner: feature-based opinion mining 

product  reviews 

93 

93 

structuring ebay product reviews (2) 

94  

  

a peek at product oriented opinion mining 

     vocabulary acquisition  

    polarity keywords 

    domain/product keywords  

the battery life and picture quality are great (+),  

 but the view finder is small (-) (* contextual orientation) 

 

    lexical patterns 

     the battery life is [not bad (-)] (+) 

   <topic1><not><neg> -> (+) 

 

    this camcorder [would be great (+)](-)  if the view finder was not so small 

   <condition><pos><*><if>     (-) 

 

95  

dictionary-based vocabulary acquisition 

    resources: 

    the general inquirer (http://www.wjh.harvard.edu/~inquirer) 
    sentiid138  (http://patty.isti.cnr.it/~esuli/software/sentiid138) 

 

  

96  

incomplete, does not capture 
 
- domain vocabulary 
- emotional expressions 
-typos 

corpus-based vocabulary extraction 

    extracting word collocations:  

    examples: battery life, going back, turn off, etc. 

 

    term weighting: extract domain and polarity terms 

    learn from examples:  

    product vocabulary: compare different domains 

    polarity words: positive vs. negative reviews 

97  

vocabulary extraction: domain vocabulary   

example: camcorder domain vocabulary 

 

keyword extractor 

camcorder reviews 

camera 
video 
hd 
camcorder  
recording 
battery 
battery life 
image 
screen 
movie 
film 
shot 
picture 
stabilization 
    

  stroller reviews 

camcorder domain 
keywords 

98  

compared to    not camcorder    

vocabulary extraction:  polarity keywords 

waste 
not turn 
poor 
return 
turned 
worst 
terrible 
audio 
radio 
setting 
horrible 
broke 
    

  
  
  

    
  
  
  
   
   
  

  

negative polarity words 

 

keyword extractor 

camcorder    
negative reviews 

compared to: 

  camcorder   

positive reviews   

99 

review cleaning: products vs. sellers vs. ads 

product reviews  

item exactly what was promised on sellers webpage.  
delivery was quick and professional.  i am a very happy 
camper and would buy again in the future. 

seller reviews  

fast email responses and fast shipping!! 

horrible customer service 

great seller...great service.... 

spam 

i found www.yourfreeconsole.co.uk and got one for free! 

i went to www.gadgets.ipodgifts.net and signed up and complete one offer.   i would recommend 
either the total credit check or the eauction tutor offer they are both free and i kept the 
eauction tutor offer to use with ebay. another cool offer is the king.com. 

100  

get a free playstation3! not a scam! the real thing! 

product review quality filtering 

goal: overall product rating based only on product review ratings 

 
review filter 

   product review 
  

classification 

product reviews 
 
seller feedbacks 
  
          ads 

training 

101   

automated product review cleaning: lab tests (2008) 

    training: manually annotated data sets 

books 
dvds 
video game 
systems 

total number 

good 

536 
520 
515 

58 
36 
43 

fair 
184 
141 
123 

bad 
167 
234 
160 

mixed 

feedback 

spam 

46 
20 
35 

80 
87 
135 

1 
2 
19 

    product review classification results 

 

good + fair +bad     review 
mixed + feedback     seller feedback 

 

review 

cross 
domain 

seller 

feedback 

ads 

p 
r 
p 
r 
p 
r 

93.82 
97.56 
93.04 
82.60 
67.65 
54.76 

  
(id166 multi-classification model) 

102  

roadmap 

1.

introduction 

2. sentiment identification & classification 

3. key applications 

4. examples 

5. conclusions 

103 

id31 summary 

direct opinions 
(document-, sentence-, and attribute-levels) 
counting 
polarity 
words 
rule-based  use label sequential pattern (lsp) matching to define 

use polarity dictionary (pre-defined or automatically 
learnt) to identify and classify sentiments 

syntactic patterns for identification and classification of 
sentiments, e.g. [chaovalit & zhou, hicss2005; kim & 
hovy, coling2004; turney, acl2002; yu & 
hazivassiloglou, emnlp2003] 
use machine learning techniques (na  ve bayesian, 
maximum id178, support vector machines, and 
id28) to build classifiers for identification 
and classification of sentiments, e.g. [goldberg & zhu, 
workshop on textgraphs, at hlt-naal2006; mullen & 
collier, emnlp2004; pang & lee, acl2005; wiebe & 
rilloff, cicling2005] 

supervised 
learning 

104 

comparative  opinions 
(sentence-level) 
use comparative and 
superlative words (mainly 
adjectives and adverbs) as 
well as special keywords 
(e.g. same, similar) to learn 
patterns around keywords. 
specialized sequence 
classification algorithms are 
often used, e.g. [jindal & liu, 
2006] 

conclusions 

    id31 tackles challenging tasks that involve 

nlp and id111 

    being well studied yet many challenging problems not 

solved 

    strong commercial interest 

    companies want to know how their products are being perceived 

    prospective consumers want to know what existing users think 

    many start-ups emerging 

105 

current trends 

    http://www.opfine.com/ examines sentiment in market news 

       buzz    is quantified and compared with other metrics such as price, 

trading-volume, etc. 

    reliable and representative data sources are necessary 

106 

political campaigns and sentiments 

    http://www.nytimes.com/2010/11/01/technology/01sentiment.html 

discusses applications in political campaigns 

    http://www.feeltiptop.com/obama.php examines sentiment trends to 

allow comparison with other timelines 

everyone is interested!!! 

    http://www.wired.co
m/dangerroom/2009
/10/exclusive-us-
spies-buy-stake-in-
twitter-blog-
monitoring-firm/ 
reports on interest 
from intelligence 
agencies    

future research     challenges    

    identify implicit opinions, e.g. 

       i will never go back to any other camera    
       every person on earth should own one of these    

    determine whether the subject and the object are relevant to 

the    true object   , e.g. in a phone review 
       my mom was mad at me because i didn   t tell her i bought a new 

phone    

    identify irony/sarcasm, e.g. 

       do you know there is a search engine called google    (in a poor 

search context) 

       this product is apparently designed by high school students    

    standard data set for training/testing/evaluation is in great 

demand 

    social aspect: mine opinions from similar people only 

109 

acknowledgements 

    bing liu @ uic 

    nitin indurkhya @ ebay research labs 

    sean huang @ ebay search science 

    eric brill @ ebay research labs 

    dennis decoste @ ebay research labs 

110 

online resources 

    bing liu. id31 and subjectivity, in handbook of natural language processing, second edition 

(editors: n. indurkhya and f. j. damerau), 2010 

    bo pang and lillian lee. opinion mining and id31, in foundations and trends in information 

retrieval, 2(1-2), pp. 1   135, 2008. 

    id31 symposium: http://www.sentimentanalysissymposium.com/index.html 

    opennlp: http://incubator.apache.org/opennlp/ 

    an organizational center for open source projects related to natural language processing, hosting a variety of 

java-based nlp tools which perform sentence detection, id121, pos-tagging, chunking and parsing, 
named-entity detection, and co-reference using the opennlp maxent machine learning package. 

    sentiid138: http://sentiid138.isti.cnr.it/ 

    a polarity dictionary with strength scores. 

    opinionfinder in mpqa: http://www.cs.pitt.edu/mpqa/ 

    a system that processes documents and automatically identifies subjective sentences as well as various 

aspects of subjectivity within sentences, including agents who are sources of opinion, direct subjective 
expressions and speech events, and sentiment expressions. opinionfinder was developed by researchers at 
the university of pittsburgh, cornell university, and the university of utah.  

   

lingpipe:  http://alias-i.com/lingpipe/ 

    a tool kit for id111 tasks such as id31 (http://alias-

i.com/lingpipe/demos/tutorial/sentiment/read-me.html), named entity extraction, id147, etc. 

    voxpop: http://blog.typeslashcode.com/voxpop/ 

    a thesis project on id31 by andrew mahon and zeke shore at parsons school of design, 

aiming to explore reader sentiment within comments of new york times articles 

111 

references (1) 

    a. aue and m. gamon. customizing sentiment classifiers to new domains: a case study. in proceedings of recent 

advances in natural language processing (ranlp), 2005. 

    s. baccianella, a. esuli, and f. sebastiani. sentiid138 3.0: an enhanced lexical resource for id31 

and opinion mining. in proceedings of the seventh conference on international language resources and evaluation 
(lrec), pp. 2200-2204, 2010. 

    d. blei, a. ng, and m. jordan. id44. journal of machine learning research (jmlr), 3, pp. 993-1022, 

2003. 

   

j. blitzer, m. dredze, and f. pereira. biographies, bollywood, boom-boxes and blenders: id20 for sentiment 
classification. in proceedings of the association for computational linguistics (acl), 2007. 

    eric brill. a simple rule-based part of speech tagger. in proceedings of the third conference on applied natural 

language processing (canlp), pp. 152-155, 1992. 

    p. chaovalit and l. zhou. movie review mining: a comparison between supervised and unsupervised classification 

approaches. in proceedings of the 38th annual hawaii international conference on system sciences (hicss'05) , 2005. 

    k. dave, s. lawrence, and d. m. pennock. mining the peanut gallery: opinion extraction and semantic classification of 

product reviews,    proceedings of www, pp. 519   528, 2003. 

    c. fellbaum, eds. id138: an electronic lexical database. mit press, 1998. 

    m. gamon. sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of 

linguistic analysis,    proceedings of the international conference on computational linguistics (coling), 2004. 

    m. gamon, a. aue, s. corston-oliver, and e. ringger. pulse: mining customer opinions from free text. in proceedings of 

the international symposium on intelligent data analysis (ida), pp. 121   132, 2005. 

    a. ghose and p. g. ipeirotis. designing novel review ranking systems: predicting usefulness and impact of reviews. in 

proceedings of the international conference on electronic commerce (icec), 2007. 

112 

references (2) 

    v. hatzivassiloglou and k. mckeown. predicting the semantic orientation of adjectives. in proceedings of the joint 

acl/eacl conference, pp. 174   181, 1997. 

    v. hatzivassiloglou and j. wiebe. effects of adjective orientation and gradability on sentence subjectivity. in proceedings 

of the international conference on computational linguistics (coling), 2000. 

    m. hu and b. liu. mining and summarizing customer reviews. in proceedings of the acm sigkdd conference on 

knowledge discovery and data mining (kdd), pp. 168   177, 2004. 

    n. jindal and b. liu. review spam detection. in proceedings of www, 2007. 

    n. jindal and b. liu. opinion spam and analysis. in proceedings of the conference on web search and web data mining 

(wsdm), pp. 219   230, 2008. 

    s. kim and e. hovy. determining the sentiment of opinions. in proceedings of the international conference on 

computational linguistics (coling), 2004. 

    s. kim, p. pantel, t. chklovski, and m. pennacchiotti. automatically assessing review helpfulness. in proceedings of the 
conference on empirical methods in natural language processing (emnlp), pp. 423   430, sydney, australia, july 2006. 

    b. liu. web data mining: exploring hyperlinks, contents, and usage data. springer, 2006. 

    b. liu, m. hu, and j. cheng. opinion observer: analyzing and comparing opinions on the web. in proceedings of www, 

2005. 

    y. lu, m. castellanos, u. dayal, and c. zhai. automatic construction of a context-aware sentiment lexicon: an optimization 

approach. in proceedings of 20th international world wide web conference, 2011. 

   

t. mullen and n. collier. id31 using support vector machines with diverse information sources. in 
proceedings of the conference on empirical methods in natural language processing (emnlp), 2004. 

    v. ng, s. dasgupta, and s. arifin. examining the role of linguistic knowledge sources in the automatic identification and 

classification of reviews,    proceedings of the coling/acl main conference poster sessions, pp. 611   618, sydney, 
australia, july 2006. 

113 

references (3) 

    b. pang and l. lee. seeing stars: exploiting class relationships for sentiment categorization with respect to rating 

scales. in proceedings of the association for computational linguistics (acl), pp. 115   124, 2005. 

    b. pang and l. lee. opinion mining and id31. foundations and trends in information retrieval 2(1-2), pp. 1   

135, 2008. 

    b. pang, l. lee, and s. vaithyanathan. thumbs up? sentiment classification using machine learning techniques. in 

proceedings of the conference on empirical methods in natural language processing (emnlp), pp. 79   86, 2002. 

    p. turney. thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. in 

proceedings of the association for computational linguistics (acl), pp. 417   424, 2002. 

   

   

   

j. wiebe, r. f. bruce, and t. p. o   hara. development and use of a gold standard data set for subjectivity classifications. 
in proceedings of the association for computational linguistics (acl), pp. 246   253, 1999. 

t. wilson, j. wiebe, and r. hwa. just how mad are you? finding strong and weak opinion clauses,    proceedings of 
aaai, pp. 761   769, 2004. 

t. wilson, j. wiebe, and p. hoffmann. recognizing contextual polarity in phrase-level id31. in proceedings 
of the human language technology conference and the conference on empirical methods in natural language processing 
(hlt/emnlp), pp. 347   354, 2005. 

    h. yang, l. si, and j. callan. knowledge transfer and opinion detection in the trec2006 blog track. in proceedings of 

trec, 2006. 

    h. yu and v. hatzivassiloglou. towards answering opinion questions: separating facts from opinions and identifying the 

polarity of opinion sentences,    proceedings of the conference on empirical methods in natural language processing 
(emnlp), 2003. 

z. zhai, bing liu, hua xu, and peifa jia. id91 product features for opinion mining. in proceedings of the fourth acm 
international conference on web search and data mining, 2011. 

z. zhang and b. varadarajan. utility scoring of product reviews. in proceedings of the acm conference on information and 
knowledge management (cikm), pp. 51   57, 2006. 

l. zhuang, f. jing, and x. zhu. movie review mining and summarization. in proceedings of the 15th acm conference on 
information and knowledge management (cikm), 2006. 

   

   

   

114 

