faloutsos, kolda, sun

icml'07

cmu scs

mining large time-evolving data 
using matrix and tensor tools

christos faloutsos carnegie mellon univ.
christos faloutsos carnegie mellon univ.
tamara g. kolda    sandia national labs
jimeng sun

carnegie mellon univ.

cmu scs

about the tutorial

    introduce matrix and tensor tools through 

real mining applications
real mining applications

    goal: find patterns, rules, clusters, 

outliers,    
    in matrices and
    in tensors

faloutsos, kolda, sun

1-2

1

faloutsos, kolda, sun

icml'07

cmu scs

    matrix tools

what is this tutorial about?
    singular value decomposition (svd)
    principal component analysis (pca)
    webpage ranking algorithms: hits, id95
webpage ranking algorithms: hits id95
    cur decomposition
    co-id91
    nonnegative id105 (nmf)

    tensor tools

    tucker decomposition
    parallel factor analysis (parafac)
    dedicom
dedicom
    missing values
    nonnegativity
    incrementalization

    applications, software demo

faloutsos, kolda, sun

1-3

cmu scs

what is this tutorial not about?

    classification methods
    kernel methods
    kernel methods
    discriminative models

    id156 (lda)
    canonical correlation analysis (cca)
    probabilistic latent variable models

    probabilistic pca
    probabilistic id45
    id44

faloutsos, kolda, sun

1-4

2

faloutsos, kolda, sun

icml'07

cmu scs

motivation 1: why    matrix   ?

    why matrices are important?

faloutsos, kolda, sun

1-5

cmu scs

examples of matrices: 
graph - social network

j h
john

peter
peter

mary
mary

nick
nick

...

0
5 

...
...
...

11
0 

...
...
...

22
6

...
...
...

55 ...
7 ...
...
...
...

...
...
...

john
peter
mary
nick
...

faloutsos, kolda, sun

1-6

3

faloutsos, kolda, sun

icml'07

cmu scs

examples of matrices:

cloud of n-d points

h l#
chol#

13
5

bl
d#
blood#
11
4

...
...
...

...
...
...

john
peter
mary
nick
...

...
...
...

age
age

..

...

22
6

...
...
...

55 ...
7 ...
...
...
...

faloutsos, kolda, sun

1-7

cmu scs

examples of matrices:

market basket

    market basket as in association rules

milk

bread
11
4

choc.

wine

...

22
6

...
...
...

55 ...
7 ...
...
...
...

...
...
...

13
5

...
...
...

john
peter
mary
nick
...

...
...
...

faloutsos, kolda, sun

1-8

4

faloutsos, kolda, sun

icml'07

cmu scs

examples of matrices:
documents and terms

data

classif.
22
6

mining
11
4

...
...
...

13
5

...
...
...

...
...
...

paper#1
paper#2
paper#3
paper#4

...

...
...
...

tree

...

55 ...
7 ...
...
...
...

faloutsos, kolda, sun

1-9

cmu scs

examples of matrices:

authors and terms

data

classif.
22
6

mining
11
4

...
...
...

13
5

...
...
...

...
...
...

john
peter
mary
nick
...

...
...
...

tree

...

55 ...
7 ...
...
...
...

faloutsos, kolda, sun

1-10

5

faloutsos, kolda, sun

icml'07

cmu scs

examples of matrices:
sensor-ids and time-ticks

temp1
13
5

temp2
11
4

humid. pressure

...

22
6

...
...
...

55 ...
7 ...
...
...
...

...
...
...

...
...
...

t1
t2
t3
t4

...

...
...
...

faloutsos, kolda, sun

1-11

cmu scs

motivation 2: why tensor?

    q: what is a tensor?

faloutsos, kolda, sun

1-12

6

faloutsos, kolda, sun

icml'07

cmu scs

motivation 2: why tensor?

    a: n-d generalization of matrix:

icml   07

data

john
peter
mary
nick
...

...
...
...

classif.
22
6

mining
11
4

...
...
...

13
5

...
...
...

...
...
...

tree

...

55 ...
7 ...
...
...
...

faloutsos, kolda, sun

1-13

cmu scs

motivation 2: why tensor?

    a: n-d generalization of matrix:

icml   05

icml   06

icml   07

data

john
peter
mary
nick
...

...
...
...

classif.
22
6

mining
11
4

...
...
...

13
5

...
...
...

...
...
...

tree

...

55 ...
7 ...
...
...
...

faloutsos, kolda, sun

1-14

7

faloutsos, kolda, sun

icml'07

cmu scs

tensors are useful for 3 or more 

terminology:    mode    (or    aspect   ):

modes 

3rd mode

2nd mode

data

classif.
22
6

mining
11
4

...
...
...

13
5

...
...
...

...
...
...

...
...
...

tree

...

55 ...
7 ...
...
...
...

1st mode
faloutsos, kolda, sun

1-15

cmu scs

motivating applications 

    why matrices are important?
wh tensors are sef l?
    why tensors are useful? 
    p1: environmental sensors
    p2: data center monitoring (   autonomic   )
    p3: social networks
    p4: network forensics
    p5: web mining
    p6: face recognition

faloutsos, kolda, sun

1-16

8

faloutsos, kolda, sun

icml'07

cmu scs

p1: environmental sensor monitoring 

data in three modes
(time, location, type)

8000

10000

2000

4000

6000

time (min)

temperature

e
u
a
a
v
v

l

30

25

20

15
15

10

5

0
0

40

30

20

10

e
u
a
v

l

0
0

2000

4000

6000

8000

10000

time (min)

humidity

faloutsos, kolda, sun

600

500

400

300

200

100

e
u
a
v

l

0
0

2000

2.5

2

1.5

e
u
a
v

l

1

0.5

0
0

2000

4000

6000

time (min)

light

4000

6000

time (min)

8000

10000

8000

10000

voltage

5-17

cmu scs

p2: clusters/data center monitoring

data in three modes
data in three modes
(time, machine, type)

    monitor correlations of multiple measurements
    automatically flag anomalous behavior
intemon: intelligent monitoring system 
   
    prof. greg ganger and pdl 
    >100 machines in a data center
    warsteiner.db.cs.cmu.edu/demo/intemon.jsp

faloutsos, kolda, sun

1-18

9

faloutsos, kolda, sun

icml'07

cmu scs

p3: social network analysis

    traditionally, people focus on static networks and 

find community structures

y

    we plan to monitor the change of the community 

structure over time
keywords

2004

i

data in three modes
d t
(time, author, keyword)

th

d

faloutsos, kolda, sun

1-19

dm

db

db

1990

s
r
o
h
t
u
a

cmu scs

p4: network forensics

    directional network flows
    a large isp with 100 pops, each pop 10gbps link 

capacity [hotnets2004]
    450 gb/hour with compression

data in three modes
data in three modes

p
(time, source, destination)

g

    task: identify abnormal traffic pattern and find out the 

cause

abnormal traffic

500
n
450
o
o
400
400
i
t
350
a
300
n
250
i
t
200
s
150
e
100
d
50

n
o
i
t
a
n
i
t
s
e
d

100

500

200

300

400

source

source
faloutsos, kolda, sun

normal traffic

500

450

400
400

350

300

250

200

150

100

50

n
o
o
i
t
n
a
o
i
t
a
n
n
i
t
s
i
e
d
t
s
e
d

100

200

300

source

source

400

500

1-20

10

faloutsos, kolda, sun

icml'07

cmu scs

p5: web graph mining

    how to order the importance of web pages?

    kleinberg s algorithm hits
kleinberg   s algorithm hits
    id95
    tensor extension on hits (tophits)

    context-sensitive hypergraph analysis 

data in three modes

(source, destination, text)

faloutsos, kolda, sun

1-21

cmu scsp6. face recognition and compression

(vasilescu & terzopoulos, 2002; vasilescu & terzopoulos, 2003)

people

expressions

views

faloutsos, kolda, sun

illuminations

1-22

11

faloutsos, kolda, sun

icml'07

cmu scs

static data model 

    tensor

    formally,
formally, 
    generalization of matrices
    represented as multi-array, (~ data cube).
3rd

2nd

1st

order

correspondence

vector

example

sensors

s
r
o
h
u
a

t

matrix

keywords
keywords

3d array

rts
port

s
n
o

i
t

a
n

i
t
s
e
d

faloutsos, kolda, sun

sources

1-23

cmu scs

dynamic data model

    tensor streams

    a sequence of mth order tensor

where

t is increasing over time
2nd

1st

order

correspondence

multiple streams

time evolving graphs

3rd

3d arrays

example

sensors

   

keyword

t
t
i

m
e

r
o
h
t
u
a

faloutsos, kolda, sun

orts
po

s
n
o

i
t

a
n

i
t
s
e
d

sources

   

1-24

12

faloutsos, kolda, sun

icml'07

cmu scs

roadmap

matri

    motivation
    matrix tools
tools
    tensor basics
    tensor extensions
    software demo
    case studies

t di

c

faloutsos, kolda, sun

1-25

13

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

    motivation
    matrix tools
matrix tools
    tensor basics
    tensor extensions
    software demo
    case studies

    svd, pca
    hits id95
hits, id95
    cur
    co-id91
    nonnegative matrix 

factorization

faloutsos, kolda, sun

2-1

cmu scs

singular value decomposition (svd)

x
x

x(1) x(2)

x(m) =

u1 u2

x = u  vt
u
u

  1

  2

.

uk

  

vt

.

v1
v2

v
vk

  
  k

input data

left singular 
vectors

singular values

right singular vectors 

faloutsos, kolda, sun

2-4

1

faloutsos, kolda, sun

icml   07

cmu scs

svd as spectral decomposition

n

a

m

  1u1  v1
  1u1 v1

n

   u   v
  2u2 v2

   

m

  

vt
+

u
u

    best rank-k approximation in l2 and frobenius 
    svd only works for static matrices (a single 2nd

order tensor)

see also parafac

faloutsos, kolda, sun

2-5

cmu scs

svd example

    a = u    vt =   1u1   v1 +   2u2   v2 +   

1st factor

2nd factor

cs-doc u1md-doc u2cs weight 

  1

retrieval

inf.

data
1 1
2 2
1 1
5 5
0 0
0 0
0 0

brainlung
0
0
0
0
2
3
1

0
0
0
0
2
3
1

=
=

1
2
1
5
0
0
0

cs

md

0.18 0
0.36 0
0.18 0
0 90 0
0.90 0
0
0
0

0.53
0.80
0.27

6

6

x
x

9.64 0
0
0

5 29
5.29

x
x
md weight   2

0

0.58 0.58 0.58 0
0
0
cs term v1

0

0.71 0.71
md term v2

2

faloutsos, kolda, sun

icml   07

cmu scs

svd properties

    v are the eigenvectors of the covariance 

matrix xtx since
matrix x x, since

    u are the eigenvectors of the gram (inner-

product) matrix xxt since
product) matrix xx , since 

further reading:
1. ian t. jolliffe, principal component analysis (2nd ed), springer, 2002.
2-7
2. gilbert strang, id202 and its applications (4th ed), brooks cole, 2005.

faloutsos, kolda, sun

cmu scs

svd - interpretation

   documents   ,    terms    and    concepts   :
q: if a is the document-to-term matrix, what 

is at a?

a: term-to-term ([m x m]) similarity matrix
q: a at ?
a: document to document ([n x n]) similarity
a: document-to-document ([n x n]) similarity 

matrix

faloutsos, kolda, sun

2-8

3

faloutsos, kolda, sun

icml   07

cmu scs

principal component analysis (pca)

    svd

n

a

m

rk
k

r

k
k

n

vt

k
r

m

u

  

loading

pcs

    pca is an important application of svd
    note that u and v are dense and may have negative entries

faloutsos, kolda, sun

2-9

cmu scs

pca interpretation

    best axis to project on: (   best    = min sum of 

squares of projection errors)
q
)

p j

term2 (   lung   )

faloutsos, kolda, sun

term1 (   data   )

2-10

4

faloutsos, kolda, sun

icml   07

cmu scs

pca - interpretation 

term2 (   lung   )
term2 ( lung )

pca projects points
onto the    best    axis

1
v1

first singular vector

    minimum rms error

term1 (   data   )

faloutsos, kolda, sun

2-11

cmu scs

roadmap

    motivation
    matrix tools
matrix tools
    tensor basics
    tensor extensions
    software demo
    case studies

    svd, pca
    hits id95
hits, id95
    cur
    co-id91
    nonnegative matrix 

factorization

faloutsos, kolda, sun

2-12

5

faloutsos, kolda, sun

icml   07

cmu scs

kleinberg   s algorithm hits
    problem dfn: given the web and a query
    find the most    authoritative    web pages for 
eb pages for

find the most    a thoritati e   
this query

step 0: find all pages containing the query terms
step 1: expand by one move forward and backward
step 1: expand by one move forward and backward

further reading:
1. j. kleinberg. authoritative sources in a hyperlinked environment. soda 1998

faloutsos, kolda, sun

2-13

cmu scs

kleinberg   s algorithm hits
    step 1: expand by one move forward and 

backward
backward

faloutsos, kolda, sun

2-14

6

faloutsos, kolda, sun

icml   07

cmu scs

kleinberg   s algorithm hits
    on the resulting graph, give high score (= 
   authorities   ) to nodes that many important
authorities ) to nodes that many important 
nodes point to

    give high importance score (   hubs   ) to 

nodes that point to good    authorities   

hubs

authorities

faloutsos, kolda, sun

2-15

cmu scs

kleinberg   s algorithm hits

observations
    recursive definition!
rec rsi e definition!
    each node (say,    i   -th node) has both an 
authoritativeness score ai and a hubness 
score hi

faloutsos, kolda, sun

2-16

7

faloutsos, kolda, sun

icml   07

cmu scs

kleinberg   s algorithm: hits

let a be the adjacency matrix: 

the (i j) entry is 1 if the edge from i to j exists
the (i,j) entry is 1 if the edge from i to j exists

let h and a be  [n x 1] vectors with the 

   hubness    and    authoritativiness    scores.

then:

faloutsos, kolda, sun

2-17

cmu scs

kleinberg   s algorithm: hits

then:

k
l

m

i

ai = hk + hl + hm
h + h + h

that is
ai = sum (hj)     over all j that 

(j,i) edge exists

or
or
a = at h

faloutsos, kolda, sun

2-18

8

faloutsos, kolda, sun

icml   07

cmu scs

i
i

kleinberg   s algorithm: hits

symmetrically, for the    hubness   :

n

p

q

h
hi = an + ap + aq

+ +

that is
hi = sum (qj)     over all j that 

(i,j) edge exists

or
or
h = a a

faloutsos, kolda, sun

2-19

cmu scs

kleinberg   s algorithm: hits

in conclusion, we want vectors h and a such 

that:
that:

that is:

h = a a
a = at h

a = ata a
a = ata a

faloutsos, kolda, sun

2-20

9

faloutsos, kolda, sun

icml   07

cmu scs

kleinberg   s algorithm: hits
a is a right singular vector of the adjacency 

matrix a (by dfn!), a.k.a the eigenvector of 
ata

( y

),

g

starting from random a    and iterating, we   ll 

eventually converge

hi h f

q t
? h ?
q: to which of all the eigenvectors? why?
a: to the one of the strongest eigenvalue, 

ll th

i

t

(at a ) k  a =   1

ka

faloutsos, kolda, sun

2-21

cmu scs

kleinberg   s algorithm - discussion
       authority    score can be used to find    similar 

pages    (how?)
pages  (how?)

    closely related to    citation analysis   , social 

networks /    small world    phenomena

see also tophits

faloutsos, kolda, sun

2-22

10

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

    motivation
    matrix tools
matrix tools
    tensor basics
    tensor extensions
    software demo
    case studies

    svd, pca
    hits id95
hits, id95
    cur
    co-id91
    nonnegative matrix 

factorization

faloutsos, kolda, sun

2-23

cmu scs

motivating problem: id95

given a directed graph, find its most 

interesting/central node

a node is important,
if it is connected 
with important nodes
(recursive, but ok!)

faloutsos, kolda, sun

2-24

11

faloutsos, kolda, sun

icml   07

cmu scs

motivating problem     id95 

solution

given a directed graph, find its most 

interesting/central node

proposed solution: random walk; spot most 
   popular    node (-> steady state prob. (ssp))

g

a node has high ssp,
p
if it is connected 
with high ssp nodes
(recursive, but ok!)

faloutsos, kolda, sun

2-25

cmu scs

(simplified) id95 algorithm

    let a be the transition matrix (= adjacency
    let a be the transition matrix (= adjacency 

matrix); let a become row-normalized - then

1

4

2

5

from

3

to

a

 
 

 
 

1 
 
1
 
 

 
 
1/2  

 
1/2

 

1 
 
1/2 1/2  

  

faloutsos, kolda, sun

p1
p2
p3
p4
p5

=

p1
p2
p3
p4
p5

2-26

12

faloutsos, kolda, sun

icml   07

cmu scs

(simplified) id95 algorithm
    a p = p

a                     p    =      p

1

4

3

2

5

 
 

 
 

1 
 
1
 
 

 
 
1/2  

 
1/2

 

1 
 
1/2 1/2  

p1
p2
p3
p4
p5

=

  

faloutsos, kolda, sun

p1
p2
p3
p4
p5

2-27

cmu scs

(simplified) id95 algorithm
    a p = 1 * p
th s p is the eigen ector that corresponds
    thus, p is the eigenvector that corresponds 
to the highest eigenvalue (=1, since the matrix is 
row-normalized)

    why does it exist such a p? 
p
g
[perron   frobenius theorem]

,

    p exists if a is nxn, nonnegative, irreducible 

,

faloutsos, kolda, sun

2-28

13

faloutsos, kolda, sun

icml   07

cmu scs

(simplified) id95 algorithm
    in short: imagine a particle randomly 

moving along the edges
moving along the edges

    compute its steady-state probabilities (ssp)

full version of algo:  with occasional random 

jumps
jumps

why? to make the matrix irreducible

faloutsos, kolda, sun

2-29

cmu scs

full algorithm

    with id203 1-c, fly-out to a random 

node
node

    then, we have

p = c a p + (1-c)/n 1 =>
p = (1-c)/n  [i - c a] -1 1

faloutsos, kolda, sun

2-30

14

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

    motivation
    matrix tools
matrix tools
    tensor basics
    tensor extensions
    software demo
    case studies

    svd, pca
    hits id95
hits, id95
    cur
    co-id91
    nonnegative matrix 

factorization

faloutsos, kolda, sun

2-31

cmu scs

motivation of cur or cmd

    svd, pca all transform data into some 
abstract space (specified by a set basis)
abstract space (specified by a set basis)
    interpretability problem
    loss of sparsity

faloutsos, kolda, sun

2-32

15

faloutsos, kolda, sun

icml   07

cmu scs

interpretability problem

    each column of projection matrix ui is a linear 

combination of all dimensions along certain 
mode ui(:,1) = [0.5; -0.5; 0.5; 0.5]

    all the data are projected onto the span of ui
    it is hard to interpret the projections

faloutsos, kolda, sun

2-33

cmu scs

the sparsity problem     pictorially:

svd/pca:
destroys sparsity
destroys sparsity

u      vt

cur: maintains sparsity

=
=

=

c   u   r

faloutsos, kolda, sun

2-34

16

faloutsos, kolda, sun

icml   07

cmu scs

cur

    example-based projection: use actual rows and columns 

to specify the subspace

    given a matrix a   rm  n find three matrices c    rm  c
    given a matrix a   rm  n, find three matrices c    rm  c, 

u    rc  r, r    rr   n , such that ||a-cur|| is small

n

a

m

n
    rx

r 

m

c

c

u is the pseudo-inverse of x

example-based
orthogonal 
projection

faloutsos, kolda, sun

2-35

cmu scs

cur (cont.)

    key question:

how to select/sample the columns and rows?
    how to select/sample the columns and rows?

    uniform sampling [williams & seeger nips    00]
    biased sampling

    cur w/ absolute error bound
    cur w/ relative error bound
cur w/ relative error bound

reference:
1. tutorial: randomized algorithms for matrices and massive datasets, sdm   06
2. drineas et al. subspace sampling and relative-error matrix approximation: column-

row-based methods, esa2006

3. drineas et al., fast monte carlo algorithms for matrices iii: computing a 

faloutsos, kolda, sun

compressed approximate matrix decomposition, siam journal on computing, 2006.

2-36

17

faloutsos, kolda, sun

icml   07

cmu scs

the sparsity property
sparse and small

svd: a = u    vt
svd: a = u    vt

big but sparse

big and dense
dense but small

cur: a = c u r

big but sparse

big but sparse

faloutsos, kolda, sun

2-37

cmu scs

the sparsity property (cont.)

svd
cur
cmd

102

101

o

i
i
t
t

a
a
r
 

e
c
a
p
s

0

0.2

accuracy

data

102

o
o

i
i
t
t

a
a
r
 

e
c
a
p
s

0.8

1

101

0

0.2

svd
cur
cmd

0.8

1

0.4
0.6
accuracy

network

0.4
0.6
accuracy
dblp

    cmd uses much smaller space to achieve the same 

p

    cur limitation: duplicate columns and rows
    svd limitation: orthogonal projection densifies the 

2-38
reference:
sun et al. less is more: compact matrix decomposition for large sparse graphs, sdm   07

faloutsos, kolda, sun

18

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

    motivation
    matrix tools
matrix tools
    tensor basics
    tensor extensions
    software demo
    case studies

    svd, pca
    hits id95
hits, id95
    cur
    co-id91 etc
    nonnegative matrix 

factorization

faloutsos, kolda, sun

2-39

cmu scs

co-id91

    given data matrix and the number of row 

and column groups k and l
and column groups k and l

    simultaneously

    cluster rows of p(x, y)  into k disjoint groups 
    cluster columns of p(x, y)  into l disjoint groups

faloutsos, kolda, sun

2-40

19

faloutsos, kolda, sun

icml   07

cmu scs

co-id91

    let x and y be discrete random variables 

    x and y take values in {1, 2,    , m} and {1, 2,    , n}
x  and y  take values in {1, 2,    , m} and {1, 2,    , n}
    p(x, y)  denotes the joint id203 distribution   if 

not known, it is often estimated based on co-occurrence
data

    application areas: id111, market-basket analysis, 

    key obstacles in id91 contingency tables 

i cl

t bl

t

analysis of browsing behavior, etc. 
k ob t
i c ti
    high dimensionality, sparsity, noise
    need for robust and scalable algorithms

l

reference:
1. dhillon et al. information-theoretic co-id91, kdd   03

faloutsos, kolda, sun

2-41

cmu scs

yxp
=),(

m

l

36.
0

l
[
03.
   
   
k
3.0
      
      
2.2.
)  ,  ( yxp

m

k
   
   
05.
0
   
   
05.
0
   
   
05.0
05.0
   
   
0
5.0
   
   
      
      
   
   
0
5.0
)  |(
xxp

0
0

0
0

n
   
05.05.05.
   
05.05.05.
0
0
   
0
0
   
04.04.
   
      
   
04.04.04.
n
28.
0

36.
0

   
0
   
0
05.05.05.0
   
05.05.05.0
   
04.04.04.0
      
   
   
04.04.0
]
   
0
=
36.

0
36.

0
28.

)  |(
yyp

#parameters that determine q(x,y) are:
faloutsos, kolda, sun

km
(

)
+   

kl
(

 

   
054.
   
054.
0
   
0
   
036.
   
      
   
036.

054.
054.
0
0
036.
036.

042.
042.
0
0
028
028.

0
0
042.
042.
028.
028.

0
0
054.
054.
036.
036.

   
0
   
0
054.
   
054.
   
036.
   
      
   
036.

),( yxq
ln
)1
(
)
   +   

2-45

20

faloutsos, kolda, sun

icml   07

cmu scs

problem with information theoretic 

co-id91

    number of row and column groups must be 

specified
specified

desiderata:
(cid:57) simultaneously discover row and column groups
(cid:57) simultaneously discover row and column groups

fully automatic: no    magic numbers   

(cid:57) scalable to large graphs

faloutsos, kolda, sun

2-46

cmu scs

cross-association

desiderata:
(cid:57) simultaneously discover row and column groups
(cid:57) simultaneously discover row and column groups
(cid:57) fully automatic: no    magic numbers   
(cid:57) scalable to large matrices

reference:
1. chakrabarti et al. fully automatic cross-associations, kdd   04

faloutsos, kolda, sun

2-47

21

faloutsos, kolda, sun

icml   07

cmu scs

s
p
u
o
r
g
w
o
r

 

cmu scs

s
p
u
o
r
g
w
o
r

 

what makes a cross-association 

   good   ?

why is this 

y
better?

versus

s
p
u
o
r
g
w
o
r

 

column 
groups

column 
groups

faloutsos, kolda, sun

2-48

what makes a cross-association 

   good   ?

why is this 

y
better?

versus

s
p
u
o
r
g
w
o
r

 

column 
groups

column 
groups

simpler; easier to describe
easier to compress!

faloutsos, kolda, sun

2-49

22

faloutsos, kolda, sun

icml   07

cmu scs

what makes a cross-association 

   good   ?

problem definition: given an encoding scheme
    decide on the # of col. and row groups k and l
    and reorder rows and columns,
    to achieve best compression

d

d

d

l

faloutsos, kolda, sun

2-50

cmu scs

details

main idea

good 

compression
compression

better 
id91
id91

total encoding cost =

  i

sizei * h(xi) + cost of describing 
cross-associations

code cost

cost
minimize the total cost (# bits)

description 

for lossless compression

faloutsos, kolda, sun

2-51

23

faloutsos, kolda, sun

icml   07

cmu scs

algorithm

l = 5 col groups

 

k
 
=
=
5
 
r
o
w
g
r
o
u
p
s

 

k=1, 
l=2

k=2, 
l=2

k=2, 
l=3

k=3, 
l=3

k=3, 
l=4

k=4, 
l=4

faloutsos, kolda, sun

k=4, 
l=5

2-52

cmu scs

roadmap

    motivation
    matrix tools
matrix tools
    tensor basics
    tensor extensions
    software demo
    case studies

    svd, pca
    hits id95
hits, id95
    cur
    co-id91, etc
    nonnegative matrix 

factorization

faloutsos, kolda, sun

2-55

24

faloutsos, kolda, sun

icml   07

cmu scs

nonnegative id105
    coming up soon with nonnegative tensor 

factorization
factorization

faloutsos, kolda, sun

2-56

25

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

matri

    motivation
    matrix tools
tools
    tensor basics
    tensor extensions
    software demo
    case studies

t di

c

    tensor basics
tucker
    tucker
    tucker 1 
    tucker 2 
    tucker 3
    parafac

3-1

cmu scs

tensor basics

1

faloutsos, kolda, sun

icml   07

cmu scs

a tensor is a multidimensional array

an i    j    k tensor

column (mode-1) 

fibers

row (mode-2)

fibers

tube (mode-3)

fibers

i

xijk

j

3rd order tensor
3 order tensor

mode 1 has dimension i
mode 2 has dimension j
mode 3 has dimension k
note: tutorial focus is 

on 3rd order, but 
everything can be 
extended to higher 

orders.

horizontal slices

lateral slices

frontal slices

3-3

cmu scs

matricization : converting a tensor to 

a matrix

matricize
(unfolding)
(unfolding)

(i j k)
(i,j,k)

reverse 
matricize

(i   ,j   )

(i   ,j   )

(i,j,k)

x(n): the mode-n fibers are 
rearranged to be the columns 
of a matrix 

g

5   7
6   8

1   3
2   4

3-4

2

faloutsos, kolda, sun

icml   07

cmu scs

tensor mode-n multiplication

    tensor times matrix
tensor times matrix

    tensor times vector
tensor times vector

multiply each 
row (mode-2) 

fiber by b

compute the dot 
product of a and 

each column 
(mode-1) fiber

3-5

cmu scs

pictorial view of mode-n matrix 

multiplication

(cid:144)

(cid:144)

mode-2 multiplication

(lateral slices)

(cid:144)

mode-1 multiplication

(frontal slices)

mode-3 multiplication

(horizontal slices)

3-6

3

faloutsos, kolda, sun

icml   07

cmu scs

mode-n product example

    tensor times a matrix

n
o
i
t
a
c
o
l

ti
time

time

  time

s
r
e
t
s
u
l
c

n
o
i
t
a
c
o
l

clusters

cmu scs

mode-n product example

    tensor times a vector

n
o
i
t
a
c
o
l

ti
time

  time

e
m
t

i

n
o
i
t
a
c
o
l

3-7

3-8

4

faloutsos, kolda, sun

icml   07

cmu scs

outer, kronecker, & 
khatri-rao products

3-way outer product

review: matrix kronecker product

=

rank-1 tensor

m x n p x q

mp x nq

matrix khatri-rao product

m x r n x r

mn x r

observe: for two vectors a and b, a     b and a     b have the same 
elements, but one is shaped into a matrix and the other into a vector.

3-9

cmu scs

specially structured tensors

5

faloutsos, kolda, sun

icml   07

cmu scs

specially structured tensors
    kruskal tensor

    tucker tensor

our 

notation

our 

notation

   core   

i x j x k
i x j x k

i x r
i x r

=

u

j x s
j s

v

r x s x t

i x j x k
i x j x k

w1
w1
i x r
i x r

wr
wr
j x r
j r

=

=

+   +

r x r x r

vr

v

ur

v1
u
u1

cmu scs

specially structured tensors
    kruskal tensor

    tucker tensor

in matrix form:

in matrix form:

3-11

3-12

6

faloutsos, kolda, sun

icml   07

cmu scs

what is the ho analogue of the 

matrix svd?

matrix svd:

=

  1
=

  2

+

  r
+l+

tucker tensor (finding bases for each subspace):

kruskal tensor (sum of rank-1 components):

3-13

cmu scs

tensor decompositions

7

faloutsos, kolda, sun

icml   07

cmu scs

tucker decomposition

i x j x k

i x r
i x r

   

a

j x s
j x s

b

r x s x t

given a, b, c, the optimal core is:

proposed by tucker (1966)

   
    aka: three-mode factor analysis, three-mode 

y

,

pca, orthogonal array decomposition
    a, b, and c generally assumed to be 

orthonormal (generally assume they have full 
column rank)

is not diagonal 

   
    not unique

recall the equations for 

converting a tensor to a matrix

g

3-15

cmu scs

tucker variations

see kroonenberg & de leeuw, psychometrika,1980 for discussion.

    tucker2
i x j x k

identity matrix

j x s
j x s

b

r x s x k

i x r
i x r

   

a

    tucker1

i x j x k

i x r

   

a

r x j x k finding principal components in only mode 1

can be solved via rank-r matrix svd

3-16

8

faloutsos, kolda, sun

icml   07

cmu scs

solving for tucker

given a, b, c orthonormal, the optimal core is:

tensor norm is the square 
root of the sum of all the 

elements squared

eliminate the core to get:

i x j x k

i x r
   

a

j x s

b

r x s x t

minimize 

s.t. a,b,c orthonormal
l

t a b c th

fixed

maximize this

if b & c are fixed, then we can solve for a as follows:

optimal a is r left leading singular vectors for 

3-17

cmu scs

higher order svd (ho-svd)

i x j x k
i x j x k

i x r

   

a

j x s

b

r x s x t

not optimal, but 
often used to 
often used to 
initialize tucker-
als algorithm.

(observe connection to tucker1)

de lathauwer, de moor, & vandewalle, simax, 1980 

3-18

9

faloutsos, kolda, sun

icml   07

cmu scs

tucker-alternating least squares (als)

successively solve for each component (a,b,c).

i x j x k

i x r

=

a

j x s

b

r x s x t

   

initialize 
    choose r, s, t
c oose
    calculate a, b, c via ho-svd

, s,

    until converged do   

    a = r leading left singular 

vectors of x(1)(c   b)

    b = s leading left singular 

vectors of x(2)(c   a)
c  t leading left singular 
    c = t leading left singular
vectors of x(3)(b   a)

    solve for core: 

kroonenberg & de leeuw, psychometrika, 1980 

3-19

cmu scs

tucker in not unique

i x j x k

i x r

   

a

j x s

b

r x s x t

tucker decomposition is not unique. let y be 
an rxr orthogonal matrix. then   
an rxr orthogonal matrix. then   

3-20

10

faloutsos, kolda, sun

icml   07

cmu scs

candecomp/parafac 

decomposition

i x j x k
i x j x k

i x r

   

j x r

b

a

r x r x r

=

+   +

    candecomp = canonical decomposition (carroll & chang, 1970)
    parafac = parallel factors (harshman, 1970)
    core is diagonal (specified by the vector   )
    columns of a, b, and c are not orthonormal
   
    can have rank(  ) > min{i,j,k}

if r is minimal, then r is called the rank of the tensor (kruskal 1977) 
3-21

cmu scs

parafac-alternating least squares (als)

successively solve for each component (a,b,c).

=

+   +

khatri-rao product

(column-wise kronecker product)

i x j x k

find all the vectors in 
one mode at a time

hadamard product

if c, b, and    are fixed, the optimal a is given by:

repeat for b,c, etc.

3-22

11

faloutsos, kolda, sun

icml   07

cmu scs

parafac is often unique

i x j x k

c1

=

+   +

b
b1

a1

cr

b
br

ar

assume  
parafac 
siti
is exact.

d
 
decomposition 

sufficient condition for uniqueness (kruskal, 1977):

ka = k-rank of a = max number k such that every 

set of k columns of a is linearly independent

3-23

cmu scs

tucker vs. parafac decompositions
    tucker

    parafac

    variable transformation in 

each mode

    core g may be dense
    a, b, c generally 

orthonormal
    not unique

    sum of rank-1 components
    no core, i.e., superdiagonal 

core

    a, b, c may have  linearly 

dependent columns

    generally unique

i x j x k

i x r

   

a

j x s

b

r x s x t

i x j x k

c1

cr

+   +

   

b1

a1

br

ar

3-24

12

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

matri

    motivation
    matrix tools
tools
    tensor basics
    tensor extensions
    software demo
    case studies

t di

c

    tensor basics
tucker
    tucker
    tucker 1 
    tucker 2 
    tucker 3
    parafac

3-25

13

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

matri

    motivation
    matrix tools
tools
    tensor basics
    tensor extensions
    software demo
    case studies

t di

c

    other decompositions
    nonnegative parafac
    handling missing values

4-1

cmu scs

other tensor decompositions

1

faloutsos, kolda, sun

icml   07

cmu scs

combining tucker & parafac

m x i

n x j

v

u

i x j x k

m x n x p

    step 1: choose 

orthonormal matrices u, 
v, w to compress tensor 
(tucker tensor!)
    typically ho-svd can be 

used

    step 2: run parafac 

on smaller tensor

    step 3: reassemble result 

bro and andersson, 1998

4-3

cmu scs

2-way dedicom

n  n

x

=
=

n  m

a

r
r

m  n
at
a

dense, nonsymmetric m x m matrix

interactions can be nonsymmetric

    2-way dedicom introduced by harshman (1978)
    x is a matrix of interactions between n entities
   
    assumes there are    m    roles
    each entity has a weight for each role in a
    rij = interaction weight for roles i & j

4-4

2

faloutsos, kolda, sun

icml   07

cmu scs

3-way dedicom

patterns

=
=

s
s
e
e
o
r

l
l

3-way dedicom

g

    3-way dedicom due to kiers (1993)
    once again, x captures interactions among entities
    third dimension can correspond to time
    diagonal slices capture participation of each role at each time
    see bader et al., sand2006-7744 , for application to enron email data

g

4-5

cmu scs

nonnegativity

3

faloutsos, kolda, sun

icml   07

cmu scs

non-negative id105

minimize subject to elements of 

a d b b i
a and b being positive.

i i

update formulas (do not increase objective function):

elementwise multiply
(hadamard product)

elementwise divide

lee & seung, nature, 1999

4-7

cmu scs

non-negative 3-way parafac 

factorization

i i
lee-seung-like update formulas can be derived for 3d and higher:

a b d c b i
a, b and c being positive.

minimize subject to elements of 

elementwise multiply
(hadamard product)

elementwise divide

m. m  rup, l. k. hansen, j. parnas, s. m. arnfred, decomposing 
the time-frequency representation of eeg using non-negative 

matrix and multi-way factorization, 2006 

4-8

4

faloutsos, kolda, sun

icml   07

cmu scs

handling missing data

cmu scs a quick overview on handling 

missing data

    consider sparse parafac where x is missing 

data:

    typically, missing values are just set to zero
    more sophisticated approaches for handling 

missing values:
    weighted least squares id168 

    data imputation 

    ignore missing values
d i
    estimate missing values

i

    see, e.g., kiers, psychometrika, 1997 and srebro 

& jaakkola, icml 2003

4-10

5

faloutsos, kolda, sun

icml   07

cmu scs

weighted least squares

weight tensor

    weight the least squares problem so that the missing 

elements are ignored:

weighted

least squares

    but this problem is often too hard to solve directly!

cmu scs

missing value imputation

    use the current estimate to fill in the missing values
current estimate
current estimate

    the tensor for the next iteration of the algorithm is:

known values

estimates of unknowns

    challenge is finding a good initial estimate

sparse!

kruskal tensor

4-11

4-12

6

faloutsos, kolda, sun

icml   07

cmu scs

roadmap

matri

    motivation
    matrix tools
tools
    tensor basics
    tensor extensions
    software demo
    case studies

t di

c

4-13

cmu scs

computations with 

tensors

7

faloutsos, kolda, sun

icml   07

cmu scs

tensor toolbox for matlab

http://csmr.ca.sandia.gov/~tgkolda/tensortoolbox

    six object-oriented tensor classes
    working with tensors is easy
    working with tensors is easy

    most comprehensive set of kernel 

operations in any language
    e.g., arithmetic, logical, 
multiplication operations 

    sparse tensors are unique

    speed-ups of two orders of 

magnitude for smaller problems

    larger problems than ever 

before

bader & kolda, acm toms 2006 & sand2006-7592

    free for research or 
evaluations purposes

p p

    297 unique registered users 

from all over the world 
(as of january 17, 2006)

4-15

cmu scs

dense tensors

    largest tensor that can be 
stored on a laptop is 200 x
stored on a laptop is 200 x 
200 x 200

    typically, tensor 

operations are reduced to 
matrix operations 
    requires permuting and 

reshaping the tensor

    example: mode-n tensor-

matrix multiply

i x j x k

example: mode-1 matrix multiply

m x j x k

i x j x k

m x i

m x jk

m x i

i x jk
4-16

8

faloutsos, kolda, sun

icml   07

cmu scs

sparse tensors: only store nonzeros

example: tensor-vector multiply (in all modes) 

store just the 

nonzeros of a tensor
(assume coordinate 

format)
format)

pth nonzero

2nd subscript 

1st subscript 

of pth 
nonzero

of pth 
nonzero

3rd subscript 

of pth 
nonzero
4-17

cmu scs

tucker tensors:

store core & factors

t k  t
 b  
tucker tensor stores the core (which can be 
dense, sparse, or structured) and the factors.

 ( hi h 

 th  

  t

example: mode-3 tensor-vector multiply

result is a 
tucker tensor
4-18

9

faloutsos, kolda, sun

icml   07

cmu scs

kruskal example: 

store factors

kruskal tensors store factor 
matrices and scaling vector.

+   +

i x j x k

i x r
i r

=

u

j x r

v

=

r x r x r

example: norm

i x j x k

i x r j x r k x r

r x r

r x r

r x r

4-19

10

faloutsos, kolda, sun

icml   07

cmu scs

incrementalization

cmu scs

incremental tensor decomposition

    dynamic data model

tensor streams
    tensor streams

    dynamic tensor decomposition (dta)
    streaming tensor decomposition (sta)
    window-based tensor decomposition 

(wta)
(wta)

faloutsos, kolda, sun

5-2

1

faloutsos, kolda, sun

icml   07

dynamic tensor stream

cmu scs

source

n
o

i
t

a
n

i
t
s
e
d

time

    streams come with structure
    (time, source, destination, port)
    (time, author, keyword)

    how to summarize tensor streams effectively and 

incrementally?

faloutsos, kolda, sun

5-3

cmu scs

dynamic data model

    tensor streams

    a sequence of mth order tensor

where

n is increasing over time
2nd

1st

order

correspondence

multiple streams

time evolving graphs

3rd

3d arrays

example

t
i

m
m
e

r
o
h
t
u
a

sensors

   

keyword

ports

s
n
o

i
t

a
n

i
t
s
e
d

faloutsos, kolda, sun

sources

   

5-4

2

faloutsos, kolda, sun

icml   07

cmu scs

incremental tensor decomposition

   dynamic data model

    tensor streams
    tensor streams

    dynamic tensor decomposition (dta)
    streaming tensor decomposition (sta)
    window-based tensor decomposition 

(wta)
(wta)

1. jimeng sun, spiros papadimitriou, philip yu. window-based tensor analysis on 

high-dimensional and multi-aspect streams, icdm 2006

2. jimeng  sun, dacheng tao,  christos faloutsos.  beyond streams and graphs: 

dynamic tensor analysis,  kdd 2006

faloutsos, kolda, sun

5-5

cmu scs

incremental tensor decomposition

old tensors

new tensor

n
o
o

i
t

a
n

i
t
s
e
d

old cores

udestination

usource

faloutsos, kolda, sun

5-6

3

faloutsos, kolda, sun

icml   07

cmu scs

1st order dta - problem

given x1   xn where each xi    rn, find 
u   rn  r such that the error e is
u   r
such that the error e is 
small: 

n
x1
   

y

ut

r

sensors

?

.

xn
sensors

indooroutdoor

note that y = xu

faloutsos, kolda, sun

5-7

e
m
m

i
t

n

cmu scs

e
m

i
i
t

old x

x

x

xt

1st order dynamic tensor analysis
input: new data vector x    rn, old variance 

matrix c    rn  n

output: new projection matrix u    rn  r
output: new projection matrix u    r
algorithm:
1. update variance matrix cnew = xtx + c
2. diagonalize u  ut = cnew
3. determine the rank r and return u

c

cnew

u

ut

diagonalization has to be done for every new x! 
5-8

faloutsos, kolda, sun

4

faloutsos, kolda, sun

icml   07

,

i

g
n
z
c
i
r
t

i

a
m

e
s
o
p
s
n
a
r
t

cmu scs

mth order dta

reconstruct variance matrix

du
t

ds
d

du

=
= dc
c

construct variance matrix of  

incremental tensor 

matricizing

  
  

dx
t
(

)

x(d)x(d)
(d)

t 
(d)

=
=
)dx
(

diagonalize 
variance matrix

du
t

ds

du

dc

u d t v i
m t i
update variance matrix

faloutsos, kolda, sun

5-9

cmu scs

mth order dta     complexity

storage: 
o(    ni), i.e., size of an input tensor at a single 

timestamp

computation:
2)
3 (or     ni
    ni
t x(d)
+     ni     ni
for low order tensor(<3), diagonalization is the main cost
for high order tensor,  id127 is the main cost

diagonalization of c
id127 x (d)
),

g

(

faloutsos, kolda, sun

5-10

5

faloutsos, kolda, sun

icml   07

cmu scs

incremental tensor decomposition

   dynamic data model

    tensor streams
    tensor streams

   dynamic tensor decomposition (dta)
    streaming tensor decomposition (sta)
    window-based tensor decomposition 

(wta)
(wta)

1. jimeng sun, spiros papadimitriou, philip yu. window-based tensor analysis on 

high-dimensional and multi-aspect streams, icdm 2006

2. jimeng  sun, dacheng tao,  christos faloutsos.  beyond streams and graphs: 

dynamic tensor analysis,  kdd 2006

faloutsos, kolda, sun

5-11

cmu scs

1st order streaming tensor 

analysis (sta)

    adjust u smoothly when new data arrive without 

diagonalization  [vldb05]

    for each new point x
    project onto current line
    estimate error
    rotate line in the direction of the error and in proportion to its 

magnitude

for each new point x and for i = 1,    , k : 
(proj. onto ui)
   
    di       di + yi

yi :=   ui

tx

2

(energy     i-th 

error

eigenval.)
ei :=   x     yiui

   
    ui     ui + (1/di) yiei
    x     x     yiui

(error)
(update estimate)
(repeat with remainder)
faloutsos, kolda, sun

u

sensor 1

5-12

2
2
 
r
o
s
n
e
s

6

faloutsos, kolda, sun

icml   07

cmu scs

mth order sta

dx
t
(

)

matricizing

x

e1

u1 updated

u1

y1

    run 1st order sta along each mode
    complexity:

    storage: o(    ni)
    computation:     ri     ni which is smaller
    computation:     ri     ni which is smaller 

than dta

faloutsos, kolda, sun

5-13

cmu scs

incremental tensor decomposition

   dynamic data model

    tensor streams
    tensor streams

   dynamic tensor decomposition (dta)
   streaming tensor decomposition (sta)
    window-based tensor decomposition 

(wta)
(wta)

1. jimeng sun, spiros papadimitriou, philip yu. window-based tensor analysis on 

high-dimensional and multi-aspect streams, icdm 2006

2. jimeng  sun, dacheng tao,  christos faloutsos.  beyond streams and graphs: 

dynamic tensor analysis,  kdd 2006

faloutsos, kolda, sun

5-14

7

faloutsos, kolda, sun

icml   07

cmu scs

window-based tensor analysis (wta)

n
o
i
t
a
c
o
l

08/13/06

time

faloutsos, kolda, sun

15

  
  1

1
u
 
n
o
i
t
a
c
o
l

time w1

1st factor

5-15

cmu scs

meta-algorithm for 

window-based tensor analysis

n
o
i
t
a
c
o
l

time
utype

d

r0

y

w

utime

ulocation

5-16

r
r1

n1

faloutsos, kolda, sun

8

faloutsos, kolda, sun

icml   07

cmu scs

moving window scheme (mw)
d(n-1,w)

    update the variance 

matrix c(i)
matrix c(i) 
incrementally

 
r
o
s
n
n
e
t

s
m
a
e
e
r
t
s

dn-w
   ...

time
dn
   ...
   ...

    diagonalize c(i) to find 

u(i)

old
cd

d(n,w)

new
cd

a good and efficient 

initialization 

update variance matrix
update variance matrix

u(d)

diagonalize

faloutsos, kolda, sun

5-17

cmu scs

roadmap

matri

    motivation
    matrix tools
tools
    tensor basics
    tensor extensions
    software demo
    case studies

t di

c

faloutsos, kolda, sun

5-18

9

faloutsos, kolda, sun

icml   07

cmu scs

p1: environmental sensor monitoring 

e
u
a
a
v
v

l

30

25

20

15
15

10

5

0
0

40

30

20

10

e
u
a
v

l

2000

4000

6000

8000

10000

time (min)

temperature

0
0

2000

4000

6000

8000

10000

time (min)

humidity

faloutsos, kolda, sun

600

500

400

300

200

100

e
u
a
v

l

0
0

2000

2.5

2

1.5

e
u
a
v

l

1

0.5

0
0

2000

cmu scs

1st factor 

scaling factor 250

time

n
o

i
t

a
c
o
l

p1: sensor monitoring 

time

500

time (min)

1000

e
e
u
u
a
v

l

0.04

0.03

0.02

0.01
0.01

0

   0.01

   0.02

0

location

0.3

0.25

0.2

e 0.15
0.15

e
e
e
u
u
a
a
v
v

l
l

0.1

0.05

0

0

20

location

40

60

e
e
u
a
v

l

0.6

0.4

0.2

0
0

   0.2

   0.4

   0.6

   0.8

4000

6000

time (min)

light

4000

6000

time (min)

8000

10000

8000

10000

voltage

1-19

type

volt humid temp

type

light

    1st factor consists of the main trends:

    daily periodicity on time
    daily periodicity on time
    uniform on all locations
    temp, light and volt are positively correlated while 

negatively correlated with humid

faloutsos, kolda, sun

5-20

10

faloutsos, kolda, sun

icml   07

cmu scs

2nd factor
scaling factor 154

p1: sensor monitoring 

time

location

e
u
u
a
a
v

l
l

0.04

0.03

0.02

0.01
0.01

0

   0.01

   0.02

0

500

time (min)

1000

type

volt humid temp

type

light

e
u
u
a
a
v

l
l

0.8

0.6

0.4

0.2

0

   0.2

   0.4

   0.6

    2nd factor captures an atypical trend:

    uniformly across all time
    concentrating on 3 locations
concentrating on 3 locations
    mainly due to voltage

    interpretation: two sensors have low battery, and the 

other one has high battery. 

faloutsos, kolda, sun

5-21

cmu scs

p3: social network analysis
    multiway id45 (lsi)

    monitor the change of the community structure 

over time

2004

keywords

dm

db

christos faloutsos

ua

michael 

stonebreaker

db

uk

   pattern   

   query   

2004

1990

1990

s
r
o
h
t
u
a

faloutsos, kolda, sun

5-22

11

faloutsos, kolda, sun

icml   07

cmu scs

p3: social network analysis (cont.)

authors
michael carey, michael
stonebreaker h jagadish
stonebreaker, h. jagadish,
hector garcia-molina

surajit chaudhuri,mitch 
cherniack,michael
stonebreaker,ugur etintemel
jiawei han,jian pei,philip s. yu,
jianyong wang,charu c. aggarwal

keywords
queri,parallel,optimization,concurr,
objectorient
objectorient

year
1995

db

distribut,systems,view,storage,servic,process,
cache

2004

streams,pattern,support, cluster, 
index,gener,queri 

dm

2004

    two groups are correctly identified: databases and data 

mining

    people and concepts are drifting over time

faloutsos, kolda, sun

5-23

cmu scs

500

450

400

350

300

250
250

200

150

100

50

n
o
i
t
a
n
i
i
t
t
s
s
e
d

p4: network anomaly detection

50

500

r
o
r
r
r
r
e
e

40

30

20

10

0

200

400

450

400

350

n
o

300

i
t

a
n
n

i
i
t
t
s
e
d

250
250

200

150

100

50

800

1000

1200

100

200

100

200

source

300

400

500

abnormal traffic

600
hours

reconstruction error 

over time

300

400

500

source

normal traffic

    reconstruction error gives indication of anomalies.
    prominent difference between normal and abnormal ones is 
i

p
mainly due to the unusual scanning activity (confirmed by the 
campus admin).

t diff

d b

b t

i

l

l

faloutsos, kolda, sun

5-24

12

faloutsos, kolda, sun

icml   07

cmu scs

p5: web graph mining

    how to order the importance of web pages?

    kleinberg s algorithm hits
kleinberg   s algorithm hits
    id95
    tensor extension on hits (tophits)

faloutsos, kolda, sun

5-25

cmu scs

kleinberg   s hubs and authorities

(the hits method)

sparse adjacency matrix and its svd:

authority scores

for 1st topic

authority scores

for 2nd topic

to

m
o
r
f

hub scores 
for 1st topic

hub scores 
for 2nd topic

faloutsos, kolda, sun

5-26

kleinberg, jacm, 1999

13

faloutsos, kolda, sun

icml   07

cmu scs

hits authorities on sample data

1st principal factor

.97 www.ibm.com
.24 www.alphaworks.ibm.com
.08 www-128.ibm.com
.05 www.developer.ibm.com
.02 www.research.ibm.com
.01 www.redbooks.ibm.com
.01 news.com.com

2nd principal factor

i

l hi h l

.99 www.lehigh.edu
.11 www2.lehigh.edu
.06 www.lehighalumni.com
06
.06 www.lehighsports.com
.02 www.bethlehem-pa.gov
.02 www.adobe.com
.02 lewisweb.cc.lehigh.edu
.02 www.leo.lehigh.edu
.02 www.distance.lehigh.edu
.02 fp1.cc.lehigh.edu

3rd principal factor

p
.75 java.sun.com
.38 www.sun.com
.36 developers.sun.com
.24 see.sun.com
.16 www.samag.com
.13 docs.sun.com
.12 blogs.sun.com
.08 sunsolve.sun.com
.08 www.sun-catalogue.com
.08 news.com.com

authority scores
authority scores

for 2nd topic

authority scores

for 1st topic

to

m
o
r
f

we started our crawl from
http://www-neos.mcs.anl.gov/neos, 
and crawled 4700 pages
and crawled 4700 pages,
resulting in 560 
cross-linked hosts.

4th principal factor

.60 www.pueblo.gsa.gov
.45 www.whitehouse.gov
.35 www.irs.gov
.31 travel.state.gov
.22 www.gsa.gov
.20 www.ssa.gov
.16 www.census.gov
.14 www.govbenefits.gov
.13 www.kids.gov
.13 www.usdoj.gov

6th principal factor

.97 mathpost.asu.edu
.18 math.la.asu.edu
.17 www.asu.edu
.04 www.act.org
.03 www.eas.asu.edu
.02 archives.math.utk.edu
.02 www.geom.uiuc.edu
.02 www.fulton.asu.edu
.02 www.amstat.org
.02 www.maa.org

hub scores 
for 1st topic

hub scores 
for 2nd topic

faloutsos, kolda, sun

5-27

cmu scs

three-dimensional view of the web

observe that this 

tensor is very sparse!

faloutsos, kolda, sun
kolda, bader, kenny, icdm05

5-28

14

faloutsos, kolda, sun

icml   07

cmu scs

topical hits (tophits)

main idea: extend the idea behind the hits model to incorporate 
term (i.e., topical) information.

term scores 
for 1st topic

term scores 
for 2nd topic

authority scores

for 1st topic

hub scores 
for 1st topic

faloutsos, kolda, sun

authority scores

for 2nd topic

hub scores 
for 2nd topic

5-29

to

m
o
r
f

cmu scs

topical hits (tophits)

main idea: extend the idea behind the hits model to incorporate 
term (i.e., topical) information.

to

m
o
r
f

term scores 
for 1st topic

term scores 
for 2nd topic

authority scores

for 1st topic

hub scores 
for 1st topic

faloutsos, kolda, sun

authority scores

for 2nd topic

hub scores 
for 2nd topic

5-30

15

faloutsos, kolda, sun

icml   07

cmu scstophits terms & authorities 

on sample data

1st principal factor

tophits uses 3d analysis to find 
the dominant groupings of web 
pages and terms
pages and terms.

wk = # unique links using term k

3rd principal factor

.20 no-readable-text .99 www.lehigh.edu
.06 www2.lehigh.edu
.16 faculty
.16 search
.16 search
.03 www.lehighalumni.com
.03 www.lehighalumni.com
.16 news
.16 libraries
.16 computing
.12 lehigh

.86 java.sun.com
.23 java
.38 developers.sun.com
.18 sun
2nd principal factor
.16 docs.sun.com
.17 platform
.14 see.sun.com
.16 solaris
.14 www.sun.com
.16 developer
15 no readable text 97 www ibm com
.15 no-readable-text .97 www.ibm.com
09
.09 www.samag.com
15 edition
.15 edition
.18 www.alphaworks.ibm.com
.15 ibm
.07 developer.sun.com
.15 download
.07 www-128.ibm.com
.12 services
.06 sunsolve.sun.com
.14 info
.87 www.pueblo.gsa.gov
.26 information
.05 www.developer.ibm.com
.12 websphere
.05 access1.sun.com
.12 software
.24 www.irs.gov
.24 federal
.12 web
.02 www.redbooks.ibm.com
.12 no-readable-text .05 iforce.sun.com
.23 www.whitehouse.gov
.23 citizen
.11 developerworks .01 www.research.ibm.com
6th principal factor
.22 other
.19 travel.state.gov
.11 linux
.26 president
.18 www.gsa.gov
.19 center
.11 resources
.25 no-readable-text .18 www.irs.gov
.09 www.consumer.gov
.19 languages
.11 technologies
.25 bush
.09 www.kids.gov
.15 u.s
.10 downloads
.25 welcome
.15 publications
.07 www.ssa.gov
.17 white
.05 www.forms.gov
.14 consumer
.16 u.s
.13 free
.04 www.govbenefits.gov
.15 house
.13 budget
13 presidents
.13 presidents
.11 office

4th principal factor

tensor parafac

to

m
o
r
f

term scores 
for 1st topic

term scores 
for 2nd topic

authority scores

for 1st topic

authority scores

for 2nd topic

hub scores 
for 2nd topic

hub scores 
for 1st topic

.87 www.whitehouse.gov

.75 optimization
.58 software
.08 decision
.07 neos
.06 tree
05 guide
.05 guide
.05 search
.05 engine
.05 control
.05 ilog

i

.99 www.adobe.com

16th principal factor
16th principal factor

12th principal factor

.35 www.palisade.com
.35 www.solver.com
13th principal factor
.33 plato.la.asu.edu
.29 www.mat.univie.ac.at
.28 www.ilog.com
ti
26
d h ti
.26 www.dashoptimization.com
.81 www.weather.gov
.26 www.grabitech.com
.41 www.spc.noaa.gov
.25 www-fp.mcs.anl.gov
.30 lwf.ncdc.noaa.gov
.22 www.spyderopts.com
.17 www.mosek.com

.16 travel.state.gov
.10 www.gsa.gov
.08 www.ssa.gov
.05 www.govbenefits.gov
.04 www.census.gov
.46 adobe
.04 www.usdoj.gov
.45 reader
04 www kids gov
.04 www.kids.gov
45 acrobat
.45 acrobat
.02 www.forms.gov
.30 free
.50 weather
.30 no-readable-text
.24 office
.29 here
.23 center
.29 copy
.19 no-readable-text .15 www.cpc.ncep.noaa.gov
.73 www.irs.gov
.22 tax
.05 download
.17 organization
.43 travel.state.gov
.17 taxes
.15 nws
.15 child
.22 www.ssa.gov
.15 severe
.08 www.govbenefits.gov
.15 retirement
.15 fire
.06 www.usdoj.gov
.14 benefits
.15 policy
.03 www.census.gov
.14 state
.14 climate
.03 www.usmint.gov
.14 income
5-31
.02 www.nws.noaa.gov
.13 service
.02 www.gsa.gov
.13 revenue
.12 credit
.01 www.annualcreditreport.com

.14 www.nhc.noaa.gov
.09 www.prh.noaa.gov
.07 aviationweather.gov
.06 www.nohrsc.nws.gov
.06 www.srh.noaa.gov

19th principal factor

faloutsos, kolda, sun

cmu scs

tensor faces

(vasilescu & terzopoulos, 2002; vasilescu & terzopoulos, 2003)

people

expressions

views

faloutsos, kolda, sun

illuminations

5-32

16

faloutsos, kolda, sun

icml   07

cmu scs

eigenfaces

    facial images (identity change)

    eigenfaces bases vectors capture the variability in facial 

appearance (do not decouple pose, illumination,    )

faloutsos, kolda, sun

5-33

cmu scs

data organization

d

    linear/pca: data matrix

    rpixels x images
    a matrix of image vectors

s
l
e
x
i
p

    multilinear: data tensor
d

    rpeople x views x illums x express x pixels
    n-dimensional matrix
n dimensional matrix
    28 people, 45 images/person
    5 views, 3 illuminations, 

3 expressions per person

faloutsos, kolda, sun

d
d

s
n
o
i
t
a
n
m
u

i

l
l
i

images

views

,i
vpp

il
,

,

ex

5-34

17

faloutsos, kolda, sun

icml   07

cmu scs

eigen-
faces

tensor-
faces
faces

faloutsos, kolda, sun

5-35

cmu scs strategic data compression = 

perceptual quality

    tensorfaces data reduction in illumination space primarily degrades 

illumination effects (cast shadows, highlights)

    pca has lower mean square error but higher perceptual error
    pca has lower mean square error but higher perceptual error

tensorfaces

original

6 illum + 11 people param.

176 basis vectors

66 basis vectors

tensorfaces

pca

mean sq. err. = 409.15
3 illum + 11 people param.

33 basis vectors

mean sq. err. = 85.75

33 parameters
33 basis vectors

faloutsos, kolda, sun

5-36

18

faloutsos, kolda, sun

icml   07

cmu scs

tensorfaces: an application of the tucker 

decomposition

    example: 7942 pixels x 16 illuminations x 11 subjects
    pca (eigenfaces): svd of 7942 x 176 matrix
    tensorfaces: tucker decomposition of 7942 x 16 x 11 tensor

f 7942 16 11 t

t k

iti

t

d

f

m.a.o. vasilescu & d. terzopoulos, cvpr   03

7942 x 33

176 x 33

7942 x 3 x 11

16 x 3

11 x 11

eigenfaces

loadings

an image is represented by a linear 

combination of 33 eigenfaces.

tensorfaces

illumination

subjects

an image is represented by a multilinear 

combination of 33 tensorfaces using the outer 
product (or kronecker product) of a length-3 

illumination vector and a length-11 person vector.

g

p

faloutsos, kolda, sun

5-37

cmu scs

methods
svd, pca

cur, cmd

co-id91

tucker

parafac
parafac

summary

pros
optimal in l2 
and frobenius
interpretability, 
sparse bases
interpretability

cons
dense representation, 
negative entries
not optimal like svd, 
dense core
local minimum

flexible 
representation
i t
t bilit
interpretability, 
efficient parse 
computation

interpretability, non-
uniqueness, dense core
sl
slow convergence

incrementalization efficiency
nonnegativity

interpretability, 
sparse results

non-optimal
local minimum, non-
uniqueness
faloutsos, kolda, sun

applications
lsi, id95, 
hits
dna snp data, 
network forensics
social networks, 
microarray data
tensorfaces

tophists
tophists

tensor streams
image 
segmentation
5-38

19

faloutsos, kolda, sun

icml   07

cmu scs

conclusion

    real data are often in high dimensions with 

multiple aspects (modes)
multiple aspects (modes)

    matrix and tensor provide elegant theory 

and algorithms for such data

    however, many problems are still open

    skew distribution, anomaly detection, streaming 
g

y

,

,

algorithm, distributed/parallel algorithms, 
efficient out-of-core processing

faloutsos, kolda, sun

5-39

cmu scs

thank you!

    christos faloutsos 

www.cs.cmu.edu/~christos

    tamara kolda  

csmr.ca.sandia.gov/~tgkolda

    jimeng sun           

ji
www.cs.cmu.edu/~jimeng

s

faloutsos, kolda, sun

5-40

20

