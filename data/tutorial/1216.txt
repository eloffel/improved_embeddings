deep learning

tutorial

icml, atlanta, 2013-06-16

y lecun
ma ranzato

yann lecun
center for data science & courant institute, nyu
yann@cs.nyu.edu
http://yann.lecun.com

marc'aurelio ranzato
google
ranzato@google.com
http://www.cs.toronto.edu/~ranzato

deep learning = learning representations/features

y lecun
ma ranzato

the traditional model of pattern recognition (since the late 50's)
fixed/engineered features (or fixed kernel) + trainable 
classifier

hand-crafted

feature extractor

   simple    trainable 

classifier

end-to-end learning / id171 / deep learning

trainable features (or kernel) + trainable classifier

trainable

feature extractor

trainable 
classifier

this basic model has not evolved much since the 50's

y lecun
ma ranzato

the first learning machine: the id88 

built at cornell in 1960

the id88 was a linear classifier on 
top of a simple feature extractor
the vast majority of practical applications 
of ml today use glorified linear classifiers 
or glorified template matching.
designing a feature extractor requires 
considerable efforts by experts.

e
a
t
u
r
e
 
e
x
t
r
a
c
t
o
r

a f
y=sign(   

i=1

n

wi

w i f i ( x )+b)

architecture of    mainstream   pattern recognition systems

y lecun
ma ranzato

modern architecture for pattern recognition

id103: early 90's     2011

mfcc

mix of gaussians

unsupervised
object recognition: 2006 - 2012

fixed

classifier

supervised

sift
hog

fixed

id116

sparse coding
unsupervised

low-level
features

mid-level
features

pooling

classifier

supervised

deep learning = learning hierarchical representations

y lecun
ma ranzato

it's deep if it has more than one stage of non-linear feature 
transformation

low-level
feature

mid-level
feature

high-level

feature

trainable 
classifier

feature visualization of convolutional net trained on id163 from [zeiler & fergus 2013]

trainable feature hierarchy

y lecun
ma ranzato

hierarchy of representations with increasing level of abstraction
each stage is a kind of trainable feature transform
image recognition
 edge 

 texton 

 object

 motif 

 part 

pixel 

   

   

   

   

   

text

character 

   

 word 

   

 word group 

   

 clause 

   

 sentence 

   

 story

speech

sample 
    
word 

   

 spectral band 

   

 sound 

       

     

 phone 

   

 phoneme 

   
 

learning representations: a challenge for
ml, cv, ai, neuroscience, cognitive science...

y lecun
ma ranzato

how do we learn representations of the perceptual 
world?

how can a perceptual system build itself by 
looking at the world?
how much prior structure is necessary

trainable feature

transform

ml/ai: how do we learn features or feature hierarchies?
what is the fundamental principle? what is the 
learning algorithm? what is the architecture?

trainable feature

transform

neuroscience: how does the cortex learn perception?

does the cortex    run    a single, general 
learning algorithm? (or a small number of 
them)

cogsci: how does the mind learn abstract concepts on 
top of less abstract ones?

trainable feature

transform

trainable feature

transform

deep learning addresses the problem of learning 
hierarchical representations with a single algorithm

the mammalian visual cortex is hierarchical

y lecun
ma ranzato

the ventral (recognition) pathway in the visual cortex has multiple stages
retina - lgn - v1 - v2 - v4 - pit - ait ....
lots of intermediate representations

[picture from simon thorpe]

[gallant & van essen] 

let's be inspired by nature, but not too much

y lecun
ma ranzato

it's nice imitate nature,
but we also need to understand

how do we know which 
details are important?
which details are merely the 
result of evolution, and the 
constraints of biochemistry?

for airplanes, we developed 
aerodynamics and compressible 
fluid dynamics.

we figured that feathers and 
wing flapping weren't crucial

question: what is the 
equivalent of aerodynamics for 
understanding intelligence?

l'avion iii de cl  ment ader, 1897
(mus  e du cnam, paris)
his eole took off from the ground in 1890,
13 years before the wright brothers, but you 
probably never heard of it.

trainable feature hierarchies: end-to-end learning

y lecun
ma ranzato

a hierarchy of trainable feature transforms

each module transforms its input representation into a higher-level 
one.
high-level features are more global and more invariant
low-level features are shared among categories

trainable
feature
transform

trainable
feature
transform

trainable
classifier/
predictor

learned internal representations

how can we make all the modules trainable and get them to learn 
appropriate representations?

three types of deep architectures

y lecun
ma ranzato

feed-forward: multilayer neural nets, convolutional nets

feed-back: stacked sparse coding, deconvolutional nets

bi-drectional: deep id82s, stacked auto-encoders

three types of training protocols

purely supervised

initialize parameters randomly
train in supervised mode

y lecun
ma ranzato

typically with sgd, using backprop to compute gradients 

used in most practical systems for speech and image 
recognition

unsupervised, layerwise + supervised classifier on top 

train each layer unsupervised, one after the other
train a supervised classifier on top, keeping the other layers 
fixed
good when very few labeled samples are available

unsupervised, layerwise + global supervised fine-tuning

train each layer unsupervised, one after the other
add a classifier layer, and retrain the whole thing supervised
good when label set is poor (e.g. pedestrian detection)

unsupervised pre-training often uses regularized auto-encoders

do we really need deep architectures?

y lecun
ma ranzato

theoretician's dilemma:    we can approximate any function as close as we 
want with shallow architecture. why would we need deep ones?   

kernel machines (and 2-layer neural nets) are    universal   .

deep learning machines

deep machines are more efficient for representing certain classes of 
functions, particularly those involved in visual recognition

they can represent more complex functions with less    hardware    

we need an efficient parameterization of the class of functions that are 
useful for    ai    tasks (vision, audition, nlp...)

why would deep architectures be more efficient?
[bengio & lecun 2007    scaling learning algorithms towards ai   ]

y lecun
ma ranzato

a deep architecture trades space for time (or breadth for depth)

more layers (more sequential computation), 
but less hardware (less parallel computation).

example1: n-bit parity

requires n-1 xor gates in a tree of depth log(n).
even easier if we use threshold gates
requires an exponential number of gates of we restrict ourselves 
to 2 layers (dnf formula with exponential number of minterms).

example2:  circuit for addition of 2 n-bit binary numbers

requires o(n) gates, and o(n) layers using n one-bit adders with 
ripple carry propagation.
requires lots of gates (some polynomial in n) if we restrict 
ourselves to two layers (e.g. disjunctive normal form).
bad news: almost all boolean functions have a dnf formula with 
an exponential number of minterms o(2^n).....

y lecun
ma ranzato

which models are deep?

2-layer models are not deep (even if 
you train the first layer) 

because there is no feature 
hierarchy

neural nets with 1 hidden layer are not 
deep
id166s and kernel methods are not deep
layer1: kernels; layer2: linear
the first layer is    trained    in 
with the simplest unsupervised 
method ever devised: using 
the samples as templates for 
the id81s.

classification trees are not deep

no hierarchy of features. all 
decisions are made in the input 
space

are id114 deep?

y lecun
ma ranzato

there is no opposition between id114 and deep learning. 
many deep learning models are formulated as factor graphs
some id114 use deep architectures inside their factors

id114 can be deep (but most are not).
factor graph: sum of energy functions

over inputs x, outputs y and latent variables z. trainable parameters: w

   log p ( x ,y , z /w )   e ( x ,y , z ,w )=   i ei( x ,y ,z ,w i)
e1(x1,y1)

e3(z2,y1)

e4(y3,y4)

e2(x2,z1,z2)

x1

z1

x2

z2

y1

z3

y2

each energy function can contain a deep network
the whole factor graph can be seen as a deep network

deep learning: a theoretician's nightmare? 

y lecun
ma ranzato

deep learning involves non-convex id168s
with non-convex losses, all bets are off
then again, every id103 system ever deployed 
has used non-id76 (gmms are non convex).

but to some of us all    interesting    learning is non convex

convex learning is invariant to the order in which sample are 
presented (only depends on asymptotic sample frequencies).
human learning isn't like that: we learn simple concepts 
before complex ones. the order in which we learn things 
matter.

deep learning: a theoretician's nightmare? 

y lecun
ma ranzato

no generalization bounds?

actually, the usual vc bounds apply: most deep learning 
systems have a finite vc dimension
we don't have tighter bounds than that. 
but then again, how many bounds are tight enough to be 
useful for model selection?

it's hard to prove anything about deep learning systems

then again, if we only study models for which we can prove 
things, we wouldn't have speech, handwriting, and visual 
object recognition systems today.

deep learning: a theoretician's paradise? 

y lecun
ma ranzato

deep learning is about representing high-dimensional data

there has to be interesting theoretical questions there
what is the geometry of natural signals?
is there an equivalent of statistical learning theory for 
unsupervised learning?
what are good criteria on which to base unsupervised 
learning?

deep learning systems are a form of latent variable factor graph

internal representations can be viewed as latent variables to 
be inferred, and id50 are a particular type of 
latent variable models.
the most interesting deep belief nets have intractable loss 
functions: how do we get around that problem?

lots of theory at the 2012 ipam summer school on deep learning

wright's parallel sgd methods, mallat's    scattering transform   , 
osher's    split bregman    methods for sparse modeling, 
morton's    algebraic geometry of dbn   ,....

deep learning and id171 today

y lecun
ma ranzato

deep learning has been the hottest topic in id103 in the last 2 years

a few long-standing performance records were broken with deep 
learning methods
microsoft and google have both deployed dl-based speech 
recognition system in their products
microsoft, google, ibm, nuance, at&t, and all the major academic 
and industrial players in id103 have projects on deep 
learning

deep learning is the hottest topic in id161

feature engineering is the bread-and-butter of a large portion of the 
cv community, which creates some resistance to id171
but the record holders on id163 and semantic segmentation are 
convolutional nets

deep learning is becoming hot in natural language processing
deep learning/id171 in applied mathematics

the connection with applied math is through sparse coding, 
non-id76, stochastic gradient algorithms, etc... 

in many fields, id171 has caused a revolution
(methods used in commercially deployed systems)

y lecun
ma ranzato

id103 i (late 1980s)

trained mid-level features with gaussian mixtures (2-layer classifier)

handwriting recognition and ocr (late 1980s to mid 1990s)

supervised convolutional nets operating on pixels

face & people detection (early 1990s to mid 2000s)

supervised convolutional nets operating on pixels (ylc 1994, 2004, 
garcia 2004) 
haar features generation/selection (viola-jones 2001)

object recognition i (mid-to-late 2000s: ponce, schmid, yu, ylc....)

trainable mid-level features (id116 or sparse coding)

low-res object recognition: road signs, house numbers (early 2010's)

supervised convolutional net operating on pixels

id103 ii (circa 2011)

deep neural nets for acoustic modeling

object recognition iii, semantic labeling (2012, hinton, ylc,...)

supervised convolutional nets operating on pixels

shallow

boosting

id88

ae

d-ae

deep
y lecun
ma ranzato

neural net

id56
conv. net

id166

rbm

dbn dbm

gmm

sparse 
coding

bayesnp

decisiontree

   
   
shallow

boosting

neural networks

id88

ae

d-ae

neural net

id56
conv. net

deep
y lecun
ma ranzato

id166

rbm

dbn dbm

gmm

sparse 
coding

bayesnp

probabilistic models

decisiontree

   
   
shallow

boosting

neural networks

id88

ae

d-ae

neural net

id56
conv. net

id166

rbm

dbn dbm

deep
y lecun
ma ranzato

gmm

sparse 
coding

bayesnp

probabilistic models

decisiontree

supervised

unsupervised

supervised

   
   
shallow

boosting

perceptro
n

ae

d-ae

deep
y lecun
ma ranzato

neural net

id56

id166

rbm

dbn dbm

conv. net

gmm

sparse 
coding

bayesnp

decisiontree

in this talk, we'll focus on the 
simplest and typically most 

effective methods.

   
   
y lecun
ma ranzato

what are

good feature?

discovering the hidden structure in high-dimensional data
the manifold hypothesis

y lecun
ma ranzato

learning representations of data:

discovering & disentangling the independent 
explanatory factors

the manifold hypothesis:

natural data lives in a low-dimensional (non-linear) manifold
because variables in natural data are mutually dependent

discovering the hidden structure in high-dimensional data

y lecun
ma ranzato

example: all face images of a person

1000x1000 pixels = 1,000,000 dimensions
but the face has 3 cartesian coordinates and 3 euler angles
and humans have less than about 50 muscles in the face
hence the manifold of face images for a person has <56 dimensions

the perfect representations of a face image:

its coordinates on the face manifold
its coordinates away from the manifold

we do not have good and general methods to learn functions that turns an 
image into this kind of representation

ideal
feature
extractor

[ 1.2
   2...] face/not face

pose
lighting
expression
-----

   3
0.2

disentangling factors of variation

the ideal disentangling feature extractor

y lecun
ma ranzato

view

pixel n

ideal
feature
extractor

pixel 2

pixel 1

expression

data manifold & invariance: 
some variations must be eliminated

y lecun
ma ranzato

azimuth-elevation manifold. ignores lighting.

[hadsell et al. cvpr 2006]

basic idea fpr invariant id171

y lecun
ma ranzato

embed the input non-linearly into a high(er) dimensional space

in the new space, things that were non separable may become 
separable

pool regions of the new space together

bringing together things that are semantically similar. like 
pooling.

non-linear
function

pooling

or

aggregation

input

high-dim

unstable/non-smooth 

 features

stable/invariant

features

non-linear expansion     pooling

y lecun
ma ranzato

entangled data manifolds

non-linear dim

expansion,
disentangling

pooling.

aggregation

sparse non-linear expansion     pooling

y lecun
ma ranzato

use id91 to break things apart, pool together similar things

id91,
quantization,
sparse coding

pooling.

aggregation

overall architecture: 
id172     filter bank     non-linearity     pooling

y lecun
ma ranzato

norm

filter
bank 

non-
linear

feature
pooling 

norm

filter
bank 

non-
linear

feature
pooling 

classifier

stacking multiple stages of  

[id172 

 filter bank 

   

   

 non-linearity 

   

 pooling].

id172: variations on whitening

subtractive: average removal, high pass filtering
divisive: local contrast id172, variance id172
filter bank: dimension expansion, projection on overcomplete basis
non-linearity: sparsification, saturation, lateral inhibition....

rectification (relu), component-wise shrinkage, tanh, 
winner-takes-all
x i; l p: p    x i

p ; prob : 1

pooling: aggregation over space or feature type

b log(   

i

e bx i)

 

y lecun
ma ranzato

deep supervised learning

(modular approach)

multimodule systems: cascade

y lecun
ma ranzato

complex learning machines can be 
built by assembling modules into 
networks
 simple example: sequential/layered 
feed-forward architecture (cascade)
forward propagation:

multimodule systems: implementation

y lecun
ma ranzato

each module is an object

contains trainable 
parameters
inputs are arguments
output is returned, but also 
stored internally
example: 2 modules m1, m2

torch7 (by hand)

hid  =  m1:forward(in)
out  =  m2:forward(hid)

torch7 (using the nn.sequential class)

model  =  nn.sequential()
model:add(m1)
model:add(m2)
out  =  model:forward(in)  

computing the gradient in multi-layer systems

y lecun
ma ranzato

computing the gradient in multi-layer systems

y lecun
ma ranzato

computing the gradient in multi-layer systems

y lecun
ma ranzato

jacobians and dimensions

y lecun
ma ranzato

back propgation

y lecun
ma ranzato

multimodule systems: implementation

y lecun
ma ranzato

id26 through a module

contains trainable parameters
inputs are arguments
gradient with respect to input is 
returned. 
arguments are input and 
gradient with respect to output

torch7 (by hand)
hidg  =  
m2:backward(hid,outg)
ing  =  m1:backward(in,hidg)

torch7 (using the nn.sequential class)

ing  =  
model:backward(in,outg)  

linear module

y lecun
ma ranzato

tanh module (or any other pointwise function)

y lecun
ma ranzato

euclidean distance module

y lecun
ma ranzato

any architecture works

y lecun
ma ranzato

any connection is permissible

networks with loops must be 
   unfolded in time   .

any module is permissible

as long as it is continuous and 
differentiable almost everywhere 
with respect to the parameters, and 
with respect to non-terminal inputs.

module-based deep learning with torch7

y lecun
ma ranzato

torch7 is based on the lua language

simple and lightweight scripting language, dominant in the game industry
has a native just-in-time compiler (fast!)
has a simple foreign function interface to call c/c++ functions from lua

torch7 is an extension of lua with

a multidimensional array engine with cuda and openmp backends
a machine learning library that implements multilayer nets, convolutional 
nets, unsupervised pre-training, etc
various libraries for data/image manipulation and id161
a quickly growing community of users

single-line installation on ubuntu and mac osx:

curl -s https://raw.github.com/clementfarabet/torchinstall/master/install | bash

torch7 machine learning tutorial (neural net, convnet, sparse auto-encoder):

http://code.cogbits.com/wiki/doku.php

example: building a neural net in torch7

y lecun
ma ranzato

net for svhn digit recognition

10 categories

input is 32x32 rgb (3 channels)

1500 hidden units

creating a 2-layer net

make a cascade module

reshape input to vector

add linear module

add tanh module

add linear module

add log softmax layer

noutputs  =  10;  
nfeats  =  3;  width  =  32;  height  =  32
ninputs  =  nfeats*width*height
nhiddens  =  1500

      simple  2  layer  neural  network
model  =  nn.sequential()
model:add(nn.reshape(ninputs))
model:add(nn.linear(ninputs,nhiddens))
model:add(nn.tanh())
model:add(nn.linear(nhiddens,noutputs))
model:add(nn.logsoftmax())

criterion  =  nn.classnllcriterion()

create id168 module

 see torch7 example at http://bit.ly/16tylax

example: training a neural net in torch7

y lecun
ma ranzato

create a    closure    feval(x) that takes the 
parameter vector as argument and returns 
the loss and its gradient on the batch.

one epoch over training set

get next batch of samples

run model on batch

for  t  =  1,traindata:size(),batchsize  do
    inputs,outputs  =  getnextbatch()
    local  feval  =  function(x)
        parameters:copy(x)
        gradparameters:zero()
        local  f  =  0
        for  i  =  1,#inputs  do
            local  output  =  model:forward(inputs[i])
            local  err  =  criterion:forward(output,targets[i])
            f  =  f  +  err
            local  df_do  =  criterion:backward(output,targets[i])
            model:backward(inputs[i],  df_do)
        end
        gradparameters:div(#inputs)
        f  =  f/#inputs
        return  f,gradparameters
    end           of  feval
    optim.sgd(feval,parameters,optimstate)
end

backprop

normalize by size of batch

return loss and gradient

call the stochastic gradient optimizer

toy code (matlab): neural net trainer

y lecun
ma ranzato

% f-prop
for i = 1 : nr_layers - 1
  [h{i}  jac{i}]  =  nonlinearity(w{i} * h{i-1} +  b{i});
end
h{nr_layers-1}  =  w{nr_layers-1} * h{nr_layers-2}  +   b{nr_layers-1};
prediction  =  softmax(h{l-1});
% cross id178 loss
loss  =  -  sum(sum(log(prediction)  .*  target)) / batch_size;
% b-prop
dh{l-1}  =  prediction  -  target;
for i = nr_layers     1 : -1 : 1
  wgrad{i}  =  dh{i} * h{i-1}';
  bgrad{i}  =  sum(dh{i}, 2);        
  dh{i-1}  =  (w{i}' * dh{i})  .*  jac{i-1};        
end
% update
for i = 1 : nr_layers - 1
  w{i}  =  w{i}       (lr / batch_size)  *  wgrad{i}; 
  b{i}  =  b{i}       (lr / batch_size)  *  bgrad{i}; 
end

deep supervised learning is non-convex

y lecun
ma ranzato

example: what is the id168 for the simplest 2-layer neural net ever

function: 1-1-1 neural net. map 0.5 to 0.5 and -0.5 to -0.5 
(identity function) with quadratic cost: 

backprop in practice

y lecun
ma ranzato

use relu non-linearities (tanh and logistic are falling out of favor)
use cross-id178 loss for classification
use stochastic id119 on minibatches
shuffle the training samples
normalize the input variables (zero mean, unit variance)
schedule to decrease the learning rate
use a bit of l1 or l2 id173 on the weights (or a combination)

but it's best to turn it on after a couple of epochs

use    dropout    for id173

hinton et al 2012 http://arxiv.org/abs/1207.0580

lots more in [lecun et al.    efficient backprop    1998]
lots, lots more in    neural networks, tricks of the trade    (2012 edition) 
edited by g. montavon, g. b. orr, and k-r m  ller (springer)

y lecun
ma ranzato

deep learning

in id103

case study #1: acoustic modeling

y lecun
ma ranzato

a typical id103 system:

feature
extraction

neural
network

decoder

transducer

&

language

model

h

 

i
,
 
h
o
w
a
r
e
 
y
o
u
?

case study #1: acoustic modeling

y lecun
ma ranzato

a typical id103 system:

h

 

i
,
 
h
o
w
a
r
e
 
y
o
u
?

feature
extraction

neural
network

decoder

transducer

&

language

model

 here, we focus only on the prediction of phone states from 
short time-windows of spectrogram.
 for simplicity, we will use a fully connected neural network 
(in practice, a convolutional net does better).

mohamed et al.    dbns for phone recognition    nips workshop 2009
zeiler et al.    on rectified linear units for id103    icassp 2013

data

y lecun
ma ranzato

 us english: voice search, voice typing, read data
 billions of training samples
 input: log-energy filter bank outputs

40 frequency bands
26 input frames 

 output: 8000 phone states

zeiler et al.    on rectified linear units for id103    icassp 2013

architecture

y lecun
ma ranzato

 from 1 to 12 hidden layers
 for simplicity, the same number of hidden units at each layer:  
1040     2560     2560             2560     8000
 non-linearities: __/   output = max(0, input)

zeiler et al.    on rectified linear units for id103    icassp 2013

energy & loss

y lecun
ma ranzato

 since it is a standard classification problem, the energy is:

e     x , y   =    y f     x    

y 1-of-n vector

 the loss is the negative log-likelihood:

l=e    x , y      log         y exp      e     x ,    y         

zeiler et al.    on rectified linear units for id103    icassp 2013

optimization

y lecun
ma ranzato

 sgd with schedule on learning rate

    l
       t   1

   t        t    1       t

   t=

   

max    1, t
t    

 mini-batches of size 40
 asynchronous sgd (using 100 copies of the network on a few 
hundred machines). this speeds up training at google but it is not 
crucial.

zeiler et al.    on rectified linear units for id103    icassp 2013

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

fprop

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

fprop
h2= f    x ;w 1   

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

fprop
h2= f    h1 ;w 2   

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

fprop
hn= f    hn   1   

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

fprop

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

bprop
    l
    hn   1
    l
   w n

=     l
    hn
=     l
    hn

    hn
    hn   1
    hn
    w n

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

bprop
=     l
    l
    h2
    h1
=     l
    l
    h2
   w 2

    h2
    h1
    h2
   w 2

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

bprop
=     l
    h1

    l
   w 1

    h1
   w 1

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

label y

training

 given an input mini-batch

y lecun
ma ranzato

negative 

log-likelihood

max    0,w n hn   1   

max    0,w 2 h1   

max    0,w 1 x    

parameter
update

                 

    l
      

label y

training

y lecun
ma ranzato

word error rate

y lecun
ma ranzato

number of 
hidden layers

word error rate %

1

2

4

8

12

16

12.8

11.4

10.9

11.1

gmm baseline: 15.4%

zeiler et al.    on rectified linear units for id103    icassp 2013

y lecun
ma ranzato

convolutional

networks

convolutional nets

y lecun
ma ranzato

are deployed in many practical applications

image recognition, id103, google's and baidu's 
photo taggers

have won several competitions

id163, kaggle facial expression, kaggle multimodal 
learning, german traffic signs, connectomics, handwriting....

are applicable to array data where nearby values are correlated

images, sound, time-frequency representations, video, 
volumetric images, rgb-depth images,.....

one of the few models that can be trained purely supervised

layer 1
64x75x7
5

input
83x83

layer 2
64@14x14

layer 3
256@6x6

layer 4
256@1x1

output
101

9x9
convolution
(64 kernels)

10x10 pooling,
5x5 subsampling

9x9
convolution
(4096 kernels)

6x6 pooling
4x4 subsamp

fully-connected neural net in high dimension

y lecun
ma ranzato

example: 200x200 image

fully-connected, 400,000 hidden units = 16 billion parameters
locally-connected, 400,000 hidden units 10x10 fields = 40 
million params
local connections capture local dependencies

y lecun
ma ranzato

example: 200x200 image

400,000 hidden units with 
10x10 fields = 1000 
params
10 feature maps of size 
200x200, 10 filters of size 
10x10

shared weights & convolutions: 
exploiting stationarity

features that are useful on one part of the image 
and probably useful elsewhere.
all units share the same set of weights
shift equivariant processing: 

when the input shifts, the output also 
shifts but stays otherwise unchanged.

convolution 

with a learned kernel (or filter)
non-linearity: relu (rectified linear)

aij=   kl w kl x i + j. k+ l
z ij=max(0, aij)

the filtered    image    z is called a feature map 

multiple convolutions with different kernels

y lecun
ma ranzato

detects multiple motifs at each 
location
the collection of units looking at 
the same patch is akin to a 
feature vector for that patch.
the result is a 3d array, where 
each slice is a feature map.

multiple 
convolutions

early hierarchical feature models for vision

y lecun
ma ranzato

[hubel & wiesel 1962]: 

simple cells detect local features
complex cells    pool    the outputs of simple 
cells within a retinotopic neighborhood. 

   simple cells   

   complex 
cells   

multiple 
convolutions

pooling 
subsampling

cognitron & neocognitron [fukushima 1974-1982]

the convolutional net model 
(multistage hubel-wiesel system)

y lecun
ma ranzato

   simple cells   

   complex cells   

training is supervised
with stochastic gradient 
descent

multiple 
convolutions

pooling 
subsampling

retinotopic feature maps

[lecun et al. 89]
[lecun et al. 98]

feature transform: 
id172     filter bank     non-linearity     pooling

y lecun
ma ranzato

norm

filter
bank 

non-
linear

feature
pooling 

norm

filter
bank 

non-
linear

feature
pooling 

classifier

stacking multiple stages of  

[id172 

 filter bank 

   

   

 non-linearity 

   

 pooling].

id172: variations on whitening

subtractive: average removal, high pass filtering
divisive: local contrast id172, variance id172

filter bank: dimension expansion, projection on overcomplete basis
non-linearity: sparsification, saturation, lateral inhibition....

rectification, component-wise shrinkage, tanh, winner-takes-all

pooling: aggregation over space or feature type, subsampling
 x i; l p: p    x i

p ; prob : 1

b log(   

i

e bx i)

feature transform: 
id172     filter bank     non-linearity     pooling

y lecun
ma ranzato

norm

filter
bank 

non-
linear

feature
pooling 

norm

filter
bank 

non-
linear

feature
pooling 

classifier

filter bank     non-linearity = non-linear embedding in high dimension
feature pooling = contraction, id84, smoothing
learning the filter banks at every stage
creating a hierarchy of features
basic elements are inspired by models of the visual (and auditory) cortex

simple cell + complex cell model of [hubel and wiesel 1962]
many    traditional    feature extraction methods are based on this
sift, gist, hog, surf...

 [fukushima 1974-1982], [lecun 1988-now], 

since the mid 2000: hinton, seung, poggio, ng,....

convolutional network (convnet)

y lecun
ma ranzato

layer 1
64x75x75

layer 2
64@14x14

input
83x83

layer 3
256@6x6

layer 4
256@1x1 output

101

9x9
convolution
(64 kernels)

10x10 pooling,
5x5 subsampling

9x9
convolution
(4096 kernels)

6x6 pooling
4x4 subsamp

non-linearity: half-wave rectification, shrinkage function, sigmoid
pooling: average, l1, l2, max
training: supervised (1988-2006), unsupervised+supervised (2006-now)

convolutional network architecture

y lecun
ma ranzato

convolutional network (vintage 1990) 

y lecun
ma ranzato

filters     tanh     average-tanh     filters     tanh     average-tanh     filters     tanh

curved
manifold

flatter
manifold

   mainstream    object recognition pipeline 2006-2012:
 somewhat similar to convnets

y lecun
ma ranzato

filter
bank 

non-

linearity

feature
pooling 

filter
bank 

non-

linearity

feature
pooling 

classifier

oriented
 edges

winner
takes all

histogram
(sum)

id116 
sparse coding

spatial max
or average

any simple
classifier

fixed (sift/hog/...)

unsupervised

supervised

fixed features + unsupervised mid-level features + simple classifier 

sift + vector quantization + pyramid pooling + id166

[lazebnik et al. cvpr 2006]

sift + local sparse coding macrofeatures + pyramid pooling + id166

[boureau et al. iccv 2011]

sift + fisher vectors + deformable parts pooling + id166

[perronin et al. 2012]

tasks for which deep convolutional nets are the best

y lecun
ma ranzato

handwriting recognition mnist (many), arabic hwx (idsia)
ocr in the wild [2011]: streetview house numbers (nyu and others)
traffic sign recognition [2011] gtsrb competition (idsia, nyu)
pedestrian detection [2013]: inria datasets and others (nyu)
volumetric brain image segmentation [2009] connectomics (idsia, mit)
human action recognition [2011] hollywood ii dataset (stanford)
object recognition [2012] id163 competition
scene parsing [2012] stanford bgd, siftflow, barcelona (nyu) 
scene parsing from depth images [2013] nyu rgb-d dataset (nyu)
id103 [2012] acoustic modeling (ibm and google)
breast cancer cell mitosis detection [2011] mitos (idsia)

the list of perceptual tasks for which convnets hold the record is growing.
most of these tasks (but not all) use purely supervised convnets. 

ideas from neuroscience and psychophysics

y lecun
ma ranzato

the whole architecture: simple cells and complex cells
local receptive fields
self-similar receptive fields over the visual field (convolutions)
pooling (complex cells)
non-linearity: rectified linear units (relu)
lgn-like band-pass filtering and contrast id172 in the input
divisive contrast id172 (from heeger, simoncelli....)

lateral inhibition

sparse/overcomplete representations (olshausen-field....)
id136 of sparse representations with lateral inhibition
sub-sampling ratios in the visual cortex
between 2 and 3 between v1-v2-v4

crowding and visual metamers give cues on the size of the pooling areas

simple convnet applications with state-of-the-art performance

y lecun
ma ranzato

traffic sign recognition (gtsrb)

german traffic sign reco 
bench 
99.2% accuracy
#1: idsia; #2 nyu

house number recognition (google) 

street view house numbers
94.3 % accuracy

prediction of epilepsy seizures from intra-cranial eeg

y lecun
ma ranzato

piotr mirowski, deepak mahdevan (nyu neurology), yann lecun

epilepsy prediction

temporal convolutional net

y lecun
ma ranzato

32

outputs

32

integration of
all channels 
and all features
across several 
time samples

feature extraction
over short time

windows

for individual

channels

(we look for

10 sorts

of features)

s
l
e
n
n
a
h
c
 
g
e
e

32

   

64

4

   

   

   

   

   

       

   
   

   

   

   

10

8

integration of

all channels and all features
across several time samples

   

   

   

   

inputs

384

time, in samples

convnet in connectomics 
[jain, turaga, seung 2007-present]

3d convnet to segment volumetric images

y lecun
ma ranzato

object recognition [krizhevsky, sutskever, hinton 2012]

y lecun
ma ranzato

won the 2012 id163 lsvrc. 60 million parameters, 832m mac ops
4m
16m
37m

full connect
full 4096/relu
full 4096/relu

16m
37m

4mflop

max pooling

conv 3x3/relu 256fm
conv 3x3relu 384fm
conv 3x3/relu 384fm
max pooling 2x2sub

local contrast norm
conv 11x11/relu 256fm

max pool 2x2sub

local contrast norm
conv 11x11/relu 96fm

442k
1.3m
884k

307k

35k

74m
224m
149m

223m

105m

object recognition: ilsvrc 2012 results

y lecun
ma ranzato

id163 large scale visual recognition challenge
1000 categories, 1.5 million labeled training samples

object recognition [krizhevsky, sutskever, hinton 2012]

y lecun
ma ranzato

method: large convolutional net

650k neurons, 832m synapses, 60m parameters 
trained with backprop on gpu
trained    with all the tricks yann came up with in 
the last 20 years, plus dropout    (hinton, nips 
2012)
rectification, contrast id172,...

error rate: 15% (whenever correct class isn't in top 5)
previous state of the art: 25% error

a revolution in id161

acquired by google in jan 2013
deployed in google+ photo tagging in may 2013

object recognition [krizhevsky, sutskever, hinton 2012]

y lecun
ma ranzato

object recognition [krizhevsky, sutskever, hinton 2012]

test 
image

retrieved images

y lecun
ma ranzato

convnet-based google+ photo tagger

searched my personal collection for    bird   

y lecun
ma ranzato

samy
bengio
???

another id163-trained convnet 
[zeiler & fergus 2013]

y lecun
ma ranzato

convolutional net with 8 layers, input is 224x224 pixels

conv-pool-conv-pool-conv-conv-conv-full-full-full
rectified-linear units (relu):  y = max(0,x)
divisive contrast id172 across features [jarrett et al. 
iccv 2009]

trained on id163 2012 training set

1.3m images, 1000 classes
10 different crops/flips per image

id173: dropout

[hinton 2012]
zeroing random subsets of units

stochastic id119 

for 70 epochs (7-10 days)
with learning rate annealing

object recognition on-line demo [zeiler & fergus 2013]

y lecun
ma ranzato

http://horatio.cs.nyu.edu

convnet trained on id163 [zeiler & fergus 2013]

y lecun
ma ranzato

features are generic: caltech 256

y lecun
ma ranzato

state of the art with 
only 6 training examples

network first 
trained on 
id163.

last layer 
chopped off
last layer trained 
on caltech 256,
first layers n-1 
kept fixed.

state of the art 
accuracy with only 
6 training 
samples/class

3: [bo, ren, fox. cvpr, 2013]   16: [sohn, jung, lee, hero iccv 2011]

features are generic: pascal voc 2012

y lecun
ma ranzato

network first trained on id163. 
last layer trained on pascal voc, keeping n-1 first layers fixed.

[15] k. sande, j. uijlings, c. snoek, and a. smeulders. hybrid coding for selective search. in 
pascal voc classification challenge 2012, 
[19] s. yan, j. dong, q. chen, z. song, y. pan, w. xia, z. huang, y. hua, and s. shen. generalized 
hierarchical matching for sub-category aware object classification. in pascal voc classification 
challenge 2012

semantic labeling:
labeling every pixel with the object it belongs to

y lecun
ma ranzato

would help identify obstacles, targets, landing sites, dangerous areas
would help line up depth map with edge maps

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling: convnet architecture

y lecun
ma ranzato

each output sees a large input context:

46x46 window at full rez; 92x92 at    rez; 184x184 at    rez
[7x7conv]->[2x2pool]->[7x7conv]->[2x2pool]->[7x7conv]->
trained supervised on fully-labeled images

categories

laplacian
pyramid

level 1 
features

level 2
features

upsampled
level 2 features

scene parsing/labeling: performance

y lecun
ma ranzato

stanford background dataset [gould 1009]: 8 categories

[farabet et al. ieee t. pami 2013]

scene parsing/labeling: performance

y lecun
ma ranzato

sift flow dataset
[liu 2009]: 
33 categories

barcelona dataset
[tighe 2010]: 
170 categories.

[farabet et al. ieee t. pami 2012]

scene parsing/labeling: sift flow dataset (33 categories)

y lecun
ma ranzato

samples from the sift-flow dataset (liu)

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling: sift flow dataset (33 categories)

y lecun
ma ranzato

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling

y lecun
ma ranzato

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling

y lecun
ma ranzato

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling

y lecun
ma ranzato

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling

y lecun
ma ranzato

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling

y lecun
ma ranzato

no post-processing
frame-by-frame
convnet runs at 50ms/frame on virtex-6 fpga hardware

but communicating the features over ethernet limits system 
performance

scene parsing/labeling: temporal consistency

y lecun
ma ranzato

causal method for temporal consistency

[couprie, farabet, najman, lecun iclr 2013, icip 2013]

nyu rgb-depth indoor scenes dataset

y lecun
ma ranzato

407024 rgb-d images of apartments
1449 labeled frames, 894 object categories

[silberman et al. 2012]

scene parsing/labeling on rgb+depth images

y lecun
ma ranzato

with temporal consistency

[couprie, farabet, najman, lecun iclr 2013, icip 2013]

scene parsing/labeling on rgb+depth images

y lecun
ma ranzato

with temporal consistency

[couprie, farabet, najman, lecun iclr 2013, icip 2013]

semantic segmentation on rgb+d images and videos

y lecun
ma ranzato

[couprie, farabet, najman, lecun iclr 2013, icip 2013]

y lecun
ma ranzato

energy-based 

unsupervised learning

energy-based unsupervised learning

y lecun
ma ranzato

learning an energy function (or contrast function) that takes

low values on the data manifold
higher values everywhere else

y2

y1

capturing dependencies between variables 
with an energy function

y lecun
ma ranzato

the energy surface is a    contrast function    that takes low values on the 
data manifold, and higher values everywhere else

special case: energy = negative log density
example: the samples live in the manifold 

y 2=(y 1)2

y2

y1

transforming energies into probabilities (if necessary)

y lecun
ma ranzato

the energy can be interpreted as an unnormalized negative log density
gibbs distribution: id203 proportional to exp(-energy)

beta parameter is akin to an inverse temperature 

don't compute probabilities unless you absolutely have to

because the denominator is often intractable

p(y|w)

e(y,w)

y

y

learning the energy function

y lecun
ma ranzato

parameterized energy function e(y,w)

make the energy low on the samples
make the energy higher everywhere else
making the energy low on the samples is easy
but how do we make it higher everywhere else?

seven strategies to shape the energy function

y lecun
ma ranzato

 1. build the machine so that the volume of low energy stuff is constant

pca, id116, gmm, square ica

 2. push down of the energy of data points, push up everywhere else

max likelihood (needs tractable partition function)

 3. push down of the energy of data points, push up on chosen locations

 contrastive divergence, ratio matching, noise contrastive 
estimation, minimum id203 flow

 4. minimize the gradient and maximize the curvature around data points 

score matching

 5. train a dynamical system so that the dynamics goes to the manifold

denoising auto-encoder

 6. use a regularizer that limits the volume of space that has low energy

sparse coding, sparse auto-encoder, psd

 7. if e(y) = ||y - g(y)||^2, make g(y) as "constant" as possible.

contracting auto-encoder, saturating auto-encoder

#1: constant volume of low energy

y lecun
ma ranzato

 1. build the machine so that the volume of low energy stuff is constant

pca, id116, gmm, square ica...

pca
e (y )=   w t wy    y   2

id116,  
z constrained to 1-of-k code
e (y )=minz   i   y    w i z i   2

#2: push down of the energy of data points, 
push up everywhere else

y lecun
ma ranzato

max likelihood (requires a tractable partition function)

maximizing p(y|w) on training 
samples

make this big

p(y)

make this small

minimizing -log p(y,w) on training 
samples

e(y)

make this small

make this big

y

y

#2: push down of the energy of data points, 
push up everywhere else

y lecun
ma ranzato

gradient of the  negative log-likelihood loss for one sample y:

id119:

e(y)

pushes down on the
energy of the samples

pulls up on the
energy of low-energy y's

y

y

#3. push down of the energy of data points, 
push up on chosen locations

y lecun
ma ranzato

contrastive divergence, ratio matching, noise contrastive estimation, 
minimum id203 flow
contrastive divergence: basic idea

pick a training sample, lower the energy at that point
from the sample, move down in the energy surface with noise
stop after a while
push up on the energy of the point where we stopped
this creates grooves in the energy surface around data manifolds
cd can be applied to any energy function (not just rbms)

persistent cd: use a bunch of    particles    and remember their positions

make them roll down the energy surface with noise
push up on the energy wherever they are
faster than cd

rbm 

e (y ,z )=   z t wy

e (y )=   log   z ez t wy

#6. use a regularizer that limits 
the volume of space that has low energy

y lecun
ma ranzato

 sparse coding, sparse auto-encoder, predictive saprse decomposition

y lecun
ma ranzato

sparse modeling,

sparse auto-encoders,

predictive sparse decomposition

lista

how to speed up id136 in a generative model?

y lecun
ma ranzato

factor graph with an asymmetric factor
id136 z     y is easy

run z through deterministic decoder, and sample y

id136 y     z is hard, particularly if decoder function is many-to-one

map: minimize sum of two factors with respect to z
z* =  argmin_z  distance[decoder(z), y] + factorb(z)

examples: id116 (1of k), sparse coding (sparse), factor analysis

generative model
factor a

distance

decoder

factor b

input

y

z

latent
variable

sparse coding & sparse modeling

y lecun
ma ranzato

sparse linear reconstruction
energy  = reconstruction_error + code_prediction_error + code_sparsity

[olshausen & field 1997]

e (y i , z )=   y i   w d z   2+       j

   z j   

   y i       y   2

factor

w d z
deterministic
function

input y

       j .

features 

z

   z j   

variable

id136 is slow

y       z =argmin z e (y , z )

encoder architecture

y lecun
ma ranzato

examples: most ica models, product of experts

factor b

z

latent
variable

input y

fast feed-forward model

factor a'

encoder

distance

encoder-decoder architecture

y lecun
ma ranzato

[kavukcuoglu, ranzato, lecun, rejected by every conference, 2008-2009]
train a    simple    feed-forward function to predict the result of a complex 
optimization on the data points of interest

generative model
factor a

distance

decoder

factor b

input

y

fast feed-forward model

factor a'

encoder

distance

z

latent
variable

1. find optimal zi for all yi; 2. train encoder to predict zi from yi

why limit the information content of the code?

y lecun
ma ranzato

training sample

input vector which is not a training sample
feature vector

input space

feature 
space

why limit the information content of the code?

y lecun
ma ranzato

training sample

input vector which is not a training sample
feature vector

training based on minimizing the reconstruction error 
over the training set
input space

feature 
space

why limit the information content of the code?

y lecun
ma ranzato

training sample

input vector which is not a training sample
feature vector

bad: machine does not learn structure from training data!! 
it just copies the data.
input space

feature 
space

why limit the information content of the code?

y lecun
ma ranzato

training sample

input vector which is not a training sample
feature vector

idea: reduce number of available codes.

input space

feature 
space

why limit the information content of the code?

y lecun
ma ranzato

training sample

input vector which is not a training sample
feature vector

idea: reduce number of available codes.

input space

feature 
space

why limit the information content of the code?

y lecun
ma ranzato

training sample

input vector which is not a training sample
feature vector

idea: reduce number of available codes.

input space

feature 
space

predictive sparse decomposition (psd): sparse auto-encoder

y lecun
ma ranzato

[kavukcuoglu, ranzato, lecun, 2008     arxiv:1010.3467],
prediction the optimal code with a trained encoder

energy  = reconstruction_error + code_prediction_error + code_sparsity

e    y i , z    =   y i   w d z   2      z    ge   w e ,y i      2          j
ge(w e , y i)=shrinkage(w ey i)

   z j   

   y i       y   2

w d z

input y

       j .

z

   z j   

features 

ge   w e ,y i   

   z        z   2

psd: basis functions on mnist

basis functions (and encoder matrix) are digit parts

y lecun
ma ranzato

predictive sparse decomposition (psd): training

y lecun
ma ranzato

training on natural images 
patches. 
12x12
256 basis functions

learned features on natural patches: 
v1-like receptive fields

y lecun
ma ranzato

better idea: give the    right    structure to the encoder

y lecun
ma ranzato

ista/fista: iterative algorithm that converges to optimal sparse code

[gregor & lecun, icml 2010], [bronstein et al. icml 2012], [rolfe & lecun iclr 2013]

input y

w e

+

sh()

z

lateral inhibition

s

lista: train we and s matrices 
to give a good approximation quickly

y lecun
ma ranzato

think of the fista flow graph as a recurrent neural net where we and s are 
trainable parameters

input y

w e

+

z

sh()

s

time-unfold the flow graph for k iterations
learn the we and s matrices with    backprop-through-time   
get the best approximate solution within k iterations

y

w e

+

sh()

s

+

sh()

s

z

learning ista (lista) vs ista/fista

y lecun
ma ranzato

lista with partial mutual inhibition matrix

y lecun
ma ranzato

learning coordinate descent (lcod): faster than lista

y lecun
ma ranzato

discriminative recurrent sparse auto-encoder (drsae)

y lecun
ma ranzato

architecture

x

w e

encoding
filters

lateral
inhibition

decoding
filters

()+

s

+

()+

can be repeated

l1

w d
w c

  z

  x

  y

0

x

y

 rectified linear units
classification loss: cross-id178
reconstruction loss: squared error
sparsity penalty: l1 norm of last hidden layer
rows of wd and columns of we constrained in unit sphere

[rolfe & lecun iclr 2013]

drsae discovers manifold structure of handwritten digits

y lecun
ma ranzato

image = prototype + sparse sum of    parts    (to move around the manifold)

convolutional sparse coding

y lecun
ma ranzato

replace the dot products with dictionary element by convolutions.

input y is a full image
each code component zk is a feature map (an image)
each dictionary element is a convolution kernel

regular sparse coding

convolutional s.c.

y

=

.

   

k

wk

*

zk

   deconvolutional networks    [zeiler, taylor, fergus cvpr 2010]

convolutional psd: encoder with a soft sh() function 

y lecun
ma ranzato

convolutional formulation

extend sparse coding from patch to image

patch based learning

convolutional learning

convolutional sparse auto-encoder on natural images

y lecun
ma ranzato

filters and basis functions obtained with 1, 2, 4, 8, 16, 32, and 64 filters.

using psd to train a hierarchy of features

y lecun
ma ranzato

phase 1: train first layer using psd

   y i      y   2

w d z

y

      .

z

   z j   

g e(w e ,y i)

   z       z   2

features 

using psd to train a hierarchy of features

y lecun
ma ranzato

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor

y

g e(w e ,y i)

   z j   

features 

using psd to train a hierarchy of features

y lecun
ma ranzato

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd

y

g e(w e ,y i)

   y i      y    2

w d z

   z j   

y

      .

z

   z j   

g e(w e ,y i)

   z       z   2

features 

using psd to train a hierarchy of features

y lecun
ma ranzato

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd
phase 4: use encoder + absolute value as 2nd feature extractor

y

g e(w e ,y i)

   z j   

   z j   

g e(w e ,y i)

features 

using psd to train a hierarchy of features

y lecun
ma ranzato

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd
phase 4: use encoder + absolute value as 2nd feature extractor
phase 5: train a supervised classifier on top
phase 6 (optional): train the entire system with supervised back-propagation

y

g e(w e ,y i)

   z j   

   z j   

classifier

g e(w e ,y i)

features 

pedestrian detection, face detection

y lecun
ma ranzato

[osadchy,miller lecun jmlr 2007],[kavukcuoglu et al. nips 2010] [sermanet et al. cvpr 2013]

convnet architecture with multi-stage features

y lecun
ma ranzato

feature maps from all stages are pooled/subsampled and sent to the final 
classification layers

pooled low-level features: good for textures and local motifs
high-level features: good for    gestalt    and global shape

2040 9x9
filters+tanh
68 feat maps

av pooling
2x2

filter+tanh

input
78x126xyuv

7x7 filter+tanh
38 feat maps

l2 pooling
3x3

[sermanet, chintala, lecun cvpr 2013]

pedestrian detection: inria dataset. miss rate vs false 
positives

y lecun
ma ranzato

convnet
color+skip
supervised

convnet
color+skip
unsup+sup

convnet
b&w
supervised

convnet
b&w
unsup+sup

[kavukcuoglu et al. nips 2010] [sermanet et al. arxiv 2012]

results on    near scale    images (>80 pixels tall, no occlusions)

y lecun
ma ranzato

daiid113r
p=217
90

eth
p=8
04

inria
p=288

tudbrussels
p=508

results on    reasonable    images (>50 pixels tall, few occlusions)

y lecun
ma ranzato

daiid113r
p=217
90

eth
p=8
04

inria
p=288

tudbrussels
p=508

unsupervised pre-training with convolutional psd 

y lecun
ma ranzato

128 stage-1 filters on y channel. 

unsupervised training with convolutional predictive sparse decomposition

unsupervised pre-training with convolutional psd 

y lecun
ma ranzato

stage 2 filters. 

unsupervised training with convolutional predictive sparse decomposition

applying a convnet on 
sliding windows is very cheap!

y lecun
ma ranzato

output: 3x3

96x96

input:120x12
0 

 traditional detectors/classifiers must be applied to every location on 
a large input image, at multiple scales.
 convolutional nets can replicated over large images very cheaply.
 the network is applied to multiple scales spaced by 1.5.

y lecun
ma ranzato

building a detector/recognizer: 
replicated convolutional nets

 computational cost for replicated convolutional net:

96x96 -> 4.6 million multiply-accumulate operations
120x120 -> 8.3 million multiply-accumulate ops
240x240 -> 47.5 million multiply-accumulate ops
480x480 -> 232 million multiply-accumulate ops

 computational cost for a non-convolutional detector 
of the same size, applied every 12 pixels:

96x96 -> 4.6 million multiply-accumulate operations
120x120 -> 42.0 million multiply-accumulate 
operations
240x240 -> 788.0 million multiply-accumulate ops 
480x480 -> 5,083 million multiply-accumulate ops

96x96 window

12 pixel shift

84x84 overlap

y lecun
ma ranzato

y lecun
ma ranzato

musical genre recognition with psd feature

y lecun
ma ranzato

input:    constant q transform    over 46.4ms windows (1024 samples)
96 filters, with frequencies spaced every quarter tone (4 
octaves)

architecture:

input: sequence of contrast-normalized cqt vectors
1: psd features, 512 trained filters; shrinkage function 
rectification
3: pooling over 5 seconds
4: linear id166 classifier. pooling of id166 categories over 30 
seconds

    

gtzan dataset

1000 clips, 30 second each
10 genres: blues, classical, country, disco, hiphop, jazz, 
metal, pop, reggae and rock. 

results

84% correct classification

architecture: contrast norm     filters     shrink     max pooling

y lecun
ma ranzato

c
o
n
t
r
a
s
t
 
n
o
r
m
a
l
i
z
a
t
i
o
n

s
u
b
t
r
a
c
t
i
v
e
+
d

i
v
i
s
i
v
e
 

f

i
l
t
e
r
s

 

s
h
r
i

n
k
a
g
e

 

m
a
x
p
o
o
l
i

n
g

 
(
5
s
)

i

l
n
e
a
r
 
c
l
a
s
s
i
f
i
e
r

single-stage convolutional network
training of filters: psd (unsupervised)

constant q transform over 46.4 ms     contrast id172

y lecun
ma ranzato

subtractive+divisive contrast id172

convolutional psd features on time-frequency signals

y lecun
ma ranzato

octave-wide features                                     full 4-octave features

minor 3rd

perfect 4th

perfect 5th

quartal chord

major triad

transient

psd features on constant-q transform

y lecun
ma ranzato

octave-wide features 

encoder basis 
functions 

decoder basis 
functions

time-frequency features

octave-wide features on 
8 successive acoustic 
vectors 

almost no 
temporal 
structure in the 
filters!

y lecun
ma ranzato

accuracy on gtzan dataset (small, old, etc...)

y lecun
ma ranzato

accuracy: 83.4%. state of the art: 84.3%
very fast

y lecun
ma ranzato

unsupervised learning:

invariant features

learning invariant features with l2 group sparsity

y lecun
ma ranzato

unsupervised psd ignores the spatial pooling step.
could we devise a similar method that learns the pooling layer as well?
idea [hyvarinen & hoyer 2001]: group sparsity on pools of features

minimum number of pools must be non-zero
number of features that are on within a pool doesn't matter
pools tend to regroup similar features

e (y,z )=   y    w d z   2+   z    g e (w e ,y )   2+   

j        

k     p j

2
z k

   y i      y   2

w d z

input y

z

   (    z k
2)

g e(w e ,y i)

   z      z   2

l2 norm within 
each pool

      .

features 

learning invariant features with l2 group sparsity

y lecun
ma ranzato

idea: features are pooled in group. 

sparsity: sum over groups of l2 norm of activity in group.

[hyv  rinen hoyer 2001]:    subspace ica    

decoder only, square

[welling, hinton, osindero nips 2002]: pooled product of experts 

encoder only, overcomplete, log student-t penalty on l2 pooling

[kavukcuoglu, ranzato, fergus lecun, cvpr 2010]: invariant psd

encoder-decoder (like psd), overcomplete, l2 pooling

[le et al. nips 2011]: reconstruction ica

same as [kavukcuoglu 2010] with linear encoder and tied decoder 

[gregor & lecun arxiv:1006:0448,  2010] [le et al. icml 2012]

locally-connect non shared (tiled) encoder-decoder

input

y

encoder only (poe, ica),

decoder only or

encoder-decoder (ipsd, rica)

simple 
features 

l2 norm within 
each pool

z

   (    z k
2)

      .

invariant
features 

groups are local in a 2d topographic map

y lecun
ma ranzato

the filters arrange 
themselves spontaneously 
so that similar filters enter 
the same pool.
the pooling units can be 
seen as complex cells
outputs of pooling units are 
invariant to local 
transformations of the input

for some it's 
translations, for others 
rotations, or other 
transformations.

image-level training, local filters but no weight sharing

y lecun
ma ranzato

training on 115x115 images. kernels are 15x15 (not shared across 
space!)

decoder

[gregor & lecun 2010]
local receptive fields
no shared weights
4x overcomplete
l2 pooling
group sparsity over pools

reconstructed input

(inferred) code

predicted code

input

encoder

image-level training, local filters but no weight sharing

y lecun
ma ranzato

training on 115x115 images. kernels are 15x15 (not shared across 
space!)

topographic maps

k obermayer and gg blasdel, journal of 
neuroscience, vol 13, 4114-4129 (monkey)

y lecun
ma ranzato

119x119 image input

100x100 code

20x20 receptive field size

sigma=5

michael c. crair, et. al. the journal of neurophysiology 
vol. 77 no. 6 june 1997, pp. 3381-3385 (cat)

image-level training, local filters but no weight sharing

y lecun
ma ranzato

color indicates orientation (by fitting gabors)

invariant features lateral inhibition

y lecun
ma ranzato

replace the l1 sparsity term by a lateral inhibition matrix
easy way to impose some structure on the sparsity 

[gregor, szlam, lecun nips 2011]

invariant features via lateral inhibition: structured sparsity

y lecun
ma ranzato

 each edge in the tree indicates a zero in the s matrix (no mutual inhibition)
sij is larger if two neurons are far away in the tree

invariant features via lateral inhibition: topographic maps

y lecun
ma ranzato

non-zero values in s form a ring in a 2d topology

input patches are high-pass filtered

invariant features through temporal constancy 

y lecun
ma ranzato

object is cross-product of object type and instantiation parameters

mapping units [hinton 1981], capsules [hinton 2011]

object type

[karol gregor et al.]

small

medium
object size

large

what-where auto-encoder architecture

y lecun
ma ranzato

decoder

st

st-1

st-2

predicted

input

w1

w1

w1

w2

t

c1

t

c1

t-1

c1

t-1

c1

t-2

c1

t-2

c1

f       w 1

f       w 1

f       w 1

encoder

st

st-1

t

c2

t

c2

f

inferred 

code

predicted

code

  w 2
  w 2
st-2

  w 2

input

low-level filters connected to each complex cell

y lecun
ma ranzato

c1
(where)

c2
(what)

generating images

generating images

input

y lecun
ma ranzato

y lecun
ma ranzato

future

challenges

the graph of deep learning 

   

 sparse modeling 

   

 neuroscience

y lecun
ma ranzato

basis/matching pursuit
[mallat 93; donoho 94]

stochastic optimization
[nesterov, bottou
nemirovski,....]

architecture of v1
[hubel, wiesel 62]

sparse modeling
[olshausen-field 97]

backprop
[many 85]

neocognitron
[fukushima 82]

sparse modeling
[bach, sapiro. elad]

convolutional net
[lecun 89]

scattering
transform
[mallat 10]

sparse auto-encoder
[lecun 06; ng 07]

object reco
[lecun 10]

id172
[simoncelli 94]

visual metamers
[simoncelli 12]

mcmc, hmc
cont. div.
[neal, hinton]

compr. sensing
[cand  s-tao 04]

l2-l1 optim
[nesterov,
nemirovski
daubechies,
osher....]

restricted 
boltzmann
machine 
[hinton 05]

id103
[goog, ibm, msft 12]

object recog
[hinton 12]

scene labeling
[lecun 12]

connectomics
[seung 10]

integrating feed-forward and feedback

y lecun
ma ranzato

marrying feed-forward convolutional nets with 
generative    deconvolutional nets   

deconvolutional networks

[zeiler-graham-fergus iccv 2011]

feed-forward/feedback networks allow 
reconstruction, multimodal prediction, restoration, 
etc...

deep id82s can do this, but 
there are scalability issues with training

trainable feature

transform

trainable feature

transform

trainable feature

transform

trainable feature

transform

integrating deep learning and id170

y lecun
ma ranzato

deep learning systems can be assembled into 
factor graphs

energy function is a sum of factors
factors can embed whole deep learning 
systems
x: observed variables (inputs)
z: never observed (latent variables)
y: observed on training set (output 
variables)

id136 is energy minimization (map) or free 
energy minimization (marginalization) over z 
and y given an x

e(x,y,z)

energy model
(factor graph)

z 

(unobserved)

x 

(observed)

y

(observed on
training set)

integrating deep learning and id170

y lecun
ma ranzato

deep learning systems can be assembled into 
factor graphs

energy function is a sum of factors
factors can embed whole deep learning 
systems
x: observed variables (inputs)
z: never observed (latent variables)
y: observed on training set (output 
variables)

id136 is energy minimization (map) or free 
energy minimization (marginalization) over z 
and y given an x

f(x,y) = min_z e(x,y,z)
f(x,y) = -log sum_z exp[-e(x,y,z) ]

f(x,y) = marg_z e(x,y,z)

e(x,y,z)

energy model
energy model
(factor graph)
(factor graph)

z 

(unobserved)

x 

(observed)

y

(observed on
training set)

integrating deep learning and id170

y lecun
ma ranzato

integrting deep learning and structured 
prediction is a very old idea

in fact, it predates structured 
prediction

globally-trained convolutional-net + 
id114 

trained discriminatively at the word 
level
loss identical to crf and structured 
id88
compositional movable parts model

a system like this was reading 10 to 20% 
of all the checks in the us around 1998

integrating deep learning and id170

y lecun
ma ranzato

deep learning systems can be assembled into 
factor graphs

energy function is a sum of factors
factors can embed whole deep learning 
systems
x: observed variables (inputs)
z: never observed (latent variables)
y: observed on training set (output 
variables)

id136 is energy minimization (map) or free 
energy minimization (marginalization) over z 
and y given an x

f(x,y) = min_z e(x,y,z)
f(x,y) = -log sum_z exp[-e(x,y,z) ]

f(x,y) = marg_z e(x,y,z)

e(x,y,z)

energy model
energy model
(factor graph)
(factor graph)

z 

(unobserved)

x 

(observed)

y

(observed on
training set)

future challenges

y lecun
ma ranzato

integrated feed-forward and feedback

deep id82 do this, but there are issues of scalability.

integrating supervised and unsupervised learning in a single algorithm

again, deep id82s do this, but....

integrating deep learning and id170 (   reasoning   )

this has been around since the 1990's but needs to be revived

learning representations for complex reasoning

   recursive    networks that operate on vector space representations 
of knowledge [pollack 90's] [bottou 2010] [socher, manning, ng 
2011]

representation learning in natural language processing

[y. bengio 01],[collobert weston 10], [mnih hinton 11] [socher 12]

better theoretical understanding of deep learning and convolutional nets
e.g. stephane mallat's    scattering transform   , work on the sparse 
representations from the applied math community....

y lecun
ma ranzato

software

torch7: learning library that supports neural net training
    http://www.torch.ch
    http://code.cogbits.com/wiki/doku.php  (tutorial with demos by c. farabet)
- http://eblearn.sf.net (c++ library with convnet support by p. sermanet)
python-based learning library  (u. montreal) 
- http://deeplearning.net/software/theano/  (does automatic differentiation)
id56
    www.fit.vutbr.cz/~imikolov/id56lm (id38)
    http://sourceforge.net/apps/mediawiki/id56l/index.php  (lstm)
cudamat & gnumpy
    code.google.com/p/cudamat
    www.cs.toronto.edu/~tijmen/gnumpy.html
misc
    www.deeplearning.net//software_links

references

y lecun
ma ranzato

convolutional nets
    lecun, bottou, bengio and haffner: gradient-based learning applied to document 
recognition, proceedings of the ieee, 86(11):2278-2324, november 1998
- krizhevsky, sutskever, hinton    id163 classification with deep convolutional neural 
networks    nips 2012
    jarrett, kavukcuoglu, ranzato, lecun: what is the best multi-stage architecture for 
object recognition?, proc. international conference on id161 (iccv'09), 
ieee, 2009
- kavukcuoglu, sermanet, boureau, gregor, mathieu, lecun: learning convolutional 
feature hierachies for visual recognition, advances in neural information processing 
systems (nips 2010), 23, 2010
    see  yann.lecun.com/exdb/publis  for references on many different kinds of convnets.
    see http://www.cmap.polytechnique.fr/scattering/ for scattering networks (similar to 
convnets but with less learning and stronger mathematical foundations)

references

y lecun
ma ranzato

applications of convolutional nets
    farabet, couprie, najman, lecun,    scene parsing with multiscale id171, 
purity trees, and optimal covers   , icml 2012
    pierre sermanet, koray kavukcuoglu, soumith chintala and yann lecun: pedestrian 
detection with unsupervised multi-stage id171, cvpr 2013
- d. ciresan, a. giusti, l. gambardella, j. schmidhuber. deep neural networks 
segment neuronal membranes in electron microscopy images. nips 2012
- raia hadsell, pierre sermanet, marco scoffier, ayse erkan, koray kavackuoglu, urs 
muller and yann lecun: learning long-range vision for autonomous off-road driving, 
journal of field robotics, 26(2):120-144, february 2009
    burger, schuler, harmeling: image denoisng: can plain neural networks compete 
with bm3d?, id161 and pattern recognition, cvpr 2012,

references

y lecun
ma ranzato

applications of id56s
    mikolov    statistical language models based on neural networks    phd thesis 2012
    boden    a guide to id56s and id26    tech report 2002
    hochreiter, schmidhuber    long short term memory    neural computation 1997
    graves    offline arabic handwrting recognition with multidimensional neural networks    
springer 2012
    graves    id103 with deep recurrent neural networks    icassp 2013

references

y lecun
ma ranzato

deep learning & energy-based models
    y. bengio, learning deep architectures for ai, foundations and trends in machine 
learning, 2(1), pp.1-127, 2009.
    lecun, chopra, hadsell, ranzato, huang: a tutorial on energy-based learning, in 
bakir, g. and hofman, t. and sch  lkopf, b. and smola, a. and taskar, b. (eds), 
predicting structured data, mit press, 2006
    m. ranzato ph.d. thesis    unsupervised learning of feature hierarchies    nyu 2009

practical guide
    y. lecun et al. efficient backprop, neural networks: tricks of the trade, 1998
    l. bottou, stochastic id119 tricks, neural networks, tricks of the trade 
reloaded, lncs 2012.
    y. bengio, practical recommendations for gradient-based training of deep 
architectures, arxiv 2012

