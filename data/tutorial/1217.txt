id4

thang luong
kyunghyun cho

christopher manning

@lmthang    @kchonyc    @chrmanning

acl 2016 tutorial     https://sites.google.com/site/acl16id4/

1a. intro to (neural) machine translation
ideas connecting phrase-based statistical  mt and id4
neural language models

2

machine translation

the classic test of language understanding!

both language analysis & generation

big mt needs     for humanity     and commerce

translation is a us$40 billion a year industry

huge in europe, growing in asia
large social/government/military 
as well as commercial needs

3

the need for machine translation

huge commercial use

google translates over 100 billion words a day
facebook has just rolled out new homegrown mt
   when we turned [mt] off for some people, they 
went nuts!   

ebay uses mt to enable cross-border trade

http://www.commonsenseadvisory.com/abstractview.aspx?articleid=36540
https://googleblog.blogspot.com/2016/04/ten-years-of-google-translate.html
https://techcrunch.com/2016/05/23/facebook-translation/

4

scenarios for machine translation

1. the dream of fully automatic high-quality mt 

(fahqmt)

this still seems a distant goal

2. user- or platform-initiated low quality 

translation

the current mainstay of mt

google translate
bing translator

5

scenarios for machine translation

3. author-initiated high quality translation

mt with human post-editing or mt as a translation 
aid is clearly growing ... but remains painful
great opportunities for a much brighter future where 
mt assists humans: e.g., matecat or lilt

https://lilt.com/
talk in sess 1c!

6

7

8

iwslt 2015, ted talk mt, english-german

id7 (cased)

30.85

26.18

26.02

24.96

22.51

20.08

human evaluation 

(hter )

28.18

21.84

22.67

23.42

26
%

16.16

30
25
20
15
10
5
0

35
30
25
20
15
10
5
0

9

progress in machine translation

[edinburgh en-de wmt newstest2013 cased id7; id4 2015 from u. montr  al]

phrase-based smt

syntax-based smt

neural mt

25

20

15

10

5

0

2013

2014

2015

2016

from [sennrich 2016, http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf] 

phrase-based statistical machine 
translation
a marvelous use of big data but     it   s mined out?!?

1519   600                                                      
                                                                     
in 1519, six hundred spaniards landed in mexico to conquer the aztec empire with a 
population of a few million. they lost two thirds of their soldiers in the first clash.

translate.google.com (2009): 1519 600 spaniards landed in mexico, millions of people to 
conquer the aztec empire, the first two-thirds of soldiers against their loss.
translate.google.com (2013): 1519 600 spaniards landed in mexico to conquer the aztec 
empire, hundreds of millions of people, the initial confrontation loss of soldiers two-thirds.
translate.google.com (2014): 1519 600 spaniards landed in mexico, millions of people to 
conquer the aztec empire, the first two-thirds of the loss of soldiers they clash.
translate.google.com (2015): 1519 600 spaniards landed in mexico, millions of people to 
conquer the aztec empire, the first two-thirds of the loss of soldiers they clash.
translate.google.com (2016): 1519 600 spaniards landed in mexico, millions of people to 

conquer the aztec empire, the first two-thirds of the loss of soldiers they clash. }

neural mt is good   !

12

neural mt went from a fringe 
research activity in 2014 to the 
widely-adopted leading way to 

do mt in 2016.

amazing   !

13

what is neural mt (id4)?

id4 is the 
approach of modeling the entire mt 
process via one big artificial neural 
network*

*but sometimes we compromise this goal a little

14

neural encoder-decoder architectures

input
text

encoder

   0.2
   0.1
0.1
0.4
   0.3
1.1

decoder

translated
text

15

id4 system for translating a single word

16

id4 system for translating a single word

17

id4 system for translating a single word

18

softmax function: standard map
from    v to a id203 distribution

exponentiate to
make positive

normalize to
give id203

19

neural mt: the bronze age

[allen 1987 ieee 1st iid98]
3310 en-es pairs constructed on 31 
en, 40 es words, max 10/11 word 
sentence; 33 used as test set
the grandfather offered the little girl a book    
el abuelo le ofrecio un libro a la nina pequena
binary encoding of words     50 
inputs, 66 outputs; 1 or 3 hidden 
150-unit layers. ave wer: 1.3 words
20

neural mt: the bronze age

[chrisman 1992 connection science]
dual-ported raam architecture 
[pollack 1990 artificial intelligence] 
applied to corpus of 216 parallel 
pairs of simple en-es sentences:

you are not angry    usted no esta furioso

split 50/50 as train/test, 75% of 
sentences correctly translated!

21

coincidence?

22

modern sequence models for id4
[sutskever et al. 2014, bahdanau et al. 2014, et seq.] 
following [jordan 1986] and more closely [elman 1990]

the      protests  escalated    over         the      weekend   <eos>

translation 
generated

0.1
0.3
0.1
-0.4
0.2

0.2
-0.2
-0.1
0.1
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.4
-0.6
0.2
-0.3
0.4

0.4
0.4
0.3
-0.2
-0.3

0.1
0.3
-0.1
-0.7
0.1

0.2
-0.3
-0.1
-0.4
0.2

0.5
0.5
0.9
-0.3
-0.2

0.2
0.6
-0.1
-0.4
0.1

0.2
0.4
0.1
-0.5
-0.2

0.2
0.6
-0.1
-0.5
0.1

0.2
-0.8
-0.1
-0.5
0.1

0.4
-0.2
-0.3
-0.4
-0.2

-0.1
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
-0.1
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

-0.4
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.3
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
0.3
0.1

0.2
0.6
-0.1
-0.7
0.1

0.4
0.4
-0.1
-0.7
0.1

-0.1
0.6
-0.1
0.3
0.1

-0.1
0.3
-0.1
-0.7
0.1

-0.2
0.6
-0.1
-0.7
0.1

0.2
0.4
-0.1
0.2
0.1

-0.2
0.6
0.1
0.3
0.1

-0.4
0.6
-0.1
-0.7
0.1

0.3
0.6
-0.1
-0.5
0.1

-0.4
0.5
-0.5
0.4
0.1

-0.3
0.5
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

die      proteste    waren am  wochenende eskaliert <eos>   the      protests   escalated   over        the     weekend

feeding in 
last word

a deep recurrent neural network

sentence 
meaning 
is built up

source 
sentence

the three big wins of neural mt

1. end-to-end training

all parameters are simultaneously optimized to 
minimize a id168 on the network   s output 

2. distributed representations share strength
better exploitation of  word and phrase similarities

3. better exploitation of context

id4 can use a much bigger context     both source 
and partial target text     to translate more accurately

24

what wasn   t on that list?
1. explicit use of syntactic or 

semantic structures

2. explicit use of discourse 
structure, anaphora, etc.

3. black box component models for 
reordering, id68, etc.

25

the current baseline and its enduring ideas
1b. ideas connecting phrase-based 
statistical  mt and id4

26

t
i
a
n
e
t
r
a
p
p
a

x
u
a

s
e
n
o
t
h
c
o
t
u
a

word alignments
phrase-based smt aligned words in a 
preprocessing-step, usually using em

the
balance
was
the
territory
of
the
aboriginal
people

le
reste

appartenait

aux

autochtones

    models of attention
[bahdanau et al. 2014; iclr 2015]
part 3b later

e
t
s
e
r

e
l

the
balance
was
the
territory
of
the
aboriginal
people

constraints on    distortion    
(displacement) and fertility
smt: alignment id203 depends on positions 
of the words, and position relative to neighbors

the likelihood of an alignment
depends on how many words
align to a certain position

constraints on    distortion    
(displacement) and fertility
    constraints on attention [cohn, hoang, vymolova, yao, 
dyer & haffari naacl 2016; feng, liu, li, zhou 2016 arxiv; 
yang, hu, deng, dyer, smola 2016 arxiv]. 

automatic evaluation method for learning 
before usually id7;  id4     usually 
differentiable lm score, i.e., predict each word

reference translation 1:
reference translation 1:

the u.s. island of guam is maintaining 
the u.s. island of guam is maintaining 
a high state of alert after the guam 
a high state of alert after the guam 
airport and its offices both received an 
airport and its offices both received an 
e-mail from someone calling himself 
e-mail from someone calling himself 
the saudi arabian osama bin laden 
the saudi arabian osama bin laden 
and threatening a biological/chemical 
and threatening a biological/chemical 
attack against public places such as 
attack against public places such as 
the airport .
the airport .

id7 score against 
4 reference 
translations

reference translation 2:
reference translation 2:

guam international airport and its 
guam international airport and its
offices are maintaining a high state of 
offices are maintaining a high state of 
alert after receiving an e-mail that was 
alert after receiving an e-mail that was 
from a person claiming to be the 
from a person claiming to be the 
wealthy saudi arabian businessman 
wealthy saudi arabian businessman 
bin laden and that threatened to 
bin laden and that threatened to 
launch a biological and chemical attack 
launch a biological and chemical attack 
on the airport and other public places .
on the airport and other public places .

machine translation:
machine translation:

the american [?] international airport 
the american [?] international airport 
and its the office all receives one calls 
and its the office all receives one calls 
self the sand arab rich business [?] 
self the sand arab rich business [?] 
and so on electronic mail , which 
and so on electronic mail , which
sends out ;  the threat will be able 
sends out ;  the threat will be able 
after public place and so on the 
after public place and so on the
airport to start the biochemistry attack 
airport to start the biochemistry attack
, [?] highly alerts after the 
, [?] highly alerts after the
maintenance.
maintenance.

[papineni et al. 2002]

reference translation 4:
reference translation 4:

us guam international airport and its 
us guam international airport and its 
office received an email from mr. bin 
office received an email from mr. bin 
laden and other rich businessman 
laden and other rich businessman 
from saudi arabia . they said there 
from saudi arabia . they said there 
would be biochemistry air raid to guam 
would be biochemistry air raid to guam 
airport and other public places . guam 
airport and other public places . guam 
needs to be in high precaution about 
needs to be in high precaution about 
this matter . 
this matter . 

reference translation 3:
reference translation 3:

the us international airport of guam 
the us international airport of guam 
and its office has received an email 
and its office has received an email 
from a self-claimed arabian millionaire 
from a self-claimed arabian millionaire 
named laden , which threatens to 
named laden , which threatens to 
launch a biochemical attack on such 
launch a biochemical attack on such 
public places as airport . guam 
public places as airport . guam 
authority has been on alert . 
authority has been on alert . 

phrase-based statistical mt:
pharaoh/moses
ich

morgen fliege

nach kanada

[koehn et al, 2003]

zur konferenz

tomorrow i

will fly

to the conference in canada

source input segmented into phrases

       phrase    is a subsequence of words     not linguistic phrase

    do we need phrases in id4?

or not, as have in-context word translation?
cf. [kalchbrenner & blunsom 2013] source id98 and 
[eriguchi, hashimoto & tsuruoka 2016] source tree

smt phrase table weights gave
a context-independent translation score
each phrase is probabilistically translated

    p(in spite |       )      
    p(in spite of the fact |       )

       ||| development ||| (0) ||| (0) ||| -2.97 -2.72 -0.86 -0.95
       ||| development of ||| (0) ||| (0) () ||| -3.41 -2.72 -3.22 -3.50
              ||| that carries out a supervisory ||| (1,2,3) (4) ||| () (0) (0) (0) (1) ||| 0.0 -3.68 -7.27 -21.24
              ||| carries out a supervisory ||| (0,1,2) (3) ||| (0) (0) (0) (1) ||| 0.0 -3.68 -7.27 -17.17
       ||| supervisory ||| (0) ||| (0) ||| -1.03 -0.80 -3.68 -3.24
              ||| supervisory inspection ||| (0) (1) ||| (0) (1) ||| 0.0 -2.33 -6.07 -4.85
       ||| inspection ||| (0) ||| (0) ||| -1.54 -1.53 -2.05 -1.60
       ||| in spite ||| (1) ||| () (0) ||| -0.90 -0.50 -3.56 -6.14
       ||| in spite of ||| (1) ||| () (0) () ||| -1.11 -0.50 -3.93 -8.68
       ||| in spite of the ||| (1) ||| () (0) () () ||| -1.06 -0.50 -4.77 -10.50
       ||| in spite of the fact ||| (1) ||| () (0) () () () ||| -1.18 -0.50 -6.54 -18.19

phrase-based smt:
log-linear feature-based mt models
   = argmaxe 1.9  log p(e) + 1.0  log p(f | e) + 1.1  

log length(e) +    

=  argmaxe   i wi  i
we have two things:

       features      , such as log translation model score
    weights w for each feature for how good it is 

the weights were learned
feature scores from separately trained models

language models (lm)
a language model     p(e)     gives the 
id203 of a sequence of words
most important feature! why not 
just do more with language models?
e.g., generate a translation with lm 
also conditioned on source     use nlm

34

mt decoder: id125
3rd target
word

2nd target
word

1st target
word

coverage set:
what has been 
translated

start

decoder explores 
multiple source 
positions
should one 
sample attention?

4th target
word

end

all source
words
covered

[jelinek, 1969; 
brown et al, 1996 
us patent; och, 
ueffing, and ney, 
2001]

    id4 uses a similar beam decoder.
it can be simpler, because contextual
conditioning is much better: a beam of ~8 is sufficient.
work modeling coverage: [tu, lu, liu, liu, li, acl 2016]

an id4 system is an nlm with extra conditioning!
1c. neural language models

36

language models: sentence probabilities

p(x1, x2, . . . , xt ) =

tyt=1

p(xt|x1, . . . , xt 1)
[chain rule]

the

crude

force

   

science

there are way too many histories once you   re 
into a sentence a few words! exponentially many.

37

traditional fix: markov assumption

an nth order markov assumption assumes each 
word depends only on a short linear history 

p(x1, x2, . . . , xt ) =

   

38

tyt=1
tyt=1

p(xt|x1, . . . , xt 1)

p(xt|xt n, . . . , xt 1)

2016-08-07

problems of traditional markov 
model assumptions (1): sparsity
issue: very small window gives bad prediction; 
statistics for even a modest window are sparse
example:

p(w0|w   3, w   2, w   1)   |v| = 100,000    1015 contexts

most have not been seen
the traditional answer is to use various backoff
and smoothing techniques, but no good solution

39

neural language models
the neural approach [bengio, ducharme, 
vincent & jauvin jmlr 2003] represents 
words as dense distributed vectors so 
there can be sharing of statistical 
weight between similar words
doing just this solves the sparseness 
problem of conventional id165 models

neural (probabilistic) language model 
[bengio, ducharme, vincent & jauvin jmlr 2003]

41

neural (probabilistic) language model 
[bengio, ducharme, vincent & jauvin jmlr 2003]

42

neural (probabilistic) language model 
[bengio, ducharme, vincent & jauvin jmlr 2003]

43

problems of traditional markov 
model assumptions (2): context
issue: dependency beyond the window is ignored
example:
the same stump which had impaled the car of 
many a guest in the past thirty years and which he 
refused to have removed

44

a non-markovian language model

can we directly model the true id155?

tyt=1

p(x1, x2, . . . , xt ) =

p(xt|x1, . . . , xt 1)

can we build a neural language model for this?
1. feature extraction:
2. prediction:
how can f  take a variable-length input?

ht = f (x1, x2, . . . , xt)
p(xt+1|x1, . . . , xt 1) = g(ht)

45

a non-markovian language model

can we directly model the true id155?

tyt=1

p(xt|x1, . . . , xt 1)
h

p(x1, x2, . . . , xt ) =

initialization

recursive construction of f
h0 = 0
1.
ht = f (xt, ht 1)

2. recursion

ht

we call        a hidden state or memory
ht

summarizes the history

(x1, . . . , xt)

46

2016-08-07

f

xt

a non-markovian language model

p(the, cat, is, eating)

example:  
(1) initialization: 
(2) recursion with prediction:

h0 = 0

h1 = f (h0,hbosi) ! p(the) = g(h1)
h2 = f (h1, cat) ! p(cat|the) = g(h2)
h3 = f (h2, is) ! p(is|the, cat) = g(h3)
h4 = f (h3, eating) ! p(eating|the, cat, is) = g(h4)

(3) combination:

p(the, cat, is, eating) = g(h1)g(h2)g(h3)g(h4)

47

read, update and predict

2016-08-07

a recurrent neural network language 
model solves the second problem!
example:  p(the, cat, is, eating)

read, update and predict

48

building a recurrent language model
transition function
inputs

ht = f (ht 1, xt)

i. current word 
ii. previous state

xt 2 {1, 2, . . . ,|v |}
ht 1 2 rd

parameters

input weight matrix

i.
ii. transition weight matrix
iii. bias vector

w 2 r|v |   d
u 2 rd   d

b 2 rd

49

building a recurrent language model
transition function
na  ve transition function

ht = f (ht 1, xt)

f (ht 1, xt) = tanh(w [xt] + u ht 1 + b)

element-wise nonlinear 
transformation

trainable word vector

linear transformation of
previous state

50

building a recurrent language model

prediction function p(xt+1 = w|x   t) = gw(ht)
inputs

i. current state

parameters

softmax matrix

i.
ii. bias vector

ht 2 rd
r 2 r|v |   d

c 2 r|v |

51

building a recurrent language model

prediction function p(xt+1 = w|x   t) = gw(ht)
p(xt+1 = w|x   t) = gw(ht) =

exp(r [w]> ht + cw)
i=1 exp(r [i]> ht + ci)

p|v |

normalize

compatibility between 
trainable word vector 
and hidden state

exponentiate

52

training a recurrent language model

having determined the model form, we:
1.

initialize all parameters of the models, including the 
word representations with small random numbers
2. define a id168: how badly we predict actual 

next words [log loss or cross-id178 loss]

3. repeatedly attempt to predict each next word
4. backpropagate our loss to update all parameters
5. just doing this learns good word representations 
and good prediction functions     it   s almost magic

53

neural language models as mt 
components
you can just replace the target-side language model of a 
conventional phrase-based smt system with an nlm

nlm / continuous space language models
[schwenk, costa-juss   & fonollosa 2006; schwenk 2007; auli & gao 2013; 
vaswani, zhao, fossum & chiang 2013] 

you can use the source as well as target words to 
predict next target word, usually using phrase alignment

neural joint language models
[auli, galley, quirk & zweig 2013; devlin, zbib, huang, lamar, schwartz & 
makhool 2014]

54

however,

we want to move on to the 
goal of an end-to-end trained 

neural translation model   !

55

recurrent language model

example)  p(the, cat, is, eating)

read, update and predict

56

2016-08-07

2a. training a recurrent language model 
id113 with stochastic gradient 
descent and id26 through time

57

training a recurrent language model

   

   
   

log-id203 of one training sentence

log p(xn

1 , xn

2 , . . . , xn

t n) =

log p(xn

1 , . . . , xn

t |xn

t 1)

t nxt=1

training set
log-likelihood functional

d = x 1, x 2, . . . , x n 
t nxt=1
nxn=1
 l(   , d)

log p(xn

t |xn

1
n

1 , . . . , xn

l(   , d) =

minimize             !! 

t 1)

58

2016-08-07

id119

    move slowly in the steepest descent direction

               rl(   , d)

    computational cost of a single update: 
    not suitable for a large corpus

o(n )

59

2016-08-07

stochastic id119

   

estimate the steepest direction with a minibatch

rl(   , d)     rl(   , x 1, . . . , x n )

    until the convergence (w.r.t. a validation set)
|l(   , dval)   l(         l(   , d), dval)|        

60

2016-08-07

stochastic id119
    not trivial to build a minibatch

sentence 1

sentence 2

sentence 3

sentence 4

1. padding and masking: suitable for gpu   s, but wasteful

    wasted computation
sentence 1

sentence 2

0   s

sentence 3

sentence 4

0   s

0   s

61

2016-08-07

stochastic id119

1. padding and masking: suitable for gpu   s, but wasteful

    wasted computation

sentence 1

sentence 2

0   s

sentence 3

sentence 4

0   s

0   s

2. smarter padding and masking: minimize the waste

   
   

62

ensure that the length differences are minimal.
sort the sentences and sequentially build a minibatch

sentence 1
sentence 2

sentence 3

sentence 4

0   s
0   s

0   s
2016-08-07

id26 through time

how do we compute               ? 
    cost as a sum of per-sample cost function

rl(   , d)

per-sample cost as a sum of per-step cost functions

rl(   , d) = xx2d
txt=1

rl(   , x) =

rl(   , x)

r log p(xt|x<t,    )

log p(xt|x<t)

2016-08-07

   

63

id26 through time
how do we compute                         ?
r log p(xt|x<t,    )
    compute per-step cost function from time 

t = t

1. cost derivative
@ log p(xt|x<t)/@g
2. gradient w.r.t.      :
r    @g/@r
ht    @g/@ht + @ht+1/@ht
3. gradient w.r.t.      :
4. gradient w.r.t.      :
u    @ht/@u
5. gradient w.r.t.     and       :
w
b
   @ht/@w

6. accumulate the gradient and 

   @ht/@b

and

t   t   1

log p(xt|x<t)

64

2016-08-07

id26 through time

intuitively, what   s happening here?
1. measure the influence of the past on the future

@ log p(xt+n|x<t+n)

@ log p(xt+n|x<t+n)

=

@ht

@ht+n 1       
p(xt+n|x<t+n)
2. how does the perturbation at   affect                     ?

@ht+n

@g

t

@g

@ht+n

@ht+1
@ht

?

   

xt

65

2016-08-07

id26 through time
intuitively, what   s happening here?

1. measure the influence of the past on the future

@ log p(xt+n|x<t+n)

@ log p(xt+n|x<t+n)

=

@ht

@ht+n 1       
p(xt+n|x<t+n)
2. how does the perturbation at   affect                     ?

@ht+n

@g

t

@g

@ht+n

@ht+1
@ht

?

   

xt

3. change the parameters to maximize 

66

p(xt+n|x<t+n)

2016-08-07

id26 through time
intuitively, what   s happening here?

1. measure the influence of the past on the future

@ log p(xt+n|x<t+n)

@ht

=

@ log p(xt+n|x<t+n)

@g

@g

@ht+n

@ht+n

@ht+n 1       

@ht+1
@ht

2. with a na  ve transition function

f(ht 1, xt 1) = tanh(w [xt 1] + u ht 1 + b)

we get

@jt+n
@ht

=

@jt+n

@g

@g

@ht+n

nyn=1
|

u>diag    @ tanh(at+n)

@at+n

problematic!

{z

   
}

67

[bengio, ieee 1994]

2016-08-07

id26 through time
gradient either vanishes or explodes

    what happens?

@jt+n
@ht

=

@jt+n

@g

@g

@ht+n

nyn=1
|

u>diag    @ tanh(at+n)

@at+n

{z

   
}

1. the gradient likely explodes if 

1

emax  

max tanh0(x)

= 1

2. the gradient likely vanishes if 

emax <

1

max tanh0(x)

= 1

, where

emax

: largest eigenvalue of 

u

68

[bengio, simard, frasconi, tnn1994; 
hochreiter, bengio, frasconi, schmidhuber, 2001]

2016-08-07

id26 through time

addressing exploding gradient

   

   when gradients explode so does the curvature 
along v, leading to a wall in the error surface   

on the di culty of training recurrent neural networks

    gradient clipping
1. norm clipping

  r      c

krkr ,if krk   c
r
,otherwise

2. element-wise clipping

ri   min(c,|ri|)sgn(ri), for all i 2 {1, . . . , dimr}

figure 6. we plot the error surface of a single hidden unit
recurrent network, highlighting the existence of high cur-
vature walls. the solid lines depicts standard trajectories
that id119 might follow. using dashed arrow
the diagram shows what would happen if the gradients is
rescaled to a    xed size when its norm is above a threshold.

2016-08-07

[pascanu, mikolov, bengio, icml 2013]

69

id26 through time
vanishing gradient is super-problematic

    when we only observe

    

@ht+n

@ht      =     

nyn=1

    we cannot tell whether

u>diag    @ tanh(at+n)

@at+n

,

         ! 0

1. no dependency between t and t+n in data, or
2. wrong configuration of parameters: 

emax(u ) <

1

max tanh0(x)

70

2016-08-07

2b. id149
vanishing gradient, id149 and long short-
term memory units

71

gated recurrent unit

   

is the problem with the na  ve transition function?
f (ht 1, xt) = tanh(w [xt] + u ht 1 + b)

    with it, the temporal derivative is 
= u> @ tanh(a)

@ht+1
@ht

@a

   

72

it implies that the error must be backpropagated
through all the intermediate nodes:

2016-08-07

gated recurrent unit

   

   

73

it implies that the error must backpropagate through 
all the intermediate nodes:

perhaps we can create shortcut connections.

2016-08-07

gated recurrent unit

   

perhaps we can create adaptive shortcut connections.

    candidate update 
    update gate 

f (ht 1, xt) = ut     ht + (1 + ut)   ht 1
  ht = tanh(w [xt] + u ht 1 + b)

ut =  (wu [xt] + uuht 1 + bu)

74

2016-08-07

 : element-wise multiplication

gated recurrent unit

   

let the net prune unnecessary connections adaptively.

    candidate update 
   
    update gate  

reset gate

f (ht 1, xt) = ut     ht + (1 + ut)   ht 1

  ht = tanh(w [xt] + u (rt   ht 1) + b)

rt =  (wr [xt] + urht 1 + br)
ut =  (wu [xt] + uuht 1 + bu)

75

2016-08-07

gated recurrent unit
tanh-id56    .

registers

h

execution

h
1. read the whole register
2. update the whole register 

h   tanh(w [x] + u h + b)

76

2016-08-07

gated recurrent unit
gru    

registers

h

execution

r

r   h

1. select a readable subset
2. read the subset
3. select a writable subset u
4. update the subset
h   u     h + (1   ut)   h

clearly id149 are much more realistic.

77

2016-08-07

gated recurrent unit

two most widely used id149
gated recurrent unit
[cho et al., emnlp2014; 
chung, gulcehre, cho, bengio, dlufl2014]
ht = ut     ht + (1   ut)   ht 1
  h = tanh(w [xt] + u (rt   ht 1) + b)
ut =  (wu [xt] + uuht 1 + bu)
rt =  (wr [xt] + urht 1 + br)

long short-term memory 
[hochreiter&schmidhuber, nc1999; 
gers, thesis2001]
ht = ot   tanh(ct)
ct = ft   ct 1 + it     ct
  ct = tanh(wc [xt] + ucht 1 + bc)
ot =  (wo [xt] + uoht 1 + bo)
it =  (wi [xt] + uiht 1 + bi)
ft =  (wf [xt] + uf ht 1 + bf )

78

training an id56
a few well-established + my personal wisdoms

1. use lstm or gru: makes your life so much simpler

2.

3.

initialize recurrent matrices to be orthogonal

initialize other matrices with a sensible scale

4. use adaptive learning rate algorithms: adam, adadelta,    

5. clip the norm of the gradient:    1    seems to be a reasonable 

threshold when used together with adam or adadelta.

6. be patient!

79

[saxe et al., iclr2014; 
ba, kingma, iclr2015; 
zeiler, arxiv2012; 
pascanu et al., icml2013]

2016-08-07

now, go build and train a 
recurrent language model!

any questions?

80

8/7/16

2c. conditional recurrent language model
encoder-decoder network for machine translation

81

recurrent language model can

1. score a given sentence very well

log p(the, cat, is, sitting, on, a, couch, .)

    mere reranking significantly improves machine translation and 

   

id103 quality [schwenk, 2007; schwenk, 2012]
very good at sentence completion without much task-specific 
engineering [tran, ..., monz, naacl 2016]

2. generate a long, coherent text

   

observed earlier by mikolov [2010, in his thesis] and 
sutskever et al. [2011]

82

2016-08-07

conditional recurrent language model

le chat assis sur le tapis.

?

the cat sat on the mat.

encoder

y

83

2016-08-07

recurrent neural network encoder

h0

h1

le 

h2

h3

   

chat

assis

h7

.

    read a source sentence one symbol at a time.
    the last hidden state     summarizes the entire source sentence.
    any recurrent activation function can be used: 

y

tanh

    hyperbolic tangent 
    gated recurrent unit [cho et al., 2014]
    long short-term memory [sutskever et al., 2014]
    convolutional network [kalchbrenner&blunsom, 2013]

decoder: recurrent language model

the

z0

cat

z1

sat

z2

on

z3

   

y = h7

the

cat

sat

1. transition

zt = f (zt 1, xt, y )

    usual recurrent language model, except
2. id26 xt
    same learning strategy as usual: id113 with sgd 
t nxt=1
nxn=1

l(   , d) =

1 , . . . , xn

log p(xn

t |xn

@zt/@y

1
n

t 1, y )

with conditional recurrent language model,

1. score a translation

log p(the, cat, is, sitting, on, a, couch, .|

le, chat, est, assis, sur, un, canap  e, .) =?

2. directly generate a translation

le,chat, est, assis, sur, un, canap  e, .

the

7! the, cat, is, sitting, on, a, couch, .

z0

cat

z1

sat

z2

on

z3

   

the

cat

sat

86

h0

h1

le 

h2

h3

   

h7

chat
2016-08-07

assis

.

2d. decoding strategies
ancestral sampling, greedy decoding and id125

87

decoding (0)     exhaustive search

    simple and exact decoding algorithm
    score each and every possible translation
    pick the best one
do not even think 
of trying it out!*

the

cat

sat

z2

z0

z1

on

z3

   

h0

88

* perhaps with quantum computer and quantum annealing?

the

cat

sat

h1

le 

h2

h3

   

h7

chat

assis

.

decoding (1)     ancestral sampling

    efficient, unbiased sampling
    one symbol at a time from 
    until 

the

  xt = heosi

  xt     xt|xt 1, . . . , x1, y
cat

sat

x0|y

z0

x1|x0, y

x2|x1, x0, y

z1

z2

z3

y = h7

89

decoding (1)     ancestral sampling

    pros:

1. unbiased (asymptotically exact)

    cons:

1. high variance
2. pretty inefficient

the

cat

sat

x0|y

z0

x1|x0, y

x2|x1, x0, y

z1

z2

z3

y = h7

90

decoding (2)     greedy search

    efficient, but heavily suboptimal search
    pick the most likely symbol each time
log p(x|x<t, y )

  xt = arg max

x

    until 
    pros:

  xt = heosi

1. super-efficient

    both computation and memory

    cons:

1. heavily suboptimal

91

  xt = arg max

j

log p(xt = j|  x<t).

decoding (3) 
    id125

this continues until a special marker indicating the end of the sequence is selected.
this greedy approach is computationally ef   cient, but is likely too crude. any early choice based
on a high id155 can easily turn out to be unlikely one due to low conditional
probabilities later on. this issue is closely related to the garden path sentence problem (see sec. 3.2.4
of [17].)

3.2 id125

be a set of current hypotheses at time t. then, from each current hypothesis the following |v |
candidate hypotheses are generated:

be a set of current hypotheses at time t. then, from each current hypothesis the following |v |
t , k = 1, . . . , k are selected
candidate hypotheses are generated:

id125 improves upon the greedy decoding strategy by maintaining k hypotheses at each time
step, instead of a single one. let
2, . . . ,   xk

be a set of current hypotheses at time t. then, from each current hypothesis the following |v |
    pretty, but not quite efficient
candidate hypotheses are generated:
t 1, v|v |)  ,
t = (  xk
    maintain k hypotheses at a time
2, . . . ,   xk
2, . . . ,   xk
1,   xk
t 1, v1), (  xk
hk
1,   xk
t 1, v2), . . . , (  xk
2, . . . ,   xk
1,   xk
t = (  xk
t 1, v2), . . . , (  xk
2, . . . ,   xk
2, . . . ,   xk
hk
1,   xk
t 1, v1), (  xk
1,   xk
t 1) 
ht 1 = (  x1
where vj denotes the j-th symbols in the vocabulary v .
2, . . . ,   x2
1,   x1
1 ,   xk
1,   x2
t 1), (  x2
2, . . . ,   x1
2 , . . . ,   xk
t 1), . . . , (  xk
where vj denotes the j-th symbols in the vocabulary v .
    expand each hypothesis
the top-k hypotheses from the union of all such hypotheses sets hk
based on their scores. in other words,
the top-k hypotheses from the union of all such hypotheses sets hk
t 1, v|v |)  ,
t = (  xk
3
based on their scores. in other words,
t 1, v1), (  xk
hk
2, . . . ,   xk
t 1, v2), . . . , (  xk
k=1bk,
    pick top-k hypotheses from the union                              where
where vj denotes the j-th symbols in the vocabulary v .
ht = [k
the top-k hypotheses from the union of all such hypotheses sets hk
k0=1hk0
bk = arg max
t .
based on their scores. in other words,
  x2ak

1,   xk
k=1bk,
t , k = 1, . . . , k are selected
log p(   x|y ), ak = ak 1   bk 1, and a1 = [k
log p(   x|y ), ak = ak 1   bk 1, and a1 = [k
ht = [k
among the top-k hypotheses, we consider the ones whose last symbols are the special marker for
the end of sequence to be complete and stop expanding such hypotheses. all the other hypotheses

among the top-k hypotheses, we consider the ones whose last symbols are the special marker for

t , k = 1, . . . , k are selected

bk = arg max
  x2ak

1,   xk
ht = [k

k=1bk,

2, . . . ,   xk

2, . . . ,   xk

92
where

where

where

1,   xk

1,   xk

decoding (3) 
    id125

    asymptotically exact, as
    but, not necessarily monotonic improvement w.r.t.  
    k should be selected to maximize the translation quality on a 

k ! 1

k

validation set.

93

decoding

    en-cz: 12m training sentence pairs

strategy

# chains

ancestral sampling
greedy decoding

beamsearch
beamsearch

50
-
5
10

valid set

test set

nll
22.98
27.88
20.18
19.92

id7
15.64
15.50
17.03
17.13

nll
26.25
26.49
22.81
22.44

id7
16.76
16.66
18.56
18.59

94

[cho, arxiv 2016]

decoding

    greedy search

    computationally efficient
    not great quality

    id125

    computationally expensive
    not easy to parallelize
    much better quality

is there anything in-between?

95

[cho, arxiv 2016]

2d. ensemble of neural mt
decoding from an ensemble of encoder-decoder   s.

96

ensemble of conditional recurrent lm

y

r
e
d
o
c
n
e

r
e
d
o
c
n

ye

.
s
i
p
a
t
 
e
l
 
r
u
s

s
i
s
s
a
 
t
a
h
c
 
e
l

97

z0

z1

z2

z3

p(x1)

    x1

p(x1)

z0

p(x2|x1)
    x2
p(x2|x1)

p(x3|x2)
    x3
p(x3|x2)

z1

z2

z3

2016-08-07

t

p(xens

m=1p(xm

1. majority voting scheme (or):

ensemble of conditional recurrent lm
    step-wise ensemble:  
<t , y ) =  m
|xens
<t, y )
    ensemble operator          implementations
 
mxm=1
m=1pens =
 m
2. consensus building scheme (and): 
pm!1/m
m=1pens =  mym=1

t |xm

 m

1
m

pm

27
25
23
21
19
17
15

single 
(med)

en-de

en-cs

en-ru

en-fi

[jung, cho & bengio, acl2016]

2016-08-07

wrap up

1. training a recurrent language model efficiently

2. building a better model with id149

3. building a conditional recurrent language model

4. generating a translation from a trained conditional 

recurrent language model

99

2016-08-07

do i smell coffee..?

100

have we convinced you about id4?

je

suis   tudiant

_

decoder

i

am

a student

_

je

suis   tudiant

encoder

101

3. advancing id4
a. the vocabulary aspect

    goal: extend the vocabulary coverage.

b. the memory aspect

    goal: translate long sentences better.
c. the language complexity aspect
    goal: handle more language variations.

d. the data aspect

    goal: utilize more data sources.

102

3. advancing id4
a. the vocabulary aspect

    goal: extend the vocabulary coverage.

b. the memory aspect

    goal: translate long sentences better.
c. the language complexity aspect
    goal: handle more language variations.

d. the data aspect

    goal: utilize more data sources.

103

the word generation problem

softmax

parameters

hidden 
state

je

suis   tudiant

je

suis   tudiant

_

p(je|    )

i

am

a student

_

je

suis   tudiant

|v|

104

the word generation problem

    word generation problem

softmax

parameters

hidden 
state

je

suis   tudiant

je

suis   tudiant

_

p(je|    )

i

am

a student

_

je

suis   tudiant

softmax computation is expensive.

|v|

105

the word generation problem

    word generation problem

    vocabs are modest: 50k.

je

suis   tudiant

the  ecotax portico 
in  pont-de-buis
le  portique    cotaxe de  pont-de-buis

i

am

a student

_

je

the  <unk> portico 
in  <unk>
le  <unk> <unk> de  <unk>

106

first thought: scale the softmax
    lots of ideas from the neural lm literature!
    id187: tree-structured vocabulary

    [morin & bengio, aistats   05], [mnih & hinton, nips   09].
    complex, sensitive to tree structures.

    noise-contrastive estimation: binary classification

    [mnih & teh, icml   12], [vaswani et al., emnlp   13].
    different noise samples per training example.*

not gpu-friendly

107

*we   ll mention a simple fix for this!

large-vocab id4

    gpu-friendly.
    training: a subset of the vocabulary at a time.
    testing: smart on the set of possible translations.

fast at both train & test time.

s  bastien jean, kyunghyun cho, roland memisevic, yoshua bengio. on using very 

large target vocabulary for id4. acl   15.

108

training

    each time train on a smaller vocab v       v

|v  |

109

how do we 
select v  ?

training

    each time train on a smaller vocab v       v

|v  |

    each subset has      distinct target words, |v  | =     .

    partition training data in subsets:

110

training     segment data
    sequentially select examples: |v  | = 5.

v   = {she, loves, cats, he, likes}

she loves cats
he likes dogs
cats have tails
dogs have tails
dogs chase cats
she loves dogs
cats hate dogs

111

training     segment data
    sequentially select examples: |v  | = 5.

v   = {cats, have, tails, dogs, chase}

she loves cats
he likes dogs
cats have tails
dogs have tails
dogs chase cats
she loves dogs
cats hate dogs

112

training     segment data
    sequentially select examples: |v  | = 5.

she loves cats
he likes dogs
cats have tails
dogs have tails
dogs chase cats
she loves dogs
cats hate dogs

v   = {she, loves, dogs, cats, hate}

    practice: |v| = 500k, |v  | = 30k or 50k.

113

testing     select candidate words

    k most frequent words: unigram prob.

de,
,
la
.
et
des
les
   

114

testing     select candidate words

    k most frequent words: unigram prob.

    candidate target words

    k   choices per source word. k   = 3.

de,
,
la
.
et
des
les
   

elle
celle
ceci
she

aime
amour
aimer
loves

chats
chat
f  lin
cats

115

testing     select candidate words

k  

elle
celle
ceci
she

aime
amour
aimer
loves

chats
chat
f  lin
cats

+

k
de,
,
la
.
et
des
les
   

= candidate 

list

    produce translations within the candidate list
    practice: k   = 10 or 20, k = 15k, 30k, or 50k.

116

more on large-vocab techniques

       blackout: speeding up recurrent neural 
network language models with very large 
vocabularies        [ji, vishwanathan, satish, 
anderson, dubey, iclr   16].
    good survey over many techniques.

       simple, fast noise contrastive estimation for 
large id56 vocabularies        [zoph, vaswani, 
may, knight, naacl   16].
    use the same samples per minibatch. gpu efficient.

117

2nd thought on word generation

    scaling softmax is insufficient:

    new names, new numbers, etc., at test time.

    but previous mt models can copy words.

why can   t 

id4?

118

copy mechanism

    simple way to track target <unk>.

    treat any id4 as a black box.

    annotate training data.
    post-process translations.
complementary to softmax scaling!

thang luong, ilya sutskever, quoc le, oriol vinyals, wojciech zaremba. addressing 

the rare word problem in id4. acl   15.

119

training annotation

    learn alignments

the  ecotax portico 

in  pont-de-buis

le  portique   cotaxe de  pont-de-buis

    add relative positions

the  <unk> portico 

in  <unk>

le  unk1 unk-1 de  unk0

120

training annotation

    learn alignments

the  ecotax portico 

in  pont-de-buis

le  portique   cotaxe de  pont-de-buis

    add relative positions

the  <unk> portico 

in  <unk>

le  unk1 unk-1 de  unk0

121

training annotation

    learn alignments

the  ecotax portico 

in  pont-de-buis

le  portique   cotaxe de  pont-de-buis

    add relative positions

the  <unk> portico 

in  <unk>

le  unk1 unk-1 de  unk0

122

post-processing

test sentence the  <unk>   portico 

ecotax

pont-de-buis

in  <unk>

translation

le  portique unk-1 de  unk0

123

post-processing

test sentence the  <unk>   portico 

ecotax

pont-de-buis

in  <unk>

translation

le  portique unk-1 de  unk0

dictionary
translation

le portique   cotaxe de  pont-de-buis

post-edit

translation

124

post-processing

test sentence the  <unk>   portico 

ecotax

pont-de-buis

in  <unk>

translation

le  portique unk-1 de  unk0

identity

copy

le portique   cotaxe de  pont-de-buis

post-edit

translation

125

effects of translating rare words

u
e
l
b

40

35

30

25

126

durrani et al. (37.0)

sutskever et al. (34.8)

luong et al. (37.5)

sentences ordered by average word frequency rank

first sota id4!

sample translations

source

human

trans

trans+
unk

this trader , richard usher , left rbs in 2010 and is understand to 
have be given leave from his current position as european head of 
forex spot trading at jpmorgan .
ce trader , richard usher , a quitt   rbs en 2010 et aurait   t   mis
suspendu de son poste de responsable europ  en du trading au 
comptant pour les devises chez jpmorgan .
ce unk0 , richard unk0 , a quitt   unk1 en 2010 et a compris qu' il est
autoris      quitter son poste actuel en tant que leader europ  en du 
march   des points de vente au unk5 .
ce n  gociateur , richard usher , a quitt   rbs en 2010 et a compris
qu' il est autoris      quitter son poste actuel en tant que leader 
europ  en du march   des points de vente au jpmorgan .

    translates well long sentences
    correct: jpmorgan vs. jpmorgan.

127

copy mechanism     old but useful!

    later, we   ll discuss better techniques!

    but it   s useful when adapting to new tasks!

    text summarization: [gu, lu, li, li, acl   16], 
[gulcehre, ahn, nallapati, zhou, bengio, acl   16]
    id29: [jia, liang, acl   16]
learn to decide when to copy.

128

3. advancing id4
a. the vocabulary aspect

    goal: extend the vocabulary coverage.

b. the memory aspect

    goal: translate long sentences better.
c. the language complexity aspect
    goal: handle more language variations.

d. the data aspect

    goal: utilize more data sources.

129

vanilla id195 & long sentences

je

suis   tudiant

_

i

am

a student

_

je

suis   tudiant

problem: fixed-dimensional representations

130

attention mechanism

started in id161!
[larochelle & hinton, 2010],
[denil, bazzani, larochelle, 

je

suis   tudiant

freitas, 2012]
_

pool of 
source 
states

i

am

a student

_

je

suis   tudiant

    solution: random access memory

    retrieve as needed.

131

learning both 
translation & alignment

132

dzmitry bahdanau, kyunghuyn cho, and yoshua bengio. neural machine 

translation by jointly learning to translate and align. iclr   15.

attention mechanism

attention layer
context 
vector

suis

?

i

am

a student

_

je

simplified version of (bahdanau et al., 2015)

133

attention mechanism     scoring

suis

attention layer
context 
vector
3

?

i

am

a student

_

je

    compare target and source hidden states.

134

attention mechanism     scoring

suis

attention layer
context 
vector
3
5

?

i

am

a student

_

je

    compare target and source hidden states.

135

attention mechanism     scoring

suis

attention layer
context 
vector
5
3

1

?

i

am

a student

_

je

    compare target and source hidden states.

136

attention mechanism     scoring

suis

attention layer
context 
vector
5
3

1

1

?

i

am

a student

_

je

    compare target and source hidden states.

137

attention mechanism     id172

suis

attention layer
context 
vector
0.3 0.5 0.1

0.1

?

i

am

a student

_

je

    convert into alignment weights.

138

attention mechanism     context

suis

context vector

?

i

am

a student

_

je

    build context vector: weighted average.

139

attention mechanism     hidden state

suis

context vector

i

am

a student

_

je

    compute the next hidden state.

140

attention mechanisms+
    simplified mechanism & more functions:

thang luong, hieu pham, and chris manning. effective approaches to 

attention-based id4. emnlp   15.

141

attention mechanisms+
    simplified mechanism & more functions:

bilinear form: 
well-adopted.

142

global vs. local
    avoid focusing on everything at each time

global: all source states.

local: subset of source states.

potential for long sequences!

thang luong, hieu pham, and chris manning. effective approaches to 

attention-based id4. emnlp   15.

143

better translation of long sentences

25

20

15

10

(cid:0)
(cid:0)
(cid:0)
(cid:0)
(cid:0)
u
e
l
b

 

10

20

144

attention

 

ours, no attn (id7 13.9)
ours, local   p attn (id7 20.9)
ours, best system (id7 23.0)
wmt   14 best (id7 20.7)
jeans et al., 2015 (id7 21.6)

no attention
40

30

sent lengths

50

60

70

better translation of long sentences

25

20

15

10

(cid:0)
(cid:0)
(cid:0)
(cid:0)
(cid:0)
u
e
l
b

 

new 
sota!

ours, no attn (id7 13.9)
ours, local   p attn (id7 20.9)
ours, best system (id7 23.0)
wmt   14 best (id7 20.7)
jeans et al., 2015 (id7 21.6)

 

10

20

30

40

sent lengths

50

60

70

145

sample english-german translations

source orlando bloom and miranda kerr still love each other 
human orlando bloom und miranda kerr lieben sich noch immer
+attn orlando bloom und miranda kerr lieben einander noch
base orlando bloom und lucas miranda lieben einander noch

immer . 

immer . 

    translates names correctly.

146

sample english-german translations

source

human

+attn

base

we     re pleased the faa recognizes that an enjoyable passenger experience is 
not incompatible with safety and security , said roger dow , ceo of the u.s. 
travel association . 
wir freuen uns , dass die faa erkennt , dass ein angenehmes
passagiererlebnis nicht im wider- spruch zur sicherheit steht , sagte roger 
dow , ceo der u.s. travel association . 
wir freuen uns , dass die faa anerkennt , dass ein angenehmes ist nicht mit
sicherheit und sicherheit unvereinbar ist , sagte roger dow , ceo der us -
die . 
wir freuen uns u   ber die <unk> , dass ein <unk> <unk> mit sicherheit nicht 
vereinbar ist mit sicherheit und sicherheit , sagte roger cameron , ceo der 
us - <unk> . 

    translates a doubly-negated phrase correctly.
   passenger experience   .

147

sample english-german translations

source

human

+attn

base

we     re pleased the faa recognizes that an enjoyable passenger experience is 
not incompatible with safety and security , said roger dow , ceo of the u.s. 
travel association . 
wir freuen uns , dass die faa erkennt , dass ein angenehmes
passagiererlebnis nicht im wider- spruch zur sicherheit steht , sagte roger 
dow , ceo der u.s. travel association . 
wir freuen uns , dass die faa anerkennt , dass ein angenehmes ist nicht mit
sicherheit und sicherheit unvereinbar ist , sagte roger dow , ceo der us -
die . 
wir freuen uns u   ber die <unk> , dass ein <unk> <unk> mit sicherheit nicht 
vereinbar ist mit sicherheit und sicherheit , sagte roger cameron , ceo der 
us - <unk> . 

    translates a doubly-negated phrase correctly.
   passenger experience   .

148

more attention! the idea of coverage

    id134

how to not miss an 
important image patch?

149

xu, ba, kiros, cho, courville, salakhutdinov, zemel, bengio. show, attend and 

tell: neural image id134 with visual attention. icml   15

doubly attention

    sum to 1 in both dimensions

per image patch

sum across 
caption words

= 1

coverage set 
exists long time 
ago in smt!

    1

xu, ba, kiros, cho, courville, salakhutdinov, zemel, bengio. show, attend and 

tell: neural image id134 with visual attention. icml   15

150

extend to id4     linguistic insights

    [cohn, hoang, vymolova, yao, dyer, haffari, 
naacl   16]: position (ibm2) + markov (id48) + 
fertility (ibm3-5) + alignment symmetry 
(berkeleyaligner).

per source word

source word fertility

    [tu, lu, liu, liu, li, acl   16]: linguistic & nn-based 

coverage models.

151

if you feel jetlagged     see when mt fails

sale of chicken murder

go back toward your behind

152

deep fried baby

meat muscle stupid bean sprouts

3. advancing id4
a. the vocabulary aspect

    goal: extend the vocabulary coverage.

b. the memory aspect

    goal: translate long sentences better.
c. the language complexity aspect
    goal: handle more language variations.

d. the data aspect

    goal: utilize more data sources.

153

extend id4 to more languages

       copy    mechanisms are not sufficient.

    id68: christopher     kry  tof
    multi-word alignment: solar system     sonnensystem

    need to handle large, open vocabulary

    rich morphology: nejneobhospoda  ov  vateln  j    mu

    informal spelling: goooooood morning !!!!!

(   to the worst farmable one   )

be able to operate at sub-word levels.

154

sub-word modeling

again, lots of inspirations 
from neural id38!

155

character-based lstm

   

   

u

n
(unfortunately)

l

y

bi-lstm builds word 

representations

ling, lu  s, marujo, astudillo, amir, dyer, black, trancoso. finding function in form: 

compositional character models for open vocabulary word representation. emnlp   15.
156

character-based lstm

the

bank

was

closed

the

bank

was

recurrent language model

   

   

u

n
(unfortunately)

l

y

bi-lstm builds word 

representations

ling, lu  s, marujo, astudillo, amir, dyer, black, trancoso. finding function in form: 

compositional character models for open vocabulary word representation. emnlp   15.
157

character
convnet

yoon kim, yacine jernite, david sontag, and alexander m. rush. 

character-aware neural language models. aaai 2016.

158

highway layer    
like gru but 

applied vertically.

159

sub-word id4: two trends

    same id195 architecture:

    use smaller units.
    [sennrich, haddow, birch, acl   16a], [chung, cho, 
bengio, acl   16].

    hybrid architectures:

    id56 for words + something else for characters.
    [costa-juss   & fonollosa, acl   16], [luong & 
manning, acl   16].

160

byte pair encoding

    a compression algorithm:

    most frequent byte pair     a new byte.

replace bytes with character ngrams

rico sennrich, barry haddow, and alexandra birch. neural machine 

translation of rare words with subword units. acl 2016.

161

byte pair encoding

    a id40 algorithm:

    start with a vocabulary of characters.
    most frequent ngram pairs     a new ngram.

162

byte pair encoding

    a id40 algorithm:

    start with a vocabulary of characters.
    most frequent ngram pairs     a new ngram.
dictionary

vocabulary

5 l o w
2   l o w e r
6   n e w e s t 
3   w i d e s t

163

l, o, w, e, r, n, w, s, t, i, d

start with all characters 

in vocab

(example from sennrich)

byte pair encoding

    a id40 algorithm:

    start with a vocabulary of characters.
    most frequent ngram pairs     a new ngram.
dictionary

vocabulary

5 l o w
2   l o w e r
6   n e w es t 
3   w i d es t

164

l, o, w, e, r, n, w, s, t, i, d, es

add a pair (e, s) with freq 9

(example from sennrich)

byte pair encoding

    a id40 algorithm:

    start with a vocabulary of characters.
    most frequent ngram pairs     a new ngram.
dictionary

vocabulary

5 l o w
2   l o w e r
6   n e w est
3   w i d est

165

l, o, w, e, r, n, w, s, t, i, d, es, est

add a pair (es, t) with freq 9

(example from sennrich)

byte pair encoding

    a id40 algorithm:

    start with a vocabulary of characters.
    most frequent ngram pairs     a new ngram.
dictionary

vocabulary

5 lo w
2   lo w e r
6   n e w est
3   w i d est

166

l, o, w, e, r, n, w, s, t, i, d, es, est, lo

add a pair (l, o) with freq 7

(example from sennrich)

byte pair encoding

    a id40 algorithm:

    start with a vocabulary of characters.
    most frequent ngram pairs     a new ngram.
    automatically decide vocabs for id4

    word-level: asinine situation     asinin-situation
    bpe-level: as in ine situation     as in in- situation

top places in wmt 2016!

https://github.com/rsennrich/nematus

167

bpe     characters

works for many language pairs.

junyoung chung, kyunghyun cho, yoshua bengio. a character-level decoder without explicit 

segmentation for id4. acl 2016.

168

sub-word id4: two trends

    same id195 architecture:

    use smaller units.
    (sennrich et al., acl   16), (chung et al., acl   16).

    hybrid architectures:

    id56 for words + something else for characters.
    [costa-juss   & fonollosa, acl   16], [luong & 
manning, acl   16].

169

character-level encoder

    useful when source
language is complex:
    similar architecture 
[kim, jernite, sontag, 
rush, aaai   15].
+3 id7 for 
german-english 
translation.

170

marta r. costa-juss   and jos   a. r. fonollosa. 

character-based id4. acl   16.

hybrid id4

    a best-of-both-worlds architecture:

    translate mostly at the word level
    only go the character level when needed.

    more than 2 id7 improvement over copy 
mechanism.

thang luong and chris manning. achieving open vocabulary neural machine 

translation with hybrid word-character models. acl 2016.

171

hybrid id4

word-level 
(4 layers)

172

end-to-end training

8-stacking lstm layers.

2-stage decoding
    word-level id125

173

2-stage decoding
    word-level id125
    char-level id125 
for <unk>.

init with word 
hidden states.

174

english-czech results

    train on wmt   15 data (12m sentence pairs)

    newstest2015

systems

id7

winning wmt   15 (bojar & tamchyna, 2015)

18.8

30x data
3 systems

word-level id4 (jean et al., 2015)

18.3

large vocab

+ copy mechanism

175

english-czech results

    train on wmt   15 data (12m sentence pairs)

    newstest2015

systems

id7

winning wmt   15 (bojar & tamchyna, 2015)

18.8

word-level id4 (jean et al., 2015)

hybrid id4 (luong & manning, 2016)*

18.3

20.7

30x data
3 systems

large vocab

+ copy mechanism

new 
sota!

176

effects of vocabulary sizes

word

word	+	copy	mechanism

hybrid

+2.1

+3.5

+4.5

+11.4

20
18
16
14
12
10
8
6
4
2
0
more than +2.0 id7 over copy mechanism!

20k
10k
vocabulary	size

50k

1k

u
e
l
b

177

rare id27s

loveless
spiritless

heartlessly
heartlessness

narrow   mindedness

narrow   minded

wholeheartedness

nonconscious
uncontroversial

unattainableness
inabilities

impossibilities

untrustworthy

unrealizable

possible

necessary

impossible

acceptable
satisfactory

unacceptable

unsatisfactory
unsuitable

insufficiency

uncomfortable
insensitive

unaffected

noticeable
perceptible

immobile

immoveable

disrespectful
ungraceful
regretful

illiberal

unconcern

admittanceadmitting

founder

chooses
nominated

sponsor

antagonist

antagonize

obvious

advance

explicit

evidently

acknowledgement

developments

develop

unsighted
unfeathered
unfledged

practice

governance

management

0.6

0.7

connect
link
0.8

0.1

0.2

0.3

0.4

0.5

    word & character-based embeddings.

admissionadmit

decide

choose

cofounders

companionships
1

0.9

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

178

rare id27s

loveless
spiritless

heartlessly
heartlessness

narrow   mindedness

narrow   minded

wholeheartedness

nonconscious
uncontroversial

unattainableness
inabilities

impossibilities

untrustworthy

unrealizable

possible

necessary

impossible

acceptable
satisfactory

unacceptable

unsatisfactory
unsuitable

insufficiency

uncomfortable
insensitive

unaffected

noticeable
perceptible

immobile

immoveable

disrespectful
ungraceful
regretful

illiberal

unconcern

admittanceadmitting

founder

chooses
nominated

sponsor

antagonist

antagonize

obvious

advance

explicit

evidently

acknowledgement

developments

develop

unsighted
unfeathered
unfledged

practice

governance

management

0.6

0.7

connect
link
0.8

0.1

0.2

0.3

0.4

0.5

    word & character-based embeddings.

admissionadmit

decide

choose

cofounders

companionships
1

0.9

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

179

sample english-czech translations

source her 11-year-old daughter , shani bart , said it felt a little bit weird
human jej   jeden  ctilet   dcera shani bartov   prozradila ,   e je to trochu

zvl    tn  
jej   <unk> dcera <unk> <unk>   ekla ,   e je to trochu divn  
jej   11-year-old dcera shani ,   ekla ,   e je to trochu divn  

word

hybrid

jej   <unk> dcera , <unk> <unk> ,   ekla ,   e je to <unk> <unk>
jej   jeden  ctilet   dcera , graham bart ,   ekla ,   e c  t   trochu divn  
    hybrid: correct, 11-year-old     jeden  ctilet  ..

180

sample english-czech translations

source her 11-year-old daughter , shani bart , said it felt a little bit weird
human jej   jeden  ctilet   dcera shani bartov   prozradila ,   e je to trochu

zvl    tn  
jej   <unk> dcera <unk> <unk>   ekla ,   e je to trochu divn  
jej   11-year-old dcera shani ,   ekla ,   e je to trochu divn  

word

hybrid

jej   <unk> dcera , <unk> <unk> ,   ekla ,   e je to <unk> <unk>
jej   jeden  ctilet   dcera , graham bart ,   ekla ,   e c  t   trochu divn  

    word-based: identity copy fails.

181

3. advancing id4
a. the vocabulary aspect

    goal: extend the vocabulary coverage.

b. the memory aspect

    goal: translate long sentences better.
c. the language complexity aspect
    goal: handle more language variations.

d. the data aspect

    goal: utilize more data sources.

182

can we utilize other data sources?

    multi-lingual: learn from many language pairs? 
    smt-inspired: utilize monolingual data? 
    multi-task: combine id195 tasks?

translation

parsing

183

can we utilize other data sources?

    multi-lingual: learn from many language pairs? 
    smt-inspired: utilize monolingual data? 
    multi-task: combine id195 tasks?

more later by cho!

184

integrating language models

    score interpolation:

language model scores

hyperparameter

    deep fusion: combine hidden states instead.

    controller learns interpolation weights.
    better than shallow score interpolation.
improve low-resource language pairs

gulcehre, firat, xu, cho, barrault, lin, bougares, schwenk, bengio. 

on using monolingual corpora in id4. arxiv 2015.

185

autoencoders

    shared encoders & decoders: 3 tasks

german (translation)

english

english (unsupervised)

german (unsupervised)

    small amount of mono data as id173.

    +0.9 id7 improvements
how to utilize more monolingual data?

186

thang luong, quoc le, ilya sutskever, oriol vinyals, lukasz kaiser. 

multi-task sequence to sequence learning. iclr 2016.

enriching parallel data

    dummy source sentences

she loves cute cats elle aime les chats mignons

(parallel)

<null>

elle aime les chiens mignons

(mono)

small gain +0.4-1.0 id7.

difficult to add more mono data.

rico sennrich, barry haddow, and alexandra birch. improving neural 

machine translation models with monolingual data. acl 2016.

187

enriching parallel data

    synthetic source sentences

she loves cute cats elle aime les chats mignons

(parallel)

she likes cute cats

elle aime les chiens mignons

(mono)

back translated

large gain +2.1-3.4 id7.

rico sennrich, barry haddow, and alexandra birch. improving neural 

machine translation models with monolingual data. acl 2016.

188

prevent over-fitting

with synthetic source

189

4. future of id4

a. id72

b. larger context

c. mobile devices

d. beyond id113

190

4. future of id4

a. id72

b. larger context

c. mobile devices

d. beyond id113

191

multilingual translation

language-agnostic 
continuous space

192

[dong et al., acl2015; luong et al., iclr2016; firat et al., naacl2016]

multilingual translation: expectations

1. positive language transfer
2. # of parameters grows linearly w.r.t. # of languages
3. multi-source translation [zoph&knight, naacl2016]

language-agnostic 
continuous space

193

[dong et al., acl2015; luong et al., iclr2016; firat et al., naacl2016]

multilingual translation 

with shared alignment

    encoder per source language

    seq. of source symbols     seq. of context vectors

194

[firat et al., naacl2016]

multilingual translation 

with shared alignment

    shared attention mechanism

    target hidden state, source context vector 
    attention weight

195

[firat et al., naacl2016]

multilingual translation 

with shared alignment

    decoder per target language

    aligned context vector     target symbol

196

[firat et al., naacl2016]

multilingual translation: training
    no multi-way parallel corpus assumed

    bilingual sentence pairs only
    each sentence pair activates/updates one encoder, decoder 
and shared attention

197

[dong et al., acl2015; luong et al., iclr2016; firat et al., naacl2016]

multilingual translation: first result
    10 language pair-directions

    en     {fr, cs, de, ru, fi} + {fr, cs, de, ru, fi}     en

    60+ million bilingual sentence pairs
    comparable to 10 single-pair models

30

25

20

15

10

198

to english

from english

30

25

20

15

10

single

multi

fr

cs

de

ru

fi

fr

cs

de

ru

[firat et al., naacl2016]

multilingual translation: looking ahead

    low-resource translation

    positive language transfer from high-resource to 
low-resource language pair-directions

199

[firat et al., under review]

multilingual translation: looking ahead

    low-resource translation: example

uz-en: 6.45
uz-en + tr-en: 9.34

uz-en + tr-en + es-en: 10.34

uz-en + tr-en + es-en + en-tr: 9.41
ensemble: 12.99
    3x uz-en + tr-en + es-en
    3x uz-en + tr-en + es-en + en-tr
200

alignment

[firat et al., 2016c]

multilingual translation: looking ahead

    zero-resource translation

    translation without any direct parallel resource

training

testing

201

[firat et al., emnlp2016]

multilingual translation: looking ahead

    zero-resource translation

    finetuning with pseudo-parallel corpus 
[sennrich et al., acl2016]
    closely related to unsupervised learning

202

pseudo-corpus generation

finetuning

[firat et al., emnlp2016]

multilingual translation: looking ahead
    zero-resource translation

    some initial result, but long way to go   

203

[firat et al., emnlp2016]

multilingual translation: looking ahead
    multi-modal, multitask translation

[luong et al., iclr2016; caglayan et al., wmt2016]

204

4. future of id4

a. id72

b. larger context

c. mobile devices

d. beyond id113

205

larger-context id4

    beyond sentence level:

    paragraphs, articles, books, etc.

    challenges?

    extremely long sequences.
    maintain across sentences:

    coherent style
    discourse structure

206

solution: hierarchical architectures?

    effective attention mechanism for long sequences

207

solution: hierarchical architectures?
    id103 [chan, jaity, le, vinyals, icassp   15].

208

solution: hierarchical architectures?
    id103 [chan, jaity, le, vinyals, icassp   15].

speech signals: 

thousands of frames

209

solution: hierarchical architectures?
    id103 [chan, jaity, le, vinyals, icassp   15].
speech transcription: 
   how much would a 
woodchuck chuck   

pyramid structure

speech signals: 

thousands of frames

210

solution: hierarchical architectures?

    effective attention mechanism for long sequences

    id103 [chan, jaity, le, vinyals, 
icassp   15].

    tracking states over time

211

solution: hierarchical architectures?
    dialogue systems [serban, sordoni, bengio, courville, 

pineau , aaai   15].

212

solution: hierarchical architectures?
    dialogue systems [serban, sordoni, bengio, courville, 

pineau , aaai   15].

utterance level

213

solution: hierarchical architectures
    dialogue systems [serban, sordoni, bengio, courville, 

pineau , aaai   15].

214

context level

solution: hierarchical architectures?

    effective attention mechanism for long sequences
    id103 [chan, jaity, le, vinyals, icassp   15].

    tracking states over many sentences

    dialogue systems [serban, sordoni, bengio, courville, 
pineau , aaai   15].

what else?

215

4. future of id4

a. id72

b. larger context

c. mobile devices

d. beyond id113

216

mobile devices

    id4 has small memory footprint:

    no gigantic phrase tables & lms compared to smt.

    still, require large nns for sota results

can we 
address this?

217

model pruning

    explore the redundancy structure in id4

e
r
o
c
s
u
e
l
b

25

20

15

10

5

0

0

original model

pruned
pruned and retrained

10

20

30

40

50

60

70

80

90

percentage pruned

218

abigail see, thang luong, chris manning. compression of id4 

figure 1: performance of pruned models, immediately after pruning and after
retraining.

models via pruning. conll   16.

model pruning

    id4 redundancy via pruning & retraining:

e
r
o
c
s
u
e
l
b

25

20

15

10

5

0

0

prune 

smallest 
weights

pruned
pruned and retrained

10

20

30

40

50

60

70

80

90

percentage pruned

219

abigail see, thang luong, chris manning. compression of id4 

figure 1: performance of pruned models, immediately after pruning and after
retraining.

models via pruning. conll   16.

model pruning

    id4 redundancy via pruning & retraining:

e
r
o
c
s
u
e
l
b

25

20

15

10

5

0

0

prune 

smallest 
weights

prune 40% 
with little loss

pruned
pruned and retrained

10

20

30

40

50

60

70

80

90

percentage pruned

220

abigail see, thang luong, chris manning. compression of id4 

figure 1: performance of pruned models, immediately after pruning and after
retraining.

models via pruning. conll   16.

model pruning

    id4 redundancy via pruning & retraining:

e
r
o
c
s
u
e
l
b

25

20

15

10

5

0

0

prune & 
retrain 

prune 80% 
without loss

pruned
pruned and retrained

10

20

30

40

50

60

70

80

90

percentage pruned

221

abigail see, thang luong, chris manning. compression of id4 

figure 1: performance of pruned models, immediately after pruning and after
retraining.

models via pruning. conll   16.

it was just a baby!

next, really putting id4 

onto mobile devices!

222

knowledge distillation 

223

yoon kim, alexander m. rush. 

sequence-level knowledge distillation. emnlp   16.

knowledge distillation 

large model

224

yoon kim, alexander m. rush. 

sequence-level knowledge distillation. emnlp   16.

knowledge distillation 

large model

small model

225

yoon kim, alexander m. rush. 

sequence-level knowledge distillation. emnlp   16.

knowledge distillation 

matching softmax

distributions

large model

small model

226

yoon kim, alexander m. rush. 

sequence-level knowledge distillation. emnlp   16.

knowledge distillation 

    sequence-level knowledge distillation:
    match the final distribution over sequences
    id125 to create new training data
    student model: no need id125.

10 times faster with only 0.2 id7 loss! 

https://github.com/harvardnlp/id4-android

227

yoon kim, alexander m. rush. 

sequence-level knowledge distillation. emnlp   16.

4. future of id4

a. id72

b. larger context

c. mobile devices

d. beyond id113

228

id113
for sequence modelling
    given a ground-truth trajectory, maximize the 
predictability of a next action: 
max log p(xt|x<t)
    maximum (log-)likelihood estimation
    two issues
1. weak correlation with a true reward
2. mismatch between training and  

p(the, cat, is, eating)

id136

229

2016-08-07

beyond maximum likelihood
    maximize the sequence-wise global loss
    incorporate id136 into training

    stochastic id136

    policy gradient [ranzato et al., iclr2016; bahdanau et al., arxiv2016]
    minimum risk training [shen et al., acl2016]

    deterministic id136

    learning to search [wiseman & rush, arxiv2016]

230

2016-08-07

what have we learnt today?

1. history of mt and where neural mt fits in
2. language modelling & id4

a.
b.
c.

feedforward and recurrent language models
recurrent neural network and its learning
conditional language model: learning and decoding

thank you!

3. advanced id4

a.
b.
c.
d.

scaling softmax and copy mechanism
attention-based models
subword-level translation
incorporating monolingual corpora

4. and, the future!

https://sites.google.com/site
/acl16id4/home/resources

231

references (1)

[bahdanau et al., iclr   15] neural translation by jointly learning to align and translate. 
http://arxiv.org/pdf/1409.0473.pdf
[chung, cho, bengio, acl   16]. a character-level decoder without explicit segmentation for 
id4. http://arxiv.org/pdf/1603.06147.pdf
[cohn, hoang, vymolova, yao, dyer, haffari, naacl   16] incorporating structural alignment 
biases into an attentional neural translation model. https://arxiv.org/pdf/1601.01085.pdf
[dong, wu, he, yu, wang, acl   15]. id72 for multiple language translation. 
http://www.aclweb.org/anthology/p15-1166
[firat, cho, bengio, naacl   16]. multi-way, multilingual id4 with a shared 
attention mechanism. https://arxiv.org/pdf/1601.01073.pdf
[gu, lu, li, li, acl   16] incorporating copying mechanism in sequence-to-sequence learning. 
https://arxiv.org/pdf/1603.06393.pdf
[gulcehre, ahn, nallapati, zhou, bengio, acl   16] pointing the unknown words. 
http://arxiv.org/pdf/1603.08148.pdf
[hochreiter & schmidhuber, 1997] long short-term memory. 
http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
[kim, jernite, sontag, rush, aaai   16]. character-aware neural language models. 
https://arxiv.org/pdf/1508.06615.pdf

   

   

   

   

   

   

   

   

   

232

references (2)

[ji, haffari, eisenstein, naacl   16] a latent variable recurrent neural network for discourse-driven language 
models. https://arxiv.org/pdf/1603.01913.pdf
[ji, vishwanathan, satish, anderson, dubey, iclr   16] blackout: speeding up recurrent neural network 
language models with very large vocabularies. http://arxiv.org/pdf/1511.06909.pdf
[jia, liang, acl   16]. data recombination for neural id29. https://arxiv.org/pdf/1606.03622.pdf
[ling, lu  s, marujo, astudillo, amir, dyer, black, trancoso, emnlp   15]. finding function in form: compositional 
character models for open vocabulary word representation. http://arxiv.org/pdf/1508.02096.pdf
[luong et al., acl   15a] addressing the rare word problem in id4. 
http://www.aclweb.org/anthology/p15-1002
[luong et al., acl   15b] effective approaches to attention-based id4. 
https://aclweb.org/anthology/d/d15/d15-1166.pdf
[luong & manning, iwslt   15] stanford id4 systems for spoken language domain. 
http://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf
[mnih & hinton, nips   09] a scalable hierarchical distributed language model. 
https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf
[mnih & teh, icml   12] a fast and simple algorithm for training neural probabilistic language models. 
https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf
[mnih et al., nips   14] recurrent models of visual attention.  http://papers.nips.cc/paper/5542-recurrent-models-
of-visual-attention.pdf
[morin & bengio, aistats   05] hierarchical probabilistic neural network language model. 
http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf

   

   

   
   

   

   

   

   

   

   

   

233

references (3)

[sennrich, haddow, birch, acl   16a]. improving id4 models with monolingual 
data. http://arxiv.org/pdf/1511.06709.pdf
[sennrich, haddow, birch, acl   16b]. id4 of rare words with subword units. 
http://arxiv.org/pdf/1508.07909.pdf
[sutskever et al., nips   14] sequence to sequence learning with neural networks. 
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
[tu, lu, liu, liu, li, acl   16] modeling coverage for id4. 
http://arxiv.org/pdf/1601.04811.pdf
[vaswani, zhao, fossum, chiang, emnlp   13] decoding with large-scale neural language models 
improves translation. http://www.isi.edu/~avaswani/nce-nplm.pdf
[wang, cho, acl   16]. larger-context language modelling with recurrent neural network. 
http://aclweb.org/anthology/p/p16/p16-1125.pdf
[xu, ba, kiros, cho, courville, salakhutdinov, zemel, bengio, icml   15] show, attend and tell: neural 
image id134 with visual attention. http://jmlr.org/proceedings/papers/v37/xuc15.pdf
[zoph, knight, naacl   16]. multi-source neural translation. http://www.isi.edu/natural-
language/mt/multi-source-neural.pdf
[zoph, vaswani, may, knight, naacl   16] simple, fast noise contrastive estimation for large id56 
vocabularies. http://www.isi.edu/natural-language/mt/simple-fast-noise.pdf

   

   

   

   

   

   

   
   
   

234

