matrices, vector spaces, and information retrieval

michael w. berry(cid:3), zlatko drma (cid:20)cy, and elizabeth r. jessupz

abstract. the evolution of digital libraries and the internet has dramatically transformed
the processing, storage, and retrieval of information. e(cid:11)orts to digitize text, images, video, and
audio now consume a substantial portion of both academic and industrial activity. even when
there is no shortage of textual materials on a particular topic, procedures for indexing or extracting
the knowledge or conceptual information contained in them can be lacking. recently developed
information retrieval technologies are based on the concept of a vector space. data are modeled as
a matrix, and a user   s query of the database is represented as a vector. relevant documents in the
database are then identi(cid:12)ed via simple vector operations. orthogonal factorizations of the matrix
provide mechanisms for handling uncertainty in the database itself. the purpose of this paper is to
show how such fundamental mathematical concepts from id202 can be used to manage and
index large text collections.

key words. information retrieval, id202, qr factorization, singular value decomposi-

tion, vector spaces

ams subject classi(cid:12)cations. 15-01, 15a03, 15a18, 65f50, 68p20

1. problems in indexing large text collections. traditional indexing
mechanisms for scienti(cid:12)c research papers are constructed from such information as
their titles, author lists, abstracts, key word lists, and subject classi(cid:12)cations. it is
not necessary to read any of those items in order to understand a paper: they exist
primarily to enable researchers to (cid:12)nd the paper in a literature search. for example,
the key words and subject classi(cid:12)cations listed above enumerate what we consider to
be the major mathematical topics covered in this paper. in particular, the subject
classi(cid:12)cation 68p20 identi(cid:12)es this paper as one concerned with information retrieval.
before the advent of modern computing systems, researchers seeking particular infor-
mation could only search through the indexing information manually, perhaps in a
card catalog. if an abstract or key word list were not provided, a professional indexer
or cataloger could have written one.

these manual methods of indexing are succumbing to problems of both capacity
and consistency. at the time of this writing, about 156,000 periodicals are published
in print worldwide with roughly 12,000 additions each year [58]. there are nearly 1.4
million books in print in the united states alone with approximately 60,000 new titles
appearing there annually [15, 16]. the library of congress maintains a collection of
more than 17 million books and receives new items at a rate of 7; 000 per working
day [31]. while these numbers imply daunting indexing problems, the scale is even
greater in the digital domain. there are currently about 300 million web pages on
the internet [13, 39], and a typical search engine updates or acquires pointers to as
many as ten million web pages in a single day [32]. because the pages are indexed at

(cid:3) department of computer science, university of tennessee, knoxville, tn, 37996-1301
(berry@cs.utk.edu). the work of this author was supported in part by the national science foun-
dation under grant no. aci-94-11394.

y department of computer science, university of colorado, boulder, co 80309-0430
(zlatko@cs.colorado.edu). the work of this author was supported by the national science foun-
dation under grants no. aci-93-57812 and aci-96-25912.

z department of computer science, university of colorado, boulder, co 80309-0430
(jessup@cs.colorado.edu). the work of this author was supported by the national science foun-
dation under grant no. aci-93-57812 and by the department of energy under grant no. de-fg03-
97er25325.

1

matrices, vector spaces, and information retrieval

2

a much slower rate, the indexed collection of the largest search engine presently totals
about 100 million documents [13, 32, 39].

even when subsets of data can be managed manually, it is di(cid:14)cult to maintain
consistency in human-generated indexes: the extraction of concepts and key words
from documentation can depend on the experiences and opinions of the indexer. de-
cisions about important key words and concepts can be based on such attributes as
age, cultural background, education, language, and even political bias. for instance,
while we chose to include only higher level concepts in this paper   s key word list, a
reader might think that the words vector and matrix should also have been selected.
our editor noted that the words expository and application did not appear in the list
even though they describe the main purpose of this paper. experiments have shown
that there is a 20% disparity on average in the terms chosen as appropriate to describe
a given document by two di(cid:11)erent professional indexers [29].

these problems of scale and consistency have fueled the development of auto-
mated information retrieval (ir) techniques. when implemented on high-performance
computer systems, such methods can be applied to extremely large databases, and
they can, without prejudice, model the concept-document association patterns that
constitute the semantic structure of a document collection. nonetheless, while au-
tomated systems are the answer to some concerns of information management, they
have their own problems. disparities between the vocabulary of the systems    authors
and users pose di(cid:14)culties when information is processed without human intervention.
complexities of language itself present other quandaries. words can have many mean-
ings: a bank can be a section of computer memory, a (cid:12)nancial institution, a steep
slope, a collection of some sort, an airplane maneuver, or even a billiard shot. it can
be hard to distinguish those meanings automatically. similarly, authors of medical
literature may write about myocardial infarctions, but the person who has had a mi-
nor heart attack may not realize that the two phrases are synonymous when using the
public library   s on-line catalog to search for information on treatments and prognosis.
formally, polysemy (words having multiple meanings) and synonymy (multiple words
having the same meaning) are two major obstacles to retrieving relevant information
from a database.

polysemy and synonymy are two of the fundamental problems that any concep-
tual indexing scheme must overcome. other issues such as the breadth and depth of
concept extraction, zoning (indexing of parts of a document like its title, abstract or
(cid:12)rst few paragraphs as opposed to the entire document), and term or phrase weight-
ing may also a(cid:11)ect retrieval performance [37]. indexing approaches (automated and
otherwise) are generally judged in terms of their recall and precision ratings. recall
is the ratio of the number of relevant documents retrieved to the total number of rel-
evant documents in the collection, and precision is the ratio of the number of relevant
documents retrieved to the total number of documents retrieved.

standardized evaluation of information retrieval began in 1992 with the initiation
of the annual text retrieval conference (trec) sponsored by the defense advanced
research projects agency (darpa) and the national institute of standards and
technology (nist) [30]. trec participants competitively index a large text collec-
tion (gigabytes in size) and are provided search statements and relevancy judgments
in order to judge the success of their approaches. another darpa-sponsored e(cid:11)ort in
standardization is being lead by the tipster working group [33]. the focus of this
group is to specify an architecture of an ir system (a set of protocols for document
processing) without legislating how that architecture should be implemented. par-

matrices, vector spaces, and information retrieval

3

ticipants try to determine ways of integrating new methods of information retrieval
using a consistent interface.

the purpose of this paper is to show how id202 can be used in automated
information retrieval. the most basic mechanism is the vector space model [52, 18]
of ir in which each document is encoded as a vector, where each vector component
re(cid:13)ects the importance of a particular term in representing the semantics or meaning
of that document. the vectors for all documents in a database are stored as the
columns of a single matrix. in section 2 of this paper, we show how to translate a
collection of documents into a matrix and how to compare a user   s query to those
documents through basic vector operations. the smart system, introduced in 1983
[52], was one of the (cid:12)rst to use the vector space model. the smart system [18],
tuned using sophisticated heuristic techniques, has been a top performer at trec
conferences.

the newer method of id45 (lsi) or latent semantic analy-
sis (lsa) is a variant of the vector space model in which a low-rank approximation to
the vector space representation of the database is employed [9, 19]. that is, we replace
the original matrix by another matrix that is as close as possible to the original matrix
but whose column space is only a subspace of the column space of the original matrix.
reducing the rank of the matrix is a means of removing extraneous information or
noise from the database it represents. rank reduction is used in various applications
of id202 and statistics [14, 28, 35] as well as in image processing [2], data com-
pression [48], cryptography [45], and seismic tomography [17, 54]. lsi has achieved
average or above average performance for several trec collections [21, 22].

in this paper, we do not review lsi but rather show how to apply the vector
space model directly to a low-rank approximation of the database matrix. the oper-
ations performed in this version of the vector space model admit an easier geometric
interpretation than do those underlying lsi. we refer the interested reader to [9, 19]
for the details of lsi.

we begin our exposition with the qr factorization, the orthogonal factorization
with which most students are familiar. while the latter factorization has not actually
been used in ir methods tested to date, it su(cid:14)ces for showing the features of rank
reduction while being simpler than the svd. in section 3, we show how the qr
factorization of the database matrix can be used to provide a geometric interpretation
of the vector space model. in section 4, we demonstrate how using the factorization
to reduce the rank of the matrix can help to account for uncertainties in the database.
in sections 5 and 6, we move on to combine the vector space model with the
singular value decomposition (svd). the svd is a form of orthogonal matrix fac-
torization that is more powerful than the qr factorization. although the svd is
not often included in introductory id202 courses, students comfortable with
the qr factorization should be able to read and understand these sections. we (cid:12)rst
introduce the svd and compare the low-rank approximations computed from the
svd and the qr factorization. we then explain how to formulate the comparison of
queries and documents via the svd.

in section 7, we explain what motivates the use of the svd in place of the
qr factorization in practice by showing how relationships between terms can be
discovered in the vector space model. such comparisons aid in the re(cid:12)nement of
searches based on the vector space model. the svd allows such comparisons of
terms with terms as well as documents with documents while the qr factorization
permits only the latter. in section 8, we depart from the basic mathematics to cover

matrices, vector spaces, and information retrieval

4

the more advanced techniques necessary to make vector space and svd-based models
work in practice. finally, in section 9, we provide a brief outline of further reading
material in information retrieval.

sections 2 through 7 of this paper should be accessible to anyone familiar with
orthogonal factorization (like the qr factorization.) section 8 is more di(cid:14)cult; the
recent research results and questions about the practical implementation details of
svd-based models in it may be challenging reading for students.

2. the vector space model.

2.1. a vector space representation of information. in the vector space ir
model, a vector is used to represent each item or document in a collection. each com-
ponent of the vector re(cid:13)ects a particular concept, key word, or term associated with
the given document. the value assigned to that component re(cid:13)ects the importance of
the term in representing the semantics of the document. typically, the value is a func-
tion of the frequency with which the term occurs in the document or in the document
collection as a whole [20, 56]. suppose a document is described for indexing purposes
by the three terms applied, linear, and algebra. it can then be represented by a
vector in the three corresponding dimensions. figure 1 depicts that vector when the
terms have respective weights 0:5, 2:5, and 5:0. in this case, the word algebra is
the most signi(cid:12)cant term in the document with linear of secondary importance and
applied of even less importance.

(2.5, 5, 0.5)

d
e
i
l

p
p
a

0.5

0.4

0.3

0.2

0.1

0
5

4

3

2

1

algebra

0

0

1

0.5

2

1.5

linear

3

2.5

fig. 1. vector representation of applied id202.

a database containing a total of d documents described by t terms is represented
as a t (cid:2) d term-by-document matrix a. the d vectors representing the d documents
form the columns of the matrix. thus, the matrix element aij is the weighted fre-
quency at which term i occurs in document j [9]. in the parlance of the vector space
model, the columns of a are the document vectors, and the rows of a are the term
vectors. the semantic content of the database is wholly contained in the column
space of a, meaning that the document vectors span that content. not every vec-

matrices, vector spaces, and information retrieval

5

tor represented in the column space of a has a speci(cid:12)c interpretation in terms of
the document collection itself (i.e., a linear combination of vectors corresponding to
two document titles may not translate directly into a meaningful document title.)
what is important from an information retrieval perspective, however, is that we can
exploit geometric relationships between document vectors to model similarities and
di(cid:11)erences in content. we can also compare term vectors geometrically in order to
identify similarities and di(cid:11)erences in term usage.

a variety of schemes are available for weighting the matrix elements. the elements
aij of the term-by-document matrix a are often assigned two-part values aij = lij gi.
in this case, the factor gi is a global weight that re(cid:13)ects the overall value of term
i as an indexing term for the entire collection. as one example, consider a very
common term like computer within a collection of articles on personal computers.
it is not important to include that term in the description of a document as all of
the documents are known to be about computers (whether or not they use the actual
term computer) so a small value of the global weight gi is appropriate.

the factor lij is a local weight that re(cid:13)ects the importance of term i within
document j itself. local weights range in complexity from simple binary values (0 or
1) to functions involving logarithms of term frequencies. the latter functions have a
smoothing e(cid:11)ect in that high frequency terms having limited discriminatory value are
assigned low weights. global weighting schemes range from simple id172s to
advanced statistics-based approaches. see [20] and [56] for more details about term
weighting.

for text collections spanning many contexts (e.g., an encyclopedia), the number
of terms is often much greater than the number of documents: t (cid:29) d. in the case of
the internet, the situation is reversed. a term-by-document matrix using the content
of the largest english language dictionary as terms and the set of all web pages
as documents would be about 300; 000 (cid:2) 300; 000; 000 [4, 13, 39]. as a document
generally uses only a small subset of the entire dictionary of terms generated for a
given database, most of the elements of a term-by-document matrix are zero.

in a vector space ir scheme, a user queries the database to (cid:12)nd relevant docu-
ments, somehow using the vector space representation of those documents. the query
is a set of terms, perhaps with weights, represented just like a document. again, it is
likely that many of the terms in the database do not appear in the query meaning that
many of the query vector components are zero. query matching is (cid:12)nding the docu-
ments most similar to the query in use and weighting of terms. in the vector space
model, the documents selected are those geometrically closest to the query according
to some measure.

one common measure of similarity is the cosine of the angle between the query and
document vectors. if the term-by-document matrix a has columns aj; j = 1; : : : ; d,
those d cosines are computed according to the formula

(1)

cos (cid:18)j =

at
j q

k aj k2k q k2

=

pt

i=1 a2

i=1 aijqi

ijqpt

qpt

;

i=1 q2
i

i=1 x2

pxt x = pt

for j = 1; : : : ; d; where the euclidean vector norm k x k2 is de(cid:12)ned by k x k2 =
i for any real t-dimensional vector x. because the query and doc-
ument vectors are typically sparse, the dot product and norms in equation (1) are
generally inexpensive to compute. furthermore, the document vector norms k aj k2
need be computed only once for any given term-by-document matrix. note that mul-
tiplying either aj or q by a constant does not change the cosine value. thus, we

matrices, vector spaces, and information retrieval

6

may scale the document vectors or queries by any convenient value. other similarity
measures are reviewed in [34].

2.2. an example. figure 2 demonstrates how a simple collection of (cid:12)ve titles
described by six terms leads to a 6(cid:2)5 term-by-document matrix. because the content
of a document is determined by the relative frequencies of the terms and not by the
total number of times particular terms appear, the matrix elements in this example
are scaled so that the euclidean norm of each column is one. that is, k aj k2 = 1 for
columns aj, j = 1; : : : ; 5. in this way, we use term frequency as the local weight lij
and apply no global weighting (i.e., gi = 1).

the choice of terms used to describe the database determines not only its size
but also its utility. in our example, we used only the terms directly related to cooking
meaning that the reader interested in french cooking in particular would have no
way of retrieving relevant documents.
in this case, adding the terms french and
viennese to describe the nationalities covered would broaden the representation of
the database semantics in a helpful way. on the other hand, including very common
terms like to or the would do little to improve the quality of the term-by-document
matrix. the process of excluding such high frequency words is known as stoplisting
[25].

in constructing a term-by-document matrix, terms are usually identi(cid:12)ed by their
word stems [37]. in our example, the word pastries counts as the term pastry, and the
word baking counts as the term bake. the use of id30 in information retrieval
dates back to the 1960s [43]. id30 reduces storage requirements by decreasing
the number of words maintained [50].

2.3. query matching. using the small collection of titles from figure 2, we can
illustrate query matching based on angles in a 6-dimensional vector space. suppose
that a user in search of cooking information initiates a search for books about baking
bread. the corresponding query would be written as the vector

q(1) = ( 1 0 1 0 0 0 )t

with nonzero entries for the terms baking and bread. the search for relevant doc-
uments is carried out by computing the cosines of the angles (cid:18)j between the query
vector q(1) and the document vectors aj by equation (1). a document is returned
as relevant only if the cosine of the angle it makes with the query vector is greater
than some threshold or cuto(cid:11) value. a practical implementation might use a stringent
cuto(cid:11) like 0:9 [9], but for our small example we use a cosine threshold of 0:5.

for the query q(1), the only nonzero cosines are cos (cid:18)1 = 0:8165 and cos (cid:18)4 =
0:5774. hence, all of the documents concerning baking bread (the (cid:12)rst and fourth)
are returned as relevant. the second, third and (cid:12)fth documents, which concern neither
of these topics, are correctly ignored.

if the user had simply requested books about baking, however, the results would

have been markedly di(cid:11)erent. in this case, the query vector is given by

q(2) = ( 1 0 0 0 0 0 )t ;

and the cosines of the angles between the query and (cid:12)ve document vectors are, in
order, 0:5774, 0, 0, 0:4082, and 0. only the (cid:12)rst document, a book about baking bread,
makes the cosine cuto(cid:11). the fourth document, which is in fact a more comprehensive
reference about baking, is not returned as relevant.

matrices, vector spaces, and information retrieval

7

the t = 6 terms:

t1:
t2:
t3:
t4:
t5:
t6:

bak(e,ing)
recipes
bread
cake
pastr(y,ies)
pie

the d = 5 document titles:

d1: how to bake bread without recipes
d2: the classic art of viennese pastry
d3: numerical recipes: the art of scienti(cid:12)c computing
d4: breads, pastries, pies and cakes : quantity baking recipes
d5: pastry: a book of best french recipes

the 6 (cid:2) 5 term-by-document matrix before id172, where the
element ^aij is the number of times term i appears in document title j:

^a =

0bbbbb@

1 0 0 1 0
1 0 1 1 1
1 0 0 1 0
0 0 0 1 0
0 1 0 1 1
0 0 0 1 0

1ccccca

the 6 (cid:2) 5 term-by-document matrix with unit columns:
0:4082

0

0

a =

0:5774
0:5774
0:5774

0
0
0

0bbbbb@

0
0
0
0

1:0000

0

1:0000 0:4082 0:7071

0
0
0
0

0
0

0:4082
0:4082
0:4082 0:7071
0:4082

0

1ccccca

fig. 2. the construction of a term-by-document matrix a.

the ir community has developed a variety of approaches to respond to such
failures of the basic vector space model. those techniques typically a(cid:11)ect how the data
are represented in the term-by-document matrix. examples include term weighting
schemes, use of a controlled vocabulary (a speci(cid:12)ed set of allowed terms [37]), and
replacing the exact term-by-document matrix by a low-rank approximation to that
matrix. the latter approach is the basis of the method of lsi [9, 19] and of the new

matrices, vector spaces, and information retrieval

8

method described in this paper. in sections 3-6 of this paper, we work through the
process of rank-reduction, (cid:12)rst using the qr factorization and then proceeding to the
perhaps less familiar singular value decomposition used in lsi. in both cases, we use
the factorizations to illustrate the geometry of query matching and to explain what
can constitute a reasonable approximation to the term-by-document matrix. the
latter is important as proponents of lsi argue that the rank-reduction step serves to
remove noise from the database representation.

3. the qr factorization. in this section, we show how the qr factorization
can be used to identify and remove redundant information in the matrix representation
of the database. in id202 terms, we identify the rank of the term-by-document
matrix. this process leads us directly to a geometric interpretation of the vector space
model. in section 4, we show how to lower the rank of the matrix further by removing
components that can be attributed to the natural uncertainties present in any large
database. the rank reduction steps allow us to set portions of the matrix to zero and
thus to ignore them in subsequent computations. doing so lowers the cost of query
matching and helps to recoup some of the expense of computing the factorization.

note that the 6 (cid:2) 5 term-by-document matrix of the example in figure 2 is of
rank four because column (cid:12)ve is the sum of columns two and three. even greater
dependence can be expected in practice: a database of library materials can contain
di(cid:11)erent editions of the same book and a database of internet sites can contain several
mirrors of the same web page. as in our example, dependencies can also involve more
than simple copies of information: binary vectors representing the documents applied
id202 and computer graphics sum to the binary vector representing
id202 applied to computer graphics (where the preposition to is not
considered to be a term), so any database containing all three documents would have
dependencies among its columns.

3.1. identifying a basis for the column space. our (cid:12)rst step in the rank-
reduction process is to identify dependence between the columns or rows of the term-
by-document matrix. for a rank ra matrix, the ra basis vectors of its column space
serve in place of its d column vectors to represent its column space. one set of basis
vectors is found by computing the qr factorization of the term-by-document matrix

a = qr;

where r is a t (cid:2) d upper triangular matrix and q is a t (cid:2) t orthogonal matrix. a
square matrix q is orthogonal if its columns are orthonormal. that is, if qj represents a
j qj = 1,

column of an orthogonal matrix q, it has unit euclidean norm (k qj k2 =qqt
for j = 1; : : : ; t) and it is orthogonal to all other columns of q (qqt

i 6= j.) the rows of q are also orthonormal meaning that qt q = qqt = i.
this factorization exists for any matrix a. see [27] for methods of computing the
qr factorization. the relation a = qr shows that the columns of a are all linear
combinations of the columns of q. thus, a subset of ra of the columns of q form a
basis for the column space of a.

j qi = 0, for all

we now demonstrate how to identify the basis vectors of the example term-by-
document matrix a from figure 2 by using the qr factorization. if a = qr, the
factors are

matrices, vector spaces, and information retrieval

9

(2)

q =

0bbbbbb@

0 (cid:0)0:7071
(cid:0)0:5774
0 (cid:0)0:4082
(cid:0)0:5774
0:8165
0
0
0:0000
(cid:0)0:5774
0 (cid:0)0:4082
0
0:7071
0 (cid:0)0:7071
0
0
0 (cid:0)1:0000
0
0
0 (cid:0)0:7071
0
0

0
0
0
0 (cid:0)0:7071
0
0
0
0:7071

;

1cccccca

(3)

r =

0bbbbbb@

(cid:0)1:0001
0 (cid:0)0:5774 (cid:0)0:7070 (cid:0)0:4082
0 (cid:0)0:4082 (cid:0)0:7071
0 (cid:0)1:0000
0:5774
0
0
0
0 (cid:0)0:5774
0
0
0
0
0
0
0
0
0
0
0
0
0

0:8165

:

1cccccca

in equation (2), we have partitioned the matrix q to separate the (cid:12)rst four column
vectors from the remaining columns. in equation (3), we have partitioned the ma-
trix r to separate the nonzero part from the 2 (cid:2) 5 zero part. we now rewrite the
factorization a = qr as

(4)

a = ( qa q?a )(cid:18) ra
0 (cid:19)
= qara + q?a (cid:1) 0 = qara;

where qa is the 6 (cid:2) 4 matrix holding the (cid:12)rst four columns of q, q?a is the 6 (cid:2) 2
remaining submatrix of q, and ra covers the nonzero rows of r. this partitioning
clearly reveals that the columns of q?a do not contribute to the value of a and that
the ranks of a, r, and ra are equal. thus, the four columns of qa constitute a basis
for the column space of a.

it is important to note here that the clear partitioning of r into zero and nonzero
parts is a feature of the particular matrix a. in general, it is necessary to use column
pivoting during the qr factorization to ensure that the zeros appear at the bottom
of the matrix [27]. when column pivoting is used, the computed factorization is
ap = qr, where p is a permutation matrix. with column pivoting, the (cid:12)rst ra
columns of q form a basis for the column space of the rank ra matrix a, and the
elements of the (cid:12)rst ra rows of r provide the coe(cid:14)cients for the linear combinations of
those basis vectors that constitute the columns of a. in particular, if qa is the t(cid:2) ra
matrix having the basis vectors as columns and if rj represents the jth column of the
matrix r, the jth column of ap is given by the matrix-vector product ap ej = qarj.
the remaining columns of q (the columns of q?a) are a basis for the orthogonal
complement of the column space of ap and so of the column space of a. column
pivoting provides important numerical advantages without changing the database as
permuting the columns of a results only in a reordering of the document vectors.
because they describe the same constructs, we henceforth use the matrix a in place
of ap for clarity of presentation.

the semantic content of a database is fully described by any basis for the column
space of the associated term-by-document matrix, and query matching proceeds with
the factors qr in place of the matrix a. the cosines of the angles (cid:18)j between a query

matrices, vector spaces, and information retrieval

10

vector q and the document vectors aj are then given by

(5)

cos (cid:18)j =

at
j q

k aj k2k q k2

=

(qarj )t q

k qarj k2k q k2

=

rt
j (qt
aq)
k rj k2k q k2

;

for j = 1; : : : ; d: in this relation, we have used the fact that multiplying a vector by
any matrix with orthonormal columns leaves the vector norm unchanged, i.e.,

k qarj k2 =q(qarj )t qarj =qrt

j qt

aqarj =qrt

j irj =qrt

j rj = k rj k2:

we now revisit the example term-by-document matrix from figure 2 using the query
vector q(1) (baking bread) and observe that there is no loss of information from
using its factored form. as expected, the cosines computed via equation (5) are the
same as those computed using equation (1): 0.8165, 0, 0, 0.5774, and 0.

3.2. the geometry of the vector space model. the partitioned represen-
tation of the term-by-document matrix in equation (4) also aids in a geometric in-
terpretation of the query matching procedure. note that, for the orthogonal matrix
q,

i = qqt = ( qa q?a ) ( qa q?a )t = qaqt

a + q?a(q?a)t :

therefore, we can write the query vector q as the sum of its components in the column
space of a and in the orthogonal complement of the column space as follows:

(6)

q = iq = qqt

= [qaqt
= qaqt
= qa + q?a :

a + q?a(q?a)t ]q
aq + q?a(q?a)t q

the column space component qa = qaqt
aq is called the orthogonal projection of q
into the space spanned by the columns of qa. note that qa is in fact the closest
approximation of the query vector q in the column space of a. more precisely,

kq (cid:0) qak2 = minfkq (cid:0) xk2; x from the column space of ag:

the proof relies on the fact that if the vectors qa and x are both in the column space
of a, the vector qa (cid:0) x is also. the vector q (cid:0) qa (cid:17) q?a is orthogonal to any vector in
that space by de(cid:12)nition. using the pythagorean theorem,

kq (cid:0) xk2

2 = kq (cid:0) qa + qa (cid:0) xk2

2 = kq (cid:0) qak2

2 + kqa (cid:0) xk2

2 (cid:21) kq (cid:0) qak2
2:

substituting equation (6) into equation (5) reveals that only the component qa
actually contributes to the dot products used to compute the cosines between the
query and document vectors:

cos (cid:18)j =

j q?a
at
j qa + at
k aj k2k q k2

=

at
j qa + at

j q?a(q?a)t q

k aj k2k q k2

:

because aj is a column of a, it is orthogonal to the columns of q?a which implies that
j q?a = 0 and that the cosine formula simpli(cid:12)es to
at

cos (cid:18)j =

j qa + 0 (cid:1) (q?a)t q
at
k aj k2k q k2

=

at
j qa

k aj k2k q k2

:

matrices, vector spaces, and information retrieval

11

one interpretation of this result is that the user   s imperfect query is automatically
replaced in the dot product computation with its best approximation from the content
of the database. the component q?a which cannot share content with any part of the
column space of a is ignored. if we take that observation one step farther, we can
replace q with its projection altogether and compute a new measure of similarity:

(7)

cos (cid:18)j0 =

at
j qa

k aj k2k qa k2

:

that is, we compare the projection of the user   s query to the document vectors. for
a given index j, the two cosines are related by

(8)

cos (cid:18)j = cos (cid:18)j0k qa k2
k q k2

= cos (cid:18)j0

k qa k2

qk qa k2

2 + k q?a k2

2

:

as the ratio of norms on the right hand side of equation (8) is bounded above by
one, the cosines computed using q are always less than or equal to those computed
using qa. as a result, a query vector nearly orthogonal to the column space of a
is more likely to be judged relevant when using qa than when using q even though
such a vector has only a tiny component in that space. in other words, while use of
equation (7) may help to identify more of the relevant documents, it may also increase
the number of irrelevant ones. in ir terminology, this phenomenon is referred to as
an increase in recall at the risk of reduced precision [37].

4. the low-rank approximation. up to this point, we have used the qr
factorization to explain the geometry of the query matching procedure. in addition,
the qr factorization gives us a means of dealing with uncertainties in the database.
just as measurement errors can lead to uncertainty in experimental data, the very
process of indexing the database can lead to uncertainty in the term-by-document
matrix. a database and its matrix representation may be built up over a long period
of time, by many people with di(cid:11)erent experiences and di(cid:11)erent opinions about how
the database content should be categorized. for instance, in the example of figure 2,
one could argue that the (cid:12)fth document is relevant to baking since it is about pastry
recipes which are simply instructions for baking pastry. under that interpreta-
tion the (unnormalized) term-by-document matrix ^a would have the entry ^a15 = 1.
because the best translation from data to matrix is subject to interpretation, a term-
by-document matrix a might be better represented by a matrix sum a + e, where
the uncertainty matrix e may have any number of values re(cid:13)ecting missing or in-
complete information about documents or even di(cid:11)erent opinions on the relevancy of
documents to certain subjects.

now, if we accept the fact that our matrix a is only one representative of a whole
family of relatively close matrices representing the database, it is reasonable to ask if
it makes sense to attempt to determine its rank exactly [57]. for instance, if we (cid:12)nd
the rank ra and, using id202, conclude that changing a by adding a small
change e would result in a matrix a + e of lesser rank k, then we may as well argue
that our problem has a rank-k matrix representation and that the column space of a
is not necessarily the best represention of the semantic content of the database. next
we show how lowering the rank may help to remove extraneous information or noise
from the matrix representation of the database.

to proceed, we need a notion of the size of a matrix. in particular, we need to be
able to say when a matrix is small in comparison to another matrix. if we generalize

matrices, vector spaces, and information retrieval

12

the euclidean vector norm to matrices the result is the so-called frobenius matrix
norm which is de(cid:12)ned for the real t (cid:2) d matrix x by

kxkf =vuut
txi=1

x2
ij :

dxj=1

(9)

(10)

the frobenius norm can also be de(cid:12)ned in terms of the matrix trace trace(x) which
equals the sum of the diagonal elements of the matrix x t x:

kxkf =qtrace(x t x) =qtrace(xx t ):

using the latter de(cid:12)nition, we show that premultiplying the matrix x by a t (cid:2) t
orthogonal matrix o leaves the frobenius norm unchanged:

koxkf =qtrace((ox)t (ox)) =qtrace(x t ot ox) =qtrace(x t x) = kxkf :

similarly, kxv kf = kxkf for any orthogonal d (cid:2) d matrix v .
our aim is to (cid:12)nd a reasonable low rank approximation to the matrix a. we focus
on the upper triangular matrix r, recalling that the ranks of r and a are equal. while
the rank of a is not generally obvious, the rank of r is easy to determine as it is equal
to the number of nonzero entries on its diagonal. the qr factorization with column
pivoting aids us in manipulating the rank of r as it tends to separate the large and
small parts of the matrix, pushing the larger entries toward the upper left corner of
the matrix and the smaller ones toward the lower right. if this separation is successful,
the matrix r can be partitioned to isolate the small part. for example, the factor r
for our example problem can be partitioned as follows:

r =

0bbbbbb@

0 (cid:0)0:5774 (cid:0)0:7070 (cid:0)0:4082
(cid:0)1:0001
0 (cid:0)0:4082 (cid:0)0:7071
0 (cid:0)1:0000
0:5774
0
0
0
0 (cid:0)0:5774
0
0
0
0
0
0
0
0
0
0
0
0
0

0:8165

r22(cid:19) :
=(cid:18) r11 r12

0

1cccccca

under this partitioning, the submatrix r22 is a relatively small part of the matrix r.
speci(cid:12)cally, k r22 kf =k r kf = 0:5774=2:2361 = 0:2582:
we now create a new upper triangular matrix ~r by setting the small matrix r22
equal to the zero matrix. the new matrix ~r has rank three, as does the matrix
a + e = q ~r. the uncertainty matrix e is then given by the di(cid:11)erence

0 (cid:19) (cid:0) q(cid:18) r11 r12
r22(cid:19)

0

e = (a + e) (cid:0) a

0

0

= q(cid:18) r11 r12
0 (cid:0)r22(cid:19) :
= q(cid:18) 0
0 (cid:0)r22(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)f

0

note that k e kf = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:18) 0

= k r22 kf . because k a kf = k r kf ,
k e kf =k a kf = k r22 kf =k r kf = 0:2582: in words, making a 26% relative change

matrices, vector spaces, and information retrieval

13

in the value of r makes the same sized change in a, and that change reduces the
ranks of both matrices by one. recall that uncertainties of roughly this order may
be introduced simply by disagreement between indexers [29]. thus, we may deem it
acceptable to use the rank-3 approximation a + e in place of the original term-by-
document matrix a for query matching. if we compute the cosines using equation
(5), we need never compute the matrix a + e explicitly but rather can use, from its
qr factors, the (cid:12)rst three columns of q and the triangular matrix ~r which has three
zero rows.

to verify that we have not caused undue loss of accuracy, we return to the example
of figure 2 using the matrix a + e in place of the original term-by-document matrix
a. the cosines computed for query q(1) (baking bread) are 0.8165, 0, 0, 0.7071,
and 0, and the cosines computed for query q2 (baking) are 0.5774, 0, 0, 0.5000, and
0. in both of these cases, the results are actually improved, meaning that our rank-3
approximation a + e appears to be a better representation of our database than is
the original term-by-document matrix a.

to push the rank reduction farther, we repartition the matrix r so that its third
row and column are also included in r22. in this case, k r22 kf =k r kf = 0:5146, and
discarding r22 to create a rank-two approximation of the term-by-document matrix
introduces a 52% relative change in that matrix. the cosines for q (1) are now 0.8165,
0, 0.8165, 0.7071, and 0.4082 and for q2 are 0.5774, 0, 0.5774, 0.5000, and 0.2887.
in both cases, some irrelevant documents are incorrectly identi(cid:12)ed, meaning that the
52% relative change in r and a is unacceptably large.

in general, it is not possible to explain why one variant of the term-by-document
matrix works better than another for a given set of queries. we have seen, however,
that it can be possible to improve the performance of the method by reducing the rank
of the term-by-document matrix. note that even the 26% change that we   ve chosen
as acceptable in our example is quite large in the context of scienti(cid:12)c or engineering
applications where accuracies of three or more decimal places (0.1% error or better)
are typically required.

5. the singular value decomposition. in sections 3 and 4, we show how
to use the qr factorization in the context of a vector space model of information
retrieval. note that while that approach gives us a reduced rank basis for the column
space of the term-by-document matrix, it gives us no such information about its
row space. thus, there is no mechanism for term{term comparison (as described in
section 7 of this paper.) in this section, we introduce an alternate approach, based on
the svd, that, while more expensive computationally than the qr factorization [27],
simultaneously gives us reduced rank approximations to both spaces. furthermore,
the svd has the additional mathematical feature of allowing us to (cid:12)nd a rank-k
approximation to a matrix a with minimal change to that matrix for a given value
of k.

the fundamental mathematical construct underlying this new approach is the
singular value decomposition (svd) of the term-by-document matrix a. that de-
composition is written

a = u (cid:6)v t ;

where u is the t (cid:2) t orthogonal matrix having the left singular vectors of a as its
columns, v is the d(cid:2) d orthogonal matrix having the right singular vectors of a as its
columns, and (cid:6) is the t(cid:2)d diagonal matrix having the singular values (cid:27)1 (cid:21) (cid:27)2 (cid:21) : : : (cid:21)
(cid:27)min(t;d) of a in order along its diagonal. this factorization exists for any matrix a.

matrices, vector spaces, and information retrieval

14

see [27] and the papers cited in section 8.3 of this paper for methods for computing
the svd. figure 3 shows the relative sizes of the matrices u , (cid:6), and v when t > d
and when t < d. all entries not explicitly listed in the singular value matrices are
zero.

the rank ra of the matrix a is equal to the number of nonzero singular values.
it then follows directly from the orthogonal invariance of the frobenius norm that
k a kf is de(cid:12)ned in terms of those values:

(cid:27)2
j :

? ? ?
? ? ?

? ? ?35
}
{z

v t

26664
|

(cid:15)

(cid:15)

(cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3)

= 26664
|

? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?

k a kf = k u (cid:6)v t kf = k (cid:6)v t kf = k (cid:6) kf =vuut
raxj=1
37775
37775
}
}
26664
|

? ? ?35
{z
}

= 24
|

? ? ?
? ? ?

37775
}

26664
|

24
|

35
}

35
}

24
|

{z

{z

{z

{z

{z

(cid:15)

(cid:15)

(cid:15)

(cid:15)

a

a

(cid:6)

(cid:6)

u

u

(cid:3) (cid:3) (cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3) (cid:3) (cid:3)

t > d :

t < d :

24
|

? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?

v t

{z

37775
}

fig. 3. the singular value and singular vector matrices.

there are many parallels between the svd a = u (cid:6)v t and the qr factorization
ap = qr. just as the rank ra of the matrix a equals the number of nonzero diagonal
elements of r, so does it equal the number of nonzero diagonal elements of (cid:6). just
as the (cid:12)rst ra columns of q are a basis for the column space of a, so are the (cid:12)rst ra
columns of u . (in addition, the (cid:12)rst ra rows of v t are a basis for the row space of
a.) just as we created a rank-k approximation to a, where k (cid:20) ra by setting all but
the (cid:12)rst k rows of r equal to zero, so can we create a rank-k approximation ak to
the matrix a by setting all but the k largest singular values of a equal to zero.

a fundamental di(cid:11)erence between the two factorizations is in the theoretical un-
derpinnings of that approximation. more precisely, a classic theorem by eckart and
young [23, 44] states that the distance between a and its rank-k approximations is
minimized by the approximation ak. the theorem further shows how the norm of

matrices, vector spaces, and information retrieval

15

that distance is related to singular values of a. it reads

(11)

ka (cid:0) akkf = min

rank(x)(cid:20)k ka (cid:0) xkf =q(cid:27)2

k+1 + (cid:1)(cid:1)(cid:1) + (cid:27)2
ra:

here ak = uk(cid:6)kv t
k , where uk is the t (cid:2) k matrix whose columns are the (cid:12)rst k
columns of u , vk is the d(cid:2) k matrix whose columns are the (cid:12)rst k columns of v , and
(cid:6)k is the k (cid:2) k diagonal matrix whose diagonal elements are the k largest singular
values of a.
we return now to the example matrix from figure 2. the svd of that matrix is

a = u (cid:6)v t , where

0
0
0
0 (cid:0)0:7071
0
0
0:7071
0

;

1cccccca

u =

0bbbbbb@
0bbbbbb@
v = 0bbbb@

(cid:6) =

0:5308 (cid:0)0:2847 (cid:0)0:7071
0:2670 (cid:0)0:2567
0:7479 (cid:0)0:3981 (cid:0)0:5249
0
0:0816
0:5308 (cid:0)0:2847
0:2670 (cid:0)0:2567
0:7071
0:1182 (cid:0)0:0127
0:6394
0:2774
0:0838 (cid:0)0:1158
0:8423
0:5198
0:1182 (cid:0)0:0127
0:6394
0:2774
0
1:6950
0 1:1158
0
0
0
0

0 0
0 0
0 0
0 0:4195 0
0 0
0
0
0 0

0
0
0 0:8403
0
0
0

;

1cccccca

0:4366 (cid:0)0:4717
0:3688 (cid:0)0:6715
0
0:0998 (cid:0)0:2760 (cid:0)0:5000
0:7549
0:3067
0:4412 (cid:0)0:3568 (cid:0)0:6247
0:1945 (cid:0)0:5000
0:4909 (cid:0)0:0346
0:6571
0
0:5711
0:2815 (cid:0)0:3712 (cid:0)0:0577
0:7071
0:5288

:

1cccca

this rank-4 matrix has four nonzero singular values, and the two zero rows of (cid:6) signal
that the (cid:12)rst four columns of u constitute a basis for the column space of a.

using equation (11), we establish that ka (cid:0) a3kf = (cid:27)4 = 0:4195 and that, since
kakf = 2:2361, ka (cid:0) a3kf =kakf (cid:25) 0:1876. similarly, ka (cid:0) a2kf =kakf (cid:25) 0:4200.
therefore, only a 19% relative change is required to reduce the rank of a from four
to three while it would take a 42% relative change to reduce it from four to two. if
we consider 19% a reasonably small change and 42% too large compared to the initial
uncertainty in our model, then we can accept rank three as the best for our model.

the columns of a3 span a three-dimensional subspace of the column space of a.
that subspace is a good choice because a relatively small change is required to a lower
the rank of a by one (thereby obtaining a3) while a relatively large one is required
to lower its rank one unit farther. in our model, we believe that that subspace and
the corresponding matrix a3 represent the structure of the database well. in other
words, we assume that the true content of the database cannot be easily reduced to
a lower dimensional subspace. thus, in our small example, we are able to identify a
rank (k = 3) that provides a reasonable compromise between accuracy and problem
size. how to choose the rank that provides optimal performance of lsi for any given
database remains an open question and is normally decided via empirical testing [9].
for very large databases, the number of dimensions used usually ranges between 100
and 300 [42], a choice made for computational feasibility as opposed to accuracy. using

matrices, vector spaces, and information retrieval

16

the original term-by-document matrix:

the rank-3 approximation computed using the qr factorization:

a =

~a =

0bbbbb@
0bbbbb@

0:5774
0:5774
0:5774

0
0
0

0
0
0
0

1:0000

0

0

0:4082

0

1:0000 0:4082 0:7071

0
0
0
0

0
0

0:4082
0:4082
0:4082 0:7071
0:4082

0

0:5774
0:5774
0:5774

0
0
0

0
0
0
0

1:0000

0

0

0:4082

0

1:0000 0:4082 0:7071

0
0
0
0

0:4082

0

0
0

0:4082 0:7071

0

0

1ccccca
1ccccca

the rank-3 approximation computed using the svd:

a3 =

0bbbbbb@

0:4971 (cid:0)0:0330
0:0232 0:4867 (cid:0)0:0069
0:0094
0:6003
0:7091
0:9933 0:3858
0:4971 (cid:0)0:0330
0:0232 0:4867 (cid:0)0:0069
0:0740 (cid:0)0:0522 0:2320
0:0155
0:1801
(cid:0)0:0326
0:7043
0:0094 0:4402
0:9866
0:0740 (cid:0)0:0522 0:2320
0:1801
0:0155

1cccccca

fig. 4. the term-by-document matrix a and its two rank-3 approximations.

the svd to (cid:12)nd the approximation ak, however, guarantees that the approximation
is the best we can create for any given choice of k.

as expected, the relative changes of 19% and 42% required to reduce the rank
of the matrix via the svd are less than the corresponding changes of 26% and 52%
required to do it by the qr factorization. keeping these numbers in mind, it is inter-
esting to make a visual comparison of the original term-by-document matrix a and
the two rank-3 approximations. as shown in figure 4, the qr-based approximation
~a looks a lot more like the original matrix than does the more accurate svd-based ap-
proximation a3. these results demonstrate the danger of making assumptions about
accuracy based on appearance.

recall that the original term-by-document matrix a was constructed from term
frequencies, thus all of its entries are nonnegative. the presence of negative elements
in ak is not a problem but rather a re(cid:13)ection of the fact that the entries are linear
combinations of the entries of a. keep in mind that the database content is modeled
by the geometric relationships between the document vectors (columns of ak) not by
the individual components of those vectors.

matrices, vector spaces, and information retrieval

17

6. the reduced-rank vector space model. just as we did for the qr
factorization, we can develop a formulation of query matching based on the svd.
one form of the vector space model that uses the svd is id45
(lsi) as de(cid:12)ned in [9, 19]. in this section, we introduce a new svd-based variant of
the vector space model that follows more directly from the preceding discussion than
does lsi. we compare a query vector q to the columns of the approximation ak to
the term-by-document matrix a. if we de(cid:12)ne ej to be the jth canonical vector of
dimension d (the jth column of the d (cid:2) d identity matrix), the jth column of ak is
given by akej. the cosines of the angles between the query vector q and approximate
document vectors are then computed by

cos (cid:18)j =

(akej)t q

k akej k2 k q k2

=

(uk(cid:6)kv t

k ej)t q

k uk(cid:6)kv t

k ej k2 k q k2

=

for j = 1; : : : ; d. if we de(cid:12)ne the vector sj = (cid:6)kv t

et
j vk(cid:6)k(u t

k q)

;

k (cid:6)kv t

k ej k2 k q k2
k ej, the formula reduces to

(12)

cos (cid:18)j =

st
j (u t

k q)

k sj k2 k q k2

;

j = 1; : : : ; d, and the cosines can be computed without explicitly forming the t (cid:2) d
matrix ak. the norms k sj k2 are computed once for each term-by-document matrix
and subsequently used for all queries.
equation (12) is rich in geometry. the k elements of the vector sj are the coordi-
nates of the jth column of ak in the basis de(cid:12)ned by the columns of uk. in addition,
the k elements of the vector u t
k q are the coordinates in that basis of the projection
uku t
k q of the query vector q into the column space of ak. these observations parallel
those made for the qr factorization in section 3.2 and so imply that an alternative
formula for the comparison could be formed solely from the projected query vector:

(13)

cos (cid:18)j0 =

j (u t
st
k q)
k sj k2 k u t
k q k2

;

j = 1; : : : ; d. in this case, the cosine computation uses only k-dimensional vectors after
the one-time computation of u t
k q. because the query vector q is typically very sparse,
k q is itself low. for all document vectors, cos (cid:18)j0 (cid:21) cos (cid:18)j so
the cost of computing u t
that recall may be improved at the expense of precision if equation (13) is used in
place of equation (12).

just as was the case for the qr factorization, lowering the rank lowers the cost
of query matching. in practice (for lsi), lowering the rank also lowers the cost of the
factorization itself. it is never necessary to compute the full svd of a|it is su(cid:14)cient
to compute only the select singular values and singular vectors that make up (cid:6)k, uk
and vk.

let us now revisit the query q(1) for books about baking bread in the example
of figure 2. using the rank-3 approximation a3 (k = 3) and equation (12), the
cosines are 0.7327, -0.0469, 0.0330, 0.7161, and -0.0097. the (cid:12)rst and fourth books
are still correctly identi(cid:12)ed|this time with nearly equal relevance ratings. the re-
maining cosines are no longer zero but are still tiny with respect to the cuto(cid:11) of 0.5
meaning that no irrelevant books are incorrectly returned. using the second query
vector q(2) about baking and a3 results in the cosines 0.5181, -0.0332, 0.0233, 0.5064,
and -0.0069, so that both books about baking are returned, again with very similar
relevance ratings.

matrices, vector spaces, and information retrieval

18

running the same tests with the rank-2 approximation a2 leads to the cosines
0.5181, -0.1107, 0.5038, 0.3940, and 0.2362 for the query vector q (1) (baking bread).
now the unlikely title numerical recipes is ranked as highly as the appropriate (cid:12)rst
document, but the fourth document (perhaps the best match) does not make the 0.5
cuto(cid:11). the cosines computed for the query vector q(2) (baking) are all less than
0.5. these nonsensical results con(cid:12)rm our suspicion that a2 is not a reasonable
approximation to a.

we note again that, unlike the qr factorization, the svd provides us a mecha-
nism for low rank representation of both the row and column spaces of the term-by-
document matrix a. thus, the svd-based method can be used both for the query-
document comparison just described and for a reduced-rank version of the term{term
comparison that is the subject of section 7.

7. term-term comparison. to this point, we have been concerned with the
vector space model as a mechanism for comparing queries with documents. with
minor variation, the model can also be used to compare terms with terms. when
implemented as part of a search engine, term-term comparison provides a tool to help
re(cid:12)ne the results of a search automatically. an example is presented in figure 5. the
(cid:12)ve titles listed in that (cid:12)gure are the results of a search of a large and diverse collection
of book titles using the single polysemous key word run as the query. the titles re(cid:13)ect
three of the many meanings of the key word. we use term-term comparison to help
focus the result.

to begin, we create a new 7 (cid:2) 5 term-by-document matrix g using the docu-
ment vectors returned in the search. term-term comparison is then carried out by
computing the cosines of the angles !ij between all pairs of term vectors i and j:

(14)

cos !ij =

(et

i g)(gt ej)

k gt ei k2k gt ej k2

;

for i; j = 1; : : : ; 7, where el denotes the lth canonical vector of dimension t (the lth
column of the t (cid:2) t identity matrix.) the cosines are listed in the matrix c where
cij = cos !ij: for clarity of presentation, only the entries in the top half of the
symmetric matrix c are shown.

the entry cij reveals how closely term i is associated with term j. if the entry
is near one, the term vectors for the two terms are nearly parallel and the terms are
closely correlated. in this case, the terms are similarly used across the collection of
documents and so have similar function in describing the semantics of those docu-
ments. geometrically, the vectors occupy nearby locations in the row space of the
term-by-document matrix. if the entry cij is near zero, the vectors are nearly orthog-
onal and the corresponding terms are not related. as expected, the cosine matrix c
shows a nonzero cosine for the angles between the term vector for run and all other
term vectors. it is the cosines of the angles between the remaining term vectors that
are more interesting. they show that the remaining term vectors divide into three
geometrically distinct groups, where the (cid:12)rst group corresponds to terms two through
four, the second to terms (cid:12)ve and six, and the third to term seven alone.

the geometric separation of the term vectors translates into three semantically
independent groups of terms. the terms bike, endurance and training identify
documents about the sport of running while the other two groups of terms are associ-
ated with other meanings of the term run. the process of grouping terms according
to their related content in this way is known as id91 [37]. using these results, an
automated indexing tool can prompt the user to identify which of the three meanings

matrices, vector spaces, and information retrieval

19

the t = 7 terms:

run(ning)
t1:
bike
t2:
endurance
t3:
training
t4:
t5:
band
t6: music
t7:
(cid:12)shes

the d = 5 document titles:

d1: complete triathlon endurance training manual : swim, bike, run
d2: lake, river and sea-run fishes of canada
d3: middle distance running: training and competition
d4: music law: how to run your band   s business
d5: running: learning, training, competing

the 7 (cid:2) 5 term-by-document matrix with unit columns:

0:5000 0:7071 0:7071 0:5774 0:7071
0:5000
0:5000
0:5000

0:7071

0:7071

0
0
0

0
0

0
0

g =

0bbbbbbb@

0
0
0

0
0
0
0
0

0:7071

0
0
0

0:5774
0:5774

0

0
0
0

1ccccccca

cosines of angles between term vectors:

c =

0bbbbbbbb@

1:0000 0:3464 0:3464 0:7746 0:4000 0:4000 0:4899

1:0000 1:0000 0:4472 0
1:0000 0:4472 0
1:0000 0

0
0
0

0
0
0
1:0000 1:0000 0
1:0000 0

1:0000

1cccccccca

fig. 5. identifying polysemy via term comparison.

of run is of interest and so help the user to re(cid:12)ne the search. id91 thus serves
as one mechanism for dealing with polysemy.

it is this application of the vector space model that justi(cid:12)es use of the svd for
the reduced-rank method when ak = uk(cid:6)kv t
k replaces a. recall that the columns
of uk formed a basis for the column space of ak and so that those columns could be

matrices, vector spaces, and information retrieval

20

used in place of the columns of ak for query matching. in the same way, the rows of
vk are a basis for the row space of ak and so can replace the rows of ak in equation
(14). thus, in a reduced rank approximation, the cosine becomes

(et

cos !ij =

i uk(cid:6)kv t

(et
k (cid:6)ku t
for i = 1; : : : ; t and j = 1; : : : ; d. de(cid:12)nining bj = (cid:6)ku t

k )(vk(cid:6)ku t
k ei k2k vk(cid:6)ku t

k ej)
k ej k2

k vk(cid:6)ku t

i uk(cid:6)k)((cid:6)ku t
k ei k2k (cid:6)ku t
k ej, we have

=

k ej)
k ej k2

;

cos !ij =

bt
i bj

k bi k2k bj k2

;

for i = 1; : : : ; t and j = 1; : : : ; d.

in addition to the geometric measure of similarity used in figure 5, techniques
based on graph theoretic concepts (e.g., links, cliques, connected components) are
sometimes used to produce approximate clusters directly from the term frequencies
stored in the original term-by-document matrix. clusters formed by any method may
be merged or split depending on the level of similarities among clusters. id91
information can be used to generate statistical thesauri in which terms are grouped
according to how often they co-occur under the assumption that terms that typically
appear together should be associated with similar concepts. such thesauri are used
for id183 and re(cid:12)nement. see [37] for a more thorough discussion of term
(and document) id91 techniques.

8. what we do to really make ir work. scienti(cid:12)c computing is rarely
implementing vector space
a straightforward application of textbook algorithms.
methods in information retrieval is no di(cid:11)erent.
in this section, we discuss some
techniques used in practice to index and manage large collections of documents using
lsi. they also apply directly to the svd-based method described in section 6. most
of the material in this section is more advanced than that covered in the preceding
sections, so we review only the basic ideas and provide references to more detailed
explanations.

8.1. relevance feedback. an ideal ir system would achieve high precision
for high levels of recall. that is, it would identify all relevant documents without also
returning any irrelevant ones. unfortunately, due to problems such as polysemy and
synonymy (described in section 1), a list of documents retrieved for a given query is
almost never perfect, and the user has to ignore some of the items.

in practice, precision can be improved using relevance feedback [51], i.e., spec-
ifying which documents from a returned set are most relevant to the information
sought and using those documents to clarify the intent of the original query. the
term-term comparison procedure described in section 7 provides one mechanism for
relevance feedback by which the user can improve a query based on term cluster-
ing information. relevance feedback can also be carried out in the column space of
the term-by-document matrix.
in particular, the query can be supplemented with
or replaced by the vector sum of the most relevant documents returned in order to
focus the search nearer to those document vectors. we now provide a mathematical
description of the latter relevance feedback procedure.

because we are concerned only with information contained within the column
if
k q).

space of ak, we assume that the original query vector q lies within that space.
not, we replace the query vector with its projection into that space (q   uku t

matrices, vector spaces, and information retrieval

21

to provide a common ground for the query and document vectors, it is convenient
to describe them both in terms of the same basis for the rank-k approximation to
the column space. we choose the basis given by the columns of uk and then work
directly with the k-dimensional vectors that provide the coordinates of the vectors
in that basis instead of working directly with the much larger t-dimensional query
and document vectors. the coordinates of the query vector are the elements of the
vector u t
k ej of the matrix ak are
the elements of the vector (cid:6)kv t

k q, and the coordinates of the jth column uk(cid:6)kv t

k ej.

suppose that the most relevant result of a user   s search is the single document

aj. the new and improved query is then the sum

qnew = uku t
= uku t
= uk(u t

k q + aj
k q + uk(cid:6)kv t
k q + (cid:6)kv t

k ej
k ej):

(if the document vector aj is not in the column space of ak it should also be replaced
with its projection into that space|aj   uku t
k aj.) if a larger collection of documents
is relevant, the new query can be written as

(15)

qnew = q +

dxj=1

wjaj = uk(u t

k q + (cid:6)kv t

k w);

where the vector element wj is 1 if aj is relevant and 0 otherwise. if the query vector is
replaced by the sum of document vectors, the vector q is replaced by zero in equation
(15). the vector u t
k q was formed in the original cosine computation, so the new
query is formed e(cid:14)ciently via the sum of vectors of dimension k. we can now repeat
the comparison using qnew in place of q. if (as we did for equation (12)) we de(cid:12)ne
the vector sj = (cid:6)kv t

k ej, the cosine formula is now

cos (cid:18)j =

j (u t
st

k qnew)

k sj k2 k qnew k2

;

for j = 1; : : : ; d: there is empirical evidence that replacing the query with a combi-
nation of a few of the most relevant documents returned can markedly improve the
performance of lsi in some cases [20].

8.2. managing dynamic collections. like the weather, databases rarely
stay the same. information is constantly added or removed meaning that catalogues
and indexes become obsolete or incomplete (sometimes in the matter of seconds). for
the lsi model, the most obvious approach to accommodate additions (new terms
or documents) is to recompute the svd of the new term-by-document matrix, but,
for large databases, this procedure is very costly in time and space. less expensive
alternatives, folding-in and svd-updating, have been examined in [9, 46, 55]. the
(cid:12)rst of these procedures is very inexpensive computationally but results in an inexact
representation of the database. it is generally appropriate to fold documents in only
occasionally. updating, while more expensive, preserves (or restores) our representa-
tion of the database. in this section, we brie(cid:13)y review both procedures.

8.2.1. folding-in. folding a new document vector into the column space of an
existing term-by-document matrix amounts to (cid:12)nding coordinates for that document
in the basis uk. the (cid:12)rst step in folding a new t (cid:2) 1 document vector ^p into the

matrices, vector spaces, and information retrieval

22

column space is to project it onto that space. let p represent the projection of ^p,
then, following the discussions in section 6,

(16)

p = uku t

k ^p:

k ^p:

this equation shows that the coordinates of p in the basis uk are given by the elements
of the vector u t

the new document is then folded in by appending the k-dimensional vector u t

k ^p
as a new column of the k (cid:2) d matrix (cid:6)kv t
k . because the latter matrix product is not
actually computed, the folding in is carried out implicitly by appending ^pt uk(cid:6)(cid:0)1
k as
a new row of vk to form a new matrix vk0. the implicit product (cid:6)kvk0 is then the
desired result. note that the matrix vk0 is no longer orthonormal. in addition, the
row space of the matrix vk0t
does not represent the row space of the new term-by-
document matrix. furthermore, if the new document p is nearly orthogonal to the
columns of uk, most information about that document is lost in the projection step.
similarly, to fold in a d (cid:2) 1 term vector ^w whose elements specify the documents
associated with a term, ^w is projected into the row space of ak. let w represent the
term projection of ^w, then

w = vkv t

k ^w:

the coordinates v t
k ^w of the projected vector w are then appended to the matrix uk
as a new row. in this case, the orthogonal representation of the column space of the
term-by-document matrix breaks down [9, 46].

8.2.2. svd-updating. an alternative to folding-in that accounts for the ef-
fects that new terms and documents might have on term-document associations while
still maintaining orthogonality was (cid:12)rst described in [9, 46]. this approach comprises
the following three steps: updating terms, updating documents, and updating term
weights. as pointed out by simon and zha [55], the operations discussed in [9, 46]
may not produce the exact svd of the modi(cid:12)ed reduced-rank lsi model (i.e., ak
from section 4.) those authors provide alternative algorithms for all three steps of
svd-updating, and we now review them. for consistency with our earlier discussion,
we use column pivoting in the qr factorizations although it is not used in [9, 46, 55].

updating terms. suppose that r term vectors are added to an existing lsi
if t is the r (cid:2) d matrix of new term vectors, the new
database of d documents.
term-by-document matrix is formed by appending t to the rows of the rank-k t (cid:2) d
matrix ak = uk(cid:6)kv t

k . the result is the (t + r) (cid:2) d matrix

b =(cid:18) ak
t (cid:19) :

by construction, the rank of b is greater than or equal to the rank k of ak. an
approximation to the svd of b is used to obtain a new rank-k factorization bk =
ub(cid:6)bv t
b re(cid:13)ecting the change in semantic structure of the database caused by the
addition of terms.

the factorization of b proceeds in two stages.

in the (cid:12)rst, we create a block
trapezoidal matrix pre- and post-multiplied by matrices with orthonormal columns
and rows, respectively. in the second, we modify that factorization to produce the
approximate svd of b. to begin, we replace ak by its svd and factor out the

matrices, vector spaces, and information retrieval

23

singular vector matrices:

k

0

t

b = (cid:18) ak
= (cid:18) uk
= (cid:18) uk
= (cid:18) uk

(cid:19)
t (cid:19) =(cid:18) uk(cid:6)kv t
t vk(cid:19) v t
k +(cid:18)
i(cid:19)(cid:20)(cid:18) (cid:6)k
i(cid:19)(cid:18) (cid:6)k
i(cid:19)(cid:18)
i(cid:19) ( vk
i(cid:19)(cid:18) (cid:6)k

t vk

t vk

0

0

0

0

0

0

0

t (i (cid:0) vkv t
v t
k

0

k )(cid:19)(cid:21)
k )(cid:19)

t (i (cid:0) vkv t

(i (cid:0) vkv t

k )t t )t

(17)

at this point, the interior matrix in the matrix product is triangular and the left
exterior matrix is orthogonal, but the right exterior matrix is not. while the columns
of vk are orthonormal, the columns of ~vk = (i (cid:0) vkv t
k )t t are not. it is, however,
the case that the columns of ~vk belong to the orthogonal complement of the column
space of vk. (each column of that matrix is formed by subtracting the projection of
a column of t t into the column space of vk from that column of t t .) the remedy
comes in the form of a variant of the qr factorization. let rv be the rank of ~vk, then
~vk can be factored into the product ~vk(cid:5)v = ^vkrr where (cid:5)v is an r (cid:2) r permutation
matrix, ^vk is a d (cid:2) rv matrix having orthonormal columns, and rr is a rv (cid:2) r upper
trapezoidal matrix.

then, in equation (17),

( vk

(i (cid:0) vkv t

k )t t ) = ( vk

^vkrr(cid:5)t

v )

so that

b = (cid:18) uk
= (cid:18) uk

0

0

t vk

0

i(cid:19)(cid:18) (cid:6)k
i(cid:19)(cid:18) (cid:6)k

0

0

i(cid:19) ( vk

0

^vkrr(cid:5)t

v )t

r (cid:19)(cid:18) v t
k (cid:19) :

k
^v t

t vk (cid:5)v rt

in this factorization, the left matrix is a (t + r) (cid:2) (k + r) matrix with orthonormal
columns, the interior matrix is a (k + r) (cid:2) (k + rv ) block lower trapezoidal matrix,
and the right matrix is a (k + rv ) (cid:2) d matrix having orthonormal rows. (note that
when column pivoting is used, the small elements of rr are pushed toward the lower
right corner. as discussed in section 3, it may be possible to ignore those elements
and thereby reduce the cost of the svd computation in the following step.)

if we de(cid:12)ne the svd of the interior matrix by

^b =(cid:18) (cid:6)k

t vk (cid:5)v rt

0

r (cid:19) = ( pk p ?k )(cid:18) ^(cid:6)k

0

0

^(cid:6)r(cid:19) ( qk q?k )t ;

where pk is a (k + r) (cid:2) k matrix with orthonormal columns, qk is a (k + rv ) (cid:2) k
matrix with orthonormal columns, and ^(cid:6)k is a k (cid:2) k diagonal matrix, then the best
rank-k approximation of b is

(18)

bk =(cid:18) u t

k
0

0

ir(cid:19) pk ^(cid:6)k (( vk

^vk ) qk)t :

using equation (18), the new factors of rank-k approximation of the updated term-
by-document matrix b are

ub =(cid:18) u t

k
0

0

ir(cid:19) pk and vb = ( vk

^vk ) qk;

matrices, vector spaces, and information retrieval

24

respectively, and its singular values are (cid:6)b = ^(cid:6)k.

updating documents. adding s document vectors to a database is similar to
adding r term vectors. let d denote the t (cid:2) s document vectors where t is the
number of terms. the matrix d is appended to the columns of the original ak matrix
so that

b = ( ak d ) ;

where b is a t (cid:2) (d + s) matrix of rank at least k. an approximation to the svd of
b given by bk = ub(cid:6)bv t
b then accounts for changes in the document set [9]. if the
qr factorization of (i (cid:0) uku t

k )d is represented as
(i (cid:0) uku t

k )d(cid:5)u = ^ukrs;

where rs is an s(cid:2) s upper triangular (or trapezoidal) matrix, then it follows [55] that

b = ( ak d ) = ( uk

using the svd of

(cid:18) (cid:6)k u t

k d
0 rs(cid:5)t

u(cid:19) = ( pk p ?k )(cid:18) ^(cid:6)k

0

^uk )(cid:18) (cid:6)k u t

k d
0 rs(cid:5)t

0

k
0

u(cid:19)(cid:18) v t

is(cid:19) :
^(cid:6)s(cid:19) ( qk q?k )t ;

0

where pk and qk are (s + k) (cid:2) k matrices with orthonormal columns and ^(cid:6)k is again
a k (cid:2) k diagonal matrix, the best rank-k approximation of b is given by

(19)

bk = ( uk

:

0

0

^uk ) pk ^(cid:6)k(cid:20)(cid:18) vk
^uk ) pk and vb =(cid:18) vk

is(cid:19) qk(cid:21)t
is(cid:19) qk;

0

0

ub = ( uk

using equation (19), the term and document vectors for the updated lsi model are

respectively, and the updated singular values are the diagonal elements of (cid:6)b = ^(cid:6)k.

updating term weights. the weights assigned to j of the terms can be changed

by computing the matrix sum

b = ak + y z t ;

where the elements of the d (cid:2) j matrix z specify the di(cid:11)erences between the old and
new weights for the j terms and the elements of the a t (cid:2) j matrix y are either 0 or
1 depending on which elements of ak are to be updated [9].
this updating procedure also depends on the qr factorization. in particular, by

substituting the two factorizations

(i (cid:0) uku t
(i (cid:0) vkv t

k )y (cid:5)y = qy ry ;
k )z(cid:5)z = qzrz :

matrices, vector spaces, and information retrieval

25

we can write the updated matrix as the product [55]

0(cid:19) +(cid:18) u t
b = ( uk qy )(cid:20)(cid:18) (cid:6)k
z(cid:19) :
= ( uk qy ) ^b(cid:18) v t

k
qt

0

0

k y
ry (cid:5)t

y (cid:19) ( z t vk (cid:5)zrt

z(cid:19)
z )(cid:21)(cid:18) v t

k
qt

in this product, the left matrix is a t(cid:2) (k + j) matrix with orthonormal columns, the
interior matrix ^b is a rank j update to a (k + j) (cid:2) (k + j) diagonal matrix, and the
right matrix is a (k + j) (cid:2) d matrix having orthonormal rows. (cid:5)y and (cid:5)z are j (cid:2) j
permutation matrices. if the svd of ^b is

^b = ( ^uk

^u?k )(cid:18) ^(cid:6)k

0

0

^(cid:6)j(cid:19) ( ^vk

^v ?k )t ;

the best rank k approximation of the svd of b is found by setting ^(cid:6)j to zero, resulting
in

bk = (( uk qy ) ^uk) ^(cid:6)k(( vk qz ) ^vk)t = ub(cid:6)bv t
b :

the new singular values and singular vectors computed from any of the updating
steps re(cid:13)ect changes in the vector space due to the addition of terms and documents.
svd updating, in one sense, is a dynamic ir model that can accurately re(cid:13)ect the
impact that new or modi(cid:12)ed information can have on a current index of terms and
documents.

8.2.3. downdating. using svd downdating, the lsi model can be modi(cid:12)ed
to re(cid:13)ect the removal of terms and documents and any subsequent changes to term
weights. downdating can be useful for information (cid:12)ltering [24] (e.g., parental screen-
ing of internet sites) and evaluating the importance of a term or document with respect
to forming or breaking clusters of semantically related information. see [12] and [60]
for more details on the e(cid:11)ects of downdating and how it can be implemented.

8.3. sparsity . the sparsity of a term-by-document matrix is a function of the
word usage patterns and topic domain associated with the document collection. the
more new terms each document brings to the global dictionary, the sparser is the
matrix overall. the sample ir matrices studied in [5] are typically no more than 1%
dense, i.e., the ratio of nonzeros to the product of the row and column dimensions
is barely 0:01. experience has shown that these matrices typically lack any regular
nonzero pattern but some recent e(cid:11)orts in the use of both spectral (based on the
eigendecomposition or svd) and non-spectral (usually graph theoretic) approaches
to generate banded or envelope matrix forms are promising [11].

in order to compute the svd of sparse term-by-document matrices, it is important
to store and use only the nonzero elements of the matrix. special matrix storage
formats (e.g., harwell-boeing) have been developed for this purpose (see [3]). special
techniques for computing the svd of a sparse matrix include iterative methods such
as arnoldi [41], lanczos [38, 47], subspace iteration [49, 47], and trace minimization
[53]. all of these methods reference the sparse matrix a only through matrix-vector
multiplication operations, and all can be implemented in terms of the sparse storage
formats.

implementations of the aforementioned methods are available at www.netlib.org.
these include software for arnoldi-based methods (arpack) as discussed in [40, 41]

matrices, vector spaces, and information retrieval

26

and implementations of lanczos, subspace iteration, and trace minimization (svd-
pack (fortran 77) [6] and svdpackc (ansi c) [8]) as discussed in [5]. simple
descriptions of lanczos-based methods with matlab examples are available in [3], and
a good survey of public-domain software for lanczos-type methods is available in [7].
whereas most of the iterative methods mentioned thus far are serial in nature, an in-
teresting asynchronous technique for computing several of the largest singular triplets
of a sparse matrix on a network of workstations is described in [59].

for relatively small order term-by-document matrices, it may be most convenient
to ignore sparsity altogether and consider the matrix a as dense. one fortran li-
brary including the svd of dense matrices is lapack [1]. matlab also provides
a dense svd routine called by [u,sigma,v]=svd(a) if a is stored as a dense ma-
trix or by [u,sigma,v]=svd(full(a)) if a is stored as a sparse matrix. matlab
(version 5.1) also provides a function to compute a few of the largest singular val-
ues and corresponding singular vectors of a sparse matrix. if the k largest singular
values and corresponding left and right singular vectors are required, the matlab call
is [uk,sigmak,vk] = svds(a,k). the sparse svd function svds is based on the
arnoldi methods described in [40]. note that, for practical purposes, less expensive
factorizations such as qr or ulv may su(cid:14)ce in place of the svd [10].

presently, no e(cid:11)ort is made to preserve sparsity in the svd of the sparse term-
by-document matrices. since the singular vector matrices are often dense, the storage
requirements for uk, (cid:6)k and vk can vastly exceed those of the original term-by-
document matrix. for example, a sparse 5; 526 (cid:2) 1; 033 term-by-document matrix a
generated from the medline collection [26] of medical abstracts requires 0:4 mbytes
to store the original matrix, whereas the storage needed for the corresponding single
precision matrices uk,(cid:6)k,vk is 2:6 mbytes when k = 100.

the semi-discrete decomposition (or sdd) [36] provides one means of reducing
the storage requirements of lsi. in sdd, only the three values (cid:0)1; 0; 1 (represented
by two bits each) are used to de(cid:12)ne the elements of uk and vk, and an integer
programming problem is solved to produce the decomposition. another possible way
to remedy the problem of (cid:12)ll is to replace the singular vector matrix with a less
accurate but more compact form.
in particular, we can replace small elements of
the matrices uk and vk with zeros and store the results in sparse formats. in the
medline example, if we replace all entries of less than 0:0025 with zero we reduce
uk from 100% dense to 61% dense. it can be shown that the error in computing the
cosines using the sparser approximation in place of uk is equal to 0:0976 which may
be acceptable in some circumstances.

9. further reading. in addition to the numerous lsi-related journal articles
and technical reports cited in this paper, two recommended sources of background
material on information retrieval systems are the textbooks by frakes and baeza-
yates [25] and by kowalski [37]. both of these books are used in undergraduate
and graduate courses in ir, and both provide good references on the design and
performance of ir systems. while the book by frakes and baeza-yates [25] does
provide some accompanying c software, kowalski   s [37] does not elaborate on the
computational (or software) issues associated with automatic indexing. foundational
concepts in information retrieval are covered in the salton   s book [50]. salton and
mcgill later published a more modern study of information retrieval methods in [52].
certainly more data and information management tutorials and handbooks will
be available in the near future as the need for skills in information-based technologies
continues to grow. we hope that our presentation of the more mathematical side

matrices, vector spaces, and information retrieval

27

of information modeling will spur new interest in computational mathematics and
attract students and faculty to pursue interdisciplinary research in id202 and
information science.

acknowledgments. the authors thank mark anderson, laura mather, and
jim martin for their reviews of a draft of this paper. the website www.amazon.com
was a source of material for the examples in this paper. the numerical results were
computed using matlab.

references

[1] e. anderson, z. bai, c. bischof, j. demmel, j. dongarra, j. du croz, a. greenbaum,
s. hammarling, a. mckenney, s. ostrouchov, and d. sorensen, lapack users   
guide, siam, philadelphia, second ed., 1995.

[2] h. andrews and c. patterson, outer product expansions and their uses in digital image

processing, amer. math. monthly, 82 (1975), pp. 1{13.

[3] r. barrett, m. berry, t. chan, j. demmel, j. donato, j. dongarra, v. eijkhout,
r. pozo, c. romine, and h. van der vorst, templates for the solution of linear
systems: building blocks for iterative methods, siam, philadelphia, 1994.

[4] d. berg, a guide to the oxford english dictionary, oxford university press, oxford, 1993.
[5] m. berry, large scale singular value computations, international journal of supercomputer

applications, 6 (1992), pp. 13{49.

[6]

[7]

, svdpack: a fortran 77 software library for the sparse singular value decomposition,

tech. rep. cs{92{159, university of tennessee, knoxville, tn, june 1992.

, survey of public-domain lanczos-based software, in proceedings of the cornelius lanc-
zos centenary conference, j. brown, m. chu, d. ellison, and r. plemmons, eds., 1997,
pp. 332{334.

[8] m. berry, t. do, g. o   brien, v. krishna, and s. varadhan, svdpackc: version 1.0
user   s guide, tech. rep. cs{93{194, university of tennessee, knoxville, tn, october 1993.
[9] m. berry, s. dumais, and g. o   brien, using id202 for intelligent information re-

trieval, siam review, 37 (1995), pp. 573{595.

[10] m. berry and r. fierro, low-rank orthogonal decompositions for information retrieval ap-

plications, numerical id202 with applications, 3 (1996), pp. 301{328.

[11] m. berry, b. hendrickson, and p. raghavan, sparse matrix reordering schemes for brows-
ing hypertext, in lectures in applied mathematics vol. 32: the mathematics of numerical
analysis, j. renegar, m. shub, and s. smale, eds., american mathematical society, 1996,
pp. 99{123.

[12] m. berry and d. witter, intelligent information management using latent semantic index-

ing, in proceedings of interface   97, interface of north america foundation, 1997.

[13] k. bharat and a. broder, estimating the relative size and overlap of public web search
engines, in 7th international world wide web conference, elsevier science, 1998. paper
fp37.

[14] (cid:23)a. bj(cid:127)orck, numerical methods for least squares problems, siam, philadelphia, 1996.
[15] d. bogard, ed., the bowker annual library and book trade almanac, r.r. bowker co.,

new providence, n.j., 43rd ed., 1998.

[16] books in print, r.r. bowker co., new york, 1997/8.
[17] r. bording, a. gertsztenkorn, l. lines, j. scales, and s. treitel, applications of

seismic travel-time tomography, geophys. j. r. astr. soc., 90 (1987), pp. 285{304.

[18] c. buckley, g. salton, j. allan, and a. singhanl, automatic id183 using
smart: trec 3, in overview of the third text retrieval conference, d. harman, ed.,
national institute of standards and technology special publication 500-226, april 1995.
[19] s. deerwester, s. dumais, g. furnas, t. landauer, and r. harshman, indexing by latent
semantic analysis, journal of the american society for information science, 41 (1990),
pp. 391{407.

[20] s. dumais, improving the retrieval of information from external sources, behavior research

methods, instruments, & computers, 23 (1991), pp. 229{236.

[21]

, lsi meets trec: a status report., in the first text retrieval conference, d. harman,
ed., national institute of standards and technology special publication 500-207, march
1993, pp. 137{152.

[22]

, id45 (lsi) and trec-2, in the second text retrieval confer-

matrices, vector spaces, and information retrieval

28

ence, d. harman, ed., national institute of standards and technology special publication
500-215, march 1994, pp. 105{116.

[23] c. eckart and g. young, the approximation of one matrix by another lower rank, psy-

chometrika, 1 (1936), pp. 211{218.

[24] p. foltz and s. dumais, personalized information delivery: an analysis of information (cid:12)l-

tering methods, communications of the acm, 35 (1992), pp. 51{60.

[25] w. frakes and r. baeza-yates, information retrieval: data structures & algorithms,

prentice-hall, englewood cli(cid:11)s, nj, 1992.

[26] ftp://ftp.cs.cornell.edu/pub/smart/med/. may 27, 1998.
[27] g. golub and c. van loan, matrix computations, johns-hopkins, baltimore, third ed., 1996.
[28] g. h. golub, v. klema, and g. w. stewart, rank degeneracy and least squares problems,
technical report tr-456, department of computer science, university of maryland, 1976.
[29] d. harman, overview of the third text retrieval conference (trec-3), in overview of the
third text retrieval conference, d. harman, ed., national institute of standards and
technology special publication 500-226, april 1995, pp. 1{21.

[30] d. harman and e. voorhees, overview of the (cid:12)fth text retrieval conference (trec-5),
in information technology: the fifth text retrieval conference (trec-5), d. harman
and e. voorhees, eds., national institute of standards and technology special publication
500-238, november 1996, pp. 1{28.

[31] http://lcweb.loc.gov/. april 24, 1998.
[32] http://www.searchenginewatch.com/. march 25, 1998.
[33] http://www.tipster.org. may 28, 1998.
[34] w. jones and g. furnas, pictures of relevance: a geometric analysis of similarity measures,

journal of the american society for information science, 38 (1987), pp. 420{442.

[35] w. kahan, conserving con(cid:13)uence curbs ill-conditioning, technical report 6, computer sci-

ence department, university of california, berkeley, 1972.

[36] t. kolda and d. o   leary, a semi-discrete matrix decomposition for id45
in information retrieval, acm transactions on information systems, (1998). to appear.
[37] g. kowalski, information retrieval systems: theory and implementation, kluwer academic

publishers, boston, 1997.

[38] c. lanczos, an iteration method for the solution of the eigenvalue problem of linear di(cid:11)erential
and integral operators, journal of research of the national bureau of standards, 45 (1950),
pp. 255{282.

[39] s. lawrence and c. giles, searching the world wide web, science, 280 (1998), pp. 98{100.
[40] r. lehoucq, analysis and implementation of an implicitly restarted arnoldi iteration, phd

thesis, rice university, houston, tx, 1995.

[41] r. lehoucq and d. sorensen, de(cid:13)ation techniques for an implicitly restarted arnoldi itera-

tion, siam journal on matrix analysis and applications, 17 (1996), pp. 789{821.

[42] t. letsche and m. berry, large-scale information retrieval with id45,

information sciences, 100 (1997), pp. 105{137.

[43] j. lovins, development of a id30 algorithm, mechanical translation and computational

linguistics, 11 (1968), pp. 22{31.

[44] l. mirsky, symmetric gauge functions and unitarily invariant norms, the quarterly journal

of mathematics, 11 (1960), pp. 50{59.

[45] c. moler and d. morrison, singular value analysis of cryptograms, amer. math. monthly,

90 (1983), pp. 78{87.

[46] g. o   brien, information management tools for updating an svd-encoded indexing scheme,

master   s thesis, university of tennessee, knoxville, tn, 1994.

[47] b. parlett, the symmetric eigenvalue problem, prentice hall, englewood cli(cid:11)s, nj, 1980.
[48] l. rabiner and r. schafer, digital processing of speech signals, prentice hall, englewood

cli(cid:11)s, n.j., (cid:12)rst ed., 1978.

[49] h. rutishauser, simultaneous iteration method for symmetric matrices, numerische mathe-

matik, 16 (1970), pp. 205{223.

[50] g. salton, automatic information organization and retrieval, mcgraw hill, new york, 1968.
[51] g. salton and c. buckley, improving retrieval performance by relevance feedback, journal

of the american society for information science, 41 (1990), pp. 288{297.

[52] g. salton and m. mcgill, introduction to modern information retrieval, mcgraw hill, new

york, 1983.

[53] a. sameh and j. wisniewski, a trace minimization algorithm for the generalized eigenvalue

problem, siam journal on numerical analysis, 19 (1982), pp. 1243{1259.

[54] j. scales, p. dochery, and a. gerszternkorn, id173 of nonlinear inverse prob-
lems: imaging the near-surface weathering layer, inverse problems, 6 (1990), pp. 115{131.

matrices, vector spaces, and information retrieval

29

[55] h. simon and h. zha, on updating problems in id45, tech. rep. cse-97-

011, the pennsylvania state university, 1997.

[56] k. sparck jones, a statistical interpretation of term speci(cid:12)city and its applications in re-

trieval, journal of documentation, 28 (1972), pp. 11{21.

[57] g. stewart, rank degeneracy, siam journal on scienti(cid:12)c and statistical computing, 5 (1984),

pp. 403{413.

[58] ulrich   s international periodicals directory, r.r. bowker co., new york, 1998.
[59] s. varadhan, m. berry, and g. golub, approximating dominant singular triplets of large

sparse matrices via modi(cid:12)ed moments, numerical algorithms, 13 (1996), pp. 123{152.

[60] d. witter, downdating the id45 model for information retrieval, master   s

thesis, university of tennessee, knoxville, tn, 1997.

