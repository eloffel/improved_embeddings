id104 for nlp 

matt lease 

school of information 

university of texas at austin 

ml@ischool.utexas.edu 

see also: o. alonso and m. lease, wsdm 2011 tutorial 

jane saw the man with the binoculars 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

2 

traditional annotation / data collection 

    setup data collection software / harness 
    recruit volunteers (often undergrads) 
    pay a flat fee for experiment or hourly wage 

 

    characteristics 

    slow 
    expensive 
    tedious 
    sample bias 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

3 

id104 

    take a job traditionally 

performed by a known agent 
(often an employee)  

    outsource it to an undefined, 

generally large group of 
people via an open call 

    new application of principles 
from open source movement 
 
february 28, 2011 

matt lease - ml@ischool.utexas.edu 

4 

wisdom of crowds 

requires 
    diversity 
    independence 
    decentralization 
    aggregation 

 

input: large, diverse sample  
          (increases likelihood of overall pool quality) 
output: consensus, selection, distribution 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

5 

community q&a / social search / 

public polling 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

6 

amazon mechanical turk (amt, mturk) 

 

    id104 platform / marketplace 
    on-demand workforce (latency near-real time) 
    went online in 2005 
    progammer   s api &    dashboard    gui 

february 28, 2011 

 

matt lease - ml@ischool.utexas.edu 

7 

human intelligence task (hit) 

 

    specify # of    assignments    (workers) desired 
    set pay for each assignment (+ 10% surcharge) 
8 

matt lease - ml@ischool.utexas.edu 

february 28, 2011 

road map 

    example: annotate pp-attachment 
    id104 & human computation 
    id104 models and worker incentives 
    what   s it good for?     examples 
    id104 human subjects research 
    quality control 
    trends, challenges, and opportunities 

 

    bonus:  who are the workers?     demographics 

 

 

id104 & human computation 

 

other id104 examples 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

11 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

12 

 
 
 
 
 

     http://www.mturk-tracker.com (p. ipeirotis   10) 
 
from 1/09     4/10, 7m hits from 10k requestors 
worth $500,000 usd (assumes only 1 worker/hit) 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

13 

the mechanical turk 

the original, constructed and 
unveiled in 1770 by wolfgang 
von kempelen (1734   1804) 

gavin turk, "the mechanical turk" (2008) 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

14 

artificial artificial intelligence 

j. pontin. artificial intelligence, with help from the humans. ny times (march 25, 2007)  

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

15 

the turing test (alan turing, 1950) 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

16 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

17 

what is a computer? 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

18 

 

princeton university press, 2005 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

19 

davis et al. (2010) the hpu. 

 

hpu 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

20 

the turing test (alan turing, 1950) 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

21 

human computation 

    people become    computists    once more 

    humans do tasks computers cannot (do well) 

    system makes opaque    external call    to the    hpu    

    block robots (captcha        reverse turing test   )   

    collect data 

    typically    stupid parallelism   , minimal post-processing 

    deliver new functionality 

    blend cpu + hpu computation , hpu part of core architecture 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

22 

 

data collection examples 

example     sheep market 

    collection of 10,000 sheep made by workers 
    payment $0.02 to draw a sheep facing left 
 

www.thesheepmarket.com 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

24 

kovashka & lease, crowdconf   10 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

25 

example     dialect identification 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

26 

example     id147 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

27 

a new class of applications 

hybrid applications blend automation with human 

computation to achieve new capabilities  

    s. cooper et al. (2010). predicting protein structures 

with a multiplayer online game.  

    b. bederson et al. (2010). translation by iterative 

collaboration between monolingual users. 

    t. yan et al. (2010). crowdsearch: exploiting crowds for 

accurate real-time image search on mobile phones.  

    m. bernstein et al.  (2010). soylent: a word processor 

with a crowd inside.   

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

29 

models & incentives 

    why do workers do it?  
    how do i crowdsource effectively? 

models & incentives 

    pay (e.g. mturk) 
    fun (or avoid boredom) 
    socialize 
    earn acclaim/prestige 
    altruism 
    learn something new (e.g. english) 
    invisible by-product (e.g. re-captcha) 
    create self-serving resource (e.g. wikipedia)  
 
multiple incentives are often offered in tandem 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

31 

altruism 

    contribute knowledge 
    help others (who need knowledge) 
    help workers (e.g. samasource) 
    charity 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

32 

games with a purpose (l. von ahn) 

    players have fun, creators get data as by-product 

 
 
 

 
 

    distinct from serious gaming / edutainment 

    player learning / training / education is by-product 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

33 

invisible by-product 

l. von ahn et al. (2008). recaptcha    in science. 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

34 

who are 

the 

workers? 

 

    a. baio, november 2008. the faces of mechanical turk. 

    p. ipeitorotis. march 2010. the new demographics of 

mechanical turk 

    j. ross, et al. who are the crowdworkers?... chi 2010. 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

35 

worker demographics 

    2008-2009 studies found 

less global and diverse 
than previously thought 
    us 
    female 
    educated 
    bored 
    money is secondary  

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

36 

2010 shows increasing diversity 

 

  47% us, 34% india, 19% other (p. ipeitorotis. march 2010) 

 
 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

37 

 

human subjects research: 

when workers become participants 

tutorials & examples 

 

    w. mason and s. suri. conducting behavioral research on 

amazon's mechanical turk. (october 12, 2010).  
 

    l. schmidt. id104 for human subjects research. 

crowdconf'10.  
 

    michael d. buhrmester, tracy kwang, and samuel d. 
gosling. amazon's mechanical turk: a new source of 
inexpensive, yet high-quality, data?  ut austin, in press. 
 

    m. buhrmester's amazon mechanical turk guide for social scientists  

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

39 

mturk: no native support for    

    verifying participant demographics 
    randomizing stimuli (eg question order) 
    controlling against same participant joining 

multiple trials 

       waiting room    for gathering multiple subjects 

for multi-participant studies 
    see mason and suri (2010).  

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

40 

institutional review board (irb) 

    what id104 studies fall under irb purvue? 
    how to explain id104 to inexperienced irbs? 

 

two nlp examples (with workers rather than participants) : 
 
1.  yinon bentor, ut austin cs: 

    determination: work falls outside scope of irb 
    no direct contact with study participants  
    no collection of confidential data 

 

 
2. chris callison-burch, johns hopkins university  
 

 

 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

41 

cheap and fast, but is it good? 

 
 
 
 
 
 
 

snow et al. (2008). emnlp 

    5 tasks 

    affect recognition 
    word similarity 
    recognizing id123 
    event temporal ordering 
    id51 

    22k labels for $26 
    high agreement between turk             
annotations and expert    gold    labels  

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

43 

structuring labor / organizing workers 

    flat: all workers exchangeable 
    hierarchical: laymen vs. experts or multi-level 

    s. kochar et al., hcomp   10 

    structured (e.g. find-verify or find-fix-verify) 
    quinn and b. bederson   09, m. bernstein et al.   10 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

44 

consensus 

    majority vote 

    simple, common consensus method 
    what if 2 say yes, 3 say no? 
    flat: collect more judgments   
    hierarchical: use higher-level workers to break ties 

 

    how many labels to collect? 

    fixed n (simple) 
    vary n based on example difficulty and target confidence 

    better consensus methods exist 

    various machine learning questions & work 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

45 

consensus questions 

    how to measure worker quality 

    p. ipeitotis. worker evaluation in id104: gold 

data or multiple workers?   september, 2010 

    balanced vs. imbalanced data (e.g. accuracy vs. p/r) 
    how to estimate worker from sparse data 

    how to use worker quality for better consensus 
    how to maximize labeling effort for learning: 

improve label accuracy or label new examples? 
    v.s. sheng et al. kdd   08, kumar & lease, csdm   11 
    sigir   11 poster (in review)  

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

46 

quality control 

    approach as    overall    quality     not just workers 
    bi-directional channel 

    you may think the worker is doing a bad job. 
    the same worker may think you are a lousy requester 

    assess worker quality as you go 

       trap questions    with known answers (   honey pots   ) 
    measure inner-annotator agreement between workers 

    risk: confusing valid ambiguity or diversity,    tail    behaviors 
    distinguish bias from noise (e.g. personal scales) 

    normalize before aggregating across individuals 

    defend against    spammers    

    common    attacks   : constant, random, majority label  

 
february 28, 2011 
 

matt lease - ml@ischool.utexas.edu 

47 

quality control - mturk 

    approval rate: easy to use, easy to defeat 

    p. ipeirotis. be a top mechanical turk worker: you need $5 and 5 minutes.  (10/10) 

    geographic restrictions (e.g. us only) 
    qualification test 

    pre-screen workers    ability to do task (accurately) 

    difficult with subject judgment tasks 

    can get user familiar with task before you pay 
    may slow down experiment 

    block worker 

    affects worker reputation as well 
    not to be used lightly 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

48 

other issues 

    usability factors (e.g. grady & lease, naacl   10 mturk workshop) 

    provide clear, concise labels that use plain language 

    avoid unfamiliar jargon and terminology 

    may contradict traditional usability (kittur et al. chi   08)   

    hr issues: recruiting, selection, & retention  

    build your reputation (disclose identity?) 

    tweet tasks, design a better qualification test, give bonuses 

    experiments go faster once established 

    always request written feedback from workers  

    often get label justifications (for free / minimal cost) 

    quasi-captcha, though automatic verification may be difficult 

 
february 28, 2011 

matt lease - ml@ischool.utexas.edu 

49 

dealing with bad workers 

    pay for    bad    work instead of rejecting it? 

    pro: preserve reputation, admit if poor design at fault 
    con: promote fraud, undermine approval rating system 

    use bonus as incentive 

    pay the minimum $0.01 and $0.01 for bonus 
    better than rejecting a $0.02 task 

    detect and block spammers 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

50 

other practical tips 

    sign up as worker and do some hits 
       eat your own dog food    

    do it yourself, then have friends do it 

    scale incrementally: first data, then workers 
    monitor discussion forums, address feedback  
    everything counts!  

    overall design only as strong as weakest link 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

51 

 

unreasonable effectiveness of data 

    massive free web data 
changed how we train 
learning systems 
    banko and brill (2001). 
human language tech. 

    halevy et al. (2009). ieee 

intelligent systems. 

 

    how might access to cheap & plentiful labeled data 

change the balance again? 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

53 

modeling uncertainty 

    ai tenet: expose, model, and propagate 
    labeling variance often viewed as    noise    to resolve 

via consensus algorithms (e.g. majority vote)  

    variance may really reflect inherent ambiguities or 

distributional nature of the data 

    while consensus simplifies, could model uncertainty: 

    learning (cf. g. lugosi 1992, p. smyth, 1995) 
    evaluation 
    active learning 

    example informativeness vs. id203 of being labeled correctly 

 

    temporal uncertainty of hpu: yet another dimension    

february 28, 2011 

 

matt lease - ml@ischool.utexas.edu 

54 

crowd wisdom & id108 

    combine multiple models to improve performance 

    can use many weak learners to make a strong one 

    compensate for poor models with extra computation 

    tend to work better when significant diversity 

    using less diverse strong learners better than dumbing-

down models to promote diversity (gashler et al.   08) 

    cf. nips   10 workshop 

    computational social science & the wisdom of crowds 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

55 

mapreduce with human computation 

    commonalities 

    large task divided into smaller sub-problems 
    work distributed among worker nodes (turkers) 
    collect all answers and combine them 
    varying performance of heterogenous cpus/hpus 

    variations 

    human response latency / size of    cluster    
    some tasks are not suitable 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

56 

current mturk limitations 

    no tools for data analysis 
    no integration with databases 
    limited search and browse features 
    limited quality control mechanisms 
    no ratings / recommendations (e.g. books) 
    no work routing: who are the right workers 

given task nature and constraints? 

    human subjects research issues noted earlier   

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

57 

questions 

    usabiliy / human factors / design vs. outcomes? 
    how to price tasks automatically? 
    how to predict worker quality from observable properties? 

 

    what   s the best    mix    of hpu for a task? 
    what are the tasks suitable for hpu? 
    what level of abstraction for invoking hpu? a new language? 

    crowdsource(task,5) 

    how to measure inner-annotator agreement when every   

example labeled by a distinct set of annotators? 

 
 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

58 

wrap-up 

conclusions 

    id104 here to stay 

    shift in practice for conducting research  
    new phenomenon to be studied in its own right 

    fast, cheap, ~ easy, ~ accurate data collection 

    higher level infrastructure still needed to let 

researchers focus on tasks, not platform 

    can collect bad data faster and easier than ever! 

    still need careful experimental design, & effective design for 

new id104 environment 

    a new class of nlp applications will integrate 
automation with hpu to utilize best of both  

       hot    emerging area with many open problems    

february 28, 2011 

 

matt lease - ml@ischool.utexas.edu 

60 

mturk worker forums & resources 

    turker nation: http://turkers.proboards.com 
    http://www.turkalert.com (and its blog) 
    turkopticon: report/avoid shady requestors  
    amazon forum for mturk 
 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

61 

blogs & sites 

 

blogs 
    behind enemy lines (p.g. ipeirotis, nyu) 
    deneme: a mechanical turk experiments blog (gret little, mit)  
    crowdflower blog  
    http://experimentalturk.wordpress.com  
    jeff howe  
 
sites 
    the crowdsortium  
    id104.org 
    daily crowdsource  

 
 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

62 

mturk qa: tools and packages 

    qa infrastructure layers atop mturk promote 

useful separation-of-concerns  from task  
    turkit 

    quik turkit provides nearly realtime services 

    turkit-online (??) 
    get another label (& qmturk) 
    turk surveyor 
    cv-web-annotation-toolkit (image labeling) 
    soylent 
    boto (python library) 

    turkpipe: submit batches of jobs using the command line.  

    more needed    
february 28, 2011 

 

matt lease - ml@ischool.utexas.edu 

63 

past workshops and conferences 

    2011: wsdm-csdm: id104 for search and data mining  
    2010  

    amta: collaborative translation: tech., id104,  & translator perspective  
    coling: 2nd people's web meets nlp: collaboratively constructed     resources  
    crowdconf 2010: 1st conference on the future of distributed work  
    cvpr-acvhl: advancing id161 with humans in the loop  
    icwe: enterprise id104  
    kdd-hcomp: 2nd human computation  
    naacl: creating speech and language data with amazon's mechanical turk  
    nips: computational social science and the wisdom of crowds  
    sigir-cse: id104 for search evaluation  
    ubicomp: ubiquitous id104 
    maryland workshop on id104 and translation 

    2009  

    kdd-hcomp: 1st human computation  
    acl/ijcnlp: 1st people's web meets nlp: collaboratively constructed     resources  

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

64 

resources & upcoming events 

special issue of springer   s information retrieval 
journal on id104 (papers due may 6, 2011) 
 
upcoming conferences & workshops 
    aaai-hcomp (papers due april 22, 2011) 
    chi 2011 workshop (may 8)  
    crowdconf 2011 (tba) 
    sigir 2011 workshop? (in review)  
    trec 2011 id104 track 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

65 

thank you! 
    omar alonso, microsoft bing 
    students 

 

    catherine grady (ischool) 
    hyun joon jung (ece) 
    adriana kovashka (cs) 
    abhimanu kumar (cs) 

    support 

    john p. commons 

 
 
 

ir.ischool.utexas.edu/crowd 

ut mechanical turk & id104 google group 

 

matt lease - ml@ischool.utexas.edu 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

67 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

68 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

69 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

70 

 

february 28, 2011 

matt lease - ml@ischool.utexas.edu 

71 

