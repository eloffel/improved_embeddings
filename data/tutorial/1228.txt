to appear in handbook of natural language processing, second edition, (editors: n. indurkhya and f. j. damerau), 2010 

id31 and subjectivity 

bing liu 

department of computer science 
university of illinois at chicago 

liub@cs.uic.edu 

 
textual  information  in  the  world  can  be  broadly  categorized  into  two  main  types:  facts  and  opinions. 
facts are objective expressions about entities, events and their properties. opinions are usually subjective 
expressions  that  describe  people   s  sentiments,  appraisals  or  feelings  toward  entities,  events  and  their 
properties. the concept of opinion is very broad. in this chapter, we only focus on opinion expressions 
that  convey  people   s  positive  or  negative  sentiments.  much  of  the  existing  research  on  textual 
information processing has been focused on mining and retrieval of factual information, e.g., information 
retrieval, web search, text classification, text id91 and many other id111 and natural language 
processing  tasks.  little  work  had  been  done  on  the  processing  of  opinions  until  only  recently.  yet, 
opinions are so important that whenever we need to make a decision we want to hear others    opinions. 
this is not only true for individuals but also true for organizations.  
one of the main reasons for the lack of study on opinions is the fact that there was little opinionated text 
available before the world wide web. before the web, when an individual needed to make a decision, 
he/she typically asked for opinions from friends and families. when an organization wanted to find the 
opinions or sentiments of the general public about its products and services, it conducted opinion polls, 
surveys,  and  focus  groups.  however,  with  the  web,  especially  with  the  explosive  growth  of  the  user-
generated content on the web in the past few years, the world has been transformed.  
the web has dramatically changed the way that people express their views and opinions. they can now 
post reviews of products at merchant sites and express their views on almost anything in internet forums, 
discussion groups, and blogs, which are collectively called the user-generated content. this online word-
of-mouth  behavior  represents  new  and  measurable  sources  of  information  with  many  practical 
applications. now if one wants to buy a product, he/she is no longer limited to asking his/her friends and 
families because there are many product reviews on the web which give opinions of existing users of the 
product.  for  a  company,  it  may  no  longer  be  necessary  to  conduct  surveys,  organize  focus  groups  or 
employ  external  consultants  in  order  to  find  consumer  opinions  about  its  products  and  those  of  its 
competitors because the user-generated content on the web can already give them such information.  
however, finding opinion sources and monitoring them on the web can still be a formidable task because 
there are a large number of diverse sources, and each source may also have a huge volume of opinionated 
text (text with opinions or sentiments). in many cases, opinions are hidden in long forum posts and blogs. 
it is difficult for a human reader to find relevant sources, extract related sentences with opinions, read 
them,  summarize  them,  and  organize  them  into  usable  forms.  thus,  automated  opinion  discovery  and 
summarization systems are needed. id31, also known as opinion mining, grows out of this 
need. it is a challenging natural language processing or id111 problem. due to its tremendous value 
for  practical  applications,  there  has  been  an  explosive  growth  of  both  research  in  academia  and 
applications in the industry. there are now at least 20-30 companies that offer id31 services 
in usa alone. this chapter introduces this research field. it focuses on the following topics:  
1. the problem of id31: as for any scientific problem, before solving it we need to define 
or to formalize the problem. the formulation will introduce the basic definitions, core concepts and 
issues, sub-problems and target objectives. it also serves as a common framework to unify different 
research directions. from an application point of view, it tells practitioners what the main tasks are, 
their inputs and outputs, and how the resulting outputs may be used in practice.  

2. sentiment  and  subjectivity  classification:  this  is  the  area  that  has  been  researched  the  most  in 
academia. it treats id31 as a text classification problem. two sub-topics that have been 

 

1 

extensively studied are: (1) classifying an opinionated document as expressing a positive or negative 
opinion, and (2) classifying a sentence or a clause of the sentence as subjective or objective, and for a 
subjective sentence or clause classifying it as expressing a positive, negative or neutral opinion. the 
first  topic,  commonly  known  as  sentiment  classification  or  document-level  sentiment  classification, 
aims to find the general sentiment of the author in an opinionated text. for example, given a product 
review, it determines whether the reviewer is positive or negative about the product. the second topic 
goes to individual sentences to determine whether a sentence expresses an opinion or not (often called 
subjectivity classification), and if so, whether the opinion is positive or negative (called sentence-level 
sentiment classification).  

3. feature-based  sentiment  analysis:  this  model  first  discovers  the  targets  on  which  opinions  have 
been  expressed  in  a  sentence,  and  then  determines  whether  the  opinions  are  positive,  negative  or 
neutral.  the  targets  are  objects,  and  their  components,  attributes  and  features.  an  object  can  be  a 
product, service, individual, organization, event, topic, etc. for instance, in a product review sentence, 
it identifies product features that have been commented on by the reviewer and determines whether the 
comments are positive or negative. for example, in the sentence,    the battery life of this camera is too 
short,    the comment is on    battery life    of the camera object and the opinion is negative. many real-
life applications require this level of detailed analysis because in order to make product improvements 
one  needs  to  know  what  components  and/or  features  of  the  product  are  liked  and  disliked  by 
consumers. such information is not discovered by sentiment and subjectivity classification.   

4. id31 of comparative sentences: evaluation of an object can be done in two main ways, 
direct  appraisal  and  comparison.  direct  appraisal,  called  direct  opinion,  gives  positive  or  negative 
opinion about the object without mentioning any other similar objects. comparison means to compare 
the  object  with  some  other  similar  objects  (e.g.,  competing  products).  for  example,     the  picture 
quality of this camera is poor    expresses a direct opinion, while    the picture quality of this camera is 
better than that of camera-x.    expresses a comparison. clearly, it is useful to identify such sentences, 
extract  comparative  opinions  expressed  in  them  and  determine  which  objects  are  preferred  by  the 
sentence authors (in the above example, camera-x is preferred with respect to the picture quality).    

5. opinion search and retrieval: since the general web search has been so successful in many aspects, 
it is not hard to imagine that opinion search will be very useful as well. for example, given a keyword 
query    gay marriage   , one wants to find positive and negative opinions on the issue from an opinion 
search engine. for such a query, two tasks need to be performed: (1) retrieving documents or sentences 
that are relevant to the query, and (2) identifying and ranking opinionated documents or sentences from 
these retrieved. opinion search is thus a combination of information retrieval and id31.    
6. opinion spam and utility of opinions: as opinions on the web are important for many applications, 
it is no surprise that people have started to game the system. opinion spam refers to fake  or bogus 
opinions that try to deliberately mislead readers or automated systems by giving undeserving positive 
opinions to some target objects in order to promote the objects and/or by giving malicious negative 
opinions  to  some  other  objects  in  order  to  damage  their  reputations.  detecting  such  spam  is  very 
important  for  applications.  the  utility  of  opinions  refers  to  the  usefulness  or  quality  of  opinions. 
automatically assigning utility values to opinions is useful as opinions can then be ranked based on 
their utility values. with the ranking, the reader can focus on those quality opinions. we should note, 
however, that spam and utility are very different concepts, as we will see later.  

in  [72],  pang  and  lee  wrote  a  comprehensive  survey  of  the  sentiment  analysis  and  opinion  mining 
research.  this  chapter  is  not  meant  to  be  another  such  survey,  but  instead  to  introduce  the  field  for 
teaching and learning. it focuses on the core topics of the research that are  also essential for practical 
applications. it introduces the topics in sufficient detail so that the reader can have a good understanding 
of the main ideas without referring to the original papers. another key characteristic of this chapter is that 
it  takes  a  structured  approach  to  exploring  the  problem.  in  non-nlp  literature,  natural  language 
documents  are  regarded  as  unstructured  data,  while  the  data  in  relational  databases  are  referred  to  as 
structured data. the structured approach means to turn unstructured text to structured data, which enables 
traditional data management tools to be applied to slice, dice, and visualize the results in many ways. this 

 

2 

is extremely important for applications because it allows the user to gain insights through both qualitative 
and quantitative analysis.  

1.  the problem of id31 
id31 or opinion mining is the computational study of opinions, sentiments and emotions 
expressed in text. we use the following review segment on iphone to introduce the problem (an number 
is associated with each sentence for easy reference): 

   (1) i bought an iphone a few days ago. (2) it was such a nice phone. (3) the touch screen was 
really cool. (4) the voice quality was clear too. (5) although the battery life was not long, that is 
ok for me. (6) however, my mother was mad with me as i did not tell her before i bought it. (7) 
she also thought the phone was too expensive, and wanted me to return it to the shop.          

the question is: what we want to mine or extract from this review? the first thing that we may notice is 
that there are several opinions in this review. sentences (2), (3) and (4) express positive opinions, while 
sentences (5), (6) and (7) express negative opinions or emotions. then we also notice that the opinions all 
have some targets or objects on which the opinions are expressed. the opinion in sentence (2) is on the 
iphone  as  a  whole,  and  the  opinions  in  sentences  (3),  (4)  and  (5)  are  on  the     touch  screen   ,     voice 
quality    and    battery life    of the iphone respectively. the opinion in sentence (7) is on the price of the 
iphone, but the opinion/emotion in sentence (6) is on    me   , not iphone. this is an important point. in an 
application,  the  user  may  be  interested  in  opinions  on  certain  targets  or  objects,  but  not  on  all  (e.g., 
unlikely on    me   ). finally, we may also notice the sources or holders of opinions. the source or holder of 
the opinions in sentences (2), (3), (4) and (5) is the author of the review (   i   ), but in sentences (6) and (7) 
is    my mother   . with this example in mind, we now formally define the id31 or opinion 
mining problem. we start with the opinion target.  
in  general,  opinions  can  be  expressed  on  anything,  e.g.,  a  product,  a  service,  an  individual,  an 
organization,  an  event,  or  a  topic.  we  use  the  term  object  to  denote  the  target  entity  that  has  been 
commented on. an object can have a set of components (or parts) and a set of attributes (or properties). 
each component may have its own sub-components and its set of attributes, and so on. thus, an object 
can be hierarchically decomposed based on the part-of relation. formally, we have the following [55]:  
definition (object): an object o is an entity which can be a product, person, event, organization, or topic. 
it is associated with a pair, o: (t, a), where t is a hierarchy of components (or parts), sub-components, 
and so on, and a is a set of attributes of o. each component has its own set of sub-components and 
attributes.  

example 1: a particular brand of cellular phone is an object. it has a set of components, e.g., battery, and 
screen, and also a set of attributes, e.g., voice quality, size, and weight. the battery component also has 
its set of attributes, e.g., battery life, and battery size.  

based on this definition, an object can be represented as a tree, hierarchy or taxonomy. the root of the 
tree is the object itself. each non-root node is a component or sub-component of the object. each link is a 
part-of  relation.  each  node  is  also  associated  with  a  set  of  attributes  or  properties.  an  opinion  can  be 
expressed on any node and any attribute of the node.  
example 2: following example 1, one can express an opinion on the cellular phone itself (the root node), 
e.g.,     i  do  not  like  this  phone   ,  or  on  one  of  its  attributes,  e.g.,     the  voice  quality  of  this  phone  is 
lousy   .  likewise,  one  can  also  express  an  opinion  on  any  one  of  the  phone   s  components  or  any 
attribute of the component.  

in  practice,  it  is  often  useful  to  simplify  this  definition  due  to  two  reasons:  first,  natural  language 
processing is a difficult task. to effectively study the text at an arbitrary level of detail as described in the 
definition  is  extremely  challenging.  second,  for  an  ordinary  user,  it  is  probably  too  complex  to  use  a 
hierarchical representation of an object and opinions on the object. thus, we flatten the tree to omit the 

 

3 

hierarchy and use the term features to represent both components and attributes. in this simplification, the 
object itself can also be seen as a feature (but a special feature), which is the root of the original tree. an 
opinionated comment on the object itself is called a general opinion on the object (e.g.,    i like iphone   ). 
an opinionated comment on any specific feature is called a specific opinion on a feature of the object, 
e.g.,    the touch screen of iphone is really cool   , where    touch screen    is a feature of iphone.  
using features for an object is quite common in the product domain as people often use the term product 
features.  however,  when  the  objects  are  events  and  topics,  the  term  feature  may  not  sound  natural. 
indeed in some other domains, researchers also use the term topic [46] or aspect [50, 84] to mean feature. 
in this chapter, we choose to use the term feature along with the term object. we should note that both 
terms  are  needed  because  in  most  applications  the  primary  concern  of  the  user  is  a  set  of  objects  of 
interest (e.g., a set of competing products). then we need to know each feature talked about in an opinion 
document belonging to which object. one issue with the term feature is that it can confuse with the term 
feature used in machine learning, where a feature means a data attribute. to avoid the confusion, we will 
use the term object feature to mean feature of an object whenever such confusion may arise.   
let an opinionated document be d, which can be a product review, a forum post or a blog that evaluates a 
set of objects. in the most general case, d consists of a sequence of sentences d =    s1, s2,    , sm   .  
definition (opinion passage on a feature): an opinion passage on a feature f of an object o evaluated in 

d is a group of consecutive sentences in d that expresses a positive or negative opinion on f.  

it is possible that a sequence of sentences (at least one) in an opinionated document together expresses an 
opinion on an object or a feature of the object. it is also possible that a single sentence expresses opinions 
on more than one feature, e.g.,  

   the voice quality of this phone is good, but the battery life is short   . 

much of the current research focuses on sentences, i.e., each passage consisting of a single sentence. in 
the subsequent discussion, we also treat each sentence as the basic information unit.   
definition (explicit and implicit feature): if a feature f or any of its synonyms appears in a sentence s, f 
is called an explicit feature in s. if neither f nor any of its synonyms appear in s but f is implied, then f 
is called an implicit feature in s.  

example 3:    battery life    in the following sentence is an explicit feature: 

   the battery life of this phone is too short   . 
size  is  an  implicit  feature  in  the  following  sentence  as  it  does  not  appear  in  the  sentence  but  it  is 
implied:  

   this phone is too large   . 

here,    large   , which is not a synonym of size, is called a feature indicator. many feature indicators are 
adjectives and adverbs. some adjectives and adverbs are general and can be used to modify anything, 
e.g.,  good,  bad,  and  great,  but  many  actually  indicate  the  types  of  features  that  they  are  likely  to 
modify,  e.g.,  beautiful  (appearance),  and  reliably  (reliability).  thus,  such  feature  indicators  may  be 
directly mapped to their underlying features. we will discuss this again in section 3.1.2.   

definition (opinion holder): the holder of an opinion is the person or organization that expresses the 

opinion.  

opinion holders are also called opinion sources [101]. in the case of product reviews and blogs, opinion 
holders are usually the authors of the posts. opinion holders are more important in news articles because 
they  often  explicitly  state  the  person  or  organization  that  holds  a  particular  opinion  [5,  14,  46].  for 
example, the opinion holder in the sentence    john expressed his disagreement on the treaty    is    john   .  
definition  (opinion):  an  opinion  on  a  feature  f  is  a  positive  or  negative  view,  attitude,  emotion  or 

appraisal on f from an opinion holder. 

 

4 

definition  (opinion  orientation):  the  orientation  of  an  opinion  on  a  feature  f  indicates  whether  the 

opinion is positive, negative or neutral.  

opinion orientation is also known as sentiment orientation, polarity of opinion, or semantic orientation.   
we now put everything together to define a model of an object, a model of an opinionated text, and the 
mining objective, which are collectively called the feature-based id31 model [36, 55, 56].  
model of an object: an object o is represented with a finite set of features, f = {f1, f2,    , fn}, which 
includes the object itself as a special feature. each feature fi     f can be expressed with any one of a 
finite set of words or phrases wi ={wi1, wi2,    , wim}, which are synonyms of the feature, or indicated 
by any one of a finite set of feature indicators ii = {ii1, ii2,    , iiq} of the feature. 

model of an opinionated document: a general opinionated document d contains opinions on a set of 
objects {o1, o2,    , oq} from a set of opinion holders {h1, h2,    , hp}. the opinions on each object oj are 
expressed on a subset fj of features of oj. an opinion can be any one of the following two types:  
1.  direct opinion: a direct opinion is a quintuple (oj, fjk, ooijkl, hi, tl), where oj is an object, fjk is a 
feature of the object oj, ooijkl is the orientation or polarity of the opinion on feature fjk of object oj, 
hi  is  the  opinion  holder  and  tl  is  the  time  when  the  opinion  is  expressed  by  hi.  the  opinion 
orientation ooijkl can be positive, negative or neutral (or measured based on a more granular scale 
to express different strengths of opinions [103]). for feature fjk that opinion holder hi comments 
on, he/she chooses a word or phrase from the corresponding synonym set wjk, or a word or phrase 
from  the  corresponding  feature  indicator  set  ijk  to  describe  the  feature,  and  then  expresses  a 
positive, negative or neutral opinion on the feature. 

2.  comparative opinion: a comparative opinion expresses a relation of similarities or differences 
between two or more objects, and/or object preferences of the opinion holder based on some of the 
shared features of the objects. a comparative opinion is usually expressed using the comparative 
or superlative form of an adjective or adverb, although not always. more detailed discussions will 
be given in section 4. the discussion below focuses only on direct opinions.  

this  opinionated  text  model  covers  the  essential  but  not  all  the  interesting  information  or  all  possible 
cases. for example, it does not cover the situation described in the following sentence:    the view-finder 
and the lens of this camera are too close   , which expresses a negative opinion on the distance of the two 
components. we will follow this simplified model in the rest of this chapter as it is often sufficient for 
practical applications.  
on  direct  opinions,  there  are  in  fact  two  main  sub-types.  in  the  first  sub-type,  opinions  are  directly 
expresses on an object or features of the object, e.g.,    the voice quality of this phone is great.    in the 
second sub-type, opinions on an object are expressed based on its effect on some other objects. this sub-
type often occurs in the medical domain when patients express opinions on drugs or describe their side 
effects. for example, the sentence    after taking this drug, my left knee felt great    describes a desirable 
effect of the drug on the knee, and thus implies a positive opinion on the drug. we call both types direct 
opinions in this chapter for the sake of simplicity and to distinguish them from comparative opinions.    
before  going  further,  let  us  also  have  some  more  discussions  about  the  strength  of  an  opinion  (ooijkl). 
opinions come in different strengths [103]. some are very strong, e.g.,    this phone is a piece of junk    
and some are weak, e.g.,    i think this phone is fine   . hence, the strength of opinions can be interpreted as 
scaled. for example, a positive opinion may express a feeling of contented, happy, joyous, or ecstatic, 
from  the  low  intensity  value  of  contented  to  the  maximally  high  intensity  value  of  ecstatic  [61].  in  a 
practical application, we can choose the number of strength values or levels depending on the application 
need.  for  example,  for  positive  opinions,  we  may  only  need  two  levels,  i.e.,  grouping  contented  and 
happy  into  one  level,  and  joyous  and  ecstatic  into  the  other  level.  this  discussion  in  fact  touches  the 
concept of emotions.  
definition (emotions): emotions are our subjective feelings and thoughts.  

 

5 

emotions  have  been  studied  in  many  fields,  e.g.,  psychology,  philosophy,  sociology,  biology,  etc. 
however, there is still not a set of agreed basic emotions of people among researchers. based on [75], 
people have 6 types of primary emotions, i.e., love, joy, surprise, anger, sadness and fear, which can be 
sub-divided into many secondary and tertiary emotions. each emotion can also have different intensities. 
the strengths of opinions are closely related to the intensities of certain emotions, e.g., joy and anger. 
however,  the  concepts  of  emotions  and  opinions  are  not  equivalent  although  they  have  a  large 
intersection.  
when  discussing  subjective  feelings  of  emotions  or  opinions,  it  is  useful  to  distinguish  two  different 
notions: people   s mental states (or feelings) and language expressions used to describe the mental states. 
although there are only 6 types of emotions, there are a large number of language expressions that can be 
used to express them. similarly, there are also a large (seemly unlimited) number of opinion expressions 
that  describe  positive  or  negative  sentiments.  sentiment  analysis  or  opinion  mining  essentially  tries  to 
infer people   s sentiments based on their language expressions.  
we  now  describe  the  objective  of  sentiment  analysis  or  opinion  mining,  which  not  only  aims  to  infer 
positive  or  negative  opinions/sentiments  from  text,  but  also  to  discover  the  other  pieces  of  associated 
information which are important for practical applications of the opinions.  
objective of mining direct opinions: given an opinionated document d,   

1.  discover all opinion quintuples (oj, fjk, ooijkl, hi, tl) in d, and  
2. 

identify all the synonyms (wjk) and feature indicators ijk of each feature fjk in d.   

some remarks about this feature-based id31 or opinion mining model are as follows: 
1.  it should be stressed that the five pieces of information in the quintuple need to correspond to one 
another. that is, the opinion ooijkl must be given by opinion holder hi on feature fjk of object oj at time 
tl. this requirement gives some clue why id31 is such a challenging problem because 
even identifying each piece of information itself is already very difficult, let alone finding all five and 
match  them.  to  make  matters  worse,  a  sentence  may  not  explicitly  mention  some  pieces  of 
information, but they are implied due to pronouns, language conventions, and the context. let us see 
an  example  blog  (the  number  before  each  sentence  is  added  as  the  sentence  id  to  facilitate  the 
discussion below):  
example 4:    (1) this past saturday, i bought a nokia phone and my girlfriend bought a motorola 
phone. (2) we called each other when we got home. (3) the voice on my phone was not so 
clear, worse than my previous phone. (4) the camera was good. (5) my girlfriend was quite 
happy with her phone. (6) i wanted a phone with good voice quality. (7) so my purchase was a 
real disappointment. (8) i returned the phone yesterday.    

the objects to be discovered in this blog are    motorola phone    and    nokia phone   , which are by no 
means  easy  to  identify  in  practice.  to  figure  out  what  is     my  phone     and  what  is     her  phone     in 
sentences (3) and (5) is even more challenging. sentence (4) does not mention any phone and does 
not  have  a  pronoun.  then  the  question  is  which  phone     the  camera     belongs  to.  sentence  (6) 
seemingly expresses a positive opinion about a phone and its voice quality, but of course that is not 
the case. in sentences (7) and (8), it is hard to know what    my purchase    is and what    the phone    is. 
the opinion holder of all the opinions is the author of the blog except sentence (5) whose opinion 
holder is    my girlfriend.    

2.  in  practice  not  all  five  pieces  of  information  in  the  quintuple  needs  to  be  discovered  for  every 
application  because  some  of  them  may  be  known  or  not  needed.  for  example,  in  the  context  of 
product  reviews,  the  object  (product)  evaluated  in  each  review,  the  time  when  the  review  is 
submitted, and the opinion holder are all known as a review site typically records and displays such 
information.  of  course,  one  still  needs  to  extract  such  information  from  the  web  page,  which  is 
usually a structured data extraction problem (see chapter 9 of [55]).  

 

6 

example 4 above revealed another issue, namely, subjectivity. that is, in a typical document (even an 
opinionated document), some sentences express opinions and some do not. for example, sentences (1), 
(2), (6) and (8) do not express any opinions. the issue of subjectivity has been extensively studied in the 
literature [34, 35, 79, 80, 97, 99, 100, 102, 103, 104].  
definition (sentence subjectivity): an objective sentence expresses some factual information about the 

world, while a subjective sentence expresses some personal feelings or beliefs.  

for example, in example 4, sentences (1), (2) and (8) are objective sentences, while all other sentences 
are subjective sentences. subjective expressions come in many forms, e.g., opinions, allegations, desires, 
beliefs, suspicions, and speculations [79, 97]. thus, a subjective sentence may not contain an opinion. for 
example, sentence (6) in example 4 is subjective but it does not express a positive or negative opinion on 
any specific phone. similarly, we should also note that not every objective sentence contains no opinion 
as the second sentence in example 5 below shows.  
definition  (explicit  and  implicit  opinion):  an  explicit  opinion  on  feature  f  is  an  opinion  explicitly 
expressed on f in a subjective sentence. an implicit opinion on feature f is an opinion on f implied in an 
objective sentence.  

example 5: the following sentence expresses an explicit positive opinion:  

   the voice quality of this phone is amazing.    

the following sentence expresses an implicit negative opinion: 

   the earphone broke in two days.    

although  this  sentence  states  an  objective  fact,  it  implicitly  indicates  a  negative  opinion  on  the 
earphone. in fact, sentence (8) in example 4 can also be said to imply a negative opinion. in general, 
objective sentences that imply positive or negative opinions often state the reasons for the opinions.  

definition  (opinionated  sentence):  an  opinionated  sentence  is  a  sentence  that  expresses  explicit  or 

implicit positive or negative opinions. it can be a subjective or objective sentence.  

as we can see, the concepts of subjective sentences and opinionated sentences are not the same, although 
opinionated sentences are often a subset of subjective sentences. the approaches for identifying them are 
similar. thus for simplicity of presentation, this chapter uses the two terms interchangeably. the task of 
determining whether a sentence is subjective or objective is called subjectivity classification.  
clearly,  the  idea  of  opinionated  can  also  be  applied  to  documents.  so  far  we  have  taken  opinionated 
documents  for  granted  in  the  above  definitions.  in  practice,  they  may  also  need  to  be  identified.  for 
example,  many  forum  posts  are  questions  and  answers  with  no  opinions.  it  is  reasonable  to  say  that 
whether a document is opinionated depends entirely on whether some of its sentences are opinionated. 
thus, we may define a document to be opinionated if any of its sentences is opinionated. this definition, 
however, may not be suitable for all cases. for example, an objective news report may quote someone   s 
opinion. it does not make good sense to say that the report is subjective or opinionated. it is perhaps more 
appropriate to say that the report contains some opinions. a more fair definition may be one that is based 
on the author   s intension, i.e., whether he/she intends to express opinions on something using the text. 
product reviews fit this definition, i.e., they are opinionated. whether a sentence is opinionated or not is 
more clear-cut. in a typical document, some sentences are opinionated and some are not.  
with  the  abstract  model  and  mining  objectives  defined,  we  now  see  how  the  mining  results  may  be 
presented to the user in applications. although this step is not so much of academic research, it is crucial 
to applications. it also gives us some gleams of how an industrial user wants to see the results, which in 
turn also motivates our research. what we discuss below has already been used in the industry.  
to start, we should note that for most opinion based applications, it is important to study a collection of 
opinions rather than only one because one opinion only represents the subjective view of a single person, 

 

7 

 
 

 
 

positive:  125 
negative: 7 

<individual review sentences> 
<individual review sentences> 

cellular phone 1:  
  phone: 
 
 
  feature: voice quality 
 
 
  feature: size 
 
 
      
figure 1. an example of a feature-based summary of opinions. 

<individual review sentences> 
<individual review sentences> 

<individual review sentences> 
<individual review sentences> 

positive:  80       
negative: 12 

positive:  120 
negative: 8 

 
 

positive 

phone  

picture  

battery  

camera 

size 

weight 

negative 

cellular phone 1 

(a) visualization of feature-based summary of opinions on a cellular phone

positive 

phone  

picture  

battery  

camera 

size 

weight 

negative 

cellular phone 1 

cellular phone 2

(b) visual opinion comparison of two cellular phones 

figure 2. visualization of feature-based summaries of opinions  

 

 

which  is  usually  not  significant  for  action.  this  clearly  indicates  that  some  form  of  summary  of  the 
mining  results  is  needed  because  it  does  not  make  sense  to  list  all  quintuples  (opinions)  to  the  user. 
below, we use product reviews as an example to present some ideas.  
recall we mentioned at the beginning of the chapter that we wanted to turn unstructured natural language 
texts  to  structured  data.  the  quintuple  output  does  exactly  that.  all  the  discovered  quintuples  can  be 
easily stored in database tables. a whole suite of database and visualization tools can then be applied to 
view  the  results  in  all  kinds  of  ways  to  gain  insights  of  consumer  opinions,  which  are  usually  called 
structured summaries and are visualized as bar charts and/or pie charts.  
structured opinion summary: a simple way to use the results is to produce a feature-based summary of 

opinions on an object or multiple competing objects [36, 56].  

example  6:  assume  we  summarize  the  reviews  of  a  particular  cellular  phone,  cellular  phone  1.  the 
summary looks like that in figure 1, which was proposed by hu and liu [36]. in the figure,    phone    
represents  the  phone  itself  (the  root  node  of  the  object  hierarchy).  125  reviews  expressed  positive 

 

8 

opinions on the phone and 7 reviews expressed negative opinions on the phone.    voice quality    and 
   size    are two product features. 120 reviews expressed positive opinions on the voice quality, and only 
8 reviews expressed negative opinions. the <individual review sentences> link points to the specific 
sentences and/or the whole reviews that give positive or negative comments about the feature. with 
such a summary, the user can easily see how existing customers feel about the cellular phone. if he/she 
is  interested  in  a  particular  feature,  he/she  can  drill  down  by  following  the  <individual  review 
sentences> link to see why existing customers like it and/or what they are not satisfied with.  
such a summary can also be visualized easily using a bar chart [56]. figure 2(a) shows the summary 
of  opinions  in  a  set  of  reviews  of  a  cellular  phone.  in  the  figure,  each  bar  above  the  x-axis  in  the 
middle shows the number of positive opinions on a feature (given at the top), and the bar below the x-
axis  shows  the  number  of  negative  opinions  on  the  same  feature.  obviously,  other  similar 
visualizations are also possible. for example, we may only show the percent of positive opinions (the 
percent of negative opinions is just one minus the percent of positive opinions) for each feature. to see 
the actual review sentences behind each bar, the bar can be programmed in such a way that clicking on 
the bar will show all the review sentences in a popup window.  
comparing opinion summaries of a few competing products is even more interesting [56]. figure 2(b) 
shows a visual comparison of consumer opinions on two competing phones. we can clearly see how 
consumers view different features of each product. cellular phone 1 is definitely superior to cellular 
phone  2.  most  customers  have  negative  opinions  about  the  voice  quality,  battery  and  camera  of 
cellular phone 2. however, on the same three features, customers are mostly positive about cellular 
phone 1. regarding the size and the weight, customers have similar opinions about both phones. for 
the  phone  itself  (   phone   ),  most  people  are  positive  about  cellular  phone  1,  but  negative  about 
cellular  phone  2.  hence,  the  visualization  enables  users  to  see  how  the  phones  compare  with  each 
other along different feature dimensions.  

clearly,  many  other  types  of  visualizations  are  possible,  see  [72]  for  a  survey  of  other  techniques. 
incidentally, opinion summary of product reviews in microsoft bing search uses a bar chart similar to the 
one in figure 2(a). at the time when this chapter was written, it did not provide the facility for side-by-
side opinion comparison of different products as in figure 2(b).  
in fact, many types of summaries without opinions are also useful. we give some examples below.  
feature buzz summary: this summary shows the relative frequency of feature mentions. it can tell a 
company what their customers really care about. for example, in an online banking study, the most 
mentioned feature may be the transaction security.  

object buzz summary: this summary shows the frequency of mentions of different competing products. 

this is useful because it tells the popularity of different products or brands in the market place. 

since the time of the opinion is recorded in each quintuple, we can easily monitor changes of every aspect 
using trend tracking. 
trend tracking: if the time dimension is added to the above summaries, we get their trend reports. these 
reports can be extremely helpful in practice because the user always wants to know how things change 
over time [94].  

all these summaries can be produced and visualized easily as they are just the results of some database 
queries with no additional mining. this shows the power of the structured output of opinion quintuples.  
finally, we note that researchers have also studied the summarization of opinions in the tradition fashion, 
e.g., producing a short textual summary based on multiple reviews or even a single review [4, 9, 52, 83, 
88]. such a summary gives the reader a quick overview of what people think about a product or service. 
however,  one  weakness  of  such  a  text-based  summary  is  that  it  is  often  not  quantitative  but  only 
qualitative, which is not suitable for analytical purposes, although it may be suitable for human reading. 
for  example,  a  traditional  text  summary  may  say     most  people  do  not  like  this  product   .  however,  a 

 

9 

quantitative summary may say that 60% of the people do not like this product and 40% of them like it. in 
most  opinion  analysis  applications,  the  quantitative  aspect  is  crucial  just  like  in  the  traditional  survey 
research  (in  fact,  reviews  can  be  seen  as  open-ended  surveys).  in  the  survey  research,  structured 
summaries displayed as bar charts and pie charts are the most common approaches because they give the 
user a concise, quantitative and visual view.  
note that instead of generating a text summary directly from input reviews, it is also possible to generate 
a text summary based on the mining results as displayed in figures 1 and 2. for example, it is easy to 
generate some natural language summary sentences based on what is shown on the bar chart using some 
predefined templates. for instance, the first two bars in figure 2(b) can be summarized as    most people 
are positive about cellular phone 1 and negative about cellular phone 2.    

2.  sentiment and subjectivity classification 
we now discuss some key research topics of id31. sentiment classification is perhaps the 
most widely studied topic [3, 6, 8, 12, 13, 15, 16, 18, 27, 28, 33, 34, 35, 44, 45, 62, 64, 66, 67, 68, 70, 71, 
73, 79, 80, 86, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 111]. it classifies an opinionated 
document  (e.g.,  a  product  review)  as  expressing  a  positive  or  negative  opinion.  the  task  is  also 
commonly known as the document-level sentiment classification because it considers the whole document 
as  the  basic  information  unit.  the  existing  research  assumes  that  the  document  is  known  to  be 
opinionated.  naturally  the  same  sentiment  classification  can  also  be  applied  to  individual  sentences. 
however, here each sentence is not assumed to be opinionated in the literature. the task of classifying a 
sentence as opinionated or not opinionated is called subjectivity classification. the resulting opinionated 
sentences  are  also  classified  as  expressing  positive or  negative  opinions,  which  is  called  the sentence-
level sentiment classification.  

2.1 document-level sentiment classification 
given a set of opinionated documents d, it determines whether each document d     d expresses a positive 
or negative opinion (or sentiment) on an object. formally,  
task: given an opinionated document d which comments on an object o, determine the orientation oo of 
the opinion expressed on o, i.e., discover the opinion orientation oo on feature f in the quintuple (o, f, 
so, h, t), where f = o and h, t, o are assumed to be known or irrelevant.  

existing research on sentiment classification makes the following assumption:  
assumption: the opinionated document d (e.g., a product review) expresses opinions on a single object 

o and the opinions are from a single opinion holder h.  

this assumption holds for customer reviews of products and services. however, it may not hold for a 
forum and blog post because in such a post the author may express opinions on multiple products and 
compare them using comparative and superlative sentences.  
most existing techniques for document-level sentiment classification are based on supervised learning, 
although there are also some unsupervised methods. we give an introduction to them below.  

2.1.1   classification based on supervised learning 
sentiment  classification  can  obviously  be  formulated  as  a  supervised  learning  problem  with  two  class 
labels  (positive  and  negative).  training  and  testing  data  used  in  existing  research  are  mostly  product 
reviews, which is not surprising due to the above assumption. since each review at a typical review site 
already  has  a  reviewer-assigned  rating  (e.g.,  1-5  stars),  training  and  testing  data  are  readily  available. 
typically, a review with  4-5 stars is considered a  positive review (thumbs-up), and a review with 1-2 
stars is considered a negative review (thumbs-down).  

 

10 

sentiment classification is similar to but also different from classic topic-based text classification, which 
classifies  documents  into  predefined  topic  classes,  e.g.,  politics,  sciences,  sports,  etc.  in  topic-based 
classification, topic related words are important. however, in sentiment classification, topic-related words 
are  unimportant.  instead,  sentiment  or  opinion  words  that  indicate  positive  or  negative  opinions  are 
important, e.g., great, excellent, amazing, horrible, bad, worst, etc.  
existing  supervised  learning  methods  can  be  readily  applied  to  sentiment  classification,  e.g.,  na  ve 
bayesian, and support vector machines (id166), etc. pang et al. [73] took this approach to classify movie 
reviews into two classes, positive and negative. it was shown that using unigrams (a bag of individual 
words) as features in classification performed well with either na  ve bayesian or id166. neutral reviews 
were not used in this work, which made the problem easier. note that features here are data attributes 
used in machine learning, not object features referred to in the previous section.  
subsequent  research  used  many  more  kinds  of  features  and  techniques  in  learning.  as  most  machine 
learning applications, the main task of sentiment classification is to engineer a suitable set of features. 
some  of  the  example  features  used  in  research  and  possibly  in  practice  are  listed  below.  for  a  more 
comprehensive survey of features used, please refer to [72].   
terms  and  their  frequency:  these  features  are  individual  words  or  word  id165s  and  their  frequency 
counts. in some cases, word positions may also be considered. the tf-idf weighting scheme from 
information retrieval may be applied too. these features are also commonly used in traditional topic-
based text classification. they have been shown quite effective in sentiment classification as well.   

part  of  speech  tags:  it  was  found  in  many  early  researches  that  adjectives  are  important  indicators  of 

subjectivities and opinions. thus, adjectives have been treated as special features. 

opinion  words  and  phrases:  opinion  words  are  words  that  are  commonly  used  to  express  positive  or 
negative  sentiments.  for  example,  beautiful,  wonderful,  good,  and  amazing  are  positive  opinion 
words, and bad, poor, and terrible  are negative opinion words. although many opinion words are 
adjectives and adverbs, nouns (e.g., rubbish, junk, and crap) and verbs (e.g., hate and like) can also 
indicate opinions. apart from individual words, there are also opinion phrases and idioms, e.g., cost 
someone  an  arm  and  a  leg.  opinion  words  and  phrases  are  instrumental  to  sentiment  analysis  for 
obvious reasons. we will discuss them further later in this section.  

syntactic dependency: words dependency based features generated from parsing or dependency trees are 

also tried by several researchers. 

negation:  clearly  negation  words  are  important  because  their  appearances  often  change  the  opinion 
orientation.  for  example,  the  sentence     i  don   t  like  this  camera     is  negative.  however,  negation 
words  must  be  handled  with  care  because  not  all  occurrences  of  such  words  mean  negation.  for 
example,    not    in    not only     but also    does not change the orientation direction. we will discuss 
these issues again in section 3.2.  

apart from classification or prediction of positive or negative sentiments, research has also been done on 
predicting the rating scores (e.g., 1-5 stars) of reviews [71]. in this case, the problem is formulated as a 
regression problem since the rating scores are ordinal. another interesting research direction that has been 
investigated  is  the  transfer  learning  or  domain  adaptation  as  it  has  been  shown  that  sentiment 
classification is highly sensitive to the domain from  which the training data are extracted. a classifier 
trained using opinionated texts from one domain often performs poorly when it is applied or tested on 
opinionated texts from another domain. the reason is that words and even language constructs used in 
different domains for expressing opinions can be substantially different. to make matters worse, the same 
word  in  one  domain  may  mean  positive,  but  in  another  domain  may  mean  negative.  for  example,  as 
observed  in  [95],  the  adjective  unpredictable  may  have  a  negative  orientation  in  a  car  review  (e.g., 
   unpredictable steering   ), but it could have a positive orientation in a movie review (e.g.,    unpredictable 
plot   ). thus, id20 is needed. existing research has used labeled data from one domain and 
unlabeled data from the target domain and general opinion words as features for adaptation [3, 6, 105].    

 

11 

2.1.2  classification based on unsupervised learning 
it  is  not  hard  to  imagine  that  opinion  words  and  phrases  are  the  dominating  indicators  for  sentiment 
classification. thus, using unsupervised learning based on such words and phrases would be quite natural. 
the method in [95] is such a technique. it performs classification based on some fixed syntactic phrases 
that are likely to be used to express opinions. the algorithm consists of three steps:  
step 1: it extracts phrases containing adjectives or adverbs. the reason for doing this is that research has 
shown that adjectives and adverbs are good indicators of subjectivity and opinions. however, although 
an  isolated  adjective  may  indicate  subjectivity,  there  may  be  an  insufficient  context  to determine  its 
opinion orientation. therefore, the algorithm extracts two consecutive words, where one member of the 
pair is an adjective/adverb and the other is a context word. two consecutive words are extracted if their 
pos tags conform to any of the patterns in table 1. for example, the pattern in line 2 means that two 
consecutive words are extracted if the first word is an adverb and the second word is an adjective, but 
the third word (which is not extracted) cannot be a noun. 

first word  
 

table 1. patterns of pos tags for extracting two-word phrases 
 
 
1.   jj  
2.   rb, rbr, or rbs 
3.   jj  
4.   nn or nns  
5.   rb, rbr, or rbs  vb, vbd, vbn, or vbg   anything 

second word  
 
nn or nns  
jj  
jj  
jj  

third word 
(not extracted) 
anything 
not nn nor nns 
not nn nor nns 
not nn nor nns 

example 7: in the sentence,    this camera produces beautiful pictures   ,    beautiful pictures    will be 

extracted as it satisfies the first pattern.  

 

step 2: it estimates the orientation of the extracted phrases using the pointwise mutual information (pmi) 

measure given in equation 1:  

pmi

term
(
1

,

term

2

)

=

log

2

   
      
   

pr(
pr(

term
1
term
)
1

   
pr(

term
2
term

)

2

)

 

   
.
      
   

(1)

here,  pr(term1       term2)  is  the  co-occurrence  id203  of  term1  and  term2,  and  pr(term1)pr(term2) 
gives the id203 that the two terms co-occur if they are statistically independent. the ratio between 
pr(term1       term2)  and  pr(term1)pr(term2)  is  thus  a  measure  of  the  degree  of  statistical  dependence 
between them. the log of this ratio is the amount of information that we acquire about the presence of 
one of the words when we observe the other. 
the  opinion  orientation  (oo)  of  a  phrase  is  computed  based  on  its  association  with  the  positive 
reference word    excellent    and its association with the negative reference word    poor   : 

oo(phrase) = pmi(phrase,    excellent   )     pmi(phrase,    poor   ). 

(2)

the probabilities are calculated by issuing queries to a search engine and collecting the number of hits. 
for each search query, a search engine usually gives the number of relevant documents to the query, 
which is the number of hits. thus, by searching the two terms together and separately, we can estimate 
the probabilities in equation 1. turney [95] used the altavista search engine because it has a near 
operator,  which  constrains  the  search  to  documents  that  contain  the  words  within  ten  words  of  one 
another, in either order. let hits(query) be the number of hits returned. equation 2 can be rewritten as: 

oo

(

phrase

)

=

log

   
      
   

2

hits
hits

(
(

 
phrase
 
phrase

near
" 
near
" 

hits
excellent"
)
)
poor"
hits
excellent"
poor
"(
)

)"

"(

(3)

 

   
.
      
   

 

12 

step 3: given a review, the algorithm computes the average oo of all phrases in the review, and classifies 

the review as recommended if the average oo is positive, not recommended otherwise.  

apart from this method many other unsupervised methods exist. see [16] for another example.  

2.2   sentence-level subjectivity and sentiment classification 
we now move to the sentence-level to perform the similar task [35, 79, 80, 98, 103, 104, 107].  
task: given a sentence s, two sub-tasks are performed: 

1.  subjectivity classification: determine whether s is a subjective sentence or an objective sentence,  
2.  sentence-level  sentiment  classification:  if  s  is  subjective,  determine  whether  it  expresses  a 

positive or negative opinion. 

notice  that  the  quintuple  (o,  f,  oo,  h,  t)  is  not  used  in  defining  the  task  here  because  sentence-level 
classification  is  often  an  intermediate  step.  in  most  applications,  one  needs  to  know  what  object  or 
features of the object the opinions are on. however, the two sub-tasks of the sentence-level classification 
are still very important because (1) it filters out those sentences which contain no opinion, and (2) after 
we  know  what  objects  and  features  of  the  objects  are  talked  about  in  a  sentence,  this  step  helps  to 
determine whether the opinions on the objects and their features are positive or negative.   
most existing researches study both problems, although some of them only focus on one. both problems 
are  classification  problems.  thus,  traditional  supervised  learning  methods  are  again  applicable.  for 
example, one of the early works reported by wiebe et al. [98] performed subjectivity classification using 
the na  ve bayesian classifier. subsequent research also used other learning algorithms [35, 80, 103, 107]. 
one of the bottlenecks in applying supervised learning is the manual effort involved in annotating a large 
number  of  training  examples.  to  save  the  manual  labeling  effort,  a  id64  approach  to  label 
training data automatically is reported in [80, 81]. the algorithm works by first using two high precision 
classifiers (hp-subj and hp-obj) to automatically identify some subjective and objective sentences. the 
high-precision classifiers use lists of lexical items (single words or id165s) that are good subjectivity 
clues. hp-subj classifies a sentence as subjective if it contains two or more strong subjective clues. hp-
obj classifies a sentence as objective if there are no strongly subjective clues. these classifiers will give 
very high precision but low recall. the extracted sentences are then added to the training data to learn 
patterns.  the  patterns  (which  form  the  subjectivity  classifiers  in  the  next  iteration)  are  then  used  to 
automatically identify more subjective and objective sentences, which are then added to the training set, 
and the next iteration of the algorithm begins.  
for  pattern  learning,  a  set  of  syntactic  templates  are  provided  to  restrict  the  kinds  of  patterns  to  be 
learned. some example syntactic templates and example patterns are shown below.  

syntactic template 
<subj> passive-verb  
<subj> active-verb  
active-verb <dobj>  
noun aux <dobj>  
passive-verb prep <np>  

example pattern 
<subj> was satisfied 
<subj> complained 
endorsed <dobj> 
fact is <dobj> 
was worried about <np> 

before discussing algorithms which also perform sentiment classification of subjective sentences, let us 
point out an assumption made in much of the research on the topic. 
assumption of sentence-level sentiment classification: the sentence expresses a single opinion from a 
single opinion holder.  
this assumption is only appropriate for simple sentences with a single opinion, e.g.,    the picture quality 
of this camera is amazing.    however, for compound sentences, a single sentence may express more than 

 

13 

one  opinion.  for  example,  the  sentence,     the  picture  quality  of  this  camera  is  amazing  and  so  is  the 
battery life, but the viewfinder is too small for such a great camera   , expresses both positive and negative 
opinions (one may say that it has a mixed opinion). for    picture quality    and    battery life   , the sentence 
is positive, but for    viewfinder   , it is negative. it is also positive for the camera as a whole.   
in [107], yu and hazivassiloglou reported a study which tries to classify subjective sentences and also 
determine  their  opinion  orientations.  for  subjective  or  opinion  sentence  identification,  it  applied 
supervised  learning.  three  learning  methods  were  evaluated:  sentence  similarity,  na  ve  bayesian 
classification,  and  multiple  na  ve  bayesian  classifiers.  for  sentiment  classification  of  each  identified 
subjective  sentence,  it  used  a  similar  method  to  the  method  in  [95],  but  with  many  more  seed  words 
(rather than only two used in [95]), and the score function was log-likelihood ratio. the same problem is 
studied in [35] considering gradable adjectives. in [28], a semi-supervised learning method is applied, and 
in  [46],  the  decision  is  made  by  simply  summing  up  opinion  words  in  a  sentence.  [47,  48,  49]  build 
models to identify some specific types of opinions in reviews.  
as we mentioned earlier, sentence-level classification is not suitable for compound sentences. wilson et 
al.  [103]  pointed  out  that  not  only  a  single  sentence  may  contain  multiple  opinions,  but  also  both 
subjective and factual  clauses. it is useful to pinpoint such clauses. it is  also important to identify the 
strength of opinions. a study of automatic sentiment classification was presented to classify clauses of 
every sentence by the strength of the opinions being expressed in individual clauses, down to four levels 
deep  (neutral,  low,  medium,  and  high).  the  strength  of  neutral  indicates  the  absence  of  opinion  or 
subjectivity. strength classification thus subsumes the task of classifying language as subjective versus 
objective.  in [104],  the  problem  is  studied  further using  supervised  learning by  considering  contextual 
sentiment influencers such as negation (e.g., not and never) and contrary (e.g., but and however). a list of 
influencers can be found in [76].  
finally, as mentioned in section 1, we should bear in mind that subjective sentences are only a subset of 
opinionated  sentences,  and  many  objective  sentences  can  also  imply  opinions.  thus,  to  mine  opinions 
from text one needs to mine them from both types of sentences.  

2.3   opinion lexicon generation 
in preceding sections, we mentioned that opinion words are employed in many sentiment classification 
tasks. we now discuss how such words are generated. in the research literature, opinion words are also 
known as polar words, opinion-bearing words, and sentiment words. positive opinion words are used to 
express desired states while negative opinion words are used to  express undesired states.  examples of 
positive  opinion  words  are:  beautiful,  wonderful,  good,  and  amazing.  examples  of  negative  opinion 
words  are  bad,  poor,  and  terrible.  apart  from  individual  words,  there  are  also  opinion  phrases  and 
idioms, e.g., cost someone an arm and a leg. collectively, they are called the opinion lexicon. they are 
instrumental for id31 for obvious reasons.  
opinion words can, in fact, be divided into two types, the base type and the comparative type. all the 
examples  above  are  of  the  base  type.  opinion  words  of  the  comparative  type  are  used  to  express 
comparative and superlative opinions. examples of such words are better, worse, best, worst, etc, which 
are  comparative  and  superlative  forms  of  their  base  adjectives  or  adverbs,  e.g.,  good  and  bad.  unlike 
opinion  words  of  the  base  type,  the  words  of  the  comparative  type  do  not  express  a  direction 
opinion/sentiment on an object, but a comparative opinion/sentiment on more than one object, e.g.,    car-
x is better than car-y   . this sentence tells something quite interesting. it does not express an opinion that 
any of the two cars is good or bad. it just says that comparing to car-y, car-x is better, and comparing to 
car-x,  car-y  is  worse.  thus,  although  we  still  can  assign  a  comparative  word  as  positive  or  negative 
based on whether it represents a desirable or undesirable state, we cannot use it in the same way as an 
opinion word of the base type. we will discuss this issue further when we study id31 of 
comparative sentences. this section focuses on opinion words of the base type.  

 

14 

to  compile  or  collect  the  opinion  word  list,  three  main  approaches  have  been  investigated:  manual 
approach,  dictionary-based  approach,  and  corpus-based  approach.  manual  approach  is  very  time-
consuming  [15,  65,  94,  106]  and  thus  it  is  not  usually  used  alone,  but  combined  with  automated 
approaches  as  the  final  check  because  automated  methods  make  mistakes.  below,  we  discuss  the  two 
automated approaches.  
dictionary based approach: one of the simple techniques in this approach is based on id64 
using a small set of seed opinion words and an online dictionary, e.g., id138 [25]. the strategy is to 
first collect a small set of opinion words manually with known orientations, and then to grow this set by 
searching in the id138 for their synonyms and antonyms. the newly found words are added to the 
seed list. the next iteration starts. the iterative process stops when no more new words are found. this 
approach is used in [36, 46]. after the process completes, manual inspection can be carried out to remove 
and/or correct errors. researchers have also used additional information (e.g., glosses) in id138 and 
additional techniques (e.g., machine learning) to generate better lists [1, 21, 22, 24, 43]. so far, several 
opinion word lists have been generated [19, 23, 36, 87, 98].  
the dictionary based approach and the opinion words collected from it have a major shortcoming. the 
approach is unable to find opinion words with domain specific orientations, which is quite common. for 
example, for a speakerphone, if it is quiet, it is usually negative. however, for a car, if it is quiet, it is 
positive. the corpus-based approach can help deal with this problem. 
corpus-based approach and sentiment consistency: the methods in the corpus-based approach rely on 
syntactic or co-occurrence patterns and also a seed list of opinion words to find other opinion words in a 
large  corpus.  one  of  the  key  ideas  is  the  one  proposed  by  hazivassiloglou  and  mckeown  [34].  the 
technique  starts  with  a  list  of  seed  opinion  adjective  words,  and  uses  them  and  a  set  of  linguistic 
constraints  or  conventions  on  connectives  to  identify  additional  adjective  opinion  words  and  their 
orientations.  one  of  the  constraints  is  about  conjunction  (and),  which  says  that  conjoined  adjectives 
usually have the same orientation. for example, in the sentence,    this car is beautiful and spacious,    if 
   beautiful    is known to be positive, it can be inferred that    spacious    is also positive. this is so because 
people usually express the same opinion on both sides of a conjunction. the following sentence is rather 
unnatural,     this  car  is  beautiful  and  difficult  to  drive   .  if  it  is  changed  to     this  car  is  beautiful  but 
difficult to drive   , it becomes acceptable. rules or  constraints are also designed for other  connectives, 
or,  but,  either-or,  and  neither-nor.  we  call  this  idea  sentiment  consistency.  of  course,  in 
practice it is not always consistent. learning using the log-linear model is applied to a large corpus to 
determine  if  two  conjoined  adjectives  are  of  the  same  or  different  orientations.  same  and  different-
orientation  links  between  adjectives  forms  a  graph.  finally,  id91  is  performed  on  the  graph  to 
produce  two  sets  of  words:  positive  and  negative.  in  [44],  kanayama  and  nasukawa  expanded  this 
approach  by  introducing  the  idea  of  intra-sentential  (within  a  sentence)  and  inter-sentential  (between 
neighboring sentences) sentiment consistency (called coherency in [44]). the intra-sentential consistency 
is similar to that in [34]. inter-sentential consistency applies the idea to neighboring sentences. that is, 
the same opinion orientation (positive or negative) is usually expressed in a few consecutive sentences. 
opinion  changes  are  indicated  by  adversative  expressions  such  as  but  and  however.  some  criteria  to 
determine whether to add a word to the positive or negative lexicon are also proposed. this study was 
based on japanese text. other related work include [42, 100].  
in  [78],  qiu  et  al  proposed  another  method  to  extract  domain  specific  sentiment  words  from  reviews 
using also some seed opinion words. the  main idea is to exploit certain syntactic relations of opinion 
words and object features for extraction. they showed that opinion words are almost always associated 
with  object  features  in  some  ways.  thus,  opinion words  can  be  recognized  by  identified  features,  and 
features  can  be  identified by  known  opinion  words  (no  seed  feature  is  needed).  the  extracted  opinion 
words and features are utilized to identify new opinion words and new features, which are used again to 
extract more opinion words and features. this propagation or id64 process ends when no more 
opinion words or features can be found. as the process involves propagation through both opinion words 
and  features,  the  method  is  called  double  propagation.  the  extraction  rules  are  designed  based  on 

 

15 

different relations between opinion words and features, and also opinion words and features themselves. 
dependency grammar [91] was adopted to describe these relations.  
using the corpus-based approach alone to identify all opinion words, however, is not as effective as the 
dictionary-based  approach  because  it  is  hard  to  prepare  a  huge  corpus  to  cover  all  english  words. 
however,  as  we  mentioned  above,  this  approach  has  a  major  advantage  that  the  dictionary-based 
approach does not have. it can help find domain specific opinion words and their orientations if a corpus 
from only the specific domain is used in the discovery process.  
in  [19],  ding  and  liu  explores  the  idea  of  intra-sentential  and  inter-sentential  sentiment  consistency 
further. instead of finding domain dependent opinion words, they showed that the same word might have 
different orientations in different contexts even in the same domain. for example, in the digital camera 
domain,  the  word  long  expresses  different  opinions  in  the  two  sentences:     the  battery  life  is  long    
(positive)  and     the  time  taken  to  focus  is  long     (negative).  thus,  finding  domain  dependent  opinion 
words  is  still  insufficient.  they  then  proposed  to  consider  both  opinion  words  and  object  features 
together,  and  use  the  pair  (object_feature,  opinion_word)  as  the  opinion  context.  their  method  thus 
determines opinion words and their orientations together with the object features that they modify. the 
above rules about connectives were still applied. the work in [29] adopts the same context definition but 
used  it  for  sentiment  analysis  of  comparative  sentences.  in  fact,  the  method  in  [90,  95]  can  also  be 
considered  as  a  method  for  finding  context  specific  opinions.  however,  it  does  not  use  the  sentiment 
consistency idea. its opinion context is based on syntactic pos patterns rather than object features and 
opinion words that modify them. in [8], breck et al. went further to study the problem of extracting any 
opinion  expressions,  which  can  have  any  number  of  words.  the  conditional  random  fields  (crf) 
method [54] was used as the sequence learning technique for extraction.  
finally, we should note that populating an opinion lexicon (domain dependent or not) is different from 
determining whether a word or phrase is actually expressing an opinion and what its orientation is in a 
particular sentence. just because a word or phrase is listed in an opinion lexicon does not mean that it 
actually is expressing an opinion in a sentence. for example, in the sentence,    i am looking for a good 
health insurance for my family,       good    here does not express either a positive or negative opinion on 
any particular insurance. and the same is true for polarity/opinion orientation. we should also realize that 
opinion words and phrases are not the only expressions that bear opinions. there are many others as we 
will see in section 3.2 when we discuss rules of opinions.  

3.   feature-based id31  
although classifying opinionated texts at the document level or at the sentence level is useful in many 
cases, they do not provide the necessary detail needed for some other applications. a positive opinionated 
document  on  a  particular  object  does  not  mean  that  the  author  has  positive  opinions  on  all  aspects  or 
features of the object. likewise, a negative opinionated document does not mean that the author dislikes 
everything.  in  a  typical  opinionated  text,  the  author  writes  both  positive  and  negative  aspects  of  the 
object,  although  the  general  sentiment  on  the  object  may  be  positive  or  negative.  document-level  and 
sentence-level classification does not provide such information. to obtain such details, we need to go to 
the object feature level, which means we need the full model of section 1. recall, at the feature level, the 
mining  task  is  to  discover  every  quintuple  (oj,  fjk,  ooijkl,  hi,  tl)  and  identify  all  the  synonyms  (wjk)  and 
feature indicators ijk of feature fjk. in this section, we mainly focus on two key mining tasks:  
1.  identify  object  features  that  have  been  commented  on.  for  instance,  in  the  sentence,     the  picture 

quality of this camera is amazing,    the object feature is    picture quality   .  

2.  determine  whether  the  opinions  on  the  features  are  positive,  negative  or  neutral.  in  the  above 

sentence, the opinion on the feature    picture quality    is positive.  

opinion holder, object and time extraction: in some applications, it is useful to identify and extract 
opinion holders, i.e., persons or organizations that expressed certain opinions. as we mentioned earlier, 

 

16 

opinion  holders  are  more  useful  for  news  articles  or  other  types  of  formal  documents,  in  which  the 
persons or organizations who expressed opinions are stated explicitly in the text. such holders need to be 
identified by the system [5, 14, 46]. in the case of the user-generated content on the web, the opinion 
holders  are  often  the  authors  of  discussion  posts,  bloggers,  or  reviewers,  whose  login  ids  are  known 
although their true identities in the real world may be unknown.  
however,  object  name  extraction  is  needed  for  discussion  posts,  blogs  and  also  reviews.  note  that 
although  a  review  focuses  on  evaluating  a  particular  object,  it  may  compare  it  with  other  competing 
objects. time extraction is also needed in the web context. since each web site usually displays the time 
when every post is submitted. so, the extraction is easy. however, in news and other types of documents 
time  extraction  is  also  an  issue.  all  these  three  extraction  tasks  are  collectively  known  as  the  named 
entity recognition (ner) in the information extraction community. they have been studied extensively. 
see  a  comprehensive  survey  of  information  extraction  tasks  and  algorithms  in  [82].  there  is  also  a 
chapter in this book on information extraction.  
coreference resolution: in product reviews, the reviewed objects are usually known. however, this is 
not the case  for opinions expressed in blogs and discussion posts. for example, in the post,    i have a 
canon  s50  camera  purchased  from  amazon.  it  takes  great  photos,     two  interesting  questions  can  be 
asked: (1) what object does the post praise and (2) what    it    means in the second sentence? clearly, we 
humans  know  that  the  post  praises     canon  s50  camera   ,  which  is  the  problem  of  object  extraction 
discussed above, and we also know that    it    here means    canon s50 camera   , which is the problem of 
coreference resolution. coreference resolution has been studied extensively in nlp. however, it is still a 
major challenge. we will not discuss it here. a study of the problem in the id31 context is 
reported in [88].  
in the next two subsections, we focus on the two tasks listed above.  

3.1  feature extraction 
current research on object feature extraction is mainly carried out in online product reviews. we thus also 
focus on such reviews here. there are two common review formats on the web. different formats may 
need different techniques to perform the feature extraction task [55, 56].  
format  1       pros,  cons  and  the  detailed  review:  the  reviewer  is  asked  to  describe  pros  and  cons 
separately and also write a detailed/full review. an example of such a review is given in figure 3.    
format  2       free  format:  the  reviewer  can  write  freely,  i.e.,  no  separation  of  pros  and  cons.  an 

example of such a review is given in figure 4. 

3.1.1  feature extraction from pros and cons of format 1 
we describe a supervised pattern learning approach to extracting product features from pros and cons in 
reviews of format 1 (not the detailed review, which is the same as that in format 2). the key observation 
is  that  pros  and  cons  are  usually  very  brief,  consisting  of  short  phrases  or  sentence  segments.  each 
sentence segment contains only one feature, and sentence segments are separated by commas, periods, 
semi-colons, hyphens, &, and, but, etc.  
example 8: pros in figure 3 can be separated into three segments:  

great photos  
easy to use    
very small 

   photo    
   use    
   small            size   . 

cons in figure 3 can be separated into two segments: 

battery usage 
included memory is stingy  

   battery    
   memory    

 

 

17 

my slr is on the shelf 
by camerafun4. aug 09    04 
pros: great photos, easy to use, very small 
cons: battery usage; included memory is stingy. 
i had never used a digital camera prior to purchasing this canon a70. i have always used a slr     read the 
full review 

figure 3. an example review of format 1 

great camera., jun 3, 2004  
reviewer: jprice174 from atlanta, ga. 
i did a lot of research last year before i bought this camera... it kinda hurt to leave behind my beloved nikon 
35mm slr, but i was going to italy, and i needed something smaller, and digital.  
the pictures coming out of this camera are amazing. the 'auto' feature takes great pictures most of the time. 
and with digital, you're not wasting film if the picture doesn't come out.     

figure 4. an example review of format 2 

we can see that each segment describes a product feature, which is given within        . notice that    small    is 
a feature indicator for feature    size   . clearly, there are many methods that can be used to extract features, 
e.g., id49 (crf) [54]. here, we describe a sequential rule based method [56].   
the rules are called label sequential rules (lsr), which are generated from sequential patterns in data 
mining. a label sequential rule (lsr) is of the following form, x     y, where y is a sequence and x is a 
sequence produced from y by replacing some of its items with wildcards. a wildcard, denoted by a    *   , 
can match any item.  
the learning process is as follows: each segment is first converted to a sequence. each sequence element 
is a word, which is represented by both the word itself and its pos tag in a set. in the training data, all 
object  features  are  manually  labeled  and  replaced  by  the  label  $feature.  an  object  feature  can  be 
expressed with a noun, adjective, verb or adverb. thus, they represent both explicit features and implicit 
feature indicators. the labels and their pos tags used in mining lsrs are: {$feature, nn}, {$feature, jj}, 
{$feature, vb} and {$feature, rb}, where $feature denotes a feature to be extracted, and nn stands for 
noun, vb for verb, jj for adjective, and rb for adverb. note that to simplify the presentation, we use nn 
and vb to represent all forms of nouns and verbs respectively.  
for example, the sentence segment,    included memory is stingy   , is turned into the sequence  

   {included, vb}{memory, nn}{is, vb}{stingy, jj}   .  

after labeling, it becomes (note that    memory    is an object feature):  

 

   {included, vb}{$feature, nn}{is, vb}{stingy, jj}   , 

 

   {easy, jj }{to}{*, vb}           {easy, jj }{to}{$feature, vb}     

all the resulting sequences are then used to mine lsrs. an example rule is:  
 
where the confidence is the id155, pr(y | x), which measures the accuracy of the rule.  
feature extraction is performed by matching the patterns with each sentence segment in a new review to 
extract object features. that is, the word in the sentence segment that matches $feature in a pattern is 
extracted. in the pattern match, only the right-hand side of each rule is used. in rule generation, both the 
right- and the left-hand sides are needed to compute the id155 or confidence. details of 
sequential pattern mining and lsr mining can be found in [55].  
 

confidence = 90% 

 

18 

3.1.2   feature extraction from reviews of format 2  
pros  and  cons  of  format  1  mainly  consist  of  short  phrases  and  incomplete  sentences.  the  reviews  of 
format 2 usually use complete sentences. to extract features from such reviews, the above algorithm can 
also be applied. however, experiments show that it is not effective because complete sentences are more 
complex and contain a large amount of noise. below, we describe some unsupervised methods for finding 
explicit features that are nouns and noun phrases. the first method is due to [36]. the method requires a 
large number of reviews, and consists of two steps:  
1. finding frequent nouns and noun phrases. nouns and noun phrases (or groups) are identified by using 
a  pos  tagger.  their  occurrence  frequencies  are  counted,  and  only  the  frequent  ones  are  kept.  a 
frequency threshold can be decided experimentally. the reason for using this approach is that when 
people comment on product features, the vocabulary that they use usually converges, and most product 
features  are  nouns.  thus,  those  nouns  that  are  frequently  talked  about  are  usually  genuine  and 
important features. irrelevant contents in reviews are often diverse and thus infrequent, i.e., they are 
quite different in different reviews. thus, those nouns that are infrequent are likely to be non-features 
or less important features.    

2. finding infrequent features by making use of opinion words. opinion words are usually adjectives and 
adverbs that express positive or negative opinions. the idea is as follows: the same opinion word can 
be  used  to  describe  different  object  features.  opinion  words  that  modify  frequent  features  can  also 
modify infrequent features, and thus can be used to extract infrequent features. for example,    picture    
is found to be a frequent feature, and we have the sentence, 

   the pictures are absolutely amazing.    

if  we  know  that     amazing     is  a  positive  opinion  word,  then     software     can  also  be  extracted  as  a 
feature from the following sentence,  

   the software is amazing.    

because the two sentences follow the same pattern and    software    in the sentence is also a noun. 

the  precision  of  step  1  of  the  above  algorithm  was  improved  by  popescu  and  etzioni  in  [77].  their 
algorithm tries to remove those noun phrases that may not be product features. it evaluates each noun 
phrase  by  computing  a  pointwise  mutual  information  (pmi)  score  between  the  phrase  and  meronymy 
discriminators associated with the product class, e.g., a scanner class. the meronymy discriminators for 
the  scanner  class  are,     of  scanner   ,     scanner  has   ,     scanner  comes  with   ,  etc.,  which  are  used  to  find 
components or parts of scanners by searching on the web. the pmi measure is a simplified version of the 
measure in [95] (also see section 2.1.2). 

pmi

(

df
,

)

=

hits
(
hits
f
(

d
f
)
   
hits
d
)
)(

 

,

(4)

where f is a candidate feature identified in step 1 and d is a discriminator. web search is used to find the 
number of hits of individuals and also their co-occurrences. the idea of this approach is clear. if the pmi 
value of a candidate feature is too low, it may not be a component of the product because f and d do not 
co-occur frequently. the algorithm also distinguishes components/parts from attributes/properties using 
id138   s is-a hierarchy (which enumerates different kinds of properties) and morphological cues (e.g., 
   -iness   ,    -ity    suffixes).  
the double propagation method in [78], which has  been described in section 2.3, can also be used to 
extract features. it in fact exploits and extends the idea in step 2 above (without using step 1), and starts 
with only a set of seed opinion words (no seed features required). that is, it utilizes the association or 
dependency  relations  of  opinion  words  and  features,  i.e.,  opinion  words  always  modify  features.  the 
associations are described using the dependency grammar [91], which results in a set of syntactic rules for 
the extraction of both opinion words and object features in an iterative fashion.  

 

19 

other related works on feature extraction mainly use the ideas of id96 and id91 to capture 
topics/features in reviews [58, 62, 89, 93, 106]. for example, in [63], mei et al. proposed a probabilistic 
model called topic-sentiment mixture to capture the mixture of features and sentiments simultaneously. 
one  topic  model  and  two  sentiment  models  were  defined  based  on  language  models  to  capture  the 
probabilistic distribution of words in different topics/features with their associated opinion orientations. 
su  et  al.  [89]  also  proposed  a  id91  based  method  with  mutual  reinforcement  to  identify  implicit 
features.  
after the extraction of object features, two additional problems need to be solved:  
group synonyms: it is common that people use different words or phrases to describe the same feature. 
for  example,  photo  and  picture  refer  to  the  same  feature  in  digital  camera  reviews.  identifying  and 
grouping synonyms is essential for applications. although id138 [25] and other thesaurus dictionaries 
help  to  some  extent,  they  are  far  from  sufficient  due  to  the  fact  that  many  synonyms  are  domain 
dependent. for example, picture and movie are synonyms in movie reviews, but they are not synonyms in 
digital camera reviews as picture is more related to photo while movie refers to video. carenini et al. [10] 
proposed a method based on several similarity metrics similar to those in information integration [55]. it 
requires  a  taxonomy  of  features  to  be  given  for  a  particular  domain.  the  algorithm  merges  each 
discovered feature to a feature node in the taxonomy. the similarity metrics are defined based on string 
similarity, synonyms and other distances measured using id138. experiments based on digital camera 
and dvd reviews show promising results.  
mapping to implicit features: feature extraction may discover many feature indicators. adjectives and 
adverbs are perhaps the most common type of feature indicators. it is known that many adjectives and 
adverbs modify or describe some specific attributes or properties of objects. this step maps such feature 
indicators to features. for example, the adjective heavy usually describes the weight of an object, and thus 
should be mapped to the weight feature. beautiful is usually used to describe the appearance of an object, 
and thus should be mapped to the appearance feature. however, this needs to be done with care as the 
usage of many adjectives can be quite versatile. their exact meaning may be domain/context dependent. 
for example,    heavy    in the sentence    the traffic is heavy    does not describe the weight of the traffic. 
one way to map indicator words to (implicit) features is to manually compile a list of such mappings 
during training data annotation, which can then be used in the same domain in the future. however, it is 
not clear whether this is an effective approach as little research has been done.  

3.2.  opinion orientation identification 
we now discuss how to identify the orientation of opinions expressed on an object feature in a sentence. 
clearly, the sentence-level and clause-level sentiment classification methods discussed in section 2 are 
applicable here. that is, they can be applied to each sentence or clause which contains object features, 
and the features in it will take its opinion orientation. here, we only describe a lexicon-based approach to 
solving the problem [19, 36]. see a more complex method based on relaxation labeling in [77].   
the  lexicon-based  approach  basically  uses  opinion  words  and  phrases  in  a  sentence  to  determine  the 
orientation of the opinion. apart from the opinion lexicon, negations and but-clauses in a sentence are 
also crucial and need to be handled. the approach works as follows [36, 19]:  
1.  identifying opinion words and phrases: given a sentence that contains an object feature, this step 
identifies all opinion words and phrases. each positive word is assigned the opinion score of +1, each 
negative word is assigned the opinion score of -1, and each context dependent word is assigned the 
opinion score of 0. for example, we have the sentence,    the picture quality of this camera is not 
great, but the battery life is long.    after this step, the sentence is turned into    the picture quality of 
this  camera  is  not  great[+1],  but  the  battery  life  is  long[0]     because     great     is  a  positive  opinion 
word and    long    is context dependent. the object features are italicized.   

2.  handling negations: negation words and phrases are used to revise the opinion scores obtained in 

 

20 

step 1 based on some negation handling rules. after this step, the above sentence is turned into    the 
picture  quality  of  this  camera  is  not  great[-1],  but  the  battery  life  is  long[0]     due  to  the  negation 
word    not   . we note that not every    not    means negation, e.g.,    not only     but also   . such non-
negation phrases containing negation words need to be considered separately.  

3.  but-clauses: in english, but means contrary. a sentence containing but is handled by applying the 
following rule: the opinion orientation before but and after but are opposite to each other.  after this 
step, the above sentence is turned into    the picture quality of this camera is not great[-1], but the 
battery life is long[+1]    due to    but   . apart from but, phrases such as    with the exception of   ,    except 
that   , and    except for    behave similarly to but and are handled in the same way. as in the case of 
negation, not every but means contrary, e.g.,    not only     but also   . such non-but phrases containing 
   but    also need to be considered separately.  

4.  aggregating  opinions:  this  step  applies  an  opinion  aggregation  function  to  the  resulting  opinion 
scores to determine the final orientation of the opinion on each object feature in the sentence. let the 
sentence be s, which contains a set of object features {f1,    , fm} and a set of opinion words or phrases 
{op1,    , opn} with their opinion scores obtained from steps 1, 2 and 3. the opinion orientation on 
each feature fi in s is determined by the following opinion aggregation function:  

 

score

(

sf
),
i

=

op
j
opd
(

so
.
f
,

j

)

i

s

 

,

(5) 

   

   

j

op

where opj is an opinion word in s, d(opj, fi) is the distance between feature fi and opinion word opj in s. 
opj.so is the orientation or the opinion score of opi. the multiplicative inverse in the formula is used to 
give low weights to opinion words that are far away from feature fi. if the final score is positive, then 
the opinion on feature fi in s is positive. if the final score is negative, then the opinion on the feature is 
negative. it is neutral otherwise.  

this simple algorithm is useful but not sufficient in many cases. one major shortcoming is that opinion 
words and phrases do not cover all expressions that convey or imply opinions. there are in fact many 
others. below, we present basic rules of opinions.  

basic rules of opinions 
a rule of opinion is an implication with an expression on the left and an implied opinion on the right. the 
expression is a conceptual one as it represents a concept, which can be expressed in many ways in an 
actual sentence.  the application of opinion words/phrases above can also be represented as such rules. 
let  neg  be  a  negative  opinion  word/phrase  and  pos  be  a  positive  opinion  word/phrase.  the  rules  for 
applying opinion words/phrases in a sentence are as follow:  

1.  neg     negative  
2.  pos     positive  

these rules say that neg implies a negative opinion (denoted by negative) and pos implies a positive 
opinion (denoted by positive) in a sentence. the effect of negations can be represented as well:  

3.  negation neg     positive  
4.  negation pos     negative 

the rules state that negated opinion words/phrases take their opposite orientations in a sentence. note that 
the above use of    but    is not considered an opinion rule but a language convention that people often use 
to indicate a possible opinion change. we now describe some additional rules of opinions.    
deviation from the norm or some desired value range: in some domains, an object feature may have an 
expected or desired value range or norm. if it is above and/or below the normal range, it is negative, 
e.g.,    this drug causes low (or high) blood pressure.    we then have the following rules 

5.  desired value range     positive 

 

21 

6.  below or above the desired value range     negative 

decreased and increased quantities of opinionated items: this set of rules is similar to the negation rules 
above. decreasing or increasing the quantities associated with some opinionated items may change the 
orientations of the opinions. for example,    this drug reduced my pain significantly.    here,    pain    is a 
negative opinion word, and the reduction of    pain    indicates a desirable effect of the drug. hence, the 
decreased pain implies a positive opinion on the drug. the concept of    decreasing    also extends to 
   removal    or    disappearance   , e.g.,    my pain has disappeared after taking the drug.     

7.  decreased neg      positive 
8.  decreased pos     negative 
9.  increased neg     negative  
10. increased pos     positive 

the last two rules may not be needed as there is no change of opinion orientations.  

producing  and  consuming  resources  and  wastes:  if  an  object  produces  resources,  it  is  positive.  if  it 
consumes  resources,  especially  a  large  quantity  of  them,  it  is  negative.  for  example,     money     is  a 
resource. the sentence,    company-x charges a lot of money    gives a negative opinion on    company-
x   . likewise, if an object produces wastes, it is negative. if it consumes wastes, it is positive. these 
give us the following rules:  

11. consume resource     negative 
12. produce resource     positive 
13. consume waste     positive 
14. produce waste     negative 

these basic rules can also be combined to produce compound rules, e.g.,    consume decreased waste     
negative    which is a combination of rules 7 and 13. to build a practical system, all these rules and their 
combinations need to be considered.  
as noted above, these are conceptual rules. they can be expressed in many ways using different words 
and phrases in an actual text, and in different domains they may also manifest differently. however, by 
no  means,  we  claim  these  are  the  only  basic  rules  that  govern  expressions  of  positive  and  negative 
opinions.  with  further  research,  additional  new  rules  may  be  discovered  and  the  current  rules  may  be 
refined or revised. neither do we claim that any manifestation of such rules imply opinions in a sentence. 
like  opinion  words  and  phrases,  just  because  a  rule  is  satisfied  in  a  sentence  does  not  mean  that  it 
actually is expressing an opinion, which makes id31 a very challenging task.  

4.  id31 of comparative sentences 
directly  expressing  positive  or  negative  opinions  on  an  object  and  its  features  is  only  one  form  of 
evaluation. comparing the object with some other similar objects is another. comparisons are related to 
but are also quite different from direct opinions. they not only have different semantic meanings, but also 
different syntactic forms. for example, a typical direct opinion sentence is    the picture quality of this 
camera is great.    a typical comparison sentence is    the picture quality of camera-x is better than that of 
camera-y.     this  section  first  defines  the  problem,  and  then  presents  some  existing  methods  for  their 
analysis [29, 38, 39].  

4.1   problem definition 
in general, a comparative sentence expresses a relation based on similarities or differences of more than 
one  object.  the  comparison  is  usually  conveyed  using  the  comparative  or  superlative  form  of  an 
adjective or adverb. a comparative is used to state that one object has more of a certain quantity than 
another object. a superlative is used to state that one object has the most or least of a certain quantity. in 

 

22 

general, a comparison can be between two or more objects, groups of objects, and one object and the rest 
of the objects. it can also be between an object and its previous or future versions. 
two types of comparatives: in english, comparatives are usually formed by adding the suffix    -er    and 
superlatives are formed by adding the suffix       est    to their base adjectives and adverbs. for example, in 
   the battery life of camera-x is longer than that of camera-y   ,    longer    is the comparative form of the 
adjective    long   . in    the battery life of this camera is the longest   ,    longest    is the superlative form of 
the  adjective     long   .  we  call  this  type  of  comparatives  and  superlatives  type  1  comparatives  and 
superlatives. for simplicity, we will use type 1 comparatives to mean both from now on.  
adjectives  and  adverbs  with  two  syllables  or  more  and  not  ending  in  y  do  not  form  comparatives  or 
superlatives by adding       er    or       est   . instead, more, most, less and least are used before such words, 
e.g., more beautiful. we call this type of comparatives and superlatives type 2 comparatives and type 2 
superlatives. both type 1 and type 2 are called regular comparatives and superlatives respectively.  
in english, there are also some irregular comparatives and superlatives, which do not follow the above 
rules,  i.e., more, most, less, least, better, best, worse, worst, further/farther  and furthest/farthest.  they 
behave similarly to type 1 comparatives and superlatives and thus are grouped under type 1.  
apart from these standard comparatives and superlatives, many other words can also be used to express 
comparisons,  e.g.,  prefer  and  superior.  for  example,  the  sentence,     camera-x   s  quality  is  superior  to 
camera-y   , says that    camera-x    is preferred. in [38], jindal and liu identified a list such words. since 
these words behave similarly to type 1 comparatives, they are also grouped under type 1. 
further analysis also shows that comparatives can be grouped into two categories according to whether 
they express increased or decreased values, which are useful in id31.  

increasing comparatives: such a comparative expresses an increased value of a quantity, e.g., more and 

longer.  

decreasing comparatives: such a comparative expresses a decreased value of a quantity, e.g., less and 

fewer.  

types of comparative relations: comparative relations can be grouped into four main types. the first 
three types are called gradable comparisons and the last one is called the non-gradable comparison.  
1. non-equal gradable comparisons: relations of the type greater or less than that express an ordering of 
some objects with regard to some of their features, e.g.,    the intel chip is faster than that of amd   . 
this type also includes user preferences, e.g.,    i prefer intel to amd   .  

2. equative comparisons: relations of the type equal to that state two objects are equal with respect to 

some of their features, e.g.,    the picture quality of camera x is as good as that of camera y    

3. superlative comparisons: relations of the type greater or less than all others that rank one object over 

all others, e.g.,    the intel chip is the fastest   . 

4. non-gradable comparisons: relations that compare features of two or more objects, but do not grade 

them. there are three main sub-types:  
     object  a  is  similar  to  or  different  from  object  b  with  regard  to  some  features,  e.g.,     coke  tastes 

     object a has feature f1, and object b has feature f2 (f1 and f2 are usually substitutable), e.g.,    desktop 

pcs use external speakers but laptops use internal speakers    

     object a has feature f, but object b does not have, e.g.,    cell phone x has an earphone, but cell 

mining objective: given an opinionated document d, comparison mining consists of two tasks:  

1.  identify comparative sentences in d, and classify the identified comparative sentences into different 

2.  extract  comparative  opinions  from  the  identified  sentences.  a  comparative  opinion  in  a 

differently from pepsi    

phone yoes not have    

types or classes.  

 

23 

comparative sentence is expressed with:  

 

(o1, o2, f, po, h, t),  

where o1 and o2 are the object sets being compared based on their shared features f (objects in o1 
appear before objects in o2 in the sentence), po is the preferred object set of the opinion holder h, 
and t is the time when the comparative opinion is expressed.  

as for direct opinions, not every piece of information is needed in an application. in many cases, h and t 
may not be required by applications.  
example 9: consider the comparative sentence    canon   s optics is better than those of sony and nikon.    

written by john on may 1, 2009. the extracted comparative opinion is: 

 

({canon}, {sony, nikon}, {optics},  preferred:{canon}, john, may-1-2009).  

 

the  object  set  o1  is  {canon},  the  object  set  o2  is  {sony,  nikon},  their  shared  feature  set  f  being 
compared is {optics}, the preferred object set is {canon}, the opinion holder h is john and the time t 
when this comparative opinion was written is may-1-2009. 

below, we study the problem of identifying comparative sentences and mining comparative opinions.   

identification of comparative sentences  

4.2 
although  most  comparative  sentences  contain  comparative  adjectives  and  comparative  adverbs,  e.g., 
better, and longer, many sentences that contain such words are not comparatives, e.g.,    i cannot agree 
with you more   . similarly, many sentences that do not contain such indicators are comparative sentences 
(usually non-gradable), e.g.,    cellphone-x has bluetooth, but cellphone-y does not have.     
an interesting phenomenon about comparative sentences is that such a sentence usually has a keyword or 
a key phrase indicating comparison. it is shown in [38] that using a set of 83 keywords and key phrases, 
98% of the comparative sentences (recall = 98%) can be identified with a precision of 32% using the 
authors    data set. the keywords and key phrases are: 
1. comparative  adjectives  (jjr)  and  comparative  adverbs  (rbr),  e.g.,  more,  less,  better,  and  words 

2. superlative adjectives (jjs) and superlative adverbs (rbs), e.g., most, least, best, and words ending 

ending with -er.  

with -est. 

3. other indicative words such as same, similar, differ, as well as, favor, beat, win, exceed, outperform, 

prefer, ahead, than, superior, inferior, number one, up against, etc.  

since keywords alone are able to achieve a high recall, the set of keywords can be used to filter out those 
sentences  that  are  unlikely  to  be  comparative  sentences.  we  can  then  improve  the  precision  of  the 
remaining set of sentences.  
it  is  also  observed  that  comparative  sentences  have  strong  patterns  involving  comparative  keywords, 
which is not surprising. these patterns can be used as features in machine learning. to discover these 
patterns, class sequential rule (csr) mining is used in [38]. class sequential rule mining is a sequential 
pattern mining method from data mining. each training example used for mining csrs is a pair (si, yi), 
where  si  is  a  sequence  and  yi  is  a  class,  e.g.,  yi       {comparative,  non-comparative}.  the  sequence  is 
generated from a sentence. instead of using each full sentence, only words near a comparative keyword 
are  used  to  generate  each  sequence.  each  sequence is  also  labeled  with  a  class  indicating whether  the 
sentence is a comparative sentence or not. using the training data, csrs can be generated. details of the 
mining algorithm can be found in [38, 55].  
for classification model building, the left-hand side sequence patterns of the rules with high conditional 
probabilities are used as data features in [38]. if the sentence matches a pattern, the corresponding feature 
value for the pattern is 1, and otherwise it is 0. bayesian classification is employed for model building.  

 

24 

classify comparative sentences into three types: this step classifies comparative sentences obtained 
from the last step into one of the three types, non-equal gradable, equative, and superlative (non-gradable 
may also be added). for this task, keywords alone are already sufficient. that is, the set of keywords is 
used as data features for machine learning. it is shown in [38] that id166 gives the best results. 

4.3   extraction of objects and object features in comparative sentences 
to  extract  objects  and  object  features  being  compared,  many  information  extraction  methods  can  be 
applied,  e.g.,  conditional  random  fields  (crf),  hidden  markov  models  (id48),  and  others.  for  a 
survey of information extraction techniques, see [82]. in [39], jindal and liu used label sequential rules 
(lsr) and crf to perform the extraction. the algorithm makes the following assumptions:  
1. there is only one comparative relation in a sentence. in practice, this is violated only in a very small 

number of cases.  

2. objects  or  their  features  are  nouns  (includes  nouns,  plural  nouns  and  proper  nouns)  and  pronouns. 
these cover most cases. however, a feature can sometimes be a noun used in its verb form or some 
action described as a verb (e.g.,    intel costs more   ;    costs    is a verb and an object feature). these are 
adverbial comparisons and are not considered in [39].   

in [7], bos and nissim also proposed a method to extract some useful items from superlative sentences.  

identification of preferred objects in comparative sentences 

4.4 
similar to id31 of normal sentences, id31 of comparative sentences also needs 
to determine whether a comparative sentence is opinionated or not. however, unlike normal sentences, it 
does  not  make  good  sense  to  apply  sentiment  classification  to  comparative  sentences  because  an 
opinionated  comparative  sentence  does  not  express  a  direct  positive  or  negative  opinion.  instead,  it 
compares multiple objects by ranking the objects based on their shared features to give a comparative 
opinion. in other words, it presents a preference order of the objects based on the comparison of some of 
their shared features. since most comparative sentences compare only two sets of objects, analysis of an 
opinionated comparative sentence means to identify the preferred object set. since little research has been 
done on classifying whether a comparative sentence is opinionated or not, below we only briefly describe 
a method [29] for identifying the preferred objects.  
the approach bears some resemblance to the lexicon-based approach to identifying opinion orientations 
on object features. thus, it needs opinion words used for comparisons. similar to normal opinion words, 
these words can also be divided into two categories.   
1.  comparative opinion words: for type 1 comparatives, this category includes words such as better, 
worse, etc, which have explicit and domain independent opinions. in sentences involving such words, 
it is normally easy to determine which object set is the preferred one of the sentence author.  
in  the  case  of  type  2  comparatives,  formed  by  adding  more,  less,  most,  and  least  before 
adjectives/adverbs, the preferred object set is determined by both words. the following rules apply: 

<increasing comparative> negative       negative comparative opinion 
<increasing comparative> positive       positive comparative opinion 
<decreasing comparative> negative      positive comparative opinion 
<decreasing comparative> positive       negative comparative opinion 

  the  first  rule  says  that  the  combination  of  an  increasing  comparative  (e.g.,  more)  and  a  negative 
opinion word (e.g., awful) implies a negative type 2 comparative. the other rules are similar. note 
that  the  positive  (or  negative)  opinion  word  is  of  the  base  type,  while  the  positive  (or  negative) 
comparative opinion is of the comparative type.  

2.  context-dependent  comparative  opinion  words:  in  the  case  of  type  1  comparatives,  such  words 
include higher, lower, etc. for example,    car-x has higher mileage per gallon than car-y    carries a 

 

25 

positive  sentiment  on     car-x     and  a  negative  sentiment  on     car-y     comparatively,  i.e.,     car-x     is 
preferred.  however,  without  domain  knowledge  it  is  hard  to  know  whether     higher     is  positive  or 
negative. the combination of    higher    and    mileage    with the domain knowledge tells us that    higher 
mileage    is desirable.  
in the case of type 2 comparatives, the situation is similar. however, in this case, the comparative 
word  (more,  most,  less  or  least),  the  adjective/adverb  and  the  object  feature  are  all  important  in 
determining  the  opinion  or  preference.  if  we  know  whether  the  comparative  word  is  increasing  or 
decreasing (which is easy since there are only four of them), then the opinion can be determined by 
applying the four rules in (1) above.  
as  discussed  in  section  2.3,  the  pair  (object_feature,  opinion_word)  forms  an  opinion  context.  to 
determine whether a pair is positive or negative, the algorithm in [29] resorts to external information, 
i.e.,  a  large  corpus  of  pros  and  cons  from  product  reviews.  it  basically  determines  whether  the 
object_feature and opinion_word are more associated with each other in pros or in cons. if they are 
more  associated  in  pros,  it  is  positive.  otherwise,  it  is  negative.  using  pros  and  cons  is  natural 
because they are short phrases and thus have little noise, and their opinion orientations are known.  

to  obtain  comparative  opinion  words,  due  to  the  observation  below  we  can  simply  convert  opinion 
adjectives/adverbs  to  their  comparative  forms,  which  can  be  done  automatically  based  on  the  english 
comparative formation rules described above and the id138. 
observation: if an adjective or adverb is positive (or negative), then its comparative or superlative form is 

also positive (or negative), e.g., good, better and best.  

after the conversion, these words are manually categorized into increasing and decreasing comparatives. 
once  all  the  information  is  available,  determining  which  object  set  is  preferred  is  relatively  simple. 
without negation, if the comparative is positive (or negative), then the objects before (or after) than is 
preferred.  otherwise, the objects after (or before) than are preferred. additional details can be found in 
[29]. in [26], fiszman et al. studied the problem of identifying which object has more of certain features 
in comparative sentences in biomedical texts, but it does not analyze opinions.  

5.  opinion search and retrieval 

as web search has proven to be very important, it is not hard to imagine that opinion search will also be 
of  great  use.  one  can  crawl  the  user-generated  content  on  the  web  and  enable  people  to  search  for 
opinions on any subject matter. two typical kinds of opinion search queries may be issued:    
1. find public opinions on a particular object or a feature of the object, e.g., find customer opinions on a 
digital camera or the picture quality of the camera, and find public opinions on a political topic. recall 
that an object can be a product, organization, event, or topic.  

2. find opinions of a person or organization (i.e., opinion holder) on a particular object or a feature of the 
object, e.g., find barack obama   s opinion on abortion. this type of search is particularly relevant to 
news articles, where individuals or organizations who express opinions are explicitly stated.  

for the first type of queries, the user may simply give the name of the object or the name of the feature 
and the name of the object. for the second type of queries, the user may give the name of the opinion 
holder and the name of the object.  
similar  to  traditional  web  search,  opinion  search  also  has  two  major  tasks:  1)  retrieving  relevant 
documents/sentences to the user query, and 2) ranking the retrieved documents/sentences. however, there 
are also major differences. on retrieval, opinion search needs to perform two sub-tasks:  
1.  find documents or sentences that are relevant to the query topic. this is the only task performed in 

the traditional web search or information retrieval.  

2.  determine  whether  the  documents  or  sentences  express  opinions  and  whether  the  opinions  are 

 

26 

positive or negative. this is the task of id31. traditional search does not perform this 
sub-task. it is this sub-task that makes the opinion search more complex than traditional search.  

as for ranking, traditional web search engines rank web pages based on authority and relevance scores 
[55]. the basic premise is that the top ranked pages (ideally the first page) contain sufficient information 
to satisfy the user   s information need. this paradigm is adequate for factual information search because 
one fact equals to any number of the same fact. that is, if the first page contains the required information, 
there is no need to see the rest of the relevant pages. for opinion search, this paradigm is fine for the 
second type of queries because the opinion holder usually has only one opinion on a particular object or 
topic, and the opinion is contained in a single document or page. however, for the first type of opinion 
queries, this paradigm needs to be modified because ranking in opinion search has two objectives. first, it 
needs to rank those opinionated documents or sentences with high utilities or information contents at the 
top  (see  section  6.2).  second,  it  also  needs  to  reflect  the  natural  distribution  of  positive  and  negative 
opinions. this second objective is important because in most practical applications the actual proportions 
of positive and negative opinions are the most important pieces of information as in traditional opinion 
surveys.  only  reading  the  top  ranked  results  as  in  the  traditional  search  is  problematic  because  one 
opinion does not equal to multiple opinions. the top result only represents the opinion of a single person 
or organization. thus, ranking in opinion search needs to capture the natural distribution of the positive 
and negative sentiments of the whole population. one simple solution is to produce two rankings, one for 
positive opinions and one for negative opinions. the numbers of positive and negative opinions indicate 
the distribution.   
providing  a  feature-based  summary  for  each  opinion  search  will  be  even  better.  however,  it  is  an 
extremely challenging problem as we have seen that feature extraction, feature grouping and associating 
objects  to  its  features  are  all  very  difficult  problems.  like  opinion  search,  comparison  search  will  be 
useful too. for example, when one wants to register for a free email account, one most probably wants to 
know which email system is the best for him/her, e.g., hotmail, gmail or yahoo! mail. wouldn   t it be nice 
if one can find comparisons of features of these email systems from existing users by issuing a search 
query    hotmail vs. gmail vs. yahoo mail.   ? so far, little research has been done in this direction although 
the work in [29, 38, 39] can be of use in this context.  
to give a favor of what an opinion search system looks like, we present an example system [109], which 
is the winner of the blog track in the 2007 trec evaluation (http://trec.nist.gov/). the task of this track is 
exactly  opinion  search  (or  retrieval).  this  system  has  two  components.  the  first  component  is  for 
retrieving  relevant  documents  for  each  query.  the  second  component  is  for  classifying  the  retrieved 
documents as opinionated or not-opinionated (subjectivity classification). the opinionated documents are 
further classified into positive, negative or mixed (containing both positive and negative opinions).  
retrieval component: this component performs the traditional information retrieval (ir) task. unlike a 
normal  ir  system,  which  is  based  on  keyword  match,  this  component  considers  both  keywords  and 
concepts. concepts are named entities (e.g., names of people or organizations) or various types of phrases 
from dictionaries and other sources (e.g., wikipedia entries). the strategy for processing a user query is 
as follows [108, 109]: it first recognizes and disambiguates the concepts within the user query. it then 
broadens  the  search  query  with  its  synonyms.  after  that,  it  recognizes  concepts  in  the  retrieved 
documents,  and  also  performs  pseudo-feedback  to  automatically  extract  relevant  words  from  the  top-
ranked  documents  to  expand  the  query.  finally,  it  computes  a  similarity  (or  relevance  score)  of  each 
document with the expanded query using both concepts and keywords.  
opinion classification component: this component performs two tasks: (1) classifying each document 
into  one  of  the  two  categories,  opinionated  and  not-opinionated,  and  (2)  classifying  each  opinionated 
document as expressing a positive, negative or mixed opinion. for both tasks, the system uses supervised 
learning. for the first task, it obtains a large amount of opinionated (subjective) training data from review 
sites such as rateitall.com and epinion.com. the data are also collected from different domains involving 
consumer  goods  and  services  as  well  as  government  policies  and  political  viewpoints.  the  not-

 

27 

opinionated training data are obtained from sites that give objective information such as wikipedia. from 
these training data, a id166 classifier is constructed.  
this classifier is then applied to each retrieved document as follows: the document is first partitioned 
into  sentences.  the  id166  classifier  then  classifies  a  sentence  as  opinionated  or  not-opinionated.  if  a 
sentence is classified to be opinionated, its strength as determined by id166 is also noted. a document is 
regarded opinionated if there is at least one sentence that is classified as opinionated. to ensure that the 
opinion  of  the  sentence  is  directed  to  the  query  topic,  the  system  requires  that  enough  query 
concepts/words are found in its vicinity. the totality of the opinionated sentences and their strengths in a 
document together with the document   s similarity with the query is used to rank the document relative to 
other documents.  
to determine whether an opinionated document express a positive, negative or mixed opinion, the second 
classifier is constructed. the training data are reviews from review sites containing review ratings (e.g., 
rateitall.com). a low rating indicates a negative opinion while a high rating indicates a positive opinion. 
using  positive  and  negative  reviews  as  training  data,  a  sentiment  classifier  is  built  to  classify  each 
document as expressing positive, negative, or mixed opinion.  
there are many other approaches for opinion retrieval. the readers are encouraged to read the papers at 
the  trec  site  (http://trec.nist.gov/pubs/trec16/t16_proceedings.html),  and  the  overview  paper  of  2007 
trec blog track [60]. other related work includes [20, 27, 66].  

6.  opinion spam and utility of opinions  
email  spam  and  web  spam  are  quite  familiar  to  most  people.  email  spam  refers  to  unsolicited 
commercial  emails  selling  products  and  services,  while  web  spam  refers  to  the  use  of     illegitimate 
means    to boost the search rank positions of target web pages. the reason for spam is  mainly due to 
economics. for example, in the web context, the economic and/or publicity value of the rank position of 
a page returned by a search engine is of great importance. if someone searches for a product that your 
web site sells, but the product page of your site is ranked very low (e.g., beyond the top 20) by a search 
engine, then the chance that the person will go to your page is extremely low, let alone to buy the product 
from  your  site.  this  is  certainly  bad  for  the  business.  there  are  now  many  companies  that  are  in  the 
business of helping others improve their page ranking by exploiting the characteristics and weaknesses of 
current  search  ranking  algorithms.  these  companies  are  called  search  engine  optimization  (seo) 
companies.  some  seo  activities  are  ethical  and  some,  which  generate  spam,  are  not.  for  more 
information on web spam, please refer to [55].  
in the context of opinions, we have a similar spam problem [40, 41]. due to the explosive growth of the 
user-generated content, it has become a common practice for people to find and to read opinions on the 
web for many purposes. for example, a person plans to buy a camera. most probably, he/she will go to a 
merchant or review site (e.g., amazon.com) to read the reviews of some cameras. if he/she find that most 
reviews are positive about a camera, he/she is very likely to buy the camera. however, if most reviews 
are  negative,  he/she  will  almost  certainly  choose  another  camera.  positive  opinions  can  result  in 
significant financial gains and/or fames for organizations and individuals. this, unfortunately, also gives 
good incentives for opinion spam, which refers to human activities (e.g., write spam reviews) that try to 
deliberately  mislead  readers  or  automated  opinion  mining  systems  by  giving  undeserving  positive 
opinions to some target objects in order to promote the objects and/or by giving unjust or false negative 
opinions to some other objects to damage their reputations. such opinions are also called fake opinions or 
bogus opinions. they have become an intense discussion topic in blogs and forums,  and also in press 
(e.g.,  http://travel.nytimes.com/2006/02/07/business/07guides.html),  which  show  that  review  spam  has 
become  a  problem.  we  can  predict  that  as  opinions  on  the  web  are  increasingly  used  in  practice  by 
consumers  and  organizations,  the  problem  of  detecting  spam  opinions  will  become  more  and  more 
critical.  

 

28 

a related problem that has also been studied in the past few years is the determination of the usefulness, 
helpfulness or utility of a review [31, 49, 57, 110]. the idea is to determine how helpful a review is to a 
user. this is a useful task as it is desirable to rank reviews based on utilities or qualities when showing the 
reviews to the user, with the most useful reviews at the top. in fact, many review aggregation sites have 
been  practicing  this  for  years.  they  obtain  the  helpfulness  or  utility  score  of  each  review  by  asking 
readers  to  provide  helpfulness  feedbacks  to  each  review.  for  example,  in  amazon.com,  the  reader  can 
indicate whether he/she finds a review helpful by responding to the question    was the review helpful to 
you?     just below  each  review.  the  feedback  results  from  all  those  responded  are  then  aggregated  and 
displayed right before each review, e.g.,    15 of 16 people found the following review helpful   . although 
most review sites already provide the service, automatically determining the quality or the usefulness of a 
review is still useful because  many reviews have few or no feedbacks. this is especially true for new 
reviews and reviews of products that are not very popular.  
this  section  uses  customer  reviews  of  products  as  an  example  to  study  opinion  spam  and  utilities  of 
opinions. however, most of the analyses are also applicable to opinions expressed in other forms of the 
user-generated content, e.g., forum posts and blogs.  

6.1   opinion spam 
there are generally three types of spam reviews as defined by jindal and liu in [40, 41].  
     type 1 (untruthful opinions): these are reviews that deliberately mislead readers or opinion mining 
systems by giving undeserving positive reviews to some target objects in order to promote the objects 
and/or by giving unjust or malicious negative reviews to some other objects in order to damage their 
reputation.  untruthful  reviews  are  also  commonly  known  as  fake  reviews  or  bogus  reviews  as  we 
mentioned earlier.  

     type 2 (opinions on brands only): these are reviews that do not comment on the specific products 
that they are supposed to review, but only comment on the brands, the manufacturers or the sellers of 
the products. although they may be useful, they are considered as spam because they are not targeted 
at the specific products and are often biased. for example, in a review for a hp printer, the reviewer 
only wrote    i hate hp. i never buy any of their products   .  

     type 3 (non-opinions): these are not reviews or opinionated although they appear as reviews. there 
are two main sub-types: (1) advertisements, and (2) other irrelevant texts containing no opinions (e.g., 
questions, answers, and random texts).  

in general, spam detection can be formulated as a classification problem with two classes, spam and non-
spam. due to the specific nature of the different types of spam, they need to be dealt with differently. for 
spam reviews of type 2 and type 3, they can be detected based on traditional classification learning using 
manually  labeled  spam  and  non-spam  reviews  because  these  two  types  of  spam  reviews  are  easily 
recognizable manually. the main task is to find a set of effective data features for model building. note 
again that here the features refer to features in machine learning not object features used in feature-based 
id31. in [40, 41], three sets of features were identified for learning,  
review centric features: these are features about the content of each review. example features are actual 
text  of  the  review,  number  of  times  that  brand  names  are  mentioned,  percentage  of  opinion  words, 
review length, and number of helpful feedbacks.  

reviewer centric features: these are features about a reviewer. example features are average rating given 
by the reviewer, standard deviation in rating, ratio of the number of reviews that the reviewer wrote 
which were the first reviews of the products to the total number of reviews that he/she wrote, and ratio 
of the number of cases in which he/she was the only reviewer. 

product centric features: these are features about each product. example features are price of the product, 
sales rank of the product (amazon.com assigns sales rank to    now selling products    according to their 
sales volumes), average rating, and standard deviation in ratings of the reviews on the product. 

 

29 

table 2. spam reviews vs. product quality 

 

positive spam 

review 

negative spam 

review 

good quality product 
bad quality product 
average quality product 

1 
3 
5 

2 
4 
6 

id28 was used in learning. experimental results based on a large number of amazon.com 
reviews showed that type 2 and types 3 spam reviews are fairly easy to detect.  
however, this cannot be said about type 1 spam, untruthful opinions or fake reviews. in fact, it is very 
difficult to detect such reviews because manually labeling training data is very hard, if not impossible. 
the  problem  is  that  identifying  spam  reviews  by  simply  reading  the  reviews  is  extremely  difficult 
because a spammer can carefully craft a spam review that is just like any innocent review.  
in order to detect such spam, let us analyze fake reviews in greater detail. as indicated above, there are 
two main objectives for spam:  
     write  undeserving positive  reviews  for  some  target  objects  in  order  to  promote  them.  we  call  such 

     write unfair or malicious negative reviews for some target objects to damage their reputations. we call 

spam reviews hype spam.  

such spam reviews defaming spam.  

in certain cases, the spammer may want to achieve both objectives, while in others, he/she only aims to 
achieve one of them because either he/she does not have an object to promote or there is no competition. 
we now discuss what kinds of reviews are harmful and are likely to be spammed. table 2 gives a simple 
view of type 1 spam. spam reviews in regions 1, 3 and 5 are typically written by manufacturers of the 
product or persons with direct economic or other interests in the product. their goal is to promote the 
product. although opinions expressed in region 1 may be true, reviewers do not announce their conflict 
of interests. note that good, bad and average products can be defined based on average review ratings 
given  to  the  product.  spam  reviews  in  regions  2,  4,  and  6  are  likely  to  be  written  by  competitors. 
although  opinions  in  reviews  of  region  4  may  be  true,  reviewers  do  not  announce  their  conflict  of 
interests and have malicious intensions. clearly, spam reviews in region 1 and 4 are not so damaging, 
while spam reviews in regions 2, 3, 5 and 6 are very harmful. thus, spam detection techniques should 
focus on identifying reviews in these regions. one important observation from this table is that harmful 
fake reviews are often outlier reviews. in other words, deviating from the norm is the necessary condition 
for harmful spam reviews, but not sufficient because many outlier reviews may be truthful.    
since manually labeling training data is extremely difficult, other ways have to be explored in order to 
find training examples for detecting possible type 1 spam. in [41], it exploits duplicate reviews. in their 
study  of  5.8  million  reviews,  2.14  million  reviewers  and  6.7  million  products  from  amazon.com,  they 
found  a  large  number  of  duplicate  and  near-duplicate  reviews,  which  indicates  that  review  spam  is 
widespread. these duplicates (which include near-duplicates) can be divided into four groups: 
1.  duplicates from the same userid on the same product.  
2.  duplicates from different userids on the same product. 
3.  duplicates from the same userid on different products. 
4.  duplicates from different userids on different products.  
the first type of duplicates can be the results of reviewers mistakenly clicking the submit button multiple 
times (which of course can be detected based on the submission dates and times), or the same reviewers 
coming  back  to  write  updated  reviews  after  using  the  product  for  some  time.  however,  the  last  three 
kinds  of  duplicates  are  almost  certainly  type  1  spam  reviews.  further  sanity  check  was  performed  on 

 

30 

these  duplicate  reviews  because  amazon.com  cross-posts  reviews  to  different  formats  of  the  same 
product, e.g., hardcover and paperback of the same book. manually checking a large number of duplicate 
reviews  showed  that  only  a  small  percentage  of  them  falls  into  this  category.  one  reason  for  the  low 
percentage  could  be  because  the  reviews  being  studied  were  all  from  manufactured  products,  which 
perhaps have fewer formats of the same product (unlike books).  
in the work reported in [41], these three types of duplicates and near duplicates are treated as type 1 spam 
reviews, and the rest of the reviews are treated as non-spam reviews. id28 is used to build a 
classification model. the experiments show some tentative but interesting results.  
     negative outlier reviews (whose ratings have significant negative deviations from the average rating) 
tend to be heavily spammed. the reason for such spam is quite intuitive. positive outlier reviews are 
not badly spammed.   

     those  reviews  that  are  the  only  reviews  of  some  products  are  likely  to  be  spammed.  this  can  be 

explained by the tendency of promoting an unpopular product by writing a spam review.  

     top-ranked  reviewers  are  more  likely  to  be  spammers.  amazon.com  gives  a  rank  to  each 
member/reviewer  based  on  the  frequency  that  he/she  gets  helpful  feedback  on  his/her  reviews. 
additional  analysis  shows  that  top-ranked  reviewers  generally  write  a  large  number  of  reviews. 
people  who  wrote  a  large  number  of  reviews  are  natural  suspects.  some  top  reviewers  wrote 
thousands or even tens of thousands of reviews, which is unlikely for an ordinary consumer.  

     spam reviews can get good helpful feedbacks and non-spam reviews can get bad feedbacks. this is 
important  as  it  shows  that  if  usefulness  or  quality  of  a  review  is  defined  based  on  the  helpful 
feedbacks that the review gets, people can be readily fooled by spam reviews. note that the number 
of helpful feedbacks can be spammed too.  

     products  of  lower  sale  ranks  are  more  likely  to  be  spammed.  this  is  good  news  because  spam 
activities seem to be limited to low selling products, which is actually quite intuitive as it is difficult 
to damage the reputation of a popular product by writing a spam review. 

finally, it should be noted again that these results are only tentative because 1) it is not confirmed that the 
three types of duplicates are absolutely spam reviews, and 2) many spam reviews are not duplicated and 
they are not considered as spam in model building but are treated as non-spam due to the difficulty of 
manual labeling. for additional analysis and  more spam detection strategies,  please refer to [41]. this 
research is still at its infancy. much work needs to be done. as we mentioned at the beginning of the 
section,  with  more  and  more  people  and  organizations  rely  on  opinions  on  the  web,  devising  good 
techniques to detect opinion spam is urgently needed. we do not want to wait until the day when the 
opinions on the web are so heavily spammed that they become completely useless.   

6.2   utility of reviews 
determining the utility of reviews is usually formulated as a regression problem. the learned model then 
assigns a utility value to each review, which can be used in review ranking. in this area of research, the 
ground truth data used for both training and testing are usually the user-helpfulness feedbacks given to 
each review, which as we discussed above are provided for each review at many review aggregation sites. 
so unlike fake review detection, the training and testing data here is not an issue.  
researchers have used many types of data features for model building [31, 49, 110]. example features 
include review length, review ratings (the number of stars), counts of some specific pos tags, opinion 
words, tf-idf weighting scores, wh-words, product attribute mentions, product brands, comparison with 
product specifications, and comparison with editorial reviews, and many more. subjectivity classification 
is  also  applied  in  [31].  in  [57],  liu  et  al.  formulated  the  problem  slightly  differently,  as  a  binary 
classification problem. instead of using the original helpfulness feedbacks as the classification target or 
dependent variable, they performed manual annotation based on whether a review comments on many 
product attributes/features or not.  

 

31 

finally,  we  note  again  that  review  utility  regression/classification  and  review  spam  detections  are 
different  concepts.  not-helpful  or  low  quality  reviews  are  not  necessarily  fake  reviews  or  spam,  and 
helpful reviews may not be non-spam. a user often determines whether a review is helpful or not based 
on  whether  the  review  expresses  opinions  on  many  attributes/features  of  the  product.  a  spammer  can 
satisfy this requirement by carefully crafting a review that is just like a normal helpful review. using the 
number of helpful feedbacks to define review quality is also problematic because user feedbacks can be 
spammed too. feedback spam is a sub-problem of click fraud in search advertising, where a person or 
robot clicks on some online advertisements to give the impression of real customer clicks. here, a robot 
or a human spammer can also click on helpful feedback button to increase the helpfulness of a review. 
another important point is that a low quality review is still a valid review and should not be discarded, 
but a spam review is untruthful and/or malicious and should be removed once detected.  

7.  conclusions  
this  chapter  gave  an  introduction  to  sentiment  analysis  and  subjectivity  (or  opinion  mining).  due  to 
many challenging research problems and a wide variety of practical applications, it has been a very active 
research area in recent years. in fact, it has spread from computer science to management science [e.g., 2, 
11,  17,  32,  37,  58,  74].  this  chapter  first  presented  an  abstract  model  of  sentiment  analysis,  which 
formulates the problem and provides a common framework to unify different research directions. it then 
discussed  the  most  widely  studied  topic  of  sentiment  and  subjectivity  classification,  which  determines 
whether a document or sentence is opinionated, and if so whether it carries a positive or negative opinion. 
we then described feature-based id31 which exploits the full power of the abstract model. 
after that we discussed the problem of analyzing comparative and superlative sentences. such sentences 
represent  a different type of evaluation from direct opinions which have been the focus of the current 
research. the topic of opinion search or retrieval was introduced as well, as a parallel to the general web 
search. last but not least, we discussed opinion spam, which is increasingly becoming an important issue 
as more and more people are relying on opinions on the web for decision making. this gives more and 
more incentive for spam. there is still no effective technique to combat opinion spam.  
finally, we conclude the chapter by saying that all the id31 tasks are very challenging. our 
understanding and knowledge of the problem and its solution are still limited. the main reason is that it is 
a  natural  language  processing  task,  and  natural  language  processing  has  no  easy  problems.  another 
reason  may  be  due  to  our  popular  ways  of  doing  research.  we  probably  relied  too  much  on  machine 
learning  algorithms.  some  of  the  most  effective  machine  learning  algorithms,  e.g.,  support  vector 
machines  and  conditional  random  fields,  produce  no  human  understandable  results  such  that  although 
they  may  achieve  improved  accuracy,  we  know  little about  how  and  why  apart  from  some  superficial 
knowledge gained in the manual feature engineering process. however, that being said, we have indeed 
made significant progresses over the past few years. this is evident from the large number of start-up 
companies that offer id31 or opinion mining services. there is a real and huge need in the 
industry for such services because every company wants to know how consumers perceive their products 
and  services  and  those  of  their  competitors.  the  same  can  also  be  said  about  consumers  because 
whenever one wants to buy something, one wants to know the opinions of existing users. these practical 
needs and the technical challenges will keep the field vibrant and lively for years to come.  

acknowledgements 
i am very grateful to theresa wilson for her insightful and detailed comments and suggestions, which 
have helped me improve the chapter significantly. i thank my former and current students for working 
with  me  on  this  fascinating  topic:  xiaowen  ding, murthy  ganapathibhotla,  minqing  hu,  nitin  jindal, 
guang qiu (visiting student from zhejiang university) and lei zhang. i would also like to express my 
gratitude to birgit k  nig (mckinsey&company) for many valuable discussions which have helped shape 
my understanding of the practical side of id31 and its related issues.  

 

32 

references 
[1].  a. andreevskaia and s. bergler,    mining id138 for a fuzzy sentiment: sentiment tag extraction 
from  id138  glosses.     proceedings  of  the  european  chapter  of  the  association  for 
computational linguistics (eacl), 2006.  

[2].  n.  archak,  a.  ghose,  and  p.  ipeirotis,     show  me  the  money!  deriving  the  pricing  power  of 
product features by mining consumer reviews,    proceedings of the acm sigkdd conference on 
knowledge discovery and data mining (kdd), 2007. 

[3].  a.  aue  and  m.  gamon,     customizing  sentiment  classifiers  to  new  domains:  a  case  study,    

[4]. 

[5]. 

[6]. 

[7]. 

proceedings of recent advances in natural language processing (ranlp), 2005. 
p. beineke, t. hastie, c. manning, and s. vaithyanathan.    exploring sentiment summarization,    
proceedings  of  the  aaai  spring  symposium  on  exploring  attitude  and  affect  in  text,  aaai 
technical report ss-04-07, 2004. 
s. bethard, h. yu, a. thornton, v. hatzivassiloglou, and d. jurafsky,    automatic extraction of 
opinion  propositions  and  their  holders,     proceedings  of  the  aaai  spring  symposium  on 
exploring attitude and affect in text, 2004. 
j.  blitzer,  m.  dredze,  and  f.  pereira,     biographies,  bollywood,  boom-boxes  and  blenders: 
domain  adaptation 
for 
computational linguistics (acl), 2007. 
j.  bos,  and  m.  nissim,     an  empirical  approach  to  the  interpretation  of  superlatives.    
proceedings of the conference on empirical methods in natural language processing (emnlp), 
2006. 

for  sentiment  classification,     proceedings  of 

the  association 

[8].  e. breck, y. choi, and c. cardie,    identifying expressions of opinion in context,    proceedings of 

the international joint conference on artificial intelligence (ijcai), 2007. 

[9].  g.  carenini,  r.  ng,  and  a.  pauls,     multi-document  summarization  of  evaluative  text,    
proceedings of the european chapter of the association for computational linguistics (eacl), 
pp. 305   312, 2006. 

[10].  g. carenini, r. t. ng, and e. zwart,    extracting knowledge from evaluative text,    proceedings of 

international conference on knowledge capture (k-cap), pp. 11-18, 2005.  

[11].  y.  chen  and  j.  xie,     online  consumer  review:  word-of-mouth  as  a  new  element  of  marketing 

communication mix,    management science, vol. 54, pp. 477   491, 2008. 

[12].  p.  chesley,  b.  vincent,  l.  xu,  and  r.  srihari,     using  verbs  and  adjectives  to  automatically 
classify  blog  sentiment,     in  aaai  symposium  on  computational  approaches  to  analysing 
weblogs (aaai-caaw), pp. 27   29, 2006. 

[13].  y.  choi,  e.  breck,  and  c.  cardie,     joint  extraction  of  entities  and  relations  for  opinion 
recognition,     proceedings  of  the  conference  on  empirical  methods  in  natural  language 
processing (emnlp), 2006. 

[14].  y.  choi,  c.  cardie,  e.  riloff,  and  s.  patwardhan,     identifying  sources  of  opinions  with 
conditional  random  fields  and  extraction  patterns,     proceedings  of  the  human  language 
technology  conference  and  the  conference  on  empirical  methods  in  natural  language 
processing (hlt/emnlp), 2005. 

[15].  s.  r.  das  and  m.  y.  chen,     yahoo!  for  amazon:  sentiment  extraction  from  small  talk  on  the 

web,    management science, vol. 53, pp. 1375   1388, 2007. 

[16].  k. dave, s. lawrence, and d. m. pennock,    mining the peanut gallery: opinion extraction and 

semantic classification of product reviews,    proceedings of www, pp. 519   528, 2003. 

[17].  c.  dellarocas,  x.  zhang,  and  n.  f.  awad,     exploring  the  value  of  online  product  ratings  in 
revenue forecasting: the case of motion pictures,    journal of interactive marketing, vol. 21, pp. 
23   45, 2007. 

[18].  a.  devitt  and  k.  ahmad,     sentiment  analysis  in  financial  news:  a  cohesion  based  approach,    

proceedings of the association for computational linguistics (acl), pp. 984   991, 2007. 

[19].  x.  ding,  b.  liu,  and  p.  s.  yu,     a  holistic  lexicon-based  approach  to  opinion  mining,    

proceedings of the conference on web search and web data mining (wsdm), 2008. 

 

33 

[20].  k.  eguchi  and  v.  lavrenko,     sentiment  retrieval  using  generative  models,     proceedings  of  the 
conference on empirical methods in natural language processing (emnlp), pp. 345   354, 2006. 
[21].  a.  esuli  and  f.  sebastiani,     determining  the  semantic  orientation  of  terms  through  gloss 
analysis,     proceedings  of  the  acm  conference  on  information  and  knowledge  management 
(cikm), 2005. 

[22].  a.  esuli  and  f.  sebastiani,     determining  term  subjectivity  and  term  orientation  for  opinion 
mining,    proceedings of the european chapter of the association for computational linguistics 
(eacl), 2006. 

[23].  a.  esuli  and  f.  sebastiani,     sentiid138:  a  publicly  available  lexical  resource  for  opinion 

mining,    proceedings of language resources and evaluation (lrec), 2006. 

[24].  a. esuli and f. sebastiani,    id95ing id138 synsets: an application to opinion mining,    

proceedings of the association for computational linguistics (acl), 2007. 

[25].  c. fellbaum, ed., id138: an electronic lexical database. mit press, 1998. 
[26].  m. fiszman, d. demner-fushman, f. lang, p. goetz and t. rindflesch,    interpreting comparative 

constructions in biomedical text,    bionlp, 2007.   

[27].  m. gamon,    sentiment classification on customer feedback data: noisy data, large feature vectors, 
and the role of linguistic analysis,    proceedings of the international conference on computational 
linguistics (coling), 2004. 

[28].  m. gamon, a. aue, s. corston-oliver, and e. ringger,    pulse: mining customer opinions from 
free  text,     proceedings  of  the  international  symposium  on  intelligent  data  analysis  (ida),  pp. 
121   132, 2005. 

[29].  g.  ganapathibhotla  and  b.  liu.     identifying  preferred  entities  in  comparative  sentences,    

proceedings of the international conference on computational linguistics, coling, 2008.  

[30].  r.  ghani,  k.  probst,  y.  liu,  m.  krema,  and  a.  fano,     text  mining  for  product  attribute 

extraction,    sigkdd explorations newsletter, vol. 8, pp. 41   48, 2006. 

[31].  a. ghose and p. g. ipeirotis,    designing novel review ranking systems: predicting usefulness and 
impact of reviews,    proceedings of the international conference on electronic commerce (icec), 
2007.  

[32].  a. ghose, p. g. ipeirotis, and a. sundararajan,    opinion mining using econometrics: a case study 
on  reputation  systems,     proceedings  of  the  association  for  computational  linguistics  (acl), 
2007. 

[33].  n. godbole, m. srinivasaiah, and s. skiena,    large-scale id31 for news and blogs,    

proceedings of the international conference on weblogs and social media (icwsm), 2007. 

[34].  v.  hatzivassiloglou  and  k.  mckeown,     predicting  the  semantic  orientation  of  adjectives,    

proceedings of the joint acl/eacl conference, pp. 174   181, 1997. 

[35].  v.  hatzivassiloglou  and  j.  wiebe,     effects  of  adjective  orientation  and  gradability  on  sentence 
subjectivity,     proceedings  of  the  international  conference  on  computational  linguistics 
(coling), 2000. 

[36].  m.  hu  and  b.  liu,     mining  and  summarizing  customer  reviews,     proceedings  of  the  acm 

sigkdd conference on knowledge discovery and data mining (kdd), pp. 168   177, 2004. 

[37].  n. hu, p. a. pavlou, and j. zhang,    can online reviews reveal a product   s true quality?: empirical 
findings  and  analytical  modeling  of  online  word-of-mouth  communication,     proceedings  of 
electronic commerce (ec), pp. 324   330, usa, new york, ny: acm, 2006. 

[38].  n. jindal and b. liu,    identifying comparative sentences in text documents,    proceedings of the 

acm special interest group on information retrieval (sigir), 2006. 

[39].  n. jindal and b. liu,    mining comparative sentences and relations,    proceedings of aaai, 2006. 
[40].  n. jindal and b. liu,    review spam detection,    proceedings of www, 2007. (poster paper). 
[41].  n. jindal and b. liu,    opinion spam and analysis,    proceedings of the conference on web search 

and web data mining (wsdm), pp. 219   230, 2008. 

[42].  n. kaji and m. kitsuregawa,    building lexicon for id31 from massive collection of 
html  documents,     proceedings  of  the  joint  conference  on  empirical  methods  in  natural 

 

34 

language  processing  and  computationalnatural  language  learning  (emnlp-conll),  pp. 
1075   1083, 2007. 

[43].  j.  kamps,  m.  marx,  r.  j.  mokken  and  m.  de  rijke.  using  id138  to  measure  semantic 

orientation of adjectives. in proc. of lrec   04, pp. 1115-1118, 2004. 

[44].  h.  kanayama  and  t.  nasukawa,     fully  automatic  lexicon  expansion  for  domain-oriented 
id31,    proceedings of the conference on empirical methods in natural language 
processing (emnlp), pp. 355   363, july 2006. 

[45].  a. kennedy and d. inkpen,    sentiment classification of movie reviews using contextual valence 

shifters,    computational intelligence, vol. 22, pp. 110   125, 2006. 

[46].  s.-m.  kim  and  e.  hovy,     determining  the  sentiment  of  opinions,     proceedings  of  the 

international conference on computational linguistics (coling), 2004. 

[47].  s.-m.  kim  and  e.  hovy,     automatic  identification  of  pro  and  con  reasons  in  online  reviews,    

proceedings of the coling/acl main conference poster sessions, pp. 483   490, 2006. 

[48].  s.-m. kim and e. hovy,    crystal: analyzing predictive opinions on the web,    proceedings of the 
joint  conference  on  empirical  methods  in  natural  language  processing  and  computational 
natural language learning (emnlp/conll), 2007. 

[49].  s.-m.  kim,  p.  pantel,  t.  chklovski,  and  m.  pennacchiotti,     automatically  assessing  review 
helpfulness,     proceedings  of  the  conference  on  empirical  methods  in  natural  language 
processing (emnlp), pp. 423   430, sydney, australia, july 2006. 

[50].  n. kobayashi, k. inui and y. matsumoto. extracting aspect-evaluation and aspect-of relations 
in opinion mining. proceedings of the 2007 joint conference on empirical methods in natural 
language processing and computational natural language learning, pp. 1065   1074, 2007. 

[51].  n.  kobayashi,  k.  inui,  y.  matsumoto,  k.  tateishi,  and  t.  fukushima.  collecting  evaluative 
expressions  for  opinion  extraction.  proceedings  of  the  1st  international  joint  conference  on 
natural language processing (ijcnlp), pages 584   589, 2004.  

[52].  l.-w. ku, y.-t. liang, and h.-h. chen,    opinion extraction, summarization and tracking in news 
and  blog  corpora,     in  aaai  symposium  on  computational  approaches  to  analysing  weblogs 
(aaai-caaw), pp. 100   107, 2006. 

[53].  l.-w.  ku,  y.-t.  liang,  and  h.-h.  chen,  novel  relationship  discovery  using  opinions  mined 

from the web. aaai 2006.  

[54].  j.  lafferty,  a.  mccallum,  and  f.  pereira,     conditional  random  fields:  probabilistic  models  for 

segmenting and labeling sequence data,    proceedings of icml, pp. 282   289, 2001. 

[55].  b. liu, web data mining: exploring hyperlinks, contents, and usage data. springer, 2006. 
[56].  b. liu, m. hu, and j. cheng,    opinion observer: analyzing and comparing opinions on the web,    

proceedings of www, 2005.  

[57].  j.  liu,  y.  cao,  c.-y.  lin,  y.  huang,  and  m.  zhou.     low-quality  product  review  detection  in 
opinion  summarization,     proceedings  of  the  joint  conference  on  empirical  methods  in  natural 
language processing and computational natural language learning (emnlp-conll), pp. 334   
342, 2007. (poster paper). 

[58].  y.  liu,  j.  huang,  a.  an,  and  x.  yu.     arsa:  a  sentiment-aware  model  for  predicting  sales 
performance  using  blogs,     proceedings  of  the  acm  special  interest  group  on  information 
retrieval (sigir), 2007. 

[59].  y. lu and c. x. zhai. opinion integration through semi-supervised id96. proceedings 

of 2008 international www conference (www   08), pp. 121   130, 2008. 

[60].  c.  macdonald,  i.  ounis,  and  i.  soboroff.  overview  of  the  trec2007  blog  track.  2007. 

http://trec.nist.gov/pubs/trec16/papers/blog.overview16.pdf. 

[61].  j. martin and p. white. the language of evaluation, appraisal in english, palgrave macmillan, 

london, new york, 2005. 

[62].  r. mcdonald, k. hannan, t. neylon, m. wells, and j. reynar,    structured models for fine-to-
coarse id31,    proceedings of the association for computational linguistics (acl), 
pp. 432   439, prague, czech republic: june 2007. 

 

35 

[63].  q. mei, x. ling, m. wondra, h. su, and c. x. zhai,    topic sentiment mixture: modeling facets 

and opinions in weblogs,    proceedings of www, pp. 171   180, 2007.  

[64].  r. mihalcea, c. banea, and j. wiebe,    learning multilingual subjective language via cross-lingual 
projections,    proceedings of the association for computational linguistics (acl), pp. 976   983, 
2007. 

[65].  s. morinaga, k. yamanishi, k. tateishi, and t. fukushima,    mining product reputations on the 
web,    proceedings of the acm sigkdd conference on knowledge discovery and data mining 
(kdd), pp. 341   349, 2002. (industry track). 

[66].  t.  nasukawa  and  j.  yi,     sentiment  analysis:  capturing  favorability  using  natural  language 

processing,    proceedings of the conference on knowledge capture (k-cap), 2003. 

[67].  v. ng, s. dasgupta, and s. m. n. arifin,    examining the role of linguistic knowledge sources in 
the  automatic  identification  and  classification  of  reviews,     proceedings  of  the  coling/acl 
main conference poster sessions, pp. 611   618, sydney, australia: july 2006. 

[68].  x.  ni,  g.-r.  xue,  x.  ling,  y.  yu,  and  q.  yang,     exploring  in  the  weblog  space  by  detecting 

informative and affective articles,    proceedings of www, 2007. 

[69].  k. nigam and m. hurst,    towards a robust metric of polarity,    in computing attitude and affect 
in  text:  theories  and  applications,  number  20  in  the  information  retrieval  series,  (j.  g. 
shanahan, y. qu, and j. wiebe, eds.), 2006. 

[70].  b.  pang  and  l.  lee,     a  sentimental  education:  sentiment  analysis  using  subjectivity 
summarization  based  on  minimum  cuts,     proceedings  of  the  association  for  computational 
linguistics (acl), pp. 271   278, 2004. 

[71].  b. pang and l. lee,    seeing stars: exploiting class relationships for sentiment categorization with 
respect to rating scales,    proceedings of the association for computational linguistics (acl), pp. 
115   124, 2005. 

[72].  b.  pang  and  l.  lee,     opinion  mining  and  sentiment  analysis.     foundations  and  trends  in 

information retrieval 2(1-2), pp. 1   135, 2008. 

[73].  b.  pang,  l.  lee,  and  s.  vaithyanathan,     thumbs  up?  sentiment  classification  using  machine 
learning techniques,    proceedings of the conference on empirical methods in natural language 
processing (emnlp), pp. 79   86, 2002. 

[74].  d.-h. park, j. lee, and i. han,    the effect of on-line consumer reviews on consumer purchasing 
intention:  the  moderating  role  of  involvement,     international  journal  of  electronic  commerce, 
vol. 11, pp. 125   148, 2007. 

[75].  w. parrott. emotions in social psychology, psychology press, philadelphia, 2001. 
[76].  l. polanyi and a. zaenen,    contextual lexical valence shifters,    proceedings of the aaai spring 

symposium on exploring attitude and affect in text, 2004. 

[77].  a.-m.  popescu  and  o.  etzioni,     extracting  product  features  and  opinions  from  reviews,    
proceedings  of  the  human  language technology conference  and  the  conference  on  empirical 
methods in natural language processing (hlt/emnlp), 2005. 

[78].  g.  qiu,  b.  liu,  j.  bu  and  c.  chen.  expanding  domain  sentiment  lexicon  through  double 

propagation, international joint conference on artificial intelligence (ijcai-09), 2009. 

[79].  e. riloff, s. patwardhan, and j. wiebe,    feature subsumption for opinion analysis,    proceedings 

of the conference on empirical methods in natural language processing (emnlp), 2006. 

[80].  e. riloff and j. wiebe,    learning extraction patterns for subjective expressions,    proceedings of 

the conference on empirical methods in natural language processing (emnlp), 2003. 

[81].  e.  riloff,  j.  wiebe,  and  t.  wilson,     learning  subjective  nouns  using  extraction  pattern 
id64,    proceedings of the conference on natural language learning (conll), pp. 25   
32, 2003. 

[82].  s.  sarawagi,     information  extraction,     to  appear  in  foundations  and  trends  in  information 

retrieval, 2009.  

[83].  y. seki, k. eguchi, n. kando, and m. aono,    opinion-focused summarization and its analysis at 
duc 2006,    proceedings of the document understanding conference (duc), pp. 122   130, 2006. 

 

36 

1966. 

[84].  b.  snyder  and  r.  barzilay.  multiple  aspect  ranking  using  the  good  grief  algorithm. 
proceedings of human language technology conference of the north american chapter of the 
association of computational linguistics, proceedings, 2007 hlt-naacl 2007: 300-307. 

[85].  x.  song,  y.  chi,  k.  hino,  and  b.  tseng,     identifying  opinion  leaders  in  the  blogosphere,    
proceedings  of  the  conference  on  information  and  knowledge  management  (cikm),  pp.  971   
974, 2007. 

[86].  a. stepinski and v. mittal,    a fact/opinion classifier for news articles,    proceedings of the acm 

special interest group on information retrieval (sigir), pp. 807   808, 2007.  

[87].  p.  j.  stone. the  general  inquirer:  a computer  approach  to  content  analysis.  the  mit  press, 

[88].  v.  stoyanov  and  c.  cardie,     partially  supervised  coreference  resolution  for  opinion 
summarization  through  structured  rule  learning,     proceedings  of  the  conference  on  empirical 
methods in natural language processing (emnlp), pp. 336   344, sydney, australia: july 2006. 

[89].  q. su, x. xu, h. guo, x. wu, x. zhang, b. swen, and z. su. hidden sentiment association in 

chinese web opinion mining. proceedings of www   08, pp. 959-968, 2008. 

[90].  h.  takamura,  t.  inui,  and  m.  okumura,     extracting  semantic  orientations  of  phrases  from 
dictionary,    proceedings of the joint human language technology/north american chapter of 
the acl conference (hlt-naacl), 2007. 

[91].  l. tesni  re,   l  ments de syntaxe structurale, klincksieck, paris 1959. 
[92].  m.  thomas,  b.  pang,  and  l.  lee,     get  out  the  vote:  determining  support  or  opposition  from 
congressional floor-debate transcripts,    proceedings of the conference on empirical methods in 
natural language processing (emnlp), pp. 327   335, 2006. 
i.  titov  and  r.  mcdonald.  a  joint  model  of  text  and  aspect  ratings  for  sentiment 
summarization.  proceedings  of  46th  annual  meeting  of  the  association  for  computational 
linguistics (acl   08), 2008.  

[94].  r. m. tong,    an operational system for detecting and tracking opinions in on-line discussion.    

[93]. 

proceedings of the workshop on operational text classification (otc), 2001. 

[95].  p.  turney,     thumbs  up  or  thumbs  down?  semantic  orientation  applied  to  unsupervised 
classification of reviews,    proceedings of the association for computational linguistics (acl), 
pp. 417   424, 2002. 

[96].  x.  wan.     using  bilingual  knowledge  and  ensemble  techniques  for  unsupervised  chinese 

id31.    in proceedings of emnlp08, pp. 553-561, 2008. 

[97].  j. wiebe,    learning subjective adjectives from corpora,    proceedings of aaai, 2000. 
[98].  j. wiebe, r. f. bruce, and t. p. o   hara.    development and use of a gold standard data set for 
subjectivity classifications.    proceedings of the association for computational linguistics (acl), 
pp. 246   253, 1999. 

[99].  j.  wiebe  and  r.  mihalcea.     word  sense  and  subjectivity.     proceedings  of  the  conference  on 

computational linguistics / association for computational linguistics (coling/acl), 2006. 

[100].  j.  wiebe  and  t.  wilson,     learning  to  disambiguate  potentially  subjective  expressions,    

proceedings of the conference on natural language learning (conll), pp. 112   118, 2002. 

[101].  j. wiebe, t. wilson and c. cardie. annotating expressions of opinions and emotions in language. 

language resources and evaluation, 1(2), 2005.  

[102].  j.  wiebe,  t.  wilson,  r.  bruce,  m.  bell,  and  m.  martin,     learning  subjective  language,    

computational linguistics, vol. 30, pp. 277   308, september 2004. 

[103].  t.  wilson,  j.  wiebe,  and  r.  hwa,     just  how  mad  are  you?  finding  strong  and  weak  opinion 

clauses,    proceedings of aaai, pp. 761   769, 2004. 

[104].  t. wilson, j. wiebe, and p. hoffmann,    recognizing contextual polarity in phrase-level sentiment 
analysis.     proceedings  of  the  human  language  technology  conference  and  the  conference  on 
empirical methods in natural language processing (hlt/emnlp), pp. 347   354, 2005. 

[105].  h. yang, l. si, and j. callan,    knowledge transfer and opinion detection in the trec2006 blog 

track,    proceedings of trec, 2006. 

 

37 

[106].  j.  yi,  t.  nasukawa,  r.  bunescu,  and  w.  niblack,     sentiment  analyzer:  extracting  sentiments 
about  a  given  topic  using  natural  language  processing  techniques,     proceedings  of  the  ieee 
international conference on data mining (icdm), 2003. 

[107].  h.  yu  and  v.  hatzivassiloglou,     towards  answering  opinion  questions:  separating  facts  from 
opinions  and  identifying  the  polarity  of  opinion  sentences,     proceedings  of  the  conference  on 
empirical methods in natural language processing (emnlp), 2003. 

[108].  w.  zhang,  l.  jia,  c.  yu,  w.  meng.  improve  the  effectiveness  of  the  opinion  retrieval  and 
opinion  polarity  classification.  acm  17th  conference  on  information  and  knowledge 
management (cikm 2008), 2008. 

[109].  w. zhang, and c. yu. uic at trec 2007 blog report., 2007. 
http://trec.nist.gov/pubs/trec16/papers/uic-zhang.blog.final.pdf 

[110].  z.  zhang  and  b.  varadarajan,     utility  scoring  of  product  reviews,     proceedings  of  the  acm 

conference on information and knowledge management (cikm), pp. 51   57, 2006. 

[111].  l.  zhuang,  f.  jing,  x.-y.  zhu,  and  l.  zhang,     movie  review  mining  and  summarization,    
proceedings of the acm conference on information and knowledge management (cikm), 2006. 

 
 

 

38 

