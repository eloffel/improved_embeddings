tutorial given at www-2008, april 21, 2008 in beijing
tutorial given at www 2008, april 21, 2008 in beijing

opinion mining & summarization
opinion mining & summarization

- id31y

bing liu

department of computer science
university of illinois at chicago

liub@cs.uic.edu

http://www cs uic edu/~liub
http://www.cs.uic.edu/~liub

introduction     facts and opinions
(cid:132) two main types of textual information. 

(cid:137) facts and opinions

d o i

f t

i

(cid:132) most current information processing technique 
(e g search engines) work with facts (assume
(e.g., search engines) work with facts (assume 
they are true)
(cid:137) facts can be expressed with topic keywords.
(cid:137) facts can be expressed with topic keywords.

(cid:132) e.g., search engines do not search for opinions

(cid:137) opinions are hard to express with a few keywords

p

p

y

(cid:132) how do people think of motorola cell phones?

(cid:137) current search ranking strategy is not appropriate for 

opinion retrieval/search
opinion retrieval/search.

bing liu, uic                                                   www-2008 tutorial

2

introduction     user generated content

(cid:132) word-of-mouth on the web

(cid:137) one can express personal experiences and opinions on 

almost anything, at review sites, forums, discussion groups, 
(called the user generated content )
blogs ... (called the user generated content.)
blogs

(cid:137) they contain valuable information
(cid:137) web/global scale: no longer     one   s circle of friends
(cid:132) our interest: to mine opinions expressed in the user-

g

g

generated content
(cid:137) an intellectually very challenging problem.
(cid:137) practically very useful. 

bing liu, uic                                                   www-2008 tutorial

3

introduction     applications

(cid:132) businesses and organizations: product and service benchmarking. 

market intelligence
market intelligence. 
(cid:137) business spends a huge amount of money to find consumer 

sentiments and opinions.
consultants surveys and focused groups etc
(cid:132) consultants, surveys and focused groups, etc

(cid:132) individuals: interested in other   s opinions when 

(cid:137) purchasing a product or using a service, 
(cid:137) finding opinions on political topics, 

(cid:132) ads placements: placing ads in the user-generated content

(cid:137) place an ad when one praises a product
(cid:137) place an ad when one praises a product. 
(cid:137) place an ad from a competitor if one criticizes a product.  

(cid:132) opinion retrieval/search: providing general search for opinions. 

bing liu, uic                                                   www-2008 tutorial

4

two types of evaluation

(cid:132) direct opinions: sentiment expressions on 

some objects, e.g., products, events, topics, 
persons.
e g    the picture quality of this camera is great   
(cid:137) e.g.,  the picture quality of this camera is great
(cid:137) subjective

(cid:132) comparisons: relations expressing
(cid:132) comparisons: relations expressing 

j

similarities or differences of more than one 
object. usually expressing an ordering. 
g
(cid:137) e.g.,    car x is cheaper than car y.   
(cid:137) objective or subjective.

p

g

y

bing liu, uic                                                   www-2008 tutorial

5

opinion search (liu, web data mining book, 2007)

p

(cid:132) can you search for opinions as conveniently 
y

p

y

as general web search?

(cid:132) whenever you need to make a decision, you 

, y

y

may want some opinions from others, 
(cid:137) wouldn   t it be nice? you can find them on a search 

y

system instantly, by issuing queries such as 
(cid:132) opinions:    motorola cell phones   
n ki    
(cid:132) comparisons:    motorola vs. nokia   

   m t

c

i

l

(cid:132) cannot be done yet! very hard!

bing liu, uic                                                   www-2008 tutorial

6

typical opinion search queries
yp

p

q

(cid:132) find the opinion of a person or organization (opinion 

( p

p

p

g

holder) on a particular object or a feature of the object. 
(cid:137) e.g., what is bill clinton   s opinion on abortion?

(cid:132) find positive and/or negative opinions on a particular 

object (or some features of the object), e.g., 
(cid:137) customer opinions on a digital camera
(cid:137) customer opinions on a digital camera.
(cid:137) public opinions on a political topic. 

g
(cid:132) find how opinions on an object change over time. 
(cid:132) how object a compares with object b?

p

j

(cid:137) gmail vs. hotmail

bing liu, uic                                                   www-2008 tutorial

find the opinion of a person on x

(cid:132) in some cases the general search engine
(cid:132) in some cases, the general search engine 
can handle it, i.e., using suitable keywords. 
(cid:137) bill clinton   s opinion on abortion
(cid:137) bill clinton s opinion on abortion

(cid:132) reason: 

(cid:137) one person or organization usually has only one
(cid:137) one person or organization usually has only one 

opinion on a particular topic. 

(cid:137) the opinion is likely contained in a single 

p

g

y

document.

(cid:137) thus, a good keyword query may be sufficient. 

bing liu, uic                                                   www-2008 tutorial

7

8

find opinions on an object
we use product reviews as an example:
searching for opinions in product reviews is different
(cid:132) searching for opinions in product reviews is different 
from general web search.
(cid:137) e.g., search for opinions on    motorola razr v3   
(cid:132) general web search (for a fact): rank pages 

g ,

p

according to some authority and relevance scores. 
t)
(cid:137) the user views the first page (if the search is perfect). 
(cid:137) one fact = multiple facts

th fi

(if th

th

h i

t

f

i

(cid:132) opinion search: rank is desirable, however

p

,

(cid:137) reading only the review ranked at the top is not appropriate 

because it is only the opinion of one person. 

(cid:137) one opinion     multiple opinions
(cid:137) one opinion     multiple opinions

bing liu, uic                                                   www-2008 tutorial

9

search opinions (contd)

(cid:132) ranking: 

(cid:137) produce two rankings

ki

d

(cid:132) positive opinions and negative opinions
(cid:132) some kind of summary of both, e.g., # of each

g

y

(cid:137) or, one ranking but 

(cid:132) the top (say 30) reviews should reflect the natural distribution 

of all reviews (assume that there is no spam) i e with the
of all reviews (assume that there is no spam), i.e., with the 
right balance of positive and negative reviews. 

(cid:132) questions:

(cid:137) should the user reads all the top reviews? or
(cid:137) should the system prepare a summary of the reviews?

bing liu, uic                                                   www-2008 tutorial

10

reviews are similar to surveys

(cid:132) reviews can be regarded as traditional 

surveys.
(cid:137) in traditional survey, returned survey forms are 

treated as raw data. 

d

d

(cid:137) analysis is performed to summarize the survey 

results
results. 
(cid:132) e.g., % against or for a particular issue, etc. 

(cid:132) in opinion search
(cid:132) in opinion search, 

(cid:137) can a summary be produced?  
(cid:137) what should the summary be?
(cid:137) what should the summary be?

bing liu, uic                                                   www-2008 tutorial

11

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

12

opinion mining     the abstraction
(hu and liu, kdd-04; liu, web data mining book 2007)

(cid:132) basic components of an opinion
(cid:132) basic components of an opinion

(cid:137) opinion holder: the person or organization that holds a 

specific opinion on a particular object.
obj

(cid:137) object: on which an opinion is expressed
d
(cid:137) opinion: a view, attitude, or appraisal on an object from an 

hi h

t

i

i

i

opinion holder. 

(cid:132) objectives of opinion mining: many ... 
(cid:132) let us abstract the problem

(cid:137) put existing research into a common framework

(cid:132) we use consumer reviews of products to develop the 

ideas other opinionated contexts are similar
ideas. other opinionated contexts are similar. 

bing liu, uic                                                   www-2008 tutorial

13

object/entity
(cid:132) definition (object): an object o is an entity which 
can be a product, person, event, organization, or
can be a product, person, event, organization, or 
topic. o is represented as 
(cid:137) a hierarchy of components, sub-components, and so on.  
(cid:137) each node represents a component and is associated with a
(cid:137) each node represents a component and is associated with a 

set of attributes of the component.

(cid:137) o is the root node (which also has a set of attributes)

(cid:132) an opinion can be expressed on any node or attribute
(cid:132) an opinion can be expressed on any node or attribute 

of the node. 

(cid:132) to simplify our discussion, we use    features    to 

represent both components and attributes
represent both components and attributes.
(cid:137) the term    feature    should be understood in a broad sense,

(cid:132) product feature, topic or sub-topic, event or sub-event, etc  

(cid:132) note: the object o itself is also a feature. 

t o it

n t

lf i

th

bj

f

t

l

bing liu, uic                                                   www-2008 tutorial

14

model of a review
(cid:132) an object o is represented with a finite set of features, 

f }
f  {f1, f2,    , fn}. 
f = {f1 f2
(cid:137) each feature fi in f can be expressed with a finite set of words 

or phrases wi, which are synonyms. 

that is to say: we have a set of corresponding synonym sets w =
that is to say: we have a set of corresponding synonym sets w = 

{w1, w2,    , wn} for the features. 

(cid:132) model of a review: an opinion holder j comments on a
(cid:132) model of a review: an opinion holder j comments on a 

subset of the features sj     f of object o. 
(cid:137) for each feature fk     sj that j comments on, he/she 

(cid:132) chooses a word or phrase from wk to describe the 

feature, and 

(cid:132) expresses a positive, negative or neutral opinion on fk. 
k

p

p

g

p

,

bing liu, uic                                                   www-2008 tutorial

15

opinion mining tasks
(cid:132) at the document (or review) level:
task: sentiment classification of reviews
task: sentiment classification of reviews
(cid:132) classes: positive, negative, and neutral
(cid:132) assumption: each document (or review) focuses on a single 

object (not true in many discussion posts) and contains
object (not true in many discussion posts) and contains 
opinion from a single opinion holder.

(cid:132) at the sentence level:
ti

bj

t k 1 id tif i
task 1: identifying subjective/opinionated sentences

t d

/

t

i

i

(cid:132) classes: objective and subjective (opinionated)

task 2: sentiment classification of sentences

(cid:132) classes: positive, negative and neutral.
(cid:132) assumption: a sentence contains only one opinion 

(cid:137) not true in many cases. 

(cid:132) then we can also consider clauses or phrases.

bing liu, uic                                                   www-2008 tutorial

16

opinion mining tasks (contd)

(cid:132) at the feature level:

task 1: identify and extract object features that have been 
commented on by an opinion holder (e.g., a reviewer). 

task 2: determine whether the opinions on the features are
task 2: determine whether the opinions on the features are 

positive, negative or neutral.  
task 3: group feature synonyms.
(cid:137) produce a feature-based opinion summary of multiple 

reviews (more on this later). 

i

h ld

(cid:132) opinion holders: identify holders is also useful, e.g., 

o i
in news articles, etc, but they are usually known in 
the user generated content, i.e., authors of the posts. 
e pos s

e use ge e a ed co e , e , au o s o

tif h ld

f l

id

i

l

bing liu, uic                                                   www-2008 tutorial

17

more at the feature level

(cid:132) problem 1: both f and w are unknown. 

(cid:137) we need to perform all three tasks:

(cid:132) problem 2: f is known but w is unknown. 

(cid:137) all three tasks are still needed. task 3 is easier. it 
becomes the problem of matching the discovered 
features with the set of given features f
features with the set of given features f. 

(cid:132) problem 3: w is known (f is known too). 

(cid:137) only task 2 is needed. 

y

f: the set of features
w: synonyms of each feature
w: synonyms of each feature

bing liu, uic                                                   www-2008 tutorial

18

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

19

sentiment classification

(cid:132) classify documents (e.g., reviews) based on the 
overall sentiments expressed by opinion holders
overall sentiments expressed by opinion holders 
(authors), 
(cid:137) positive, negative, and (possibly) neutral
,
(cid:137) since in our model an object o itself is also a feature, then 
sentiment classification essentially determines the opinion 
expressed on o in each document (e g review)
expressed on o in each document (e.g., review). 
(cid:132) similar but different from topic-based text 

(p

y)

g

,

classification.
(cid:137) in topic-based text classification, topic words are important. 
(cid:137) in sentiment classification, sentiment words are more 

important e g great excellent horrible bad worst etc
important, e.g., great, excellent, horrible, bad, worst, etc. 

bing liu, uic                                                   www-2008 tutorial

20

unsupervised review classification
(turney, acl-02)

(cid:132) data: reviews from epinions com on
(cid:132) data: reviews from epinions.com on 

automobiles, banks, movies, and travel 
destinations.

(cid:132) the approach: three steps
(cid:132) step 1:
(cid:132) step 1:

(cid:137) part-of-speech tagging
(cid:137) extracting two consecutive words (two-word
(cid:137) extracting two consecutive words (two-word 
phrases) from reviews if their tags conform to 
some given patterns, e.g., (1) jj, (2) nn.

bing liu, uic                                                   www-2008 tutorial

21

(cid:132) step 2: estimate the semantic orientation
(cid:132) step 2: estimate the semantic orientation 

(so) of the extracted phrases
(cid:137) use pointwise mutual information

pmi

(

word
1

,

word

2

)

=

log

2

   
      
   
   

p
word
(
1
p
word
word
p
(
)
(
)
1
1

   
p
p

word
2
word
word
(
(

)
)
)

2
2

   
      
   
   

(cid:137) semantic orientation (so): 

so(phrase) = pmi(phrase,    excellent   )

- pmi(phrase,    poor   )

(cid:137) using altavista near operator to do search to find 

the number of hits to compute pmi and so
the number of hits to compute pmi and so. 

bing liu, uic                                                   www-2008 tutorial

22

(cid:132) step 3: compute the average so of all
(cid:132) step 3: compute the average so of all 

phrases
(cid:137) classify the review as recommended if average
(cid:137) classify the review as recommended if average 

so is positive, not recommended otherwise. 

(cid:132) final classification accuracy:

(cid:137) automobiles - 84%
(cid:137) automobiles  84%
(cid:137) banks - 80%
(cid:137) movies - 65.83 
(cid:137) travel destinations - 70.53%

bing liu, uic                                                   www-2008 tutorial

23

sentiment classification using machine 
learning methods (pang et al, emnlp-02)
(cid:132) this paper directly applied several machine
this paper directly applied several machine 
learning techniques to classify movie reviews 
into positive and negative. 

(cid:132) three classification techniques were tried:

(cid:137) na  ve bayes
(cid:137) maximum id178
(cid:137) support vector machine
p
i
(single words), bigram, pos tag, position.

(cid:132) pre-processing settings: negation tag, unigram 

tti

ti

t

i

(cid:132) id166: the best accuracy 83% (unigram)
(cid:132) id166: the best accuracy 83% (unigram) 

bing liu, uic                                                   www-2008 tutorial

24

review classification by scoring features 
(dave, lawrence and pennock, www-03)

(cid:132) it first selects a set of features f = f
(cid:132) it first selects a set of features f = f1, f2,        

f

(cid:137) note: machine learning features, but product features. 
(
(
(

cfpcfp
(
(
cfpcfp
(

(cid:132) score the features 

score
(
(

   
+

)
)
)

=

|
|
|

|
|
|

)
)

f
f

f

f

i
i

i
i

i

)'
)
)'

i

i

(cid:137) c and c    are classes

(cid:132) classification of a 

review dj (using sign):
review dj (using sign): 

(cid:132) accuracy of 84-88%
(cid:132) accuracy of 84 88%. 

class

(

d

j

)

eval

(

d

)

j

=

c
   
   
c
'
   
   
   =
score

eval
eval
f
(

)
)

j

j
j

>
<

0
0

d
(
d
(
)

i

i

bing liu, uic                                                   www-2008 tutorial

25

other related works

(cid:132) using pmi, syntactic relations and other attributes with 

id166 (mullen and collier, emnlp-04).
id166 (mullen and collier, emnlp 04). 

(cid:132) sentiment classification considering rating scales (pang 

and lee, acl-05).

(cid:132) comparing supervised and unsupervised methods
(cid:132) comparing supervised and unsupervised methods 

(chaovalit and zhou, hicss-05)

(cid:132) using semi-supervised learning (goldberg and zhu, 

workshop on textgraphs at hlt-naal-06)
workshop on textgraphs, at hlt naal 06). 

(cid:132) review identification and sentiment classification of 

reviews (ng, dasgupta and arifin, acl-06). 

(cid:132) sentiment classification on customer feedback data
(cid:132) sentiment classification on customer feedback data 

(gamon, coling-04). 

(cid:132) comparative experiments (cui et al. aaai-06)
(cid:132) many more
(cid:132) many more    

bing liu, uic                                                   www-2008 tutorial

26

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

27

sentence-level id31

(cid:132) document-level sentiment classification is too coarse
document level sentiment classification is too coarse 
for most applications. 

(cid:132) let us move to the sentence level. 
(cid:132) much of the work on sentence level sentiment 

l

ti

analysis focuses on identifying subjective sentences
i
in news articles.
(cid:137) classification: objective and subjective. 
(cid:137) all techniques use some forms of machine learning.
(cid:137) all techniques use some forms of machine learning. 
(cid:137) e.g., using a na  ve bayesian classifier with a set of data 

features/attributes extracted from training sentences (wiebe 
et al acl 99)
et al. acl-99).

bing liu, uic                                                   www-2008 tutorial

28

using learnt patterns (rilloff  and wiebe, emnlp-03)

(cid:132) a id64 approach.
fi

ifi

i

l

(cid:137) a high precision classifier is first used to automatically 

a hi h
i
identify some subjective and objective sentences.
(cid:132) two high precision (but low recall) classifiers are used, 

d

ll

i

i

(cid:137) a high precision subjective classifier
(cid:137) a high precision objective classifier
(cid:137) based on manually collected lexical items, single words and n-

grams which are good subjective clues
grams, which are good subjective clues.

(cid:137) a set of patterns are then learned from these identified 

subjective and objective sentences. 
(cid:132) syntactic templates are provided to restrict the kinds of patterns
(cid:132) syntactic templates are provided to restrict the kinds of patterns 

to be discovered, e.g., <subj> passive-verb.

(cid:137) the learned patterns are then used to extract more subject 

and objective sentences (the process can be repeated).
and objective sentences (the process can be repeated). 

bing liu, uic                                                   www-2008 tutorial

29

subjectivity and polarity (orientation) 
(yu and hazivassiloglou, emnlp-03)
(cid:132) for subjective or opinion sentence identification, three 

methods are tried
methods are tried:
(cid:137) sentence similarity.
(cid:137) na  ve bayesian classification.
(cid:137) multiple na  ve bayesian (nb) classifiers. 

y

(cid:132) for opinion orientation (positive, negative or neutral) 

(also called polarity) classification it uses a similar
(also called polarity) classification, it uses a similar 
method to (turney, acl-02), but 
(cid:137) with more seed words (rather than two) and based on log-

lik lih d
(llr)
likelihood ratio (llr). 

ti

(cid:137) for classification of each word, it takes the average of llr 
scores of words in the sentence and use cutoffs to decide 
l
positive, negative or neutral. 

iti

ti

t

bing liu, uic                                                   www-2008 tutorial

30

other related work

(cid:132) consider gradable adjectives (hatzivassiloglou and wiebe, coling-

(cid:132) semi-supervised learning with the initial training set identified by 

some strong patterns and then applying nb or self-training (wiebe 
and riloff, cicling-05).

(cid:132) finding strength of opinions at the clause level (wilson et al. aaai-

00)00)

04).

(cid:132) sum up orientations of opinion words in a sentence (or within some 

word window) (kim and hovy, coling-04). 
coling 04)

d i d

) (ki

d h

(cid:132) find clause or phrase polarities based on priori opinion words and 

(cid:132) semi-supervised learning to classify sentences in reviews (gamon 

i d l

(g

t

t

i

i

i

classification (wilson et al. emnlp-05)
s
et al. ida-05).

if

i

l

(cid:132) sentiment sentence retrieval (eguchi and lavrendo, emnlp-06)

bing liu, uic                                                   www-2008 tutorial

31

let us go further?

(cid:132) sentiment classification at both document and 

sentence (or clause) levels are useful, but 
(cid:137) they do not find what the opinion holder liked and disliked.

(cid:132) an negative sentiment on an object 

(cid:137) does not mean that the opinion holder dislikes everything 

about the object
about the object.

(cid:132) a positive sentiment on an object 

(cid:137) does not mean that the opinion holder likes everything about 

the object.

(cid:132) we need to go to the feature level.

g

bing liu, uic                                                   www-2008 tutorial

32

but before we go further

(cid:132) let us discuss opinion words or phrases (also 

called polar words opinion bearing words etc) e g
called polar words, opinion bearing words, etc). e.g., 
(cid:137) positive: beautiful, wonderful, good, amazing, 
(cid:137) negative: bad, poor, terrible, cost someone an arm and a leg 

(idiom)
(idiom). 

(cid:132) they are instrumental for opinion mining (obviously)
(cid:132) three main ways to compile such a list:

(cid:137) manual approach: not a bad idea, only an one-time effort
(cid:137) corpus-based approaches
(cid:137) dictionary-based approaches

(cid:132) important to note: 

(cid:137) some opinion words are context independent (e.g., good).
some are context dependent (e g long)
(cid:137) some are context dependent (e.g., long).

bing liu, uic                                                   www-2008 tutorial

33

corpus-based approaches

(cid:132) rely on syntactic or co-occurrence patterns in large 
corpora. (hazivassiloglou and mckeown, acl-97; turney, acl-
02; yu and hazivassiloglou, emnlp-03; kanayama and nasukawa, 
emnlp-06; ding and liu sigir-07)
(cid:137) can find domain (not context!) dependent orientations 

t!) d
c fi d d
(positive, negative, or neutral). 

d t

t ti

(

t

t

i

i

(cid:132) (turney, acl-02) and (yu and hazivassiloglou, 

emnlp-03) are similar. 
(cid:137) assign opinion orientations (polarities) to words/phrases. 
(cid:137) (yu and hazivassiloglou emnlp-03) is different from
(cid:137) (yu and hazivassiloglou, emnlp 03) is different from 

(turney, acl-02)
(cid:132) use more seed words (rather than two) and use log-

likelihood ratio (rather than pmi).
likelihood ratio (rather than pmi). 

bing liu, uic                                                   www-2008 tutorial

34

corpus-based approaches (contd)

(cid:132) use constraints (or conventions) on connectives to identify 

opinion words (hazivassiloglou and mckeown, acl-97; kanayama 
and nasukawa, emnlp-06; ding and liu, 2007). e.g.,
(cid:132) conjunction: conjoined adjectives usually have the same
conjunction: conjoined adjectives usually have the same 
orientation (hazivassiloglou and mckeown, acl-97). 

(cid:132) e.g.,    this car is beautiful and spacious.    (conjunction)

(cid:137) and or but either or and neither nor have similar
(cid:137) and, or, but, either-or, and neither-nor have similar 

constraints.

(cid:137) learning using

(cid:132)

log linear model: determine if two conjoined adjectives are of the same or
log-linear model: determine if two conjoined adjectives are of the same or 
different orientations. 

(cid:132) id91: produce two sets of words: positive and negative

(cid:137) corpus: 21 million word 1987 wall street journal corpus
(cid:137) corpus: 21 million word 1987 wall street journal corpus. 

bing liu, uic                                                   www-2008 tutorial

35

corpus-based approaches (contd)
(cid:132) (kanayama and nasukawa, emnlp-06) takes a 

similar approach to (hazivassiloglou and mckeown, 
acl 97) but for japanese words:
acl-97) but for japanese words:
(cid:137) instead of using learning, it uses two criteria to determine 

whether to add a word to positive or negative lexicon. 
have an initial seed lexicon of positive and negative words
(cid:137) have an initial seed lexicon of positive and negative words. 

(cid:132) (ding and liu, 2007) also exploits constraints on 

connectives, but with two differences
(cid:137) it uses them to assign opinion orientations to product 

i
features (more on this later). 
(cid:132) one word may indicate different opinions in the 

h

d

i

i

i

i

i

same domain
same domain. 
(cid:137)    the battery life is long    (+) and    it takes a long time to focus    (-).

(cid:132) find domain opinion words is insufficient.
it can be used without a large corpus
(cid:137) it can be used without a large corpus.

bing liu, uic                                                   www-2008 tutorial

36

dictionary-based approaches

(cid:132) typically use id138   s synsets and hierarchies to 

acq ire opinion ords
acquire opinion words
(cid:137) start with a small seed set of opinion words.
(cid:137) use the set to search for synonyms and antonyms in 

y

y

y

id138 (hu and liu, kdd-04; kim and hovy, coling-04).

(cid:137) manual inspection may be used afterward.

(cid:132) use additional information (e g glosses) from
(cid:132) use additional information (e.g., glosses) from 

id138 (andreevskaia and bergler, eacl-06) and 
learning (esuti and sebastiani, cikm-05).
w k
(cid:132) weakness of the approach: do not find context 
t
dependent opinion words, e.g., small, long, fast. 

t fi d

h d

f th

t

bing liu, uic                                                   www-2008 tutorial

37

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

38

feature-based opinion mining and 
summarization (hu and liu, kdd-04)

(cid:132) again focus on reviews (easier to work in a concrete 

domain!)

(cid:132) objective: find what reviewers (opinion holders)
(cid:132) objective: find what reviewers (opinion holders) 

liked and disliked
(cid:137) product features and opinions on the features

(cid:132) since the number of reviews on an object can be 
large, an opinion summary should be produced. 
(cid:137) desirable to be a structured summary
(cid:137) desirable to be a structured summary.
(cid:137) easy to visualize and to compare.
(cid:137) analogous to but different from multi-document 

summarization
summarization. 

bing liu, uic                                                   www-2008 tutorial

39

the tasks

(cid:132) recall the three tasks in our model. 

task 1: extract object features that have been 

commented on in each review. 

task 2: determine whether the opinions on the 

features are positive, negative or neutral.  

t

f

t k 3 g
task 3: group feature synonyms.
(cid:137) produce a summary 
t k 2
t b
format of reviews. 

(cid:132) task 2 may not be needed depending on the 

d d d

th

di

bing liu, uic                                                   www-2008 tutorial

40

different review format 

i

format 1 - pros, cons and detailed review: the 
reviewer is asked to describe pros and cons 
separately and also write a detailed review. 
epinions com uses this format
epinions.com uses this format. 

k d t d

ib p

d c

i

format 2 - pros and cons: the reviewer is 

k d t d

asked to describe pros and cons separately. 
cnet.com used to use this format. 

ib p

d c

t l

f

t 3 f

it
f
format 3 - free format: the reviewer can write 
freely, i.e., no separation of pros and cons. 
amazon com uses this format
amazon.com uses this format. 

t th

i

bing liu, uic                                                   www-2008 tutorial

41

format 1

format 2

format 3
format 3
great camera., jun 3, 2004 
reviewer: jprice174 from atlanta, ga.

i did a lot of research last year before i bought
i did a lot of research last year before i bought 
this camera... it kinda hurt to leave behind my 
beloved nikon 35mm slr, but i was going to 
italy, and i needed something smaller, and 
digital.
digital. 
the pictures coming out of this camera are 
amazing. the 'auto' feature takes great 
pictures most of the time. and with digital, 
you're not wasting film if the picture doesn't
you re not wasting film if the picture doesn t 
come out. 

bing liu, uic                                                   www-2008 tutorial

42

feature-based opinion summary (hu and liu, 
kdd-04)

feature based summary:

great camera., jun 3, 2004 
reviewer: jprice174 from atlanta, 

ga.
i did a lot of research last year
before i bought this camera
before i bought this camera... it
it
kinda hurt to leave behind my
beloved nikon 35mm slr, but i
was going to italy, and i needed
something smaller and digital
something smaller, and digital.
the pictures coming out of this
camera are amazing. the 'auto'
feature takes great pictures
th
the time. and with
f
most of
ith
digital, you're not wasting film if
the picture doesn't come out.    

a d

ti

t

feature1: picture
p
positive: 12
(cid:132) the pictures coming out of this camera 

are amazing. 

(cid:132) overall this is a good camera with a 

really good picture clarity.

d i t

it

ll

l

   
negative: 2
(cid:132) the pictures come out hazy if your 
hands shake even for a moment
hands shake even for a moment 
during the entire process of taking a 
picture.

(cid:132) focusing on a display rack about 20 
feet away in a brightly lit room during 
day time, pictures produced by this 
day time pictures produced by this
camera were blurry and in a shade of 
orange.

   .

feature2: battery life
feature2: battery life
   

bing liu, uic                                                   www-2008 tutorial

43

visual summarization & comparison
(cid:132) summary of 

+
+

reviews of 
digital camera 1

zoom

size 

weight

(cid:132) comparison of 

_
picture battery
comparison of +
+
reviews of 
digital camera 1
digital camera 1 
digital camera 2

_

bing liu, uic                                                   www-2008 tutorial

44

feature extraction from pros and cons of 
format 1 (liu et al www-03; hu and liu, aaai-caaw-05)
(cid:132) observation: each sentence segment in pros or 

cons contains only one feature. sentence segments 
can be separated by commas, periods, semi-colons, 
hyphens,    &      s,    and      s,    but      s, etc. 

(cid:132) pros in example 1 can be separated into 3 segments:

great photos
great photos 
easy to use   
very small

<photo>
<photo>
<use>
<small>     <size>

(cid:132) cons can be separated into 2 segments:

battery usage
<battery>
<memory>
included memory is stingy  <memory>
included memory is stingy

bing liu, uic                                                   www-2008 tutorial

45

extraction using label sequential rules

(cid:132) label sequential rules (lsr) are a special kind of 
sequential patterns discovered from sequences
sequential patterns, discovered from sequences. 
(cid:132) lsr mining is supervised (liu   s web mining book 2006).
(cid:132) the training data set is a set of sequences e g
(cid:132) the training data set is a set of sequences, e.g., 

   included memory is stingy   

is turned into a sequence with pos tags
is turned into a sequence with pos tags. 

   {included, vb}{memory, nn}{is, vb}{stingy, jj}   

then turned into
then turned into 

   {included, vb}{$feature, nn}{is, vb}{stingy, jj}   

bing liu, uic                                                   www-2008 tutorial

46

using lsrs for extraction
(cid:132) based on a set of training sequences, we can 

mine label sequential rules e g
mine label sequential rules, e.g., 
   {easy, jj }{to}{*, vb}           {easy, jj}{to}{$feature, vb}   

[sup = 10%, conf = 95%]
%

%

f

feature extraction

(cid:137) only the right hand side of each rule is needed.
(cid:137) the word in the sentence segment of a new review 

th t
t d
that matches $feature is extracted. 

t h

$f

t

t

i

(cid:137) we need to deal with conflict resolution also 

(multiple rules are applicable
(multiple rules are applicable. 

bing liu, uic                                                   www-2008 tutorial

47

extraction of features of formats 2 and 3

(cid:132) reviews of these formats are usually 

complete sentences
e.g.,    the pictures are very clear.   
(cid:137) explicit feature: picture

k t

(cid:132)    it is small enough to fit easily in a coat 

pocket or purse.   
   
(cid:137) implicit feature: size
extraction: frequency based approach
(cid:132) extraction: frequency based approach
(cid:137) frequent features
(cid:137) infrequent features
(cid:137) infrequent features

bing liu, uic                                                   www-2008 tutorial

48

frequency based approach
(hu and liu, kdd-04; liu, web data mining book 2007)

(cid:132) frequent features: those features that have been talked
frequent features: those features that have been talked 
about by many reviewers. 

(cid:132) use sequential pattern mining
h?
(cid:132) why the frequency based approach? 

wh th f
(cid:137) different reviewers tell different stories (irrelevant)
(cid:137) when product features are discussed, the words that
(cid:137) when product features are discussed, the words that 

b

d

they use converge. 

(cid:137) they are main features. 
s

(cid:132) sequential pattern mining finds frequent phrases.
(cid:132) froogle has an implementation of the approach (no pos 

f

f

restriction).
es c o )

bing liu, uic                                                   www-2008 tutorial

49

using part-of relationship and the web
(popescu and etzioni, emnlp-05)
(cid:132) improved (hu and liu, kdd-04) by removing those 

frequent noun phrases that may not be features: 
better precision (a small drop in recall). 
it id
(cid:137) each noun phrase is given a pointwise mutual information 

(cid:132) it identifies part-of relationship
hi

tifi

ti

t

f

l

score between the phrase and part discriminators
associated with the product class, e.g., a scanner class. 

p

p

(cid:137) the part discriminators for the scanner class are,    of 

scanner       scanner has       scanner comes with    etc which
scanner ,  scanner has ,  scanner comes with , etc, which 
are used to find components or parts of scanners by 
searching on the web: the knowitall approach, (etzioni et 
al www 04)
al, www-04). 

bing liu, uic                                                   www-2008 tutorial

50

infrequent features extraction

ti

(cid:132) how to find the infrequent features?
d
(cid:132) observation: the same opinion word can be used 
d

ob
b
to describe different features and objects. 
(cid:137)    the pictures are absolutely amazing.   
g
(cid:137)    the software that comes with it is amazing.   

th

p

y

i

i

(cid:132) frequent 
features

(cid:132) infrequent 

features

(cid:132) opinion words

p

bing liu, uic                                                   www-2008 tutorial

51

identify feature synonyms
(cid:132) liu et al (www-05) made an attempt using only 

id138
id138.

(cid:132) carenini et al (k-cap-05) proposed a more 

sophisticated method based on several similarity 
metrics, but it requires a taxonomy of features to be 
given. 
(cid:137) the system merges each discovered feature to a feature 

f f

g

y

node in the taxonomy. 

(cid:137) the similarity metrics are defined based on string similarity, 

synonyms and other distances measured using id138. 
y

g

y

(cid:137) experimental results based on digital camera and dvd 

reviews show promising results. 

(cid:132) many ideas in information integration are applicable
(cid:132) many ideas in information integration are applicable.

bing liu, uic                                                   www-2008 tutorial

52

identify opinion orientation on feature
(cid:132) for each feature, we identify the sentiment or opinion 

orientation expressed by a reviewer.
orientation expressed by a reviewer. 

(cid:132) we work based on sentences, but also consider,

(cid:137) a sentence can contain multiple features. 
different features may have different opinions
(cid:137) different features may have different opinions. 
(cid:137) e.g., the battery life and picture quality are great (+), but the 

view founder is small (-).  

(cid:132) almost all approaches make use of opinion words
(cid:132) almost all approaches make use of opinion words

and phrases. but notice again:
(cid:137) some opinion words have context independent orientations, 

e g    great   
e.g.,  great .

(cid:137) some other opinion words have context dependent 

orientations, e.g.,    small   
(cid:132) many ways to use them
(cid:132) many ways to use them. 

bing liu, uic                                                   www-2008 tutorial

53

aggregation of opinion words 
(h
2008)
(hu and liu, kdd-04; ding and liu, 2008)

d li kdd 04 di

d li

(cid:132) input: a pair (f, s), where f is a product feature and s is a 

sentence that contains f. 
f

th t

t

t

i

(cid:132) output: whether the opinion on f in s is positive, negative, or 

neutral. 
two steps:
(cid:132) two steps: 
(cid:137) step 1: split the sentence if needed based on but words 

(but, except that, etc). 

(cid:132) in (ding et al, wsdm-08), step 2 is changed to     n
    =i

(cid:137) step 2: work on the segment s containing f let the set of
(cid:137) step 2: work on the segment sf containing f. let the set of 
opinion words in sf be w1, .., wn. sum up their orientations 
(1, -1, 0), and assign the orientation to (f, s) accordingly. 
i ow .
(
fwd1
,
(
)
i
with better results. wi.o is the opinion orientation of wi. d(wi, f) 
is the distance from f to wi.
i

),

g

p

g

,

bing liu, uic                                                   www-2008 tutorial

54

context dependent opinions

p

p

(cid:132) popescu and etzioni (emnlp-05) used

(cid:137) constraints of connectives in (hazivassiloglou and mckeown, 

acl-97), and some additional constraints, e.g., 
morphological relationships, synonymy and antonymy, and 

(cid:137) relaxation labeling to propagate opinion orientations to words 

and features.

(cid:132) ding et al (2008) used
(cid:132) ding et al (2008) used 

(cid:137) constraints of connectives both at intra-sentence and inter-

sentence levels, and 

(cid:137) additional constraints of e g too but negation
(cid:137) additional constraints of, e.g.,  too, but, negation,    .
to directly assign opinions to (f, s) with good results (>
0.85 of f-score). 

bing liu, uic                                                   www-2008 tutorial

55

some other related work

(cid:132) morinaga et al. (kdd-02). 
yi et al (icdm 03)
(cid:132) yi et al. (icdm-03)
(cid:132) kobayashi et al. (aaai-caaw-05)
(cid:132) ku et al (aaai-caaw-05)
(cid:132) ku et al. (aaai caaw 05)
(cid:132) carenini et al (eacl-06)
(cid:132) kim and hovy (acl-06a)
(cid:132) kim and hovy (acl-06b)
(cid:132) eguchi and lavrendo (emnlp-06)
(cid:132) zhuang et al (cikm-06)
(cid:132) mei et al (www-2007)
(cid:132) many more 

m

bing liu, uic                                                   www-2008 tutorial

56

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

57

extraction of comparatives
(jinal and liu, sigir-06, aaai-06; liu   s web data mining book)

(cid:132) recall: two types of evaluation
(cid:132) recall: two types of evaluation
(cid:137) direct opinions:    this car is bad    
(cid:137) comparisons:    car x is not as good as car y   
(cid:132) they use different language constructs. 
(cid:132) direct expression of sentiments are good. 

i

c
comparison may be better. 
(cid:137) good or bad, compared to what?
(cid:132) comparative sentence mining
(cid:132) comparative sentence mining

b b tt

(cid:137) identify comparative sentences, and 
(cid:137) extract comparative relations from them. 

p

bing liu, uic                                                   www-2008 tutorial

58

linguistic perspective
(cid:132) comparative sentences use morphemes like

(cid:137) more/most-er/-estless/leastand as
(cid:137) more/most, er/est, less/leastand as.

(cid:132) thanand asare used to make a    standard    against

which an entity is compared.

limitations
(cid:132) limited coverage

it d

li
(cid:137) ex:    in market capital, intel is way ahead of amd   

(cid:132) non-comparatives with comparative words
(cid:132) non comparatives with comparative words

(cid:137) ex1:    in the context of speed, faster means better   

(cid:132) for human consumption; no computational methods

bing liu, uic                                                   www-2008 tutorial

59

types of comparatives: gradable
(cid:132) gradable

(cid:137) non-equal gradable: relations of the type greater or
(cid:137) non equal gradable: relations of the type greater or 

less than
(cid:132) keywords like better, ahead, beats, etc
(cid:132) ex:    optics of camera a is better than that of camera b   

(cid:137) equative: relations of the type equal to

(cid:132) keywords and phrases like equal to, same as, both, all
(cid:132) keywords and phrases like equal to, same as, both, all
(cid:132) ex:    camera a and camera b both come in 7mp   

(cid:137) superlative: relations of the type greater or less than 

th

all others
ll
(cid:132) keywords and phrases like best, most, better than all
(cid:132) ex:    camera a is the cheapest camera available in market   

p

bing liu, uic                                                   www-2008 tutorial

60

types of comparatives: non-gradable

(cid:132) non-gradable: sentences that compare
(cid:132) non gradable: sentences that compare 

features of two or more objects, but do not 
grade them. sentences which imply: 
g
p y
(cid:137) object a is similar to or different from object b 

with regard to some features. 

(cid:137) object a has feature f1, object b has feature f2

(f1 and f2 are usually substitutable). 

(cid:137) object a has feature f, but object b does not 

have. 

bing liu, uic                                                   www-2008 tutorial

61

comparative relation: gradable

(cid:132) definition: a gradable comparative relation

captures the essence of a gradable comparative 
sentence and is represented with the following:
(relationword, features, entitys1, entitys2, type)
(cid:137) relationword: the keyword used to express a 

comparative relation in a sentence
comparative relation in a sentence.

(cid:137) features: a set of features being compared.
(cid:137) entitys1 and entitys2: sets of entities being
(cid:137) entitys1 and entitys2: sets of entities being 

compared. 

(cid:137) type: non-equal gradable, equative or superlative.  

bing liu, uic                                                   www-2008 tutorial

62

examples: comparative relations

(cid:132)

(cid:132)

(cid:132)

(cid:132)
(cid:132)

g

q

ex1:    car x has better controls than car y   
(relationword = better, features = controls, entitys1 = car x, 
entitys2 = car y, type = non-equal-gradable)
ex2:    car x and car y have equal mileage   
(relationword = equal, features = mileage, entitys1 = car x, 
entitys2 = car y, type = equative)
ex3:    car x is cheaper than both car y and car z   
ex3:  car x is cheaper than both car y and car z
(relationword = cheaper, features = null, entitys1 = car x, entitys2
= {car y, car z}, type = non-equal-gradable )
ex4:    company x produces a variety of cars but still
ex4:  company x produces a variety of cars, but still 
best cars come from company y   
(relationword = best, features = cars, entitys1 = company y, 
entitys2 = null, type = superlative)
entitys2
 superlative)

null, type 

bing liu, uic                                                   www-2008 tutorial

63

tasks

given a collection of evaluative texts
given a collection of evaluative texts
task 1: identify comparative sentences.
task 2: categorize different types of
task 2: categorize different types of 

comparative sentences.  

t k 2 e t
task 2: extract comparative relations from the 

l ti

th

ti

f

t
sentences. 

bing liu, uic                                                   www-2008 tutorial

64

y

identify comparative sentences 
(jinal and liu, sigir-06)
keyword strategy
gy
(cid:132) an observation:  it is easy to find a small set of 

keywords that covers almost all comparative 
sentences, i.e., with a very high recall and a 
reasonable precision 

y g

,

,

(cid:132) we have compiled a list of 83 keywords used in 

comparative sentences which includes:
comparative sentences, which includes:
(cid:137) words with pos tags of jjr, jjs, rbr, rbs

(cid:132) pos tags are used as keyword instead of individual 

words.
words.

(cid:132) exceptions: more, less, most and least

(cid:137) other indicative words like beat, exceed, ahead, etc
(cid:137) phrases like in the lead on par with etc
(cid:137) phrases like in the lead, on par with, etc

bing liu, uic                                                   www-2008 tutorial

65

2-step learning strategy

p

(cid:132) step1: extract sentences which contain at 
least a keyword (recall = 98%, precision = 
32% on our data set for gradables)

(cid:132) step2: use the na  ve bayes (nb) classifier to 

y

classify sentences into two classes 
(cid:137) comparative and non-comparative. 
(cid:137) attributes: class sequential rules (csrs) 

generated from sentences in step1, e.g., 
   {1}{3}{7, 8}        classi [sup = 2/5, conf = 3/4]

bing liu, uic                                                   www-2008 tutorial

66

1. sequence data preparation

(cid:137) use words within radius r of a keyword to form a 

sequence (words are replaced with pos tags)

(cid:137)    .

2. csr generation
i
i

t

(cid:137) use different minimum supports for different 
t

u diff
diff
keywords (multiple minimum supports)

t

f

(cid:137) 13 manual rules which were hard to generate
(cid:137) 13 manual rules, which were hard to generate 

automatically.

3 learning using a nb classifier
3. learning using a nb classifier

(cid:137) use csrs and manual rules as attributes to build 

a final classifier.

bing liu, uic                                                   www-2008 tutorial

67

classify different types of comparatives
classify different types of comparatives

(cid:132) classify comparative sentences into three
classify comparative sentences into three 
types: non-equal gradable, equative, and 
superlative
superlative
(cid:137) id166 learner gave the best result.
(cid:137) attribute set is the set of keywords
(cid:137) attribute set is the set of keywords.
(cid:137) if the sentence has a particular keyword in the 

attribute set, the corresponding value is 1,
attribute set, the corresponding value is 1, 
and 0 otherwise.

bing liu, uic                                                   www-2008 tutorial

68

extraction of comparative relations
(jindal and liu, aaai-06; liu   s web mining book 2006)

assumptions
assumptions
(cid:132) there is only one relation in a sentence. 
(cid:132) entities and features are nouns (includes nouns, 

plural nouns and proper nouns) and pronouns.
(cid:137) adjectival comparatives
(cid:137) does not deal with adverbial comparatives

p

(

3 steps
(cid:132) sequence data generation
label sequential rule (lsr) generation
(cid:132) label sequential rule (lsr) generation
(cid:132) build a sequential cover/extractor from lsrs

bing liu, uic                                                   www-2008 tutorial

69

sequence data generation 

label set =  {$entitys1, $entitys2, $feature}
(cid:132)
(cid:132) three labels are used as pivots to generate 

sequences.
(cid:137) radius of 4 for optimal results

(cid:132) following words are also added

d

(cid:137) distance words = {l1, l2, l3, l4, r1, r2, r3, r4}, 
{l1 l2 l3 l4 1 2 3 4}

di t
where    li    means distance of i to the left of the 
pivot.
p
   ri    means the distance of i to the right of pivot.

(cid:137) special words #start and #end are used to mark 

the start and the end of a sentence
the start and the end of a sentence.

bing liu, uic                                                   www-2008 tutorial

70

sequence data generation example

the comparative sentence
   canon/nnp has/vbz better/jjr optics/nns    has 

$entitys1    canon    and $feature    optics   .

sequences are:
sequences are:
(cid:132)    {#start}{l1}{$entitys1, nnp}{r1}{has, vbz }{r2 }

{better jjr}{r3}{$feature nns}{r4}{#end}   
{better, jjr}{r3}{$feature, nns}{r4}{#end}   

(cid:132)    {#start}{l4}{$entitys1, nnp}{l3}{has, vbz}{l2}
(cid:132)    {#start}{l4}{$entitys1, nnp}{l3}{has, vbz}{l2}

{better, jjr}{l1}{$feature, nns}{r1}{#end}   

bing liu, uic                                                   www-2008 tutorial

71

build a sequential cover from lsrs

lsr:    {* nn}{vbz}           {$entitys1 nn}{vbz}   
lsr:    { , nn}{vbz}           {$entitys1, nn}{vbz}   
(cid:137) select the lsr rule with the highest confidence. 
replace the matched elements in the sentences 
that satisfy the rule with the labels in the rule.

p

(cid:137) recalculate the confidence of each remaining rule 

based on the modified data from step 1.

(cid:137) repeat step 1 and 2 until no rule left with 

confidence higher than the minconf value (we
confidence higher than the minconf value (we 
used 90%).

(details skipped)
(details skipped)

bing liu, uic                                                   www-2008 tutorial

72

experimental results (jindal and liu, aaai-06)

(cid:132) identifying gradable comparative sentences
(cid:132) identifying gradable comparative sentences

(cid:137) precision = 82% and recall = 81%.

(cid:132) classification into three gradable types
(cid:132) classification into three gradable types

(cid:137) id166 gave accuracy of 96%

(cid:132) extraction of comparative relations
(cid:132) extraction of comparative relations

(cid:137) lsr (label sequential rules): f-score = 72%

bing liu, uic                                                   www-2008 tutorial

73

some other work

(cid:132) (bos and nissim 2006) proposes a method to
(cid:132) (bos and nissim 2006) proposes a method to 

extract items from superlative sentences. it 
does not study sentiments either. 

y

(cid:132) (fiszman et al 2007) tried to identify which 

entity has more of a certain property in a
entity has more of a certain property in a 
comparative sentence.

(cid:132) (ding and liu 2008 submitted) studies
(cid:132) (ding and liu 2008 submitted) studies 

id31 of comparatives, i.e., 
identifying which entity is preferred.  

y g

p

y

bing liu, uic                                                   www-2008 tutorial

74

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

75

review spam (jindal and liu 2008)

(cid:132) fake/untruthful review: promote or damage a 

p

g

product   s reputation

(cid:132) different from finding usefulness of reviews

(cid:132) increasing mention in blogosphere
(cid:132) increasing mention in blogosphere
(cid:132) articles in leading news media

(cid:137) id98, ny times

(cid:132) increasing number of customers wary of fake 

reviews (biased reviews, paid reviews)

by leading pr firm burson-marsteller
by leading pr firm burson marsteller

bing liu, uic                                                   www-2008 tutorial

76

experiments with amazon reviews

(cid:132) june 2006

(cid:137) 5.8mil reviews, 1.2mil products and 2.1mil reviewers.

(cid:132) a review has 8 parts

(cid:132) <product id> <reviewer id> <rating> <date> <review title> 

<review body> <number of helpful feedbacks> <number of 
feedbacks> <number of helpful feedbacks>

(cid:132) industry manufactured products    mproducts   
(cid:132) industry manufactured products  mproducts

e.g. electronics, computers, accessories, etc

(cid:137) 228k reviews, 36k products and 165k reviewers.

bing liu, uic                                                   www-2008 tutorial

77

log-log plot
reviews, reviewers 
and products

(cid:132)fig. 1 reviews and reviewers

(cid:132)fig. 2 reviews and products

bing liu, uic                                                   www-2008 tutorial

(cid:132)fig. 3 reviews and feedbacks78

review ratings

   rating of 5
   rating of 5

60% reviews
45% of products
59% of members
59% of members
reviews and feedbacks

1st review     80% positive feedbacks
10th review     70% positive feedbacks

bing liu, uic                                                   www-2008 tutorial

79

duplicate reviews

two reviews which have similar contents are
two reviews which have similar contents are 

called duplicates

bing liu, uic                                                   www-2008 tutorial

80

categorization of review spam

(cid:132) type 1 (untruthful opinions, fake reviews)
)

p

(

yp
ex:

(cid:132) type 2 (reviews on brands only) (?)

ex:    i don   t trust hp and never bought anything from them   
ex:
i don t trust hp and never bought anything from them

(cid:132) type 3 (non-reviews)

(cid:137) advertisements

e    d t
   
ex:    detailed product specs: 802.11g, imr compliant,       

802 11 imr
      buy this product at: compuplus.com   

d t

il d

li

t

(cid:137) other non-reviews
e    wh t
ex:    what port is it for   
   

it f

t i

   the other review is too funny   
   go eagles go   

bing liu, uic                                                   www-2008 tutorial

81

spam detection

(cid:132) type 2 and type 3 spam reviews
(cid:132) type 2 and type 3 spam reviews

(cid:137) supervised learning

(cid:132) type 1 spam reviews

manual labeling extremely hard
(cid:137) manual labeling extremely hard
(cid:137) propose to use duplicate and near-duplicate 

reviews to help
reviews to help

bing liu, uic                                                   www-2008 tutorial

82

detecting type 2 & type 3 spam 
reviews

(cid:132) binary classification
(cid:137) id28
(cid:137) id28
(cid:132) id203 estimates
(cid:132) practical applications, like give weights to each review, 

g

g

pp
rank them, etc

p

(cid:132) poor performance on other models
d l
(cid:132) na  ve bayes, id166 and id90

th

f

bing liu, uic                                                   www-2008 tutorial

83

three types of features

only review content features are not sufficient
only review content features are not sufficient.
we use: 
review centric features (content)
(cid:132) review centric features (content)
(cid:137) features about reviews
r i
t
(cid:137) features about the reviewers
p d t
(cid:137) features about products reviewed.

(cid:132) reviewer centric features

(cid:132) product centric features

t i

t i

f

f

t

bing liu, uic                                                   www-2008 tutorial

84

review centric features

(cid:132) number of feedbacks (f1), number (f2) and 

),

(

(

)

percent (f3) of helpful feedbacks

(cid:132) length of the review title (f4) and length of review 

body (f5). 

(cid:132)    
(cid:132) textual features

(cid:137) percent of positive (f10) and negative (f11) 

opinion-bearing words in the review

(cid:137) cosine similarity (f12) of review and product 

f
features 

t

(cid:137)    bing liu, uic                                                   www-2008 tutorial

85

reviewer centric features

(cid:132) ratio of the number of reviews that the
(cid:132) ratio of the number of reviews that the 

reviewer wrote which were the first reviews 
(f22) of the products to the total number of 
(
reviews that he/she wrote, and 

p

)

(cid:132) ratio of the number of cases in which he/she
(cid:132) ratio of the number of cases in which he/she 

was the only reviewer (f23).

(cid:132) average rating given by reviewer (f24)
(cid:132) average rating given by reviewer (f24), 

standard deviation in rating (f25)

(cid:132)(cid:132)       

bing liu, uic                                                   www-2008 tutorial

86

product centric features

(cid:132) price (f33) of the product
(cid:132) price (f33) of the product.
(cid:132) sales rank (f34) of the product
average rating (f35) of the product
(cid:132) average rating (f35) of the product
(cid:132) standard deviation in ratings (f36) of the 

reviews on the product.
d t

th

i

bing liu, uic                                                   www-2008 tutorial

87

experimental results
    evaluation criteria

    area under curve (auc)
area under curve (auc)
    10-fold cross validation

    high auc -> easy to detect
    high auc -> easy to detect
    equally well on type 2 and type 3 spam
text features alone not sufficient
   
feedbacks unhelpful (feedback spam)
    feedbacks unhelpful (feedback spam)

bing liu, uic                                                   www-2008 tutorial

88

deal with type 1 (untruthful reviews)

(cid:132) we have a problem: because
(cid:132) we have a problem: because

(cid:137) it is extremely hard to label fake/untruthful reviews 

manually.y

(cid:137) without training data, we cannot do supervised 

learning.

(cid:132) possible solution:

(cid:137) can we make use certain duplicate reviews as 

fake reviews (which are almost certainly 
untruthful)?

bing liu, uic                                                   www-2008 tutorial

89

recall: four types of duplicates

1 same userid same product
1. same userid, same product
2. different userid, same product
3 same userid different products
3. same userid, different products
4. different userid, different products

(cid:132) the last three types are very likely to be fake!

bing liu, uic                                                   www-2008 tutorial

90

predictive power of duplicates

(cid:132) representative of all kinds of spam
(cid:132) only 3% duplicates accidental
(cid:132) duplicates as positive examples, rest of the reviews as negative 

p

y

examples

reasonable predictive power

   
    maybe we can use duplicates as type 1 spam reviews(?)

bing liu, uic                                                   www-2008 tutorial

91

type 1 spam reviews

(cid:132) hype spam     promote one   s own products
(cid:132) defaming spam     defame one   s competitors    products

(cid:132) very hard to detect manually

(cid:132)harmful regions

bing liu, uic                                                   www-2008 tutorial

92

harmful spam are outlier reviews?

(cid:132) outliers reviews: reviews which deviate from 

average product rating

(cid:132) harmful spam reviews: outliers - necessary, but 
not sufficient, condition for harmful spam reviews. 

(cid:132) model building: id28

m d l b ildi
(cid:137) training: duplicates as type 1 reviews (positive) and the 

l

i

i

i

rest as non-spam reviews (negative)
rest as non-spam reviews (negative)

(cid:137) predicting outlier reviews

bing liu, uic                                                   www-2008 tutorial

93

lift curve for outlier reviews

biased reviewers -> all 
good or bad reviews on 
products of a brand

(cid:132) -ve deviation reviews 
more likely to be spam
(cid:137) biased reviews most 

p

y

likely

(cid:132) +ve deviation reviews 
least likely to be spam
least likely to be spam
except,
(cid:137) average reviews on bad 

products
biased re ie ers
(cid:137) biased reviewers

bing liu, uic                                                   www-2008 tutorial

94

some other interesting reviews

(cid:132) the model is able to predict outlier reviews to
(cid:132) the model is able to predict outlier reviews to 
some extend (we are not saying outliers are 
spam)

(cid:132) let us use the model to analysis some other 

g

interesting reviews
(cid:137) only reviews
(cid:137) reviews from top ranked members
(cid:137) reviews with different feedbacks
(cid:137) reviews on products with different sales ranks

bing liu, uic                                                   www-2008 tutorial

95

only reviews

(cid:132) 46% of reviewed products have only one review
(cid:132) only reviews have high lift curve
(cid:132) only reviews have high lift curve

bing liu, uic                                                   www-2008 tutorial

96

reviews from top-ranked reviewers

reviews by top ranked reviewers given higher 
(cid:132) reviews by top ranked reviewers given higher
probabilities of spam
(cid:137) top ranked members write larger number of reviews
(cid:137) deviate a lot from product rating write a lot of only reviews
(cid:137) deviate a lot from product rating, write a lot of only reviews

bing liu, uic                                                   www-2008 tutorial

97

reviews with different levels of feedbacks

(cid:132) random distribution

(cid:137) spam reviews can get good feedbacks

bing liu, uic                                                   www-2008 tutorial

98

reviews of products with varied sales ranks

(cid:132) product sales rank
(cid:132) product sales rank
(cid:137) important feature

(cid:132) high sales rank     low levels of spam
(cid:132) spam activities linked to low selling products
g p

p

bing liu, uic                                                   www-2008 tutorial

99

roadmap

(cid:132) opinion mining     the abstraction
(cid:132) document level sentiment classification
(cid:132) sentence level id31
(cid:132) sentence level id31
(cid:132) feature-based opinion mining and 

summarization
summarization

(cid:132) comparative sentence and relation 

extraction
extraction

(cid:132) opinion spam
summary
(cid:132) summary

bing liu, uic                                                   www-2008 tutorial

100

summary

two types of opinions have been discussed
(cid:132) direct opinions

(cid:137) document level, sentence level and feature level
(cid:137) structured summary of multiple reviews

(cid:132) comparisons

(cid:137) identification of comparative sentences
identification of comparative sentences
(cid:137) extraction of comparative relations

(cid:132) very challenging problems but there are already
(cid:132) very challenging problems, but there are already 

some applications of opinion mining.

(cid:132) detecting opinion spam or fake reviews is very hard.

bing liu, uic                                                   www-2008 tutorial

101

