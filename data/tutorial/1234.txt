id54

ani nenkova 

university of pennsylvania

sameer maskey 

ibm research

yang liu

university of texas at dallas

why summarize?

1

22

1

text summarization

news articles

emails

social media 
streams

scientific articles

books

websites

speech summarization

phone conversation

lecture

meeting

talk shows

chat

classroom

broadcast news

radio news

3

4

2

tutorial

how to 

summarize

text & speech?

-algorithms

-issues

-challenges

-systems

motivation & definition

topic models

frequency, lexical chains, tf*idf,
topic words, topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

5

6

3

motivation: where does summarization 
help?
  single document summarization 

(cid:2) simulate the work of intelligence analyst
(cid:2) judge if a document is relevant to a topic of interest

   summaries as short as 17% of the full text length speed up 
decision making twice, with no significant degradation in 
accuracy.   

   query-focused summaries enable users to find more relevant 

documents more accurately, with less need to consult the full text 
of the document.   

[mani et al., 2002]

7

motivation: id57 
helps in compiling and presenting

  reduce search time, especially when the goal of the 

user is to find as much information as possible about a 
given topic
(cid:2) writing better reports, finding more relevant information, 

quicker

  cluster similar articles and provide a multi-document 

summary of the similarities

  single document summary of the information unique to 

an article

[roussinov and chen, 2001; mana-lopez et al., 2004; mckeown et al., 2005 ]

8

4

benefits from speech summarization

  voicemail

(cid:2) shorter time spent on listening (call centers)

  meetings

(cid:2) easier to find main points

  broadcast news

(cid:2) summary of story from mulitiple channels

  lectures

(cid:2) useful for reviewing of course materials

[he et al., 2000; tucker and whittaker, 2008; murray et al., 2009]

assessing summary quality: overview

  responsiveness

(cid:2) assessor directly rate each summary on a scale
(cid:2) in official evaluations but rarely reported in papers

  pyramid

(cid:2) assessors create model summaries
(cid:2) assessors identifies semantic overlap between summary 

and models

  id8

(cid:2) assessors create model summaries
(cid:2) id8 automatically computes word overlap

9

10

5

tasks in summarization

content (sentence) selection

(cid:2) extractive summarization

information ordering

(cid:2) in what order to present the selected sentences, especially 

in id57

automatic editing, information fusion and compression

(cid:2) abstractive summaries

11

extractive (multi-document) summarization

input text1

input text2

input text3

1. selection
2. ordering
3. fusion

summary

compute informativeness

12

6

computing informativeness

  topic models (unsupervised)

(cid:2) figure out what the topic of the input

  frequency, lexical chains, tf*idf

  lsa, content models (em, bayesian) 

(cid:2) select informative sentences based on the topic

  graph models (unsupervised)

(cid:2) sentence centrality

  supervised approaches

(cid:2) ask people which sentences should be in a summary

(cid:2) use any imaginable feature to learn to predict human 

choices

13

motivation & definition

topic models

frequency, lexical chains, tf*idf, 
topic words,topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

14

7

frequency as document topic proxy

10 incarnations of an intuition

  simple intuition, look only at the document(s)

(cid:2) words that repeatedly appear in the document are likely to 

be related to the topic of the document

(cid:2) sentences that repeatedly appear in different input 

documents represent themes in the input

  but what appears in other documents is also helpful 

in determining the topic
(cid:2) background corpus probabilities/weights for word 

15

what is an article about?

  word id203/frequency

(cid:2) proposed by luhn in 1958 [luhn 1958]

(cid:2) frequent content words would be indicative of the 

topic of the article

  in id57, words or 
facts repeated in the input are more likely to 
appear in human summaries [nenkova et al., 2006]

16

8

word id203/weights 

input

bombing

pan am

libya

gadafhi

trail

suspects

uk and 
usa

word id203 table

word

id203

pan

am

libya

0.0798

0.0825

0.0096

suspects

0.0341

gadafhi

trail

   .

usa

0.0911

0.0002

0.0007

how?

1717

summary

libya refuses 
to surrender 
two pan am 
bombing 
suspects 

how: main steps in sentence selection 
according to word probabilities

step 1 estimate word weights (probabilities)
step 2 estimate sentence weights

weight

(

sent

)

=

wcf
i

(

   

sent

)

step 3 choose best sentence
step 4 update word weights
step 5 go to 2 if desired length not reached

1818

9

more specific choices [vanderwende et al., 2007; yih et al., 

2007; haghighi and vanderwende, 2009]

  select highest scoring sentence

score

s
)(

=

1

|

s

|

   

sw
   

wp
)(

  update word probabilities for the selected sentence 

to reduce redundancy

pnew (w) = pold (w).pold (w)

  repeat until desired summary length

19

is this a reasonable approach: yes, people 
seem to be doing something similar
  simple test

(cid:2) compute word id203 table from the input
(cid:2) get a batch of summaries written by h(umans) and s(ystems)

(cid:2) compute the likelihood of the summaries given the word 

id203 table 

  results

(cid:2) human summaries have higher likelihood

low

high likelihood

hsssssssssshssshsshhshhhhh

2020

10

obvious shortcomings of the pure 
frequency approaches
  does not take account of related words

(cid:2) suspects -- trail

(cid:2) gadhafi     libya

  does not take into account evidence from 

other documents
(cid:2) function words: prepositions, articles, etc.

(cid:2) domain words:    cell    in cell biology articles
  does not take into account many other 

aspects

two easy fixes

  lexical chains [barzilay and elhadad, 1999, silber and mccoy, 

2002, gurevych and nahnsen, 2005]

(cid:2) exploits existing lexical resources (id138)

  tf*idf weights [most summarizers]

(cid:2) incorporates evidence from a background corpus

21

22

11

lexical chains and id138 relations

  lexical chains

(cid:2) id51 is performed 
(cid:2) then topically related words represent a topic

  synonyms, hyponyms, hypernyms

(cid:2) importance is determined by frequency of the words in a 

topic rather than a single word

(cid:2) one sentence per topic is selected 

  concepts based on id138 [schiffman et al., 2002, ye et al., 

2007]
(cid:2) no id51 is performed

  {war, campaign, warfare, effort, cause, operation}

  {concern, carrier, worry, fear, scare}

tf*idf weights for words

combining evidence for document topics from the 

input and from a background corpus

  term frequency (tf)

(cid:2) times a word occurs in the input 

  inverse document frequency (idf)

(cid:2) number of documents (df) from a background 

corpus of n documents that contain the word

tf

*

idf

  =

tf

log(

dfn

/

)

23

24

12

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

25

topic words (topic signatures)

  which words in the input are most descriptive?

(cid:2) instead of assigning probabilities or weights to all words, 

divide words into two classes: descriptive or not

(cid:2) for iterative sentence selection approach, the binary 

distinction is key to the advantage over frequency and 
tf*idf

(cid:2) systems based on topic words have proven to be the most 

successful in official summarization evaluations 

26

13

example input and associated topic words

  input for summarization: articles relevant to the 

following user need

title: human toll of tropical 
storms narrative: what has been the human toll in death or injury 

of tropical storms in recent years? where and when have each of 
the storms caused human casualties? what are the approximate 
total number of casualties attributed to each of the storms?

topic words
ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, 
coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, 
flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, 
insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, 
north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, 
st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, 
west, winds, yesterday.

27

formalizing the problem of identifying topic 
words 

  given

(cid:2) t: a word that appears in the input
(cid:2) t: cluster of articles on a given topic (input)
(cid:2) nt: articles not on topic t (background corpus)

  decide if t is a topic word or not

  words that have (almost) the same id203 in t 

and nt are not topic words

28

14

computing probabilities

  view a text as a sequence of bernoulli trails

(cid:2) a word is either our term of interest t or not
(cid:2) the likelihood of observing term t which occurs with 

id203 p in a text consisting of n words is given by 

t

  estimate the id203 of t in three ways

(cid:2) input + background corpus combines
(cid:2) input only
(cid:2) background only

testing which hypothesis is more 
likely: log-likelihood ratio test

   =

likelihood of the data given h1

likelihood of the data given h2

-2 log   

has a known statistical distribution: chi-square 

at a given significance level, we can decide if a word is 
descriptive of the input or not.

this feature is used in the best performing systems for 
id57 of news [lin and hovy, 
2000; conroy et al., 2006]

29

30

15

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

31

the background corpus takes more 
central stage

  learn topics from the background corpus

(cid:2) topic ~ themes often discusses in the background

(cid:2) topic representation ~ word id203 tables

(cid:2) usually one time training step

  to summarize an input 

(cid:2) select sentences from the input that correspond 

to the most prominent topics

32

16

latent semantic analysis (lsa) [gong and liu, 

2001, hachey et al., 2006, steinberger et al., 2007]

a =

tupv

  discover topics from the background corpus with n unique 

words and d documents
(cid:2) represent the background corpus as nxd matrix a
(cid:2) rows correspond to words
(cid:2) aij=number of times word i appears in document j
(cid:2) use standard change of coordinate system and dimensionality 

reduction techniques

(cid:2) in the new space each row corresponds to the most important 

topics in the corpus

(cid:2) select the best sentence to cover each topic

33

notes on lsa and other approaches

  the original article that introduced lsa for 

single document summarization of news did 
not find significant difference with tf*idf

  for id57 of news 

lsa approaches have not outperformed topic 
words or extensions of frequency approaches

  other topic/content models have been much 

more influential

34

17

domain dependent content models

  get sample documents from the domain

(cid:2) background corpus

  cluster sentences from these documents 

(cid:2) implicit topics

  obtain a word id203 table for each topic

(cid:2) counts only from the cluster representing the 

topic

  select sentences from the input with highest 

id203 for main topics 

text structure can be learnt

  human-written examples from a domain

location, time

damage

magnitude

relief efforts

35

36
36

18

topic = cluster of similar sentences from 
the background corpus

  sentences cluster from earthquake articles

  topic    earthquake location   

  the athens seismological institute said the temblor   s epicenter 

was located 380 kilometers (238 miles) south of the capital.

  seismologists in pakistan   s northwest frontier province said the 
temblor   s epicenter was about 250 kilometers (155 miles) north of 
the provincial capital peshawar.

  the temblor was centered 60 kilometers (35 miles) north- west of 

the provincial capital of kunming, about 2,200 kilometers (1,300
miles) southwest of beijing, a bureau seismologist said.

content model [barzilay and lee, 2004, pascale et al., 2003] 

  hidden markov model (id48)-based 

(cid:2) states - clusters of related sentences    topics   
(cid:2) transition prob. - sentence precedence in corpus
(cid:2) emission prob. - bigram language model

generating 
sentence in 
current topic

p

(
<

s
i

1
+

,

h
i

1
+

<>

|

hs
i
i

,

=>

)

hp
i

(

t

transition 
from previous 
topic

|

h
i

)

   

(
sp
i

e

1
+

|

h
i

1
+

)

1
+

earthquake reports

location, 
magnitude

casualties

relief efforts

37

38
38

19

learning the content model

  many articles from the same domain

  cluster sentences: each cluster represents a topic from 

the domain
(cid:2) word id203 tables for each topic

  transitions between clusters can be computed from 

sentence adjacencies in the original articles  
(cid:2) probabilities of going from one topic to another

  iterate between id91 and transition id203 

estimation to obtain domain model

39

to select a summary

  find main topics in the domain

(cid:2) using a small collection of summary-input pairs

  find the most likely topic for each sentence in 

the input 

  select the best sentence per main topic

40

20

historical note

  some early approaches to multi-document 

summarization relied on id91 the 
sentences in the input alone [mckeown et al., 1999, 
siddharthan et al., 2004]

(cid:2) clusters of similar sentences represent a theme in 

the input

(cid:2) clusters with more sentences are more important

(cid:2) select one sentence per important cluster

41

example cluster

choose one sentence to represent the cluster

1. pal was devastated by a pilots' strike in june and by the 

region's currency crisis.

2. in june, pal was embroiled in a crippling three-week 

pilots' strike.

3. tan wants to retain the 200 pilots because they stood by 

him when the majority of pal's pilots staged a 
devastating strike in june.

42

21

bayesian content models

  takes a batch of inputs for summarization

  many word id203 tables

(cid:2) one for general english
(cid:2) one for each of the inputs to be summarized
(cid:2) one for each document in any input

to select a summary s with l words from 
document collection d given as input

s* = mins:words(s)   lkl(pd||ps)

the goal is to select the summary, not a 
sentence. greedy selection vs. global will 
be discussed in detail later

kl divergence

  distance between two id203 distributions: p, q

kl (p || q) =

pp (w) log 2

   

w

pp (w)
pq (w)

  p, q: input and summary word distributions  

43

44

22

intriguing side note

  in the full bayesian topic models, word 

probabilities for all words is more important 
than binary distinctions of topic and non-topic 
word

  haghighi and vanderwende report that a 

system that chooses the summary with 
highest expected number of topic words 
performs as sumbasic

review

  frequency based informativeness has been 

used in building summarizers

  topic words probably more useful

  topic models

(cid:2) latent semantic analysis

(cid:2) domain dependent content model

(cid:2) bayesian content model

45

46

23

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

47

using graph representations [erkan and radev, 

2004; mihalcea and tarau, 2004; leskovec et al., 2005 ]

  nodes

(cid:2) sentences

(cid:2) discourse entities

  edges

(cid:2) between similar sentences

(cid:2) between syntactically related entities

  computing sentence similarity

(cid:2) distance between their tf*idf weighted vector 

representations

48

24

sentence :

iraqi vice president   

sim(d1s1, d3s2)

49

50

sentence :

ivanov contended   

25

advantages of the graph model

  combines word frequency and sentence 

id91

  gives a formal model for computing 

importance: id93
(cid:2) normalize weights of edges to sum to 1

(cid:2) they now represent probabilities of transitioning 

from one node to another

id93 for summarization

  represent the input text as graph

  start traversing from node to node 

(cid:2) following the transition probabilities 

(cid:2) occasionally hopping to a new node

  what is the id203 that you are in any 

particular node after doing this process for a 
certain time? 
(cid:2) standard solution (stationary distribution)

(cid:2) this id203 is the weight of the sentence

51

52

26

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

53

supervised methods 

  for extractive summarization, the task can be 

represented as binary classification
(cid:2) a sentence is in the summary or not

  use statistical classifiers to determine the score of a 

sentence: how likely it   s included in the summary
(cid:2) feature representation for each sentence
(cid:2) classification models trained from annotated data

  select the sentences with highest scores (greedy for 

now, see other selection methods later)

54

27

features

  sentence length

(cid:2) long sentences tend to be more important

  sentence weight

(cid:2) cosine similarity with documents

(cid:2) sum of term weights for all words in a sentence

(cid:2) calculate term weight after applying lsa

features

  sentence position

(cid:2) beginning is often more important

(cid:2) some sections are more important (e.g., in 

conclusion section)

  cue words/phrases 

(cid:2) frequent id165s

(cid:2) cue phrases (e.g., in summary, as a conclusion)

(cid:2) named entities

55

56

28

features

  contextual features

(cid:2) features from context sentences

(cid:2) difference of a sentence and its neighboring ones 

  speech related features (more later):

(cid:2) acoustic/prosodic features

(cid:2) speaker information (who said the sentence, is the 

speaker dominant?)

(cid:2) id103 confidence measure 

classifiers

  can classify each sentence individually, or 

use sequence modeling

  maximum id178 [osborne, 2002]
  condition random fields (crf) [galley, 2006]
  classic bayesian method [kupiec et al., 1995]
  id48 [conroy and o'leary, 2001; maskey, 2006 ]
  id110s 
  id166s [xie and liu, 2010]
  regression [murray et al., 2005]
  others

57

58

29

so that is it with supervised methods? 

  it seems it is a straightforward classification 

problem

  what are the issues with this method?

(cid:2) how to get good quality labeled training data

(cid:2) how to improve learning

  some recent research has explored a few 

directions
(cid:2) discriminative training, regression, sampling, co-

training, active learning

59

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

60

30

improving supervised methods: different 
training approaches

  what are the problems with standard training 

methods?
(cid:2) classifiers learn to determine a sentence   s label 

(in summary or not)   

(cid:2) sentence-level accuracy is different from 
summarization evaluation criterion (e.g., 
summary-level id8 scores)

(cid:2) training criterion is not optimal

(cid:2) sentences    labels used in training may be too 

strict (binary classes)

61

improving supervised methods: mert 
discriminative training

  discriminative training based on mert [aker et 

al., 2010]

(cid:2) in training, generate multiple summary candidates 

(using id67 algorithm)

(cid:2) adjust model parameters (feature weights) 

iteratively to optimize id8 scores

note: mert has been used for machine translation discriminative training

62

31

improving supervised methods: ranking 
approaches 

  ranking approaches [lin et al. 2010]

(cid:2) pair-wise training

  not classify each sentence individually
  input to learner is a pair of sentences
  use rank id166 to learn the order of two sentences

(cid:2) direct optimization

  learns how to correctly order/rank summary candidates 

(a set of sentences)

  use adarank [xu and li 2007] to combine weak rankers

63

improving supervised methods: regression 
model

  use regression model [xie and liu, 2010]

(cid:2) in training, a sentence   s label is not +1 and -1

(cid:2) each one is labeled with numerical values to 

represent their importance
  keep +1 for summary sentence
  for non-summary sentences (-1), use their similarity to 

the summary as labels

(cid:2) train a regression model to better discriminate 

sentence candidates

64

32

improving supervised methods: sampling

  problems -- in binary classification setup for 

summarization, the two classes are 
imbalanced
(cid:2) summary sentences are minority class. 

(cid:2) imbalanced data can hurt classifier training

  how can we address this?

(cid:2) sampling to make distribution more balanced to 

train classifiers

(cid:2) has been studied a lot in machine learning

65

improving supervised methods: sampling

  upsampling: increase minority samples

(cid:2) replicate existing minority samples

(cid:2) generate synthetic examples (e.g., by some kind 

of interpolation)

  downsampling: reduce majority samples
(cid:2) often randomly select from existing majority 

samples

66

33

improving supervised methods: sampling

  sampling for summarization [xie and liu, 2010]

(cid:2) different from traditional upsampling and downsampling
(cid:2) upsampling

  select non-summary sentences that are like summary 

sentences based on cosine similarity or id8 scores

  change their label to positive 

(cid:2) downsampling: 

  select those that are different from summary sentences

(cid:2) these also address some human annotation disagreement

  the instances whose labels are changed are often the ones 

that humans have problems with

67

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-raining

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

68

34

supervised methods: data issues

  need labeled data for model training

  how do we get good quality training data? 
(cid:2) can ask human annotators to select extractive 

summary sentences

(cid:2) however, human agreement is generally low

  what if data is not labeled at all? or it only 

has abstractive summary?

69

do humans agree on summary sentence 
selection?

human agreement on word/sentence/fact selection

  distributions of content units and words are similar

  few units are expressed by everyone; many units 

are expressed by only one person

7070

35

supervised methods: semi-supervised 
learning

  question     can we use unlabeled data to 

help supervised methods? 

  a lot of research has been done on semi-

supervised learning for various tasks

  co-training and active learning have been 

used in summarization

co-training

  use co-training to leverage unlabeled data

(cid:2) feature sets represent different views

(cid:2) they are conditionally independent given the 

class label

(cid:2) each is sufficient for learning

(cid:2) select instances based on one view, to help the 

other classifier

71

72

36

co-training in summarization

  in text summarization [wong et al., 2008]

(cid:2) two classifiers (id166, na  ve bayes) are used on 

the same feature set

  in speech summarization [xie et al., 2010]

(cid:2) two different views: acoustic and lexical features

(cid:2) they use both sentence and document as 

selection units

73

active learning in summarization

  select samples for humans to label

(cid:2) typically hard samples, machines are not 

confident, informative ones

  active learning in lecture summarization [zhang 

et al. 2009]

(cid:2) criterion: similarity scores between the extracted 

summary sentences and the sentences in the 
lecture slides are high

74

37

supervised methods: using labeled 
abstractive summaries

  question -- what if i only have abstractive 
summaries, but not extractive summaries? 

  no labeled sentences to use for classifier 

training in extractive summarization 

  can use reference abstract summary to 
automatically create labels for sentences
(cid:2) use similarity of a sentence to the human written 

abstract (or id8 scores, other metrics)

comment on supervised performance

  easier to incorporate more information
  at the cost of requiring a large set of human 

annotated training data

  human agreement is low, therefore labeled 

training data is noisy

  need matched training/test conditions

(cid:2) may not easily generalize to different domains

  effective features vary for different domains

(cid:2) e.g., position is important for news articles

75

76

38

comments on supervised performance

  seems supervised methods are more 

successful in speech summarization than in 
text
(cid:2) speech summarization is almost never multi-

document

(cid:2) there are fewer indications about the topic of the 

input in speech domains

(cid:2) text analysis techniques used in speech 

summarization are relatively simpler 

77

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

78

39

parameters to optimize

  in summarization methods we try to find 

1. most significant sentences

2. remove redundant ones

3. keep the summary under given length

(cid:2) can we combine all 3 steps in one?

(cid:2) optimize all 3 parameters at once

79

summarization as an optimization problem

  summarization problem 

select sentences such that summary relevance is 
maximized while keeping total length under x words

  knapsack optimization problem 

select boxes such that amount of money is 
maximized while keeping total weight under x kg

  many other similar optimization problems  

  general idea: maximize a function given a set of 

constraints

80

40

optimization methods for summarization

  different flavors of solutions

(cid:2) greedy algorithm

  choose highest valued boxes
  choose the most relevant sentence 

(cid:2) id145 algorithm
  save intermediate computations
  look at both relevance and length

(cid:2) integer id135

  exact id136
  scaling issues

we will now discuss these 3 types of optimization solutions

81

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

82

41

greedy optimization algorithms

  greedy solution is an approximate algorithm which 

may not be optimal

  choose the most relevant + least redundant 

sentence if the total length does not exceed the 
summary length
(cid:2) maximal marginal relevance is one such greedy algorithm 

proposed by [carbonell et al., 1998]

83

maximal marginal relevance (mmr) 

[carbonell et al., 1998]

  summary: relevant and non-redundant information

(cid:2) many summaries are built based on sentences ranked by 

relevance

(cid:2) e.g. extract most relevant 30% of sentences

relevance

vs.

redundancy

  summary should maximize relevant information as 

well as reduce redundancy

84

42

marginal relevance

     marginal relevance    or    relevant novelty   

(cid:2) measure relevance and novelty separately
(cid:2) linearly combine these two measures

  high marginal relevance if

(cid:2) sentence is relevant to story (significant information)
(cid:2) contains minimal similarity to previously selected sentences 

(new novel information)

  maximize marginal relevance to get summary that 

has significant non-redundant information

85

relevance with query or centroid

  we can compute relevance of text snippet 

with respect to query or centroid

  centroid as defined in [radev, 2004]

(cid:2) based on the content words of  a document 

(cid:2) tf*idf vector of all documents in corpus

(cid:2) select words above a threshold : remaining vector 

is a centroid vector

86

43

maximal marginal relevance (mmr) 

[carbonell et al., 1998]

mmr     argmax(di   r   s)[  (sim1(di, q))   (1     )max(dj    s)sim2(di, dj)]

  q     document centroid/user query

  d     document collection

  r     ranked listed

  s     subset of documents in r already selected

  sim     similarity metric 

  lambda =1 produces most significant ranked list

  lambda = 0 produces most diverse ranked list

mmr based summarization [zechner, 2000]

iteratively select next sentence

next sentence = 

centroid

frequency vector 
of all content words

87

88

44

mmr based summarization

  why this iterative sentence selection process 

works?
(cid:2) 1st term: find relevant sentences similar to 

centroid of the document

(cid:2) 2nd term: find redundancy     sentences that are 

similar to already selected sentences are not 
selected

sentence selection in mmr

  mmr is an iterative sentence selection 

process
(cid:2) decision made for each sentence

(cid:2) is this selected sentence globally optimal?

sentence with same level of relevance but shorter may not be 
selected if a longer relevant sentence is already selected

89

90

45

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

91

global id136

  modify our greedy algorithm 

(cid:2) add constraints for sentence length as well

  let us define document d with tn textual 

units

d=t1, t2, , tn   1, tn

92

46

global id136

  let us define

rel(i)

relevance of ti to be in the 
summary

red(i,j)

redundancy between ti and tj

l(i)

length of ti

id136 problem [mcdonald, 2007]

  let us define id136 problem as 

summary score

maximum length

pairwise redundancy

93

94

47

greedy solution [mcdonald, 2007]

sort by relevance

no consideration of
sentence length

select sentence

  sorted list may have longer sentences at the top

  solve it using id145
  create table and fill it based on length and redundancy 

requirements

95

id145 solution [mcdonald, 2007]

high scoring 
summary of
length k-l(i) +

ti

high scoring summary

of length k and i-1

text units

higher ?

96

48

id145 algorithm [mcdonald, 2007]

  better than the previously shown greedy 

algorithm

  maximizes the space utilization by not 

inserting longer sentences

  these are still approximate algorithms: 

performance loss?

id136 algorithms comparison

[mcdonald, 2007]

sentence length

system

50

100

200

baseline

26.6/5.3

33.0/6.8

39.4/9.6

greedy

26.8/5.1

33.5/6.9

40.1/9.5

dynamic program

27.9/5.9

34.8/7.3

41.2/10.0

summarization results: id8-1/id8-2

97

98

49

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

99

integer id135 (ilp) [gillick 

and favre, 2009; gillick et al., 2009; mcdonald, 2007]

  greedy algorithm is an approximate solution
  use exact solution algorithm with ilp (scaling issues 

though)

  ilp is constrained optimization problem

(cid:2) cost and constraints are linear in a set of integer variables

  many solvers on the web
  define the constraints based on relevance and 

redundancy for summarization
(cid:2) sentence based ilp
(cid:2) id165 based ilp

100

50

sentence-level ilp formulation [mcdonald, 

2007]

optimization function

1 if ti in summary

constraints

id165 ilp formulation [gillick and favre, 2009; 

gillick et al., 2009]

  sentence-ilp constraint on redundancy is 

based on sentence pairs

  improve by modeling id165-level 

redundancy

  redundancy implicitly defined

(cid:1)i wici

ci indicates presence
of id165 i in summary 
and its weight is wi

101

102

51

id165 ilp formulation [gillick and favre, 2009]

optimization function

id165 level ilp has different  optimization 
function than one shown before

constraints

sentence vs. id165 ilp

system

id8-2

pyramid

baseline

0.058

0.186

sentence ilp

[mcdonald, 2007]

id165 ilp

[gillick and favre, 2009]

0.072

0.295

0.110

0.345

103

104

52

other optimization based summarization 
algorithms

  submodular selection [lin et al., 2009]

(cid:2) submodular set functions for optimization

  modified greedy algorithm [filatova, 2004]

(cid:2) event based features

  stack decoding algorithm [yih et al., 2007]

(cid:2) multiple stacks, each stack represents hypothesis of different 

length

  id67 [aker et al., 2010]

(cid:2) use scoring and heuristic functions

submodular selection for summarization 

[lin et al., 2009]

  summarization setup

(cid:2) v     set of all sentences in document
(cid:2) s     set of extraction sentences
(cid:2) f(.) scores the quality of the summary

  submodularity been used in solving many 

optimization problems in near polynomial time

  for summarization: 

select subset s (sentences) representative of v 

given the constraint |s| =< k (budget)

105

106

53

submodular selection [lin et al., 2009]

  if v are nodes in a graph g=(v,e) representing 

sentences

  and e represents edges (i,j) such that w(i,j) 

represents similarity between sentences i and j

  introduce submodular set functions which measures 

   representative    s of entire set v

 

[lin et al., 2009] presented 4 submodular set functions

submodular selection for summarization 
[lin et al., 2009]

comparison of results using different methods

107

108

54

review: optimization methods

  global optimization methods have shown to be 

superior than 2-step selection process and reduce 
redundancy

  3 parameters are optimized together

(cid:2) relevance
(cid:2) redundancy
(cid:2) length

  various algorithms for global id136

(cid:2) greedy
(cid:2) id145 
(cid:2) integer id135
(cid:2) submodular selection

109

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

110

55

speech summarization

  increasing amount of data available in 

speech form
(cid:2) meetings, lectures, broadcast, youtube, voicemail

  browsing is not as easy as for text domains

(cid:2) users need to listen to the entire audio

  summarization can help effective information 

access

  summary output can be in the format of text 

or speech

domains 

  broadcast news

  lectures/presentations

  multiparty meetings

  telephone conversations

  voicemails

111

112

56

example

meeting transcripts and summary sentences (in red)

broadcast news transcripts and summary (in red)

me010

me010

me010

me010

there there are a variety of ways of doing it

uh let me just mention something that i don   t want 
to pursue today

which is there are technical ways of doing it

uh i- i slipped a paper to bhaskara and about noisy-
or   s and noisy-maxes

me010

and

california's strained power grid is getting a boost today which 
might help increasingly taxed power supplies

a unit at diablo canyon nuclear plant is expected to resume 
production today

it had been shut down for maintenance

coupled with another unit, it can provide enough power for about
2 million people

me010

there   re ways to uh sort of back off on the purity of 
your bayes-net-edness

meanwhile, a cold snap in the pacific northwest is putting an 
added strain on power supplies

me003

mmm

me010

me010

me010

me003

uh so if you co- you could ima- and i-

now i don   t know that any of these actually apply in 
this case

but there is some technology you could try to apply

so it   s possible that we could do something like a 
summary node of some sort that

me010

yeah

the area shares power across many states

energy officials are offering tips to conserve electricity, they say, 
to delay holiday lighting until after at night

set your thermostat at 68 degrees when you're home, 55 
degrees when  you're away

try to use electrical appliances before p.m. and after p.m. and 
turn off computers, copiers and lights when they're not being 
used

113

speech vs. text summarization: similarities

  when high quality transcripts are available

(cid:2) not much different from text summarization

(cid:2) many similar approaches have been used

(cid:2) some also incorporate acoustic information

  for genres like broadcast news, style is also 

similar to text domains

114

57

speech vs. text summarization: differences

  challenges in speech summarization

(cid:2) id103 errors can be very high

(cid:2) sentences are not as well formed as in most text 

domains: disfluencies, ungrammatical

(cid:2) there are not clearly defined sentences

(cid:2) information density is also low (off-topic 

discussions, chit chat, etc.)

(cid:2) multiple participants

115

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

116

58

what should be extraction units in speech 
summarization?

  text domain

(cid:2) typically use sentences (based on punctuation 

marks)

  speech domain

(cid:2) sentence information is not available

(cid:2) sentences are not as clearly defined

utterance from previous example:

there there are a variety of ways of doing it uh let me just mention something 
that i don   t want to pursue today which is there are technical ways of doing it

automatic sentence segmentation (side note) 

  for a word boundary, determine whether it   s a sentence 

boundary

  different approaches: 

(cid:2) generative: id48

(cid:2) discriminative: id166, boosting, maxent, crf

(cid:2) information used: word id165, part-of-speech, parsing 

information, acoustic info (pause, pitch, energy)

117

118

59

what is the effect of different 
units/segmentation on summarization?

  research has used different units in speech 

summarization
(cid:2) human annotated sentences or dialog acts

(cid:2) automatic sentence segmentation

(cid:2) pause-based segments

(cid:2) adjacency pairs

(cid:2) intonational phrases 

(cid:2) words

119

what is the effect of different 
units/segmentation on summarization?

  findings from previous studies

(cid:2) using intonational phrases (ip) is better than 

automatic sentence segmentation, pause-based 
segmentation [maskey, 2008 ]
  ips are generally smaller than sentences, also 

linguistically meaningful

(cid:2) using sentences is better than words, between 

filler segments [furui et al., 2004]

(cid:2) using human annotated dialog acts is better than 

automatically generated ones [liu and xie, 2008]

120

60

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

121

using acoustic information in 
summarization

  acoustic/prosodic features: 

(cid:2) f0 (max, min, mean, median, range)

(cid:2) energy (max, min, mean, median, range)

(cid:2) sentence duration

(cid:2) speaking rate (# of words or letters)

(cid:2) need proper id172

  widely used in supervised methods, in 

combination with textual features

122

61

using acoustic information in 
summarization
  are acoustic features useful when combining 

it with lexical information?

  results vary depending on the tasks and 

domains 
(cid:2) often lexical features are ranked higher
(cid:2) but acoustic features also contribute to overall 

system performance

(cid:2) some studies showed little impact when adding 

speech information to textual features [penn and zhu, 
2008]

123

using acoustic information in 
summarization

  can we use acoustic information only for speech 

summarization?
(cid:2) transcripts may not be available
(cid:2) another way to investigate contribution of acoustic 

information

  studies showed using just acoustic information can 

achieve similar performance to using lexical 
information [maskey and hirschberg, 2005; xie et al., 2009; zhu et al., 
2009]

(cid:2) caveat: in some experiments, lexical information is used 

(e.g., define the summarization units)

124

62

id103 errors

  asr is not perfect, often high word error rate

(cid:2) 10-20% for read speech

(cid:2) 40% or even higher for conversational speech

  recognition errors generally have negative 

impact on summarization performance
(cid:2) important topic indicative words are incorrectly 

recognized

(cid:2) can affect term weighting and sentence scores

125

id103 errors

  some studies evaluated effect of recognition 

errors on summarization by varying word 
error rate [christensen et al., 2003; penn and zhu, 2008; lin et al., 
2009]

  degradation is not much when word error 

rate is not too low (similar to spoken 
document retrieval)
(cid:2) reason: better recognition accuracy in summary 

sentences than overall  

126

63

what can we do about asr errors? 

  deliver summary using original speech 
(cid:2) can avoid showing recognition errors in the 

delivered text summary

(cid:2) but still need to correctly identify summary 

sentences/segments

  use recognition confidence measure and 

multiple candidates to help better summarize

127

address problems due to asr errors

  re-define summarization task: select 

sentences that are most informative, at the 
same time have high recognition accuracy
(cid:2) important words tend to have high recognition 

accuracy

  use asr confidence measure or id165 
language model scores in summarization
(cid:2) unsupervised methods [zechner, 2002; kikuchi et al., 2003; 

maskey, 2008]

(cid:2) use as a feature in supervised methods

128

64

address problems due to asr errors

  use multiple recognition candidates

(cid:2) n-best lists [liu et al., 2010]
(cid:2) lattices [lin et al., 2010]
(cid:2) confusion network [xie and liu, 2010]

  use in mmr framework
  summarization segment/unit contains all the word 
candidates (or pruned ones based on probabilities)
  term weights (tf, idf) use candidate   s posteriors
  improved performance over using 1-best recognition 

output

129

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

130

65

disfluencies and summarization

  disfluencies (filler words, repetitions, revisions, 

restart, etc) are frequent in conversational speech
(cid:2) example from meeting transcript:

so so does i- just remind me of what what you were going to do with the 

what what what what's

y- you just described what you've been doing

  existence may hurt summarization systems, also 

affect human readability of the summaries

disfluencies and summarization

  natural thought: remove disfluenices 
  word-based selection can avoid disfluent 

words 
(cid:2) using id165 scores tends to select fluent 

parts [hori and furui, 2001]

  remove disfluencies first, then perform 

summarization 
(cid:2) does it work? not consistent results 

  small improvement [maskey, 2008; zechner, 2002]
  no improvement [liu et al., 2007]

131

132

66

disfluencies and summarization

  in supervised classification, information related to 

disfluencies can be used as features for 
summarization 
(cid:2) small improvement on switchboard data [zhu and penn, 2006]

  going beyond disfluency removal, can perform 

sentence compression in conversational speech to 
remove un-necessary words [liu and liu, 2010]
(cid:2) help improve sentence readability
(cid:2) output is more like abstractive summaries
(cid:2) compression helps summarization

review on speech summarization

  speech summarization has been performed 

for different domains

  a lot of text-based approaches have been 

adopted

  some speech specific issues have been 

investigated
(cid:2) segmentation 
(cid:2) asr errors
(cid:2) disfluencies
(cid:2) use acoustic information

133

134

67

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

135

manual evaluations

  task-based evaluations 

(cid:2) too expensive

(cid:2) bad decisions possible, hard to fix

  assessors rate summaries on a scale

(cid:2) responsiveness

  assessors compare with gold-standards

(cid:2) pyramid

136

68

automatic and fully automatic 
evaluation
  automatically compare with gold-standard

(cid:2) precision/recall (sentence level)

(cid:2) id8 (word level)

  no human gold-standard is used

(cid:2) automatically compare input and summary

precision and recall for extractive 
summaries
  ask a person to select the most important 

sentences

recall: system-human choice 

overlap/sentences chosen by human

precision: system-human choice 

overlap/sentences chosen by system

137

138

69

problems?

  different people choose different sentences

  the same summary can obtain a recall score 

that is between 25% and 50% different 
depending on which of two available human 
extracts is used for evaluation

  recall more important/informative than 

precision?

139

more problems?

  granularity

we need help. fires have spread in the nearby 

forest and threaten several villages in this remote 
area.

  semantic equivalence

(cid:2) especially in id57

(cid:2) two sentences convey almost the same 

information: only one will be chosen in the human 
summary

140

70

evaluation methods for content

model 
summaries

manual comparison/ 
ratings

pyramid

responsiveness

id8

fully automatic

pyramid method [nenkova and passonneau, 2004; nenkova et al., 

2007]

  based on semantic content units (scu)

  emerge from the analysis of several texts

  link different surface realizations with the 

same meaning

141

142

71

scu example

s1 pinochet arrested in london on oct 16 at a 

spanish judge   s request for atrocities against 
spaniards in chile.

s2 former chilean dictator augusto pinochet has 

been arrested in london at the request of the 
spanish government.

s3 britain caused international controversy and 

chilean turmoil by arresting former chilean 
dictator pinochet in london.

143

scu: label, weight, contributors 

label london was where pinochet was 

arrested
weight=3
s1 pinochet arrested in london on oct 16 at a spanish 

judge   s request for atrocities against spaniards in chile.

s2 former chilean dictator augusto pinochet has been

arrested in london at the request of the spanish
government.

s3 britain caused international controversy and chilean 

turmoil by arresting former chilean dictator pinochet in 
london.

144

72

ideally informative summary

  does not include an scu from a lower tier 

unless all scus from higher tiers are 
included as well

ideally informative summary

  does not include an scu from a lower tier 

unless all scus from higher tiers are 
included as well

145

146

73

ideally informative summary

  does not include an scu from a lower tier 

unless all scus from higher tiers are 
included as well

ideally informative summary

  does not include an scu from a lower tier 

unless all scus from higher tiers are 
included as well

147

148

74

ideally informative summary

  does not include an scu from a lower tier 

unless all scus from higher tiers are 
included as well

ideally informative summary

  does not include an scu from a lower tier 

unless all scus from higher tiers are 
included as well

149

150

75

different equally good summaries

  pinochet arrested

  arrest in london
  pinochet is a former 

chilean dictator

  accused of atrocities 

against spaniards

different equally good summaries

  pinochet arrested

  arrest in london
  on spanish warrant

  chile protests

151

152

76

diagnostic     why is a summary bad?

  good

  less relevant 

summary

importance of content 

  can observe distribution in human 

summaries
(cid:2) assign relative importance

(cid:2) empirical rather than subjective

  the more people agree, the more important

153

154

77

pyramid score for evaluation

  new summary with n content units

weight

i

n

   

i

=

1
n

ideal
i

   

i

1
=

observedwe

ight

=

idealwight

  estimates the percentage of information that is 

maximally important

id8 [lin, 2004]

  de facto standard for evaluation in text 

summarization
(cid:2) high correlation with manual evaluations in that 

domain

  more problematic for some other domains, 

particularly speech
(cid:2) not highly correlated with manual evaluations

(cid:2) may fail to distinguish human and machine 

summaries

155

156

78

id8 details

  in fact a suite of id74

(cid:2) unigram
(cid:2) bigram
(cid:2) skip bigram
(cid:2) longest common subsequence

  many settings concerning

(cid:2) stopwords
(cid:2) id30
(cid:2) dealing with multiple models

how to evaluate without human 
involvement? [louis and nenkova, 2009]

  a good summary should be similar to the 

input

  multiple ways to measure similarity

(cid:2) cosine similarity

(cid:2) kl divergence

(cid:2) js divergence

  not all work!

157

158

79

js divergence between input and 
summary

  distance between two distributions as 

average kl divergence from their mean 
distribution

js

(

inp

||

)
summ

=

1
2

[

(
kl

inp

||

)
a

+

(
kl

summ

||

a

)]

a

=

inp

+

summ

2

,

mean

distributi

on

of

input

and

summary

159

summary likelihood given the input

  id203 that summary is generated according to 

term distribution in the input

higher likelihood ~ better summary

  unigram model

p

inp

(

w
1

)

n
1

p

inp

(

w

2

)

n

2

k

p

(

w

r

)

inp

n

r

r

   

summary

vocabulary

n

i

=

count

in

summary

of

word

w

i

  multinomial model

!
n
!
n
k
r

!

n
1

wpwp
2

inp

inp

(

)

(

1

n
1

n
2

)

k

wp
r

inp

(

n
r

)

n

=    

n
i

i

=

summary

size

160

80

topic words identified by log-likelihood 
test

  fraction of summary = input   s topic words

  % of input   s topic words also appearing in summary 

(cid:2) capture variety

  cosine similarity: input   s topic words and all summary 

words
(cid:2) fewer dimensions, more specific vectors

161

how good are these metrics? 

pyramid

responsiveness

jsd

-0.880

% input   s topic in summary

0.795

kl div summ-input

cosine similarity

-0.763

0.712

% of summary = topic words

0.712

topic word similarity

kl div input-summ

multinomial summ prob.

unigram summ prob.

-0.699

-0.688

0.222

-0.188

48 inputs, 57 systems

-0.736

0.627

-0.694

0.647

0.602

0.629

-0.585

0.235

-0.101

spearman correlation on macro level for the query focused task.

162

81

how good are these metrics?

jsd

r1-recall

r2-recall

pyramid

-0.88

0.85

0.90

resp.

-0.73

0.80

0.87

  jsd correlations with pyramid scores even better than 

r1-recall

  r2-recall is consistently better

  can extend features using higher order id165s 

163

motivation & definition

topic models

frequency, tf*idf, topic words
topic models [lsa, em, bayesian]

graph based methods

supervised techniques

features, discriminative training

sampling, data, co-training

global optimization methods

iterative, greedy, id145

ilp, sub-modular selection

speech summarization

segmentation, asr

acoustic information, disfluency 

evaluation

manual (pyramid), automatic (id8, f-measure)

fully automatic

164

82

current summarization research  
  summarization for various new genres

(cid:2) scientific articles
(cid:2) biography
(cid:2) social media (blog, twitter)
(cid:2) other text and speech data 

  new task definition 

(cid:2) update summarization 
(cid:2) opinion summarization

  new summarization approaches 

(cid:2) incorporate more information (deep linguistic knowledge, information 

from the web)

(cid:2) adopt more complex machine learning techniques

  evaluation issues

(cid:2) better automatic metrics
(cid:2) extrinsic evaluations

and more   

165

  check out summarization papers at acl this 

year

  workshop at acl-hlt 2011:

(cid:2) id54 for different genres, 

media, and languages [june 23, 2011]
  http://www.summarization2011.org/

166

83

references

 

 

 

 

 

 

 

 

 

 

 

 

 

ahmet aker, trevor cohn, robert gaizauska. 2010. id57 using id67 and 
discriminative training. proc. of emnlp.
r. barzilay and m. elhadad. 2009. text summarizations with lexical chains. in: i. mani and m. maybury (eds.): 
advances in automatic text summarization.
jaime carbonell and jade goldstein. 1998. the use of mmr, diversity-based reranking for reordering 
documents and producing summaries. proceedings of the 21st annual international acm sigir conference on 
research and development in information retrieval.
h. christensen, y. gotoh, b. killuru, and s. renals. 2003. are extractive text summarization techniques 
portable to broadcast news? proc. of asru.
john conroy and dianne o'leary. 2001. text summarization via id48. proc. of sigir.
j. m. conroy, j. d. schlesinger, and d. p. oleary. 2006. topic-focused id57 using an 
approximate oracle score. proc. coling/acl 2006. pp. 152-159.
thomas cormen, charles e. leiserson, and ronald l. rivest.1990. introduction to algorithms. mit press.
g. erkan and d. r. radev.2004. lexrank: graph-based centrality as salience in text summarization. journal of 
artificial intelligence research (jair).
pascale fung, grace ngai, and percy cheung. 2003. combining optimal id91 and id48 for 
extractive summarization. proceedings of acl workshop on multilingual summarization.sadoki furui, t. kikuchi, 
y. shinnaka, and c. hori. 2004. speech-to-text and speech-to-speech summarization of spontaneous speech. 
ieee transactions on audio, speech, and language processing. 12(4), pages 401-408.
michel galley. 2006. a skip-chain conditional random field for ranking meeting utterances by importance. 
proc. of emnlp.
dan gillick, benoit favre. 2009. a scalable global model for summarization. proceedings of the workshop on 
integer id135 for natural language processing.
dan gillick, korbinian riedhammer, benoit favre, dilek hakkani-tur. 2009. a global optimization framework for 
meeting summarization. proceedings of icassp.
jade goldstein, vibhu mittal, jaime carbonell, and mark kantrowitz. 2000. id57 by 
sentence extraction. proceedings of the 2000 naacl-anlp workshop on id54.

167

references

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

y. gong and x. liu. 2001. generic text summarization using relevance measure and latent semantic analysis. 
proc. acm sigir.
i. gurevych and t. nahnsen. 2005. adapting lexical chaining to summarize conversational dialogues. proc. 
ranlp.
b. hachey, g. murray, and d. reitter.2006. id84 aids term co-occurrence based multi-
document summarization. in: sumqa 06: proceedings of the workshop on task-focused summarization and 
id53. 
aria haghighi and lucy vanderwende. 2009. exploring content models for id57. proc. of 
naacl-hlt.
l. he, e. sanocki, a. gupta, and j. grudin. 2000. comparing presentation summaries: slides vs. reading vs.
listening. proc. of sigchi on human factors in computing systems.
c. hori and sadaoki furui. 2001. advances in automatic speech summarization. proc. of eurospeech.
t. kikuchi, s. furui, and c. hori. 2003. automatic speech summarization based on sentence extractive and 
compaction. proc. of icslp.  
julian kupiec, jan pedersen, and francine chen. 1995. a trainable document summarizer. proc. of sigir.
j. leskovec, n. milic-frayling, and m. grobelnik. 2005. impact of linguistic analysis on the semantic graph 
coverage and learning of document extracts. proc. aaai.
chin-yew lin. 2004. id8: a package for automatic evaluation of summaries, workshop on text 
summarization branches out. 
c.y. lin and e. hovy. 2000. the automated acquisition of topic signatures for text summarization. proc. coling.
hui lin and jeff bilmes. 2010. id57 via budgeted maximization of submodular functions. 
proc. of naacl.
hui lin and jeff bilmes and shasha xie. 2009. graph-based submodular selection for extractive summarization. 
proceedings of asru.
shih-hsiang lin and berlin chen. 2009. improved speech summarization with multiple-hypothesis 
representations and id181 measures. proc. of interspeech.
shih-hsiang lin, berlin chen, and h. min wang. 2009. a comparative study of probabilistic ranking models for 
chinese spoken document summarization. acm transactions on asian language information processing.

168

84

references

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

shih hsiang lin, yu mei chang, jia wen liu, berlin chen. 2010 leveraging evaluation metric-related training 
criteria for speech summarization. proc. of icassp.
fei liu and yang liu. 2009. from extractive to abstractive meeting summaries: can it be done by sentence 
compression? proc. of acl.
fei liu and yang liu. 2010. using spoken utterance compression for meeting summarization: a pilot study. proc. 
of ieee slt.
yang liu and shasha xie. 2008. impact of automatic sentence segmentation on meeting summarization. proc. of 
icassp.
yang liu, feifan liu, bin li, and shasha xie. 2007. do disfluencies affect meeting summarization: a pilot study 
on the impact of disfluencies. poster at mlmi.
yang liu, shasha xie, and fei liu. 2010. using n-best recognition output for extractive summarization and 
keyword extraction in meeting speech. proc. of icassp.
annie louis and ani nenkova. 2009. automatically evaluating content selection in summarization without 
human models. proceedings of emnlp
h.p. luhn. 1958. the automatic creation of literature abstracts. ibm journal of research and development 2(2).
inderjeet mani, gary klein, david house, lynette hirschman, therese firmin, and beth sundheim. 2002. 
summac: a text summarization evaluation. natual language engineering. 8,1 (march 2002), 43-68.
manuel j. mana-lopez, manuel de buenaga, and jose m. gomez-hidalgo. 2004. multidocument summarization: 
an added value to id91 in interactive retrieval. acm trans. inf. systems.
sameer maskey. 2008. automatic broadcast news summarization. ph.d thesis. columbia university.
sameer maskey and julia hirschberg.  2005. comparing lexical, acoustic/prosodic, discourse and structural 
features for speech summarization. proceedings of interspeech.
sameer maskey and julia hirschberg. 2006. summarizing speech without text using id48. 
proc. of hlt-naacl.
ryan mcdonald. 2007. a study of global id136 algorithms in id57. lecture notes in 
computer science. advances in information retrieval. 
kathleen mckeown, rebecca j. passonneau, david k. elson, ani nenkova, and julia hirschberg. 2005. do 
summaries help?. proc. of sigir.
k. mckeown, j. l. klavans, v. hatzivassiloglou, r. barzilay, and e. eskin.1999. towards multidocument
summarization by reformulation: progress and prospects. proc. aaai 1999.

169

references

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

r. mihalcea and p. tarau .2004. textrank: bringing order into texts. proc. of emnlp 2004. 
g. murray, s. renals, j. carletta, j. moore. 2005. evaluating automatic summaries of meeting recordings. proc. 
of acl workshop on intrinsic and extrinsic evaluation measures for machine translation.
g. murray, t. kleinbauer, p. poller, t. becker, s. renals, and j. kilgour. 2009. extrinsic summarization 
evaluation: a decision audit task. acm transactions on speech and language processing.
a. nenkova and r. passonneau. 2004. evaluating content selection in summarization: the pyramid method. 
proc. hlt-naacl.
a. nenkova, l. vanderwende, and k. mckeown. 2006. a compositional context sensitive multi-document 
summarizer: exploring the factors that influence summarization. proc. acm sigir.
a. nenkova, r. passonneau, and k. mckeown. 2007. the pyramid method: incorporating human content 
selection variation in summarization evaluation. acm trans. speech lang. processing.
miles osborne. 2002. using maximum id178 for sentence extraction. proc. of acl workshop on automatic 
summarization.
gerald penn and xiaodan zhu. 2008. a critical reassessement of evaluation baselines for speech 
summarization. proc. of acl-hlt.
dmitri g. roussinov and hsinchun chen. 2001. information navigation on the web by id91 and summarizing 
query results. inf. process. manage. 37, 6 (october 2001), 789-816. 
b. schiffman, a. nenkova, and k. mckeown. 2002. experiments in multidocument summarization. proc. hlt.
a. siddharthan, a. nenkova, and k. mckeown.2004. syntactic simplification for improving content selection in 
id57. proc. coling.
h. grogory silber and kathleen f. mccoy. 2002. efficiently computed lexical chains as an intermediate 
representation for automatic text summarization. computational. linguist. 28, 4 (december 2002), 487-496.
j. steinberger, m. poesio, m. a. kabadjov, and k. jeek. 2007. two uses of id2 in summarization. 
inf. process. manage. 43(6).
s. tucker and s. whittaker. 2008. temporal compression of speech: an evaluation. ieee transactions on audio, 
speech and language processing, pages 790-796.
l. vanderwende, h. suzuki, c. brockett, and a. nenkova. 2007. beyond sumbasic: task-focused summarization 
with id11 and lexical expansion. information processing and management 43.

170

85

references

 

 

 

 

 

 

kam-fai wong, mingli wu, and wenjie li. 2008. extractive summarization using supervised and semi-supervised 
learning. proc. of acl.
shasha xie and yang liu. 2010. improving supervised learning for meeting summarization using sampling and 
regression. computer speech and language. v24, pages 495-514.
shasha xie and yang liu. 2010. using confusion networks for speech summarization. proc. of naacl.
shasha xie, dilek hakkani-tur, benoit favre, and yang liu. 2009. integrating prosodic features in extractive 
meeting summarization. proc. of asru.
shasha xie, hui lin, and yang liu. 2010. semi-supervised extractive speech summarization via co-training 
algorithm. proc. of interspeech.
s. ye, t.-s. chua, m.-y. kan, and l. qiu. 2007. document concept lattice for text understanding and 
summarization. information processing and management 43(6).

  w. yih, j. goodman, l. vanderwende, and h. suzuki. 2007. id57 by maximizing 

 

 

 

 

 

informative content-words. proc. ijcai 2007.
klaus zechner. 2002. id54 of open-domain multiparty dialogues in diverse genres. 
computational linguistics. v28, pages 447-485.
klaus zechner and alex waibel. 2000. minimizing word error rate in textual summaries of spoken language. 
proceedings of the 1st north american chapter of the association for computational linguistics conference.
justin zhang and pascale fung. 2009. extractive speech summarization by active learning. proc. of asru.
xiaodan zhu and gerald penn. 2006. comparing the roles of textual, acoustic and spoken-language features 
on spontaneous conversation summarization. proc. of hlt-naacl.
xiaodan zhu, gerald penn, and f. rudzicz. 2009. summarizing multiple spoken documents: finding evidence 
from untranscribed audio. proc. of acl.

171

86

