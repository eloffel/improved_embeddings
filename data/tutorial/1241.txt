w. bruce croft
donald metzler
trevor strohman

search engines

information retrieval in practice

  w.b. croft, d. metzler, t. strohman, 2015
this book was previously published by: pearson education, inc.

preface

this book provides an overview of the important issues in information retrieval,
and how those issues a   ect the design and implementation of search engines. not
every topic is covered at the same level of detail. we focus instead on what we
consider to be the most important alternatives to implementing search engine
components and the information retrieval models underlying them. web search
engines are obviously a major topic, and we base our coverage primarily on the
technology we all use on the web,1 but search engines are also used in many other
applications. that is the reason for the strong emphasis on the information re-
trieval theories and concepts that underlie all search engines.

the target audience for the book is primarily undergraduates in computer sci-
ence or computer engineering, but graduate students should also find this useful.
we also consider the book to be suitable for most students in information sci-
ence programs. finally, practicing search engineers should benefit from the book,
whatever their background. there is mathematics in the book, but nothing too
esoteric. there are also code and programming exercises in the book, but nothing
beyond the capabilities of someone who has taken some basic computer science
and programming classes.

the exercises at the end of each chapter make extensive use of a java   -based
open source search engine called galago. galago was designed both for this book
and to incorporate lessons learned from experience with the lemur and indri
projects. in other words, this is a fully functional search engine that can be used
to support real applications. many of the programming exercises require the use,
modification, and extension of galago components.

1 in keeping with common usage, most uses of the word    web    in this book are not cap-

italized, except when we refer to the world wide web as a separate entity.

vi

preface

contents

in the first chapter, we provide a high-level review of the field of information re-
trieval and its relationship to search engines. in the second chapter, we describe
the architecture of a search engine. this is done to introduce the entire range of
search engine components without getting stuck in the details of any particular
aspect. in chapter 3, we focus on crawling, document feeds, and other techniques
for acquiring the information that will be searched. chapter 4 describes the sta-
tistical nature of text and the techniques that are used to process it, recognize im-
portant features, and prepare it for indexing. chapter 5 describes how to create
indexes for e   cient search and how those indexes are used to process queries. in
chapter 6, we describe the techniques that are used to process queries and trans-
form them into better representations of the user   s information need.

ranking algorithms and the retrieval models they are based on are covered
in chapter 7. this chapter also includes an overview of machine learning tech-
niques and how they relate to information retrieval and search engines. chapter
8 describes the evaluation and performance metrics that are used to compare and
tune search engines. chapter 9 covers the important classes of techniques used for
classification, filtering, id91, and dealing with spam. social search is a term
used to describe search applications that involve communities of people in tag-
ging content or answering questions. search techniques for these applications and
peer-to-peer search are described in chapter 10. finally, in chapter 11, we give an
overview of advanced techniques that capture more of the content of documents
than simple word-based approaches. this includes techniques that use linguistic
features, the document structure, and the content of nontextual media, such as
images or music.

information retrieval theory and the design, implementation, evaluation, and
use of search engines cover too many topics to describe them all in depth in one
book. we have tried to focus on the most important topics while giving some
coverage to all aspects of this challenging and rewarding subject.

supplements

a range of supplementary material is provided for the book. this material is de-
signed both for those taking a course based on the book and for those giving the
course. specifically, this includes:
    extensive lecture slides (in pdf and ppt format)

preface

vii

    solutions to selected end   of   chapter problems (instructors only)
    test collections for exercises
    galago search engine
the supplements are available at www.search-engines-book.com.

acknowledgments

first and foremost, this book would not have happened without the tremen-
dous support and encouragement from our wives, pam aselton, anne-marie
strohman, and shelley wang. the university of massachusetts amherst provided
material support for the preparation of the book and awarded a conti faculty fel-
lowship to croft, which sped up our progress significantly. the sta    at the center
for intelligent information retrieval (jean joyce, kate moruzzi, glenn stowell,
and andre gauthier) made our lives easier in many ways, and our colleagues and
students in the center provided the stimulating environment that makes work-
ing in this area so rewarding. a number of people reviewed parts of the book and
we appreciated their comments. finally, we have to mention our children, doug,
eric, evan, and natalie, or they would never forgive us.

b             c            
d       m                  
t                s                     

2015 update

this version of the book is being made available for free download. it has been
edited to correct the minor errors noted in the 5 years since the book   s publica-
tion. the authors, meanwhile, are working on a second edition.

contents

1

search engines and information retrieval . . . . . . . . . . . . . . . . . . . . . . .
1.1 what is information retrieval? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 the big issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 search engines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 search engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
1
4
6
9

2 architecture of a search engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1 what is an architecture? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.2 basic building blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3 breaking it down . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3.1 text acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3.2 text transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.3 index creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.3.4 user interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.3.5 ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.6 evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.4 how does it really work? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

3 crawls and feeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.1 deciding what to search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.2 id190 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.2.1 retrieving web pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.2.2 the web crawler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.2.3 freshness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.2.4 focused crawling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.5 deep web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

x

contents

4

3.2.6 sitemaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.2.7 distributed crawling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.3 crawling documents and email . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.4 document feeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.5 the conversion problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.5.1 character encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.6 storing the documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.6.1 using a database system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.6.2 random access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.6.3 compression and large files . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.6.4 update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.6.5 bigtable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.7 detecting duplicates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.8 removing noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

processing text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.1 from words to terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.2 text statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.2.1 vocabulary growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.2.2 estimating collection and result set sizes . . . . . . . . . . . . . . 83
4.3 document parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.3.1 overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.3.2 tokenizing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.3.3 stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.3.4 id30 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
4.3.5 phrases and id165s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
4.4 document structure and markup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.5 link analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.5.1 anchor text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.5.2 id95 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.5.3 link quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.6 information extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4.6.1 id48 for extraction . . . . . . . . . . . . . . . . . 115
4.7 internationalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

contents

xi

5 ranking with indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
5.1 overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
5.2 abstract model of ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.3 inverted indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.3.1 documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.3.2 counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.3.3 positions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5.3.4 fields and extents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.3.5 scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.3.6 ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.4 compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.4.1 id178 and ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
5.4.2 delta encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
5.4.3 bit-aligned codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.4.4 byte-aligned codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
5.4.5 compression in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.4.6 looking ahead. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
5.4.7 skipping and skip pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
5.5 auxiliary structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
5.6 index construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
5.6.1 simple construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
5.6.2 merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.6.3 parallelism and distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 158
5.6.4 update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.7 query processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
5.7.1 document-at-a-time evaluation . . . . . . . . . . . . . . . . . . . . . . . 166
5.7.2 term-at-a-time evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5.7.3 optimization techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.7.4 structured queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
5.7.5 distributed evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.7.6 caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

6 queries and interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.1 information needs and queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.2 query transformation and refinement . . . . . . . . . . . . . . . . . . . . . . . 190
6.2.1 stopping and id30 revisited . . . . . . . . . . . . . . . . . . . . . 190
6.2.2 spell checking and suggestions . . . . . . . . . . . . . . . . . . . . . . . 193

xii

contents

6.2.3 id183 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
6.2.4 relevance feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
6.2.5 context and personalization . . . . . . . . . . . . . . . . . . . . . . . . . . 211
6.3 showing the results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.3.1 result pages and snippets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.3.2 advertising and search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
6.3.3 id91 the results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
6.4 cross-language search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

7 retrieval models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
7.1 overview of retrieval models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
7.1.1 boolean retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.1.2 the vector space model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
7.2 probabilistic models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
7.2.1 information retrieval as classification . . . . . . . . . . . . . . . . . 244
7.2.2 the bm25 ranking algorithm . . . . . . . . . . . . . . . . . . . . . . . . 250
7.3 ranking based on language models . . . . . . . . . . . . . . . . . . . . . . . . . 252
7.3.1 query likelihood ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
7.3.2 relevance models and pseudo-relevance feedback . . . . . . 261
7.4 complex queries and combining evidence . . . . . . . . . . . . . . . . . . . 267
7.4.1 the id136 network model
. . . . . . . . . . . . . . . . . . . . . . . . 268
7.4.2 the galago query language . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.5 web search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
7.6 machine learning and information retrieval . . . . . . . . . . . . . . . . . . 283
7.6.1 learning to rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
7.6.2 topic models and vocabulary mismatch . . . . . . . . . . . . . . . . 288
7.7 application-based models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291

8 evaluating search engines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
8.1 why evaluate? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
8.2 the evaluation corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.3 logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
8.4 e   ectiveness metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.4.1 recall and precision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.4.2 averaging and interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 313
8.4.3 focusing on the top documents . . . . . . . . . . . . . . . . . . . . . . 318
8.4.4 using preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321

contents xiii

8.5 e   ciency metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
8.6 training, testing, and statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
8.6.1 significance tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
8.6.2 setting parameter values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
8.6.3 online testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
8.7 the bottom line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333

9 classification and id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
9.1 classification and categorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
9.1.1 na  ve bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
9.1.2 support vector machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
9.1.3 evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
9.1.4 classifier and feature selection . . . . . . . . . . . . . . . . . . . . . . . . 359
9.1.5 spam, sentiment, and online advertising . . . . . . . . . . . . . . 364
9.2 id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
9.2.1 hierarchical and id116 id91 . . . . . . . . . . . . . . . . . . 375
9.2.2 k nearest neighbor id91 . . . . . . . . . . . . . . . . . . . . . . . 384
9.2.3 evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
9.2.4 how to choose k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
9.2.5 id91 and search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389

10 social search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
10.1 what is social search? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
10.2 user tags and manual indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
10.2.1 searching tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
10.2.2 inferring missing tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
10.2.3 browsing and tag clouds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
10.3 searching with communities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
10.3.1 what is a community? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
10.3.2 finding communities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
10.3.3 community-based id53 . . . . . . . . . . . . . . . . 415
10.3.4 collaborative searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
10.4 filtering and recommending . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
10.4.1 document filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
10.4.2 id185 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
10.5 peer-to-peer and metasearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
10.5.1 distributed search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438

xiv contents

10.5.2 p2p networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442

11 beyond bag of words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
11.1 overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
11.2 feature-based retrieval models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
11.3 term dependence models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
11.4 structure revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
11.4.1 xml retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
11.4.2 entity search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
11.5 longer questions, better answers . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
11.6 words, pictures, and music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
11.7 one search fits all? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479

references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487

index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511

list of figures

1.1

search engine design and the core information retrieval issues . . .

9

2.1 the indexing process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2 the query process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

3.1 a uniform resource locator (url), split into three parts . . . . . . . 33
3.2 id190. the web crawler connects to web servers to
find pages. pages may link to other pages on the same server or
on di   erent servers.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.3 an example robots.txt file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.4 a simple crawling thread implementation . . . . . . . . . . . . . . . . . . . . 37
3.5 an http head request and server response . . . . . . . . . . . . . . . 38
3.6 age and freshness of a single page over time . . . . . . . . . . . . . . . . . . 39
3.7 expected age of a page with mean change frequency    = 1/7

(one week) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.8 an example sitemap file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.9 an example rss 2.0 feed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.10 an example of text in the trec web compound document

format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.11 an example link with anchor text . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.12 bigtable stores data in a single logical table, which is split into

many smaller tablets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.13 a bigtable row . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.14 example of fingerprinting process . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.15 example of simhash fingerprinting process . . . . . . . . . . . . . . . . . . . 64
3.16 main content block in a web page . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

xvi

list of figures

3.17 tag counts used to identify text blocks in a web page . . . . . . . . . . . 66
3.18 part of the dom structure for the example web page . . . . . . . . . . 67

4.1 rank versus id203 of occurrence for words assuming

zipf    s law (rank    id203 = 0.1) . . . . . . . . . . . . . . . . . . . . . . . . 76

4.2 a log-log plot of zipf    s law compared to real data from ap89.
the predicted relationship between id203 of occurrence
and rank breaks down badly at high ranks.

. . . . . . . . . . . . . . . . . . . 79

4.3 vocabulary growth for the trec ap89 collection compared

to heaps    law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

4.4 vocabulary growth for the trec gov2 collection compared

to heaps    law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.5 result size estimate for web search . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.6 comparison of stemmer output for a trec query. stopwords

have also been removed.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
4.7 output of a pos tagger for a trec query . . . . . . . . . . . . . . . . . . . 98
4.8
part of a web page from wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4.9 html source for example wikipedia page . . . . . . . . . . . . . . . . . . 103
4.10 a sample    internet    consisting of just three web pages. the

arrows denote links between the pages.

. . . . . . . . . . . . . . . . . . . . . . 108
4.11 pseudocode for the iterative id95 algorithm . . . . . . . . . . . . . . 110
4.12 trackback links in blog postings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.13 text tagged by information extraction . . . . . . . . . . . . . . . . . . . . . . . 114
4.14 sentence model for statistical entity extractor . . . . . . . . . . . . . . . . . 116
4.15 chinese segmentation and bigrams . . . . . . . . . . . . . . . . . . . . . . . . . . 119

5.1 the components of the abstract model of ranking: documents,

features, queries, the retrieval function, and document scores . . . . 127

5.2 a more concrete model of ranking. notice how both the query

and the document have feature functions in this model. . . . . . . . . 128
5.3 an inverted index for the documents (sentences) in table 5.1 . . . 132
5.4 an inverted index, with word counts, for the documents in

table 5.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134

5.5 an inverted index, with word positions, for the documents in

table 5.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

5.6 aligning posting lists for    tropical    and    fish    to find the phrase

   tropical fish    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

list of figures xvii

5.7 aligning posting lists for    fish    and title to find matches of the

word    fish    in the title field of a document.
. . . . . . . . . . . . . . . . . . . 138
pseudocode for a simple indexer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157

5.8
5.9 an example of index merging. the first and second indexes are

merged together to produce the combined index.

. . . . . . . . . . . . . 158
5.10 mapreduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.11 mapper for a credit card summing algorithm. . . . . . . . . . . . . . . . . . 162
5.12 reducer for a credit card summing algorithm . . . . . . . . . . . . . . . . . 162
5.13 mapper for documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
5.14 reducer for word postings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.15 document-at-a-time query evaluation. the numbers (x:y)

represent a document number (x) and a word count (y).

. . . . . . . 166
5.16 a simple document-at-a-time retrieval algorithm . . . . . . . . . . . . . . 167
5.17 term-at-a-time query evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5.18 a simple term-at-a-time retrieval algorithm . . . . . . . . . . . . . . . . . . . 169
5.19 skip pointers in an inverted list. the gray boxes show skip

pointers, which point into the white boxes, which are inverted
list postings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.20 a term-at-a-time retrieval algorithm with conjunctive processing 173
5.21 a document-at-a-time retrieval algorithm with conjunctive

processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

5.22 maxscore retrieval with the query    eucalyptus tree   . the gray

boxes indicate postings that can be safely ignored during scoring. 176

5.23 evaluation tree for the structured query #combine(#od:1(tropical

fish) #od:1(aquarium fish) fish) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

6.1 top ten results for the query    tropical fish    . . . . . . . . . . . . . . . . . . . 209
6.2 geographic representation of cape cod using bounding

rectangles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
6.3 typical document summary for a web search . . . . . . . . . . . . . . . . . . 215
6.4 an example of a text span of words (w) bracketed by significant

words (s) using luhn   s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

6.5 advertisements displayed by a search engine for the query    fish

tanks    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

6.6 clusters formed by a search engine from top-ranked documents

for the query    tropical fish   . numbers in brackets are the
number of documents in the cluster. . . . . . . . . . . . . . . . . . . . . . . . . . 222

xviii list of figures

6.7 categories returned for the query    tropical fish    in a popular

online retailer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6.8
subcategories and facets for the    home & garden    category . . . . 225
6.9 cross-language search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
6.10 a french web page in the results list for the query    pecheur

france    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

7.1 term-document matrix for a collection of four documents . . . . . . 239
7.2 vector representation of documents and queries . . . . . . . . . . . . . . . 240
7.3 classifying a document as relevant or non-relevant . . . . . . . . . . . . 245
7.4 example id136 network model
. . . . . . . . . . . . . . . . . . . . . . . . . . 269
7.5
id136 network with three nodes . . . . . . . . . . . . . . . . . . . . . . . . . 271
7.6 galago query for the dependence model
. . . . . . . . . . . . . . . . . . . . . 282
7.7 galago query for web data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282

8.1 example of a trec topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
8.2 recall and precision values for two rankings of six relevant

documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
8.3 recall and precision values for rankings from two di   erent queries314
8.4 recall-precision graphs for two queries. . . . . . . . . . . . . . . . . . . . . . . 315
8.5
interpolated recall-precision graphs for two queries . . . . . . . . . . . . 316
8.6 average recall-precision graph using standard recall levels. . . . . . . 317
8.7 typical recall-precision graph for 50 queries from trec . . . . . . . 318
8.8

id203 distribution for test statistic values assuming the
null hypothesis. the shaded area is the region of rejection for a
one-sided test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
8.9 example distribution of query e   ectiveness improvements . . . . . . 335

9.1

9.2

illustration of how documents are represented in the multiple-
bernoulli event space. in this example, there are 10 documents
(each with a unique id), two classes (spam and not spam), and a
vocabulary that consists of the terms    cheap   ,    buy   ,    banking   ,
   dinner   , and    the   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
illustration of how documents are represented in the
multinomial event space. in this example, there are 10
documents (each with a unique id), two classes (spam and not
spam), and a vocabulary that consists of the terms    cheap   ,
   buy   ,    banking   ,    dinner   , and    the   . . . . . . . . . . . . . . . . . . . . . . . . . . 349

list of figures xix

9.3 data set that consists of two classes (pluses and minuses). the
data set on the left is linearly separable, whereas the one on the
right is not.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352

9.4 graphical illustration of support vector machines for the
linearly separable case. here, the hyperplane defined by w is
shown, as well as the margin, the decision regions, and the
support vectors, which are indicated by circles. . . . . . . . . . . . . . . . . 353

9.5 generative process used by the na  ve bayes model. first, a class

is chosen according to p (c), and then a document is chosen
according to p (d|c).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360

9.6 example data set where non-parametric learning algorithms,

such as a nearest neighbor classifier, may outperform parametric
algorithms. the pluses and minuses indicate positive and
negative training examples, respectively. the solid gray line
shows the actual decision boundary, which is highly non-linear.

. 361
9.7 example output of spamassassin email spam filter . . . . . . . . . . . . 365
9.8 example of web page spam, showing the main page and some

of the associated term and link spam . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.9 example product review incorporating sentiment . . . . . . . . . . . . . 370
9.10 example semantic class match between a web page about
rainbow fish (a type of tropical fish) and an advertisement
for tropical fish food. the nodes    aquariums   ,    fish   , and
   supplies    are example nodes within a semantic hierarchy.
the web page is classified as    aquariums - fish    and the ad is
classified as    supplies - fish   . here,    aquariums    is the least
common ancestor. although the web page and ad do not share
any terms in common, they can be matched because of their
semantic similarity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372

9.11 example of divisive id91 with k = 4. the id91

proceeds from left to right and top to bottom, resulting in four
clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376

9.12 example of agglomerative id91 with k = 4. the
id91 proceeds from left to right and top to bottom,
resulting in four clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

9.13 dendrogram that illustrates the agglomerative id91 of the

points from figure 9.12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

xx

list of figures

9.14 examples of clusters in a graph formed by connecting nodes

representing instances. a link represents a distance between the
two instances that is less than some threshold value.. . . . . . . . . . . . 379
9.15 illustration of how various id91 cost functions are computed 381
9.16 example of overlapping id91 using nearest neighbor

id91 with k = 5. the overlapping clusters for the black
points (a, b, c, and d) are shown. the five nearest neighbors
for each black point are shaded gray and labeled accordingly. . . . . 385

9.17 example of overlapping id91 using parzen windows. the
clusters for the black points (a, b, c, and d) are shown. the
shaded circles indicate the windows used to determine cluster
membership. the neighbors for each black point are shaded
gray and labeled accordingly.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388

9.18 cluster hypothesis tests on two trec collections. the top
two compare the distributions of similarity values between
relevant-relevant and relevant-nonrelevant pairs (light gray) of
documents. the bottom two show the local precision of the
relevant documents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390

10.1 search results used to enrich a tag representation. in this

example, the tag being expanded is    tropical fish   . the query
   tropical fish    is run against a search engine, and the snippets
returned are then used to generate a distribution over related
terms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403

10.2 example of a tag cloud in the form of a weighted list. the

tags are in alphabetical order and weighted according to some
criteria, such as popularity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407

10.3 illustration of the hits algorithm. each row corresponds to a
single iteration of the algorithm and each column corresponds
to a specific step of the algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . 412

10.4 example of how nodes within a directed graph can be

represented as vectors. for a given node p, its vector
representation has component q set to 1 if p     q. . . . . . . . . . . . . . 413

list of figures xxi

10.5 overview of the two common collaborative search scenarios.
on the left is co-located collaborative search, which involves
multiple participants in the same location at the same time.
on the right is remote collaborative search, where participants
are in di   erent locations and not necessarily all online and
searching at the same time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421

10.6 example of a static filtering system. documents arrive over time
and are compared against each profile. arrows from documents
to profiles indicate the document matches the profile and is
retrieved.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425

10.7 example of an adaptive filtering system. documents arrive

over time and are compared against each profile. arrows from
documents to profiles indicate the document matches the
profile and is retrieved. unlike static filtering, where profiles are
static over time, profiles are updated dynamically (e.g., when a
new match occurs).. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428

10.8 a set of users within a recommender system. users and their
ratings for some item are given. users with question marks
above their heads have not yet rated the item. it is the goal of
the recommender system to fill in these question marks. . . . . . . . . 434

10.9 illustration of id185 using id91. groups
of similar users are outlined with dashed lines. users and their
ratings for some item are given. in each group, there is a single
user who has not judged the item. for these users, the unjudged
item is assigned an automatic rating based on the ratings of
similar users.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435

10.10 metasearch engine architecture. the query is broadcast to

multiple web search engines and result lists are merged.

. . . . . . . . 439

10.11 network architectures for distributed search: (a) central hub;
(b) pure p2p; and (c) hierarchical p2p. dark circles are hub
or superpeer nodes, gray circles are provider nodes, and white
circles are consumer nodes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443

10.12 neighborhoods (ni) of a hub node (h) in a hierarchical p2p

network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445

xxii list of figures

11.1 example markov random field model assumptions, including

full independence (top left), sequential dependence (top
right), full dependence (bottom left), and general dependence
(bottom right) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

11.2 graphical model representations of the relevance model

technique (top) and latent concept expansion (bottom) used
for pseudo-relevance feedback with the query    hubble telescope
achievements    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459

11.3 functions provided by a search engine interacting with a simple

database system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461

11.4 example of an entity search for organizations using the trec

wall street journal 1987 collection . . . . . . . . . . . . . . . . . . . . . . . . . 464
11.5 id53 system architecture . . . . . . . . . . . . . . . . . . . . . . 467
11.6 examples of ocr errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
11.7 examples of speech recognizer errors . . . . . . . . . . . . . . . . . . . . . . . . 473
11.8 two images (a fish and a flower bed) with color histograms.

the horizontal axis is hue value. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474

11.9 three examples of content-based id162. the collection

for the first two consists of 1,560 images of cars, faces, apes,
and other miscellaneous subjects. the last example is from a
collection of 2,048 trademark images. in each case, the leftmost
image is the query.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
11.10 key frames extracted from a trec video clip . . . . . . . . . . . . . . . . 476
11.11 examples of automatic text annotation of images . . . . . . . . . . . . . . 477
11.12 three representations of bach   s    fugue #10   : audio, midi, and

conventional music notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478

list of tables

1.1

some dimensions of information retrieval . . . . . . . . . . . . . . . . . . . .

4

3.1 utf-8 encoding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

4.1
statistics for the ap89 collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.2 most frequent 50 words from ap89 . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.3 low-frequency words from ap89 . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.4 example word frequency ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.5

proportions of words occurring n times in 336,310 documents
from the trec volume 3 corpus. the total vocabulary size
(number of unique words) is 508,209.

. . . . . . . . . . . . . . . . . . . . . . . 80

4.6 document frequencies and estimated frequencies for word
combinations (assuming independence) in the gov2 web
collection. collection size (n) is 25,205,179. . . . . . . . . . . . . . . . . . 84

4.7 examples of errors made by the original porter stemmer. false

positives are pairs of words that have the same stem. false
negatives are pairs that have di   erent stems.

. . . . . . . . . . . . . . . . . . 93
4.8 examples of words with the arabic root ktb . . . . . . . . . . . . . . . . . . 96
4.9 high-frequency noun phrases from a trec collection and

u.s. patents from 1996 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
4.10 statistics for the google id165 sample . . . . . . . . . . . . . . . . . . . . . . 101
four sentences from the wikipedia entry for tropical fish . . . . . . . 132
5.1
5.2 elias-   code examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
5.3 elias-   code examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.4
space requirements for numbers encoded in v-byte . . . . . . . . . . . . 149

xxiv list of tables

5.5
5.6

sample encodings for v-byte . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
skip lengths (k) and expected processing steps . . . . . . . . . . . . . . . . 152

6.1

partial entry for the medical subject (mesh) heading    neck
pain    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.2 term association measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
6.3 most strongly associated words for    tropical    in a collection of
trec news stories. co-occurrence counts are measured at the
document level.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

6.4 most strongly associated words for    fish    in a collection of

trec news stories. co-occurrence counts are measured at the
document level.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

6.5 most strongly associated words for    fish    in a collection of
trec news stories. co-occurrence counts are measured in
windows of five words.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

7.1 contingency table of term occurrences for a particular query . . . 248
7.2 bm25 scores for an example document . . . . . . . . . . . . . . . . . . . . . . 252
7.3 query likelihood scores for an example document . . . . . . . . . . . . . 260
7.4 highest-id203 terms from relevance model for four

example queries (estimated using top 10 documents) . . . . . . . . . . . 266

7.5 highest-id203 terms from relevance model for four

example queries (estimated using top 50 documents) . . . . . . . . . . . 267
7.6 conditional probabilities for example network . . . . . . . . . . . . . . . 272
7.7 highest-id203 terms from four topics in lda model
. . . . . 290

8.1

8.2
8.3

statistics for three example text collections. the average number
of words per document is calculated without id30. . . . . . . . . 301
statistics for queries from example text collections . . . . . . . . . . . . . 301
sets of documents defined by a simple search with binary
relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
precision values at standard recall levels calculated using
interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
8.5 definitions of some important e   ciency metrics . . . . . . . . . . . . . . 323
8.6 artificial e   ectiveness data for two retrieval algorithms (a and

8.4

b) over 10 queries. the column b     a gives the di   erence in
e   ectiveness.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328

list of tables xxv

9.1 a list of kernels that are typically used with id166s. for each
kernel, the name, value, and implicit dimensionality are given.

. . 357

10.1 example questions submitted to yahoo! answers. . . . . . . . . . . . . . 416
10.2 translations automatically learned from a set of question and

answer pairs. the 10 most likely translations for the terms
   everest   ,    xp   , and    search    are given.. . . . . . . . . . . . . . . . . . . . . . . . . 419

10.3 summary of static and adaptive filtering models. for each, the
profile representation and profile updating algorithm are given.

. 430

10.4 contingency table for the possible outcomes of a filtering
system. here, tp (true positive) is the number of relevant
documents retrieved, fn (false negative) is the number of
relevant documents not retrieved, fp (false positive) is the
number of non-relevant documents retrieved, and tn (true
negative) is the number of non-relevant documents not retrieved. 431

11.1 most likely one- and two-word concepts produced using latent

concept expansion with the top 25 documents retrieved for
the query    hubble telescope achievements    on the trec
robust collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460

11.2 example trec qa questions and their corresponding

question categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469

1

search engines and information
retrieval

   mr. helpmann, i   m keen to get into
information retrieval.   

sam lowry, brazil

1.1 what is information retrieval?

this book is designed to help people understand search engines, evaluate and
compare them, and modify them for specific applications. searching for infor-
mation on the web is, for most people, a daily activity. search and communi-
cation are by far the most popular uses of the computer. not surprisingly, many
people in companies and universities are trying to improve search by coming up
with easier and faster ways to find the right information. these people, whether
they call themselves computer scientists, software engineers, information scien-
tists, search engine optimizers, or something else, are working in the field of in-
formation retrieval.1 so, before we launch into a detailed journey through the
internals of search engines, we will take a few pages to provide a context for the
rest of the book.

gerard salton, a pioneer in information retrieval and one of the leading figures
from the 1960s to the 1990s, proposed the following definition in his classic 1968
textbook (salton, 1968):

information retrieval is a field concerned with the structure, analysis, or-
ganization, storage, searching, and retrieval of information.

despite the huge advances in the understanding and technology of search in the
past 40 years, this definition is still appropriate and accurate. the term    informa-

1 information retrieval is often abbreviated as ir. in this book, we mostly use the full
term. this has nothing to do with the fact that many people think ir means    infrared   
or something else.

2

1 search engines and information retrieval

tion    is very general, and information retrieval includes work on a wide range of
types of information and a variety of applications related to search.

the primary focus of the field since the 1950s has been on text and text docu-
ments. web pages, email, scholarly papers, books, and news stories are just a few
of the many examples of documents. all of these documents have some amount
of structure, such as the title, author, date, and abstract information associated
with the content of papers that appear in scientific journals. the elements of this
structure are called attributes, or fields, when referring to database records. the
important distinction between a document and a typical database record, such as
a bank account record or a flight reservation, is that most of the information in
the document is in the form of text, which is relatively unstructured.

to illustrate this di   erence, consider the information contained in two typical
attributes of an account record, the account number and current balance. both are
very well defined, both in terms of their format (for example, a six-digit integer
for an account number and a real number with two decimal places for balance)
and their meaning. it is very easy to compare values of these attributes, and conse-
quently it is straightforward to implement algorithms to identify the records that
satisfy queries such as    find account number 321456    or    find accounts with
balances greater than $50,000.00   .

now consider a news story about the merger of two banks. the story will have
some attributes, such as the headline and source of the story, but the primary con-
tent is the story itself. in a database system, this critical piece of information would
typically be stored as a single large attribute with no internal structure. most of
the queries submitted to a web search engine such as google2 that relate to this
story will be of the form    bank merger    or    bank takeover   . to do this search,
we must design algorithms that can compare the text of the queries with the text
of the story and decide whether the story contains the information that is being
sought. defining the meaning of a word, a sentence, a paragraph, or a whole news
story is much more di   cult than defining an account number, and consequently
comparing text is not easy. understanding and modeling how people compare
texts, and designing computer algorithms to accurately perform this comparison,
is at the core of information retrieval.

increasingly, applications of information retrieval involve multimedia docu-
ments with structure, significant text content, and other media. popular infor-
mation media include pictures, video, and audio, including music and speech. in

2 http://www.google.com

1.1 what is information retrieval?

3

some applications, such as in legal support, scanned document images are also
important. these media have content that, like text, is di   cult to describe and
compare. the current technology for searching non-text documents relies on text
descriptions of their content rather than the contents themselves, but progress is
being made on techniques for direct comparison of images, for example.

in addition to a range of media, information retrieval involves a range of tasks
and applications. the usual search scenario involves someone typing in a query to
a search engine and receiving answers in the form of a list of documents in ranked
order. although searching the world wide web (web search) is by far the most
common application involving information retrieval, search is also a crucial part
of applications in corporations, government, and many other domains. vertical
search is a specialized form of web search where the domain of the search is re-
stricted to a particular topic. enterprise search involves finding the required infor-
mation in the huge variety of computer files scattered across a corporate intranet.
web pages are certainly a part of that distributed information store, but most
information will be found in sources such as email, reports, presentations, spread-
sheets, and structured data in corporate databases. desktop search is the personal
version of enterprise search, where the information sources are the files stored
on an individual computer, including email messages and web pages that have re-
cently been browsed. peer-to-peer search involves finding information in networks
of nodes or computers without any centralized control. this type of search began
as a file sharing tool for music but can be used in any community based on shared
interests, or even shared locality in the case of mobile devices. search and related
information retrieval techniques are used for advertising, for intelligence analy-
sis, for scientific discovery, for health care, for customer support, for real estate,
and so on. any application that involves a collection3of text or other unstructured
information will need to organize and search that information.

search based on a user query (sometimes called ad hoc search because the range
of possible queries is huge and not prespecified) is not the only text-based task
that is studied in information retrieval. other tasks include filtering, classification,
and id53. filtering or tracking involves detecting stories of interest
based on a person   s interests and providing an alert using email or some other
mechanism. classification or categorization uses a defined set of labels or classes

3 the term database is often used to refer to a collection of either structured or unstruc-
tured data. to avoid confusion, we mostly use the term document collection (or just
collection) for text. however, the terms web database and search engine database are so
common that we occasionally use them in this book.

4

1 search engines and information retrieval

(such as the categories listed in the yahoo! directory4) and automatically assigns
those labels to documents. id53 is similar to search but is aimed
at more specific questions, such as    what is the height of mt. everest?   . the goal
of id53 is to return a specific answer found in the text, rather than
a list of documents. table 1.1 summarizes some of these aspects or dimensions of
the field of information retrieval.

examples of

content

text
images
video

audio
music

scanned documents

examples of
applications
web search
vertical search
enterprise search
desktop search

peer-to-peer search

examples of

tasks

ad hoc search

filtering

classification

id53

table 1.1. some dimensions of information retrieval

1.2 the big issues

information retrieval researchers have focused on a few key issues that remain just
as important in the era of commercial web search engines working with billions
of web pages as they were when tests were done in the 1960s on document col-
lections containing about 1.5 megabytes of text. one of these issues is relevance.
relevance is a fundamental concept in information retrieval. loosely speaking, a
relevant document contains the information that a person was looking for when
she submitted a query to the search engine. although this sounds simple, there are
many factors that go into a person   s decision as to whether a particular document
is relevant. these factors must be taken into account when designing algorithms
for comparing text and ranking documents. simply comparing the text of a query
with the text of a document and looking for an exact match, as might be done in
a database system or using the grep utility in unix, produces very poor results in
terms of relevance. one obvious reason for this is that language can be used to ex-

4 http://dir.yahoo.com/

1.2 the big issues

5

press the same concepts in many di   erent ways, often with very di   erent words.
this is referred to as the vocabulary mismatch problem in information retrieval.

it is also important to distinguish between topical relevance and user relevance.
a text document is topically relevant to a query if it is on the same topic. for ex-
ample, a news story about a tornado in kansas would be topically relevant to the
query    severe weather events   . the person who asked the question (often called
the user) may not consider the story relevant, however, if she has seen that story
before, or if the story is five years old, or if the story is in chinese from a chi-
nese news agency. user relevance takes these additional features of the story into
account.

to address the issue of relevance, researchers propose retrieval models and test
how well they work. a retrieval model is a formal representation of the process of
matching a query and a document. it is the basis of the ranking algorithm that is
used in a search engine to produce the ranked list of documents. a good retrieval
model will find documents that are likely to be considered relevant by the person
who submitted the query. some retrieval models focus on topical relevance, but
a search engine deployed in a real environment must use ranking algorithms that
incorporate user relevance.

an interesting feature of the retrieval models used in information retrieval is
that they typically model the statistical properties of text rather than the linguistic
structure. this means, for example, that the ranking algorithms are typically far
more concerned with the counts of word occurrences than whether the word is a
noun or an adjective. more advanced models do incorporate linguistic features,
but they tend to be of secondary importance. the use of word frequency infor-
mation to represent text started with another information retrieval pioneer, h.p.
luhn, in the 1950s. this view of text did not become popular in other fields of
computer science, such as natural language processing, until the 1990s.

another core issue for information retrieval is evaluation. since the quality of
a document ranking depends on how well it matches a person   s expectations, it
was necessary early on to develop evaluation measures and experimental proce-
dures for acquiring this data and using it to compare ranking algorithms. cyril
cleverdon led the way in developing evaluation methods in the early 1960s, and
two of the measures he used, precision and recall, are still popular. precision is a
very intuitive measure, and is the proportion of retrieved documents that are rel-
evant. recall is the proportion of relevant documents that are retrieved. when
the recall measure is used, there is an assumption that all the relevant documents
for a given query are known. such an assumption is clearly problematic in a web

6

1 search engines and information retrieval

search environment, but with smaller test collections of documents, this measure
can be useful. a test collection5 for information retrieval experiments consists of
a collection of text documents, a sample of typical queries, and a list of relevant
documents for each query (the relevance judgments). the best-known test collec-
tions are those associated with the trec6 evaluation forum.

evaluation of retrieval models and search engines is a very active area, with
much of the current focus on using large volumes of log data from user interac-
tions, such as clickthrough data, which records the documents that were clicked
on during a search session. clickthrough and other log data is strongly correlated
with relevance so it can be used to evaluate search, but search engine companies
still use relevance judgments in addition to log data to ensure the validity of their
results.

the third core issue for information retrieval is the emphasis on users and their
information needs. this should be clear given that the evaluation of search is user-
centered. that is, the users of a search engine are the ultimate judges of quality.
this has led to numerous studies on how people interact with search engines and,
in particular, to the development of techniques to help people express their in-
formation needs. an information need is the underlying cause of the query that
a person submits to a search engine. in contrast to a request to a database system,
such as for the balance of a bank account, text queries are often poor descriptions
of what the user actually wants. a one-word query such as    cats    could be a request
for information on where to buy cats or for a description of the broadway musi-
cal. despite their lack of specificity, however, one-word queries are very common
in web search. techniques such as query suggestion, id183, and relevance
feedback use interaction and context to refine the initial query in order to produce
better ranked lists.

these issues will come up throughout this book, and will be discussed in con-
siderably more detail. we now have su   cient background to start talking about
the main product of research in information retrieval   namely, search engines.

1.3 search engines

a search engine is the practical application of information retrieval techniques
to large-scale text collections. a web search engine is the obvious example, but as

5 also known as an evaluation corpus (plural corpora).
6 text retrieval conference   http://trec.nist.gov/

1.3 search engines

7

has been mentioned, search engines can be found in many di   erent applications,
such as desktop search or enterprise search. search engines have been around for
many years. for example, medline, the online medical literature search sys-
tem, started in the 1970s. the term    search engine    was originally used to refer
to specialized hardware for text search. from the mid-1980s onward, however, it
gradually came to be used in preference to    information retrieval system    as the
name for the software system that compares queries to documents and produces
ranked result lists of documents. there is much more to a search engine than the
ranking algorithm, of course, and we will discuss the general architecture of these
systems in the next chapter.

search engines come in a number of configurations that reflect the applica-
tions they are designed for. web search engines, such as google and yahoo!,7 must
be able to capture, or crawl, many terabytes of data, and then provide subsecond
response times to millions of queries submitted every day from around the world.
enterprise search engines   for example, autonomy8   must be able to process
the large variety of information sources in a company and use company-specific
knowledge as part of search and related tasks, such as data mining. data mining
refers to the automatic discovery of interesting structure in data and includes tech-
niques such as id91. desktop search engines, such as the microsoft vista   
search feature, must be able to rapidly incorporate new documents, web pages,
and email as the person creates or looks at them, as well as provide an intuitive
interface for searching this very heterogeneous mix of information. there is over-
lap between these categories with systems such as google, for example, which is
available in configurations for enterprise and desktop search.

open source search engines are another important class of systems that have
somewhat di   erent design goals than the commercial search engines. there are a
number of these systems, and the wikipedia page for information retrieval9 pro-
vides links to many of them. three systems of particular interest are lucene,10
lemur,11 and the system provided with this book, galago.12 lucene is a popular
java-based search engine that has been used for a wide range of commercial ap-
plications. the information retrieval techniques that it uses are relatively simple.

7 http://www.yahoo.com
8 http://www.autonomy.com
9 http://en.wikipedia.org/wiki/information_retrieval
10 http://lucene.apache.org
11 http://www.lemurproject.org
12 http://www.search-engines-book.com

8

1 search engines and information retrieval

lemur is an open source toolkit that includes the indri c++-based search engine.
lemur has primarily been used by information retrieval researchers to compare
advanced search techniques. galago is a java-based search engine that is based on
the lemur and indri projects. the assignments in this book make extensive use of
galago. it is designed to be fast, adaptable, and easy to understand, and incorpo-
rates very e   ective information retrieval techniques.

the    big issues    in the design of search engines include the ones identified for
information retrieval: e   ective ranking algorithms, evaluation, and user interac-
tion. there are, however, a number of additional critical features of search engines
that result from their deployment in large-scale, operational environments. fore-
most among these features is the performance of the search engine in terms of mea-
sures such as response time, query throughput, and indexing speed. response time
is the delay between submitting a query and receiving the result list, throughput
measures the number of queries that can be processed in a given time, and index-
ing speed is the rate at which text documents can be transformed into indexes
for searching. an index is a data structure that improves the speed of search. the
design of indexes for search engines is one of the major topics in this book.

another important performance measure is how fast new data can be incorpo-
rated into the indexes. search applications typically deal with dynamic, constantly
changing information. coverage measures how much of the existing information
in, say, a corporate information environment has been indexed and stored in the
search engine, and recency or freshness measures the    age    of the stored informa-
tion.

search engines can be used with small collections, such as a few hundred emails
and documents on a desktop, or extremely large collections, such as the entire
web. there may be only a few users of a given application, or many thousands.
scalability is clearly an important issue for search engine design. designs that
work for a given application should continue to work as the amount of data and
the number of users grow. in section 1.1, we described how search engines are used
in many applications and for many tasks. to do this, they have to be customizable
or adaptable. this means that many di   erent aspects of the search engine, such as
the ranking algorithm, the interface, or the indexing strategy, must be able to be
tuned and adapted to the requirements of the application.

practical issues that impact search engine design also occur for specific appli-
cations. the best example of this is spam in web search. spam is generally thought
of as unwanted email, but more generally it could be defined as misleading, inap-
propriate, or non-relevant information in a document that is designed for some

1.4 search engineers

9

commercial benefit. there are many kinds of spam, but one type that search en-
gines must deal with is spam words put into a document to cause it to be retrieved
in response to popular queries. the practice of    spamdexing    can significantly de-
grade the quality of a search engine   s ranking, and web search engine designers
have to develop techniques to identify the spam and remove those documents.
figure 1.1 summarizes the major issues involved in search engine design.

fig. 1.1. search engine design and the core information retrieval issues

based on this discussion of the relationship between information retrieval and
search engines, we now consider what roles computer scientists and others play in
the design and use of search engines.

1.4 search engineers

information retrieval research involves the development of mathematical models
of text and language, large-scale experiments with test collections or users, and
a lot of scholarly paper writing. for these reasons, it tends to be done by aca-
demics or people in research laboratories. these people are primarily trained in
computer science, although information science, mathematics, and, occasionally,
social science and computational linguistics are also represented. so who works

5hohydqfh-(iihfwlyh uid25lqj (ydoxdwlrq-7hvwlqj dqg phdvxulqj,qirupdwlrq qhhgv-8vhu lqwhudfwlrq3huirupdqfh-(iilflhqw vhdufk dqg lqgh[lqj ,qfrusrudwlqj qhz gdwd-&ryhudjh dqg iuhvkqhvv6fdodelolw\-*urzlqj zlwk gdwd dqg xvhuv$gdswdelolw\-7xqlqj iru dssolfdwlrqv6shflilf sureohpv-(.j., vsdp,qirupdwlrq 5hwulhydo6hdufk (qjlqhv10

1 search engines and information retrieval

with search engines? to a large extent, it is the same sort of people but with a more
practical emphasis. the computing industry has started to use the term search
engineer to describe this type of person. search engineers are primarily people
trained in computer science, mostly with a systems or database background. sur-
prisingly few of them have training in information retrieval, which is one of the
major motivations for this book.

what is the role of a search engineer? certainly the people who work in the
major web search companies designing and implementing new search engines are
search engineers, but the majority of search engineers are the people who modify,
extend, maintain, or tune existing search engines for a wide range of commercial
applications. people who design or    optimize    content for search engines are also
search engineers, as are people who implement techniques to deal with spam. the
search engines that search engineers work with cover the entire range mentioned
in the last section: they primarily use open source and enterprise search engines
for application development, but also get the most out of desktop and web search
engines.

the importance and pervasiveness of search in modern computer applications
has meant that search engineering has become a crucial profession in the com-
puter industry. there are, however, very few courses being taught in computer
science departments that give students an appreciation of the variety of issues that
are involved, especially from the information retrieval perspective. this book is in-
tended to give potential search engineers the understanding and tools they need.

references and further reading

in each chapter, we provide some pointers to papers and books that give more
detail on the topics that have been covered. this additional reading should not
be necessary to understand material that has been presented, but instead will give
more background, more depth in some cases, and, for advanced topics, will de-
scribe techniques and research results that are not covered in this book.

the classic references on information retrieval, in our opinion, are the books
by salton (1968; 1983) and van rijsbergen (1979). van rijsbergen   s book remains
popular, since it is available on the web.13 all three books provide excellent de-
scriptions of the research done in the early years of information retrieval, up to
the late 1970s. salton   s early book was particularly important in terms of defining

13 http://www.dcs.gla.ac.uk/keith/preface.html

1.4 search engineers

11

the field of information retrieval for computer science. more recent books include
baeza-yates and ribeiro-neto (1999) and manning et al. (2008).

research papers on all the topics covered in this book can be found in the
proceedings of the association for computing machinery (acm) special in-
terest group on information retrieval (sigir) conference. these proceedings
are available on the web as part of the acm digital library.14 good papers
on information retrieval and search also appear in the european conference
on information retrieval (ecir), the conference on information and knowl-
edge management (cikm), and the web search and data mining conference
(wsdm). the wsdm conference is a spin-o    of the world wide web confer-
ence (www), which has included some important papers on web search. the
proceedings from the trec workshops are available online and contain useful
descriptions of new research techniques from many di   erent academic and indus-
try groups. an overview of the trec experiments can be found in voorhees and
harman (2005). an increasing number of search-related papers are beginning to
appear in database conferences, such as vldb and sigmod. occasional papers
also show up in language technology conferences, such as acl and hlt (as-
sociation for computational linguistics and human language technologies),
machine learning conferences, and others.

exercises

1.1. think up and write down a small number of queries for a web search engine.
make sure that the queries vary in length (i.e., they are not all one word). try
to specify exactly what information you are looking for in some of the queries.
run these queries on two commercial web search engines and compare the top
10 results for each query by doing relevance judgments. write a report that an-
swers at least the following questions: what is the precision of the results? what
is the overlap between the results for the two search engines? is one search engine
clearly better than the other? if so, by how much? how do short queries perform
compared to long queries?
1.2. site search is another common application of search engines. in this case,
search is restricted to the web pages at a given website. compare site search to
web search, vertical search, and enterprise search.

14 http://www.acm.org/dl

12

1 search engines and information retrieval

1.3. use the web to find as many examples as you can of open source search en-
gines, information retrieval systems, or related technology. give a brief descrip-
tion of each search engine and summarize the similarities and di   erences between
them.
1.4. list five web services or sites that you use that appear to use search, not includ-
ing web search engines. describe the role of search for that service. also describe
whether the search is based on a database or grep style of matching, or if the search
is using some type of ranking.

2

architecture of a search engine

   while your first question may be the most per-
tinent, you may or may not realize it is also the
most irrelevant.   

the architect, matrix reloaded

2.1 what is an architecture?

in this chapter, we describe the basic software architecture of a search engine. al-
though there is no universal agreement on the definition, a software architecture
generally consists of software components, the interfaces provided by those com-
ponents, and the relationships between them. an architecture is used to describe
a system at a particular level of abstraction. an example of an architecture used to
provide a standard for integrating search and related language technology compo-
nents is uima (unstructured information management architecture).1 uima
defines interfaces for components in order to simplify the addition of new tech-
nologies into systems that handle text and other unstructured data.

our search engine architecture is used to present high-level descriptions of
the important components of the system and the relationships between them. it
is not a code-level description, although some of the components do correspond
to software modules in the galago search engine and other systems. we use this
architecture in this chapter and throughout the book to provide context to the
discussion of specific techniques.

an architecture is designed to ensure that a system will satisfy the application

requirements or goals. the two primary goals of a search engine are:
    e   ectiveness (quality): we want to be able to retrieve the most relevant set of

documents possible for a query.

    e   ciency (speed): we want to process queries from users as quickly as possi-

ble.

1 http://www.research.ibm.com/uima

14

2 architecture of a search engine

we may have more specific goals, too, but usually these fall into the categories
of e   ectiveness or e   ciency (or both). for instance, the collection of documents
we want to search may be changing; making sure that the search engine immedi-
ately reacts to changes in documents is both an e   ectiveness issue and an e   ciency
issue.

the architecture of a search engine is determined by these two requirements.
because we want an e   cient system, search engines employ specialized data struc-
tures that are optimized for fast retrieval. because we want high-quality results,
search engines carefully process text and store text statistics that help improve the
relevance of results.

many of the components we discuss in the following sections have been used
for decades, and this general design has been shown to be a useful compromise
between the competing goals of e   ective and e   cient retrieval. in later chapters,
we will discuss these components in more detail.

2.2 basic building blocks

search engine components support two major functions, which we call the index-
ing process and the query process. the indexing process builds the structures that
enable searching, and the query process uses those structures and a person   s query
to produce a ranked list of documents. figure 2.1 shows the high-level    building
blocks    of the indexing process. these major components are text acquisition, text
transformation, and index creation.

the task of the text acquisition component is to identify and make available
the documents that will be searched. although in some cases this will involve sim-
ply using an existing collection, text acquisition will more often require building
a collection by crawling or scanning the web, a corporate intranet, a desktop, or
other sources of information. in addition to passing documents to the next com-
ponent in the indexing process, the text acquisition component creates a docu-
ment data store, which contains the text and metadata for all the documents.
metadata is information about a document that is not part of the text content,
such the document type (e.g., email or web page), document structure, and other
features, such as document length.

the text transformation component transforms documents into index terms
or features. index terms, as the name implies, are the parts of a document that
are stored in the index and used in searching. the simplest index term is a word,
but not every word may be used for searching. a    feature    is more often used in

2.2 basic building blocks

15

fig. 2.1. the indexing process

the field of machine learning to refer to a part of a text document that is used to
represent its content, which also describes an index term. examples of other types
of index terms or features are phrases, names of people, dates, and links in a web
page. index terms are sometimes simply referred to as    terms.    the set of all the
terms that are indexed for a document collection is called the index vocabulary.
the index creation component takes the output of the text transformation
component and creates the indexes or data structures that enable fast searching.
given the large number of documents in many search applications, index creation
must be e   cient, both in terms of time and space. indexes must also be able to be
e   ciently updated when new documents are acquired. inverted indexes, or some-
times inverted files, are by far the most common form of index used by search
engines. an inverted index, very simply, contains a list for every index term of the
documents that contain that index term. it is inverted in the sense of being the
opposite of a document file that lists, for every document, the index terms they
contain. there are many variations of inverted indexes, and the particular form of
index used is one of the most important aspects of a search engine.

figure 2.2 shows the building blocks of the query process. the major compo-

nents are user interaction, ranking, and evaluation.

the user interaction component provides the interface between the person
doing the searching and the search engine. one task for this component is accept-
ing the user   s query and transforming it into index terms. another task is to take
the ranked list of documents from the search engine and organize it into the re-

email, web pages, news articles, memos, letterstext acquisitiontext transformationindex creationindexdocument data store16

2 architecture of a search engine

fig. 2.2. the query process

sults shown to the user. this includes, for example, generating the snippets used to
summarize documents. the document data store is one of the sources of informa-
tion used in generating the results. finally, this component also provides a range
of techniques for refining the query so that it better represents the information
need.

the ranking component is the core of the search engine. it takes the trans-
formed query from the user interaction component and generates a ranked list of
documents using scores based on a retrieval model. ranking must be both e   -
cient, since many queries may need to be processed in a short time, and e   ective,
since the quality of the ranking determines whether the search engine accom-
plishes the goal of finding relevant information. the e   ciency of ranking depends
on the indexes, and the e   ectiveness depends on the retrieval model.

the task of the evaluation component is to measure and monitor e   ectiveness
and e   ciency. an important part of that is to record and analyze user behavior
using log data. the results of evaluation are used to tune and improve the ranking
component. most of the evaluation component is not part of the online search
engine, apart from logging user and system data. evaluation is primarily an o   ine
activity, but it is a critical part of any search application.

user interactionevaluationrankingindexdocument data storelog data2.3 breaking it down

17

2.3 breaking it down

we now look in more detail at the components of each of the basic building
blocks. not all of these components will be part of every search engine, but to-
gether they cover what we consider to be the most important functions for a broad
range of search applications.

2.3.1 text acquisition

crawler

in many applications, the crawler component has the primary responsibility for
identifying and acquiring documents for the search engine. there are a number of
di   erent types of crawlers, but the most common is the general web crawler. a web
crawler is designed to follow the links on web pages to discover and download new
pages. although this sounds deceptively simple, there are significant challenges in
designing a web crawler that can e   ciently handle the huge volume of new pages
on the web, while at the same time ensuring that pages that may have changed
since the last time a crawler visited a site are kept    fresh    for the search engine. a
web crawler can be restricted to a single site, such as a university, as the basis for
site search. focused, or topical, web crawlers use classification techniques to restrict
the pages that are visited to those that are likely to be about a specific topic. this
type of crawler may be used by a vertical or topical search application, such as a
search engine that provides access to medical information on web pages.

for enterprise search, the crawler is adapted to discover and update all docu-
ments and web pages related to a company   s operation. an enterprise document
crawler follows links to discover both external and internal (i.e., restricted to the
corporate intranet) pages, but also must scan both corporate and personal di-
rectories to identify email, word processing documents, presentations, database
records, and other company information. document crawlers are also used for
desktop search, although in this case only the user   s personal directories need to
be scanned.

feeds

document feeds are a mechanism for accessing a real-time stream of documents.
for example, a news feed is a constant stream of news stories and updates. in con-
trast to a crawler, which must discover new documents, a search engine acquires

text acquisitioncrawlerfeedsconversiondocument data store18

2 architecture of a search engine

new documents from a feed simply by monitoring it. rss2 is a common standard
used for web feeds for content such as news, blogs, or video. an rss    reader   
is used to subscribe to rss feeds, which are formatted using xml.3 xml is a
language for describing data formats, similar to html.4 the reader monitors
those feeds and provides new content when it arrives. radio and television feeds
are also used in some search applications, where the    documents    contain auto-
matically segmented audio and video streams, together with associated text from
closed captions or id103.

conversion

the documents found by a crawler or provided by a feed are rarely in plain text.
instead, they come in a variety of formats, such as html, xml, adobe pdf,
microsoft word   , microsoft powerpoint  , and so on. most search engines require
that these documents be converted into a consistent text plus metadata format.
in this conversion, the control sequences and non-content data associated with
a particular format are either removed or recorded as metadata. in the case of
html and xml, much of this process can be described as part of the text trans-
formation component. for other formats, the conversion process is a basic step
that prepares the document for further processing. pdf documents, for example,
must be converted to text. various utilities are available that perform this conver-
sion, with varying degrees of accuracy. similarly, utilities are available to convert
the various microsoft o   ce   formats into text.

another common conversion problem comes from the way text is encoded in a
document. ascii5 is a common standard single-byte character encoding scheme
used for text. ascii uses either 7 or 8 bits (extended ascii) to represent either
128 or 256 possible characters. some languages, however, such as chinese, have
many more characters than english and use a number of other encoding schemes.
unicode is a standard encoding scheme that uses 16 bits (typically) to represent
most of the world   s languages. any application that deals with documents in dif-
ferent languages has to ensure that they are converted into a consistent encoding
scheme before further processing.

2 rss actually refers to a family of standards with similar names (and the same initials),

such as really simple syndication or rich site summary.

3 extensible markup language
4 hypertext markup language
5 american standard code for information interchange

2.3 breaking it down

19

document data store

the document data store is a database used to manage large numbers of docu-
ments and the structured data that is associated with them. the document con-
tents are typically stored in compressed form for e   ciency. the structured data
consists of document metadata and other information extracted from the docu-
ments, such as links and anchor text (the text associated with a link). a relational
database system can be used to store the documents and metadata. some applica-
tions, however, use a simpler, more e   cient storage system to provide very fast
retrieval times for very large document stores.

although the original documents are available on the web, in the enterprise
database, the document data store is necessary to provide fast access to the doc-
ument contents for a range of search engine components. generating summaries
of retrieved documents, for example, would take far too long if the search engine
had to access the original documents and reprocess them.

2.3.2 text transformation

parser

the parsing component is responsible for processing the sequence of text tokens
in the document to recognize structural elements such as titles, figures, links, and
headings. tokenizing the text is an important first step in this process. in many
cases, tokens are the same as words. both document and query text must be trans-
formed into tokens in the same manner so that they can be easily compared. there
are a number of decisions that potentially a   ect retrieval that make tokenizing
non-trivial. for example, a simple definition for tokens could be strings of al-
phanumeric characters that are separated by spaces. this does not tell us, however,
how to deal with special characters such as capital letters, hyphens, and apostro-
phes. should we treat    apple    the same as    apple   ? is    on-line    two words or one
word? should the apostrophe in    o   connor    be treated the same as the one in
   owner   s   ? in some languages, tokenizing gets even more interesting. chinese, for
example, has no obvious word separator like a space in english.

document structure is often specified by a markup language such as html
or xml. html is the default language used for specifying the structure of web
pages. xml has much more flexibility and is used as a data interchange format
for many applications. the document parser uses knowledge of the syntax of the
markup language to identify the structure.

text transformationparserstoppingid30link analysisinformation extractionclassifier20

2 architecture of a search engine

both html and xml use tags to define document elements. for example,
<h2> search </h2> defines    search    as a second-level heading in html. tags and
other control sequences must be treated appropriately when tokenizing. other
types of documents, such as email and presentations, have a specific syntax and
methods for specifying structure, but much of this may be be removed or simpli-
fied by the conversion component.

stopping

the stopping component has the simple task of removing common words from
the stream of tokens that become index terms. the most common words are typ-
ically function words that help form sentence structure but contribute little on
their own to the description of the topics covered by the text. examples are    the   ,
   of    ,    to   , and    for   . because they are so common, removing them can reduce the
size of the indexes considerably. depending on the retrieval model that is used as
the basis of the ranking, removing these words usually has no impact on the search
engine   s e   ectiveness, and may even improve it somewhat. despite these potential
advantages, it can be di   cult to decide how many words to include on the stop-
word list. some stopword lists used in research contain hundreds of words. the
problem with using such lists is that it becomes impossible to search with queries
like    to be or not to be    or    down under   . to avoid this, search applications may
use very small stopword lists (perhaps just containing    the   ) when processing doc-
ument text, but then use longer lists for the default processing of query text.

id30

id30 is another word-level transformation. the task of the id30 com-
ponent (or stemmer) is to group words that are derived from a common stem.
grouping    fish   ,    fishes   , and    fishing    is one example. by replacing each member
of a group with one designated word (for example, the shortest, which in this case
is    fish   ), we increase the likelihood that words used in queries and documents
will match. id30, in fact, generally produces small improvements in ranking
e   ectiveness. similar to stopping, id30 can be done aggressively, conserva-
tively, or not at all. aggressive id30 can cause search problems. it may not be
appropriate, for example, to retrieve documents about di   erent varieties of fish in
response to the query    fishing   . some search applications use more conservative
id30, such as simply identifying plural forms using the letter    s   , or they may

2.3 breaking it down

21

do no id30 when processing document text and focus on adding appropriate
word variants to the query.

some languages, such as arabic, have more complicated morphology than en-
glish, and id30 is consequently more important. an e   ective id30 com-
ponent in arabic has a huge impact on search e   ectiveness. in contrast, there is
little word variation in other languages, such as chinese, and for these languages
id30 is not e   ective.

link extraction and analysis

links and the corresponding anchor text in web pages can readily be identified
and extracted during document parsing. extraction means that this information
is recorded in the document data store, and can be indexed separately from the
general text content. web search engines make extensive use of this information
through link analysis algorithms such as id95 (brin & page, 1998). link
analysis provides the search engine with a rating of the popularity, and to some
extent, the authority of a page (in other words, how important it is). anchor text,
which is the clickable text of a web link, can be used to enhance the text content
of a page that the link points to. these two factors can significantly improve the
e   ectiveness of web search for some types of queries.

information extraction

information extraction is used to identify index terms that are more complex than
single words. this may be as simple as words in bold or words in headings, but in
general may require significant additional computation. extracting syntactic fea-
tures such as noun phrases, for example, requires some form of syntactic analysis
or part-of-speech tagging. research in this area has focused on techniques for ex-
tracting features with specific semantic content, such as named entity recognizers,
which can reliably identify information such as person names, company names,
dates, and locations.

classifier

the classifier component identifies class-related metadata for documents or parts
of documents. this covers a range of functions that are often described separately.
classification techniques assign predefined class labels to documents. these labels
typically represent topical categories such as    sports   ,    politics   , or    business   . two

22

2 architecture of a search engine

important examples of other types of classification are identifying documents as
spam, and identifying the non-content parts of documents, such as advertising.
id91 techniques are used to group related documents without predefined
categories. these document groups can be used in a variety of ways during ranking
or user interaction.

2.3.3 index creation

document statistics

the task of the document statistics component is simply to gather and record
statistical information about words, features, and documents. this information
is used by the ranking component to compute scores for documents. the types
of data generally required are the counts of index term occurrences (both words
and more complex features) in individual documents, the positions in the doc-
uments where the index terms occurred, the counts of occurrences over groups
of documents (such as all documents labeled    sports    or the entire collection of
documents), and the lengths of documents in terms of the number of tokens. the
actual data required is determined by the retrieval model and associated rank-
ing algorithm. the document statistics are stored in lookup tables, which are data
structures designed for fast retrieval.

weighting

index term weights reflect the relative importance of words in documents, and
are used in computing scores for ranking. the specific form of a weight is deter-
mined by the retrieval model. the weighting component calculates weights using
the document statistics and stores them in lookup tables. weights could be calcu-
lated as part of the query process, and some types of weights require information
about the query, but by doing as much calculation as possible during the indexing
process, the e   ciency of the query process will be improved.

one of the most common types used in older retrieval models is known as tf.idf
weighting. there are many variations of these weights, but they are all based on a
combination of the frequency or count of index term occurrences in a document
(the term frequency, or tf ) and the frequency of index term occurrence over the
entire collection of documents (inversedocumentfrequency, oridf ). theidf weight
is called inverse document frequency because it gives high weights to terms that
occur in very few documents. a typical formula for idf is log n /n, where n is the

index creationdocument statisticsweightinginversiondistribution2.3 breaking it down

23

total number of documents indexed by the search engine and n is the number of
documents that contain a particular term.

inversion

the inversion component is the core of the indexing process. its task is to change
the stream of document-term information coming from the text transformation
component into term-document information for the creation of inverted indexes.
the challenge is to do this e   ciently, not only for large numbers of documents
when the inverted indexes are initially created, but also when the indexes are up-
dated with new documents from feeds or crawls. the format of the inverted in-
dexes is designed for fast query processing and depends to some extent on the
ranking algorithm used. the indexes are also compressed to further enhance e   -
ciency.

index distribution

the index distribution component distributes indexes across multiple computers
and potentially across multiple sites on a network. distribution is essential for
e   cient performance with web search engines. by distributing the indexes for a
subset of the documents (document distribution), both indexing and query pro-
cessing can be done in parallel. distributing the indexes for a subset of terms (term
distribution) can also support parallel processing of queries. replication is a form
of distribution where copies of indexes or parts of indexes are stored in multiple
sites so that query processing can be made more e   cient by reducing communi-
cation delays. peer-to-peer search involves a less organized form of distribution
where each node in a network maintains its own indexes and collection of docu-
ments.

2.3.4 user interaction

query input

the query input component provides an interface and a parser for a query lan-
guage. the simplest query languages, such as those used in most web search in-
terfaces, have only a small number of operators. an operator is a command in the
query language that is used to indicate text that should be treated in a special way.
in general, operators help to clarify the meaning of the query by constraining how

user interactionquery inputquery transformationresults output24

2 architecture of a search engine

text in the document can match text in the query. an example of an operator in
a simple query language is the use of quotes to indicate that the enclosed words
should occur as a phrase in the document, rather than as individual words with no
relationship. a typical web query, however, consists of a small number of keywords
with no operators. a keyword is simply a word that is important for specifying the
topic of a query. because the ranking algorithms for most web search engines are
designed for keyword queries, longer queries that may contain a lower proportion
of keywords typically do not work well. for example, the query    search engines   
may produce a better result with a web search engine than the query    what are
typical implementation techniques and data structures used in search engines   .
one of the challenges for search engine design is to give good results for a range
of queries, and better results for more specific queries.

more complex query languages are available, either for people who want to
have a lot of control over the search results or for applications using a search en-
gine. in the same way that the sql query language (elmasri & navathe, 2006)
is not designed for the typical user of a database application (the end user), these
query languages are not designed for the end users of search applications. boolean
query languages have a long history in information retrieval. the operators in this
language include boolean and, or, and not, and some form of proximity opera-
tor that specifies that words must occur together within a specific distance (usually
in terms of word count). other query languages include these and other operators
in a probabilistic framework designed to allow specification of features related to
both document structure and content.

query transformation

the query transformation component includes a range of techniques that are de-
signed to improve the initial query, both before and after producing a document
ranking. the simplest processing involves some of the same text transformation
techniques used on document text. tokenizing, stopping, and id30 must be
done on the query text to produce index terms that are comparable to the docu-
ment terms.

spell checking and query suggestion are query transformation techniques that
produce similar output. in both cases, the user is presented with alternatives to
the initial query that are likely to either correct spelling errors or be more spe-
cific descriptions of their information needs. these techniques often leverage the
extensive query logs collected for web applications. id183 techniques

2.3 breaking it down

25

also suggest or add additional terms to the query, but usually based on an analy-
sis of term occurrences in documents. this analysis may use di   erent sources of
information, such as the whole document collection, the retrieved documents, or
documents on the user   s computer. relevance feedback is a technique that expands
queries based on term occurrences in documents that are identified as relevant by
the user.

results output

the results output component is responsible for constructing the display of ranked
documents coming from the ranking component. this may include tasks such as
generating snippets to summarize the retrieved documents, highlighting impor-
tant words and passages in documents, id91 the output to identify related
groups of documents, and finding appropriate advertising to add to the results
display. in applications that involve documents in multiple languages, the results
may be translated into a common language.

2.3.5 ranking

scoring

the scoring component, also called query processing, calculates scores for docu-
ments using the ranking algorithm, which is based on a retrieval model. the de-
signers of some search engines explicitly state the retrieval model they use. for
other search engines, only the ranking algorithm is discussed (if any details at all
are revealed), but all ranking algorithms are based implicitly on a retrieval model.
the features and weights used in a ranking algorithm, which may have been de-
rived empirically (by testing and evaluation), must be related to topical and user
relevance, or the search engine would not work.

many di   erent retrieval models and methods of deriving ranking algorithms
have been proposed. the basic form of the document score calculated by many of
these models is

   

qi.di

i

where the summation is over all of the terms in the vocabulary of the collection,
qi is the query term weight of the ith term, and di is the document term weight.
the term weights depend on the particular retrieval model being used, but are
generally similar to tf.idf weights. in chapter 7, we discuss the ranking algorithms

rankingscoringoptimizationdistribution26

2 architecture of a search engine

based on the bm25 and query likelihood retrieval models (as well as others) in
more detail.

the document scores must be calculated and compared very rapidly in order to
determine the ranked order of the documents that are given to the results output
component. this is the task of the performance optimization component.

performance optimization

performance optimization involves the design of ranking algorithms and the as-
sociated indexes to decrease response time and increase query throughput. given
a particular form of document scoring, there are a number of ways to calculate
those scores and produce the ranked document output. for example, scores can
be computed by accessing the index for a query term, computing the contribution
for that term to a document   s score, adding this contribution to a score accumula-
tor, and then accessing the next index. this is referred to as term-at-a-time scoring.
another alternative is to access all the indexes for the query terms simultaneously,
and compute scores by moving pointers through the indexes to find the terms
present in a document. in this document-at-a-time scoring, the final document
score is calculated immediately instead of being accumulated one term at a time.
in both cases, further optimizations are possible that significantly decrease the
time required to compute the top-ranked documents. safe optimizations guaran-
tee that the scores calculated will be the same as the scores without optimization.
unsafe optimizations, which do not have this property, can in some cases be faster,
so it is important to carefully evaluate the impact of the optimization.

distribution

given some form of index distribution, ranking can also be distributed. a query
broker decides how to allocate queries to processors in a network and is responsi-
ble for assembling the final ranked list for the query. the operation of the broker
depends on the form of index distribution. caching is another form of distribu-
tion where indexes or even ranked document lists from previous queries are left in
local memory. if the query or index term is popular, there is a significant chance
that this information can be reused with substantial time savings.

2.3 breaking it down

27

2.3.6 evaluation

logging

logs of the users    queries and their interactions with the search engine are one
of the most valuable sources of information for tuning and improving search ef-
fectiveness and e   ciency. query logs can be used for spell checking, query sug-
gestions, query caching, and other tasks, such as helping to match advertising to
searches. documents in a result list that are clicked on and browsed tend to be
relevant. this means that logs of user clicks on documents (clickthrough data)
and information such as the dwell time (time spent looking at a document) can
be used to evaluate and train ranking algorithms.

ranking analysis

given either log data or explicit relevance judgments for a large number of (query,
document) pairs, the e   ectiveness of a ranking algorithm can be measured and
compared to alternatives. this is a critical part of improving a search engine and
selecting values for parameters that are appropriate for the application. a variety
of evaluation measures are commonly used, and these should also be selected to
measure outcomes that make sense for the application. measures that emphasize
the quality of the top-ranked documents, rather than the whole list, for example,
are appropriate for many types of web queries.

performance analysis

the performance analysis component involves monitoring and improving overall
system performance, in the same way that the ranking analysis component mon-
itors e   ectiveness. a variety of performance measures are used, such as response
time and throughput, but the measures used also depend on the application. for
example, a distributed search application should monitor network usage and ef-
ficiency in addition to other measures. for ranking analysis, test collections are
often used to provide a controlled experimental environment. the equivalent for
performance analysis is simulations, where actual networks, processors, storage
devices, and data are replaced with mathematical models that can be adjusted us-
ing parameters.

evaluationloggingranking analysisperformance analysis28

2 architecture of a search engine

2.4 how does it really work?

now you know the names and the basic functions of the components of a search
engine, but we haven   t said much yet about how these components actually per-
form these functions. that   s what the rest of the book is about. each chapter de-
scribes, in depth, how one or more components work. if you still don   t understand
a component after finishing the appropriate chapter, you can study the galago
code, which is one implementation of the ideas presented, or the references de-
scribed at the end of each chapter.

references and further reading

detailed references on the techniques and models mentioned in the compo-
nent descriptions will be given in the appropriate chapters. there are a few gen-
eral references for search architectures. a database textbook, such as elmasri and
navathe (2006), provides descriptions of database system architecture and the
associated query languages that are interesting to compare with the search en-
gine architecture discussed here. there are some similarities at the high level, but
database systems focus on structured data and exact match rather than on text and
ranking, so most of the components are very di   erent.

the classic research paper on web search engine architecture, which gives an
overview of an early version of google, is brin and page (1998). another system
overview for an earlier general-purpose search engine (inquery) is found in callan
et al. (1992). a comprehensive description of the lucene architecture and com-
ponents can be found in hatcher and gospodnetic (2004).

exercises

2.1. find some examples of the search engine components described in this chap-
ter in the galago code.
2.2. a more-like-this query occurs when the user can click on a particular docu-
ment in the result list and tell the search engine to find documents that are similar
to this one. describe which low-level components are used to answer this type of
query and the sequence in which they are used.

2.4 how does it really work?

29

2.3. document filtering is an application that stores a large number of queries or
user profiles and compares these profiles to every incoming document on a feed.
documents that are su   ciently similar to the profile are forwarded to that person
via email or some other mechanism. describe the architecture of a filtering engine
and how it may di   er from a search engine.

3

crawls and feeds

   you   ve stuck your webs into my business for the
last time.   

doc ock, spider man 2

3.1 deciding what to search

this book is about the details of building a search engine, from the mathematics
behind ranking to the algorithms of query processing. although we focus heav-
ily on the technology that makes search engines work, and great technology can
make a good search engine even better, it is the information in the document col-
lection that makes search engines useful. in other words, if the right documents
are not stored in the search engine, no search technique will be able to find rele-
vant information.

the title of this section implies the question,    what should we search?    the
simple answer is everything you possibly can. every document answers at least one
question (i.e.,    now where was that document again?   ), although the best doc-
uments answer many more. every time a search engine adds another document,
the number of questions it can answer increases. on the other hand, adding many
poor-quality documents increases the burden on the ranking process to find only
the best documents to show to the user. web search engines, however, show how
successful search engines can be, even when they contain billions of low-quality
documents with little useful content.

even useful documents can become less useful over time. this is especially true
of news and financial information where, for example, many people want to know
about today   s stock market report, but only a few care about what happened yes-
terday. the frustration of finding out-of-date web pages and links in a search re-
sult list is, unfortunately, a common experience. search engines are most e   ective
when they contain the most recent information in addition to archives of older
material.

32

3 crawls and feeds

this chapter introduces techniques for finding documents to search, whether
on the web, on a file server, on a computer   s hard disk, or in an email program.
we will discuss strategies for storing documents and keeping those documents
up-to-date. along the way, we will discuss how to pull data out of files, navigating
through issues of character encodings, obsolete file formats, duplicate documents,
and textual noise. by the end of this chapter you will have a solid grasp on how to
get document data into a search engine, ready to be indexed.

3.2 id190

to build a search engine that searches web pages, you first need a copy of the pages
that you want to search. unlike some of the other sources of text we will consider
later, web pages are particularly easy to copy, since they are meant to be retrieved
over the internet by browsers. this instantly solves one of the major problems of
getting information to search, which is how to get the data from the place it is
stored to the search engine.

finding and downloading web pages automatically is called crawling, and a
program that downloads pages is called a web crawler.1 there are some unique
challenges to crawling web pages. the biggest problem is the sheer scale of the
web. there are at least tens of billions of pages on the internet. the    at least    in
the last sentence is there because nobody is sure how many pages there are. even
if the number of pages in existence today could be measured exactly, that number
would be immediately wrong, because pages are constantly being created. every
time a user adds a new blog post or uploads a photo, another web page is created.
most organizations do not have enough storage space to store even a large fraction
of the web, but web search providers with plenty of resources must still constantly
download new content to keep their collections current.

another problem is that web pages are usually not under the control of the
people building the search engine database. even if you know that you want to
copy all the pages from www.company.com, there is no easy way to find out how
many pages there are on the site. the owners of that site may not want you to copy
some of the data, and will probably be angry if you try to copy it too quickly or
too frequently. some of the data you want to copy may be available only by typing
a request into a form, which is a di   cult process to automate.

1 crawling is also occasionally referred to as spidering, and a crawler is sometimes called
a spider.

3.2 id190

33

3.2.1 retrieving web pages

each web page on the internet has its own unique uniform resource locator, or
url. any url used to describe a web page has three parts: the scheme, the host-
name, and the resource name (figure 3.1). web pages are stored on web servers,
which use a protocol called hypertext transfer protocol, or http, to exchange
information with client software. therefore, most urls used on the web start
with the scheme http, indicating that the url represents a resource that can
be retrieved using http. the hostname follows, which is the name of the com-
puter that is running the web server that holds this web page. in the figure, the
computer   s name is www.cs.umass.edu, which is a computer in the university of
massachusetts computer science department. this url refers to a page on that
computer called /csinfo/people.html.

fig. 3.1. a uniform resource locator (url), split into three parts

web browsers and web crawlers are two di   erent kinds of web clients, but both
fetch web pages in the same way. first, the client program connects to a domain
name system (dns) server. the dns server translates the hostname into an inter-
net protocol (ip) address. this ip address is a number that is typically 32 bits long,
but some networks now use 128-bit ip addresses. the program then attempts to
connect to a server computer with that ip address. since that server might have
many di   erent programs running on it, with each one listening to the network for
new connections, each program listens on a di   erent port. a port is just a 16-bit
number that identifies a particular service. by convention, requests for web pages
are sent to port 80 unless specified otherwise in the url.

once the connection is established, the client program sends an http re-
quest to the web server to request a page. the most common http request type
is a get request, for example:

get /csinfo/people.html http/1.0

this simple request asks the server to send the page called /csinfo/people.html
back to the client, using version 1.0 of the http protocol specification. after

http://www.cs.umass.edu/csinfo/people.htmlhttpwww.cs.umass.edu/csinfo/people.htmlschemehostnameresource34

3 crawls and feeds

sending a short header, the server sends the contents of that file back to the client.
if the client wants more pages, it can send additional requests; otherwise, the client
closes the connection.

a client can also fetch web pages using post requests. a post request is
like a get request, except that it can send additional request information to the
server. by convention, get requests are used for retrieving data that already exists
on the server, whereas post requests are used to tell the server something. a
post request might be used when you click a button to purchase something or
to edit a web page. this convention is useful if you are running a web crawler, since
sending only get requests helps make sure your crawler does not inadvertently
order a product.

fig. 3.2. id190. the web crawler connects to web servers to find pages. pages
may link to other pages on the same server or on di   erent servers.

www.cs.umass.eduwww.bbc.co.ukwww.id98.comwww.whitehouse.gov/index.html/courses/news/index.html/today/2005/story.html/index.html/2006/09/story.html/2003/04/story.html/index.html/news.html/about.htmlcrawler.searchengine.com3.2 id190

35

3.2.2 the web crawler
figure 3.2 shows a diagram of the web from a simple web crawler   s perspective.
the web crawler has two jobs: downloading pages and finding urls.

the crawler starts with a set of seeds, which are a set of urls given to it as
parameters. these seeds are added to a url request queue. the crawler starts
fetching pages from the request queue. once a page is downloaded, it is parsed
to find link tags that might contain other useful urls to fetch. if the crawler
finds a new url that it has not seen before, it is added to the crawler   s request
queue, or frontier. the frontier may be a standard queue, or it may be ordered so
that important pages move to the front of the list. this process continues until
the crawler either runs out of disk space to store pages or runs out of useful links
to add to the request queue.

if a crawler used only a single thread, it would not be very e   cient. notice
that the web crawler spends a lot of its time waiting for responses: it waits for
the dns server response, then it waits for the connection to the web server to
be acknowledged, and then it waits for the web page data to be sent from the
server. during this waiting time, the cpu of the web crawler machine is idle and
the network connection is unused. to reduce this ine   ciency, web crawlers use
threads and fetch hundreds of pages at once.

fetching hundreds of pages at once is good for the person running the web
crawler, but not necessarily good for the person running the web server on the
other end. just imagine how the request queue works in practice. when a web
page like www.company.com is fetched, it is parsed and all of the links on that
page are added to the request queue. the crawler will then attempt to fetch all of
those pages at once. if the web server for www.company.com is not very powerful, it
might spend all of its time handling requests from the crawler instead of handling
requests from real users. this kind of behavior from web crawlers tends to make
web server administrators very angry.

to avoid this problem, web crawlers use politeness policies. reasonable web
crawlers do not fetch more than one page at a time from a particular web server.
in addition, web crawlers wait at least a few seconds, and sometimes minutes, be-
tween requests to the same web server. this allows web servers to spend the bulk
of their time processing real user requests. to support this, the request queue is
logically split into a single queue per web server. at any one time, most of these
per-server queues are o   -limits for crawling, because the crawler has fetched a
page from that server recently. the crawler is free to read page requests only from
queues that haven   t been accessed within the specified politeness window.

36

3 crawls and feeds

when using a politeness window, the request queue must be very large in order
to achieve good performance. suppose a web crawler can fetch 100 pages each
second, and that its politeness policy dictates that it cannot fetch more than one
page each 30 seconds from a particular web server. the web crawler needs to have
urls from at least 3,000 di   erent web servers in its request queue in order to
achieve high throughput. since many urls will come from the same servers, the
request queue needs to have tens of thousands of urls in it before a crawler can
reach its peak throughput.

user-agent: *
disallow: /private/
disallow: /confidential/
disallow: /other/
allow: /other/public/

user-agent: favoredcrawler
disallow:

sitemap: http://mysite.com/sitemap.xml.gz

fig. 3.3. an example robots.txt file

even crawling a site slowly will anger some web server administrators who ob-
ject to any copying of their data. web server administrators who feel this way can
store a file called /robots.txt on their web servers. figure 3.3 contains an ex-
ample robots.txt file. the file is split into blocks of commands that start with a
user-agent: specification. the user-agent: line identifies a crawler, or group
of crawlers, a   ected by the following rules. following this line are allow and
disallow rules that dictate which resources the crawler is allowed to access. in
the figure, the first block indicates that all crawlers need to ignore resources that
begin with /private/, /confidential/, or /other/, except for those that begin
with /other/public/. the second block indicates that a crawler named favored-
crawler gets its own set of rules: it is allowed to copy everything.

the final block of the example is an optional sitemap: directive, which will be

discussed later in this section.

figure 3.4 shows an implementation of a crawling thread, using the crawler
building blocks we have seen so far. assume that the frontier has been initialized

3.2 id190

37

procedure c                  t               (frontier)

while not frontier.done() do
website     frontier.nextsite()
url     website.nexturl()
if website.permitscrawl(url) then

text     retrieveurl(url)
storedocument(url, text)
for each url in parse(text) do

frontier.addurl(url)

end for

end if
frontier.releasesite(website)

end while
end procedure

fig. 3.4. a simple crawling thread implementation

with a few urls that act as seeds for the crawl. the crawling thread first retrieves
a website from the frontier. the crawler then identifies the next url in the web-
site   s queue. in permitscrawl, the crawler checks to see if the url is okay to crawl
according to the website   s robots.txt file. if it can be crawled, the crawler uses re-
trieveurl to fetch the document contents. this is the most expensive part of the
loop, and the crawler thread may block here for many seconds. once the text has
been retrieved, storedocument stores the document text in a document database
(discussed later in this chapter). the document text is then parsed so that other
urls can be found. these urls are added to the frontier, which adds them to
the appropriate website queues. when all this is finished, the website object is
returned to the frontier, which takes care to enforce its politeness policy by not
giving the website to another crawler thread until an appropriate amount of time
has passed. in a real crawler, the timer would start immediately after the document
was retrieved, since parsing and storing the document could take a long time.

3.2.3 freshness

web pages are constantly being added, deleted, and modified. to keep an accu-
rate view of the web, a web crawler must continually revisit pages it has already
crawled to see if they have changed in order to maintain the freshness of the docu-
ment collection. the opposite of a fresh copy is a stale copy, which means a copy
that no longer reflects the real content of the web page.

38

3 crawls and feeds

client request:

head /csinfo/people.html http/1.1
host: www.cs.umass.edu

server response:

http/1.1 200 ok
date: thu, 03 apr 2008 05:17:54 gmt
server: apache/2.0.52 (centos)
last-modified: fri, 04 jan 2008 15:28:39 gmt
etag: "239c33-2576-2a2837c0"
accept-ranges: bytes
content-length: 9590
connection: close
content-type: text/html; charset=iso-8859-1

fig. 3.5. an http head request and server response

the http protocol has a special request type called head that makes it easy
to check for page changes. the head request returns only header information
about the page, but not the page itself. figure 3.5 contains an example head
request and response. the last-modified value indicates the last time the page
content was changed. notice that the date is also sent along with the response, as
well as in response to a get request. this allows the web crawler to compare the
date it received from a previous get request with the last-modified value from
a head request.

a head request reduces the cost of checking on a page, but does not elimi-
nate it. it simply is not possible to check every page every minute. not only would
that attract more negative reactions from web server administrators, but it would
cause enormous load on the web crawler and the incoming network connection.
thankfully, most web pages are not updated every few minutes. some of
them, like news websites, do change frequently. others, like a person   s home page,
change much less often. even within a page type there can be huge variations in the
modification rate. for example, some blogs are updated many times a day, whereas
others go months between updates. it does little good to continuously check sites
that are rarely updated. therefore, one of the crawler   s jobs is to measure the rate
at which each page changes. over time, this data can be used to estimate how
frequently each page changes.

given that a web crawler can   t update every page immediately as it changes, the
crawler needs to have some metric for measuring crawl freshness. in this chapter,
we   ve used freshness as a general term, but freshness is also the name of a metric.

3.2 id190

39

fig. 3.6. age and freshness of a single page over time

under the freshness metric, a page is fresh if the crawl has the most recent copy of a
web page, but stale otherwise. freshness is then the fraction of the crawled pages
that are currently fresh.

keeping freshness high seems like exactly what you   d want to do, but optimiz-
ing for freshness can have unintended consequences. suppose that http://www.ex-
ample.com is a popular website that changes its front page slightly every minute.
unless your crawler continually polls http://www.example.com, you will almost al-
ways have a stale copy of that page. notice that if you want to optimize for fresh-
ness, the appropriate strategy is to stop crawling this site completely! if it will
never be fresh, it can   t help your freshness value. instead, you should allocate your
crawler   s resources to pages that change less frequently.

of course, users will revolt if you decide to optimize your crawler for freshness.
they will look at http://www.example.com and wonder why your indexed copy is
months out of date.

age is a better metric to use. you can see the di   erence between age and fresh-
ness in figure 3.6. in the top part of the figure, you can see that pages become
fresh immediately when they are crawled, but once the page changes, the crawled
page becomes stale. under the age metric, the page has age 0 until it is changed,
and then its age grows until the page is crawled again.

suppose we have a page with change frequency   , meaning that we expect it
to change    times in a one-day period. we can calculate the expected age of a page
t days after it was last crawled:

   

age(  , t) =

0

t

p (page changed at time x)(t     x)dx

freshnessagecrawlcrawlcrawlupdatesupdate40

3 crawls and feeds

fig. 3.7. expected age of a page with mean change frequency    = 1/7 (one week)
the (t     x) expression is an age: we assume the page is crawled at time t, but
that it changed at time x. we multiply that by the id203 that the page actu-
ally changed at time x. studies have shown that, on average, web page updates fol-
low the poisson distribution, meaning that the time until the next update is gov-
erned by an exponential distribution (cho & garcia-molina, 2003). this gives us
a formula to plug into the p (page changed at time x) expression:

   

age(  , t) =

0

t

   (cid:21)x(t     x)dx

  e

figure 3.7 shows the result of plotting this expression for a fixed    = 1/7,
indicating roughly one change a week. notice how the expected age starts at zero,
and rises slowly at first. this is because the page is unlikely to have changed in the
first day. as the days go by, the id203 that the page has changed increases. by
the end of the week, the expected age of the page is about 2.6 days. this means
that if your crawler crawls each page once a week, and each page in your collection
has a mean update time of once a week, the pages in your index will be 2.6 days
old on average just before the crawler runs again.

notice that the second derivative of the age function is always positive. that
is, the graph is not only increasing, but its rate of increase is always increasing. this
positive second derivative means that the older a page gets, the more it costs you
to not crawl it. optimizing this metric will never result in the conclusion that
optimizing for freshness does, where sometimes it is economical to not crawl a
page at all.

3.2.4 focused crawling

3.2 id190

41

some users would like a search engine that focuses on a specific topic of informa-
tion. for instance, at a website about movies, users might want access to a search
engine that leads to more information about movies. if built correctly, this type of
vertical search can provide higher accuracy than general search because of the lack
of extraneous information in the document collection. the computational cost
of running a vertical search will also be much less than a full web search, simply
because the collection will be much smaller.

the most accurate way to get web pages for this kind of engine would be to
crawl a full copy of the web and then throw out all unrelated pages. this strategy
requires a huge amount of disk space and bandwidth, and most of the web pages
will be discarded at the end.

a less expensive approach is focused, or topical, crawling. a focused crawler
attempts to download only those pages that are about a particular topic. focused
crawlers rely on the fact that pages about a topic tend to have links to other pages
on the same topic. if this were perfectly true, it would be possible to start a crawl at
one on-topic page, then crawl all pages on that topic just by following links from
a single root page. in practice, a number of popular pages for a specific topic are
typically used as seeds.

focused crawlers require some automatic means for determining whether a
page is about a particular topic. chapter 9 will introduce text classifiers, which
are tools that can make this kind of distinction. once a page is downloaded, the
crawler uses the classifier to decide whether the page is on topic. if it is, the page is
kept, and links from the page are used to find other related sites. the anchor text in
the outgoing links is an important clue of topicality. also, some pages have more
on-topic links than others. as links from a particular web page are visited, the
crawler can keep track of the topicality of the downloaded pages and use this to
determine whether to download other similar pages. anchor text data and page
link topicality data can be combined together in order to determine which pages
should be crawled next.

3.2.5 deep web

not all parts of the web are easy for a crawler to navigate. sites that are di   cult
for a crawler to find are collectively referred to as the deep web (also called the
hidden web). some studies have estimated that the deep web is over a hundred

42

3 crawls and feeds

times larger than the traditionally indexed web, although it is very di   cult to
measure this accurately.

most sites that are a part of the deep web fall into three broad categories:

    private sites are intentionally private. they may have no incoming links, or may
require you to log in with a valid account before using the rest of the site. these
sites generally want to block access from crawlers, although some news pub-
lishers may still want their content indexed by major search engines.

    form results are sites that can be reached only after entering some data into a
form. for example, websites selling airline tickets typically ask for trip infor-
mation on the site   s entry page. you are shown flight information only after
submitting this trip information. even though you might want to use a search
engine to find flight timetables, most crawlers will not be able to get through
this form to get to the timetable information.

    scripted pages are pages that use javascript   , flash  , or another client-side lan-
guage in the web page. if a link is not in the raw html source of the web
page, but is instead generated by javascript code running on the browser, the
crawler will need to execute the javascript on the page in order to find the link.
although this is technically possible, executing javascript can slow down the
crawler significantly and adds complexity to the system.
sometimes people make a distinction between static pages and dynamic pages.
static pages are files stored on a web server and displayed in a web browser un-
modified, whereas dynamic pages may be the result of code executing on the web
server or the client. typically it is assumed that static pages are easy to crawl, while
dynamic pages are hard. this is not quite true, however. many websites have dy-
namically generated web pages that are easy to crawl; wikis are a good example
of this. other websites have static pages that are impossible to crawl because they
can be accessed only through web forms.

web administrators of sites with form results and scripted pages often want
their sites to be indexed, unlike the owners of private sites. of these two categories,
scripted pages are easiest to deal with. the site owner can usually modify the pages
slightly so that links are generated by code on the server instead of by code in
the browser. the crawler can also run page javascript, or perhaps flash as well,
although these can take a lot of time.

the most di   cult problems come with form results. usually these sites are
repositories of changing data, and the form submits a query to a database system.
in the case where the database contains millions of records, the site would need to

3.2 id190

43

expose millions of links to a search engine   s crawler. adding a million links to the
front page of such a site is clearly infeasible. another option is to let the crawler
guess what to enter into forms, but it is di   cult to choose good form input. even
with good guesses, this approach is unlikely to expose all of the hidden data.

3.2.6 sitemaps

as you can see from the last two sections, the biggest problems in crawling arise
because site owners cannot adequately tell crawlers about their sites. in section
3.2.3, we saw how crawlers have to make guesses about when pages will be updated
because polling is costly. in section 3.2.5, we saw that site owners sometimes have
data that they would like to expose to a search engine, but they can   t because there
is no reasonable place to store the links. sitemaps solve both of these problems.

<?xml version="1.0" encoding="utf-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">

<url>

<loc>http://www.company.com/</loc>
<lastmod>2008-01-15</lastmod>
<changefreq>monthly</changefreq>
<priority>0.7</priority>

</url>
<url>

<loc>http://www.company.com/items?item=truck</loc>
<changefreq>weekly</changefreq>

</url>
<url>

<loc>http://www.company.com/items?item=bicycle</loc>
<changefreq>daily</changefreq>

</url>
</urlset>

fig. 3.8. an example sitemap file

a robots.txt file can contain a reference to a sitemap, like the one shown in
figure 3.8. a sitemap contains a list of urls and data about those urls, such as
modification time and modification frequency.

44

3 crawls and feeds

there are three url entries shown in the example sitemap. each one contains
a url in a loc tag. the changefreq tag indicates how often this resource is likely
to change. the first entry includes a lastmod tag, which indicates the last time
it was changed. the first entry also includes a priority tag with a value of 0.7,
which is higher than the default of 0.5. this tells crawlers that this page is more
important than other pages on this site.

why would a web server administrator go to the trouble to create a sitemap?
one reason is that it tells search engines about pages it might not otherwise find.
look at the second and third urls in the sitemap. suppose these are two prod-
uct pages. there may not be any links on the website to these pages; instead, the
user may have to use a search form to get to them. a simple web crawler will not
attempt to enter anything into a form (although some advanced crawlers do), and
so these pages would be invisible to search engines. a sitemap allows crawlers to
find this hidden content.

the sitemap also exposes modification times. in the discussion of page fresh-
ness, we mentioned that a crawler usually has to guess when pages are likely to
change. the changefreq tag gives the crawler a hint about when to check a page
again for changes, and the lastmod tag tells the crawler when a page has changed.
this helps reduce the number of requests that the crawler sends to a website with-
out sacrificing page freshness.

3.2.7 distributed crawling

for crawling individual websites, a single computer is su   cient. however, crawl-
ing the entire web requires many computers devoted to crawling. why would a
single crawling computer not be enough? we will consider three reasons.

one reason to use multiple computers is to put the crawler closer to the sites it
crawls. long-distance network connections tend to have lower throughput (fewer
bytes copied per second) and higher latency (bytes take longer to cross the net-
work). decreased throughput and increased latency work together to make each
page request take longer. as throughput drops and latency rises, the crawler has
to open more connections to copy pages at the same rate.

for example, suppose a crawler has a network connection that can transfer
1mb each second. with an average web page size of 20k, it can copy 50 pages
each second. if the sites that are being crawled are close, the data transfer rate
from them may be 1mb a second. however, it can take 80ms for the site to start
sending data, because there is some transmission delay in opening the connection

3.2 id190

45

and sending the request. let   s assume each request takes 100ms (80ms of latency
and 20ms of data transfer). multiplying 50 by 100ms, we see that there is 5 seconds
of waiting involved in transferring 50 pages. this means that five connections will
be needed to transfer 50 pages in one second. if the sites are farther away, with an
average throughput of 100k per second and 500ms of latency, then each request
would now take 600ms. since 50    600ms = 30 seconds, the crawler would need
to keep 30 connections open to transfer pages at the same rate.

another reason for multiple crawling computers is to reduce the number of
sites the crawler has to remember. a crawler has to remember all of the urls it
has already crawled, and all of the urls that it has queued to crawl. these urls
must be easy to access, because every page that is crawled contains new links that
need to be added to the crawl queue. since the crawler   s queue should not contain
duplicates or sites that have already been crawled, each new url must be checked
against everything in the queue and everything that has been crawled. the data
structure for this lookup needs to be in ram; otherwise, the computer   s crawl
speed will be severely limited. spreading crawling duties among many computers
reduces this bookkeeping load.

yet another reason is that crawling can use a lot of computing resources, includ-
ing cpu resources for parsing and network bandwidth for crawling pages. crawl-
ing a large portion of the web is too much work for a single computer to handle.
a distributed crawler is much like a crawler on a single computer, except in-
stead of a single queue of urls, there are many queues. the distributed crawler
uses a hash function to assign urls to crawling computers. when a crawler sees
a new url, it computes a hash function on that url to decide which crawl-
ing computer is responsible for it. these urls are gathered in batches, then sent
periodically to reduce the network overhead of sending a single url at a time.
the hash function should be computed on just the host part of each url.
this assigns all the urls for a particular host to a single crawler. although this
may promote imbalance since some hosts have more pages than others, politeness
rules require a time delay between url fetches to the same host. it is easier to
maintain that kind of delay by using the same crawling computers for all urls for
the same host. in addition, we would expect that sites from domain.com will have
lots of links to other pages on domain.com. by assigning domain.com to a single
crawl host, we minimize the number of urls that need to be exchanged between
crawling computers.

46

3 crawls and feeds

3.3 crawling documents and email

even though the web is a tremendous information resource, a huge amount of
digital information is not stored on websites. in this section, we will consider in-
formation that you might find on a normal desktop computer, such as email, word
processing documents, presentations, or spreadsheets. this information can be
searched using a desktop search tool. in companies and organizations, enterprise
search will make use of documents on file servers, or even on employee desktop
computers, in addition to local web pages.

many of the problems of web crawling change when we look at desktop data.
in web crawling, just finding the data can be a struggle. on a desktop computer,
the interesting data is stored in a file system with familiar semantics. finding all
the files on a hard disk is not particularly di   cult, since file systems have directo-
ries that are easy to discover. in some ways, a file system is like a web server, but
with an automatically generated sitemap.

there are unique challenges in crawling desktop data, however. the first con-
cerns update speed. in desktop search applications, users demand search results
based on the current content of their files. this means, for example, being able to
search for an email the instant it is received, and being able to search for a docu-
ment as soon as it has been saved. notice that this is a much di   erent expectation
than with web search, where users can tolerate crawling delays of hours or days.
crawling the file system every second is impractical, but modern file systems can
send change notifications directly to the crawler process so that it can copy new
files immediately. remote file systems from file servers usually do not provide this
kind of change notification, and so they must be crawled just like a web server.

disk space is another concern. with a web crawler, we assume that we need to
keep a copy of every document that is found. this is less true on a desktop system,
where the documents are already stored locally, and where users will be unhappy
if a large proportion of the hard disk is taken by the indexer. a desktop crawler
instead may need to read documents into memory and send them directly to the
indexer. we will discuss indexing more in chapter 5.

since websites are meant to be viewed with web browsers, most web content
is stored in html. on the other hand, each desktop program   the word pro-
cessor, presentation tool, email program, etc.   has its own file format. so, just
finding these files is not enough; eventually they will need to be converted into a
format that the indexer can understand. in section 3.5 we will revisit this conver-
sion issue.

3.4 document feeds

47

finally, and perhaps most importantly, crawling desktop data requires a focus
on data privacy. desktop systems can have multiple users with di   erent accounts,
and user a should not be able to find emails from user b   s account through the
search feature. this is especially important when we consider crawling shared net-
work file systems, as in a corporate network. the file access permissions of each file
must be recorded along with the crawled data, and must be kept up-to-date.

3.4 document feeds

in general web or desktop crawling, we assume that any document can be created
or modified at any time. however, many documents are published, meaning that
they are created at a fixed time and rarely updated again. news articles, blog posts,
press releases, and email are some of the documents that fit this publishing model.
most information that is time-sensitive is published.

since each published document has an associated time, published documents
from a single source can be ordered in a sequence called a document feed. a docu-
ment feed is particularly interesting for crawlers, since the crawler can easily find
all the new documents by examining only the end of the feed.

we can distinguish two kinds of document feeds, push and pull. a push feed
alerts the subscriber to new documents. this is like a telephone, which alerts you
to an incoming phone call; you don   t need to continually check the phone to see
if someone is calling. a pull feed requires the subscriber to check periodically for
new documents; this is like checking your mailbox for new mail to arrive. news
feeds from commercial news agencies are often push feeds, but pull feeds are over-
whelmingly popular for free services. we will focus primarily on pull feeds in this
section.

the most common format for pull feeds is called rss. rss has at least three
definitions: really simple syndication, rdf site summary, or rich site sum-
mary. not surprisingly, rss also has a number of slightly incompatible imple-
mentations, and a similar competing format exists called the atom syndication
format. the proliferation of standards is the result of an idea that gained popu-
larity too quickly for developers to agree on a single standard.

figure 3.9 shows an rss 2.0 feed from an example site called http://www.search-
engine-news.org. this feed contains two articles: one is about an upcoming sigir
conference, and the other is about a textbook. notice that each entry contains a
time indicating when it was published. in addition, near the top of the rss feed
there is an tag named ttl, which means time to live, measured in minutes. this

48

3 crawls and feeds

<?xml version="1.0"?>
<rss version="2.0">

<channel>

<title>search engine news</title>
<link>http://www.search-engine-news.org/</link>
<description>news about search engines.</description>
<language>en-us</language>
<pubdate>tue, 19 jun 2008 05:17:00 gmt</pubdate>
<ttl>60</ttl>

<item>

<title>upcoming sigir conference</title>
<link>http://www.sigir.org/conference</link>
<description>the annual sigir conference is coming!

mark your calendars and check for cheap
flights.</description>

<pubdate>tue, 05 jun 2008 09:50:11 gmt</pubdate>
<guid>http://search-engine-news.org#500</guid>

</item>

<item>

<title>new search engine textbook</title>
<link>http://www.cs.umass.edu/search-book</link>
<description>a new textbook about search engines

will be published soon.</description>

<pubdate>tue, 05 jun 2008 09:33:01 gmt</pubdate>
<guid>http://search-engine-news.org#499</guid>

</item>
</channel>

</rss>

fig. 3.9. an example rss 2.0 feed

3.5 the conversion problem

49

feed states that its contents should be cached only for 60 minutes, and informa-
tion more than an hour old should be considered stale. this gives a crawler an
indication of how often this feed file should be crawled.

rss feeds are accessed just like a traditional web page, using http get re-
quests to web servers that host them. therefore, some of the crawling techniques
we discussed before apply here as well, such as using http head requests to
detect when rss feeds change.

from a crawling perspective, document feeds have a number of advantages
over traditional pages. feeds give a natural structure to data; even more than with
a sitemap, a web feed implies some relationship between the data items. feeds
are easy to parse and contain detailed time information, like a sitemap, but also
include a description field about each page (and this description field sometimes
contains the entire text of the page referenced in the url). most importantly,
like a sitemap, feeds provide a single location to look for new data, instead of hav-
ing to crawl an entire site to find a few new documents.

3.5 the conversion problem

search engines are built to search through text. unfortunately, text is stored on
computers in hundreds of incompatible file formats. standard text file formats
include raw text, rtf, html, xml, microsoft word, odf (open document
format) and pdf (portable document format). there are tens of other less com-
mon word processors with their own file formats. but text documents aren   t the
only kind of document that needs to be searched; other kinds of files also contain
important text, such as powerpoint slides and excel  spreadsheets. in addition to
all of these formats, people often want to search old documents, which means that
search engines may need to support obsolete file formats. it is not uncommon for
a commercial search engine to support more than a hundred file types.

the most common way to handle a new file format is to use a conversion tool
that converts the document content into a tagged text format such as html
or xml. these formats are easy to parse, and they retain some of the important
formatting information (font size, for example). you can see this on any major
web search engine. search for a pdf document, but then click on the    cached   
link at the bottom of a search result. you will be taken to the search engine   s view
of the page, which is usually an html rendition of the original document. for
some document types, such as powerpoint, this cached version can be nearly un-
readable. fortunately, readability isn   t the primary concern of the search engine.

50

3 crawls and feeds

the point is to copy this data into the search engine so that it can be indexed and
retrieved. however, translating the data into html has an advantage: the user
does not need to have an application that can read the document   s file format in
order to view it. this is critical for obsolete file formats.

documents could be converted to plain text instead of html or xml.
however, doing this would strip the file of important information about head-
ings and font sizes that could be useful to the indexer. as we will see later, headings
and bold text tend to contain words that describe the document content well, so
we want to give these words preferential treatment during scoring. accurate con-
version of formatting information allows the indexer to extract these important
features.

3.5.1 character encodings
even html files are not necessarily compatible with each other because of char-
acter encoding issues. the text that you see on this page is a series of little pictures
we call letters or glyphs. of course, a computer file is a stream of bits, not a collec-
tion of pictures. a character encoding is a mapping between bits and glyphs. for
english, the basic character encoding that has been around since 1963 is ascii.
ascii encodes 128 letters, numbers, special characters, and control characters
in 7 bits, extended with an extra bit for storage in bytes. this scheme is fine for
the english alphabet of 26 letters, but there are many other languages, and some
of those have many more glyphs. the chinese language, for example, has more
than 40,000 characters, with over 3,000 in common use. for the cjk (chinese-
japanese-korean) family of east asian languages, this led to the development of a
number of di   erent 2-byte standards. other languages, such as hindi or arabic,
also have a range of di   erent encodings. note that not all encodings even agree on
english. the ebcdic encoding used on mainframes, for example, is completely
di   erent than the ascii encoding used by personal computers.

the computer industry has moved slowly in handling complicated character
sets such as chinese and arabic. until recently, the typical approach was to use
di   erent language-specific encodings, sometimes called code pages. the first 128
values of each encoding are reserved for typical english characters, punctuation,
and numbers. numbers above 128 are mapped to glyphs in the target language,
from hebrew to arabic. however, if you use a di   erent encoding for each lan-
guage, you can   t write in hebrew and japanese in the same document. addition-
ally, the text itself is no longer self-describing. it   s not enough to just store data in
a text file; you must also record what encoding was used.

3.5 the conversion problem

51

to solve this mess of encoding issues, unicode was developed. unicode is a
single mapping from numbers to glyphs that attempts to include all glyphs in
common use in all known languages. this solves the problem of using multiple
languages in a single file. unfortunately, it does not fully solve the problems of bi-
nary encodings, because unicode is a mapping between numbers and glyphs, not
bits and glyphs. it turns out that there are many ways to translate unicode num-
bers to glyphs! some of the most popular include utf-8, utf-16, utf-32, and
ucs-2 (which is deprecated).

the proliferation of encodings comes from a need for compatibility and to
save space. encoding english text in utf-8 is identical to the ascii encod-
ing. each ascii letter requires just one byte. however, some traditional chinese
characters can require as many as 4 bytes. the trade-o    for compactness for west-
ern languages is that each character requires a variable number of bytes, which
makes it di   cult to quickly compute the number of characters in a string or to
jump to a random location in a string. by contrast, utf-32 (also known as ucs-
4) uses exactly 4 bytes for every character. jumping to the twentieth character in
a utf-32 string is easy: just jump to the eightieth byte and start reading. un-
fortunately, utf-32 strings are incompatible with all old ascii software, and
utf-32 files require four times as much space as utf-8. because of this, many
applications use utf-32 as their internal text encoding (where random access is
important), but use utf-8 to store text on disk.

hexadecimal
0   7f
80   7ff
800   d7ff

decimal
0   127
128   2047
2048   55295
55296   57343 d800   dfff undefined
57344   65535
65536   1114111 10000   10ffff 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx

encoding
0xxxxxxx
110xxxxx 10xxxxxx
1110xxxx 10xxxxxx 10xxxxxx

1110xxxx 10xxxxxx 10xxxxxx

e000   ffff

table 3.1. utf-8 encoding

table 3.1 shows an encoding table for utf-8. the left columns represent
ranges of decimal values, and the rightmost column shows how these values are
encoded in binary. the x characters represent binary digits. for example, the
greek letter pi (  ) is unicode symbol number 960. in binary, that number is
00000011 11000000 (3c0 in hexadecimal). the second row of the table tells us

52

3 crawls and feeds

that this letter will require 2 bytes to encode in utf-8. the high 5 bits of the
character go in the first byte, and the next 6 bits go in the second byte. the final
encoding is 11001111 10000000 (cf80 in hexadecimal). the bold binary digits
are the same as the digits from the table, while the x letters from the table have
been filled in by binary digits from the unicode number.

3.6 storing the documents

after documents have been converted to some common format, they need to be
stored in preparation for indexing. the simplest document storage is no docu-
ment storage, and for some applications this is preferable. in desktop search, for
example, the documents are already stored in the file system and do not need to be
copied elsewhere. as the crawling process runs, it can send converted documents
immediately to an indexing process. by not storing the intermediate converted
documents, desktop search systems can save disk space and improve indexing la-
tency.

most other kinds of search engines need to store documents somewhere. fast
access to the document text is required in order to build document snippets2 for
each search result. these snippets of text give the user an idea of what is inside the
retrieved document without actually needing to click on a link.

even if snippets are not necessary, there are other reasons to keep a copy of
each document. crawling for documents can be expensive in terms of both cpu
and network load. it makes sense to keep copies of the documents around instead
of trying to fetch them again the next time you want to build an index. keep-
ing old documents allows you to use head requests in your crawler to save on
bandwidth, or to crawl only a subset of the pages in your index.

finally, document storage systems can be a starting point for information ex-
traction (described in chapter 4). the most pervasive kind of information ex-
traction happens in web search engines, which extract anchor text from links to
store with target web documents. other kinds of extraction are possible, such as
identifying names of people or places in documents. notice that if information
extraction is used in the search application, the document storage system should
support modification of the document data.

we now discuss some of the basic requirements for a document storage system,
including random access, compression, and updating, and consider the relative

2 we discuss snippet generation in chapter 6.

3.6 storing the documents

53

benefits of using a database system or a customized storage system such as google   s
bigtable.

3.6.1 using a database system

if you have used a relational database before, you might be thinking that a database
would be a good place to store document data. for many applications, in fact, a
database is an excellent place to store documents. a database takes care of the
di   cult details of storing small pieces of data, such as web pages, and makes it
easy to update them later. most databases also run as a network server, so that the
documents are easily available on the network. this could support, for example,
a single computer serving documents for snippets while many other computers
handle queries. databases also tend to come with useful import and analysis tools
that can make it easier to manage the document collection.

many companies that run web search engines are reluctant to talk about their
internal technologies. however, it appears that few, if any, of the major search
engines use conventional id208 to store documents. one problem
is the sheer volume of document data, which can overwhelm traditional database
systems. database vendors also tend to expect that database servers will use the
most expensive disk systems, which is impractical given the collection size. we
discuss an alternative to a relational database at the end of this section that ad-
dresses some of these concerns.

3.6.2 random access

to retrieve documents quickly in order to compute a snippet for a search result,
the document store needs to support random access. compared to a full relational
database, however, only a relatively simple lookup criterion is needed. we want a
data store such that we can request the content of a document based on its url.
the easiest way to handle this kind of lookup is with hashing. using a hash
function on the url gives us a number we can use to find the data. for small
installations, the hash function can tell us which file contains the document. for
larger installations, the hash function tells us which server contains the document.
once the document location has been narrowed down to a single file, a b-tree or
sorted data structure can be used to find the o   set of the document data within
the file.

54

3 crawls and feeds

3.6.3 compression and large files

regardless of whether the application requires random access to documents, the
document storage system should make use of large files and compression.

even a document that seems long to a person is small by modern computer
standards. for example, this chapter is approximately 10,000 words, and those
words require about 70k of disk space to store. that is far bigger than the average
web page, but a modern hard disk can transfer 70k of data in about a millisecond.
however, the hard disk might require 10 milliseconds to seek to that file in order
to start reading. this is why storing each document in its own file is not a very
good idea; reading these small files requires a substantial overhead to open them.
a better solution is to store many documents in a single file, and for that file to be
large enough that transferring the file contents takes much more time than seeking
to the beginning. a good size choice might be in the hundreds of megabytes. by
storing documents close together, the indexer can spend most of its time reading
data instead of seeking for it.

the galago search engine includes parsers for three compound document for-
mats: arc, trec text, and trec web. in each format, many text documents
are stored in the same file, with short regions of document metadata separating
the documents. figure 3.10 shows an example of the trec web format. notice
that each document block begins with a <doc> tag and ends with a </doc> tag.
at the beginning of the document, the <dochdr> tag marks a section containing
the information about the page request, such as its url, the date it was crawled,
and the http headers returned by the web server. each document record also
contains a <docno> field that includes a unique identifier for the document.

even though large files make sense for data transfer from disk, reducing the
total storage requirements for document collections has obvious advantages. for-
tunately, text written by people is highly redundant. for instance, the letter q is al-
most always followed by the letter u. shannon (1951) showed that native english
speakers are able to guess the next letter of a passage of english text with 69% ac-
curacy. html and xml tags are even more redundant. compression techniques
exploit this redundancy to make files smaller without losing any of the content.
we will cover compression as it is used for document indexing in chapter 5,
in part because compression for indexing is rather specialized. while research
continues into text compression, popular algorithms like deflate (deutsch,
1996) and lzw (welch, 1984) can compress html and xml text by 80%.
this space savings reduces the cost of storing a lot of documents, and also reduces

3.6 storing the documents

55

<doc>
<docno>wtx001-b01-10</docno>
<dochdr>
http://www.example.com/test.html 204.244.59.33 19970101013145 text/html 440
http/1.0 200 ok
date: wed, 01 jan 1997 01:21:13 gmt
server: apache/1.0.3
content-type: text/html
content-length: 270
last-modified: mon, 25 nov 1996 05:31:24 gmt
</dochdr>
<html>
<title>tropical fish store</title>
coming soon!
</html>
</doc>
<doc>
<docno>wtx001-b01-109</docno>
<dochdr>
http://www.example.com/fish.html 204.244.59.33 19970101013149 text/html 440
http/1.0 200 ok
date: wed, 01 jan 1997 01:21:19 gmt
server: apache/1.0.3
content-type: text/html
content-length: 270
last-modified: mon, 25 nov 1996 05:31:24 gmt
</dochdr>
<html>
<title>fish information</title>
this page will soon contain interesting
information about tropical fish.
</html>
</doc>

fig. 3.10. an example of text in the trec web compound document format

56

3 crawls and feeds

the amount of time it takes to read a document from the disk since there are fewer
bytes to read.

compression works best with large blocks of data, which makes it a good fit
for big files with many documents in them. however, it is not necessarily a good
idea to compress the entire file as a single block. most compression methods do
not allow random access, so each block can only be decompressed sequentially. if
you want random access to the data, it is better to consider compressing in smaller
blocks, perhaps one block per document, or one block for a few documents. small
blocks reduce compression ratios (the amount of space saved) but improve re-
quest latency.

3.6.4 update

as new versions of documents come in from the crawler, it makes sense to update
the document store. the alternative is to create an entirely new document store by
merging the new, changed documents from the crawler with document data from
the old document store for documents that did not change. if the document data
does not change very much, this merging process will be much more expensive
than updating the data in place.

<a href="http://example.com" >example website</a>

fig. 3.11. an example link with anchor text

another important reason to support update is to handle anchor text. fig-
ure 3.11 shows an example of anchor text in an html link tag. the html
code in the figure will render in the web browser as a link, with the text example
website that, when clicked, will direct the user to http://example.com. anchor text
is an important feature because it provides a concise summary of what the target
page is about. if the link comes from a di   erent website, we may also believe that
the summary is unbiased, which also helps us rank documents (see chapters 4
and 7).

collecting anchor text properly is di   cult because the anchor text needs to
be associated with the target page. a simple way to approach this is to use a data
store that supports update. when a document is found that contains anchor text,
we find the record for the target page and update the anchor text portion of the
record. when it is time to index the document, the anchor text is all together and
ready for indexing.

3.6 storing the documents

57

3.6.5 bigtable

although a database can perform the duties of a document data store, the very
largest document collections demand custom document storage systems. bigtable
is the most well known of these systems (chang et al., 2006). bigtable is a working
system in use internally at google, although at least two open source projects are
taking a similar approach. in the next few paragraphs, we will look at the bigtable
architecture to see how the problem of document storage influenced its design.
bigtable is a distributed database system originally built for the task of storing
web pages. a bigtable instance really is a big table; it can be over a petabyte in
size, but each database contains only one table. the table is split into small pieces,
called tablets, which are served by thousands of machines (figure 3.12).

fig. 3.12. bigtable stores data in a single logical table, which is split into many smaller
tablets

if you are familiar with id208, you will have encountered sql
(structured query language). sql allows users to write complex and computa-
tionally expensive queries, and one of the tasks of the database system is to opti-
mize the processing of these queries to make them as fast as possible. because some
of these queries could take a very long time to complete, a large relational database
requires a complex locking system to ensure that the many users of the database
do not corrupt it by reading or writing data simultaneously. isolating users from
each other is a di   cult job, and many papers and books have been written about
how to do it well.

logical tabletablets58

3 crawls and feeds

the bigtable approach is quite di   erent. there is no query language, and
therefore no complex queries, and it includes only row-level transactions, which
would be considered rather simple by relational database standards. however, the
simplicity of the model allows bigtable to scale up to very large database sizes
while using inexpensive computers, even though they may be prone to failure.

most of the engineering in bigtable involves failure recovery. the tablets,
which are the small sections of the table, are stored in a replicated file system
that is accessible by all bigtable tablet servers. any changes to a bigtable tablet
are recorded to a transaction log, which is also stored in a shared file system. if
any tablet server crashes, another server can immediately read the tablet data and
transaction log from the file system and take over.

most id208 store their data in files that are constantly modified.
in contrast, bigtable stores its data in immutable (unchangeable) files. once file
data is written to a bigtable file, it is never changed. this also helps in failure
recovery. in relational database systems, failure recovery requires a complex series
of operations to make sure that files were not corrupted because only some of
the outstanding writes completed before the computer crashed. in bigtable, a file
is either incomplete (in which case it can be thrown away and re-created from
other bigtable files and the transaction log), or it is complete and therefore is not
corrupt. to allow for table updates, the newest data is stored in ram, whereas
older data is stored in a series of files. periodically the files are merged together to
reduce the total number of disk files.

fig. 3.13. a bigtable row

bigtables are logically organized by rows (figure 3.13). in the figure, the row
stores the data for a single web page. the url, www.example.com, is the row key,
which can be used to find this row. the row has many columns, each with a unique
name. each column can have many di   erent timestamps, although that is not
shown in the figure. the combination of a row key, a column key, and a times-

www.example.comtextanchor:other.comanchor:null.comtitledocument textexampleclick hereexample site3.6 storing the documents

59

tamp point to a single cell in the row. the cell holds a series of bytes, which might
be a number, a string, or some other kind of data.

in the figure, notice that there is a text column for the full text of the docu-
ment as well as a title column, which makes it easy to quickly find the document
title without parsing the full document text. there are two columns for anchor
text. one, called anchor:other.com, includes anchor text from a link from the site
other.com to example.com; the text of the link is    example   , as shown in the cell.
the anchor:null.com describes a link from null.com to example.com with anchor text
   click here   . both of these columns are in the anchor column group. other columns
could be added to this column group to add information about more links.

bigtable can have a huge number of columns per row, and while all rows have
the same column groups, not all rows have the same columns. this is a major de-
parture from traditional database systems, but this flexibility is important, in part
because of the lack of tables. in a relational database system, the anchor columns
would be stored in one table and the document text in another. because bigtable
has just one table, all the anchor information needs to be packed into a single
record. with all the anchor data stored together, only a single disk read is neces-
sary to read all of the document data. in a two-table relational database, at least
two reads would be necessary to retrieve this data.

rows are partitioned into tablets based on their row keys. for instance, all
urls beginning with a could be located in one tablet, while all those starting with
b could be in another tablet. using this kind of range-based partitioning makes
it easy for a client of bigtable to determine which server is serving each row. to
look up a particular row, the client consults a list of row key ranges to determine
which tablet would hold the desired row. the client then contacts the appropriate
tablet server to fetch the row. the row key ranges are cached in the client, so that
most of the network tra   c is between clients and tablet servers.

bigtable   s architecture is designed for speed and scale through massive num-
bers of servers, and for economy by using inexpensive computers that are expected
to fail. in order to achieve these goals, bigtable sacrifices some key relational
database features, such as a complex query language and multiple-table databases.
however, this architecture is well suited for the task of storing and finding web
pages, where the primary task is e   cient lookups and updates on individual rows.

60

3 crawls and feeds

3.7 detecting duplicates

duplicate and near-duplicate documents occur in many situations. making copies
and creating new versions of documents is a constant activity in o   ces, and keep-
ing track of these is an important part of information management. on the web,
however, the situation is more extreme. in addition to the normal sources of dupli-
cation, plagiarism and spam are common, and the use of multiple urls to point
to the same web page and mirror sites can cause a crawler to generate large num-
bers of duplicate pages. studies have shown that about 30% of the web pages in a
large crawl are exact or near duplicates of pages in the other 70% (e.g., fetterly et
al., 2003).

documents with very similar content generally provide little or no new infor-
mation to the user, but consume significant resources during crawling, indexing,
and search. in response to this problem, algorithms for detecting duplicate doc-
uments have been developed so that they can be removed or treated as a group
during indexing and ranking.

detecting exact duplicates is a relatively simple task that can be done using
checksumming techniques. a checksum is a value that is computed based on the
content of the document. the most straightforward checksum is a sum of the
bytes in the document file. for example, the checksum for a file containing the
text    tropical fish    would be computed as follows (in hex):
h sum
t r
54 72 6f 70 69 63 61 6c 20 66 69 73 68 508

o p

a

c

s

f

i

i

l

any document file containing the same text would have the same checksum. of
course, any document file containing text that happened to have the same check-
sum would also be treated as a duplicate. a file containing the same characters
in a di   erent order would have the same checksum, for example. more sophis-
ticated functions, such as a cyclic redundancy check (crc), have been developed
that consider the positions of the bytes.

the detection of near-duplicate documents is more di   cult. even defining a
near-duplicate is challenging. web pages, for example, could have the same text
content but di   er in the advertisements, dates, or formatting. other pages could
have small di   erences in their content from revisions or updates. in general, a
near-duplicate is defined using a threshold value for some similarity measure be-
tween pairs of documents. for example, a document d1 could be defined as a
near-duplicate of document d2 if more than 90% of the words in the documents
were the same.

3.7 detecting duplicates

61

there are two scenarios for near-duplicate detection. one is the search sce-
nario, where the goal is to find near-duplicates of a given document d. this, like
all search problems, conceptually involves the comparison of the query document
to all other documents. for a collection containing n documents, the number of
comparisons required will be o(n ). the other scenario, discovery, involves find-
ing all pairs of near-duplicate documents in the collection. this process requires
o(n 2) comparisons. although information retrieval techniques that measure
similarity using word-based representations of documents have been shown to
be e   ective for identifying near-duplicates in the search scenario, the computa-
tional requirements of the discovery scenario have meant that new techniques
have been developed for deriving compact representations of documents. these
compact representations are known as fingerprints.

the basic process of generating fingerprints is as follows:

1. the document is parsed into words. non-word content, such as punctuation,

html tags, and additional whitespace, is removed (see section 4.3).

2. the words are grouped into contiguous id165s for some n. these are usually
overlapping sequences of words (see section 4.3.5), although some techniques
use non-overlapping sequences.

3. some of the id165s are selected to represent the document.
4. the selected id165s are hashed to improve retrieval e   ciency and further

reduce the size of the representation.

5. the hash values are stored, typically in an inverted index.

there are a number of fingerprinting algorithms that use this general approach,
and they di   er mainly in how subsets of the id165s are selected. selecting a fixed
number of id165s at random does not lead to good performance in terms of
finding near-duplicates. consider two near-identical documents, d1 and d2. the
fingerprints generated from id165s selected randomly from document d1 are
unlikely to have a high overlap with the fingerprints generated from a di   erent
set of id165s selected randomly from d2. a more e   ective technique uses pre-
specified combinations of characters, and selects id165s that begin with those
characters. another popular technique, called 0 mod p, is to select all id165s
whose hash value modulo p is zero, where p is a parameter.

figure 3.14 illustrates the fingerprinting process using overlapping 3-grams,
hypothetical hash values, and the 0 mod p selection method with a p value of 4.
note that after the selection process, the document (or sentence in this case) is
represented by fingerprints for the id165s    fish include fish   ,    found in tropical   ,

62

3 crawls and feeds

fig. 3.14. example of fingerprinting process

   the world including   , and    including both freshwater   . in large-scale applications,
such as finding near-duplicates on the web, the id165s are typically 5   10 words
long and the hash values are 64 bits.3

near-duplicate documents are found by comparing the fingerprints that repre-
sent them. near-duplicate pairs are defined by the number of shared fingerprints
or the ratio of shared fingerprints to the total number of fingerprints used to rep-
resent the pair of documents. fingerprints do not capture all of the information
in the document, however, and consequently this leads to errors in the detection
of near-duplicates. appropriate selection techniques can reduce these errors, but
not eliminate them. as we mentioned, evaluations have shown that comparing
word-based representations using a similarity measure such as the cosine correla-
tion (see section 7.1.2) is generally significantly more e   ective than fingerprinting
methods for finding near-duplicates. the problem with these methods is their ef-
ficiency.

3 the hash values are usually generated using rabin fingerprinting (broder et al., 1997),
named after the israeli computer scientist michael rabin.

tropical fish include fish found in tropical environments around the world, including both freshwater and salt water species.                                                        (a) original text  tropical fish include, fish include fish, include fish found, fish found in, found in tropical, in tropical environments, tropical environments around, environments around the, around the world, the world including, world including both, including both freshwater, both freshwater and, freshwater and salt, and salt water, salt water species                                                          (b) 3-grams  938  664  463  822  492  798  78  969  143  236  913  908  694  553  870  779                                                       (c) hash values                                                        664  492  236  908                                       (d) selected hash values using 0 mod 4 3.8 removing noise

63

a recently developed fingerprinting technique called simhash (charikar, 2002)
combines the advantages of the word-based similarity measures with the e   -
ciency of fingerprints based on hashing. it has the unusual property for a hashing
function that similar documents have similar hash values. more precisely, the sim-
ilarity of two pages as measured by the cosine correlation measure is proportional
to the number of bits that are the same in the fingerprints generated by simhash.

the procedure for calculating a simhash fingerprint is as follows:

1. process the document into a set of features with associated weights. we will
assume the simple case where the features are words weighted by their fre-
quency. other weighting schemes are discussed in chapter 7.

2. generate a hash value with b bits (the desired size of the fingerprint) for each

word. the hash value should be unique for each word.

3. in b-dimensional vector v , update the components of the vector by adding
the weight for a word to every component for which the corresponding bit in
the word   s hash value is 1, and subtracting the weight if the value is 0.

4. after all words have been processed, generate a b-bit fingerprint by setting the

ith bit to 1 if the ith component of v is positive, or 0 otherwise.
figure 3.15 shows an example of this process for an 8-bit fingerprint. note
that common words (stopwords) are removed as part of the text processing. in
practice, much larger values of b are used. henzinger (2006) describes a large-scale
web-based evaluation where the fingerprints had 384 bits. a web page is defined
as a near-duplicate of another page if the simhash fingerprints agree on more than
372 bits. this study showed significant e   ectiveness advantages for the simhash
approach compared to fingerprints based on id165s.

3.8 removing noise

many web pages contain text, links, and pictures that are not directly related to
the main content of the page. for example, figure 3.16 shows a web page contain-
ing a news story. the main content of the page (the story) is outlined in black.
this content block takes up less than 20% of the display area of the page, and the
rest is made up of banners, advertisements, images, general navigation links, ser-
vices (such as search and alerts), and miscellaneous information, such as copy-
right. from the perspective of the search engine, this additional material in the
web page is mostly noise that could negatively a   ect the ranking of the page. a
major component of the representation of a page used in a search engine is based

64

3 crawls and feeds

fig. 3.15. example of simhash fingerprinting process

on word counts, and the presence of a large number of words unrelated to the
main topic can be a problem. for this reason, techniques have been developed to
detect the content blocks in a web page and either ignore the other material or
reduce its importance in the indexing process.

finn et al. (2001) describe a relatively simple technique based on the obser-
vation that there are less html tags in the text of the main content of typical
web pages than there is in the additional material. figure 3.17 (also known as a
document slope curve) shows the cumulative distribution of tags in the example
web page from figure 3.16, as a function of the total number of tokens (words or
other non-tag strings) in the page. the main text content of the page corresponds
to the    plateau    in the middle of the distribution. this flat area is relatively small
because of the large amount of formatting and presentation information in the
html source for the page.

tropical fish include fish found in tropical environments around the world, including both freshwater and salt water species.                                                        (a) original text  tropical 2  fish 2  include 1  found 1  environments 1  around 1  world 1   including 1  both 1  freshwater 1  salt 1  water 1  species 1                                                    (b) words with weights  tropical   01100001fish 10101011include 11100110found 00011110environments00101101around 10001011world 00101010including11000000both10101110freshwater 00111111salt 10110101water00100101species 11101110                                                       (c) 8 bit hash values                                               1    5  9    9  3  1  3  3                                       (d) vector v formed by summing weights                                                1  0  1  0  1  1  1  1                                           (e) 8-bit fingerprint formed from v 3.8 removing noise

65

fig. 3.16. main content block in a web page

one way to detect the largest flat area of the distribution is to represent a web
page as a sequence of bits, where bn = 1 indicates that the nth token is a tag, and
bn = 0 otherwise. certain tags that are mostly used to format text, such as font
changes, headings, and table tags, are ignored (i.e., are represented by a 0 bit). the
detection of the main content can then be viewed as an optimization problem
where we find values of i and j to maximize both the number of tags below i and
above j and the number of non-tag tokens between i and j. this corresponds to
maximizing the corresponding objective function:

i   1   

j   
(1     bn) +

n   1   

bn

bn +

n=0

n=i

n=j+1

content block66

3 crawls and feeds

fig. 3.17. tag counts used to identify text blocks in a web page

where n is the number of tokens in the page. this can be done simply by scanning
the possible values for i and j and computing the objective function. note that
this procedure will only work when the proportion of text tokens in the non-
content section is lower than the proportion of tags, which is not the case for the
web page in figure 3.17. pinto et al. (2002) modified this approach to use a text
window to search for low-slope sections of the document slope curve.

the structure of the web page can also be used more directly to identify the
content blocks in the page. to display a web page using a browser, an html
parser interprets the structure of the page specified using the tags, and creates a
document object model (dom) representation. the tree-like structure repre-
sented by the dom can be used to identify the major components of the web
page. figure 3.18 shows part of the dom structure4 for the example web page in
figure 3.16. the part of the structure that contains the text of the story is indicated
by the comment id98articlecontent. gupta et al. (2003) describe an approach that

4 this was generated using the dom inspector tool in the firefox browser.

010020030040050060070080090010000500100015002000250030003500tag counttoken counttext area3.8 removing noise

67

navigates the dom tree recursively, using a variety of filtering techniques to re-
move and modify nodes in the tree and leave only content. html elements such
as images and scripts are removed by simple filters. more complex filters remove
advertisements, lists of links, and tables that do not have    substantive    content.

fig. 3.18. part of the dom structure for the example web page

the dom structure provides useful information about the components of a
web page, but it is complex and is a mixture of logical and layout components. in
figure 3.18, for example, the content of the article is buried in a table cell (td tag)
in a row (tr tag) of an html table (table tag). the table is being used in this
case to specify layout rather than semantically related data. another approach to

 68

3 crawls and feeds

identifying the content blocks in a page focuses on the layout and presentation of
the web page. in other words, visual features   such as the position of the block,
the size of the font used, the background and font colors, and the presence of
separators (such as lines and spaces)   are used to define blocks of information
that would be apparent to the user in the displayed web page. yu et al. (2003)
describe an algorithm that constructs a hierarchy of visual blocks from the dom
tree and visual features.

the first algorithm we discussed, based on the distribution of tags, is quite
e   ective for web pages with a single content block. algorithms that use the dom
structure and visual analysis can deal with pages that may have several content
blocks. in the case where there are several content blocks, the relative importance
of each block can be used by the indexing process to produce a more e   ective
representation. one approach to judging the importance of the blocks in a web
page is to train a classifier that will assign an importance category based on visual
and content features (r. song et al., 2004).

references and further reading

cho and garcia-molina (2002, 2003) wrote a series of influential papers on web
crawler design. our discussion of page refresh policies is based heavily on cho
and garcia-molina (2003), and section 3.2.7 draws from cho and garcia-molina
(2002).

there are many open source web crawlers. the heritrix crawler,5 developed for
the internet archive project, is a capable and scalable example. the system is de-
veloped in modules that are highly configurable at runtime, making it particularly
suitable for experimentation.

focused crawling attracted much attention in the early days of web search.
menczer and belew (1998) and chakrabarti et al. (1999) wrote two of the most
influential papers. menczer and belew (1998) envision a focused crawler made of
autonomous software agents, principally for a single user. the user enters a list of
both urls and keywords. the agent then attempts to find web pages that would
be useful to the user, and the user can rate those pages to give feedback to the
system. chakrabarti et al. (1999) focus on crawling for specialized topical indexes.
their crawler uses a classifier to determine the topicality of crawled pages, as well
as a distiller, which judges the quality of a page as a source of links to other topical

5 http://crawler.archive.org

3.8 removing noise

69

pages. they evaluate their system against a traditional, unfocused crawler to show
that an unfocused crawler seeded with topical links is not su   cient to achieve a
topical crawl. the broad link structure of the web causes the unfocused crawler to
quickly drift to other topics, while the focused crawler successfully stays on topic.
the unicode specification is an incredibly detailed work, covering tens of
thousands of characters (unicode consortium, 2006). because of the nature of
some non-western scripts, many glyphs are formed from grouping a number of
unicode characters together, so the specification must detail not just what the
characters are, but how they can be joined together. characters are still being
added to unicode periodically.

bergman (2001) is an extensive study of the deep web. even though this study
is old by web standards, it shows how sampling through search engines can be used
to help estimate the amount of unindexed content on the web. this study esti-
mated that 550 billion web pages existed in the deep web, compared to 1 billion
in the accessible web. he et al. (2007) describe a more recent survey that shows
that the deep web has continued to expand rapidly in recent years. an example
of a technique for generating searchable representations of deep web databases,
called query probing, is described by ipeirotis and gravano (2004).

sitemaps, robots.txt files, rss feeds, and atom feeds each have their own spec-
ifications, which are available on the web.6 these formats show that successful
web standards are often quite simple.

as we mentioned, database systems can be used to store documents from a
web crawl for some applications. our discussion of database systems was, how-
ever, limited mostly to a comparison with bigtable. there are a number of text-
books, such as garcia-molina et al. (2008), that provide much more informa-
tion on how databases work, including details about important features such as
query languages, locking, and recovery. bigtable, which we referenced frequently,
was described in chang et al. (2006). other large internet companies have built
their own database systems with similar goals: large-scale distribution and high
throughput, but without an expressive query language or detailed transaction sup-
port. the dynamo system from amazon has low latency guarantees (decandia
et al., 2007), and yahoo! uses their udb system to store large datasets (baeza-
yates & ramakrishnan, 2008).

6 http://www.sitemaps.org
http://www.robotstxt.org
http://www.rssboard.org/rss-specification
http://www.rfc-editor.org/rfc/rfc5023.txt

70

3 crawls and feeds

we mentioned deflate (deutsch, 1996) and lzw (welch, 1984) as spe-
cific document compression algorithms in the text. deflate is the basis for the
popular zip, gzip, and zlib compression tools. lzw is the basis of the unix com-
press command, and is also found in file formats such as gif, postscript, and
pdf. the text by witten et al. (1999) provides detailed discussions about text
and image compression algorithms.

hoad and zobel (2003) provide both a review of fingerprinting techniques
and a comparison to word-based similarity measures for near-duplicate detection.
their evaluation focused on finding versions of documents and plagiarized docu-
ments. bernstein and zobel (2006) describe a technique for using full fingerprint-
ing (no selection) for the task of finding co-derivatives, which are documents de-
rived from the same source. bernstein and zobel (2005) examined the impact of
duplication on evaluations of retrieval e   ectiveness. they showed that about 15%
of the relevant documents for one of the trec tracks were redundant, which
could significantly a   ect the impact of the results from a user   s perspective.

henzinger (2006) describes a large-scale evaluation of near-duplicate detec-
tion on the web. the two techniques compared were a version of broder   s    shin-
gling    algorithm (broder et al., 1997; fetterly et al., 2003) and simhash (charikar,
2002). henzinger   s study, which used 1.6 billion pages, showed that neither meth-
od worked well for detecting redundant documents on the same site because of
the frequent use of    boilerplate    text that makes di   erent pages look similar.
for pages on di   erent sites, the simhash algorithm achieved a precision of 50%
(meaning that of those pages that were declared    near-duplicate    based on the
similarity threshold, 50% were correct), whereas the broder algorithm produced
a precision of 38%.

a number of papers have been written about techniques for extracting content
from web pages. yu et al. (2003) and gupta et al. (2003) are good sources for
references to these papers.

exercises

3.1. suppose you have two collections of documents. the smaller collection is full
of useful, accurate, high-quality information. the larger collection contains a few
high-quality documents, but also contains lower-quality text that is old, out-of-
date, or poorly written. what are some reasons for building a search engine for
only the small collection? what are some reasons for building a search engine
that covers both collections?

3.8 removing noise

71

3.2. suppose you have a network connection that can transfer 10mb per second.
if each web page is 10k and requires 500 milliseconds to transfer, how many
threads does your web crawler need to fully utilize the network connection? if
your crawler needs to wait 10 seconds between requests to the same web server,
what is the minimum number of distinct web servers the system needs to contact
each minute to keep the network connection fully utilized?

3.3. what is the advantage of using head requests instead of get requests dur-
ing crawling? when would a crawler use a get request instead of a head re-
quest?

3.4. why do crawlers not use post requests?

3.5. name the three types of sites mentioned in the chapter that compose the
deep web.

3.6. how would you design a system to automatically enter data into web forms in
order to crawl deep web pages? what measures would you use to make sure your
crawler   s actions were not destructive (for instance, so that it doesn   t add random
blog comments).

3.7. write a program that can create a valid sitemap based on the contents of a
directory on your computer   s hard disk. assume that the files are accessible from
a website at the url http://www.example.com. for instance, if there is a file in
your directory called homework.pdf, this would be available at http://www.exam-
ple.com/homework.pdf. use the real modification date on the file as the last modi-
fied time in the sitemap, and to help estimate the change frequency.

3.8. suppose that, in an e   ort to crawl web pages faster, you set up two crawl-
ing machines with di   erent starting seed urls. is this an e   ective strategy for
distributed crawling? why or why not?

3.9. write a simple single-threaded web crawler. starting from a single input url
(perhaps a professor   s web page), the crawler should download a page and then
wait at least five seconds before downloading the next page. your program should
find other pages to crawl by parsing link tags found in previously crawled docu-
ments.

3.10. utf-16 is used in java and windows  . compare it to utf-8.

3.11. how does bigtable handle hardware failure?

72

3 crawls and feeds

3.12. design a compression algorithm that compresses html tags. your algo-
rithm should detect tags in an html file and replace them with a code of your
own design that is smaller than the tag itself. write an encoder and decoder pro-
gram.

3.13. generate checksums for a document by adding the bytes of the document
and by using the unix command cksum. edit the document and see if both check-
sums change. can you change the document so that the simple checksum does not
change?

3.14. write a program to generate simhash fingerprints for documents. you can
use any reasonable hash function for the words. use the program to detect du-
plicates on your home computer. report on the accuracy of the detection. how
does the detection accuracy vary with fingerprint size?

3.15. plot the document slope curves for a sample of web pages. the sample
should include at least one page containing a news article. test the accuracy of
the simple optimization algorithm for detecting the main content block. write
your own program or use the code from http://www.aidanf.net/software/bte-body-
text-extraction. describe the cases where the algorithm fails. would an algorithm
that searched explicitly for low-slope areas of the document slope curve be suc-
cessful in these cases?

3.16. give a high-level outline of an algorithm that would use the dom structure
to identify content information in a web page. in particular, describe heuristics
you would use to identify content and non-content elements of the structure.

4

processing text

4.1 from words to terms

   i was trying to comprehend the meaning of the
words.   

spock, star trek: the final frontier

after gathering the text we want to search, the next step is to decide whether it
should be modified or restructured in some way to simplify searching. the types
of changes that are made at this stage are called text transformation or, more often,
text processing. the goal of text processing is to convert the many forms in which
words can occur into more consistent index terms. index terms are the represen-
tation of the content of a document that are used for searching.

the simplest decision about text processing would be to not do it at all. a good
example of this is the    find    feature in your favorite word processor. by the time
you use the find command, the text you wish to search has already been gathered:
it   s on the screen. after you type the word you want to find, the word processor
scans the document and tries to find the exact sequence of letters that you just
typed. this feature is extremely useful, and nearly every text editing program can
do this because users demand it.

the trouble is that exact text search is rather restrictive. the most annoying
restriction is case-sensitivity: suppose you want to find    computer hardware   , and
there is a sentence in the document that begins with    computer hardware   . your
search query does not exactly match the text in the sentence, because the first letter
of the sentence is capitalized. fortunately, most word processors have an option
for ignoring case during searches. you can think of this as a very rudimentary form
of online text processing. like most text processing techniques, ignoring case in-
creases the id203 that you will find a match for your query in the document.
many search engines do not distinguish between uppercase and lowercase let-
ters. however, they go much further. as we will see in this chapter, search engines

74

4 processing text

can strip punctuation from words to make them easier to find. words are split
apart in a process called id121. some words may be ignored entirely in or-
der to make query processing more e   ective and e   cient; this is called stopping.
the system may use id30 to allow similar words (like    run    and    running   )
to match each other. some documents, such as web pages, may have formatting
changes (like bold or large text), or explicit structure (like titles, chapters, and cap-
tions) that can also be used by the system. web pages also contain links to other
web pages, which can be used to improve document ranking. all of these tech-
niques are discussed in this chapter.

these text processing techniques are fairly simple, even though their e   ects
on search results can be profound. none of these techniques involves the com-
puter doing any kind of complex reasoning or understanding of the text. search
engines work because much of the meaning of text is captured by counts of word
occurrences and co-occurrences,1 especially when that data is gathered from the
huge text collections available on the web. understanding the statistical nature of
text is fundamental to understanding retrieval models and ranking algorithms, so
we begin this chapter with a discussion of text statistics. more sophisticated tech-
niques for natural language processing that involve syntactic and semantic analysis
of text have been studied for decades, including their application to information
retrieval, but to date have had little impact on ranking algorithms for search en-
gines. these techniques are, however, being used for the task of question answer-
ing, which is described in chapter 11. in addition, techniques involving more
complex text processing are being used to identify additional index terms or fea-
tures for search. information extraction techniques for identifying people   s names,
organization names, addresses, and many other special types of features are dis-
cussed here, and classification, which can be used to identify semantic categories,
is discussed in chapter 9.

finally, even though this book focuses on retrieving english documents, in-
formation retrieval techniques can be used with text in many di   erent languages.
in this chapter, we show how di   erent languages require di   erent types of text
representation and processing.

1 word co-occurrence measures the number of times groups of words (usually pairs) oc-
cur together in documents. a collocation is the name given to a pair, group, or sequence
of words that occur together more often than would be expected by chance. the term
association measures that are used to find collocations are discussed in chapter 6.

4.2 text statistics

4.2 text statistics

75

although language is incredibly rich and varied, it is also very predictable. there
are many ways to describe a particular topic or event, but if the words that occur
in many descriptions of an event are counted, then some words will occur much
more frequently than others. some of these frequent words, such as    and    or    the,   
will be common in the description of any event, but others will be characteristic
of that particular event. this was observed as early as 1958 by luhn, when he
proposed that the significance of a word depended on its frequency in the docu-
ment. statistical models of word occurrences are very important in information
retrieval, and are used in many of the core components of search engines, such
as the ranking algorithms, query transformation, and indexing techniques. these
models will be discussed in later chapters, but we start here with some of the basic
models of word occurrence.

one of the most obvious features of text from a statistical point of view is that
the distribution of word frequencies is very skewed. there are a few words that
have very high frequencies and many words that have low frequencies. in fact, the
two most frequent words in english (   the    and    of    ) account for about 10% of all
word occurrences. the most frequent six words account for 20% of occurrences,
and the most frequent 50 words are about 40% of all text! on the other hand,
given a large sample of text, typically about one half of all the unique words in
that sample occur only once. this distribution is described by zipf   s law,2 which
states that the frequency of the rth most common word is inversely proportional
to r or, alternatively, the rank of a word times its frequency (f) is approximately
a constant (k):

r    f = k

we often want to talk about the id203 of occurrence of a word, which is just
the frequency of the word divided by the total number of word occurrences in the
text. in this case, zipf    s law is:

r    pr = c

where pr is the id203 of occurrence for the rth ranked word, and c is a con-
stant. for english, c     0.1. figure 4.1 shows the graph of zipf    s law with this
constant. this clearly shows how the frequency of word occurrence falls rapidly
after the first few most common words.
2 named after the american linguist george kingsley zipf.

76

4 processing text

fig. 4.1. rank versus id203 of occurrence for words assuming zipf    s law (rank   
id203 = 0.1)

to see how well zipf    s law predicts word occurrences in actual text collec-
tions, we will use the associated press collection of news stories from 1989 (called
ap89) as an example. this collection was used in trec evaluations for several
years. table 4.1 shows some statistics for the word occurrences in ap89. the vo-
cabulary size is the number of unique words in the collection. even in this rela-
tively small collection, the vocabulary size is quite large (nearly 200,000 unique
words). a large proportion of these words (70,000) occur only once. words that
occur once in a text corpus or book have long been regarded as important in text
analysis, and have been given the special name of hapax legomena.3

table 4.2 shows the 50 most frequent words from the ap89 collection, to-
gether with their frequencies, ranks, id203 of occurrence (converted to a
percentage of total occurrences), and the r.pr value. from this table, we can see

3 the name was created by scholars studying the bible. since the 13th century, people
have studied the word occurrences in the bible and, of particular interest, created con-
cordances, which are indexes of where words occur in the text. concordances are the
ancestors of the inverted files that are used in modern search engines. the first concor-
dance was said to have required 500 monks to create.

00.010.020.030.040.050.060.070.080.090.10102030405060708090100rank (by decreasing frequency)id203(of occurrence)4.2 text statistics

77

total documents
total word occurrences
vocabulary size
words occurring > 1000 times
words occurring once

84,678
39,749,179
198,763
4,169
70,064

table 4.1. statistics for the ap89 collection

that zipf    s law is quite accurate, in that the value of r.pr is approximately con-
stant, and close to 0.1. the biggest variations are for some of the most frequent
words. in fact, it is generally observed that zipf    s law is inaccurate for low and
high ranks (high-frequency and low-frequency words). table 4.3 gives some ex-
amples for lower-frequency words from ap89.

figure 4.2 shows a log-log plot4 of the r.pr values for all words in the ap89
collection. zipf    s law is shown as a straight line on this graph since log pr =
log(c    r
   1) = log c     log r. this figure clearly shows how the predicted re-
lationship breaks down at high ranks (approximately rank 10,000 and above). a
number of modifications to zipf    s law have been proposed,5 some of which have
interesting connections to cognitive models of language.

it is possible to derive a simple formula for predicting the proportion of words
with a given frequency from zipf    s law. a word that occurs n times has rank
rn = k/n. in general, more than one word may have the same frequency. we
assume that the rank rn is associated with the last of the group of words with the
same frequency. in that case, the number of words with the same frequency n will
be given by rn     rn+1, which is the rank of the last word in the group minus
the rank of the last word of the previous group of words with a higher frequency
(remember that higher-frequency words have lower ranks). for example, table
4.4 has an example of a ranking of words in decreasing order of their frequency.
the number of words with frequency 5,099 is the rank of the last member of that

4 the x and y axes of a log-log plot show the logarithm of the values of x and y, not the

values themselves.
5 the most well-known is the derivation by the mathematician benoit mandelbrot (the
same person who developed fractal geometry), which is (r +   )(cid:11)    pr =   , where   ,
  , and    are parameters that can be tuned for a particular text. in the case of the ap89
collection, however, the fit for the frequency data is not noticeably better than the zipf
distribution.

78

4 processing text

r.pr word

freq

r pr(%)

word
the
of
to
a
and
in
said
for
that
was
on
he
is
with
at
by
it
from
as
be
were
an
have
his
but

r pr(%)
freq.
1
2,420,778
2
1,045,733
3
968,882
4
892,429
5
865,644
6
847,825
7
504,593
8
363,865
347,072
9
293,027 10
291,947 11
250,919 12
245,843 13
223,846 14
210,064 15
209,586 16
195,621 17
189,451 18
181,714 19
157,300 20
153,913 21
152,576 22
149,749 23
142,285 24
140,880 25

6.49 0.065 has
2.80 0.056 are
2.60 0.078 not
2.39 0.096 who
2.32 0.120 they
2.27 0.140 its
1.35 0.095 had
0.98 0.078 will
0.93 0.084 would
0.79 0.079 about
0.78 0.086 i
0.67 0.081 been
0.65 0.086 this
0.60 0.084 their
0.56 0.085 new
0.56 0.090 or
0.52 0.089 which
0.51 0.091 we
0.49 0.093 more
0.42 0.084 after
0.41 0.087 us
0.41 0.090 percent
0.40 0.092 up
0.38 0.092 one
0.38 0.094 people

136,007 26
130,322 27
127,493 28
116,364 29
111,024 30
111,021 31
103,943 32
102,949 33
99,503 34
92,983 35
92,005 36
88,786 37
87,286 38
84,638 39
83,449 40
81,796 41
80,385 42
80,245 43
76,388 44
75,165 45
72,045 46
71,956 47
71,082 48
70,266 49
68,988 50

r.pr
0.37 0.095
0.35 0.094
0.34 0.096
0.31 0.090
0.30 0.089
0.30 0.092
0.28 0.089
0.28 0.091
0.27 0.091
0.25 0.087
0.25 0.089
0.24 0.088
0.23 0.089
0.23 0.089
0.22 0.090
0.22 0.090
0.22 0.091
0.22 0.093
0.21 0.090
0.20 0.091
0.19 0.089
0.19 0.091
0.19 0.092
0.19 0.092
0.19 0.093

table 4.2. most frequent 50 words from ap89

word
assistant
sewers
toothbrush
hazmat

freq.
5,095
100
10
1

r
1,021
17,110
51,555
166,945

pr(%)
.013

.000256
.000025
.000002

r.pr
0.13
0.04
0.01
0.04

table 4.3. low-frequency words from ap89

4.2 text statistics

79

fig. 4.2. a log-log plot of zipf    s law compared to real data from ap89. the predicted
relationship between id203 of occurrence and rank breaks down badly at high ranks.

group (   chemical   ) minus the rank of the last member of the previous group with
higher frequency (   summit   ), which is 1006     1002 = 4.

rank
1000
1001
1002
1003
1004
1005
1006
1007

word
concern
spoke
summit
bring
star
immediate
chemical
african

frequency
5,100
5,100
5,100
5,099
5,099
5,099
5,099
5,098

table 4.4. example word frequency ranking

 1e-008 1e-007 1e-006 1e-005 0.0001 0.001 0.01 0.1 1 1 10 100 1000 10000 100000 1e+006prrankzipfap8980

4 processing text

given that the number of words with frequency n is rn     rn+1 = k/n    
k/(n + 1) = k/n(n + 1), then the proportion of words with this frequency
can be found by dividing this number by the total number of words, which will
be the rank of the last word with frequency 1. the rank of the last word in the
vocabulary is k/1 = k. the proportion of words with frequency n, therefore, is
given by 1/n(n + 1). this formula predicts, for example, that 1/2 of the words in
the vocabulary will occur once. table 4.5 compares the predictions of this formula
with real data from a di   erent trec collection.

number of
occurrences

(n)
1
2
3
4
5
6
7
8
9
10

actual

predicted
actual
proportion proportion number of
(1/n(n+1))

0.500
0.167
0.083
0.050
0.033
0.024
0.018
0.014
0.011
0.009

0.402
0.132
0.069
0.046
0.032
0.024
0.019
0.016
0.014
0.012

words
204,357
67,082
35,083
23,271
16,332
12,421
9,766
8,200
6,907
5,893

table 4.5. proportions of words occurring n times in 336,310 documents from the
trec volume 3 corpus. the total vocabulary size (number of unique words) is 508,209.

4.2.1 vocabulary growth
another useful prediction related to word occurrence is vocabulary growth. as the
size of the corpus grows, new words occur. based on the assumption of a zipf dis-
tribution for words, we would expect that the number of new words that occur in
a given amount of new text would decrease as the size of the corpus increases. new
words will, however, always occur due to sources such as invented words (think
of all those drug names and start-up company names), spelling errors, product
numbers, people   s names, email addresses, and many others. the relationship be-
tween the size of the corpus and the size of the vocabulary was found empirically
by heaps (1978) to be:

4.2 text statistics

81

v = k    n(cid:12)

where v is the vocabulary size for a corpus of size n words, and k and    are pa-
rameters that vary for each collection. this is sometimes referred to as heaps    law.
typical values for k and    are often stated to be 10     k     100 and        0.5.
heaps    law predicts that the number of new words will increase very rapidly when
the corpus is small and will continue to increase indefinitely, but at a slower rate
for larger corpora. figure 4.3 shows a plot of vocabulary growth for the ap89
collection compared to a graph of heaps    law with k = 62.95 and    = 0.455.
clearly, heaps    law is a good fit. the parameter values are similar for many of
the other trec news collections. as an example of the accuracy of this predic-
tion, if the first 10,879,522 words of the ap89 collection are scanned, heaps   
law predicts that the number of unique words will be 100,151, whereas the actual
value is 100,024. predictions are much less accurate for small numbers of words
(< 1,000).

fig. 4.3. vocabulary growth for the trec ap89 collection compared to heaps    law

 0 20000 40000 60000 80000 100000 120000 140000 160000 180000 200000 0 5e+006 1e+007 1.5e+007 2e+007 2.5e+007 3e+007 3.5e+007 4e+007words in vocabularywords in collectionap89heaps 62.95, 0.45582

4 processing text

web-scale collections are considerably larger than the ap89 collection. the
ap89 collection contains about 40 million words, but the (relatively small) trec
web collection gov26 contains more than 20 billion words. with that many
words, it seems likely that the number of new words would eventually drop to
near zero and heaps    law would not be applicable. it turns out this is not the case.
figure 4.4 shows a plot of vocabulary growth for gov2 together with a graph of
heaps    law with k = 7.34 and    = 0.648. this data indicates that the number of
unique words continues to grow steadily even after reaching 30 million. this has
significant implications for the design of search engines, which will be discussed
in chapter 5. heaps    law provides a good fit for this data, although the parameter
values are very di   erent than those for other trec collections and outside the
boundaries established as typical with these and other smaller collections.

fig. 4.4. vocabulary growth for the trec gov2 collection compared to heaps    law

6 web pages crawled from websites in the .gov domain during early 2004. see section 8.2

for more details.

 0 5e+006 1e+007 1.5e+007 2e+007 2.5e+007 3e+007 3.5e+007 4e+007 4.5e+007 0 5e+009 1e+010 1.5e+010 2e+010 2.5e+010words in vocabularywords in collectiongov2heaps 7.34, 0.6484.2 text statistics

83

4.2.2 estimating collection and result set sizes

word occurrence statistics can also be used to estimate the size of the results from a
web search. all web search engines have some version of the query interface shown
in figure 4.5, where immediately after the query (   tropical fish aquarium    in this
case) and before the ranked list of results, an estimate of the total number of results
is given. this is typically a very large number, and descriptions of these systems
always point out that it is just an estimate. nevertheless, it is always included.

fig. 4.5. result size estimate for web search

to estimate the size of a result set, we first need to define    results.    for the
purposes of this estimation, a result is any document (or web page) that contains
all of the query words. some search applications will rank documents that do not
contain all the query words, but given the huge size of the web, this is usually
not necessary. if we assume that words occur independently of each other, then
the id203 of a document containing all the words in the query is simply the
product of the probabilities of the individual words occurring in a document. for
example, if there are three query words a, b, and c, then:

p (a     b     c) = p (a)    p (b)    p (c)

where p (a    b    c) is the joint id203, or the id203 that all three words
occur in a document, and p (a), p (b), and p (c) are the probabilities of each word
occurring in a document. a search engine will always have access to the number of
documents that a word occurs in (fa, fb, and fc),7 and the number of documents
in the collection (n), so these probabilities can easily be estimated as p (a) =
fa/n, p (b) = fb/n, and p (c) = fc/n. this gives us

fabc = n    fa/n    fb/n    fc/n = (fa    fb    fc)/n 2

where fabc is the estimated size of the result set.
7 note that these are document occurrence frequencies, not the total number of word oc-
currences (there may be many occurrences of a word in a document).

tropical fish aquarium       searchweb results  page 1 of 3,880,000 results 84

4 processing text

word(s)
tropical
fish
aquarium
breeding
tropical fish
tropical aquarium
tropical breeding
fish aquarium
fish breeding
aquarium breeding
tropical fish aquarium
tropical fish breeding

document
frequency
120,990
1,131,855
26,480
81,885
18,472
1,921
5,510
9,722
36,427
1,848
1,529
3,629

estimated
frequency

5,433
127
393
1,189
3,677
86
6
18

table 4.6. document frequencies and estimated frequencies for word combinations (as-
suming independence) in the gov2 web collection. collection size (n) is 25,205,179.

table 4.6 gives document occurrence frequencies for the words    tropical   ,
   fish   ,    aquarium   , and    breeding   , and for combinations of those words in the
trec gov2 web collection. it also gives the estimated size of the frequencies of
the combinations based on the independence assumption. clearly, this assump-
tion does not lead to good estimates for result size, especially for combinations of
three words. the problem is that the words in these combinations do not occur
independently of each other. if we see the word    fish    in a document, for example,
then the word    aquarium    is more likely to occur in this document than in one
that does not contain    fish   .

better estimates are possible if word co-occurrence information is also avail-
able from the search engine. obviously, this would give exact answers for two-
word queries. for longer queries, we can improve the estimate by not assuming
independence. in general, for three words

p (a     b     c) = p (a     b)    p (c|(a     b))

where p (a    b) is the id203 that the words a and b co-occur in a document,
and p (c|(a   b)) is the id203 that the word c occurs in a document given that
the words a and b occur in the document.8 if we have co-occurrence information,
8 this is called a id155.

4.2 text statistics

85

we can approximate this id203 using either p (c|a) or p (c|b), whichever is
the largest. for the example query    tropical fish aquarium    in table 4.6, this means
we estimate the result set size by multiplying the number of documents containing
both    tropical    and    aquarium    by the id203 that a document contains    fish   
given that it contains    aquarium   , or:

ftropical   f ish   aquarium = ftropical   aquarium    ff ish   aquarium/faquarium

= 1921    9722/26480 = 705

similarly, for the query    tropical fish breeding   :

ftropical   f ish   breeding = ftropical   breeding    ff ish   breeeding/fbreeding

= 5510    36427/81885 = 2451

these estimates are much better than the ones produced assuming indepen-
dence, but they are still too low. rather than storing even more information, such
as the number of occurrences of word triples, it turns out that reasonable esti-
mates of result size can be made using just word frequency and the size of the cur-
rent result set. search engines estimate the result size because they do not rank all
the documents that contain the query words. instead, they rank a much smaller
subset of the documents that are likely to be the most relevant. if we know the
proportion of the total documents that have been ranked (s) and the number of
documents found that contain all the query words (c), we can simply estimate
the result size as c/s, which assumes that the documents containing all the words
are distributed uniformly.9 the proportion of documents processed is measured
by the proportion of the documents containing the least frequent word that have
been processed, since all results must contain that word.

for example, if the query    tropical fish aquarium    is used to rank gov2 doc-
uments in the galago search engine, after processing 3,000 out of the 26,480 doc-
uments that contain    aquarium   , the number of documents containing all three
words is 258. this gives a result size estimate of 258/(3,000    26,480) = 2,277.
after processing just over 20% of the documents, the estimate is 1,778 (compared
to the actual figure of 1,529). for the query    tropical fish breeding   , the estimates
after processing 10% and 20% of the documents that contain    breeding    are 4,076

9 we are also assuming document-at-a-time processing, where the inverted lists for all
query words are processed at the same time, giving complete document scores (see
chapter 5).

86

4 processing text

and 3,762 (compared to 3,629). these estimates, as well as being quite accurate,
do not require knowledge of the total number of documents in the collection.

estimating the total number of documents stored in a search engine is, in fact,
of significant interest to both academia (how big is the web?) and business (which
search engine has better coverage of the web?). a number of papers have been
written about techniques to do this, and one of these is based on the concept
of word independence that we used before. if a and b are two words that occur
independently, then

and

fab/n = fa/n    fb/n
n = (fa    fb)/fab

to get a reasonable estimate of n, the two words should be independent and,
as we have seen from the examples in table 4.6, this is often not the case. we can
be more careful about the choice of query words, however. for example, if we use
the word    lincoln    (document frequency 771,326 in gov2), we would expect
the words in the query    tropical lincoln    to be more independent than the word
pairs in table 4.6 (since the former are less semantically related). the document
frequency of    tropical lincoln    in gov2 is 3,018, which means we can estimate
the size of the collection as n = (120,990    771,326)/3,018 = 30,922,045. this
is quite close to the actual number of 25,205,179.

4.3 document parsing

4.3.1 overview

document parsing involves the recognition of the content and structure of text
documents. the primary content of most documents is the words that we were
counting and modeling using the zipf distribution in the previous section. rec-
ognizing each word occurrence in the sequence of characters in a document is
called tokenizing or lexical analysis. apart from these words, there can be many
other types of content in a document, such as metadata, images, graphics, code,
and tables. as mentioned in chapter 2, metadata is information about a doc-
ument that is not part of the text content. metadata content includes docu-
ment attributes such as date and author, and, most importantly, the tags that are
used by markup languages to identify document components. the most popular

4.3 document parsing

87

markup languages are html (hypertext markup language) and xml (exten-
sible markup language).

the parser uses the tags and other metadata recognized in the document to
interpret the document   s structure based on the syntax of the markup language
(syntactic analysis) and to produce a representation of the document that includes
both the structure and content. for example, an html parser interprets the
structure of a web page as specified using html tags, and creates a document
object model (dom) representation of the page that is used by a web browser.
in a search engine, the output of a document parser is a representation of the con-
tent and structure that will be used for creating indexes. since it is important for
a search index to represent every document in a collection, a document parser for
a search engine is often more tolerant of syntax errors than parsers used in other
applications.

in the first part of our discussion of document parsing, we focus on the recog-
nition of the tokens, words, and phrases that make up the content of the docu-
ments. in later sections, we discuss separately the important topics related to doc-
ument structure, namely markup, links, and extraction of structure from the text
content.

4.3.2 tokenizing

tokenizing is the process of forming words from the sequence of characters in
a document. in english text, this appears to be simple. in many early systems, a
   word    was defined as any sequence of alphanumeric characters of length 3 or
more, terminated by a space or other special character. all uppercase letters were
also converted to lowercase.10 this means, for example, that the text

bigcorp   s 2007 bi-annual report showed profits rose 10%.

would produce the following sequence of tokens:
bigcorp 2007 annual report showed profits rose

although this simple tokenizing process was adequate for experiments with small
test collections, it does not seem appropriate for most search applications or even
experiments with trec collections, because too much information is discarded.
some examples of issues involving tokenizing that can have significant impact on
the e   ectiveness of search are:
10 this is sometimes referred to as case folding, case id172, or downcasing.

88

4 processing text

    small words (one or two characters) can be important in some queries, usually
in combinations with other words. for example, xp, ma, pm, ben e king, el paso,
master p, gm, j lo, world war ii.11

    both hyphenated and non-hyphenated forms of many words are common. in
some cases the hyphen is not needed. for example, e-bay, wal-mart, active-x,
cd-rom, t-shirts. at other times, hyphens should be considered either as part of
the word or a word separator. for example, winston-salem, mazda rx-7, e-cards,
pre-diabetes, t-mobile, spanish-speaking.

    special characters are an important part of the tags, urls, code, and other

important parts of documents that must be correctly tokenized.

    capitalized words can have di   erent meaning from lowercase words. for ex-

ample,    bush    and    apple   .

    apostrophes can be a part of a word, a part of a possessive, or just a mistake.
for example, rosie o   donnell, can   t, don   t, 80   s, 1890   s, men   s straw hats, master   s
degree, england   s ten largest cities, shriner   s.

    numbers can be important, including decimals. for example, nokia 3250, top
10 courses, united 93, quicktime 6.5 pro, 92.3 the beat, 288358 (yes, this was a
real query; it   s a patent number).

    periods can occur in numbers, abbreviations (e.g.,    i.b.m.   ,    ph.d.   ), urls,

ends of sentences, and other situations.
from these examples, tokenizing seems more complicated than it first appears.
the fact that these examples come from queries also emphasizes that the text pro-
cessing for queries must be the same as that used for documents. if di   erent to-
kenizing processes are used for queries and documents, many of the index terms
used for documents will simply not match the corresponding terms from queries.
mistakes in tokenizing become obvious very quickly through retrieval failures.

to be able to incorporate the range of language processing required to make
matching e   ective, the tokenizing process should be both simple and flexible.
one approach to doing this is for the first pass of tokenizing to focus entirely
on identifying markup or tags in the document. this could be done using a tok-
enizer and parser designed for the specific markup language used (e.g., html),
but it should accommodate syntax errors in the structure, as mentioned previ-
ously. a second pass of tokenizing can then be done on the appropriate parts of
the document structure. some parts that are not used for searching, such as those
containing html code, will be ignored in this pass.

11 these and other examples were taken from a small sample of web queries.

4.3 document parsing

89

given that nearly everything in the text of a document can be important for
some query, the tokenizing rules have to convert most of the content to search-
able tokens. instead of trying to do everything in the tokenizer, some of the more
di   cult issues, such as identifying word variants or recognizing that a string is
a name or a date, can be handled by separate processes, including id30, in-
formation extraction, and query transformation. information extraction usually
requires the full form of the text as input, including capitalization and punctua-
tion, so this information must be retained until extraction has been done. apart
from this restriction, capitalization is rarely important for searching, and text can
be reduced to lowercase for indexing. this does not mean that capitalized words
are not used in queries. they are, in fact, used quite often, but in queries where
the capitalization does not reduce ambiguity and so does not impact e   ective-
ness. words such as    apple    that are often used in examples (but not so often in
real queries) can be handled by query reformulation techniques (chapter 6) or
simply by relying on the most popular pages (section 4.5).

if we take the view that complicated issues are handled by other processes, the
most general strategy for hyphens, apostrophes, and periods would be to treat
them as word terminators (like spaces). it is important that all the tokens pro-
duced are indexed, including single characters such as    s    and    o   . this will mean,
for example, that the query12    o   connor    is equivalent to    o connor   ,    bob   s    is
equivalent to    bob s   , and    rx-7    is equivalent to    rx 7   . note that this will also
mean that a word such as    rx7    will be a di   erent token than    rx-7    and therefore
will be indexed separately. the task of relating the queries rx 7, rx7, and rx-7 will
then be handled by the query transformation component of the search engine.

on the other hand, if we rely entirely on the query transformation component
to make the appropriate connections or id136s between words, there is the risk
that e   ectiveness could be lowered, particularly in applications where there is not
enough data for reliable id183. in these cases, more rules can be incor-
porated into the tokenizer to ensure that the tokens produced by the query text
will match the tokens produced from document text. for example, in the case
of trec collections, a rule that tokenizes all words containing apostrophes by
the string without the apostrophe is very e   ective. with this rule,    o   connor   
would be tokenized as    oconnor    and    bob   s    would produce the token    bobs   .
another e   ective rule for trec collections is to tokenize all abbreviations con-
12 we assume the common syntax for web queries where    <words>    means match exactly

the phrase contained in the quotes.

90

4 processing text

taining periods as the string without periods. an abbreviation in this case is any
string of alphabetic single characters separated by periods. this rule would tok-
enize    i.b.m.    as    ibm   , but    ph.d.    would still be tokenized as    ph d   .

in summary, the most general tokenizing process will involve first identify-
ing the document structure and then identifying words in text as any sequence
of alphanumeric characters, terminated by a space or special character, with ev-
erything converted to lowercase. this is not much more complicated than the
simple process we described at the start of the section, but it relies on informa-
tion extraction and query transformation to handle the di   cult issues. in many
cases, additional rules are added to the tokenizer to handle some of the special
characters, to ensure that query and document tokens will match.

4.3.3 stopping

human language is filled with function words: words that have little meaning
apart from other words. the most popular      the,       a,       an,       that,    and    those      are
determiners. these words are part of how we describe nouns in text, and express
concepts like location or quantity. prepositions, such as    over,       under,       above,   
and    below,    represent relative position between two nouns.

two properties of these function words cause us to want to treat them in a
special way in text processing. first, these function words are extremely common.
table 4.2 shows that nearly all of the most frequent words in the ap89 collection
fall into this category. keeping track of the quantity of these words in each docu-
ment requires a lot of disk space. second, both because of their commonness and
their function, these words rarely indicate anything about document relevance on
their own. if we are considering individual words in the retrieval process and not
phrases, these function words will help us very little.

in information retrieval, these function words have a second name: stopwords.
we call them stopwords because text processing stops when one is seen, and they
are thrown out. throwing out these words decreases index size, increases retrieval
e   ciency, and generally improves retrieval e   ectiveness.

constructing a stopword list must be done with caution. removing too many
words will hurt retrieval e   ectiveness in particularly frustrating ways for the user.
for instance, the query    to be or not to be    consists entirely of words that are usu-
ally considered stopwords. although not removing stopwords may cause some
problems in ranking, removing stopwords can cause perfectly valid queries to re-
turn no results.

4.3 document parsing

91

a stopword list can be constructed by simply using the top n (e.g., 50) most
frequent words in a collection. this can, however, lead to words being included
that are important for some queries. more typically, either a standard stopword
list is used,13 or a list of frequent words and standard stopwords is manually edited
to remove any words that may be significant for a particular application. it is also
possible to create stopword lists that are customized for specific parts of the doc-
ument structure (also called fields). for example, the words    click   ,    here   , and
   privacy    may be reasonable stopwords to use when processing anchor text.

if storage space requirements allow, it is best to at least index all words in the
documents. if stopping is required, the stopwords can always be removed from
queries. by keeping the stopwords in the index, there will be a number of possi-
ble ways to execute a query with stopwords in it. for instance, many systems will
remove stopwords from a query unless the word is preceded by a plus sign (+). if
keeping stopwords in an index is not possible because of space requirements, as
few as possible should be removed in order to maintain maximum flexibility.

4.3.4 id30

part of the expressiveness of natural language comes from the huge number of
ways to convey a single idea. this can be a problem for search engines, which rely
on matching words to find relevant documents. instead of restricting matches to
words that are identical, a number of techniques have been developed to allow a
search engine to match words that are semantically related. id30, also called
conflation, is a component of text processing that captures the relationships be-
tween di   erent variations of a word. more precisely, id30 reduces the dif-
ferent forms of a word that occur because of inflection (e.g., plurals, tenses) or
derivation (e.g., making a verb into a noun by adding the su   x -ation) to a com-
mon stem.

suppose you want to search for news articles about mark spitz   s olympic
swimming career. you might type    mark spitz swimming    into a search engine.
however, many news articles are usually summaries of events that have already
happened, so they are likely to contain the word    swam    instead of    swimming.   
it is the job of the stemmer to reduce    swimming    and    swam    to the same stem
(probably    swim   ) and thereby allow the search engine to determine that there is
a match between these two words.
13 such as the one distributed with the lemur toolkit and included with galago.

92

4 processing text

in general, using a stemmer for search applications with english text produces
a small but noticeable improvement in the quality of results. in applications in-
volving highly inflected languages, such as arabic or russian, id30 is a cru-
cial part of e   ective search.

there are two basic types of stemmers: algorithmic and dictionary-based. an
algorithmic stemmer uses a small program to decide whether two words are re-
lated, usually based on knowledge of word su   xes for a particular language. by
contrast, a dictionary-based stemmer has no logic of its own, but instead relies on
pre-created dictionaries of related terms to store term relationships.

the simplest kind of english algorithmic stemmer is the su   x-s stemmer. this
kind of stemmer assumes that any word ending in the letter    s    is plural, so cakes
    cake, dogs     dog. of course, this rule is not perfect. it cannot detect many
plural relationships, like    century    and    centuries   . in very rare cases, it detects a
relationship where it does not exist, such as with    i    and    is   . the first kind of error
is called a false negative, and the second kind of error is called a false positive.14

more complicated algorithmic stemmers reduce the number of false negatives
by considering more kinds of su   xes, such as -ing or -ed. by handling more su   x
types, the stemmer can find more term relationships; in other words, the false
negative rate is reduced. however, the false positive rate (finding a relationship
where none exists) generally increases.

the most popular algorithmic stemmer is the porter stemmer.15 this has been
used in many information retrieval experiments and systems since the 1970s, and
a number of implementations are available. the stemmer consists of a number
of steps, each containing a set of rules for removing su   xes. at each step, the rule
for the longest applicable su   x is executed. some of the rules are obvious, whereas
others require some thought to work out what they are doing. as an example, here
are the first two parts of step 1 (of 5 steps):

step 1a:
- replace sses by ss (e.g., stresses     stress).
- delete s if the preceding word part contains a vowel not immediately be-
fore the s (e.g., gaps     gap but gas     gas).
(e.g., ties     tie, cries     cri).

- replace ied or ies by i if preceded by more than one letter, otherwise by ie

14 these terms are used in any binary decision process to describe the two types of errors.

this includes evaluation (chapter 8) and classification (chapter 9).

15 http://tartarus.org/martin/porterstemmer/

4.3 document parsing

93

if su   x is us or ss do nothing (e.g., stress     stress).

-
step 1b:
- replace eed, eedly by ee if it is in the part of the word after the first non-

vowel following a vowel (e.g., agreed     agree, feed     feed).
- delete ed, edly, ing, ingly if the preceding word part contains a vowel, and
then if the word ends in at, bl, or iz add e (e.g., fished     fish, pirating    
pirate), or if the word ends with a double letter that is not ll, ss, or zz, remove
the last letter (e.g., falling    fall, dripping     drip), or if the word is short, add
e (e.g., hoping     hope).

- whew!
the porter stemmer has been shown to be e   ective in a number of trec eval-
uations and search applications. it is di   cult, however, to capture all the subtleties
of a language in a relatively simple algorithm. the original version of the porter
stemmer made a number of errors, both false positives and false negatives. table
4.7 shows some of these errors. it is easy to imagine how confusing    execute    with
   executive    or    organization    with    organ    could cause significant problems in
the ranking. a more recent form of the stemmer (called porter2)16 fixes some of
these problems and provides a mechanism to specify exceptions.

false negatives
european/europe
cylinder/cylindrical

false positives
organization/organ
generalization/generic
numerical/numerous matrices/matrix
urgency/urgent
policy/police
create/creation
university/universe
addition/additive
analysis/analyses
useful/usefully
negligible/negligent
noise/noisy
execute/executive
past/paste
decompose/decomposition
sparse/sparsity
ignore/ignorant
resolve/resolution
special/specialized
head/heading
triangle/triangular

table 4.7. examples of errors made by the original porter stemmer. false positives are
pairs of words that have the same stem. false negatives are pairs that have di   erent stems.

16 http://snowball.tartarus.org

94

4 processing text

a dictionary-based stemmer provides a di   erent approach to the problem of
id30 errors. instead of trying to detect word relationships from letter pat-
terns, we can store lists of related words in a large dictionary. since these word
lists can be created by humans, we can expect that the false positive rate will be
very low for these words. related words do not even need to look similar; a dic-
tionary stemmer can recognize that    is,       be,    and    was    are all forms of the same
verb. unfortunately, the dictionary cannot be infinitely long, so it cannot react
automatically to new words. this is an important problem since language is con-
stantly evolving. it is possible to build stem dictionaries automatically by statistical
analysis of a text corpus. since this is particularly useful when id30 is used
for id183, we discuss this technique in section 6.2.1.

another strategy is to combine an algorithmic stemmer with a dictionary-
based stemmer. typically, irregular words such as the verb    to be    are the oldest
in the language, while new words follow more regular grammatical conventions.
this means that newly invented words are likely to work well with an algorith-
mic stemmer. a dictionary can be used to detect relationships between common
words, and the algorithmic stemmer can be used for unrecognized words.

a well-known example of this hybrid approach is the krovetz stemmer (kro-
vetz, 1993). this stemmer makes constant use of a dictionary to check whether
the word is valid. the dictionary in the krovetz stemmer is based on a general
english dictionary but also uses exceptions that are generated manually. before
being stemmed, the dictionary is checked to see whether a word is present; if it
is, it is either left alone (if it is in the general dictionary) or stemmed based on
the exception entry. if the word is not in the dictionary, it is checked for a list
of common inflectional and derivational su   xes. if one is found, it is removed
and the dictionary is again checked to see whether the word is present. if it is
not found, the ending of the word may be modified based on the ending that was
removed. for example, if the ending -ies is found, it is replaced by -ie and checked
in the dictionary. if it is found in the dictionary, the stem is accepted; otherwise
the ending is replaced by y. this will result in calories     calorie, for example. the
su   xes are checked in a sequence (for example, plurals before -ion endings), so
multiple su   xes may be removed.

the krovetz stemmer has a lower false positive rate than the porter stemmer,
but also tends to have a higher false negative rate, depending on the size of the ex-
ception dictionaries. overall, the e   ectiveness of the two stemmers is comparable
when used in search evaluations. the krovetz stemmer has the additional advan-
tage of producing stems that, in most cases, are full words, whereas the porter

4.3 document parsing

95

fig. 4.6. comparison of stemmer output for a trec query. stopwords have also been
removed.

stemmer often produces stems that are word fragments. this is a concern if the
stems are used in the search interface.

figure 4.6 compares the output of the porter and krovetz stemmers on the
text of a trec query. the output of the krovetz stemmer is similar in terms of
which words are reduced to the same stems, although    marketing    is not reduced
to    market    because it was in the dictionary. the stems produced by the krovetz
stemmer are mostly words. the exception is the stem    agrochemic   , which oc-
curred because    agrochemical    was not in the dictionary. note that text process-
ing in this example has removed stopwords, including single characters. this re-
sulted in the removal of    u.s.    from the text, which could have significant conse-
quences for some queries. this can be handled by better id121 or informa-
tion extraction, as we discuss in section 4.6.

as in the case of stopwords, the search engine will have more flexibility to an-
swer a broad range of queries if the document words are not stemmed but instead
are indexed in their original form. id30 can then be done as a type of query
expansion, as explained in section 6.2.1. in some applications, both the full words
and their stems are indexed, in order to provide both flexibility and e   cient query
processing times.

we mentioned earlier that id30 can be particularly important for some
languages, and have virtually no impact in others. incorporating language-specific

original text: document will describe marketing strategies carried out by u.s. companies for their agricultural chemicals, report predictions for market share of such chemicals, or report market statistics for agrochemicals, pesticide, herbicide, fungicide, insecticide, fertilizer, predicted sales, market share, stimulate demand, price cut, volume of sales.  porter stemmer:  document describ market strategi carri compani agricultur chemic report predict market share chemic report market statist agrochem pesticid herbicid fungicid insecticid fertil predict sale market share stimul demand price cut volum sale  krovetz stemmer:  document describe marketing strategy carry company agriculture chemical report prediction market share chemical report market statistic agrochemic pesticide herbicide fungicide insecticide fertilizer predict sale stimulate demand price cut volume sale 96

4 processing text

id30 algorithms is one of the most important aspects of customizing, or in-
ternationalizing, a search engine for multiple languages. we discuss other aspects
of internationalization in section 4.7, but focus on the id30 issues here.

as an example, table 4.8 shows some of the arabic words derived from the
same root. a id30 algorithm that reduced arabic words to their roots would
clearly not work (there are less than 2,000 roots in arabic), but a broad range of
prefixes and su   xes must be considered. highly inflectional languages like ara-
bic have many word variants, and id30 can make a large di   erence in the
accuracy of the ranking. an arabic search engine with high-quality id30
can be more than 50% more e   ective, on average, at finding relevant documents
than a system without id30. in contrast, improvements for an english search
engine vary from less than 5% on average for large collections to about 10% for
small, domain-specific collections.

kitab
kitabi
alkitab
kitabuki
kitabuka
kitabuhu
kataba
maktaba
maktab

a book
my book
the book
your book (f )
your book (m)
his book
to write
library, bookstore
o   ce

table 4.8. examples of words with the arabic root ktb

fortunately, stemmers for a number of languages have already been developed
and are available as open source software. for example, the porter stemmer is avail-
able in french, spanish, portuguese, italian, romanian, german, dutch, swedish,
norwegian, danish, russian, finnish, hungarian, and turkish.17 in addition, the
statistical approach to building a stemmer that is described in section 6.2.1 can be
used when only a text corpus is available.

17 http://snowball.tartarus.org/

4.3 document parsing

97

4.3.5 phrases and id165s
phrases are clearly important in information retrieval. many of the two- and
three-word queries submitted to search engines are phrases, and finding docu-
ments that contain those phrases will be part of any e   ective ranking algorithm.
for example, given the query    black sea   , documents that contain that phrase are
much more likely to be relevant than documents containing text such as    the sea
turned black   . phrases are more precise than single words as topic descriptions
(e.g.,    tropical fish    versus    fish   ) and usually less ambiguous (e.g.,    rotten ap-
ple    versus    apple   ). the impact of phrases on retrieval can be complex, however.
given a query such as    fishing supplies   , should the retrieved documents contain
exactly that phrase, or should they get credit for containing the words    fish   ,    fish-
ing   , and    supplies    in the same paragraph, or even the same document? the de-
tails of how phrases a   ect ranking will depend on the specific retrieval model that
is incorporated into the search engine, so we will defer this discussion until chap-
ter 7. from the perspective of text processing, the issue is whether phrases should
be identified at the same time as tokenizing and id30, so that they can be
indexed for faster query processing.

there are a number of possible definitions of a phrase, and most of them have
been studied in retrieval experiments over the years. since a phrase has a grammat-
ical definition, it seems reasonable to identify phrases using the syntactic structure
of sentences. the definition that has been used most frequently in information re-
trieval research is that a phrase is equivalent to a simple noun phrase. this is often
restricted even further to include just sequences of nouns, or adjectives followed
by nouns. phrases defined by these criteria can be identified using a part-of-speech
(pos) tagger. a pos tagger marks the words in a text with labels corresponding
to the part-of-speech of the word in that context. taggers are based on statistical
or rule-based approaches and are trained using large corpora that have been man-
ually labeled. typical tags that are used to label the words include nn (singular
noun), nns (plural noun), vb (verb), vbd (verb, past tense), vbn (verb, past
participle), in (preposition), jj (adjective), cc (conjunction, e.g.,    and   ,    or   ),
prp (pronoun), and md (modal auxiliary, e.g.,    can   ,    will   ).

figure 4.7 shows the output of a pos tagger for the trec query text used
in figure 4.6. this example shows that the tagger can identify phrases that are
sequences of nouns, such as    marketing/nn strategies/nns   , or adjectives fol-
lowed by nouns, such as    agricultural/jj chemicals/nns   . taggers do, however,
make mistakes. the words    predicted/vbn sales/nns    would not be identified
as a noun phrase, because    predicted    is tagged as a verb.

98

4 processing text

fig. 4.7. output of a pos tagger for a trec query

table 4.9 shows the high-frequency simple noun phrases from a trec cor-
pus consisting mainly of news stories and a corpus of comparable size consisting
of all the 1996 patents issued by the united states patent and trademark of-
fice (pto). the phrases were identified by id52. the frequencies of the
example phrases indicate that phrases are used more frequently in the pto col-
lection, because patents are written in a formal, legal style with considerable repe-
tition. there were 1,100,000 phrases in the trec collection that occurred more
than five times, and 3,700,000 phrases in the pto collection. many of the trec
phrases are proper nouns, such as    los angeles    or    european union   , or are topics
that will be important for retrieval, such as    peace process    and    human rights   .
two phrases are associated with the format of the documents (   article type   ,    end
recording   ). on the other hand, most of the high-frequency phrases in the pto
collection are standard terms used to describe all patents, such as   present inven-
tion    and    preferred embodiment   , and relatively few are related to the content
of the patents, such as    carbon atoms    and    ethyl acetate   . one of the phrases,
   group consisting   , was the result of a frequent tagging error.

although id52 produces reasonable phrases and is used in a number
of applications, in general it is too slow to be used as the basis for phrase index-
ing of large collections. there are simpler and faster alternatives that are just as
e   ective. one approach is to store word position information in the indexes and
use this information to identify phrases only when a query is processed. this pro-
vides considerable flexibility in that phrases can be identified by the user or by
using id52 on the query, and they are not restricted to adjacent groups of

original text: document will describe marketing strategies carried out by u.s. companies for their agricultural chemicals, report predictions for market share of such chemicals, or report market statistics for agrochemicals, pesticide, herbicide, fungicide, insecticide, fertilizer, predicted sales, market share, stimulate demand, price cut, volume of sales.  brill tagger: document/nn will/md describe/vb marketing/nn strategies/nns carried/vbd out/in by/in u.s./nnp companies/nns for/in their/prp agricultural/jj chemicals/nns ,/, report/nn predictions/nns for/in market/nn share/nn of/in such/jj chemicals/nns ,/, or/cc report/nn market/nn statistics/nns for/in agrochemicals/nns ,/, pesticide/nn ,/, herbicide/nn ,/, fungicide/nn ,/, insecticide/nn ,/, fertilizer/nn ,/, predicted/vbn sales/nns ,/, market/nn share/nn ,/, stimulate/vb demand/nn ,/, price/nn cut/nn ,/, volume/nn of/in sales/nns ./. 4.3 document parsing

99

trec data
frequency
65824
61327
33864
18062
17788
17308
15513
15009
12869
12799
12067
10811
9912
8127
7640
7620
7524
7436
7362
7086
6792
6348
6157
5955
5837

phrase
united states
article type
los angeles
hong kong
north korea
new york
san diego
orange county
prime minister
first time
soviet union
russian federation
united nations
southern california
south korea
end recording
european union
south africa
san francisco
news conference
city council
middle east
peace process
human rights
white house

patent data
frequency
975362
191625
147352
95097
87903
81809
78458
75850
66407
59828
58724
56715
54619
54117
52195
52003
46299
41694
40554
37911
35827
34881
33947
32338
30193

phrase
present invention
u.s. pat
preferred embodiment
carbon atoms
group consisting
room temperature
seq id
brief description
prior art
perspective view
first embodiment
reaction mixture
detailed description
ethyl acetate
example 1
block diagram
second embodiment
accompanying drawings
output signal
first end
second end
appended claims
distal end
cross-sectional view
outer surface

table 4.9. high-frequency noun phrases from a trec collection and u.s. patents from
1996

words. the identification of syntactic phrases is replaced by testing word proxim-
ity constraints, such as whether two words occur within a specified text window.
we describe position indexing in chapter 5 and retrieval models that exploit word
proximity in chapter 7.

in applications with large collections and tight constraints on response time,
such as web search, testing word proximities at query time is also likely to be too
slow. in that case, we can go back to identifying phrases in the documents dur-

100

4 processing text

ing text processing, but use a much simpler definition of a phrase: any sequence
of n words. this is also known as an id165. sequences of two words are called
bigrams, and sequences of three words are called trigrams. single words are called
unigrams. id165s have been used in many text applications and we will mention
them again frequently in this book, particularly in association with language mod-
els (section 7.3). in this discussion, we are focusing on word id165s, but character
id165s are also used in applications such as ocr, where the text is    noisy    and
word matching can be di   cult (section 11.6). character id165s are also used
for indexing languages such as chinese that have no word breaks (section 4.7).
id165s, both character and word, are generated by choosing a particular value
for n and then moving that    window    forward one unit (character or word) at
a time. in other words, id165s overlap. for example, the word    tropical    con-
tains the following character bigrams: tr, ro, op, pi, ic, ca, and al. indexes based on
id165s are obviously larger than word indexes.

the more frequently a word id165 occurs, the more likely it is to correspond
to a meaningful phrase in the language. id165s of all lengths form a zipf distri-
bution, with a few common phrases occurring very frequently and a large number
occurring with frequency 1. in fact, the rank-frequency data for id165s (which
includes single words) fits the zipf distribution better than words alone. some of
the most common id165s will be made up of stopwords (e.g.,    and the   ,    there
is   ) and could be ignored, although as with words, we should be cautious about
discarding information. our previous example query    to be or not to be    could
certainly make use of id165s. we could potentially index all id165s in a doc-
ument text up to a specific length and make them available to the ranking algo-
rithm. this would seem to be an extravagant use of indexing time and disk space
because of the large number of possible id165s. a document containing 1,000
words, for example, would contain 3,990 instances of word id165s of length
2     n     5. many web search engines, however, use id165 indexing because
it provides a fast method of incorporating phrase features in the ranking.

google recently made available a file of id165s derived from web pages.18 the
statistics for this sample are shown in table 4.10. an analysis of id165s on the
web (yang et al., 2007) found that    all rights reserved    was the most frequent
trigram in english, whereas    limited liability corporation    was the most frequent
in chinese. in both cases, this was due to the large number of corporate sites, but

18 http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-

you.html

4.4 document structure and markup

101

it also indicates that id165s are not dominated by common patterns of speech
such as    and will be   .

number of tokens:
number of sentences:
number of unigrams:
number of bigrams:
number of trigrams:
number of fourgrams:
number of fivegrams:

1,024,908,267,229
95,119,665,584
13,588,391
314,843,401
977,069,902
1,313,818,354
1,176,470,663

table 4.10. statistics for the google id165 sample

4.4 document structure and markup

in database applications, the fields or attributes of database records are a critical
part of searching. queries are specified in terms of the required values of these
fields. in some text applications, such as email or literature search, fields such as
author and date will have similar importance and will be part of the query specifi-
cation. in the case of web search, queries usually do not refer to document struc-
ture or fields, but that does not mean that this structure is unimportant. some
parts of the structure of web pages, indicated by html markup, are very signifi-
cant features used by the ranking algorithm. the document parser must recognize
this structure and make it available for indexing.

as an example, figure 4.8 shows part of a web page for a wikipedia19 entry.
the page has some obvious structure that could be used in a ranking algorithm.
the main heading for the page,    tropical fish   , indicates that this phrase is particu-
larly important. the same phrase is also in bold and italics in the body of the text,
which is further evidence of its importance. other words and phrases are used as
the anchor text for links and are likely to be good terms to represent the content
of the page.

the html source for this web page (figure 4.9) shows that there is even
more structure that should be represented for search. each field or element in
html is indicated by a start tag (such as <h1>) and an optional end tag (e.g.,

19 the web encyclopedia, http://en.wikipedia.org/.

102

4 processing text

fig. 4.8. part of a web page from wikipedia

</h1>).20 elements can also have attributes (with values), given by attribute_name
=    value    pairs. the <head> element of an html document contains metadata
that is not displayed by a browser. the metadata element for keywords (<meta
name=   keywords   ) gives a list of words and phrases that can be used as additional
content terms for the page. in this case, these are the titles of other wikipedia
pages. the <title> metadata element gives the title for the page (which is di   erent
from the main heading).

the <body> element of the document contains the content that is displayed.
the main heading is indicated by the <h1> tag. other headings, of di   erent sizes
and potentially di   erent importance, would be indicated by <h2> through <h6>
tags. terms that should be displayed in bold or italic are indicated by <b> and
<i> tags. unlike typical database fields, these tags are primarily used for format-
ting and can occur many times in a document. they can also, as we have said, be
interpreted as a tag indicating a word or phrase of some importance.

links, such as <a href=   /wiki/fish    title=   fish   >fish</a>, are very common. they
are the basis of link analysis algorithms such as id95 (brin & page, 1998), but
also define the anchor text. links and anchor text are of particular importance to
web search and will be described in the next section. the title attribute for a link
is used to provide extra information about that link, although in our example it
is the words in the last part of the url for the associated wikipedia page. web
search engines also make use of the url of a page as a source of additional meta-
data. the url for this page is:

http://en.wikipedia.org/wiki/tropical_fish

20 in xml the end tag is not optional.

tropical fish from wikipedia, the free encyclopedia tropical fish include fish found in tropical environments around the world, including bothfreshwater and salt water species. fishkeepers often use the term tropical fish to refer only those requiring fresh water, with saltwater tropical fish referred to as marinefish.tropical fish are popular aquarium fish , due to their often bright coloration. in freshwater fish, this coloration typically derives from iridescence, while salt water fish are generally pigmented.4.4 document structure and markup

103

the fact that the words    tropical    and    fish    occur in the url will increase the
importance of those words for this page. the depth of a url (i.e., the number
of directories deep the page is) can also be important. for example, the url
www.ibm.com is more likely to be the home page for ibm than a page with the
url:

www.pcworld.com/businesscenter/article/698/ibm_buys_apt!

fig. 4.9. html source for example wikipedia page

in html, the element types are predefined and are the same for all docu-
ments. xml, in contrast, allows each application to define what the element types
are and what tags are used to represent them. xml documents can be described
by a schema, similar to a database schema. xml elements, consequently, are more
closely tied to the semantics of the data than html elements. search applica-

<html> <head><meta name="keywords" content="tropical fish, airstone, albinism, algae eater, aquarium, aquarium fish feeder, aquarium furniture, aquascaping, bath treatment (fishkeeping),berlin method, biotope" />      <title>tropical fish - wikipedia, the free encyclopedia</title> </head><body>    <h1 class="firstheading">tropical fish</h1>       <p><b>tropical fish</b> include <a href="/wiki/fish" title="fish">fish</a> found in <a href="/wiki/tropics" title="tropics">tropical</a> environments around the world, including both <a href="/wiki/fresh_water" title="fresh water">freshwater</a> and <a href="/wiki/sea_water" title="sea water">salt water</a> species. <a href="/wiki/fishkeeping" title="fishkeeping">fishkeepers</a> often use the term <i>tropical fish</i> to refer only those requiring fresh water, with saltwater tropical fish referred to as <i><a href="/wiki/list_of_marine_aquarium_fish_species" title="list of marine aquarium fish species">marine fish</a></i>.</p> <p>tropical fish are popular <a href="/wiki/aquarium" title="aquarium">aquarium</a> fish , due to their often bright coloration. in freshwater fish, this coloration typically derives from <a href="/wiki/iridescence" title="iridescence">iridescence</a>, while salt water fish are generally <a href="/wiki/pigment" title="pigment">pigmented</a>.</p>    </body></html> 104

4 processing text

tions often use xml to record semantic annotations in the documents that are
produced by information extraction techniques, as described in section 4.6. a
document parser for these applications would record the annotations, along with
the other document structure, and make them available for indexing.

the query language xquery21 has been defined by the database community
for searching structured data described using xml. xquery supports queries that
specify both structural and content constraints, which raises the issue of whether
a database or information retrieval approach is better for building a search engine
for xml data. we discuss this topic in more detail in section 11.4, but the gen-
eral answer is that it will depend on the data, the application, and the user needs.
for xml data that contains a substantial proportion of text, the information re-
trieval approach is superior. in chapter 7, we will describe retrieval models that
are designed for text documents that contain both structure and metadata.

4.5 link analysis

links connecting pages are a key component of the web. links are a power-
ful navigational aid for people browsing the web, but they also help search en-
gines understand the relationships between the pages. these detected relation-
ships help search engines rank web pages more e   ectively. it should be remem-
bered, however, that many document collections used in search applications such
as desktop or enterprise search either do not have links or have very little link
structure. for these collections, link analysis will have no impact on search per-
formance.

as we saw in the last section, a link in a web page is encoded in html with

a statement such as:

for more information on this topic, please go to <a
href=   http://www.somewhere.com   >the somewhere page</a>.

when this page appears in your web browser, the words    the somewhere page   
will be displayed di   erently than regular text, usually underlined or in a di   erent
color (or both). when you click on that link, your browser will then load the web
page http://www.somewhere.com. in this link,    the somewhere page    is called the
anchor text, and http://www.somewhere.com is the destination. both components
are useful in the ranking process.

21 http://www.w3.org/xml/query/

4.5 link analysis

105

4.5.1 anchor text

anchor text has two properties that make it particularly useful for ranking web
pages. first, it tends to be very short, perhaps two or three words, and those
words often succinctly describe the topic of the linked page. for instance, links
to www.ebay.com are highly likely to contain the word    ebay    in the anchor text.
many queries are very similar to anchor text in that they are also short topical de-
scriptions of web pages. this suggests a very simple algorithm for ranking pages:
search through all links in the collection, looking for anchor text that is an exact
match for the user   s query. each time there is a match, add 1 to the score of the
destination page. pages would then be ranked in decreasing order of this score.
this algorithm has some glaring faults, not the least of which is how to handle the
query    click here   . more generally, the collection of all the anchor text in links
pointing to a page can be used as an additional text field for that page, and incor-
porated into the ranking algorithm.

anchor text is usually written by people who are not the authors of the des-
tination page. this means that the anchor text can describe a destination page
from a di   erent perspective, or emphasize the most important aspect of the page
from a community viewpoint. the fact that the link exists at all is a vote of impor-
tance for the destination page. although anchor text is not mentioned as often
as link analysis algorithms (for example, id95) in discussions of web search
engines, trec evaluations have shown that it is the most important part of the
representation of a page for some types of web search. in particular, it is essential
for searches where the user is trying to find a home page for a particular topic,
person, or organization.

4.5.2 id95

there are tens of billions of web pages, but most of them are not very interesting.
many of those pages are spam and contain no useful content at all. other pages are
personal blogs, wedding announcements, or family picture albums. these pages
are interesting to a small audience, but probably not broadly. on the other hand,
there are a few pages that are popular and useful to many people, including news
sites and the websites of popular companies.

the huge size of the web makes this a di   cult problem for search engines.
suppose a friend had told you to visit the site for ebay, and you didn   t know that
www.ebay.com was the url to use. you could type    ebay    into a search engine,
but there are millions of web pages that contain the word    ebay   . how can the

106

4 processing text

search engine choose the most popular (and probably the correct) one? one very
e   ective approach is to use the links between web pages as a way to measure pop-
ularity. the most obvious measure is to count the number of inlinks (links point-
ing to a page) for each page and use this as a feature or piece of evidence in the
ranking algorithm. although this has been shown to be quite e   ective, it is very
susceptible to spam. measures based on link analysis algorithms are designed to
provide more reliable ratings of web pages. of these measures, id95, which
is associated with the google search engine, is most often mentioned.

id95 is based on the idea of a random surfer (as in web surfer). imagine
a person named alice who is using her web browser. alice is extremely bored, so
she wanders aiid113ssly between web pages. her browser has a special    surprise me   
button at the top that will jump to a random web page when she clicks it. each
time a web page loads, she chooses whether to click the    surprise me    button or
whether to click one of the links on the web page. if she clicks a link on the page,
she has no preference for any particular link; instead, she just picks one randomly.
alice is su   ciently bored that she intends to keep browsing the web like this for-
ever.22

to put this in a more structured form, alice browses the web using this algo-

rithm:
1. choose a random number r between 0 and 1.
2. if r <   :
3. if r       :

    click the    surprise me    button.

    click a link at random on the current page.

4. start again.

typically we assume that    is fairly small, so alice is much more likely to click
a link than to pick the    surprise me    button. even though alice   s path through the
web pages is random, alice will still see popular pages more often than unpopular
ones. that   s because alice often follows links, and links tend to point to popular
pages. so, we expect that alice will end up at a university website, for example,
more often than a personal website, but less often than the id98 website.

22 the id95 calculation corresponds to finding what is known as the stationary prob-
ability distribution of a random walk on the graph of the web. a random walk is a spe-
cial case of a markov chain in which the next state (the next page visited) depends solely
on the current state (current page). the transitions that are allowed between states are
all equally probable and are given by the links.

4.5 link analysis

107

suppose that id98 has posted a story that contains a link to a professor   s web
page. alice now becomes much more likely to visit that professor   s page, because
alice visits the id98 website frequently. a single link at id98 might influence
alice   s activity more than hundreds of links at less popular sites, because alice
visits id98 far more often than those less popular sites.

because of alice   s special    surprise me    button, we can be guaranteed that
eventually she will reach every page on the internet.23 since she plans to browse
the web for a very long time, and since the number of web pages is finite, she will
visit every page a very large number of times. it is likely, however, that she will
visit a popular site thousands of times more often than an unpopular one. note
that if she did not have the    surprise me    button, she would get stuck on pages
that did not have links, pages whose links no longer pointed to any page, or pages
that formed a loop. links that point to the first two types of pages, or pages that
have not yet been crawled, are called dangling links.

now suppose that while alice is browsing, you happened to walk into her
room and glance at the web page on her screen. what is the id203 that
she will be looking at the id98 web page when you walk in? that id203
is id98   s id95. every web page on the internet has a id95, and it is
uniquely determined by the link structure of web pages. as this example shows,
id95 has the ability to distinguish between popular pages (those with many
incoming links, or those that have links from popular pages) and unpopular ones.
the id95 value can help search engines sift through the millions of pages that
contain the word    ebay    to find the one that is most popular (www.ebay.com).

alice would have to click on many billions of links in order for us to get a
reasonable estimate of id95, so we can   t expect to compute it by using actual
people. fortunately, we can compute id95 in a much more e   cient way.

suppose for the moment that the web consists of just three pages, a, b, and
c. we will suppose that page a links to pages b and c, page b links to page c,
and page c links to page a, as shown in figure 4.10.

the id95 of page c, which is the id203 that alice will be looking
at this page, will depend on the id95 of pages a and b. since alice chooses
randomly between links on a given page, if she starts in page a, there is a 50%
chance that she will go to page c (because there are two outgoing links). another
way of saying this is that the id95 for a page is divided evenly between all the

23 the    surprise button    makes the random surfer model an ergodic markov chain, which

guarantees that the iterative calculation of id95 will converge.

108

4 processing text

fig. 4.10. a sample    internet    consisting of just three web pages. the arrows denote links
between the pages.

outgoing links. if we ignore the    surprise me    button, this means that the page-
rank of page c, represented as p r(c), can be calculated as:

more generally, we could calculate the id95 for any page u as:

p r(c) =

p r(a)

+

p r(b)

1

2

   

p r(u) =

p r(v)

v   bu

lv

where bu is the set of pages that point to u, and lv is the number of outgoing
links from page v (not counting duplicate links).

there is an obvious problem here: we don   t know the id95 values for the
pages, because that is what we are trying to calculate. if we start by assuming that
the id95 values for all pages are the same (1/3 in this case), then it is easy to
see that we could perform multiple iterations of the calculation. for example, in
the first iteration, p r(c) = 0.33/2 + 0.33 = 0.5, p r(a) = 0.33, and p r(b)
= 0.17. in the next iteration, p r(c) = 0.33/2 + 0.17 = 0.33, p r(a) = 0.5,
and p r(b) = 0.17. in the third iteration, p r(c) = 0.42, p r(a) = 0.33, and
p r(b) = 0.25. after a few more iterations, the id95 values converge to the
final values of p r(c) = 0.4, p r(a) = 0.4, and p r(b) = 0.2.

if we take the    surprise me    button into account, part of the id95 for page
c will be due to the id203 of coming to that page by pushing the button.
given that there is a 1/3 chance of going to any page when the button is pushed,

abc4.5 link analysis

109

the contribution to the id95 for c for the button will be   /3. this means
that the total id95 for c is now:

p r(c) =

+ (1       )    (

  
3

p r(a)

similarly, the general formula for id95 is:
+ (1       )   

p r(u) =

  
n

+

p r(b)

1

)

p r(v)

2

   

v   bu

lv

where n is the number of pages being considered. the typical value for    is 0.15.

this can also be expressed as a matrix equation:

r = tr

where r is the vector of id95 values and t is the matrix representing the
transition probabilities for the random surfer model. the element tij represents
the id203 of going from page i to page j, and:
+ (1       )

tij =

  
n

1
li

those of you familiar with id202 may recognize that the solution r is an
eigenvector of the matrix t.

figure 4.11 shows some pseudocode for computing id95. the algorithm
takes a graph g as input. graphs are composed of vertices and edges, so g =
(v, e). in this case, the vertices are web pages and the edges are links, so the pseu-
docode uses the letters p and l instead. a link is represented as a pair (p, q),
where p and q are the source and destination pages. dangling links, which are
links where the page q does not exist, are assumed to be removed. pages with no
outbound links are rank sinks, in that they accumulate id95 but do not dis-
tribute it. in this algorithm, we assume that these pages link to all other pages in
the collection.

the first step is to make a guess at the id95 value for each page. without
any better information, we assume that the id95 is the same for each page.
since id95 values need to sum to 1 for all pages, we assign a id95 of
1/|p| to each page in the input vector i. an alternative that may produce faster
convergence would be to use a value based on the number of inlinks.

end for
while r has not converged do
for all entries ri     r do

110

4 processing text

ii     1/|p|

ri       /|p|

1: procedure p         r         (g)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end procedure

end for
end while
return r

end if
i     r

end for

else

    g is the web graph, consisting of vertices (pages) and edges (links).
    split graph into pages and links
    the current id95 estimate
    the resulting better id95 estimate

(p, l)     g
i     a vector of length |p|
r     a vector of length |p|
for all entries ii     i do

    start with each page being equally likely

    each page has a   /|p| chance of random selection

end for
for all pages p     p do
q     the set of pages such that (p, q)     l and q     p
if |q| > 0 then

for all pages q     q do

rq     rq + (1       )ip/|q|     id203 ip of being at page p

end for
for all pages q     p do

rq     rq + (1       )ip/|p|

    update our current id95 estimate

fig. 4.11. pseudocode for the iterative id95 algorithm

in each iteration, we start by creating a result vector, r, and storing   /|p| in
each entry. this is the id203 of landing at any particular page because of a
random jump. the next step is to compute the id203 of landing on a page
because of a clicked link. we do that by iterating over each web page in p . at
each page, we retrieve the estimated id203 of being at that page, ip. from
that page, the user has a    chance of jumping randomly, or 1        of clicking on a
link. there are |q| links to choose from, so the id203 of jumping to a page
q     q is (1      )ip/|q|. we add this quantity to each entry rq. in the event that

4.5 link analysis

111

there are no usable outgoing links, we assume that the user jumps randomly, and
therefore the id203 (1       )ip is spread evenly among all |p| pages.

to summarize, id95 is an important example of query-independent meta-
data that can improve ranking for web search. web pages have the same id95
value regardless of what query is being processed. search engines that use page-
rank will prefer pages with high id95 values instead of assuming that all
web pages are equally likely to satisfy a query. id95 is not, however, as im-
portant in web search as the conventional wisdom holds. it is just one of many
features used in ranking. it does, however, tend to have the most impact on pop-
ular queries, which is a useful property.

the hits24 algorithm (kleinberg, 1999) for link analysis was developed at
about the same time as id95 and has also been very influential. this algo-
rithm estimates the value of the content of a page (the authority value) and the
value of the links to other pages (the hub value). both values are computed us-
ing an iterative algorithm based solely on the link structure, similar to id95.
the hits algorithm, unlike id95, calculates authority and hub values for
a subset of pages retrieved by a given query.25 this can be an advantage in terms
of the impact of the hits metadata on ranking, but may be computationally in-
feasible for search engines with high query tra   c. in chapter 10, we discuss the
application of the hits algorithm to finding web communities.

4.5.3 link quality

it is well known that techniques such as id95 and anchor text extraction are
used in commercial search engines, so unscrupulous web page designers may try
to create useless links just to improve the search engine placement of one of their
web pages. this is called link spam. even typical users, however, can unwittingly
fool simple search engine techniques. a good example of this is with blogs.

many blog posts are comments about other blog posts. suppose author a reads
a post called b in author b   s blog. author a might write a new blog post, called
a, which contains a link to post b. in the process of posting, author a may post a
trackback to post b in author b   s blog. a trackback is a special kind of comment
that alerts author b that a reply has been posted in author a   s blog.

24 hypertext induced topic search
25 query-independent versions of hits and topic-dependent versions of id95 have

also been defined.

112

4 processing text

fig. 4.12. trackback links in blog postings

as figure 4.12 shows, a cycle has developed between post a and post b. post
a links to post b, and post b contains a trackback link to post a. intuitively we
would say that post b is influential, because author a has decided to write about
it. however, from the id95 perspective, a and b have links to each other,
and therefore neither is more influential than the other. the trouble here is that
a trackback is a fundamentally di   erent kind of link than one that appears in a
post.

the comments section of a blog can also be a source of link spam. page authors
may try to promote their own websites by posting links to them in the comments
section of popular blogs. based on our discussion of id95, we know that a
link from a popular website can make another website seem much more impor-
tant. therefore, this comments section is an attractive target for spammers.

in this case, one solution is for search engine companies to automatically de-
tect these comment sections and e   ectively ignore the links during indexing. an
even easier way to do this is to ask website owners to alter the unimportant links so
that search engines can detect them. this is the purpose behind the rel=nofollow
link attribute.

most blog software is now designed to modify any link in a blog comment to

contain the rel=nofollow attribute. therefore, a post like this:

blog bblog apost apost btrackback linkslink4.6 information extraction

113

come visit my <a href=   http://www.page.com   >web page</a>.

becomes something like this:

come visit my <a rel=nofollow href=   http://www.page.com   >web page</a>.
the link still appears on the blog, but search engines are designed to ignore all
links marked rel=nofollow. this helps preserve the integrity of id95 calcula-
tion and anchor text harvesting.

4.6 information extraction

information extraction is a language technology that focuses on extracting struc-
ture from text. information extraction is used in a number of applications, and
particularly for text data mining. for search applications, the primary use of in-
formation extraction is to identify features that can be used by the search engine to
improve ranking. some people have speculated that information extraction tech-
niques could eventually transform text search into a database problem by extract-
ing all of the important information from text and storing it in structured form,
but current applications of these techniques are a very long way from achieving
that goal.

some of the text processing steps we have already discussed could be consid-
ered information extraction. identifying noun phrases, titles, or even bolded text
are examples. in each of these cases, a part of the text has been recognized as hav-
ing some special property, and that property can be described using a markup lan-
guage, such as xml. if a document is already described using html or xml,
the recognition of some of the structural features (such as titles) is straightfor-
ward, but others, such as phrases, require additional processing before the feature
can be annotated using the markup language. in some applications, such as when
the documents in the collection are input through ocr, the document has no
markup and even simple structures such as titles must be recognized and anno-
tated.

these types of features are very general, but most of the recent research in in-
formation extraction has been concerned with features that have specific seman-
tic content, such as named entities, relationships, and events. although all of these
features contain important information, id39 has been used
most often in search applications. a named entity is a word or sequence of words
that is used to refer to something of interest in a particular application. the most

114

4 processing text

common examples are people   s names, company or organization names, locations,
time and date expressions, quantities, and monetary values. it is easy to come up
with other    entities    that would be important for specific applications. for an e-
commerce application, for example, the recognition of product names and model
numbers in web pages and reviews would be essential. in a pharmaceutical appli-
cation, the recognition of drug names, dosages, and medical conditions may be
important. given the more specific nature of these features, the process of rec-
ognizing them and tagging them in text is sometimes called semantic annotation.
some of these recognized entities would be incorporated directly into the search
using, for example, facets (see chapter 6), whereas others may be used as part of
browsing the search results. an example of the latter is the search engine feature
that recognizes addresses in pages and provides links to the appropriate map.

fig. 4.13. text tagged by information extraction

figure 4.13 shows a sentence and the corresponding xml markup after us-
ing information extraction. in this case, the extraction was done by a well-known
word processing program.26 in addition to the usual structure markup (<p> and
<b>), a number of tags have been added that indicate which words are part of
named entities. it shows, for example, that an address consisting of a street (   10
water street   ), a city (   springfield   ), and a state (   ma   ) was recognized in the
text.

two main approaches have been used to build named entity recognizers: rule-
based and statistical. a rule-based recognizer uses one or more lexicons (lists of
words and phrases) that categorize names. some example categories would be
locations (e.g., towns, cities, states, countries, places of interest), people   s names
(given names, family names), and organizations (e.g., companies, government

26 microsoft word

fred smith, who lives at 10 water street, springfield, ma, is a long  time collector of tropical fish. <p ><personname><givenname>fred</givenname> <sn>smith</sn> </personname>, who lives at <address><street >10 water street</street>, <city>springfield</city>, <state>ma</state></address>, is a long  time collector of <b>tropical fish.</b></p> 4.6 information extraction

115

agencies, international groups). if these lists are su   ciently comprehensive, much
of the extraction can be done simply by lookup. in many cases, however, rules or
patterns are used to verify an entity name or to find new entities that are not in the
lists. for example, a pattern such as    <number> <word> street    could be used to
identify street addresses. patterns such as    <street address>, <city>    or    in <city>   
could be used to verify that the name found in the location lexicon as a city was
indeed a city. similarly, a pattern such as    <street address>, <city>, <state>    could
also be used to identify new cities or towns that were not in the lexicon. new per-
son names could be recognized by rules such as    <title> <name>   , where <title>
would include words such as    president   ,    mr.   , and    ceo   . names are generally
easier to extract in mixed-case text, because capitalization often indicates a name,
but many patterns will apply to all lower- or uppercase text as well. rules incor-
porating patterns are developed manually, often by trial and error, although an
initial set of rules can also be used as seeds in an automated learning process that
can discover new rules.27

a statistical entity recognizer uses a probabilistic model of the words in and
around an entity. a number of di   erent approaches have been used to build these
models, but because of its importance, we will briefly describe the hidden markov
model (id48) approach. id48s are used for many applications in speech and
language. for example, pos taggers can be implemented using this approach.

4.6.1 id48 for extraction

one of the most di   cult parts of entity extraction is that words can have many
di   erent meanings. the word    bush   , for example, can describe a plant or a person.
similarly,    marathon    could be the name of a race or a location in greece. people
tell the di   erence between these di   erent meanings based on the context of the
word, meaning the words that surround it. for instance, if    marathon    is preceded
by    boston   , the text is almost certainly describing a race. we can describe the
context of a word mathematically by modeling the generation28 of the sequence
of words in a text as a process with the markov property, meaning that the next
word in the sequence depends on only a small number of the previous words.

27 gate (http://gate.ac.uk) is an example of an open source toolkit that provides both
an information extraction component and an environment for customizing extraction
for a specific application.

28 we discuss generative models in more detail in chapter 7.

116

4 processing text

more formally, a markov model describes a process as a collection of states with
transitions between them. each of the transitions has an associated id203.
the next state in the process depends solely on the current state and the transition
probabilities. in a hidden markov model, each state has a set of possible outputs
that can be generated. as with the transitions, each output also has a id203
associated with it.

figure 4.14 shows a state diagram representing a very simple model for sen-
tence generation that could be used by a named entity recognizer. in this model,
the words in a sentence are assumed to be either part of an entity name (in this
case, either a person, organization, or location) or not part of one. each of these
entity categories is represented by a state, and after every word the system may stay
in that state (represented by the arrow loops) or transition to another state. there
are two special states representing the start and end of the sentence. associated
with each state representing an entity category, there is a id203 distribution
of the likely sequences of words for that category.

fig. 4.14. sentence model for statistical entity extractor

one possible use of this model is to construct new sentences. suppose that
we begin in the start state, and then the next state is randomly chosen according

endstartpersonorgan-izationlocationnot-an-entity<every entitycategory>4.6 information extraction

117

to the start state   s transition id203 table. for example, we may transition to
the person state. once we have entered the person state, we complete the tran-
sition by choosing an output according to the person state   s output id203
distribution. an example output may be the word    thomas   . this process would
continue, with a new state being transitioned to and an output being generated
during each step of the process. the final result is a set of states and their associated
outputs.

although such models can be used to generate new sentences, they are more
commonly used to recognize entities in a sentence. to do this for a given sentence,
a sequence of entity categories is found that gives the highest id203 for the
words in that sentence. only the outputs generated by state transitions are visible
(i.e., can be observed); the underlying states are    hidden.    for the sentence in fig-
ure 4.13, for example, the recognizer would find the sequence of states

<start><name><not-an-entity><location><not-an-entity><end>

to have the highest id203 for that model. the words that were associated
with the entity categories in this sequence would then be tagged. the problem
of finding the most likely sequence of states in an id48 is solved by the viterbi
algorithm,29 which is a id145 algorithm.

the key aspect of this approach to entity recognition is that the probabili-
ties in the sentence model must be estimated from training data. to estimate the
transition and output probabilities, we generate training data that consists of text
manually annotated with the correct entity tags. from this training data, we can
directly estimate the id203 of words associated with a given category (i.e.,
output probabilities), and the id203 of transitions between categories. to
build a more accurate recognizer, features that are highly associated with named
entities, such as capitalized words and words that are all digits, would be included
in the model. in addition, the transition probabilities could depend on the pre-
vious word as well as the previous category.30 for example, the occurrence of the
word    mr.    increases the id203 that the next category is person.

although such training data can be useful for constructing accurate id48s,
collecting it requires a great deal of human e   ort. to generate approximately one
million words of annotated text, which is the approximate size of training data
required for accurate estimates, people would have to annotate the equivalent of

29 named after the electrical engineer andrew viterbi.
30 bikel et al. (1997) describe one of the first named entity recognizers based on the

id48 approach.

118

4 processing text

more than 1,500 news stories. this may require considerably more e   ort than de-
veloping rules for a simple set of features. both the rule-based and statistical ap-
proaches have recognition e   ectiveness of about 90%31 for entities such as name,
organization, and location, although the statistical recognizers are generally the
best. other entity categories, such as product names, are considerably more di   -
cult. the choice of which entity recognition approach to use will depend on the
application, the availability of software, and the availability of annotators.

interestingly, there is little evidence that named entities are useful features
for general search applications. id39 is a critical part of
question-answering systems (section 11.5), and can be important in domain-
specific or vertical search engines for accurately recognizing and indexing domain
terms. id39 can also be useful for query analysis in applica-
tions such as local search, and as a tool for understanding and browsing search
results.

4.7 internationalization

the web is used all over the world, and not just by english speakers. although
65   70% of the web is written in english, that percentage is continuing to de-
crease. more than half of the people who use the web, and therefore search the
web, do not use english as their primary language. other search applications,
such as desktop search and corporate search, are being used in many di   erent
languages every day. even an application designed for an environment that has
mostly english-speaking users can have many non-english documents in the col-
lection. try using    poissons tropicaux    (tropical fish) as a query for your favorite
web search engine and see how many french web pages are retrieved.32

a monolingual search engine is, as the name suggests, a search engine designed
for a particular language.33 many of the indexing techniques and retrieval models
we discuss in this book work well for any language. the di   erences between lan-
guages that have the most impact on search engine design are related to the text
processing steps that produce the index terms for searching.

31 by this we mean that about 9 out of 10 of the entities found are accurately identified,
and 9 out of 10 of the existing entities are found. see chapter 8 for details on evaluation
measures.

32 you would find many more french web pages, of course, if you used a french version

of the search engine, such as http://fr.yahoo.com.

33 we discuss cross-language search engines in section 6.4.

4.7 internationalization

119

as we mentioned in the previous chapter, character encoding is a crucial issue
for search engines dealing with non-english languages, and unicode has become
the predominant character encoding standard for the internationalization of soft-
ware.

other text processing steps also need to be customized for di   erent languages.
the importance of id30 for highly inflected languages has already been men-
tioned, but each language requires a customized stemmer. tokenizing is also im-
portant for many languages, especially for the cjk family of languages. for these
languages, the key problem is id40, where the breaks correspond-
ing to words or index terms must be identified in the continuous sequence of
characters (spaces are generally not used). one alternative to segmenting is to in-
dex overlapping character bigrams (pairs of characters, see section 4.3.5). figure
4.15 shows id40 and bigrams for the text    impact of droughts in
china   . although the ranking e   ectiveness of search based on bigrams is quite
good, id40 is preferred in many applications because many of the
bigrams do not correspond to actual words. a segmentation technique can be im-
plemented based on statistical approaches, such as a hidden markov model, with
su   cient training data. segmentation can also be an issue in other languages. ger-
man, for example, has many compound words (such as    fischzuchttechniken    for
   fish farming techniques   ) that should be segmented for indexing.

fig. 4.15. chinese segmentation and bigrams

in general, given the tools that are available, it is not di   cult to build a search
engine for the major languages. the same statement holds for any language that

(cid:7201)(cid:9902)(cid:3416)(cid:1117)(cid:3373)(cid:18000)(cid:6208)(cid:11444)(cid:5537)(cid:2813)(the impact of droughts in china)1. original text(cid:7201)(cid:9902)(cid:3416)(cid:1117)(cid:3373)(cid:18000)(cid:6208)(cid:11444)(cid:5537)(cid:2813)drought   at     china    make           impact 2. id40(cid:7201)(cid:9902)(cid:9902)(cid:3416)(cid:3416)(cid:1117)(cid:1117)(cid:3373)(cid:3373)(cid:18000)(cid:18000)(cid:6208)(cid:6208)(cid:11444)(cid:11444)(cid:5537)(cid:5537)(cid:2813)3. bigrams120

4 processing text

has a significant amount of online text available on the web, since this can be used
as a resource to build and test the search engine components. there are, however,
a large number of other so-called    low-density    languages that may have many
speakers but few online resources. building e   ective search engines for these lan-
guages is more of a challenge.

references and further reading

the properties and statistics of text and document collections has been studied for
some time under the heading of bibliometrics, which is part of the field of library
and information science. information science journals such as the journal of the
american society of information science and technology (jasist) or information
processing and management (ipm) contain many papers in this general area. in-
formation retrieval has, from the beginning, emphasized a statistical view of text,
and researchers from ir and information science have always worked closely to-
gether. belew (2000) contains a good discussion of the cognitive aspects of zipf    s
law and other properties of text in relationship to ir. with the shift to statistical
methods in the 1990s, natural language processing researchers also became inter-
ested in studying the statistical properties of text. manning and sch  tze (1999)
is a good summary of text statistics from this perspective. ha et al. (2002) give an
interesting result showing that phrases (or id165s) also generally follow zipf    s
law, and that combining the phrases and words results in better predictions for
frequencies at low ranks.

the paper by anagnostopoulos et al. (2005) describes a technique for estimat-
ing query result size and also points to much of the relevant literature in this area.
similarly, broder et al. (2006) show how to estimate corpus size and compare
their estimation with previous techniques.

not much is written about tokenizing or stopping. both are considered suf-
ficiently    well known    that they are hardly mentioned in papers. as we have
pointed out, however, getting these basic steps right is crucial for the overall sys-
tem   s e   ectiveness. for many years, researchers used the stopword list published
in van rijsbergen (1979). when it became clear that this was not su   cient for
the larger trec collections, a stopword list developed at the university of mas-
sachusetts and distributed with the lemur toolkit has frequently been used. as
mentioned previously, this list contains over 400 words, which will be too long
for many applications.

4.7 internationalization

121

the original paper describing the porter stemmer was written in 1979, but was
reprinted in porter (1997). the paper by krovetz (1993) describes his id30
algorithm but also takes a more detailed approach to studying the role of mor-
phology in a stemmer.34 the krovetz stemmer is available on the lemur website.
stemmers for other languages are available from various websites (including the
lemur website and the porter stemmer website). a description of arabic stem-
ming techniques can be found in larkey et al. (2002).

research on the use of phrases in searching has a long history. croft et al.
(1991) describe retrieval experiments with phrases derived by both syntactic and
statistical processing of the query, and showed that e   ectiveness was similar to
phrases selected manually. many groups that have participated in the trec eval-
uations have used phrases as part of their search algorithms (voorhees & harman,
2005).

church (1988) described an approach to building a statistical (or stochastic)
part-of-speech tagger that is the basis for many current taggers. this approach
uses manually tagged training data to train a probabilistic model of sequences of
parts of speech, as well as the id203 of a part of speech for a specific word.
for a given sentence, the part-of-speech tagging that gives the highest id203
for the whole sentence is used. this method is essentially the same as that used
by a statistical entity extractor, with the states being parts of speech instead of en-
tity categories. the brill tagger (brill, 1994) is a popular alternative approach that
uses rules that are learned automatically from tagged data. manning and sch  tze
(1999) provide a good overview of part-of-speech tagging methods.

many variations of id95 can be found in the literature. many of these
variations are designed to be more e   cient to compute or are used in di   erent ap-
plications. the topic-dependent version of id95 is described in haveliwala
(2002). both id95 and hits have their roots in the citation analysis algo-
rithms developed in the field of bibliometrics.

the idea of enhancing the representation of a hypertext document (i.e., a web
page) using the content of the documents that point to it has been around for
some time. for example, croft and turtle (1989) describe a retrieval model based
on incorporating text from related hypertext documents, and dunlop and van
rijsbergen (1993) describe how documents with little text content (such as those
containing images) could be retrieved using the text in linked documents. re-

34 morphology is the study of the internal structure of words, and id30 is a form of

morphological processing.

122

4 processing text

stricting the text that is incorporated to the anchor text associated with inlinks
was first mentioned by mcbryan (1994). anchor text has been shown to be es-
sential for some categories of web search in trec evaluations, such as in ogilvie
and callan (2003).

techniques have been developed for applying link analysis in collections with-
out explicit link structure (kurland & lee, 2005). in this case, the links are based
on similarities between the content of the documents, calculated by a similarity
measure such as the cosine correlation (see chapter 7).

information extraction techniques were developed primarily in research pro-
grams such as tipster and muc (cowie & lehnert, 1996). using named en-
tity extraction to provide additional features for search was also studied early in
the trec evaluations (callan et al., 1992, 1995). one of the best-known rule-
based information extraction systems is fastus (hobbs et al., 1997). the bbn
system identifinder (bikel et al., 1999), which is based on an id48, has been
used in many projects.

a detailed description of id48s and the viterbi algorithm can be found in
manning and sch  tze (1999). mccallum (2005) provides an overview of infor-
mation extraction, with references to more recent advances in the field. statistical
models that incorporate more complex features than id48s, such as conditional
random fields, have become increasingly popular for extraction (sutton & mccal-
lum, 2007).

detailed descriptions of all the major encoding schemes can be found in
wikipedia. fujii and croft (1993) was one of the early papers that discussed the
problems of text processing for search with cjk languages. an entire journal,
acm transactions on asian language information processing,35 has now been de-
voted to this issue. peng et al. (2004) describe a statistical model for chinese word
segmentation and give references to other approaches.

exercises

4.1. plot rank-frequency curves (using a log-log graph) for words and bigrams in
the wikipedia collection available through the book website (http://www.search-
engines-book.com). plot a curve for the combination of the two. what are the best
values for the parameter c for each curve?

35 http://talip.acm.org/

4.7 internationalization

123

4.2. plot vocabulary growth for the wikipedia collection and estimate the pa-
rameters for heaps    law. should the order in which the documents are processed
make any di   erence?

4.3. try to estimate the number of web pages indexed by two di   erent search en-
gines using the technique described in this chapter. compare the size estimates
from a range of queries and discuss the consistency (or lack of it) of these esti-
mates.

4.4. modify the galago tokenizer to handle apostrophes or periods in a di   erent
way. describe the new rules your tokenizer implements. give examples of where
the new tokenizer does a better job (in your opinion) and examples where it does
not.

4.5. examine the lemur stopword list and list 10 words that you think would
cause problems for some queries. give examples of these problems.

4.6. process five wikipedia documents using the porter stemmer and the krovetz
stemmer. compare the number of stems produced and find 10 examples of dif-
ferences in the id30 that could have an impact on ranking.

4.7. use the gate pos tagger to tag a wikipedia document. define a rule or
rules to identify phrases and show the top 10 most frequent phrases. now use the
pos tagger on the wikipedia queries. are there any problems with the phrases
identified?

4.8. find the 10 wikipedia documents with the most inlinks. show the collec-
tion of anchor text for those pages.

4.9. compute id95 for the wikipedia documents. list the 20 documents
with the highest id95 values together with the values.

4.10. figure 4.11 shows an algorithm for computing id95. prove that the
entries of the vector i sum to 1 every time the algorithm enters the loop on line
9.

4.11. implement a rule-based recognizer for cities (you can choose a subset of
cities to make this easier). create a test collection that you can scan manually to
find cities mentioned in the text and evaluate your recognizer. summarize the
performance of the recognizer and discuss examples of failures.

124

4 processing text

4.12. create a small test collection in some non-english language using web
pages. do the basic text processing steps of tokenizing, id30, and stopping
using tools from the book website and from other websites. show examples of the
index term representation of the documents.

5

ranking with indexes

   must go faster.   

david levinson, independence day

5.1 overview

as this is a fairly technical book, if you have read this far, you probably understand
something about data structures and how they are used in programs. if you want
to store a list of items, linked lists and arrays are good choices. if you want to
quickly find an item based on an attribute, a hash table is a better choice. more
complicated tasks require more complicated structures, such as b-trees or priority
queues.

why are all these data structures necessary? strictly speaking, they aren   t. most
things you want to do with a computer can be done with arrays alone. however,
arrays have drawbacks: unsorted arrays are slow to search, and sorted arrays are
slow at insertion. by contrast, hash tables and trees are fast for both search and
insertion. these structures are more complicated than arrays, but the speed dif-
ference is compelling.

text search is very di   erent from traditional computing tasks, so it calls for
its own kind of data structure, the inverted index. the name    inverted index    is
really an umbrella term for many di   erent kinds of structures that share the same
general philosophy. as you will see shortly, the specific kind of data structure used
depends on the ranking function. however, since the ranking functions that rank
documents well have a similar form, the most useful kinds of inverted indexes are
found in nearly every search engine.

this chapter is about how search engine queries are actually processed by a
computer, so this whole chapter could arguably be called query processing. the
last section of this chapter is called that, and the query processing algorithms pre-
sented there are based on the data structures presented earlier in the chapter.

126

5 ranking with indexes

e   cient query processing is a particularly important problem in web search,
as it has reached a scale that would have been hard to imagine just 10 years ago.
people all over the world type in over half a billion queries every day, searching
indexes containing billions of web pages. inverted indexes are at the core of all
modern web search engines.

there are strong dependencies between the separate components of a search
engine. the query processing algorithm depends on the retrieval model, and dic-
tates the contents of the index. this works in reverse, too, since we are unlikely to
choose a retrieval model that has no e   cient query processing algorithm. since
we will not be discussing retrieval models in detail until chapter 7, we start this
chapter by describing an abstract model of ranking that motivates our choice of
indexes. after that, there are four main parts to the chapter. in the first part, we
discuss the di   erent types of inverted index and what information about docu-
ments is captured in each index. the second part gives an overview of compres-
sion techniques, which are a critical part of the e   cient implementation of in-
verted indexes for text retrieval. the third part of the chapter describes how in-
dexes are constructed, including a discussion of the mapreduce framework that
can be used for very large document collections. the final part of the chapter fo-
cuses on how the indexes are used to generate document rankings in response to
queries.

5.2 abstract model of ranking

before we begin to look at how to build indexes for a search system, we will start by
considering an abstract model of ranking. all of the techniques we will consider
in this chapter can be seen as implementations of this model.

figure 5.1 shows the basic components of our model. on the left side of the
figure is a sample document. documents are written in natural human languages,
which are di   cult for computers to analyze directly. so, as we saw in chapter 4,
the text is transformed into index terms or document features. for the purposes of
this chapter, a document feature is some attribute of the document we can express
numerically. in the figure, we show two kinds of features. on top, we have topical
features, which estimate the degree to which the document is about a particular
subject. on the bottom of the figure, we see two possible document quality fea-
tures. one feature is the number of web pages that link to this document, and an-
other is the number of days since this page was last updated. these features don   t
address whether the document is a good topical match for a query, but they do

5.2 abstract model of ranking

127

fig. 5.1. the components of the abstract model of ranking: documents, features, queries,
the retrieval function, and document scores

address its quality: a page with no incoming links that hasn   t been edited in years
is probably a poor match for any query. each of these feature values is generated
using a feature function, which is just a mathematical expression that generates
numbers from document text. in chapter 4 we discussed some of the important
topical and quality features, and in chapter 7 you will learn about the techniques
for creating good feature functions. in this chapter, we assume that reasonable
feature values have already been created.

on the right side of the figure, we see a cloud representing the ranking func-
tion. the ranking function takes data from document features combined with the
query and produces a score. for now, the contents of that cloud are unimportant,
except for the fact that most reasonable ranking functions ignore many of the doc-
ument features, and focus only on the small subset that relate to the query. this
fact makes the inverted index an appealing data structure for search.

the final output of the ranking function is a score, which we assume is some
real number. if a document gets a high score, this means that the system thinks
that document is a good match for the query, whereas lower numbers mean that
the system thinks the document is a poor match for the query. to build a ranked
list of results, the documents are sorted by score so that the highest-scoring doc-
uments come first.

suppose that you are a human search engine, trying to sort documents in an
appropriate order in response to a user query. perhaps you would place the docu-
ments in piles, like    good,       not so good,    and    bad.    the computer is doing essen-
tially the same thing with scoring. however, you might also break ties by looking
carefully at each document to decide which one is more relevant. unfortunately,

fred's tropical fish shop is the best place to !nd tropical !sh at low, low prices.  whether you're looking for a little !sh or a big !sh, we've got what you need.  we even have fake seaweed for your !shtank (and little sur"oards too).documenttopical features!shtropicaltropical !shseaweedsur"oards9.74.222.18.24.2quality featuresincoming linksdays since last update143ranking functiondocument scorequerytropical !sh24.5128

5 ranking with indexes

finding deep meaning in documents is di   cult for computers to do, so search en-
gines focus on identifying good features and scoring based on those features.

a more concrete ranking model

later in the chapter we will look at query evaluation techniques that assume some-
thing stronger about what happens in the ranking function. specifically, we as-
sume that the ranking function r takes the following form:

   

r(q, d) =

gi(q)fi(d)

i

here, fi is some feature function that extracts a number from the document
text. gi is a similar feature function that extracts a value from the query. these two
functions form a pair of feature functions. each pair of functions is multiplied
together, and the results from all pairs are added to create a final document score.

fig. 5.2. a more concrete model of ranking. notice how both the query and the docu-
ment have feature functions in this model.

figure 5.2 shows an example of this model. just as in the abstract model of
ranking, various features are extracted from the document. this picture shows
only a few features, but in reality there will be many more. these correspond
to the fi(d) functions in the equation just shown. we could easily name these
ftropical(d) or ffish(d); these values will be larger for documents that contain
the words    tropical    or    fish    more often or more prominently.

fred's tropical fish shop is the best place to !nd tropical !sh at low, low prices.  whether you're looking for a little !sh or a big !sh, we've got what you need.  we even have fake seaweed for your !shtank (and little sur"oards too).documenttopical features!shtropicaltropical !shseaweedsur"oards9.74.222.18.24.2quality featuresincoming linksupdate count143document scorequerytropical !sh303.01tropical !shtropical!shchichlidsbarbs5.23.49.91.20.7topical featuresquality featuresincoming links1.2update count0.9figi5.3 inverted indexes

129

the document has some features that are not topical. for this example docu-
ment, we see that the search engine notices that this document has been updated
three times, and that it has 14 incoming links. although these features don   t tell
us anything about whether this document would match the subject of a query,
they do give us some hints about the quality of the document. we know that it
wasn   t just posted to the web and then abandoned, since it gets updated occa-
sionally. we also know that there are 14 other pages that have links pointing to it,
which might mean that it has some useful information on it.

notice that there are also feature functions that act on the query. the feature
function gtropical(q) evaluates to a large value because    tropical    is in the query.
however, gbarbs(q) also has a small non-zero value because it is related to other
terms in the query. these values from the query feature functions are multiplied
by the document feature functions, then summed to create a document score.

the query also has some feature values that aren   t topical, such as the update
count feature. of course, this doesn   t mean that the query has been updated. the
value of this feature indicates how important document updates are to relevance
for this query. for instance, if the query was    today   s weather in london   , we would
prefer documents that are updated frequently, since a document that isn   t updated
at least daily is unlikely to say anything interesting about today   s weather. this
query should have a high value for the update count feature. by contrast, a docu-
ment that never changed could be very relevant for the query    full text of moby
dick   . this query could have a low feature value for update count.

if a retrieval system had to perform a sum over millions of features for every
document, text search systems would not be practical. in practice, the query fea-
tures (gi(q)) are mostly zero. this means that the sum for each document is only
over the non-zero gi(q) values.

5.3 inverted indexes

all modern search engine indexes are based on inverted indexes. other index
structures have been used in the past, most notably signature files,1 but inverted
indexes are considered the most e   cient and flexible index structure.
1 a signature is a concise representation of a block of text (or document) as a sequence
of bits, similar to the fingerprints discussed in chapter 3. a hash function is used for
each word in the text block to set bits in specific positions in the signature to one.

130

5 ranking with indexes

an inverted index is the computational equivalent of the index found in the
back of this textbook. you might want to look over the index in this book as an
example. the book index is arranged in alphabetical order by index term. each
index term is followed by a list of pages about that word. if you want to know
more about id30, for example, you would look through the index until you
found words starting with    s   . then, you would scan the entries until you came to
the word    id30.    the list of page numbers there would lead you to chapter 4.
similarly, an inverted index is organized by index term. the index is inverted
because usually we think of words being a part of documents, but if we invert this
idea, documents are associated with words. index terms are often alphabetized
like a traditional book index, but they need not be, since they are often found di-
rectly using a hash table. each index term has its own inverted list that holds the
relevant data for that term. in an index for a book, the relevant data is a list of
page numbers. in a search engine, the data might be a list of documents or a list
of word occurrences. each list entry is called a posting, and the part of the posting
that refers to a specific document or location is often called a pointer. each doc-
ument in the collection is given a unique number to make it e   cient for storing
document pointers.

indexes in books store more than just location information. for important
words, often one of the page numbers is marked in boldface, indicating that this
page contains a definition or extended discussion about the term. inverted files
can also have extended information, where postings can contain a range of in-
formation other than just locations. by storing the right information along with
each posting, the feature functions we saw in the last section can be computed
e   ciently.

finally, by convention, the page numbers in a book index are printed in as-
cending order, so that the smallest page numbers come first. traditionally, in-
verted lists are stored the same way. these document-ordered lists are ordered by
document number, which makes certain kinds of query processing more e   cient
and also improves list compression. however, some inverted files we will consider
have other kinds of orderings.

alternatives to inverted files generally have one or more disadvantages. the sig-
nature file, for example, represents each document in the collection as a small set
of bits. to search a signature file, the query is converted to a signature and the bit
patterns are compared. in general, all signatures must be scanned for every search.
even if the index is encoded compactly, this is a lot of processing. the inverted
file   s advantage is that only a small fraction of the index needs to be considered

5.3 inverted indexes

131

to process most queries. also, matches in signature files are noisy, so a signature
match is not guaranteed to be a match in the document text. most importantly,
it is di   cult to generalize signature file techniques for ranked search (zobel et al.,
1998).

another approach is to use spatial data structures, such as k-d trees. in this ap-
proach, each document is encoded as a point in some high-dimensional space, and
the query is as well. the spatial data structure can then be used to find the closest
documents to the query. although many ranking approaches are fundamentally
spatial, most spatial data structures are not designed for the number of dimen-
sions associated with text applications.2 as a result, it tends to be much faster to
use an inverted list to rank documents than to use a typical spatial data structure.
in the next few sections, we will look at some di   erent kinds of inverted files. in
each case, the inverted file organization is dictated by the ranking function. more
complex ranking functions require more information in the index. these more
complicated indexes take additional space and computational power to process,
but can be used to generate more e   ective document rankings. index organization
is by no means a solved problem, and research is ongoing into the best way to
create indexes that can more e   ciently produce e   ective document rankings.

5.3.1 documents

the simplest form of an inverted list stores just the documents that contain each
word, and no additional information. this kind of list is similar to the kind of
index you would find at the back of this textbook.

figure 5.3 shows an index of this type built from the four sentences in table 5.1
(so in this case, the    documents    are sentences). the index contains every word
found in all four sentences. next to each word, there are a list of boxes, and each
one contains the number of a sentence. each one of these boxes is a posting. for
example, look at the word    fish   . you can quickly see that this word appears in
all four sentences, because the numbers 1, 2, 3, and 4 appear by it. you can also
quickly determine that    fish    is the only word that appears in all the sentences.
two words come close:    tropical    appears in every sentence but s4, and    water   
is not in s3.

2 every term in a document corresponds to a dimension, so there are tens of thousands
of dimensions in e   ect. this is in comparison to a typical database application with
tens of dimensions at most.

132

5 ranking with indexes

s1 tropical fish include fish found in tropical environments around

the world, including both freshwater and salt water species.

s2 fishkeepers often use the term tropical fish to refer only those
requiring fresh water, with saltwater tropical fish referred to as
marine fish.

s3 tropical fish are popular aquarium fish, due to their often bright

coloration.

s4 in freshwater fish, this coloration typically derives from irides-

cence, while salt water fish are generally pigmented.

table 5.1. four sentences from the wikipedia entry for tropical fish

fig. 5.3. an inverted index for the documents (sentences) in table 5.1

and1aquarium3are34around1as2both1bright3coloration34derives4due3environments1   sh1234   shkeepers2found1fresh2freshwater14from4generally4in14include1including1iridescence4marine2often23only2pigmented4popular3refer2referred2requiring2salt14saltwater2species1term2the12their3this4those2to23tropical123typically4use2water124while4with2world15.3 inverted indexes

133

notice that this index does not record the number of times each word appears;
it only records the documents in which each word appears. for instance, s2 con-
tains the word    fish    twice, whereas s1 contains    fish    only once. the inverted list
for    fish    shows no distinction between sentences 1 and 2; both are listed in the
same way. in the next few sections, we will look at indexes that include informa-
tion about word frequencies.

inverted lists become more interesting when we consider their intersection.
suppose we want to find the sentence that contains the words    coloration    and
   freshwater   . the inverted index tells us that    coloration    appears in s3 and s4,
while    freshwater    appears in s1 and s4. we can quickly tell that only s4 contains
both    coloration    and    freshwater   . since each list is sorted by sentence number,
finding the intersection of these lists takes o(max(m, n)) time, where m and n
are the lengths of the two lists. the algorithm is the same as in merge sort. with list
skipping, which we will see later in the chapter, this cost drops to o(min(m, n)).

5.3.2 counts

remember that our abstract model of ranking considers each document to be
composed of features. with an inverted index, each word in the index corre-
sponds to a document feature. this feature data can be processed by a ranking
function into a document score. in an inverted index that contains only docu-
ment information, the features are binary, meaning they are 1 if the document
contains a term, 0 otherwise. this is important information, but it is too coarse
to find the best few documents when there are a lot of possible matches.

for instance, consider the query    tropical fish   . three sentences match this
query: s1, s2, and s3. the data in the document-based index (figure 5.3) gives
us no reason to prefer any of these sentences over any other.

now look at the index in figure 5.4. this index looks similar to the previous
one. we still have the same words and the same number of postings, and the first
number in each posting is the same as in the previous index. however, each post-
ing now has a second number. this second number is the number of times the
word appears in the document. this small amount of additional data allows us to
prefer s2 over s1 and s3 for the query    tropical fish   , since s2 contains    tropical   
twice and    fish    three times.

in this example, it may not be obvious that s2 is much better than s1 or s3,
but in general, word counts can be a powerful predictor of document relevance. in
particular, word counts can help distinguish documents that are about a particular

134

5 ranking with indexes

fig. 5.4. an inverted index, with word counts, for the documents in table 5.1

subject from those that discuss that subject in passing. imagine two documents,
one about tropical fish and another about tropical islands. the document about
tropical islands would probably contain the word    fish   , but only a few times. on
the other hand, the document about tropical fish would contain the word    fish   
many times. using word occurrence counts helps us rank the most relevant doc-
ument highest in this example.

5.3.3 positions

when looking for matches for a query like    tropical fish   , the location of the
words in the document is an important predictor of relevance. imagine a doc-
ument about food that included a section on tropical fruits followed by a section
on saltwater fish. so far, none of the indexes we have considered contain enough
information to tell us that this document is not relevant. although a document

and1:1aquarium3:1are3:14:1around1:1as2:1both1:1bright3:1coloration3:14:1derives4:1due3:1environments1:1   sh1:22:33:24:2   shkeepers2:1found1:1fresh2:1freshwater1:14:1from4:1generally4:1in1:14:1include1:1including1:1iridescence4:1marine2:1often2:13:1only2:1pigmented4:1popular3:1refer2:1referred2:1requiring2:1salt1:14:1saltwater2:1species1:1term2:1the1:12:1their3:1this4:1those2:1to2:23:1tropical1:22:23:1typically4:1use2:1water1:12:14:1while4:1with2:1world1:15.3 inverted indexes

135

fig. 5.5. an inverted index, with word positions, for the documents in table 5.1

that contains the words    tropical    and    fish    is likely to be relevant, we really want
to know if the document contains the exact phrase    tropical fish   .

to determine this, we can add position information to our index, as in fig-
ure 5.5. this index shares some structural characteristics with the previous in-
dexes, in that it has the same index terms and each list contains some postings.
these postings, however, are di   erent. each posting contains two numbers: a doc-
ument number first, followed by a word position. in the previous indexes, there
was just one posting per document. now there is one posting per word occur-
rence.

look at the long list for the word    fish   . in the other indexes, this list contained
just four postings. now the list contains nine postings. the first two postings tell
us that the word    fish    is the second word and fourth word in s1. the next three
postings tell us that    fish    is the seventh, eighteenth, and twenty-third word in s2.

and1,15aquarium3,5are3,34,14around1,9as2,21both1,13bright3,11coloration3,124,5derives4,7due3,7environments1,8   sh1,21,42,72,182,233,23,64,34,13   shkeepers2,1found1,5fresh2,13freshwater1,144,2from4,8generally4,15in1,64,1include1,3including1,12iridescence4,9marine2,22often2,23,10only2,10pigmented4,16popular3,4refer2,9referred2,19requiring2,12salt1,164,11saltwater2,16species1,18term2,5the1,102,4their3,9this4,4those2,11to2,82,203,8tropical1,11,72,62,173,1typically4,6use2,3water1,172,144,12while4,10with2,15world1,11136

5 ranking with indexes

fig. 5.6. aligning posting lists for    tropical    and    fish    to find the phrase    tropical fish   

this information is most interesting when we look at intersections with other
posting lists. using an intersection with the list for    tropical   , we find where the
phrase    tropical fish    occurs. in figure 5.6, the two inverted lists are lined up next
to each other. we see that the word    tropical    is the first word in s1, and    fish    is
the second word in s1, which means that s1 must start with the phrase    tropical
fish   . the word    tropical    appears again as the seventh word in s1, but    fish    does
not appear as the eighth word, so this is not a phrase match. in all, there are four
occurrences of the phrase    tropical fish    in the four sentences. the phrase matches
are easy to see in the figure; they happen at the points where the postings are lined
up in columns.

this same technique can be extended to find longer phrases or more general
proximity expressions, such as    find tropical within 5 words of fish.    suppose that
the word    tropical    appears at position p. we can then look in the inverted list
for    fish    for any occurrences between position p     5 and p + 5. any of those
occurrences would constitute a match.

5.3.4 fields and extents

real documents are not just lists of words. they have sentences and paragraphs
that separate concepts into logical units. some documents have titles and head-
ings that provide short summaries of the rest of the content. special types of doc-
uments have their own sections; for example, every email contains sender infor-
mation and a subject line. all of these are instances of what we will call document
fields, which are sections of documents that carry some kind of semantic meaning.
it makes sense to include information about fields in the index. for example,
suppose you have a professor named dr. brown. dr. brown sent you an email
about when course projects are due, but you can   t find it. you can type    brown   
into your email program   s search box, but the result you want will be mixed in
with other uses of the word    brown   , such as brown university or brown socks. a
search for    brown    in the from: line of the email will focus your search on exactly
what you want.

tropical1,11,72,62,173,1   sh1,21,42,72,182,233,23,64,34,135.3 inverted indexes

137

field information is useful even when it is not used explicitly in the query. ti-
tles and headings tend to be good summaries of the rest of a document. therefore,
if a user searches for    tropical fish   , it makes sense to prefer documents with the
title    tropical fish,    even if a document entitled    mauritius    mentions the words
   tropical    and    fish    more often. this kind of preference for certain document
fields can be integrated into the ranking function.

in order to handle these kinds of searches, the search engine needs to be able to
determine whether a word is in a particular field. one option is to make separate
inverted lists for each kind of document field. essentially, you could build one
index for document titles, one for document headings, and one for body text.
searching for words in the title is as simple as searching the title index. however,
finding a word in any section of the document is trickier, since you need to fetch
inverted lists from many di   erent indexes to make that determination.

another option is to store information in each word posting about where the
word occurred. for instance, we could specify that the number 0 indicates a title
and 1 indicates body text. each inverted list posting would then contain a 0 or a 1
at the end. this data could be used to quickly determine whether a posting was in
a title, and it would require only one bit per posting. however, if you have more
fields than just a title, the representation will grow.

both of these suggestions have problems when faced with more complicated
kinds of document structure. for instance, suppose we want to index books. some
books, like this one, have more than one author. somewhere in the xml descrip-
tion of this book, you might find:

<author>w. bruce croft</author>,
<author>donald metzler</author>, and
<author>trevor strohman</author>
suppose you would like to find books by an author named croft donald. if
you type the phrase query    croft donald    into a search engine, should this book
match? the words    croft    and    donald    appear in it, and in fact, they appear next
to each other. however, they are in two distinct author fields. this probably is
not a good match for the query    croft donald   , but the previous two methods for
dealing with fields (bits in the posting list, separate indexes) cannot make this kind
of distinction.

this is where extent lists come in. an extent is a contiguous region of a doc-
ument. we can represent these extents using word positions. for example, if the
title of a book started on the fifth word and ended just before the ninth word,

138

5 ranking with indexes

we could encode that as (5,9). for the author text shown earlier, we could write
author: (1,4), (4,6), (7,9). the (1,4) means that the first three words (   w. bruce
croft   ) constitute the first author, followed by the second author (   donald metz-
ler   ), which is two words. the word    and    is not in an author field, but the next
two words are, so the last posting is (7,9).

fig. 5.7. aligning posting lists for    fish    and title to find matches of the word    fish    in the
title field of a document.

figure 5.7 shows how this works in practice. here we have the same positions
posting list for    fish    that we used in the previous example. we also have an ex-
tent list for the title field. for clarity, there are gaps in the posting lists so that the
appropriate postings line up next to each other. at the very beginning of both
lists, we see that document 1 has a title that contains the first two words (1 and
2, ending just before the third word). we know that this title includes the word
   fish   , because the inverted list for    fish    tells us that    fish    is the second word in
document 1. if the user wants to find documents with the word    fish    in the title,
document 1 is a match. document 2 does not match, because its title ends just be-
fore the fifth word, but    fish    doesn   t appear until the seventh word. document
3 apparently has no title at all, so no matches are possible. document 4 has a title
that starts at the ninth word (perhaps the document begins with a date or an au-
thor declaration), and it does contain the word    fish   . in all, this example shows
two matching documents: 1 and 4.

this concept can be extended to all kinds of fields, such as headings, para-
graphs, or sentences. it can also be used to identify smaller pieces of text with
specific meaning, such as addresses or names, or even just to record which words
are verbs.

5.3.5 scores

if the inverted lists are going to be used to generate feature function values, why
not just store the value of the feature function? this is certainly possible, and some
very e   cient search engines do just this. this approach makes it possible to store
feature function values that would be too computationally intensive to compute

   sh1,21,42,72,182,233,23,64,34,13title1:(1,3)2:(1,5)4:(9,15)5.3 inverted indexes

139

during the query processing phase. it also moves complexity out of the query pro-
cessing engine and into the indexing code, where it may be more tolerable.

let   s make this more concrete. in the last section, there was an example about
how a document with the title    tropical fish    should be preferred over a docu-
ment    mauritius    for the query    tropical fish   , even if the mauritius document
contains the words    tropical    and    fish    many times. computing the scores that
reflect this preference requires some complexity at query evaluation time. the
postings for    tropical fish    have to be segregated into groups, so we know which
ones are in the title and which ones aren   t. then, we have to define some score for
the title postings and the non-title postings and mix those numbers together, and
this needs to be done for every document.

an alternate approach is to store the final value right in the inverted list. we
could make a list for    fish    that has postings like [(1:3.6), (3:2.2)], meaning that
the total feature value for    fish    in document 1 is 3.6, and in document 3 it is 2.2.
presumably the number 3.6 came from taking into account how many times    fish   
appeared in the title, in the headings, in large fonts, in bold, and in links to the
document. maybe the document doesn   t contain the word    fish    at all, but instead
many names of fish, such as    carp    or    trout   . the value 3.6 is then some indicator
of how much this document is about fish.

storing scores like this both increases and decreases the system   s flexibility. it
increases flexibility because computationally expensive scoring becomes possible,
since much of the hard work of scoring documents is moved into the index. how-
ever, flexibility is lost, since we can no longer change the scoring mechanism once
the index is built. more importantly, information about word proximity is gone
in this model, meaning that we can   t include phrase information in scoring unless
we build inverted lists for phrases, too. these precomputed phrase lists require
considerable additional space.

5.3.6 ordering

so far, we have assumed that the postings of each inverted list would be ordered
by document number. although this is the most popular option, this is not the
only way to order an inverted list. an inverted list can also be ordered by score,
so that the highest-scoring documents come first. this makes sense only when the
lists already store the score, or when only one kind of score is likely to be com-
puted from the inverted list. by storing scores instead of documents, the query
processing engine can focus only on the top part of each inverted list, where the

140

5 ranking with indexes

highest-scoring documents are recorded. this is especially useful for queries con-
sisting of a single word. in a traditional document-ordered inverted list, the query
processing engine would need to scan the whole list to find the top k scoring doc-
uments, whereas it would only need to read the first k postings in a score-sorted
list.

5.4 compression

there are many di   erent ways to store digital information. usually we make a sim-
ple distinction between persistent and transient storage. we use persistent stor-
age to store things in files and directories that we want to keep until we choose to
delete them. disks, cds, dvds, flash memory, and magnetic tape are commonly
used for this purpose. dynamic ram (random access memory), on the other
hand, is used to store transient information, which is information we need only
while the computer is running. we expect that when we turn o    the computer,
all of that information will vanish.

we can make finer distinctions between types of storage based on speed and
capacity. magnetic tape is slow, disks are faster, but dynamic ram is much faster.
modern computers are so fast that even dynamic ram isn   t fast enough to keep
up, so microprocessors contain at least two levels of cache memory. the very
fastest kind of memory makes up the processor registers. in a perfect world, we
could use registers or cache memory for all transient storage, but it is too expen-
sive to be practical.

the reality, then, is that modern computers contain a memory hierarchy. at
the top of the hierarchy we have memory that is tiny, but fast. the base consists
of memory that is huge, but slow. the performance of a search engine strongly
depends on how it makes use of the properties of each type of memory.

compression techniques are the most powerful tool for managing the mem-
ory hierarchy. the inverted lists for a large collection are themselves very large. in
fact, when it includes information about word position and document extents,
the index can be comparable in size3 to the document collection. compression
allows the same inverted list data to be stored in less space. the obvious ben-
efit is that this could reduce disk or memory requirements, which would save

3 as an example, indexes for trec collections built using the indri open source search
engine range from 25   50% of the size of the collection. the lower figure is for a col-
lection of web pages.

5.4 compression

141

money. more importantly, compression allows data to move up the memory hi-
erarchy. if index data is compressed by a factor of four, we can store four times
more useful data in the processor cache, and we can feed data to the processor
four times faster. on disk, compression also squeezes data closer together, which
reduces seek times. in multicore and multiprocessor systems, where many proces-
sors share one memory system, compressing data allows the processors to share
memory bandwidth more e   ciently.

unfortunately, nothing is free. the space savings of compression comes at a
cost: the processor must decompress the data in order to use it. therefore, it isn   t
enough to pick the compression technique that can store the most data in the
smallest amount of space. in order to increase overall performance, we need to
choose a compression technique that reduces space and is easy to decompress.

to see this mathematically, suppose some processor can process p inverted list
postings per second. this processor is attached to a memory system that can sup-
ply the processor with m postings each second. the number of postings processed
each second is then min(m, p). if p > m, then the processor will spend some of
its time waiting for postings to arrive from memory. if m > p, the memory sys-
tem will sometimes be idle.

suppose we introduce compression into the system. our compression system
has a compression ratio of r, meaning that we can now store r postings in the
same amount of space as one uncompressed posting. this lets the processor read
mr postings each second. however, the processor first needs to decompress each
posting before processing it. this slows processing by a decompression factor,
d, and lets the processor process dp postings each second. now we can process
min(mr, dp) postings each second.

when we use no compression at all, r = 1 and d = 1. any reasonable com-
pression technique gives us r > 1, but d < 1. we can see that compression is a
useful performance technique only when the p > m, that is, when the processor
can process inverted list data faster than the memory system can supply it. a very
simple compression scheme will raise r a little bit and reduce d a little bit. a com-
plicated compression scheme will raise r a lot, while reducing d a lot. ideally we
would like to pick a compression scheme such that min(mr, dp) is maximized,
which should happen when mr = dp.

in this section, we consider only lossless compression techniques. lossless tech-
niques store data in less space, but without losing information. there are also lossy
data compression techniques, which are often used for video, images, and audio.
these techniques achieve very high compression ratios (r in our previous discus-

142

5 ranking with indexes

sion), but do this by throwing away the least important data. inverted list prun-
ing techniques, which we discuss later, could be considered a lossy compression
technique, but typically when we talk about compression we mean only lossless
methods.

in particular, our goal with these compression techniques is to reduce the size
of the inverted lists we discussed previously. the compression techniques in this
section are particularly well suited for document numbers, word counts, and doc-
ument position information.

5.4.1 id178 and ambiguity

by this point in the book, you have already seen many examples of id203
distributions. compression techniques are based on probabilities, too. the fun-
damental idea behind compression is to represent common data elements with
short codes while representing uncommon data elements with longer codes. the
inverted lists that we have discussed are essentially lists of numbers, and with-
out compression, each number takes up the same amount of space. since some of
those numbers are more frequent than others, if we encode the frequent numbers
with short codes and the infrequent numbers with longer codes, we can end up
with space savings.

for example, consider the numbers 0, 1, 2, and 3. we can encode these num-

bers using two binary bits. a sequence of numbers, like:

0, 1, 0, 3, 0, 2, 0

can be encoded in a sequence of binary digits:

00 01 00 10 00 11 00

note that the spaces in the binary sequence are there to make it clear where each
number starts and stops, and are not actually part of the encoding.

in our example sequence, the number 0 occurs four times, whereas each of the
other numbers occurs just once. we may decide to save space by encoding 0 using
just a single 0 bit. our first attempt at an encoding might be:

0 01 0 10 0 11 0

this looks very successful because this encoding uses just 10 bits instead of the 14
bits used previously. this encoding is, however, ambiguous, meaning that it is not

5.4 compression

143

clear how to decode it. remember that the spaces in the code are only there for
our convenience and are not actually stored. if we add some di   erent spaces, we
arrive at a perfectly valid interpretation of this encoding:

0 01 01 0 0 11 0

which, when decoded, becomes:

0, 1, 1, 0, 0, 3, 0

unfortunately, this isn   t the data we encoded. the trouble is that when we see 010
in the encoded data, we can   t be sure whether (0, 2) or (1, 0) was encoded.

the uncompressed encoding was not ambiguous. we knew exactly where to
put the spaces because we knew that each number took exactly 2 bits. in our com-
pressed code, encoded numbers consume either 1 or 2 bits, so it is not clear where
to put the spaces. to solve this problem, we need to restrict ourselves to unam-
biguous codes, which are confusingly called both prefix codes and prefix-free codes.
an unambiguous code is one where there is only one valid way to place spaces in
encoded data.

let   s fix our code so that it is unambiguous:

number code
0
1
2
3

0
101
110
111

this results in the following encoding:

0 101 0 111 0 110 0

this encoding requires 13 bits instead of the 14 bits required by the uncom-
pressed version, so we are still saving some space. however, unlike the last code
we considered, this one is unambiguous. notice that if a code starts with 0, it con-
sumes 1 bit; if a code starts with 1, it is 3 bits long. this gives us a deterministic
algorithm for placing spaces in the encoded stream.

in the    exercises    section, you will prove that there is no such thing as an un-
ambiguous code that can compress every possible input; some inputs will get big-
ger. this is why it is so important to know something about what kind of data we

144

5 ranking with indexes

want to encode. in our example, we notice that the number 0 appears frequently,
and we can use that fact to reduce the amount of space that the encoded version
requires. id178 measures the predictability of the input. in our case, the input
seems somewhat predictable, because the number 0 is more likely to appear than
other numbers. we leverage this id178 to produce a usable code for our pur-
poses.

5.4.2 delta encoding

all of the coding techniques we will consider in this chapter assume that small
numbers are more likely to occur than large ones. this is an excellent assump-
tion for word count data; many words appear just once in a document, and some
appear two or three times. only a small number of words appear more than 10
times. therefore, it makes sense to encode small numbers with small codes and
large numbers with large codes.

however, document numbers do not share this property. we expect that a
typical inverted list will contain some small document numbers and some very
large document numbers. it is true that some documents contain more words,
and therefore will appear more times in the inverted lists, but otherwise there is
not a lot of id178 in the distribution of document numbers in inverted lists.

the situation is di   erent if we consider the di   erences between document
numbers instead of the document numbers themselves. remember that inverted
list postings are typically ordered by document number. an inverted list without
counts, for example, is just a list of document numbers, like these:

1, 5, 9, 18, 23, 24, 30, 44, 45, 48

since these document numbers are ordered, we know that each document number
in the sequence is more than the one before it and less than the one after it. this
fact allows us to encode the list of numbers by the di   erences between adjacent
document numbers:

1, 4, 4, 9, 5, 1, 6, 14, 1, 3

this encoded list starts with 1, indicating that 1 is the first document number. the
next entry is 4, indicating that the second document number is 4 more than the
first: 1 + 4 = 5. the third number, 4, indicates that the third document number
is 4 more than the second: 5 + 4 = 9.

this process is called delta encoding, and the di   erences are often called d-gaps.
notice that delta encoding does not define the bit patterns that are used to store

5.4 compression

145

the data, and so it does not save any space on its own. however, delta encoding is
particularly successful at changing an ordered list of numbers into a list of small
numbers. since we are about to discuss methods for compressing lists of small
numbers, this is a useful property.

before we move on, consider the inverted lists for the words    id178    and
   who.    the word    who    is very common, so we expect that most documents will
contain it. when we use delta encoding on the inverted list for    who,    we would
expect to see many small d-gaps, such as:

1, 1, 2, 1, 5, 1, 4, 1, 1, 3, ...

by contrast, the word    id178    rarely appears in text, so only a few documents
will contain it. therefore, we would expect to see larger d-gaps, such as:

109, 3766, 453, 1867, 992, ...

however, since    id178    is a rare word, this list of large numbers will not be very
long. in general, we will find that inverted lists for frequent terms compress very
well, whereas infrequent terms compress less well.

5.4.3 bit-aligned codes
the code we invented in section 5.4.1 is a bit-aligned code, meaning that the
breaks between the coded regions (the spaces) can happen after any bit posi-
tion. in this section we will describe some popular bit-aligned codes. in the next
section, we will discuss methods where code words are restricted to end on byte
boundaries. in all of the techniques we   ll discuss, we are looking at ways to store
small numbers in inverted lists (such as word counts, word positions, and delta-
encoded document numbers) in as little space as possible.

one of the simplest codes is the unary code. you are probably familiar with bi-
nary, which encodes numbers with two symbols, typically 0 and 1. a unary num-
ber system is a base-1 encoding, which means it uses a single symbol to encode
numbers. here are some examples:

number code
0
1
2
3
4
5

0
10
110
1110
11110
111110

146

5 ranking with indexes

in general, to encode a number k in unary, we output k 1s, followed by a 0. we

need the 0 at the end to make the code unambiguous.

this code is very e   cient for small numbers such as 0 and 1, but quickly be-
comes very expensive. for instance, the number 1023 can be represented in 10
binary bits, but requires 1024 bits to represent in unary code.

now we know about two kinds of numeric encodings. unary is convenient
because it is compact for small numbers and is inherently unambiguous. binary
is a better choice for large numbers, but it is not inherently unambiguous. a rea-
sonable compression scheme needs to encode frequent numbers with fewer bits
than infrequent numbers, which means binary encoding is not useful on its own
for compression.

elias-(cid:13) codes

the elias-   (elias gamma) code combines the strengths of unary and binary
codes. to encode a number k using this code, we compute two quantities:
    kd =    log2 k   
    kr = k     2

   log2 k   

suppose you wrote k in binary form. the first value, kd, is the number of binary
digits you would need to write. assuming k > 0, the leftmost binary digit of k is
1. if you erase that digit, the remaining binary digits are kr.

if we encode kd in unary and kr in binary (in kd binary digits), we get the

elias-   code. some examples are shown in table 5.2.

number (k) kd kr code

0 0
0 10 0
1 10 1
2 110 10
7 1110 111
0 11110 0000

1 0
2 1
3 1
6 2
15 3
16 4
255 7 127 11111110 1111111
1023 9 511 1111111110 111111111
table 5.2. elias-   code examples

5.4 compression

147

the trick with this code is that the unary part of the code tells us how many bits
to expect in the binary part. we end up with a code that uses no more bits than the
unary code for any number, and for numbers larger than 2, it uses fewer bits. the
savings for large numbers is substantial. we can, for example, now encode 1023
in 19 bits, instead of 1024 using just unary code.
for any number k, the elias-   code requires    log2k    + 1 bits for kd in unary
code and    log2k    bits for kr in binary. therefore, 2   log2k    + 1 bits are required
in all.

elias-(cid:14) codes

although the elias-   code is a major improvement on the unary code, it is not
ideal for inputs that might contain large numbers. we know that a number k can
be expressed in log2 k binary digits, but the elias-   code requires twice as many
bits in order to make the encoding unambiguous.

the elias-   code attempts to solve this problem by changing the way that kd
is encoded. instead of encoding kd in unary, we can encode kd + 1 in elias-  . in
particular, we split kd into:
    kdd =    log2(kd + 1)   
    kdr = (kd + 1)     2
notice that we use kd + 1 here, since kd may be zero, but log2 0 is undefined.

   log2(kd+1)   

we then encode kdd in unary, kdr in binary, and kr in binary. the value of kdd is
the length of kdr, and kdr is the length of kr, which makes this code unambiguous.
table 5.3 gives some examples of elias-   encodings.

number (k) kd kr kdd kdr code

1 0
0
2 1
0
3 1
1
6 2
2
15 3
7
16 4
0
255 7 127
1023 9 511

0
1
1
1
2
2
3
3

0 0
0 10 0 0
0 10 0 1
1 10 1 10
0 110 00 111
1 110 01 0000
0 1110 000 1111111
2 1110 010 111111111

table 5.3. elias-   code examples

148

5 ranking with indexes

elias-   sacrifices some e   ciency for small numbers in order to gain e   ciency
at encoding larger numbers. notice that the code for the number 2 has increased
to 4 bits instead of the 3 bits required by the elias-   code. however, for numbers
larger than 16, the elias-   code requires no more space than the elias-   code, and
for numbers larger than 32, the elias-   requires less space.
specifically, the elias-   code requires    log2(   log2 k    + 1)    + 1 bits for kdd in
unary, followed by    log2(   log2 k    + 1)    bits for kdr in binary, and    log2 k    bits
for kr in binary. the total cost is approximately 2 log2 log2 k + log2 k.

5.4.4 byte-aligned codes
even though a few tricks can help us decode bit-aligned codes quickly, codes of
variable bit length are cumbersome on processors that process bytes. the proces-
sor is built to handle bytes e   ciently, not bits, so it stands to reason that byte-
aligned codes would be faster in practice.

there are many examples of byte-aligned compression schemes, but we con-
sider only one popular method here. this is the code commonly known as v-byte,
which is an abbreviation for    variable byte length.    the v-byte method is very
similar to utf-8 encoding, which is a popular way to represent text (see section
3.5.1).

like the other codes we have studied so far, the v-byte method uses short codes
for small numbers and longer codes for longer numbers. however, each code is a
series of bytes, not bits. so, the shortest v-byte code for a single integer is one byte.
in some circumstances, this could be very space-ine   cient; encoding the number
1 takes eight times as much space in v-byte as in elias-  . typically, the di   erence
in space usage is not quite so dramatic.

the v-byte code is really quite simple. the low seven bits of each byte con-
tain numeric data in binary. the high bit is a terminator bit. the last byte of each
code has its high bit set to 1; otherwise, it is set to 0. any number that can be
represented in seven binary digits requires one byte to encode. more information
about space usage is shown in table 5.4.

some example encodings are shown in table 5.5. numbers less than 128 are
stored in a single byte in traditional binary form, except that the high bit is set.
for larger numbers, the least significant seven bits are stored in the first byte. the
next seven bits are stored in the next byte until all of the non-zero bits have been
stored.

storing compressed data with a byte-aligned code has many advantages over a
bit-aligned code. byte-aligned codes compress and decompress faster, since pro-

5.4 compression

149

number of bytes
k
1
k < 27
27     k < 214 2
214     k < 221 3
221     k < 228 4

table 5.4. space requirements for numbers encoded in v-byte

k
1
6
127
128
130

1 0000001
1 0000110
1 1111111
0 0000001 1 0000000
0 0000001 1 0000010
20000 0 0000001 0 0011100 1 0100000

binary code hexadecimal
81
86
ff
01 80
01 82
01 1c a0

table 5.5. sample encodings for v-byte

cessors (and programming languages) are designed to process bytes instead of bits.
for these reasons, the galago search engine associated with this book uses v-byte
exclusively for compression.

5.4.5 compression in practice

the compression techniques we have covered are used to encode inverted lists in
real retrieval systems. in this section, we   ll look at how galago uses compression
to encode inverted lists in the positionlistwriter class.

figure 5.5 illustrates how position information can be stored in inverted lists.

consider just the inverted list for tropical:

(1, 1)(1, 7)(2, 6)(2, 17)(3, 1)

in each pair, the first number represents the document and the second number
represents the word position. for instance, the third entry in this list states that
the word tropical is the sixth word in document 2. because it helps the example,
we   ll add (2, 197) to the list:

(1, 1)(1, 7)(2, 6)(2, 17)(2, 197)(3, 1)

150

5 ranking with indexes

we can group the positions for each document together so that each document
has its own entry, (document, count, [positions]), where count is the number of
occurrences in the document. our example data now looks like this:

(1, 2, [1, 7])(2, 3, [6, 17, 197])(3, 1, [1])

the word count is important because it makes this list decipherable even with-
out the parentheses and brackets. the count tells us how many positions lie within
the brackets, and we can interpret these numbers unambiguously, even if they
were printed as follows:

1, 2, 1, 7, 2, 3, 6, 17, 197, 3, 1, 1

however, we will leave the brackets in place for now for clarity.
these are small numbers, but with delta encoding we can make them smaller.
notice that the document numbers are sorted in ascending order, so we can safely
use delta encoding to encode them:

(1, 2, [1, 7])(1, 3, [6, 17, 197])(1, 1, [1])

the second entry now starts with a 1 instead of a 2, but this 1 means    this
document number is one more than the last document number.    since position
information is also sorted in ascending order, we can delta-encode the positions
as well:

(1, 2, [1, 6])(1, 3, [6, 11, 180])(1, 1, [1])

we can   t delta-encode the word counts, because they   re not in ascending order.
if we did delta-encode them, some of the deltas might be negative, and the com-
pression techniques we have discussed do not handle negative numbers without
some extra work.

now we can remove the brackets and consider this inverted list as just a list of

numbers:

1, 2, 1, 6, 1, 3, 6, 11, 180, 1, 1, 1

since most of these numbers are small, we can compress them with v-byte to

save space:

81 82 81 86 81 83 86 8b 01 b4 81 81 81

5.4 compression

151

the 01 b4 is 180, which is encoded in two bytes. the rest of the numbers were

encoded as single bytes, giving a total of 13 bytes for the entire list.

5.4.6 looking ahead

this section described three compression schemes for inverted lists, and there are
many others in common use. even though compression is one of the older areas
of computer science, new compression schemes are developed every year.

why are these new schemes necessary? remember at the beginning of this sec-
tion we talked about how compression allows us to trade processor computation
for data throughput. this means that the best choice for a compression algorithm
is tightly coupled with the state of modern cpus and memory systems. for a long
time, cpu speed was increasing much faster than memory throughput, so com-
pression schemes with higher compression ratios became more attractive. how-
ever, the dominant hardware trend now is toward many cpu cores with lower
clock speeds. depending on the memory throughput of these systems, lower com-
pression ratios may be attractive.

more importantly, modern cpus owe much of their speed to clever tricks
such as branch prediction, which helps the processor guess about how code will
execute. code that is more predictable can run much faster than unpredictable
code. many of the newest compression schemes are designed to make the decode
phase more predictable, and therefore faster.

5.4.7 skipping and skip pointers

for many queries, we don   t need all of the information stored in a particular in-
verted list. instead, it would be more e   cient to read just the small portion of the
data that is relevant to the query. skip pointers help us achieve that goal.

consider the boolean query    galago and animal   . the word    animal    occurs
in about 300 million documents on the web versus approximately 1 million for
   galago.    if we assume that the inverted lists for    galago    and    animal    are in doc-
ument order, there is a very simple algorithm for processing this query:

    let dg be the first document number in the inverted list for    galago.   
    let da be the first document number in the inverted list for    animal.   
    while there are still documents in the lists for    galago    and    animal,    loop:

    if dg < da, set dg to the next document number in the    galago    list.
    if da < dg, set da to the next document number in the    animal    list.

152

5 ranking with indexes

    if da = dg, the document da contains both    galago    and    animal   . move
both dg and da to the next documents in the inverted lists for    galago    and
   animal,    respectively.

unfortunately, this algorithm is very expensive. it processes almost all docu-
ments in both inverted lists, so we expect the computer to process this loop about
300 million times. over 99% of the processing time will be spent processing the
299 million documents that contain    animal    but do not contain    galago.   

we can change this algorithm slightly by skipping forward in the    animal    list.
every time we find that da < dg, we skip ahead k documents in the    animal    list
to a new document, sa. if sa < dg, we skip ahead by another k documents. we
do this until sa     dg. at this point, we have narrowed our search down to a range
of k documents that might contain dg, which we can search linearly.

how much time does the modified algorithm take? since the word    galago   
appears 1 million times, we know that the algorithm will perform 1 million lin-
ear searches of length k, giving an expected cost of 500, 000    k steps. we also
expect to skip forward 300, 000, 000/k times. this algorithm then takes about
500, 000    k + 300, 000, 000/k steps in total.

k steps
5 62.5 million
10 35 million
20 25 million
25 24.5 million
40 27.5 million
50 31 million
100 53 million

table 5.6. skip lengths (k) and expected processing steps

table 5.6 shows the number of processing steps required for some example
values of k. we get the best expected performance when we skip 25 documents at
a time. notice that at this value of k, we expect to have to skip forward 12 times
in the    animal    list for each occurrence of    galago.    this is because of the cost of
linear search: a larger value of id116 more elements to check in the linear search.
if we choose a binary search instead, the best value of k rises to about 208, with
about 9.2 million expected steps.

5.4 compression

153

if binary search combined with skipping is so much more e   cient, why even
consider linear search at all? the problem is compression. for binary search to
work, we need to be able to jump directly to elements in the list, but after com-
pression, every element could take a di   erent amount of space. in addition, delta
encoding may be used on the document numbers, meaning that even if we could
jump to a particular location in the compressed sequence, we would need to de-
compress everything up to that point in order to decode the document numbers.
this is discouraging because our goal is to reduce the amount of the list we need
to process, and it seems that compression forces us to decompress the whole list.
we can solve the compression problem with a list of skip pointers. skip pointer
lists are small additional data structures built into the index to allow us to skip
through the inverted lists e   ciently.

a skip pointer (d, p) contains two parts, a document number d and a byte
(or bit) position p. this means that there is an inverted list posting that starts at
position p, and that the posting immediately before it is for document d. notice
that this definition of the skip pointer solves both of our compression problems:
we can start decoding at position p, and since we know that d is the document
immediately preceding p, we can use it for decoding.

as a simple example, consider the following list of document numbers, un-

compressed:

5, 11, 17, 21, 26, 34, 36, 37, 45, 48, 51, 52, 57, 80, 89, 91, 94, 101, 104, 119

if we delta-encode this list, we end up with a list of d-gaps like this:

5, 6, 6, 4, 5, 9, 2, 1, 8, 3, 3, 1, 5, 23, 9, 2, 3, 7, 3, 15

we can then add some skip pointers for this list, using 0-based positions (that

is, the number 5 is at position 0 in the list):

(17, 3), (34, 6), (45, 9), (52, 12), (89, 15), (101, 18)

suppose we try decoding using the skip pointer (34, 6). we move to position
6 in the d-gaps list, which is the number 2. we add 34 to 2, to decode document
number 36.

more generally, if we want to find document number 80 in the list, we scan
the list of skip pointers until we find (52, 12) and (89, 15). 80 is larger than 52
but less than 89, so we start decoding at position 12. we find:

154

5 ranking with indexes

    52 + 5 = 57
    57 + 23 = 80

at this point, we have successfully found 80 in the list. if instead we were

searching for 85, we would again start at skip pointer (52, 12):

    52 + 5 = 57
    57 + 23 = 80
    80 + 9 = 89

at this point, since 85 < 89, we would know that 85 is not in the list.
in the analysis of skip pointers for the    galago and animal    example, the e   ec-
tiveness of the skip pointers depended on the fact that    animal    was much more
common than    galago.    we found that 25 was a good value for k given this query,
but we only get to choose one value for k for all queries. the best way to choose
k is to find the best possible k for some realistic sample set of queries. for most
collections and query loads, the optimal skip distance is around 100 bytes.

5.5 auxiliary structures

the inverted file is the primary data structure in a search engine, but usually other
structures are necessary for a fully functional system.

vocabulary and statistics

an inverted file, as described in this chapter, is just a collection of inverted lists.
to search the index, some kind of data structure is necessary to find the inverted
list for a particular term. the simplest way to solve this problem is to store each
inverted list as a separate file, where each file is named after the corresponding
search term. to find the inverted list for    dog,    the system can simply open the file
named dog and read the contents. however, as we saw in chapter 4, document
collections can have millions of unique words, and most of these words will occur
only once or twice in the collection. this means that an index, if stored in files,
would consist of millions of files, most of which are very small.

unfortunately, modern file systems are not optimized for this kind of stor-
age. a file system typically will reserve a few kilobytes of space for each file, even
though most files will contain just a few bytes of data. the result is a huge amount
of wasted space. as an example, in the ap89 collection, over 70,000 words occur

5.5 auxiliary structures

155

just once (see table 4.1). these inverted lists would require about 20 bytes each,
for a total of about 2mb of space. however, if the file system requires 1kb for
each file, the result is 70mb of space used to store 2mb of data. in addition, many
file systems still store directory information in unsorted arrays, meaning that file
lookups can be very slow for large file directories.

to fix these problems, inverted lists are usually stored together in a single file,
which explains the name inverted file. an additional directory structure, called
the vocabulary or lexicon, contains a lookup table from index terms to the byte
o   set of the inverted list in the inverted file.

in many cases, this vocabulary lookup table will be small enough to fit into
memory. in this case, the vocabulary data can be stored in any reasonable way on
disk and loaded into a hash table at search engine startup. if the search engine
needs to handle larger vocabularies, some kind of tree-based data structure, such
as a b-tree, should be used to minimize disk accesses during the search process.

galago uses a hybrid strategy for its vocabulary structure. a small file in each
index, called vocabulary, stores an abbreviated lookup table from vocabulary
terms to o   sets in the inverted file. this file contains just one vocabulary entry
for each 32k of data in the inverted file. therefore, a 32tb inverted file would
require less than 1gb of vocabulary space, meaning that it can always be stored
in memory for collections of a reasonable size. the lists in the inverted file are
stored in alphabetical order. to find an inverted list, the search engine uses bi-
nary search to find the nearest entry in the vocabulary table, and reads the o   set
from that entry. the engine then reads 32kb of the inverted file, starting at the
o   set. this approach finds each inverted list with just one disk seek.

to compute some feature functions, the index needs to contain certain vo-
cabulary statistics, such as the term frequency or document frequency (discussed
in chapter 4). when these statistics pertain to a specific term, they can be eas-
ily stored at the start of the inverted list. some of these statistics pertain to the
corpus, such as the total number of documents stored. when there are just a few
of these kinds of statistics, e   cient storage considerations can be safely ignored.
galago stores these collection-wide statistics in an xml file called manifest.

documents, snippets, and external systems

the search engine, as described so far, returns a list of document numbers and
scores. however, a real user-focused search engine needs to display textual infor-
mation about each document, such as a document title, url, or text summary

156

5 ranking with indexes

(chapter 6 explains this in more detail). in order to get this kind of information,
the text of the document needs to be retrieved.

in chapter 3, we saw some ways that documents can be stored for fast access.
there are many ways to approach this problem, but in the end, a separate system is
necessary to convert search engine results from numbers into something readable
by people.

5.6 index construction

before an index can be used for query processing, it has to be created from the text
collection. building a small index is not particularly di   cult, but as input sizes
grow, some index construction tricks can be useful. in this section, we will look at
simple in-memory index construction first, and then consider the case where the
input data does not fit in memory. finally, we will consider how to build indexes
using more than one computer.

5.6.1 simple construction

pseudocode for a simple indexer is shown in figure 5.8. the process involves only
a few steps. a list of documents is passed to the buildindex function, and the
function parses each document into tokens, as discussed in chapter 4. these to-
kens are words, perhaps with some additional processing, such as downcasing or
id30. the function removes duplicate tokens, using, for example, a hash ta-
ble. then, for each token, the function determines whether a new inverted list
needs to be created in i, and creates one if necessary. finally, the current docu-
ment number, n, is added to the inverted list.

the result is a hash table of tokens and inverted lists. the inverted lists are
just lists of integer document numbers and contain no special information. this
is enough to do very simple kinds of retrieval, as we saw in section 5.3.1.

as described, this indexer can be used for many small tasks   for example, in-
dexing less than a few thousand documents. however, it is limited in two ways.
first, it requires that all of the inverted lists be stored in memory, which may not
be practical for larger collections. second, this algorithm is sequential, with no
obvious way to parallelize it. the primary barrier to parallelizing this algorithm is
the hash table, which is accessed constantly in the inner loop. adding locks to the
hash table would allow parallelism for parsing, but that improvement alone will

5.6 index construction

157

    d is a set of text documents
    inverted list storage
    document numbering

    parse document into tokens

procedure b            i            (d)

i     hashtable()
n     0
for all documents d     d do

n     n + 1
t     parse(d)
remove duplicates from t
for all tokens t     t do
it     array()

if it       i then

end if
it.append(n)

end for

end for
return i

end procedure

fig. 5.8. pseudocode for a simple indexer

not be enough to make use of more than a handful of cpu cores. handling large
collections will require less reliance on memory and improved parallelism.

5.6.2 merging
the classic way to solve the memory problem in the previous example is by merg-
ing. we can build the inverted list structure i until memory runs out. when that
happens, we write the partial index i to disk, then start making a new one. at the
end of this process, the disk is filled with many partial indexes, i1, i2, i3, ..., in.
the system then merges these files into a single result.

by definition, it is not possible to hold even two of the partial index files in
memory at one time, so the input files need to be carefully designed so that they
can be merged in small pieces. one way to do this is to store the partial indexes in
alphabetical order. it is then possible for a merge algorithm to merge the partial
indexes using very little memory.

figure 5.9 shows an example of this kind of merging procedure. even though
this figure shows only two indexes, it is possible to merge many at once. the algo-
rithm is essentially the same as the standard merge sort algorithm. since both i1
and i2 are sorted, at least one of them points to the next piece of data necessary
to write to i. the data from the two files is interleaved to produce a sorted result.

158

5 ranking with indexes

fig. 5.9. an example of index merging. the first and second indexes are merged together
to produce the combined index.

since i1 and i2 may have used the same document numbers, the merge function
renumbers documents in i2.

this merging process can succeed even if there is only enough memory to store
two words (w1 and w2), a single inverted list posting, and a few file pointers. in
practice, a real merge function would read large chunks of i1 and i2, and then
write large chunks to i in order to use the disk most e   ciently.

this merging strategy also shows a possible parallel indexing strategy. if many
machines build their own partial indexes, a single machine can combine all of
those indexes together into a single, final index. however, in the next section,
we will explore more recent distributed indexing frameworks that are becoming
popular.

5.6.3 parallelism and distribution

the traditional model for search engines has been to use a single, fast machine to
create the index and process queries. this is still the appropriate choice for a large
number of applications, but it is no longer a good choice for the largest systems.
instead, for these large systems, it is increasingly popular to use many inexpen-
sive servers together and use distributed processing software to coordinate their
activities. mapreduce is a distributed processing tool that makes this possible.

two factors have forced this shift. first, the amount of data to index in the
largest systems is exploding. modern web search engines already index tens of bil-
lions of pages, but even larger indexes are coming. consider that if each person on
earth wrote one blog post each day, the web would increase in size by over two
trillion pages every year. optimistically, one typical modern computer can handle
a few hundred million pages, although not with the kind of response times that

aardvark2appleaardvarkactor1542683452469aardvark2appleactor1542683452469index aindex bcombined indexaardvark2appleaardvarkactor1542683452469index aindex b5.6 index construction

159

most users expect. this leaves a huge gulf between the size of the web and what we
can handle with current single-computer technology. note that this problem is
not restricted to a few major web search companies; many more companies want
to analyze the content of the web instead of making it available for public search.
these companies have the same scalability problem.

the second factor is simple economics. the incredible popularity of personal
computers has made them very powerful and inexpensive. in contrast, large com-
puters serve a very small market, and therefore have fewer opportunities to de-
velop economies of scale. over time, this di   erence in scale has made it di   cult
to make a computer that is much more powerful than a personal computer that
is still sold for a reasonable amount of money. many large information retrieval
systems ran on mainframes in the past, but today   s platform of choice consists of
many inexpensive commodity servers.

inexpensive servers have a few disadvantages when compared to mainframes.
first, they are more likely to break, and the likelihood of at least one server fail-
ure goes up as you add more servers. second, they are di   cult to program. most
programmers are well trained for single-threaded programming, less well trained
for threaded or multi-process programming, and not well trained at all for coop-
erative network programming. many programming toolkits have been developed
to help address this kind of problem. rpc, corba, java rmi, and soap have
been developed to allow function calls across machine boundaries. mpi provides
a di   erent abstraction, called message passing, which is popular for many scientific
tasks. none of these techniques are particularly robust against system failures, and
the programming models can be complex. in particular, these systems do not help
distribute data evenly among machines; that is the programmer   s job.

data placement

before diving into the mechanics of distributed processing, consider the problems
of handling huge amounts of data on a single computer. distributed processing
and large-scale data processing have one major aspect in common, which is that
not all of the input data is available at once. in distributed processing, the data
might be scattered among many machines. in large-scale data processing, most of
the data is on the disk. in both cases, the key to e   cient data processing is placing
the data correctly.

let   s take a simple example. suppose you have a text file that contains data
about credit card transactions. each line of the file contains a credit card number

160

5 ranking with indexes

and an amount of money. how might you determine the number of unique credit
card numbers in the file?

if the file is not very big, you could read each line, parse the credit card num-
ber, and store the credit card number in a hash table. once the entire file had been
read, the hash table would contain one entry for each unique credit card number.
counting the number of entries in the hash table would give you the answer. un-
fortunately, for a big file, the hash table would be too large to store in memory.

now suppose you had the very same credit card data, but the transactions in
the file were ordered by credit card number. counting the number of unique
credit card numbers in this case is very simple. each line in the file is read and
the credit card number on the line is parsed. if the credit card number found is
di   erent than the one on the line before it, a counter is incremented. when the
end of the file is reached, the counter contains a count of the unique credit card
numbers in the file. no hash table is necessary for this to work.

now, back to distributed computation. suppose you have more than one com-
puter to use for this counting task. you can split the big file of transactions into
small batches of transactions. each computer can count its fraction, and then the
results can be merged together to produce a final result.

initially, we start with an unordered file of transactions. we split that file into
small batches of transactions and count the unique credit card numbers in each
batch. how do we combine the results? we could add the number of credit card
numbers found in each batch, but this is incorrect, since the same credit card num-
ber might appear in more than one batch, and therefore would be counted more
than once in the final total. instead, we would need to keep a list of the unique
credit card numbers found in each batch, and then merge those lists together to
make a final result list. the size of this final list is the number of unique credit card
numbers in the whole set.

in contrast, suppose the transactions are split into batches with more care, so
that all transactions made with the same credit card end up in the same batch.
with this extra restriction, each batch can be counted individually, and then the
counts from each batch can be added to make a final result. no merge is necessary,
because there is no possibility of double-counting. each credit card number will
appear in precisely one batch.

these examples might be a little bit tedious, but the point is that proper data
grouping can radically change the performance characteristics of a task. using a
sorted input file made the counting task easy, reduced the amount of memory
needed to nearly zero, and made it possible to distribute the computation easily.

mapreduce

5.6 index construction

161

mapreduce is a distributed programming framework that focuses on data place-
ment and distribution. as we saw in the last few examples, proper data placement
can make some problems very simple to compute. by focusing on data placement,
mapreduce can unlock the parallelism in some common tasks and make it easier
to process large amounts of data.

mapreduce gets its name from the two pieces of code that a user needs to
write in order to use the framework: themapper and thereducer. the mapreduce
library automatically launches many mapper and reducer tasks on a cluster of
machines. the interesting part about mapreduce, though, is the path the data
takes between the mapper and the reducer.

before we look at how the mapper and reducer work, let   s look at the founda-
tions of the mapreduce idea. the functions map and reduce are commonly found
in functional languages. in very simple terms, the map function transforms a list
of items into another list of items of the same length. the reduce function trans-
forms a list of items into a single item. the mapreduce framework isn   t quite
so strict with its definitions: both mappers and reducers can return an arbitrary
number of items. however, the general idea is the same.

fig. 5.10. mapreduce

mapreduceshuffleinputoutput162

5 ranking with indexes

we assume that the data comes in a set of records. the records are sent to the
mapper, which transforms these records into pairs, each with a key and a value.
the next step is the shu   e, which the library performs by itself. this operation
uses a hash function so that all pairs with the same key end up next to each other
and on the same machine. the final step is the reduce stage, where the records are
processed again, but this time in batches, meaning all pairs with the same key are
processed at once. the mapreduce steps are summarized in figure 5.10.

procedure m      c               c            (input)

while not input.done() do
record     input.next()
card     record.card
amount     record.amount
emit(card, amount)

end while
end procedure

fig. 5.11. mapper for a credit card summing algorithm

procedure r               c               c            (key, values)

total     0
card     key
while not values.done() do
amount     values.next()
total     total + amount

end while
emit(card, total)

end procedure

fig. 5.12. reducer for a credit card summing algorithm

the credit card data example we saw in the previous section works well as
a mapreduce task. in the mapper (figure 5.11), each record is split into a key
(the credit card number) and a value (the money amount in the transaction). the
shu   e stage sorts the data so that the records with the same credit card number
end up next to each other. the reduce stage emits a record for each unique credit

5.6 index construction

163

card number, so the total number of unique credit card numbers is the number of
records emitted by the reducer (figure 5.12).

typically, we assume that both the mapper and reducer are idempotent. by
idempotent, we mean that if the mapper or reducer is called multiple times on
the same input, the output will always be the same. this idempotence allows the
mapreduce library to be fault tolerant. if any part of the computation fails, per-
haps because of a hardware machine failure, the mapreduce library can just pro-
cess that part of the input again on a di   erent machine. even when machines
don   t fail, sometimes machines can be slow because of misconfiguration or slowly
failing parts. in this case, a machine that appears to be normal could return re-
sults much more slowly than other machines in a cluster. to guard against this,
as the computation nears completion, the mapreduce library issues backup map-
pers and reducers that duplicate the processing done on the slowest machines.
this ensures that slow machines don   t become the bottleneck of a computation.
the idempotence of the mapper and reducer are what make this possible. if the
mapper or reducer modified files directly, for example, multiple copies of them
could not be run simultaneously.

let   s look at the problem of indexing a corpus with mapreduce. in our simple

indexer, we will store inverted lists with word positions.

procedure m      d                        t   p                     (input)

while not input.done() do

document     input.next()
number     document.number
position     0
tokens     parse(document)
for each word w in tokens do

emit(w, document:position)
position = position + 1

end for
end while
end procedure

fig. 5.13. mapper for documents

mapdocumentstopostings (figure 5.13) parses each document in the input.
at each word position, it emits a key/value pair: the key is the word itself, and
the value is document:position, which is the document number and the position

164

5 ranking with indexes

procedure r               p                     t   l            (key, values)

word     key
writeword(word)
while not input.done() do

encodeposting(values.next())

end while
end procedure

fig. 5.14. reducer for word postings

concatenated together. when reducepostingstolists (figure 5.14) is called, the
emitted postings have been shu   ed so that all postings for the same word are
together. the reducer calls writeword to start writing an inverted list and then
uses encodeposting to write each posting.

5.6.4 update

so far, we have assumed that indexing is a batch process. this means that a set of
documents is given to the indexer as input, the indexer builds the index, and then
the system allows users to run queries. in practice, most interesting document col-
lections are constantly changing. at the very least, collections tend to get bigger
over time; every day there is more news and more email. in other cases, such as
web search or file system search, the contents of documents can change over time
as well. a useful search engine needs to be able to respond to dynamic collections.
we can solve the problem of update with two techniques: index merging and
result merging. if the index is stored in memory, there are many options for quick
index update. however, even if the search engine is evaluating queries in mem-
ory, typically the index is stored on a disk. inserting data in the middle of a file
is not supported by any common file system, so direct disk-based update is not
straightforward. we do know how to merge indexes together, though, as we saw
in section 5.6.2. this gives us a simple approach for adding data to the index: make
a new, smaller index (i2) and merge it with the old index (i1) to make a new in-
dex containing all of the data (i). postings in i1 for any deleted documents can
be ignored during the merge phase so they do not appear in i.

index merging is a reasonable update strategy when index updates come in
large batches, perhaps many thousands of documents at a time. for single docu-
ment updates, it isn   t a very good strategy, since it is time-consuming to write the
entire index to disk. for these small updates, it is better to just build a small index

5.7 query processing

165

for the new data, but not merge it into the larger index. queries are evaluated sep-
arately against the small index and the big index, and the result lists are merged to
find the top k results.

result merging solves the problem of how to handle new documents: just put
them in a new index. but how do we delete documents from the index? the com-
mon solution is to use a deleted document list. during query processing, the sys-
tem checks the deleted document list to make sure that no deleted documents
enter the list of results shown to the user. if the contents of a document change,
we can delete the old version from the index by using a deleted document list and
then add a new version to the recent documents index.

results merging allows us to consider a small, in-memory index structure to
hold new documents. this in-memory structure could be a hash table of arrays,
as shown in figure 5.8, and therefore would be simple and quick to update, even
with only a single document.

to gain even more performance from the system, instead of using just two
indexes (an in-memory index and a disk-based index), we can use many indexes.
using too many indexes is a bad idea, since each new index slows down query
processing. however, using too few indexes results in slow index build throughput
because of excessive disk tra   c. a particularly elegant solution to this problem is
geometric partitioning. in geometric partitioning, the smallest index, i0, contains
about as much data as would fit in memory. the next index, i1, contains about r
times as much data as i1. if m is the amount of bytes of memory in the machine,
index in then contains between mrn and (m + 1)rn bytes of data. if index in
ever contains more than (m + 1)rn, it is merged into index in+1. if r = 2, the
system can hold 1000m bytes of index data using just 10 indexes.

5.7 query processing

once an index is built, we need to process the data in it to produce query results.
even with simple algorithms, processing queries using an index is much faster than
it is without one. however, clever algorithms can boost query processing speed by
ten to a hundred times over the simplest versions. we will explore the simplest two
query processing techniques first, called document-at-a-time and term-at-a-time,
and then move on to faster and more flexible variants.

166

5 ranking with indexes

5.7.1 document-at-a-time evaluation

document-at-a-time retrieval is the simplest way, at least conceptually, to per-
form retrieval with an inverted file. figure 5.15 is a picture of document-at-a-time
retrieval for the query    salt water tropical   . the inverted lists are shown horizon-
tally, although the postings have been aligned so that each column represents a
di   erent document. the inverted lists in this example hold word counts, and the
score, for this example, is just the sum of the word counts in each document. the
vertical gray lines indicate the di   erent steps of retrieval. in the first step, all the
counts for the first document are added to produce the score for that document.
once the scoring for the first document has completed, the second document is
scored, then the third, and then the fourth.

fig. 5.15. document-at-a-time query evaluation. the numbers (x:y) represent a docu-
ment number (x) and a word count (y).

figure 5.16 shows a pseudocode implementation of this strategy. the param-
eters are q, the query; i, the index; f and g, the sets of feature functions; and k,
the number of documents to retrieve. this algorithm scores documents using the
abstract model of ranking described in section 5.2. however, in this simplified
example, we assume that the only non-zero feature values for g(q) correspond to
the words in the query. this gives us a simple correspondence between inverted
lists and features: there is one list for each query term, and one feature for each
list. later in this chapter we will explore structured queries, which are a standard
way of moving beyond this simple model.

for each word wi in the query, an inverted list is fetched from the index. these
inverted lists are assumed to be sorted in order by document number. the invert-
edlist object starts by pointing at the first posting in each list. all of the fetched
inverted lists are stored in an array, l.

1:2tropical2:23:11:1salt4:11:1water2:14:11:4score2:33:14:25.7 query processing

167

procedure d                     a   at         r                        (q, i, f, g, k)

l     array()
r     priorityqueue(k)
for all terms wi in q do

li     invertedlist(wi, i)
l.add( li )

end for
for all documents d     i do

sd     0
for all inverted lists li in l do
sd     sd + gi(q)fi(li)

if li.getcurrentdocument() = d then

end if
li.movepastdocument( d )

end for
r.add( sd, d )

end for
return the top k results from r

end procedure

    update the document score

fig. 5.16. a simple document-at-a-time retrieval algorithm

in the main loop, the function loops once for each document in the collection.
at each document, all of the inverted lists are checked. if the document appears
in one of the inverted lists, the feature function fi is evaluated, and the docu-
ment score sd is computed by adding up the weighted function values. then, the
inverted list pointer is moved to point at the next posting. at the end of each doc-
ument loop, a new document score has been computed and added to the priority
queue r.

for clarity, this pseudocode is free of even simple performance-enhancing
changes. realistically, however, the priority queue r only needs to hold the top
k results at any one time. if the priority queue ever contains more than k results,
the lowest-scoring documents can be removed until only k remain, in order to
save memory. also, looping over all documents in the collection is unnecessary;
we can change the algorithm to score only documents that appear in at least one
of the inverted lists.

the primary benefit of this method is its frugal use of memory. the only major
use of memory comes from the priority queue, which only needs to store k entries

168

5 ranking with indexes

at a time. however, in a realistic implementation, large portions of the inverted
lists would also be bu   ered in memory during evaluation.

5.7.2 term-at-a-time evaluation

figure 5.17 shows term-at-a-time retrieval, using the same query, scoring func-
tion, and inverted list data as in the document-at-a-time example (figure 5.15).
notice that the computed scores are exactly the same in both figures, although
the structure of each figure is di   erent.

fig. 5.17. term-at-a-time query evaluation

as before, the gray lines indicate the boundaries between each step. in the first
step, the inverted list for    salt    is decoded, and partial scores are stored in accumu-
lators. these scores are called partial scores because they are only a part of the final
document score. the accumulators, which get their name from their job, accumu-
late score information for each document. in the second step, partial scores from
the accumulators are combined with data from the inverted list for    water    to
produce a new set of partial scores. after the data from the list for    tropical    is
added in the third step, the scoring process is complete.

the figure implies that a new set of accumulators is created for each list. al-
though this is one possible implementation technique, in practice accumulators
are stored in a hash table. the information for each document is updated as the

1:1salt4:11:1water2:14:11:1partial scores4:11:2new partial scores4:22:11:1old partial scores4:11:2tropical2:23:11:2old partial scores4:22:11:4final scores2:33:1 4:25.7 query processing

169

inverted list data is processed. the hash table contains the final document scores
after all inverted lists have been processed.

procedure t         a   at         r                        (q, i, f, g k)

a     hashtable()
l     array()
r     priorityqueue(k)
for all terms wi in q do

li     invertedlist(wi, i)
l.add( li )

end for
for all lists li     l do

while li is not finished do

d     li.getcurrentdocument()
ad     ad + gi(q)f (li)
li.movetonextdocument()

end while

end for
for all accumulators ad in a do

sd     ad
r.add( sd, d )

end for
return the top k results from r

end procedure

    accumulator contains the document score

fig. 5.18. a simple term-at-a-time retrieval algorithm

the term-at-a-time retrieval algorithm for the abstract ranking model (fig-
ure 5.18) is similar to the document-at-a-time version at the start. it creates a pri-
ority queue and fetches one inverted list for each term in the query, just like the
document-at-a-time algorithm. however, the next step is di   erent. instead of a
loop over each document in the index, the outer loop is over each list. the inner
loop then reads each posting of the list, computing the feature functions fi and gi
and adding its weighted contribution to the accumulator ad. after the main loop
completes, the accumulators are scanned and added to a priority queue, which de-
termines the top k results to be returned.

the primary disadvantage of the term-at-a-time algorithm is the memory us-
age required by the accumulator table a. remember that the document-at-a-time

170

5 ranking with indexes

strategy requires only the small priority queue r, which holds a limited number
of results. however, the term-at-a-time algorithm makes up for this because of its
more e   cient disk access. since it reads each inverted list from start to finish, it
requires minimal disk seeking, and it needs very little list bu   ering to achieve high
speeds. in contrast, the document-at-a-time algorithm switches between lists and
requires large list bu   ers to help reduce the cost of seeking.

in practice, neither the document-at-a-time nor term-at-a-time algorithms
are used without additional optimizations. these optimizations dramatically im-
prove the running speed of the algorithms, and can have a large e   ect on the mem-
ory footprint.

5.7.3 optimization techniques

there are two main classes of optimizations for query processing. the first is to
read less data from the index, and the second is to process fewer documents. the
two are related, since it would be hard to score the same number of documents
while reading less data. when using feature functions that are particularly com-
plex, focusing on scoring fewer documents should be the main concern. for sim-
ple feature functions, the best speed comes from ignoring as much of the inverted
list data as possible.

list skipping

in section 5.4.7, we covered skip pointers in inverted lists. this kind of forward
skipping is by far the most popular way to ignore portions of inverted lists (figure
5.19). more complex approaches (for example, tree structures) are also possible
but not frequently used.

fig. 5.19. skip pointers in an inverted list. the gray boxes show skip pointers, which point
into the white boxes, which are inverted list postings.

skip pointers do not improve the asymptotic running time of reading an in-
verted list. suppose we have an inverted list that is n bytes long, but we add skip
pointers after each c bytes, and the pointers are k bytes long. reading the entire

5.7 query processing

171

list requires reading (cid:2)(n) bytes, but jumping through the list using the skip point-
ers requires (cid:2)(kn/c) time, which is equivalent to (cid:2)(n). even though there is
no asymptotic gain in runtime, the factor of c can be huge. for typical values of
c = 100 and k = 4, skipping through a list results in reading just 2.5% of the
total data.

notice that as c gets bigger, the amount of data you need to read to skip
through the list drops. so, why not make c as big as possible? the problem is that
if c gets too large, the average performance drops. let   s look at this problem in
more detail.

suppose you want to find p particular postings in an inverted list, and the list is
n bytes long, with k-byte skip pointers located at c-byte intervals. therefore, there
are n/c total intervals in the list. to find those p postings, we need to read kn/c
bytes in skip pointers, but we also need to read data in p intervals. on average, we
assume that the postings we want are about halfway between two skip pointers,
so we read an additional pc/2 bytes to find those postings. the total number of
bytes read is then:

kn
c

+

pc
2

notice that this analysis assumes that p is much smaller than n/c; that   s what
allows us to assume that each posting lies in its own interval. as p grows closer
to n/c, it becomes likely that some of the postings we want will lie in the same
intervals. however, notice that once p gets close to n/c, we need to read almost
all of the inverted list, so the skip pointers aren   t very helpful.

coming back to the formula, you can see that while a larger value of c makes
the first term smaller, it also makes the second term bigger. therefore, picking the
perfect value for c depends on the value of p, and we don   t know what p is until
a query is executed. however, it is possible to use previous queries to simulate
skipping behavior and to get a good estimate for c. in the exercises, you will be
asked to plot some of graphs of this formula and to solve for the equilibrium point.
although it might seem that list skipping could save on disk accesses, in prac-
tice it rarely does. modern disks are much better at reading sequential data than
they are at skipping to random locations. because of this, most disks require a skip
of about 100,000 postings before any speedup is seen. even so, skipping is still use-
ful because it reduces the amount of time spent decoding compressed data that has
been read from disk, and it dramatically reduces processing time for lists that are
cached in memory.

172

5 ranking with indexes

conjunctive processing

the simplest kind of query optimization is conjunctive processing. by conjunctive
processing, we just mean that every document returned to the user needs to con-
tain all of the query terms. conjunctive processing is the default mode for many
web search engines, in part because of speed and in part because users have come to
expect it. with short queries, conjunctive processing can actually improve e   ec-
tiveness and e   ciency simultaneously. in contrast, search engines that use longer
queries, such as entire paragraphs, will not be good candidates for conjunctive
processing.

conjunctive processing works best when one of the query terms is rare, as in
the query    fish locomotion   . the word    fish    occurs about 100 times as often as
the word    locomotion   . since we are only interested in documents that contain
both words, the system can skip over most of the inverted list for    fish    in order
to find only the postings in documents that also contain the word    locomotion   .
conjunctive processing can be employed with both term-at-a-time and docu-
ment-at-a-time systems. figure 5.20 shows the updated term-at-a-time algorithm
for conjunctive processing. when processing the first term, (i = 0), processing
proceeds normally. however, for the remaining terms, (i > 0), the algorithm
processes postings starting at line ??. it checks the accumulator table for the next
document that contains all of the previous query terms, and instructs list li to
skip forward to that document if there is a posting for it (line ??). if there is a
posting, the accumulator is updated. if the posting does not exist, the accumulator
is deleted (line ??).

the document-at-a-time version (figure 5.21) is similar to the old document-
at-a-time version, except in the inner loop. it begins by finding the largest docu-
ment d currently pointed to by an inverted list (line 13). this document d is not
guaranteed to contain all the query terms, but it is a reasonable candidate. the
next loop tries to skip all lists forward to point at d (line 16). if this is not success-
ful, the loop terminates and another document d is chosen. if it is successful, the
document is scored and added to the priority queue.

in both algorithms, the system runs fastest when the first list (l0) is the shortest
and the last list (ln) is the longest. this results in the biggest possible skip distances
in the last list, which is where skipping will help most.

5.7 query processing

173

else

if i = 0 then

end for
for all lists li     l do

li     invertedlist(wi, i)
l.add( li )

d0        1
while li is not finished do

a     map()
l     array()
r     priorityqueue(k)
for all terms wi in q do

d     li.getcurrentdocument()
ad     ad + gi(q)f (li)
li.movetonextdocument()
d     li.getcurrentdocument()
        a.getnextaccumulator(d)
d
a.removeaccumulatorsbetween(d0,d
if d = d

1: procedure t         a   at         r                        (q, i, f, g, k)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35: end procedure

ad     ad + gi(q)f (li)
li.movetonextdocument()

end for
for all accumulators ad in a do

end for
return the top k results from r

sd     ad
r.add( sd, d )

liskipforwardtodocument(d

end if
end while

end if
d0     d

then

else

)

   

   

   

   

)

    accumulator contains the document score

fig. 5.20. a term-at-a-time retrieval algorithm with conjunctive processing

174

5 ranking with indexes

end if

li     invertedlist(wi, i)
l.add( li )

if li.getcurrentdocument() > d then
d     li.getcurrentdocument()

sd     0
for all inverted lists li in l do

l     array()
r     priorityqueue(k)
for all terms wi in q do

end for
d        1
while all lists in l are not finished do

1: procedure d                     a   at         r                        (q, i, f, g, k)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30: end procedure

end for
if d >    1 then r.add( sd, d )
end if
end while
return the top k results from r

sd     sd + gi(q)fi(li)
li.movepastdocument( d )
d        1
break

li.skipforwardtodocument(d)
if li.getcurrentdocument() = d then

end for
for all inverted lists li in l do

end if

else

    update the document score

fig. 5.21. a document-at-a-time retrieval algorithm with conjunctive processing

threshold methods

so far, the algorithms we have considered do not do much with the parameter k
until the very last statement. remember that k is the number of results requested
by the user, and for many search applications this number is something small, such
as 10 or 20. because of this small value of k, most documents in the inverted lists

5.7 query processing

175

will never be shown to the user. threshold methods focus on this k parameter in
order to score fewer documents.

in particular, notice that for every query, there is some minimum score that
each document needs to reach before it can be shown to the user. this minimum
score is the score of the kth-highest scoring document. any document that does
not score at least this highly will never be shown to the user. in this section, we will
use the greek letter tau (  ) to represent this value, which we call the threshold.

if we could know the appropriate value for    before processing the query, many
query optimizations would be possible. for instance, since a document needs a
score of at least    in order to be useful to the user, we could avoid adding docu-
ments to the priority queue (in the document-at-a-time case) that did not achieve
a score of at least   . in general, we could safely ignore any document with a score
less than   .

   . we want   

coming up with an estimate for   

unfortunately, we don   t know how to compute the true value of    without
evaluating the query, but we can approximate it. these approximations will be
          , so that we can safely ignore any document with a score
called   
    gets to   , the faster our algorithm
   . of course, the closer our estimate   
less than   
will run, since it can ignore more documents.
    is easy with a document-at-a-time strategy.
remember that r maintains a list of the top k highest-scoring documents seen
    to the score of the lowest-scoring
so far in the evaluation process. we can set   
document currently in r, assuming r already has k documents in it. with term-
at-a-time evaluation, we don   t have full document scores until the query evalua-
    to be the kth-largest score in
tion is almost finished. however, we can still set   
the accumulator table.

maxscore

   , it is possible to start ignoring some of the data
with reasonable estimates for   
   , represents a lower bound on the score a doc-
in the inverted lists. this estimate,   
ument needs in order to enter the final ranked list. therefore, with a little bit of
clever math, we can ignore parts of the inverted lists that will not generate docu-
ment scores above   

let   s look more closely at how this might happen with a simple example. con-
sider the query    eucalyptus tree   . the word    tree    is about 100 times more com-
mon than the word    eucalyptus   , so we expect that most of the time we spend
evaluating this query will be spent scoring documents that contain the word    tree   

   .

176

5 ranking with indexes

and not    eucalyptus   . this is a poor use of time, since we   re almost certain to find
a set of top k documents that contain both words.

fig. 5.22. maxscore retrieval with the query    eucalyptus tree   . the gray boxes indicate
postings that can be safely ignored during scoring.

figure 5.22 shows this e   ect in action. we see the inverted lists for    eucalyp-
tus    and    tree    extending across the page, with the postings lined up by document,
as in previous figures in this chapter. this figure shows that there are many doc-
uments that contain the word    tree    and don   t contain the word    eucalyptus   .
suppose that the indexer computed the largest partial score in the    tree    list, and
that value is called   tree. this is the maximum score (hence maxscore) that any
document that contains just this word could have.

suppose that we are interested only in the top three documents in the ranked
list (i.e., k is 3). the first scored document contains just the word    tree   . the next
    to represent
three documents contain both    eucalyptus    and    tree   . we will use   
the lowest score from these three documents. at this point, it is highly likely that
   
    is the score of a document that contains both query terms,
  
whereas   tree is a query score for a document that contains just one of the query
terms. this is where the gray boxes come into the story. once   
>   tree, we can
safely skip over all of the gray postings, since we have proven that these documents
will not enter the final ranked list.

>   tree, because   

the postings data in the figure is fabricated, but for real inverted lists for    euca-
lyptus    and    tree   , 99% of the postings for    tree    would be gray boxes, and there-
fore would be safe to ignore. this kind of skipping can dramatically reduce query
times without a   ecting the quality of the query results.

   

early termination

the maxscore approach guarantees that the result of query processing will be ex-
actly the same in the optimized version as it is without optimization. in some
cases, however, it may be interesting to take some risks with quality and process
queries in a way that might lead to di   erent results than the same queries in an
unoptimized system.

eucalyptustree5.7 query processing

177

why might we choose to do this? one reason is that some queries are much,
much more expensive than others. consider the phrase query    to be or not to
be   . this query uses very common terms that would have very long inverted lists.
running this query to completion could severely reduce the amount of system
resources available to serve other queries. truncating query processing for this ex-
pensive query can help ensure fairness for others using the same system.

another reason is that maxscore is necessarily conservative. it will not skip
over regions of the inverted list that might have a usable candidate document.
because of this, maxscore can spend a lot of time looking for a document that
might not exist. taking a calculated risk to ignore these improbable documents
can pay o    in decreased system resource consumption.

how might early termination work? in term-at-a-time systems, we can termi-
nate processing by simply ignoring some of the very frequent query terms. this is
not so di   erent from using a stopword list, except that in this case we would be
ignoring words that usually would not be considered stopwords. alternatively,
we might decide that after some constant number of postings have been read, no
other terms will be considered. the reasoning here is that, after processing a sub-
stantial number of postings, the ranking should be fairly well established. reading
more information will only change the rankings a little. this is especially true for
queries with many (e.g., hundreds) of terms, which can happen when query ex-
pansion techniques are used.

in document-at-a-time systems, early termination means ignoring the docu-
ments at the very end of the inverted lists. this is a poor idea if the documents are
sorted in random order, but this does not have to be the case. instead, documents
could be sorted in order by some quality metric, such as id95. terminating
early in that case would mean ignoring documents that are considered lower qual-
ity than the documents that have already been scored.

list ordering
so far, all the examples in this chapter assume that the inverted lists are stored
in the same order, by document number. if the document numbers are assigned
randomly, this means that the document sort order is random. the net e   ect is
that the best documents for a query can easily be at the very end of the lists. with
good documents scattered throughout the list, any reasonable query processing
algorithm must read or skip through the whole list to make sure that no good
documents are missed. since these lists can be long, it makes sense to consider a
more intelligent ordering.

178

5 ranking with indexes

one way to improve document ordering is to order documents based on doc-
ument quality, as we discussed in the last section. there are plenty of quality met-
rics that could be used, such as id95 or the total number of user clicks. if the
smallest document numbers are assigned to the highest-quality documents, it be-
comes reasonable to consider stopping the search early if many good documents
have been found. the threshold techniques from the maxscore section can be
used here. if we know that documents in the lists are decreasing in quality, we can
compute an upper bound on the scores of the documents remaining in the lists at
    rises above the highest possible remaining
every point during retrieval. when   
document score, retrieval can be stopped safely without harming e   ectiveness.

another option is to order each list by partial score. for instance, for the
   food    list, we could store documents that contain many instances of the word
   food    first. in a web application, this may correspond to putting restaurant pages
early in the inverted list. for a    dog    list, we could store pages about dogs (i.e.,
containing many instances of    dog   ) first. evaluating a query about food or dogs
then becomes very easy. other queries, however, can be more di   cult. for ex-
ample, how do we evaluate the query    dog food   ? the best way to do it is to use
an accumulator table, as in term-at-a-time retrieval. however, instead of reading
a whole list at once, we read just small pieces of each list. once the accumulator
table shows that many good documents have been found, we can stop looking.
as you can imagine, retrieval works fastest with terms that are likely to appear
together, such as    tortilla guacamole   . when the terms are not likely to appear
together   for example,    dirt cheese      it is likely to take much longer to find the
top documents.

5.7.4 structured queries
in the query evaluation examples we have seen so far, our assumption is that each
inverted list corresponds to a single feature, and that we add those features to-
gether to create a final document score. although this works in simple cases, we
might want a more interesting kind of scoring function. for instance, in figure 5.2
the query had plenty of interesting features, including a phrase (   tropical fish   ), a
synonym (   chichlids   ), and some non-topical features (e.g., incoming links).

one way to do this is to write specialized ranking code in the retrieval system
that detects these extra features and uses inverted list data directly to compute
scores, but in a way that is more complicated than just a linear combination of
features. this approach greatly increases the kinds of scoring that you can use,
and is very e   cient. unfortunately, it isn   t very flexible.

5.7 query processing

179

another option is to build a system that supports structured queries. struc-
tured queries are queries written in a query language, which allows you to change
the features used in a query and the way those features are combined. the query
language is not used by normal users of the system. instead, a query translator
converts the user   s input into a structured query representation. this translation
process is where the intelligence of the system goes, including how to weight word
features and what synonyms to use. once this structured query has been created,
it is passed to the retrieval system for execution.

you may already be familiar with this kind of model, since database systems
work this way. id208 are controlled using structured query lan-
guage (sql). many important applications consist of a user interface and a struc-
tured query generator, with the rest of the logic controlled by a database. this sep-
aration of the application logic from the database logic allows the database to be
both highly optimized and highly general.

galago contains a structured query processing system that is described in de-

tail in chapter 7. this query language is also used in the exercises.

fig. 5.23. evaluation tree for the structured query #combine(#od:1(tropical
#od:1(aquarium fish) fish)

fish)

figure 5.23 shows a tree representation of a structured query written in the
galago structured query language: #combine(#od:1(tropical fish) #od:1(aquarium
fish) fish).

this query indicates that the document score should be a combination of the
scores from three subqueries. the first query is #od:1(tropical fish). in the galago
query language, the #od:1 operator means that the terms inside it need to appear
next to each other, in that order, in a matching document. the same is true of

#combine#od:1tropicalaquariumfish#od:1feature combinationslist dataproximity expressions180

5 ranking with indexes

#od:1(aquarium fish). the final query term is fish. each of these subqueries acts as a
document feature that is combined using the #combine operator.

this query contains examples of the main types of structured query expres-
sions. at the bottom of the tree, we have index terms. these are terms that corre-
spond to inverted lists in the index. above that level, we have proximity expres-
sions. these expressions combine inverted lists to create more complex features,
such as a feature for    fish    occurring in a document title, or    tropical fish    occur-
ring as a phrase. at the top level, the feature data computed from the inverted lists
is combined into a document score. at this level, the position information from
the inverted lists is ignored.

galago evaluates structured queries by making a tree of iterator objects that
looks just like the tree shown in figure 5.23. for instance, an iterator is created
that returns the matching documents for #od:1(tropical fish). the iterator finds
these matching documents by using data from inverted list iterators for    tropical   
and    fish   . the #combine operator is an iterator of document scores, which uses
iterators for #od:1(tropical fish), #od:1(aquarium fish), and fish. once a tree of itera-
tors like this is made, scoring documents is just a matter of using the root iterator
to step through the documents.

5.7.5 distributed evaluation
a single modern machine can handle a surprising load, and is probably enough
for most tasks. however, dealing with a large corpus or a large number of users
may require using more than one machine.

the general approach to using more than one machine is to send all queries to
a director machine. the director then sends messages to many index servers, which
do some portion of the query processing. the director then organizes the results
of this process and returns them to the user.

the easiest distribution strategy is called document distribution. in this strat-
egy, each index server acts as a search engine for a small fraction of the total doc-
ument collection. the director sends a copy of the query to each of the index
servers, each of which returns the top k results, including the document scores
for these results. these results are merged into a single ranked list by the director,
which then returns the results to the user.

some ranking algorithms rely on collection statistics, such as the number of
occurrences of a term in the collection or the number of documents containing a
term. these statistics need to be shared among the index servers in order to pro-
duce comparable scores that can be merged e   ectively. in very large clusters of

5.7 query processing

181

machines, the term statistics at the index server level can vary wildly. if each in-
dex server uses only its own term statistics, the same document could receive very
di   erent kinds of scores, depending on which index server is used.

another distribution method is called term distribution. in term distribution,
a single index is built for the whole cluster of machines. each inverted list in that
index is then assigned to one index server. for instance, the word    dog    might
be handled by the third server, while    cat    is handled by the fifth server. for a
system with n index servers and a k term query, the id203 that all of the
query terms would be on the same server is 1/nk   1. for a cluster of 10 machines,
this id203 is just 1% for a three-term query. therefore, in most cases the data
to process a query is not stored all on one machine.

one of the index servers, usually the one holding the longest inverted list, is
chosen to process the query. if other index servers have relevant data, that data
is sent over the network to the index server processing the query. when query
processing is complete, the results are sent to a director machine.

the term distribution approach is more complex than document distribution
because of the need to send inverted list data between machines. given the size of
inverted lists, the messages involved in shipping this data can saturate a network.
in addition, each query is processed using just one processor instead of many,
which increases overall query latency versus document distribution. the main ad-
vantage of term distribution is seek time. if we have a k-term query and n index
servers, the total number of disk seeks necessary to process a query is o(kn) for
a document-distributed system, but just o(k) in a term-distributed system. for
a system that is disk-bound, and especially one that is seek-bound, term distribu-
tion might be attractive. however, recent research shows that term distribution
is rarely worth the e   ort.

5.7.6 caching

we saw in chapter 4 how word frequencies in text follow a zipfian distribution: a
few words occur very often, but a huge number of words occur very infrequently.
it turns out that query distributions are similar. some queries, such as those about
popular celebrities or current events, tend to be very popular with public search
engines. however, about half of the queries that a search engine receives each day
are unique.

this leads us into a discussion of caching. broadly speaking, caching means
storing something you might want to use later. with search engines, we usually

182

5 ranking with indexes

want to cache ranked result lists for queries, but systems can also cache inverted
lists from disk.

caching is perfectly suited for search engines. queries and ranked lists are
small, meaning it doesn   t take much space in a cache to store them. by contrast,
processing a query against a large corpus can be very computationally intensive.
this means that once a ranked list is computed, it usually makes sense to keep it
around.

however, caching does not solve all of our performance problems, because
about half of all queries received each day are unique. therefore, the search en-
gine itself must be built to handle query tra   c very quickly. this leads to com-
petition for resources between the search engine and the caching system. recent
research suggests that when memory space is tight, caching should focus on the
most popular queries, leaving plenty of room to cache the index. unique queries
with multiple terms may still share a term and use the same inverted list. this
explains why inverted list caching can have higher hit rates than query caching.
once the whole index is cached, all remaining resources can be directed toward
caching query results.

when using caching systems, it is important to guard against stale data. cach-
ing works because we assume that query results will not change over time, but
eventually they do. cache entries need acceptable timeouts that allow for fresh
results. this is easier when dealing with partitioned indexes like the ones we dis-
cussed in section 5.6.4. each cache can be associated with a particular index par-
tition, and when that partition is deleted, the cache can also be deleted. keep in
mind that a system that is built to handle a certain peak throughput with caching
enabled will handle a much smaller throughput with caching o   . this means that
if your system ever needs to destroy its cache, be prepared to have a slow system
until the cache becomes suitably populated. if possible, cache flushes should hap-
pen at o   -peak load times.

references and further reading

this chapter contains information about many topics: indexing, query process-
ing, compression, index update, caching, and distribution just to name a few. all
these topics are in one chapter to highlight how these components work together.
because of how interconnected these components are, it is useful to look at
studies of real, working systems. brin and page (1998) wrote a paper about the
early google system that is an instructive overview of what it takes to build a fully

5.7 query processing

183

working system. later papers show how the google architecture has changed over
time   for example barroso et al. (2003). the mapreduce paper, by dean and
ghemawat (2008), gives more detail than this chapter does about how mapre-
duce was developed and how it works in practice.

the inner workings of commercial search engines are often considered trade
secrets, so the exact details of how they work is not often published. one im-
portant exception is the todobr engine, a popular brazilian web search engine.
before todobr was acquired by google, their engineers frequently published
papers about its workings. one example is their paper on a two-level caching
scheme (saraiva et al., 2001), but there are many others.

the book managing gigabytes (witten et al., 1999) is the standard reference
for index construction, and is particularly detailed in its discussion of compres-
sion techniques. work on compression for inverted lists continues to be an active
area of research. one of the recent highlights of this research is the pfor se-
ries of compressors from zukowski et al. (2006), which exploit the performance
characteristics of modern processors to make a scheme that is particularly fast for
decompressing small integers. b  ttcher and clarke (2007) did a recent study on
how compression schemes compare on the latest hardware.

zobel and mo   at (2006) wrote a review article that outlines all of the impor-
tant recent research in inverted indexes, both in index construction and in query
processing. this article is the best place to look for an understanding of how this
research fits together.

turtle and flood (1995) developed the maxscore series of algorithms. fagin
et al. (2003) took a similar approach with score-sorted inputs, although they did
not initially apply their ideas to information retrieval. anh and mo   at (2006)
refined these ideas to make a particularly e   cient retrieval system.

anh and mo   at (2005) and metzler et al. (2008) cover methods for comput-
ing scores that can be stored in inverted lists. in particular, these papers describe
how to compute scores that are both useful in retrieval and can be stored com-
pactly in the list. strohman (2007) explores the entire process of building scored
indexes and processing queries e   ciently with them.

many of the algorithms from this chapter are based on merging two sorted in-
puts; index construction relies on this, as does any kind of document-at-a-time
retrieval process. knuth wrote an entire volume on just sorting and searching,
which includes large amounts of material on merging, including disk-based merg-
ing (knuth, 1998). if the knuth book is too daunting, any standard algorithms
textbook should be able to give you more detail about how merging works.

184

5 ranking with indexes

lester et al. (2005) developed the geometric partitioning method for index
update. b  ttcher et al. (2006) added some extensions to this model, focusing on
how very common terms should be handled during update. strohman and croft
(2006) show how to update the index without halting query processing.

exercises

5.1. section 5.2 introduced an abstract model of ranking, where documents and
queries are represented by features. what are some advantages of representing
documents and queries by features? what are some disadvantages?
5.2. our model of ranking contains a ranking function r(q, d), which com-
pares each document with the query and computes a score. those scores are then
used to determine the final ranked list.

an alternate ranking model might contain a di   erent kind of ranking func-
tion, f (a, b, q), where a and b are two di   erent documents in the collection
and q is the query. when a should be ranked higher than b, f (a, b, q) eval-
uates to 1. when a should be ranked below b, f (a, b, q) evaluates to    1.

if you have a ranking function r(q, d), show how you can use it in a system
that requires one of the form f (a, b, q). why can you not go the other way (use
f (a, b, q) in a system that requires r(q, d))?
5.3. suppose you build a search engine that uses one hundred computers with a
million documents stored on each one, so that you can search a collection of 100
million documents. would you prefer a ranking function like r(q, d) or one
like f (a, b, q) (from the previous problem). why?
5.4. suppose your search engine has just retrieved the top 50 documents from
your collection based on scores from a ranking function r(q, d). your user in-
terface can show only 10 results, but you can pick any of the top 50 documents to
show. why might you choose to show the user something other than the top 10
documents from the retrieved document set?
5.5. documents can easily contain thousands of non-zero features. why is it im-
portant that queries have only a few non-zero features?
5.6. indexes are not necessary to search documents. your web browser, for in-
stance, has a find function in it that searches text without using an index. when
should you use an inverted index to search text? what are some advantages to
using an inverted index? what are some disadvantages?

5.7 query processing

185

5.7. section 5.3 explains many di   erent ways to store document information in
inverted lists. what kind of inverted lists might you build if you needed a very
small index? what kind would you build if you needed to find mentions of cities,
such as kansas city or s  o paulo?

5.8. write a program that can build a simple inverted index of a set of text docu-
ments. each inverted list will contain the file names of the documents that contain
that word.

suppose the file a contains the text    the quick brown fox   , and file b contains

   the slow blue fox   . the output of your program would be:

% ./your-program a b
blue b
brown a
fox a b
quick a
slow b
the a b

5.9. in section 5.4.1, we created an unambiguous compression scheme for 2-bit
binary numbers. find a sequence of numbers that takes up more space when it is
   compressed    using our scheme than when it is    uncompressed.   

5.10. suppose a company develops a new unambiguous lossless compression
scheme for 2-bit numbers called supershrink. its developers claim that it will re-
duce the size of any sequence of 2-bit numbers by at least 1 bit. prove that the
developers are lying. more specifically, prove that either:
    supershrink never uses less space than an uncompressed encoding, or
    there is an input to supershrink such that the compressed version is larger

than the uncompressed input
you can assume that each 2-bit input number is encoded separately.

5.11. why do we need to know something about the kind of data we will com-
press before choosing a compression algorithm? focus specifically on the result
from exercise 5.10.

5.12. develop an encoder for the elias-   code. verify that your program produces
the same codes as in table 5.2.

186

5 ranking with indexes

5.13. identify the optimal skip distance k when performing a two-term boolean
and query where one term occurs 1 million times and the other term appears
100 million times. assume that a linear search will be used once an appropriate
region is found to search in.

5.14. in section 5.7.3, we saw that the optimal skip distance c can be determined
by minimizing the quantity kn/c + pc/2, where k is the skip pointer length, n
is the total inverted list size, c is the skip interval, and p is the number of postings
to find.

plot this function using k = 4, n = 1,000,000, and p = 1,000, but varying c.
then, plot the same function, but set p = 10,000. notice how the optimal value
for c changes.

finally, take the derivative of the function kn/c + pc/2 in terms of c to find

the optimum value for c for a given set of other parameters (k, n, and p).

5.15. in chapter 4, you learned about zipf    s law, and how approximately 50%
of words found in a collection of documents will occur only once. your job is to
design a program that will verify zipf    s law using mapreduce.
your program will output a list of number pairs, like this:

195840,1
70944,2
34039,3
...
1,333807

this sample output indicates that 195,840 words appeared once in the collection,
70,944 appeared twice, and 34,039 appeared three times, but one word appeared
333,807 times. your program will print this kind of list for a document collection.
your program will use mapreduce twice (two map phases and two reduce

phases) to produce this output.

5.16. write the program described in exercise 5.15 using the galago search toolkit.
verify that it works by indexing the wikipedia collection provided on the book
website.

6

queries and interfaces

   this is information retrieval, not information
dispersal.   

jack lint, brazil

6.1 information needs and queries

although the index structures and ranking algorithms are key components of a
search engine, from the user   s point of view the search engine is primarily an in-
terface for specifying queries and examining results. people can   t change the way
the ranking algorithm works, but they can interact with the system during query
formulation and reformulation, and while they are browsing the results. these in-
teractions are a crucial part of the process of information retrieval, and can deter-
mine whether the search engine is viewed as providing an e   ective service. in this
chapter, we discuss techniques for query transformation and refinement, and for
assembling and displaying the search results. we also discuss cross-language search
engines here because they rely heavily on the transformation of queries and results.
in chapter 1, we described an information need as the motivation for a person
using a search engine. there are many types of information needs, and researchers
have categorized them using dimensions such as the number of relevant docu-
ments being sought, the type of information that is needed, and the tasks that
led to the requirement for information. it has also been pointed out that in some
cases it can be di   cult for people to define exactly what their information need
is, because that information is a gap in their knowledge.1 from the point of view
of the search engine designer, there are two important consequences of these ob-
servations about information needs:
    queries can represent very di   erent information needs and may require dif-
ferent search techniques and ranking algorithms to produce the best rankings.
1 this is belkin   s well-known anomalous state of knowledge (ask) hypothesis (belkin
et al., 1982/1997).

188

6 queries and interfaces

    a query can be a poor representation of the information need. this can happen
because the user finds it di   cult to express the information need. more often,
however, it happens because the user is encouraged to enter short queries, both
by the search engine interface and by the fact that long queries often fail.

the first point is discussed further in chapter 7. the second point is a major theme
in this chapter. we present techniques   such as id147, query expan-
sion, and relevance feedback   that are designed to refine the query, either auto-
matically or through user interaction. the goal of this refinement process is to
produce a query that is a better representation of the information need, and con-
sequently to retrieve better documents. on the output side, the way that results
are displayed is an important part of helping the user understand whether his in-
formation need has been met. we discuss techniques such as snippet generation,
result id91, and document highlighting, that are designed to help this process
of understanding the results.

short queries consisting of a small number of keywords (between two and
three on average in most studies of web search) are by far the most popular form of
query currently used in search engines. given that such short queries can be am-
biguous and imprecise,2 why don   t people use longer queries? there are a number
of reasons for this. in the past, query languages for search engines were designed to
be used by expert users, or search intermediaries. they were called intermediaries
because they acted as the interface between the person looking for information
and the search engine. these query languages were quite complex. for example,
here is a query made up by an intermediary for a search engine that provides legal
information:

user query: are there any cases that discuss negligent maintenance or fail-
ure to maintain aids to navigation such as lights, buoys, or channel mark-
ers?
intermediary query: neglect! fail! neglig! /5 maint! repair! /p nav-
igat! /5 aid equip! light buoy    channel marker   

this query language uses wildcard operators and various forms of proximity op-
erators to specify the information need. a wildcard operator is used to define the
minimum string match required for a word to match the query. for example, ne-
glect! will match    neglected   ,    neglects   , or just    neglect   . a proximity operator

2 would you go up to a person and say,    tropical fish?   , or even worse,    fish?   , if you

wanted to ask what types of tropical fish were easiest to care for?

6.1 information needs and queries

189

is used to define constraints on the distance between words that are required for
them to match the query. one type of proximity constraint is adjacency. for ex-
ample, the quotes around    channel marker    specify that the two words must
occur next to each other. the more general window operator specifies a width (in
words) of a text window that is allowed for the match. for example, /5 specifies
that the words must occur within five words of each other. other typical prox-
imity operators are sentence and paragraph proximity. for example, /p specifies
that the words must occur in the same paragraph. in this query language, if no
constraint is specified, it is assumed to be a boolean or.

some of these query language operators are still available in search engine in-
terfaces, such as using quotes for a phrase or a    +    to indicate a mandatory term,
but in general there is an emphasis on simple keyword queries (sometimes called
   natural language    queries) in order to make it possible for most people to do
their own searches.3 but if we want to make querying as natural as possible, why
not encourage people to type in better descriptions of what they are looking for
instead of just a couple of keywords? indeed, in applications where people expect
other people to answer their questions, such as the community-based question
answering systems described in section 10.3, the average query length goes up to
around 30 words. the problem is that current search technology does not do a
good job with long queries. most web search engines, for example, only rank doc-
uments that contain all the query terms. if a person enters a query with 30 words
in it, the most likely result is that nothing will be found. even if documents con-
taining all the words can be found, the subtle distinctions of language used in a
long, grammatically correct query will often be lost in the results. search engines
use ranking algorithms based primarily on a statistical view of text as a collection
of words, not on syntactic and semantic features.

given what happens to long queries, people have quickly learned that they will
get the most reliable results by thinking of a few keywords that are likely to be as-
sociated with the information they are looking for, and using these as the query.
this places quite a burden on the user, and the query refinement techniques de-
scribed here are designed to reduce this burden and compensate for poor queries.

3 note that the search engine may still be using a complex query language (such as that

described in section 7.4.2) internally, but not in the interface.

190

6 queries and interfaces

6.2 query transformation and refinement

6.2.1 stopping and id30 revisited

as mentioned in the last section, the most common form of query used in cur-
rent search engines consists of a small number of keywords. some of these queries
use quotes to indicate a phrase, or a    +    to indicate that a word must be present,
but for the remainder of this chapter we will make the simplifying assumption
that the query is simply text.4 the initial stages of processing a text query should
mirror the processing steps that are used for documents. words in the query text
must be transformed into the same terms that were produced by document texts,
or there will be errors in the ranking. this sounds obvious, but it has been a source
of problems in a number of search projects. despite this restriction, there is scope
for some useful di   erences between query and document transformation, par-
ticularly in stopping and id30. other steps, such as parsing the structure or
tokenizing, will either not be needed (keyword queries have no structure) or will
be essentially the same.

we mentioned in section 4.3.3 that stopword removal can be done at query
time instead of during document indexing. retaining the stopwords in the in-
dexes increases the flexibility of the system to deal with queries that contain stop-
words. stopwords can be treated as normal words (by leaving them in the query),
removed, or removed except under certain conditions (such as being used with
quote or    +    operators).

query-based id30 is another technique for increasing the flexibility of the
search engine. if the words in documents are stemmed during indexing, the words
in the queries must also be stemmed. there are circumstances, however, where
id30 the query words will reduce the accuracy of the results. the query    fish
village    will, for example, produce very di   erent results from the query    fishing
village   , but many id30 algorithms would reduce    fishing    to    fish   . by not
id30 during document indexing, we are able to make the decision at query
time whether or not to stem    fishing   . this decision could be based on a number
of factors, such as whether the word was part of a quoted phrase.

for query-based id30 to work, we must expand the query using the ap-
propriate word variants, rather than reducing the query word to a word stem. this
is because documents have not been stemmed. if the query word    fishing    was re-

4 based on a recent sample of web queries, about 1.5% of queries used quotes, and less

than 0.5% used a    +    operator.

6.2 query transformation and refinement

191

placed with the stem    fish   , the query would no longer match documents that
contained    fishing   . instead, the query should be expanded to include the word
   fish   . this expansion is done by the system (not the user) using some form of
synonym operator, such as that described in section 7.4.2. alternatively, we could
index the documents using stems as well as words. this will make query execution
more e   cient, but increases the size of the indexes.

every id30 algorithm implicitly generates stem classes. a stem class is the
group of words that will be transformed into the same stem by the id30 al-
gorithm. they are created by simply running the id30 algorithm on a large
collection of text and recording which words map to a given stem. stem classes
can be quite large. for example, here are three stem classes created with the porter
stemmer on trec news collections (the first entry in each list is the stem):

/bank banked banking bankings banks
/ocean oceaneering oceanic oceanics oceanization oceans
/polic polical polically police policeable policed
-policement policer policers polices policial
-policically policier policiers policies policing
-policization policize policly policy policying policys
these classes are not only long (the    polic    class has 22 entries), but they also
contain a number of errors. the words relating to    police    and    policy    should not
be in the same class, and this will cause a loss in ranking accuracy. other words are
not errors, but may be used in di   erent contexts. for example,    banked    is more
often used in discussions of flying and pool, but this stem class will add words
that are more common in financial discussions. the length of the lists is an issue if
the stem classes are used to expand the query. adding 22 words to a simple query
will certainly negatively impact response time and, if not done properly using a
synonym operator, could cause the search to fail.

both of these issues can be addressed using an analysis of word co-occurrence
in the collection of text. the assumption behind this analysis is that word variants
that could be substitutes for each other should co-occur often in documents. more
specifically, we do the following steps:
1. for all pairs of words in the stem classes, count how often they co-occur in

text windows of w words. w is typically in the range 50   100.

2. compute a co-occurrence or association metric for each pair. this measures

how strong the association is between the words.

192

6 queries and interfaces

3. construct a graph where the vertices represent words and the edges are be-

tween words whose co-occurrence metric is above a threshold t .

4. find the connected components of this graph. these are the new stem classes.
the term association measure used in trec experiments was based on dice   s
coe   cient. this measure has been used since the earliest studies of term similarity
and automatic thesaurus construction in the 1960s and 1970s. if na is the num-
ber of windows (or documents) containing word a, nb is the number of windows
containing word b, nab is the number of windows containing both words a and b,
and n is the number of text windows in the collection, then dice   s coe   cient is
defined as 2    nab/(na + nb). this is simply the proportion of term occurrences
that are co-occurrences. there are other possible association measures, which will
be discussed later in section 6.2.3.

two vertices are in the same connected component of a graph if there is a path
between them. in the case of the graph representing word associations, the con-
nected components will be clusters or groups of words, where each word has an
association above the threshold t with at least one other member of the cluster.
the parameter t is set empirically. we will discuss this and other id91 tech-
niques in section 9.2.

applying this technique to the three example stem classes, and using trec
data to do the co-occurrence analysis, results in the following connected compo-
nents:

/policies policy
/police policed policing
/bank banking banks
the new stem classes are smaller, and the inappropriate groupings (e.g., pol-
icy/police) have been split up. in general, experiments show that this technique
produces good ranking e   ectiveness with a moderate level of id183.

what about the    fishing village    query? the relevant stem class produced by

the co-occurrence analysis is

/fish fished fishing

which means that we have not solved that problem. as mentioned before, the
query context determines whether id30 is appropriate. it would be reason-
able to expand the query    fishing in alaska    with the words    fish    and    fished   ,
but not the query    fishing village   . the co-occurrence analysis described earlier

6.2 query transformation and refinement

193

uses context in a general way, but not at the level of co-occurrence with specific
query words.

with the recent availability of large query logs in applications such as web
search, the concept of validating or even generating stem classes through statistical
analysis can be extended to these resources. in this case, the analysis would look for
word variants that tended to co-occur with the same words in queries. this could
be a solution to the fish/fishing problem, in that    fish    is unlikely to co-occur with
   village    in queries.

comparing this id30 technique to those described in section 4.3.4, it
could be described as a dictionary-based approach, where the dictionary is gen-
erated automatically based on input from an algorithmic stemmer (i.e., the stem
classes). this technique can also be used for id30 with languages that do
not have algorithmic stemmers available. in that case, the stem classes are based
on very simple criteria, such as grouping all words that have similar id165s. a
simple example would be to generate classes from words that have the same first
three characters. these initial classes are much larger than those generated by an
algorithmic stemmer, but the co-occurrence analysis reduces the final classes to
similar sizes. retrieval experiments confirm that typically there is little di   erence
in ranking e   ectiveness between an algorithmic stemmer and a stemmer based
on id165 classes.

6.2.2 spell checking and suggestions

spell checking is an extremely important part of query processing. approximately
10   15% of queries submitted to web search engines contain spelling errors, and
people have come to rely on the    did you mean: ...    feature to correct these er-
rors. query logs contain plenty of examples of simple errors such as the following
(taken from a recent sample of web queries):

poiner sisters
brimingham news
catamarn sailing
hair extenssions
marshmellow world
miniture golf courses
psyhics
home doceration

194

6 queries and interfaces

these errors are similar to those that may be found in a word processing docu-
ment. in addition, however, there will be many queries containing words related
to websites, products, companies, and people that are unlikely to be found in any
standard spelling dictionary. some examples from the same query log are:

realstateisting.bc.com
akia 1080i manunal
ultimatwarcade
mainscourcebank
dellottitouche
the wide variety in the type and severity of possible spelling errors in queries
presents a significant challenge. in order to discuss which id147 tech-
niques are the most e   ective for search engine queries, we first have to review how
id147 is done for general text.

the basic approach used in many spelling checkers is to suggest corrections
for words that are not found in the spelling dictionary. suggestions are found by
comparing the word that was not found in the dictionary to words that are in the
dictionary using a similarity measure. the most common measure for compar-
ing words (or more generally, strings) is the id153, which is the number of
operations required to transform one of the words into the other. the damerau-
levenshtein distance metric counts the minimum number of insertions, deletions,
substitutions, or transpositions of single characters required to do the transfor-
mation.5 studies have shown that 80% or more of spelling errors are caused by an
instance of one of these types of single-character errors.

as an example, the following transformations (shown with the type of error
involved) all have damerau-levenshtein distance 1 since only a single operation
or edit is required to produce the correct word:
extenssions     extensions (insertion error)
poiner     pointer (deletion error)
marshmellow     marshmallow (substitution error)
brimingham     birmingham (transposition error)
the transformation doceration     decoration, on the other hand, has edit dis-

tance 2 since it requires two edit operations:

5 the levenshtein distance is similar but does not include transposition as a basic opera-
tion.

6.2 query transformation and refinement

195

doceration     deceration
deceration     decoration
a variety of techniques and data structures have been used to speed up the
calculation of id153s between the misspelled word and the words in the
dictionary. these include restricting the comparison to words that start with the
same letter (since spelling errors rarely change the first letter), words that are of the
same or similar length (since spelling errors rarely change the length of the word),
and words that sound the same.6 in the latter case, phonetic rules are used to map
words to codes. words with the same codes are considered as possible corrections.
the soundex code is a simple type of phonetic encoding that was originally used
for the problem of matching names in medical records. the rules for this encoding
are:
1. keep the first letter (in uppercase).
2. replace these letters with hyphens: a, e, i, o, u, y, h, w.
3. replace the other letters by numbers as follows:

1: b, f, p, v
2: c, g, j, k, q, s, x, z
3: d, t
4: l
5: m, n
6: r

4. delete adjacent repeats of a number.
5. delete the hyphens.
6. keep the first three numbers or pad out with zeros.
some examples of this code are:

extenssions     e235; extensions     e235
marshmellow     m625; marshmallow     m625
brimingham     b655; birmingham     b655
poiner     p560; pointer     p536

the last example shows that the correct word may not always have the same
soundex code. more elaborate phonetic encodings have been developed specifi-
cally for id147 (e.g., the gnu aspell checker7 uses a phonetic code).

6 a word that is pronounced the same as another word but di   ers in meaning is called a
homophone.
7 http://aspell.net/

196

6 queries and interfaces

these encodings can be designed so that the id153 for the codes can be used
to narrow the search for corrections.

a given spelling error may have many possible corrections. for example, the
spelling error    lawers    has the following possible corrections (among others) at
id153 1: lawers     lowers, lawyers, layers, lasers, lagers. the spelling correc-
tor has to decide whether to present all of these to the user, and in what order to
present them. a typical policy would be to present them in decreasing order of
their frequency in the language. note that this process ignores the context of the
spelling error. for example, if the error occurred in the query    trial lawers   , this
would have no impact on the presentation order of the suggested corrections. the
lack of context in the id147 process also means that errors involv-
ing valid words will be missed. for example, the query    miniature golf curses    is
clearly an example of a single-character deletion error, but this error has produced
the valid word    curses    and so would not be detected.

the typical interface for the    did you mean:...    feature requires the spell check-
er to produce the single best suggestion. this means that ranking the suggestions
using context and frequency information is very important for query spell check-
ing compared to spell checking in a word processor, where suggestions can be
made available on a pull-down list. in addition, queries contain a large number of
run-on errors, where word boundaries are skipped or mistyped. the two queries
   ultimatwarcade    and    mainscourcebank    are examples of run-on errors that also
contain single-character errors. with the appropriate framework, leaving out a
separator such as a blank can be treated as just another class of single-character
error.

the id87 for id147 is a general framework that
can address the issues of ranking, context, and run-on errors. the model is called
a    noisy channel    because it is based on shannon   s theory of communication
(shannon & weaver, 1963). the intuition is that a person chooses a word w to
output (i.e., write), based on a id203 distribution p (w). the person then
tries to write the word w, but the noisy channel (presumably the person   s brain)
causes the person to write the word e instead, with id203 p (e|w).

the probabilities p (w), called the language model, capture information about
the frequency of occurrence of a word in text (e.g., what is the id203 of the
word    lawyer    occurring in a document or query?), and contextual information
such as the id203 of observing a word given that another word has just been
observed (e.g., what is the id203 of    lawyer    following the word    trial   ?). we

6.2 query transformation and refinement

197

will have more to say about language models in chapter 7, but for now we can just
assume it is a description of word occurrences in terms of probabilities.
the probabilities p (e|w), called the error model, represent information about
the frequency of di   erent types of spelling errors. the probabilities for words (or
strings) that are id153 1 away from the word w will be quite high, for ex-
ample. words with higher id153s will generally have lower probabilities,
although homophones will have high probabilities. note that the error model will
have probabilities for writing the correct word (p (w|w)) as well as probabilities
for spelling errors. this enables the spelling corrector to suggest a correction for
all words, even if the original word was correctly spelled. if the highest-id203
correction is the same word, then no correction is suggested to the user. if, how-
ever, the context (i.e., the language model) suggests that another word may be
more appropriate, then it can be suggested. this, in broad terms, is how a spelling
corrector can suggest    course    instead of    curse    for the query    golf curse   .
so how do we estimate the id203 of a correction? what the person writes
is the word e, so we need to calculate p (w|e), which is the id203 that the
correct word is w given that we can see the person wrote e. if we are interested in
finding the correction with the maximum value of this id203, or if we just
want to rank the corrections, it turns out we can use p (e|w)p (w), which is the
product of the error model id203 and the language model id203.8

to handle run-on errors and context, the language model needs to have in-
formation about pairs of words in addition to single words. the language model
id203 for a word is then calculated as a mixture of the id203 that the
word occurs in text and the id203 that it occurs following the previous word,
or

  p (w) + (1       )p (w|wp)

where    is a parameter that specifies the relative importance of the two probabili-
ties, and p (w|wp) is the id203 of a word w following the previous word wp.
as an example, consider the spelling error    fish tink   . to rank the possible correc-
tions for    tink   , we multiply the error model probabilities for possible corrections
by the language model probabilities for those corrections. the words    tank    and
   think    will both have high error-model probabilities since they require only a sin-
gle character correction. in addition, both words will have similar probabilities for
p (w) since both are quite common. the id203 p (tank|f ish), however,
8 bayes    rule, which is discussed in chapter 7, is used to express p (w|e) in terms of the
component probabilities.

198

6 queries and interfaces

will be much higher than p (think|f ish), and this will result in    tank    being a
more likely correction than    think   .

where does the information for the language model probabilities come from?
in many applications, the best source for statistics about word occurrence in text
will be the collection of documents that is being searched. in the case of web search
(and some other applications), there will also be a query log containing millions of
queries submitted to the search engine. since our task is to correct the spelling of
queries, the query log is likely to be the best source of information. it also reduces
the number of pairs of words that need to be recorded in the language model,
compared to analyzing all possible pairs in a large collection of documents. in
addition to these sources, if a trusted dictionary is available for the application, it
should be used.
the estimation of the p (e|w) probabilities in the error model can be relatively
simple or quite complex. the simple approach is to assume that all errors with the
same id153 have equal id203. additionally, only strings within a cer-
tain id153 (usually 1 or 2) are considered. more sophisticated approaches
have been suggested that base the id203 estimates on the likelihood of mak-
ing certain types of errors, such as typing an    a    when the intention was to type
an    e    . these estimates are derived from large collections of text by finding many
pairs of correctly and incorrectly spelled words.

cucerzan and brill (2004) describe an iterative process for spell checking
queries using information from a query log and dictionary. the steps, in simplified
form, are as follows:
1. tokenize the query.
2. for each token, a set of alternative words and pairs of words is found using
an id153 modified by weighting certain types of errors, as described
earlier. the data structure that is searched for the alternatives contains words
and pairs from both the query log and the trusted dictionary.

3. the id87 is then used to select the best correction.
4. the process of looking for alternatives and finding the best correction is re-

peated until no better correction is found.

by having multiple iterations, the spelling corrector can potentially make sugges-
tions that are quite far (in terms of id153) from the original query. as an
example, given the query    miniture golfcurses   , the spelling corrector would go
through the following iterations:

6.2 query transformation and refinement

199

miniture golfcurses
miniature golfcourses
miniature golf courses

experiments with this spelling corrector show that the language model from the
query log is the most important component in terms of correction accuracy. in
addition, using context in the form of word pairs in the language model is crit-
ical. having at least two iterations in the correction process also makes a signif-
icant di   erence. the error model was less important, however, and using a sim-
ple model where all errors have the same id203 was nearly as e   ective as the
more sophisticated model. other studies have shown that the error model is more
important when the language model is based just on the collection of documents,
rather than the query log.

the best approach to building a spelling corrector for queries in a search ap-
plication obviously depends on the data available. if a large amount of query
log information is available, then this should be incorporated. otherwise, the
sources available will be the collection of documents for the application and, in
some cases, a trusted dictionary. one approach would be to use a general-purpose
spelling corrector, such as aspell, and create an application-specific dictionary.
building a spelling corrector based on the id87, however, is likely
to be more e   ective and more adaptable, even if query log data is not available.

6.2.3 id183
in the early development of search engines, starting in the 1960s, an online the-
saurus was considered an essential tool for the users of the system. the thesaurus
described the indexing vocabulary that had been used for the document collec-
tion, and included information about synonyms and related words or phrases.
this was particularly important because the collection had typically been manu-
ally indexed (tagged)9 using the terms in the thesaurus. because the terms in the
thesaurus were carefully chosen and subject to quality control, the thesaurus was
also referred to as a controlled vocabulary. using the thesaurus, users could deter-
mine what words and phrases could be used in queries, and could expand an ini-
tial query using synonyms and related words. table 6.1 shows part of an entry in
9 in information retrieval, indexing is often used to refer to the process of representing
a document using an index term, in addition to the process of creating the indexes to
search the collection. more recently, the process of manual indexing has been called
tagging, particularly in the context of social search applications (see chapter 10).

200

6 queries and interfaces

the medical subject (mesh) headings thesaurus that is used in the national li-
brary of medicine search applications.10 the    tree number    entries indicate, using
a numbering scheme, where this term is found in the tree of broader and narrow
terms. an    entry term    is a synonym or related phrase for the term.

mesh heading
tree number
tree number
tree number
entry term
entry term
entry term
entry term
entry term
entry term
entry term
entry term
entry term

neck pain

c10.597.617.576

c23.888.592.612.553

c23.888.646.501

cervical pain

neckache

anterior cervical pain
anterior neck pain

cervicalgia
cervicodynia
neck ache

posterior cervical pain
posterior neck pain

table 6.1. partial entry for the medical subject (mesh) heading    neck pain   

although the use of an explicit thesaurus is less common in current search ap-
plications, a number of techniques have been proposed for automatic and semi-
automatic id183. a semi-automatic technique requires user interac-
tion, such as selecting the expansion terms from a list suggested by the expansion
technique. web search engines, for example, provide query suggestions to the user
in the form of the original query words expanded with one or more additional
words, or replaced with alternative words.

id183 techniques are usually based on an analysis of word or term
co-occurrence, in either the entire document collection, a large collection of
queries, or the top-ranked documents in a result list. from this perspective, query-
based id30 can also be regarded as a id183 technique, with the
expansion terms limited to word variants. automatic expansion techniques that
use a general thesaurus, such as id138,11 have not been shown to be e   ective.

10 http://www.nlm.nih.gov/mesh/meshhome.html
11 http://id138.princeton.edu/

6.2 query transformation and refinement

201

the key to e   ective expansion is to choose words that are appropriate for the
context, or topic, of the query. for example,    aquarium    may be a good expan-
sion term for    tank    in the query    tropical fish tanks   , but not appropriate for the
query    armor for tanks   . a general thesaurus lists related terms for many di   er-
ent contexts, which is why it is di   cult to use automatically. the techniques we
will describe use a variety of approaches to address this problem, such as using all
the words in a query to find related words rather than expanding each word sepa-
rately. one well-known expansion technique, called pseudo-relevance feedback, is
discussed in the next section along with techniques that are based on user feed-
back about the relevance of documents in the results list.

term association measures are an important part of many approaches to query
expansion, and consequently a number of alternatives have been suggested. one
of these, dice   s coe   cient, was mentioned in section 6.2.1. the formula for this
measure is

2    nab
na + nb

rank=

nab

na + nb

where rank= means that the formula is rank equivalent (produces the same ranking
of terms).12

another measure, mutual information, has been used in a number of studies

of word collocation. for two words (or terms) a and b, it is defined as

log p (a, b)
p (a)p (b)

and measures the extent to which the words occur independently.13 p (a) is the
id203 that word a occurs in a text window of a given size, p (b) is the prob-
ability that word b occurs in a text window, and p (a, b) is the id203 that
a and b occur in the same text window. if the occurrences of the words are in-
dependent, p (a, b) = p (a)p (b) and the mutual information will be 0. as an
example, we might expect that the two words    fishing    and    automobile    would
occur relatively independently of one another. if two words tend to co-occur, for

12 more formally, two functions are defined to be rank equivalent if they produce the
same ordering of items when sorted according to function value. monotonic trans-
forms (such as log), scaling (multiplying by a constant), and translation (adding a con-
stant) are all examples of rank-preserving operations.

13 this is actually the pointwise mutual information measure, just to be completely accu-

rate.

202

6 queries and interfaces

example    fishing    and    boat   , p (a, b) will be greater than p (a)p (b) and the mu-
tual information will be higher.

to calculate mutual information, we use the following simple normalized fre-
quency estimates for the probabilities: p (a) = na/n, p (b) = nb/n, and
p (a, b) = nab/n, where na is the number of windows (or documents) contain-
ing word a, nb is the number of windows containing word b, nab is the number of
windows containing both words a and b, and n is the number of text windows
in the collection. this gives the formula:

log p (a, b)
p (a)p (b)

= log n.

nab
na.nb

rank=

nab
na.nb

a problem that has been observed with this measure is that it tends to favor
low-frequency terms. for example, consider two words with frequency 10 (i.e.,
na = nb = 10) that co-occur half the time (nab = 5). the association measure
for these two terms is 5  10
   2. for two terms with frequency 1,000 that co-occur
half the time (nab = 500), the association measure is 5    10
   4. the expected
mutual information measure addresses this problem by weighting the mutual in-
formation value using the id203 p (a, b). although the expected mutual in-
formation in general is calculated over all combinations of the events of word oc-
currence and non-occurrence, we are primarily interested in the case where both
terms occur, giving the formula:

p (a, b). log p (a, b)
p (a)p (b)

=

nab
n

log(n.

nab
na.nb

) rank= nab. log(n.

nab
na.nb

)

if we take the same example as before and assume n = 106, this gives an as-
sociation measure of 23.5 for the low-frequency terms, and 1,350 for the high-
frequency terms, clearly favoring the latter case. in fact, the bias toward high-
frequency terms can be a problem for this measure.

another popular association measure that has been used in a variety of applica-
tions is pearson   s chi-squared (  2) measure. this measure compares the number of
co-occurrences of two words with the expected number of co-occurrences if the
two words were independent, and normalizes this comparison by the expected
number. using our estimates, this gives the formula:
(nab     1

n . nb

n )2

rank=

n .na.nb)2
na.nb

(nab     n. na
n . nb

n. na

n

6.2 query transformation and refinement

203

n . nb

the term n.p (a).p (b) = n. na
n is the expected number of co-occurrences
if the two terms occur independently. the   2 test is usually calculated over all
combinations of the events of word occurrence and non-occurrence, similar to
the expected mutual information measure, but our focus is on the case where both
terms co-occur. in fact, when n is large, this restricted form of   2 produces the
same term rankings as the full form. it should also be noted that   2 is very similar
to the mutual information measure and may be expected to favor low-frequency
terms.

table 6.2 summarizes the association measures we have discussed. to see how
they work in practice, we have calculated the top-ranked terms in a trec news
collection using each measure for the words in the sample query    tropical fish   .

expected mutual information nab. log(n. nab
na:nb

)

measure

mutual information

(m im)

(em im)
chi-square

(  2)

dice   s coe   cient

(dice)

formula

nab
na:nb

(nab    1

n :na:nb)2
na:nb

nab

na+nb

table 6.2. term association measures

table 6.3 shows the strongly associated words for    tropical    assuming an un-
limited window size (in other words, co-occurrences are counted at the document
level). there are two obvious features to note. the first is that the ranking for
  2 is identical to the one for mim. the second is that mim and   2 favor low-
frequency words, as expected. these words are not unreasonable (   itto   , for ex-
ample, is the international tropical timber organization, and    xishuangbanna   
is a chinese tropical botanic garden), but they are so specialized that they are un-
likely to be much use for many queries. the top terms for the emim and dice
measures are much more general and, in the case of emim, sometimes too gen-
eral (e.g.,    most   ).

table 6.4 shows the top-ranked words for    fish   . note that because this is a
higher-frequency term, the rankings for mim and   2 are no longer identical, al-
though both still favor low-frequency terms. the top-ranked words for emim

204

6 queries and interfaces

mim
trmm
itto

ortuno
kuroshio
ivirgarzama
biofunction
kapiolani

bstilla
almagreb
jackfruit

adeo

xishuangbanna

frangipani

yuca

anthurium

emim
forest
tree
rain
island
like
fish
most
water
fruit
area
world
america
some
live
plant

  2
trmm
itto

ortuno
kuroshio
ivirgarzama
biofunction
kapiolani

bstilla
almagreb
jackfruit

adeo

xishuangbanna

frangipani

yuca

anthurium

dice
forest
exotic
timber
rain
banana

deforestation
plantation
coconut
jungle
tree

rainforest

palm

hardwood
greenhouse

logging

table 6.3. most strongly associated words for    tropical    in a collection of trec news
stories. co-occurrence counts are measured at the document level.

and dice are quite similar, although in di   erent ordering. to show the e   ect of
changing the window size, table 6.5 gives the top-ranked words found using a
window of five words. the small window size has an e   ect on the results, although
both mim and   2 still find low-frequency terms. the words for emim are some-
what improved, being more specific. overall, it appears that the simple dice   s co-
e   cient is the most stable and reliable over a range of window sizes.

the most significant feature of these tables, however, is that even the best rank-
ings contain virtually nothing that would be useful to expand the query    tropical
fish   ! instead, the words are associated with other contexts, such as tropical forests
and fruits, or fishing conservation. one way to address this problem would be to
find words that are strongly associated with the phrase    tropical fish   . using dice   s
coe   cient with the same collection of trec documents as the previous tables,
this produces the following 10 words at the top of the ranking:

goldfish, reptile, aquarium, coral, frog, exotic, stripe, regent, pet, wet

clearly, this is doing much better at finding words associated with the right con-
text. to use this technique, however, we would have to find associations for every
group of words that could be used in a query. this is obviously impractical, but

6.2 query transformation and refinement

205

mim

zoologico
zapanta
wrint
wpfmc
weighout
waterdog
longfin

veracruzana

ungutt
ulocentra
needlefish
tunaboat
tsolwana
olivacea
motoroller

fisherman

emim
water
species
wildlife
fishery

sea

boat
area
habitat
vessel
marine
land
river
food

endanger

  2
arlsq

happyman
outerlimit

sportk
lingcod
longfin
bontadelli
sportfisher

billfish
needlefish
damaliscu
bontebok
taucher

orangemouth
sheepshead

dice
species
wildlife
fishery
water

fisherman

boat
sea

habitat
vessel
marine
endanger

conservation

river
catch
island

table 6.4. most strongly associated words for    fish    in a collection of trec news stories.
co-occurrence counts are measured at the document level.

mim
zapanta

plar
mbmo
gefilte
hapc
odfw

tai   e
mollie
frampton

idfg

billingsgate

sealord
longline

emim
wildlife
vessel
boat
fishery
species
tuna
trout

salmon
catch
nmf
trawl
halibut
meat
shellfish

  2
gefilte
mbmo
zapanta

plar
hapc
odfw

tai   e
mollie
frampton

idfg

billingsgate

sealord
longline

dice
wildlife
vessel
boat
fishery
species
catch
water
sea
meat
interior
fisherman

game
salmon
tuna
caught

southpoint
southpoint
anadromous fisherman anadromous

table 6.5. most strongly associated words for    fish    in a collection of trec news stories.
co-occurrence counts are measured in windows of five words.

206

6 queries and interfaces

there are other approaches that accomplish the same thing. one alternative would
be to analyze the word occurrences in the retrieved documents for a query. this
is the basis of pseudo-relevance feedback, which is discussed in the next section.
another approach that has been suggested is to index every word in the collection
by the words that co-occur with it, creating a virtual document14 representing that
word. for example, the following list is the top 35 most strongly associated words
for    aquarium    (using dice   s coe   cient):

zoology, cranmore, jouett, zoo, goldfish, fish, cannery, urchin, reptile,
coral, animal, mollusk, marine, underwater, plankton, mussel, oceanog-
raphy, mammal, species, exhibit, swim, biologist, cabrillo, saltwater, crea-
ture, reef, whale, oceanic, scuba, kelp, invertebrate, park, crustacean, wild,
tropical

these words would form the index terms for the document representing    aquar-
ium   . to find expansion words for a query, these virtual documents are ranked in
the same way as regular documents, giving a ranking for the corresponding words.
in our example, the document for    aquarium    contains the words    tropical    and
   fish    with high weights, so it is likely that it would be highly ranked for the query
   tropical fish   . this means that    aquarium    would be a highly ranked expansion
term. the document for a word such as    jungle   , on the other hand, would contain
   tropical    with a high weight but is unlikely to contain    fish   . this document, and
the corresponding word, would be much further down the ranking than    aquar-
ium   .

all of the techniques that rely on analyzing the document collection face both
computational and accuracy challenges due to the huge size and variability in
quality of the collections in search applications. at the start of this section, it was
mentioned that instead of analyzing the document collection, either the result
list or a large collection of queries could be used. recent studies and experience
indicate that a large query log is probably the best resource for id183.
not only do these logs contain many short pieces of text that are are easier to an-
alyze than full text documents, they also contain other data, such as information
on which documents were clicked on during the search (i.e., clickthrough data).
as an example of how the query log can be used for expansion, the following
list shows the 10 most frequent words associated with queries that contain    trop-
ical fish    in a recent query log sample from a popular web search engine:

14 sometimes called a context vector.

6.2 query transformation and refinement

207

stores, pictures, live, sale, types, clipart, blue, freshwater, aquarium, sup-
plies

these words indicate the types of queries that people tend to submit about trop-
ical fish (sales, supplies, pictures), and most would be good words to suggest for
id183. in current systems, suggestions are usually made in the form of
whole queries rather than expansion words, and here again the query log will be
extremely useful in producing the best suggestions. for example,    tropical fish
supplies    will be a much more common query than    supplies tropical fish    and
would be a better suggestion for this expansion.

from this perspective, id183 can be reformulated as a problem of
finding similar queries, rather than expansion terms. similar queries may not al-
ways contain the same words. for example, the query    pet fish sales    may be a
reasonable suggestion as an alternative to    tropical fish   , even though it doesn   t
contain the word    tropical   . it has long been recognized that semantically similar
queries can be found by grouping them based on the relevant documents they have
in common, rather than just the words. clickthrough data is very similar to rele-
vance data, and recent studies have shown that queries can be successfully grouped
or clustered based on the similarity of their clickthrough data. this means that ev-
ery query is represented using the set of pages that are clicked on for that query,
and the similarity between the queries is calculated using a measure such as dice   s
coe   cient, except that in this case nab will be the number of clicked-on pages the
two queries have in common, and na, nb are the number of pages clicked on for
each query.

in summary, both automatic and semi-automatic id183 methods
have been proposed, although the default in many search applications is to sug-
gest alternative queries to the user. some term association measures are better than
others, but term association based on single words does not produce good expan-
sion terms, because it does not capture the context of the query. the best way to
capture query context is to use a query log, both to analyze word associations and
to find similar queries based on clickthrough data. if there is no query log avail-
able, the best alternative would be to use pseudo-relevance feedback, as described
in the next section. of the methods described for constructing an automatic the-
saurus based on the document collection, the best alternative is to create virtual
documents for each word and rank them for each query.

208

6 queries and interfaces

6.2.4 relevance feedback
relevance feedback is a id183 and refinement technique with a long
history. first proposed in the 1960s, it relies on user interaction to identify rel-
evant documents in a ranking based on the initial query. other semi-automatic
techniques were discussed in the last section, but instead of choosing from lists of
terms or alternative queries, in relevance feedback the user indicates which docu-
ments are interesting (i.e., relevant) and possibly which documents are completely
o   -topic (i.e., non-relevant). based on this information, the system automatically
reformulates the query by adding terms and reweighting the original terms, and a
new ranking is generated using this modified query.

this process is a simple example of using machine learning in information
retrieval, where training data (the identified relevant and non-relevant docu-
ments) is used to improve the system   s performance. modifying the query is in fact
equivalent to learning a classifier that distinguishes between relevant and non-
relevant documents. we discuss classification and classification techniques fur-
ther in chapters 7 and 9. relative to many other applications of machine learning,
however, the amount of training data generated in relevance feedback is extremely
limited since it is based on the user   s input for this query session only, and not on
historical data such as clickthrough.

the specific method for modifying the query depends on the underlying re-
trieval model. in the next chapter, we describe how relevance feedback works in
the vector space model and the probabilistic model. in general, however, words
that occur more frequently in the relevant documents than in the non-relevant
documents, or in the collection as a whole, are added to the query or increased
in weight. the same general idea is used in the technique of pseudo-relevance feed-
back, where instead of asking the user to identify relevant documents, the system
simply assumes that the top-ranked documents are relevant. words that occur fre-
quently in these documents may then be used to expand the initial query. once
again, the specifics of how this is done depend on the underlying retrieval model.
we describe pseudo-relevance feedback based on the language model approach to
retrieval in the next chapter. the expansion terms generated by pseudo-relevance
feedback will depend on the whole query, since they are extracted from docu-
ments ranked highly for that query, but the quality of the expansion will be de-
termined by how many of the top-ranked documents in the initial ranking are in
fact relevant.

as a simple example of how this process works, consider the ranking shown
in figure 6.1, which was generated using a popular search engine with the query

6.2 query transformation and refinement

209

fig. 6.1. top ten results for the query    tropical fish   

   tropical fish   . to expand this query using pseudo-relevance feedback, we might
assume that all these top 10 documents were relevant. by analyzing the full text
of these documents, the most frequent terms, with their frequencies, can be iden-
tified as:

a (926), td (535), href (495), http (357), width (345), com (343), nbsp
(316), www (260), tr (239), htm (233), class (225), jpg (221)

1.badmanstropicalfisha freshwater aquarium page covering all aspects of the tropicalfish hobby. ... to badman's tropicalfish. ... world of aquariology with badman's tropicalfish. ... 2.tropicalfishnotes on a few species and a gallery of photos of african cichlids. 3.thetropical tank homepage - tropicalfish and aquariumsinfo on tropicalfish and tropical aquariums, large fish species index with ... here you will find lots of information on tropicalfish and aquariums. ... 4.tropicalfish centreoffers a range of aquarium products, advice on choosing species, feeding, and health care, and a discussion board. 5.tropicalfish - wikipedia, the free encyclopediatropicalfish are popular aquarium fish , due to their often bright coloration. ... practical fishkeeping     tropicalfish hobbyist     koi. aquarium related companies: ... 6.tropicalfish findhome page for tropicalfish internet directory ... stores, forums, clubs, fish facts, tropicalfish compatibility and aquarium ... 7.breedingtropicalfish... intrested in keeping and/or breeding tropical, marine, pond and coldwater fish. ... breeding tropicalfish ... breeding tropical, marine, coldwater & pond fish. ... 8.fishloreincludes tropical freshwater aquarium how-to guides, faqs, fish profiles, articles, and forums. 9.cathy'stropicalfish keepinginformation on setting up and maintaining a successful freshwater aquarium. 10.tropicalfish placetropicalfish information for your freshwater fish tank ... great amount of information about a great hobby, a freshwater tropicalfish tank. ... 210

6 queries and interfaces

clearly, these words are not appropriate to use as expansion terms, because they
consist of stopwords and html expressions that will be common in the whole
collection. in other words, they do not represent the topics covered in the top-
ranked documents. a simple way to refine this process is to count words in the
snippets of the documents and ignore stopwords. this analysis produces the fol-
lowing list of frequent words:

tropical (26), fish (28), aquarium (8), freshwater (5), breeding (4),
information (3), species (3), tank (2), badman   s (2), page (2), hobby (2),
forums (2)

these words are much better candidates for id183, and do not have
the problem of inadequate context that occurs when we try to expand    tropical   
and    fish    separately. if the user was, however, specifically interested in breeding
tropical fish, the expansion terms could be improved using true relevance feed-
back, where the document ranked seventh would be explicitly tagged as relevant.
in this case, the most frequent terms are:

breeding (4), fish (4), tropical (4), marine (2), pond (2), coldwater (2),
keeping (1), interested (1)

the major e   ect of using this list would be to increase the weight of the expansion
term    breeding   . the specific weighting, as we have said, depends on the underly-
ing retrieval model.

both relevance feedback and pseudo-relevance feedback have been extensively
investigated in the research literature, and have been shown to be e   ective tech-
niques for improving ranking. they are, however, seldom incorporated into oper-
ational search applications. in the case of pseudo-relevance feedback, this appears
to be primarily because the results of this automatic process can be unpredictable.
if the initial ranking does not contain many relevant documents, the expansion
terms found by pseudo-relevance feedback are unlikely to be helpful and, for some
queries, can make the ranking significantly worse. to avoid this, the candidate ex-
pansion terms could be shown to the user, but studies have shown that this is not
particularly e   ective. suggesting alternative queries based on an analysis of query
logs is a more reliable alternative for semi-automatic id183.

relevance feedback, on the other hand, has been used in some applications,
such as document filtering. filtering involves tracking a person   s interests over
time, and some applications allow people to modify their profiles using relevance
feedback. another simple use of relevance feedback is the    more like this    fea-
ture in some early web search engines. this feature allowed users to click on a link

6.2 query transformation and refinement

211

associated with each document in a result list in order to generate a ranked list
of other documents similar to the clicked-on document. the new ranked list of
documents was based on a query formed by extracting and weighting important
words from the clicked-on document. this is exactly the relevance feedback pro-
cess, but limited to a single relevant document for training data.

although these applications have had some success, the alternative approach
of asking users to choose a di   erent query from a list of suggested queries is cur-
rently more popular. there is no guarantee, of course, that the suggested queries
will contain exactly what the user is looking for, and in that sense relevance feed-
back supports more precise query reformulation. there is an assumption, how-
ever, underlying the use of relevance feedback: that the user is looking for many
relevant documents, not just the one or two that may be in the initial ranked list.
for some queries, such as looking for background information on a topic, this may
be true, but for many queries in the web environment, the user will be satisfied
with the initial ranking and will not need relevance feedback. lists of suggested
queries will be helpful when the initial query fails, whereas relevance feedback is
unlikely to help in that case.

6.2.5 context and personalization

one characteristic of most current search engines is that the results of a query will
be the same regardless of who submitted the query, why the query was submit-
ted, where the query was submitted, or what other queries were submitted in the
same session. all that matters is what words were used to describe the query. the
other factors, known collectively as the query context, will a   ect the relevance of
retrieved documents and could potentially have a significant impact on the rank-
ing algorithm. most contextual information, however, has proved to be di   cult
to capture and represent in a way that provides consistent e   ectiveness improve-
ments.

much research has been done, in particular, on learning user models or pro-
files to represent a person   s interests so that a search can be personalized. if the
system knew that a person was interested in sports, for example, the documents
retrieved for the query    vikings    may be di   erent than those retrieved by the same
query for a person interested in history. although this idea is appealing, there are
a number of problems with actually making it work. the first is the accuracy of
the user models. the most common proposal is to create the profiles based on the
documents that the person looks at, such as web pages visited, email messages,

212

6 queries and interfaces

or word processing documents on the desktop. this type of profile represents a
person using words weighted by their importance. words that occur frequently
in the documents associated with that person, but are not common words in gen-
eral, will have the highest weights. given that documents contain hundreds or
even thousands of words, and the documents visited by the person represent only
a snapshot of their interests, these models are not very specific. experiments have
shown that using such models does not improve the e   ectiveness of ranking on
average.

an alternative approach would be to ask the user to describe herself using pre-
defined categories. in addition to requiring additional (and optional) interactions
that most people tend to avoid, there is still the fundamental problem that some-
one with a general interest in sports may still want to ask a question about history.
this suggests that a category of interest could be specified for each query, such as
specifying the    history    category for the query    vikings   , but this is no di   erent
than simply entering a less ambiguous query. it is much more e   ective for a person
to enter an extra word or two in her query to clarify it   such as    vikings quarter-
backs    or    vikings exploration   , for example   than to try to classify a query into
a limited set of categories.

another issue that is raised by any approach to personalization based on user
models is privacy. people have understandable concerns about personal details be-
ing recorded in corporate and government databases. in response, techniques for
maintaining anonymity while searching and browsing on the web are becoming
an increasingly popular area for research and development. given this, a search
engine that creates profiles based on web activity may be viewed negatively, espe-
cially since the benefit of doing this is currently not clear.

problems with user modeling and privacy do not mean that contextual infor-
mation is not useful, but rather that the benefits of any approach based on context
need to be examined carefully. there are examples of applications where the use
of contextual information is clearly e   ective. one of these is the use of query logs
and clickthrough data to improve web search. the context in this case is the his-
tory of previous searches and search sessions that are the same or very similar. in
general, this history is based on the entire user population. a particular person   s
search history may be useful for    caching    results for common search queries, but
learning from a large number of queries across the population appears to be much
more e   ective.

another e   ective application of context is local search, which uses geographic
information derived from the query, or from the location of the device that the

6.2 query transformation and refinement

213

query comes from, to modify the ranking of search results. for example, the query
   fishing supplies    will generate a long list of web pages for suppliers from all over
the country (or the world). the query    fishing supplies cape cod   , however,
should use the context provided by the location    cape cod    to rank suppliers in
that region higher than any others. similarly, if the query    fishing supplies    came
from a mobile device in a town in cape cod, then this information could be used
to rank suppliers by their proximity to that town.

local search based on queries involves the following steps:

1. identify the geographic region associated with web pages. this is done either
by using location metadata that has been manually added to the document,
or by automatically identifying locations, such as place names, city names, or
country names, in the document text.

2. identify the geographic region associated with the query using automatic
techniques. analysis of query logs has shown that 10   15% of queries contain
some location reference.

3. rank web pages using a comparison of the query and document location in-

formation in addition to the usual text- and link-based features.
automatically identifying the location information in text is a specific exam-
ple of the information extraction techniques mentioned in chapter 4. location
names are mapped to specific regions and coordinates using a geographic ontol-
ogy15 and algorithms developed for spatial reasoning in geographic information
systems. for example, the location    cape cod    in a document might be mapped
to bounding rectangles based on latitude and longitude, as shown in figure 6.2,
whereas a town location would be mapped to more specific coordinates (or a
smaller bounding rectangle). although this sounds straightforward, there are
many issues involved in identifying location names (for example, there are more
than 35 places named springfield in the united states), deciding which locations
are significant (if a web page discusses the    problems with washington lobbyists   ,
should    washington    be used as location metadata?), and combining multiple lo-
cation references in a document.
15 an ontology is essentially the same thing as a thesaurus. it is a representation of the con-
cepts in a domain and the relationships between them, whereas a thesaurus describes
words, phrases, and relationships between them. ontologies usually have a richer set of
relationships than a thesaurus. a taxonomy is another term used to describe categories
of concepts.

214

6 queries and interfaces

the geographic comparison used in the ranking could involve inclusion (for
example, the location metadata for a supplier   s web page indicates that the sup-
plier is located in the bounding box that represents cape cod), distance (for ex-
ample, the supplier is within 10 miles of the town that the query mentioned), or
other spatial relationships. from both an e   ciency and e   ectiveness perspective,
there will be implications for exactly how and when the geographic information
is incorporated into the ranking process.

fig. 6.2. geographic representation of cape cod using bounding rectangles

to summarize, the most useful contextual information for improving search
quality is based on past interactions with the search engine (i.e., the query log
and session history). local search based on geographic context can also produce
substantial improvements for a subset of queries. in both cases, context is used to
provide additional features to enhance the original query (id183 pro-
vides additional words, and local search provides geographic distance). to under-
stand the context for a specific query, however, there is no substitute for the user
providing a more specific query. indeed, local search in most cases relies on the
location being specified in the query. typically, more specific queries come from
users examining the results and then reformulating the query. the results display,
which we discuss next, must convey the context of the query term matches so that
the user can decide which documents to look at in detail or how to reformulate
the query.

6.3 showing the results

215

6.3 showing the results

6.3.1 result pages and snippets
successful interactions with a search engine depend on the user understanding
the results. many di   erent visualization techniques have been proposed for dis-
playing search output (hearst, 1999), but for most search engines the result pages
consist of a ranked list of document summaries that are linked to the actual doc-
uments or web pages. a document summary for a web search typically contains
the title and url of the web page, links to live and cached versions of the page,
and, most importantly, a short text summary, or snippet, that is used to convey the
content of the page. in addition, most result pages contain advertisements con-
sisting of short descriptions and links. query words that occur in the title, url,
snippet, or advertisements are highlighted to make them easier to identify, usually
by displaying them in a bold font.

figure 6.3 gives an example of a document summary from a result page for
a web search. in this case, the snippet consists of two partial sentences. figure
6.1 gives more examples of snippets that are sometimes full sentences, but often
text fragments, extracted from the web page. some of the snippets do not even
contain the query words. in this section, we describe some of the basic features of
the algorithms used for snippet generation.

fig. 6.3. typical document summary for a web search

snippet generation is an example of text summarization. summarization tech-
niques have been developed for a number of applications, but primarily have been
tested using news stories from the trec collections. a basic distinction is made
between techniques that produce query-independent summaries and those that
produce query-dependent summaries. snippets in web search engine result pages
are clearly query-dependent summaries, since the snippet that is generated for a
page will depend on the query that retrieved it, but some query-independent fea-
tures, such as the position of the text in the page and whether the text is in a head-
ing, are also used.

tropicalfishone of the u.k.s leading suppliers of tropical, coldwater, marine fish and invertebrates plus.. . next day fish delivery service ...  www.tropicalfish.org.uk/tropical_fish.htmcached page216

6 queries and interfaces

the development of text summarization techniques started with h. p. luhn
in the 1950s (luhn, 1958). luhn   s approach was to rank each sentence in a doc-
ument using a significance factor and to select the top sentences for the summary.
the significance factor for a sentence is calculated based on the occurrence of sig-
nificant words. significant words are defined in his work as words of medium fre-
quency in the document, where    medium    means that the frequency is between
predefined high-frequency and low-frequency cuto    values. given the significant
words, portions of the sentence that are    bracketed    by these words are consid-
ered, with a limit set for the number of non-significant words that can be between
two significant words (typically four). the significance factor for these bracketed
text spans is computed by dividing the square of the number of significant words
in the span by the total number of words. figure 6.4 gives an example of a text
span for which the significance factor is 42/7 = 2.3. the significance factor for
a sentence is the maximum calculated for any text span in the sentence.

fig. 6.4. an example of a text span of words (w) bracketed by significant words (s) using
luhn   s algorithm

to be more specific about the definition of a significant word, the following is a
frequency-based criterion that has been used successfully in more recent research.
if fd;w is the frequency of word w in document d, then w is a significant word if
it is not a stopword (which eliminates the high-frequency words), and

          7     0.1    (25     sd), if sd < 25

7 + 0.1    (sd     40), otherwise,

7,

if 25     sd     40

fd;w    

where sd is the number of sentences in document d. as an example, the second
page of chapter 1 of this book contains less than 25 sentences (roughly 20), and so
the significant words will be non-stopwords with a frequency greater than or equal

w  w  w  w  w  w  w  w  w  w  w.                         (initial sentence)  w  w  s  w  s  s  w  w  s  w  w.                 (identify significant words)  w  w [s  w  s  s  w  w  s] w  w. (text span bracketed by significant words)  6.3 showing the results

217

to 6.5. the only words that satisfy this criterion are    information    (frequency 9),
   story    (frequency 8), and    text    (frequency 7).

most work on summarization since luhn has involved improvements to this
basic approach, including better methods of selecting significant words and se-
lecting sentences or sentence fragments. snippet generation techniques can also
be viewed as variations of luhn   s approach with query words being used as the
significant words and di   erent sentence selection criteria.

typical features that would be used in selecting sentences for snippets to sum-
marize a text document such as a news story would include whether the sentence
is a heading, whether it is the first or second line of the document, the total num-
ber of query terms occurring in the sentence, the number of unique query terms
in the sentence, the longest contiguous run of query words in the sentence, and
a density measure of query words, such as luhn   s significance factor. in this ap-
proach, a weighted combination of features would be used to rank sentences. web
pages, however, often are much less structured than a news story, and can contain
a lot of text that would not be appropriate for snippets. to address this, snip-
pet sentences are often selected from the metadata associated with the web page,
such as the    description    identified by the <meta name=   description    content= ...>
html tags, or from external sources, such as web directories.16 certain classes
of web pages, such as wikipedia entries, are more structured and have snippet
sentences selected from the text.

although many variations are possible for snippet generation and document
summaries in result pages, some basic guidelines for e   ective summaries have been
derived from an analysis of clickthrough data (clarke et al., 2007). the most im-
portant is that whenever possible, all of the query terms should appear in the
summary, showing their relationship to the retrieved page. when query terms
are present in the title, however, they need not be repeated in the snippet. this al-
lows for the possibility of using sentences from metadata or external descriptions
that may not have query terms in them. another guideline is that urls should
be selected and displayed in a manner that emphasizes their relationship to the
query by, for example, highlighting the query terms present in the url. finally,
search engine users appear to prefer readable prose in snippets (such as complete
or near-complete sentences) rather than lists of keywords and phrases. a feature
that measures readability should be included in the computation of the ranking
for snippet selection.

16 for example, the open directory project, http://www.dmoz.org.

218

6 queries and interfaces

the e   cient implementation of snippet generation will be an important part
of the search engine architecture since the obvious approach of finding, opening,
and scanning document files would lead to unacceptable overheads in an envi-
ronment requiring high query throughput. instead, documents must be fetched
from a local document store or cache at query time and decompressed. the docu-
ments that are processed for snippet generation should have all html tags and
other    noise    (such as javascript) removed, although metadata must still be distin-
guished from text content. in addition, sentence boundaries should be identified
and marked at indexing time, to avoid this potentially time-consuming operation
when selecting snippets.

6.3.2 advertising and search

advertising is a key component of web search engines since that is how companies
generate revenue. in the case of advertising presented with search results (spon-
sored search), the goal is to find advertisements that are appropriate for the query
context. when browsing web pages, advertisements are selected for display based
on the contents of pages. contextual advertising is thought to lead to more user
clicks on advertisements (clickthrough), which is how payments for advertising
are determined. search engine companies maintain a database of advertisements,
which is searched to find the most relevant advertisements for a given query or
web page. an advertisement in this database usually consists of a short text de-
scription and a link to a web page describing the product or service in more detail.
searching the advertisement database can therefore be considered a special case of
general text search.

nothing is ever that simple, however. advertisements are not selected solely
based on their ranking in a simple text search. instead, advertisers bid for key-
words that describe topics associated with their product. the amount bid for a
keyword that matches a query is an important factor in determining which adver-
tisement is selected. in addition, some advertisements generate more clickthrough
because they are more appealing to the user population. the popularity of an ad-
vertisement, as measured by the clickthrough over time that is captured in the
query log, is another significant factor in the selection process. the popularity
of an advertisement can be measured over all queries or on a query-specific ba-
sis. query-specific popularity can be used only for queries that occur on a regular
basis. for the large number of queries that occur infrequently (so-called long-tail

6.3 showing the results

219

queries17), the general popularity of advertisements can be used. by taking all of
these factors into account, namely relevance, bids, and popularity, the search en-
gine company can devise strategies to maximize their expected profit.

as an example, a pet supplies company that specializes in tropical fish may
place the highest bid for the keywords    aquarium    and    tropical fish   . given the
query    tropical fish   , this keyword is certainly relevant. the content of the ad-
vertisement for that company should also contain words that match the query.
given that, this company   s advertisement will receive a high score for relevance
and a high score based on the bid. even though it has made the highest bid, how-
ever, there is still some chance that another advertisement will be chosen if it is
very popular and has a moderately high bid for the same keywords.

much ongoing research is directed at developing algorithms to maximize the
advertiser   s profit, drawing on fields such as economics and game theory. from
the information retrieval perspective, the key issues are techniques for matching
short pieces of text (the query and the advertisement) and selecting keywords to
represent the content of web pages.

when searching the web, there are usually many pages that contain all of the
query terms. this is not the case, however, when queries are compared to adver-
tisements. advertisements contain a small number of words or keywords relative
to a typical page, and the database of advertisements will be several orders of mag-
nitude smaller than the web. it is also important that variations of advertisement
keywords that occur in queries are matched. for example, if a pet supply com-
pany has placed a high bid for    aquarium   , they would expect to receive some
tra   c from queries about    fish tanks   . this, of course, is the classic vocabulary
mismatch problem, and many techniques have been proposed to address this, such
as id30 and id183. since advertisements are short, techniques for
expanding the documents as well as the queries have been considered.

two techniques that have performed well in experiments are query reformu-
lation based on user sessions in query logs (jones et al., 2006) and expansion of
queries and documents using external sources, such as the web (metzler et al.,
2007).

studies have shown that about 50% of the queries in a single session are refor-
mulations, where the user modifies the original query through word replacements,

17 the term    long-tail    comes from the long tail of the zipf distribution described in
chapter 4. assuming that a query refers to a specific combination of words, most
queries occur infrequently, and a relatively small number account for the majority of
the query instances that are processed by search engines.

220

6 queries and interfaces

insertions, and deletions. given a large number of candidate associations between
queries and phrases in those queries, statistical tests, such as those described in
section 6.2.3, can be used to determine which associations are significant. for ex-
ample, the association between the phrases    fish tank    and    aquarium    may occur
often in search sessions as users reformulate their original query to find more web
pages. if this happens often enough relative to the frequency of these phrases, it
will be considered significant. the significant associations can be used as potential
substitutions, so that, given an initial query, a ranked list of query reformulations
can be generated, with the emphasis on generating queries that contain matches
for advertising keywords.

the expansion technique consists of using the web to expand either the query,
the advertisement text, or both. a form of pseudo-relevance feedback is used
where the advertisement text or keywords are used as a query for a web search,
and expansion words are selected from the highest-ranking web pages. experi-
ments have shown that the most e   ective relevance ranking of advertisements is
when exact matches of the whole query are ranked first, followed by exact matches
of the whole query with words replaced by stems, followed by a probabilistic sim-
ilarity match of the expanded query with the expanded advertisement. the type
of similarity match used is described in section 7.3.

fig. 6.5. advertisements displayed by a search engine for the query    fish tanks   

fishtanks at targetfindfishtanks online. shop & save at target.com today. www.target.comaquariums540+ aquariums at great prices. fishbowls.pronto.comfreshwaterfish specieseverything you need to know to keep your setup clean and beautiful www.fishchannel.com pet supplies at shop.comshop millions of products and buy from our trusted merchants. shop.comcustomfishtankschoose from 6,500+ pet supplies. save on custom fishtanks! shopzilla.com 6.3 showing the results

221

as an example, figure 6.5 shows the list of advertisements generated by a
search engine for the query    fish tanks   . two of the advertisements are obvious
matches, in that    fish tanks    occurs in the titles. two of the others (the second and
fourth) have no words in common with the query, although they are clearly rele-
vant. using the simple pseudo-relevance feedback technique described in section
6.2.4 would produce both    aquarium    (frequency 10) and    acrylic    (frequency
7) as expansion terms based on the top 10 results. this would give advertisements
containing    aquarium   , such as the second one, a higher relevance score in the se-
lection process. the fourth advertisement has presumably been selected because
the pet supplier has bid on the keyword    aquarium   , and potentially because many
people have clicked on this advertisement. the third advertisement is similar and
matches one of the query words.

in the case of contextual advertising for web pages, keywords typically are ex-
tracted from the contents of the page and then used to search the advertising
database to select advertisements for display along with the contents of the page.
keyword selection techniques are similar to the summarization techniques de-
scribed in the last section, with the focus on keywords rather than sentences. a
simple approach would be to select the top words ranked by a significance weight
based on relative frequencies in the document and the collection of documents.
a more e   ective approach is to use a classifier based on machine learning tech-
niques, as described in chapter 9. a classifier uses a weighted combination of
features to determine which words and phrases are significant. typical features
include the frequency in the document, the number of documents in which the
word or phrase occurs, functions of those frequencies (such as taking the log or
normalizing), frequency of occurrence in the query log, location of the word or
phrase in the document (e.g., the title, body, anchor text, metadata, url), and
whether the word or phrase was capitalized or highlighted in some way. the most
useful features are the document and query log frequency information (yih et al.,
2006).

6.3.3 id91 the results

the results returned by a search engine are often related to di   erent aspects of the
query topic. in the case of an ambiguous query, these groups of documents can
represent very di   erent interpretations of the query. for example, we have seen
how the query    tropical fish    retrieves documents related to aquariums, pet sup-
plies, images, and other subtopics. an even simpler query, such as    fish   , is likely

222

6 queries and interfaces

to retrieve a heterogeneous mix of documents about the sea, software products, a
rock singer, and anything else that happens to use the name    fish   . if a user is in-
terested in a particular aspect of a query topic, scanning through many pages on
di   erent aspects could be frustrating. this is the motivation for the use of cluster-
ing techniques on search results. id91 groups documents that are similar in
content and labels the clusters so they can be quickly scanned for relevance.

fig. 6.6. clusters formed by a search engine from top-ranked documents for the query
   tropical fish   . numbers in brackets are the number of documents in the cluster.

figure 6.6 shows a list of clusters formed by a web search engine from the top-
ranked documents for the query    tropical fish   . this list, where each cluster is de-
scribed or labeled using a single word or phrase and includes a number indicating
the size of the cluster, is displayed to the side of the usual search results. users that
are interested in one of these clusters can click on the cluster label to see a list of
those documents, rather than scanning the ranked list to find documents related
to that aspect of the query. in this example, the clusters are clearly related to the
subtopics we mentioned previously, such as supplies and pictures.

id91 techniques are discussed in detail in chapter 9. in this section, we
focus on the specific requirements for the task of id91 search results. the first
of these requirements is e   ciency. the clusters that are generated must be specific
to each query and are based on the top-ranked documents for that query. the clus-
ters for popular queries could be cached, but clusters will still need to be generated
online for most queries, and this process has to be e   cient. one consequence of
this is that cluster generation is usually based on the text of document snippets,

pictures (38) aquarium fish (28) tropical fish aquarium (26) exporter (31) supplies (32) plants, aquatic (18) fish tank (15) breeding (16) marine fish (16) aquaria (9) 6.3 showing the results

223

rather than the full text. snippets typically contain many fewer words than the full
text, which will substantially speed up calculations that involve comparing word
overlap. snippet text is also designed to be focused on the query topic, whereas
documents can contain many text passages that are only partially relevant.

the second important requirement for result clusters is that they are easy to
understand. in the example in figure 6.6, each cluster is labeled by a single word
or phrase, and the user will assume that every document in that cluster will be
described by that concept. in the cluster labeled    pictures   , for example, it is rea-
sonable to expect that every document would contain some pictures of fish. this
is an example of a monothetic classification, where every member of a class has
the property that defines the class.18 this may sound obvious, but in fact it is not
the type of classification produced by most id91 algorithms. membership of
a class or cluster produced by an algorithm such as id11619 is based on word
overlap. in other words, members of clusters share many properties, but there is
no single defining property. this is known as a polythetic classification. for result
id91, techniques that produce monothetic classifications (or, at least, those
that appear to be monothetic) are preferred because they are easier to understand.
as an example, consider documents in the search results d1, d2, d3, and d4
that contain the terms (i.e., words or phrases){a, b, c, d, e, f, g}. the sets of terms
representing each document are:

d1 = {a, b, c}
d2 = {a, d, e}
d3 = {d, e, f, g}
d4 = {f, g}

a monothetic algorithm may decide that a and e are significant terms and pro-
duce the two clusters {d1, d2} (with cluster label a) and {d2, d3} (labeled e).
note that these clusters are overlapping, in that a document may belong to more
than one cluster. a polythetic algorithm may decide that, based on term overlap,
the only significant cluster is{d2, d3, d4}   d2 has two terms in common with
d3, and d3 has two terms in common with d4. note that these three documents
have no single term in common, and it is not clear how this cluster would be la-
beled.
18 this is also the definition of a class proposed by aristotle over 2,400 years ago.
19 id116 id91 is described in chapter 9, but basically a document is compared

to representatives of the existing clusters and added to the most similar cluster.

224

6 queries and interfaces

if we consider the list of snippets shown in figure 6.1, a simple id91 based

on the non-stopwords that occur in more than one document would give us:

(2, 3, 4)
(1, 5, 10)
(6, 8)

aquarium (5) (documents 1, 3, 4, 5, 8)
freshwater (4) (1, 8, 9, 10)
species (3)
hobby (3)
forums (2)
in an actual implementation of this technique, both words and phrases would
be considered and many more of the top-ranking snippets (say, 200) would be
used. additional features of the words and phrases, such as whether they occurred
in titles or snippets, the length of the phrase, and the collection frequency of the
phrase, as well as the overlap of the resulting clusters, would be considered in
choosing the final set of clusters.

an alternative approach for organizing results into meaningful groups is to
use faceted classification or, more simply, facets. a faceted classification consists of
a set of categories, usually organized into a hierarchy, together with a set of facets
that describe the important properties associated with the category. a product de-
scribed by a faceted classification, for example, could be labeled by more than one
category and will have values for every facet. faceted classifications are primar-
ily manually defined, although it is possible to support faceted browsing for data
that has been structured using a database schema, and techniques for construct-
ing faceted classifications automatically are being studied. the major advantage
of manually defining facets is that the categories are in general easier for the user
to understand than automatically generated cluster labels. the disadvantages are
that a classification has to be defined for each new application and domain, and
manual classifications tend to be static and not as responsive to new data as dy-
namically constructed clusters.

facets are very common in e-commerce sites. figure 6.7 shows the set of cat-
egories returned for the query    tropical fish    for a search on a popular retailer   s
site. the numbers refer to the number of products in each category that match
the query. these categories are displayed to the side of the search results, similar
to the clusters discussed earlier. if the    home & garden    category is selected, fig-
ure 6.8 shows that what is displayed is a list of subcategories, such as    pet supplies   ,
together with facets for this category, which include the brand name, supplier or
vendor name, discount level, and price. a given product, such as an aquarium, can
be found under    pet supplies    and in the appropriate price level, discount level,

6.3 showing the results

225

etc. this type of organization provides the user with both guidance and flexibility
in browsing the search results.

fig. 6.7. categories returned for the query    tropical fish    in a popular online retailer

fig. 6.8. subcategories and facets for the    home & garden    category

books (7,845) home & garden (2,477) apparel (236) home improvement (169) jewelry & watches (76) sports & outdoors (71) office products (68) toys & games (62) everything else (44) electronics (26) baby (25)   dvd (12) music (11) software (10) gourmet food (6) beauty (4) automotive (4) magazine subscriptions (3) health & personal care (3) wireless accessories (2) video games (1)   home & garden  kitchen & dining (149) furniture & d  cor (1,776) pet supplies (368) bedding & bath (51) patio & garden (22) art & craft supplies (12) home appliances (2) vacuums, cleaning & storage (107)  brand        <brand names> seller       <vendor names>  discount   up to 25% off (563) 25%    50% off (472) 50%    70% off (46) 70% off or more (46)  price   $0  $24 (1,032) $25  $49 (394) $50  $99 (797) $100  $199 (206) $200  $499 (39) $500  $999 (9) $1000  $1999 (5) $5000  $9999 (7)  226

6 queries and interfaces

6.4 cross-language search

by translating queries for one or more monolingual search engines covering dif-
ferent languages, it is possible to do cross-language search20 (see figure 6.9). a
cross-language search engine receives a query in one language (e.g., english) and
retrieves documents in a variety of other languages (e.g., french and chinese).
users typically will not be familiar with a wide range of languages, so a cross-
language search system must do the query translation automatically. since the
system also retrieves documents in multiple languages, some systems also trans-
late these for the user.

fig. 6.9. cross-language search

the most obvious approach to automatic translation would be to use a large
bilingual dictionary that contained the translation of a word in the source lan-
guage (e.g., english) to the target language (e.g., french). sentences would then be
translated by looking up each word in the dictionary. the main issue is how to deal
with ambiguity, since many words have multiple translations. simple dictionary-
based translations are generally poor, but a number of techniques have been de-
veloped, such as id183 (section 6.2.3), that reduce ambiguity and in-

20 also called cross-language information retrieval (clir), cross-lingual search, and mul-

tilingual search.

translated queriessearch engines for other languagesretrieved documents in other languagesusertranslatequerytranslate6.4 cross-language search

227

crease the ranking e   ectiveness of a cross-language system to be comparable to a
monolingual system.

the most e   ective and general methods for automatic translation are based on
id151 models (manning & sch  tze, 1999). when translat-
ing a document or a web page, in contrast to a query, not only is ambiguity a prob-
lem, but the translated sentences should also be grammatically correct. words can
change order, disappear, or become multiple words when a sentence is translated.
statistical translation models represent each of these changes with a id203.
this means that the model describes the id203 that a word is translated into
another word, the id203 that words change order, and the id203 that
words disappear or become multiple words. these probabilities are used to calcu-
late the most likely translation for a sentence.21

although a model that is based on word-to-word translation probabilities has
some similarities to a dictionary-based approach, if the translation probabilities
are accurate, they can make a large di   erence to the quality of the translation. un-
usual translations for an ambiguous word can then be easily distinguished from
more typical translations. more recent versions of these models, called phrase-
based translation models, further improve the use of context in the translation by
calculating the probabilities of translating sequences of words, rather than just in-
dividual words. a word such as    flight   , for example, could be more accurately
translated as the phrase    commercial flight   , instead of being interpreted as    bird
flight   .

the probabilities in id151 models are estimated pri-
marily by using parallel corpora. these are collections of documents in one lan-
guage together with the translations into one or more other languages. the cor-
pora are obtained primarily from government organizations (such as the united
nations), news organizations, and by mining the web, since there are hundreds
of thousands of translated pages. the sentences in the parallel corpora are aligned
either manually or automatically, which means that sentences are paired with
their translations. the aligned sentences are then used for training the translation
model.
21 the simplest form of a machine translation model is actually very similar to the query
likelihood model described in section 7.3.1. the main di   erence is the incorporation
of a translation id203 p (wi|wj), which is the id203 that a word wj can be
translated into the word wi, in the estimation of p (q|d). p (q|d) is the id203
of generating a query from a document, which in the translation model becomes the
id203 that a query is a translation of the document.

228

6 queries and interfaces

special attention has to be paid to the translation of unusual words, espe-
cially proper nouns such as people   s names. for these words in particular, the
web is a rich resource. automatic id68 techniques are also used to ad-
dress the problem of people   s names. proper names are not usually translated into
another language, but instead are transliterated, meaning that the name is writ-
ten in the characters of another language according to certain rules or based on
similar sounds. this can lead to many alternative spellings for the same name.
for example, the libyan leader muammar qaddafi   s name can found in many
di   erent transliterated variants on web pages, such as qathafi, kaddafi, qadafi,
gadafi, gaddafi, kathafi, kadhafi, qadhafi, qazzafi, kazafi, qaddafy, qadafy,
quadha   , gadhdhafi, al-qaddafi, al-qaddafi, and al qaddafi. similarly, there
are a number of variants of    bill clinton    on arabic web pages.

although they are not generally regarded as cross-language search systems,
web search engines can often retrieve pages in a variety of languages. for that
reason, many search engines have made translation available on the result pages.
figure 6.10 shows an example of a page retrieved for the query    pecheur france   ,
where the translation option is shown as a hyperlink. clicking on this link pro-
duces a translation of the page (not the snippet), which makes it clear that the
page contains links to archives of the sports magazine le p  cheur de france, which
is translated as    the fisherman of france   . although the translation provided is
not perfect, it typically provides enough information for someone to understand
the contents and relevance of the page. these translations are generated automat-
ically using machine translation techniques, since any human intervention would
be prohibitively expensive.

fig. 6.10. a french web page in the results list for the query    pecheur france   

references and further reading

this chapter has covered a wide range of topics that have been studied for a num-
ber of years. consequently, there are many references that are relevant and provide

lep  cheur de france archives @ peche poissons- [ translate this page ]lep  cheur de france les m  dia revues de p  che revue de presse archives de la revue lep  cheur de france janvier 2003 n  234 le p  cheur de france mars 2003 ... 6.4 cross-language search

229

more detail than we are able to cover here. the following papers and books rep-
resent some of the more significant contributions, but each contains pointers to
other work for people interested in gaining a deeper understanding of a specific
topic.

the advantages and disadvantages of boolean queries relative to    natural lan-
guage    or keyword queries has been discussed for more than 30 years. this debate
has been particularly active in the legal retrieval field, which saw the introduction
of the first search engines using ranking and simple queries on large collections in
the early 1990s. turtle (1994) describes one of the few quantitative comparisons
of expert boolean searching to ranking based on simple queries, and found that
simple queries are surprisingly e   ective, even in this professional environment.
the next chapter contains more discussion of the boolean retrieval model.

a more detailed description of query-based id30 based on corpus analy-
sis can be found in j. xu and croft (1998). a good source for the earlier history
of association measures such as dice   s coe   cient that have been used for informa-
tion retrieval is van rijsbergen (1979). peng et al. (2007) describe a more recent
version of corpus-based id30 for web search.

kukich (1992) provides an overview of id147 techniques. for
a more detailed introduction to minimum id153 and the noisy channel
model for id147, see jurafsky and martin (2006). guo et al. (2008)
describe an approach that combines query refinement steps, such as spelling cor-
rection, id30, and identification of phrases, into a single model. their results
indicate that the unified model can potentially improve e   ectiveness relative to
carrying out these steps as separate processes.

id183 has been the subject of much research. efthimiadis (1996)
gives a general overview and history of id183 techniques, including
thesaurus-based expansion. as mentioned before, van rijsbergen (1979) describes
the development of association measures for information retrieval, including the
mutual information measure. in computational linguistics, the paper by church
and hanks (1989) is often referred to for the use of the mutual information mea-
sure in constructing lexicons (dictionaries). manning and sch  tze (1999) give
a good overview of these and the other association measures mentioned in this
chapter.

jing and croft (1994) describe a technique for constructing an    association
thesaurus    from virtual documents consisting of the words that co-occur with
other words. the use of query log data to support expansion is described in
beeferman and berger (2000) and cui et al. (2003).

230

6 queries and interfaces

rocchio (1971) pioneered the work on relevance feedback, which was then
followed up by a large amount of work that is reviewed in salton and mcgill
(1983) and van rijsbergen (1979). j. xu and croft (2000) is a frequently cited
paper on pseudo-relevance feedback that compared    local    techniques based on
top-ranked documents to    global    techniques based on the term associations in
the collection. the book, based on 10 years of trec experiments (voorhees
& harman, 2005), contains many descriptions of both relevance feedback and
pseudo-relevance feedback techniques.

context and personalization is a popular topic. many publications can be
found in workshops and conferences, such as the information interaction in con-
text symposium (iiix).22 wei and croft (2007) describe an experiment that raises
questions about the potential benefit of user profiles. chen et al. (2006) and zhou
et al. (2005) both discuss index structures for e   ciently processing local search
queries, but also provide general overviews of local search. v. zhang et al. (2006)
discusses local search with an emphasis on analyzing query logs.

the original work on text summarization was done by luhn (1958). goldstein
et al. (1999) describe more recent work on summarization based on sentence se-
lection. the work of berger and mittal (2000), in contrast, generates summaries
based on statistical models of the document. sun et al. (2005) describe a tech-
niques based on clickthrough data. the papers of clarke et al. (2007) and turpin
et al. (2007) focus specifically on snippet generation.

feng et al. (2007) give a general overview of the issues in sponsored search.
metzler et al. (2007) and jones et al. (2006) discuss specific techniques for match-
ing queries to short advertisements. a discussion of the issues in contextual adver-
tising (providing advertisements while browsing), as well as a specific technique
for selecting keywords from a web page, can be found in yih et al. (2006).

as mentioned earlier, many visualization techniques have been proposed over
the years for search results, and we have ignored most of these in this book. hearst
(1999) provides a good overview of the range of techniques. leouski and croft
(1996) presented one of the first evaluations of techniques for result id91.
hearst and pedersen (1996) show the potential benefits of this technique, and
zamir and etzioni (1999) emphasize the importance of clusters that made sense
to the user and were easy to label. lawrie and croft (2003) discuss a technique for
building a hierarchical summary of the results, and zeng et al. (2004) focus on the

22 this conference grew out of the information retrieval in context (irix) workshops,

whose proceedings can also be found on the web.

6.4 cross-language search

231

selection of phrases from the results as the basis of clusters. the relative advantages
and disadvantages of id91 and facets are discussed in hearst (2006).

more generally, there is a whole community of hci23 (human-computer in-
teraction) researchers concerned with the design and evaluation of interfaces for
information systems. shneiderman et al. (1998) is an example of this type of re-
search, and marchionini (2006) gives a good overview of the importance of the
search interface for interactive, or exploratory, search.

cross-language search has been studied at trec (voorhees & harman, 2005)
and at a european evaluation forum called clef24 for a number of years. the
first collection of papers in this area was in grefenstette (1998). issues that arise
in specific clir systems, such as id68 (abduljaleel & larkey, 2003),
are discussed in many papers in the literature. manning and sch  tze (1999) and
jurafsky and martin (2006) give overviews of id151 mod-
els.

finally, there has been a large body of work in the information science lit-
erature that has looked at how people actually search and interact with search
engines. this research is complementary to the more systems-oriented approach
taken in this chapter, and is a crucial part of understanding the process of looking
for information and relevance. the journal of the american society of informa-
tion science and technology (jasist) is the best source for these type of papers,
and ingwersen and j  rvelin (2005) provide an interesting comparison of the com-
puter science and information science perspectives on search.

exercises

6.1. using the wikipedia collection provided at the book website, create a sample
of stem clusters by the following process:
1. index the collection without id30.
2. identify the first 1,000 words (in alphabetical order) in the index.
3. create stem classes by id30 these 1,000 words and recording which

words become the same stem.

4. compute association measures (dice   s coe   cient) between all pairs of stems

in each stem class. compute co-occurrence at the document level.

23 sometimes referred to as chi.
24 http://clef.isti.cnr.it/

232

6 queries and interfaces

5. create stem clusters by thresholding the association measure. all terms that

are still connected to each other form the clusters.

compare the stem clusters to the stem classes in terms of size and the quality (in
your opinion) of the groupings.

6.2. create a simple spelling corrector based on the id87. use a
single-word language model, and an error model where all errors with the same
id153 have the same id203. only consider id153s of 1 or 2.
implement your own id153 calculator (example code can easily be found
on the web).

6.3. implement a simple pseudo-relevance feedback algorithm for the galago
search engine. provide examples of the id183s that your algorithm does,
and summarize the problems and successes of your approach.

6.4. assuming you had a gazetteer of place names available, sketch out an algo-
rithm for detecting place names or locations in queries. show examples of the
types of queries where your algorithm would succeed and where it would fail.

6.5. describe the snippet generation algorithm in galago. would this algorithm
work well for pages with little text content? describe in detail how you would
modify the algorithm to improve it.

6.6. pick a commercial web search engine and describe how you think the query
is matched to the advertisements for sponsored search. use examples as evidence
for your ideas. do the same thing for advertisements shown with web pages.

6.7. implement a simple algorithm that selects phrases from the top-ranked pages
as the basis for result clusters. phrases should be considered as any two-word se-
quence. your algorithm should take into account phrase frequency in the results,
phrase frequency in the collection, and overlap in the clusters associated with the
phrases.

6.8. find four di   erent types of websites that use facets, and describe them with
examples.

6.9. give five examples of web page translation that you think is poor. why do
you think the translation failed?

7

retrieval models

   there is no certainty, only opportunity.   

v, v for vendetta

7.1 overview of retrieval models

during the last 45 years of information retrieval research, one of the primary goals
has been to understand and formalize the processes that underlie a person mak-
ing the decision that a piece of text is relevant to his information need. to develop
a complete understanding would probably require understanding how language
is represented and processed in the human brain, and we are a long way short of
that. we can, however, propose theories about relevance in the form of mathemat-
ical retrieval models and test those theories by comparing them to human actions.
good models should produce outputs that correlate well with human decisions
on relevance. to put it another way, ranking algorithms based on good retrieval
models will retrieve relevant documents near the top of the ranking (and conse-
quently will have high e   ectiveness).

how successful has modeling been? as an example, ranking algorithms for
general search improved in e   ectiveness by over 100% in the 1990s, as measured
using the trec test collections. these changes in e   ectiveness corresponded to
improvements in the associated retrieval models. web search e   ectiveness has
also improved substantially over the past 10 years. in experiments with trec
web collections, the most e   ective ranking algorithms come from well-defined
retrieval models. in the case of commercial web search engines, it is less clear what
the retrieval models are, but there is no doubt that the ranking algorithms rely on
solid mathematical foundations.

it is possible to develop ranking algorithms without an explicit retrieval model
through trial and error. using a retrieval model, however, has generally proved to
be the best approach. retrieval models, like all mathematical models, provide a

234

7 retrieval models

framework for defining new tasks and explaining assumptions. when problems
are observed with a ranking algorithm, the retrieval model provides a structure
for testing alternatives that will be much more e   cient than a brute force (try ev-
erything) approach.

in this discussion, we must not overlook the fact that relevance is a complex
concept. it is quite di   cult for a person to explain why one document is more
relevant than another, and when people are asked to judge the relevance of docu-
ments for a given query, they can often disagree. information scientists have writ-
ten volumes about the nature of relevance, but we will not dive into that material
here. instead, we discuss two key aspects of relevance that are important for both
retrieval models and evaluation measures.

the first aspect is the di   erence between topical and user relevance, which was
mentioned in section 1.1. a document is topically relevant to a query if it is judged
to be on the same topic. in other words, the query and the document are about
the same thing. a web page containing a biography of abraham lincoln would
certainly be topically relevant to the query    abraham lincoln   , and would also be
topically relevant to the queries    u.s. presidents    and    civil war   . user relevance
takes into account all the other factors that go into a user   s judgment of relevance.
this may include the age of the document, the language of the document, the
intended target audience, the novelty of the document, and so on. a document
containing just a list of all the u.s. presidents, for example, would be topically
relevant to the query    abraham lincoln    but may not be considered relevant to
the person who submitted the query because they were looking for more detail
on lincoln   s life. retrieval models cannot incorporate all the additional factors
involved in user relevance, but some do take these factors into consideration.

the second aspect of relevance that we consider is whether it is binary or mul-
tivalued. binary relevance simply means that a document is either relevant or
not relevant. it seems obvious that some documents are less relevant than others,
but still more relevant than documents that are completely o   -topic. for exam-
ple, we may consider the document containing a list of u.s. presidents to be less
topically relevant than the lincoln biography, but certainly more relevant than
an advertisement for a lincoln automobile. based on this observation, some re-
trieval models and evaluation measures explicitly introduce relevance as a multi-
valued variable. multiple levels of relevance are certainly important in evaluation,
when people are asked to judge relevance. having just three levels (relevant, non-
relevant, unsure) has been shown to make the judges    task much easier. in the case
of retrieval models, however, the advantages of multiple levels are less clear. this

7.1 overview of retrieval models

235

is because most ranking algorithms calculate a id203 of relevance and can
represent the uncertainty involved.

many retrieval models have been proposed over the years. two of the oldest
are the boolean and vector space models. although these models have been largely
superseded by probabilistic approaches, they are often mentioned in discussions
about information retrieval, and so we describe them briefly before going into the
details of other models.

7.1.1 boolean retrieval

the boolean retrieval model was used by the earliest search engines and is still in
use today. it is also known as exact-match retrieval since documents are retrieved
if they exactly match the query specification, and otherwise are not retrieved. al-
though this defines a very simple form of ranking, boolean retrieval is not gener-
ally described as a ranking algorithm. this is because the boolean retrieval model
assumes that all documents in the retrieved set are equivalent in terms of rele-
vance, in addition to the assumption that relevance is binary. the name boolean
comes from the fact that there only two possible outcomes for query evaluation
(true and false) and because the query is usually specified using operators from
boolean logic (and, or, not). as mentioned in chapter 6, proximity operators
and wildcard characters are also commonly used in boolean queries. searching
with a regular expression utility such as grep is another example of exact-match
retrieval.

there are some advantages to boolean retrieval. the results of the model are
very predictable and easy to explain to users. the operands of a boolean query can
be any document feature, not just words, so it is straightforward to incorporate
metadata such as a document date or document type in the query specification.
from an implementation point of view, boolean retrieval is usually more e   cient
than ranked retrieval because documents can be rapidly eliminated from consid-
eration in the scoring process.

despite these positive aspects, the major drawback of this approach to search
is that the e   ectiveness depends entirely on the user. because of the lack of a so-
phisticated ranking algorithm, simple queries will not work well. all documents
containing the specified query words will be retrieved, and this retrieved set will
be presented to the user in some order, such as by publication date, that has lit-
tle to do with relevance. it is possible to construct complex boolean queries that
narrow the retrieved set to mostly relevant documents, but this is a di   cult task

236

7 retrieval models

that requires considerable experience. in response to the di   culty of formulat-
ing queries, a class of users known as search intermediaries (mentioned in the last
chapter) became associated with boolean search systems. the task of an interme-
diary is to translate a user   s information need into a complex boolean query for
a particular search engine. intermediaries are still used in some specialized areas,
such as in legal o   ces. the simplicity and e   ectiveness of modern search engines,
however, has enabled most people to do their own searches.

as an example of boolean query formulation, consider the following queries
for a search engine that has indexed a collection of news stories. the simple query:

lincoln

would retrieve a large number of documents that mention lincoln cars and places
named lincoln in addition to stories about president lincoln. all of these doc-
uments would be equivalent in terms of ranking in the boolean retrieval model,
regardless of how many times the word    lincoln    occurs or in what context it oc-
curs. given this, the user may attempt to narrow the scope of the search with the
following query:

president and lincoln

this query will retrieve a set of documents that contain both words, occurring
anywhere in the document. if there are a number of stories involving the manage-
ment of the ford motor company and lincoln cars, these will be retrieved in the
same set as stories about president lincoln, for example:

ford motor company today announced that darryl hazel will succeed
brian kelley as president of lincoln mercury.

if enough of these types of documents were retrieved, the user may try to eliminate
documents about cars by using the not operator, as follows:

president and lincoln and not (automobile or car)

this would remove any document that contains even a single mention of the
words    automobile    or    car    anywhere in the document. the use of the not op-
erator, in general, removes too many relevant documents along with non-relevant
documents and is not recommended. for example, one of the top-ranked docu-
ments in a web search for    president lincoln    was a biography containing the
sentence:

lincoln   s body departs washington in a nine-car funeral train.

7.1 overview of retrieval models

237

using not (automobile or car) in the query would have removed this document.
if the retrieved set is still too large, the user may try to further narrow the query
by adding in additional words that should occur in biographies:

president and lincoln and biography and life and birthplace and get-
tysburg and not (automobile or car)

unfortunately, in a boolean search engine, putting too many search terms into the
query with the and operator often results in nothing being retrieved. to avoid
this, the user may try using an or instead:

president and lincoln and (biography or life or birthplace or gettysburg)
and not (automobile or car)

this will retrieve any document containing the words    president    and    lincoln   ,
along with any one of the words    biography   ,    life   ,    birthplace   , or    gettysburg   
(and does not mention    automobile    or    car   ).

after all this, we have a query that may do a reasonable job at retrieving a set
containing some relevant documents, but we still can   t specify which words are
more important or that having more of the associated words is better than any one
of them. for example, a document containing the following text was retrieved at
rank 500 by a web search (which does use measures of word importance):

president   s day - holiday activities - crafts, mazes, word searches, ...    the
life of washington    read the entire book online! abraham lincoln re-
search site ...

a boolean retrieval system would make no distinction between this document
and the other 499 that are ranked higher by the web search engine. it could, for
example, be the first document in the result list.

the process of developing queries with a focus on the size of the retrieved set
has been called searching by numbers, and is a consequence of the limitations of
the boolean retrieval model. to address these limitations, researchers developed
models, such as the vector space model, that incorporate ranking.

7.1.2 the vector space model

the vector space model was the basis for most of the research in information re-
trieval in the 1960s and 1970s, and papers using this model continue to appear
at conferences. it has the advantage of being a simple and intuitively appealing
framework for implementing term weighting, ranking, and relevance feedback.

238

7 retrieval models

historically, it was very important in introducing these concepts, and e   ective
techniques have been developed through years of experimentation. as a retrieval
model, however, it has major flaws. although it provides a convenient computa-
tional framework, it provides little guidance on the details of how weighting and
ranking algorithms are related to relevance.

in this model, documents and queries are assumed to be part of a t-dimensional
vector space, where t is the number of index terms (words, stems, phrases, etc.). a
document di is represented by a vector of index terms:

di = (di1, di2, . . . , dit),

where dij represents the weight of the jth term. a document collection contain-
ing n documents can be represented as a matrix of term weights, where each row
represents a document and each column describes weights that were assigned to
a term for a particular document:

t erm1 t erm2 . . . t ermt

doc1
doc2
...
docn

d11
d21
...
dn1

d12
d22

. . .
. . .

d1t
d2t

dn2

. . .

dnt

figure 7.1 gives a simple example of the vector representation for four docu-
ments. the term-document matrix has been rotated so that now the terms are the
rows and the documents are the columns. the term weights are simply the count
of the terms in the document. stopwords are not indexed in this example, and
the words have been stemmed. document d3, for example, is represented by the
vector (1, 1, 0, 2, 0, 1, 0, 1, 0, 0, 1).

queries are represented the same way as documents. that is, a query q is rep-

resented by a vector of t weights:

q = (q1, q2, . . . , qt),

where qj is the weight of the jth term in the query. if, for example the query was
   tropical fish   , then using the vector representation in figure 7.1, the query would
be (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1). one of the appealing aspects of the vector space
model is the use of simple diagrams to visualize the documents and queries. typ-
ically, they are shown as points or vectors in a three-dimensional picture, as in

7.1 overview of retrieval models

239

fig. 7.1. term-document matrix for a collection of four documents

figure 7.2. although this can be helpful for teaching, it is misleading to think
that an intuition developed using three dimensions can be applied to the actual
high-dimensional document space. remember that the t terms represent all the
document features that are indexed. in enterprise and web applications, this cor-
responds to hundreds of thousands or even millions of dimensions.

given this representation, documents could be ranked by computing the dis-
tance between the points representing the documents and the query. more com-
monly, a similarity measure is used (rather than a distance or dissimilarity mea-
sure), so that the documents with the highest scores are the most similar to the
query. a number of similarity measures have been proposed and tested for this
purpose. the most successful of these is the cosine correlation similarity measure.
the cosine correlation measures the cosine of the angle between the query and
the document vectors. when the vectors are normalized so that all documents
and queries are represented by vectors of equal length, the cosine of the angle be-
tween two identical vectors will be 1 (the angle is zero), and for two vectors that
do not share any non-zero terms, the cosine will be 0. the cosine measure is de-
fined as:

d1   tropical freshwater aquarium fish. d2   tropical fish, aquarium care, tank setup. d3   keeping tropical fish and goldfish in aquariums,          and fish bowls. d4   the tropical tank homepage - tropical fish and          aquariums. terms                            documents                              d1       d2       d3       d4          aquarium    1    1    1    1 bowl        0    0    1    0 care    0    1    0    0  fish         1    1    2    1 freshwater   1    0    0    0 goldfish                  0    0    1    0   homepage     0    0    0    1 keep    0    0    1    0 setup    0    1    0    0 tank  0    1    0    1 tropical         1    1    1    2 240

7 retrieval models

fig. 7.2. vector representation of documents and queries

   

cosine(di, q) =

t   
t   

j=1

dij    qj
t   

2   

dij

2

qj

j=1

j=1

the numerator of this measure is the sum of the products of the term weights
for the matching query and document terms (known as the dot product or in-
ner product). the denominator normalizes this score by dividing by the product
of the lengths of the two vectors. there is no theoretical reason why the cosine
correlation should be preferred to other similarity measures, but it does perform
somewhat better in evaluations of search quality.

as an example, consider two documents d1 = (0.5, 0.8, 0.3) and d2 =
(0.9, 0.4, 0.2) indexed by three terms, where the numbers represent term weights.
given the query q = (1.5, 1.0, 0) indexed by the same terms, the cosine mea-
sures for the two documents are:

term2term3doc1querydoc2241

7.1 overview of retrieval models

1.55

(0.5    1.5) + (0.8    1.0)

   
   
(0.52 + 0.82 + 0.32)(1.52 + 1.02)
(0.98    3.25)
   
   
(0.92 + 0.42 + 0.22)(1.52 + 1.02)
(1.01    3.25)

(0.9    1.5) + (0.4    1.0)

= 0.87

= 0.97

1.75

cosine(d1, q) =

=

cosine(d2, q) =

=

the second document has a higher score because it has a high weight for the first
term, which also has a high weight in the query. even this simple example shows
that ranking based on the vector space model is able to reflect term importance
and the number of matching terms, which is not possible in boolean retrieval.

in this discussion, we have yet to say anything about the form of the term
weighting used in the vector space model. in fact, many di   erent weighting schemes
have been tried over the years. most of these are variations on tf.idf weighting,
which was described briefly in chapter 2. the term frequency component, tf, re-
flects the importance of a term in a document di (or query). this is usually com-
puted as a normalized count of the term occurrences in a document, for example
by

tfik =

t   

fik

fij

j=1

where tfik is the term frequency weight of term k in document di, and fik is
the number of occurrences of term k in the document. in the vector space model,
id172 is part of the cosine measure. a document collection can contain
documents of many di   erent lengths. although id172 accounts for this
to some degree, long documents can have many terms occurring once and others
occurring hundreds of times. retrieval experiments have shown that to reduce the
impact of these frequent terms, it is e   ective to use the logarithm of the number
of term occurrences in tf weights rather than the raw count.

the inverse document frequency component (idf) reflects the importance of
the term in the collection of documents. the more documents that a term occurs
in, the less discriminating the term is between documents and, consequently, the
less useful it will be in retrieval. the typical form of this weight is

242

7 retrieval models

idfk = log n
nk

where idfk is the inverse document frequency weight for term k, n is the number
of documents in the collection, and nk is the number of documents in which term
k occurs. the form of this weight was developed by intuition and experiment,
although an argument can be made that idf measures the amount of information
carried by the term, as defined in id205 (robertson, 2004).

the e   ects of these two weights are combined by multiplying them (hence
the name tf.idf). the reason for combining them this way is, once again, mostly
empirical. given this, the typical form of document term weighting in the vector
space model is:

   

dik =

t   
(log(fik) + 1)    log(n /nk)
[(log(fik) + 1.0)    log(n /nk)]2

k=1

the form of query term weighting is essentially the same. adding 1 to the term fre-
quency component ensures that terms with frequency 1 have a non-zero weight.
note that, in this model, term weights are computed only for terms that occur in
the document (or query). given that the cosine measure id172 is incor-
porated into the weights, the score for a document is computed using simply the
dot product of the document and query vectors.

although there is no explicit definition of relevance in the vector space model,
there is an implicit assumption that relevance is related to the similarity of query
and document vectors. in other words, documents    closer    to the query are more
likely to be relevant. this is primarily a model of topical relevance, although fea-
tures related to user relevance could be incorporated into the vector representa-
tion. no assumption is made about whether relevance is binary or multivalued.
in the last chapter we described relevance feedback, a technique for query
modification based on user-identified relevant documents. this technique was
first introduced using the vector space model. the well-known rocchio algorithm
(rocchio, 1971) was based on the concept of an optimal query, which maximizes
the di   erence between the average vector representing the relevant documents
and the average vector representing the non-relevant documents. given that only
limited relevance information is typically available, the most common (and e   ec-
tive) form of the rocchio algorithm modifies the initial weights in query vector
q to produce a new query q

    according to

   
j =   .qj +   .

q

1|rel|

   

di   rel

7.2 probabilistic models

243

   

dij       .

1

|n onrel|

dij

di   n onrel

where qj is the initial weight of query term j, rel is the set of identified relevant
documents, n onrel is the set of non-relevant documents, |.| gives the size of a
set, dij is the weight of the jth term in document i, and   ,   , and    are parame-
ters that control the e   ect of each component. previous studies have shown that
the set of non-relevant documents is best approximated by all unseen documents
(i.e., all documents not identified as relevant), and that reasonable values for the
parameters are 8, 16, and 4 for   ,   , and   , respectively.

this formula modifies the query term weights by adding a component based
on the average weight in the relevant documents and subtracting a component
based on the average weight in the non-relevant documents. query terms with
weights that are negative are dropped. this results in a longer or expanded query
because terms that occur frequently in the relevant documents but not in the orig-
inal query will be added (i.e., they will have non-zero positive weights in the mod-
ified query). to restrict the amount of expansion, typically only a certain number
(say, 50) of the terms with the highest average weights in the relevant documents
will be added to the query.

7.2 probabilistic models

one of the features that a retrieval model should provide is a clear statement about
the assumptions upon which it is based. the boolean and vector space approaches
make implicit assumptions about relevance and text representation that impact
the design and e   ectiveness of ranking algorithms. the ideal situation would be
to show that, given the assumptions, a ranking algorithm based on the retrieval
model will achieve better e   ectiveness than any other approach. such proofs are
actually very hard to come by in information retrieval, since we are trying to for-
malize a complex human activity. the validity of a retrieval model generally has
to be validated empirically, rather than theoretically.

one early theoretical statement about e   ectiveness, known as the probabil-
ity ranking principle (robertson, 1977/1997), encouraged the development of
probabilistic retrieval models, which are the dominant paradigm today. these
models have achieved this status because id203 theory is a strong founda-
tion for representing and manipulating the uncertainty that is an inherent part

244

7 retrieval models

of the information retrieval process. the id203 ranking principle, as origi-
nally stated, is as follows:

if a reference retrieval system   s1 response to each request is a ranking of
the documents in the collection in order of decreasing id203 of rel-
evance to the user who submitted the request, where the probabilities are
estimated as accurately as possible on the basis of whatever data have been
made available to the system for this purpose, the overall e   ectiveness of
the system to its user will be the best that is obtainable on the basis of those
data.
given some assumptions, such as that the relevance of a document to a query
is independent of other documents, it is possible to show that this statement is
true, in the sense that ranking by id203 of relevance will maximize preci-
sion, which is the proportion of relevant documents, at any given rank (for exam-
ple, in the top 10 documents). unfortunately, the id203 ranking principle
doesn   t tell us how to calculate or estimate the id203 of relevance. there are
many probabilistic retrieval models, and each one proposes a di   erent method for
estimating this id203. most of the rest of this chapter discusses some of the
most important probabilistic models.

in this section, we start with a simple probabilistic model based on treating
information retrieval as a classification problem. we then describe a popular and
e   ective ranking algorithm that is based on this model.

7.2.1 information retrieval as classification

in any retrieval model that assumes relevance is binary, there will be two sets of
documents, the relevant documents and the non-relevant documents, for each
query. given a new document, the task of a search engine could be described as
deciding whether the document belongs in the relevant set or the non-relevant2
set. that is, the system should classify the document as relevant or non-relevant,
and retrieve it if it is relevant.

given some way of calculating the id203 that the document is relevant
and the id203 that it is non-relevant, then it would seem reasonable to clas-
sify the document into the set that has the highest id203. in other words,

1 a    reference retrieval system    would now be called a search engine.
2 note that we never talk about    irrelevant    documents in information retrieval; instead

they are    non-relevant.   

7.2 probabilistic models

245

we would decide that a document d is relevant if p (r|d) > p (n r|d), where
p (r|d) is a id155 representing the id203 of relevance
given the representation of that document, and p (n r|d) is the conditional
id203 of non-relevance (figure 7.3). this is known as the bayes decision
rule, and a system that classifies documents this way is called a bayes classifier.

in chapter 9, we discuss other applications of classification (such as spam fil-
tering) and other classification techniques, but here we focus on the ranking algo-
rithm that results from this probabilistic retrieval model based on classification.

fig. 7.3. classifying a document as relevant or non-relevant

the question that faces us now is how to compute these probabilities. to start
with, let   s focus on p (r|d). it   s not clear how we would go about calculating
this, but given information about the relevant set, we should be able to calcu-
late p (d|r). for example, if we had information about how often specific words
occurred in the relevant set, then, given a new document, it would be relatively
straightforward to calculate how likely it would be to see the combination of
words in the document occurring in the relevant set. let   s assume that the prob-
ability of the word    president    in the relevant set is 0.02, and the id203 of
   lincoln    is 0.03. if a new document contains the words    president    and    lincoln   ,
we could say that the id203 of observing that combination of words in the

relevantdocumentsnon  relevant documentsthe rain in spain fallsmainly in the plain the rain in spain fallsmainly in the plain the rain in spain fallsmainly in the plain the rain in spain fallsmainly in the plaindocumentp(r|d)p(nr|d)246

7 retrieval models

relevant set is 0.02    0.03 = 0.0006, assuming that the two words occur inde-
pendently.3
so how does calculating p (d|r) get us to the id203 of relevance? it
turns out there is a relationship between p (r|d) and p (d|r) that is expressed
by bayes    rule:4

p (r|d) =

p (d|r)p (r)

p (d)

where p (r) is the a priori id203 of relevance (in other words, how likely
any document is to be relevant), and p (d) acts as a normalizing constant. given
this, we can express our decision rule in the following way: classify a document as
relevant if p (d|r)p (r) > p (d|n r)p (n r). this is the same as classifying
a document as relevant if:

p (d|r)
p (d|n r)

>

p (n r)
p (r)

the left-hand side of this equation is known as the likelihood ratio. in most clas-
sification applications, such as spam filtering, the system must decide which class
the document belongs to in order to take the appropriate action. for information
retrieval, a search engine only needs to rank documents, rather than make that de-
cision (which is hard). if we use the likelihood ratio as a score, the highly ranked
documents will be those that have a high likelihood of belonging to the relevant
set.
to calculate the document scores, we still need to decide how to come up
with values for p (d|r) and p (d|n r). the simplest approach is to make the
same assumptions that we made in our earlier example; that is, we represent doc-
uments as a combination of words and the relevant and non-relevant sets using
word probabilities. in this model, documents are represented as a vector of binary
features, d = (d1, d2, . . . , dt), where di = 1 if term i is present in the document,
   
and 0 otherwise. the other major assumption we make is term independence (also
known as the na  ve bayes assumption). this means we can estimate p (d|r) by
i=1 p (di|r) (and similarly for
the product of the individual term probabilities
p (d|n r)). because this model makes the assumptions of term independence
and binary features in documents, it is known as the binary independence model.
3 given two events a and b, the joint id203 p (a     b) is the id203 of both
events occurring together. in general, p (a     b) = p (a|b)p (b). if a and b are
independent, this means that p (a     b) = p (a)p (b).
4 named after thomas bayes, a british mathematician.

t

7.2 probabilistic models

247

words obviously do not occur independently in text. if the word    microsoft   
occurs in a document, it is very likely that the word    windows    will also occur.
the assumption of term independence, however, is a common one since it usually
simplifies the mathematics involved in the model. models that allow some form
of dependence between terms will be discussed later in this chapter.

recall that a document in this model is a vector of 1s and 0s representing the
presence and absence of terms. for example, if there were five terms indexed, one
of the id194s might be (1, 0, 0, 1, 1), meaning that the doc-
ument contains terms 1, 4, and 5. to calculate the id203 of this document
occurring in the relevant set, we need the probabilities that the terms are 1 or 0
in the relevant set. if pi is the id203 that term i occurs (has the value 1) in
a document from the relevant set, then the id203 of our example document
occurring in the relevant set is p1    (1     p2)    (1     p3)    p4    p5. the prob-
ability (1     p2) is the id203 of term 2 not occurring in the relevant set. for
the non-relevant set, we use si to represent the id203 of term i occurring.5

going back to the likelihood ratio, using pi and si gives us a score of

where
the document. we can now do a bit of mathematical manipulation to get:

i:di=1 means that it is a product over the terms that have the value 1 in

   

   

pi
si

i:di=1

=

i:di=1

p (d|r)
p (d|n r)
   
1     si
1     pi
   
pi(1     si)
si(1     pi)

   

   (

i:di=1

=

  

i:di=1

   

   

i:di=0

  

pi
si

1     pi
1     si
   

1     pi
1     si

i:di=1

i:di=0

)   

1     pi
1     si
   

1     pi
1     si

  

i

the second product is over all terms and is therefore the same for all documents,
so we can ignore it for ranking. since multiplying lots of small numbers can lead
to problems with the accuracy of the result, we can equivalently use the logarithm
of the product, which means that the scoring function is:

   

i:di=1

log pi(1     si)
si(1     pi)

5 in many descriptions of this model, pi and qi are used for these probabilities. we use
si to avoid confusion with the qi used to represent query terms.

248

7 retrieval models

you might be wondering where the query has gone, given that this is a doc-
ument ranking algorithm for a specific query. in many cases, the query provides
us with the only information we have about the relevant set. we can assume that,
in the absence of other information, terms that are not in the query will have the
same id203 of occurrence in the relevant and non-relevant documents (i.e.,
pi = si). in that case, the summation will only be over terms that are both in
the query and in the document. this means that, given a query, the score for a
document is simply the sum of the term weights for all matching terms.

if we have no other information about the relevant set, we could make the
additional assumptions that pi is a constant and that si could be estimated by
using the term occurrences in the whole collection as an approximation. we make
the second assumption based on the fact that the number of relevant documents is
much smaller than the total number of documents in the collection. with a value
of 0.5 for pi in the scoring function described earlier, this gives a term weight for
term i of

log 0.5(1     ni
n )
n (1     0.5)

ni

= log n     ni

ni

where ni is the number of documents that contain term i, and n is the number of
documents in the collection. this shows that, in the absence of information about
the relevant documents, the term weight derived from the binary independence
model is very similar to an idf weight. there is no tf component, because the
documents were assumed to have binary features.

if we do have information about term occurrences in the relevant and non-
relevant sets, it can be summarized in a contingency table, shown in table 7.1. this
information could be obtained through relevance feedback, where users identify
relevant documents in initial rankings. in this table, ri is the number of relevant
documents containing term i, ni is the number of documents containing term i,
n is the total number of documents in the collection, and r is the number of
relevant documents for this query.

relevant

non-relevant

total
di = 1
ni
di = 0 r     ri n     ni     r + ri n     ni
total

ni     ri
n     r

n

ri

r

table 7.1. contingency table of term occurrences for a particular query

7.2 probabilistic models

249

given this table, the obvious estimates6 for pi and si would be pi = ri/r (the
number of relevant documents that contain a term divided by the total number of
relevant documents) and si = (ni     ri)/(n     r) (the number of non-relevant
documents that contain a term divided by the total number of non-relevant doc-
uments). using these estimates could cause a problem, however, if some of the
entries in the contingency table were zeros. if ri was zero, for example, the term
weight would be log 0. to avoid this, a standard solution is to add 0.5 to each
count (and 1 to the totals), which gives us estimates of pi = (ri + 0.5)/(r + 1)
and si = (ni   ri +0.5)/(n    r+1.0). putting these estimates into the scoring
function gives us:   

(ri + 0.5)/(r     ri + 0.5)

log

(ni     ri + 0.5)/(n     ni     r + ri + 0.5)

i:di=qi=1

although this document score sums term weights for just the matching query
terms, with relevance feedback the query can be expanded to include other impor-
tant terms from the relevant set. note that if we have no relevance information,
we can set r and r to 0, which would give a pi value of 0.5, and would produce
the idf-like term weight discussed before.

so how good is this document score when used for ranking? not very good,
it turns out. although it does provide a method of incorporating relevance in-
formation, in most cases we don   t have this information and instead would be
using term weights that are similar to idf weights. the absence of a tf compo-
nent makes a significant di   erence to the e   ectiveness of the ranking, and most
e   ectiveness measures will drop by about 50% if the ranking ignores this informa-
tion. this means, for example, that we might see 50% fewer relevant documents
in the top ranks if we used the binary independence model ranking instead of the
best tf.idf ranking.

it turns out, however, that the binary independence model is the basis for one

of the most e   ective and popular ranking algorithms, known as bm25.7

6 we use the term estimate for a id203 value calculated using data such as a contin-
gency table because this value is only an estimate for the true value of the id203
and would change if more data were available.

7 bm stands for best match, and 25 is just a numbering scheme used by robertson and

his co-workers to keep track of weighting variants (robertson & walker, 1994).

250

7 retrieval models

7.2.2 the bm25 ranking algorithm

bm25 extends the scoring function for the binary independence model to in-
clude document and query term weights. the extension is based on probabilistic
arguments and experimental validation, but it is not a formal model.

bm25 has performed very well in trec retrieval experiments and has influ-
enced the ranking algorithms of commercial search engines, including web search
engines. there are some variations of the scoring function for bm25, but the most
common form is:

log

(ri + 0.5)/(r     ri + 0.5)

(ni     ri + 0.5)/(n     ni     r + ri + 0.5)

   (k1 + 1)fi
k + fi

   (k2 + 1)qfi
k2 + qfi

   

i   q

where the summation is now over all terms in the query; and n, r, ni, and ri are
the same as described in the last section, with the additional condition that r and
r are set to zero if there is no relevance information; fi is the frequency of term
i in the document; qfi is the frequency of term i in the query; and k1, k2, and k
are parameters whose values are set empirically.

the constant k1 determines how the tf component of the term weight changes
as fi increases. if k1 = 0, the term frequency component would be ignored and
only term presence or absence would matter. if k1 is large, the term weight compo-
nent would increase nearly linearly with fi. in trec experiments, a typical value
for k1 is 1.2, which causes the e   ect of fi to be very non-linear, similar to the use
of log f in the term weights discussed in section 7.1.2. this means that after three
or four occurrences of a term, additional occurrences will have little impact. the
constant k2 has a similar role in the query term weight. typical values for this pa-
rameter are in the range 0 to 1,000, meaning that performance is less sensitive to
k2 than it is to k1. this is because query term frequencies are much lower and less
variable than document term frequencies.

k is a more complicated parameter that normalizes the tf component by doc-

ument length. specifically

k = k1((1     b) + b    dl

avdl

)

where b is a parameter, dl is the length of the document, and avdl is the average
length of a document in the collection. the constant b regulates the impact of the
length id172, where b = 0 corresponds to no length id172, and

7.2 probabilistic models

251

b = 1 is full id172. in trec experiments, a value of b = 0.75 was found
to be e   ective.

as an example calculation, let   s consider a query with two terms,    president   
and    lincoln   , each of which occurs only once in the query (qf = 1). we will con-
sider the typical case where we have no relevance information (r and r are zero).
let   s assume that we are searching a collection of 500,000 documents (n), and
that in this collection,    president    occurs in 40,000 documents (n1 = 40, 000)
and    lincoln    occurs in 300 documents (n2 = 300). in the document we are scor-
ing (which is about president lincoln),    president    occurs 15 times (f1 = 15)
and    lincoln    occurs 25 times (f2 = 25). the document length is 90% of the aver-
age length (dl/avdl = 0.9). the parameter values we use are k1 = 1.2, b = 0.75,
and k2 = 100. with these values, k = 1.2    (0.25 + 0.75    0.9) = 1.11, and
the document score is:

bm 25(q, d) =

(0 + 0.5)/(0     0 + 0.5)

(40000     0 + 0.5)/(500000     40000     0 + 0 + 0.5)

   (100 + 1)1
100 + 1
(0 + 0.5)/(0     0 + 0.5)

(300     0 + 0.5)/(500000     300     0 + 0 + 0.5)

log
  (1.2 + 1)15
1.11 + 15

+ log
  (1.2 + 1)25
1.11 + 25

   (100 + 1)1
100 + 1

= log 460000.5/40000.5    33/16.11    101/101
+ log 499700.5/300.5    55/26.11    101/101

= 2.44    2.05    1 + 7.42    2.11    1
= 5.00 + 15.66 = 20.66

notice the impact from the first part of the weight that, without relevance in-
formation, is nearly the same as an idf weight (as we discussed in section 7.2.1).
because the term    lincoln    is much less frequent in the collection, it has a much
higher idf component (7.42 versus 2.44). table 7.2 gives scores for di   erent num-
bers of term occurrences. this shows the importance of the    lincoln    term and
that even one occurrence of a term can make a large di   erence in the score. re-
ducing the number of term occurrences from 25 or 15 to 1 makes a significant but

252

7 retrieval models

not dramatic di   erence. this example also demonstrates that it is possible for a
document containing a large number of occurrences of a single important term to
score higher than a document containing both query terms (15.66 versus 12.74).

frequency of frequency of
   president   

   lincoln   

15
15
15
1
0

25
1
0
25
25

bm25
score
20.66
12.74
5.00
18.2
15.66

table 7.2. bm25 scores for an example document

the score calculation may seem complicated, but remember that some of the
calculation of term weights can occur at indexing time, before processing any
query. if there is no relevance information, scoring a document simply involves
adding the weights for matching query terms, with a small additional calculation
if query terms occur more than once (i.e., if qf > 1). another important point is
that the parameter values for the bm25 ranking algorithm can be tuned (i.e., ad-
justed to obtain the best e   ectiveness) for each application. the process of tuning
is described further in section 7.7 and chapter 8.

to summarize, bm25 is an e   ective ranking algorithm derived from a model
of information retrieval viewed as classification. this model focuses on topical
relevance and makes an explicit assumption that relevance is binary. in the next
section, we discuss another probabilistic model that incorporates term frequency
directly in the model, rather than being added in as an extension to improve per-
formance.

7.3 ranking based on language models

language models are used to represent text in a variety of language technologies,
such as id103, machine translation, and handwriting recognition.
the simplest form of language model, known as a unigram language model, is a
id203 distribution over the words in the language. this means that the lan-
guage model associates a id203 of occurrence with every word in the in-

7.3 ranking based on language models

253

dex vocabulary for a collection. for example, if the documents in a collection
contained just five di   erent words, a possible language model for that collection
might be (0.2, 0.1, 0.35, 0.25, 0.1), where each number is the id203 of a
word occurring. if we treat each document as a sequence of words, then the proba-
bilities in the language model predict what the next word in the sequence will be.
for example, if the five words in our language were    girl   ,    cat   ,    the   ,    boy   , and
   touched   , then the probabilities predict which of these words will be next. these
words cover all the possibilities, so the probabilities must add to 1. because this
is a unigram model, the previous words have no impact on the prediction. with
this model, for example, it is just as likely to get the sequence    girl cat    (id203
0.2    0.1) as    girl touched    (id203 0.2    0.1).

in applications such as id103, id165 language models that pre-
dict words based on longer sequences are used. an id165 model predicts a word
based on the previous n     1 words. the most common id165 models are bi-
gram (predicting based on the previous word) and trigram (predicting based on
the previous two words) models. although bigram models have been used in in-
formation retrieval to represent two-word phrases (see section 4.3.5), we focus
our discussion on unigram models because they are simpler and have proven to
be very e   ective as the basis for ranking algorithms.

for search applications, we use language models to represent the topical con-
tent of a document. a topic is something that is talked about often but rarely de-
fined in information retrieval discussions. in this approach, we define a topic as a
id203 distribution over words (in other words, a language model). for exam-
ple, if a document is about fishing in alaska, we would expect to see words associ-
ated with fishing and locations in alaska with high probabilities in the language
model. if it is about fishing in florida, some of the high-id203 words will be
the same, but there will be more high id203 words associated with locations
in florida. if instead the document is about fishing games for computers, most of
the high-id203 words will be associated with game manufacturers and com-
puter use, although there will still be some important words about fishing. note
that a topic language model, or topic model for short, contains probabilities for all
words, not just the most important. most of the words will have    default    proba-
bilities that will be the same for any text, but the words that are important for the
topic will have unusually high probabilities.

a language model representation of a document can be used to    generate    new
text by sampling words according to the id203 distribution. if we imagine
the language model as a big bucket of words, where the probabilities determine

254

7 retrieval models

how many instances of a word are in the bucket, then we can generate text by
reaching in (without looking), drawing out a word, writing it down, putting the
word back in the bucket, and drawing again. note that we are not saying that we
can generate the original document by this process. in fact, because we are only
using a unigram model, the generated text is going to look pretty bad, with no
syntactic structure. important words for the topic of the document will, however,
appear often. intuitively, we are using the language model as a very approximate
model for the topic the author of the document was thinking about when he was
writing it.

when text is modeled as a finite sequence of words, where at each point in
the sequence there are t di   erent possible words, this corresponds to assuming
a multinomial distribution over words. although there are alternatives, multino-
mial language models are the most common in information retrieval.8 one of the
limitations of multinomial models that has been pointed out is that they do not
describe text burstiness well, which is the observation that once a word is    pulled
out of the bucket,    it tends to be pulled out repeatedly.

in addition to representing documents as language models, we can also repre-
sent the topic of the query as a language model. in this case, the intuition is that
the language model is a representation of the topic that the information seeker
had in mind when she was writing the query. this leads to three obvious possi-
bilities for retrieval models based on language models: one based on the proba-
bility of generating the query text from a document language model, one based
on generating the document text from a query language model, and one based on
comparing the language models representing the query and document topics. in
the next two sections, we describe these retrieval models in more detail.

7.3.1 query likelihood ranking

in the query likelihood retrieval model, we rank documents by the id203
that the query text could be generated by the document language model. in other
words, we calculate the id203 that we could pull the query words out of the
   bucket    of words representing the document. this is a model of topical relevance,
in the sense that the id203 of query generation is the measure of how likely
it is that a document is about the same topic as the query.
since we start with a query, we would in general like to calculate p (d|q) to

rank the documents. using bayes    rule, we can calculate this by

8 we discuss the multinomial model in the context of classification in chapter 9.

7.3 ranking based on language models

255

p(d|q) rank= p (q|d)p (d)

where the symbol rank= , as we mentioned previously, means that the right-hand
side is rank equivalent to the left-hand side (i.e., we can ignore the normalizing
constant p (q)), p (d) is the prior id203 of a document, and p (q|d) is
the query likelihood given the document. in most cases, p (d) is assumed to be
uniform (the same for all documents), and so will not a   ect the ranking. mod-
els that assign non-uniform prior probabilities based on, for example, document
date or document length can be useful in some applications, but we will make
the simpler uniform assumption here. given that assumption, the retrieval model
specifies ranking documents by p (q|d), which we calculate using the unigram
language model for the document

n   

i=1

p (q|d) =

p (qi|d)

to calculate this score, we need to have estimates for the language model prob-

where qi is a query word, and there are n words in the query.
abilities p (qi|d). the obvious estimate would be
fqi;d|d|

p (qi|d) =

where fqi;d is the number of times word qi occurs in document d, and |d| is
the number of words in d. for a multinomial distribution, this is the maximum
likelihood estimate, which means this this is the estimate that makes the observed
value of fqi;d most likely. the major problem with this estimate is that if any of
the query words are missing from the document, the score given by the query like-
lihood model for p (q|d) will be zero. this is clearly not appropriate for longer
queries. for example, missing one word out of six should not produce a score of
zero. we will also not be able to distinguish between documents that have di   er-
ent numbers of query words missing. additionally, because we are building a topic
model for a document, words associated with that topic should have some prob-
ability of occurring, even if they were not mentioned in the document. for ex-
ample, a language model representing a document about computer games should
have some non-zero id203 for the word    rpg    even if that word was not
mentioned in the document. a small id203 for that word will enable the
document to receive a non-zero score for the query    rpg computer games   , al-
though it will be lower than the score for a document that contains all three words.

256

7 retrieval models

smoothing is a technique for avoiding this estimation problem and overcom-
ing data sparsity, which means that we typically do not have large amounts of
text to use for the language model id203 estimates. the general approach
to smoothing is to lower (or discount) the id203 estimates for words that
are seen in the document text, and assign that    leftover    id203 to the esti-
mates for the words that are not seen in the text. the estimates for unseen words
are usually based on the frequency of occurrence of words in the whole document
collection. if p (qi|c) is the id203 for query word i in the collection language
model for document collection c, then the estimate we use for an unseen word in
a document is   dp (qi|c), where   d is a coe   cient controlling the id203
assigned to unseen words.9 in general,   d can depend on the document. in order
that the probabilities sum to one, the id203 estimate for a word that is seen
in a document is (1       d)p (qi|d) +   dp (qi|c).

to make this clear, consider a simple example where there are only three words,
w1, w2, and w3, in our index vocabulary. if the collection probabilities for these
three words, based on maximum likelihood estimates, are 0.3, 0.5, and 0.2, and the
document probabilities based on maximum likelihood estimates are 0.5, 0.5, and
0.0, then the smoothed id203 estimates for the document language model
are:

p (w1|d) = (1       d)p (w1|d) +   dp (w1|c)
= (1       d)    0.5 +   d    0.3
p (w2|d) = (1       d)    0.5 +   d    0.5
p (w3|d) = (1       d)    0.0 +   d    0.2 =   d    0.2

note that term w3 has a non-zero id203 estimate, even though it did not
occur in the document text. if we add these three probabilities, we get
p (w1|d) + p (w2|d) + p (w3|d) = (1       d)    (0.5 + 0.5)
+  d    (0.3 + 0.5 + 0.2)

= 1       d +   d

= 1

which confirms that the probabilities are consistent.

9 the collection language model id203 is also known as the background language
model id203, or just the background id203.

7.3 ranking based on language models

257

di   erent forms of estimation result from specifying the value of   d. the sim-
plest choice would be to set it to a constant, i.e.,   d =   . the collection language
model id203 estimate we use for word qi is cqi/|c|, where cqi is the num-
ber of times a query word occurs in the collection of documents, and |c| is the
total number of word occurrences in the collection. this gives us an estimate for
p (qi|d) of:

p(qi|d) = (1       )

fqi;d|d| +   

cqi
|c|

this form of smoothing is known as the jelinek-mercer method. substituting this
estimate in the document score for the query-likelihood model gives:

n   
((1       )

p (q|d) =

fqi;d|d| +   

cqi
|c|)

as we have said before, since multiplying many small numbers together can lead to
accuracy problems, we can use logarithms to turn this score into a rank-equivalent
sum as follows:

log p (q|d) =

log((1       )

fqi;d|d| +   

cqi
|c| )

i=1

n   

i=1

small values of    produce less smoothing, and consequently the query tends
to act more like a boolean and since the absence of any query word will penalize
the score substantially. in addition, the relative weighting of words, as measured
by the maximum likelihood estimates, will be important in determining the score.
as    approaches 1, the relative weighting will be less important, and the query acts
more like a boolean or or a coordination level match.10 in trec evaluations, it
has been shown that values of    around 0.1 work well for short queries, whereas
values around 0.7 are better for much longer queries. short queries tend to contain
only significant words, and a low    value will favor documents that contain all the
query words. with much longer queries, missing a word is much less important,
and a high    places more emphasis on documents that contain a number of the
high-id203 words.

at this point, it may occur to you that the query likelihood retrieval model
doesn   t have anything that looks like a tf.idf weight, and yet experiments show

10 a coordination level match simply ranks documents by the number of matching query

terms.

258

7 retrieval models

that it is as least as e   ective as the bm25 ranking algorithm. we can, however,
demonstrate a relationship to tf.idf weights by manipulating the query likelihood
score in the following way:
log p (q|d) =

   

fqi;d|d| +   

cqi
|c| )
cqi
fqi;d|d| +   
|c|) +
n   

log(  

cqi
|c| )

i:fqi;d=0

log(  

cqi
|c| )

+

i=1

=

log((1       )

i=1

n   
log((1       )
   
   
   

i:fqi;d>0

=

rank=

i:fqi;d>0

i:fqi;d>0

log

   cqi|c|

log ((1       ) fqi;d|d| +    cqi|c|)
      
      ((1       ) fqi;d|d|
   

   cqi|c|

+ 1

log(  

cqi
|c|)

i:fqi;d>0

in the second line, we split the score into the words that occur in the document
and those that don   t occur (fqi;d = 0). in the third line, we add

to the last term and subtract it from the first (where it ends up in the denomina-
tor), so there is no net e   ect. the last term is now the same for all documents and
can be ignored for ranking. the final expression gives the document score in terms
of a    weight    for matching query terms. although this weight is not identical to
a tf.idf weight, there are clear similarities in that it is directly proportional to the
document term frequency and inversely proportional to the collection frequency.
a di   erent form of estimation, and one that is generally more e   ective, comes
from using a value of   d that is dependent on document length. this approach is
known as dirichlet smoothing, for reasons we will discuss later, and uses

  d =

  |d| +   

where    is a parameter whose value is set empirically. substituting this expression
for   d in (1       d)p (qi|d) +   dp (qi|c) results in the id203 estimation
formula

7.3 ranking based on language models

259

which in turn leads to the following document score:

fqi;d +    cqi|c|
|d| +   

p(qi|d) =
n   

log p (q|d) =

log fqi;d +    cqi|c|
|d| +   

i=1

similar to the jelinek-mercer smoothing, small values of the parameter (   in
this case) give more importance to the relative weighting of words, and large val-
ues favor the number of matching terms. typical values of    that achieve the best
results in trec experiments are in the range 1,000 to 2,000 (remember that col-
lection probabilities are very small), and dirichlet smoothing is generally more
e   ective than jelinek-mercer, especially for the short queries that are common in
most search applications.

so where does dirichlet smoothing come from? it turns out that a dirichlet
distribution11 is the natural way to specify prior knowledge when estimating the
probabilities in a multinomial distribution. the process of bayesian estimation
determines id203 estimates based on this prior knowledge and the observed
text. the resulting id203 estimate can be viewed as combining actual word
counts from the text with pseudo-counts from the dirichlet distribution. if we had
no text, the id203 estimate for term qi would be   (cqi/|c|)/  , which is a
reasonable guess based on the collection. the more text we have (i.e., for longer
documents), the less influence the prior knowledge will have.

we can demonstrate the calculation of query likelihood document scores us-
ing the example given in section 7.2.2. the two query terms are    president    and
   lincoln   . for the term    president   , fqi;d = 15, and let   s assume that cqi = 160,000.
for the term    lincoln   , fqi;d = 25, and we will assume that cqi = 2,400. the num-
ber of word occurrences in the document|d| is assumed to be 1,800, and the num-
ber of word occurrences in the collection is 109 (500,000 documents times an
average of 2,000 words). the value of    used is 2,000. given these numbers, the
score for the document is:
11 named after the german mathematician johann peter gustav lejeune dirichlet (the

first name used seems to vary).

260

7 retrieval models

ql(q, d) = log 15 + 2000    (1.6    105/109)
+ log 25 + 2000    (2400/109)
1800 + 2000
= log(15.32/3800) + log(25.005/3800)
=    5.51 +    5.02 =    10.53

1800 + 2000

a negative number? remember that we are taking logarithms of probabilities in
this scoring function, and the probabilities of word occurrence are small. the im-
portant issue is the e   ectiveness of the rankings produced using these scores. ta-
ble 7.3 shows the query likelihood scores for the same variations of term occur-
rences that were used in table 7.2. although the scores look very di   erent for
bm25 and ql, the rankings are similar, with the exception that the document
containing 15 occurrences of    president    and 1 of    lincoln    is ranked higher than
the document containing 0 occurrences of    president    and 25 occurrences of    lin-
coln    in the ql scores, whereas the reverse is true for bm25.

frequency of frequency of
   president   

   lincoln   

15
15
15
1
0

25
1
0
25
25

ql
score
   10.53
   13.75
   19.05
   12.99
   14.40

table 7.3. query likelihood scores for an example document

to summarize, query likelihood is a simple probabilistic retrieval model that
directly incorporates term frequency. the problem of coming up with e   ective
term weights is replaced by id203 estimation, which is better understood
and has a formal basis. the basic query likelihood score with dirichlet smooth-
ing has similar e   ectiveness to bm25, although it does do better on most trec
collections. if more sophisticated smoothing based on topic models is used (de-
scribed further in section 7.6), query likelihood consistently outperforms bm25.
this means that instead of smoothing using the collection probabilities for words,
we instead use word probabilities from similar documents.

the simplicity of the language model framework, combined with the ability to
describe a variety of retrieval applications and the e   ectiveness of the associated

7.3 ranking based on language models

261

ranking algorithms, make this approach a good choice for a retrieval model based
on topical relevance.

7.3.2 relevance models and pseudo-relevance feedback
although the basic query likelihood model has a number of advantages, it is lim-
ited in terms of how it models information needs and queries. it is di   cult, for
example, to incorporate information about relevant documents into the ranking
algorithm, or to represent the fact that a query is just one of many possible queries
that could be used to describe a particular information need. in this section, we
show how this can be done by extending the basic model.

in the introduction to section 7.3, we mentioned that it is possible to represent
the topic of a query as a language model. instead of calling this the query language
model, we use the name relevance model since it represents the topic covered by
relevant documents. the query can be viewed as a very small sample of text gener-
ated from the relevance model, and relevant documents are much larger samples
of text from the same model. given some examples of relevant documents for a
query, we could estimate the probabilities in the relevance model and then use
this model to predict the relevance of new documents. in fact, this is a version of
the classification model presented in section 7.2.1, where we interpret p (d|r) as
the id203 of generating the text in a document given a relevance model. this
is also called the document likelihood model. although this model, unlike the bi-
nary independence model, directly incorporates term frequency, it turns out that
p (d|r) is di   cult to calculate and compare across documents. this is because
documents contain a large and extremely variable number of words compared
to a query. consider two documents da and db, for example, containing 5 and
500 words respectively. because of the large di   erence in the number of words
involved, the comparison of p (da|r) and p (db|r) for ranking will be more
di   cult than comparing p (q|da) and p (q|db), which use the same query and
smoothed representations for the documents. in addition, we still have the prob-
lem of obtaining examples of relevant documents.

there is, however, another alternative. if we can estimate a relevance model
from a query, we can compare this language model directly with the model for a
document. documents would then be ranked by the similarity of the document
model to the relevance model. a document with a model that is very similar to
the relevance model is likely to be on the same topic. the obvious next question
is how to compare two language models. a well-known measure from probabil-
ity theory and id205, the id181 (referred to as

262

7 retrieval models

kl-divergence in this book),12 measures the di   erence between two id203
distributions. given the true id203 distribution p and another distribution
q that is an approximation to p , the kl divergence is defined as:

   

x

kl(p||q) =

p (x) log p (x)
q(x)

since kl-divergence is always positive and is larger for distributions that are fur-
ther apart, we use the negative kl-divergence as the basis for the ranking function
(i.e., smaller di   erences mean higher scores). in addition, kl-divergence is not
symmetric, and it matters which distribution we pick as the true distribution. if
we assume the true distribution to be the relevance model for the query (r) and
the approximation to be the document language model (d), then the negative
kl-divergence can be expressed as

p (w|r) log p (w|d)    

p (w|r) log p (w|r)

   

w   v

   

w   v

where the summation is over all words w in the vocabulary v . the second term on
the right-hand side of this equation does not depend on the document, and can be
ignored for ranking. given a simple maximum likelihood estimate for p (w|r),
based on the frequency in the query text (fw;q) and the number of words in the
query (|q|), the score for a document will be:

   

w   v

fw;q|q| log p (w|d)

although this summation is over all words in the vocabulary, words that do not
occur in the query have a zero maximum likelihood estimate and will not con-
tribute to the score. also, query words with frequency k will contribute k   
log p (w|d) to the score. this means that this score is rank equivalent to the
query likelihood score described in the previous section. in other words, query
likelihood is a special case of a retrieval model that ranks by comparing a rele-
vance model based on a query to a document language model.

the advantage of the more general model is that it is not restricted to the sim-
ple method of estimating the relevance model using query term frequencies. if we

12 kl-divergence is also called information divergence, information gain, or relative en-

tropy.

7.3 ranking based on language models

263

regard the query words as a sample from the relevance model, then it seems rea-
sonable to base the id203 of a new sample word on the query words we have
seen. in other words, the id203 of pulling a word w out of the    bucket    rep-
resenting the relevance model should depend on the n query words we have just
pulled out. more formally, we can relate the id203 of w to the conditional
id203 of observing w given that we just observed the query words q1 . . . qn
by the approximation:

p (w|r)     p (w|q1 . . . qn)

by definition, we can express the id155 in terms of the joint
id203 of observing w with the query words:

p (w|r)     p (w, q1 . . . qn)
p (q1 . . . qn)

p (q1 . . . qn) is a normalizing constant and is calculated as:

p (q1 . . . qn) =

w   v

p (w, q1 . . . qn)

now the question is how to estimate the joint id203 p (w, q1 . . . qn). given
a set of documents c represented by language models, we can calculate the joint
id203 as follows:

p (w, q1 . . . qn) =

d   c
we can also make the assumption that:

p(d)p (w, q1 . . . qn|d)

n   

p (qi|d)

p (w, q1 . . . qn|d) = p (w|d)

i=1

   

when we substitute this expression for p (w, q1 . . . qn|d) into the previous equa-
tion, we get the following estimate for the joint id203:

n   
p (qi|d)
   
how do we interpret this formula? the prior id203 p (d) is usually as-
i=1 p (qi|d) is, in fact,
sumed to be uniform and can be ignored. the expression

p (d)p (w|d)

p (w, q1 . . . qn) =

d   c

i=1

n

   

   

264

7 retrieval models

the query likelihood score for the document d. this means that the estimate for
p (w, q1 . . . qn) is simply a weighted average of the language model probabilities
for w in a set of documents, where the weights are the query likelihood scores for
those documents.

ranking based on relevance models actually requires two passes. the first pass
ranks documents using query likelihood to obtain the weights that are needed
for relevance model estimation. in the second pass, we use kl-divergence to rank
documents by comparing the relevance model and the document model. note
also that we are in e   ect adding words to the query by smoothing the relevance
model using documents that are similar to the query. many words that had zero
probabilities in the relevance model based on query frequency estimates will now
have non-zero values. what we are describing here is exactly the pseudo-relevance
feedback process described in section 6.2.4. in other words, relevance models pro-
vide a formal retrieval model for pseudo-relevance feedback and id183.
the following is a summary of the steps involved in ranking using relevance mod-
els:
1. rank documents using the query likelihood score for query q.
2. select some number of the top-ranked documents to be the set c.
3. calculate the relevance model probabilities p (w|r) using the estimate for

p (w, q1 . . . qn).

4. rank documents again using the kl-divergence score:13

   

w

p (w|r) log p (w|d)

some of these steps require further explanation. in steps 1 and 4, the docu-
ment language model probabilities (p (w|d)) should be estimated using dirich-
let smoothing. in step 2, the model allows the setc to be the whole collection, but
because low-ranked documents have little e   ect on the estimation of p (w|r),
usually only 10   50 of the top-ranked documents are used. this also makes the
computation of p (w|r) substantially faster.

for similar reasons, the summation in step 4 is not done over all words in
the vocabulary. typically only a small number (10   25) of the highest-id203
words are used. in addition, the importance of the original query words is em-
phasized by combining the original query frequency estimates with the relevance

13 more accurately, this score is the negative cross id178 because we removed the term

w   v p (w|r) log p (w|r).

   

7.3 ranking based on language models

265

model estimates using a similar approach to jelinek-mercer, i.e.,   p (w|q)+(1   
  )p (w|r), where    is a mixture parameter whose value is determined empiri-
cally (0.5 is a typical value for trec experiments). this combination makes it
clear that estimating relevance models is basically a process for id183
and smoothing.

the next important question, as for all retrieval models, is how well it works.
based on trec experiments, ranking using relevance models is one of the best
pseudo-relevance feedback techniques. in addition, relevance models produce a
significant improvement in e   ectiveness compared to query likelihood ranking
averaged over a number of queries. like all current pseudo-relevance feedback
techniques, however, the improvements are not consistent, and some queries can
produce worse rankings or strange results.

tables 7.4 and 7.5 show the 16 highest-id203 words from relevance mod-
els estimated using this technique with some example queries and a large collec-
tion of trec news stories from the 1990s.14 table 7.4 uses the top 10 documents
from the query likelihood ranking to construct the relevance model, whereas ta-
ble 7.5 uses the top 50 documents.

the first thing to notice is that, although the words are reasonable, they are
very dependent on the collection of documents that is used. in the trec news
collection, for example, many of the stories that mention abraham lincoln are on
the topic of the lincoln bedroom in the white house, which president clinton
used for guests and president lincoln used as an o   ce during the civil war. these
types of stories are reflected in the top id203 words for the queries    president
lincoln    and    abraham lincoln   . expanding the query using these words would
clearly favor the retrieval of this type of story rather than more general biographies
of lincoln. the second observation is that there is not much di   erence between
the words based on 10 documents and the words based on 50 documents. the
words based on 50 documents are, however, somewhat more general because the
larger set of documents contains a greater variety of topics. in the case of the query
   tropical fish   , the relevance model words based on 10 documents are clearly more
related to the topic.

in summary, ranking by comparing a model of the query to a model of the
document using kl-divergence is a generalization of query likelihood scoring.

14 this is a considerably larger collection than was used to generate the term association
tables in chapter 6. those tables were based on the robust track data, which con-
sists of just over half a million documents. these tables were generated using all the
trec news collections, which total more than six million documents.

266

7 retrieval models

president lincoln

abraham lincoln

lincoln
president

room

bedroom

house
white
america
guest
serve
bed

old
o   ce
war
long

abraham

washington

lincoln
america
president

faith
guest

abraham

new
room
christian
history
public
bedroom

war

politics

old

national

fishing
fish
farm
salmon
new
wild
water
caught
catch
tag
time
eat
raise
city
people
fishermen

boat

tropical fish

fish
tropic
japan

aquarium

water
species
aquatic

fair
china
coral
source
tank
reef
animal
tarpon
fishery

table 7.4. highest-id203 terms from relevance model for four example queries (es-
timated using top 10 documents)

this generalization allows for more accurate queries that reflect the relative im-
portance of the words in the topic that the information seeker had in mind when
he was writing the query. relevance model estimation is an e   ective pseudo-
relevance feedback technique based on the formal framework of language mod-
els, but as with all these techniques, caution must be used in applying relevance
model   based id183 to a specific retrieval application.

language models provide a formal but straightforward method of describing
retrieval models based on topical relevance. even more sophisticated models can
be developed by incorporating term dependence and phrases, for example. top-
ical relevance is, however, only part of what is needed for e   ective search. in the
next section, we focus on a retrieval model for combining all the pieces of evidence
that contribute to user relevance, which is what people who use a search engine
really care about.

7.4 complex queries and combining evidence

267

president lincoln

abraham lincoln

lincoln
president
america

new

national
great
white
war

clinton
house
history
time
center
kennedy
room

washington

lincoln
president
america
abraham

war
man
civil
new
history
two
room
booth
time
politics
public
guest

fishermen

fishing
fish
water
catch
reef

river
new
year
time
bass
boat
world
farm
angle
fly
trout

tropical fish

fish
tropic
water
storm
species
boat
sea
river
country

tuna
world
million
state
time
japan
mile

table 7.5. highest-id203 terms from relevance model for four example queries (es-
timated using top 50 documents)

7.4 complex queries and combining evidence

e   ective retrieval requires the combination of many pieces of evidence about
a document   s potential relevance. in the case of the retrieval models described
in previous sections, the evidence consists of word occurrences that reflect top-
ical content. in general, however, there can be many other types of evidence that
should be considered. even considering words, we may want to take into account
whether certain words occur near each other, whether words occur in particular
document structures, such as section headings or titles, or whether words are re-
lated to each other. in addition, evidence such as the date of publication, the doc-
ument type, or, in the case of web search, the id95 number will also be im-
portant. although a retrieval algorithm such as query likelihood or bm25 could
be extended to include some of these types of evidence, it is di   cult not to resort
to heuristic    fixes    that make the retrieval algorithm di   cult to tune and adapt to
new retrieval applications. instead, what we really need is a framework where we
can describe the di   erent types of evidence, their relative importance, and how
they should be combined. the id136 network retrieval model, which has been

268

7 retrieval models

used in both commercial and open source search engines (and is incorporated in
galago), is one approach to doing this.

the id136 network model is based on the formalism of id110s
and is a probabilistic model. the model provides a mechanism for defining and
evaluating operators in a query language. some of these operators are used to spec-
ify types of evidence, and others describe how it should be combined. the version
of the id136 network we will describe uses language models to estimate the
probabilities that are needed to evaluate the queries.

in this section, we first give an overview of the id136 network model, and
then show how that model is used as the basis of a powerful query language for
search applications. in the next section, we describe web search and explain how
the id136 network model would be used to combine the many sources of evi-
dence required for e   ective ranking.

queries described using the id136 network query language appear to be
much more complicated than a simple text query with two or three words. most
users will not understand this language, just as most relational database users do
not understand structured query language (sql). instead, applications trans-
late simple user queries into more complex id136 network versions. the more
complex query incorporates additional features and weights that reflect the best
combination of evidence for e   ective ranking. this point will become clearer as
we discuss examples in the next two sections.

7.4.1 the id136 network model

a id110 is a probabilistic model that is used to specify a set of events
and the dependencies between them. the networks are directed, acyclic graphs
(dags), where the nodes in the graph represent events with a set of possible
outcomes and arcs represent probabilistic dependencies between the events. the
id203, or belief,15 of a particular event outcome can be determined given
the probabilities of the parent events (or a prior id203 in the case of a root
node). when used as a retrieval model, the nodes represent events such as observ-
ing a particular document, or a particular piece of evidence, or some combination
of pieces of evidence. these events are all binary, meaning that true and false
are the only possible outcomes.

15 belief network is the name for a range of techniques used to model uncertainty. a

id110 is a probabilistic belief network.

7.4 complex queries and combining evidence

269

fig. 7.4. example id136 network model

figure 7.4 shows an id136 net where the evidence being combined are
words in a web page   s title, body, and <h1> headings. in this figure, d is a docu-
ment node. this node corresponds to the event that a document (the web page) is
observed. there is one document node for every document in the collection, and
we assume that only one document is observed at any time. the ri or represen-
tation nodes are document features (evidence), and the probabilities associated
with those features are based on language models    estimated using the parame-
ters   . there is one language model for each significant document structure (title,
body, or headings). in addition to features based on word occurrence, ri nodes
also represent proximity features. proximity features take a number of di   erent
forms, such as requiring words to co-occur within a certain    window    (length)
of text, and will be described in detail in the next section. features that are not
based on language models, such as document date, are allowed but not shown in
this example.

the query nodes qi are used to combine evidence from representation nodes
and other query nodes. these nodes represent the occurrence of more complex ev-
idence and document features. a number of forms of combination are available,
with boolean and and or being two of the simplest. the network as a whole
computes p (i|d,   ), which is the id203 that an information need is met

id  title  body  h1r1rn   r1rn   r1rn   q1q2  title  body  h1270

7 retrieval models

given the document and the parameters   . the information need node i is a spe-
cial query node that combines all of the evidence from the other query nodes into
a single id203 or belief score. this score is used to rank documents. con-
ceptually, this means we must evaluate an id136 network for every document
in the collection, but as with every other ranking algorithm, indexes are used to
speed up the computation. in general, representation nodes are indexed, whereas
query nodes are specified for each query by the user or search application. this
means that indexes for a variety of proximity features, in addition to words, will be
created (as described in chapter 5), significantly expanding the size of the indexes.
in some applications, the probabilities associated with proximity features are com-
puted at query time in order to provide more flexibility in specifying queries.

the connections in the id136 network graph are defined by the query and
the representation nodes connected to every document in the collection. the
probabilities for the representation nodes are estimated using language models
for each document. note that these nodes do not represent the occurrence of a
particular feature in a document, but instead capture the id203 that the fea-
ture is characteristic of the document, in the sense that the language model could
generate it. for example, a node for the word    lincoln    represents the binary event
that a document is about that topic (or not), and the language model for the doc-
ument is used to calculate the id203 of that event being true.

since all the events in the id136 network are binary, we cannot really use
a multinomial model of a document as a sequence of words. instead, we use a
multiple-bernoulli16 model, which is the basis for the binary independence model
in section 7.2.1. in that case, a document is represented as a binary feature vec-
tor, which simply records whether a feature is present or not. in order to capture
term frequency information, a di   erent multiple-bernoulli model is used where
the document is represented by a multiset17 of vectors, with one vector for each
term occurrence (metzler, lavrenko, & croft, 2004). it turns out that with the
appropriate choice of parameters, the id203 estimate based on the multiple-
bernoulli distribution is the same as the estimate for the multinomial distribution
with dirichlet smoothing, which is

16 named after the swiss mathematician jakob bernoulli (also known as james or jacques,
and one of eight famous mathematicians in the same family). the multiple-bernoulli
model is discussed further in chapter 9.

17 a multiset (also called a bag) is a set where each member has an associated number

recording the number of times it occurs.

7.4 complex queries and combining evidence

271

p (ri|d,   ) =

fri;d +   p (ri|c)

|d| +   

where fi;d is the number of times feature ri occurs in document d, p (ri|c) is
the collection id203 for feature ri, and    is the dirichlet smoothing param-
eter. to be more precise, for the model shown in figure 7.4 we would use fi;d
counts, collection probabilities, and a value for    that are specific to the docu-
ment structure of interest. for example, if fi;d was the number of times feature ri
occurs in a document title, the collection probabilities would be estimated from
the collection of all title texts, and the    parameter would be specific to titles. also
note that the same estimation formula is used for proximity-based features as for
words. for example, for a feature such as    new york    where the words must occur
next to each other, fi;d is the number of times    new york    occurs in the text.

the query nodes, which specify how to combine evidence, are the basis of the
operators in the query language. although id110s permit arbitrary
combinations (constrained by the laws of id203), the id136 network re-
trieval model is based on operators that can be computed e   ciently. at each node
in the network, we need to specify the id203 of each outcome given all pos-
sible states of the parent nodes. when the number of parent nodes is large, this
could clearly get expensive. fortunately, many of the interesting combinations can
be expressed as simple formulas.

as an example of the combination process and how it can be done e   ciently,
consider boolean and. given a simple network for a query node q with two par-
ent nodes a and b, as shown in figure 7.5, we can describe the conditional prob-
abilities as shown in table 7.6.

fig. 7.5. id136 network with three nodes

we can refer to the values in the first column of table 7.6 using pij, where i
and j refer to the states of the parents. for example, p10 refers to the id203

abq272

7 retrieval models

p (q = true|a, b)

0
0
0
1

a

b

false false
false true
true false
true true

table 7.6. conditional probabilities for example network

that q is true given that a is true and b is false. to compute the id203 of
q, we use this table and the probabilities of the parent nodes (which come from
the representation nodes) as follows:

beland(q) = p00p (a = false)p (b = false)

+p01p (a = false)p (b = true)
+p10p (a = true)p (b = false)
+p11p (a = true)p (b = true)

= 0    (1     pa)(1     pb) + 0    (1     pa)pb + 0    pa(1     pb) + 1    papb
= papb

where pa is the id203 that a is true, and pb is the id203 that b is true.
we use the name beland(q) to indicate that this is the belief value (id203)
that results from an and combination.

this means that the and combination of evidence is computed by simply mul-
tiplying the probabilities of the parent nodes. if one of the parent probabilities
is low (or zero if smoothing is not used), then the combination will have a low
id203. this seems reasonable for this type of combination. we can define a
number of other combination operators in the same way. if a q node has n par-
ents with id203 of being true pi, then the following list defines the common
operators:

belor(q) = 1     n   
belnot(q) = 1     p1
n   

i

beland(q) =

pi

i

(1     pi)

7.4 complex queries and combining evidence

273

pwti
i

n   
   
   
i wtipi   

n
i pi
n
n

n
i wti

belwand(q) =
belmax(q) = max{p1, p2, . . . , pn}
belsum(q) =

i

belwsum(q) =

where wti is a weight associated with the ith parent, which indicates the relative
importance of that evidence. note that not is a unary operator (i.e., has only one
parent).

the weighted and operator is very important and one of the most commonly
used in the query language described in the next section. using this form of com-
bination and restricting the evidence (representation nodes) to individual words
gives the same ranking as query likelihood.

given this description of the underlying model and combination operators, we
can now define a query language that can be used in a search engine to produce
rankings based on complex combinations of evidence.

7.4.2 the galago query language

the galago query language presented here is similar to query languages used
in open source search engines that are based on the id136 network retrieval
model.18 this version focuses on the most useful aspects of those languages for a
variety of search applications, and adds the ability to use arbitrary features. note
that the galago search engine is not based on a specific retrieval model, but in-
stead provides an e   cient framework for implementing retrieval models.

although the query language can easily handle simple unstructured text docu-
ments, many of the more interesting features make use of evidence based on docu-
ment structure. we assume that structure is specified using tag pairs, as in html
or xml. consider the following document:

<html>
<head>
<title>department descriptions</title>
</head>

18 such as inquery and indri.

274

7 retrieval models

<body>
the following list describes ...
<h1>agriculture</h1> ...
<h1>chemistry</h1> ...
<h1>computer science</h1> ...
<h1>electrical engineering</h1> ...
</body>
</html>

in the galago query language, a document is viewed as a sequence of text that may
contain arbitrary tags. in the example just shown, the document consists of text
marked up with html tags.

for each tag type t within a document (e.g., title, body, h1, etc.), we define
the context19 of t to be all of the text and tags that appear within tags of type t.
in the example, all of the text and tags appearing between <body> and </body>
tags define the body context. a single context is generated for each unique tag
name. therefore, a context defines a subdocument. note that because of nested
tags, certain word occurrences may appear in many contexts. it is also the case that
there may be nested contexts. for example, within the <body> context there is a
nested <h1> context made up of all of the text and tags that appear within the
body context and within <h1> and </h1> tags. here are the tags for the title, h1,
and body contexts in this example document:

title context:
<title>department descriptions</title>
h1 context:
<h1>agriculture</h1>
<h1>chemistry</h1> ...
<h1>computer science</h1> ...
<h1>electrical engineering</h1> ...
body context:
<body> the following list describes ...
<h1>agriculture</h1> ...
<h1>chemistry</h1> ...

19 contexts are sometimes referred to as fields.

7.4 complex queries and combining evidence

275

<h1>computer science</h1> ...
<h1>electrical engineering</h1> ...
</body>
each context is made up of one or more extents. an extent is a sequence of
text that appears within a single begin/end tag pair of the same type as the con-
text. for this example, in the <h1> context, there are extents <h1>agriculture</h1>,
<h1>chemistry<h1>, etc. both the title and body contexts contain only a single ex-
tent because there is only a single pair of <title> and <body> tags, respectively. the
number of extents for a given tag type is determined by the number of tag pairs
of that type that occur within the document.

in addition to the structure defined when a document is created, contexts are
also used to represent structure added by feature extraction tools. for example,
dates, people   s names, and addresses can be identified in text and tagged by a fea-
ture extraction tool. as long as this information is represented using tag pairs, it
can be referred to in the query language in the same way as other document struc-
tures.

terms are the basic building blocks of the query language, and correspond to
representation nodes in the id136 network model. a variety of types of terms
can be defined, such as simple terms, ordered and unordered phrases, synonyms,
and others. in addition, there are a number of options that can be used to specify
that a term should appear within a certain context, or that it should be scored
using a language model that is estimated using a given context.

simple terms:
term     term that will be normalized and stemmed.
   term        term is not normalized or stemmed.
examples:
presidents
   nasa   
proximity terms:
#od:n( ... )     ordered window     terms must appear ordered, with at most
n-1 terms between each.
#od( ... )     unlimited ordered window     all terms must appear ordered
anywhere within current context.
#uw:n( ... )     unordered window     all terms must appear within a window
of length n in any order.

276

7 retrieval models

#uw( ... )     unlimited unordered window     all terms must appear within
current context in any order.
examples:
#od:1(white house)     matches    white house    as an exact phrase.
#od:2(white house)     matches    white * house    (where * is any word or null).
#uw:2(white house)     matches    white house    and    house white   .
synonyms:
#syn( ... )
#wsyn( ... )
the first two expressions are equivalent. they each treat all of the terms
listed as synonyms. the #wsyn operator treats the terms as synonyms, and
allows weights to be assigned to each term. the arguments given to these
operators can only be simple terms or proximity terms.
examples:
#syn(dog canine)     simple synonym based on two terms.
#syn( #od:1(united states) #od:1(united states of america) )     creates a syn-
onym from two proximity terms.
#wsyn( 1.0 donald 0.8 don 0.5 donnie )     weighted synonym indicating
relative importance of terms.
anonymous terms:
#any:.()     used to match extent types
examples:
#any:person()     matches any occurrence of a person extent.
#od:1(lincoln died in #any:date())     matches exact phrases of the form:   lincoln
died in <date>   </date>   .
context restriction and evaluation:
expression.c1   ...,cn     matches when the expression appears in all con-
texts c1 through cn.
expression.(c1,...,cn)     evaluates the expression using the language model
defined by the concatenation of contexts c1...cn within the document.
examples:
dog.title     matches the term    dog    appearing in a title extent.
#uw(smith jones).author     matches when the two names    smith    and    jones   
appear in an author extent.
dog.(title)     evaluates the term based on the title language model for the

7.4 complex queries and combining evidence

277

document. this means that the estimate of the id203 of occurrence
for dog for a given document will be based on the number of times that the
word occurs in the title field for that document and will be normalized us-
ing the number of words in the title rather than the document. similarly,
smoothing is done using the probabilities of occurrence in the title field
over the whole collection.
#od:1(abraham lincoln).person.(header)     builds a language model from all
of the    header    text in the document and evaluates #od:1(abraham lin-
coln).person in that context (i.e., matches only the exact phrase appearing
within a person extent within the header context).

belief operators are used to combine evidence about terms, phrases, etc. there are
both unweighted and weighted belief operators. with the weighted operator, the
relative importance of the evidence can be specified. this allows control over how
much each expression within the query impacts the final score. the filter operator
is used to screen out documents that do not contain required evidence. all belief
operators can be nested.

belief operators:
#combine(...)     this operator is a normalized version of the beland(q) op-
erator in the id136 network model. see the discussion later for more
details.
#weight(...)     this is a normalized version of the belwand(q) operator.
#filter(...)     this operator is similar to #combine, but with the di   erence
that the document must contain at least one instance of all terms (simple,
proximity, synonym, etc.). the evaluation of nested belief operators is not
changed.
examples:
#combine( #syn(dog canine) training )     rank by two terms, one of which is
a synonym.
#combine( biography #syn(#od:1(president lincoln) #od:1(abraham lincoln)) )
    rank using two terms, one of which is a synonym of    president lincoln   
and    abraham lincoln   .
#weight( 1.0 #od:1(civil war) 3.0 lincoln 2.0 speech )     rank using three terms,
and weight the term    lincoln    as most important, followed by    speech   ,
then    civil war   .
#filter( aquarium #combine(tropical fish) )     consider only those documents
containing the word    aquarium    and    tropical    or    fish   , and rank them

278

7 retrieval models

according to the query #combine(aquarium #combine(tropical fish)).
#filter( #od:1(john smith).author) #weight( 2.0 europe 1.0 travel )     rank
documents about    europe    or    travel    that have    john smith    in the au-
thor context.
as we just described, the #combine and #weight operators are normalized ver-
sions of the beland and belwand operators, respectively. the beliefs of these oper-
ators are computed as follows:

n   
n   

i

belcombine =

belweight =

i

   

p1/n
i

pwti/
i

n

i    wti   

this id172 is done in order to make the operators behave more like the
original belsum and belwsum operators, which are both normalized. one advan-
tage of the id172 is that it allows us to describe the belief computa-
tion of these operators in terms of various types of means (averages). for exam-
ple, belsum computes the arithmetic mean over the beliefs of the parent nodes,
whereas belwsum computes a weighted arithmetic mean. similarly, belcombine and
belwand compute a geometric mean and weighted geometric mean, respectively.
the filter operator also could be used with numeric and date field operators so
that non-textual evidence can be combined into the score. for example, the query

#filter(news.doctype #dateafter(12/31/1999).docdate
#uw:20( brown.person #any:company() #syn( money cash payment ) )

ranks documents that are news stories, that appeared after 1999, and that con-
tained at least one text segment of length 20 that mentioned a person named
   brown   , a company name, and at least one of the three words dealing with money.
the id136 network model can easily deal with the combination of this type of
evidence, but for simplicity, we have not implemented these operators in galago.
another part of the id136 network model that we do support in the galago
query language is document priors. document priors allow the specification of
a prior id203 over the documents in a collection. these prior probabilities
influence the rankings by preferring documents with certain characteristics, such
as those that were written recently or are short.

7.5 web search

279

prior:
#prior:name()     uses the document prior specified by the name given. pri-
ors are files or functions that provide prior probabilities for each docu-
ment.
example:
#combine(#prior:recent() global warming)     uses a prior named recent to
give greater weight to documents that were published more recently.
as a more detailed example of the use of this query language, in the next sec-
tion we discuss web search and the types of evidence that have to be combined
for e   ective ranking. the use of the #feature operator to define arbitrary features
(new evidence) is discussed in chapter 11.

7.5 web search

measured in terms of popularity, web search is clearly the most important search
application. millions of people use web search engines every day to carry out an
enormous variety of tasks, from shopping to research. given its importance, web
search is the obvious example to use for explaining how the retrieval models we
have discussed are applied in practice.

there are some major di   erences between web search and an application that
provides search for a collection of news stories, for example. the primary ones are
the size of the collection (billions of documents), the connections between doc-
uments (i.e., links), the range of document types, the volume of queries (tens of
millions per day), and the types of queries. some of these issues we have discussed
in previous chapters, and others, such as the impact of spam, will be discussed
later. in this section, we will focus on the features of the queries and documents
that are most important for the ranking algorithm.

there are a number of di   erent types of search in a web environment. one
popular way of describing searches was suggested by broder (2002). in this tax-
onomy, searches are either informational, navigational, or transactional. an in-
formational search has the goal of finding information about some topic that may
be on one or more web pages. since every search is looking for some type of in-
formation, we call these topical searches in this book. a navigational search has
the goal of finding a particular web page that the user has either seen before or

280

7 retrieval models

assumes must exist.20 a transactional search has the goal of finding a site where
a task such as shopping or downloading music can be performed. each type of
search has an information need associated with it, but a di   erent type of infor-
mation need. retrieval models based on topical relevance have focused primarily
on the first type of information need (and search). to produce e   ective rankings
for the other types of searches, a retrieval model that can combine evidence related
to user relevance is required.

commercial web search engines incorporate hundreds of features (types of ev-
idence) in their ranking algorithms, many derived from the huge collection of user
interaction data in the query logs. these can be broadly categorized into features
relating to page content, page metadata, anchor text, links (e.g., id95), and
user behavior. although anchor text is derived from the links in a page, it is used
in a di   erent way than features that come from an analysis of the link structure
of pages, and so is put into a separate category. page metadata refers to informa-
tion about a page that is not part of the content of the page, such as its    age,    how
often it is updated, the url of the page, the domain name of its site, and the
amount of text content in the page relative to other material, such as images and
advertisements.

it is interesting to note that understanding the relative importance of these
features and how they can be manipulated to obtain better search rankings for
a web page is the basis of search engine optimization (seo). a search engine op-
timizer may, for example, improve the text used in the title tag of the web page,
improve the text in heading tags, make sure that the domain name and url con-
tain important keywords, and try to improve the anchor text and link structure
related to the page. some of these techniques are not viewed as appropriate by the
web search engine companies, and will be discussed further in section 9.1.5.

in the trec environment, retrieval models have been compared using test
collections of web pages and a mixture of query types. the features related to user
behavior and some of the page metadata features, such as frequency of update,
are not available in the trec data. of the other features, the most important for
navigational searches are the text in the title, body, and heading (h1, h2, h3, and h4)
parts of the document; the anchor text of all links pointing to the document; the
id95 number; and the inlink count (number of links pointing to the page).

20 in the trec world, navigational searches are called home-page and named-page
searches. topical searches are called ad hoc searches. navigational searches are similar to
known-item searches, which have been discussed in the information retrieval literature
for many years.

7.5 web search

281

note that we are not saying that other features do not a   ect the ranking in web
search engines, just that these were the ones that had the most significant impact
in trec experiments.

given the size of the web, many pages will contain all the query terms. some
ranking algorithms rank only those pages which, in e   ect, filters the results us-
ing a boolean and. this can cause problems if only a subset of the web is used
(such as in a site search application) and is particularly risky with topical searches.
for example, only about 50% of the pages judged relevant in the trec topical
web searches contain all the query terms. instead of filtering, the ranking algo-
rithm should strongly favor pages that contain all query terms. in addition, term
proximity will be important. the additional evidence of terms occurring near each
other will significantly improve the e   ectiveness of the ranking. a number of re-
trieval models incorporating term proximity have been developed. the following
approach is designed to work in the id136 network model, and produces good
results.21

the dependence model is based on the assumption that query terms are likely to
appear in close proximity to each other within relevant documents. for example,
given the query    green party political views   , relevant documents will likely con-
tain the phrases    green party    and    political views    within relatively close prox-
imity to one another. if the query is treated as a set of terms q, we can define
sq as the set of all non-empty subsets of q. a galago query attempts to capture
dependencies between query terms as follows:
1. every s     sq that consists of contiguous query terms is likely to appear as an
exact phrase in a relevant document (i.e., represented using the #od:1 opera-
tor).
2. every s     sq such that |s| > 1 is likely to appear (ordered or unordered)
within a reasonably sized window of text in a relevant document (i.e., in a
window represented as #uw:8 for |s| = 2 and #uw:12 for |s| = 3 ).

as an example, this model produces the galago query language representation
shown in figure 7.6 for the trec query    embryonic stem cells   , where the
weights were determined empirically to produce the best results.

given the important pieces of evidence for web search ranking, we can now
give an example of a galago query that combines this evidence into an e   ective
ranking. for the trec query    pet therapy   , we would produce the galago query
shown in figure 7.7. the first thing to note about this query is that it clearly shows

21 the formal model is described in metzler and croft (2005b).

282

7 retrieval models

#weight(

0.8 #combine(embryonic stem cells)
0.1 #combine( #od:1(stem cells) #od:1(embryonic stem)

#od:1(embryonic stem cells))

0.1 #combine( #uw:8(stem cells) #uw:8(embryonic cells)

#uw:8(embryonic stem) #uw:12(embryonic stem cells)))

fig. 7.6. galago query for the dependence model

how a complex query expression can be generated from a simple user query. a
number of proximity terms have been added, and all terms are evaluated using
contexts based on anchor text, title text, body text, and heading text. from an
e   ciency perspective, the proximity terms may be indexed, even though this will
increase the index size substantially. the benefit is that these relatively large query
expressions will be able to be evaluated very e   ciently at query time.

#weight(

0.1 #weight( 0.6 #prior(id95) 0.4 #prior(inlinks))
1.0 #weight(

0.9 #combine(

#weight( 1.0 pet.(anchor) 1.0 pet.(title)

3.0 pet.(body) 1.0 pet.(heading))

#weight( 1.0 therapy.(anchor) 1.0 therapy.(title)

3.0 therapy.(body) 1.0 therapy.(heading)))

0.1 #weight(

1.0 #od:1(pet therapy).(anchor) 1.0 #od:1(pet therapy).(title)
3.0 #od:1(pet therapy).(body) 1.0 #od:1(pet therapy).(heading))

0.1 #weight(

1.0 #uw:8(pet therapy).(anchor) 1.0 #uw:8(pet therapy).(title)
3.0 #uw:8(pet therapy).(body) 1.0 #uw:8(pet therapy).(heading)))

)

fig. 7.7. galago query for web data

the id95 and inlink evidence is incorporated into this query as prior
probabilities. in other words, this evidence is independent of specific queries and
can be calculated at indexing time. the weights in the query were determined by

7.6 machine learning and information retrieval

283

experiments with trec web page collections, which are based on a crawl of the
.gov domain. the relative importance of the evidence could be di   erent for the
full web or for other collections. the text in the main body of the page was found
to be more important than the other parts of the document and anchor text, and
this is reflected in the weights.

experiments with the trec data have also shown that much of the evi-
dence that is crucial for e   ective navigational search is not important for top-
ical searches. in fact, the only features needed for topical search are the simple
terms and proximity terms for the body part of the document. the other fea-
tures do not improve e   ectiveness, but they also do not reduce it. another dif-
ference between topical and navigational searches is that id183 using
pseudo-relevance feedback was found to help topical searches, but made naviga-
tional searches worse. navigational searches are looking for a specific page, so it is
not surprising that smoothing the query by adding a number of extra terms may
increase the    noise    in the results. if a search was known to be in the topical cat-
egory, id183 could be used, but this is di   cult to determine reliably,
and since the potential e   ectiveness benefits of expansion are variable and some-
what unpredictable, this technique is generally not used. given that the evidence
needed to identify good sites for transaction searches seems to be similar to that
needed for navigational searches, this means that the same ranking algorithm can
be used for the di   erent categories of web search.

other research has shown that user behavior information, such as clickthrough
data (e.g., which documents have been clicked on in the past, which rank posi-
tions were clicked) and browsing data (e.g., dwell time on page, links followed),
can have a significant impact on the e   ectiveness of the ranking. this type of ev-
idence can be added into the id136 network framework using additional op-
erators, but as the number of pieces of evidence grows, the issue of how to deter-
mine the most e   ective way of combining and weighting the evidence becomes
more important. in the next section, we discuss techniques for learning both the
weights and the ranking algorithm using explicit and implicit feedback data from
the users.

7.6 machine learning and information retrieval

there has been considerable overlap between the fields of information retrieval
and machine learning. in the 1960s, relevance feedback was introduced as a tech-
nique to improve ranking based on user feedback about the relevance of docu-

284

7 retrieval models

ments in an initial ranking. this was an example of a simple machine-learning
algorithm that built a classifier to separate relevant from non-relevant documents
based on training data. in the 1980s and 1990s, information retrieval researchers
used machine learning approaches to learn ranking algorithms based on user feed-
back. in the last 10 years, there has been a lot of research on machine-learning ap-
proaches to text categorization. many of the applications of machine learning to
information retrieval, however, have been limited by the amount of training data
available. if the system is trying to build a separate classifier for every query, there
is very little data about relevant documents available, whereas other machine-
learning applications may have hundreds or even thousands of training examples.
even the approaches that tried to learn ranking algorithms by using training data
from all the queries were limited by the small number of queries and relevance
judgments in typical information retrieval test collections.

with the advent of web search engines and the huge query logs that are col-
lected from user interactions, the amount of potential training data is enormous.
this has led to the development of new techniques that are having a significant
impact in the field of information retrieval and on the design of search engines.
in the next section, we describe techniques for learning ranking algorithms that
can combine and weight the many pieces of evidence that are important for web
search.

another very active area of machine learning has been the development of so-
phisticated statistical models of text. in section 7.6.2, we describe how these mod-
els can be used to improve ranking based on language models.

7.6.1 learning to rank

all of the probabilistic retrieval models presented so far fall into the category of
generative models. a generative model for text classification assumes that docu-
ments were generated from some underlying model (in this case, usually a multi-
nomial distribution) and uses training data to estimate the parameters of the
model. the id203 of belonging to a class (i.e., the relevant documents for
a query) is then estimated using bayes    rule and the document model. a discrim-
inative model, in contrast, estimates the id203 of belonging to a class directly
from the observed features of the document based on the training data.22 in gen-
eral classification problems, a generative model performs better with low num-
bers of training examples, but the discriminative model usually has the advantage

22 we revisit the discussion of generative versus discriminative classifiers in chapter 9.

7.6 machine learning and information retrieval

285

given enough data. given the amount of potential training data available to web
search engines, discriminative models may be expected to have some advantages
in this application. it is also easier to incorporate new features into a discrimina-
tive model and, as we have mentioned, there can be hundreds of features that are
considered for web ranking.

early applications of learning a discriminative model (discriminative learning)
in information retrieval used id28 to predict whether a document
belonged to the relevant class. the problem was that the amount of training data
and, consequently, the e   ectiveness of the technique depended on explicit rele-
vance judgments obtained from people. even given the resources of a commer-
cial web search company, explicit relevance judgments are costly to obtain. on
the other hand, query logs contain a large amount of implicit relevance informa-
tion in the form of clickthroughs and other user interactions. in response to this,
discriminative learning techniques based on this form of training data have been
developed.

the best-known of the approaches used to learn a ranking function for search
is based on the support vector machine (id166) classifier. this technique will be
discussed in more detail in chapter 9, so in this section we will just give a brief
description of how a ranking id166 can learn to rank.23

the input to the ranking id166 is a training set consisting of partial rank in-

formation for a set of queries

(q1, r1), (q2, r2), . . . , (qn, rn)

where qi is a query and ri is partial information about the desired ranking, or rele-
vance level, of documents for that query. this means that if document da should
be ranked higher than db, then (da, db)     ri; otherwise, (da, db) /    ri. where do
these rankings come from? if relevance judgments are available, the desired rank-
ing would put all documents judged to be at a higher relevance level above those
at a lower level. note that this accommodates multiple levels of relevance, which
are often used in evaluations of web search engines.

if relevance judgments are not available, however, the ranking can be based
on clickthrough and other user data. for example, if a person clicks on the third
document in a ranking for a query and not on the first two, we can assume that
it should be ranked higher in r. if d1, d2, and d3 are the documents in the first,

23 this description is based on joachims    paper on learning to rank using clickthrough

data (joachims, 2002b).

286

7 retrieval models

second, and third rank of the search output, the clickthrough data will result in
pairs (d3, d1) and (d3, d2) being in the desired ranking for this query. this ranking
data will be noisy (because clicks are not relevance judgments) and incomplete,
but there will be a lot of it, and experiments have shown that this type of training
data can be used e   ectively.

let   s assume that we are learning a linear ranking function    w.   da, where    w is a
weight vector that is adjusted by learning, and    da is the vector representation of
the features of document da. these features are, as we described in the last sec-
tion, based on page content, page metadata, anchor text, links, and user behavior.
instead of language model probabilities, however, the features used in this model
that depend on the match between the query and the document content are usu-
ally simpler and less formal. for example, there may be a feature for the number
of words in common between the query and the document body, and similar fea-
tures for the title, header, and anchor text. the weights in the    w vector determine
the relative importance of these features, similar to the weights in the id136
network operators. if a document is represented by three features with integer val-
ues    d = (2, 4, 1) and the weights    w = (2, 1, 2), then the score computed by the
ranking function is just:

   w.   d = (2, 1, 2).(2, 4, 1) = 2.2 + 1.4 + 2.1 = 10

given the training set of queries and rank information, we would like to find a
weight vector    w that would satisfy as many of the following conditions as possible:

   (di, dj)     r1 :    w.   di >    w.   dj
   (di, dj)     rn :    w.   di >    w.   dj

. . .

this simply means that for all document pairs in the rank data, we would like the
score for the document with the higher relevance rating (or rank) to be greater
than the score for the document with the lower relevance rating. unfortunately,
there is no e   cient algorithm to find the exact solution for    w. we can, however,
reformulate this problem as a standard id166 optimization as follows:

7.6 machine learning and information retrieval

287

   

minimize:
subject to:

1
2

   w.    w + c

  i;j;k

   (di, dj)     r1 :    w.   di >    w.   dj + 1       i;j;1
   (di, dj)     rn :    w.   di >    w.   dj + 1       i;j;n

. . .

   i   j   k :   i, j, k     0

where   , known as a slack variable, allows for misclassification of di   cult or noisy
training examples, and c is a parameter that is used to prevent overfitting. overfit-
ting happens when the learning algorithm produces a ranking function that does
very well at ranking the training data, but does not do well at ranking documents
for a new query. software packages are available24 that do this optimization and
produce a classifier.

where did this optimization come from? the impatient reader will have to
jump ahead to the explanation for a general id166 classifier in chapter 9. for the
time being, we can say that the id166 algorithm will find a classifier (i.e., the vector
   w) that has the following property. each pair of documents in our training data
can be represented by the vector (   di       dj). if we compute the score for this pair as
   w.(   di       dj), the id166 classifier will find a    w that makes the smallest score as large
as possible. the same thing is true for negative examples (pairs of documents that
are not in the rank data). this means that the classifier will make the di   erences
in scores as large as possible for the pairs of documents that are hardest to rank.
note that this model does not specify the features that should be used. it could
even be used to learn the weights for features corresponding to scores from com-
pletely di   erent retrieval models, such as bm25 and language models. combin-
ing multiple searches for a given query has been shown to be e   ective in a number
of experiments, and is discussed further in section 10.5.1. it should also be noted
that the weights learned by ranking id166 (or some other discriminative tech-
nique) can be used directly in the id136 network query language.

although linear discriminative classifiers such as ranking id166 may have an
advantage for web search, there are other search applications where there will be
less training data and less features available. for these applications, the generative
models of topical relevance may be more e   ective, especially as the models con-
tinue to improve through better estimation techniques. the next section discusses

24 such as sv m light; see http://id166light.joachims.org.

288

7 retrieval models

how estimation can be improved by modeling a document as a mixture of topic
models.

7.6.2 topic models and vocabulary mismatch

one of the important issues in general information retrieval is vocabulary mis-
match. this refers to a situation where relevant documents do not match a query,
because they are using di   erent words to describe the same topic. in the web en-
vironment, many documents will contain all the query words, so this may not ap-
pear to be an issue. in search applications with smaller collections, however, it will
be important, and even in web search, trec experiments have shown that topi-
cal queries produce better results using id183. id183 (using,
for example, pseudo-relevance feedback) is the standard technique for reducing
vocabulary mismatch, although id30 also addresses this issue to some extent.
a di   erent approach would be to expand the documents by adding related terms.
for documents represented as language models, this is equivalent to smoothing
the probabilities in the language model so that words that did not occur in the
text have non-zero probabilities. note that this is di   erent from smoothing us-
ing the collection probabilities, which are the same for all documents. instead, we
need some way of increasing the probabilities of words that are associated with
the topic of the document.

a number of techniques have been proposed to do this. if a document is
known to belong to a category or cluster of documents, then the probabilities of
words in that cluster can be used to smooth the document language model. we
describe the details of this in chapter 9. a technique known as latent seman-
tic indexing, or lsi,25 maps documents and terms into a reduced dimensionality
space, so that documents that were previously indexed using a vocabulary of hun-
dreds of thousands of words are now represented using just a few hundred fea-
tures. each feature in this new space is a mixture or cluster of many words, and it
is this mixing that in e   ect smooths the id194.

the id44 (lda) model, which comes from the machine
learning community, models documents as a mixture of topics. a topic is a lan-
guage model, just as we defined previously. in a retrieval model such as query like-
lihood, each document is assumed to be associated with a single topic. there are,

25 this technique is also called latent semantic analysis or lsa (deerwester et al.,

1990). note that    latent    is being used in the sense of    hidden.   

7.6 machine learning and information retrieval

289

in e   ect, as many topics as there are documents in the collection. in the lda ap-
proach, in contrast, the assumption is that there is a fixed number of underlying
(or latent) topics that can be used to describe the contents of documents. each
document is represented as a mixture of these topics, which achieves a smoothing
e   ect that is similar to lsi. in the lda model, a document is generated by first
picking a distribution over topics, and then, for the next word in the document,
we choose a topic and generate a word from that topic.

using our    bucket    analogy for language models, we would need multiple
buckets to describe this process. for each document, we would have one bucket of
topics, with the number of instances of each topic depending on the distribution
of topics we had picked. for each topic, there would be another bucket containing
words, with the number of instances of the words depending on the probabilities
in the topic language model. then, to generate a document, we first select a topic
from the topic bucket (still without looking), then go to the bucket of words for
the topic that had been selected and pick out a word. the process is then repeated
for the next word.

more formally, the lda process for generating a document is:

1. for each document d, pick a multinomial distribution   d from a dirichlet

distribution with parameter   .

2. for each word position in document d:

a) pick a topic z from the multinomial distribution   d.
b) choose a word w from p (w|z,   ), a multinomial id203 condi-

tioned on the topic z with parameter   .

a variety of techniques are available for learning the topic models and the   
distributions using the collection of documents as the training data, but all of
these methods tend to be quite slow. once we have these distributions, we can
produce language model probabilities for the words in documents:
p (w|z,   )p (z|  d)

plda(w|d) = p (w|  d,   ) =

   

z

these probabilities can then be used to smooth the id194 by
mixing them with the query likelihood id203 as follows:

(

)

p (w|d) =   

fw;d +    cw|c|
|d| +   

+ (1       )plda(w|d)

290

7 retrieval models

so the final language model probabilities are, in e   ect, a mixture of the maximum
likelihood probabilities, collection probabilities, and the lda probabilities.

if the lda probabilities are used directly as the id194, the
e   ectiveness of the ranking will be significantly reduced because the features are
too smoothed. in trec experiments, k (the number of topics) has a value of
around 400. this means that all documents in the collection are represented as
mixtures of just 400 topics. given that there can be millions of words in a col-
lection vocabulary, matching on topics alone will lose some of the precision of
matching individual words. when used to smooth the document language model,
however, the lda probabilities can significantly improve the e   ectiveness of
query likelihood ranking. table 7.7 shows the high-id203 words from four
lda topics (out of 100) generated from a sample of trec news stories.26 note
that the names of the topics were not automatically generated.

arts
new
film
show
music
movie
play

musical

best
actor
first
york
opera
theater
actress
love

budgets
million

tax

program
budget
billion
federal
year

spending

new
state
plan
money
programs
government

congress

children
children
women
people
child
years
families
work
parents

says
family
welfare
men
percent

care
life

education
school
students
schools
education
teachers

high
public
teacher
bennett
manigat
namphy

state

president
elementary

haiti

table 7.7. highest-id203 terms from four topics in lda model

the main problem with using lda for search applications is that estimating
the probabilities in the model is expensive. until faster methods are developed,

26 this table is from blei et al. (2003).

7.7 application-based models

291

this technique will be limited to smaller collections (hundreds of thousands of
documents, but not millions).

7.7 application-based models

in this chapter we have described a wide variety of retrieval models and ranking
algorithms. from the point of view of someone involved in designing and imple-
menting a search application, the question is which of these techniques should be
used and when? the answer depends on the application and the tools available.
most search applications involve much smaller collections than the web and a lot
less connectivity in terms of links and anchor text. ranking algorithms that work
well in web search engines often do not produce the best rankings in other appli-
cations. customizing a ranking algorithm for the application will nearly always
produce the best results.

the first step in doing this is to construct a test collection of queries, docu-
ments, and relevance judgments so that di   erent versions of the ranking algo-
rithm can be compared quantitatively. evaluation is discussed in detail in chapter
8, and it is the key to an e   ective search engine.

the next step is to identify what evidence or features might be used to rep-
resent documents. simple terms and proximity terms are almost always useful.
significant document structure   such as titles, authors, and date fields   are also
nearly always important for search. in some applications, numeric fields may be
important. text processing techniques such as id30 and stopwords also must
be considered.

another important source of information that can be used for query expan-
sion is an application-specific thesaurus. these are surprisingly common since of-
ten an attempt will have been made to build them either manually or automati-
cally for a previous information system. although they are often very incomplete,
the synonyms and related words they contain can make a significant di   erence to
ranking e   ectiveness.

having identified the various document features and other evidence, the next
task is to decide how to combine it to calculate a document score. an open source
search engine such as galago makes this relatively easy since the combination and
weighting of evidence can be expressed in the query language and many variations
can be tested quickly. other search engines do not have this degree of flexibility.
if a search engine based on a simple retrieval model is being used for the search
application, the descriptions of how scores are calculated in the bm25 or query

292

7 retrieval models

likelihood models and how they are combined in the id136 network model
can be used as a guide to achieve similar e   ects by appropriate query transforma-
tions and additional code for scoring. for example, the synonym and related word
information in a thesaurus should not be used to simply add words to a query. un-
less some version of the #syn operator is used, the e   ectiveness of the ranking will
be reduced. the implementation of #syn in galago can be used as an example of
how to add this operator to a search engine.

much of the time spent in developing a search application will be spent on
tuning the retrieval e   ectiveness of the ranking algorithm. doing this without
some concept of the underlying retrieval model can be very unrewarding. the re-
trieval models described in this chapter (namely bm25, query likelihood, rele-
vance models, id136 network, and ranking id166) provide the best possible
blueprints for a successful ranking algorithm. for these models, good parame-
ter values and weights are already known from extensive published experiments.
these values can be used as a starting point for the process of determining whether
modifications are needed for an application. if enough training data is available,
a discriminative technique such as ranking id166 will learn the best weights di-
rectly.

references and further reading

since retrieval models are one of the most important topics in information re-
trieval, there are many papers describing research in this area, starting in the 1950s.
one of the most valuable aspects of van rijsbergen   s book (van rijsbergen, 1979)
is the coverage of the older research in this area. in this book, we will focus on
some of the major papers, rather than attempting to be comprehensive. these ref-
erences will be discussed in the order of the topics presented in this chapter.

the discussion of the nature of relevance has, understandably, been going on
in information retrieval for a long time. one of the earlier papers that is often
cited is saracevic (1975). a more recent article gives a review of work in this area
(mizzaro, 1997).

on the topic of boolean versus ranked search, turtle (1994) carried out an
experiment comparing the performance of professional searchers using the best
boolean queries they could generate against keyword searches using ranked out-
put and found no advantage for the boolean search. when simple boolean queries
are compared against ranking, as in turtle and croft (1991), the e   ectiveness of
ranking is much higher.

7.7 application-based models

293

the vector space model was first mentioned in salton et al. (1975), and is de-
scribed in detail in salton and mcgill (1983). the most comprehensive paper in
weighting experiments with this model is salton and buckley (1988), although
the term-weighting techniques described in section 7.1.2 are a later improvement
on those described in the paper.

the description of information retrieval as a classification problem appears in
van rijsbergen (1979). the best paper on the application of the binary indepen-
dence model and its development into the bm25 ranking function is sparck jones
et al. (2000).

the use of language models in information retrieval started with ponte and
croft (1998), who described a retrieval model based on multiple-bernoulli lan-
guage models. this was quickly followed by a number of papers that developed
the multinomial version of the retrieval model (hiemstra, 1998; f. song & croft,
1999). miller et al. (1999) described the same approach using a hidden markov
model. berger and la   erty (1999) showed how translation probabilities for words
could be incorporated into the language model approach. we will refer to this
translation model again in section 10.3. the use of non-uniform prior probabili-
ties was studied by kraaij et al. (2002). a collection of papers relating to language
models and information retrieval appears in croft and la   erty (2003).

zhai and la   erty (2004) give an excellent description of smoothing tech-
niques for id38 in information retrieval. smoothing using clusters
and nearest neighbors is described in liu and croft (2004) and kurland and lee
(2004).

an early term-dependency model was described in van rijsbergen (1979). a
bigram language model for information retrieval was described in f. song and
croft (1999), but the more general models in gao et al. (2004) and metzler and
croft (2005b) produced significantly better retrieval results, especially with larger
collections.

the relevance model approach to id183 appeared in lavrenko and
croft (2001). la   erty and zhai (2001) proposed a related approach that built a
query model and compared it to document models.

there have been many experiments reported in the information retrieval liter-
ature showing that the combination of evidence significantly improves the rank-
ing e   ectiveness. croft (2000) reviews these results and shows that this is not
surprising, given that information retrieval can be viewed as a classification prob-
lem with a huge choice of features. turtle and croft (1991) describe the infer-
ence network model. this model was used as the basis for the inquery search en-

294

7 retrieval models

gine (callan et al., 1992) and the win version of the commercial search engine
westlaw (pritchard-schoch, 1993). the extension of this model to include
language model probabilities is described in metzler and croft (2004). this ex-
tension was implemented as the indri search engine (strohman et al., 2005; met-
zler, strohman, et al., 2004). the galago query language is based on the query
language for indri.

the approach to web search described in section 7.5, which scores documents
based on a combination or mixture of language models representing di   erent
parts of the document structure, is based on ogilvie and callan (2003). the
bm25f ranking function (robertson et al., 2004) is an extension of bm25 that is
also designed to e   ectively combine information from di   erent document fields.
spam is of such importance in web search that an entire subfield, called ad-
versarial information retrieval, has developed to deal with search techniques for
document collections that are being manipulated by parties with di   erent inter-
ests (such as spammers and search engine optimizers). we discuss the topic of
spam in chapter 9.

the early work on learning ranking functions includes the use of logistic re-
gression (cooper et al., 1992). fuhr and buckley (1991) were the first to de-
scribe clearly how using features that are independent of the actual query words
(e.g., using a feature like the number of matching terms rather than which terms
matched) enable the learning of ranking functions across queries. the use of
ranking id166 for information retrieval was described by joachims (2002b). cao
et al. (2006) describe modifications of this approach that improve ranking e   ec-
tiveness. ranknet (c. burges et al., 2005) is a neural network approach to learn-
ing a ranking function that is used in the microsoft web search engine. agichtein,
brill, and dumais (2006) describe how user behavior features can be incorporated
e   ectively into ranking based on ranknet. both ranking id166s and ranknet
learn using partial rank information (i.e., pairwise preferences). another class of
learning models, called listwise models, use the entire ranked list for learning. ex-
amples of these models include the linear discriminative model proposed by gao
et al. (2005), which learns weights for features that are based on language models.
this approach has some similarities to the id136 network model being used
to combine language model and other features. another listwise approach is the
term dependence model proposed by metzler and croft (2005b), which is also
based on a linear combination of features. both the gao and metzler models pro-
vide a learning technique that maximizes average precision (an important infor-

7.7 application-based models

295

mation retrieval metric) directly. more information about listwise learning mod-
els can be found in xia et al. (2008).

hofmann (1999) described a probabilistic version of lsi (plsi) that intro-
duced the modeling of documents as a mixture of topics. the lda model was
described by blei et al. (2003). a number of extensions of this model have been
proposed since then, but they have not been applied to information retrieval.
the application of lda to information retrieval was described in wei and croft
(2006).

exercises

7.1. use the    advanced search    feature of a web search engine to come up with
three examples of searches using the boolean operators and, or, and not that
work better than using the same query in the regular search box. do you think the
search engine is using a strict boolean model of retrieval for the advanced search?

7.2. can you think of another measure of similarity that could be used in the vec-
tor space model? compare your measure with the cosine correlation using some
example documents and queries with made-up weights. browse the ir literature
on the web and see whether your measure has been studied (start with van rijs-
bergen   s book).

7.3. if each term represents a dimension in a t-dimensional space, the vector space
model is making an assumption that the terms are orthogonal. explain this as-
sumption and discuss whether you think it is reasonable.

7.4. derive bayes    rule from the definition of a id155:

p (a|b) =

p (a     b)
p (b)

give an example of a conditional and a joint id203 using the occurrence of
words in documents as the events.

7.5. implement a bm25 module for galago. show that it works and document
it.

7.6. show the e   ect of changing parameter values in your bm25 implementation.

296

7 retrieval models

7.7. what is the    bucket    analogy for a bigram language model? give examples.

7.8. using the galago implementation of query likelihood, study the impact of
short queries and long queries on e   ectiveness. do the parameter settings make a
di   erence?

7.9. implement the relevance model approach to pseudo-relevance feedback in
galago. show it works by generating some expansion terms for queries and doc-
ument it.

7.10. show that the belwand operator computes the query likelihood score with
simple terms. what does the belwsum operator compute?

7.11. implement a #not operator for the id136 network query language in
galago. show some examples of how it works.

7.12. do a detailed design for numeric operators for the id136 network query
language in galago.

7.13. write an interface program that will take a user   s query as text and trans-
form it into an id136 network query. make sure you use proximity operators.
compare the performance of the simple queries and the transformed queries.

8

evaluating search engines

   evaluation, mr. spock.   

captain kirk, star trek: the motion picture

8.1 why evaluate?

evaluation is the key to making progress in building better search engines. it is
also essential to understanding whether a search engine is being used e   ectively
in a specific application. engineers don   t make decisions about a new design for a
commercial aircraft based on whether it feels better than another design. instead,
they test the performance of the design with simulations and experiments, eval-
uate everything again when a prototype is built, and then continue to monitor
and tune the performance of the aircraft after it goes into service. experience has
shown us that ideas that we intuitively feel must improve search quality, or models
that have appealing formal properties often have little or no impact when tested
using quantitative experiments.

one of the primary distinctions made in the evaluation of search engines is
between e   ectiveness and e   ciency. e   ectiveness, loosely speaking, measures the
ability of the search engine to find the right information, and e   ciency measures
how quickly this is done. for a given query, and a specific definition of relevance,
we can more precisely define e   ectiveness as a measure of how well the ranking
produced by the search engine corresponds to a ranking based on user relevance
judgments. e   ciency is defined in terms of the time and space requirements for
the algorithm that produces the ranking. viewed more generally, however, search
is an interactive process involving di   erent types of users with di   erent informa-
tion problems. in this environment, e   ectiveness and e   ciency will be a   ected
by many factors, such as the interface used to display search results and query re-
finement techniques, such as query suggestion and relevance feedback. carrying
out this type of holistic evaluation of e   ectiveness and e   ciency, while impor-

298

8 evaluating search engines

tant, is very di   cult because of the many factors that must be controlled. for this
reason, evaluation is more typically done in tightly defined experimental settings,
and this is the type of evaluation we focus on here.

e   ectiveness and e   ciency are related in that techniques that give a small
boost to e   ectiveness may not be included in a search engine implementation
if they have a significant adverse e   ect on an e   ciency measure such as query
throughput. generally speaking, however, information retrieval research focuses
on improving the e   ectiveness of search, and when a technique has been estab-
lished as being potentially useful, the focus shifts to finding e   cient implemen-
tations. this is not to say that research on system architecture and e   ciency is
not important. the techniques described in chapter 5 are a critical part of build-
ing a scalable and usable search engine and were primarily developed by research
groups. the focus on e   ectiveness is based on the underlying goal of a search en-
gine, which is to find the relevant information. a search engine that is extremely
fast is of no use unless it produces good results.

so is there a trade-o    between e   ciency and e   ectiveness? some search en-
gine designers discuss having    knobs,    or parameters, on their system that can be
turned to favor either high-quality results or improved e   ciency. the current sit-
uation, however, is that there is no reliable technique that significantly improves
e   ectiveness that cannot be incorporated into a search engine due to e   ciency
considerations. this may change in the future.

in addition to e   ciency and e   ectiveness, the other significant consideration
in search engine design is cost. we may know how to implement a particular
search technique e   ciently, but to do so may require a huge investment in pro-
cessors, memory, disk space, and networking. in general, if we pick targets for any
two of these three factors, the third will be determined. for example, if we want a
particular level of e   ectiveness and e   ciency, this will determine the cost of the
system configuration. alternatively, if we decide on e   ciency and cost targets, it
may have an impact on e   ectiveness. two extreme cases of choices for these fac-
tors are searching using a pattern-matching utility such as grep, or searching using
an organization such as the library of congress. searching a large text collection
using grep will have poor e   ectiveness and poor e   ciency, but will be very cheap.
searching using the sta    analysts at the library of congress will produce excel-
lent results (high e   ectiveness) due to the manual e   ort involved, will be e   cient
in terms of the user   s time (although it will involve a delay waiting for a response
from the analysts), and will be very expensive. searching directly using an e   ective
search engine is designed to be a reasonable compromise between these extremes.

8.2 the evaluation corpus

299

an important point about terminology is the meaning of    optimization    as
it is discussed in the context of evaluation. the retrieval and indexing techniques
in a search engine have many parameters that can be adjusted to optimize perfor-
mance, both in terms of e   ectiveness and e   ciency. typically the best values for
these parameters are determined using training data and a cost function. training
data is a sample of the real data, and the cost function is the quantity based on
the data that is being maximized (or minimized). for example, the training data
could be samples of queries with relevance judgments, and the cost function for a
ranking algorithm would be a particular e   ectiveness measure. the optimization
process would use the training data to learn parameter settings for the ranking
algorithm that maximized the e   ectiveness measure. this use of optimization is
very di   erent from    search engine optimization   , which is the process of tailoring
web pages to ensure high rankings from search engines.

in the remainder of this chapter, we will discuss the most important evalu-
ation measures, both for e   ectiveness and e   ciency. we will also describe how
experiments are carried out in controlled environments to ensure that the results
are meaningful.

8.2 the evaluation corpus

one of the basic requirements for evaluation is that the results from di   erent
techniques can be compared. to do this comparison fairly and to ensure that ex-
periments are repeatable, the experimental settings and data used must be fixed.
starting with the earliest large-scale evaluations of search performance in the
1960s, generally referred to as the cranfield1 experiments (cleverdon, 1970), re-
searchers assembled test collections consisting of documents, queries, and relevance
judgments to address this requirement. in other language-related research fields,
such as linguistics, machine translation, or id103, a text corpus is a
large amount of text, usually in the form of many documents, that is used for sta-
tistical analysis of various kinds. the test collection, or evaluation corpus, in in-
formation retrieval is unique in that the queries and relevance judgments for a
particular search task are gathered in addition to the documents.

test collections have changed over the years to reflect the changes in data and
user communities for typical search applications. as an example of these changes,

1 named after the place in the united kingdom where the experiments were done.

300

8 evaluating search engines

the following three test collections were created at intervals of about 10 years,
starting in the 1980s:
    cacm: titles and abstracts from the communications of the acm from
1958   1979. queries and relevance judgments generated by computer scien-
tists.

    ap: associated press newswire documents from 1988   1990 (from trec
disks 1   3). queries are the title fields from trec topics 51   150. topics and
relevance judgments generated by government information analysts.

    gov2: web pages crawled from websites in the .gov domain during early
2004. queries are the title fields from trec topics 701   850. topics and rel-
evance judgments generated by government analysts.

the cacm collection was created when most search applications focused on bib-
liographic records containing titles and abstracts, rather than the full text of doc-
uments. table 8.1 shows that the number of documents in the collection (3,204)
and the average number of words per document (64) are both quite small. the
total size of the document collection is only 2.2 megabytes, which is considerably
less than the size of a single typical music file for an mp3 player. the queries for
this collection of abstracts of computer science papers were generated by students
and faculty of a computer science department, and are supposed to represent ac-
tual information needs. an example of a cacm query is:

security considerations in local networks, network operating systems, and dis-
tributed systems.

relevance judgments for each query were done by the same people, and were rel-
atively exhaustive in the sense that most relevant documents were identified. this
was possible since the collection is small and the people who generated the ques-
tions were very familiar with the documents. table 8.2 shows that the cacm
queries are quite long (13 words on average) and that there are an average of 16
relevant documents per query.

the ap and gov2 collections were created as part of the trec conference
series sponsored by the national institute of standards and technology (nist).
the ap collection is typical of the full-text collections that were first used in the
early 1990s. the availability of cheap magnetic disk technology and online text
entry led to a number of search applications for full-text documents such as news

8.2 the evaluation corpus

301

collection number of
documents

size

average number
of words/doc.

cacm

ap

gov2

2.2 mb
3,204
242,918
0.7 gb
25,205,179 426 gb

64
474
1073

table 8.1. statistics for three example text collections. the average number of words per
document is calculated without id30.

stories, legal documents, and encyclopedia articles. the ap collection is much big-
ger (by two orders of magnitude) than the cacm collection, both in terms of the
number of documents and the total size. the average document is also consider-
ably longer (474 versus 64 words) since they contain the full text of a news story.
the gov2 collection, which is another two orders of magnitude larger, was de-
signed to be a testbed for web search applications and was created by a crawl of
the .gov domain. many of these government web pages contain lengthy policy de-
scriptions or tables, and consequently the average document length is the largest
of the three collections.

collection number of average number of average number of
relevant docs/query

words/query

queries

cacm

ap

gov2

64
100
150

13.0
4.3
3.1

16
220
180

table 8.2. statistics for queries from example text collections

the queries for the ap and gov2 collections are based on trec topics. the
topics were created by government information analysts employed by nist. the
early trec topics were designed to reflect the needs of professional analysts in
government and industry and were quite complex. later trec topics were sup-
posed to represent more general information needs, but they retained the trec
topic format. an example is shown in figure 8.1. trec topics contain three fields
indicated by the tags. the title field is supposed to be a short query, more typical
of a web application. the description field is a longer version of the query, which
as this example shows, can sometimes be more precise than the short query. the
narrative field describes the criteria for relevance, which is used by the people do-

302

8 evaluating search engines

ing relevance judgments to increase consistency, and should not be considered as
a query. most recent trec evaluations have focused on using the title field of
the topic as the query, and our statistics in table 8.2 are based on that field.

fig. 8.1. example of a trec topic

the relevance judgments in trec depend on the task that is being evalu-
ated. for the queries in these tables, the task emphasized high recall, where it is
important not to miss information. given the context of that task, trec an-
alysts judged a document as relevant if it contained information that could be
used to help write a report on the query topic. in chapter 7, we discussed the
di   erence between user relevance and topical relevance. although the trec rel-
evance definition does refer to the usefulness of the information found, analysts
are instructed to judge all documents containing the same useful information as
relevant. this is not something a real user is likely to do, and shows that trec
is primarily focused on topical relevance. relevance judgments for the cacm
collections are binary, meaning that a document is either relevant or it is not.
this is also true of most of the trec collections. for some tasks, multiple lev-
els of relevance may be appropriate. some trec collections, including gov2,
were judged using three levels of relevance (not relevant, relevant, and highly rel-
evant). we discuss e   ectiveness measures for both binary and graded relevance

<top> <num> number: 794  <title> pet therapy  <desc> description: how are pets or animals used in therapy for humans and what are the benefits?  <narr> narrative: relevant documents must include details of how pet   or animal  assisted therapy is or has been used.  relevant details include information about pet therapy programs, descriptions of the circumstances in which pet therapy is used, the benefits of this type of therapy, the degree of success of this therapy, and any laws or regulations governing it.  </top>  8.2 the evaluation corpus

303

in section 8.4. di   erent retrieval tasks can a   ect the number of relevance judg-
ments required, as well as the type of judgments and the e   ectiveness measure.
for example, in chapter 7 we described navigational searches, where the user is
looking for a particular page. in this case, there is only one relevant document for
the query.

creating a new test collection can be a time-consuming task. relevance judg-
ments in particular require a considerable investment of manual e   ort for the
high-recall search task. when collections were very small, most of the documents
in a collection could be evaluated for relevance. in a collection such as gov2,
however, this would clearly be impossible. instead, a technique called pooling is
used. in this technique, the top k results (for trec, k varied between 50 and
200) from the rankings obtained by di   erent search engines (or retrieval algo-
rithms) are merged into a pool, duplicates are removed, and the documents are
presented in some random order to the people doing the relevance judgments.
pooling produces a large number of relevance judgments for each query, as shown
in table 8.2. however, this list is incomplete and, for a new retrieval algorithm
that had not contributed documents to the original pool, this could potentially be
a problem. specifically, if a new algorithm found many relevant documents that
were not part of the pool, they would be treated as being not relevant, and conse-
quently the e   ectiveness of that algorithm could be significantly underestimated.
studies with the trec data, however, have shown that the relevance judgments
are complete enough to produce accurate comparisons for new search techniques.
trec corpora have been extremely useful for evaluating new search tech-
niques, but they have limitations. a high-recall search task and collections of
news articles are clearly not appropriate for evaluating product search on an e-
commerce site, for example. new trec    tracks    can be created to address im-
portant new applications, but this process can take months or years. on the other
hand, new search applications and new data types such as blogs, forums, and an-
notated videos are constantly being developed. fortunately, it is not that di   cult
to develop an evaluation corpus for any given application using the following ba-
sic guidelines:
    use a document collection that is representative for the application in terms of
the number, size, and type of documents. in some cases, this may be the actual
collection for the application; in others it will be a sample of the actual collec-
tion, or even a similar collection. if the target application is very general, then
more than one collection should be used to ensure that results are not corpus-

304

8 evaluating search engines

specific. for example, in the case of the high-recall trec task, a number of
di   erent news and government collections were used for evaluation.

    the queries that are used for the test collection should also be representative of
the queries submitted by users of the target application. these may be acquired
either from a query log from a similar application or by asking potential users
for examples of queries. although it may be possible to gather tens of thou-
sands of queries in some applications, the need for relevance judgments is a
major constraint. the number of queries must be su   cient to establish that
a new technique makes a significant di   erence. an analysis of trec experi-
ments has shown that with 25 queries, a di   erence in the e   ectiveness measure
map (section 8.4.2) of 0.05 will result in the wrong conclusion about which
system is better in about 13% of the comparisons. with 50 queries, this error
rate falls below 4%. a di   erence of 0.05 in map is quite large. if a significance
test, such as those discussed in section 8.6.1, is used in the evaluation, a relative
di   erence of 10% in map is su   cient to guarantee a low error rate with 50
queries. if resources or the application make more relevance judgments possi-
ble, in terms of generating reliable results it will be more productive to judge
more queries rather than to judge more documents from existing queries (i.e.,
increasing k). strategies such as judging a small number (e.g., 10) of the top-
ranked documents from many queries or selecting documents to judge that
will make the most di   erence in the comparison (carterette et al., 2006) have
been shown to be e   ective. if a small number of queries are used, the results
should be considered indicative, not conclusive. in that case, it is important
that the queries should be at least representative and have good coverage in
terms of the goals of the application. for example, if algorithms for local search
were being tested, the queries in the test collection should include many dif-
ferent types of location information.

    relevance judgments should be done either by the people who asked the ques-
tions or by independent judges who have been instructed in how to determine
relevance for the application being evaluated. relevance may seem to be a very
subjective concept, and it is known that relevance judgments can vary depend-
ing on the person making the judgments, or even vary for the same person
at di   erent times. despite this variation, analysis of trec experiments has
shown that conclusions about the relative performance of systems are very
stable. in other words, di   erences in relevance judgments do not have a sig-
nificant e   ect on the error rate for comparisons. the number of documents
that are evaluated for each query and the type of relevance judgments will de-

8.3 logging

305

pend on the e   ectiveness measures that are chosen. for most applications, it
is generally easier for people to decide between at least three levels of rele-
vance: definitely relevant, definitely not relevant, and possibly relevant. these
can be converted into binary judgments by assigning the    possibly relevant   
level to either one of the other levels, if that is required for an e   ectiveness
measure. some applications and e   ectiveness measures, however, may support
more than three levels of relevance.
as a final point, it is worth emphasizing that many user actions can be consid-
ered implicit relevance judgments, and that if these can be exploited, this can sub-
stantially reduce the e   ort of constructing a test collection. for example, actions
such as clicking on a document in a result list, moving it to a folder, or sending it
to a printer may indicate that it is relevant. in previous chapters, we have described
how query logs and clickthrough can be used to support operations such as query
expansion and id147. in the next section, we discuss the role of query
logs in search engine evaluation.

8.3 logging

query logs that capture user interactions with a search engine have become an
extremely important resource for web search engine development. from an eval-
uation perspective, these logs provide large amounts of data showing how users
browse the results that a search engine provides for a query. in a general web search
application, the number of users and queries represented can number in the tens
of millions. compared to the hundreds of queries used in typical trec collec-
tions, query log data can potentially support a much more extensive and realistic
evaluation. the main drawback with this data is that it is not as precise as explicit
relevance judgments.

an additional concern is maintaining the privacy of the users. this is par-
ticularly an issue when query logs are shared, distributed for research, or used
to construct user profiles (see section 6.2.5). various techniques can be used to
anonymize the logged data, such as removing identifying information or queries
that may contain personal data, although this can reduce the utility of the log for
some purposes.

a typical query log will contain the following data for each query:

    user identifier or user session identifier. this can be obtained in a number of
ways. if a user logs onto a service, uses a search toolbar, or even allows cookies,

306

8 evaluating search engines

this information allows the search engine to identify the user. a session is a
series of queries submitted to a search engine over a limited amount of time.
in some circumstances, it may be possible to identify a user only in the context
of a session.

    query terms. the query is stored exactly as the user entered it.
    list of urls of results, their ranks on the result list, and whether they were

clicked on.2

    timestamp(s). the timestamp records the time that the query was submit-
ted. additional timestamps may also record the times that specific results were
clicked on.
the clickthrough data in the log (the third item) has been shown to be highly
correlated with explicit judgments of relevance when interpreted appropriately,
and has been used for both training and evaluating search engines. more detailed
information about user interaction can be obtained through a client-side applica-
tion, such as a search toolbar in a web browser. although this information is not
always available, some user actions other than clickthroughs have been shown to
be good predictors of relevance. two of the best predictors are page dwell time and
search exit action. the page dwell time is the amount of time the user spends on
a clicked result, measured from the initial click to the time when the user comes
back to the results page or exits the search application. the search exit action is
the way the user exits the search application, such as entering another url, clos-
ing the browser window, or timing out. other actions, such as printing a page, are
very predictive but much less frequent.

although clicks on result pages are highly correlated with relevance, they can-
not be used directly in place of explicit relevance judgments, because they are very
biased toward pages that are highly ranked or have other features such as being
popular or having a good snippet on the result page. this means, for example,
that pages at the top rank are clicked on much more frequently than lower-ranked
pages, even when the relevant pages are at the lower ranks. one approach to re-
moving this bias is to use clickthrough data to predict user preferences between
pairs of documents rather than relevance judgments. user preferences were first
mentioned in section 7.6, where they were used to train a ranking function. a
preference for document d1 compared to document d2 means that d1 is more rel-

2 in some logs, only the clicked-on urls are recorded. logging all the results enables
the generation of preferences and provides a source of    negative    examples for various
tasks.

8.3 logging

307

evant or, equivalently, that it should be ranked higher. preferences are most ap-
propriate for search tasks where documents can have multiple levels of relevance,
and are focused more on user relevance than purely topical relevance. relevance
judgments (either multi-level or binary) can be used to generate preferences, but
preferences do not imply specific relevance levels.

the bias in clickthrough data is addressed by    strategies,    or policies that gen-
erate preferences. these strategies are based on observations of user behavior and
verified by experiments. one strategy that is similar to that described in section
7.6 is known as skip above and skip next (agichtein, brill, dumais, & ragno,
2006). this strategy assumes that given a set of results for a query and a clicked
result at rank position p, all unclicked results ranked above p are predicted to be
less relevant than the result at p. in addition, unclicked results immediately fol-
lowing a clicked result are less relevant than the clicked result. for example, given
a result list of ranked documents together with click data as follows:

d1
d2
d3 (clicked)
d4,

this strategy will generate the following preferences:

d3 > d2
d3 > d1
d3 > d4

since preferences are generated only when higher-ranked documents are ignored,
a major source of bias is removed.

the    skip    strategy uses the clickthrough patterns of individual users to gener-
ate preferences. this data can be noisy and inconsistent because of the variability
in users    behavior. since query logs typically contain many instances of the same
query submitted by di   erent users, clickthrough data can be aggregated to remove
potential noise from individual di   erences. specifically, click distribution infor-
mation can be used to identify clicks that have a higher frequency than would be
expected based on typical click patterns. these clicks have been shown to corre-
late well with relevance judgments. for a given query, we can use all the instances
of that query in the log to compute the observed click frequency o(d, p) for the
result d in rank position p. we can also compute the expected click frequency
e(p) at rank p by averaging across all queries. the click deviation cd(d, p) for a
result d in position p is computed as:

308

8 evaluating search engines

cd(d, p) = o(d, p)     e(p).

we can then use the value of cd(d, p) to    filter    clicks and provide more reliable
click information to the skip strategy.

a typical evaluation scenario involves the comparison of the result lists for two
or more systems for a given set of queries. preferences are an alternate method of
specifying which documents should be retrieved for a given query (relevance judg-
ments being the typical method). the quality of the result lists for each system is
then summarized using an e   ectiveness measure that is based on either prefer-
ences or relevance judgments. the following section describes the measures that
are most commonly used in research and system development.

8.4 effectiveness metrics

8.4.1 recall and precision
the two most common e   ectiveness measures, recall and precision, were intro-
duced in the cranfield studies to summarize and compare search results. intu-
itively, recall measures how well the search engine is doing at finding all the rele-
vant documents for a query, and precision measures how well it is doing at reject-
ing non-relevant documents.

the definition of these measures assumes that, for a given query, there is a set
of documents that is retrieved and a set that is not retrieved (the rest of the doc-
uments). this obviously applies to the results of a boolean search, but the same
definition can also be used with a ranked search, as we will see later. if, in addition,
relevance is assumed to be binary, then the results for a query can be summarized
as shown in table 8.3. in this table, a is the relevant set of documents for the
query, a is the non-relevant set, b is the set of retrieved documents, and b is the
set of documents that are not retrieved. the operator     gives the intersection of
two sets. for example, a     b is the set of documents that are both relevant and
retrieved.

a number of e   ectiveness measures can be defined using this table. the two

we are particularly interested in are:

recall =

p recision =

|a     b|
|a|
|a     b|
|b|

8.4 e   ectiveness metrics

309

retrieved

not retrieved

relevant
a     b
a     b

non-relevant

a     b
a     b

table 8.3. sets of documents defined by a simple search with binary relevance

where |.| gives the size of the set. in other words, recall is the proportion of rel-
evant documents that are retrieved, and precision is the proportion of retrieved
documents that are relevant. there is an implicit assumption in using these mea-
sures that the task involves retrieving as many of the relevant documents as pos-
sible and minimizing the number of non-relevant documents retrieved. in other
words, even if there are 500 relevant documents for a query, the user is interested
in finding them all.

we can also view the search results summarized in table 8.3 as the output of a
binary classifier, as was mentioned in section 7.2.1. when a document is retrieved,
it is the same as making a prediction that the document is relevant. from this per-
spective, there are two types of errors that can be made in prediction (or retrieval).
these errors are called false positives (a non-relevant document is retrieved) and
false negatives (a relevant document is not retrieved). recall is related to one type
of error (the false negatives), but precision is not related directly to the other type
of error. instead, another measure known as fallout,3 which is the proportion of
non-relevant documents that are retrieved, is related to the false positive errors:

f allout =

|a     b|
|a|

3 in the classification and signal detection literature, the errors are known as type i and
type ii errors. recall is often called the true positive rate, or sensitivity. fallout is called
the false positive rate, or the false alarm rate. another measure, specificity, is 1     fallout.
precision is known as the positive predictive value, and is often used in medical diag-
nostic tests where the id203 that a positive test is correct is particularly important.
the true positive rate and the false positive rate are used to draw roc (receiver op-
erating characteristic) curves that show the trade-o    between these two quantities as
the discrimination threshold varies. this threshold is the value at which the classifier
makes a positive prediction. in the case of search, the threshold would correspond to a
position in the document ranking. in information retrieval, recall-precision graphs are
generally used instead of roc curves.

310

8 evaluating search engines

given that fallout and recall together characterize the e   ectiveness of a search
as a classifier, why do we use precision instead? the answer is simply that precision
is more meaningful to the user of a search engine. if 20 documents were retrieved
for a query, a precision value of 0.7 means that 14 out of the 20 retrieved doc-
uments would be relevant. fallout, on the other hand, will always be very small
because there are so many non-relevant documents. if there were 1,000,000 non-
relevant documents for the query used in the precision example, fallout would be
6/1000000 = 0.000006. if precision fell to 0.5, which would be noticeable to the
user, fallout would be 0.00001. the skewed nature of the search task, where most
of the corpus is not relevant to any given query, also means that evaluating a search
engine as a classifier can lead to counterintuitive results. a search engine trained
to minimize classification errors would tend to retrieve nothing, since classifying
a document as non-relevant is always a good decision!

the f measure is an e   ectiveness measure based on recall and precision that
is used for evaluating classification performance and also in some search applica-
tions. it has the advantage of summarizing e   ectiveness in a single number. it is
defined as the harmonic mean of recall and precision, which is:

f =

1

1

2( 1

r + 1
p )

=

2rp

(r + p )

why use the harmonic mean instead of the usual arithmetic mean or average?
the harmonic mean emphasizes the importance of small values, whereas the arith-
metic mean is a   ected more by values that are unusually large (outliers). a search
result that returned nearly the entire document collection, for example, would
have a recall of 1.0 and a precision near 0. the arithmetic mean of these values
is 0.5, but the harmonic mean will be close to 0. the harmonic mean is clearly a
better summary of the e   ectiveness of this retrieved set.4

most of the retrieval models we have discussed produce ranked output. to use
recall and precision measures, retrieved sets of documents must be defined based
on the ranking. one possibility is to calculate recall and precision values at every
4 the more general form of the f measure is the weighted harmonic mean, which allows
weights reflecting the relative importance of recall and precision to be used. this mea-
sure is f = rp /(  r + (1       )p ), where    is a weight. this is often transformed
using    = 1/(  2 + 1), which gives f(cid:12) = (  2 + 1)rp /(r +   2p ). the common f
measure is in fact f1, where recall and precision have equal importance. in some eval-
uations, precision or recall is emphasized by varying the value of   . values of    > 1
emphasize recall.

8.4 e   ectiveness metrics

311

rank position. figure 8.2 shows the top ten documents of two possible rankings,
together with the recall and precision values calculated at every rank position for
a query that has six relevant documents. these rankings might correspond to, for
example, the output of di   erent retrieval algorithms or search engines.

at rank position 10 (i.e., when ten documents are retrieved), the two rankings
have the same e   ectiveness as measured by recall and precision. recall is 1.0 be-
cause all the relevant documents have been retrieved, and precision is 0.6 because
both rankings contain six relevant documents in the retrieved set of ten docu-
ments. at higher rank positions, however, the first ranking is clearly better. for
example, at rank position 4 (four documents retrieved), the first ranking has a re-
call of 0.5 (three out of six relevant documents retrieved) and a precision of 0.75
(three out of four retrieved documents are relevant). the second ranking has a
recall of 0.17 (1/6) and a precision of 0.25 (1/4).

fig. 8.2. recall and precision values for two rankings of six relevant documents

if there are a large number of relevant documents for a query, or if the relevant
documents are widely distributed in the ranking, a list of recall-precision values
for every rank position will be long and unwieldy. instead, a number of techniques
have been developed to summarize the e   ectiveness of a ranking. the first of these
is simply to calculate recall-precision values at a small number of predefined rank
positions. in fact, to compare two or more rankings for a given query, only the

recall    0.17  0.17   0.33   0.5   0.67  0.83  0.83  0.83  0.83   1.0precision     1.0  0.5    0.67  0.75   0.8   0.83  0.71  0.63  0.56   0.6recall    0.0   0.17   0.17  0.17  0.33   0.5   0.67   0.67  0.83   1.0precision     0.0    0.5    0.33  0.25  0.4    0.5   0.57    0.5    0.56   0.6= the relevant documentsranking #1ranking #2312

8 evaluating search engines

precision at the predefined rank positions needs to be calculated. if the precision
for a ranking at rank position p is higher than the precision for another ranking,
the recall will be higher as well. this can be seen by comparing the corresponding
recall-precision values in figure 8.2. this e   ectiveness measure is known as pre-
cision at rank p. there are many possible values for the rank position p, but this
measure is typically used to compare search output at the top of the ranking, since
that is what many users care about. consequently, the most common versions are
precision at 10 and precision at 20. note that if these measures are used, the im-
plicit search task has changed to finding the most relevant documents at a given
rank, rather than finding as many relevant documents as possible. di   erences in
search output further down the ranking than position 20 will not be considered.
this measure also does not distinguish between di   erences in the rankings at po-
sitions 1 to p, which may be considered important for some tasks. for example,
the two rankings in figure 8.2 will be the same when measured using precision at
10.

another method of summarizing the e   ectiveness of a ranking is to calculate
precision at fixed or standard recall levels from 0.0 to 1.0 in increments of 0.1.
each ranking is then represented using 11 numbers. this method has the advan-
tage of summarizing the e   ectiveness of the ranking of all relevant documents,
rather than just those in the top ranks. using the recall-precision values in figure
8.2 as an example, however, it is clear that values of precision at these standard re-
call levels are often not available. in this example, only the precision values at the
standard recall levels of 0.5 and 1.0 have been calculated. to obtain the precision
values at all of the standard recall levels will require interpolation.5 since standard
recall levels are used as the basis for averaging e   ectiveness across queries and gen-
erating recall-precision graphs, we will discuss interpolation in the next section.

the third method, and the most popular, is to summarize the ranking by av-
eraging the precision values from the rank positions where a relevant document
was retrieved (i.e., when recall increases). if a relevant document is not retrieved
for some reason,6 the contribution of this document to the average is 0.0. for the
first ranking in figure 8.2, the average precision is calculated as:

(1.0 + 0.67 + 0.75 + 0.8 + 0.83 + 0.6)/6 = 0.78

5 interpolation refers to any technique for calculating a new point between two existing

6 one common reason is that only a limited number of the top-ranked documents (e.g.,

data points.

1,000) are considered.

8.4 e   ectiveness metrics

313

for the second ranking, it is:

(0.5 + 0.4 + 0.5 + 0.57 + 0.56 + 0.6)/6 = 0.52

average precision has a number of advantages. it is a single number that is based
on the ranking of all the relevant documents, but the value depends heavily on the
highly ranked relevant documents. this means it is an appropriate measure for
evaluating the task of finding as many relevant documents as possible while still
reflecting the intuition that the top-ranked documents are the most important.
all three of these methods summarize the e   ectiveness of a ranking for a single
query. to provide a realistic assessment of the e   ectiveness of a retrieval algorithm,
it must be tested on a number of queries. given the potentially large set of results
from these queries, we will need a method of summarizing the performance of
the retrieval algorithm by calculating the average e   ectiveness for the entire set of
queries. in the next section, we discuss the averaging techniques that are used in
most evaluations.

8.4.2 averaging and interpolation

in the following discussion of averaging techniques, the two rankings shown in
figure 8.3 are used as a running example. these rankings come from using the
same ranking algorithm on two di   erent queries. the aim of an averaging tech-
nique is to summarize the e   ectiveness of a specific ranking algorithm across a
collection of queries. di   erent queries will often have di   erent numbers of rel-
evant documents, as is the case in this example. figure 8.3 also gives the recall-
precision values calculated for the top 10 rank positions.

given that the average precision provides a number for each ranking, the sim-
plest way to summarize the e   ectiveness of rankings from multiple queries would
be to average these numbers. this e   ectiveness measure, mean average precision,7
or map, is used in most research papers and some system evaluations.8 since
7 this sounds a lot better than average average precision!
8 in some evaluations the geometric mean of the average precision (gmap) is used in-
stead of the arithmetic mean. this measure, because it multiplies average precision val-
ues, emphasizes the impact of queries with low performance. it is defined as

n   

i=1

gm ap = exp 1
n

log api

where n is the number of queries, and api is the average precision for query i.

314

8 evaluating search engines

fig. 8.3. recall and precision values for rankings from two di   erent queries

it is based on average precision, it assumes that the user is interested in finding
many relevant documents for each query. consequently, using this measure for
comparison of retrieval algorithms or systems can require a considerable e   ort to
acquire the relevance judgments, although methods for reducing the number of
judgments required have been suggested (e.g., carterette et al., 2006).

for the example in figure 8.3, the mean average precision is calculated as fol-

lows:

average precision query 1 = (1.0 + 0.67 + 0.5 + 0.44 + 0.5)/5 = 0.62
average precision query 2 = (0.5 + 0.4 + 0.43)/3 = 0.44
mean average precision = (0.62 + 0.44)/2 = 0.53
the map measure provides a very succinct summary of the e   ectiveness of
a ranking algorithm over many queries. although this is often useful, sometimes
too much information is lost in this process. recall-precision graphs, and the ta-
bles of recall-precision values they are based on, give more detail on the e   ective-
ness of the ranking algorithm at di   erent recall levels. figure 8.4 shows the recall-
precision graph for the two queries in the example from figure 8.3. graphs for
individual queries have very di   erent shapes and are di   cult to compare. to gen-

recall   0.2    0.2   0.4    0.4    0.4    0.6    0.6    0.6    0.8    1.0precision    1.0    0.5    0.67  0.5    0.4    0.5   0.43  0.38  0.44   0.5recall   0.0   0.33   0.33  0.33  0.67  0.67  1.0    1.0    1.0    1.0precision    0.0    0.5    0.33 0.25  0.4   0.33   0.43  0.38  0.33  0.3= relevant documents for query 1ranking #1ranking #2= relevant documents for query 28.4 e   ectiveness metrics

315

fig. 8.4. recall-precision graphs for two queries

erate a recall-precision graph that summarizes e   ectiveness over all the queries,
the recall-precision values in figure 8.3 should be averaged. to simplify the aver-
aging process, the recall-precision values for each query are converted to precision
values at standard recall levels, as mentioned in the last section. the precision val-
ues for all queries at each standard recall level can then be averaged.9

the standard recall levels are 0.0 to 1.0 in increments of 0.1. to obtain pre-
cision values for each query at these recall levels, the recall-precision data points,
such as those in figure 8.3, must be interpolated. that is, we have to define a func-
tion based on those data points that has a value at each standard recall level. there
are many ways of doing interpolation, but only one method has been used in in-

   

9 this is called a macroaverage in the literature. a macroaverage computes the measure
of interest for each query and then averages these measures. a microaverage combines
all the applicable data points from every query and computes the measure from the
combined data. for example, a microaverage precision at rank 5 would be calculated
n
i=1 ri/5n, where ri is the number of relevant documents retrieved in the top five
as
documents by query i, and n is the number of queries. macroaveraging is used in most
retrieval evaluations.

00.20.40.60.8100.20.40.60.81precisionrecall316

8 evaluating search engines

formation retrieval evaluations since the 1970s. in this method, we define the pre-
cision p at any standard recall level r as

p (r) = max{p

   

        r     (r

   

, p

   

)     s}

: r

where s is the set of observed (r, p ) points. this interpolation, which defines
the precision at any recall level as the maximum precision observed in any recall-
precision point at a higher recall level, produces a step function, as shown in figure
8.5.

fig. 8.5. interpolated recall-precision graphs for two queries

because search engines are imperfect and nearly always retrieve some non-
relevant documents, precision tends to decrease with increasing recall (although
this is not always true, as is shown in figure 8.4). this interpolation method is
consistent with this observation in that it produces a function that is monoton-
ically decreasing. this means that precision values always go down (or stay the
same) with increasing recall. the interpolation also defines a precision value for
the recall level of 0.0, which would not be obvious otherwise! the general intu-
ition behind this interpolation is that the recall-precision values are defined by the

00.20.40.60.8100.20.40.60.81precisionrecall8.4 e   ectiveness metrics

317

sets of documents in the ranking with the best possible precision values. in query
1, for example, there are three sets of documents that would be the best possi-
ble for the user to look at in terms of finding the highest proportion of relevant
documents.

the average precision values at the standard recall levels are calculated by sim-
ply averaging the precision values for each query. table 8.4 shows the interpolated
precision values for the two example queries, along with the average precision val-
ues. the resulting average recall-precision graph is shown in figure 8.6.

recall
0.0
ranking 1 1.0
ranking 2 0.5
average

1.0
0.5
0.43 0.43 0.43 0.43 0.43 0.43 0.43
0.75 0.75 0.75 0.59 0.55 0.47 0.47 0.47 0.47 0.47 0.47

0.3
0.5
0.67 0.67 0.5
0.5

0.1
1.0
0.5

0.2
1.0
0.5

0.8
0.5

0.9
0.5

0.7
0.5

0.6
0.5

0.4

table 8.4. precision values at standard recall levels calculated using interpolation

fig. 8.6. average recall-precision graph using standard recall levels

(cid:1004)(cid:1004)(cid:856)(cid:1006)(cid:1004)(cid:856)(cid:1008)(cid:1004)(cid:856)(cid:1010)(cid:1004)(cid:856)(cid:1012)(cid:1005)(cid:1004)(cid:1004)(cid:856)(cid:1006)(cid:1004)(cid:856)(cid:1008)(cid:1004)(cid:856)(cid:1010)(cid:1004)(cid:856)(cid:1012)(cid:1005)(cid:87)(cid:396)(cid:286)(cid:272)(cid:349)(cid:400)(cid:349)(cid:381)(cid:374)(cid:90)(cid:286)(cid:272)(cid:258)(cid:367)(cid:367)318

8 evaluating search engines

the average recall-precision graph is plotted by simply joining the average pre-
cision points at the standard recall levels, rather than using another step function.
although this is somewhat inconsistent with the interpolation method, the inter-
mediate recall levels are never used in evaluation. when graphs are averaged over
many queries, they tend to become smoother. figure 8.7 shows a typical recall-
precision graph from a trec evaluation using 50 queries.

fig. 8.7. typical recall-precision graph for 50 queries from trec

8.4.3 focusing on the top documents

in many search applications, users tend to look at only the top part of the ranked
result list to find relevant documents. in the case of web search, this means that
many users look at just the first page or two of results. in addition, tasks such
as navigational search (chapter 7) or id53 (chapter 1) have just a
single relevant document. in these situations, recall is not an appropriate measure.
instead, the focus of an e   ectiveness measure should be on how well the search
engine does at retrieving relevant documents at very high ranks (i.e., close to the
top of the ranking).

00.20.40.60.8100.20.40.60.81precisionrecall8.4 e   ectiveness metrics

319

one measure with this property that has already been mentioned is precision
at rank p, where p in this case will typically be 10. this measure is easy to compute,
can be averaged over queries to produce a single summary number, and is readily
understandable. the major disadvantage is that it does not distinguish between
di   erent rankings of a given number of relevant documents. for example, if only
one relevant document was retrieved in the top 10, according to the precision
measure a ranking where that document is in the top position would be the same
as one where it was at rank 10. other measures have been proposed that are more
sensitive to the rank position.

the reciprocal rank measure has been used for applications where there is typ-
ically a single relevant document. it is defined as the reciprocal of the rank at
which the first relevant document is retrieved. the mean reciprocal rank (mrr)
is the average of the reciprocal ranks over a set of queries. for example, if the top
five documents retrieved for a query were dn, dr, dn, dn, dn, where dn is a non-
relevant document and dr is a relevant document, the reciprocal rank would be
1/2 = 0.5. even if more relevant documents had been retrieved, as in the rank-
ing dn, dr, dn, dr, dn, the reciprocal rank would still be 0.5. the reciprocal rank
is very sensitive to the rank position. it falls from 1.0 to 0.5 from rank 1 to 2, and
the ranking dn, dn, dn, dn, dr would have a reciprocal rank of 1/5 = 0.2. the
mrr for these two rankings would be (0.5 + 0.2)/2 = 0.35.

the discounted cumulative gain (dcg) has become a popular measure for eval-
uating web search and related applications (j  rvelin & kek  l  inen, 2002). it is
based on two assumptions:
    highly relevant documents are more useful than marginally relevant docu-

ments.

    the lower the ranked position of a relevant document (i.e., further down the
ranked list), the less useful it is for the user, since it is less likely to be examined.
these two assumptions lead to an evaluation that uses graded relevance as a
measure of the usefulness, or gain, from examining a document. the gain is accu-
mulated starting at the top of the ranking and may be reduced, or discounted, at
lower ranks. the dcg is the total gain accumulated at a particular rank p. specif-
ically, it is defined as:

p   

dcgp = rel1 +

i=2

reli
log2 i

320

8 evaluating search engines

where reli is the graded relevance level of the document retrieved at rank i. for
example, web search evaluations have been reported that used manual relevance
judgments on a six-point scale ranging from    bad    to    perfect    (i.e., 0     reli    
5). binary relevance judgments can also be used, in which case reli would be either
0 or 1.

the denominator log2 i is the discount or reduction factor that is applied to
the gain. there is no theoretical justification for using this particular discount fac-
tor, although it does provide a relatively smooth (gradual) reduction.10 by varying
the base of the logarithm, the discount can be made sharper or smoother. with
base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3. as an example, con-
sider the following ranking where each number is a relevance level on the scale
0   3 (not relevant   highly relevant):

3, 2, 3, 0, 0, 1, 2, 2, 3, 0

these numbers represent the gain at each rank. the discounted gain would be:

3, 2/1, 3/1.59, 0, 0, 1/2.59, 2/2.81, 2/3, 3/3.17, 0 =
3, 2, 1.89, 0, 0, 0.39, 0.71, 0.67, 0.95, 0

the dcg at each rank is formed by accumulating these numbers, giving:

3, 5, 6.89, 6.89, 6.89, 7.28, 7.99, 8.66, 9.61, 9.61

similar to precision at rank p, specific values of p are chosen for the evaluation,
and the dcg numbers are averaged across a set of queries. since the focus of this
measure is on the top ranks, these values are typically small, such as 5 and 10. for
this example, dcg at rank 5 is 6.89 and at rank 10 is 9.61. to facilitate averaging
across queries with di   erent numbers of relevant documents, these numbers can
be normalized by comparing the dcg at each rank with the dcg value for the
perfect ranking for that query. for example, if the previous ranking contained all

10 in some publications, dcg is defined as:

p   
(2reli     1)/log(1 + i)

dcgp =

i=1

for binary relevance judgments, the two definitions are the same, but for graded rele-
vance this definition puts a strong emphasis on retrieving highly relevant documents.
this version of the measure is used by some search engine companies and, because of
this, may become the standard.

8.4 e   ectiveness metrics

321

the relevant documents for that query, the perfect ranking would have gain values
at each rank of:

3, 3, 3, 2, 2, 2, 1, 0, 0, 0

which would give ideal dcg values of:

3, 6, 7.89, 8.89, 9.75, 10.52, 10.88, 10.88, 10.88, 10.88

normalizing the actual dcg values by dividing by the ideal values gives us the
normalized discounted cumulative gain (ndcg) values:

1, 0.83, 0.87, 0.76, 0.71, 0.69, 0.73, 0.8, 0.88, 0.88

note that the ndcg measure is     1 at any rank position. to summarize, the
ndcg for a given query can be defined as:

n dcgp =

dcgp
idcgp

where idcg is the ideal dcg value for that query.

8.4.4 using preferences

in section 8.3, we discussed how user preferences can be inferred from query logs.
preferences have been used for training ranking algorithms, and have been sug-
gested as an alternative to relevance judgments for evaluation. currently, how-
ever, there is no standard e   ectiveness measure based on preferences.

in general, two rankings described using preferences can be compared using
the kendall tau coe   cient (  ). if p is the number of preferences that agree and q
is the number that disagree, kendall   s    is defined as:

p     q
p + q

   =

this measure varies between 1 (when all preferences agree) and    1 (when they all
disagree). if preferences are derived from clickthrough data, however, only a par-
tial ranking is available. experimental evidence shows that this partial informa-
tion can be used to learn e   ective ranking algorithms, which suggests that e   ec-
tiveness can be measured this way. instead of using the complete set of preferences
to calculate p and q, a new ranking would be evaluated by comparing it to the
known set of preferences. for example, if there were 15 preferences learned from

322

8 evaluating search engines

clickthrough data, and a ranking agreed with 10 of these, the    measure would be
(10     5)/15 = 0.33. although this seems reasonable, no studies are available
that show that this e   ectiveness measure is useful for comparing systems.

for preferences derived from binary relevance judgments, the bpref11 mea-
sure has been shown to be robust with partial information and to give similar re-
sults (in terms of system comparisons) to recall-precision measures such as map.
in this measure, the number of relevant and non-relevant documents is balanced
to facilitate averaging across queries. for a query with r relevant documents, only
the first r non-relevant documents are considered. this is equivalent to using
r    r preferences (all relevant documents are preferred to all non-relevant doc-
uments). given this, the measure is defined as:

   
(1     ndr
r

dr

)

bp ref =

1
r

where dr is a relevant document and ndr gives the number of non-relevant doc-
uments (from the set of r non-relevant documents that are considered) that are
ranked higher than dr. if this is expressed in terms of preferences, ndr is actually
a method for counting the number of preferences that disagree (for binary rele-
vance judgments). since r    r is the number of preferences being considered,
an alternative definition of bpref is:

bp ref =

p

p + q

which means it is very similar to kendall   s   . the main di   erence is that bpref
varies between 0 and 1. given that bpref is a useful e   ectiveness measure, this
suggests that the same measure or    could be used with preferences associated
with graded relevance.

8.5 efficiency metrics

compared to e   ectiveness, the e   ciency of a search system seems like it should be
easier to quantify. most of what we care about can be measured automatically with
a timer instead of with costly relevance judgments. however, like e   ectiveness, it
is important to determine exactly what aspects of e   ciency we want to measure.
table 8.5 shows some of the measures that are used.
11 binary preference

8.5 e   ciency metrics

323

metric name
elapsed indexing time measures the amount of time necessary to build a docu-

description

ment index on a particular system.

query throughput
query latency

indexing processor time measures the cpu seconds used in building a document
index. this is similar to elapsed time, but does not count
time waiting for i/o or speed gains from parallelism.
number of queries processed per second.
the amount of time a user must wait after issuing a query
before receiving a response, measured in milliseconds.
this can be measured using the mean, but is often more
instructive when used with the median or a percentile
bound.

indexing temporary space amount of temporary disk space used while creating an

index size

index.
amount of storage necessary to store the index files.

table 8.5. definitions of some important e   ciency metrics

the most commonly quoted e   ciency metric is query throughput, measured
in queries processed per second. throughput numbers are comparable only for
the same collection and queries processed on the same hardware, although rough
comparisons can be made between runs on similar hardware. as a single-number
metric of e   ciency, throughput is good because it is intuitive and mirrors the
common problems we want to solve with e   ciency numbers. a real system user
will want to use throughput numbers for capacity planning, to help determine
whether more hardware is necessary to handle a particular query load. since it is
simple to measure the number of queries per second currently being issued to a
service, it is easy to determine whether a system   s query throughput is adequate to
handle the needs of an existing service.

the trouble with using throughput alone is that it does not capture latency.
latency measures the elapsed time the system takes between when the user issues
a query and when the system delivers its response. psychology research suggests
that users consider any operation that takes less than about 150 milliseconds to
be instantaneous. above that level, users react very negatively to the delay they
perceive.

this brings us back to throughput, because latency and throughput are not
orthogonal: generally we can improve throughput by increasing latency, and re-

324

8 evaluating search engines

ducing latency leads to poorer throughput. to see why this is so, think of the dif-
ference between having a personal chef and ordering food at a restaurant. the per-
sonal chef prepares your food with the lowest possible latency, since she has no
other demands on her time and focuses completely on preparing your food. un-
fortunately, the personal chef has low throughput, since her focus on you leads to
idle time when she is not completely occupied. the restaurant is a high through-
put operation with lots of chefs working on many di   erent orders simultaneously.
having many orders and many chefs leads to certain economies of scale   for in-
stance, when a single chef prepares many identical orders at the same time. note
that the chef is able to process these orders simultaneously precisely because some
latency has been added to some orders: instead of starting to cook immediately
upon receiving an order, the chef may decide to wait a few minutes to see if any-
one else orders the same thing. the result is that the chefs are able to cook food
with high throughput but at some cost in latency.

query processing works the same way. it is possible to build a system that han-
dles just one query at a time, devoting all resources to the current query, just like
the personal chef devotes all her time to a single customer. this kind of system is
low throughput, because only one query is processed at a time, which leads to idle
resources. the radical opposite approach is to process queries in large batches. the
system can then reorder the incoming queries so that queries that use common
subexpressions are evaluated at the same time, saving valuable execution time.
however, interactive users will hate waiting for their query batch to complete.

like recall and precision in e   ectiveness, low latency and high throughput
are both desirable properties of a retrieval system, but they are in conflict with
each other and cannot be maximized at the same time. in a real system, query
throughput is not a variable but a requirement: the system needs to handle every
query the users submit. the two remaining variables are latency (how long the
users will have to wait for a response) and hardware cost (how many processors
will be applied to the search problem). a common way to talk about latency is
with percentile bounds, such as    99% of all queries will complete in under 100
milliseconds.    system designers can then add hardware until this requirement is
met.

query throughput and latency are the most visible system e   ciency metrics,
but we should also consider the costs of indexing. for instance, given enough time
and space, it is possible to cache every possible query of a particular length. a
system that did this would have excellent query throughput and query latency,
but at the cost of enormous storage and indexing costs. therefore, we also need

8.6 training, testing, and statistics

325

to measure the size of the index structures and the time necessary to create them.
because indexing is often a distributed process, we need to know both the total
amount of processor time used during indexing and the elapsed time. since the
process of inversion often requires temporary storage, it is interesting to measure
the amount of temporary storage used.

8.6 training, testing, and statistics

8.6.1 significance tests

retrieval experiments generate data, such as average precision values or ndcg
values. in order to decide whether this data shows that there is a meaningful dif-
ference between two retrieval algorithms or search engines, significance tests are
needed. every significance test is based on a null hypothesis. in the case of a typical
retrieval experiment, we are comparing the value of an e   ectiveness measure for
rankings produced by two retrieval algorithms. the null hypothesis is that there
is no di   erence in e   ectiveness between the two retrieval algorithms. the alter-
native hypothesis is that there is a di   erence. in fact, given two retrieval algorithms
a and b, where a is a baseline algorithm and b is a new algorithm, we are usually
trying to show that the e   ectiveness of b is better than a, rather than simply find-
ing a di   erence. since the rankings that are compared are based on the same set of
queries for both retrieval algorithms, this is known as a matched pair experiment.
we obviously cannot conclude that b is better than a on the basis of the re-
sults of a single query, since a may be better than b on all other queries. so how
many queries do we have to look at to make a decision about which is better? if,
for example, b is better than a for 90% of 200 queries in a test collection, we
should be more confident that b is better for that e   ectiveness measure, but how
confident? significance tests allow us to quantify the confidence we have in any
judgment about e   ectiveness.

more formally, a significance test enables us to reject the null hypothesis in fa-
vor of the alternative hypothesis (i.e., show that b is better than a) on the basis of
the data from the retrieval experiments. otherwise, we say that the null hypoth-
esis cannot be rejected (i.e., b might not be better than a). as with any binary
decision process, a significance test can make two types of error. a type i error
is when the null hypothesis is rejected when it is in fact true. a type ii error is
when the null hypothesis is accepted when it is in fact false.12 significance tests

12 compare to the discussion of errors in section 8.4.1.

326

8 evaluating search engines

are often described by their power, which is the id203 that the test will reject
the null hypothesis correctly (i.e., decide that b is better than a). in other words,
a test with high power will reduce the chance of a type ii error. the power of a
test can also be increased by increasing the sample size, which in this case is the
number of queries in the experiment. increasing the number of queries will also
reduce the chance of a type i error.

the procedure for comparing two retrieval algorithms using a particular set of

queries and a significance test is as follows:
1. compute the e   ectiveness measure for every query for both rankings.
2. compute a test statistic based on a comparison of the e   ectiveness measures
for each query. the test statistic depends on the significance test, and is simply
a quantity calculated from the sample data that is used to decide whether or
not the null hypothesis should be rejected.

3. the test statistic is used to compute a p-value, which is the id203 that a
test statistic value at least that extreme could be observed if the null hypothesis
were true. small p-values suggest that the null hypothesis may be false.
4. the null hypothesis (no di   erence) is rejected in favor of the alternate hypoth-
esis (i.e., b is more e   ective than a) if the p-value is      , the significance level.
values for    are small, typically 0.05 and 0.1, to reduce the chance of a type i
error.

in other words, if the id203 of getting a specific test statistic value is very
small assuming the null hypothesis is true, we reject that hypothesis and conclude
that ranking algorithm b is more e   ective than the baseline algorithm a.

the computation of the test statistic and the corresponding p-value is usually
done using tables or standard statistical software. the significance tests discussed
here are also provided in galago.

the procedure just described is known as a one-sided or one-tailed test since
we want to establish that b is better than a. if we were just trying to establish
that there is a di   erence between b and a, it would be a two-sided or two-tailed
test, and the p-value would be doubled. the    side    or    tail    referred to is the tail
of a id203 distribution. for example, figure 8.8 shows a distribution for the
possible values of a test statistic assuming the null hypothesis. the shaded part of
the distribution is the region of rejection for a one-sided test. if a test yielded the
test statistic value x, the null hypothesis would be rejected since the id203
of getting that value or higher (the p-value) is less than the significance level of
0.05.

8.6 training, testing, and statistics

327

fig. 8.8. id203 distribution for test statistic values assuming the null hypothesis. the
shaded area is the region of rejection for a one-sided test.

the significance tests most commonly used in the evaluation of search engines
are the t-test,13 the wilcoxon signed-rank test, and the sign test. to explain these
tests, we will use the data shown in table 8.6, which shows the e   ectiveness val-
ues of the rankings produced by two retrieval algorithms for 10 queries. the values
in the table are artificial and could be average precision or ndcg, for example,
on a scale of 0   100 (instead of 0   1). the table also shows the di   erence in the ef-
fectiveness measure between algorithm b and the baseline algorithm a. the small
number of queries in this example data is not typical of a retrieval experiment.

in general, the t-test assumes that data values are sampled from normal dis-
tributions. in the case of a matched pair experiment, the assumption is that the
di   erence between the e   ectiveness values is a sample from a normal distribution.
the null hypothesis in this case is that the mean of the distribution of di   erences
is zero. the test statistic for the paired t-test is:
   
n
.

t =

b     a
  b   a

where b     a is the mean of the di   erences,   b   a is the standard deviation14
of the di   erences, and n is the size of the sample (the number of queries). for

13 also known as student   s t-test, where    student    was the pen name of the inventor,

william gosset, not the type of person who should use it.

14 for a set of data values xi, the standard deviation can be calculated by    =

      

n

i=1(xi     x)2/n, where x is the mean.

p = 0.05test statistic valuex328

8 evaluating search engines

query

1
2
3
4
5
6
7
8
9
10

a
25
43
39
75
43
15
20
52
49
50

b
35
84
15
75
68
85
80
50
58
75

b     a
10
41
-24
0
25
70
60
-2
9
25

table 8.6. artificial e   ectiveness data for two retrieval algorithms (a and b) over 10
queries. the column b     a gives the di   erence in e   ectiveness.

the data in table 8.6, b     a = 21.4,   b   a = 29.1, and t = 2.33. for a one-
tailed test, this gives a p-value of 0.02, which would be significant at a level of    =
0.05. therefore, for this data, the t-test enables us to reject the null hypothesis and
conclude that ranking algorithm b is more e   ective than a.

there are two objections that could be made to using the t-test in search eval-
uations. the first is that the assumption that the data is sampled from normal
distributions is generally not appropriate for e   ectiveness measures, although the
distribution of di   erences can resemble a normal distribution for large n. recent
experimental results have supported the validity of the t-test by showing that it
produces very similar results to the randomization test on trec data (smucker
et al., 2007). the randomization test does not assume the data comes from normal
distributions, and is the most powerful of the nonparametric tests.15 the random-
ization test, however, is much more expensive to compute than the t-test.

the second objection that could be made is concerned with the level of mea-
surement associated with e   ectiveness measures. the t-test (and the randomiza-
tion test) assume the the evaluation data is measured on an interval scale. this
means that the values can be ordered (e.g., an e   ectiveness of 54 is greater than an
e   ectiveness of 53), and that di   erences between values are meaningful (e.g., the
di   erence between 80 and 70 is the same as the di   erence between 20 and 10).
some people have argued that e   ectiveness measures are an ordinal scale, which

15 a nonparametric test makes less assumptions about the data and the underlying distri-

bution than parametric tests.

8.6 training, testing, and statistics

329

means that the magnitude of the di   erences are not significant. the wilcoxon
signed-rank test and the sign test, which are both nonparametric, make less as-
sumptions about the e   ectiveness measure. as a consequence, they do not use all
the information in the data, and it can be more di   cult to show a significant dif-
ference. in other words, if the e   ectiveness measure did satisfy the conditions for
using the t-test, the wilcoxon and sign tests have less power.

the wilcoxon signed-rank test assumes that the di   erences between the ef-
fectiveness values for algorithms a and b can be ranked, but the magnitude is
not important. this means, for example, that the di   erence for query 8 in table
8.6 will be ranked first because it is the smallest non-zero absolute value, but the
magnitude of 2 is not used directly in the test. the test statistic is:

n   

w =

ri

i=1

where ri is a signed-rank, and n is the number of di   erences   = 0. to compute
the signed-ranks, the di   erences are ordered by their absolute values (increasing),
and then assigned rank values (ties are assigned the average rank). the rank values
are then given the sign of the original di   erence. the null hypothesis for this test
is that the sum of the positive ranks will be the same as the sum of the negative
ranks.

for example, the nine non-zero di   erences from table 8.6, in rank order of

absolute value, are:

2, 9, 10, 24, 25, 25, 41, 60, 70

the corresponding signed-ranks are:

   1, +2, +3,    4, +5.5, +5.5, +7, +8, +9

summing these signed-ranks gives a value of w = 35. for a one-tailed test, this
gives a p-value of approximately 0.025, which means the null hypothesis can be
rejected at a significance level of    = 0.05.

the sign test goes further than the wilcoxon signed-ranks test, and completely
ignores the magnitude of the di   erences. the null hypothesis for this test is that
2. in other words, over a large sample we would ex-
p (b > a) = p (a > b) = 1
pect that the number of pairs where b is    better    than a would be the same as the
number of pairs where a is    better    than b. the test statistic is simply the num-
ber of pairs where b > a. the issue for a search evaluation is deciding what dif-
ference in the e   ectiveness measure is    better.    we could assume that even small

330

8 evaluating search engines

di   erences in average precision or ndcg   such as 0.51 compared to 0.5   are
significant. this has the risk of leading to a decision that algorithm b is more ef-
fective than a when the di   erence is, in fact, not noticeable to the users. instead,
an appropriate threshold for the e   ectiveness measure should be chosen. for ex-
ample, an old rule of thumb in information retrieval is that there has to be at least
5% di   erence in average precision to be noticeable (10% for a more conserva-
tive threshold). this would mean that a di   erence of 0.51     0.5 = 0.01 would
be considered a tie for the sign test. if the e   ectiveness measure was precision at
rank 10, on the other hand, any di   erence might be considered significant since
it would correspond directly to additional relevant documents in the top 10.

for the data in table 8.6, we will consider any di   erence to be significant. this
means there are seven pairs out of ten where b is better than a. the corresponding
p-value is 0.17, which is the chance of observing seven    successes    in ten trials
where the id203 of success is 0.5 (just like flipping a coin). using the sign
test, we cannot reject the null hypothesis. because so much information from the
e   ectiveness measure is discarded in the sign test, it is more di   cult to show a
di   erence, and more queries are needed to increase the power of the test. on the
other hand, it can be used in addition to the t-test to provide a more user-focused
perspective. an algorithm that is significantly more e   ective according to both
the t-test and the sign test, perhaps using di   erent e   ectiveness measures, is more
likely to be noticeably better.

8.6.2 setting parameter values

nearly every ranking algorithm has parameters that can be tuned to improve the
e   ectiveness of the results. for example, bm25 has the parameters k1, k2, and b
used in term weighting, and query likelihood with dirichlet smoothing has the
parameter   . ranking algorithms for web search can have hundreds of parameters
that give the weights for the associated features. the values of these parameters
can have a major impact on retrieval e   ectiveness, and values that give the best ef-
fectiveness for one application may not be appropriate for another application, or
even for a di   erent document collection. not only is choosing the right parameter
values important for the performance of a search engine when it is deployed, it is
an important part of comparing the e   ectiveness of two retrieval algorithms. an
algorithm that has had its parameters tuned for optimal performance for the test
collection may appear to be much more e   ective than it really is when compared
to a baseline algorithm with poor parameter values.

8.6 training, testing, and statistics

331

the appropriate method of setting parameters for both maximizing e   ective-
ness and making fair comparisons of algorithms is to use a training set and a test set
of data. the training set is used to learn the best parameter values, and the test set
is used for validating these parameter values and comparing ranking algorithms.
the training and test sets are two separate test collections of documents, queries,
and relevance judgments, although they may be created by splitting a single col-
lection. in trec experiments, for example, the training set is usually documents,
queries, and relevance judgments from previous years. when there is not a large
amount of data available, cross-validation can be done by partitioning the data into
k subsets. one subset is used for testing, and k     1 are used for training. this is
repeated using each of the subsets as a test set, and the best parameter values are
averaged across the k runs.

using training and test sets helps to avoid the problem of overfitting (men-
tioned in chapter 7), which occurs when the parameter values are tuned to fit a
particular set of data too well. if this was the only data that needed to be searched
in an application, that would be appropriate, but a much more common situa-
tion is that the training data is only a sample of the data that will be encountered
when the search engine is deployed. overfitting will result in a choice of parame-
ter values that do not generalize well to this other data. a symptom of overfitting
is that e   ectiveness on the training set improves but e   ectiveness on the test set
gets worse.

a fair comparison of two retrieval algorithms would involve getting the best
parameter values for both algorithms using the training set, and then using those
values with the test set. the e   ectiveness measures are used to tune the parameter
values in multiple retrieval runs on the training data, and for the final comparison,
which is a single retrieval run, on the test data. the    cardinal sin    of retrieval exper-
iments, which should be avoided in nearly all situations, is testing on the training
data. this typically will artificially boost the measured e   ectiveness of a retrieval
algorithm. it is particularly problematic when one algorithm has been trained in
some way using the testing data and the other has not. although it sounds like
an easy problem to avoid, it can sometimes occur in subtle ways in more complex
experiments.

given a training set of data, there a number of techniques for finding the
best parameter settings for a particular e   ectiveness measure. the most common
method is simply to explore the space of possible parameter values by brute force.
this requires a large number of retrieval runs with small variations in parameter
values (a parameter sweep). although this could be computationally infeasible for

332

8 evaluating search engines

large numbers of parameters, it is guaranteed to find the parameter settings that
give the best e   ectiveness for any given e   ectiveness measure. the ranking id166
method described in section 7.6 is an example of a more sophisticated procedure
for learning good parameter values e   ciently with large numbers of parameters.
this method, as well as similar optimization techniques, will find the best possi-
ble parameter values if the function being optimized meets certain conditions.16
because many of the e   ectiveness measures we have described do not meet these
conditions, di   erent functions are used for the optimization, and the parame-
ter values are not guaranteed to be optimal. this is, however, a very active area
of research, and new methods for learning parameters are constantly becoming
available.

8.6.3 online testing

all of the evaluation strategies described thus far have assumed that training and
testing are done o   ine. that is, we have assumed that all of the training and test
data are fixed ahead of time. however, with real search engines, it may be possible
to test (or even train) using live tra   c. this is often called online testing. for ex-
ample, suppose that you just developed a new sponsored-search advertising algo-
rithm. rather than evaluating your system using human relevance judgments, it is
possible to deploy the new ranking algorithm and observe the amount of revenue
generated using the new algorithm versus some baseline algorithm. this makes
it possible to test various search engine components, such as ranking algorithms,
query suggestion algorithms, and snippet generation algorithms, using live tra   c
and real users. notice that this is similar to logging, which was discussed earlier
in this chapter. with logging, evaluations are typically done retrospectively on
   stale    data, whereas online testing uses live data.

there are several benefits to online testing. first, it allows real users to inter-
act with the system. these interactions provide information, such as click data,
that can be used for various kinds of evaluation. second, online testing is less bi-
ased, since the evaluation is being done over a real sample of users and tra   c. this

16 specifically, the function should be convex (or concave; a function f (x) is concave if
and only if    f (x) is convex). a convex function is a continuous function that satisfies
the following constraint for all    in [0,1]:

f (  x1 + (1       )x2)       f (x1) + (1       )f (x2)

8.7 the bottom line

333

is valuable because it is often di   cult to build test collections that accurately re-
flect real search engine users and tra   c. finally, online testing can produce a large
amount of data very cheaply, since it does not require paying any humans to do
relevance judgments.

unfortunately, online testing also has its fair share of drawbacks. the primary
drawback is that the data collected is typically very noisy. there are many di   erent
reasons why users behave the way they do in an online environment. for example,
if a user does not click on a search result, it does not necessarily mean the result is
bad. the user may have clicked on an advertisement instead, lost interest, or sim-
ply gone to eat dinner. therefore, typically a very large amount of online testing
data is required to eliminate noise and produce meaningful conclusions. another
drawback to online testing is that it requires live tra   c to be altered in poten-
tially harmful ways. if the algorithm being tested is highly experimental, then it
may significantly degrade retrieval e   ectiveness and drive users away. for this rea-
son, online testing must be done very carefully, so as not to negatively a   ect the
user experience. one way of minimizing the e   ect of an online test on the general
user population is to use the experimental algorithm only for a small percentage,
such as 1% to 5%, of the live tra   c. finally, online tests typically provide only
a very specific type of data   click data. as we described earlier in this section,
click data is not always ideal for evaluating search engines, since the data is noisy
and highly biased. however, for certain search engine id74, such as
clickthrough rate17 and revenue, online testing can be very useful.

therefore, online testing can be a useful, inexpensive way of training or testing
new algorithms, especially those that can be evaluated using click data. special
care must be taken to ensure that the data collected is analyzed properly and that
the overall user experience is not degraded.

8.7 the bottom line

in this chapter, we have presented a number of e   ectiveness and e   ciency mea-
sures. at this point, it would be reasonable to ask which of them is the right mea-
sure to use. the answer, especially with regard to e   ectiveness, is that no single
measure is the correct one for any search application. instead, a search engine
should be evaluated through a combination of measures that show di   erent as-

17 the percentage of times that some item is clicked on.

334

8 evaluating search engines

pects of the system   s performance. in many settings, all of the following measures
and tests could be carried out with little additional e   ort:
    mean average precision - single number summary, popular measure, pooled

relevance judgments.

    average ndcg - single number summary for each rank level, emphasizes top
ranked documents, relevance judgments needed only to a specific rank depth
(typically to 10).

    recall-precision graph - conveys more information than a single number mea-

sure, pooled relevance judgments.

    average precision at rank 10 - emphasizes top ranked documents, easy to un-

derstand, relevance judgments limited to top 10.

using map and a recall-precision graph could require more e   ort in relevance
judgments, but this analysis could also be limited to the relevant documents found
in the top 10 for the ndcg and precision at 10 measures.

all these evaluations should be done relative to one or more baseline searches.
it generally does not make sense to do an e   ectiveness evaluation without a good
baseline, since the e   ectiveness numbers depend strongly on the particular mix
of queries and documents in the test collection. the t-test can be used as the sig-
nificance test for the average precision, ndcg, and precision at 10 measures.

all of the standard evaluation measures and significance tests are available us-

ing the evaluation program provided as part of galago.

in addition to these evaluations, it is also very useful to present a summary of
the number of queries that were improved and the number that were degraded,
relative to a baseline. figure 8.9 gives an example of this summary for a trec
run, where the query numbers are shown as a distribution over various percentage
levels of improvement for a specific evaluation measure (usually map). each bar
represents the number of queries that were better (or worse) than the baseline by
the given percentage. this provides a simple visual summary showing that many
more queries were improved than were degraded, and that the improvements were
sometimes quite substantial. by setting a threshold on the level of improvement
that constitutes    noticeable,    the sign test can be used with this data to establish
significance.

given this range of measures, both developers and users will get a better pic-
ture of where the search engine is performing well and where it may need improve-
ment. it is often necessary to look at individual queries to get a better understand-

8.7 the bottom line

335

fig. 8.9. example distribution of query e   ectiveness improvements

ing of what is causing the ranking behavior of a particular algorithm. query data
such as figure 8.9 can be helpful in identifying interesting queries.

references and further reading

despite being discussed for more than 40 years, the measurement of e   ectiveness
in search engines is still a hot topic, with many papers being published in the ma-
jor conferences every year. the chapter on evaluation in van rijsbergen (1979)
gives a good historical perspective on e   ectiveness measurement in information
retrieval. another useful general source is the trec book (voorhees & harman,
2005), which describes the test collections and evaluation procedures used and
how they evolved.

saracevic (1975) and mizzaro (1997) are the best papers for general reviews
of the critical topic of relevance. the process of obtaining relevance judgments
and the reliability of retrieval experiments are discussed in the trec book.
zobel (1998) shows that some incompleteness of relevance judgments does not
a   ect experiments, although buckley and voorhees (2004) suggest that substan-
tial incompleteness can be a problem. voorhees and buckley (2002) discuss the

152025qi0510<   100%[  100%,  75%][  75%,  50%][  50%,  25%][  25%,0%][0%,25%][25%,50%][50%,75%][75%,100%]> 100%queriespercentagegain or loss336

8 evaluating search engines

error rates associated with di   erent numbers of queries. sanderson and zobel
(2005) show how using a significance test can a   ect the reliability of compar-
isons and also compare shallow versus in-depth relevance judgments. carterette
et al. (2006) describe a technique for reducing the number of relevance judgments
required for reliable comparisons of search engines. kelly and teevan (2003) re-
view approaches to acquiring and using implicit relevance information. fox et
al. (2005) studied implicit measures of relevance in the context of web search,
and joachims et al. (2005) introduced strategies for deriving preferences based
on clickthrough data. agichtein, brill, dumais, and ragno (2006) extended this
approach and carried out more experiments introducing click distributions and
deviation, and showing that a number of features related to user behavior are use-
ful for predicting relevance.
the f measure was originally proposed by van rijsbergen (1979) in the form
of e = 1   f . he also provided a justification for the e measure in terms of mea-
surement theory, raised the issue of whether e   ectiveness measures were interval
or ordinal measures, and suggested that the sign and wilcoxon tests would be
appropriate for significance. cooper (1968) wrote an important early paper that
introduced the expected search length (esl) measure, which was the expected
number of documents that a user would have to look at to find a specified num-
ber of relevant documents. although this measure has not been widely used, it was
the ancestor of measures such as ndcg (j  rvelin & kek  l  inen, 2002) that fo-
cus on the top-ranked documents. another measure of this type that has recently
been introduced is rank-biased precision (mo   at et al., 2007).

yao (1995) provides one of the first discussions of preferences and how they
could be used to evaluate a search engine. the paper by joachims (2002b) that
showed how to train a linear feature-based retrieval model using preferences also
used kendall   s    as the e   ectiveness measure for defining the best ranking. the
recent paper by carterette and jones (2007) shows how search engines can be
evaluated using relevance information directly derived from clickthrough data,
rather than converting clickthrough to preferences.

a number of recent studies have focused on interactive information retrieval.
these studies involve a di   erent style of evaluation than the methods described in
this chapter, but are more formal than online testing. belkin (2008) describes the
challenges of evaluating interactive experiments and points to some interesting
papers on this topic.

another area related to e   ectiveness evaluation is the prediction of query e   ec-
tiveness. cronen-townsend et al. (2006) describe the clarity measure, which is

8.7 the bottom line

337

used to predict whether a ranked list for a query has good or bad precision. other
measures have been suggested that have even better correlations with average pre-
cision.

there are very few papers that discuss guidelines for e   ciency evaluations of

search engines. zobel et al. (1996) is an example from the database literature.

exercises

8.1. find three other examples of test collections in the information retrieval lit-
erature. describe them and compare their statistics in a table.
8.2. imagine that you were going to study the e   ectiveness of a search engine for
blogs. specify the retrieval task(s) for this application, and then describe the test
collection you would construct and how you would evaluate your ranking algo-
rithms.
8.3. for one query in the cacm collection (provided at the book website), gen-
erate a ranking using galago, and then calculate average precision, ndcg at 5
and 10, precision at 10, and the reciprocal rank by hand.
8.4. for two queries in the cacm collection, generate two uninterpolated recall-
precision graphs, a table of interpolated precision values at standard recall levels,
and the average interpolated recall-precision graph.
8.5. generate the mean average precision, recall-precision graph, average ndcg
at 5 and 10, and precision at 10 for the entire cacm query set.
8.6. compare the map value calculated in the previous problem to the gmap
value. which queries have the most impact on this value?
8.7. another measure that has been used in a number of evaluations is r-precision.
this is defined as the precision at r documents, where r is the number of relevant
documents for a query. it is used in situations where there is a large variation in
the number of relevant documents per query. calculate the average r-precision
for the cacm query set and compare it to the other measures.
8.8. generate another set of rankings for 10 cacm queries by adding structure
to the queries manually. compare the e   ectiveness of these queries to the simple
queries using map, ndcg, and precision at 10. check for significance using the
t-test, wilcoxon test, and the sign test.

338

8 evaluating search engines

8.9. for one query in the cacm collection, generate a ranking and calculate
bpref. show that the two formulations of bpref give the same value.

8.10. consider a test collection that contains judgments for a large number of
time-sensitive queries, such as    olympics    and    miss universe   . suppose that the
judgments for these queries were made in 2002. why is this a problem? how can
online testing be used to alleviate the problem?

9

classification and id91

   what kind of thing? i need a clear definition.   
ripley, alien

we now take a slight detour from search to look at classification and id91.
classification and id91 have many things in common with document re-
trieval. in fact, many of the techniques that proved to be useful for ranking doc-
uments can also be used for these tasks. classification and id91 algorithms
are heavily used in most modern search engines, and thus it is important to have
a basic understanding of how these techniques work and how they can be applied
to real-world problems. we focus here on providing general background knowl-
edge and a broad overview of these tasks. in addition, we provide examples of
how they can be applied in practice. it is not our goal to dive too deeply into the
details or the theory, since there are many other excellent references devoted en-
tirely to these subjects, some of which are described in the    references and future
reading    section at the end of this chapter. instead, at the end of this chapter, you
should know what classification and id91 are, the most commonly used algo-
rithms, examples of how they are applied in practice, and how they are evaluated.
on that note, we begin with a brief description of classification and id91.

classification, also referred to as categorization, is the task of automatically ap-
plying labels to data, such as emails, web pages, or images. people classify items
throughout their daily lives. it would be infeasible, however, to manually label ev-
ery page on the web according to some criteria, such as    spam    or    not spam.   
therefore, there is a need for automatic classification and categorization tech-
niques. in this chapter, we describe several classification algorithms that are ap-
plicable to a wide range of tasks, including spam detection, id31,
and applying semantic labels to web advertisements.

id91, the other topic covered in this chapter, can be broadly defined as
the task of grouping related items together. in classification, each item is assigned a

340

9 classification and id91

label, such as    spam    or    not spam.    in id91, however, each item is assigned to
one or more clusters, where the cluster does not necessarily correspond to a mean-
ingful concept, such as    spam    or    not spam.    instead, as we will describe later
in this chapter, items are grouped together according to their similarity. there-
fore, rather than mapping items onto a predefined set of labels, id91 allows
the data to    speak for itself     by uncovering the implicit structure that relates the
items.

both classification and id91 have been studied for many years by infor-
mation retrieval researchers, with the aim of improving the e   ectiveness, or in
some cases the e   ciency, of search applications. from another perspective, these
two tasks are classic machine learning problems. in machine learning, the learning
algorithms are typically characterized as supervised or unsupervised. in supervised
learning, a model is learned using a set of fully labeled items, which is often called
the training set. once a model is learned, it can be applied to a set of unlabeled
items, called the test set, in order to automatically apply labels. classification is
often cast as a supervised learning problem. for example, given a set of emails
that have been labeled as    spam    or    not spam    (the training set), a classification
model can be learned. the model then can be applied to incoming emails in order
to classify them as    spam    or    not spam   .

unsupervised learning algorithms, on the other hand, learn entirely based on
unlabeled data. unsupervised learning tasks are often posed di   erently than su-
pervised learning tasks, since the input data is not mapped to a predefined set of
labels. id91 is the most common example of unsupervised learning. as we
will show, id91 algorithms take a set of unlabeled data as input and then
group the items using some notion of similarity.

there are many other types of learning paradigms beyond supervised and un-
supervised, such as semi-supervised learning, active learning, and online learning.
however, these subjects are well beyond the scope of this book. instead, in this
chapter, we provide an overview of basic yet e   ective classification and id91
algorithms and methods for evaluating them.

9.1 classification and categorization

applying labels to observations is a very natural task, and something that most of
us do, often without much thought, in our everyday lives. for example, consider
a trip to the local grocery store. we often implicitly assign labels such as    ripe    or
   not ripe,       healthy    or    not healthy,    and    cheap    or    expensive    to the groceries

9.1 classification and categorization

341

that we see. these are examples of binary labels, since there are only two options
for each. it is also possible to apply multivalued labels to foods, such as    starch,   
   meat,       vegetable,    or    fruit.    another possible labeling scheme would arrange
categories into a hierarchy, in which the    vegetable    category would be split by
color into subcategories, such as    green,       red,    and    yellow.    under this scheme,
foods would be labeled according to their position within the hierarchy. these
di   erent labeling or categorization schemes, which include binary, multivalued,
and hierarchical, are called ontologies (see chapter 6).

it is important to choose an ontology that is appropriate for the underlying
task. for example, for detecting whether or not an email is spam, it is perfectly
reasonable to choose a label set that consists of    spam    and    not spam   . however,
if one were to design a classifier to automatically detect what language a web page
is written in, then the set of all possible languages would be a more reasonable
ontology. typically, the correct choice of ontology is dictated by the problem,
but in cases when it is not, it is important to choose a set of labels that is expres-
sive enough to be useful for the underlying task. however, since classification is
a supervised learning task, it is important not to construct an overly complex on-
tology, since most learning algorithms will fail (i.e., not generalize well to unseen
data) when there is little or no data associated with one or more of the labels. in
the web page language classifier example, if we had only one example page for
each of the asian languages, then, rather than having separate labels for each of
the languages, such as    chinese   ,    korean   , etc., it would be better to combine
all of the languages into a single label called    asian languages   . the classifier will
then be more likely to classify things as    asian languages    correctly, since it has
more training examples.

in order to understand how machine learning algorithms work, we must first
take a look at how people classify items. returning to the grocery store example,
consider how we would classify a food as    healthy    or    not healthy.    in order to
make this classification, we would probably look at the amount of saturated fat,
cholesterol, sugar, and sodium in the food. if these values, either separately or in
combination, are above some threshold, then we would label the food    healthy   
or    unhealthy.    to summarize, as humans we classify items by first identifying a
number of important features that will help us distinguish between the possible
labels. we then extract these features from each item. we then combine evidence
from the extracted features in some way. finally, we classify the item using some
decision mechanism based on the combined evidence.

342

9 classification and id91

in our example, the features are things such as the amount of saturated fat and
the amount of cholesterol. the features are extracted by reading the nutritional
information printed on the packaging or by performing laboratory tests. there
are various ways to combine the evidence in order to quantify the    healthiness   
(denoted h) of the food, but one simple way is to weight the importance of each
feature and then add the weighted feature values together, such as:

h(f ood)     wf atf at(f ood) + wcholchol(f ood) +

wsugarsugar(f ood) + wsodiumsodium(f ood)

where wf at, wchol, etc., are the weights associated with each feature. of course, in
this case, it is likely that each of the weights would be negative.

once we have a healthiness score, h, for a given food, we must apply some
decision mechanism in order to apply a    healthy    or    not healthy    label to the
food. again, there are various ways of doing this, but one of the most simple is
to apply a simple threshold rule that says    a food is healthy if h(f ood)     t    for
some threshold value t.

although this is an idealized model of how people classify items, it provides
valuable insights into how a computer can be used to automatically classify items.
indeed, the two classification algorithms that we will now describe follow the
same steps as we outlined earlier. the only di   erence between the two algorithms
is in the details of how each step is actually implemented.

9.1.1 na  ve bayes

we are now ready to describe how items can be automatically classified. one of the
most straightforward yet e   ective classification techniques is called na  ve bayes.
we introduced the bayes classifier in chapter 7 as a framework for a probabilistic
retrieval model. in that case, there were just two classes of interest, the relevant
class and the non-relevant class. in general, classification tasks can involve more
than two labels or classes. in that situation, bayes    rule, which is the basis of a
bayes classifier, states that:

p (c|d) =

=

p (d|c)p (c)
   

p (d)

p (d|c)p (c)

c   c p (d|c = c)p (c = c)

9.1 classification and categorization

343

where c and d are random variables. random variables are commonly used when
modeling uncertainty. such variables do not have a fixed (deterministic) value.
instead, the value of the variable is random. every random variable has a set of
possible outcomes associated with it, as well as a id203 distribution over the
outcomes. as an example, the outcome of a coin toss can be modeled as a random
variable x. the possible outcomes of the random variable are    heads    (h) and
   tails    (t). given a fair coin, the id203 associated with both the heads out-
come and the tails outcome is 0.5. therefore, p (x = h) = p (x = t) = 0.5.
consider another example, where you have the algebraic expression y =
10 + 2x. if x was a deterministic variable, then y would be deterministic as
well. that is, for a fixed x, y would always evaluate to the same value. how-
ever, if x is a random variable, then y is also a random variable. suppose that x
had possible outcomes    1 (with id203 0.1), 0 (with id203 0.25), and
1 (with id203 0.65). the possible outcomes for y would then be 8, 10, and
12, with p (y = 8) = 0.1, p (y = 10) = 0.25, and p (y = 12) = 0.65.

in this chapter, we denote random variables with capital letters (e.g., c, d)
and outcomes of random variables as lowercase letters (e.g., c, d). furthermore, we
denote the entire set of outcomes with caligraphic letters (e.g., c, d). finally, for
notational convenience, instead of writing p (x = x), we write p (x). similarly
for conditional probabilities, rather than writing p (x = x|y = y), we write
p (x|y).
bayes    rule is important because it allows us to write a id155
(such as p (c|d)) in terms of the    reverse    conditional (p (d|c)). this is a very
powerful theorem, because it is often easy to estimate or compute the conditional
id203 in one direction but not the other. for example, consider spam classifi-
cation, where d represents a document   s text and c represents the class label (e.g.,
   spam    or    not spam   ). it is not immediately clear how to write a program that
detects whether a document is spam; that program is represented by p (c|d).
however, it is easy to find examples of documents that are and are not spam. it is
possible to come up with estimates for p (d|c) given examples or training data.
the magic of bayes    rule is that it tells us how to get what we want (p (c|d)),
but may not immediately know how to estimate, from something we do know
how to estimate (p (d|c)).

it is straightforward to use this rule to classify items if we let c be the ran-
dom variable associated with observing a class label and let d be the random
variable associated with observing a document, as in our spam example. given

344

9 classification and id91

a document1 d (an outcome of random variable d) and a set of classes c =
c1, . . . , cn (outcomes of the random variable c), we can use bayes    rule to com-
pute p (c1|d), . . . , p (cn|d), which computes the likelihood of observing class
label ci given that document d was observed. document d can then be labeled
with the class with the highest id203 of being observed given the document.
that is, na  ve bayes classifies a document d as follows:

class(d) = arg max

c   c p (c|d)
   

p (d|c)p (c)
c   c p (d|c)p (c)

= arg max
c   c

where arg maxc   c p (c|d) means    return the class c, out of the set of all possible
classes c, that maximizes p (c|d).    this is a mathematical way of saying that we
are trying to find the most likely class c given the document d.
instead of computing p (c|d) directly, we can compute p (d|c) and p (c) in-
stead and then apply bayes    rule to obtain p (c|d). as we explained before, one
reason for using bayes    rule is when it is easier to estimate the probabilities of
one conditional, but not the other. we now explain how these values are typically
estimated in practice.

we first describe how to estimate the class prior, p (c). the estimation is

straightforward. it is estimated according to:

p (c) =

nc
n

where nc is the number of training instances that have label c, and n is the total
number of training instances. therefore, p (c) is simply the proportion of training
instances that have label c.
estimating p (d|c) is a little more complicated because the same    counting   
estimate that we were able to use for estimating p (c) would not work. (why? see
exercise 9.3.) in order to make the estimation feasible, we must impose the sim-
plifying assumption that d can be represented as d = w1, . . . , wn and that wi
is independent of wj for every i   = j. simply stated, this says that document d
1 throughout most of this chapter, we assume that the items being classified are textual
documents. however, it is important to note that the techniques described here can
be used in a more general setting and applied to non-textual items such as images and
videos.

9.1 classification and categorization

345

can be factored into a set of elements (terms) and that the elements (terms) are
independent of each other.2 this assumption is the reason for calling the classi-
fier na  ve, because it requires documents to be represented in an overly simpli-
fied way. in reality, terms are not independent of each other. however, as we will
show in chapter 11, properly modeling term dependencies is possible, but typi-
cally more di   cult. despite the independence assumption, the na  ve bayes clas-
sifier has been shown to be robust and highly e   ective for various classification
tasks.

this na  ve independence assumption allows us to invoke a classic result from
id203 that states that the joint id203 of a set of (conditionally) inde-
pendent random variables can be written as the product of the individual condi-
tional probabilities. that means that p (d|c) can be written as:

n   

i=1

p (d|c) =

p (wi|c)

therefore, we must estimate p (w|c) for every possible term w in the vocabulary
v and class c in the ontology c. it turns out that this is a much easier task than
estimating p (d|c) since there is a finite number of terms in the vocabulary and a
finite number of classes, but an infinite number of possible documents. the inde-
pendence assumption allows us to write the id203 p (c|d) as:

   
   

p (c|d) =

=

p (d|c)p (c)
   v
c   c p (d|c)p (c)
   v
i=1 p (wi|c)p (c)
i=1 p (wi|c)p (c)

c   c

the only thing left to describe is how to estimate p (w|c). before we can esti-
mate the id203, we must first decide on what the id203 actually means.
for example, p (w|c) could be interpreted as    the id203 that term w is re-
lated to class c,       the id203 that w has nothing to do with class c,    or any
number of other things. in order to make the meaning concrete, we must explic-
itly define the event space that the id203 is defined over. an event space is the

2 this is the same assumption that lies at the heart of most of the retrieval models de-
scribed in chapter 7. it is also equivalent to the bag of words assumption discussed in
chapter 11.

346

9 classification and id91

set of possible events (or outcomes) from some process. a id203 is assigned
to each event in the event space, and the sum of the probabilities over all of the
events in the event space must equal one.

the id203 estimates and the resulting classification will vary depending
on the choice of event space. we will now briefly describe two of the more popular
event spaces and show how p (w|c) is estimated in each.

multiple-bernoulli model

the first event space that we describe is very simple. given a class c, we define a
binary random variable wi for every term in the vocabulary. the outcome for the
binary event is either 0 or 1. the id203 p (wi = 1|c) can then be interpreted
as    the id203 that term wi is generated by class c.    conversely, p (wi = 0|c)
can be interpreted as    the id203 that term wi is not generated by class c.    this
is exactly the event space used by the binary independence model (see chapter 7),
and is known as the multiple-bernoulli event space.

under this event space, for each term in some class c, we estimate the prob-
ability that the term is generated by the class. for example, in a spam classifier,
p (cheap = 1|spam) is likely to have a high id203, whereas p (dinner =
1|spam) is going to have a much lower id203.

document id cheap buy banking dinner the

class

1
2
3
4
5
6
7
8
9
10

0
1
0
1
1
0
0
0
0
1

0
0
0
0
1
0
1
0
0
1

0
1
0
1
0
1
1
0
0
0

0
0
0
0
0
0
0
0
0
1

spam

spam
spam

1 not spam
1
1 not spam
1
1
1 not spam
1 not spam
1 not spam
1 not spam
1 not spam

fig. 9.1. illustration of how documents are represented in the multiple-bernoulli event
space. in this example, there are 10 documents (each with a unique id), two classes (spam
and not spam), and a vocabulary that consists of the terms    cheap   ,    buy   ,    banking   ,    din-
ner   , and    the   .

9.1 classification and categorization

347

figure 9.1 shows how a set of training documents can be represented in this
event space. in the example, there are 10 documents, two classes (spam and not
spam), and a vocabulary that consists of the terms    cheap   ,    buy   ,    banking   ,    din-
ner   , and    the   . in this example, p (spam) = 3
10. next,
we must estimate p (w|c) for every pair of terms and classes. the most straight-
forward way is to estimate the probabilities using what is called the maximum
likelihood estimate, which is:

10 and p (not spam) = 7

p (w|c) =

dfw;c
nc

where dfw;c is the number of training documents with class label c in which term
w occurs, and nc is the total number of training documents with class label c.
as we see, the maximum likelihood estimate is nothing more than the propor-
tion of documents in class c that contain term w. using the maximum likelihood
estimate, we can easily compute p (the|spam) = 1, p (the|not spam) = 1,
p (dinner|spam) = 0, p (dinner|not spam) = 1
using the multiple-bernoulli model, the document likelihood, p (d|c), can be

7, and so on.

written as:

p (d|c) =

p (w|c)(cid:14)(w;d) (1     p (w|c))1   (cid:14)(w;d)

   

w   v

where   (w, d) is 1 if and only if term w occurs in document d.

in practice, it is not possible to use the maximum likelihood estimate because
of the zero id203 problem. in order to illustrate the zero id203 problem,
let us return to the spam classification example from figure 9.1. suppose that we
receive a spam email that happens to contain the term    dinner   . no matter what
other terms the email does or does not contain, the id203 p (d|c) will always
be zero because p (dinner|spam) = 0 and the term occurs in the document
(i.e.,   dinner;d = 1). therefore, any document that contains the term    dinner   
will automatically have zero id203 of being spam. this problem is more gen-
eral, since a zero id203 will result whenever a document contains a term that
never occurs in one or more classes. the problem here is that the maximum likeli-
hood estimate is based on counting occurrences in the training set. however, the
training set is finite, so not every possible event is observed. this is known as data
sparseness. sparseness is often a problem with small training sets, but it can also
happen with relatively large data sets. therefore, we must alter the estimates in
such a way that all terms, including those that have not been observed for a given

348

9 classification and id91

class, are given some id203 mass. that is, we must ensure that p (w|c) is non-
zero for all terms in v. by doing so, we will avoid all of the problems associated
with the zero id203 problem.

as was described in chapter 7, smoothing is a useful technique for overcoming
the zero id203 problem. one popular smoothing technique is often called
bayesian smoothing, which assumes some prior id203 over models and uses a
maximum a posteriori estimate. the resulting smoothed estimate for the multiple-
bernoulli model has the form:

p (w|c) =

dfw;c +   w

nc +   w +   w

where   w and   w are parameters that depend on w. di   erent settings of these
parameters result in di   erent estimates. one popular choice is to set   w = 1 and
  w = 0 for all w, which results in the following estimate:

p (w|c) =

dfw;c + 1
nc + 1

n and   w =   (1    nw
another choice is to set   w =    nw
n ) for all w, where nw is
the total number of training documents in which term w occurs, and    is a single
tunable parameter. this results in the following estimate:

p (w|c) =

dfw;c +    nw
n

nc +   

this event space only captures whether or not the term is generated; it fails
to capture how many times the term occurs, which can be an important piece of
information. we will now describe an event space that takes term frequency into
account.

multinomial model

the binary event space of the multiple-bernoulli model is overly simplistic, as it
does not model the number of times that a term occurs in a document. term fre-
quency has been shown to be an important feature for retrieval and classifica-
tion, especially when used on long documents. when documents are very short,
it is unlikely that many terms will occur more than one time, and therefore the
multiple-bernoulli model will be an accurate model. however, more often than

9.1 classification and categorization

349

not, real collections contain documents that are both short and long, and there-
fore it is important to take term frequency and, subsequently, document length
into account.

the multinomial event space is very similar to the multiple-bernoulli event
space, except rather than assuming that term occurrences are binary (   term oc-
curs    or    term does not occur   ), it assumes that terms occur zero or more times
(   term occurs zero times   ,    term occurs one time   , etc.).

document id cheap buy banking dinner the

class

1
2
3
4
5
6
7
8
9
10

0
3
0
2
5
0
0
0
0
1

0
0
0
0
2
0
1
0
0
1

0
1
0
3
0
1
1
0
0
0

0
0
0
0
0
0
0
0
0
1

spam

spam
spam

2 not spam
1
1 not spam
2
1
1 not spam
1 not spam
1 not spam
1 not spam
2 not spam

fig. 9.2. illustration of how documents are represented in the multinomial event space.
in this example, there are 10 documents (each with a unique id), two classes (spam and
not spam), and a vocabulary that consists of the terms    cheap   ,    buy   ,    banking   ,    dinner   ,
and    the   .

figure 9.2 shows how the documents from our spam classification example
are represented in the multinomial event space. the only di   erence between this
representation and the multiple-bernoulli representation is that the events are no
longer binary. the maximum likelihood estimate for the multinomial model is
very similar to the multiple-bernoulli model. it is computed as:

p (w|c) =

tfw;c
|c|

where tfw;c is the number of times that term w occurs in class c in the training set,
and|c| is the total number of terms that occur in training documents with class la-
20, p (the|not spam) =
bel c. in the spam classification example, p (the|spam) = 4
15, p (dinner|spam) = 0, and p (dinner|not spam) = 1
15.

9

350

9 classification and id91

since terms are now distributed according to a multinomial distribution, the

likelihood of a document d given a class c is computed according to:

tfw1;d, tfw2;d, . . . , tfwv;d

p (w|c)tfw;d

)

   

!

w   v

   
p (d|c) = p (|d|)

   

w   v

(
p (w|c)tfw;d
(

where tfw;d is the number of times that term w occurs in document d, |d| is the
total number of terms that occur in d, p (|d|) is the id203 of generating
a document of length |d|, and
! is the multinomial
coe   cient.3 notice that p (|d|) and the multinomial coe   cient are document-
dependent and, for the purposes of classification, can be ignored.

tfw1;d, tfw2;d, . . . , tfwv;d

the bayesian smoothed estimates of the term likelihoods are computed ac-

)

cording to:

   

p (w|c) =

tfw;c +   w
|c| +

w   v   w

where   w is a parameter that depends on w. as with the multiple-bernoulli
model, di   erent settings of the smoothing parameters result in di   erent types
of estimates. setting   w = 1 for all w is one possible option. this results in the
following estimate:

p (w|c) =

tfw;c + 1
|c| + |v|

another popular choice is to set   w =    cfw|c| , where cfw is the total number of
times that term w occurs in any training document, |c| is the total number of
terms in all training documents, and   , as before, is a tunable parameter. under
this setting, we obtain the following estimate:

p (w|c) =

tfw;c +    cfw|c|

|c| +   

this estimate may look familiar, as it is exactly the dirichlet smoothed language
modeling estimate that was described in chapter 7.

3 the multinomial coe   cient is a generalization of the binomial coe   cient. it is com-
puted as (n1, n2, . . . , nk)! =
n1!n2!      nk!. it counts the total number of unique
ways that

i ni items (terms) can be arranged given that item i occurs ni times.

n !

   

9.1 classification and categorization

351

in practice, the multinomial model has been shown to consistently outper-
form the multiple-bernoulli model. implementing a classifier based on either of
these models is straightforward. training consists of computing simple term oc-
currence statistics. in most cases, these statistics can be stored in memory, which
means that classification can be done e   ciently. the simplicity of the model, com-
bined with good accuracy, makes the na  ve bayes classifier a popular and attrac-
tive choice as a general-purpose classification algorithm.

9.1.2 support vector machines

unlike the na  ve bayes classifier, which is based purely on probabilistic princi-
ples, the next classifier we describe is based on geometric principles. support vec-
tor machines, often called id166s, treat inputs such as documents as points in some
geometric space. for simplicity, we first describe how id166s are applied to classi-
fication problems with binary class labels, which we will refer to as the    positive   
and    negative    classes. in this setting, the goal of id166s is to find a hyperplane4
that separates the positive examples from the negative examples.

in the na  ve bayes model, documents were treated as binary vectors in the
multiple-bernoulli model and as term frequency vectors in the multinomial case.
id166s provide more flexibility in terms of how documents can be represented.
with id166s, rather than defining some underlying event space, we must instead
define a set of feature functions f1(  ), . . . , fn (  ) that take a document as input and
produce what is known as a feature value. given a document d, the document is
represented in an n-dimensional space by the vector xd = [f1(d), . . . , fn (d)].
given a set of training data, we can use the feature functions to embed the training
documents in this n-dimensional space. notice that di   erent feature functions
will result in di   erent embeddings. since id166s find a hyperplane that separates
the data according to classes, it is important to choose feature functions that will
help discriminate between the di   erent classes.

two common feature functions are fw(d) =   (w, d) and fw(d) = tfw;d. the
first feature function is 1 if term w occurs in d, which is analogous to the multiple-
bernoulli model. the second feature function counts the number of times that w
occurs in d, which is analogous to the multinomial model. notice that these fea-
ture functions are indexed by w, which means that there is a total of|v| such func-
tions. this results in documents being embedded in a |v|-dimensional space. it is
also possible to define similar feature functions over bigrams or trigrams, which

4 a hyperplane generalizes the notion of a plane to n-dimensional space.

352

9 classification and id91

would cause the dimensionality of the feature space to explode. furthermore,
other information can be encoded in the feature functions, such as the document
length, the number of sentences in the document, the last time the document was
updated, and so on.

fig. 9.3. data set that consists of two classes (pluses and minuses). the data set on the left
is linearly separable, whereas the one on the right is not.

now that we have a mechanism for representing documents in an n-dimen-
sional space, we describe how id166s actually classify the points in this space. as
described before, the goal of id166s is to find a hyperplane that separates the neg-
ative and positive examples. the hyperplane is learned from the training data. an
unseen test point is classified according to which side of the hyperplane the point
falls on. for example, if the point falls on the negative side, then we classify it as
negative. similarly, if it falls on the positive side, then it is classified as positive. it
is not always possible to draw a hyperplane that perfectly separates the negative
training data from the positive training data, however, since no such hyperplane
may exist for some embedding of the training data. for example, in figure 9.3, it
is possible to draw a line (hyperplane) that separates the positive class (denoted
by    +   ) from the negative class (denoted by          ) in the left panel. however, it is
impossible to do so in the right panel. the points in the left panel are said to be
linearly separable, since we can draw a linear hyperplane that separates the points.
it is much easier to define and find a good hyperplane when the data is linearly
separable. therefore, we begin our explanation of how id166s work by focusing
on this special case. we will then extend our discussion to the more general and
common case where the data points are not linearly separable.

9.1 classification and categorization

353

case 1: linearly separable data

suppose that you were given a linearly separable training set, such as the one in
figure 9.3, and were asked to find the optimal hyperplane that separates the data
points. how would you proceed? you would very likely first ask what exactly is
meant by optimal. one might first postulate that optimal means any hyperplane
that separates the positive training data from the negative training data. however,
we must also consider the ultimate goal of any classification algorithm, which is
to generalize well to unseen data. if a classifier can perfectly classify the training
data but completely fails at classifying the test set data, then it is of little value.
this scenario is known as overfitting.

fig. 9.4. graphical illustration of support vector machines for the linearly separable case.
here, the hyperplane defined by w is shown, as well as the margin, the decision regions,
and the support vectors, which are indicated by circles.

in order to avoid overfitting, id166s choose the hyperplane that maximizes the
separation between the positive and negative data points. this selection criteria
makes sense intuitively, and is backed up by strong theoretical results as well. as-
suming that our hyperplane is defined by the vector w, we want to find the w that
separates the positive and negative training data and maximizes the separation be-

++++++++                           +   +w  x>0w  x<0w  x=0w  x=1w  x=  1354

9 classification and id91

tween the data points. the maximal separation is defined as follows. suppose that
    is the closest negative training point to the hyperplane and that x+ is the clos-
x
est positive training point to the hyperplane.5 then, we define the margin as the
    to the hyperplane plus the distance from x+ to the hyperplane.
distance from x
figure 9.4 shows a graphical illustration of the margin, with respect to the hy-
   ). the margin can be computed
perplane and the support vectors (i.e., x+ and x
using simple vector mathematics as follows:
|w    x

margin(w) =

   | + |w    x+|
||w||

where    is the dot product (inner product) between two vectors, and ||w|| =
(w    w)1/2 is the length of the vector w. the id166 algorithm   s notion of an opti-
mal hyperplane, therefore, is the hyperplane w that maximizes the margin while
still separating the data. in order to simplify things, it is typically assumed that
w    x
=    1 and w    x+ = 1. these assumptions, which do not change the
   
solution to the problem, result in the margin being equal to 2||w||. an alternative
yet equivalent formulation is to find the hyperplane w that solves the following
optimization problem:

minimize:
subject to:

||w||2

1
2

w    xi     1    i s.t. class(i) = +
w    xi        1    i s.t. class(i) =    

this formulation is often used because it is easier to solve. in fact, this optimization
problem can be solved using a technique called quadratic programming, the details
of which are beyond the scope of this book. however, there many excellent open
source id166 packages available. in the    references and further reading    section
at the end of this chapter we provide pointers to several such software packages.
once the best w has been found, an unseen document d can be classified using

the following rule:

{

class(d) =

+ if w    xd > 0
    otherwise

   

and x+ are known as support vectors. the optimal hyperplane w is a
5 the vectors x
linear combination of these vectors. therefore, they provide thesupport for the decision
boundary. this is the origin of the name    support vector machine.   

9.1 classification and categorization

355

therefore, the rule classifies documents based on which side of the hyperplane the
document   s feature vector is on. referring back to figure 9.4, we see that in this
example, those points to the left of the hyperplane are classified as positive exam-
ples and those to the right of the hyperplane are classified as negative examples.

case 2: non-linearly separable data

very few real-world data sets are actually linearly separable. therefore, the id166
formulation just described must be modified in order to account for this. this can
be achieved by adding a penalty factor to the problem that accounts for training
instances that do not satisfy the constraints of the linearly separable formulation.
suppose that, for some training point x in the positive class, w    x =    0.5.
this violates the constraint w    x     1. in fact, x falls on the entirely wrong side
of the hyperplane. since the target for w    x is (at least) 1, we can apply a linear
penalty based on the di   erence between the target and actual value. that is, the
penalty given to x is 1     (   0.5) = 1.5. if w    x = 1.25, then no penalty would
be assigned, since the constraint would not be violated. this type of penalty is
known as the hinge id168. it is formally defined as:

{

l(x) =

max(1     w    x, 0) if class(i) = +
max(1 + w    x, 0) if class(i) =    

this id168 is incorporated into the id166 optimization as follows:

   

minimize: 1
subject to:

2

||w||2 + c
n
i=1   i
   i s.t. class(i) = +
w    xi     1       i
w    xi        1 +   i    i s.t. class(i) =    
   i

  i     0

where   i is known as a slack variable that allows the target values to be violated.
the slack variables enforce the hinge id168. notice that if all of the con-
straints are satisfied, all of the slack variables would be equal to 0, and therefore
the id168 would reduce to the linearly separable case. in addition, if any
constraint is violated, then the amount by which it is violated is added into the
objective function and multiplied by c, which is a free parameter that controls
how much to penalize constraint violations. it is standard to set c equal to 1. this

356

9 classification and id91

optimization problem finds a hyperplane that maximizes the margin while allow-
ing for some slack. as in the linearly separable case, this optimization problem can
be solved using quadratic programming. in addition, classification is performed
in the same way as the linearly separable case.

the kernel trick

the example in figure 9.3 illustrates the fact that certain embeddings of the train-
ing data are not linearly separable. it may be possible, however, that a transforma-
tion or mapping of the data into a higher dimensional space results in a set of
linearly separable points. this may result in improved classification e   ectiveness,
although it is not guaranteed.

there are many ways to map an n-dimensional vector into a higher dimen-
sional space. for example, given the vector [f1(d), . . . , fn (d)], one could aug-
ment the vector by including squared feature values. that is, the data items would
now be represented by the 2n-dimensional vector:

[

]

f1(d), . . . , fn (d), f1(d)2, . . . , fn (d)2

the higher the dimensionality of the feature vectors, however, the less e   cient the
algorithm becomes, both in terms of space requirements and computation time.
one important thing to notice is that the key mathematical operation involved
in training and testing id166s is the dot product. if there was an e   cient way to
compute the dot product between two very high-dimensional vectors without
having to store them, then it would be feasible to perform such a mapping. in
fact, this is possible for certain classes of high-dimensional mappings. this can be
achieved by using a id81. a id81 takes two n-dimensional
vectors and computes a dot product between them in a higher dimensional space.
this higher dimensional space is implicit, in that the higher dimensional vectors
are never actually constructed.
tors w = [w1w2] and x = [x1x2]. furthermore, we define (cid:8)(  ) as follows:

let us now consider an example. suppose that we have two 2-dimensional vec-

      

       x2

1   
2x1x2
x2
2

(cid:8)(x) =

here, (cid:8)(  ) maps 2-dimensional vectors into 3-dimensional vectors. as we de-
scribed before, this may be useful because the original inputs may actually be lin-
early separable in the 3-dimensional space to which (cid:8)(  ) maps the points. one can

9.1 classification and categorization

357

imagine many, many ways of mapping the original inputs into higher dimensional
spaces. however, as we will now show, certain mappings have very nice properties
that allow us to e   ciently compute dot products in the higher dimensional space.
given this mapping, the na  ve way to compute (cid:8)(w)    (cid:8)(x) would be to first
explicitly construct (cid:8)(w) and (cid:8)(x) and then perform the dot product in the 3-
dimensional space. however, surprisingly, it turns out that this is not necessary,
since:

(cid:8)(w)    (cid:8)(x) = w2
1x2

= (w    x)2

1 + 2w1w2x1x2 + w2

2x2
2

where w    x is computed in the original, 2-dimensional space. therefore, rather
than explicitly computing the dot product in the higher 3-dimensional space, we
only need to compute the dot product in the original 2-dimensional space and
then square the value. this    trick,    which is often referred to as the kernel trick,
allows us to e   ciently compute dot products in some higher dimensional space.
of course, the example given here is rather trivial. the true power of the ker-
nel trick becomes more apparent when dealing with mappings that project into
much higher dimensional spaces. in fact, some kernels perform a dot product in
an infinite dimensional space!

kernel type

linear

value

k(x1, x2) = x1    x2
k(x1, x2) = (x1    x2)p

polynomial
gaussian k(x1, x2) = exp   ||x1     x2||2/2  2

implicit dimension

(

)

n

n + p     1

n

infinite

table 9.1. a list of kernels that are typically used with id166s. for each kernel, the name,
value, and implicit dimensionality are given.

a list of the most widely used kernels is given in table 9.1. note that the gaus-
sian kernel is also often called a radial basis function (rbf) kernel. the best choice
of kernel depends on the geometry of the embedded data. each of these kernels
has been shown to be e   ective on textual features, although the gaussian kernel
tends to work well across a wide range of data sets, as long as the variance (  2) is
properly set. most standard id166 software packages have these kernels built in, so
using them is typically as easy as specifying a command-line argument. therefore,

358

9 classification and id91

given their potential power and their ease of use, it is often valuable to experiment
with each of the kernels to determine the best one to use for a specific data set and
task.

the availability of these software packages, together with the id166   s flexibil-
ity in representation and, most importantly, their demonstrated e   ectiveness in
many applications, has resulted in id166s being very widely used in classification
applications.

non-binary classification

up until this point, our discussion has focused solely on how support vector ma-
chines can be used for binary classification tasks. we will now describe two of the
most popular ways to turn a binary classifier, such as a support vector machine,
into a multi-class classifier. these approaches are relatively simple to implement
and have been shown to be work e   ectively.
the first technique is called the one versus all (ova) approach. suppose that
we have a k     2 class classification problem. the ova approach works by train-
ing k classifiers. when training the kth classifier, the kth class is treated as the
positive class and all of the other classes are treated as the negative class. that is,
each classifier treats the instances of a single class as the positive class, and the
remaining instances are the negative class. given a test instance x, it is classified
using all k classifiers. the class for x is the (positive) class associated with the
classifier that yields the largest value of w    x. that is, if wc is the    class c versus
not class c    classifier, then items are classified according to:

class(x) = arg max

c

wc    x

the other technique is called the one versus one (ovo) approach. in the ovo
approach, a binary classifier is trained for every unique pair of classes. for example,
for a ternary classification problem with the labels    excellent   ,    fair   , and    bad   , it
would be necessary to train the following classifiers:    excellent versus fair   ,    ex-
cellent versus bad   , and    fair versus bad   . in general, the ovo approach requires
k(k   1)
classifiers to be trained, which can be computationally expensive for large
data sets and large values of k. to classify a test instance x, it is run through each
of the classifiers. each time x is classified as c, a vote for c is recorded. the class
that has the most votes at the end is then assigned to x.

both the ova and ovo approaches work well in practice. there is no con-
crete evidence that suggests that either should be preferred over the other. instead,

2

9.1 classification and categorization

359

the e   ectiveness of the approaches largely depends on the underlying character-
istics of the data set.

9.1.3 evaluation

most classification tasks are evaluated using standard information retrieval met-
rics, such as accuracy,6 precision, recall, the f measure, and roc curve analysis.
each of these metrics were described in detail in chapter 8. of these metrics, the
most commonly used are accuracy and the f measure.

there are two major di   erences between evaluating classification tasks and
other retrieval tasks. the first di   erence is that the notion of    relevant    is replaced
with    is classified correctly.    the other major di   erence is that microaveraging,
which is not commonly used to evaluate retrieval tasks, is widely used in classi-
fication evaluations. macroaveraging for classification tasks involves computing
some metric for each class and then computing the average of the per-class met-
rics. on the other hand, microaveraging computes a metric for every test instance
(document) and then averages over all such instances. it is often valuable to com-
pute and analyze both the microaverage and the macroaverage, especially when
the class distribution p (c) is highly skewed.

9.1.4 classifier and feature selection

up until this point we have covered the basics of two popular classifiers. we have
described the principles the classifiers are built upon, their underlying assump-
tions, the pros and cons, and how they can be used in practice. as classification is
a deeply complex and rich subject, we cover advanced classification topics in this
section that may be of interest to those who would like a deeper or more complete
understanding of the topic.

generative, discriminative, and non-parametric models

the na  ve bayes classifier was based on probabilistic modeling. the model re-
quires us to assume that documents are generated from class labels according to a
probabilistic model that corresponds to some underlying event space. the na  ve
bayes classifier is an example of a wider class of probabilistic models called gener-
ative models. these models assume that some underlying id203 distribution

6 accuracy is another name for precision at rank 1.

360

9 classification and id91

generates both documents and classes. in the na  ve bayes case, the classes and
documents are generated as follows. first, a class is generated according to p (c).
then, a document is generated according to p (d|c). this process is summarized
in figure 9.5. generative models tend to appeal to intuition by mimicking how
people may actually generate (write) documents.

fig. 9.5. generative process used by the na  ve bayes model. first, a class is chosen accord-
ing to p (c), and then a document is chosen according to p (d|c).

of course, the accuracy of generative models largely depends on how accu-
rately the probabilistic model captures this generation process. if the model is a
reasonable reflection of the actual generation process, then generative models can
be very powerful, especially when there are very few training examples.

as the number of training examples grows, however, the power of the gener-
ative model can be limited by simplifying distributional assumptions, such as the
independence assumption in the na  ve bayes classifier. in such cases, discrimina-
tive models often outperform generative models. discriminative models are those
that do not model the generative process of documents and classes. instead, they
directly model the class assignment problem given a document as input. in this
way, they discriminate between class labels. since these models do not need to
model the generation of documents, they often have fewer distributional assump-
tions, which is one reason why they are often preferred to generative models when

class 1class2class 3class 2generate class according to p(c)generate document according to p(d|c)9.1 classification and categorization

361

there are many training examples. support vector machines are an example of a
discriminative model. notice that no assumptions about the document genera-
tion process are made anywhere in the id166 formulation. instead, id166s directly
learn a hyperplane that e   ectively discriminates between the classes.

fig. 9.6. example data set where non-parametric learning algorithms, such as a nearest
neighbor classifier, may outperform parametric algorithms. the pluses and minuses indi-
cate positive and negative training examples, respectively. the solid gray line shows the
actual decision boundary, which is highly non-linear.

non-parametric classifiers are another option when there is a large number
of training examples. non-parametric classifiers let the data    speak for itself    
by eliminating all distributional assumptions. one simple example of a non-
parametric classifier is the nearest neighbor classifier. given an unseen example,
the nearest neighbor classifier finds the training example that is nearest (accord-
ing to some distance metric) to it. the unseen example is then assigned the label
of this nearest neighbor. figure 9.6 shows an example output of a nearest neigh-
bor classifier. notice the irregular, highly non-linear decision boundary induced
by the classifier. generative and discriminative models, even id166s with a non-

   +++++++++++++++++++++++++++++++                                                                                                +   362

9 classification and id91

linear kernel, would have a di   cult time fitting a model to this data. for this rea-
son, the nearest neighbor classifier is optimal as the number of training examples
approaches infinity. however, the classifier tends to have a very high variance for
smaller data sets, which often limits its applicability.

feature selection

the id166 classifier embeds inputs, such as documents, into some feature space
that is defined by a set of feature functions. as we described, it is common to de-
fine one (or more) feature functions for every word in the vocabulary. this |v|-
dimensional feature space can be extremely large, especially for very large vocabu-
laries. since the feature set size a   ects both the e   ciency and e   ectiveness of the
classifier, researchers have devised techniques for pruning the feature space. these
are known as feature selection techniques.

the goal of feature selection is to find a small subset of the original features
that can be used in place of the original feature set with the aim of significantly
improving e   ciency (in terms of storage and time) while not hurting e   ective-
ness much. in practice, it turns out that feature selection techniques often improve
e   ectiveness instead of reducing it. the reason for this is that some of the features
eliminated during feature selection may be noisy or inaccurate, and therefore hin-
der the ability of the classification model to learn a good model.

information gain is one of the most widely used feature selection criteria for
text classification applications. information gain is based on id205
principles. as its name implies, it measures how much information about the
class labels is gained when we observe the value of some feature. let us return
to the spam classification example in figure 9.1. observing the value of the fea-
ture    cheap    provides us quite a bit of information with regard to the class labels.
if    cheap    occurs, then it is very likely that the label is    spam   , and if    cheap    does
not occur, then it is very likely that the label is    not spam   . in id205,
id178 is the expected information contained in some distribution, such as the
class distribution p (c). therefore, the information gain of some feature f mea-
sures how the id178 of p (c) changes after we observe f. assuming a multiple-
bernoulli event space, it is computed as follows:

ig(w) = h(c)     h(c|w)

=    

p (c) log p (c) +

   

c   c

   

   

w   {0;1}

p (w)

c   c

p (c|w) log p (c|w)

9.1 classification and categorization

363

where h(c) is the id178 of p (c) and h(c|w) is known as the conditional
id178. as an illustrative example, we compute the information gain for the term
   cheap    from our spam classification example:

ig(cheap) =    p (spam) log p (spam)     p (spam) log p (spam) +

p (cheap)p (spam|cheap) log p (spam|cheap) +
p (cheap)p (spam|cheap) log p (spam|cheap) +
p (cheap)p (spam|cheap) log p (spam|cheap) +
p (cheap)p (spam|cheap) log p (spam|cheap)

    7
10

=     3
10
4
10

+

log 3
10
   1
log 1
4
4

+

log 7
4
+
10
10
   0
log 0
6
6
6
10

+

   3
4
6
10

log 3
4
   6
log 6
6
6

= 0.2749

where p (cheap) is shorthand for p (cheap = 0), p (spam) means p (c =
not spam), and it is assumed that 0 log 0 = 0. the corresponding information
gains for    buy   ,    banking   ,    dinner   , and    the    are 0.0008, 0.0434, 0.3612, and 0.0,
respectively. therefore, according to the information gain,    dinner    is the most
informative word, since it is a perfect predictor of    not spam    according to the
training set. on the opposite side of the spectrum,    the    is the worst predictor,
since it appears in every document and therefore has no discriminative power.

similar information gain measures can be derived for other event spaces, such
as the multinomial event space. there are many di   erent ways to use the infor-
mation gain to actually select features. however, the most common thing to do is
to select the k features with the largest information gain and train a model using
only those features. it is also possible to select a percentage of all features or use a
threshold.

although many other feature selection criteria exist, information gain tends to
be a good general-purpose feature selection criteria, especially for text-based clas-
sification problems. we provide pointers to several other feature selection tech-
niques in the    references and further reading    section at the end of this chapter.

364

9 classification and id91

9.1.5 spam, sentiment, and online advertising

although ranking functions are a very critical part of any search engine, classifica-
tion and categorization techniques also play an important role in various search-
related tasks. in this section, we describe several real-world text classification ap-
plications. these applications are spam detection, sentiment classification, and on-
line advertisement classification.

spam, spam, spam

classification techniques can be used to help detect and eliminate various types of
spam. spam is broadly defined to be any content that is generated for malevolent
purposes,7 such as unsolicited advertisements, deceptively increasing the ranking
of a web page, or spreading a virus. one important characteristic of spam is that it
tends to have little, if any, useful content. this definition of spam is very subjective,
because what may be useful to one person may not be useful to another. for this
reason, it is often di   cult to come up with an objective definition of spam.

there are many types of spam, including email spam, advertisement spam, blog
spam, and web page spam. spammers use di   erent techniques for di   erent types
of spam. therefore, there is no one single spam classification technique that works
for all types of spam. instead, very specialized spam classifiers are built for the
di   erent types of spam, each taking into account domain-specific information.
much has been written about email spam, and filtering programs such as spam-
assassin8 are in common use. figure 9.7 shows the spamassassin output for an
example email. spamassassin computes a score for the email that is compared to
a threshold (default value 5.0) to determine whether it is spam. the score is based
on a combination of features, one of the most important of which is the output of
a bayes classifier. in this case, the url contained in the body of the email was on
a blacklist, the timestamp on the email is later than the time it was received, and
the bayes classifier gives the email a 40   60% chance of being in the class    spam   
based on the words in the message. these three features did not, however, give the
email a score over 5, so it was not classified as spam (which is a mistake).

7 the etymology of the word spam, with respect to computer abuse, is quite interesting.
the meaning is believed to have been derived from a 1970 monty python skit set in a
restaurant where everything on the menu has spam (the meat product) in it. a chorus
of vikings begins singing a song that goes,    spam, spam, spam, spam, ...    on and on,
therefore tying the word spam to repetitive, annoying behavior.

8 http://spamassassin.apache.org/

9.1 classification and categorization

365

fig. 9.7. example output of spamassassin email spam filter

since this book focuses on search engines, we will devote our attention to web
page spam, which is one of the most di   cult and widespread types of spam. de-
tecting web page spam is a di   cult task, because spammers are becoming increas-
ingly sophisticated. it seems sometimes that the spammers themselves have ad-
vanced degrees in information retrieval! there are many di   erent ways that spam-
mers target web pages. gy  ngyi and garcia-molina (2005) proposed a web spam
taxonomy that attempts to categorize the di   erent web page spam techniques that
are often used to artificially increase the ranking of a page. the two top-level cat-
egories of the taxonomy are link spam and term spam.

with link spam, spammers use various techniques to artificially increase the
link-based scores of their web pages. in particular, search engines often use mea-
sures such as inlink count and id95, which are based entirely on the link
structure of the web, for measuring the importance of a web page. however, these
techniques are susceptible to spam. one popular and easy link spam technique
involves posting links to the target web page on blogs or unmoderated message
boards. another way for a website to artificially increase its link-based score is to
join a link exchange network. link exchange networks are large networks of web-

to:      from:      subject: non profit debt x  spam  checked: this message probably not spam x  spam  score: 3.853, required: 5 x  spam  level: *** (3.853) x  spam  tests: bayes_50,date_in_future_06_12,uribl_black x  spam  report  rig:          start spamassassin (v2.6xx  cscf) results   2.0 uribl_black            contains an url listed in the uribl blacklist                              [uris: bad  debtyh.net.cn]   1.9 date_in_future_06_12   date: is 6 to 12 hours after received: date   0.0 bayes_50               body: bayesian spam id203 is 40 to 60%                              [score: 0.4857]  say good bye to debt acceptable unsecured debt includes all major credit cards, no  collateral bank loans, personal loans,  medical bills etc. http://www.bad  debtyh.net.cn  366

9 classification and id91

sites that all connect to each other, thereby increasing the number of links coming
into the site. another link spam technique is called link farming. link farms are
similar to exchange networks, except the spammer himself buys a large number of
domains, creates a large number of sites, and then links them all together. there
are various other approaches, but these account for a large fraction of link spam.
a number of alternatives to id95 have been proposed recently that attempt
to dampen the potential e   ect of link spam, including hosttrust (gy  ngyi et al.,
2004) and spamrank (bencz  r et al., 2005).

the other top-level category of spam is term spam. term spam attempts to
modify the textual representation of the document in order to make it more likely
to be retrieved for certain queries or keywords. as with link-based scores, term-
based scores are also susceptible to spam. most of the widely used retrieval mod-
els, including bm25 and id38, make use of some formulation that
involves term frequency and document frequency. therefore, by increasing the
term frequency of target terms, these models can easily be tricked into retrieving
non-relevant documents. furthermore, most web ranking functions match text
in the incoming anchor text and the url. modifying the url to match a given
term or phrase is easy. however, modifying the incoming anchor text requires
more e   ort, but can easily be done using link exchanges and link farms. another
technique, called dumping, fills documents with many unrelated words (often an
entire dictionary). this results in the document being retrieved for just about any
query, since it contains almost every combination of query terms. therefore, this
acts as a recall enhancing measure. this can be combined with the other spamming
techniques, such as repetition, in order to have high precision as well as high re-
call. phrase stitching (combining words and sentences from various sources) and
weaving (adding spam terms into a valid source such as a news story) are other
techniques for generating artificial content. all of these types of term spam should
be considered when developing a ranking function designed to prevent spam.

figure 9.8 shows an example of a web page containing spam. the page contains
both term spam with repetition of important words and link spam where related
spam sites are mentioned.

as should be apparent by now, there is an overwhelming number of types of
spam. here, we simply focused on web page spam and did not even start to con-
sider the other types of spam. indeed, it would be easy to write an entire book on
subjects related to spam. however, before we end our spam discussion, we will
describe just one of the many di   erent ways that classification has been used to
tackle the problem of detecting web page spam.

9.1 classification and categorization

367

fig. 9.8. example of web page spam, showing the main page and some of the associated
term and link spam

pro football sportsbooksnflfootball line online nflsportsbooksnflfootball gambling odds online pro nflbetting pro nflgambling online nflfootball spreads offshore football gambling online nflgamblibgspreads online football gambling line online nflbetting nflsportsbookonline onlinenflbetting spreads betting nflfootball online onlinefootball wagering online gambling online gambling football online nflfootball betting odds offshore football sportsbookonline nflfootball gambling    mvp sportsbookfootball gambling beverly hills football sportsbookplayers sb football wagering popular poker football odds virtual bookmaker football lines v wager football spreads bogartscasino football point spreads gecko casino online football betting jackpot hour online football gambling mvp casino online football wagering toucan casino nfl betting popular poker nfl gambling all tracks nfl wagering bet jockey nfl odds live horse betting nfl lines mvp racebooknfl point spreadspopular poker nfl spreads bogartspoker nfl sportsbook   website:term spam:link spam:betting nfl football pro football sportsbooks nfl football line online nfl sportsbooks nfl  players super book  when it comes to secure nfl betting and finding  the best football lines players super book is the best option! sign up and ask for 30 % in bonuses.  mvp sportsbook  football betting has never been so easy and secure! mvp sportsbook has all the nfl odds you are looking for.  sign up now and ask for up to  30 % in cash bonuses.  368

9 classification and id91

ntoulas et al. (2006) propose a method for detecting web page spam using
content (textual) analysis. the method extracts a large number of features from
each web page and uses these features in a classifier. some of the features include
the number of words on the page, number of words in the title, average length of
the words, amount of incoming anchor text, and the fraction of visible text. these
features attempt to capture very basic characteristics of a web page   s text. another
feature used is the compressibility of the page, which measures how much the
page can be reduced in size using a compression algorithm. it turns out that pages
that can be compressed more are much more likely to be spam, since pages that
contain many repeated terms and phrases are easier to compress. this has also been
shown to be e   ective for detecting email spam. the authors also use as features the
fraction of terms drawn from globally popular words9 and the fraction of globally
popular words appearing on the page. these features attempt to capture whether
or not the page has been filled with popular terms that are highly likely to match
a query term. the last two features are based on id165 likelihoods. experiments
show that pages that contain very rare and very common id165s are more likely
to be spam than those pages that contain id165s of average likelihood. all of
these features were used with a decision tree learning algorithm, which is another
type of supervised classification algorithm, and shown to achieve classification
accuracy well above 90%. the same features could easily be used in a na  ve bayes
or support vector machine classifier.

sentiment

as we described in chapter 6, there are three primary types of web queries. the
models described in chapter 7 focus primarily on informational and navigational
queries. transactional queries, the third type, present many di   erent challenges.
if a user queries for a product name, then the search engine should display a vari-
ety of information that goes beyond the standard ranked list of topically relevant
results. for example, if the user is interested in purchasing the product, then links
to online shopping sites can be provided to help the user complete her purchase.
it may also be possible that the user already owns the product and is searching for
accessories or enhancements. the search engine could then derive revenue from
the query by displaying advertisements for related accessories and services.

9 the list of    globally popular    words in this experiment was simply the n most frequent

words in the test corpus.

9.1 classification and categorization

369

another possible scenario, and the one that we focus on in detail here, is that
the user is researching the product in order to determine whether he should pur-
chase it. in this case, it would be valuable to retrieve information such as product
specifications, product reviews, and blog postings about the product. in order to
reduce the amount of information that the user needs to read through, it would be
preferable to have the system automatically aggregate all of the reviews and blog
posts in order to present a condensed, summarized view.

there are a number of steps involved with building such a system, each of
which involves some form of classification. first, when crawling and indexing
sites, the system has to automatically classify whether or not a web page contains
a review or if it is a blog posting expressing an opinion about a product. the task
of identifying opinionated text, as opposed to factual text, is called opinion de-
tection. after a collection of reviews and blog postings has been populated, an-
other classifier must be used to extract product names and their corresponding
reviews. this is the information extraction task. for each review identified for a
given product, yet another classifier must be used to determine the sentiment of
the page. typically, the sentiment of a page is either    negative    or    positive   , al-
though the classifier may choose to assign a numeric score as well, such as    two
stars    or    four stars   . finally, all of the data, including the sentiment, must be ag-
gregated and presented to the user in some meaningful way. figure 9.9 shows part
of an automatically generated product review from a web service. this sentiment-
based summary of various aspects of the product, such as    ease of use   ,    size   , and
   software   , is generated from individual user reviews.

rather than go into the details of all of these di   erent classifiers, we will focus
our attention on how sentiment classifiers work. as with our previous examples,
let us consider how a person would identify the sentiment of some piece of text.
for a majority of cases, we use vocabulary clues in order to determine the senti-
ment. for example, a positive digital camera review would likely contain words
such as    great   ,    nice   , and    amazing   . on the other hand, negative reviews would
contain words such as    awful   ,    terrible   , and    bad   . this suggests one possible so-
lution to the problem, where we build two lists. the first list will contain words
that are indicative of positive sentiment, and another list will contain words in-
dicative of negative sentiment. then, given a piece of text, we could simply count
the number of positive words and the number of negative words. if there are more
positive words, then assign the text a positive sentiment label. otherwise, label it
as having negative sentiment. even though this approach is perfectly reasonable, it
turns out that people are not very good at creating lists of words that indicate pos-

370

9 classification and id91

fig. 9.9. example product review incorporating sentiment

itive and negative sentiment. this is largely due to the fact that human language
is ambiguous and largely dependent on context. for example, the text    the digital
camera lacks the amazing picture quality promised    would likely be classified as
having positive sentiment because it contains two positive words (   amazing    and
   quality   ) and only one negative word (   lacks   ).

pang et al. (2002) proposed using machine learning techniques for sentiment
classification. various classifiers were explored, including na  ve bayes, support
vector machines, and maximum id178, which is another popular classification
technique. the features used in the classifiers were unigrams, bigrams, part-of-
speech tags, adjectives, and the position of a term within a piece of text. the au-
thors report that an id166 classifier using only unigram features exhibited the best
performance, resulting in more accurate results than a classifier trained using all of
the features. in addition, it was observed that the multiple-bernoulli event space
outperformed the multinomial event space for this particular task. this is likely
caused by the fact that most sentiment-related terms occur only once in any piece
of text, and therefore term frequency adds very little to the model. interestingly,
the machine learning models were significantly more accurate than the baseline
model that used human-generated word lists. the id166 classifier with unigrams

all user reviews general comments (148 comments)                                                                  82% positive ease of use (108 comments)                                                                 78% positive screen (92 comments)                                                                 97% positive software (78 comments)                                                                 35% positive sound quality (59 comments)                                                                 89% positive size (59 comments)                                                                76% positive 9.1 classification and categorization

371

had an accuracy of over 80%, whereas the baseline model had an accuracy of only
around 60%.

classifying advertisements

as described in chapter 6, sponsored search and content match are two di   er-
ent advertising models widely used by commercial search engines. the former
matches advertisements to queries, whereas the latter matches advertisements to
web pages. both sponsored search and content match use a pay per click pricing
model, which means that advertisers must pay the search engine only if a user
clicks on the advertisement. a user may click on an advertisement for a number
of reasons. clearly, if the advertisement is    topically relevant,    which is the stan-
dard notion of relevance discussed in the rest of this book, then the user may click
on it. however, this is not the only reason why a user may click. if a user searches
for    tropical fish   , she may click on advertisements for pet stores, local aquariums,
or even scuba diving lessons. it is less likely, however, that she would click on ad-
vertisements for fishing, fish restaurants, or mercury poisoning. the reason for
this is that the concept    tropical fish    has a certain semantic scope that limits the
type of advertisements a user may find interesting.

although it is possible to use standard information retrieval techniques such
as id183 or query reformulation analysis to find these semantic matches
for advertising, it is also possible to use a classifier that maps queries (and web
pages) into semantic classes. broder et al. (2007) propose a simple yet e   ective
technique for classifying textual items, such as queries and web pages, into a se-
mantic hierarchy. the hierarchy was manually constructed and consists of over
6,000 nodes, where each node represents a single semantic class. as one moves
deeper down the hierarchy, the classes become more specific. human judges man-
ually placed thousands of queries with commercial intent into the hierarchy based
on each query   s intended semantic meaning.

given such a hierarchy and thousands of labeled instances, there are many pos-
sible ways to classify unseen queries or web pages. for example, one could learn a
na  ve bayes model or use id166s. since there are over 6,000 classes, however, there
could be data sparsity issues, with certain classes having very few labeled instances
associated with them. a bigger problem, however, would be the e   ciency of this
approach. both na  ve bayes and id166s would be very slow to classify an item into
one of 6,000 possible classes. since queries must be classified in real time, this is
not an option. instead, broder et al. propose using cosine similarity with tf.idf

372

9 classification and id91

weighting to match queries (or web pages) to semantic classes. that is, they frame
the classification problem as a retrieval problem, where the query is the query (or
web page) to be classified and the document set consists of 6,000    documents   ,
one for each semantic class. for example, for the semantic class    sports   , the    doc-
ument    for it would consists of all of the queries labeled as    sports   . these    doc-
uments    are stored in an inverted index, which allows for e   cient retrieval for
an incoming query (or web page). this can be viewed as an example of a nearest
neighbor classifier.

fig. 9.10. example semantic class match between a web page about rainbow fish (a type of
tropical fish) and an advertisement for tropical fish food. the nodes    aquariums   ,    fish   ,
and    supplies    are example nodes within a semantic hierarchy. the web page is classified
as    aquariums - fish    and the ad is classified as    supplies - fish   . here,    aquariums    is the
least common ancestor. although the web page and ad do not share any terms in common,
they can be matched because of their semantic similarity.

rainbow fish resourcesweb pageadfishaquariumssuppliesdiscount tropical fish foodfeed your tropical fish a gourmet diet for just pennies a day!www.cheapfishfood.com9.2 id91

373

to use such a classifier in practice, one would have to preclassify every adver-
tisement in the advertising inventory. then, when a new query (or web page) ar-
rives, it is classified. there are a number of ways to use the semantic classes to
improve matching. obviously, if the semantic class of a query exactly matches the
semantic class of an advertisement, it should be given a high score. however, there
are other cases where two things may be very closely related, even though they do
not have exactly the same semantic class. therefore, broder et al. propose a way of
measuring the distance between two semantic classes within the hierarchy based
on the inverse of the least common ancestor of the two nodes in the hierarchy. a
common ancestor is a node in the hierarchy that you must pass through in order
to reach both nodes. the least common ancestor is the one with the maximum
depth in the hierarchy. the distance is 0 if the two nodes are the same and very
large if the the least common ancestor is the root node. figure 9.10 shows an ex-
ample of how a web page can be semantically matched to an advertisement using
the hierarchy. in the figure, the least common ancestor of the web page and ad
classes is    aquariums   , which is one node up the hierarchy. therefore, this match
would be given a lower score than if both the web page and ad were classified into
the same node in the hierarchy. the full advertisement score can be computed by
combining this distance based on the hierarchy with the standard cosine similar-
ity score. in this way, advertisements are ranked in terms of both topical relevance
and semantic relevance.

9.2 id91

id91 algorithms provide a di   erent approach to organizing data. unlike the
classification algorithms covered in this chapter, id91 algorithms are based
on unsupervised learning, which means that they do not require any training data.
id91 algorithms take a set of unlabeled instances and group (cluster) them
together. one problem with id91 is that it is often an ill-defined problem.
classification has very clear objectives. however, the notion of a good id91
is often defined very subjectively.

in order to gain more perspective on the issues involved with id91, let us
examine how we, as humans, cluster items. suppose, once again, that you are at a
grocery store and are asked to cluster all of the fresh produce (fruits and vegeta-
bles). how would you proceed? before you began, you would have to decide what
criteria you would use for id91. for example, you could group the items by
their color, their shape, their vitamin c content, their price, or some meaningful

374

9 classification and id91

combination of these factors. as with classification, the id91 criteria largely
depend on how the items are represented. input instances are assumed to be a fea-
ture vector that represents some object, such as a document (or a fruit). if you are
interested in id91 according to some property, it is important to make sure
that property is represented in the feature vector.

after the id91 criteria have been determined, you would have to deter-
mine how you would assign items to clusters. suppose that you decided to cluster
the produce according to color and you have created a red cluster (red grapes, red
apples) and a yellow cluster (bananas, butternut squash). what do you do if you
come across an orange? do you create a new orange cluster, or do you assign it to
the red or yellow cluster? these are important questions that id91 algorithms
must address as well. these questions come in the form of how many clusters to use
and how to assign items to clusters.

finally, after you have assigned all of the produce to clusters, how do you quan-
tify how well you did? that is, you must evaluate the id91. this is often very
di   cult, although there have been several automatic techniques proposed.

in this example, we have described clusters as being defined by some fixed set
of properties, such as the    red    cluster. this is, in fact, a very specific form of clus-
ter, called monothetic. we discussed monothetic classes or clusters in chapter 6,
and mentioned that most id91 algorithms instead produce polythetic clus-
ters, where members of a cluster share many properties, but there is no single defin-
ing property. in other words, membership in a cluster is typically based on the
similarity of the feature vectors that represent the objects. this means that a cru-
cial part of defining the id91 algorithm is specifying the similarity measure
that is used. the classification and id91 literature often refers to a distance
measure, rather than a similarity measure, and we use that terminology in the fol-
lowing discussion. any similarity measure, which typically has a value s from 0 to
1, can be converted into a distance measure by using 1     s. many similarity and
distance measures have been studied by information retrieval and machine learn-
ing researchers, from very simple measures such as dice   s coe   cient (mentioned
in chapter 6) to more complex probabilistic measures.

the reader should keep these factors in mind while reading this section, as they
will be recurring themes throughout. the remainder of this section describes three
id91 algorithms based on di   erent approaches, discusses evaluation issues,
and briefly describes id91 applications.

9.2 id91

375

9.2.1 hierarchical and id116 id91

we will now describe two di   erent id91 algorithms that start with some ini-
tial id91 of the data and then iteratively try to improve the id91 by op-
timizing some objective function. the main di   erence between the algorithms is
the objective function. as we will show, di   erent objective functions lead to dif-
ferent types of clusters. therefore, there is no one    best    id91 algorithm. the
choice of algorithm largely depends on properties of the data set and task.

throughout the remainder of this section, we assume that our goal is to cluster
some set of n instances (which could be web pages, for example), represented as
feature vectors, into k clusters, where k is a constant that is fixed a priori.

hierarchical id91

hierarchical id91 is a id91 methodology that builds clusters in a hier-
archical fashion. this methodology gives rise to a number of di   erent id91
algorithms. these algorithms are often    clustered    into two groups, depending on
how the algorithm proceeds.

divisive id91 algorithms begin with a single cluster that consists of all of
the instances. during each iteration it chooses an existing cluster and divides it
into two (or possibly more) clusters. this process is repeated until there are a total
of k clusters. the output of the algorithm largely depends on how clusters are
chosen and split.

divisive id91 is a top-down approach. the other general type of hierar-
chical id91 algorithm is called agglomerative id91, which is a bottom-up
approach. figures 9.11 and 9.12 illustrate the di   erence between the two types
of algorithms. an agglomerative algorithm starts with each input as a separate
cluster. that is, it begins with n clusters, each of which contains a single input.
the algorithm then proceeds by joining two (or possibly more) existing clusters
to form a new cluster. therefore, the number of clusters decreases after each itera-
tion. the algorithm terminates when there are k clusters. as with divisive cluster-
ing, the output of the algorithm is largely dependent on how clusters are chosen
and joined.

the hierarchy generated by an agglomerative or divisive id91 algorithm
can be conveniently visualized using a dendrogram.10 a dendrogram graphically
represents how a hierarchical id91 algorithm progresses. figure 9.13 shows

10 from the greek word dendron, meaning    tree.   

376

9 classification and id91

fig. 9.11. example of divisive id91 with k = 4. the id91 proceeds from left
to right and top to bottom, resulting in four clusters.

the dendrogram that corresponds to generating the entire agglomerative cluster-
ing hierarchy for the points in figure 9.12. in the dendrogram, points d and e
are first combined to form a new cluster called h. then, b and c are combined to
form cluster i. this process is continued until a single cluster m is created, which
consists of a, b, c, d, e, and f. in a dendrogram, the height at which instances
combine is significant and represents the similarity (or distance) value at which
the combination occurs. for example, the dendrogram shows that d and e are
the most similar pair.

algorithm 1 is a simple implementation of hierarchical agglomerative clus-
tering.11 the algorithm takes n vectors x1, . . . , xn, representing the instances,
and the desired number of clusters k as input. the array (vector) a is the assign-
ment vector. it is used to keep track of which cluster each input is associated with.
if a[i] = j, then it means that input xi is in cluster j. the algorithm considers
joining every pair of clusters. for each pair of clusters (ci, cj), a cost c(ci, cj)
is computed. the cost is some measure of how expensive it would be to merge

11 often called hac in the literature.

facebgdfacebgdfacebgdfacebgd9.2 id91

377

fig. 9.12. example of agglomerative id91 with k = 4. the id91 proceeds
from left to right and top to bottom, resulting in four clusters.

fig. 9.13. dendrogram that illustrates the agglomerative id91 of the points from
figure 9.12

facebgdfacebgdfacebgdfacebgdadebid18hijklm378

9 classification and id91

for j     ids     {i} do

a[1], . . . , a[n ]     1, . . . , n
ids     {1, . . . , n}
for c = n to k do
bestcost        
bestclustera     undefined
bestclusterb     undefined
for i     ids do

algorithm 1 agglomerative id91
1: procedure a                                    c                  (x1, . . . , xn, k)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end procedure

ci;j     cost (ci, cj)
if ci;j < bestcost then
bestcost     ci;j
bestclustera     i
bestclusterb     j

end for
ids     ids     {bestclustera}
for i = 1 to n do

if a[i] is equal to bestclustera then

a[i]     bestclusterb

end for

end for

end for

end if

end if

clusters ci and cj. we will return to how the cost is computed shortly. after all
pairwise costs have been computed, the pair of clusters with the lowest cost are
then merged. the algorithm proceeds until there are k clusters.

as shown by algorithm 1, agglomerative id91 largely depends on the cost
function. there are many di   erent ways to define the cost function, each of which
results in the final clusters having di   erent characteristics. we now describe a few
of the more popular choices and the intuition behind them.

single linkage measures the cost between clusters ci and cj by computing the
distance between every instance in cluster ci and every one in cj. the cost is then
the minimum of these distances, which can be stated mathematically as:

cost (ci, cj) = min{dist(xi, xj)|xi     ci, xj     cj}

9.2 id91

379

where dist is the distance between input xi and xj. it is typically computed us-
ing the euclidean distance12 between xi and xj, but many other distance mea-
sures have been used. single linkage relies only on the minimum distance between
the two clusters. it does not consider how far apart the remainder of the instances
in the clusters are. for this reason, single linkage could result in very    long    or
spread-out clusters, depending on the structure of the two clusters being com-
bined.

complete linkage is similar to single linkage. it begins by computing the dis-
tance between every instance in cluster ci and cj. however, rather than using
the minimum distance as the cost, it uses the maximum distance. that is, the cost
is:

cost (ci, cj) = max{dist(xi, xj)|xi     ci, xj     cj}

since the maximum distance is used as the cost, clusters tend to be more compact
and less spread out than in single linkage.

fig. 9.14. examples of clusters in a graph formed by connecting nodes representing in-
stances. a link represents a distance between the two instances that is less than some
threshold value.

to illustrate the di   erence between single-link and complete-link clusters,
consider the graph shown in figure 9.14. this graph is formed by representing in-

12 the euclidean distance between two vectors x and y is computed according to

i(xi     yi)2 where the subscript i denotes the ith component of the vector.

      

cluster acluster bcluster c380

9 classification and id91

stances (i.e., the xis) as nodes and connecting nodes where dist(xi, xj) < t ,
where t is some threshold value. in this graph, clusters a, b, and c would all
be single-link clusters. the single-link clusters are, in fact, the connected compo-
nents of the graph, where every member of the cluster is connected to at least one
other member. the complete-link clusters would be cluster a, the singleton clus-
ter c, and the upper and lower pairs of instances from cluster b. the complete-
link clusters are the cliques or maximal complete subgraphs of the graph, where
every member of the cluster is connected to every other member.

average linkage uses a cost that is a compromise between single linkage and
complete linkage. as before, the distance between every pair of instances in ci
and cj is computed. as the name implies, average linkage uses the average of all
of the pairwise costs. therefore, the cost is:

   

cost (ci, cj) =

xi   ci;xj   cj

dist(xi, xj)

|ci||cj|

where |ci| and |cj| are the number of instances in clusters ci and cj, respec-
tively. the types of the clusters formed using average linkage depends largely on
the structure of the clusters, since the cost is based on the average of the distances
between every pair of instances in the two clusters.

average group linkage is closely related to average linkage. the cost is computed

according to:

   

x   c x|c|

cost (ci, cj) = dist(  ci,   cj )
is the centroid of cluster ci. the centroid of a cluster is sim-
where   c =
ply the average of all of the instances in the cluster. notice that the centroid is also
a vector with the same number of dimensions as the input instances. therefore,
average group linkage represents each cluster according to its centroid and mea-
sures the cost by the distance between the centroids. the clusters formed using
average group linkage are similar to those formed using average linkage.

figure 9.15 provides a visual summary of the four cost functions described
up to this point. specifically, it shows which pairs of instances (or centroids) are
involved in computing the cost functions for the set of points used in figures 9.11
and 9.12.

ward   s method is the final method that we describe. unlike the previous costs,
which are based on various notions of the distance between two clusters, ward   s
method is based on the statistical property of variance. the variance of a set of
numbers measures how spread out the numbers are. ward   s method attempts to

9.2 id91

381

fig. 9.15. illustration of how various id91 cost functions are computed

minimize the sum of the variances of the clusters. this results in compact clusters
with a minimal amount of spread around the cluster centroids. the cost, which
is slightly more complicated to compute than the previous methods, is computed
according to:

   
   
   

k  =i;j

x   ck

x   ci   cj

(x       ck)    (x       ck) +
(x       ci   cj )    (x       ci   cj )

cost (ci, cj) =

where ci   cj is the union of the instances in clusters ci and cj, and   ci   cj is the
centroid of the cluster consisting of the instances in ci     cj. this cost measures
what the intracluster variance would be if clusters i and j were joined.

so, which of the five agglomerative id91 techniques is the best? once
again the answer depends on the data set and task the algorithm is being applied

facebgfacebgdfacebgddsingle linkagecomplete linkageaverage linkageaverage group linkage        382

9 classification and id91

to. if the underlying structure of the data is known, then this knowledge may be
used to make a more informed decision about the best algorithm to use. typically,
however, determining the best method to use requires experimentation and eval-
uation. in the information retrieval experiments that have involved hierarchical
id91, for example, average-link id91 has generally had the best e   ec-
tiveness. even though id91 is an unsupervised method, in the end there is
still no such thing as a free lunch, and some form of manual evaluation will likely
be required.

e   ciency is a problem with all hierarchical id91 methods. because the
computation involves the comparison of every instance to all other instances,
even the most e   cient implementations are o(n 2) for n instances. this limits
the number of instances that can be clustered in an application. the next cluster-
ing algorithm we describe, id116, is more e   cient because it produces a flat
id91, or partition, of the instances, rather than a hierarchy.

id116

the id116 algorithm is fundamentally di   erent than the class of hierarchical
id91 algorithms just described. for example, with agglomerative id91,
the algorithm begins with n clusters and iteratively combines two (or more) clus-
ters together based on how costly it is to do so. as the algorithm proceeds, the
number of clusters decreases. furthermore, the algorithm has the property that
once instances xi and xj are in the same cluster as each other, there is no way for
them to end up in di   erent clusters as the algorithm proceeds.

with the id116 algorithm, the number of clusters never changes. the al-
gorithm starts with k clusters and ends with k clusters. during each iteration
of the id116 algorithm, each instance is either kept in the same cluster or as-
signed to a di   erent cluster. this process is repeated until some stopping criteria
is met.

the goal of the id116 algorithm is to find the cluster assignments, repre-
sented by the assignment vector a[1], . . . , a[n ], that minimize the following
cost function:

k   

   

cost (a[1], . . . , a[n ]) =

dist(xi, ck)

k=1

i:a[i]=k

where dist(xi, ck) is the distance between instance xi and class ck. as with the
various hierarchical id91 costs, this distance measure can be any reasonable

measure, although it is typically assumed to be the following:

9.2 id91

383

dist(xi, ck) = ||xi       ck

||2

= (xi       ck)    (xi       ck)

which is the euclidean distance between xi and   ck squared. here, as before,   ck
is the centroid of cluster ck. notice that this distance measure is very similar to the
cost associated with ward   s method for agglomerative id91. therefore, the
method attempts to find the id91 that minimizes the intracluster variance
of the instances.

alternatively, the cosine similarity between xi and   ck can be used as the dis-
tance measure. as described in chapter 7, the cosine similarity measures the angle
between two vectors. for some text applications, the cosine similarity measure has
been shown to be more e   ective than the euclidean distance. this specific form
of id116 is often called spherical id116.

one of the most na  ve ways to solve this optimization problem is to try every
possible combination of cluster assignments. however, for large data sets this is
computationally intractable, because it requires computing an exponential num-
ber of costs. rather than finding the globally optimal solution, the id116 al-
gorithm finds an approximate, heuristic solution that iteratively tries to minimize
the cost. this solution returned by the algorithm is not guaranteed to be the global
optimal. in fact, it is not even guaranteed to be locally optimal. despite its heuris-
tic nature, the algorithm tends to work very well in practice.

algorithm 2 lists the pseudocode for one possible id116 implementation.
the algorithm begins by initializing the assignment of instances to clusters. this
can be done either randomly or by using some knowledge of the data to make a
more informed decision. an iteration of the algorithm then proceeds as follows.
each instance is assigned to the cluster that it is closest to, in terms of the distance
measure dist(xi, ck). the variable change keeps track of whether any of the in-
stances changed clusters during the current iteration. if some have changed, then
the algorithm proceeds. if none have changed, then the algorithm ends. another
reasonable stopping criterion is to run the algorithm for some fixed number of
iterations.

in practice, id116 id91 tends to converge very quickly to a solution.
even though it is not guaranteed to find the optimal solution, the solutions re-
turned are often optimal or close to optimal. when compared to hierarchical clus-
tering, id116 is more e   cient. specifically, since kn distance computations

384

9 classification and id91

change     f alse
for i = 1 to n do

algorithm 2 id116 id91
1: procedure km            c                  (x1, . . . , xn, k)
a[1], . . . , a[n ]     initial cluster assignment
2:
3:
repeat
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end procedure

^k     arg mink dist(xi, ck)
if a[i] is not equal ^k then

a[i]     ^k
change     true

end for

end if

until change is equal to f alse return a[1], . . . , a[n ]

are done in every iteration and the number of iterations is small, implementa-
tions of id116 are o(kn ) rather than the o(n 2) complexity of hierarchi-
cal methods. although the clusters produced by id116 depend on the starting
points chosen (the initial clusters) and the ordering of the input data, id116
generally produces clusters of similar quality to hierarchical methods. therefore,
id116 is a good choice for an all-purpose id91 algorithm for a wide range
of search engine   related tasks, especially for large data sets.

9.2.2 k nearest neighbor id91

even though hierarchical and id116 id91 are di   erent from an algorith-
mic point of view, one thing that they have in common is the fact that both algo-
rithms place every input into exactly one cluster, which means that clusters do not
overlap.13 therefore, these algorithms partition the input instances into k parti-
tions (clusters). however, for certain tasks, it may be useful to allow clusters to
overlap. one very simple way of producing overlapping clusters is called k near-
est neighbor id91. it is important to note that the k here is very di   erent
from the k in id116 id91, as will soon become very apparent.

13 note that this is true for hierarchical clusters at a given level of the dendrogram (i.e., at
a given similarity or distance value). clusters from di   erent levels of the dendrogram
do overlap.

9.2 id91

385

fig. 9.16. example of overlapping id91 using nearest neighbor id91 with k =
5. the overlapping clusters for the black points (a, b, c, and d) are shown. the five
nearest neighbors for each black point are shaded gray and labeled accordingly.

in k nearest neighbor id91, a cluster is formed around every input in-
stance. for input instance x, the k points that are nearest to x according to some
distance metric and x itself form a cluster. figure 9.16 shows several examples of
nearest neighbor clusters with k = 5 formed for the points a, b, c, and d. al-
though the figure only shows clusters around four input instances, in reality there
would be one cluster per input instance, resulting in n clusters.

as figure 9.16 illustrates, the algorithm often fails to find meaningful clusters.
in sparse areas of the input space, such as around d, the points assigned to cluster
d are rather far away and probably should not be placed in the same cluster as
d. however, in denser areas of the input space, such as around b, the clusters
are better defined, even though some related inputs may be missed because k is
not large enough. applications that use k nearest neighbor id91 tend to
emphasize finding a small number of closely related instances in the k nearest
neighbors (i.e., precision) over finding all the closely related instances (recall).

k nearest neighbor id91 can be rather expensive, since it requires com-
puting distances between every pair of input instances. if we assume that com-
puting the distance between two input instances takes constant time with respect

bddbbadacabdbdccccaaadbc386

9 classification and id91

to k and n, then this computation takes o(n 2) time. after all of the distances
have been computed, it takes at most o(n 2) time to find the k nearest neigh-
bors for each point. therefore, the total time complexity for k nearest neighbor
id91 is o(n 2), which is the same as hierarchical id91.

for certain applications, k nearest neighbor id91 is the best choice of
id91 algorithm. the method is especially useful for tasks with very dense in-
put spaces where it is useful or important to find a number of related items for ev-
ery input. examples of these tasks include language model smoothing, document
score smoothing, and pseudo-relevance feedback. we describe how id91 can
be applied to smoothing shortly.

9.2.3 evaluation

evaluating the output of a id91 algorithm can be challenging. since cluster-
ing is an unsupervised learning algorithm, there is often little or no labeled data
to use for the purpose of evaluation. when there is no labeled training data, it is
sometimes possible to use an objective function, such as the objective function
being minimized by the id91 algorithm, in order to evaluate the quality of
the clusters produced. this is a chicken and egg problem, however, since the eval-
uation metric is defined by the algorithm and vice versa.

if some labeled data exists, then it is possible to use slightly modified versions
of standard information retrieval metrics, such as precision and recall, to evaluate
the quality of the id91. id91 algorithms assign each input instance to
a cluster identifier. the cluster identifiers are arbitrary and have no explicit mean-
ing. for example, if we were to cluster a set of emails into two clusters, some of the
emails would be assigned to cluster identifier 1, while the rest would be assigned
to cluster 2. not only do the cluster identifiers have no meaning, but the clusters
may not have a meaningful interpretation. for example, one would hope that one
of the clusters would correspond to    spam    emails and the other to    non-spam   
emails, but this will not necessarily be the case. therefore, care must be taken when
defining measures of precision and recall.
one common procedure of measuring precision is as follows. first, the algo-
rithm clusters the input instances into k = |c| clusters. then, for each cluster ci,
we define maxclass(ci) to be the (human-assigned) class label associated with
the most instances in ci. since maxclass(ci) is associated with more of the in-
stances in ci than any other class label, it is assumed that it is the true label for
cluster ci. therefore, the precision for cluster ci is the fraction of instances in

9.2 id91

387

n

the cluster with label maxclass(ci). this measure is often microaveraged across
instances, which results in the following measure of precision:
|maxclass(ci)|

   

k
i=1

clusterp recision =

where|maxclass(ci)| is the total number of instances in cluster ci with the label
maxclass(ci). this measure has the intuitive property that if each cluster corre-
sponds to exactly one class label and every member of a cluster has the same label,
then the measure is 1.

in many search applications, id91 is only one of the technologies that are
being used. typically, the output of a id91 algorithm is used as part of some
complex end-to-end system. in these cases, it is important to analyze and evaluate
how the id91 algorithm a   ects the entire end-to-end system. for example, if
id91 is used as a subcomponent of a web search engine in order to improve
ranking, then the id91 algorithm can be evaluated and tuned by measuring
the impact on the e   ectiveness of the ranking. this can be di   cult, as end-to-end
systems are often complex and challenging to understand, and many factors will
impact the ranking.

9.2.4 how to choose k

thus far, we have largely ignored how to choose k. in hierarchical and id116
id91, k represents the number of clusters. in k nearest neighbors smooth-
ing, k represents the number of nearest neighbors used. although these two
things are fundamentally di   erent, it turns out that both are equally challenging
to set properly in a fully automated way. the problem of choosing k is one of the
most challenging issues involved with id91, since there is really no good so-
lution. no magical formula exists that will predict the optimal number of clusters
to use in every possible situation. instead, the best choice of k largely depends
on the task and data set being considered. therefore, k is most often chosen ex-
perimentally.

in some cases, the application will dictate the number of clusters to use. this,
however, is rare. most of the time, the application o   ers no clues as to the best
choice of k. in fact, even the range of values for k to try might not be obvious.
should 2 clusters be used? 10? 100? 1,000? there is no better way of getting an
understanding of the best setting for k than running experiments that evaluate
the quality of the resulting clusters for various values of k.

388

9 classification and id91

with hierarchical id91, it is possible to create the entire hierarchy of clus-
ters and then use some decision mechanism to decide what level of the hiearchy
to use for the id91. in most situations, however, the number of clusters has
to be manually chosen, even with hierarchical id91.

fig. 9.17. example of overlapping id91 using parzen windows. the clusters for the
black points (a, b, c, and d) are shown. the shaded circles indicate the windows used to
determine cluster membership. the neighbors for each black point are shaded gray and
labeled accordingly.

when forming k nearest neighbor clusters, it is possible to use an adaptive
value for k. that is, for instances in very dense regions, it may be useful to choose
a large k, since the neighbors are likely to be related. similarly, in very sparse ar-
eas, it may be best to choose only a very small number of nearest neighbors, since
moving too far away may result in unrelated neighbors being included. this idea
is closely related to parzen windows,14 which are a variant of k nearest neighbors
used for classification. with parzen windows, the number of nearest neighbors
is not fixed. instead, all of the neighbors within a fixed distance (   window   ) of
an instance are considered its nearest neighbors. in this way, instances in dense

14 named after emanuel parzen, an american statistician.

bbbcbbcccaadbc9.2 id91

389

areas will have many nearest neighbors, and those in sparse areas will have few.
figure 9.17 shows the same set of points from figure 9.16, but clustered using
a parzen window approach. we see that fewer outliers get assigned to incorrect
clusters for points in sparse areas of the space (e.g., point c), whereas points in
denser regions have more neighbors assigned to them (e.g., b). however, the clus-
ters formed are not perfect. the quality of the clusters now depends on the size
of the window. therefore, although this technique eliminates the need to choose
k, it introduces the need to choose the window size, which can be an equally
challenging problem.

9.2.5 id91 and search

a number of issues with id91 algorithms have resulted in them being less
widely used in practice than classification algorithms. these issues include the
computational costs, as well as the di   culty of interpreting and evaluating the
clusters. id91 has been used in a number of search engines for organizing
the results, as we discussed in section 6.3.3. there are very few results for a search
compared to the size of the document collection, so the e   ciency of id91 is
less of a problem. id91 is also able to discover structure in the result set for
arbitrary queries that would not be possible with a classification algorithm.

id96, which we discussed in section 7.6.2, can also be viewed as
an application of id91 with the goal of improving the ranking e   ectiveness
of the search engine. in fact, most of the information retrieval research involving
id91 has focused on this goal. the basis for this research is the well-known
cluster hypothesis. as originally stated by van rijsbergen (1979), the cluster hy-
pothesis is:

closely associated documents tend to be relevant to the same requests.

note that this hypothesis doesn   t actually mention clusters. however,    closely as-
sociated    or similar documents will generally be in the same cluster. so the hy-
pothesis is usually interpreted as saying that documents in the same cluster tend
to be relevant to the same queries.

two di   erent tests have been used to verify whether the cluster hypothesis
holds for a given collection of documents. the first compares the distribution of
similarity scores for pairs of relevant documents (for a set of queries) to the dis-
tribution for pairs consisting of a non-relevant and a relevant document. if the
cluster hypothesis holds, we might expect to see a separation between these two
distributions. on some smaller corpora, such as the cacm corpus mentioned

390

9 classification and id91

in chapter 8, this is indeed the case. if there were a number of clusters of relevant
documents, however, which were not similar to each other, then this test may
fail to show any separation. to address this potential problem, voorhees (1985)
proposed a test based on the assumption that if the cluster hypothesis holds, rel-
evant documents would have high local precision, even if they were scattered in
many clusters. local precision simply measures the number of relevant documents
found in the top five nearest neighbors for each relevant document.

fig. 9.18. cluster hypothesis tests on two trec collections. the top two compare
the distributions of similarity values between relevant-relevant and relevant-nonrelevant
pairs (light gray) of documents. the bottom two show the local precision of the relevant
documents.

0.00.20.40.60.81.0trec120.00.20.40.60.81.0robust00.20.40.60.81trec12local precisionfrequency0100020003000400000.20.40.60.81robustlocal precisionfrequency0500100015009.2 id91

391

figure 9.18 shows the results of these tests used on two trec collections.
these collections have similar types of documents, including large numbers of
news stories. the 250 queries for the robust collection are known to be harder in
terms of the typical map values obtained than the 150 queries used for trec12.
the tests on the top row of the figure show that for both collections there is poor
separation between the distributions of similarity values. the tests on the lower
row, however, show that relevant documents in the trec12 collection have high
local precision. the local precision is lower in the robust collection, which means
that relevant documents tend to be more isolated and, consequently, harder to
retrieve.

given that the cluster hypothesis holds, at least for some collections and
queries, the next question is how to exploit this in a retrieval model. there are,
in fact, a number of ways of doing this. the first approach, known as cluster-based
retrieval, ranks clusters instead of individual documents in response to a query. if
there were k clusters c1 . . . ck, for example, we could rank clusters using the
query likelihood retrieval model. this means that we rank clusters by p (q|cj),
where q is the query, and:

n   

i=1

p (q|cj) =

p (qi|cj)

the probabilities p (qi|cj) are estimated using a smoothed unigram language
model based on the frequencies of words in the cluster, as described for docu-
ments in chapter 7. after the clusters have been ranked, documents within each
cluster could be individually ranked for presentation in a result list. the intuition
behind this ranking method is that a cluster language model should provide bet-
ter estimates of the important word probabilities than document-based estimates.
in fact, a relevant document with no terms in common with the query could po-
tentially be retrieved if it were a member of a highly ranked cluster with other
relevant documents.

rather than using this two-stage process, the cluster language model can be
directly incorporated into the estimation of the document language model as fol-
lows:

p (w|d) = (1              )

fw;d|d| +   

fw;cj
|cj| +   

fw;coll
|coll|

where    and    are parameters, fw;d is the word frequency in the document d,
fw;cj is the word frequency in the cluster cj that contains d, and fw;coll is the

392

9 classification and id91

word frequency in the collection coll. the second term, which comes from the
cluster language model, increases the id203 estimates for words that occur
frequently in the cluster and are likely to be related to the topic of the document.
in other words, the cluster language model makes the document more similar to
other members of the cluster. this document language model with cluster-based
smoothing can be used directly by the query likelihood retrieval model to rank
documents as described in section 7.3.1.

the document language model can be further generalized to the case where

the document d is a member of multiple overlapping clusters, as follows:

p (w|d) = (1              )

fw;d|d| +   

fw;cj

|cj| p (d|cj) +   

fw;coll
|coll|

   

cj

in this case, the second term in the document language model id203 estimate
for a word w is the weighted sum of the probabilities from the cluster language
models for all clusters. the weight (p (d|cj)) is the id203 of the document
being a member of cluster cj. we can also make the simplifying assumption that
p (d|cj) is uniform for those clusters that contain d, and zero otherwise.

retrieval experiments have shown that retrieving clusters can yield small but
variable improvements in e   ectiveness. smoothing the document language model
with cluster-based estimates, on the other hand, provides significant and consis-
tent benefits. in practice, however, the expense of generating clusters has meant
that cluster-based techniques have not been deployed as part of the ranking al-
gorithm in operational search engines. however, promising results have recently
been obtained using query-specific id91, where clusters are constructed only
from the top-ranked (e.g., 50) documents (liu & croft, 2008; kurland, 2008).
these clusters, which can be used for either cluster-based retrieval or document
smoothing, can obviously be generated much more e   ciently.

references and further reading

classification and id91 have been thoroughly investigated in the research ar-
eas of statistics, pattern recognition, and machine learning. the books by duda et
al. (2000) and hastie et al. (2001) describe a wide range of classification and clus-
tering techniques and provide more details about the mathematical foundations
the techniques are built upon. these books also provide good overviews of other

9.2 id91

393

useful machine learning techniques that can be applied to various search engine
tasks.

for a more detailed treatment of na  ve bayes classification for text classifi-
cation, see mccallum and nigam (1998). c. j. c. burges (1998) gives a very de-
tailed tutorial on id166s that covers all of the basic concepts and theory. however,
the subject matter is not light and requires a certain level of mathematical sophis-
tication to fully understand. in addition, joachims (2002a) is an entire book de-
scribing various uses of id166s for text classification.

van rijsbergen (1979) provides a review of earlier research on id91 in
information retrieval, and describes the cluster hypothesis and cluster-based re-
trieval. diaz (2005) proposed an alternative interpretation of the cluster hypothe-
sis by assuming that closely related documents should have similar scores, given the
same query. using this assumption, diaz developed a framework for smoothing
retrieval scores using properties of k nearest neighbor clusters. language model-
ing smoothing using id116 id91 was examined in liu and croft (2004).
another id38 smoothing technique based on overlapping k near-
est neighbor clusters was proposed in kurland and lee (2004).

there are various useful software packages available for text classification. the
mallet software toolkit15 provides implementations of various machine learning
algorithms, including na  ve bayes, maximum id178, boosting, winnow, and
id49. it also provides support for parsing and tokenizing
text into features. another popular software package is id166light,16 which is an
id166 implementation that supports all of the kernels described in this chapter.
id91 methods are included in a number of packages available on the web.

exercises

9.1. provide an example of how people use id91 in their everyday lives. what
are the features that they use to represent the objects? what is the similarity mea-
sure? how do they evaluate the outcome?
9.2. assume we want to do classification using a very fine-grained ontology, such
as one describing all the families of human languages. suppose that, before train-
ing, we decide to collapse all of the labels corresponding to asian languages into a
single    asian languages    label. discuss the negative consequences of this decision.

15 http://mallet.cs.umass.edu/
16 http://id166light.joachims.org/

394

9 classification and id91

9.3. suppose that we were to estimate p (d|c) according to:

p (d|c) =

nd;c
nc

where nd;c is the number of times document d is assigned to class c in the training
set, and nc is the number of instances assigned class label c in the training set. this
is analogous to the way that p (c) is estimated. why can this estimate not be used
in practice?
9.4. for some classification data set, compute estimates for p (w|c) for all words
w using both the multiple-bernoulli and multinomial models. compare the multiple-
bernoulli estimates with the multinomial estimates. how do they di   er? do the
estimates diverge more for certain types of terms?
9.5. explain why the solution to the original id166 formulation w = arg maxw
is equivalent to the alternative formulation w = arg minw

||w||2.

2||w||

1
2

9.6. compare the accuracy of a one versus all id166 classifier and a one versus
one id166 classifier on a multiclass classification data set. discuss any di   erences
observed in terms of the e   ciency and e   ectiveness of the two approaches.

9.7. under what conditions will the microaverage equal the macroaverage?

9.8. cluster the following set of two-dimensional instances into three clusters us-
ing each of the five agglomerative id91 methods:

(   4,    2), (   3,    2), (   2,    2), (   1,    2), (1,    1), (1, 1), (2, 3), (3, 2), (3, 4), (4, 3)

discuss the di   erences in the clusters across methods. which methods pro-
duce the same clusters? how do these clusters compare to how you would manu-
ally cluster the points?

9.9. use id116 and spherical id116 to cluster the data points in exercise
9.8. how do the id91s di   er?

9.10. nearest neighbor clusters are not symmetric, in the sense that if instance a
is one of instance b   s nearest neighbors, the reverse is not necessarily true. explain
how this can happen with a diagram.

9.2 id91

395

9.11. the k nearest neighbors of a document could be represented by links to
those documents. describe two ways this representation could be used in a search
application.
9.12. can the clusterprecision evaluation metric ever be equal to zero? if so, pro-
vide an example. if not, explain why.

9.13. test the cluster hypothesis on the cacm collection using both methods
shown in figure 9.18. what do you conclude from these tests?

10

social search

   you will be assimilated.   

borg collective, star trek: first contact

10.1 what is social search?

in this chapter we will describe social search, which is rapidly emerging as a key
search paradigm on the web. as its name implies, social search deals with search
within a social environment. this can be defined as an environment where a com-
munity of users actively participate in the search process. we interpret this defi-
nition of social search very broadly to include any application involving activities
such as defining individual user profiles and interests, interacting with other users,
and modifying the representations of the objects being searched. the active role
of users in social search applications is in stark contrast to the standard search
paradigms and models, which typically treat every user the same way and restrict
interactions to query formulation.

users may interact with each other online in a variety of ways. for example,
users may visit a social media site,1 which have recently gained a great deal of
popularity. examples of these sites include digg (websites), twitter (status mes-
sages), flickr (pictures), youtube (videos), del.icio.us (bookmarks), and citeu-
like (research papers). social networking sites, such as myspace, facebook, and
linkedin, allow friends, colleagues, and people with similar interests to interact
with each other in various ways. more traditional examples of online social in-
teractions include email, instant messenger, massively multiplayer online games
(mmogs), forums, and blogs.

1 social media sites are often collectively referred to as web 2.0, as opposed to the clas-
sical notion of the web (   web 1.0   ), which consists of non-interactive html docu-
ments.

398

10 social search

as we see, the online world is a very social environment that is rich with users
interacting with each other in various forms. these social interactions provide
new and unique data resources for search systems to exploit, as well as a myriad
of privacy issues. most of the web search approaches we described in chapter 7
only consider features of the documents or the link structure of the web. in so-
cially rich environments, however, we also have a plethora of user interaction data
available that can help enhance the overall user experience in new and interesting
ways.

it would be possible to write an entire book on online social interactions and
search within such environments. the focus of this chapter is to highlight and
describe a few aspects of social search that are particularly interesting from a search
engine and information retrieval perspective.

the first topic we cover is user tags. many social media websites allow users to
assign tags to items. for example, a video-sharing website may allow users to not
only assign tags to their own videos, but also to videos created by other people.
an underwater video, for example, may have the tags    swimming   ,    underwater   ,
   tropic   , and    fish   . some sites allow multi-term tags, such as    tropical fish   , but
others allow only single-term tags. as we will describe, user tags are a form of man-
ual indexing, where the content of an object is represented by manually assigned
terms. there are many interesting search tasks related to user tags, such as search-
ing for items using tags, automatically suggesting tags, and visualizing clusters of
tags.

the second topic covered here is searching within communities, which de-
scribes online communities and how users search within such environments. on-
line communities are virtual groups of users that share common interests and in-
teract socially in various ways in an online environment. for example, a sports fan
who enjoys the outdoors and photography may be a member of baseball, hiking,
and digital camera communities. interactions in these communities range from
passive activities (reading web pages) to those that are more active (writing in
blogs and forums). these communities are virtual and ad hoc, meaning that there
is typically no formal mechanism for joining one, and consequently people are im-
plicitly rather than explicitly members of a community. therefore, being able to
automatically determine which communities exist in an online environment, and
which users are members of each, can be valuable for a number of search-related
tasks. one such task that we will describe is community-based question answer-
ing, whereby a user posts a question to an online system and members of his own
community, or the community most related to his question, provide answers to

10.1 what is social search?

399

the question. such a search task is much more social, interactive, and focused than
standard web search.

the next topics we describe are filtering and recommender systems. it may be
considered somewhat unusual to include these in a chapter on social search, be-
cause they are not typical    web 2.0    applications. both types of systems, however,
rely on representations of individual users called profiles, and for that reason fit
into our broad definition. both systems also combine elements of document re-
trieval and classification. in standard search tasks, systems return documents in
response to many di   erent queries. these queries typically correspond to short-
term information needs. in filtering, there is a fixed query (the profile) that rep-
resents some long-term information need. the search system monitors incoming
documents and retrieves only those documents that are relevant to the informa-
tion need. many online news websites provide document filtering functionality.
for example, id98 provides an alerts service, which allows users to specify various
topics of interest, such as    tropical storms   , or more general topics, such as sports
or politics. when a new story matches a user   s profile, the system alerts the user,
typically via email. in this way, the user does not need to continually search for
articles of interest. instead, the search system is tasked with finding relevant docu-
ments that match the user   s long-term information needs. recommender systems
are similar to document filtering systems, except the goal is to predict how a user
would rate some item, rather than retrieving relevant documents. for example,
amazon.com employs a recommender system that attempts to predict how much
a user would like certain items, such as books, movies, or music. recommender
systems are social search algorithms because predictions are estimated based on
ratings given by similar users, thereby implicitly linking people to a community
of users with related interests.

the final two topics covered in this chapter, peer-to-peer (p2p) search and
metasearch, deal with architectures for social search. peer-to-peer search is the
task of querying a community of    nodes    for a given information need. nodes
can be individuals, organizations, or search engines. when a user issues a query, it
is passed through the p2p network and run on one or more nodes, and then results
are returned. this type of search can be fully distributed across a large network of
nodes. metasearch is a special case of p2p search where all of the nodes are search
engines. metasearch engines run queries against a number of search engines, col-
lect the results, and then merge the results. the goal of metasearch engines is to
provide better coverage and accuracy than a single search engine.

400

10 social search

finally, we note that personalization is another area that could be regarded as
part of social search, because it covers a range of techniques for improving search
by representing individual user preferences and interests. since most of these tech-
niques provide context for the query, however, they were discussed as part of
query refinement in section 6.2.5.

10.2 user tags and manual indexing

before electronic search systems became available at libraries, patrons had to rely
on card catalogs for finding books. as their name implies, card catalogs are large
collections (catalogs) of cards. each card contains information about a particular
author, title, or subject. a person interested in a specific author, title, or subject
would go to the appropriate catalog and attempt to find cards describing relevant
books. the card catalogs, therefore, act as indexes to the information in a library.
card catalogs existed long before computers did, which means that these cards
were constructed manually. given a book, a person had to extract the author, ti-
tle, and subject headings of the book so that the various catalogs could be built.
this process is known as manual indexing. given that it is impractical to manu-
ally index the huge collections of digital media available today, search engines use
automatic indexing techniques to assign identifiers (terms, phrases, features) to
documents during index construction. since this process is automatic, the quality
and accuracy of the indexing can be much lower than that of manual indexing. the
advantages of automatic indexing, however, are that it is exhaustive, in the sense
that every word in the document is indexed and nothing is left out, and consistent,
whereas people can make mistakes indexing or have certain biases in how they in-
dex. search evaluations that have compared manual to automatic indexing have
found that automatic indexing is at least as e   ective and often much better than
manual indexing. these studies have also shown, however, that the two forms of
indexing complement each other, and that the most e   ective searches use both.
as a compromise between manually indexing every item (library catalogs)
and automatically indexing every item (search engines), social media sites pro-
vide users with the opportunity to manually tag items. each tag is typically a sin-
gle word that describes the item. for example, an image of a tiger may be assigned
the tags    tiger   ,    zoo   ,    big   , and    cat   . by allowing users to assign tags, some items
end up with tags, and others do not. of course, to make every item searchable, it is
likely that every item is automatically indexed. therefore, some items will contain

10.2 user tags and manual indexing

401

both automatic and manual identifiers. as we will show later in this section, this
results in unique challenges for retrieval models and ranking functions.

social media tagging, like card catalog generation, is called manual tagging.
this clash in naming is rather unfortunate, because the two types of indexing
are actually quite di   erent. card catalogs are manually generated by experts who
choose keywords, categories, and other descriptors from a controlled vocabulary
(fixed ontology). this ensures that the descriptors are more or less standardized.
on the other hand, social media tagging is done by end users who may or may
not be experts. there is little-to-no quality control done on the user tags. fur-
thermore, there is no fixed vocabulary from which users choose tags. instead, user
tags form their own descriptions of the important concepts and relationships in
a domain. user-generated ontologies (or taxonomies) are known as folksonomies.
therefore, a folksonomy can be interpreted as a dynamic, community-influenced
ontology.

there has been a great deal of research and interest invested in developing a
semantically tagged version of the web, which is often called the semantic web.
the goal of the semantic web is to semantically tag web content in such a way
that it becomes possible to find, organize, and share information more easily. for
example, a news article could be tagged with metadata, such as the title, subject,
description, publisher, date, and language. however, in order for the semantic
web to materialize and yield significant improvements in relevance, a standard-
ized, fixed ontology of metadata tags must be developed and used consistently
across a large number of web pages. given the growing popularity of social media
sites that are based on flexible, user-driven folksonomies, compared to the small
number of semantic web sites that are based on rigid, predefined ontologies, it
seems as though users, in general, are more open to tagging data with a relatively
unrestricted set of tags that are meaningful to them and that reflect the specific
context of the application.

given that users are typically allowed to tag items in any way that they wish,
there are many di   erent types of tags. for example, golder and huberman (2006)
described seven di   erent categories of tags. z. xu et al. (2006) proposed a sim-
plified set of five tag categories, which consists of the following:
1. content-based tags. tags describing the content of an item. examples:    car   ,

   woman   , and    sky   .

2. context-based tags. tags that describe the context of an item. examples:

   new york city    or    empire state building   .

402

10 social search

3. attribute tags. tags that describe implicit attributes of the item. examples:
   nikon    (type of camera),    black and white    (type of movie), or    homepage   
(type of web page).

4. subjective tags. tags that subjectively describe an item. examples:    pretty   ,

5. organizational tags. tags that help organize items. examples:    todo   ,    my pic-

   amazing   , and    awesome   .

tures   , and    readme   .

as we see, tags can be applied to many di   erent types of items, ranging from web
pages to videos, and used for many di   erent purposes beyond just tagging the con-
tent. therefore, tags and online collaborative tagging environments can be very
useful tools for users in terms of searching, organizing, sharing, and discovering
new information. it is likely that tags are here to stay and will become even more
widely used in the future. therefore, it is important to understand the various
issues surrounding them and how they are used within search engines today.

in the remainder of this section, we will describe how tags can be used for
search, how new tags for an item can be inferred from existing tags, and how sets
of tags can be visualized and presented to the user.

10.2.1 searching tags

since this is a book about search engines, the first tag-related task that we discuss
is searching a collection of collaboratively tagged items. one unique property of
tags is that they are almost exclusively textual keywords that are used to describe
textual or non-textual items. therefore, tags can provide a textual dimension to
items that do not explicitly have a simple textual representation, such as images
or videos. these textual representations of non-textual items can be very useful
for searching. we can apply many of the retrieval strategies described in chap-
ter 7 to the problem. despite the fact that searching within tagged collections
can be mapped to a text search problem, tags present certain challenges that are
not present when dealing with standard document or web retrieval.

the first challenge, and by far the most pervasive, is the fact that tags are
very sparse representations of very complex items. perhaps the simplest way to
search a set of tagged items is to use a boolean retrieval model. for example, given
the query    fish bowl   , one could run the query    fish and bowl   , which would
only return items that are tagged with both    fish    and    bowl   , or    fish or bowl   ,
which would return items that are tagged with either    fish    or    bowl   . conjunc-
tive (and) queries are likely to produce high-quality results, but may miss many

10.2 user tags and manual indexing

403

relevant items. thus, the approach would have high precision but low recall. at
the opposite end of the spectrum, the disjunctive (or) queries will match many
more relevant items, but at the cost of precision.

fig. 10.1. search results used to enrich a tag representation. in this example, the tag being
expanded is    tropical fish   . the query    tropical fish    is run against a search engine, and the
snippets returned are then used to generate a distribution over related terms.

of course, it is highly desirable to achieve both high precision and high re-
call. however, doing so is very challenging. consider the query    aquariums    and
a picture of a fish bowl that is tagged with    tropical fish    and    goldfish   . most re-
trieval models, including boolean retrieval, will not be able to find this item, be-
cause there is no overlap between the query terms and the tag terms. this problem,
which was described in chapter 6 in the context of advertising, is known as the
vocabulary mismatch problem. there are various ways to overcome this problem,
including simple things such as id30. other approaches attempt to enrich
the sparse tag (or query) representation by performing a form of pseudo-relevance

age of aquariums -tropical fish huge educational aquarium site for tropical fish hobbyists, promoting responsible fishkeeping internationally since 1997.the krib(aquaria and tropical fish) this site contains information about tropical fish aquariums, including archived usenetpostings and e-mail discussions, along with new ...   keeping tropical fish and goldfish in aquariums, fishbowls, and ... keeping tropical fish and goldfish in aquariums, fishbowls, and ponds at aquariumfish.net.fishtropicalaquariumsgoldfishbowlsp(w |    tropical fish    )404

10 social search

feedback. figure 10.1 illustrates how web search results may be used to enrich a
tag representation. in the example, the tag    tropical fish    is run as a query against
a search engine. the snippets returned are then processed using any of the stan-
dard pseudo-relevance feedback techniques described in section 7.3.2, such as rel-
evance modeling, which forms a distribution over related terms. in this example,
the terms    fish   ,    tropical   ,    aquariums   ,    goldfish   , and    bowls    are the terms with
the highest id203 according to the model. the query can also, optionally,
be expanded in the same way. search can then be done using the enriched query
and/or tag representations in order to maintain high levels of precision as well as
recall.

the second challenge is that tags are inherently noisy. as we have shown,
tags can provide useful information about items and help improve the quality
of search. however, like anything that users create, the tags can also be o    topic,
inappropriate, misspelled, or spam. therefore it is important to provide proper in-
centives to users to enter many high-quality tags. for example, it may be possible
to allow users to report inappropriate or spam tags, thereby reducing the incentive
to produce junk tags. furthermore, users may be given upgraded or privileged ac-
cess if they enter some number of (non-spam) tags over a given time period. this
incentive promotes more tagging, which can help improve tag quality and cover-
age.

the final challenge is that many items in a given collection may not be tagged,
which makes them virtually invisible to any text-based search system. for such
items, it would be valuable to automatically infer the missing tags and use the tags
for improving search recall. we devote the next section to diving deeper into the
details of this problem.

10.2.2 inferring missing tags

as we just described, items that have no tags pose a challenge to a search sys-
tem. although precision is obviously a very important metric for many tag-related
search tasks, recall may also be important in some cases. in such cases, it is impor-
tant to automatically infer a set of tags for items that have no manual tags assigned
to them.

let us first consider the case when the items in our collection are textual, such
as books, news articles, research papers, or web pages. in these cases, it is possible
to infer tags based solely on the textual representation of the item. one simple
approach would involve computing some weight for every term that occurs in the

10.2 user tags and manual indexing

405

text and then choosing the k terms with the highest weight as the inferred tags.
there are various measures of term importance, including a tf.idf-based weight,
such as:

wt(w) = log(fw;d + 1) log(

n
dfw

)

where fw;d is the number of times term w occurs in item d, n is the total num-
ber of items, and dfw is the number of items that term w occurs in. other term
importance measures may take advantage of document structure. for example,
terms that occur in the title of an item may be given more weight.

it is also possible to treat the problem of inferring tags as a classification prob-
lem, as was recently proposed by heymann, ramage, and garcia-molina (2008).
given a fixed ontology or folksonomy of tags, the goal is to train a binary classifier
for each tag. each of these classifiers takes an item as input and predicts whether
the associated tag should be applied to the item. this approach requires training
one classifier for every tag, which can be a cumbersome task and requires a large
amount of training data. fortunately, however, training data for this task is virtu-
ally free since users are continuously tagging (manually labeling) items! therefore,
it is possible to use all of the existing tag/item pairs as training data to train the
classifiers. heymann et al. use an id166 classifier to predict web page tags. they
compute a number of features, including tf.idf weights for terms in the page text
and anchor text, as well as a number of link-based features. results show that high
precision and recall can be achieved by using the textual features alone, especially
for tags that occur many times in the collection. a similar classification approach
can be applied to other types of items, such as images or videos. the challenge
when dealing with non-textual items is extracting useful features from the items.
the two approaches we described for inferring missing tags choose tags for
items independently of the other tags that were assigned. this may result in very
relevant, yet very redundant, tags being assigned to some item. for example, a
picture of children may have the tags    child   ,    children   ,    kid   ,    kids   ,    boy   ,    boys   ,
   girl   ,    girls      all of which would be relevant, but as you see, are rather redundant.
therefore, it is important to choose a set of tags that are both relevant and non-
redundant. this is known as the novelty problem.

carbonell and goldstein (1998) describe the maximal marginal relevance
(mmr) technique, which addresses the problem of selecting a diverse set of items.
rather than choosing tags independently of each other, mmr chooses tags iter-
atively, adding one tag to the item at a time. given an item i and the current set
of tags for the item ti, the mmr technique chooses the next tag according to the

406

10 social search

tag t that maximizes:

m m r(t; ti) =

(
  simitem(t, i)     (1       ) max
t   ti

)

simtag(ti, t)

where simitem is a function that measures that similarity between a tag t and
item i (such as those measures described in this section), simtag measures the
similarity between two tags (such as the measures described in section 10.2.1),
and    is a tunable parameter that can be used to trade o    between relevance (   =
1) and novelty (   = 0). therefore, a tag that is very relevant to the item and
not very similar to any of the other tags will have a large mmr score. iteratively
choosing tags in this way helps eliminate the production of largely redundant sets
of tags, which is useful not only when presenting the inferred tags to the user, but
also from the perspective of using the inferred tags for search, since a diverse set
of tags should help further improve recall.

10.2.3 browsing and tag clouds

as we have shown, tags can be used for searching a set of collaboratively tagged
items. however, tags can also be used to help users browse, explore, and discover
new items in a large collection of items. there are several di   erent ways that tags
can be used to aid browsing. for example, when a user is viewing a given item,
all of the item   s tags may be displayed. the user may then click on one of the tags
and be shown a list of results of items that also have that tag. the user may then
repeat this process, repeatedly choosing an item and then clicking on one of the
tags. this allows users to browse through the collection of items by following a
chain of related tags.

such browsing behavior is very focused and does not really allow the user to
explore a large range of the items in the collection. for example, if a user starts
on a picture of a tropical fish, it would likely take many clicks for the user to end
up viewing an image of an information retrieval textbook. of course, this may be
desirable, especially if the user is only interested in things closely related to tropical
fish.

one way of providing the user with a more global view of the collection is to
allow the user to view the most popular tags. these may be the most popular tags
for the entire site or for a particular group or category of items. tag popularity
may be measured in various ways, but is commonly computed as the number of
times the tag has been applied to some item. displaying the popular tags allows

10.2 user tags and manual indexing

407

the user to begin her browsing and exploration of the collection using one of these
tags as a starting point.

fig. 10.2. example of a tag cloud in the form of a weighted list. the tags are in alphabetical
order and weighted according to some criteria, such as popularity.

thus far we have described ways that tags may aid users with browsing. one of
the most important aspects of browsing is displaying a set of tags to a user in a vi-
sually appealing and meaningful manner. for example, consider displaying the 50
most popular tags to the user. the simplest way to do so is to just display the tags
in a list or table, possibly in alphabetical or sorted order according to popularity.
besides not being very visually appealing, this display also does not allow the user
to quickly observe all of the pertinent information. when visualizing tags, it is
useful to show the list of tags in alphabetical order, so that users may quickly scan
through the list or find the tags they are looking for. it is also beneficial to por-
tray the popularity or importance of a given tag. there are many ways to visualize
this information, but one of the most widely used techniques is called tag clouds.
in a tag cloud, the display size of a given tag is proportional to its popularity or
importance. tags may be arranged in a random order within a    cloud    or alpha-
betically. figure 10.2 shows an example of a tag cloud where the tags are listed
alphabetically. such a representation is also called a weighted list. based on this

animalsarchitectureartaustraliaautumnbabybandbarcelonabeachberlinbirthdayblackblackandwhitebluecaliforniacameraphonecanadacanoncarcatchicagochinachristmaschurchcitycloudscolorconcertdaydogenglandeuropefamilyfestivalfilmfloridaflowerflowersfoodfrancefriendsfungardengermanygirlgraffitigreenhalloweenhawaiiholidayhomehouseindiairelanditalyjapanjulykidslakelandscapelightlivelondonmacromemexicomusicnaturenewnewyorknightnikonnycoceanparisparkpartypeopleportraitredriverrocksanfranciscoscotlandseaseattleshowskysnowspainspringstreetsummersunsettaiwantexasthailandtokyotorontotraveltreetreestripukusavacationwashingtonwaterwedding408

10 social search

tag cloud, the user can easily see that the tags    wedding   ,    party   , and    birthday   
are all very popular. therefore, tag clouds provide a convenient, visually appealing
way of representing a set of tags.

10.3 searching with communities

10.3.1 what is a community?

the collaborative tagging environments that we just described are filled with im-
plicit social interactions. by analyzing the tags that users submit or search for, it is
possible to discover groups of users with related interests. for example, ice hockey
fans are likely to tag pictures of their favorite ice hockey players, tag their favorite
ice hockey web pages, search for ice hockey   related tags, and so on. tagging is just
one example of how interactions in an online environment can be used to infer
relationships between entities (e.g., people). groups of entities that interact in an
online environment and that share common goals, traits, or interests are an online
community. this definition is not all that di   erent from the traditional definition
of community. in fact, online communities are actually very similar to traditional
communities and share many of the same social dynamics. the primary di   erence
between our definition and that of a traditional community is that an online com-
munity can be made up of users, organizations, web pages, or just about any other
meaningful online entity.

let us return to our example of users who tag and search for ice hockey   related
items. it is easy to see that ice hockey fans form an online community. members
of the community do many other things other than tag and search. for example,
they also post to blogs, newsgroups, and other forums. they may also send instant
messages and emails to other members of the community about their ice hockey
experiences. furthermore, they may buy and sell ice hockey   related items online,
through sites such as ebay. hence, there are many ways that a user may partici-
pate in a community. it is important to note, however, that his membership in
the community is implicit. another important thing to notice is that users are
very likely to have a number of hobbies or interests, and may be members of more
than one online community. therefore, in order to improve the overall user expe-
rience, it can be useful for search engines and other online sites to automatically
determine the communities associated with a given user.

some online communities consist of non-human entities. for example, a set of
web pages that are all on the same topic form an online community that is often

10.3 searching with communities

409

called a web community. these pages form a community since they share similar
traits (i.e., they are all about the same topic). since web pages are created by users,
web communities share many of the same characteristics as communities of users.
automatically identifying web communities can be useful for improving search.
the remainder of this section covers several aspects of online communities
that are useful from a search engine perspective. we first describe several ef-
fective methods for automatically finding online communities. we then discuss
community-based id53, where people ask questions and receive
answers from other members of the community. finally, we cover collaborative
searching, which is a search paradigm that involves a group of users searching to-
gether.

10.3.2 finding communities

the first task that we describe is how to automatically find online communities.
as we mentioned before, online communities are implicitly defined by interac-
tions among a set of entities with common traits. this definition is rather vague
and makes it di   cult to design general-purpose algorithms for finding every pos-
sible type of online community. instead, several algorithms have been developed
that can e   ectively find special types of communities that have certain assumed
characteristics. we will describe several such algorithms now.

most of the algorithms used for finding communities take as input a set of en-
tities, such as users or web pages, information about each entity, and details about
how the entities interact or are related to each other. this can be conveniently rep-
resented as a graph, where each entity is a node in the graph, and interactions (or
relationships) between the entities are denoted by edges. graphs can be either di-
rected or undirected. the edges in directed graphs have directional arrows that
indicate the source node and destination node of the edge. edges in undirected
graphs do not have directional arrows and therefore have no notion of source and
destination. directed edges are useful for representing non-symmetric or causal
relationships between two entities. undirected edges are useful for representing
symmetric relationships or for simply indicating that two entities are related in
some way.

using this representation, it is easy to define two criteria for finding communi-
ties within the graph. first, the set of entities (nodes) must be similar to each other
according to some similarity measure. second, the set of entities should interact
with each other more than they interact with other entities. the first requirement

410

10 social search

makes sure that the entities actually share the same traits, whereas the second en-
sures that the entities interact in a meaningful way with each other, thereby mak-
ing them a community rather than a set of users with the same traits who never
interact with each other.

the first algorithm that we describe is the hits algorithm, which was briefly
discussed in chapter 4 in the context of id95. the hits algorithm is sim-
ilar to id95, except that it is query-dependent, whereas id95 is usually
query-independent. you may be wondering what hits has to do with finding
communities, since it was originally proposed as a method for improving web
search. both id95 and hits, however, are part of a family of general, power-
ful algorithms known as link analysis algorithms. these algorithms can be applied
to many di   erent types of data sets that can be represented as directed graphs.

given a graph of entities, we must first identify a subset of the entities that
may possibly be members of the community. we call these entities the candidate
entities. for example, if we wish to find the ice hockey online community, then
we must query each node in the graph and find all of the nodes (users) that are
interested in ice hockey. this can be accomplished by, for example, finding all users
of a system who have searched for anything hockey-related. this ensures that the
first criteria, which states that entities should be similar to each other, is satisfied.
another example is the task of finding the    fractal art    web community. here, we
could search the web for the query    fractal art    and consider only those entities
(web pages) that match the query. again, this ensures that all of the pages are
topically similar to each other. this first step finds sets of similar items, but fails
to identify the sets of entities that actively participate, via various interactions,
within the community, which is the second criteria that we identified as being
important.

given the candidate entities, the hits algorithm can be used to find the
   core    of the community. the hits algorithm takes a graph g with node set
v and edge set e as input. for finding communities, the vertex set v consists of
the candidate entities, and the edge set e consists of all of the edges between can-
didate entities. for each of the candidate entities (nodes) p in the graph, hits
computes an authority score (a(p)) and a hub score (h(p)). it is assumed that
good hubs are those that point to good authorities and that good authorities are
those that are pointed to by good hubs. notice the circularity in these definitions.
this means that the authority score depends on the hub score, which in turn de-
pends on the authority score. given a set of authority and hub scores, the hits
algorithm updates the scores according to the following equations:

10.3 searching with communities

411

for q     v do

if (p, q)     e then

a0(p)     1    p     v
h0(p)     1    p     v
for i = 1 to k do

ai(p)     0    p     v
hi(p)     0    p     v
za     0
zh     0
for p     v do

algorithm 3 hits
1: procedure hits(g = (v, e), k)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end procedure

ai(p)     ai(p)
hi(p)     hi(p)

end for
end for
for p     v do

end if
if (q, p)     e then

end for
return ak, hk

end for

end if

zh

za

hi(p)     hi(p) + ai   1(q)
zh     zh + ai   1(q)

ai(p)     ai(p) + hi   1(q)
za     za + hi   1(q)

   
   

q   p

p   q

h(q)

a(q)

a(p) =

h(p) =

where p     q indicates that an edge exists between entity p (source) and entity
q (destination). as the equations indicate, a(p) is the sum of the hub scores of
the entities that point at p, and h(p) is the sum of the authority scores pointed at
by p. thus, to be a strong authority, an entity must have many incoming edges, all

412

10 social search

with relatively moderate hub scores, or have very few incoming links that have very
large hub scores. similarly, to be a good hub, an entity must have many outgoing
edges to less authoritative pages, or few outgoing edges to very highly authorita-
tive pages.

fig. 10.3. illustration of the hits algorithm. each row corresponds to a single iteration
of the algorithm and each column corresponds to a specific step of the algorithm.

an iterative version of hits is given in algorithm 3. the algorithm begins
by initializing all hub and authority scores to 1. the algorithm then updates the
hub and authority scores according to the equations we just showed. then, the
hub scores are normalized so that the sum of the scores is 1. the same is also done
for the authority scores. the entire process is then repeated on the normalized
scores for a fixed number of iterations, denoted by k. the algorithm is guaranteed
to converge and typically does so after a small number of iterations. figure 10.3
shows an example of the hits algorithm applied to a graph with seven nodes
and six edges. the algorithm is carried out for three iterations. notice that the
nodes with many incoming edges tend to have higher authority scores, and those
with more outgoing edges tend to have larger hub scores. another characteristic

1, 11, 11, 11, 11, 11, 11, 12, 01, 10, 30, 13, 00, 10, 0.67, 0.50, .330, 10, .50.83, 00, .500,0.57, 0.43, .330,10, .42.86, 00, .420,0,,.33, 0.17, .170, .500, .17.50, 00, .170,0.33, 0.25, .140, .430, .21.42, 00, .210,0.31, 0.23, .160, .460, .19.46, 00, .190,0,,,.33, 0.17, .170, .500, .17.50, 00, .170,0,.33, 0.25, .140, .430, .21.42, 00, .21,0,0iteration 1: inputiteration 1: update scoresiteration 1: normalize scoresiteration 2: inputiteration 2: update scoresiteration 2: normalize scoresiteration 3: inputiteration 3: update scoresiteration 3: normalize scores10.3 searching with communities

413

of the hits algorithm is that nodes that are not connected to any other nodes
will always have hub and authority scores of 0.

once the hub and authority scores have been computed, the entities can be
ranked according to their authority score. this list will contain the most authori-
tative entities within the community. such entities are likely to be the    leaders    or
form the    core    of the community, based on their interactions with other mem-
bers of the community. for example, if this algorithm were applied to the com-
puter science researcher citation graph to find the information retrieval research
community, the most authoritative authors would be those that are cited many
times by prolific authors. these would arguably be the luminaries in the field and
those authors that form the core of the research community. when finding web
communities on the web graph, the algorithm will return pages that are linked to
by a large number of reputable web pages.

fig. 10.4. example of how nodes within a directed graph can be represented as vectors.
for a given node p, its vector representation has component q set to 1 if p     q.

id91 algorithms, such as the ones described in chapter 9, may also be
used for finding online communities. these algorithms easily adapt to the prob-

15437260000000000001010001100000010100000000000000000000node:vector:12345671543726136465366661515414

10 social search

lem, since community finding is an inherently unsupervised learning problem.
both agglomerative id91 and id116 can be used for finding communi-
ties. both of these id91 algorithms require a function that measures the dis-
tance between two clusters. as we discussed in chapter 9, the euclidean distance
is often used. however, it is not immediately clear how to apply the euclidean
distance to nodes in a (directed or undirected) graph. one simple way is to rep-
resent each node (entity) in the graph as a vector that has |v | components   one
for every node in the graph. for some node p, component q of its vector repre-
sentation is set to 1 if p     q, and 0 otherwise. this results in each node being
represented by the nodes it points to. figure 10.4 shows how the nodes of a graph
are represented this way. each vector may optionally be normalized. using this
representation, it is possible to use the euclidean distance and directly apply ag-
glomerative id91 or id116 to the problem. a high similarity according
to the euclidean distance will occur when two entities have edges directed toward
many of the same entities. for example, returning to the problem of finding the
information retrieval research community, two authors would be considered very
similar if they tended to cite the same set of authors. it is realistic to assume that
most members of the information retrieval research community tend to cite many
of the same authors, especially those that would be given a large authority score
by hits.

evaluating the e   ectiveness of community-finding algorithms is typically more
di   cult than evaluating traditional id91 tasks, since it is unclear how to de-
termine whether some entity should be part of a given community. in fact, it is
likely that if a group of people were asked to manually identify online communi-
ties, there would be many disagreements due to the vague definition of a commu-
nity. therefore, it is impossible to say whether hits or agglomerative id91
is better at finding communities. the best choice largely depends on the task and
data set for the application.

now that you have seen several ways of automatically finding online commu-
nities, you may be wondering how this information can be put to practical use.
there are many di   erent things that can be done after a set of communities has
been identified. for example, if a user has been identified as part of the informa-
tion retrieval research community, then when the user visits a web page, targeted
content may be displayed to the user that caters to her specific interests. search
engines could use the community information as additional contextual informa-
tion for improving the relevance of search results, by retrieving results that are also
topically related to the community or communities associated with the user. on-

10.3 searching with communities

415

line community information can also be used in other ways, including enhanced
browsing, identifying experts, website recommendation, and possibly even sug-
gesting who may be a compatible date for you!

10.3.3 community-based id53

in the last section, we described how to automatically find online communities. in
this section, we describe how such communities can be used e   ectively to help an-
swer complex information needs that would be di   cult to answer using conven-
tional search engines. for example, consider a person who is interested in learning
about potential interactions between a medicine he was just prescribed and an
herbal tea he often drinks. he could use a search engine and spend hours entering
various queries, looking at search results, and trying to find useful information
on the subject. the di   culty with using this approach is that no single page may
exist that completely satisfies the person   s information need. now, suppose that
the person could ask his question directly to a large group of other people, sev-
eral of whom are pharmacists or herbal experts. he would be much more likely
to get an answer. this search scenario, where a person submits a question to a
community consisting of both experts and non-experts in a wide range of topics,
each of whom can opt to answer the question, is called community-based ques-
tion answering (cqa). these systems harness the power of human knowledge in
order to satisfy a broad range of information needs. several popular commercial
systems of this type exist today, including yahoo! answers and naver, a korean
search portal.

there are both pros and cons to cqa systems. the pros include users being
able to get answers to complex or obscure information needs; the chance to see
multiple, possibly di   ering opinions about a topic; and the chance to interact
with other users who may share common interests, problems, and goals. the cons
include the possibility of receiving no answer at all to a question, having to wait
(possibly days) for an answer, and receiving answers that are incorrect, misleading,
o   ensive, or spam.

as we just mentioned, many of the answers that people submit are of low qual-
ity. it turns out, however, that the old computer programming adage of    garbage
in, garbage out    also applies to questions. studies have shown that low-quality an-
swers are often given to low-quality questions. indeed, there are a wide range of
questions that users can, and do, ask. table 10.1 shows a small sample of questions
submitted to yahoo! answers. some of the questions in the table are well-formed

416

10 social search

what part of mexico gets the most tropical storms?

how do you pronounce the french words, coeur and miel?

ged test?

why do i have to pay this fine?

what is schr  dinger   s cat?

what   s this song?

hi...can u ppl tell me sumthing abt death dreams??

what are the engagement and wedding traditions in egypt?

what lessons from the tao te ching do you apply to your everyday life?

fun things to do in la?

foci of a hyperbola?

what should i do today?

heather locklear?

whats a psp xmb?

why was itunes deleted from my computer?

do people in the australian defense force (raaf) pay less tax than civilians?

if c(-3, y) and d(1, 7) lie upon a line whose slope is 2, find the value of y.?

why does love make us so irrational?

am i in love?

what are some technologies that are revolutionizing business?
table 10.1. example questions submitted to yahoo! answers

and grammatically correct, whereas others are not. in addition, some of the ques-
tions have simple, straightforward answers, but many others do not.

besides allowing users to ask and answer questions, cqa services also provide
users with the ability to search the archive of previously asked questions and the
corresponding answers. this search functionality serves two purposes. first, if a
user finds that a similar question has been asked in the past, then they may not
need to ask the question and wait for responses. second, search engines may aug-
ment traditional search results with hits from the question and answer database.
for example, if a user enters the query    schr  dingers cat   , the search engine may
choose to return answers to    what is schr  dinger   s cat?    (which appears in ta-
ble 10.1) in addition to the other, more standard set of ranked web pages.

therefore, given a query,2 it is important to be able to automatically find po-
tential answers in the question and answer database. there are several possible

2 for the remainder of our discussion, we use the term    query    to refer to a question or

a search query.

10.3 searching with communities

417

ways to search this database. new queries can be matched against the archived
questions alone, the archived answers alone, or questions and answers combined.
studies have shown that it is better to match queries against the archived ques-
tions rather than answers, since generally it is easier to find related questions
(which are likely to have relevant answers) than it is to match queries directly to
answers.

matching queries to questions can be achieved using any of the retrieval mod-
els described in chapter 7, such as id38 or bm25. however, tradi-
tional retrieval models are likely to miss many relevant questions because of the
vocabulary mismatch problem. here, vocabulary mismatch is caused by the fact
that there are many di   erent ways to ask the same question. for example, suppose
we had the query    who is the leader of india?   . related questions are    who is the
prime minister of india?   ,    who is the current head of the indian government?   ,
and so on. notice that the only terms in common among any two of these ques-
tions are    who   ,    is   ,    the   ,    of    , and    india   . blindly applying any standard retrieval
model would retrieve non-related questions such as    who is the finance minister
of india?    and    who is the tallest person in all of india?   . stopword removal in this
case does not help much. instead, better matches can be achieved by generalizing
the notion of    leader    to include other concepts, such as    prime minister   ,    head   ,
and    government   .

in section 6.4, we described cross-language retrieval, where a user queries in a
source language (e.g., english) and documents are retrieved in a target language
(e.g., french). most of the retrieval methods developed for cross-language re-
trieval are based on machine translation techniques, which require learning trans-
lation probabilities of the form p (s|t), where s is a word in the source language
and t is a word in the target language. translation models can also be used to
help overcome the vocabulary mismatch problem within a single language. this
is achieved by estimating p (t|t
    are both words in the same
language. this id203 can be interpreted as the id203 that word t is
   . returning to our example, it is likely that p (leader|minister)
used in place of t
and p (leader|government) would have non-zero values, and therefore result in
more relevant questions being retrieved. we will now describe two translation-
based models that have been used for finding related questions and answers in an
archive.

), where t and t

the first model was proposed by berger and la   erty (1999). it is similar to the
query likelihood model described in section 7.3.1, except it allows query terms to

   

418

10 social search

   

   

w   q

t   v

be    translated    from other terms. given a query, related questions3 are ranked
according to:

p (q|a) =

p (w|t)p (t|a)

where q is the query, a is a related question in the archive, v is the vocabulary,
p (w|t) are the translation probabilities, and p (t|d) is the smoothed id203
of generating t given document d (see section 7.3.1 for more details). therefore,
we see that the model allows query term w to be translated from other terms t
that may occur in the question. one of the primary issues with this model is that
there is no guarantee the question will be related to the query; since every term
is translated independently, the question with the highest score may be a good
term-for-term translation of the query, but not a good overall translation.

the second model, developed by xue et al. (2008), is an extension of berger   s
model that attempts to overcome this issue by allowing matches of the original
query terms to be given more weight than matches of translated terms. under
this model, questions (or answers) are ranked using the following formula:

(1       )fw;a +   

t   v p (w|t)ft;a +    cw|c|

   

w   q

p (q|a) =

   
|a| +   

where    is a parameter between 0 and 1 that controls the influence of the transla-
tion probabilities, and    is the dirichlet smoothing parameter. notice that when
   = 0, this model is equivalent to the original query likelihood model, with no
influence from the translation model. as    approaches 1, the translation model
begins to have more impact on the ranking and becomes more similar to berger   s
model.

ranking using these models can be computationally expensive, since each in-
volves a sum over the entire vocabulary, which can be very large. query processing
speeds can be significantly improved by considering only a small number of trans-
lations per query term. for example, if the five most likely translations of each
query term are used, the number of terms in the summation will be reduced from
v to 5.

the one major issue that has been ignored thus far is how to compute the trans-
lation probabilities. in cross-language retrieval, translation probabilities can be

3 the discussion focuses on question retrieval, but the same models can be used to re-

trieve archived answers. as we said, question retrieval is generally more e   ective.

10.3 searching with communities

419

search
search
google

xp
xp

everest
everest
mountain window
install
drive

information

tallest
29,035
internet
highest computer website

mt
ft

measure

version
click
pc

feet
program
mount microsoft

web
list
free
info
page

table 10.2. translations automatically learned from a set of question and answer pairs.
the 10 most likely translations for the terms    everest   ,    xp   , and    search    are given.

1, dt

n , dt

1), . . . , (ds

automatically learned using a parallel corpus. translation probabilities are esti-
mated from pairs of documents of the form {(ds
n )}, where
i is document i written in the source language and dt
i is document i written
ds
in the target language. however, the notion of a parallel corpus becomes hazy
when dealing with intralanguage translations. a variety of approaches have been
used for estimating translation probabilities within the same language. for finding
related questions, one of the most successful approaches makes the assumption
that question/answer pairs form a parallel corpus from which translation prob-
abilities can be estimated. that is, translation probabilities are estimated from
archived pairs of the form {(q1, a1), . . . , (qn , an )}, where qi is question i
and ai is answer i. example translations estimated from a real question and an-
swer database using this approach are shown in table 10.2. pointers to algorithms
for estimating translation probabilities given a parallel corpus are given in the
   references and further reading    section at the end of this chapter.

in this section, we assumed that people in a community would provide an-
swers to questions, and an archive of questions and answers would be created by
this process. as we mentioned in chapter 1, it is also possible to design question
answering systems that find answers for a more limited range of questions in the
text of large document corpora. we describe these systems in more detail in chap-
ter 11.

420

10 social search

10.3.4 collaborative searching
the final community-based search task that we consider is collaborative search-
ing. as the name suggests, collaborative searching involves a group of users with a
common goal searching together in a collaborative setting. there are many situa-
tions where collaborative searching can be useful. for example, consider a group
of students working together on a world history report. in order to complete the
report, the students must do background research on the report topic. tradition-
ally, the students would split the topic into various subtopics, assign each group
member one of the subtopics, and then each student would search the web or an
online library catalog, independently of the other students, for information and
resources on their specific subtopic. in the end, the students would have to com-
bine all of the information from the various subtopics to form a coherent report.
each student would learn a great deal about his or her particular subtopic, and
no single student would have a thorough understanding of all of the material in
the report. clearly, every student would end up learning a great deal more if the
research process were more collaborative. a collaborative search system would al-
low the students to search the web and other resources together, so that every
member of the group could contribute and understand every subtopic of the re-
port. collaborative search can also be useful within companies, where colleagues
must collect information about various aspects of a particular project. last, but
certainly not least, recreational searchers may find collaborative search systems
particularly useful. suppose you and your friends are planning a party. a collabo-
rative search system would help everyone coordinate information-gathering tasks,
such as finding recipes, choosing decorations, selecting music, deciding on invita-
tions, etc.

there are two common types of collaborative search scenarios, depending on
where the search participants are physically located with respect to each other.
the first scenario, known as co-located collaborative search, occurs when all of the
search participants are in the same location, such as the same o   ce or same li-
brary, sitting in front of the same computer. the other scenario, known as remote
collaborative searching, occurs when the search participants are physically located
in di   erent locations. the participants may be in di   erent o   ces within the same
building, di   erent buildings within the same city, or even in completely di   erent
countries across the globe. figure 10.5 provides a schematic for these scenarios.
both situations present di   erent challenges, and the systems developed for each
have di   erent requirements in terms of how they support search. to illustrate this,
we briefly describe two examples of collaborative search systems.

10.3 searching with communities

421

fig. 10.5. overview of the two common collaborative search scenarios. on the left is co-
located collaborative search, which involves multiple participants in the same location
at the same time. on the right is remote collaborative search, where participants are in
di   erent locations and not necessarily all online and searching at the same time.

the cosearch system developed by amershi and morris (2008) is a co-located
collaborative search system. the system has a primary display, keyboard, and
mouse that is controlled by the person called the    driver   , who leads the search
task. additional participants, called    observers   , each have a mouse or a bluetooth-
enabled4 mobile phone. the driver begins the session by submitting a query to a
search engine. the search results are displayed on the primary display and on the
display of any user with a mobile phone. observers may click on search results,
which adds the corresponding page into a shared    page queue.    this allows every
participant to recommend which page should be navigated to next in a conve-
nient, centralized manner, rather than giving total control to the driver. in addi-
tion to the page queue, there is also a    query queue,    where participants submit
new queries. the query queue provides everyone with a list of potentially useful
queries to explore next, and provides the driver with a set of options generated

4 bluetooth is the name of a short-range wireless technology that allows for communi-

cation between devices, such as laptops, printers, pdas, and mobile phones.

co-located collaborative searchingremote collaborative searching422

10 social search

collectively by the group. the cosearch system provides many useful ways for
a group of people to collaboratively search together, since it allows everyone to
work toward a common task, while at the same time preserving the important di-
vision of labor that is part of collaboration, via the use of multiple input devices.
an example of a remote collaborative search system is searchtogether, devel-
oped by morris and horvitz (2007b). in this system, it is assumed that every
participant in the session is in a di   erent location and has his own computer.
furthermore, unlike co-located search, which assumes that all of the participants
are present during the entire search session, remote search makes no assumptions
about whether everyone is online at the same time. therefore, whereas co-located
search sessions tend to be transient, remote search sessions can be persistent. users
of the system may submit queries, which are logged and shared with all of the
other search participants. this allows all participants to see what others are search-
ing for, and allows them to resubmit or refine the queries. users can add ratings
(e.g.,    thumbs up    or    thumbs down   ) and comments to pages that are viewed
during the search process, which will be aggregated and made available to other
participants. in addition, a participant may explicitly recommend a given page
to another participant, which will then show up in her recommended pages list.
therefore, the searchtogether system provides most of the functionality of the
cosearch system, except it is adapted to the specific needs of remote collabora-
tion. one particular advantage of a persistent search session is that new partici-
pants, who were not previously part of the search process, can quickly be brought
up to speed by browsing the query history, page ratings, comments, and recom-
mendations.

as we have outlined, collaborative search systems provide users with a unique
set of tools to e   ectively collaborate with each other during a co-located or re-
mote search session. despite the promise of such systems, very few commercial
collaborative search systems exist today. however, such systems are beginning to
gain considerable attention in the research community. given this, and the in-
creasingly collaborative nature of the online experience, it may be only a matter
of time before collaborative search systems become more widely available.

10.4 filtering and recommending

423

10.4 filtering and recommending

10.4.1 document filtering

as we mentioned previously, one part of social search applications is representing
individual users    interests and preferences. one of the earliest applications that fo-
cused on user profiles was document filtering. document filtering, often simply
referred to as filtering, is an alternative to the standard ad hoc search paradigm.
in ad hoc search, users typically enter many di   erent queries over time, while the
document collection stays relatively static. in filtering, the user   s information need
stays the same, but the document collection itself is dynamic, with new docu-
ments arriving periodically. the goal of filtering, then, is to identify (filter) the rel-
evant new documents and send them to the user. filtering, as described in chapter
3, is a push application.

filtering is also an example of a supervised learning task, where the profile plays
the part of the training data and the incoming documents are the test items that
need to be classified as    relevant    or    not relevant.    however, unlike a spam de-
tection model, which would take thousands of labeled emails as input, a filtering
system profile may only consist of a single query, making the learning task even
more challenging. for this reason, filtering systems typically use more specialized
techniques than general classification techniques in order to overcome the lack of
training data.

although they are not as widely used as standard web search engines, there
are many examples of real-world document filtering systems. for example, many
news sites o   er filtering services. these services include alerting users when there is
breaking news, when an article is published in a certain new category (e.g., sports
or politics), or when an article is published about a certain topic, which is typically
specified using one or more keywords (e.g.,    terrorism    or    global warming   ). the
alerts come in the form of emails, sms (text messages), or even personalized news
feeds, thereby allowing the user to keep up with topics of interest without having
to continually check the news site for updates or enter numerous queries to the
site   s search engine. therefore, filtering provides a way of personalizing the search
experience by maintaining a number of long-term information needs.

document filtering systems have two key components. first, the user   s long-
term information needs must be accurately represented. this is done by construct-
ing a profile for every information need. second, given a document that has just ar-
rived in the system, a decision mechanism must be devised for identifying which
are the relevant profiles for that document. this decision mechanism must not

424

10 social search

only be e   cient, especially since there are likely to be thousands of profiles, but
it must also be highly accurate. the filtering system should not miss relevant doc-
uments and, perhaps even more importantly, should not be continually alerting
users about non-relevant documents. in the remainder of this section, we describe
the details of these two components.

profiles

in web search, users typically enter a very short query. the search engine then
faces the daunting challenge of determining the user   s underlying information
need from this very sparse piece of information. there are numerous reasons why
most search engines today expect information needs to be specified as short key-
word queries. however, one of the primary reasons is that users do not want to
(or do not have the time to) type in long, complex queries for each and every one
of their information needs. many simple, non-persistent information needs can
often be satisfied using a short query to a search engine. filtering systems, on the
other hand, cater to long-term information needs. therefore, users may be more
willing to spend more time specifying their information need in greater detail in
order to ensure highly relevant results over an extended period of time. the rep-
resentation of a user   s long-term information need is often called a filtering profile
or just a profile.

what actually makes up a filtering profile is quite general and depends on
the particular domain of interest. profiles may be as simple as a boolean or key-
word query. profiles may also contain documents that are known to be relevant or
non-relevant to the user   s information need. furthermore, they may contain other
items, such as social tags and related named entities. finally, profiles may also have
one or more relational constraints, such as    published before 1990   ,    price in the
$10   $25 range   , and so on. whereas the other constraints described act as soft
filters, relational constraints of this form act as hard filters that must be satisfied
in order for a document to be retrieved.

although there are many di   erent ways to represent a profile, the underlying
filtering model typically dictates the actual representation. filtering models are
very similar to the retrieval models described in chapter 7. in fact, many of the
widely used filtering models are simply retrieval models where the profile takes the
place of the query. there are two common types of filtering models. the first are
static models. here, static refers to the fact that the user   s profile does not change
over time, and therefore the same model can always be applied. the second are

10.4 filtering and recommending

425

adaptive models, where the user   s profile is constantly changing over time. this
scenario requires the filtering model to be dynamic over time as new information
is incorporated into the profile.

static filtering models

as we just described, static filtering models work under the assumption that the
filtering profile remains static over time. in some ways, this makes the filtering
process easier, but in other ways it makes it less robust. all of the popular static
filtering models are derived from the standard retrieval models described in chap-
ter 7. however, unlike web search, filtering systems do not return a ranked list of
documents for each profile. instead, when a new document enters the system, the
filtering system must decide whether or not it is relevant with respect to each pro-
file. figure 10.6 illustrates how a static filtering system works. as new documents
arrive, they are compared to each profile. arrows from a document to a profile
indicate that the document was deemed relevant to the profile and returned to
the user.

fig. 10.6. example of a static filtering system. documents arrive over time and are com-
pared against each profile. arrows from documents to profiles indicate the document
matches the profile and is retrieved.

document streamt = 2t = 3t = 5t = 8profile1profile2profile3t=2t=3t=5t=8426

10 social search

in the most simple case, a boolean retrieval model can be used. here, the filter-
ing profile would simply consist of a boolean query, and a new document would
be retrieved for the profile only if it satisfied the query. the boolean model, de-
spite its simplicity, can be used e   ectively for document filtering, especially where
precision is important. in fact, many web-based filtering systems make use of a
boolean retrieval model.

one of the biggest drawbacks of the boolean model is the low level of recall.
depending on the filtering domain, users may prefer to have good coverage over
very high precision results. there are various possible solutions to this problem,
including using the vector space model, the probabilistic model, bm25, or lan-
guage modeling. all of these models can be extended for use with filtering by
specifying a profile using a keyword query or a set of documents. directly apply-
ing these models to filtering, however, is not trivial, since each of them returns
a score, not a    retrieve    or    do not retrieve    answer as in the case of the boolean
model. one of the most widely used techniques for overcoming this problem is to
use a score threshold to determine whether to retrieve a document. that is, only
documents with a similarity score above the threshold will be retrieved. such a
threshold would have to be tuned in order to achieve good e   ectiveness. many
complex issues arise when applying a global score threshold, such as ensuring that
scores are comparable across profiles and over time.

as a concrete example, we describe how static filtering can be done within
the id38 framework for retrieval. given a static profile, which may
consist of a single keyword query, multiple queries, a set of documents, or some
combination of these, we must first estimate a profile language model denoted by
p . there are many ways to do this. one possibility is:

(1       )   

k
i=1   i

k   

i=1

p (w|p ) =

  i

fw;ti
|ti| +   

cw|c|

where t1, . . . , tk are the pieces of text (e.g., queries, documents) that make up
the profile, and   i is the weight (importance) associated with text ti. the other
variables and parameters are defined in detail in chapter 7.

then, given an incoming document, a document language model (d) must be
estimated. we again follow the discussion in chapter 7 and estimate d using the
following:

p (w|d) = (1       )

fw;d|d| +   

cw|c|

10.4 filtering and recommending

427

documents can then be ranked according to the negative kl-divergence be-
tween the profile language model (p ) and the document language model (d) as
follows:

   kl(p||d) =

p (w|p ) log p (w|d)    

p (w|p ) log p (w|p )

   

w   v

   

w   v

document d is then delivered to profile p if    kl(p||d)     t, where t is some
relevance threshold.

document filtering can also be treated as a machine learning problem. at its
core, filtering is a classification task that often has a very small amount of training
data (i.e., the profile). the task is then to build a binary classifier that determines
whether an incoming document is relevant to the profile. however, training data
would be necessary in order to properly learn such a model. for this task, the train-
ing data comes in the form of binary relevance judgments over profile/document
pairs. any of the classification techniques described in chapter 9 can be used.
suppose that a support vector machine with a linear kernel is used; the scoring
function would then have the following form:
s(p ; d) = w    f (p, d) = w1f1(p, d) + w2f2(p, d) + . . . + wdfd(p, d)
where w1, . . . , wd are the set of weights learned during the id166 training pro-
cess, and f1(p, d), . . . , fd(p, d) are the set of features extracted from the pro-
file/document pair. many of the features that have been successfully applied to
text classification, such as unigrams and bigrams, can also be applied to filtering.
given a large amount of training data, it is likely that a machine learning approach
will outperform the simple id38 approach just described. however,
when there is little or no training data, the id38 approach is a good
choice.

adaptive filtering models

static filtering profiles are assumed not to change over time. in such a setting, a
user would be able to create a profile, but could not update it to better reflect his
information need. the only option would be to delete the profile and create a new
one that would hopefully produce better results. this type of system is rigid and
not very robust.

adaptive filtering is an alternative filtering technique that allows for dynamic
profiles. this technique provides a mechanism for updating the profile over time.

428

10 social search

fig. 10.7. example of an adaptive filtering system. documents arrive over time and are
compared against each profile. arrows from documents to profiles indicate the document
matches the profile and is retrieved. unlike static filtering, where profiles are static over
time, profiles are updated dynamically (e.g., when a new match occurs).

profiles may be either updated using input from the user or done automatically
based on user behavior, such as click or browsing patterns. there are various rea-
sons why it may be useful to update a profile as time goes on. for example, users
may want to fine-tune their information need in order to find more specific types
of information. therefore, adaptive filtering techniques are more robust than
static filtering techniques and are designed to adapt to find more relevant docu-
ments over the life span of a profile. figure 10.7 shows an example adaptive filter-
ing system for the same set of profiles and incoming documents from figure 10.6.
unlike the static filtering case, when a document is delivered to a profile, the user
provides feedback about the document, and the profile is then updated and used
for matching future incoming documents.

as figure 10.7 suggests, one of the most common ways to adapt a profile is
in response to user feedback. user feedback may come in various forms, each of
which can be used in di   erent ways to update the user profile. in order to provide
a concrete example of how profiles can be adapted in response to user feedback,
we consider the case where users provide relevance feedback (see chapter 6) on

document streamt = 2t = 3t = 5t = 8profile1profile2profile3profile 1.1profile 2.1profile3.1t=2t=3t=5t=810.4 filtering and recommending

429

documents. that is, for some set of documents, such as the set of documents re-
trieved for a given profile, the user explicitly states whether or not the document
is relevant to the profile. given the relevance feedback information, there are a
number of ways to update the profile. as before, how the profile is represented
and subsequently updated largely depends on the underlying retrieval model that
is being used.

as described in chapter 7, the rocchio algorithm can be used to perform rel-
evance feedback in the vector space model. therefore, if profiles are represented
as vectors in a vector space model, rocchio   s algorithm can be applied to update
the profiles when the user provides relevance feedback information. given a pro-
file p , a set of non-relevant feedback documents (denoted n onrel), and a set of
    is computed
relevant feedback documents (denoted rel), the adapted profile p
as follows:

   

p

=   .p +   .

1|rel|

di       .

1

|n onrel|

   

di

di   n onrel

   

di   rel

where di is the vector representing document i, and   ,   , and    are parameters
that control how to trade o    the weighting between the initial profile, the relevant
documents, and the non-relevant documents.

chapter 7 also described how relevance models can be used with language mod-
eling for pseudo-relevance feedback. however, relevance models can also be used
for true relevance feedback as follows:

   

   
   

di   rel

di   rel

d   c
p (w|di)

p (w|p ) =

1|rel|
    1|rel|

p (w|d)p (di|d)

wherec is the set of documents in the collection, rel is the set of documents that
have been judged relevant, di is document i, and p (di|d) is the id203 that
document di is generated from document d   s language model. the approxima-
tion (   ) can be made because di is a document, and p (di|d) is going to be 1 or
very close to 1 when di = d and nearly 0 for most other documents. therefore,
the id203 of w in the profile is simply the average id203 of w in the
language models of the relevant documents. unlike the rocchio algorithm, the
non-relevant documents are not considered.

430

10 social search

if a classification technique, such as one of those described in chapter 9, is
used for filtering, then an online learning algorithm can be used to adapt the clas-
sification model as new user feedback arrives. online learning algorithms update
model parameters, such as the hyperplane w in id166s, by considering only one
new item or a batch of new items. these algorithms are di   erent from standard
supervised learning algorithms because they do not have a    memory.    that is, once
an input has been used for training, it is discarded and cannot be explicitly used
in the future to update the model parameters. only the new training inputs are
used for training. the details of online learning methods are beyond the scope of
this book. however, several references are given in the    references and further
reading    section at the end of this chapter.

model
boolean

vector space

profile representation
boolean expression

vector

profile updating

n/a

rocchio

id38 id203 distribution relevance modeling

classification

model parameters

online learning

table 10.3. summary of static and adaptive filtering models. for each, the profile repre-
sentation and profile updating algorithm are given.

both static and adaptive filtering, therefore, can be considered special cases of
many of the retrieval models and techniques described in chapters 6, 7, and 9.
table 10.3 summarizes the various filtering models, including how profiles are
represented and updated. in practice, the vector space model and language mod-
eling have been shown to be e   ective and easy to implement, both for static and
adaptive filtering. the classification models are likely to be more robust in highly
dynamic environments. however, as with all classification techniques, the model
requires training data to learn an e   ective model.

fast filtering with millions of profiles

in a full-scale production system, there may be thousands or possibly even mil-
lions of profiles that must be matched against incoming documents. fortunately,
standard information retrieval indexing and query evaluation strategies can be
applied to perform this matching e   ciently. in most situations, profiles are rep-
resented as a set of keywords or a set of feature values, which allows each profile to

10.4 filtering and recommending

431

be indexed using the strategies discussed in chapter 5. scalable indexing infras-
tructures can easily handle millions, or possibly even billions, of profiles. then,
once the profiles are indexed, an incoming document can be transformed into a
   query   , which again is represented as either a set of terms or a set of features. the
   query    is then run against the index of profiles, retrieving a ranked list of profiles.
the document is then delivered to only those profiles whose score, with respect
to the    query   , is above the relevance threshold previously discussed.

evaluation
many of the id74 described in chapter 8 can be used to evaluate
filtering systems. however, it is important to choose appropriate metrics, because
filtering di   ers in a number of ways from standard search tasks, such as news or
web search. one of the most important di   erences is the fact that filtering sys-
tems do not produce a ranking of documents for each profile. instead, relevant
documents are simply delivered to the profile as they arrive. therefore, measures
such as precision at rank k and mean average precision are not appropriate for the
task. instead, set-based measures are typically used.

relevant non-relevant

retrieved

not retrieved

tp
fn

fp
tn

table 10.4. contingency table for the possible outcomes of a filtering system. here, tp
(true positive) is the number of relevant documents retrieved, fn (false negative) is the
number of relevant documents not retrieved, fp (false positive) is the number of non-
relevant documents retrieved, and tn (true negative) is the number of non-relevant doc-
uments not retrieved.

table 10.4, which is similar to table 8.3 in chapter 8, shows all of the possi-
bilities for an incoming document with respect to some profile. a document may
be either relevant or non-relevant, as indicated by the column headings. further-
more, a document may be either retrieved or not retrieved by the filtering system,
as indicated by the row headings. all of the filtering id74 described
here can be formulated in terms of the cells in this table.

the simplest way to evaluate a filtering system is using the classical evaluation
t p +f n , re-
measures of precision and recall, which correspond to
spectively. the f measure, which is the harmonic mean of precision and recall, is

t p +f p and

t p

t p

432

10 social search

also commonly used. typically, these measures are computed for each profile in
the system and then averaged together.5

it is possible to define a more general evaluation metric that combines each of

the four cells in table 10.4 in the following way:

u =       t p +       t n +       f p +       f n

where the coe   cients   ,   ,   , and    can be set in di   erent ways to achieve di   er-
ent trade-o   s between the various components of the measure. one setting of the
coe   cients that has been widely used in filtering experiments is    = 2,    = 0,
   =    1, and    = 0. this results in true positives (relevant retrieved documents)
being given weight 2 and false negatives (relevant documents that were not re-
trieved) being penalized by a factor of 1. of course, di   erent coe   cients can be
chosen based on the actual costs of the underlying task.

10.4.2 id185

static and adaptive filtering are not social tasks, in that profiles are assumed to be
independent of each other. if we now consider the complex relationships that ex-
ist between profiles, additional useful information can be obtained. for example,
suppose that we have an adaptive filtering system with two profiles, which we call
profile a (corresponding to user a) and profile b (corresponding to user b). if
both user a and b judged a large number of the same documents to be relevant
and/or non-relevant to their respective profiles, then we can infer that the two
profiles are similar to each other. we can then use this information to improve
the relevance of the matches to both user a and b. for example, if user a judged a
document to be relevant to profile a, then it is likely that the document will also
be relevant to profile b, and so it should probably be retrieved, even if the score
assigned to it by the adaptive filtering system is below the predetermined thresh-
old. such a system is social, in the sense that a document is returned to the user
based on both the document   s topical relevance to the profile and any judgments
or feedback that users with similar profiles have given about the document.

filtering that considers the relationship between profiles (or between users)
and uses this information to improve how incoming items are matched to profiles
(or users) is called id185. id185 is often used as
a component of recommender systems. recommender systems use collaborative
5 recall that this type of averaging is known as macroaveraging.

10.4 filtering and recommending

433

filtering algorithms to recommend items (such as books or movies) to users. many
major commercial websites, such as amazon.com and netflix, make heavy use of
recommender systems to provide users with a list of recommended products in
the hopes that the user will see something she may like but may not have known
about, and consequently make a purchase. therefore, such systems can be valuable
both to the end users, who are likely to see relevant products, including some that
they may not have considered before, and to search engine companies, who can
use such systems to increase revenue.

in the remainder of this section, we focus on id185 algorithms
for recommender systems. it is important to note that these algorithms di   er from
static and adaptive filtering algorithms in a number of ways. first, when collabora-
tive filtering algorithms are used for making recommendations, they typically as-
sociate a single profile with each user. that is, the user is the profile. second, static
and adaptive filtering systems would make a binary decision (retrieve or do not
retrieve) for each incoming document, but id185 algorithms for
recommender systems provide ratings for items. these ratings may be 0 (relevant)
and 1 (non-relevant) or more complex, such as ratings on a scale of 1 through 5.
finally, id185 algorithms for recommender systems provide a rat-
ing for every incoming item, as well as every item in the database for which the
current user has not explicitly provided a judgment. on the other hand, static and
adaptive filtering algorithms only decide whether or not to send incoming docu-
ments to users and never retrospectively examine older documents to determine
whether they should be retrieved.

figure 10.8 represents a virtual space of users, where users with similar prefer-
ences and tastes are close to each other. the dialog boxes drawn above each user   s
head denote their preference for some item, such as a movie about tropical fish.
those users who have not rated the movie have question marks in their dialog
boxes. it is the job of the id185 algorithm to predict as accurately
as possible what rating these users would give to the movie.

id185 is conceptually simple, but the details can be di   cult to
get correct. for example, one must decide how to represent users and how to mea-
sure the similarity between them. after similar users have been identified, the user
ratings must be combined in some way. another important issue concerns how
id185 and, in particular, recommender systems should be evalu-
ated. we will address these issues in the remainder of this section while describing
the details of two id185 algorithms that have been used success-
fully in recommender systems.

434

10 social search

fig. 10.8. a set of users within a recommender system. users and their ratings for some
item are given. users with question marks above their heads have not yet rated the item.
it is the goal of the recommender system to fill in these question marks.

rating with user clusters
in both of the algorithms that follow, we assume that we have a set of users u
and a set of items i. furthermore, ru(i) is user u   s rating of item i, and ^ru(i) is
our system   s prediction for user u   s rating for item i. note that ru(i) is typically
undefined when user u has not provided a rating for item i, although, as we will
describe later, this does not need to be the case. therefore, the general collabora-
tive filtering task is to compute ^ru(i) for every user/item pair that does not have
an explicit rating. we assume that the only input we are given is the explicit ratings
ru(i), which will be used for making predictions. furthermore, for simplicity, we
will assume that ratings are integers in the range of 1 through m, although most
of the algorithms described will work equally well for continuous ratings.

one simple approach is to first apply one of the id91 algorithms de-
scribed in chapter 9 to the set of users. typically, users are represented by their
rating vectors ru = [ru(i1) . . . ru(i|u|)]. however, since not all users judge all
items, not every entry of the vector ru may be defined, which makes it challenging
to compute distance measures, such as cosine similarity. therefore, the distance
measures must be modified to account for the missing values. the simplest way to
do this is to fill in all of the missing ratings with some value, such as 0. another

41125?5315??11?10.4 filtering and recommending

435

possibility is to fill in the missing values with the user   s average rating, denoted by
ru, or the item   s average rating.

one of the common similarity measures used for id91 users is the corre-

lation measure, which is computed as follows:

      

   
i   iu   iu    (ru(i)     ru)2

   

i   iu   iu    (ru(i)     ru)    (ru   (i)     ru   )

i   iu   iu    (ru   (i)     ru   )2

    judged, respectively,
where iu and iu    are the sets of items that users u and u
which means that the summations are only over the set of items that both user
    judged. correlation takes on values in the range    1 to 1, with 1 being
u and u
achieved when two users have identical ratings for the same set of items, and    1
being achieved when two users rate items exactly the opposite of each other.

fig. 10.9. illustration of id185 using id91. groups of similar users
are outlined with dashed lines. users and their ratings for some item are given. in each
group, there is a single user who has not judged the item. for these users, the unjudged
item is assigned an automatic rating based on the ratings of similar users.

in figure 10.9, we provide a hypothetical id91 of the users, denoted by
dashed boundaries. after users have been clustered, any user within a cluster that

41125?5315??11?abcd436

10 social search

has not judged some item could be assigned the average rating for the item among
other users in the cluster. for example, the user who has not judged the tropical
fish movie in cluster a would be assigned a rating of 1.25, which is the average
of the ratings given to the movie by the four other users in cluster a. this can be
stated mathematically as:

   

^ru(i) =

1

|cluster(u)|

ru   (i)

u      cluster(u)

where cluster(u) represents the cluster that user u belongs to.

averaging the ratings of a group of users is one simple way of aggregating the
ratings within a cluster. another possible approach is to use the expected rating of
the item, given the ratings of the other users within the cluster, which is calculated
as:

m   
m   

x=1

x=1

^ru(i) =

=

x    p (ru(i) = x|c = cluster(u))
x    |u

: ru   (i) = x|
   
|cluster(u)|

where p (ru(i) = x|c = cluster(u)) is the id203 that user u will rate the
item with rating m, given that they are in cluster cluster(u). this id203
|u
   
|cluster(u)| , which is the proportion of users in cluster(u) who
is estimated as
have rated item i with value x. for example, if all of the users in cluster(u) rate
the item 5, then ^ru(i) will equal 5. however, if five users in cluster(u) rate the
item 1 and five users rate it 5, then ^ru(i) = 1    5

:ru    (i)=x|

10 + 5    5

one issue that arises when relying on id91 for predicting ratings is very
sparse clusters, such as cluster d in figure 10.9. what score should be assigned
to a user who does not fit nicely into a cluster and has rather unique interests and
tastes? this is a complex and challenging problem with no straightforward answer.
one simple, but not very e   ective, solution is to assign average item ratings to ev-
ery unrated item for the user. unfortunately, this explicitly assumes that    unique   
users are average, which is actually unlikely to be true.

10 = 3.

rating with nearest neighbors
an alternative strategy to using clusters is to use nearest neighbors for predicting
user ratings. this approach makes use of the k nearest neighbors id91 tech-
nique described in chapter 9. to predict ratings for user u, we first find the k

10.4 filtering and recommending

437

   

   

users who are closest to the user according to some similarity measure. once we
have found the nearest neighbors, we will use the ratings (and similarities) of the
neighbors for prediction as follows:

^ru(i) = ru +

1

u      n (u) sim(u, u   )

u      n (u)

sim(u, u

   

)(ru   (i)     ru   )

   

   

) is the similarity of user u and u

   , andn (u) is the set of u   s near-
where sim(u, u
est neighbors. this algorithm predicts user u   s rating of item i by first including
    in u   s nearest neighbor-
the user   s average item rating (ru). then, for every user u
hood, ru   (i)    ru    is weighted by sim(u, u
) and added into the predicted value.
you may wonder why ru   (i)     ru    is used instead of ru   (i). the di   erence is used
because ratings are relative. certain users may very rarely rate any item with 1,
whereas other users may never rate an item below 3. therefore, it is best to mea-
sure ratings relative to a given user   s average rating for the purpose of prediction.
although the approaches using clusters and nearest neighbors are similar in
nature, the nearest-neighbors approach tends to be more robust with respect to
noise. furthermore, when using nearest neighbors, there is no need to choose a
id91 cost function, only a similarity function, thereby simplifying things.
empirical results suggest that predicting ratings using nearest neighbors and the
correlation similarity measure tends to outperform the various id91 ap-
proaches across a range of data sets. based on this evidence, the nearest neighbor
approach, using the correlation similarity function, is a good choice for a wide
range of practical id185 tasks.

evaluation

id185 recommender systems can be evaluated in a number of
ways. standard information retrieval metrics, such as those described in chap-
ter 8, can be used, including accuracy, precision, recall, and the f measure.

however, standard information retrieval measures are very strict, since they
require the system to predict exactly the correct value. consider the case when
the actual user rating is 4 and system a predicts 3 and system b predicts 1. the
accuracy of both system a and b is zero, since they both failed to get exactly the
right answer. however, system a is much closer to the correct answer than system
b. for this reason, a number of id74 that consider the di   erence
between the actual and predicted ratings have been used. one such measure is
absolute error, which is computed as:

438

10 social search

abs =

1|u||i|

   

   

u   u

i   i

|^ru(i)     ru(i)|

where the sums are over the set of user/item pairs for which predictions have been
made. the other measure is called mean squared error, which can be calculated as:

   

   

u   u

i   i

m se =

1|u||i|

(^ru(i)     ru(i))2

the biggest di   erence between absolute error and mean squared error is that
mean squared error penalizes incorrect predictions more heavily, since the penalty
is squared.

these are the most commonly used id74 for collaborative filter-
ing recommender systems. so, in the end, which measure should you use? unfor-
tunately, that is not something we can easily answer. as we have repeated many
times throughout the course of this book, the proper evaluation measure depends
a great deal on the underlying task.

10.5 peer-to-peer and metasearch

10.5.1 distributed search

we have described social search applications that involve networks or communi-
ties of people, but a number of tools for finding and sharing information are im-
plemented using communities of    nodes,    where each node can store and search
information, and communicate with other nodes. the simplest form of this type
of distributed search environment is a metasearch engine, where each node is a
complete web search engine and the results from a relatively small number of dif-
ferent search engines are combined with the aim of improving the e   ectiveness of
the ranking. a peer-to-peer (p2p) search application, on the other hand, typically
has a large number of nodes, each with a relatively small amount of information
and only limited knowledge about other nodes.

in contrast to search applications that use only a single document collection,
all distributed search6 applications must carry out three additional, important
functions:
6 often called distributed information retrieval or federated search.

10.5 peer-to-peer and metasearch

439

    resource representation: generating a description of the information resource

(i.e., the documents) stored at a node.

    resource selection: selecting one or more resources to search based on their

descriptions.

    result merging: merging the ranked result lists from the searches carried out

at the nodes containing the selected information resources.
these functions are carried out by designated nodes that depend on the archi-
tecture of the application. the simplest assumption is that there will be one special
node that provides the directory services of selection and merging, and every other
node is responsible for providing its own representation. for a metasearch appli-
cation with the architecture shown in figure 10.10, the resource representation
and selection functions are trivial. rather than selecting which search engines to
use for a particular query, the query will instead be broadcast by the metasearch
engine to all engines being used by the application. for each search engine, this
is done using the application programming interface (api) and transforming the
query into the appropriate format for the engine. this transformation is generally
very simple since most search engine query languages are similar.

fig. 10.10. metasearch engine architecture. the query is broadcast to multiple web search
engines and result lists are merged.

more generally, in a distributed search environment, each node can be repre-
sented by the probabilities of occurrence of the words in the documents stored at

metasearchengineweb search engine 3web search engine 2web search engine 1queryresult listquerymergedresult list440

10 social search

that node. this is the unigram language model that was used to represent docu-
ments in chapter 7. in this case, there is only one language model representing all
the documents at that node, and the probabilities are estimated using word fre-
quencies summed over the documents. in other words, the documents stored at
that node are treated as one large document to estimate the language model. this
representation is compact, and has been shown to perform well in distributed
retrieval experiments. in some applications, nodes may not be actively cooperat-
ing in the distributed search protocol (process) and, in that case, only provide a
search api. a language model description of the contents of those nodes can still
be generated by query-based sampling. this involves generating a series of queries
to retrieve a sample of documents from the node, which are used to estimate the
language model. di   erent strategies have been used for selecting query terms, but
even queries based on random selection from the terms in retrieved documents
have been shown to generate an accurate language model.

resource selection in a general distributed search application involves first
ranking the nodes using their representations, and then selecting the top k ranked
nodes, or all nodes that score above some threshold value. since we are represent-
ing the nodes using a language model, the natural ranking algorithm to use is query
likelihood. this is sometimes referred to in the distributed search literature as the
kl-divergence resource ranking algorithm, since query likelihood is a special case
of kl-divergence. following the query likelihood score given in section 7.3.1, a
node n is ranked by the following score:

log p (q|n ) =

log fqi;n +   p (qi|c)

|n| +   

n   

i=1

where there are n query terms, fqi;n is the frequency of query term qi in the doc-
uments at node n, p (qi|c) is the background id203 estimated using some
large collection of text c, and |n| is the number of term occurrences in the doc-
uments stored at node n.

after nodes have been selected, local searches are carried out at each of those
nodes. the results of these searches must be merged to produce a single ranking.
if the same retrieval model (e.g., query likelihood) and the same global statistics
(e.g., the background model) are used for each local search, the merge can be based
directly on the local scores. if di   erent global statistics are used in each node, such
as calculating idf weights using only the documents at that node, then the local
scores can be recalculated by sharing these statistics before merging. if di   erent
retrieval models are used, or if global statistics cannot be shared, then the scores

10.5 peer-to-peer and metasearch

441

must be normalized before merging. a common heuristic approach to score nor-
malization is to use the score from the resource ranking of the node that returned
a document d, rd, to modify the local document score, sd, as follows:

s

d = sd(   + (1       )r
   
   
d)
   
where    is a constant, and r
d is the resource ranking score normalized with re-
spect to other resource scores. one way of normalizing the resource score is to cal-
culate the minimum and maximum possible scores for a given query, rmin and
rmax, and then:

d = (rd     rmin)/(rmax     rmin)
   

r

it is also possible to learn a score-normalizing function by comparing scores from
a sample document collection to the local scores (si & callan, 2003).

result merging in a metasearch application is somewhat di   erent than gen-
eral distributed search. the two main characteristics of metasearch are that doc-
ument scores from local searches generally are not available, and local searches
are often done over collections with very similar content. a metasearch engine
that uses multiple web search engines, for example, is essentially using di   erent
retrieval models on the same collection (the web). in this case, local searches pro-
duce ranked lists that have high overlap in terms of the documents that are re-
trieved. e   ective methods of combining ranked result lists have been developed
specifically for this situation. the most well-studied methods can be described by
   
the following formula, giving the modified score for a document, s
d, as a function
of the scores sd;i produced by the ith search engine:

k   

   
d = n(cid:13)

d

s

sd;i

i=1

where nd is the number of search engines that returned document d in the result
list,    = (   1, 0, 1), and there are k search engines that returned results. when
   =    1, the modified score is the average of the local search scores; when    = 0,
the modified score is the sum of the local scores; and when    = 1, the modified
score is the sum of the local scores weighted by the number of search engines that
returned document d. the last variation is known as combmnz (combine and
multiply by the number of non-zero results) and has been shown to be e   ective
in many search engine combination experiments.

442

10 social search

in a typical metasearch application, scores are not available, and document
ranks are used instead. in this case, the combmnz formula with scores based on
ranks can be used. this means, for example, that if m documents are retrieved in
a result list, the score for a document at rank r would be (m     r + 1)/m. this
rank-based combmnz produces a merged ranking with reasonable e   ective-
ness, although it is worse than a score-based combination. more e   ective rank-
based combinations can be achieved with techniques based on voting procedures
(montague & aslam, 2002).

in general, distributed search on non-overlapping collections can be compa-
rable in e   ectiveness to searching a single collection that is the union of all the
distributed collections. of course, in most applications it would not be possible
to build such a collection, but it does serve as a useful e   ectiveness benchmark.
experiments with trec collections indicate that when average precision at rank
10 or 20 is used as the e   ectiveness measure, a distributed search that selected
only 5   10 collections out of 200 was at least as e   ective as a centralized search,
and sometimes was more e   ective (powell et al., 2000). on the other hand, in a
p2p testbed where the collection was distributed between 2,500 nodes and only
1% of these nodes were selected, the average precision at rank 10 was 25% lower
than a centralized search (lu & callan, 2006).

metasearch, which combines di   erent searches on the same or very similar
collections, generally improves retrieval e   ectiveness compared to a single search.
trec experiments with metasearch have shown improvements of 5   20% in
mean average precision (depending on the query set used) for combinations using
four di   erent search engines, compared to the results from the single best search
engine (montague & aslam, 2002).

10.5.2 p2p networks

p2p networks are used in a range of applications involving communities of users,
although they were popularized through file-sharing applications for music and
video, such as kazaa and bearshare. search in file-sharing applications is gener-
ally restricted to finding files with a specified title or some other attribute, such
as the artist for music files. in other words, they support simple exact-match re-
trieval (see chapter 7). a number of di   erent network architectures or overlays7

7 a network overlay describes the logical connections between nodes implemented on

top of the underlying physical network, which is usually the internet.

10.5 peer-to-peer and metasearch

443

have been developed to support this type of search. figure 10.11 shows three of
these architectures.

fig. 10.11. network architectures for distributed search: (a) central hub; (b) pure p2p;
and (c) hierarchical p2p. dark circles are hub or superpeer nodes, gray circles are provider
nodes, and white circles are consumer nodes.

each node in a p2p network can act as a client, a server, or both. clients (infor-
mation consumers) issue queries to initiate search. servers (information providers)
respond to queries with files (if they have a match) and may also route queries to
other nodes. servers that maintain information about the contents of other nodes
provide a directory service and are called hubs. the architectures in figure 10.11
di   er primarily in how they route queries to providers. in the first architecture,
which was the basis of the pioneering napster file-sharing application, there is a
single central hub that provides directory services. consumers send queries to the
hub, which routes them to nodes that contain the matching files. although this

(a)(b)(c)444

10 social search

architecture is e   cient, it is also susceptible to failures or attacks that a   ect the
central hub.

in the second architecture, known as    pure    p2p (for example, gnutella 0.48),
there are no hubs. a query generated by a consumer is broadcast to the other nodes
in the network by flooding, which means a node sends the query to all the nodes
connected to it, each of those nodes sends the query to all of their connected
nodes, and so on. queries have a limited horizon, which restricts the number of
network    hops    that can be made before they expire. the connections between
nodes are random, and each node only knows about its neighbors. the problem
with this architecture is that it does not scale well, in that the network tra   c can
grow exponentially with the number of connected users.

the third architecture is the hierarchical p2p or superpeer network, which
was developed as an improvement of the pure p2p network. the gnutella 0.6
standard is an example. in a hierarchical network, there is a two-level hierarchy of
hub nodes and leaf nodes. leaf nodes can be either providers or consumers, and
connect only to hub nodes. a hub provides directory services for the leaf nodes
connected to it, and can forward queries to other hubs.

all of these network architectures could be used as the basis for full distributed
search instead of just exact match for file sharing. as we have mentioned, how-
ever, a hierarchical network has advantages in terms of robustness and scalability
(lu & callan, 2006). for a distributed search application, each provider node in
the network supports search over a local document collection. a consumer node
provides the interface for the user to specify queries. hubs acquire resource de-
scriptions for neighboring hubs and providers, which they use to provide resource
selection and result merging services. specifically, resource descriptions for neigh-
borhoods are used to route queries more e   ciently than flooding, and resource de-
scriptions for providers are used to rank the local document collections. instead
of selecting a fixed number of the top-ranked providers, in the p2p system each
hub must be able to decide how many providers to use to respond to the query.

neighborhood resource descriptions are an important part of the query rout-
ing process. a neighborhood of a hub hi in the direction of a hub hj is the set of
hubs that a query can reach in a fixed number of hops. figure 10.12 shows an ex-
ample of a hub with three neighborhoods generated by a maximum of three hops.
the advantage of this definition of neighborhoods is that the information about

8 this means it is version 0.4 of the gnutella standard. see

http://en.wikipedia.org/wiki/gnutella.

10.5 peer-to-peer and metasearch

445

the providers that can be reached by traveling several hops beyond the immediate
neighbors improves the e   ectiveness of query routing.

fig. 10.12. neighborhoods (ni) of a hub node (h) in a hierarchical p2p network

the resource description for a hub is the aggregation of the resource descrip-
tions of the providers that are connected to it. in other words, it is a language
model recording probabilities of occurrences of words. a neighborhood resource
description is the aggregation of the hub descriptions in the neighborhood, but a
hub   s contribution is reduced based on the number of hops to the hub. in other
words, the closest neighbor hubs contribute the most to the neighborhood de-
scription.

retrieval experiments with distributed search implemented on a hierarchical
p2p network show that e   ectiveness is comparable to searching using a central-
ized hub, which is the architecture we assumed in the last section. more specif-
ically, using neighborhood and provider descriptions to select about 1% of the
2,500 nodes in a p2p testbed produced the same average precision as selecting 1%
of the nodes using a centralized hub, with about one-third of the message tra   c
of a query-flooding protocol (lu & callan, 2006).

another popular architecture for file-sharing systems that we have not men-
tioned is a structured network. these networks associate each data item with a key
and distribute keys to nodes using a distributed hash table (dht). distributed
hash tables can support only exact match searching, but since they are used in a
number of applications, we describe briefly how they can be used to locate a file.

hn1n3n2446

10 social search

in a dht, all keys and nodes are represented as m-bit numbers or identifiers.
the name of a file is converted to a key using a hash function. the key and the
associated file are stored at one or more nodes whose identifiers are    close    in value
to the key. the definition of distance between keys depends on the specific dht
algorithm. in the chord dht, for example, the distance is the numeric di   erence
between the two m-bit numbers (balakrishnan et al., 2003).

to find a file, a query containing the key value of the file name is submitted to
any node. both storing and retrieving files relies on a node being able to forward
requests to a node whose identifier is    closer    to the key. this guarantees that the
request will eventually find the closest node. in chord, keys are treated as points
on a circle, and if k1 and k2 are the identifiers for two    adjacent    nodes, the node
with identifier k2 is responsible for all keys that fall between k1 and k2. each node
maintains a routing table containing the ip addresses of a node halfway around
the key    circle    from the node   s identifier, a node a quarter of the way around, a
node an eighth of the way, etc. a node forwards a request for key k to the node
from this table with the highest identifier not exceeding k. the structure of the
routing table ensures that the node responsible for k can be found in o(logn )
hops for n nodes.

references and further reading

social search is growing in popularity amongst information retrieval researchers.
in particular, there has been an increased interest in social tagging and collabora-
tive online environments over the past few years. it is likely that this interest will
continue to grow, given the amount of research and exploration of new applica-
tions that remains to be done.

the subject of the relative e   ectiveness of manual and automatic indexing
has been discussed for many years in information retrieval, and dates back to
the original cranfield studies (cleverdon, 1970). a number of papers, such as
rajashekar and croft (1995), have shown the advantages of automatic indexing
relative to manual indexing, and also that e   ectiveness improves when the two
representations are combined. several researchers have looked at the usefulness
of tags and other types of metadata for improving search e   ectiveness. research
by heymann, koutrika, and garcia-molina (2008) showed that social media tags,
such as those from deli.cio.us, are not useful for improving search, mainly due to
poor coverage and the fact that most of the tags are already present as anchor text.

10.5 peer-to-peer and metasearch

447

hawking and zobel (2007) report similar results for other types of metadata, and
discuss the implications for the semantic web.

both sahami and heilman (2006) and metzler et al. (2007) proposed similar
techniques for matching short segments of text by expanding the representations
to use web search results. although these methods were evaluated in the context
of query similarity, they can easily be applied to measuring the similarity between
tags.

tag clouds similar to figure 10.2 can be generated by software available on the

web, such as wordle.9

more details of the hits algorithm described in this section for finding com-
munities can be found in gibson et al. (1998). furthermore, hopcroft et al.
(2003) describe various agglomerative id91 approaches. other work related
to finding online communities includes that of flake et al. (2000), which de-
scribes how a variety of community-finding approaches can be implemented e   -
ciently. in addition, borgs et al. (2004) looks at the problem of identifying com-
munity structure within newsgroups, while almeida and almeida (2004) pro-
pose a community-aware search engine. finally, leuski and lavrenko (2006) de-
scribe how id91 and id38 can be used to analyze the behavior
and interactions of users in a virtual world.

jeon et al. (2005) described the approach of question ranking for community-
based id53. xue et al. (2008) extended this work with more e   ec-
tive estimation methods for the translation probabilities and showed that com-
bining the archived questions and answers produced better rankings. the prob-
lem of answer quality and its e   ect on cqa is addressed in jeon et al. (2006).
agichtein et al. (2008) incorporate more features into a prediction of question
and answer quality, and show that features derived from the community graph of
people who ask and answer questions are very important.

the concept of community-based id53 had its origins in digital
reference services (lankes, 2004). there are other search tasks, beyond community-
based id53, in which users search for human-generated answers to
questions. several of these have been explored within the information retrieval
community. finding answers to questions in faqs10 has been the subject of a
number of papers. burke et al. (1997) and berger et al. (2000), for example,
both attempt to overcome the vocabulary mismatch problem in faq retrieval

9 http://www.wordle.net/
10 faq is an abbreviation of frequently asked questions.

448

10 social search

by considering synonyms and translations. jijkoun and de rijke (2005) describe
retrieval from faq data derived from the web, and riezler et al. (2007) describe
a translation-based model for web faq retrieval. forums are another source of
questions and answers. cong et al. (2008) describe how question-answer pairs
can be extracted from forum threads to support cqa services.

there are a number of collaborative search systems beyond those we described,
including cire (romano et al., 1999) and s3 (morris & horvitz, 2007a). morris
(2008) provides a good survey of how practitioners actually use collaborative web
search systems. pickens et al. (2008) evaluate algorithms for iterative merging of
ranked lists to support collaborative search.

belkin and croft (1992) provide a perspective on the connection between
ad hoc retrieval and document filtering. y. zhang and callan (2001) describe
an e   ective method for automatically setting filtering thresholds. the work by
schapire et al. (1998) describes how boosting, an advanced machine learning tech-
nique, and rocchio   s algorithm can be applied to filtering. finally, allan (1996)
showed how incremental feedback, which is akin to online learning, can be used
to improve filtering e   ectiveness. although not covered in this chapter, a research
area within information retrieval called topic detection and tracking has largely
focused on topical filtering (tracking) of news articles. see allan (2002) for an
overview of research on the topic.

for a more complete treatment of id185 algorithms described
in this chapter, see breese et al. (1998). furthermore, herlocker et al. (2004) detail
the many aspects involved with evaluating id185 systems.

callan (2000) gives an excellent overview of research in distributed search. a
more recent paper by si and callan (2004) compares the e   ectiveness of tech-
niques for resource selection. a di   erent approach to query-based sampling for
generating resource descriptions, called query probing, is described by ipeirotis
and gravano (2004). this work is focused on providing access to deep web data-
bases, which are databases that are accessible through the web, but only through
a search interface.

there are many papers describing techniques for combining the output of mul-
tiple search engines or retrieval models. croft (2000) gives an overview of much
of this research, and montague and aslam (2002) provide pointers to more recent
work.

a general overview of p2p search and more details of how distributed search
is implemented in a hierarchical network can be found in lu and callan (2006,
2007).

10.5 peer-to-peer and metasearch

449

exercises

10.1. describe how social media tags are similar to anchor text. how are they
di   erent?
10.2. implement two algorithms for measuring the similarity between two tags.
the first algorithm should use a standard retrieval model, such as language mod-
eling. the second algorithm should use the web or another resource to expand
the tag representation. evaluate the e   ectiveness of the two algorithms on a set of
10   25 tags. describe the algorithms, id74, tag set, and results.
10.3. compute five iterations of hits (see algorithm 3) and id95 (see fig-
ure 4.11) on the graph in figure 10.3. discuss how the id95 scores compare
to the hub and authority scores produced by hits.
10.4. describe two examples of online communities that were not already dis-
cussed in this chapter. how can the community-finding algorithms presented in
this chapter be used to detect each?
10.5. find a community-based id53 site on the web and ask two
questions, one that is low-quality and one that is high-quality. describe the answer
quality of each question.
10.6. find two examples of document filtering systems on the web. how do they
build a profile for your information need? is the system static or adaptive?
10.7. list the basic operations an indexer must support to handle the following
tasks: 1) static filtering, 2) adaptive filtering, and 3) id185.
10.8. implement the nearest neighbor   based id185 algorithm.
using a publicly available id185 data set, compare the e   ective-
ness, in terms of mean squared error, of the euclidean distance and correlation
similarity.
10.9. both the id91 and nearest neighbor   based id185 algo-
rithms described in this chapter make predictions based on user/user similarity.
formulate both algorithms in terms of item/item similarity. how can the distance
between two items be measured?
10.10. form a group of 2   5 people and use a publicly available collaborative
search system. describe your experience, including the pros and cons of using such
a system.

450

10 social search

10.11. suggest how the maximum and minimum resource ranking scores, rmax
and rmin, could be estimated for a given query.

10.12. use the rank-based version of combmnz to combine the results of two
search engines for a sample set of queries. evaluate the combined ranking and
compare its e   ectiveness to the two individual result lists.

10.13. choose your favorite file-sharing application and find out how it works.
describe it and compare it to the p2p networks mentioned in this chapter.
10.14. in a p2p network with small-world properties, any two nodes are likely
to be connected by a small number of hops. these networks are characterized by
a node having local connections to nodes that are    close    and a few long-range
connections to distant nodes, where distance can be measured by content simi-
larity or some other attribute, such as latency. do you think gnutella 0.4 or 0.6
would have content-based small-world properties? what about a structured net-
work based on chord?

11

beyond bag of words

   it means the future is here, and all bets are o   .   
agent mulder, the x-files

11.1 overview

the term    bag of words    is used to refer to a simple representation of text that is
used in retrieval and classification models. in this representation, a document is
considered to be an unordered collection of words with no relationships, either
syntactic or statistical, between them.1 many of the retrieval models discussed in
chapter 7, such as the query likelihood model, the bm25 model, and even the
vector space model, are based on a bag of words representation. from a linguistic
point of view, the bag of words representation is extremely limited. no one could
read a sorted bag of words representation and get the same meaning as normal
text. the sorted version of the last sentence, for example, is    a and as bag could get
meaning no normal of one read representation same sorted text the words   .

despite its obvious limitations, the bag of words representation has been very
successful in retrieval experiments compared to more complex representations of
text content. incorporating even simple phrases and word proximity into a word-
based representation, which would seem to have obvious benefits, took many
years of research before retrieval models were developed that had significant and
consistent e   ectiveness benefits. search applications, however, have evolved be-
yond the stage where a bag of words representation of documents and queries
would be adequate. for these applications, representations and ranking based on
many di   erent features are required. features derived from the bag of words are
still important, but linguistic, structural, metadata, and non-textual content fea-
tures can also be used e   ectively in retrieval models such as the id136 network

1 in mathematics, a bag is like a set, but duplicates (i.e., multiple occurrences of a word)

are allowed.

452

11 beyond bag of words

or the ranking id166. we start this chapter by examining the general properties of
a feature-based retrieval model.

in previous chapters we have discussed a number of representation features
and how they can be used in ranking. in this chapter, we look at four aspects of
representation in more detail and describe how they could a   ect the future de-
velopment of search engines. id159s assume there is no relationship
between words, so we first look at how term dependencies can be captured and
used in a linear feature-based model. document structure is ignored in a bag of
words representation, but we have seen how it can be important in web search.
the second aspect of representation we look at is how the structured representa-
tions used in a database system could be used in a search engine. in a bag of words
representation, queries are treated the same as documents. in question-answering
applications, however, the syntactic structure of the query can be particularly im-
portant. the third aspect of representation we look at is how query structure is
used to answer questions. finally, bags of words are based on words, and there are
many applications, such as image search or music search, where the features used
to represent the objects that are retrieved may not be words. the fourth aspect of
representation we look at is what these non-text features could be, and how they
are used in ranking.

in the final section of this chapter, we indulge in some mild (not wild) specu-

lation about the future of search.

11.2 feature-based retrieval models
we described feature-based retrieval models briefly in chapter 7, and provide more
detail here because of their growing importance as the basis for modern search
engines.
for a set of documents d and a set of queries q, we can define a scoring or
ranking function s(cid:3)(d; q) parameterized by (cid:3), which is a vector of parameters.
given a query qi, the scoring function s(cid:3)(d; qi) is computed for each d     d,
and documents are then ranked in descending order according to their scores.
for linear feature-based models, we restrict the scoring function to those with
the form:

   

s(cid:3)(d; q) =

j

  j    fj(d, q) + z

where fj(d, q) is a feature function that maps query/document pairs to real
values, and z is a constant that does not depend on d (but may depend on (cid:3)

11.2 feature-based retrieval models

453

or q). the feature functions correspond to the features that we have previously
mentioned. although some models permit non-linear combinations of features,
the scoring functions that have been used in research and applications to date are
based on linear combinations. for this reason, we focus here on linear feature-
based models. note that this ranking function is a generalization of the abstract
ranking model that we described in chapter 5.

in addition to defining the form of the scoring function, we also need to spec-
ify the method for finding the best values for the parameters. to do this, we need
a set of training data t and an evaluation function e(r(cid:3);t ), where r(cid:3) is the
set of rankings produced by the scoring function for all the queries. the evalu-
ation function produces real-valued output given the set of ranked lists and the
training data. note that e is only required to consider the document rankings
and not the document scores. this is a standard characteristic of the evaluation
measures described in chapter 8, such as mean average precision, precision at 10,
or ndcg.

the goal of a linear feature-based retrieval model is to find a parameter setting
(cid:3) that maximizes the evaluation metric e for the training data. formally, this can
be stated as:

b(cid:3) = arg max

(cid:3)

e(r(cid:3);t )

where r(cid:3) are the rankings produced by the linear scoring function
fj(d, q) + z.

j   j   
for a small number of features, the optimal parameter values can be found by
a brute-force search over the entire space of possible values. for larger numbers of
features, an optimization procedure, such as that provided by the ranking id166
model, is needed. the key advantages of the linear feature-based models compared
to other retrieval models are the ease with which new features can be added to the
model, and e   cient procedures for optimizing e   ectiveness given training data.
it is these advantages that make linear feature-based models the ideal framework
for incorporating the range of representation features we discuss in this chapter.
a relatively small number of features have been used as the basis of the retrieval

models described in chapter 7 that focus on topical relevance. these include:
    term occurrence: whether or not a term occurs within a document
    term frequency: number of times a term occurs within a document
    inverse document frequency: inverse of the proportion of documents that con-

   

tain a given term

    document length: number of terms in a document

454

11 beyond bag of words

    term proximity: occurrence patterns of terms within a document (the most

common way of incorporating term dependency)
the galago query language (and the id136 network model it is based on)
provides a means of specifying a range of features and scoring documents based
on a weighted linear combination of these features. galago is more general in that
it also supports the definition and combination of arbitrary features using the
#feature operator. for example, using this operator, it is possible to have a fea-
ture based on the bm25 term-weighting function as part of the scoring function.
galago, like the id136 network model, does not specify a particular optimiza-
tion method for finding the best parameter values (i.e., the feature weights).

11.3 term dependence models

in chapter 4, we discussed the potential importance of relationships between
words that are part of phrases. in chapter 5, we showed how term proximity in-
formation can be incorporated into indexes. chapter 6 described techniques for
measuring the association between words, and chapter 7 showed how term rela-
tionships can be expressed in the galago query language. exploiting the relation-
ships between words is clearly an important part of building an e   ective search
engine, especially for applications such as web search that have large numbers of
documents containing all or most of the query words. retrieval models that make
use of term relationships are often called term dependence models because they do
not assume that words occur independently of each other. more generally, term
dependence information can be incorporated into a number of features that are
used as part of the ranking algorithm.

the galago implementation of web search described in section 7.5 is based on
a specific linear feature-based model known as the markov random field (mrf)
model (metzler & croft, 2005b). this model, in addition to allowing arbitrary
features, explicitly represents dependencies between terms. although a number
of term dependence models have been proposed, we describe the mrf model
because it has produced significant e   ectiveness benefits in both document rank-
ing and in the related process of pseudo-relevance feedback (metzler & croft,
2007a).

the mrf model works by first constructing a graph that consists of a docu-
ment node and one node per query term. these nodes represent random variables
within a markov random field, which is a general way of modeling a joint distri-

11.3 term dependence models

455

bution. thus, in the mrf model, the joint distribution over the document ran-
dom variable and query term random variables is being modeled. markov random
fields are typically represented as graphs, by what is known as a graphical model.
in particular, mrfs are undirected id114, which means the edges in
the graph are undirected. the id136 network model described in chapter 7
was an example of a directed graphical model.

fig. 11.1. example markov random field model assumptions, including full indepen-
dence (top left), sequential dependence (top right), full dependence (bottom left), and
general dependence (bottom right)

the mrf models dependencies between random variables by drawing an edge
between them. since the importance of query terms depends on the document,
the document node is always connected to every query term node. it is straight-
forward to model query term dependencies by drawing edges between the query
term nodes. there are several possible ways to determine which query term nodes
to draw an edge between. these di   erent cases are summarized in figure 11.1. in
the simplest case, no edges are drawn between the query terms. this corresponds
to the full independence assumption, where no dependencies exist between the
query terms. this is analogous to a unigram language model or any of the bag of
words models described in chapter 7. another possibility is to draw edges be-
tween adjacent query terms. this is known as the sequential dependence assump-

q1q2q3dq1q2q3dq1q2q3dq1q2q3d456

11 beyond bag of words

tion. here it is assumed that adjacent terms are dependent on each other, but not
on terms that are farther away. this type of assumption is similar to a bigram lan-
guage model. another possible assumption is that all terms are somehow depen-
dent on all other terms. this is known as the full dependence assumption. the final
possibility is that edges are drawn between the query terms in some meaningful
way, such as automatically or manually identifying terms that are dependent on
each other. this is referred to as general dependence. in practice, however, it has
been shown that using the sequential dependence assumption is the best option.
in fact, all attempts to manually or automatically determine which terms to model
dependencies between have come up short against using the simple assumption
that adjacent terms are dependent on each other.

after the mrf graph has been constructed, a set of potential functions must be
defined over the cliques of the graph. the potential functions are meant to mea-
sure the compatibility between the observed values for the random variables in
the clique. for example, in the sequential dependence graph shown in figure 11.1,
a potential function over the clique consisting of the terms q1, q2, and d might
compute how many times the exact phrase    q1 q2    occurs in document d, or how
many times the two terms occur within some window of each other. therefore,
these potential functions are quite general and can compute a variety of di   er-
ent features of the text. in this way, the mrf model is more powerful than other
models, such as id38 or bm25, because it allows dependencies and
arbitrary features to be included in a straightforward manner.

by constructing queries that have a particular form, the galago search engine
can be used to emulate one of the instantiations of the mrf model that has been
very e   ective in trec experiments. this approach was also used in section 7.5.
for example, given the query president abraham lincoln, the full independence
mrf model is computed using the following query:

#combine(president abraham lincoln)
notice that this is the most basic formulation possible and does not consider
any dependencies between terms. the sequential dependence mrf model can be
computed by issuing the following query:
#weight(0.8 #combine(president abraham lincoln)
0.1 #combine(#od:1(president abraham)

#od:1(abraham lincoln)

0.1 #combine(#uw:8(president abraham)

#uw:8(abraham lincoln)

11.3 term dependence models

457

this query formulation consists of three parts, each of which corresponds to
a specific feature type used within the mrf model. the first part scores the con-
tribution of matching individual terms. the second part scores the contribution
of matching subphrases within the query. this gives higher weight to documents
that match    president abraham    and    abraham lincoln    as exact phrases. notice
that these phrases only consist of adjacent pairs of query terms, because the se-
quential dependence model only models dependencies between adjacent query
terms. the first part of the formulation scores the contribution of matching un-
ordered windows of adjacent query terms. in particular, if the terms    president   
and    abraham    occur within a window of eight terms of each other in any or-
der, then the document   s score will be boosted. notice that each component is
weighted, with the individual term component having weight 0.8 and the exact
phrase and unordered window components being weighted 0.1. these weights,
which were derived empirically, show the relative importance of matching indi-
vidual terms versus matching phrase and proximity features within text. the in-
dividual terms are by far the most important things to match, although the other
features play an important role.

the algorithm for converting a plain text query q to a sequential dependence
mrf query in galago is very simple. the first component (for individual terms)
is simply #combine(q). the second component puts every adjacent pair of query
terms in a #od:1 operator, and then combines all such operators with a #com-
bine. the third and final component puts every adjacent pair of query terms into
a #uw:8 operator, and again combines them with a #combine operator. the three
components are then given weights 0.8, 0.1, and 0.1, as described earlier, and com-
bined within a #weight operator.

finally, the full dependence mrf model is much more complex, since many
more dependencies are modeled. however, we show the full dependence mrf
model query for the sake of completeness here:
#weight(0.8 #combine(president abraham lincoln)
0.1 #combine(#od:1(president abraham)

#od:1(abraham lincoln)
#od:1(president abraham lincoln))

0.1 #combine(#uw:8(president abraham)

#uw:8(abraham lincoln)
#uw:8(president lincoln)
#uw:12(president abraham lincoln)))

458

11 beyond bag of words

it is important to note that the #combine operator used in all of the exam-
ples here could easily be replaced with the #feature operator in order to compute
the individual term, exact phrase, and unordered phrase features di   erently. for
example, one could implement bm25 weighting using #feature and compute a
dependence model based on it instead of galago   s default weighting.

the mrf model can also be used to model dependencies in pseudo-relevance
feedback, which is an important technique for id183 that we described
in section 7.3.2. figure 11.2 compares a graphical model representation of the rel-
evance model technique used for pseudo-relevance feedback with the mrf ap-
proach, which is known as latent concept expansion.2 the relevance model graph
(the top one) represents a bag of words, or unigram, model where words occur in-
dependently of each other, given a particular document. pseudo-relevance feed-
back uses a set of highly ranked documents to estimate the id203 of expan-
sion words (the question marks) given the query words. the words with the high-
est id203 are added to the query.

in the lower latent concept expansion graph, there are dependencies rep-
resented between query words and expansion words. the process of pseudo-
relevance feedback is still the same, in that highly ranked documents are used to
estimate the probabilities for possible expansion words, but the dependencies will
change the way that those probabilities are estimated and generally produce better
results. by modeling dependencies between the expansion words, the latent con-
cept expansion model can produce multiword phrases as expansion terms rather
than only words. as an example, table 11.1 shows the top-ranked one- and two-
word concepts produced by the latent concept expansion model for the query
   hubble telescope achievements   .

to summarize, the mrf model, which is a linear feature-based retrieval model,
is an e   ective method of incorporating features based on term dependence in the
scoring function used to rank documents. latent concept expansion supports
pseudo-relevance feedback in the mrf framework. latent concept expansion
can be viewed as a    feature expansion    technique, in that it enriches the original
feature set by including new features based on the expanded query.

2 latent, or hidden, concepts are words or phrases that users have in mind but do not

mention explicitly when they express a query.

11.4 structure revisited

459

fig. 11.2. graphical model representations of the relevance model technique (top) and
latent concept expansion (bottom) used for pseudo-relevance feedback with the query
   hubble telescope achievements   

11.4 structure revisited

the goal of having a common platform for dealing with both structured and un-
structured data is a long-standing one, going back to the 1960s. a number of ap-
proaches have been suggested, both from the database and information retrieval
perspective, but the motivation for finding a solution or solutions that work has
grown tremendously since the advent of very large-scale web databases. areas that
were once the exclusive concerns of information retrieval, such as statistical infer-
ence and ranking, have now become important topics for database researchers,
and both communities have a common interest in providing e   cient indexing
and optimization techniques for web-scale data. exploiting document structure
is a critical part of web search, and combining di   erent sources of evidence ef-
fectively is an important part of many database applications. there are many pos-
sibilities for integration, such as extending a database model to more e   ectively
deal with probabilities, extending an information retrieval model to handle more
complex structures and multiple relations, or developing a unified model and sys-

hubbletelescopeachievements??dhubbletelescopeachievements??d460

11 beyond bag of words

1-word concepts

telescope
hubble
space
mirror
nasa
launch

astronomy

shuttle
test
new

discovery

time

universe
optical
light

2-word concepts
hubble telescope
space telescope
hubble space

telescope mirror
telescope hubble
mirror telescope
telescope nasa
telescope space
hubble mirror
nasa hubble

telescope astronomy

telescope optical
hubble optical

telescope discovery
telescope shuttle

table 11.1. most likely one- and two-word concepts produced using latent concept ex-
pansion with the top 25 documents retrieved for the query    hubble telescope achieve-
ments    on the trec robust collection

tem. applications such as web search, e-commerce, and data mining provide the
testbeds where these systems are being evaluated and compared.

in chapter 7, we showed how document structure can be handled in the
galago query language. from a conventional database perspective, there are major
problems with using galago to represent and query data. using relational database
terminology, there is no schema,3 no means of defining the data type of attributes,
and no joins between relations.4 instead, as described in chapter 7, a document is
represented as a (possibly nested) set of contexts defined by tag pairs. documents
are stored in a simple database with only primary-key access, where the primary
key is the document identifier. the query language supports the definition and
combination of search features based on the structure and contents of the doc-
uments. di   erent document types with di   erent contexts can be incorporated

3 a schema is a description of the logical structure of the database, which in this case

would be the names of the relations (tables) and the attributes in each relation.

4 a join connects tuples (rows) from two di   erent relations based on one or more com-
mon attributes. an example would be to connect product information with vendor
information based on the product number attribute.

11.4 structure revisited

461

into a single galago database. each document is indexed only by the contexts it
contains. although there is no way of defining the data type of contexts, opera-
tors associated with a specific data type could be defined and applied to particular
contexts. for example, a date range operator could be applied to a context that
contains the creation date for a document.

although this is very di   erent from the functionality of a full relational data-
base system, in many applications involving search engines this additional func-
tionality is not needed. the bigtable storage system described in chapter 3, for
example, does not have data types or joins. additionally, it has only a very simple
specification of tuple and attribute names. systems such as bigtable focus on pro-
viding data persistence and reliable access to data in an environment where many
components can fail, and scalable performance using distributed computing re-
sources. access to the data is provided through a simple api that allows client
applications to read, write, or delete values. figure 11.3 summarizes the functions
provided by the search engine and the database system in applications such as web
search or e-commerce. note that the indexes created by the search engine are not
stored in the database system.

fig. 11.3. functions provided by a search engine interacting with a simple database system

11.4.1 xml retrieval

xml is an important standard for both exchanging data between applications
and encoding documents. to support this more data-oriented view, the database
community has defined languages for describing the structure of xml data (xml
schema), and querying and manipulating that data (xquery and xpath). xquery
is a query language that is similar to the database language sql, with the major
di   erence that it must handle the hierarchical structure of xml data instead of

search engine  indexing  query language  ranking  scalabilitydatabase  persistence  reliability  scalabilitysimple api462

11 beyond bag of words

the simpler tabular structure of relational data. xpath is a subset of xquery that is
used to specify the search constraints for a single type of xml data or document.
xpath, for example, could be used in an xml movie database to find the movies
that were directed by a particular person and were released in a particular year.
xquery could be used to combine information about movies with information
about actors, assuming that the xml movie database contained both movie and
actor    documents.    an example would be to find movies starring actors who were
born in australia.

complex database query languages, such as xquery, that focus on the struc-
ture of data and combining data are generally less useful in text search applica-
tions. the extent to which structure is useful in queries for databases of xml
documents has been studied in the inex project.5 this project has taken a simi-
lar approach to trec for search evaluation. this means that a number of xml
search tasks have been defined, and appropriate test collections constructed for
evaluating those tasks. one type of query used in these evaluations is the content-
and-structure (cas) query. these queries contain a description of a topic and ex-
plicit references to the xml structure. cas queries are specified using a simpli-
fied version of xpath called nexi.6 the two important constructs in this query
language are paths and path filters. a path is a specification of an element (or node)
in the xml tree structure. some examples of path specifications in nexi are:
//a//b - any b element7 that is a descendant of an a element in the xml
tree. a descendant element will be contained in the parent element.
//a/* - any descendant element of an a element.

a path filter restricts the results to those that satisfy textual or numerical con-
straints. some examples are:

//a[about(.//b,   topic   )] - a elements that contain a b element that is about
   topic   . the about predicate is not defined, but is implemented using some
type of retrieval model. .//b is a relative path.
//a[.//b = 777] - a elements that contain a b element with value equal to
777.

5 initiative for the evaluation of xml retrieval, http://inex.is.informatik.uni-

duisburg.de/.

6 narrowed extended xpath (trotman & sigurbj  rnsson, 2004).
7 this means an element that has a start tag <b> and an end tag </b>.

11.4 structure revisited

463

the database used in the earlier inex experiments consisted of technical ar-
ticles from computer science publications. the following are examples of some of
the cas queries:

//article[.//fm/yr < 2000]//sec[about(.,   search engines   )]
- find articles published before 2000 (fm is the front matter of the article)
that contain sections discussing the topic    search engines   .

//article[about(.//st,+comparison) and about (.//bib,   machine learning   )]
- find articles with a section title containing the word    comparison    and
with a bibliography that mentions    machine learning   .

//*[about(.//fgc, corba architecture) and about(.//p, figure
corba architecture)]
- find any elements that contain a figure caption about    corba architecure   
and a paragraph mentioning    figure corba architecture   .
although these queries seem reasonable, inex experiments and previous re-
search indicate that people do not use structural cues in their queries or, if they are
forced to, generally will use them incorrectly. there is essentially no evidence that
structure in user queries improves search e   ectiveness. for this reason, the inex
project has increasingly focused on content-only queries, which are the same as
the queries we have been discussing throughout this book, and on techniques for
automatically ranking xml elements rather than documents.

to summarize, structure is an important part of defining features for e   ective
ranking, but not for user queries. in applications such as web search, relatively sim-
ple user queries are transformed into queries that involve many features, including
features based on document structure. the galago query language is an example
of a language that can be used to specify the features that are used for ranking, and
user queries can be transformed into galago queries. most of the structural cues
that can be specified in nexi can be incorporated into features using galago.

database systems are used in many search applications. the requirements for
these applications, however, are di   erent than those for typical database applica-
tions, such as banking. this has led to the development of storage systems that
are simple from a database schema and query language perspective, but are also
e   cient, reliable, and scalable.

464

11 beyond bag of words

11.4.2 entity search

in addition to exploiting the existing structure in documents, it is also possible to
create structure by analyzing the document contents. in chapter 4, we described
information extraction techniques that can be used to identify entities in text such
as people, organizations, and locations. entity search uses this structure to provide
a ranked list of entities in response to a query instead of a list of documents. to do
this, a representation for each entity is generated based on the words that occur
near the entity in the document texts. the simplest approach to building these
representations is to create    pseudo-documents    by accumulating all words that
occur within a specified text window (e.g., 20 words) for each occurrence of an
entity. for example, if the organization entity    california institute of technology   
occurred 65 times in a corpus, every word within 20 words of those 65 occur-
rences would be accumulated into the pseudo-id194. this is
the approach used in the early research on entity search, such as conrad and utt
(1994). these large word-based representations can be stored in a search engine
and then used to rank entities in response to a query.

figure 11.4 shows an example of entity retrieval given in the conrad paper.
the top-ranked organizations for this query will have co-occurred with words
such as    biomedical    a number of times in the corpus.

fig. 11.4. example of an entity search for organizations using the trec wall street jour-
nal 1987 collection

the approach of building representations from words that occur in the context
or locality of a target word or phrase (sometimes called context vectors) has been
used to automatically build a thesaurus that retrieves words and phrases for query

query:  biomedical research and technologytop ranked results:minneapolisresearch   signs inc.syntexcaliforniainstitute of technologymassachusettsinstitute of technologytherapeutic products11.4 structure revisited

465

expansion (jing & croft, 1994). interestingly, it has also been used by cognitive
scientists as the basis for a model of semantic memory (lund & burgess, 1996).
much of the recent research on entity search has focused on methods for find-
ing people who have expertise in a particular area or topic. this task, known as
expert search, has been studied for some time but has been evaluated more thor-
oughly since it became part of a trec track in 2005. the main contribution of
this research has been the development of probabilistic retrieval models for enti-
ties (or experts) based on the id38 approach (balog et al., 2006). in
general, given a set of documents d and a query q, we can rank candidate entities
e by the joint distribution p (e, q) of entities and query terms. we can represent
this distribution as:

p (e, q) =

p (e, q|d)p (d)

   

d   d

if we focus on the p (e, q|d) term, the problem of entity ranking can be decom-
posed into two components:

p (e, q|d) = p (q|e, d)p (e|d)

where the p (e|d) component corresponds to finding documents that provide in-
formation about an entity, and the p (q|e, d) component involves ranking en-
tities in those documents with respect to a query. di   erent ways of estimating
these probabilities result in di   erent entity ranking algorithms. if we assume
that words and entities are independent given a document, then p (e, q|d) =
p (q|d)p (e|d), and the two components could be computed separately by using
q and e as queries for probabilistic retrieval. this assumption, however, ignores the
relationship between words and entities that appear in the same document (which
is captured in the context vector approach) and consequently the e   ectiveness of
the method su   ers. instead, we can estimate the strength of association between
e and q using the proximity of co-occurrence of the query words and the entities
in a document. one way to do this, assuming a query consisting of a single term
and a single entity occurrence in a document, is to estimate p (q|e, d) as:

n   

i=1

p (q|e, d) =

1
z

  d(i, q)k(q, e)

   

where   d is an indicator function that is 1 when the term at position i in d is q and
i=1 k(q, e) is a normalizing
0 otherwise, k is a proximity-id81, z =
constant, and n is the length of the document.

n

466

11 beyond bag of words

if the query has multiple terms, we can compute the ranking score as follows:

{   

   

qi   q

d   d

}
p (qi|e, d)p (e|d)

p (e, q) rank=

petkova and croft (2007) showed that the most e   ective id81 is

the gaussian kernel, which is shown in table 9.1 and, in this case, is

exp   ||q     e||2/2  2

where q     e is the distance in words between the query q and the entity e. this
paper also showed that, for expert search, accurate id39 does
not have a large e   ect on performance and that using a simple galago query such
as #od:2( <first name> <last name>) works well for estimating p (e|d) for people
entities.

11.5 longer questions, better answers

in nearly all visions of the future that we see in movies or on television, the search
engine, disguised as a computer system such as hal 9000 in 2001: a space
odyssey or the computer in the star trek series, has evolved into a human-like as-
sistant that can answer complex questions about any subject. although web search
engines provide access to a huge range of information, we are still a long way from
achieving the capabilities of these intelligent assistants. one obvious di   erence is
that queries to web search engines are generally formulated as a small number of
keywords, rather than as actual questions expressed in natural language. in chap-
ter 10, we described how people who use community-based id53
systems describe their information needs in sentences, or even paragraphs, be-
cause they know that other people will read them and give better responses if the
problem is described well. in contrast, the same long queries will generally pro-
duce very poor responses or nothing at all from a web search engine. people are
forced to translate their problem into one or more appropriate keywords to get a
reasonable result list. a long-term goal of information retrieval research is to de-
velop retrieval models that produce accurate results from a longer, more specific
query.

the task of id53, which we mentioned briefly in chapter 1 and
again in chapter 10, involves providing a specific answer to a user   s query, rather

11.5 longer questions, better answers

467

than a ranked list of documents. this task has a long history in the fields of natural
language processing and artificial intelligence. early id53 systems
relied on detailed representations in logic of small, very specific domains such as
baseball, lunar rocks, or toy blocks. more recently, the focus has shifted to an in-
formation retrieval perspective where the task involves identifying or extracting
answers found in large corpora of text.

fig. 11.5. id53 system architecture

figure 11.5 shows the typical components of a id53 system that
retrieves answers from a text corpus. the range of questions that is handled by
such a system is usually limited to fact-based questions with simple, short answers,
such as who, where, and when questions that have people   s names, organization
names, places, and dates as answers. the following questions are a sample from
the trec id53 (qa) track:8

who invented the paper clip?
where is the valley of the kings?
when was the last major eruption of mt. st. helens?
there are, of course, other types of fact-based questions that could be asked,
and they can be asked in many di   erent ways. the task of the question analysis
and classification component of the system is to classify a question by the type of
answer that is expected. for the trec qa questions, one classification that is
frequently used has 31 di   erent major categories,9 many of which correspond to

8 the trec qa questions were drawn from the query logs of a variety of search appli-

9 this is the question classification created by bbn. this classification and others are

cations (voorhees & harman, 2005).

discussed in metzler and croft (2005a).

passage retrievalanswerselectionanswer featuresqueryquestionanalysis/classificationcorpus/webanswersquestion468

11 beyond bag of words

named entities (see chapter 4) that can be automatically identified in text. table
11.2 gives an example of a trec question for each of these categories. question
classification is a moderately di   cult task, given the large variation in question
formats. the question word what, for example, can be used for many di   erent
types of questions.

the information derived from question analysis and classification is used by
the answer selection component to identify answers in candidate text passages,
which are usually sentences. the candidate text passages are provided by the pas-
sage retrieval component based on a query generated from the question. text
passages are retrieved from a specific corpus or the web. in trec qa experi-
ments, candidate answer passages were retrieved from trec news corpora, and
the web was often used as an additional resource. the passage retrieval compo-
nent of many id53 systems simply finds passages containing all the
non-stopwords in the question. in general, however, passage retrieval is similar to
other types of search, in that features associated with good passages can be com-
bined to produce e   ective rankings. many of these features will be based on the
question analysis. text passages containing named entities of the type associated
with the question category as well as all the important question words should ob-
viously be ranked higher.

for example, with the question    where is the valley of the kings   , sentences
containing text tagged as a location and the words    valley    and    kings    would be
preferred. some systems identify text patterns associated with likely answers for
the question category, using either id111 techniques with the web or pre-
defined rules. patterns such as <question-location> in <location>, where question-
location is    valley of the kings    in this case, often may be found in answer pas-
sages. the presence of such a pattern should improve the ranking of a text passage.
another feature that has been shown to be useful for ranking passages is related
words from a thesaurus such as id138. for example, using id138 relations,
words such as    fabricates   ,    constructs   , and    makes    can be related to    manu-
factures    when considering passages for the question    who manufactures magic
chef appliances   . a linear feature-based retrieval model provides the appropriate
framework for combining features associated with answer passages and learning
e   ective weights.

the final selection of an answer from a text passage can potentially involve
more linguistic analysis and id136 than is used to rank the text passages. in
most cases, however, users of a id53 system will want to see the
context of an answer, or even multiple answers, in order to verify that it appears

11.5 longer questions, better answers

469

example question
what do you call a group of geese?
who was monet?
how many types of lemurs are there?
what is the e   ect of acid rain?
what is the street address of the white house?
boxing day is celebrated on what day?
what is sake?
what is another name for nearsightedness?
what was the famous battle in 1836 between
texas and mexico?
what is the tallest building in japan?
what type of bridge is the golden gate bridge?
what is the most popular sport in japan?
what is the capital of sri lanka?
name a gaelic language.
what is the world   s highest peak?
how much money does the sultan of brunei have?
jackson pollock is of what nationality?
who manufactures magic chef appliances?
what kind of sports team is the bu   alo sabres?
what color is yak milk?
how much of an apple is water?
who was the first russian astronaut to walk in space?
what is australia   s national flower?
what is the most heavily ca   einated soft drink?
what does the peugeot company manufacture?
how far away is the moon?
why can   t ostriches fly?
what metal has the highest melting point?
what time of day did emperor hirohito die?
what does your spleen do?
what is the best-selling book of all time?

question category

animal
biography
cardinal

cause/e   ect
contact info

date

definition
disease

facility description

geo-political entity

event
facility

game

language
location
money

nationality
organization

org. description

product description

other
percent
person
plant
product

quantity
reason
substance

time
use

work of art

table 11.2. example trec qa questions and their corresponding question categories

470

11 beyond bag of words

to be correct or possibly to make a decision about which is the best answer. for
example, a system might return    egypt    as the answer to the valley of the kings
question, but it would generally be more useful to return the passage    the val-
ley of the kings is located on the west bank of the nile near luxor in egypt.   
from this perspective, we could view search engines as providing a spectrum of
responses for di   erent types of queries, from focused text passages to entire doc-
uments. longer, more precise questions should produce more accurate, focused
responses, and in the case of fact-oriented questions such as those shown in table
11.2, this will generally be true.

the techniques used in id53 systems show how syntactic and
semantic features can be used to obtain more accurate results for some queries,
but they do not solve the more di   cult challenges of information retrieval. a
trec query such as    where have dams been removed and what has been the
environmental impact?    looks similar to a fact-based question, but the answers
need to be more comprehensive than a list of locations or a ranked list of sen-
tences. on the other hand, using id53 techniques to identify the
di   erent text expressions for dam removal should be helpful in ranking answer
passages or documents. similarly, a trec query such as    what is being done
to increase mass transit use?   , while clearly not a fact-based question, should also
benefit from techniques that could recognize discussions about the use of mass
transit. these potential benefits, however, have yet to be demonstrated in retrieval
experiments, which indicates that there are significant technical issues involved in
applying these techniques to large numbers of queries. search engines currently
rely on users learning, based on their experience, to submit queries such as    mass
transit    instead of the more precise question.

11.6 words, pictures, and music

although information retrieval has traditionally focused on text, much of the in-
formation that people are looking for, particularly on the web, is in the form of
images, videos, or audio. web search engines, as well as a number of other sites,
provide searches specifically for images and video, and online music stores are a
very popular way of finding music. all of these services are text-based, relying on
titles, captions, user-supplied    tags,    and other related text to create representa-
tions of non-text media for searching. this approach can be e   ective and is rel-
atively straightforward to implement. in some cases, however, there may not be

11.6 words, pictures, and music

471

any associated text, or the text may not capture important aspects of the object be-
ing represented. many of the videos stored at video-sharing sites do not have good
textual descriptions, for example, and because there are so many of them, user tags
do not solve this problem. another example is that titles do not provide an appro-
priate description for searching music files to find a particular melody. for these
situations, researchers have been developing content-based retrieval techniques for
non-text media.

some non-text media use words to convey information and can be converted
into text. id42 (ocr) technology, for example, is used
to convert scanned documents containing written or printed text into machine-
readable text. id103 technology10 is used to convert recorded speech
(or spoken documents) into text. both ocr and id103 produce
   noisy    text, meaning that the text has errors relative to the original printed text
or speech transcript. figure 11.6 shows two examples of the errors produced by
ocr. in both cases, the ocr output was produced by o   -the-shelf ocr soft-
ware. the first example is based on a text passage created using a word proces-
sor, printed, copied multiple times (to reduce the quality), and finally scanned
for ocr. the resulting output has some small errors, but in general the ocr er-
ror rate for high-quality printed text is low. the second example uses much lower
quality11 input that was created by scanning a photocopy of an old conference pa-
per. in this case, the ocr output contains significant errors, with words such as
   sponsorship    and    e   ectiveness    being virtually unreadable. note that ocr er-
rors occur at the character level. in other words, the errors are a result of confusion
about individual characters and cause the output of incorrect characters.

figure 11.7 shows the output of high-quality id103 software for
a news broadcast. most words are recognized correctly, but when the system en-
counters words that it has not seen before (known as out-of-vocabulary, or oov,
words), it makes some significant errors. many of the oov words come from
personal or organization names, such as    pinochet    in this example, and consid-
erable research e   ort has gone into addressing this problem. note that speech
recognition errors tend to create new words in the output, such as    coastal fish   
in the example, since the software attempts to find known words that best match
the sound patterns as well as a language model. both this type of error and the

10 sometimes referred to as asr (automatic id103).
11 by    quality    we mean image quality measured in terms of contrast, sharpness, clean

background, etc.

472

11 beyond bag of words

fig. 11.6. examples of ocr errors

character-level errors from ocr have the potential to reduce the e   ectiveness of
search.

a number of evaluations with ocr and asr data have been done at trec
and other forums. these studies indicate that retrieval e   ectiveness generally is
not significantly impacted by ocr or asr errors. the main reason for this is
the large amount of redundancy in the collections that are used for these eval-
uations. for example, in asr evaluations, there are often many relevant spoken
documents that each contain many instances of the important words for a query.
even if some instances of words are not recognized, other instances may be. in
addition, even if a query word is consistently not recognized, there are usually a
number of other words in the query that can be used to estimate relevance. a sim-
ilar situation occurs with ocr data, where some instances of a word in a scanned
document may be of higher image quality than others, and thus will be recog-
nized successfully. the only situation where ocr and asr errors were shown
to significantly reduce e   ectiveness was with short documents, which have little
redundancy, and in environments with high error rates. techniques such as char-
acter id165 indexing and expansion with related terms can improve performance
in these situations.

original:    ocr:    the fishing supplier had many items in stock, including a large variety of     tropical fish and aquariums ot ah sizes~  original:    ocr:    this work was carried out under the sp011j!0rship 01 natiolul1 setenee    foundation 0rant. nsf  0n  sb0 (studl .. in indexing depth and retrieval    eflccth   ene&&) and nsf  0n  482 (requirements study lor future    catalogs)    11.6 words, pictures, and music

473

fig. 11.7. examples of speech recognizer errors

in contrast to media that can be converted to noisy text, content-based re-
trieval of pictures12 is a more challenging problem. the features that can be ex-
tracted from an image, such as color, texture, and shape, have little semantic con-
tent relative to words. for example, one of the common features used in image
retrieval applications is the color histogram. color in images is represented using
a specific color model, such as rgb.13 the rgb model represents colors as mix-
tures of red, blue, and green, typically with 256 values (8 bits) being used for each
component. a color histogram for an image can be created by first    quantizing   
the color values to reduce the number of possible    bins    in the histogram. if the
rgb values are quantized into 8 levels instead of 256 levels, for example, the num-
ber of possible color combinations is reduced from 256  256  256 to 8  8  8 =
512 values or bins. then, for each pixel in the image, the bin corresponding to
the color value for that pixel is incremented by one. the resulting histogram can
be used to represent the images in a collection and also be indexed for fast re-
trieval. given a new image as a query, the color histogram for that image would
be compared with the histograms from the image collection using some similarity
measure, and images would be ranked using the similarity values.

12 also known as content-based id162 (cbir).
13 another common model is hsv (hue, saturation, and value).

transcript: french prosecutors are investigating former chilean strongman augusto  pinochet.  the french justice minister may seek his extradition from britain.  three french families whose relatives disappeared in chile have filed a complaint charging pinochet with crimes against humanity. the national court in spain has ruled crimes committed by the  pinochet regime fall under spanish jurisdiction.    speech recognizer output: french prosecutors are investigating former chilean strongman of coastal fish today the french justice minister may seek his extradition from britain three french families whose relatives disappeared until i have filed a complaint charging tenants say with crimes against humanity the national court in spain has ruled crimes committed by the tennessee with james all under spanish jurisdiction  474

11 beyond bag of words

fig. 11.8. two images (a fish and a flower bed) with color histograms. the horizontal axis
is hue value.

figure 11.8 shows two example images and their color histograms. in this case,
the histogram is based only on hue values (rather than rgb values), so the peaks
in the histogram correspond to peaks in colors. both the fish and the flowers are
predominantly yellow, so both histograms have a similar peak in that area of the
spectrum (on the left). the other smaller peaks are associated with greens and
blues.

the color feature is useful for finding images with strong color similarity, such
as pictures of sunsets, but two pictures with completely di   erent content can be
considered very similar based solely on their colors. the picture of the flowers in
figure 11.8, for example, may be ranked highly in comparison with the picture of
the fish because of the similar peaks in the histograms. the ability to find seman-
tically related images can be improved by combining color features with texture
and shape features. figure 11.9 shows two examples of id162 based on
texture features (the cars and the trains), and one example of retrieval based on
shape features (the trademarks). texture is broadly defined as the spatial arrange-
ment of gray levels in the image, and shape features describe the form of object
boundaries and edges. the examples show that images with similar appearance
can generally be found using these types of representations, although the second

11.6 words, pictures, and music

475

fig. 11.9. three examples of content-based id162. the collection for the first
two consists of 1,560 images of cars, faces, apes, and other miscellaneous subjects. the
last example is from a collection of 2,048 trademark images. in each case, the leftmost
image is the query.

example makes it clear that similarity in terms of texture does not guarantee se-
mantic similarity. in the case of text-based search, high-ranking documents that
are not relevant can at least be easily understood by the user. with content-based
id162, a retrieval failure such as the picture of the ape in figure 11.9 will
be di   cult to explain to a user who is looking for pictures of trains.

retrieval experiments have shown that the most e   ective way of combining
image features is with a probabilistic retrieval model. if images have text captions
or user tags, these features can easily be incorporated into the ranking, as we have
discussed previously. video retrieval applications are similar to id162,
except that they may have even more features, such as closed caption text or text
generated from id103. the image component of a video is typically
represented as a series of key frame images. to generate key frames, the video is
first segmented into shots or scenes. a video shot can be defined as a continuous
sequence of visually coherent frames, and boundaries can be detected by visual
discontinuities, such a sharp decrease in the similarity of one frame to the next.
given the segmentation into shots, a single frame (picture) is selected as the key
frame. this can be done simply by using the first frame in each shot, or by more

476

11 beyond bag of words

sophisticated techniques based on visual similarity and motion analysis. figure
11.10 shows an example of four key frames extracted from a video of a news con-
ference.

fig. 11.10. key frames extracted from a trec video clip

the id162 techniques we have discussed assume that the query is an
image. in many applications, users would prefer to describe the images they are
looking for with a text query. words cannot be compared directly to features de-
rived from the image. recent research has shown, however, that given enough
training data, a probabilistic retrieval model can learn to associate words or cat-
egories with image-based features and, in e   ect, automatically annotate images.
there are actually two questions that we need to answer using this model:
- given an image with no text annotation, how can we automatically assign
- given a text query {q1, ...qn}, how can we retrieve images that are relevant to

meaningful keywords to that image?

the query?

the relevance model described in section 7.3.2 has been shown to be e   ective
for this task. instead of estimating the joint id203 of observing a word w
with the query words p (w, q1, . . . qn), the relevance model is used to estimate
p (w, i1, . . . im), where an image is assumed to be represented by a set of image
terms{i1, ...im}. the    vocabulary    of image terms in this approach is constrained
by using image segmentation and id91 techniques to identify regions that are
visually similar.14 as an example, in one test corpus of 5,000 images, a vocabulary
of 500 image terms was used to represent the images, with each image being de-
scribed by 1   10 terms. the joint probabilities are estimated using a training set of
images that have text annotations. this model can be used to answer both of the
questions just mentioned. to retrieve images for a text query, a process similar to
pseudo-relevance feedback would be followed:

14 one such technique represents images as    blobs    (carson et al., 1999).

11.6 words, pictures, and music

477

1. use the text query to rank images that do contain text annotations.
2. estimate joint probabilities for the image vocabulary given the top-ranked

images.

3. rerank images using the query expanded with image terms to find images that

do not have text annotations.

fig. 11.11. examples of automatic text annotation of images

alternatively, the joint id203 estimates from the training set can be used
to assign keywords to images that do not have annotations. figure 11.11 shows
examples of images annotated using this approach. most of the words used to de-
scribe the images are reasonable, although the annotation for the picture of the
bear shows that significant errors can be made. retrieval experiments using auto-
matic annotation techniques have shown that this approach has promise and can
increase the e   ectiveness of image and video retrieval in some applications. sig-
nificant questions remain about the type and size of text and image vocabularies
that will be the most e   ective.

music is a media that is even less associated with words than pictures. pictures
can at least be described by words, and automatic annotation techniques can be
used to retrieve images using text queries. apart from the title, composer, per-
former, and lyrics, however, it is very di   cult to describe a piece of music using
words. music has a number of representations that are used for di   erent purposes.
figure 11.12 shows three of these representations for    fugue #10    composed by
bach. the first is the audio signal from a performance of the music. this is what
is stored in a compressed form in mp3 files. the second is a midi15 represen-
tation that provides a digital specification of    events    in the music, such as the
pitch, intensity, duration, and tempo of musical notes. midi is the standard for

15 musical instrument digital interface

people, pool, swimmers, waterfox, forest, river, watercars, formula, tracks, wallclouds, jet, plane, sky478

11 beyond bag of words

communication between electronic instruments and computers. the third rep-
resentation is conventional musical notation, which contains the most explicit
information, especially for polyphonic music made up of multiple parts or voices.

fig. 11.12. three representations of bach   s    fugue #10   : audio, midi, and conventional
music notation

a number of approaches have been developed for deriving index terms for
searching from these basic representations. in the case of audio, one of the most
successful has been the use of    signatures    created by hashing to represent the mu-
sic. a signature could be created, for example, based on the peaks in a spectrogram
of the audio in a time slice.16 this type of indexing is the basis of services that can

16 a spectrogram represents the energy or amplitude of each frequency in the audio signal

at a given time.

11.7 one search fits all?

479

identify music based on a short recording captured using a mobile phone (e.g.,
wang, 2006).

another popular approach to content-based retrieval of music is query-by-
humming. in this type of system, a user literally sings, hums, or plays a tune, and a
music collection is searched for similar melodies. this type of searching is based
on music that has a single melody line (monophonic). the query is converted into
a representation of a melody that consists of information such as the sequence of
notes, relative pitches, and intervals between notes. the music in the collection
must also be converted to the same type of representation, and this is most read-
ily done using midi as the starting point. the query will be a very noisy repre-
sentation of the relevant melody, so a number of retrieval models developed for
text searching, such as id165 matching and language models, have been adapted
for this task (dannenberg et al., 2007). search techniques for polyphonic music
based on probabilistic models have also been used for the retrieval of music scores
represented in conventional music notation.

in summary, search techniques based on retrieval models for text have been
developed for a large range of non-text media. in the case of scanned and spoken
documents, retrieval e   ectiveness is similar to text documents because ocr and
id103 tools generally have low error rates. content-based retrieval
of images and video shows promise, but search applications for these media must
rely on associated text from sources such as captions and user tags to achieve good
e   ectiveness. music is di   cult to describe in text, but e   ective music search appli-
cations have been developed because index terms that have a strong relationship
to what users look for can be derived from the audio or midi representations.

11.7 one search fits all?

so what is the future of search? certainly it does not look like we will build the
omniscient assistant system anytime soon. on the other hand, as we discussed
before, using that type of system as our goal makes it clear how much further
we have to go from current search engines. our knowledge and understanding
of search and e   ectiveness are steadily increasing, despite the field being more
than 40 years old. interestingly, this has produced a variety of di   erent search
services, rather than a single search engine with more and more capability. at the
home page for a popular web search engine, we find links to search engines for
the web, images, blogs, maps, academic papers, products, patents, news, books,

480

11 beyond bag of words

financial information, videos, government documents, and photographs. in addi-
tion, there are links to tools for desktop search, enterprise search, and advertising
search. rather than being simply multiple instances of the same search engine, it
is clear that many of these use somewhat di   erent features, ranking algorithms,
and interfaces. if we look at the research literature, an even greater range of appli-
cations, media, and approaches to search are being developed and evaluated. one
safe prediction is that this expansion of new ideas will continue.

despite the proliferation of customized search engines, there is also a growing
consensus on the principles that underlie them. researchers from information
retrieval and related fields, such as machine learning and natural language pro-
cessing, have developed similar approaches to representing text and modeling the
process of retrieval. these approaches are rapidly being expanded to include struc-
tured data and non-text media. research results based on new applications or data
have consistently reinforced the view that probabilistic models of text and linear
feature-based models for retrieval provide a powerful and e   ective framework for
understanding search. another prediction is that the sophistication of this under-
lying    theory    of search will continue to grow and provide steady improvements
in e   ectiveness for a range of applications.

what impact will this    theory    have on search engines? if there is more agree-
ment about how to represent text and other types of data, and more agreement
about how to rank potential answers in response to questions, then the search
tools that developers use will become more similar than they are currently. open
source search engines, web search engines, desktop search engines and enterprise
search engines available today all use di   erent term weighting, di   erent features,
di   erent ranking algorithms, and di   erent query languages. this is simply due to
the fact that there is no consensus yet on the right way to do these things, but
this will change. when there is more agreement on the underlying models, there
will still be a variety of search tools available, but the choice between them will be
based more on the e   ciency of the implementation, the flexibility and adaptabil-
ity for new applications, and the extent to which the tools implement the more
sophisticated algorithms suggested by the models. this is analogous to what hap-
pened with database systems, where there have been multiple vendors and orga-
nizations providing tools based on the relational model since the 1980s.

another aspect of search, which we have not devoted enough coverage to in
this book, is the fundamental importance of the interaction between users and the
search engine, and the impact of the user   s task on this process. information sci-
entists in particular have focused on these issues and have contributed important

11.7 one search fits all?

481

insights to our understanding of how people find relevant information. as social
search and social networking applications have grown, the study of interaction
has also expanded to include interaction between users as well as between a user
and a search engine. in the near future, we can expect to see theories and models of
search that incorporate users and interaction in a more explicit way than current
models. fuhr (2008) is a recent example of this development.

a common vision of the future, supported by countless films and television
series, assumes an interface based on a simple natural language input with multi-
media output. interaction in this interface is based on dialog. despite, or because
of, its simplicity, this seems like a reasonable long-term goal. in current search
interfaces, queries are certainly simple but, as we have discussed, a considerable
e   ort can go into formulating these simple queries in order to find the one that
retrieves relevant information. in addition, there is very little dialog or interaction,
and even simple ideas, such as relevance feedback, are not used. even worse, the
availability of many di   erent search engines means that another decision (which
search engine?) is added to the interaction.

developers in search engine companies and researchers are studying tech-
niques for improving interaction. some interfaces put results from multiple types
of search engines into a single results display. simple examples of this include
putting a map search engine result at the top of the result list when the query
includes an address, or a link to an academic paper at the top when the query
matches the title closely. it is clear, however, that interfaces in the future must
continue to evolve in order to more actively incorporate users and their knowl-
edge into the search process.

a final prediction is self-serving in the context of this book. search, in all
its forms, will continue to be of critical importance in future software applica-
tions. training people to understand the principles, models, and evaluation tech-
niques that underlie search engines is an important part of continuing to improve
their e   ectiveness and e   ciency. there are not enough courses that focus on this
topic, but through this book and others like it, more people will know more about
search.

references and further reading

linear feature-based retrieval models are discussed in metzler and croft (2007b).
this paper contains references to other models that were discussed in chapter 7.
another recent paper discussing linear models is gao et al. (2005).

482

11 beyond bag of words

many term dependency models have been proposed in the information re-
trieval literature, although few have produced interesting results. van rijsbergen
(1979) presented one of the most-cited dependency models as an extension of
the bayesian classification approach to retrieval. in another early paper, croft et
al. (1991) showed that phrases and term proximity could potentially improve ef-
fectiveness by modeling them as dependencies in the id136 net model. gao et
al. (2004) describe a dependence model that showed significant e   ectiveness ben-
efits, especially for sequential dependencies (or id165s). other recent research
with larger test collections have shown that term proximity information is an ex-
tremely useful feature.

from an information retrieval perspective, dealing with structure in data start-
ed in the 1970s with commercial search services such as medline and dia-
log that had boolean field restrictions. in the 1970s and 1980s, a number of
papers described the implementation of search engines using relational database
systems (e.g., crawford, 1981). e   ciency issues persisted with this approach un-
til the 1990s, although object management systems were successfully used to sup-
port indexes (e.g., brown et al., 1994). the 1990s were also the period when im-
portant work was done on developing probabilistic extensions of database models
for search applications. fuhr and his colleagues described a probabilistic relational
algebra (fuhr & r  lleke, 1997) and a probabilistic datalog system (fuhr, 2000).
in the commercial world, text retrieval had become a standard function in
database systems such as oracle by the early 1990s, but the explosion of web data
and the growth of text-based web applications later that decade made the abil-
ity to handle text e   ectively a critical part of most information systems. an in-
teresting discussion of database and search engine integration from the database
perspective can be found in chaudhuri et al. (2005).

another important line of research has been retrieval using structured and
xml documents. early work in this area dealt with o   ce documents (croft,
krovetz, & turtle, 1990) and document markup (croft et al., 1992). the xquery
query language for xml data is described in chamberlin (2002). kazai et al.
(2003) describe the inex project that evaluates retrieval methods for xml doc-
uments. trotman and lalmas (2006) describe the nexi language.

in the area of id53, metzler and croft (2005a) give an overview
of techniques for question classification. probabilistic approaches to question an-
swering that have been shown to be e   ective include the maximum id178 model
(ittycheriah et al., 2001) and the translation model (echihabi & marcu, 2003).
both are very similar to the retrieval models described in chapter 7.

11.7 one search fits all?

483

taghva et al. (1996) describe the first comprehensive set of experiments show-
ing that ocr errors generally have little e   ect on retrieval e   ectiveness. harding
et al. (1997) show how id165s can compensate for situations where there are
significant ocr error rates.

the book by coden et al. (2002) contains a collection of papers about spo-
ken document retrieval. singhal and pereira (1999) describe an expansion tech-
nique for spoken documents that gave significant e   ectiveness improvements.
the data and major results from the trec spoken document track are described
in voorhees and harman (2005).

many papers have been published about content-based id162. flickner
et al. (1995) describe qbic, one of the first commercial systems to incorporate re-
trieval using color, texture, and shape features. the photobook system (pentland
et al., 1996) also had a significant impact on other cbir projects. ravela and
manmatha (1997) describe one of the first texture-based retrieval techniques to
be evaluated using an information retrieval approach. the sift (scale-invariant
feature transform) algorithm (lowe, 2004) is currently a popular method for rep-
resenting images for search. vasconcelos (2007) gives a recent overview of the field
of cbir.

in the area of content-based retrieval of music, most research is published in
the international conference on music information retrieval (ismir).17 byrd
and crawford (2002) give a good overview of the research issues in this field.
midomi18 is an example of searching music by    humming.   

with regard to the information science perspective on search and interaction,
belkin has written a number of key papers, such as koenemann and belkin (1996)
and belkin (2008). the book by ingwersen and j  rvelin (2005) contains in-depth
discussions of the role of interaction and context in search. marchionini (2006)
discusses similar issues with an emphasis on the search interface.

exercises

11.1. can you find other    visions of the future    related to search engines on the
web or in books, films, or television? describe these systems and any unique fea-
tures they may have.

17 http://www.ismir.net/
18 http://www.midomi.com

484

11 beyond bag of words

11.2. does your favorite web search engine use a bag of words representation?
how can you tell whether it does or doesn   t?

11.3. use the galago #feature operator to create a ranking algorithm that uses
both a bm25 feature and a query likelihood feature.

11.4. show how the linear feature-based ranking function is related to the ab-
stract ranking model from chapter 5.

11.5. how many papers dealing with term dependency can you find in the sigir
proceedings since 2000? list their citations.

11.6. write a program that converts textual queries to sequential dependence
mrf queries in galago, as described in the text. run some queries against an
index, and compare the quality of the results with and without term dependence.
which types of queries are the most improved using the dependence model?
which are hurt the most?

11.7. think of five queries where you are searching for documents structured us-
ing xml. the queries must involve structure and content features. write the
queries in english and in nexi (explain the xml structure if it is not obvious).
do you think the structural part of the query will improve e   ectiveness? give a
detailed example.

11.8. find out about the text search functionality of a database system (either
commercial or open source). describe it in as much detail as you can, including
the query language. compare this functionality to a search engine.

11.9. find a demonstration of a id53 system running on the web.
using a test set of questions, identify which types of questions work and which
don   t on this system. report e   ectiveness using mrr or another measure.

11.10. using the text-based image search for a web search engine, find examples
of images that you think could be retrieved by similarity based on color. use a tool
such as photoshop to generate color histogram values based on hue (or one of the
rgb channels) for your test images. compare the histograms using some simi-
larity measure (such as the normalized sum of the di   erences in the bin values).
how well do these similarities match your visual perceptions?

11.7 one search fits all?

485

11.11. look at a sample of images or videos that have been tagged by users and
separate the tags into three groups: those you think could eventually be done au-
tomatically by image processing and object recognition, those you think would
not be possible to derive by image processing, and spam. also decide which of the
tags should be most useful for queries related to those images. summarize your
findings.

11.12. what features would you like to have to support indexing and retrieval of
personal digital photographs and videos? which of these features are available in
o   -the-shelf software? which of the features are discussed in research papers?

11.13. starting with two mp3 files of two versions of the same song (i.e., di   erent
artists), use tools available on the web to analyze and compare them. you should
be able to find tools to generate midi from the audio, spectrograms, etc. can you
find any similarities between these files that come from the melody? you could
also try recording a song using a microphone and comparing the audio file created
from the recording with the original.

references

abduljaleel, n., & larkey, l. s. (2003). statistical id68 for english-
arabic cross language information retrieval. in cikm    03: proceedings of
the twelfth international conference on information and knowledge manage-
ment (pp. 139   146). acm.

agichtein, e., brill, e., & dumais, s. (2006). improving web search ranking by
incorporating user behavior information. in sigir    06: proceedings of the
29th annual international acm sigir conference on research and develop-
ment in information retrieval (pp. 19   26). acm.

agichtein, e., brill, e., dumais, s., & ragno, r. (2006). learning user inter-
action models for predicting web search result preferences. in sigir    06:
proceedings of the 29th annual international acm sigir conference on re-
search and development in information retrieval (pp. 3   10). acm.

agichtein, e., castillo, c., donato, d., gionis, a., & mishne, g. (2008). finding
in wsdm    08: proceedings of the
high-quality content in social media.
international conference on web search and web data mining (pp. 183   194).
acm.

allan, j. (1996). incremental relevance feedback for information filtering. in
sigir    96: proceedings of the 19th annual international acm sigir con-
ference on research and development in information retrieval (pp. 270   278).
acm.
allan, j. (ed.).

(2002). topic detection and tracking: event-based information

organization. norwell, ma: kluwer academic publishers.

almeida, r. b., & almeida, v. a. f. (2004). a community-aware search engine. in
www    04: proceedings of the 13th international conference on world wide
web (pp. 413   421). acm.

488

references

amershi, s., & morris, m. r.
(2008). cosearch: a system for co-located
in chi    08: proceeding of the twenty-sixth
collaborative web search.
annual sigchi conference on human factors in computing systems (pp.
1,647   1,656). acm.

anagnostopoulos, a., broder, a., & carmel, d. (2005). sampling search-engine
results. in www    05: proceedings of the 14th international conference on
world wide web (pp. 245   256). acm.

anh, v. n., & mo   at, a.

(2005). simplified similarity scoring using term
ranks. in sigir    05: proceedings of the 28th annual international acm
sigir conference on research and development in information retrieval (pp.
226   233). acm.

anh, v. n., & mo   at, a. (2006). pruned query evaluation using pre-computed
impacts. in sigir    06: proceedings of the 29th annual international acm
sigir conference on research and development in information retrieval (pp.
372   379). new york: acm.

baeza-yates, r., & ramakrishnan, r. (2008). data challenges at yahoo! in edbt
   08: proceedings of the 11th international conference on extending database
technology (pp. 652   655). acm.

baeza-yates, r., & ribeiro-neto, b. a. (1999). modern information retrieval.

new york: acm/addison-wesley.

balakrishnan, h., kaashoek, m. f., karger, d., morris, r., & stoica, i. (2003).
looking up data in p2p systems. communications of the acm, 46(2),
43   48.

balog, k., azzopardi, l., & de rijke, m. (2006). formal models for expert finding
in enterprise corpora. in sigir    06: proceedings of the 29th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 43   50). acm.

barroso, l. a., dean, j., & h  lzle, u. (2003). web search for a planet: the google

cluster architecture. ieee micro, 23(2), 22   28.

beeferman, d., & berger, a. (2000). agglomerative id91 of a search en-
gine query log. in proceedings of the sixth acm sigkdd international
conference on knowledge discovery and data mining (pp. 407   416). acm.
belew, r. k. (2000). finding out about. cambridge, uk: cambridge university

press.

belkin, n. j. (2008).

(somewhat) grand challenges for information retrieval.

sigir forum, 42(1), 47   54.

belkin, n. j., & croft, w. b. (1992). information filtering and information re-

references

489

trieval: two sides of the same coin? communications of the acm, 35(12),
29   38.

belkin, n. j., oddy, r. n., & brooks, h. m. (1997). ask for information re-
trieval: part i.: background and theory. in readings in information retrieval
(pp. 299   304). san francisco: morgan kaufmann. (reprinted from jour-
nal of documentation, 1982, 38, 61   71)

bencz  r, a., csalog  ny, k., sarl  s, t., & uher, m. (2005). spamrank     fully
automatic link spam detection. in airweb: 1st international workshop on
adversarial information retrieval on the web (pp. 25   38).

berger, a., caruana, r., cohn, d., freitag, d., & mittal, v. (2000). bridging
the lexical chasm: statistical approaches to answer-finding. in sigir    00:
proceedings of the 23rd annual international acm sigir conference on re-
search and development in information retrieval (pp. 192   199). acm.

berger, a., & la   erty, j. (1999). information retrieval as statistical translation. in
sigir    99: proceedings of the 22nd annual international acm sigir con-
ference on research and development in information retrieval (pp. 222   229).
acm.

berger, a., & mittal, v. o.

(2000). ocelot: a system for summarizing web
pages. in sigir    00: proceedings of the 23rd annual international acm
sigir conference on research and development in information retrieval (pp.
144   151). acm.

bergman, m. k. (2001). the deep web: surfacing hidden value. journal of elec-

tronic publishing, 7(1).

bernstein, y., & zobel, j. (2005). redundant documents and search e   ective-
ness. in cikm    05: proceedings of the 14th acm international conference
on information and knowledge management (pp. 736   743). acm.

bernstein, y., & zobel, j. (2006). accurate discovery of co-derivative documents

via duplicate text detection. information systems, 31, 595   609.

bikel, d. m., miller, s., schwartz, r., & weischedel, r. (1997). nymble: a high-
performance learning name-finder. in proceedings of the fifth conference on
applied natural language processing (pp. 194   201). morgan kaufmann.

bikel, d. m., schwartz, r. l., & weischedel, r. m. (1999). an algorithm that

learns what   s in a name. machine learning, 34(1   3), 211   231.

blei, d. m., ng, a. y., & jordan, m. i. (2003). id44. journal

of machine learning research, 3, 993   1,022.

borgs, c., chayes, j., mahdian, m., & saberi, a. (2004). exploring the commu-
nity structure of newsgroups. in kdd    04: proceedings of the tenth acm

490

references

sigkdd international conference on knowledge discovery and data mining
(pp. 783   787). acm.

breese, j., heckerman, d., & kadie, c. (1998). empirical analysis of predictive
algorithms for id185. in uai    98: proceedings of the uncer-
tainty in artifical intelligence conference (pp. 43   52).

brill, e. (1994). some advances in transformation-based id52.
in aaai    94: national conference on artificial intelligence (pp. 722   727).
brin, s., & page, l. (1998). the anatomy of a large-scale hypertextual web search

engine. computer networks and isdn systems, 30(1   7), 107   117.
broder, a. (2002). a taxonomy of web search. sigir forum, 36(2), 3   10.
broder, a., fontoura, m., josifovski, v., & riedel, l. (2007). a semantic ap-
in sigir    07: proceedings of the 30th
proach to contextual advertising.
annual international acm sigir conference on research and development
in information retrieval (pp. 559   566). acm.

broder, a., fontura, m., josifovski, v., kumar, r., motwani, r., nabar, s.,     xu,
y. (2006). estimating corpus size via queries. in cikm    06: proceedings of
the 15th acm international conference on information and knowledge man-
agement (pp. 594   603). acm.

broder, a., glassman, s. c., manasse, m. s., & zweig, g.

(1997). syntactic
id91 of the web. computer networks and isdn systems, 29(8   13),
1157   1166.

brown, e. w., callan, j., croft, w. b., & moss, j. e. b.

(1994). supporting
full-text information retrieval with a persistent object store. in edbt    94:
4th international conference on extending database technology (vol. 779, pp.
365   378). springer.

buckley, c., & voorhees, e. m.

(2004). retrieval evaluation with incom-
plete information. in sigir    04: proceedings of the 27th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 25   32). acm.

burges, c., shaked, t., renshaw, e., lazier, a., deeds, m., hamilton, n., & hul-
lender, g. (2005). learning to rank using id119. in icml    05:
proceedings of the 22nd international conference on machine learning (pp.
89   96). acm.

burges, c. j. c. (1998). a tutorial on support vector machines for pattern recog-

nition. data mining and knowledge discovery, 2(2), 121   167.

burke, r. d., hammond, k. j., kulyukin, v. a., lytinen, s. l., tomuro, n., &
schoenberg, s. (1997). id53 from frequently asked question

references

491

files: experiences with the faq finder system (tech. rep.). chicago, il,
usa.

b  ttcher, s., & clarke, c. l. a. (2007). index compression is good, especially for
random access. in cikm    07: proceedings of the sixteenth acm conference
on information and knowledge management (pp. 761   770). acm.

b  ttcher, s., clarke, c. l. a., & lushman, b. (2006). hybrid index maintenance
for growing text collections. in sigir    06: proceedings of the 29th annual
international acm sigir conference on research and development in infor-
mation retrieval (pp. 356   363). acm.

byrd, d., & crawford, t. (2002). problems of music information retrieval in the

real world. information processing and management, 38(2), 249   272.

callan, j. (2000). distributed information retrieval. in advances in informa-
tion retrieval: recent research from the ciir (pp. 127   150). norwell, ma:
kluwer academic publishers.

callan, j., croft, w. b., & broglio, j. (1995). trec and tipster experiments
with inquery. information processing and management, 31(3), 327   343.
callan, j., croft, w. b., & harding, s. m. (1992). the inquery retrieval system.
in proceedings of dexa-92, 3rd international conference on database and
id109 applications (pp. 78   83).

cao, y., xu, j., liu, t.-y., li, h., huang, y., & hon, h.-w. (2006). adapting
ranking id166 to document retrieval. in sigir    06: proceedings of the 29th
annual international acm sigir conference on research and development
in information retrieval (pp. 186   193). acm.

carbonell, j., & goldstein, j. (1998). the use of mmr, diversity-based reranking
for reordering documents and producing summaries. in sigir    98: pro-
ceedings of the 21st annual international acm sigir conference on research
and development in information retrieval (pp. 335   336). acm.

carson, c., thomas, m., belongie, s., hellerstein, j. m., & malik, j. (1999). blob-
world: a system for region-based image indexing and retrieval. in visual
   99: third international conference on visual information and information
systems (pp. 509   516). springer.

carterette, b., allan, j., & sitaraman, r. (2006). minimal test collections for
retrieval evaluation. in sigir    06: proceedings of the 29th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 268   275). acm.

carterette, b., & jones, r. (2007). evaluating search engines by modeling the
relationship between relevance and clicks. in nips    07: proceedings of the

492

references

conference on neural information processing systems (pp. 217   224). mit
press.

chakrabarti, s., van den berg, m., & dom, b. (1999). focused crawling: a new
approach to topic-specific web resource discovery. computer networks,
31(11-16), 1,623   1,640.

chamberlin, d. (2002). xquery: an xml query language. ibm systems jour-

nal, 41(4), 597   615.

chang, f., dean, j., ghemawat, s., hsieh, w. c., wallach, d. a., burrows, m.,    
gruber, r. e. (2006). bigtable: a distributed storage system for structured
data. in osdi    06: proceedings of the 7th symposium on operating systems
design and implementation (pp. 205   218). usenix association.

charikar, m. s. (2002). similarity estimation techniques from rounding algo-
rithms. in stoc    02: proceedings of the annual acm symposium on theory
of computing (pp. 380   388). acm.

chaudhuri, s., ramakrishnan, r., & weikum, g. (2005). integrating db and
ir technologies: what is the sound of one hand clapping? in cidr 2005:
second biennial conference on innovative data systems research (pp. 1   12).
chen, y.-y., suel, t., & markowetz, a. (2006). e   cient query processing in
geographic web search engines. in sigmod    06: proceedings of the acm
sigmod international conference on management of data (pp. 277   288).
acm.

cho, j., & garcia-molina, h. (2002). parallel crawlers. in www 2002: pro-
ceedings of the 11th annual international world wide web conference (pp.
124   135). acm.

cho, j., & garcia-molina, h.

(2003). e   ective page refresh policies for web

crawlers. acm transactions on database systems, 28, 390   426.

church, k. w. (1988). a stochastic parts program and noun phrase parser for
unrestricted text. in proceedings of the second conference on applied natural
language processing (pp. 136   143). association for computational lin-
guistics.

church, k. w., & hanks, p. (1989). word association norms, mutual informa-
tion, and id69. in proceedings of the 27th annual meeting on asso-
ciation for computational linguistics (pp. 76   83). association for com-
putational linguistics.

clarke, c. l., agichtein, e., dumais, s., & white, r. w. (2007). the influence
of caption features on clickthrough patterns in web search. in sigir    07:
proceedings of the 30th annual international acm sigir conference on re-

references

493

search and development in information retrieval (pp. 135   142). acm.

cleverdon, c. (1970). evaluation tests of information retrieval systems. journal

of documentation, 26(1), 55   67.

coden, a., brown, e. w., & srinivasan, s. (eds.). (2002). information retrieval

techniques for speech applications. london: springer-verlag.

cong, g., wang, l., lin, c.-y., song, y.-i., & sun, y. (2008). finding question-
in sigir    08: proceedings of the 31st
answer pairs from online forums.
annual international acm sigir conference on research and development
in information retrieval (pp. 467   474). acm.

conrad, j. g., & utt, m. h. (1994). a system for discovering relationships by
feature extraction from text databases. in sigir    94: proceedings of the 17th
annual international acm sigir conference on research and development
in information retrieval (pp. 260   270). springer-verlag.

cooper, w. s. (1968). expected search length: a single measure of retrieval e   ec-
tiveness based on the weak ordering action of retrieval systems. american
documentation, 19(1), 30   41.

cooper, w. s., gey, f. c., & dabney, d. p. (1992). probabilistic retrieval based
on staged id28. in sigir    92: proceedings of the 15th annual
international acm sigir conference on research and development in infor-
mation retrieval (pp. 198   210). acm.

cowie, j., & lehnert, w. (1996). information extraction. communications of the

acm, 39(1), 80   91.

crawford, r. (1981). the relational model in information retrieval. journal of

the american society for information science, 32(1), 51   64.

croft, w. b. (2000). combining approaches to information retrieval. in advances
in information retrieval: recent research from the ciir (pp. 1   36). norwell,
ma: kluwer academic publishers.

croft, w. b., krovetz, r., & turtle, h. (1990). interactive retrieval of complex

documents. information processing and management, 26(5), 593   613.

croft, w. b., & la   erty, j. (2003). id38 for information retrieval.

norwell, ma: kluwer academic publishers.

croft, w. b., smith, l. a., & turtle, h. r. (1992). a loosely-coupled integration
of a text retrieval system and an object-oriented database system. in sigir
   92: proceedings of the 15th annual international acm sigir conference on
research and development in information retrieval (pp. 223   232). acm.
croft, w. b., & turtle, h. (1989). a retrieval model incorporating hypertext
links. in hypertext    89: proceedings of the second annual acm conference on

494

references

hypertext (pp. 213   224). acm.

croft, w. b., turtle, h. r., & lewis, d. d. (1991). the use of phrases and struc-
tured queries in information retrieval. in sigir    91: proceedings of the 14th
annual international acm sigir conference on research and development
in information retrieval (pp. 32   45). acm.

cronen-townsend, s., zhou, y., & croft, w. b. (2006). precision prediction

based on ranked list coherence. information retrieval, 9(6), 723   755.

cucerzan, s., & brill, e. (2004). id147 as an iterative process that
exploits the collective knowledge of web users. in d. lin & d. wu (eds.),
proceedings of emnlp 2004 (pp. 293   300). association for computa-
tional linguistics.

cui, h., wen, j.-r., nie, j.-y., & ma, w.-y. (2003). id183 by mining
user logs. ieee transactions on knowledge and data engineering, 15(4),
829   839.

dannenberg, r. b., birmingham, w. p., pardo, b., hu, n., meek, c., & tzane-
takis, g. (2007). a comparative evaluation of search techniques for query-
by-humming using the musart testbed. journal of the american society
for information science and technology, 58(5), 687   701.

dean, j., & ghemawat, s. (2008). mapreduce: simplified data processing on

large clusters. communications of the acm, 51(1), 107   113.

decandia, g., hastorun, d., jampani, m., kakulapati, g., lakshman, a., pilchin,
a.,     vogels, w. (2007). dynamo: amazon   s highly available key-value
store. in sosp    07: proceedings of the twenty-first acm sigops symposium
on operating systems principles (pp. 205   220). acm.

deerwester, s. c., dumais, s. t., landauer, t. k., furnas, g. w., & harshman,
r. a. (1990). indexing by latent semantic analysis. journal of the amer-
ican society of information science, 41(6), 391   407.

deutsch, p.

(1996). deflate compressed data format specification version
1.3 (rfc no. 1951). internet engineering task force. retrieved from
http://www.rfc-editor.org/rfc/rfc1951.txt

diaz, f. (2005). regularizing ad hoc retrieval scores. in cikm    05: proceedings of
the 14th acm international conference on information and knowledge man-
agement (pp. 672   679). acm.

duda, r. o., hart, p. e., & stork, d. g. (2000). pattern classification (2nd ed.).

wiley-interscience.

dunlop, m. d., & van rijsbergen, c. j. (1993). hypermedia and free text re-

trieval. information processing and management, 29(3), 287   298.

references

495

echihabi, a., & marcu, d. (2003). a noisy-channel approach to question answer-
ing. in acl    03: proceedings of the 41st annual meeting on association for
computational linguistics (pp. 16   23). association for computational
linguistics.
efthimiadis, e. n.

in m. e. williams (ed.), an-
nual review of information systems and technology (arist) (vol. 31, pp.
121   187).

(1996). id183.

elmasri, r., & navathe, s. (2006). fundamentals of database systems (5th ed.).

reading, ma: addison-wesley.

fagin, r., lotem, a., & naor, m. (2003). optimal aggregation algorithms for

middleware. journal of computer and systems sciences, 66(4), 614   656.

feng, j., bhargava, h. k., & pennock, d. m.

implementing spon-
sored search in web search engines: computational evaluation of alterna-
tive mechanisms. informs journal on computing, 19(1), 137   148.

(2007).

fetterly, d., manasse, m., & najork, m. (2003). on the evolution of clusters of
near-duplicate web pages. in la-web    03: proceedings of thefirst conference
on latin american web congress (pp. 37   45). ieee computer society.

finn, a., kushmerick, n., & smyth, b. (2001). fact or fiction: content clas-
in delos workshop: personalisation and

sification for digital libraries.
recommender systems in digital libraries.

flake, g. w., lawrence, s., & giles, c. l. (2000). e   cient identification of web
communities. in kdd    00: proceedings of the sixth acm sigkdd inter-
national conference on knowledge discovery and data mining (pp. 150   160).
acm.

flickner, m., sawhney, h. s., ashley, j., huang, q., dom, b., gorkani, m.,    
yanker, p. (1995). query by image and video content: the qbic system.
ieee computer, 28(9), 23   32.

fox, s., karnawat, k., mydland, m., dumais, s., & white, t. (2005). evaluating
implicit measures to improve web search. acm transactions on informa-
tion systems, 23(2), 147   168.

fuhr, n. (2000). probabilistic datalog: implementing logical information re-
trieval for advanced applications. journal of the american society for infor-
mation science and technology, 51(2), 95   110.

fuhr, n.

(2008). a id203 ranking principle for interactive information

retrieval. information retrieval, 11(3), 251   265.

fuhr, n., & buckley, c. (1991). a probabilistic learning approach for document

indexing. acm transactions on information systems, 9(3), 223   248.

496

references

fuhr, n., & r  lleke, t. (1997). a probabilistic relational algebra for the integra-
tion of information retrieval and database systems. acm transactions on
information systems, 15(1), 32   66.

fujii, h., & croft, w. b.

(1993). a comparison of indexing techniques for
japanese text retrieval. in sigir    93: proceedings of the 16th annual in-
ternational acm sigir conference on research and development in infor-
mation retrieval (pp. 237   246). acm.

gao, j., nie, j.-y., wu, g., & cao, g. (2004). dependence language model for
information retrieval. in sigir    04: proceedings of the 27th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 170   177). acm.

gao, j., qi, h., xia, x., & nie, j.-y. (2005). linear discriminant model for in-
formation retrieval. in sigir    05: proceedings of the 28th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 290   297). acm.

garcia-molina, h., ullman, j. d., & widom, j. d. (2008). database systems: the

complete book. prentice hall.

gibson, d., kleinberg, j., & raghavan, p. (1998). inferring web communities
from link topology. in hypertext    98: proceedings of the ninth acm
conference on hypertext and hypermedia (pp. 225   234). acm.

golder, s. a., & huberman, b. a. (2006). usage patterns of collaborative tagging

systems. journal of information science, 32(2), 198   208.

goldstein, j., kantrowitz, m., mittal, v., & carbonell, j. (1999). summarizing
text documents: sentence selection and id74. in sigir    99:
proceedings of the 22nd annual international acm sigir conference on re-
search and development in information retrieval (pp. 121   128). acm.

grefenstette, g. (1998). cross-language information retrieval. norwell, ma:

kluwer academic publishers.

guo, j., xu, g., li, h., & cheng, x. (2008). a unified and discriminative model
for query refinement. in sigir    08: proceedings of the 31st annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 379   386). acm.

gupta, s., kaiser, g., neistadt, d., & grimm, p. (2003). dom-based content
extraction of html documents. in www    03: proceedings of the 12th
international conference on world wide web (pp. 207   214). acm.

gy  ngyi, z., & garcia-molina, h. (2005). web spam taxonomy. in airweb: 1st
international workshop on adversarial information retrieval on the web (pp.

references

497

39   47).

gy  ngyi, z., garcia-molina, h., & pedersen, j. (2004). combating web spam
with trustrank. in vldb 2004: proceedings of the thirtieth international
conference on very large data bases (pp. 576   587). morgan kaufmann.

ha, l. q., sicilia-garcia, e. i., ming, j., & smith, f. j. (2002). extension of zipf    s
law to words and phrases. in proceedings of the 19th international confer-
ence on computational linguistics (pp. 1   6). association for computational
linguistics.

harding, s. m., croft, w. b., & weir, c. (1997). probabilistic retrieval of ocr
degraded text using id165s. in ecdl    97: proceedings of the first euro-
pean conference on research and advanced technology for digital libraries (pp.
345   359). springer-verlag.

hastie, t., tibshirani, r., & friedman, j. (2001). the elements of statistical learn-

ing: data mining, id136, and prediction. springer.

hatcher, e., & gospodnetic, o. (2004). lucene in action. manning publications.
haveliwala, t. h. (2002). topic-sensitive id95. in www    02: proceed-
ings of the 11th international conference on world wide web (pp. 517   526).
acm.

hawking, d., & zobel, j. (2007). does topic metadata help with web search?
journal of the american society for information science and technology,
58(5), 613   628.

he, b., patel, m., zhang, z., & chang, k. (2007). accessing the deep web. com-

munications of the acm, 50(5), 94   101.

heaps, h. (1978). information retrieval: computational and theoretical aspects.

new york: academic press.

hearst, m. a. (1999). user interfaces and visualization. in modern information

retrieval (pp. 257   324). acm/addison-wesley.

hearst, m. a. (2006). id91 versus faceted categories for information ex-

ploration. communications of the acm, 49(4), 59   61.

hearst, m. a., & pedersen, j. o. (1996). reexamining the cluster hypothesis:
scatter/gather on retrieval results. in sigir    96: proceedings of the 19th
annual international acm sigir conference on research and development
in information retrieval (pp. 76   84). acm.

henzinger, m. (2006). finding near-duplicate web pages: a large-scale evalua-
tion of algorithms. in sigir    06: proceedings of the 29th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 284   291). acm.

498

references

herlocker, j. l., konstan, j. a., terveen, l. g., & riedl, j. t. (2004). evaluating
id185 recommender systems. acm transactions on infor-
mation systems, 22(1), 5   53.

heymann, p., koutrika, g., & garcia-molina, h. (2008). can social bookmark-
ing improve web search? in wsdm    08: proceedings of the international
conference on web search and web data mining (pp. 195   206). acm.

heymann, p., ramage, d., & garcia-molina, h. (2008). social tag prediction. in
sigir    08: proceedings of the 31st annual international acm sigir con-
ference on research and development in information retrieval (pp. 531   538).
acm.

hiemstra, d. (1998). a linguistically motivated probabilistic model of infor-
mation retrieval. in ecdl    98: proceedings of the second european confer-
ence on research and advanced technology for digital libraries (pp. 569   584).
springer-verlag.

hoad, t., & zobel, j. (2003). methods for identifying versioned and plagia-
journal of the american society of information science

rised documents.
and technology, 54(3), 203   215.

hobbs, j., douglas, r., appelt, e., bear, j., israel, d., kameyama, m.,     tyson,
m. (1997). fastus: a cascaded finite-state transducer for extracting in-
formation from natural-language text. in finite state language processing
(chap. 13). cambridge, ma: mit press.

hofmann, t.

(1999). probabilistic id45.

in sigir    99:
proceedings of the 22nd annual international acm sigir conference on re-
search and development in information retrieval (pp. 50   57). acm.

hopcroft, j., khan, o., kulis, b., & selman, b.

(2003). natural communi-
ties in large linked networks. in kdd    03: proceedings of the ninth acm
sigkdd international conference on knowledge discovery and data mining
(pp. 541   546). acm.

ingwersen, p., & j  rvelin, k. (2005). the turn: integration of information seeking

and retrieval in context. secaucus, nj: springer-verlag.

ipeirotis, p. g., & gravano, l. (2004). when one sample is not enough: improv-
ing text database selection using shrinkage. in sigmod    04: proceedings of
the 2004 acm sigmod international conference on management of data
(pp. 767   778). acm.

ittycheriah, a., franz, m., zhu, w.-j., ratnaparkhi, a., & mammone, r. j. (2001).
in naacl
id53 using maximum id178 components.
   01: second meeting of the north american chapter of the association for

references

499

computational linguistics on language technologies (pp. 1   7). association
for computational linguistics.

j  rvelin, k., & kek  l  inen, j. (2002). cumulated gain-based evaluation of ir
techniques. acm transactions on information systems, 20(4), 422   446.
jeon, j., croft, w. b., & lee, j. h. (2005). finding similar questions in large
question and answer archives. in cikm    05: proceedings of the 14th acm
international conference on information and knowledge management (pp.
84   90). acm.

jeon, j., croft, w. b., lee, j. h., & park, s.

(2006). a framework to predict
the quality of answers with non-textual features. in sigir    06: proceedings
of the 29th annual international acm sigir conference on research and
development in information retrieval (pp. 228   235). acm.

jijkoun, v., & de rijke, m. (2005). retrieving answers from frequently asked
questions pages on the web. in cikm    05: proceedings of the 14th acm
international conference on information and knowledge management (pp.
76   83). acm.

jing, y., & croft, w. b. (1994). an association thesaurus for information re-
trieval. in proceedings of riao-94, 4th international conference    recherche
d   information assistee par ordinateur    (pp. 146   160).

joachims, t. (2002a). learning to classify text using support vector machines: meth-

ods, theory and algorithms. norwell, ma: kluwer academic publishers.

joachims, t. (2002b). optimizing search engines using clickthrough data. in
kdd    02: proceedings of the eighth acm sigkdd international conference
on knowledge discovery and data mining (pp. 133   142). acm.

joachims, t., granka, l., pan, b., hembrooke, h., & gay, g. (2005). accurately
interpreting clickthrough data as implicit feedback. in sigir    05: proceed-
ingsof the28thannualinternationalacmsigirconferenceonresearchand
development in information retrieval (pp. 154   161). acm.

jones, r., rey, b., madani, o., & greiner, w. (2006). generating query substi-
tutions. in www    06: proceedings of the 15th international conference on
world wide web (pp. 387   396). acm.

jurafsky, d., & martin, j. h. (2006). speech and language processing (2nd ed.).

london: prentice hall.

kazai, g., g  vert, n., lalmas, m., & fuhr, n. (2003). the inex evaluation
initiative. in h. blanken, t. grabs, h.-j. schek, r. schenkel, & g. weikum
(eds.), intelligent xml retrieval (pp. 279   293). springer.

kelly, d., & teevan, j. (2003). implicit feedback for inferring user preference: a

500

references

kleinberg, j. m. (1999). authoritative sources in a hyperlinked environment.

bibliography. sigir forum, 32(2).

journal of the acm, 46(5), 604   632.

knuth, d. e. (1998). the art of computer programming: sorting and searching

(2nd ed., vol. 3). redwood city, ca: addison-wesley longman.

koenemann, j., & belkin, n. j. (1996). a case for interaction: a study of interac-
tive information retrieval behavior and e   ectiveness. in chi    96: proceed-
ings of the sigchi conference on human factors in computing systems (pp.
205   212). acm.

kraaij, w., westerveld, t., & hiemstra, d.

(2002). the importance of prior
probabilities for entry page search. in sigir    02: proceedings of the 25th
annual international acm sigir conference on research and development
in information retrieval (pp. 27   34). acm.

krovetz, r. (1993). viewing morphology as an id136 process. in sigir
   93: proceedings of the 16th annual international acm sigir conference on
research and development in information retrieval (pp. 191   202). acm.
kukich, k. (1992). techniques for automatically correcting words in text. acm

computing surveys, 24(4), 377   439.

kurland, o. (2008). the opposite of smoothing: a language model approach to
ranking query-specific document clusters. in sigir    08: proceedings of the
31st annual international acm sigir conference on research and develop-
ment in information retrieval (pp. 171   178). acm.

kurland, o., & lee, l. (2004). corpus structure, language models, and ad hoc
information retrieval. in sigir    04: proceedings of the 27th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 194   201). acm.

kurland, o., & lee, l.

(2005). id95 without hyperlinks: structural re-
ranking using links induced by language models. in sigir    05: proceedings
of the 28th annual international acm sigir conference on research and de-
velopment in information retrieval (pp. 306   313). acm.

la   erty, j., & zhai, c. (2001). document language models, query models, and
risk minimization for information retrieval. in sigir    01: proceedings of
the 24th annual international acm sigir conference on research and de-
velopment in information retrieval (pp. 111   119). acm.

lankes, r. d. (2004). the digital reference research agenda. journal of the amer-

ican society for information science and technology, 55(4), 301   311.

larkey, l. s., ballesteros, l., & connell, m. e.

(2002).

improving id30

references

501

for arabic information retrieval: light id30 and co-occurrence anal-
ysis. in sigir    02: proceedings of the 25th annual international acm si-
gir conference on research and development in information retrieval (pp.
275   282). acm.

lavrenko, v., & croft, w. b. (2001). relevance based language models. in sigir
   01: proceedings of the 24th annual international acm sigir conference on
research and development in information retrieval (pp. 120   127). acm.
lawrie, d. j., & croft, w. b. (2003). generating hierarchical summaries for web
searches. in sigir    03: proceedings of the 26th annual international acm
sigir conference on research and development in information retrieval (pp.
457   458). acm.

leouski, a., & croft, w. (1996). an evaluation of techniques for id91 search
results (tech. rep. nos. ir   76). department of computer science, uni-
versity of massachusetts amherst.

lester, n., mo   at, a., & zobel, j.

(2005). fast on-line index construction
in cikm    05: proceedings of the 14th acm
by geometric partitioning.
international conference on information and knowledge management (pp.
776   783). new york: acm.

leuski, a., & lavrenko, v. (2006). tracking dragon-hunters with language mod-
els. in cikm    06: proceedings of the 15th acm international conference on
information and knowledge management (pp. 698   707). acm.

liu, x., & croft, w. b. (2004). cluster-based retrieval using language models. in
sigir    04: proceedings of the 27th annual international acm sigir con-
ference on research and development in information retrieval (pp. 186   193).
acm.

liu, x., & croft, w. b. (2008). evaluating text representations for retrieval of the
best group of documents. in ecir    08: proceedings of the 30th european
conference on information retrieval (pp. 454   462). springer.

lowe, d. g. (2004). distinctive image features from scale-invariant keypoints.

international journal of id161, 60(2), 91   110.

lu, j., & callan, j. (2006). full-text federated search of text-based digital libraries

in peer-to-peer networks. information retrieval, 9(4), 477   498.

lu, j., & callan, j. (2007). content-based peer-to-peer network overlay for full-
text federated search. in riao    07: proceedings of the eighth riao confer-
ence.

luhn, h. p. (1958). the automatic creation of literature abstracts. ibm journal

of research and development, 2(2), 159   165.

502

references

lund, k., & burgess, c. (1996). producing high-dimensional semantic spaces
from lexical co-occurrence. behavior research methods, instrumentation,
and computers, 28(2), 203   208.

manning, c. d., raghavan, p., & sch  tze, h. (2008). introduction to information

retrieval. new york: cambridge university press.

manning, c. d., & sch  tze, h. (1999). foundations of statistical natural language

processing. cambridge, ma: the mit press.

marchionini, g. (2006). exploratory search: from finding to understanding.

communications of the acm, 49(4), 41   46.

mcbryan, o. a. (1994). genvl and wwww: tools for taming the web. in
www    94: proceedings of the first international world wide web conference
(p. 15). cern, geneva.

mccallum, a. (2005). information extraction: distilling structured data from

unstructured text. queue, 3(9), 48   57.

mccallum, a., & nigam, k. (1998). a comparison of event models for naive
bayes text classification. in aaai-98 workshop on learning for text catego-
rization.

menczer, f., & belew, r. k. (1998). adaptive information agents in distributed
textual environments. in agents    98: proceedings of the second interna-
tional conference on autonomous agents (pp. 157   164). acm.

metzler, d., & croft, w. b. (2004). combining the language model and id136
network approaches to retrieval. information processing and management,
40(5), 735   750.

metzler, d., & croft, w. b. (2005a). analysis of statistical question classification

for fact-based questions. information retrieval, 8(3), 481   504.

metzler, d., & croft, w. b.

(2005b). a markov random field model for
term dependencies. in sigir    05: proceedings of the 28th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 472   479). acm.

metzler, d., & croft, w. b. (2007a). latent concept expansion using markov
in sigir    07: proceedings of the 30th annual interna-
random fields.
tional acm sigir conference on research and development in information
retrieval (pp. 311   318). acm.

metzler, d., & croft, w. b. (2007b). linear feature-based models for information

retrieval. information retrieval, 10(3), 257   274.

metzler, d., dumais, s. t., & meek, c. (2007). similarity measures for short
segments of text. in ecir    07: proceedings of the european conference on

references

503

information retrieval (pp. 16   27). springer.

metzler, d., lavrenko, v., & croft, w. b. (2004). formal multiple-bernoulli
models for id38. in sigir    04: proceedings of the 27th an-
nual international acm sigir conference on research and development in
information retrieval (pp. 540   541). acm.

metzler, d., strohman, t., & croft, w. b. (2008). a statistical view of binned re-
trieval models. in ecir 2008: proceedings of the 30th european conference
on information retrieval (pp. 175   186). springer.

metzler, d., strohman, t., turtle, h., & croft, w.

indri at trec
2004: terabyte track. in nist special publication 500   261: text retrieval
conference proceedings (trec 2004). national institute of standards and
technology.

(2004).

miller, d. r. h., leek, t., & schwartz, r. m. (1999). a hidden markov model
in sigir    99: proceedings of the 22nd an-
information retrieval system.
nual international acm sigir conference on research and development in
information retrieval (pp. 214   221). acm.

mizzaro, s. (1997). relevance: the whole history. journal of the american society

of information science, 48(9), 810   832.

mo   at, a., webber, w., & zobel, j. (2007). strategic system comparisons via
targeted relevance judgments. in sigir    07: proceedings of the 30th an-
nual international acm sigir conference on research and development in
information retrieval (pp. 375   382). acm.

montague, m., & aslam, j. a. (2002). condorcet fusion for improved retrieval.
in cikm    02: proceedings of the eleventh international conference on infor-
mation and knowledge management (pp. 538   548). acm.

morris, m. r. (2008). a survey of collaborative web search practices. in chi    08:
proceeding of the twenty-sixth annual sigchi conference on human factors
in computing systems (pp. 1,657   1,660). acm.

morris, m. r., & horvitz, e. (2007a). s3: storable, shareable search. in interact

(1) (pp. 120   123).

morris, m. r., & horvitz, e.

(2007b). searchtogether: an interface for col-
laborative web search. in uist    07: proceedings of the 20th annual acm
symposium on user interface software and technology (pp. 3   12). acm.

ntoulas, a., najork, m., manasse, m., & fetterly, d. (2006). detecting spam
web pages through content analysis. in www    06: proceedings of the 15th
international conference on world wide web (pp. 83   92).

ogilvie, p., & callan, j.

(2003). combining id194s for

504

references

known-item search. in sigir    03: proceedings of the 26th annual interna-
tional acm sigir conference on research and development in informaion
retrieval (pp. 143   150). acm.

pang, b., lee, l., & vaithyanathan, s. (2002). thumbs up?: sentiment classifi-
cation using machine learning techniques. in emnlp    02: proceedings of
the acl-02 conference on empirical methods in natural language processing
(pp. 79   86). association for computational linguistics.

peng, f., ahmed, n., li, x., & lu, y. (2007). context sensitive id30 for web
search. in sigir    07: proceedings of the 30th annual international acm
sigir conference on research and development in information retrieval (pp.
639   646). acm.

peng, f., feng, f., & mccallum, a. (2004). chinese segmentation and new word
detection using id49. in coling    04: proceedings
of the 20th international conference on computational linguistics (p. 562).
association for computational linguistics.

pentland, a., picard, r. w., & sclaro   , s. (1996). photobook: content-based
manipulation of image databases. internationaljournal of id161,
18(3), 233   254.

petkova, d., & croft, w. b.

(2007). proximity-based document represen-
in cikm    07: proceedings of the six-
tation for named entity retrieval.
teenth acm conference on information and knowledge management (pp.
731   740). acm.

pickens, j., golovchinsky, g., shah, c., qvarfordt, p., & back, m. (2008). al-
gorithmic mediation for collaborative exploratory search. in sigir    08:
proceedings of the 31st annual international acm sigir conference on re-
search and development in information retrieval (pp. 315   322). acm.

pinto, d., branstein, m., coleman, r., croft, w. b., king, m., li, w., & wei, x.
(2002). quasm: a system for id53 using semi-structured
data. in jcdl    02: proceedings of the 2nd acm/ieee-cs joint conference
on digital libraries (pp. 46   55). acm.

ponte, j. m., & croft, w. b. (1998). a id38 approach to infor-
in sigir    98: proceedings of the 21st annual interna-
mation retrieval.
tional acm sigir conference on research and development in information
retrieval (pp. 275   281). acm.

porter, m. f. (1997). an algorithm for su   x stripping. in readings in information

retrieval (pp. 313   316). san francisco: morgan kaufmann.

powell, a. l., french, j. c., callan, j., connell, m., & viles, c. l. (2000). the

references

505

impact of database selection on distributed searching. in sigir    00: pro-
ceedings of the 23rd annual international acm sigir conference on research
and development in information retrieval (pp. 232   239). acm.

pritchard-schoch, t. (1993). win   westlaw goes natural. online, 17(1),

101   103.

rajashekar, t. b., & croft, w. b. (1995). combining automatic and manual
index representations in probabilistic retrieval. journal of the american so-
ciety of information science, 46(4), 272   283.

ravela, s., & manmatha, r. (1997). id162 by appearance. in sigir
   97: proceedings of the 20th annual international acm sigir conference on
research and development in information retrieval (pp. 278   285). acm.
riezler, s., vasserman, a., tsochantaridis, i., mittal, v., & liu, y. (2007). statisti-
cal machine translation for id183 in answer retrieval. in proceed-
ings of the 45th annual meeting of the association of computational linguistics
(pp. 464   471). acl.

robertson, s. e. (1997). the id203 ranking principle in ir. in readings
in information retrieval (pp. 281   286). morgan kaufmann. (reprinted
from journal of documentation, 1977, 33, 294   304)

robertson, s. e. (2004). understanding inverse document frequency: on theo-

retical arguments for idf. journal of documentation, 60, 503   520.

robertson, s. e., & walker, s. (1994). some simple e   ective approximations to
the 2-poisson model for probabilistic weighted retrieval. insigir   94:pro-
ceedings of the 17th annual international acm sigir conference on research
and development in information retrieval (pp. 232   241). springer-verlag.
robertson, s. e., zaragoza, h., & taylor, m. (2004). simple bm25 extension to
multiple weighted fields. in cikm    04: proceedings of the thirteenth acm
international conference on information and knowledge management (pp.
42   49). acm.

rocchio, j. j. (1971). relevance feedback in information retrieval. in g. salton
(ed.), the smart retrieval system: experiments in automatic document
processing (pp. 313   323). englewood cli   s, nj: prentice-hall.

romano, n. c., roussinov, d., nunamaker, j. f., & chen, h. (1999). collab-
orative information retrieval environment: integration of information re-
trieval with group support systems. in hicss    99: proceedings of the thirty-
second annual hawaii international conference on system sciences-volume 1
(pp. 1,053). ieee computer society.

sahami, m., & heilman, t. d. (2006). a web-based id81 for measur-

506

references

ing the similarity of short text snippets. in www    06: proceedings of the
15th international conference on world wide web (pp. 377   386). acm.
salton, g. (1968). automatic information organization and retrieval. new york:

mcgraw-hill.

salton, g., & buckley, c. (1988). term-weighting approaches in automatic text

retrieval. information processing and management, 24(5), 513   523.

salton, g., & mcgill, m. j. (1983). introduction to modern information retrieval.

new york: mcgraw-hill.

salton, g., wong, a., & yang, c. s. (1975). a vector space model for automatic

indexing. communications of the acm, 18(11), 613   620.

sanderson, m., & zobel, j.

(2005).

information retrieval system evaluation:
e   ort, sensitivity, and reliability. in sigir    05: proceedings of the 28th an-
nual international acm sigir conference on research and development in
information retrieval (pp. 162   169). acm.

saracevic, t. (1975). relevance: a review of and a framework for the thinking
on the notion in information science. journal of the american society for
information science, 26(6), 321   343.

saraiva, p. c., de moura, e. s., ziviani, n., meira, w., fonseca, r., & riberio-
neto, b. (2001). rank-preserving two-level caching for scalable search
engines. in sigir    01: proceedings of the 24th annual international acm
sigir conference on research and development in information retrieval (pp.
51   58). new york: acm.

schapire, r. e., singer, y., & singhal, a.

(1998). boosting and rocchio ap-
plied to text filtering. in sigir    98: proceedings of the 21st annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 215   223). acm.

shannon, c. (1951). prediction and id178 in printed english. bell system

technical journal, 30, 50   64.

shannon, c., & weaver, w. (1963). a mathematical theory of communication.

champaign, il: university of illinois press.

shneiderman, b., byrd, d., & croft, w. b. (1998). sorting out searching: a user-
interface framework for text searches. communications of the acm, 41(4),
95   98.

si, l., & callan, j. (2003). a semi-supervised learning method to merge search
engine results. acmtransactionsoninformationsystems,21(4), 457   491.
si, l., & callan, j. (2004). unified utility maximization framework for resource
selection. in cikm    04: proceedings of the eleventh international conference

references

507

on information and knowledge management. acm.

singhal, a., & pereira, f. (1999). document expansion for speech retrieval. in
sigir    99: proceedings of the 22nd annual international acm sigir con-
ference on research and development in information retrieval (pp. 34   41).
acm.

smucker, m., allan, j., & carterette, b. (2007). a comparison of statistical sig-
nificance tests for information retrieval evaluation. in cikm    07: proceed-
ings of the 14th acm international conference on information and knowl-
edge management. acm.

song, f., & croft, w. b.

(1999). a general language model for information
retrieval. in cikm    99: proceedings of the eighth international conference on
information and knowledge management (pp. 316   321). acm.

song, r., liu, h., wen, j.-r., & ma, w.-y. (2004). learning block importance
models for web pages. in www    04: proceedings of the 13th international
conference on world wide web (pp. 203   211). acm.

sparck jones, k., walker, s., & robertson, s. e. (2000). a probabilistic model
of information retrieval: development and comparative experiments. in-
formation processing and management, 36(6), 779   808.

strohman, t. (2007). e   cient processing of complex features for information re-
trieval (unpublished doctoral dissertation). university of massachusetts
amherst.

strohman, t., & croft, w. b. (2006). low latency index maintenance in indri. in
osir 2006: proceedings of the second international workshop on open source
information retrieval (pp. 7   11).

strohman, t., metzler, d., turtle, h., & croft, w. b. (2005). indri: a language
model-based search engine for complex queries. in proceedings of the inter-
national conference on intelligence analysis.

sun, j.-t., shen, d., zeng, h.-j., yang, q., lu, y., & chen, z. (2005). web-
page summarization using clickthrough data. in sigir    05: proceedings
of the 28th annual international acm sigir conference on research and
development in information retrieval (pp. 194   201). acm.

sutton, c., & mccallum, a. (2007). an introduction to conditional random
fields for relational learning. in l. getoor & b. taskar (eds.), introduction
to statistical relational learning. cambridge, ma, usa: mit press.

taghva, k., borsack, j., & condit, a. (1996). evaluation of model-based retrieval
e   ectiveness with ocr text. acm transactions on information systems,
14(1), 64   93.

508

references

trotman, a., & lalmas, m.

(2006). why structural hints in queries do not
help xml-retrieval. in sigir    06: proceedings of the 29th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 711   712). acm.

trotman, a., & sigurbj  rnsson, b. (2004). narrowed extended xpath i (nexi).

in inex workshop proceedings (pp. 16   40). springer.

turpin, a., tsegay, y., & hawking, d. (2007). fast generation of result snip-
pets in web search. in sigir    07: proceedings of the 30th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 127   134). acm.

turtle, h. (1994). natural language vs. boolean query evaluation: a compari-
son of retrieval performance. in sigir    94: proceedings of the 17th annual
international acm sigir conference on research and development in infor-
mation retrieval (pp. 212   220). springer-verlag.

turtle, h., & croft, w. b. (1991). evaluation of an id136 network-based re-
trieval model. acm transactions on information systems, 9(3), 187   222.
turtle, h., & flood, j. (1995). query evaluation: strategies and optimizations.

information processing and management, 31(6), 831   850.

unicode consortium.

(2006). the unicode standard, version 5.0. addison-

van rijsbergen, c. j. (1979). information retrieval (2nd ed.). london: butter-

wesley professional.

worths.

vasconcelos, n. (2007). from pixels to semantic spaces: advances in content-

based id162. computer, 40(7), 20   26.

voorhees, e. m. (1985). the cluster hypothesis revisited. in sigir    85: proceed-
ings of the 8th annual international acm sigir conference on research and
development in information retrieval (pp. 188   196). acm.

voorhees, e. m., & buckley, c. (2002). the e   ect of topic set size on retrieval
experiment error. in sigir    02: proceedings of the 25th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 316   323). acm.

voorhees, e. m., & harman, d. (eds.). (2005). trec: experiment and evalua-

tion in information retrieval. cambridge, ma: mit press.

wang, a. (2006). the shazam music recognition service. communications of the

acm, 49(8), 44   48.

wei, x., & croft, w. b. (2006). lda-based document models for ad-hoc re-
trieval. in sigir    06: proceedings of the 29th annual international acm

references

509

sigir conference on research and development in information retrieval (pp.
178   185). acm.
wei, x., & croft, w. b.

investigating retrieval performance with
manually-built topic models. in riao    07: proceedings of the eighth riao
conference.

welch, t. a. (1984). a technique for high-performance data compression. com-

(2007).

puter, 17, 8   19.

witten, i. h., mo   at, a., & bell, t. c. (1999). managing gigabytes: compressing
and indexing documents and images (2nd ed.). san francisco, ca, usa:
morgan kaufmann.

xia, f., liu, t., wang, j., zhang, w., & li, h. (2008). listwise approach to learn-
ing to rank     theory and algorithm. in icml    08: proceedings of the 25th
annual international conference on machine learning (pp. 1,192   1,199).
omnipress.

xu, j., & croft, w. b. (1998). corpus-based id30 using cooccurrence of
word variants. acm transactions on information systems, 16(1), 61   81.
xu, j., & croft, w. b. (2000). improving the e   ectiveness of information re-
trieval with local context analysis. acm transactions on information sys-
tems, 18(1), 79   112.

xu, z., fu, y., mao, j., & su, d. (2006). towards the semantic web: collaborative
tag suggestions. in www2006: proceedings of the collaborative web tagging
workshop. edinburgh, scotland.

xue, x., jeon, j., & croft, w. b. (2008). retrieval models for question and answer
archives. in sigir    08: proceedings of the 31st annual international acm
sigir conference on research and development in information retrieval (pp.
475   482). acm.

yang, s., zhu, h., apostoli, a., & cao, p. (2007). id165 statistics in english and
chinese: similarities and di   erences. in icsc    07: international conference
on semantic computing (pp. 454   460). ieee computer society.

yao, y.

(1995). measuring retrieval e   ectiveness based on user preference of
documents. journal of the american society for information science, 46(2),
133   145.

yih, w., goodman, j., & carvalho, v. r. (2006). finding advertising keywords
on web pages. in www    06: proceedings of the 15th international confer-
ence on world wide web (pp. 213   222). acm.

yu, s., cai, d., wen, j.-r., & ma, w.-y. (2003). improving pseudo-relevance
feedback in web information retrieval using web page segmentation. in

510

references

www    03: proceedings of the 12th international conference on world wide
web (pp. 11   18). acm.

zamir, o., & etzioni, o. (1999). grouper: a dynamic id91 interface to web

search results. computer networks, 31(11   16), 1,361   1,374.

zeng, h.-j., he, q.-c., chen, z., ma, w.-y., & ma, j. (2004). learning to cluster
web search results. in sigir    04: proceedings of the 27th annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 210   217). acm.

zhai, c., & la   erty, j.

(2004). a study of smoothing methods for language
models applied to information retrieval. acm transactions on information
systems, 22(2), 179   214.

zhang, v., rey, b., stipp, e., & jones, r.

(2006). geomodification in query
rewriting. in gir    06: proceedings of the workshop on geographic information
retrieval, acm sigir 2006.

zhang, y., & callan, j.

(2001). id113 for filter-
in sigir    01: proceedings of the 24th annual interna-
ing thresholds.
tional acm sigir conference on research and development in information
retrieval (pp. 294   302). acm.

zhou, y., xie, x., wang, c., gong, y., & ma, w.-y. (2005). hybrid index struc-
tures for location-based web search. in cikm    05: proceedings of the 14th
acm international conference on information and knowledge management
(pp. 155   162). acm.

zobel, j.

(1998). how reliable are the results of large-scale information re-
trieval experiments? in sigir    98: proceedings of the 21st annual interna-
tional acm sigir conference on research and development in information
retrieval (pp. 307   314). acm.

inverted files for text search engines. acm

zobel, j., & mo   at, a.

(2006).

computing surveys, 38(2), 6.

zobel, j., mo   at, a., & ramamohanarao, k. (1996). guidelines for presentation
and comparison of indexing techniques. acm sigmod record, 25(3),
10   15.

zobel, j., mo   at, a., & ramamohanarao, k. (1998). inverted files versus signa-
ture files for text indexing. acm transactions on database systems, 23(4),
453   490.

zukowski, m., h  man, s., nes, n., & boncz, p. a. (2006). super-scalar ram-
cpu cache compression. in icde: international conference on data engi-
neering (p. 59). ieee computer society.

index

absolute error, 437
accuracy, 359
ad hoc search, 3, 280, 423
adaptive filtering, 425
adversarial information retrieval, 294
advertising, 218, 371

classifying, 371
contextual, 218   221

agglomerative id91, 375
anchor text, 21, 56, 105, 280
api, 439, 461
architecture, 13   28
authority, 21, 111
automatic indexing, 400

background id203, see collection

id203

bag of words, 345, 451
bayes classifier, 245
bayes decision rule, 245
bayes    rule, 246, 343
bayes    rule, 342
id110, 268
bibliometrics, 120
bidding, 218
bigram, 100, 253
bigtable, 57

binary independence model, 246
blog, 111
bm25, 250   252
bm25f, 294
boolean query, 235
boolean query language, 24
boolean retrieval, 235   237
boosting, 448
bpref, 322
brute force, 331
burstiness, 254

caching, 26, 181
card catalog, 400
case folding, 87
case id172, 87
categorization, see classification
cbir, see content-based id162
character encoding, 50, 119
checksum, 60
chi-squared measure, 202
cjk (chinese-japanese-korean), 50, 119
classification, 3, 339   373

faceted, 224
monothetic, 223, 374
polythetic, 223, 374

classifier, 21

512

index

clickthrough, 6, 27, 207, 285, 306
clir, see cross-language information

retrieval

cluster hypothesis, 389
cluster-based retrieval, 391
id91, 22, 222   225, 339, 373
co-occurrence, 74, 191
code page, 50
id185, 432
collaborative search, 420
collection, 3
collection language model, 256
collection id203, 256, 440
collocation, 74
color histogram, 473
combining evidence, 267   283
combining searches, 441
combmnz, 441
community-based id53,

415

complete-link clusters, 379
compression, 54
lossless, 141
lossy, 142

conditional random field, 122
conflation, see id30
connected component, 192
content match, 371
content-based id162, 473
context, 115, 201, 211   214
context vector, 206, 464
contingency table, 248
controlled vocabulary, 199, 401
conversion, 49
coordination level match, 257
corpus, 6
cosine correlation, 239
coverage, 8
cqa, 415

crawler, 17, 32
cross-language information retrieval, 226
cross-lingual search, see cross-language

information retrieval

cross-validation, 331

damerau-levenshtein distance, 194
dangling link, 107
data mining, 113
database system, 459
dcg, see discounted cumulative gain
deep web, 41, 448
delta encoding, 144
dendrogram, 375
desktop search, 3, 46
dice   s coe   cient, 192
digital reference, 447
dirichlet smoothing, 258
discounted cumulative gain, 319
discriminative model, 284, 360
distance measure, 374
distributed hash table, 445
distributed information retrieval, 438
distribution, 23
divisive id91, 375
document, 2
document conversion, 18
document crawler, 17
document data store, 19
document distribution, 180
document slope curve, 64
document statistics, 22
document structure, 101, 269, 459   466
document summary, 215
downcasing, 87
dumping, 366
duplicate documents, 60
dwell time, 27
dynamic page, 42

id153, 194
e   ectiveness, 233, 297
e   ectiveness measure, 308   322
e   ciency, 297, 322   325
elements, 20
enterprise search, 3
entity recognizer, 115
entity search, 464
id178, 362
error model, 197
estimation, 256
evaluation, 5, 15, 297   335
evaluation measures, 27
exact-match retrieval, 235
expert search, 465
extent, 137
extraction, 21

f measure, 310
facet, 224
fallout, 309
faq, 447
feature, 14
feature selection, 362
feature-based retrieval model, 452
federated search, see distributed informa-

tion retrieval

feed, 17, 47
field, 91, 136
fields, see document structure
file-sharing, 442
filtering, 3, 210
fingerprint, 61   63
focused crawling, 17, 41
folksonomy, 401
forum, 448
freshness, 8, 39
frontier, 35

generative model, 115, 284, 359

index

513

geographic information system, 213
geometric mean average precision, 313
gnutella, 444
google, 28, 100, 106
graphical model, 455

hac (hierarchical agglomerative

id91), 376
hapax legomena, 76
heaps    law, 81
hidden markov model, 115, 293
hidden web, 41
hierarchical id91, 375   382
hierarchical p2p, 444
high-dimensional space, 239
highlighting, 25, 215
hits, 111, 410
id48, see hidden markov model
homophone, 195
html, 18, 101
http, 33
hub, 111

idf, see inverse document frequency
image annotation, 476
image search, 470
index, 8
index term, 14, 73
index vocabulary, 15
indexing, 14
indexing speed, 8, 324
indri, 8, 273, 294
inex, 462
id136 network retrieval model,

267   273

information extraction, 21, 52, 113, 464
information gain, 362
information need, 6, 187
information retrieval, 1
inlink, 106

514

index

inquery, 28, 273, 294
interaction, 15, 231, 336, 480
intermediary, see search intermediary
interpolation, 315
inverse document frequency, 22, 241
inverted file, see inverted index
inverted index, 15, 125
inverted list, 130

document-ordered, 130
score-sorted, 140

jelinek-mercer smoothing, 257

k nearest neighbor id91, 384   386
id116, 223
id116 id91, 382   387
kendall   s tau, 321
id81, 356, 465
key frame, 475
keyword, 24, 189, 220, 466
kl-divergence, see kullback-leibler

divergence

known-item search, 280
id181, 261, 440

language model, 196, 252, 440
latency, 323
id44, 288
id45, 288
lda, see id44
learning to rank, 284   288
lemur, 7, 91, 120
levenshtein distance, 194
lexical analysis, 86
link analysis, 21, 105   111, 410
link extraction, 104   113
link spam, 111, 365
links, 21
local search, 212
log data, see query log

id28, 294
long-tail query, 218
low-density language, 120
lsi, see id45
lucene, 7, 28

machine learning, 208, 283   291
machine translation, 227, 417
macroaverage, 315
manual indexing, 199, 400
manual tagging, 401
map, see mean average precision
markov random field model, 454   458
markup language, 19
markup tags, 20
maximal marginal relevance, 405
maximum likelihood estimate, 255
mean average precision, 313
mean reciprocal rank, 319
mean squared error, 438
memory hierarchy, 140
mesh, 200
metadata, 14, 86, 217, 280
metasearch, 439, 441
metasearch engine, 438
microaverage, 315
midi, 477
mmr, see maximal marginal relevance
morphology, 21, 121
mrf, see markov random field model
mrr, see mean reciprocal rank
multinomial distribution, 254
multinomial model, 348   351
multiple-bernoulli distribution, 270
multiple-bernoulli model, 346   348
music information retrieval, 477
mutual information, 201

id165, 61, 100   101, 253
na  ve bayes, 246, see bayes classifier

named entity, 21, 113, 468
id39, 113
napster, 443
natural language processing, 74
ndcg, see normalized discounted

cumulative gain

near-duplicate documents, 60
nearest neighbor classifier, 361, 372
nearest neighbor id91, see k nearest

neighbor id91

network overlay, 443
nexi, 462
id87, 196
normalized discounted cumulative gain,

321

noun phrase, 97
novelty, 405

ocr, 471
online community, 408
online learning, 430
online testing, 332
ontology, 213, 341
open source, 7
opinion detection, 369
optimization

safe, 26
unsafe, 26

overfitting, 287, 331

p2p networks, 442   446
p2p search, see peer-to-peer search
id95, 21, 106   111, 280
parallel corpus, 227, 419
parameter sweep, 331
parameter value, 330
parsing, 19
part-of-speech tagger, 97
parzen windows, 388
pay per click, 371

index

515

peer-to-peer search, 3, 23, 438, 442
personalization, 211
phrase, 97   101
phrase stitching, 366
plagiarism, 60
politeness policy, 35
politeness window, 35
pooling, 303
pos tagger, see part-of-speech tagger
posting, 130
potential function, 456
precision, 308
preferences, 306, 321
privacy, 212, 305
id203 ranking principle, 243
profile, 211, 423
proximity, 188, 269
pseudo-relevance feedback, 201, 208, 264,

458, 476
publishing, 47
pull, 47
push, 47
push application, 423

id183, 6, 24, 199   207, 264
query language, 23, 188, 273
query likelihood, 227, 254   261, 391, 440
query log, 24, 27, 193, 198, 206, 212, 284,

305

query processing, 125
query reformulation, 219
query suggestion, 6, 24
query throughput, 8, 323
query-based sampling, 440
query-specific id91, 392
id53, 3, 415, 419, 466   470
question classification, 468

r-precision, 337
rabin fingerprint, 62

516

index

random variables, 343
randomization test, 328
rank equivalence, 201
ranking, 15
ranking algorithm, 5, 25
ranking id166, 285
ranknet, 294
readability, 217
recall, 308
recall-precision graph, 312
recency, 8
recommender systems, 432
relational database, 53
relevance, 4, 233, 302

binary, 234
multivalued, 234
topical, 5, 234
user, 5, 234

relevance feedback, 6, 25, 208   211, 242,

248, 429

relevance judgment, 6, 300
relevance model, 261   266, 476
replication, 23
response time, 8
retrieval model, 5, 233   292
roc curve, 309
rocchio algorithm, 242
rss, 18, 47
run-on error, 196

scalability, 8
scoring, 25
scripted page, 42
search

home-page, 280
informational, 279
named-page, 280
navigational, 279
topical, 279
transactional, 279

search engine, 6

monolingual, 118

search engine optimization, 280, 299
search engineer, 10
search intermediary, 188
seed, 35
semantic annotation, 114
semantic web, 401
sentiment, 369
seo, see search engine optimization
session, 219
shape, 474
sign test, 327
signature file, 129
significance factor, 216
significance test, 325   330
simhash, 62
similarity, 239
similarity measure, 374
simulation, 27
single-link clusters, 378
site search, 17
sitemap, 43
size estimate

collection, 86
result set, 83

small-world property, 450
smoothing, 256
snippet, 16, 52, 215
snippet generation, 215
social media site, 397
soundex, 195
spam, 8, 22, 111, 364   368
id103, 471
spell checking, 24, 193   199
spoken documents, 471
sponsored search, 371
sql, 57
static filtering, 424

static page, 42
stem class, 191
stemmer, 20

algorithmic, 92
dictionary-based, 92
krovetz, 94
porter, 92
query-based, 190
su   x-s, 92

id30, 20, 91   96, 190
stopping, 20, 90
stopword list, 20, 90
stopwords, 90
structured query, 179
summarization, 215
superpeer, 444
supervised learning, 340
support vector machine, 285, 351   358
id166, see support vector machine
synonym, 199
syntactic analysis, 87

t-test, 327
tag, 400   408
tag cloud, 407
taxonomy, 213
tdt, see topic detection and tracking
term association, 74, 192, 201
term dependence, 281, 454
term distribution, 181
term frequency, 22, 241, 260
term spam, 365
test collection, 6, 299
test set, 331, 340
text corpus, 299
text encoding, 18, see character encoding
texture, 474
tf, see term frequency
tf.idf, 22, 241
thesaurus, 192, 199

index

517

tokenizing, 19, 87   90
tokens, 19
topic, 253
topic detection and tracking, 448
topic model, 253, 288   291
training data, 208, 284
training set, 331, 340
translation-based model, 417
id68, 228
trec, 6
trigram, 100, 253

uima, 13
unicode, 51, 119
uniform resource locator (url), 33
unigram, 100, 252
unsupervised learning, 340
update, 56
user model, 211
utf-8, 51, 148

v-byte, 148
vector space model, 237   243
vertical search, 3, 17, 41
video retrieval, 475
visualization, 215
viterbi algorithm, 117
vocabulary growth, 80
vocabulary mismatch, 5, 219, 288, 403
vocabulary size, 76

weaving, 366
web 2.0, 397
web community, 409
web search, 3, 279   283
weighting, 22, 241
wilcoxon signed-rank test, 327
wildcard operator, 188
word frequency, 75
word occurrence, 75

518

index

id138, 200, 468

xml, 18, 103, 461
xquery, 104, 461

zipf distribution, 80
zipf    s law, 75

