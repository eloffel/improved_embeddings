vector space models of lexical meaning

stephen clark

university of cambridge computer laboratory
stephen.clark@cl.cam.ac.uk

a draft chapter for the wiley-blackwell handbook of contemporary semantics    
second edition, edited by shalom lappin and chris fox. this draft formatted on
25th march 2014.

page: 1

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

2

stephen clark

1 introduction

much of this handbook is based on ideas from formal semantics, in which
the meanings of phrases or sentences are represented in terms of set-theoretic
models. the key intuition behind formal semantics, very roughly, is that the
world is full of objects; objects have properties; and relations hold between ob-
jects. set-theoretic models are ideal for capturing this intuition, and have been
succcessful at providing formal descriptions of key elements of natural lan-
guage semantics, for example quanti   cation.1 this approach has also proven
attractive for computational semantics     the discipline concerned with repres-
enting, and reasoning with, the meanings of natural language utterances using
a computer. one reason is that the formalisms used in the set-theoretic ap-
proaches, e.g.    rst-order predicate calculus, have well-de   ned id136 mech-
anisms which can be implemented on a computer (blackburn & bos, 2005).
the approach to natural language semantics taken in this chapter will be
rather di   erent, and will use a di   erent branch of mathematics from the set
theory employed in most studies in formal semantics, namely the mathem-
atical framework of vector spaces and id202. the attraction of using
vector spaces is that they provide a natural mechanism for talking about
distance and similarity, concepts from geometry. why should a geometric ap-
proach to modelling natural language semantics be appropriate? there are
many aspects of semantics, particularly lexical semantics, which require a no-
tion of distance. for example, the meaning of the word cat is closer to the
meaning of the word dog than the meaning of the word car. the modelling of
such distances is now commonplace in computational linguistics, since many
examples of language technology bene   t from knowing how word meanings
are related geometrically; for example, a search engine could expand the range
of web pages being returned for a set of query terms by considering additional
terms which are close in meaning to those in the query.
the meanings of words have largely been neglected in formal semantics,
typically being represented as atomic entities such as dog(cid:48), whose interpreta-
tion is to denote some object (or set of objects) in a set-theoretic model. in
this framework semantic relations among lexical items are encoded in meaning
postulates, which are constraints on possible models. in this chapter the mean-
ings of words will be represented using vectors, as part of a high-dimensional
   semantic space   . the    ne-grained structure of this space is provided by con-
sidering the contexts in which words occur in large corpora of text. words can
easily be compared for similarity in the vector space, using any of the stand-
ard similarity or distance measures available from id202, for example
the cosine of the angle between two vectors.

the hypothesis underlying distributional models of word meanings is the
so-called distributional hypothesis: the idea that    words that occur in similar
contexts tend to have similar meanings    (turney & pantel, 2010). the next

1 see westerstahl   s chapter on generalized quanti   ers in part i of this handbook.

page: 2

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

3

section discusses how this hypothesis has its roots in theoretical linguistics.
turney & pantel (2010) o   er variations of this hypothesis which can be applied
to linguistic units other than words, for example the    bag of words hypothesis   
which can provide an indication of the meaning of documents and queries for
the document retrieval problem (described in section 2).

1.1 distributional methods in linguistics

in vector space models of word meaning, the set of contexts2 in which a
word occurs     or the distribution of the word   s contexts     is considered
key to deriving a suitable meaning representation; hence the term distribu-
tional semantics is often used to describe such models. of course distributional
techniques have a long history in theoretical linguistics. harris (1954), in the
tradition of the structural linguists, proposed that linguistic units, such as
parts of speech, could be identi   ed from corpora by observing the contexts
in which the units occur. perhaps the historical work most closely related to
modern id65 is that of firth (1957), who was interested
in the notion of collocation and how the distributional contexts of a word
could be used to explain its behaviour. firth was also interested in di   erent
word senses, arguing that the di   erent senses of an ambiguous word could be
revealed by looking at the di   erent contexts in which the word occurs (pul-
man, 2012). this idea was exploited by sch  utze (1998) in his seminal paper
on using distributional models for id51 (described in
section 3.4). finally, one classic piece of philosophical work that is often men-
tioned in the context of id65 is wittgenstein (1953). the
link here is somewhat tenuous, since wittgenstein was not concerned with
the contexts of words in corpora, but rather the conventional, social nature
of a whole language. however, wittgenstein   s slogan that    meaning is use    is
certainly applicable, under some interpretation, to id65.

pulman (2012) gives a more detailed account of how the historical use
of distributional contexts in linguistics relates to modern distributional se-
mantics, particularly in relation to the possibility of compositional distribu-
tional models (the question considered in section 4.3 of this chapter).

a related discipline in which distributional semantic models have been
studied is cognitive science. such models have been successful at simulating
a variety of semantic processing tasks, for example semantic priming (lund
& burgess, 1996), episodic memory (gri   ths et al., 2007) and text compre-
hension (landauer & dumais, 1997). in this chapter little attention will be
paid to cognitive modelling; however, one interesting link with section 4.3
is the perceived failure of distributional models in general, and connection-
ist models in particular, to provide a suitable account of compositionality in
language (fodor & pylyshyn, 1988). some further links with the cognitive
science literature, particularly smolensky (1990), will be made there.

2 the de   nition of    context    varies according to the particular technique being

used; section 3.1 considers some alternatives.

page: 3

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

4

stephen clark

1.2 outline

the chapter will begin by describing how vector space models have been used
for document retrieval     the task performed by internet search engines. these
document-based models were not intially developed for explicitly modelling
word meanings; rather they were developed to represent the meaning, or topic,
of a whole document. however, document retrieval is a natural place to start
since this application provided many of the key ideas in distributional se-
mantics, and there is a natural progression from modelling documents using
vector spaces to modelling the meanings of words.

section 3 describes the main techniques used in building distributional
models of word meanings, and considers some of the model parameters: the
de   nition of context; the similarity measure used to compute the closeness
of word vectors; and the weighting scheme used to re   ect the fact that some
contexts are more indicative of a word   s meaning than others. this section
will also consider the question of what lexical relations are being acquired
using distributional techniques.

finally, section 4.3 considers a problem which is rapidly gaining interest in
the computational linguistics community: how to enrich vector space models
of lexical semantics with some notion of compositionality. this is an interesting
problem from a practical perspective, since the ability to determine semantic
similarity of phrases, or even whole sentences, would bene   t language tech-
nology applications; but it is also interesting from a theoretical perspective,
since a solution to the problem potentially o   ers a uni   cation of the rich lexical
representations from id65 with the logic-based account of
how meanings combine to give meanings of phrases and sentences.

the perspective from which this chapter is written is largely a compu-
tational linguistics one, which re   ects both the expertise of the author and
the fact that vector space models have received less attention in mainstream
theoretical linguistics. however, it should be clear that there are questions
and ideas in this chapter which will be of interest to linguists of all types:
theoretical, computational, and cognitive.

page: 4

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

5

2 vector space models for document retrieval

the document retrieval problem in information retrieval (ir; manning et al.
(2008)) is as follows: given a query     typically represented as a set of query
terms     return a ranked list of documents from some set, ordered by relev-
ance to the query. terms here can be words or lemmas, or multi-word units,
depending on the lexical pre-processing being used.3 the complete set of doc-
uments depends on the application; in the internet-search case it could be the
whole web.

one of the features of most solutions to the document retrieval problem,
and indeed information retrieval problems in general, is the lack of sophistic-
ation of the linguistic modelling employed: both the query and the documents
are considered to be    bags of words   , i.e. multi-sets in which the frequency
of words is accounted for, but the order of words is not. from a linguistic
perspective, this is a crude assumption (to say the least), since much of the
meaning of a document is mediated by the order of the words, and the syn-
tactic structures of the sentences. however, this simplifying assumption has
worked surprisingly well, and attempts to exploit linguistic structure beyond
the word level have not usually improved performance. for the document re-
trieval problem perhaps this is not too surprising, since queries, particularly
on the web, tend to be short (a few words), and so describing the problem
as one of simple word matching between query and document is arguably
appropriate.

once the task of document retrieval is described as one of word overlap
between query and document, then a vector space model is a natural approach:
the basis vectors of the space are words, and both queries and documents are
vectors in that space (salton et al., 1975). the coe   cient of a document vector
for a particular basis vector, in the simplest case, is just the number of times
that the word corresponding to the basis appears in the document. queries
are represented in the same way, essentially treating a query as a    pseudo-
      
document   . measuring word overlap, or the similarity of a document vector
d and query vector       q , can be achieved using the dot product:

      
d ,      q ) =

(cid:88)
      
d .      q
where vi is the ith coe   cient of vector       v .

sim(

=

i

di    qi

(1)

(2)

figure 1 gives a simple example containing two short documents. (in
practice, documents would typically contain many more sentences, and para-
graphs, than this.) in this example the user is interested in    nding documents
describing the england cricketer, matthew hoggard, taking wickets against

3 in this chapter term and word will be used interchangably, even though terms can

be lemmas or multi-word units.

page: 5

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

stephen clark

6
term vocabulary: (cid:104)england, australia, pietersen, hoggard, run, wicket,
catch, century, collapse(cid:105)

document d1: australia collapsed as hoggard took 6 wickets . flinto    praised
hoggard for his excellent line and length .

document d2: flinto    took the wicket of australia    s ponting , to give him 2
wickets for the innings and 5 wickets for the match .
query q: { hoggard, australia, wickets }
      
q1 .
      
q1 .

      
d1 = (cid:104)0, 1, 0, 1, 0, 1, 0, 0, 0(cid:105) . (cid:104)0, 1, 0, 2, 0, 1, 0, 0, 1(cid:105) = 4
      
d2 = (cid:104)0, 1, 0, 1, 0, 1, 0, 0, 0(cid:105) . (cid:104)0, 1, 0, 0, 0, 3, 0, 0, 0(cid:105) = 4

figure 1. simple example of document and query similarity using the dot product,
with term-frequency providing the vector coe   cients. the documents have been
tokenised, and word matching is performed between lemmas (so wickets matches
wicket).

australia, and so creates the query { hoggard, australia, wickets }. here the
query is simply the set of these three words. the vectors are formed by assum-
ing the basis vectors given in the term vocabulary list at the top of the    gure
      
(in that order); so the coe   cient for the basis vector hoggard, for example, for
the document vector
d1, is 2 (since hoggard occurs twice in the corresponding
document).

the point of this example is to demonstrate a weakness with using just
term-frequency as the vector coe   cients: all basis vectors count equally when
calculating similarity. in this example, document d2 matches the query as well
as d1, even though d2 does not mention hoggard at all. the solution to this
problem is to recognise that some words are more indicative of the meaning
of a document (or query) than others. an extreme case is the set of function
words: we would not want a query and document to be deemed similar simply
because both contain instances of the word    the   . another useful intuition
is that, if the task of document retrieval is to separate, or rank, the relevant
documents from the non-relevant ones, given some query, then any term which
appears in all documents will be useless in making this separation.

continuing with the example, let us assume that the document set being
searched contains documents describing cricket matches. since wicket is likely
to be contained in many such documents, let us assume that this term occurs
in 100 documents in total. hoggard is more speci   c in that it describes a
particular england cricketer, so suppose this term occurs in only 5 documents.
we would like to down-weight the basis vector for wicket, relative to hoggard,
since wicket is a less discriminating term than hoggard. an obvious way to

page: 6

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

      
d1 = (cid:104)0, 1, 0, 1, 0, 1, 0, 0, 0(cid:105) . (cid:104)0, 1/10, 0, 2/5, 0, 1/100, 0, 0, 1/3(cid:105) = 0.41
      
d2 = (cid:104)0, 1, 0, 1, 0, 1, 0, 0, 0(cid:105) . (cid:104)0, 1/10, 0, 0/5, 0, 3/100, 0, 0, 0/3(cid:105) = 0.13

      
q1 .
      
q1 .

7

figure 2. simple example of document and query similarity using the dot product,
with term-frequency, inverse-document frequency providing the coe   cients for the
documents, using the same query and documents as figure 1.

achieve this is to divide the term-frequency coe   cient by the corresponding
document frequency (the number of documents in which the term occurs), or
equivalently multiply the term frequency by the inverse document frequency
(idf) (sparck jones, 1972). figure 2 shows the simple example with idf
applied (assuming that the document frequency for australia is 10 and collapse
is 3). document d1 is now a better match for the query than d2.

a useful intuition for the e   ect of idf on the vector space is that it
e   ectively    shrinks    those basis vectors corresponding to words which appear
in many documents     the words with little discriminating power as far as
the document retrieval problem is concerned     and emphasises those basis
vectors corresponding to words which appear in few documents. figure 6 in
section 3.2 demonstrates this e   ect for word models.4

finally, there is one more standard extension to the basic model, needed
to counter the fact that the dot product will favour longer documents, since
these are likely to have larger word frequencies and hence a greater numerical
overlap with the query. the extension is to normalise the document vectors
(and the query) by length, which results in the cosine of the angle between
the two vectors:

      
d ,      q ) =

sim(

      
d .      q
(cid:107)      
d (cid:107)(cid:107)      q (cid:107)
(cid:112)(cid:80)
(cid:112)(cid:80)
      
d .      q
i d2
i q2
      
i
i
d ,      q )
= cosine(

=

(3)

(4)

(5)

where (cid:107)      v (cid:107) =(cid:112)(cid:80)

i is the euclidean length of vector       v .
i v2

these main ideas of the vector space model form the basis of modern
search engines     together with some additional machinery such as the use of
id95 to introduce a bias towards more    important    pages, as determined
by the hyperlink structure of the web (brin & page, 1998). approaches which
appear more sophisticated, such as bm25 (robertson & zaragoza, 2009), are

4 another solution to eliminate the e   ect of highly frequent words is to simply
ignore them, via the use of a so-called stop list. in practice ir systems often
employ both a stop list and idf weighting.

page: 7

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

8

stephen clark

                                          

d1
0

d2
0

1/10

1/10

0
2/5
0

0

0/5

0

1/100

3/100

0
0
1/3

0
0

0/3

                                          

england
australia
pietersen
hoggard
run
wicket
catch
century
collapse

figure 3. term-document matrix for the simple running example, using tf-idf
weights but without length normalisation.

essentially twists on the basic approach. even the recent language modelling
approach to document retrieval can be seen as implementing these ideas, but
in a probabilistic setting (zhai & la   erty, 2004).

using a weighting scheme based on the frequency of terms in a document,
and the inverse of the number of documents in which a term occurs, leads to
a term frequency-inverse document frequency (tf-idf ) model. in practice, the
weighting formula is usually based on some function of tf and idf ; for example
df is often implemented as a function of the log of the document frequency, in
order to introduce a damping e   ect when the document frequencies get very
high (manning et al., 2008). it is also the case that di   erent weighting schemes
can be applied to the query compared with the documents (note that idf was
not applied at all to the query vector in the simple example in figure 2).

the reason for describing the document retrieval model in this section is
that all the ideas from the basic approach     representing the linguistic units
of interest as vectors; using words as basis vectors; normalising vectors by
length; and favouring some basis vectors over others using idf     are carried
over, in some form, into distributional models of word meaning, described in
section 3. the next subsection considers the term-document matrix, a useful
bridge between the document models from ir and the word models from
distributional lexical semantics.

2.1 term-document matrix

one useful way to think about the document vectors is in terms of a term-
document matrix. this matrix is formed by treating each term or word vector
as a row in the matrix, and each document vector as a column. figure 3 shows
the term-document matrix for our simple running example.

the main reason for introducing the term-document matrix is that it
provides an alternative perspective on the co-occurrence data, which will lead
to vectors for terms themselves. but before considering this perspective, it

page: 8

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

9

is important to mention the potential application of dimensionality reduc-
tion techniques to the matrix, such as singular value decomposition (svd).
the use of svd, or alternative id84 techniques such as
non-negative matrix factorisation (van de cruys, 2010) or random indexing
(sahlgren, 2006), will not be covered in any detail here, since the chapter has
been written assuming only a rudimentary understanding of id202.
however, it is important to mention these techniques since they are an im-
portant part of the id202 toolbox which should be considered by any
researchers working in this area. chapter 18 of manning et al. (2008) provides
an excellent textbook treatment of matrix decompositions in the context of
information retrieval.

the application of svd to the term-document matrix was introduced by
deerwester et al. (1990), who called the method latent semantic analysis
(lsa), or id45 (lsi) in the context of ir. since then
there has been a huge literature on lsa. very brie   y, lsa factors the original
term document matrix into three matrices, and then uses those three matrices
to form a smaller, low-rank approximation to the original matrix. in practice,
the reduction in size is usually substantial; e.g. from a term-document matrix
with tens of thousands of documents and vocabulary terms, to a low-rank
approximation with only a few hundred basis vectors for each document.

turney & pantel (2010) provide three useful perspectives on lsa: one, it
can be seen as uncovering latent meaning, essentially id91 words along
a small number     typically a few hundred     of semantic, or topical, dimen-
sions, which are teased out of the co-occurrence data automatically. two, lsa
is performing noise reduction, using the id84 to uncover
the true signal generating the data, and    ltering out the noise which inevit-
ably arises from a statistical sample. and three, lsa can be seen as a method
for discovering higher-order co-occurrence, i.e. recognising words as similar
when they appear in similar contexts. similarity of contexts can be de   ned
recursively in terms of lower-order co-occurrence (turney & pantel, 2010).
this last perspective, in particular, is getting us closer to a view in which the
words themselves, rather than the documents, are the main focus. it is easy
to see how the term-document matrix provides an insight into the similarity
of documents (which we have exploited already): documents (columns) will
be similar if they contain many of the same terms (rows). but we can also
use the matrix as a way of obtaining term similarity: terms will be similar if
they occur in the many of the same documents. landauer & dumais (1997)
adopted this perspective and applied the resulting term similarities to the
multiple-choice synonym questions from the toefl (teaching english as a
foreign language) test, obtaining promising results. the next section will
generalise this idea of obtaining term similarity: terms will be similar if they
occur in many of the same contexts.

page: 9

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

10

stephen clark

3 representing word meanings as vectors

the term-document matrix introduced in the previous section gives us the
basic structure for determining word similarity. there the intuition was that
words or terms are similar if they tend to occur in the same documents.
however, this is a very broad notion of word similarity, producing what we
might call topical similarity, based on a coarse notion of context. the trick in
arriving at a more re   ned notion of similarity is to think of the term-document
matrix as a term-context matrix, where, in the ir case, context was thought
of as a whole document. but we can narrow the context down to a sentence,
or perhaps even a few words either side of the target word.5

once the context has been shortened in this way, then a di   erent per-
spective is needed on the notion of context. documents are large enough to
consider which words appear in the same documents, but once we reduce the
context to a sentence, or only a few words, then similar words will tend not to
appear in the same instance of a contextual window. another intuition that is
useful here is that (near-)synonymous words, such as boat and ship, will tend
not to occur in the same sentence (or even document), since a writer will tend
to use one of the alternatives, and not the other, in the same sentence.

the new perspective is to consider single words as contexts, and count
the number of times that a context word occurs in the context of the target
word. sahlgren (2006) stresses the distinction between the two interpretations
by describing one as using syntagmatic relations as context, and the other as
using paradigmatic relations. syntagmatically related words are ones that co-
occur in the same text region, whereas paradigmatically related words are
ones whose surrounding words are often the same (sahlgren, 2006).

figure 4 gives a simple example demonstrating the construction of a word-
word (or term-term) co-occurrence matrix, based on paradigmatic relations
with a single sentence as the context window. the example is somewhat con-
trived, but demonstrates the method of constructing a term-term matrix. note
that the target words     the words for which context vectors are calculated    
do not have to be part of the term vocabulary, which provide the context. de-
termining which words are similar can be performed using the cosine measure
(equation 5), as before.

in the example, football is similar in meaning to soccer since the context
vector for football (the row corresponding to football in the term-term matrix)
has a large numerical overlap with the context vector for soccer ; i.e. many of
the words surrounding instances of football     within a contextual window of
a sentence     are the same as the words surrounding instances of soccer. a
similar pattern is seen for automobile and car, but not for the other pairings
of target words.

once the move was made from syntagmatic to paradigmatic relations as
context, then researchers began to consider alternatives to simply taking a

5 target word refers to the word for which we are attempting to obtain similar

words.

page: 10

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

11

an automobile is a wheeled motor vehicle used for transporting passengers .
a car is a form of transport, usually with four wheels and the capacity to
carry around    ve passengers .
transport for the london games is limited , with spectators strongly advised
to avoid the use of cars .
the london 2012 soccer tournament began yesterday , with plenty of goals in
the opening matches .
giggs scored the    rst goal of the football tournament at wembley , north
london .
bellamy was largely a passenger in the football match , playing no part in
either goal .
term vocab: (cid:104)wheel, transport, passenger, tournament, london, goal, match(cid:105)

         

automobile
car
soccer
football

wheel

transport passenger

tournament

london goal match

1
1
0
0

1
2
0
0

1
1
0
1

0
0
1
1

0
1
1
1

0
0
1
2

0
0
1
1

         

automobile . car = 4
automobile . soccer = 0
automobile . football = 1
car . soccer = 1
car . football = 2
soccer . football = 5

figure 4. a small example corpus and term vocabulary with the corresponding
term-term matrix, with term-frequency as the vector coe   cients. each sentence
provides a contextual window, and the sentences are assumed to have been lemmat-
ised when creating the matrix.

   xed-word window containing the target word as context. one popular ex-
tension (grefenstette, 1994; curran, 2004; pado & lapata, 2007) is to take
the words which are related syntactically to the target word, perhaps using
the type of the relation as part of the de   nition of context. section 3.1 will
consider this alternative in more detail.

the term-term matrix can have id84 techniques ap-
plied to it, such as svd, as was the case for the term-document matrix. some
researchers do apply svd, or some related technique, arguing that it produces
word vectors which are less sparse and less a   ected by statistical noise. how-
ever, it is unclear whether svd is bene   cial in the general case. one potential

page: 11

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

12

stephen clark

disadvantage of svd is that the induced hidden dimensions are di   cult to
interpret, whereas basis vectors de   ned in terms of syntactic relations, for
example, can be related to conceptual properties and given a psycholinguistic
interpretation (baroni et al., 2010). whether the potential for interpretation
is important again depends on the task or application at hand.

id65 is a corpus-based, experimental discipline, but
there has been little discussion so far of how the vector spaces are constructed
in practice, and how they are evaluated. section 3.3 will discuss some of these
issues, after alternatives for the some of the main paramaters of vector space
word models have been described.

3.1 context

the previous example in figure 4 uses what is often called a window method,
where the contextual words for a partiular instance are taken from a sequence
of words containing the target word. in the example, the window boundaries
are provided by each sentence. when the window is as large as a sentence    
or a paragraph or document     the relation that is extracted tends to be one
of topical similarity, for example relating gasoline and car. if the intention is
to extract a more    ne-grained relation, such as synonymy, then a smaller, and
more    ne-grained, notion of context is appropriate.6

curran (2004) investigates a range of context de   nitions for the task of
automatic thesaurus extraction, focusing on the synonymy relation. for the
window methods, he uses a relatively short window consisting of two words
either side of the target word. the simplest window approach is to use the
context words as basis vectors, and de   ne the vector coe   cients as (weighted)
frequencies of the number of times each context word occurs in the contextual
windows surrounding instances of the target word.

more    ne-grained de   ntions are possible, even for the window method.
one possibility is to pair a context word with a direction. now the vector
coe   cients are weighted counts of the number of times the context word ap-
pears to the left, or right, of the target word, respectively. a further possibility
is to take position into account, so that a basis vector corresponds to a context
word appearing a particular number of words to the left or right of the target
word. whether such modi   cations improve the quality of the extracted rela-
tions is not always clear, and depends on the lexical relations that one hopes
to extract. (the di   culty of evaluation will be discussed in section 3.3.)

the next step in re   ning the context de   nition is to introduce some lin-
guistic processing. one obvious extension to the window methods is to add
part-of-speech tags to the context words. a more sophisticated technique is to
only consider context words that are related syntactically to the target word,

6 of course gasoline and car are related semantically in a particular way     gasoline
is something that is put in a car as fuel     but window methods would be unable
to extract this relation in particular (excluding other semantic relations).

page: 12

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

13

and to use the syntactic relation as part of the de   nition of the basis vector.
figure 5 shows how these various re   nements pick out di   erent elements of
the target word   s linguistic environment. the pipe or bar notation (|) is simply
to create pairs, or tuples     for example pairing a word with its part-of-speech
tag. the term contextual element is used to refer to a basis vector term which
is present in the context of a particular instance of the target word.

the intuition for building the word vectors remains the same, but now the
basis vectors are more complex. for example, in the grammatical relations
case, counts are required for the number of times that goal, say, occurs as the
direct object of the verb scored ; and in an adjective modi   er relation with
   rst; and so on for all word-grammatical relation pairs chosen to constitute
the basis vectors. the idea is that these more informative linguistic relations
will be more indicative of the meaning of the target word.

the linguistic processing applied to the sentence in the example is standard
in the computational linguistics literature. the part-of-speech tags are from
the id32 tagset (marcus et al., 1993) and could be automatically
applied using any number of freely available part-of-speech taggers (brants,
2000; toutanova et al., 2003; curran & clark, 2003). the grammatical re-
lations     expressing syntactic head-dependency relations     are from the
briscoe & carroll (2006) scheme, and could be automatically applied using
e.g. the rasp (briscoe et al., 2006) or c&c (clark & curran, 2007) pars-
ers. another standard practice is to lemmatise the sentence (minnen et al.,
2001) so that the direct object of scored, for example, would be equated with
the direct object of score or scores (i.e. each of these three word-grammatical
relation pairs would correspond to the same basis vector).

for the window method, a 7-word window is chosen so that some inform-
ative words, such as scored and football, are within the context of the target
word goal in this example sentence. in practice the size of the window is a
parameter which must be set in some way (usually by trial and error, seeing
which value produces the best performance according to whatever evaluation
metric is being used to assess the output). note that, for the simplest window
method, the occurs twice in the context in the example; hence this particu-
lar instance of the target word goal would contribute a frequency of 2 to the
coe   cient of the basis vector corresponding to the.

the most detailed basis representation we have seen so far is based on
single grammatical relations. but one may wonder whether it is possible to
extend this further and use even more re   ned notions of context. for example,
in the sentence in figure 5, the target word goal is the direct object of scored,
but goal is also linked to giggs, since giggs is the subject of the same verb.
this extended notion of grammatical relation leads to the idea of dependency
paths, described by pado & lapata (2007). now the basis vectors correspond
to whole sequences of grammatical relations, relating the target word and
context word. which paths to choose is a parameter of the approach, with
the idea that some paths will be more informative than others; for example,
it is possible to reach the second instance of the in the example sentence from

page: 13

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

stephen clark

14
giggs|nnp scored|vbd the|dt    rst|jj goal|nn of |in the|dt football|nn
tournament|nn at|in wembley|nnp ,|, north|nnp london|nnp .|.

(ncmod goal    rst)
(det goal the)
(ncmod tournament football )
(det tournament the)
(ncmod london north)
(dobj at wembley)
(ncmod scored at)
(dobj of tournament)
(ncmod goal of )
(dobj scored goal )
(ncsubj scored giggs )

contextual elements for target word goal using a 7-word window method:
{scored, the,    rst, of, football}

contextual elements with parts-of-speech:
{scored|vbd, the|det,    rst|jj, of |in, football|nn}

contextual elements with direction (l for left, r for right):
{scored|l, the|l,    rst|l, of |r, the|r, football|r}

contextual elements with position (e.g. 1l is 1 word to the left):
{scored|3l, the|2l,    rst|1l, of |1r, the|2r, football|3r}

contextual elements as grammatical relations:
{   rst|ncmod, the|det, scored|dobj}

figure 5. example sentence with part-of-speech tags from the id32
tagset (marcus et al., 1993) and grammatical relations from the briscoe & carroll
(2006) scheme. contextual elements for the target word goal are shown for various
de   nitions of context.

goal (via of and football, assuming undirected dependency links). but this path
would most likely be ruled out as uninformative. (see pado & lapata (2007)
for how certain paths can be ruled out.)

one potential problem with this extended approach is data sparsity: since
the basis vectors are so detailed, the counts for many combinations of target
word and basis vector may be unreliable (or zero), even for very large corpora.
pado & lapata (2007) overcome this di   culty by ignoring the label provided
by the sequence; in the example sentence, giggs would provide a contextual
element for goal, but the basis vector would consist only of the word giggs,

page: 14

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

15

and not any of the grammatical relations. again, whether discarding the la-
bels improves performance is an empirical question, and provides yet another
parameter to the system.

pado and laptata did    nd that the syntactically-enriched models out-
performed the simple window method on a number of tasks from language
processing and cognitive science (simulating semantic priming, detection of
synonymy in the toefl tests, and ranking the senses of polysemous words
in context). curran also found that using grammatical relations as basis vec-
tors generally outperformed the window methods for the task of synonymy
extraction.

so far we have been assuming that all contextual elements are equally use-
ful as basis vectors. however, this is clearly not the case: the word the provides
very little information regarding the meaning of the word goal. one option is
to employ a stop list, as for the document retrieval case, and simply ignore
those contextual elements which are extremely frequent in a large corpus. a
more principled approach, again borrowing ideas from document retrieval, is
to weight the basis vectors. the next section describes some possible weighting
schemes.

3.2 weighting and similarity

one simple method for weighting a basis vector is to divide the corresponding
term frequency by the number of times that the term occurs in a large corpus,
or, to borrow an idea from document retrieval, the number of documents that
the term occurs in. figure 6 demonstrates the e   ect of using inverse document
frequency (idf) in this way for the extreme case of the as a basis vector. the
e   ect is a little hard to capture in two dimensions, but the idea is that, in
the vector space at the top of the    gure, the vectors for dog and cat will be
pointing much further out of the page     along the the basis     than in the
vector space at the bottom. a useful intuition for the e   ect of idf is that
it e   ectively    shrinks    those basis vectors corresponding to highly frequent
terms, reducing the impact of such bases on the position of the word vectors,
and thereby reducing the amount of overlap on those bases when calculating
word similarity.

one feature of idf is that the shrinking e   ect applies in the same way to all
target words (since idf for a basis vector is independent of any target word).
however, we may want to weight basis vectors di   erently for di   erent target
words. for example, the term wear may be highly indicative of the meaning
of jacket, but less indicative of the meaning of car. hence we would want to
emphasise the basis vector for wear when building the vector for jacket, but
emphasise other basis vectors     such as gasoline     when building the vector
for car.

curran (2004) uses collocation statistics to allow the weighting scheme
to have this dependence on the target word. for example, jacket and wear
will be highly correlated according to a collocation statistic because jacket

page: 15

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

16

stephen clark

figure 6. the e   ect of idf on a simple example vector space.

co-occurs frequently with wear (relative to other basis vector terms). here we
are e   ectively using sahlgren   s syntagmatic relations to provide a weighting
scheme to better capture paradigmatic relations. there are many collocation
statistics which could be used here (manning & schutze (1999) provide a text-
book treatment of some of the alternatives), and each statistic has parameters
which must be set empirically (e.g. the size of the collocation window). curran
(2004) investigates a range of alternatives for the task of synonymy extraction,
   nding that some do perform better than others for his particular evaluation.
rather than try to summarise the results here, the reader is referred to curran
(2004) for the details, with the summary conclusion that using some form of
weighting typically improves performance over simply using term frequencies.
we have described techniques for building word vectors; now we need a
method of comparing them, in order to determine similarity of word meaning.
again we can borrow a key idea from document retrieval, namely the cosine
similarity measure:

      
w1,

sim(

      
w2) =

      
      
w2
w1.
(cid:107)      
w1(cid:107)(cid:107)      
w2(cid:107)

(6)

page: 16

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

eatthesleepdogcateatsleepdogcatthevector space models of lexical meaning

(cid:112)(cid:80)

=

      
w1.
i w12
      
i
w1,

(cid:112)(cid:80)

      
w2
      
w2)

= cosine(

i w22
i

      
w1 and

where

      
w2 are vectors for words w1 and w2.

17

(7)

(8)

note that the dot product in the numerator is calculating numerical over-
lap between the word vectors, and dividing by the respective lengths provides
a length normalisation which leads to the cosine of the angle between the vec-
tors. normalisation is important because we would not want two word vectors
to score highly for similarity simply because those words were frequent in the
corpus (leading to high term frequency counts as the vector coe   cients, and
hence high numerical overlap).

the cosine measure is commonly used in studies of distributional se-
mantics; there are alternatives, however. rather than go into the details here,
the reader is referred to lin (1998) and chapter 4 of curran   s thesis (curran,
2004). it is di   cult to reach a conclusion from the literature regarding which
similarity measure is best; again this appears to depend on the application
and which relations one hopes to extract.

3.3 experiments

a key question when extracting similarity relations in practice is which corpus
to use. the answer may depend on the application; for example, a query
expansion system for document retrieval may build word vectors using web
data or query logs; a system extracting synonymous terms for biomedical
entities would use a corpus of biomedical research papers. most research on
id65 has tended to use standard, freely available corpora
such as the british national corpus (bnc), which contains around 100 million
words from a range of genres.

the size of the corpus is an important factor in the quality of the extracted
relations, with the general message that more data is better data (a common
theme in statistical natural language processing). curran (2004) used a corpus
containing over 2 billion words, made up from the bnc, the reuters corpus
volume 1 (rose et al., 2002), and much of the english news corpora from
the linguistic data consortium (ldc). it is worth noting that, even with
the computing resources available today, 2 billion words is still large enough
to make computational concerns a major factor in any experimental work,
and the e   cient creation and comparison of word vectors built from 2 billion
words is a non-trivial computer science task. agirre et al. (2009) used a web
corpus containing roughly 1.6 terawords, many orders of magnitude larger than
curran   s 2 billion word corpus. however, agirre et al. had access to google   s
distributed computing infrastructure, meaning that experiments could be run
in a matter of minutes, even on corpora of this size. such infrastructure is not
generally available to academic researchers.

page: 17

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

18

stephen clark

introduction: launch, implementation, advent, addition, adoption, arrival,
absence, inclusion, creation, departure, availability, elimination, emergence,
use, acceptance, abolition, array, passage, completion, announcement, . . .

evaluation: assessment, examination, appraisal, review, audit, analysis,
consultation, monitoring, testing, veri   cation, counselling, screening, audits,
consideration,
inspection, measurement, supervision, certi   cation,
checkup, . . .

inquiry,

context: perspective, signi   cance, framework, implication, regard, aspect,
dimension, interpretation, meaning, nature, importance, consideration, focus,
beginning, scope, continuation, relevance, emphasis, backdrop, subject, . . .

similarity: resemblance, parallel, contrast,    aw, discrepancy, di   erence,
a   nity, aspect, correlation, variation, contradiction, distinction, divergence,
commonality, disparity, characteristic, shortcoming, signi   cance, clue, hall-
mark, . . .

method: technique, procedure, means, approach, strategy, tool, concept,
practice, formula, tactic, technology, mechanism, form, alternative, standard,
way, guideline, methodology, model, process, . . .

result: consequence, outcome, e   ect,    nding, evidence, response, possibility,
kind, impact, datum, reason, extent, report, example, series, aspect, account,
amount, degree, basis, . . .

conclusion:    nding, outcome, interpretation, assertion, assessment, explan-
ation, judgment, assumption, decision, recommendation, verdict, completion,
id136, suggestion, result, answer, view, comment, testimony, argument, . . .

figure 7. ranked lists of synonyms for the target words in bold from curran   s
system.

when discussing experimental id65, it is instructive to
consider some example output. figure 7 shows the top 20 extracted synonyms
for a number of example target words, taken from curran (2004). the lists
for each target word are ordered in terms of similarity, so launch, for example,
is the closest synonym to introduction according to curran   s system. the
example target words are somewhat abstract in nature, since these are the
titles of curran   s thesis chapters.

the output demonstrates features which are common to many language
resources automatically extracted from corpora. first, there are many auto-
matically derived synonyms which might not immediately occur to a human
creating such lists, such as advent for introduction. second, some of the ouput
is incorrect; if the task is to extract synonyms, then elimination, for example,

page: 18

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

19

is more of an antonym of introduction than a synonym. third, automatically
extracted resources will always contain anomalous content which is di   cult
to explain, such as array as a highly ranked synonym for introduction.

evaluation methods for such resources are typically of two types: intrinsic
and extrinsic. intrinsic evaluation involves comparing the resource directly
against a manually created gold standard, such as an existing thesaurus. the
problem with this approach is that automatic methods are designed to over-
come some of the limitations of manually created resources, such as lack of
coverage. so it may be that the automatic system correctly posits a synonym
for some target word which was not considered by the lexicographer creating
the gold standard.

curran (2004) used this intrinsic method, but pooled together the output
from a number of manually created thesauri, such as roget   s and macquarie,
in an attempt to o   set the coverage problem. a standard measure for this
evaluation is precision at rank, where the precision (accuracy) of the output
for each target word is measured at various points in the synonym ranking list
for each word. a point is scored for each synonym extracted by the automatic
system which is also in the gold standard thesaurus. an overall score is ob-
tained by averaging the scores for all the target words in the test set. curran
(2004) carried out such an evaluation for a test set of 300 nouns, selected to
contain a range of concrete and abstract nouns, and to cover a range of corpus
frequencies. curran reports a score of 68% precision at rank 1, meaning that
68% of the top-ranked synonyms for the 300 target nouns in the test set were
also in the manually created gold standard. precision was 55% at rank 5 (i.e.
55% of the synonyms in the top 5 across all target nouns were in the gold
standard), 45% at rank 10, and 35% at rank 20.

extrinsic evaluation involves applying the extracted resource to some task
or language processing application, and observing performance on the task.
psycholinguistic tasks have been used for this purpose. for example, pado &
lapata (2007) use semantic priming (lowe & mcdonald, 2000), modelling the
reading time for prime-target pairs using the semantic similarity between the
prime and target. the intention is that the similarity of a word and related
prime, e.g. pain and sensation, will be higher than a word and an unrelated
prime. furthermore, the distance between the prime and target should cor-
relate with the corresponding reading times from the original psycholingusitic
study (hodgson, 1991). pado and lapata found that their dependency-based
method for building word vector spaces (described in section 3.1) produced
higher correlations than the window-based method.

pado & lapata (2007) also use the toefl (teaching of english as a
foreign language) tests, following landauer & dumais (1997). here the task
is to determine, for a number of target words, the closest synonym from a
choice of four alternatives. pado and lapata give the following example:

you will    nd the o   ce at the main intersection.

(a) place (b) crossroads (c) roundabout (d) building

page: 19

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

20

stephen clark

where the task is to determine that crossroads is the closest synonym from the
alternatives to intersection, for this particular sentence. the method applied
was to simply pick the alternative closest to the target in the automatic-
ally constructed semantic space. pado and lapata again showed that their
dependency-based method performed better than the window-based method,
scoring 73% vs. 61% accuracy.

other extrinsic methods exist in the literature; for example pado & lapata
(2007) perform a third task of ranking word senses in context. which method
is most appropriate depends on the research goals: if the goal is to model how
humans process language, then using psycholinguistic data would be appro-
priate; if the goal is to improve language processing applications, then the
automatically extracted resource should be embedded in such applications; if
the goal is to simply build a more representative thesaurus, then the intrinsic
evaluation may be most appropriate.

3.4 discussion

one issue that has been glossed over so far is the question of which lexical
semantic relations are being acquired with distributional techniques. curran
(2004) explicitly describes his work as synonymy extraction, but it is clear
from the examples in figure 7 that curran   s system is extracting more than
just synonyms. not only are there examples of antonyms in the output, but
also hyponyms (e.g. curran   s system    nds subsidiary as closely related to    rm,
where a subsidiary is arguably a kind of    rm); and also examples where the
two words are semantically related in other ways.

which relations are appropriate again depends on the task or application
at hand. for example, if the task is id183 for a search engine, it
may not be unreasonable to consider lexical relations in addition to synonymy.
if a user query contains the term    rm, it may be helpful to expand the query
with additional terms referring to kinds of    rm, such as subsidiary (since if
a user wishes to retrieve documents about    rms, the user will most likely be
interested in documents about subsidiaries, also).

there is some work on attempting to identify antonyms, and also ana-
logies and associations, as separate from synonyms (turney, 2008). there is
also a huge literature on pattern-based techniques for identifying hyponyms,
following the original insight of hearst (1992) that hyponyms could be auto-
matically extracted from text by searching for patterns such as x such as y,
for example computers such as the mac and pc (allowing mac and pc to be
extracted as kinds of computers).7 the pattern-based approaches are some-
what orthogonal to the distributional methods described in this chapter, and
perhaps more grounded in a clever insight rather than any underlying theory.

7 the previous sentence itself is such an example, allowing x such as y to be

extracted as a kind of pattern.

page: 20

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

21

turney (2006) makes the useful distinction between attributional similarity
and relational similarity, arguing that there is some confusion in the distri-
butional semantics literature regarding terminology. attributional similarity
is de   ned as correspondence between attributes, and relational similarity as
correspondence between relations (turney cites medin et al. (1990) in this
context). synonyms are cases of words with a high degree of attributional
similarity. section 3 described methods for deriving attributional similarity.

relational similarity extends the ideas in this chapter to capture analogies,
such as mason has a similar relation to stone as carpenter to wood. another
distinction made in the literature is between words that are semantically as-
sociated, such as bee-honey, and words that are semantically similar, such as
deer-pony (turney, 2006). turney argues that these are both cases of attri-
butional similarity. turney also argues that the term semantic relatedness,
argued by budanitsky & hirst (2006) to capture a di   erent relation to se-
mantic similarity, is the same as attributional similarity. our summary con-
clusion here is that distributional models can capture a variety of semantic
relations, depending on how the various model parameters are de   ned, and it
is important to be clear what relations one hopes to extract with a particular
method.

another issue that has been glossed over is that of word sense. curran
(2004) con   ates the various senses of a word, so that the gold standard syn-
onyms for company, for example, against which the output of his automatic
system are compared, include synonyms for the companionship sense of com-
pany, the armed forces sense, the gathering sense, and so on. one piece of
work in id65 which explicitly models senses is sch  utze
(1998). sch  utze uses a two-stage id91 method in which the    rst stage
derives word vectors using the methods described in section 3 (speci   cally
using the window method with a window size of 50). then, for a particular
target word in context, each of the word vectors for the context words are
added together (and divided by the number of context words) to derive a
centroid for the particular context. (the centroid is just the vector average
for a set of vectors.) the e   ect of deriving this second-order context vector is
that the contextual words will act as word sense disambiguators, and when
added together will emphasise the basis vectors pertaining to the particular
sense. the derived vector is second-order in the sense that it is a combination
of a set of    rst-order vectors built in the standard way.

sch  utze gives the example of suit. consider an instance of suit in a clothing,
rather than legal, context, so that it is surrounded by words such as tie, jacket,
wear, and so on. of course these context words are potentially ambiguous as
well, but the e   ect of adding them together is to emphasise those basis vectors
which most of the context words have in common, which in this case is basis
vectors relating to clothes.

in practice the second-order context vectors for a particular sense are
obtained by id91 the context vectors for all instances of a target word
(where, in sch  utze   s case, the number of clusters or senses needs to be known

page: 21

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

22

stephen clark

in advance). then, given a particular instance of a target word, the word can
be sense disambiguated by calculating the 2nd-order centroid context vector
for that instance, and calculating which cluster the centroid is closest to.

despite the success of sch  utze   s method in performing word sense dis-
ambiguation, it is still the case that much work in id65
ignores word senses (pulman, 2012). to repeat what is becoming a common
theme in this chapter, whether distinguishing word senses is useful in distri-
butional semantics depends on the task or application at hand. there is also a
long ongoing debate about whether id51 is useful more
generally for language processing applications (e.g. chan et al. (2007)).

finally, there are a number of ways in which the basic distributional hypo-
thesis used in this chapter     that    words that occur in similar contexts tend
to have similar meanings    (turney & pantel, 2010)     has been extended. the
work by turney (2006) on automatically extracting analogies from text has
already been mentioned. in an earlier in   uential paper, lin & pantel (2001)
extract similar patterns, such as x wrote y and x is the author of y . the
extended distributional hypothesis used to extract the patterns is the idea
that    patterns that co-occur with similar pairs tend to have similar mean-
ings    (turney & pantel, 2010; lin & pantel, 2001). the two sets of instances
of x in x wrote y and x is the author of y, derived from a large corpus, will
have a large overlap (consisting of names of authors, for example); likewise
for y. the large overlap for both argument slots in the pattern can be used
to infer similarity of pattern.

the next section considers a nascent but rapidly growing research area in
which distributional techniques are being extended to capture similarity of
phrases, and even whole sentences.

page: 22

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

23

4 compositional vector space models

this section describes a recent development in computational linguistics,
in which distributional, vector-based models of meaning have been given a
compositional treatment allowing the creation of vectors for larger phrase,
and even sentence, meanings. the bulk of the section will be a presentation of
the theoretical framework of coecke et al. (2010), which has been implemented
in grefenstette et al. (2011) and grefenstette & sadrzadeh (2011), in a form
designed to be accessible to linguists not familiar with the mathematics of
category theory on which the framework is based. the reason for focusing
on the coecke et al. (2010) work is that it has interesting links with existing
compositional frameworks in linguistics, in particular montague semantics
(dowty et al., 1981).

one way of stating the problem is that we would like a procedure which,
given vectors for each word in a phrase or sentence, combines the vectors in
some way to produce a single vector representing the meaning of the whole
phrase or sentence. why might such a procedure be desirable? the    rst reason
is that considering the problem of compositionality in natural language from
a geometric viewpoint may provide an interesting new perspective on the
problem. traditionally, compositional methods in natural language semantics,
building on the foundational work of montague, have assumed the meanings of
words to be given, and e   ectively atomic, without any internal structure. once
we assume that the meanings of words are vectors, with signi   cant internal
structure, then the problem of how to compose them takes on a new light.

a second, more practical reason is that language processing applications
would bene   t from a framework in which the meanings of whole phrases and
sentences can be easily compared. for example, suppose that a sophisticated
search engine is issued the following query: find all car showrooms with sales
on for ford cars. suppose futher that a web page has the heading cheap
fords available at the car salesroom. knowing that the above two sentences
are similar in meaning would be of huge bene   t to the search engine. further,
if the two sentence meanings could be represented in the same vector space,
then comparing meanings for similarity is easy: simply use the cosine measure
between the sentence vectors, as was used for word vectors.

one counter-argument to the above example might be that composition-
ality is not required in this case, in order to determine sentence similarity,
only similarity at the word level. for this example that may be true, but it is
uncontroversial that sentence meaning is mediated by syntactic structure. to
take another search engine example, the query a man killed his dog, entered
into google on january 5, 2012, from the university of cambridge computer
laboratory, returned a top-ranked page with the snippet dog shoots man (as
opposed to man shoots dog), and the third-ranked page had the snippet the

page: 23

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

24

stephen clark

s1

6

man killed dog
  
 

man murdered cat

 

  

 

 
 
 
 
?
man killed by dog

-

s3

 

 

  	

s2

figure 8. example vector space for sentence meanings

man who killed his friend for eating his dog after it was killed . . ..8 of
course the order of words matters when it comes to sentence meaning.

figure 8 shows the intuition behind comparing sentence vectors. the
framework described in this section will provide a mechanism for creating
vectors for sentences, based on the vectors for the words, so that man killed
dog and man murdered cat will (ideally) be relatively close in the    sentence
space   , but crucially man killed by dog will be located in another part of the
space (since in the latter case it is the animal killing the man, rather than
vice versa). note that, in the sentence space in the    gure, no commitment
has been made regarding the basis vectors of the sentence space (s1, s2 and
s3 are not sentences, but unspeci   ed basis vectors). in fact, the question of
what the basis vectors of the sentence space should be is not answered by the
compositional framework, but is left to the model developer to answer. the
mathematical framework simply provides a compositional device for combin-
ing vectors, assuming the sentence space is given.

the rest of this section is structured as follows. section 4.1 considers some
general questions related to the idea of having a vector-based, distributional
representation for sentence meanings. section 4.2 brie   y describes some exist-
ing work on this topic. finally, section 4.3 describes the theoretical framework
of coecke et al. (2010).

4.1 distributional sentence representations

the intuition underlying vector-based representations for words is clear, and
exempli   ed in the distributional hypothesis from section 1:    words that oc-
cur in similar contexts tend to have similar meanings    (turney & pantel,
2010). however, it is not clear how to extend this hypothesis to larger phrases

8 thanks to mehrnoosh sadrzadeh for this example.

page: 24

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

25

or sentences. do we want to say that sentences have similar meanings when
they occur in similar contexts? baroni & zamparelli (2010) do carry over
the intuition to phrases, but only adjective noun pairs, for which the intuition
arguably still holds. clarke (2008) develops a theoretical framework underpin-
ning all distributional semantic models, and extends the intuition to linguistic
units of any size, including sentences.

whether the meaning of a sentence can be represented or determined
by its contexts is unclear, and it could be argued that such a proposal is
in opposition to the notion of compositionality, for which the meaning of a
sentence is determined by the meanings of its parts and how those parts are
combined (werning et al., 2012), not determined by the contexts in which the
sentence as a whole is found. in this chapter we will make no commitment
to the theoretical status of a sentence space, only the idea that comparing
sentences for meaning similarity, and representing sentence meanings in a
vector space, is a sensible enterprise.

a related question concerns what a vector-based sentence representation
is providing a semantics of . one answer is that it is providing a semantics
of similarity. however, whether traditional features of natural language se-
mantics, such as logical operators, quanti   cation, id136, and so on, can be
integrated into the vector-based setting, is an open question. there is some
preliminary work in this direction, e.g. (preller & sadrzadeh, 2009; clarke,
2008; widdows, 2004), and the question is creating substantial interest in the
computational linguistics community; see e.g. the 2013 workshop towards a
formal id65, attached to the 10th international confer-
ence on computational semantics.

4.2 existing vector composition methods

much of the work in information retrieval simply uses vector addition
whenever word vectors need to be combined to obtain a distributional repres-
entation for larger linguistic units such as sentences or documents (e.g. land-
auer & dumais (1997)). mitchell & lapata (2008) compare vector addition
with a number of other binary vector operations, such as pointwise multiplic-
ation and tensor product, on the task of disambiguating verb meanings in
the context of a noun subject. here both the verb and noun are represented
using context vectors, with the window method from section 3.1. pointwise
multiplication is found to perform best on this task, perhaps because the mul-
tiplication is having the e   ect of emphasising those contextual elements that
the verb and noun have in common, and thereby performing sense disambig-
uation, as pulman (2012) argues.

as mentioned in the introduction, the cognitive science literature already
contains a large body of work addressing a similar question to the main ques-
tion of this section: how can distributional, connectionist representations be
given a compositional treatment to re   ect the compositional nature of lan-
guage? one researcher who has addressed this question in detail is smolensky

page: 25

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

26

stephen clark

(1990), who argues for the tensor product operator as the way of binding pre-
dicate vectors to their arguments. the compositional framework in section 4.3
also uses a tensor product representation, but there are two main di   erences
with the earlier cognitive science work. first, the distributional representa-
tions used in cognitive science tend to follow the neural network model, in
which the basis vectors are e   ectively induced automatically from the data,
rather than speci   ed in advance in terms of linguistic contextual elements.
second, one innovative aspect of the compositional framework described be-
low is that tensors are used to represent relational word types, such as verbs
and adjectives, which are then    reduced    when combined with an argument.
in contrast, smolensky uses the tensor product operation as a way of binding
a predicate to its argument, so that the resulting representation is, in some
sense, bigger, rather than smaller, than each combining element.

the closest existing work to the framework described below is baroni &
zamparelli (2010), in that baroni & zamparelli use the syntactic type of
an adjective as motivation for a matrix representation in the distributional
semantics. adjective noun combinations are obtained using matrix multiplic-
ation of the noun vector by the adjective. another innovation in baroni &
zamparelli (2010), similar to an idea in guevara (2011), is to learn the mat-
rix automatically from data using supervised machine learning. training data
for the learning is provided by the contexts of adjective noun combinations
attested in corpora. so one way of viewing the learning is as a way of over-
coming sparse data: with enough data the context vectors for adjective noun
combinations could be simply obtained from corpora, with no compositional
operation required. whether this is in the spirit of id152,
as discussed brie   y in section 4.1, is open to debate. it is also unclear whether
contexts for linguistic units larger than adjective noun combinations could be
obtained in the same way.

this section has only described some of the growing body of work looking
at how to combine vector representations of word meanings. some additional
work includes widdows (2008); zanzotto et al. (2010); clarke (2012). the
next section describes a framework, which, in contrast to the work mentioned
above, provides a recipe for constructing phrase and sentence vectors in step
with a syntactic, type-driven process.

4.3 the compositional framework

a key idea underlying the vector-based compositional framework of coecke
et al. (2010) is that syntax drives the compositional process, in much the same
way that it does in montague semantics (dowty et al., 1981). another key
idea borrowed from formal semantics is that the syntactic and semantic de-
scriptions will be type-driven, re   ecting the fact that many word types in
natural language, such as verbs and adjectives, have a relation, or functional,
role. in fact, the syntactic formalism assumed here will be a variant of cat-

page: 26

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

27

egorial grammar (steedman, 2000; morrill, 2010), which is the grammatical
framework also used by montague.

the next section describes pregroup grammars, which provide the syn-
tactic formalism used in coecke et al. (2010). however, it should be noted
that the use of pregroup grammars is essentially a mathematical expedi-
ent, and that other type-driven formalisms, for example combinatory cat-
egorial grammar (steedman, 2000), can be accommodated in the compos-
itional framework (grefenstette, 2013; maillard et al., 2014). the grammar
formalism is described in some detail here because the link between type-
driven syntax and semantics is a key part of the compositional framework.

the following section shows how the use of syntactic functional types leads
naturally to the use of tensor products for the meanings of words such as verbs
and adjectives, and then an example sentence space is provided, giving some
intuition for how to compose a tensor product with one of its    arguments   
    e   ectively providing the analogue of function application in the semantic
vector space.

syntactic types and pregoup grammars

the key idea in any form of categorial grammar is that all grammatical
constituents correspond to a syntactic type, which identi   es a constituent as
either a function, from one type to another, or as an argument (steedman
& baldridge, 2011). id35 (id35) (steedman,
2000), following the original work of lambek (1958), uses slash operators to
indicate the directionality of arguments. for example, the syntactic type (or
category) for a transitive verb such as likes is as follows:

likes := (s\np )/np

the way to read this category is that likes is the sort of verb which    rst
requires an np argument to its right (note the outermost slash operator point-
ing to the right), resulting in a category which requires an np argument to its
left (note the innermost slash operator pointing to the left),    nally resulting in
a sentence (s ). categories with slashes are known as complex categories; those
without slashes, such as s and np , are known as basic, or atomic, categories.
a categorial grammar lexicon is a mapping from words onto sets of possible
syntactic categories for each word. in addition to the lexicon, there is a small
number of rules which combine the categories together. in classical categorial
grammar, there are only two rules, forward (>) and backward (<) application:

x /y y     x (>)
y x\y     x (<)

(9)

(10)

these rules are technically rule schemata, in which the x and y variables
can be replaced with any category. id35 adds
a number of additional rules, such as function composition and type-raising,

page: 27

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

28

stephen clark

which can increase the power of the grammar from context-free to mildly
context-sensitive (weir, 1988).

the mathematical move in pregroup grammars, a recent incarnation of
categorial grammar due to lambek (2008), is to replace the slash operators
with di   erent kinds of categories (adjoint categories), and to adopt a more
algebraic, rather than logical, perspective compared with the original work
(lambek, 1958). the category for a transitive verb now looks as follows:

likes := np r    s    np l

the    rst di   erence to notice is notational, in that the order of the categor-
ies is di   erent: in id35 the arguments are ordered from the right in the order
in which they are cancelled; pregroups use the type-logical ordering (moort-
gat, 1997) in which left arguments appear to the left of the result, and right
arguments appear to the right.

the key di   erence is that a left argument is now represented as a right
adjoint: np r, and a right argument is represented as a left adjoint: np l. so the
adjoint categories have e   ectively replaced the slash operators in traditional
categorial grammar. one potential source of confusion is that arguments to
the left are right adjoints, and arguments to the right are left adjoints. the
reason is that the    cancellation rules    of pregroups state that:

x    x r     1
x l    x     1

(11)

(12)

that is, any category x cancels with its right adjoint to the right, and cancels
with its left adjoint to the left. figure 9 gives the pregroup derivation for an
example newspaper sentence, using the id35 lexical categories from id35bank
(hockenmaier & steedman, 2007) translated into pregroup types.9

mathematically we can be more precise about the pregroup cancellation

rules:

a pregroup is a partially ordered mono  in which each object of the
monoid has a left and right adjoint subject to the cancellation rules
above, where     is the partial order,    is the monoid operation, and 1
is the unit object of the monoid.

in the linguistic setting, the objects of the monoid are the syntactic types;
the associative monoid operation (  ) is string concatenation; the identity ele-
ment is the empty string; and the partial order (   ) encodes the derivation
relation. lambek (2008) has many examples of linguistic derivations, including

9 pregroup derivations are usually represented as    reduction diagrams    in which
cancelling types are connected with a link, similar to dependency representations
in computational lingusitics. here we show a categorial grammar-style derivation.
10 a monoid is a set, together with an associative binary operation, where one of

the members of the set is an identity element with respect to the operation.

page: 28

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

29

investors

are

appealing

to

the

exchange commission

np

np r    s [dcl ]    s [ng]l    np np r    s [ng]    pp l pp    np l np    n l n    n l

n

n

np

pp

np r    s [ng]

np r    s [dcl ]

s [dcl ]

figure 9. example pregroup derivation

a demonstration of how iterated adjoints, e.g. np ll, can deal with interesting
syntactic phenomena such as object extraction.

there are two key ideas from the syntactic description which will carry
over to the vector-based semantics. first, linguistic constituents are repres-
ented using syntactic types, many of which are functional, or relational, in
nature. second, there is a mechanism in the pregroup grammar for combining
a functional type with an argument, using the adjoint operators and the par-
tial order (e   ectively encoding cancellation rules). hence, there are two key
questions for the semantic analysis:    rst, for each syntactic type, what is the
corresponding semantic type in the world of vector spaces? and second, once
we have the semantic types represented as vectors, how can the vectors be
combined to encode a    cancellation rule    in the semantics?

semantic types and tensor products

the question we will now answer is: what are the semantic types corresponding
to the syntactic types of the pregroup grammar? we will use the type of
a transitive verb in english as an example, although in principle the same
mechanism can be applied to all syntactic types.

the syntactic type for a transitive verb in english, e.g. likes, is as follows:

likes := np r    s    np l

let us assume that noun phrases live in the vector space n and that sentences
live in the vector space s. we have methods available for building the noun
space, n, detailed earlier in the chapter; how to represent s is a key question
for this whole section     for now we simply assume that there is such a space.
following the form of the syntactic type above, the semantic type of a
transitive verb     i.e. the vector space containing the vectors for transitive
verbs such as likes     is as follows:
         
likes     n    s    n

page: 29

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

30

stephen clark

(u     v)(a,d) = ua . vd

figure 10. the tensor product of two vector spaces; (u     v)(a,d) is the coe   cient
of (u     v) on the (a, d) basis vector

now the question becomes what should the monoidal operator (  ) be in the
vector-space case? coecke et al. (2010) use category theory to motivate the
use of the tensor product as the monoidal operator which binds the individual
vector spaces together:

         
likes     n     s     n

figure 10 shows two vector spaces being combined together using the
tensor product. the vector space on the far left has two basis vectors, a and
b, and is combined with a vector space also with two basis vectors, c and d.
the resulting tensor product space has 2    2 = 4 basis vectors, given by the
cartesian product of the basis vectors of the individual spaces {a, b}    {c, d}.
hence basis vectors in the tensor product space are pairs of basis vectors from
the individual spaces; if three vector spaces are combined, the resulting tensor
product space has basis vectors consisting of triples; and so on.
figure 10 also shows two individual vectors, u and v, being combined
(u     v). the expression at the bottom of the    gure gives the coe   cient for
one of the basis vectors in the tensor product space, (a, d), for the vector u   v.
the recipe is simple: take the coe   cient of u on the basis vector corresponding
to the    rst element of the pair (coe   cient denoted ua), and multiply it by the
coe   cient of v on the basis vector corresponding to the second element of the
pair (coe   cient denoted vd). combining vectors in this way results in what is
called a simple or pure vector in the tensor product space.

one of the interesting properties of the tensor product space is that it
is much larger than the set of pure vectors; i.e. there are vectors in the
tensor product space in figure 10 which cannot be obtained by combining
vectors u and v in the manner described above. it is this property of tensor
products which allows the representation of entanglement in quantum mech-

page: 30

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

abcduv(a,c)(a,d)u x v(b,c)(b,d)vector space models of lexical meaning

31

anics (nielsen & chuang, 2000), and leads to entangled vectors in the tensor
product space. coecke et al. (2010) argue that it is also this property which
allows the interactive nature of the verb to be captured, in terms of how a
transitive verb interacts with a subject and object.

an individual vector for a transitive verb can now be written as follows

(coecke et al., 2010):

   =

(cid:88)

cijk (      ni           sj           nk)     n     s     n

(13)

ijk

here ni and nk are basis vectors in the noun space, n; sj is a basis vector
in the sentence space, s;       ni           sj           nk is alternative notation for the basis
vector in the tensor product space (i.e. the triple (cid:104)      ni,      sj ,      nk(cid:105)); and cijk is the

coe   cient for that basis vector.

the intuition we would like to convey is that the vector for the verb is
relational, or functional, in nature, as it is in the syntactic case. a similar,
and useful, intuition lies behind the use of matrices to represent the meanings
of adjectives in baroni & zamparelli (2010). baroni & zamparelli argue that
an adjective is functional in nature, taking a noun (or noun phrase) as argu-
ment and returning another noun. a natural way to represent such a function
in vector spaces is via a linear map, represented as a matrix. one way to
understand the (coecke et al., 2010) framework is that it is an extension of
baroni & zamparelli (2010) to cover all functional types, not just adjectives.
the generalisation from adjectives to transitive verbs, for example, is to move
from a matrix     in id202 known as a tensor of order 2     to a    cube   ,
which is a tensor of order 3. a cube is required to represent a transitive verb

since three indices (      ni,      sj ,      nk) are needed to specify an element of the order

3 transitive verb space.

informally, the expression in (13) for the verb vector can be read as follows:

the vector for a transitive verb can be thought of as a function, which,
given a particular basis vector from the subject, ni, and a particular
basis vector from the object, nk, returns a value cijk for each basis
vector sj in the sentence space.

we have now answered one of the key questions for this section: what is the
semantic type corresponding to a particular syntactic type? the next section
answers the remaining question: how can the vector for the transitive verb in
(13) be combined with vectors of a subject and object to give a vector in the
sentence space?

composition for an example sentence space

in this section we will use an example sentence space which can be thought of
as a    plausibility space   . note that this is a    ctitious example, in that no such
space has been built and no suggestions will be made for how it might be built.
however, it is a useful example because the sentence space is small, with only

page: 31

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

32

stephen clark

true

6

dog chases cat

  
 

 

 
 

apple chases orange

       :

 

-

false

figure 11. an example    plausibility space    for sentences

      
      
dog
            
cat
apple
               orange

   u   y run fast aggressive tasty buy juice fruit

0.8 0.8 0.7
0.9 0.8 0.6
0.0 0.0 0.0
0.0 0.0 0.0

0.6
0.3
0.0
0.0

0.1 0.5
0.0 0.5
0.9 0.9
1.0 0.9

0.0 0.0
0.0 0.0
0.8 1.0
1.0 1.0

table 1. example noun vectors in n

two dimensions, and conceptually simple and easy to understand. note also
that the compositional framework places no limits on how large the sentence
space can be. grefenstette et al. (2011) and grefenstette & sadrzadeh (2011)
describe a sentence space which has been implemented, with many more than
2 basis vectors.

figure 11 gives two example vectors in the plausibility space, which has
basis vectors corresponding to true and false (which can also be thought of as
   highly plausible    and    not at all plausible   ). the sentence dog chases cat is
considered highly plausible, since it is close to the true basis vector, whereas
apple chases orange is considered highly implausible, since it is close to the
false basis vector.

we will use the example sentence dog chases cat, and show how a vector
can be built for this sentence in the plausibility space, assuming vectors for
the nouns, and the transitive verb (which will be an order 3 tensor), already
exist. the vectors assumed for the nouns cat and dog are given in table 1,
together with example vectors for the nouns apple and orange. note that,
again, these are    ctitious counts in the table, assumed to have been obtained
using one of the methods described in section 3.

the compositional framework is agnostic towards the particular noun vec-
tors, in that it does not matter how those vectors are built. however, for

page: 32

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

33

               
         
chases
eats

(cid:104)fluffy,t,fluffy(cid:105)(cid:104)fluffy,f,fluffy(cid:105)(cid:104)fluffy,t,fast(cid:105)(cid:104)fluffy,f,fast(cid:105)(cid:104)fluffy,t,juice(cid:105)(cid:104)fluffy,f,juice(cid:105)(cid:104)tasty,t,juice(cid:105). . .

0.8
0.7

0.2
0.3

0.75
0.6

0.25
0.4

0.2
0.9

0.8
0.1

0.1
0.1

table 2. example transitive verb vectors in n     s     n

               
chases 0.8

(cid:104)fluffy,t,fluffy(cid:105)(cid:104)fluffy,f,fluffy(cid:105)(cid:104)fluffy,t,fast(cid:105)(cid:104)fluffy,f,fast(cid:105)(cid:104)fluffy,t,juice(cid:105)(cid:104)fluffy,f,juice(cid:105)(cid:104)tasty,t,juice(cid:105). . .

0.2

0.75

0.25

0.2

0.8

0.1

dog,cat 0.8,0.9

0.8,0.9

0.8,0.6

0.8,0.6

0.8,0.0

0.8,0.0

0.1,0.0

table 3. vector for chases (order 3 tensor) together with subject dog and object
cat

explanatory purposes it will be useful to think of the basis vectors of the noun
space as corresponding to properties of the noun, obtained using the output of
a dependency parser, as described in section 3.1. for example, the count for
dog corresponding to the basis vector    u   y is assumed to be some weighted,
normalised count of the number of times the adjective    u   y has modi   ed
the noun dog in some corpus; intuitively this basis vector has received a high
count for dog because dogs are generally    u   y. similarly, the basis vector buy
corresponds to the object position of the verb buy, and apple has a high count
for this basis vector because apples are the sorts of things that are bought.
table 2 gives example vectors for the verbs chases and eats. note that,
since transitive verbs live in n    s    n     i.e. are order 3 tensors     the basis
vectors across the top of the table are triples of the form (ni, sj, nk), where
ni and nk are basis vectors from n (properties of the noun) and sj is a basis
vector from s (true or false in the plausibility space).
the way to read the    ctitious numbers is as follows: chases has a high
(normalised) count for the (cid:104)   u   y,t,   u   y(cid:105) basis vector, because it is highly
plausible that    u   y things chase    u   y things. conversely, chases has a low
count for the (cid:104)   u   y,f,   u   y(cid:105) basis vector, because it is not highly implausible
that    u   y things chase    u   y things.11 similarly, eats has a low count for the
(cid:104)tasty,t,juice(cid:105) basis vector, because it is not highly plausible that tasty things
eat things which can be juice.

table 3 gives the vector for chases, together with the corresponding counts
for the subject and object in dog chases cat. for example, the (dog, cat) count
pair for the basis vectors (cid:104)   u   y,t,   u   y(cid:105) and (cid:104)   u   y,f,   u   y(cid:105) is (0.8, 0.9), mean-
ing that dogs are    u   y to an extent 0.8, and cats are    u   y to an extent 0.9.

11 the counts in the table for (ni, t, nk) and (ni, f, nk), for some ni and nk, always
sum to 1, but this is not required and no probabilistic interpretation of the counts
is being assumed.

page: 33

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

34

stephen clark

the property counts for the subject and object are taken from the noun vec-
tors in table 1.

the reason for presenting the vectors in the form in table 3 is that a
procedure for combining the vector for chases with the vectors for dog and cat
now suggests itself. how plausible is it that a dog chases a cat? well, we know
the extent to which    u   y things chase    u   y things, and the extent to which
a dog is    u   y and a cat is    u   y; we know the extent to which things that are
bought chase things that can be juice, and we know the extent to which dogs
can be bought and cats can be juice; more generally, for each property pair,
we know the extent to which the subjects and objects of chases, in general,
embody those properties, and we know the extent to which the particular
subject and object in the sentence embody those properties. so multiplying
the corresponding numbers together brings information from both the verb
and the particular subject and object.

the calculation for the true basis vector of the sentence space for dog

chases cat is as follows:
                                 
dog chases cat true = 0.8 . 0.8 . 0.9 + 0.75 . 0.8 . 0.6 + 0.2 . 0.8 . 0.0 + 0.1 . 0.1 . 0.0 + . . .

the calculation for the false basis vector is similar:

                                 
dog chases cat false = 0.2 . 0.8 . 0.9 + 0.25 . 0.8 . 0.6 + 0.8 . 0.8 . 0.0 + . . .

we would expect the coe   cient for the true basis vector to be much higher
than that for the false basis vector, since dog and cat embody exactly those
elements of the property pairs which score highly on the true basis vector for
chases, and do not embody those elements of the property pairs which score
highly on the false basis vector.

      
through a particular example of the sentence space, we have now derived
   , with

the expression in coecke et al. (2010) for combining a transitive verb,

its subject,          , and object,       o :

f (                   

             o ) =

=

(cid:88)
(cid:88)

ijk

(cid:32)(cid:88)

cijk(cid:104)         |        i(cid:105)      sj(cid:104)      o |      ok(cid:105)

(cid:33)
cijk(cid:104)         |        i(cid:105)(cid:104)      o |      ok(cid:105)

(14)

(15)

      sj

j

ik

the expression (cid:104)         |        i(cid:105) is the dirac notation for the inner product between         
and         i, and in this case the inner product is between a vector and one of its
basis vectors, so it simply returns the coe   cient of          for the         i basis vector.
particular properties of the subject,         i, and object,       ok, and combining the

from the linguistic perspective, these inner products are simply picking out

corresponding property coe   cients with the corresponding coe   cient for the
verb, cijk, for a particular basis vector in the sentence space, sj.

page: 34

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

35

bites

man
dog
np np r    s    np l np
n n     s     n n

np r    s
n     s
s
s

figure 12. example pregroup derivation with semantic types

returning to the baroni & zamparelli (2010) case for adjective-noun com-
binations, the compositional procedure there is simply id127
of the noun vector by the adjective matrix. note that the expression in (15)
reduces to id127 when the relational type is of order 2 (as it
is for an adjective). so a further useful intuition to take from baroni & zam-
parelli is that the reduction mechanism in the general case is a generalisation
of id127 to higher-order tensors. in both the matrix multiplic-
ation case, and the reduction in (15), these compositional mechanisms take
the form of linear maps.

figure 12 shows a pregroup derivation for a simple transitive verb sentence,
together with the corresponding semantic types. the point of the example is
to demonstrate how the semantic types    become smaller    as the derivation
progresses, in much the same way that the syntactic types do. the reduction,
or cancellation, rule for the semantic component is given in (15), which can be
thought of as the semantic vector-based analogue of the syntactic reduction
rules in (11) and (12). similar to a model-theoretic semantics, the semantic
vector for the verb can be thought of as encoding all the ways in which the verb
could interact with a subject and object, in order to produce a sentence, and
the introduction of a particular subject and object reduces those possibilities
to a single vector in the sentence space.
in summary, the meaning of a sentence w1        wn with the grammatical
(pregroup) structure p1        pn       s, where pi is the grammatical type of wi,
and       is the pregroup reduction to a sentence, can be represented as follows:

                     
w1        wn = f (  )(      w1                      wn)

here we have generalised the previous discussion of transitive verbs and exten-
ded the idea to syntactic reductions or derivations containing any types. the
point is that the semantic reduction mechanism described in this section can
be generalised to any syntactic reduction,   , and there is a function, f , which,
given   , produces a linear map to take the word vectors       w1                      wn to a
sentence vector                      
w1        wn. f can be thought of as montague   s    homomorphic

passage    from syntax to semantics.

page: 35

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

36

stephen clark

concluding remarks

one aspect of the compositional framework that was not stressed in the de-
scription is the close link between the mathematical structure used for the
syntax     pregroup grammars     and that used for the distributional se-
mantics     vector spaces with linear maps. indeed this was one of the main
reasons for using pregroup grammars. coecke et al. (2010) discusses this con-
nection in depth showing how both structures are examples of an abstract
mathematical object called a compact closed category.

the question of how to evaluate the models described in this section,
and compositional distributional models more generally, is an important one,
but one that is still under active investigation. pulman (2012) discusses the
issue of evaluation, and describes a recent method of using compositional
distributional models to disambiguate verbs in context (mitchell & lapata,
2008; grefenstette & sadrzadeh, 2011).

there are some obvious ways in which the existing work could be ex-
tended. first, we have only presented a procedure for building vectors for
sentences with a simple transitive verb structure. the mathematical frame-
work in coecke et al. (2010) is general and in principle applies to any syntactic
reduction from any sequence of syntactic types, but how to build relational
vectors for all complex types is an open question. second, it needs to be
demonstrated that the compositional distributional representations presented
in this section can be useful for language processing tasks and applications.
third, there is the question of whether the tensors for relational types can be
learnt automatically from data using machine learning. some related work in
this vein which uses id56s is socher et al. (2012).

page: 36

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

37

5 conclusion

vector space models of meaning have proven successful in computational lin-
guistics and cognitive science and are currently an extremely active area of
research. one of the reasons for their popularity in computational linguistics is
that they provide a level of robustness required for e   ective language techno-
logy which has notoriously not been provided by more traditional logic-based
approaches.12 even taking account of laudable attempts such as bos & mark-
ert (2006), building on earlier work such as alshawi (1992), it is probably fair
to say that the classical logic-based enterprise in natural language processing
has failed. however, whether fundamental concepts from semantics, such as
id136, can be suitably incorporated into vector space models of meaning
is an open question.

in addition to incorporating traditional concepts from semantics, another
area which is likely to grow is the incorporation of other modalities, such as
vision, into the vector representations (bruni et al., 2012). a semantic theory
incorporating distributional representations, at the word, phrase, sentence    
and perhaps even document     level; multi-modal features; and e   ective and
robust methods of id136, is a grand vision aimed at solving one of the most
di   cult and fundamental problems in arti   cial intelligence and the cognitve
sciences. we still have a long way to go.

12 a recent attempt at a more robust semantics closer to more traditional logic-based

approaches is copestake (2009).

page: 37

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

38

stephen clark

acknowledgements

almost all of the ideas in section 4 arose from discussions with merhnoosh
sadrzadeh, ed grefenstette, bob coecke and stephen pulman. much of the
author   s understanding of distributional word models described in section 3
arose from discussions with james curran in edinburgh (2002-2004) and from
proof-reading james    phd thesis.

page: 38

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

39

references

agirre, eneko, enrique alfonseca, keith b. hall, jana kravalova, & marius pas-
caand aitor soroa (2009), a study on similarity and relatedness using distribu-
tional and id138-based approaches, in proceedings of human language tech-
nologies: the 2009 annual conference of the north american chapter of the
association for computational linguistics, boulder, co, (19   27).

alshawi, hiyan (ed.) (1992), the core language engine, the mit press.
baroni, m., b. murphy, e. barbu, & m. poesio (2010), strudel: a corpus-based
semantic model based on properties and types, cognitive science 34(2):222   254.
baroni, m. & r. zamparelli (2010), nouns are vectors, adjectives are matrices:
representing adjective-noun constructions in semantic space, in conference on
empirical methods in natural language processing (emnlp-10), cambridge,
ma, (1183   1193).

blackburn, patrick & johan bos (2005), representation and id136 for natural

language. a first course in computational semantics, csli publications.

bos, johan & katja markert (2006), when logical id136 helps determining tex-
tual entailment (and when it doesn   t), in proceedings of the 2nd recognising
id123 challenge, venice, italy, (98   103).

brants, thorsten (2000), tnt - a statistical part-of-speech tagger, in proceedings of

the 6th conference on applied natural language processing, (224   231).

brin, sergey & lawrence page (1998), the anatomy of a large-scale hypertextual

web search engine, computer networks 30:107   117.

briscoe, ted & john carroll (2006), evaluating the accuracy of an unlexicalized
statistical parser on the parc depbank, in proceedings of the poster session
of the joint conference of the international committee on computational lin-
guistics and the association for computational linguistics (coling/acl-06),
sydney, austrailia, (41   48).

briscoe, ted, john carroll, & rebecca watson (2006), the second release of the
rasp system, in proceedings of the interactive demo session of the joint con-
ference of the international committee on computational linguistics and the
association for computational linguistics (coling/acl-06), sydney, aus-
trailia, (77   80).

bruni, e., g. boleda, m. baroni, & n. tran. 2012 (2012), id65
in technicolor, in proceedings of the 50th annual meeting of the association for
computational linguistics, jeju island, korea, (136   145).

budanitsky, alexander & graeme hirst (2006), evaluating id138-based measures

of semantic distance, computational linguistics 32(1):13   47.

chan, yee seng, hwee tou ng, & david chiang (2007), id51
improves id151, in proceedings of the 45th meeting of
the acl, prague, czech republic, (33   40).

clark, stephen & james r. curran (2007), wide-coverage e   cient statistical parsing

with id35 and id148, computational linguistics 33(4):493   552.

clarke, daoud (2008), context-theoretic semantics for natural language: an al-

gebraic framework, ph.d. thesis, university of sussex.

clarke, daoud (2012), a context-theoretic framework for compositionality in distri-

butional semantics, computational linguistics 38(1):41   71.

coecke, b., m. sadrzadeh, & s. clark (2010), mathematical foundations for a com-
positional distributional model of meaning, in j. van bentham, m. moortgat,

page: 39

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

40

stephen clark

& w. buszkowski (eds.), linguistic analysis (lambek festschrift), volume 36,
(345   384).

copestake, ann (2009), slacker semantics: why super   ciality, dependency and
avoidance of commitment can be the right way to go, in proceedings of the
12th meeting of the eacl (eacl-09), athens, greece, (1   9).

van de cruys, tim (2010), a non-negative tensor factorization model for selectional
preference induction, journal of natural language engineering 16(4):417   437.
curran, james r. (2004), from distributional to semantic similarity, ph.d. thesis,

university of edinburgh.

curran, james r. & stephen clark (2003), investigating gis and smoothing for
maximum id178 taggers, in proceedings of the 10th meeting of the eacl,
budapest, hungary, (91   98).

deerwester, scott, susan t. dumais, george w. furnas, thomas k. landauer, &
richard harshman (1990), indexing by latent semantic analysis, journal of the
american society for information science 41(6):391   407.

dowty, d.r., r.e. wall, & s. peters (1981), introduction to montague semantics,

dordrecht.

firth, j. r. (1957), a synopsis of linguistic theory 1930-1955, in studies in linguistic
analysis, oxford: philological society, (1   32), reprinted in f.r. palmer (ed.),
selected papers of j.r. firth 1952-1959, london: longman (1968).

fodor, jerry & zenon pylyshyn (1988), connectionism and cognitive architecture:

a critical analysis, cognition 28:3   71.

grefenstette, edward (2013), categorytheoretic quantitative compositional dis-
tributional models of natural language semantics, ph.d. thesis, university of
oxford.

grefenstette, edward & mehrnoosh sadrzadeh (2011), experimental support for a
categorical compositional distributional model of meaning, in proceedings of the
2011 conference on empirical methods in natural language processing, asso-
ciation for computational linguistics, edinburgh, scotland, uk, (1394   1404).
grefenstette, edward, mehrnoosh sadrzadeh, stephen clark, bob coecke, &
stephen pulman (2011), concrete sentence spaces for compositional distribu-
tional models of meaning, in proceedings of the 9th international conference on
computational semantics (iwcs-11), oxford, uk, (125   134).

grefenstette, gregory (1994), explorations in automatic thesaurus discovery,

kluwer.

gri   ths, thomas l., mark steyvers, & joshua b. tenenbaum (2007), topics in

semantic representation, psychological review 114(2):211   244.

guevara, emiliano raul (2011), computing semantic compositionality in distribu-
tional semantics, in proceedings of the 9th international conference on compu-
tational semantics (iwcs-11), oxford, uk.

harris, zellig (1954), distributional structure, word 10(23):146   162.
hearst, marti (1992), automatic acquisition of hyponyms from large text corpora, in
proceedings of the fourteenth international conference on computational lin-
guistics, nantes, france.

hockenmaier, julia & mark steedman (2007), id35bank: a corpus of id35 deriva-
tions and dependency structures extracted from the id32, computa-
tional linguistics 33(3):355   396.

hodgson, james m. (1991), informational constraints on pre-lexical priming, lan-

guage and cognitive processes 6:169   205.

page: 40

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

41

lambek, joachim (1958), the mathematics of sentence structure, american math-

ematical monthly (65):154   170.

lambek, joachim (2008), from word to sentence. a computational algebraic ap-

proach to grammar, polimetrica.

landauer, t. k. & s. t. dumais (1997), a solution to plato   s problem: the latent
semantic analysis theory of acquisition, induction and representation of know-
ledge, psychological review 104(2):211   240.

lin, dekang (1998), an information-theoretic de   nition of similarity, in proceedings
of the fifteenth international conference on machine learning, madison, wi,
(296   304).

lin, dekang & patrick pantel (2001), discovery of id136 rules for question an-

swering, journal of natural language engineering 7(4):343   360.

lowe, will & scott mcdonald (2000), the direct route: mediated priming in se-
mantic space, in proceedings of the 22nd annual conference of the cognitive
science society, philadelphia, pa, (675   680).

lund, k. & c. burgess (1996), producing high-dimensional semantic spaces from
lexical co-occurrence, behavior research methods, instruments & computers
28:203   208.

maillard, jean, stephen clark, & edward grefenstette (2014), a type-driven tensor-
based semantics for id35, in proceedings of the eacl 2014 type theory and
natural language semantics workshop (ttnls), gothenburg, sweden.

manning, christopher & hinrich schutze (1999), foundations of statistical natural

language processing, the mit press, cambridge, massachusetts.

manning, christopher d., prabhakar raghavan, & hinrich schutze (2008), intro-

duction to information retrieval, cambridge university press.

marcus, mitchell, beatrice santorini, & mary marcinkiewicz (1993), building a large
annotated corpus of english: the id32, computational linguistics
19(2):313   330.

medin, douglas l., robert l. goldstone, & dedre gentner (1990), similarity in-
volving attributes and relations: judgements of similarity and di   erence are not
inverses, psychological science 1(1):64   69.

minnen, guido, john carroll, & darren pearce (2001), applied morphological pro-

cessing of english, natural language engineering 7(3):207   223.

mitchell, je    & mirella lapata (2008), vector-based models of semantic composi-

tion, in proceedings of acl-08, columbus, oh, (236   244).

moortgat, michael (1997), categorial type logics, in johan van benthem & alice
ter meulen (eds.), handbook of logic and language, elsevier, amsterdam and
mit press, cambridge ma, chapter 2, (93   177).

morrill, glyn (2010), categorial grammar: logical syntax, semantics, and pro-

cessing, oxford university press, oxford.

nielsen, michael a. & isaac l. chuang (2000), quantum computation and quantum

information, cambridge university press.

pado, sebastian & mirella lapata (2007), dependency-based construction of se-

mantic space models, computational linguistics 33(2):161   199.

preller, anne & mehrnoosh sadrzadeh (2009), bell states as negation in natural

languages, entcs, qpl .

pulman, stephen (2012), distributional semantic models, in sadrzadeh heunen &
grefenstette (eds.), compositional methods in physics and linguistics, oxford
university press.

page: 41

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

42

stephen clark

robertson, stephen & hugo zaragoza (2009), the probabilistic relevance framework:
bm25 and beyond, foundations and trends in information retrieval 3(4):333   
389.

rose, t., m. stevenson, & m. whitehead (2002), the reuters corpus     from yes-
terday   s news to tomorrow   s language resources, in proceedings of the third
international conference on language resources and evaluation (lrec-02),
las palmas, canary islands, (827   832).

sahlgren, magnus (2006), the word-space model: using distributional analysis
to represent syntagmatic and paradigmatic relations between words in high-
dimensional vector spaces, ph.d. thesis, stockholm university.

salton, g., a. wang, & c. yang (1975), a vector-space model for information re-

trieval, journal of the american society for information science 18:613   620.

sch  utze, hinrich (1998), automatic word sense discrimination, computational lin-

guistics 24(1):97   124.

smolensky, paul (1990), tensor product variable binding and the representation of
symbolic structures in connectionist systems, arti   cial intelligence 46:159   216.
socher, richard, brody huval, christopher d. manning, & andrew y. ng (2012),
semantic compositionality through recursive matrix-vector spaces, in proceed-
ings of the conference on empirical methods in natural language processing,
jeju, korea, (1201   1211).

sparck jones, karen (1972), a statistical interpretation of term speci   city and its

application in retrieval, journal of documentation 28:11   21.

steedman, mark (2000), the syntactic process, the mit press, cambridge, ma.
steedman, mark & jason baldridge (2011), id35, in
robert borsley & kersti borjars (eds.), non-transformational syntax: formal
and explicit models of grammar, wiley-blackwell.

toutanova, kristina, dan klein, christopher manning, & yoram singer (2003),
feature-rich part-of-speech tagging with a cyclic dependency network, in pro-
ceedings of the hlt/naacl conference, edmonton, canada, (252   259).

turney, peter d. (2006), similarity of semantic relations, computational linguistics

32(3):379   416.

turney, peter d. (2008), a uniform approach to analogies, synonyms, antonyms,
and associations, in proceedings of the 22nd international conference on com-
putational linguistics (coling 2008), manchester, uk, (905   912).

turney, peter d. & patrick pantel (2010), from frequency to meaning: vector space

models of semantics, journal of arti   cial intelligence research 37:141   188.

weir, david (1988), characterizing mildly context-sensitive grammar formalisms,

ph.d. thesis, university of pennsylviania.

werning, markus, wolfram hinzen, & edouard machery (eds.) (2012), the oxford

handbook of compositionality, oxford university press.

widdows, dominic (2004), geometry and meaning, csli publications, stanford

university.

widdows, dominic (2008), semantic vector products: some initial investigations, in
proceedings of the second aaai symposium on quantum interaction, oxford,
uk.

wittgenstein, ludwig (1953), philosophical investigations, blackwell.
zanzotto, fabio massimo, ioannis korkontzelos, francesca fallucchi, & suresh man-
andhar (2010), estimating linear models for compositional distributional se-

page: 42

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

vector space models of lexical meaning

43

mantics, in proceedings of the 23rd international conference on computational
linguistics (coling 2010), beijing, china, (1263   1271).

zhai, chengxiang & john la   erty (2004), a study of smoothing methods for lan-
guage models applied to information retrieval, acm transactions on informa-
tion systems 2(2).

page: 43

job: clark

macro: handbook.cls

date/time: 25-mar-2014/9:41

