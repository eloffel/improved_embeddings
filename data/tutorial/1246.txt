inductive semi-supervised learningwith applicability to nlpanoop sarkar and gholamreza haffarianoop,ghaffar1@cs.sfu.caschool of computing sciencesimon fraser universityvancouver, bc, canadahttp://natlang.cs.sfu.ca/2outline   introduction to semi-supervised learning (ssl)   classifier based methods: part 1   em, stable mixing of complete and incomplete information   ssl using generative models for structured labels   classifier based methods: part 2   self-training, the yarowsky algorithm, co-training   data based methods   manifold id173, harmonic mixtures, information id173   learning predictive structure from multiple tasks   ssl using discriminative models for structured labels3learning problems   supervised learning:   given a sample consisting of object-label pairs (xi,yi), find thepredictive relationship between objects and labels.   un-supervised learning:   given a sample consisting of only objects, look for interestingstructures in the data, and group similar objects.   what is semi-supervised learning?   supervised learning + additional unlabeled data   unsupervised learning + additional labeled data4motivation for ssl   pragmatic:   unlabeled data is cheap to collect (compared to labeled data).   example: classifying web pages,   there are some annotated web pages.   a huge amount of un-annotated web pages are easily available byid190.   philosophical:   the brain can exploit unlabeled data.   learning in a setting where data is randomly labeled orlabeled by a lazy teacher.   reduces to unsupervised learning in the worst case.5why should more data help?(banko & brill, 2001)6why should unlabeled data help?   if you have labeled data, why bother with unlabeled data?   don   t! if you have sufficient labeled data or very fewparameters = no sparse data problem: rarely occurs in nlp!   injecting unlabeled data can be a way to address the sparsedata problem   too many languages: cannot afford to annotate a millionwords in each one   for task of predicting y given x, you (should?) have a goodidea of how to predict p(x)   redundantly sufficient learners can be built (mitchell, 1999)   we provide more intuition in different learning settings inthe next few slidespreliminaries ...7++__labeled data only++__++__transductive id166id166(vapnik, 2000; joachims, 1999)intuition in large-margin settingtraining a support vector machine(id166), input x, label = {+,-}key idea: avoid regions withhigh values for p(x)(zhang and oles, 2000)8intuition when using the em algorithm(castelli, 1994; castelli and cover, 1995; castelli and cover, 1996; cohen et.al., 2004)   assume we have    good    generative model for the data   that is, there is a parameter setting such that id203 p(x,y)from the generative model captures the labeled data, and   there is a parameter setting such that id203 p(x) from thegenerative model captures the id203 of the unlabeled data   then em can be used to identify which unlabeled examplesbelong to the same class without knowing the true class label   labeled data can then be used to identify the actual class labelfor each group   for example, let us consider that a mixture of two gaussians isa good model for our binary classification task(dempster, laird & rubin, 1977)9intuition when using the em algorithmfigure from (nigam et.al., 2000)the parameters   0,   1, the varianceand mixing parameters can be learntusing em which gives the bayesoptimal decision boundary d10intuition when using the em algorithmbut the clusters are not labeled yet!assume that labeled data can beused to identify the class labels foreach cluster.(castelli and cover, 1995) show thatthis process convergesexponentially wrt number of labeledexamples11intuition when using the em algorithm   note that we had to assume that the labeleddata can be used to identify the two classes   mixtures of gaussians are known to beidentifiable; the details are in (castelli &cover, 1995)   however, other kinds of models may not beidentifiable;   for more on this see the (zhu, 2005) surveywhich covers `identifiability   12   class distributions p(x|y,!) and class prior p(y|") areparameterized by ! and ", and used to derive:!"xy   unlabeled data gives information about themarginal p(x|!,") which is:   unlabeled data can be incorporated naturally(zhang & oles, 2000; seeger, 2000)intuition for generative models13   unlabeled data gives information about   ,and p(y|x) is parameterized by !.   if    affects !  then we are done!   impossible: ! and    are independent givenunlabeled data.   what is the cure?   make    and ! a priori dependent.   input dependent id173   in discriminative approach p(y|x,!) and p(x|  ) aredirectly modeled.!  xy!  xy(zhang & oles, 2000; seeger, 2000)intuition for discriminative models14semi-supervised learning methods   a wide variety of methods exist:   em with generative mixture models (mix l + u)   self-training   co-training   data based methods   transductive support vector machines (tid166s)   graph-based methods   in this tutorial we will make a distinction between:   inductive ssl methods   transductive ssl methods15   transductive: produce label only for the availaid7nlabeled data.   the output of the method is not a classifier.   inductive: not only produce label for unlabeled data,but also produce a classifier.   analogy from (zhu, 2005):   transductive learning: take-home exam   inductive learning: in-class examinductive vs.transductive16   based on our definition: a transductive id166(tid166) is an inductive learner!   this is because tid166 can be naturally used onunseen data   however, the name tid166 originates from thefollowing argument given in (vapnik, 1998)   learning on the entire data space is solving a more difficultproblem   if task is to annotate the test data, then only work on theobserved data (l+t): solve a simpler problem first!inductive vs.transductive17inductive vs.transductive   tid166 can be seen as a different way to do supervisedlearning:   we can get around i.i.d. assumption by learning a classifier gearedtowards each test case (or all test cases together)   e.g. when learning to recognize handwriting, transduction should helpif all test cases were handwritten digits by the same person; comparewith (hinton and nair, 2005)   training a tid166 is np-hard   but approximations exist: e.g. (joachims, 1999) and manyothers   (zhang and oles, 2000) argue against tid166s, but empiricallytid166s seem to be beneficial18inductive vs.transductive   (goutte et. al., 2002) use transductive id166s for findingnamed entities in medline abstracts, i.e. learns a binaryclassifier   (niu, ji & tan, 2005) provide a semi-supervised featureid91 algorithm for word-sense disambiguation; it istransductive because it clusters features from test data withthose from the training data   there are many ir related works in this area, see e.g.   (joachims, 1999) for text classification and   (okabe, umemura and yamada, 2005) for id183   if tid166 is not an example of a transductive ssl method,then what is?19   graph mincuts (blum and chawla, 2001)   pose ssl as a graph mincut (also called st-cut) problem   two class classification setting: positive labels (1) act assources and negative labels (0) as sinks   unlabeled nodes are connected to other nodes withweights based on similarity between examples (l or u)   objective is to find a minimum set of edges to remove sothat all flow from sources to sinks is blocked   in other words, given the constraint that each label yi iseither 0 or 1 the task is to minimize the function:inductive vs.transductivedo not change labelson labeled data: weightis infinityprovides the    flow   across the edges inthe graphexample20   graph mincuts have been used in nlp: (pang and lee, 2004)   to train a binary sentence classifier: subjective vs. objective   can be used to create a subjective extract/summary from a moviereview   then the extract is categorized as a positive/negative review   labeled data external to dataset was used to train sentencelevel subjective vs. objective classifiers   sentences were labeled using this classifier trained on labeled data   unlabeled data: sentences were given weights based on simpleproximity to other sentences   graph mincut method was used to extract the sentences   attracted    to the subjective classinductive vs.transductive21   for this tutorial we will focus on inductive ssl methods:why?   most graph-based transductive methods scale badly withrespect to the time complexity, which is typically o(n3)   it is possible to improve the complexity to o(n) but these ideasare based on assumptions or methods that may or may notapply to common nlp tasks   most interest in nlp is for the use of very large datasets (likethose used to train language models) and inductive methodsare more suitable for such a settingfocus on inductive ssl22   other surveys do a good job of covering transductivessl methods, see   semi-supervised learning (chappelle et.al., to appear)   semi-supervised learning literature survey (zhu, 2005)   learning with labeled and unlabeled data (seeger, 2000)   learning from l and u data: an empirical study acrosstechniques and domains (chawla & karakoulas, 2005)   in particular chapter 25 of (chappelle et. al., toappear) which is available online has a lengthydiscussion comparing semi-supervised learning andtransductive learning.focus on inductive sslnote: these surveys also cover manyinductive ssl methods!23   classifier based methods:   start from initial classifier(s), and iteratively enhance it(them)   data based methods:   discover an inherent geometry in the data, and exploit it infinding a good classifier.two algorithmic approaches24   the first classifier based method we will study uses the emalgorithm which provides the maximum likelihood (ml)estimates for unlabeled data treating the labels as hidden data   for labeled data: ml estimates usually reduce to simply therelative frequency counts   for unlabeled data: ml estimates are computed using em (aniterative re-estimation algorithm to find parameter values)   if we have a prior over models, map estimation can be usedto find a good model from the space of all models andlikelihood of the data given this model.   we will assume some familiarity with em in this tutorial (buttypically we will use it as a black box inside other algorithms)classifier based methods: part 125   basic em:1.initialize model using parameter estimation from labeleddata2.re-estimate model on unlabeled data using the emalgorithm3.return model after em converges; this model is used tomeasure performance on test dataem: combining labeled and unlabeled data(dempster, laird & rubin, 1977)forward link ...26   use em to maximize the joint log-likelihood oflabeled and unlabeled data:: log-likelihood oflabeled data: log-likelihood ofunlabeled data(dempster, laird & rubin, 1977)em: mixtures of labeled and unlabeled data27   labeled examples from xl: (xi, yi), where yi is thelabel and unlabeled examples xi from xu   each input xi is generated from a mixture mi withprob f(xi | !i)   if there are l mixture components then the densityhas mixing parameters #k which sum to 1em: mixtures of experts(miller and uyar, 1997; shahshahani and landgrebe, 1994)28   labeled examples from xl and unlabeled examplesfrom xu   the likelihood of the data xl and xu is given by:em: mixtures of experts(miller and uyar, 1997; shahshahani and landgrebe, 1994)29   (miller and uyar, 1997) show that em canbe used to learn the parameters f(xi | !i) aswell as the mixture parameters #k   they provide two variants:   (1) the mixture components and the class labelsare conflated to be the same, and   (2) the mixture components are predicted firstgiven the feature value and the class label ispredicted given the mixture componentem: mixtures of experts(miller and uyar, 1997; shahshahani and landgrebe, 1994)30em: mixtures of labeled and unlabeled data   so far we have equal contributions from labeled andunlabeled data   in practice, it is better to discount the unlabeled data   we can do this in a mixture model           $ lu + (1 - $) ll   standard ml estimation means that the value of $ isset proportional to the size of each set, ll and lu, butthis is not what we want   we generally discount the estimates from lu sincethey are less reliable31   (nigam et.al., 2000) combine labeled and unlabeleddata for document classification using em   classification model is a naive bayes model   setting is similar to (miller and uyar, 1997) exceptfor model parameters   f(xi | !i) is now a naive bayes classifier defined asthe product of all the words xij in document xi giventhe doc. class/feature value   the mixture parameters #k indicate the likelihoodof each word in a document belonging to a topic ora classem: mixtures of labeled and unlabeled data(nigam et.al, 2000)32   they provide two variants:   (1) the mixture components and the class labels areconflated to be the same and em counts are discountedusing an extra parameter $, and   (2) each class has several sub-topics each with a worddistribution; each word is conditioned on a mixturecomponent and the class label is conditioned on thecomponent (a many to one mapping)   both $ and the number of mixture components forunlabeled data are tuned on a held-out set.   in several expts, these variants of em are shown toexploit unlabeled data effectively in the documentclassification task.em: mixtures of labeled and unlabeled data(nigam et.al, 2000)33em: mixtures of labeled and unlabeled data   (callison-burch et. al., 2004) proposes a mixture model forstatistical mt   the model combines human annotated word-aligned data with eid113arnt word-alignments (using ibm models)   it uses the discounting idea; setting $ = 0.9 (almost all weight onlabelled data) seemed to perform the best in the experiments   (mcclosky et. al, 2006) use discounting to combine countsfrom parsed output with labeled data for statistical parsing:improves parse f-score from 91.3 to 92.1 for parsing wsj   (corduneanu & jaakkola, 2002) provide a general frameworkfor optimizing the log-likelihood $ lu + (1 - $) ll and theoptimal value of $skip ahead ...34   use $ to combine the log-likelihood of labeled andunlabeled data in an optimal way:      $ lu + (1 - $) ll   em can be adapted to optimize it.   additional task is determining the best value for $.(corduneanu & jaakkola 2002)stable mixing of information35   e and m steps update the value of the parameters foran objective function with particular value of $.   name these two steps together as em$ operator:   the optimal value of the parameters is a fixed pointof the em$ operator:stable mixing: em$ operator360101$$   how to choose the best $ ?   by finding the path of optimal solutions as a function of $   choosing the first $ where a bifurcation or discontinuityoccurs; after such points labeled data may not have aninfluence on the solution.!!stable mixing: path of solutions$ lu + (1 - $) ll37ssl for structured labels   generative model based:   hidden markov model (id48)   stochastic id18 (sid18)   discriminative model based (to be covered later):   co-hidden markov id88   semi-structured support vector machines (sid166)   co-structured id166 (co-sid166)   semi-kernel id49 (kcrf)38id48   hidden markov model (id48) is the standardgenerative model for sequence learning: inputsequence labeled with output sequence of samelength.   em can be used to train id48 when unlabeled dataexists: forward-backward algorithm   decoding (finding the best label sequence for giveninput sequence) can be done in linear time by theviterbi algorithm39probabilistic id18s   probabilistic id18 (pid18) is thestandard generative model for tree structure: inputsequence is labeled with a tree (input = leaves)   em can be used to train pid18 when unlabeled dataexists: inside-outside algorithm   decoding (finding the best parse tree for a giveninput) can be done in polynomial time by the viterbialgorithm40basic em for id48s   (cutting et. al., 1992) used basic em with id48s fora part-of-speech tagging task and produced greatresults by boosting performance using unlabeled dataand em   (brill, 1997) did a similar likelihood estimation in thetransformation-based learning framework   both rely on implicitly or explicitly knowing the tagdictionary for words in the unlabeled data.each word in the unlabeled data is associated with a list ofpossible tags,using id91 or morphological classes41basic em for id48s   (merialdo, 1994) and (elworthy, 1994) used varying amountsof labeled and unlabeled data to test effectiveness of basic emusing id48s for the part-of-speech tagging task   different settings corresponded to varying amounts ofsupervision (or quality of labeled data)   (merialdo, 1994) also tried various constraints to keep p(t|w)fixed or to keep the marginal id203 p(t) fixed at each emiteration -- although these were not very effective   these expts showed that em for id48s seems to work only incases of very little labeled data and hurts accuracy in all othercases with large or medium amount of labeled data42basic em for id48sfigure from (elworthy, 1994)43   the second class of classifier based methods we willstudy are id64 methods   these include methods like: self-training, theyarowsky algorithm, co-training, etc.   in these methods, we start by training model(s) onsome labeled data   then unlabeled data is labeled using model(s) andsome examples are selected to be added as newlylabeled examples   this procedure is iterated and the labeled data set isgrownclassifier based methods: part 244   self-training procedure:   a classifier is trained with a small amount of labeled data   the classifier is then used to classify the unlabeled data   typically the most confident unlabeled points, along withthe predicted labels are incorporated into the training set   the classifier is re-trained and the procedure is repeated   this is the simplest form of a id64 method   learning using em is related to self-training;   self-training only uses the mode of the predictiondistributionself-training45   (charniak, 1997) reported a single round of self-training on30m words for statistical parsing: resulted in a 0.2~0.4 point f-score improvement on wsj parsing   (mcclosky et. al., 2006) improve on vanilla self-training bydiscounting the events learnt from unlabeled data: resulted in af-score improvement from 91.3 to 92.1   (riloff et. al., 2003; phillips & riloff, 2002) use self-trainingto extract patterns that identify subjective nounsself-training46   (maeireizo et. al., 2004) use self-training between twoclassifiers to classify dialogues as emotional or non-emotional.each classifier was trained on a single class   (hindle & rooth, 1993) proposed an idea for prepositionalphrase (pp) attachment disambiguation which is nowcommonly used in self-training for nlp:   extract unambiguous cases from a large unlabeled corpus andthen use those cases as training data in a disambiguationclassifer   (ratnaparkhi, 1998) has further experiments along these lines,also for pp attachmentself-training47   (yarowsky, 1995) created a new form of self-training forword-sense disambiguation which incorporated highconfidence examples as labeled data   the algorithm is similar to self-training but also used a secondconstraint:    one sense per discourse/document    to bootstrapnew features.   we refer to this and other variants of self-training that dependon high precision models as the yarowsky algorithmself-training48the yarowsky algorithmiteration: 0+-a classifiertrained by slchoose instanceslabeled with highconfidenceiteration: 1+-add them to thepool of currentlabeled trainingdata      (yarowsky, 1995)iteration: 2+-49the yarowsky algorithmfigure from (yarowsky, 1995)50the yarowsky algorithmfigure from (yarowsky, 1995)one sense perdiscourseconstraint canhelpid64by injectingnew patterns51the yarowsky algorithmfigure from (yarowsky, 1995)52the yarowsky algorithm   input: each example x iseither labeled l(x) orunlabeled u0(x)   for u0(x) a special classlabel is used for unknown: %   classifier prediction:     y = arg maxj pr(j | x , !) ifpr(j | x , !) > threshold &     y = % otherwiset = 0loop:for each example x:estimate pr(j | x , !) using land ut(x)ut+1(x) = y, wherey = arg maxj pr(j | x , !) ifpr(j | x , !) > threshold &     y = % otherwiseif ut+1(x) = ut(x) then stopelse t = t+1 and restart loop53(abney, 2004)analysis of the yarowsky algorithm   it can be shown that some variants of the yarowsky algorithmoptimize either negative log likelihood h or an upper bound onit, called k   definition:   empirical labeling distribution    x(j)   for a labeled data point x, it is 1 if j is the label of x and 0otherwise   for an unlabeled data point x it is the uniform distribution over thepossible labels.   model   s prediction distribution "x(j) = p(j | x , !)   ! is the parameter vector of the model54   once an unlabeled example x is labeled, it remainslabeled (its label is recomputed and may change)   the threshold ( is eliminated   the resulting algorithm y1 optimizes the followingobjective function:kl-divergenceid178a modified algorithm: y155     !fjjf   each rule: f ) j   !fj : the score of feature f inpredicting the label j   ! : the parameter vector of the model   let fx to be the set of features of theexample x where |fx| = m   define the prediction distribution for theexample x :a decision list based model56initialize n[f, j] = 0, z[f] = 0for all f, jfor each example-label pair(x,j)  for each feature fincrement n[f, j] andz[f]for each feature f and label jthe empirical labelingdistribution    x(j):1.for a labeled datapoint x, it is 1 if j is thelabel of x and 0otherwise2.for an unlabeled datapoint x it is theuniform distributionover the possiblelabels.a modified algorithm: dl-1-r57the objective function k58   instances contain two sufficient sets of features   i.e. an instance is x=(x1,x2)   each set of features is called a view   two views are independent given the label:   two views are consistent:xx1x2(blum & mitchell, 1998)co-training59co-trainingiteration: t+-iteration: t+1+-      c1: a classifiertrained on view 1c2: a classifiertrained on view 2allow c1 to label some instancesallow c2 to label some instancesadd self-labeled instances to the pool of training data60(blum & mitchell, 1998)co-training   an example: build a classifier that categorizesweb pages into two classes:+ is a course web page and   is not a course web page   usual model, build a naive bayes classifier61(blum & mitchell, 1998)co-training   notice that we can choose to classify eachlabeled example in two natural ways   x1 = text in the hyperlink to the page <a href=   ...   >cse 120, fall semester</a>   x2 = text in the web page <html> ... assignment #1 </html>62(blum & mitchell, 1998)co-training   train one nb classifier for x1 and another nb classifer on x2   baseline model trained on l: px1(.) * px2(.)   co-training model is a modified version of self-training: withtwo views   the x1 classifier produces high confidence output from u andprovides it to x2 (and vice versa)   on the webkb dataset, co-training outperforms the baseline;   one trick used was to ensure that the label distribution did notchange when new labeled data was added using co-training   instead of labeling entire unlabeled set, a cache was used forcomputational efficiency63   assume we are learning binary classifiers f and g for classlabels 0 and 1. f and g use two views.   a = p(f = 1, g = 1)      c = p(f = 1, g = 0)   b = p(f = 0, g = 0)      d = p(f = 0, g = 1)   assume that a * b > c * d + !   but when g agrees with f on the unlabeled data, f still doesnot know which label to predict   we assume we have a weak predictor h for f which uses thelabeled data to return the label   theorem (blum & mitchell, 1998):co-training64   highly confident labeled data points in the view 1 providerandomly scattered data points in the view 2 (conditionalindependence).   these highly confident labeled data points are noisy, howeverlearning in view 2 is immune to noise.view 1view 2figure from (zhu, 2005)intuition behind the theorem65   (nigam & ghani, 2000) consider the following tableof variants of em and id64   using webkb they use naive bayes in the foursettings shown in the tableincremental vs. iterative andfeature splitsemco-emiterativeself-trainingco-trainingincrementaldoes not usefeature splituses featuresplitmethod66   (nigam & ghani, 2000) experiments on documentclassification show that using a feature split helped on this taskand iterating over the unlabeled set also helped   the lowest error rate was for co-em (3.3) vs. co-training (3.7);self-training (5.8) and em (8.9) did far worse on this task   they also reported results on choosing conditionallyindependent feature splits vs. a random feature split   the theory behind co-training was borne out as the randomsplit did worse than the carefully chosen splitincremental vs. iterative andfeature splits67   a side effect of the co-training: agreement betweentwo views.   what if the agreement becomes the explicit goal?   intuitively, it helps by reducing the concept space inviews to the consistent concepts.   unlabeled data is used to check consistency   labeled data is used to locate the target concept in thereduced spacesagreement maximization68history of agreement maximization   (collins & singer, 1999)  suggest a variant of co-trainingwhere the agreement of the two classifiers is explicitlyoptimized. the algorithm is a variant of the adaboostalgorithm that considers agreement on unlabeled data as partof the objective function.   (dasgupta et al, 2001) provide bound on generalization errorof the learned classifier in co-training based on the empiricallymeasurable quantities. more analysis is done in (abney,2002).   it formalizes the agreement maximization suggested by(collins & singer, 1999)69history of agreement maximization   (leskes 2005) provides theoretical justification for agreementmaximization among multiple views and suggests the agreement boostalgorithm (belongs to the boosting family of algorithms)   (banko & brill, 2001) also provide a method to maximize agreementamong a large number of bagged classifiers and show good results onid147 when using upto 10 classifiers   early versions of the idea of using multiple views in learning fromunlabeled data occur in:   (becker and hinton, 1992; becker 1995)   (de sa, 1994)   for a discussion on these and other related cases, see the survey article:(seeger, 2000)70   em: maximize likelihood of unlabeled data   co-training: maximize agreement between two viewson unlabeled data   what can this second objective buy us?   theorem (dasgupta et. al., 2001): the agreementrate between two classifiers (under certain strictconditions) is an upper bound on the error of eitherclassifier   note: the classifiers are partial, i.e. they will say    i donot know    or % if they cannot predict the class(dasgupta et al, 2001, abney 2002)analysis of co-training71analysis of co-training   assume two views x1 and x2 used to build twoclassifiers f and g, respectively   standard assumption of co-training is viewindependence   p(x1 = x1 | x2 = x2, y = y) = p(x1 = x1 | y = y)   but this does not directly tell us about the agreementrate between f and g   a new assumption: classifier independence   p(f = u | g = v, y = y) = p(f = u | y = y)   if view independence holds then classifierindependence must hold as well72analysis of co-training   an additional assumption for non-trivial cases:   minu p(f = u) > p(f * g)   to keep things simple, let us consider only classifiersover 2 classes and there   s no    i don   t know    class %   if y is the true labeling and f and g are classifierswhich satisfy classifier independence and are non-trivial then either:   p(f * y) + p(f * g) or p(f    * y) + p(f * g)   since each example is one of two classes: we definef    to be the complement of f73p(f * y) + p(f * g) or p(f    * y) + p(f * g)   this is an upper bound on error: we assume we canpick between f and f    using the labeled data   co-training learns upto a permutation of predictedlabels: the correct labeling from this permutationrequires labeled data   the precision of a classifier f is p(y = u | f = u)   (abney, 2002) shows that classifier independenceimplies that if we know the precision of f then weknow precision of g   this can result in trivial agreement between f and ganalysis of co-training74(abney, 2002)attention shifts to the other classifiergreedy agreement algorithm   input: seed rules h1 and h2   loop:   for each atomic rule g   h2 = h2 + g   evaluate cost of (h1 , h2)   keep lowest cost h2   if h2 is worse than h2 then quit   swap h1 , h275     cost(h1 , h2) = # [ upper bound of errh2(h1)  +                                    upper bound of errh1(h2) ]     disagreement , =  p(h1  $    h2 | h1 , h2 $ %)      minor id203    = minu p(h1 = u | h1 $ %)cost of a pair of classifiersestimation of the error ofh1 by the help of the classifierin the other view h276more pac analysis   (balcan, blum & yang, 2004) try to relax the strongassumptions needed to theoretically analyze theperformance of co-training; they also heuristicallyanalyze the error propagation during co-training   (balcan & blum, 2005) provide a pac-style modelfor the analysis of learning from labeled andunlabeled data; and discuss the special case of co-training with linear separators77co-training experiments   (collins & singer, 1999) proposed the co-boosting algorithmand performed expts on the named entity classification task   natural feature split: spelling vs. contextual classifier   using only 7 simple seed rules, unlabeled data was used toachieve 83.1% accuracy;   although self-training seemed to perform just as well   (barzilay & mckeown, 2001) uses co-training to find lexicaland syntactic paraphrases   natural feature split: contextual vs. paraphrase classifier78co-training experiments   (pierce & cardie, 2001) proposed a feature split for noun-phrase chunking: one view was a left-context chunker whilethe other was a right-context chunker; used a nb learner   the learning curves for co-training was disappointing   there was no analysis of the feature split so it is hard to tell ifthe conditions for co-training were satisfied in this case   problems with noise entering into the labeled data79co-training experiments   (sarkar, 2001) applied co-training to statistical parsing   a feature split was obtained by using a statistical parser and aid48-based supertagger   both learners had to label sentences using the same set ofcomplex lexical descriptions to each word (trees from a tree-adjoining grammar)   some knowledge about the unseen trees (a grammar) wasassumed in the experiment due to the small size of the seed set   co-training outperformed simply using the labeled data80co-training experiments   (steedman et. al., 2003a) also applied co-training to statisticalparsing   a feature split was obtained by using one view as the collinsparser (a id18-based model) while the other view was an tree-adjoining grammar statistical parser; the two views wereexperimentally shown to be distinct   experiments showed improvement of 2.5% for co-training vs.a decrease of 0.1% for self-training with a 500 word seed set   the same experimental setup was used for id20:a small seed set of 100 wsj sentences was added to a larger(1k) set of brown corpus sentences and after co-training onwsj, the parsers were tested on wsj data   in this setting, co-training was able to improve f-score from75.4 to 78.281co-training experiments   (steedman et. al., 2003b) is about the method for selection ofexamples for each view for inclusion into the labeled data   this paper considers alternative methods for selecting suchexamples:   above-n: the score of an example for each view is greater than or equalto n   difference-n: score for an example of one view is greater than score ofthe other by some threshold n (difference-10% performed the best)   intersection-n: an example is in the bottom n percent of one view is inthe set of the other view   s n percent highest scoring sentences   the parameter n controls the amount of newly labeled data ineach iteration; can be used to deal with noise entering the data   as in many other co-training papers, an active learningcomponent was added to correct some of the co-traininglabeled examples added to the labeled set82co-training experiments   (m  ller et. al., 2002) apply co-training to the task ofco-reference resolution   base learners used were c4.5 id90   co-reference chains were split up into individualbinary classification decisions   it is not clear if there is a natural feature split thatcould be exploited in this setting   (mostly) negative results for co-training83co-training experiments   (callison-burch, 2002) proposed a co-training algorithm forid151   the idea is to use multiple languages a, b, c, d each of whichtranslate into english e   in addition, a, b, c and d are sentence aligned with eachother, so that if a sentence from c is found to be accuratelytranslated into english then the corresponding sentences in a,b, and d now have a new labeled parallel text   expts used the eu corpus and word error rate (wer)improvement was highest for german to english (2.5%)   noise injected into the labeled set was a problem when largeamounts of co-trained data was added84co-training experiments   (clark, curran & osborne, 2003) report on co-training exptsfor part-of-speech tagging   using two previously built taggers: tnt and a maxent tagger   performs an explicit greedy agreement based co-trainingalgorithm   naive co-training (using the whole cache)   a held-out set was used to measure agreement on addition ofnewly labeled example(s)   agreement-based selection is more picky and so can reducenoise in the newly labeled data   with small seed sets (~500) there was significantimprovement, but no significant improvement was seen withlarge seed sets (all treebank)85dealing with noise   one common issue that crops up in co-training exptsis the issue of noise when a large number of newlylabeled examples are added into the training set   (goldman & zhou, 2000) deal with this issue byusing hypothesis testing to check if each example ifadded to the labeled data is likely improve accuracyfor the classifier   (zhou & goldman, 2004) use an ensemble of three ormore classifiers to vote on whether a newly labeledexample should be added into the labeled set86dealing with noise   note that in the yarowsky algorithm, we can chooseto relabel any unlabeled example or even drop apreviously labeled example from u altogether (insome versions of the algorithm)   most research in co-training has focused on keepingthe best examples from the unlabeled data rather thanre-labeling previously seen examples   this is mainly due to the computational inefficiencycaused by re-labeling the entire unlabeled set in eachiteration87data based methods   from id64 methods we now move to methods thatuse some inherent geometry in the unlabeled data. we callthese methods: data based methods   for many of these methods we represent learning as trying tomaximize some objective function   this maximum is found using standard methods such asid119   a good objective function tries to minimize error on trainingdata using a id168   and tries to find a small model so that it can generalize and sothat it does not overfit: this is done by adding an additionalfactor to the function called id173.88id168   goal: finding a good classifier, i.e. one which has theminimum expected loss or risk   often we build classifiers based on functions   example of a 2-class classifier:               c(x) = 1            if   f(x) > 0               c(x) = -1           otherwise89id168   loss(c(x), y, x) : the penalty induced by assigningclass c(x) instead of the true class y to the instance x   0-1 loss:      loss(c(x) , y , x) = 1   if c(x) $ y                        loss(c(x) , y , x) = 0   otherwise   negative log loss :                        loss(c(x) , y , x) = - log p(y | c(x) , x)90id173   expected risk or loss:                    r = - loss(f(x) , y , x) p(x) dx   but often p(x) is unknown. moreover, y is not given for all x   the expected risk is upper-bounded by empirical riskplus a id173 term (or complexity term)91id173   prefers simple functions over complex ones      r  % !i loss(f(xi) , yi , xi) + " #(f)empirical risk  + id173 parameter * complexity term   goal : find f which minimizes the upper-boundexpression   often f is searched in a function class92   gaussian id173 for a log-linear model   w is the parameter vector, and .(x,y) is the feature vector   penalizes large weights w   bayesian interpretation of the gaussian id173: it puts agaussian prior over the weights w and combines it with the log-likelihood to compute the maximum of the posterior distribution.log likelihood of the log linear modelgaussianid173example93   what is the label?   knowing the geometry affects the answer.   geometry changes the notion of similarity.   assumption: data is distributed on some low dimensionalmanifold.   unlabeled data is used to estimate the geometry.+-?data manifold94smoothness assumption   desired functions are smooth with respect to theunderlying geometry.   functions of interest do not vary much in high densityregions or clusters.   example: the constant function is very smooth, however it has torespect the labeled data.   the probabilistic version:   conditional distributions p(y|x) should be smooth withrespect to the marginal p(x).   example: in a two class problem p(y=1|x) and p(y=2|x) do not varymuch in clusters.95the decisionboundary   cluster assumption: put the decision boundary in lowdensity area.   a consequence of the smoothness assumption. a smooth function96   let                    . penalty at              :   total penalty:   p(x) is unknown, so the above quantity isestimated by the help of unlabeled data:figure from(krishnapuram, et. al.,2005)wwhat is smooth? (belkin & niyogi)97data dependentid173   where:   h is the rkhs associated with kernel k(.,.)   combinatorial laplacian can be used for smoothness term:fitness to labeled datafunction complexity:prior belieid122oothness term:unlabeled data(belkin et al, 2004)manifold id17398return to ssl for structured    the representer theorem   the representer theorem guarantees the followingform for the solution of the optimization problem:99   data is modeled by a mixture of gaussians.   assumption: look at the mean of gaussian components,they are distributed on a low dimensional manifold.   maximize the objective function:      includes mean of the gaussians and more.           is the likelihood of the data.           is taken to be the combinatorial laplacian.   its interpretation is the energy of the current configuration of thegraph.(zhu & lafferty 2005)harmonic mixtures100   i(x,y) = 0   given the label is +, wecannot guess which (x,+)has been chosen(independent).++++++   i(x,y) = 1   given the label is +, we canguess which (x,+) has beenchosen.+++---   gives the amount of variation of y in a local region q: q qmutual information101   we are after a good conditional p(y|x).   belief: decision boundary lays in low density area.   p(y|x) must not vary so much in high density area.   cover the domain with local regions, the resultingmaximization problem is:(szummer & jaakkola 2002)information id173102   a two class problem (szummer&jaakkola)+-return to smoothnessexample103(ando & zhang 2005)more intuition on multi-task..predict classifier structure   often semi-supervised learning algorithms are notreliable   they improve the performance when the labeled data issmall but may degrade the performance when the labeleddata is large   this method does not have this deficiency   the usual approach is to consider a distance measurein the input space, enforce the smoothness offunctions w.r.t. the underlying geometry   but, what is a good metric?104   first learn the common predictive structure, and thenlearn the final classifier   example: classifiers are linear, and share parameter    :   generate several auxiliary problems from unlabeleddata, learn the corresponding classifiers, and discoverthe common structureprior belieffitness to labeled datarisk of the classifier, forthe linear example:structural learning105   parameters are found by optimizing the followingobjective function:common structureparameter vectorparameters of the auxiliary problemsrisk of theclassifiermore on multi-task   recall:example: linear classifiers106ssl for structured labels   generative model based (already covered with em):   hidden markov model (id48)   stochastic id18 (sid18)   discriminative model based:   co-hidden markov id88   semi-structured support vector machines (sid166)   co-structured id166 (co-sid166)   semi-kernel id49 (kcrf)107   example: part-of-speech tagging:         the representative put chairs on the table.   the input is a complex object as well as its label.   input-output pair (x,y) is composed of simple parts.   example: label-label and obs-label edges:observationdt              nn                     vbd       nns    in      dt      nnlabeldt            nnnn              vbdthedtnntableid170108figure from(tsochantaridis et al 2004)id170: parsing109   for a given x, consider the set of all its candidate labelings asyx   for a sentence x, consider all of the parse trees yx which have x at theirleaves   we are interested in a function   instead, learn a scoring function                               over theinput-output space   in general, decoding (doing the above argmax) is intractableexcept for special casesmore on scoring functionscoring function110   construct d-nearest neighbor graph on all parts seen in thesample.   for unlabeled data, put all parts for each candidate.   belief: f(.) is smooth on this graph (manifold).--at     nnat      thew(altun et al, 2005)more on discriminative structured   manifold of    simple parts   111ssl for structured labels:semi-kcrf and semi-sid166   the final maximization problem:   the representer theorem:   r(s) is all the simple parts of labeled and unlabeled instances inthe sample.   note that f(.) is related to                     .fitness to labeled datafunction complexity:prior belieid122oothness term:unlabeled datadata dependentid173112   plugging the form of the best function in theoptimization problem gives:   where q is a constant matrix.   by introducing slack variables     :subject tomodified problem113   id168:   id166:   crf:   note that an # vector gives the f(.) which in turn gives thescoring function s(x,y). we may write s#(x,y).hammingdistancesubject tomodified problem(cont   d)more ...114   we reviewed some important recent works on ssl:   classifier based methods   em, stable mixing of complete and incomplete information   self-training, the yarowsky algorithm, co-training   data based methods   manifold id173, harmonic mixtures, informationid173   learning predictive structure from multiple tasks   ssl for id170   em for id48s and pid18s   semi-kcrf, semi-sid166   co-sid166, co-hm id88generative modelsdiscriminative modelsconclusions115conclusions   different learning methods for ssl are based ondifferent assumptions   cluster assumption, view-independence assumption, ...   fulfilling these assumptions is crucial for the guaranteedsuccess of the methods   ssl for structured domains is an exciting area ofresearch   ssl is related to active learning and sampleselection methods116thank you117some preliminariesgenerative models:discriminative models:example of a generative model: naive bayesexample of a discriminative model: id166back ...118intuition in large-margin setting++__labeled data only++_right vs. wrong marginid166_++__training a support vector machine(id166), input x, label = {+,-}(zhang & oles, 2000)back to slide ...119graph mincutslabeled examples: +1(source)labeled examples: -1(sink)use edgeweights tocompute amin-cutback ...120   classifier independence   p(f = u | g = v, y = y) = p(f = u | y = y)   dy: deviation from classifier independence: dy = (1/2) ( |p(f = u | g = v, y = y) - p(f = u | y = y)|                 + |p(f = v | g = u, y = y) - p(f = v | y = y)| )   (abney, 2002) showed that instead of classifier independencewe can use a weaker version to show the same upper bound onerror:   p(f * y) + p(f * g) or p(f    * y) + p(f * g)weak classifier dependence121weak classifier dependence(abney, 2002)p1 = minu p(f = u | y = y)p2 = minu p(g = u | y = y)   by non-triviality, p1 andp2 must be > 0.5   q1 = 1 - p1   f and g are weaklydependent if:back ...122   there are several relevant learning tasks (assumption)    good classifiers are similar to each other, or share a commonpredictive structure.   example: without knowing the metric inherent to theinput space, a good classifier should have:   similar values at a,c,d   similar values at f,eexample from(ando & zhang 2005)classifier 1classifier 2classifier 3finding good structure123wu1u2line !: the shared structure among the classifiers (!1 u1 + !2  u2 = 0) each pointis a  classifierinduce a penalty only for the part orthogonal to the common structurelinear classifier:intuition in the classifier space124linear classifier:return to structural learningu2line !u1c1c2a       c       d       b       f       ec1a       c       d       b       f       ec2   key idea: classifiers close to the shared structure, partition(cluster) the input space roughly the sameclassifier space vs input space125return to structured label..structural learning for nlp   in named entity chunking task, unlabeled data can be used togenerate lots of auxiliary problems: predict the current wordbased on other features in the current position   auxiliary problem 1: predict word1   auxiliary problem 2: predict word2         (ando & zhang 2005) have done several experiments on textchunking and the results are promising.   it shows improvement on conll   00 syntactic chunking andconll   03 named entity chunking tasks compared with the bestprevious systems126   assume s(x,y) can be written as the sum of scores foreach simple part:   r(x,y) is the set of simple parts for (x,y).   tagging example: total score is the sum of scores oflabel-label and observation-label interactions (parts)   how to find f(.)?nn              vbdnntablereturn to manifold of partsscoring function127input-output feature space   .(x,y) maps an input-output pair to a fixeddimensional feature space rd  (d is the dimension)   parsing example:   here d is the number of rules in the grammarexample fromtsochantaridis et al 2004128hm id88   primal formulation:   dual formulation:   id81s can be used to compute the inner productswithout explicitly mapping points to the feature space   training:   if the prediction is     instead of  the correct      do thefollowing until convergencetrainingpoints(altun et al 2003, collins 2002)129ssl for structured labels:co-hm id88   we are looking for a good linear scoring functionwhich separates the correct label from wrong labels foreach training example   here there is not any notion of margin, just a separator!   in the training phase, each classifier uses the predictionof the other view for each unlabeled instance.130co-hm id88   in each view, use the prediction of the classifier in the other view for eachunlabeled instance   training in view 1:   labeled instance (xi,yi) has caused a mistake:   unlabeled instance xi has caused a disagreement:         is the prediction of the view 2.                       controls the influence of an unlabeled instance   training in view 2 is similar to the above(brefeld et al 2005)131experiments   (brefeld & scheffer 2006) applied co-sid166 to the namedentity recognition (ner) and parsing tasks.   (brefeld et al 2005) applied co-sid166 and co-hm id88to the id39 task.   in the above papers, random splitting of features into two viewsresults in a good performance for ner.   better performance of co-sid166 and co-hm id88compared to their single view counterparts comes at the cost oflonger training time   co-sid166 scales quadratically and co-hm id88 scales linearly inthe number of unlabeled instancesback ...132ssl for structured labels:co-structured id166   multi-view learning methods naturally allow theinclusion of unlabeled data in discriminative learning   in co-sid166, there are two views each of which has itsown way of converting input-output (x,y) to features:.1(x,y) and .2(x,y)   the scoring function is a linear model for each view:133ssl for structured labels:co-structured id166   in each view an sid166 is learnt, the goal is tomaximize the agreement between these two sid166s   the final scoring function:view 1view 2134structured id166 with 0/1 loss   optimization problem of sid166 with hard constraints:   optimization problem of sid166 with soft constraints:subject to :subject to :upper bound on empirical errorcomplexity term135sid166 with arbitrary losssubject to :id168   however in max-margin markov networks (m3n), themargin is rescaled: (taskar et al 2003)complexity termupper bound onempirical error(tsochantaridis et al 2004)136   agreement of the two sid166s on an unlabeled point isexpressed by:   we pretend the prediction of the sid166 in the other view iscorrect, and   expect the sid166 in the current view to produce the same labelwith enough confidence.measuring the agreementprediction of the other view137co-structured id166subject to :labeled :unlabeled :(brefeld et al 2006)back ...