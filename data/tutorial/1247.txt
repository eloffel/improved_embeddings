 

id31 and 

opinion mining 

april 22, 2012 

bing liu 

liub@cs.uic.edu 

draft: due to copyediting, the published version is slightly different 

bing  liu.  sentiment  analysis  and  opinion  mining,  morgan  & 
claypool publishers, may 2012.  

 

  

 

  id31 and opinion mining 

table of contents 

preface ..............................................................................................5  
id31: a fascinating problem ...................................7  
id31 applications ..........................................8  
1.1  
id31 research ..............................................10  
1.2  
1.2.1   different levels of analysis ......................................................... 10  
1.2.2  
sentiment lexicon and its issues ................................................. 12  
1.2.3   natural language processing issues ............................................. 13  
1.3   opinion spam detection .....................................................14  
1.4   what   s ahead ......................................................................14  
the problem of id31 ..............................................16  
2.1  
problem definitions ............................................................17  
2.1.1   opinion defintion ......................................................................... 17  
2.1.2  
id31 tasks ............................................................. 21  
2.2   opinion summarization ......................................................24  
2.3   different types of opinions ................................................25  
regular and comparative opinions .............................................. 25  
explicit and implicit opinions ...................................................... 26  
2.4  
subjectivity and emotion ....................................................27  
2.5   author and reader standing point ......................................29  
2.6  
summary .............................................................................29  
document sentiment classification ...............................................30  
3.1  
sentiment classification using supervised learning .........31  
sentiment classification using unsupervised learning .....34  
3.2  
3.3  
sentiment rating prediction ................................................36  
3.4   cross-domain sentiment classification .............................38  
3.5   cross-language sentiment classification ...........................41  
3.6  
summary .............................................................................43  
sentence subjectivity and sentiment classification ......................44  

2.3.1  
2.3.2  

2 
 

id31 and opinion mining 

5.4  
5.5  

subectivity classification ....................................................45  
4.1  
4.2  
sentence sentiment classification ......................................49  
4.3   dealing with conditional sentences ...................................51  
4.4   dealing with sarcastic sentences ........................................52  
4.5   cross-language subjectivity and sentiment classification .53  
4.6   using discourse information for sentiment classification 55  
4.7  
summary .............................................................................56  
aspect-based id31 ..................................................58  
5.1   aspect sentiment classification ..........................................59  
5.2   basic rules of opinions and id152 .....62  
5.3   aspect extraction ................................................................67  
5.3.1  
finding frequent nouns and noun phrases.................................. 68  
5.3.2   using opinion and target relations ............................................ 71  
5.3.3   using supervised learning........................................................... 71  
5.3.4   using topic models ..................................................................... 73  
5.3.5   mapping implicit aspects ............................................................ 77  
identifying resource usage aspect ....................................78  
simutaneous opinion lexicon expansion and aspect 
extraction ............................................................................79  
5.6   grouping aspects into categories .......................................81  
5.7   entity, opinion holder and time extraction ......................84  
5.8   coreference resolution and id51 .86  
5.9  
summary .............................................................................88  
sentiment lexicon generation ......................................................90  
6.1   dictionary-based approach .................................................91  
6.2   corpus-based approach ......................................................95  
6.3   desirable and undesirable facts .........................................99  
6.4  
summary ...........................................................................100  
opinion summarization ...............................................................102  
7.1   aspect-based opinion summarization ..............................102  
7.2  
improvements to aspect-based opinion summarization ..105  
7.3   contrastive view summarization .....................................107  
7.4   traditional summarization ................................................108  
7.5  
summary ...........................................................................108  

 

3 

 

  id31 and opinion mining 

analysis of comparative opinions ..............................................110  
problem definitions ..........................................................110  
8.1  
8.2  
identify comparative sentences ........................................113  
identifying preferred entities ............................................115  
8.3  
8.4  
summary ...........................................................................117  
opinion search and retrieval ......................................................118  
9.1   web search vs. opinion search ........................................118  
9.2   existing opinion retrieval techniques ............................119  
9.3  
summary ...........................................................................122  
opinion spam detection ..............................................................123  
10.1   types of spam and spamming ..........................................124  
10.1.1   harmful fake reviews ............................................................... 125  
10.1.2  
individual and group spamming ................................................ 125  
10.1.3   types of data, features and detection ....................................... 126  
10.2   supervised spam detection ...............................................127  
10.3   unsupervised spam detection ..........................................130  
10.3.1   spam detection based on atypical behaviors ............................ 130  
10.3.2   spam detection using review graph ........................................ 133  
10.4   group spam detection ......................................................134  
10.5   summary ...........................................................................135  
quality of reviews ......................................................................136  
11.1   quality as regression problem .........................................136  
11.2   other methods ...................................................................138  
11.3   summary ...........................................................................140  
concluding remarks ....................................................................141  
bibliography ................................................................................143  
 

4 
 

id31 and opinion mining 

preface 

opinions are central to almost all human activities and are key influencers of 
our  behaviors.  our  beliefs  and  perceptions  of  reality,  and  the  choices  we 
make, are, to a considerable degree, conditioned upon how others see and 
evaluate the world. for this reason, when we need to make a decision we 
often seek out the opinions of others. this is not only true for individuals but 
also true for organizations.  

opinions and its related concepts such as sentiments, evaluations, attitudes, 
and  emotions  are  the  subjects  of  study  of  sentiment  analysis  and  opinion 
mining. the inception and rapid growth of the field coincide with those of 
the social media on the web, e.g., reviews, forum discussions, blogs, micro-
blogs,  twitter,  and  social  networks,  because  for  the  first  time  in  human 
history,  we  have  a  huge  volume  of  opinionated  data  recorded  in  digital 
forms. since early 2000, id31 has grown to be one of the most 
active research areas in natural language processing. it is also widely studied 
in  data  mining,  web  mining,  and  text  mining.  in  fact,  it  has  spread  from 
computer  science  to  management  sciences  and  social  sciences  due  to  its 
importance  to  business  and  society  as  a  whole.  in  recent  years,  industrial 
activities  surrounding  sentiment  analysis  have  also  thrived.  numerous 
startups  have  emerged.  many  large  corporations  have  built  their  own  in-
house capabilities. id31 systems have found their applications 
in almost every business and social domain.  

the goal of this book is to give an in-depth introduction to this fascinating 
problem  and  to  present  a  comprehensive  survey  of  all  important  research 
topics and the latest developments in the field. as evidence of that, this book 
covers  more  than  400  references  from  all  major  conferences  and  journals. 
although  the  field  deals  with  the  natural  language  text,  which  is  often 
considered  the  unstructured  data,  this  book  takes  a  structured  approach  in 
introducing  the  problem  with  the  aim  of  bridging  the  unstructured  and 
structured  worlds  and  facilitating  qualitative  and  quantitative  analysis  of 
opinions. this is crucial for practical applications. in this book, i first define 
the problem in order to provide an abstraction or structure to the problem. 
from  the  abstraction,  we  will  naturally  see  its  key  sub-problems.  the 
subsequent  chapters  discuss  the  existing  techniques  for  solving  these  sub-
problems. 

this  book  is  suitable  for  students,  researchers,  and  practitioners  who  are 
interested  in  social  media  analysis  in  general  and  sentiment  analysis  in 
particular.  lecturers  can  readily  use  it  in  class  for  courses  on  natural 

 

5 

 

  id31 and opinion mining 

language  processing,  social  media  analysis,  text  mining,  and  data  mining. 
lecture slides are also available online. 

acknowledgements 
i  would  like  to  thank  my  former  and  current  students   zhiyuan  chen, 
xiaowen ding, geli fei, murthy ganapathibhotla, minqing hu, nitin jindal, 
huayi  li,  arjun  mukherjee,  guang  qiu  (visiting  student  from  zhejiang 
university), william underwood, andrea vaccari, zhongwu zhai (visiting 
student  from  tsinghua  university),  and  lei  zhang   for  contributing 
numerous research ideas over the years. discussions with many researchers 
also  helped  shape  the  book:  malu  g.  castellanos,  dennis  chong,  umesh 
dayal,  eduard  dragut,  riddhiman  ghosh,  natalie  glance,  meichun  hsu, 
jing  jiang,  birgit  k  nig,  xiaoli  li,  tieyun  qian,  gang  xu,  philip  s.  yu, 
clement  yu,  and  chengxiang  zhai.  i  am  also  very  grateful  to  two 
anonymous reviewers. despite their busy schedules, they read the book very 
carefully  and  gave  me  many  excellent  suggestions.  i  have  taken  each  and 
every  one  of  them  into  consideration  while  improving  this  book.  on  the 
publication side, i thank the editor, dr. graeme hirst, and the president and 
ceo  of  morgan  &  claypool  publishers,  mr.  michael  morgan,  who  have 
managed to get everything done on time and provided me with many pieces 
of  valuable  advice.  finally,  my  greatest  gratitude  goes  to  my  own  family: 
yue, shelley, and kate, who have helped in so many ways.  
 

 

6 
 

id31 and opinion mining 

chapter 1 

id31: a fascinating 
problem  

 
sentiment  analysis,  also  called  opinion  mining,  is  the  field  of  study  that 
analyzes  people   s  opinions,  sentiments,  evaluations,  appraisals,  attitudes, 
and  emotions  towards  entities  such  as  products,  services,  organizations, 
individuals, issues, events, topics, and their attributes. it represents a large 
problem space. there are also many names and slightly different tasks, e.g., 
sentiment  analysis,  opinion  mining,  opinion  extraction,  sentiment  mining, 
subjectivity  analysis,  affect  analysis,  emotion  analysis,  review  mining,  etc. 
however,  they  are  now  all  under  the  umbrella  of  sentiment  analysis  or 
opinion  mining.  while  in  industry,  the  term  sentiment  analysis  is  more 
commonly used, but in academia both id31 and opinion mining 
are frequently employed. they basically represent the same field of study. 
the  term  sentiment  analysis  perhaps  first  appeared  in  (nasukawa  and  yi, 
2003), and the term opinion mining first appeared in (dave, lawrence and 
pennock, 2003). however, the research on sentiments and opinions appeared 
earlier  (das  and  chen,  2001;  morinaga  et  al.,  2002;  pang,  lee  and 
vaithyanathan,  2002;  tong,  2001;  turney,  2002;  wiebe,  2000).  in  this 
terms  sentiment  analysis  and  opinion  mining 
book,  we  use 
interchangeably. to simplify the presentation, throughout this book we will 
use  the  term  opinion  to  denote  opinion,  sentiment,  evaluation,  appraisal, 
attitude, and emotion. however, these concepts are not equivalent. we will 
distinguish  them  when  needed.  the  meaning  of  opinion  itself  is  still  very 
broad.  sentiment  analysis  and  opinion  mining  mainly  focuses  on  opinions 
which express or imply positive or negative sentiments.  

the 

although  linguistics  and  natural  language  processing  (nlp)  have  a  long 
history, little research had been done about people   s opinions and sentiments 
before the year 2000. since then, the field has become a very active research 
area.  there  are  several  reasons  for  this.  first,  it  has  a  wide  arrange  of 
applications,  almost  in  every  domain.  the  industry  surrounding  sentiment 
analysis  has  also  flourished  due  to  the  proliferation  of  commercial 
applications.  this  provides  a  strong  motivation  for  research.  second,  it 
offers  many  challenging  research problems,  which  had  never  been studied 
before. this book will systematically define and discuss these problems, and 
describe the current state-of-the-art techniques for solving them. third, for 

 

7 

 

  id31 and opinion mining 

the first time in human history, we now have a huge volume of opinionated 
data  in  the  social  media  on  the  web.  without  this  data,  a  lot  of  research 
would not have been possible. not surprisingly, the inception and the rapid 
growth of id31 coincide with those of the social media. in fact, 
sentiment  analysis  is  now  right  at  the center  of  the  social  media  research. 
hence, research in id31 not only has an important impact on 
nlp,  but  may  also  have  a  profound  impact  on  management  sciences, 
political science, economics, and social sciences as they are all affected by 
people   s opinions. although the id31 research mainly started 
from  early  2000,  there  were  some  earlier  work  on  interpretation  of 
metaphors,  sentiment  adjectives,  subjectivity,  view  points,  and  affects 
(hatzivassiloglou and mckeown, 1997; hearst, 1992; wiebe, 1990; wiebe, 
1994; wiebe, bruce and o'hara, 1999). this book serves as an up-to-date 
and comprehensive introductory text, as well as a survey to the subject.  

1.1  id31 applications  

opinions  are  central  to  almost  all  human  activities  because  they  are  key 
influencers  of  our  behaviors.  whenever  we  need  to  make  a  decision,  we 
want  to  know  others     opinions.  in  the  real  world,  businesses  and 
organizations always want to find consumer or public opinions about their 
products and services. individual consumers also want to know the opinions 
of  existing  users  of  a  product  before  purchasing  it,  and  others     opinions 
about  political  candidates  before  making  a  voting  decision  in  a  political 
election.  in  the  past,  when  an  individual  needed  opinions,  he/she  asked 
friends  and  family.  when  an  organization  or  a  business  needed  public  or 
consumer  opinions,  it  conducted  surveys,  opinion  polls,  and  focus  groups. 
acquiring public and consumer opinions has long been a huge business itself 
for marketing, public relations, and political campaign companies.  

with the explosive growth of social media (e.g., reviews, forum discussions, 
blogs, micro-blogs, twitter, comments, and postings in social network sites) 
on the web, individuals and organizations are increasingly using the content 
in  these  media  for  decision  making.  nowadays,  if  one  wants  to  buy  a 
consumer  product,  one  is  no  longer  limited  to  asking  one   s  friends  and 
family for opinions because there are many user reviews and discussions in 
public forums on the web about the product. for an organization, it may no 
longer be necessary to conduct surveys, opinion polls, and focus groups in 
order  to  gather  public  opinions  because  there  is  an  abundance  of  such 
information  publicly  available.  however,  finding  and  monitoring  opinion 
sites on the web and distilling the information contained in them remains a 

8 
 

id31 and opinion mining 

formidable  task  because  of  the  proliferation  of  diverse  sites.  each  site 
typically  contains  a  huge  volume  of  opinion  text  that  is  not  always  easily 
deciphered in long blogs and forum postings. the average human reader will 
have difficulty identifying relevant sites and extracting and summarizing the 
opinions in them. automated id31 systems are thus needed.  

in recent years, we have witnessed that opinionated postings in social media 
have helped reshape businesses, and sway public sentiments and emotions, 
which have profoundly impacted on our social and political systems. such 
postings  have  also  mobilized  masses  for   political  changes  such  as  those 
happened in some arab countries in 2011. it has thus become a necessity to 
collect  and  study  opinions on the  web. of  course,  opinionated documents 
not  only  exist  on  the  web  (called  external  data),  many  organizations  also 
have their internal data, e.g., customer feedback collected from emails and 
call centers or results from surveys conducted by the organizations.  

due  to  these  applications,  industrial  activities  have  flourished  in  recent 
years. id31 applications have spread to almost every possible 
domain, from consumer products, services, healthcare, and financial services 
to  social  events  and  political  elections.  i  myself  have  implemented  a 
id31 system called opinion parser, and worked on projects in 
all these areas in a start-up company. there have been at least 40-60 start-up 
companies in the space in the usa alone. many big corporations have also 
built  their  own  in-house  capabilities,  e.g.,  microsoft,  google,  hewlett-
packard, sap, and sas. these practical applications and industrial interests 
have provided strong motivations for research in id31.  

apart from real-life applications, many application-oriented research papers 
have  also  been  published.  for  example,  in  (liu  et  al.,  2007),  a  sentiment 
model  was  proposed  to  predict  sales  performance.  in  (mcglohon,  glance 
and  reiter,  2010),  reviews  were  used  to  rank  products  and  merchants.  in 
(hong and skiena, 2010), the relationships between the nfl betting line and 
public  opinions  in  blogs  and  twitter  were  studied.  in  (o'connor  et  al., 
2010), twitter sentiment was linked with public opinion polls. in (tumasjan 
et al., 2010), twitter sentiment was also applied to predict election results. 
in (chen et al., 2010), the authors studied political standpoints. in (yano and 
smith,  2010),  a  method  was  reported  for  predicting  comment  volumes  of 
political blogs. in (asur and huberman, 2010; joshi et al., 2010; sadikov, 
parameswaran  and  venetis,  2009),  twitter  data,  movie  reviews  and  blogs 
were used to predict box-office revenues for movies. in (miller et al., 2011), 
sentiment  flow  in  social  networks  was  investigated.  in  (mohammad  and 
yang, 2011), sentiments in mails were used to find how genders differed on 
emotional axes. in (mohammad, 2011), emotions in novels and fairy tales 
were tracked. in (bollen, mao and zeng, 2011), twitter moods were used to 

 

9 

 

  id31 and opinion mining 

predict the stock market. in (bar-haim et al., 2011; feldman et al., 2011), 
expert  investors  in  microblogs  were  identified  and  sentiment  analysis  of 
stocks  was  performed.  in  (zhang  and  skiena,  2010),  blog  and  news 
sentiment was used to study trading strategies. in (sakunkoo and sakunkoo, 
2009), social influences in online book reviews were studied. in (groh and 
hauffa, 2011), id31 was used to characterize social relations. 
a comprehensive id31 system and some case studies were also 
reported in (castellanos et al., 2011). my own group has tracked opinions 
about  movies  on  twitter  and  predicted  box-office  revenues  with  very 
accurate  results.  we  simply  used  our  opinion  parser  system  to  analyze 
positive  and  negative  opinions  about  each  movie  with  no  additional 
algorithms.  

1.2  id31 research 

as  discussed  above,  pervasive  real-life  applications  are  only  part  of  the 
reason  why  sentiment  analysis  is  a  popular  research  problem.  it  is  also 
highly  challenging  as  a  nlp  research  topic,  and  covers  many  novel  sub-
problems as we will see later. additionally, there was little research before 
the year 2000 in either nlp or in linguistics. part of the reason is that before 
then there was little opinion text available in digital forms. since the year 
2000, the field has grown rapidly to become one of the most active research 
areas in nlp. it is also widely researched in data mining, web mining, and 
information  retrieval.  in  fact,  it  has  spread  from  computer  science  to 
management  sciences  (archak,  ghose  and  ipeirotis,  2007;  chen  and  xie, 
2008;  das  and  chen,  2007;  dellarocas,  zhang  and  awad,  2007;  ghose, 
ipeirotis  and  sundararajan,  2007;  hu,  pavlou  and  zhang,  2006;  park,  lee 
and han, 2007).  

1.2.1 

different levels of analysis  

i now give a brief introduction to the main research problems based on the 
level of granularities of the existing research. in general, id31 
has been investigated mainly at three levels: 

document level: the task at this level is to classify whether a whole opinion 
document  expresses  a  positive  or  negative  sentiment  (pang,  lee  and 
vaithyanathan,  2002;  turney,  2002).  for  example,  given  a  product 
review,  the  system  determines  whether  the  review  expresses  an  overall 
positive  or  negative  opinion  about  the  product.  this  task  is  commonly 

10 
 

id31 and opinion mining 

known as document-level sentiment classification. this level of analysis 
assumes that each document expresses opinions on a single entity (e.g., a 
single product). thus, it is not applicable to documents which evaluate or 
compare multiple entities.  

sentence level: the task at this level goes to the sentences and determines 
whether each sentence expressed a positive, negative, or neutral opinion. 
neutral usually means no opinion. this level of analysis is closely related 
to  subjectivity  classification  (wiebe,  bruce  and  o'hara,  1999),  which 
distinguishes  sentences  (called  objective  sentences)  that  express  factual 
information  from  sentences  (called  subjective  sentences)  that  express 
subjective views and opinions. however, we should note that subjectivity 
is  not  equivalent  to  sentiment  as  many  objective  sentences  can  imply 
opinions, e.g.,    we bought the car last month and the windshield wiper 
has fallen off.    researchers have also analyzed clauses (wilson, wiebe 
and hwa, 2004), but the clause level is still not enough, e.g.,    apple is 
doing very well in this lousy economy.     

entity  and  aspect  level:  both  the  document  level  and  the  sentence  level 
analyses  do  not  discover  what  exactly  people  liked  and  did  not  like. 
aspect  level  performs  finer-grained  analysis.  aspect  level  was  earlier 
called  feature  level  (feature-based  opinion  mining  and  summarization) 
(hu  and  liu,  2004).  instead  of  looking  at  language  constructs 
(documents,  paragraphs,  sentences,  clauses  or  phrases),  aspect  level 
directly looks at the opinion itself. it is based on the idea that an opinion 
consists of a sentiment (positive or negative) and a target (of opinion). 
an opinion without its target being identified is of limited use. realizing 
the importance of opinion targets also helps us understand the sentiment 
analysis  problem  better.  for  example,  although  the  sentence     although 
the  service  is  not  that  great,  i  still  love  this  restaurant     clearly  has  a 
positive tone, we cannot say that this sentence is entirely positive. in fact, 
the sentence is positive about the restaurant (emphasized), but negative 
about its service (not emphasized). in many applications, opinion targets 
are described by entities and/or their different aspects. thus, the goal of 
this  level  of  analysis  is  to  discover  sentiments  on  entities  and/or  their 
aspects. for example, the sentence    the iphone   s call quality is good, but 
its  battery  life  is  short     evaluates  two  aspects,  call  quality  and  battery 
life, of iphone (entity). the sentiment on iphone   s call quality is positive, 
but  the  sentiment  on  its  battery  life  is  negative.  the  call  quality  and 
battery  life  of  iphone  are  the  opinion  targets.  based  on  this  level  of 
analysis,  a  structured  summary  of  opinions  about  entities  and  their 
aspects can be produced, which turns unstructured text to structured data 
and  can  be  used  for  all  kinds  of  qualitative  and  quantitative  analyses. 
both  the  document  level  and  sentence  level  classifications  are  already 

 

11 

 

  id31 and opinion mining 

highly challenging. the aspect-level is even more difficult. it consists of 
several sub-problems, which we will discuss in chapters 2 and 5.  

to make things even more interesting and challenging, there are two types 
of opinions, i.e., regular opinions and comparative opinions (jindal and liu, 
2006b). a regular opinion expresses a sentiment only on an particular entity 
or an aspect of the entity, e.g.,    coke tastes very good,    which expresses a 
positive  sentiment  on  the  aspect  taste  of  coke.  a  comparative  opinion 
compares multiple entities based on some of their shared aspects, e.g.,    coke 
tastes  better  than  pepsi,     which  compares  coke  and  pepsi  based  on  their 
tastes (an aspect) and expresses a preference for coke (see chapter 8).  

1.2.2 

sentiment lexicon and its issues  

not surprisingly, the most important indicators of sentiments are sentiment 
words, also called opinion words. these are words that are commonly used 
to  express  positive  or  negative  sentiments.  for  example,  good,  wonderful, 
and amazing are positive sentiment words, and bad, poor, and terrible are 
negative  sentiment  words.  apart  from  individual  words,  there  are  also 
phrases and idioms, e.g., cost someone an arm and a leg. sentiment words 
and phrases are instrumental to id31 for obvious reasons. a list 
of such words and phrases is called a sentiment lexicon (or opinion lexicon). 
over the years, researchers have designed numerous algorithms to compile 
such lexicons. we will discuss these algorithms in chapter 6.  

although sentiment words and phrases are important for id31, 
only using them is far from sufficient. the problem is much more complex. 
in  other  words,  we  can  say  that  sentiment  lexicon  is  necessary  but  not 
sufficient for id31. below, we highlight several issues: 

1.  a positive or negative sentiment word may have opposite orientations in 
different  application  domains.  for  example,     suck     usually  indicates 
negative  sentiment,  e.g.,     this  camera  sucks,     but  it  can  also  imply 
positive sentiment, e.g.,    this vacuum cleaner really sucks.     

2.  a sentence containing sentiment words may not express any sentiment. 
this  phenomenon  happens  frequently  in  several  types  of  sentences. 
question  (interrogative)  sentences  and  conditional  sentences  are  two 
important  types,  e.g.,     can  you  tell  me  which  sony  camera  is  good?    
and    if i can find a good camera in the shop, i will buy it.    both these 
sentences  contain  the  sentiment  word     good   ,  but  neither  expresses  a 
positive or  negative  opinion  on  any  specific camera.  however, not  all 
conditional sentences or interrogative sentences express no sentiments, 
e.g.,    does anyone know how to repair this terrible printer    and    if you 

12 
 

id31 and opinion mining 

are looking for a good car, get toyota camry.    we will discuss such 
sentences in chapter 4. 

3.  sarcastic  sentences  with  or  without  sentiment  words  are  hard  to  deal 
with, e.g.,    what a great car! it stopped working in two days.    sarcasms 
are  not  so  common  in  consumer  reviews  about  products  and  services, 
but  are  very  common  in  political  discussions,  which  make  political 
opinions hard to deal with. we will discuss such sentences in chapter 4. 
4.  many  sentences  without  sentiment  words  can  also  imply  opinions. 
many of these sentences are actually objective sentences that are used to 
express some factual information. again, there are many types of such 
sentences. here we just give two examples. the sentence    this washer 
uses a lot of water    implies a negative sentiment about the washer since 
it  uses  a  lot  of  resource  (water).  the  sentence     after  sleeping  on  the 
mattress for two days, a valley has formed in the middle    expresses a 
negative  opinion  about  the  mattress.  this  sentence  is  objective  as  it 
states a fact. all these sentences have no sentiment words.   

these issues all present major challenges. in fact, these are just some of the 
difficult problems. more will be discussed in chapter 5.  

1.2.3 

natural language processing issues  

finally, we must not forget id31 is a nlp problem. it touches 
every  aspect  of  nlp,  e.g.,  coreference  resolution,  negation  handling,  and 
id51, which add more difficulties since these are not 
solved problems in nlp. however, it is also useful to realize that sentiment 
analysis  is  a  highly  restricted  nlp  problem  because  the  system  does  not 
need  to  fully  understand  the  semantics  of  each  sentence  or  document  but 
only  needs  to  understand  some  aspects  of  it,  i.e.,  positive  or  negative 
sentiments and their target entities or topics. in this sense, id31 
offers a great platform for nlp researchers to make tangible progresses on 
all fronts of nlp with the potential of making a huge practical impact. in 
this book, i will describe the core problems and the current state-of-the-art 
algorithms. i hope to use this book to attract researchers from other areas of 
nlp to join force to make a concerted effort to solve the problem.  
prior  to  this  book,  there  were  a  multi-author  volume     computing  attitude 
and affect in text: theory and applications    edited by shanahan, qu, and 
wiebe (2006), and also a survey article/book by pang and lee (2008). both 
books  have  excellent  contents.  however,  they  were  published  relatively 
early in the development of the field. since then, there have been significant 
advancements  due  to  much  more  active  research  in  the  past  5  years. 

 

13 

 

  id31 and opinion mining 

researchers  now  also  have  a  much  better  understanding  of  the  whole 
spectrum  of  the  problem,  its  structure,  and  core  issues.  numerous  new 
(formal) models and methods have been proposed. the research has not only 
deepened  but  also  broadened  significantly.  earlier  research  in  the  field 
mainly  focused  on  classifying  the  sentiment  or  subjectivity  expressed  in 
documents or sentences, which is insufficient for most real-life applications. 
practical applications often demand more in-depth and fine-grained analysis. 
due to the maturity of the field, the book is also written in a structured form 
in  the  sense  that  the  problem  is  now  better  defined  and  different  research 
directions are unified around the definition.    

1.3  opinion spam detection  

a key feature of social media is that it enables anyone from anywhere in the 
world to freely express his/her views and opinions without disclosing his/her 
true  identify  and  without  the  fear  of  undesirable  consequences.  these 
opinions are thus highly valuable. however, this anonymity also comes with 
a  price.  it  allows  people  with  hidden  agendas  or  malicious  intentions  to 
easily  game  the  system  to  give  people  the  impression  that  they  are 
independent members of the public and post fake opinions to promote or to 
discredit  target  products,  services,  organizations,  or  individuals  without 
disclosing  their  true  intentions,  or  the  person  or  organization  that  they  are 
secretly working for. such individuals are called opinion spammers and their 
activities are called opinion spamming (jindal and liu, 2008; jindal and liu, 
2007).  

opinion spamming has become a major issue. apart from individuals who 
give  fake  opinions  in  reviews  and  forum  discussions,  there  are  also 
commercial companies that are in the business of writing fake reviews and 
bogus blogs for their clients. several high profile cases of fake reviews have 
been reported in the news. it is important to detect such spamming activities 
to  ensure  that  the  opinions  on  the  web  are  a  trusted  source  of  valuable 
information.  unlike  extraction  of  positive  and  negative  opinions,  opinion 
spam  detection  is  not  just  a  nlp  problem  as  it  involves  the  analysis  of 
people   s posting behaviors. it is thus also a data mining problem. chapter 10 
will discuss the current state-of-the-art detection techniques.  

1.4  what   s ahead  

in this book, we explore this fascinating topic. although the book deals with 

14 
 

id31 and opinion mining 

the natural language text, which is often called unstructured data, i take a 
structured  approach  to  writing  this  book.  the  next  chapter  will  formally 
define the problem, which allows us to see a structure of the problem. from 
the  definition,  we  will  see  the  key  tasks  of  sentiment  analysis.  in  the 
subsequent  chapters,  existing  techniques  for  performing  the  tasks  are 
described.  due  to  my  research,  consulting,  and  start-up  experiences,  the 
book  not  only  discusses  key  research  concepts  but  also  looks  at  the 
technology from an application point of view in order to help practitioners in 
the  field.  however,  i  must  apologize  that  when  i  talk  about  industrial 
systems, i cannot reveal the names of companies or their systems, partially 
because of my consulting/business agreements and partially because of the 
fact that the id31 market moves rapidly and the companies that 
i know of may have changed or improved their algorithms when you read 
this book. i do not want to create problems for them and for me.    

although i try to cover all major ideas and techniques in this book, it has 
become an impossible task. in the past decade, a huge number of research 
papers  (probably  more  than  1000)  have  been  published  on  the  topic. 
although  most  papers  appeared  in  nlp  conferences  and  journals,  many 
papers  have  also  been  published  in  data  mining,  web  mining,  machine 
learning,  information  retrieval,  e-commerce,  management  sciences,  and 
many other fields. it is thus almost impossible to write a book that covers the 
ideas in every published paper. i am sorry if your good ideas or techniques 
are overlooked. however, a major advantage of publishing this book in the 
synthesis lecture series of morgan & claypool is that the authors can always 
add new or updated materials to the book because the printing is on demand. 
so  if  you  find  that  some  important  ideas  are  not  discussed,  please  do  not 
hesitate to let me know and i will be very happy to include.  

finally, background knowledge in the following areas will be very helpful in 
reading  this  book:  natural  language  processing  (indurkhya  and  damerau, 
2010;  manning  and  schutze,  1999),  machine  learning  (bishop,  2006; 
mitchell, 1997), data mining (liu, 2006 and 2011), and information retrieval 
(manning, raghavan and schutze, 2008).  

 

15 

 

  id31 and opinion mining 

chapter 2 

the problem of id31  

 
in this chapter, we define an abstraction of the id31 or opinion 
mining problem. from a research point of view, this abstraction gives us a 
statement of the problem and enables us to see a rich set of inter-related sub-
problems which make up the id31 problem. it is often said that 
if  we  cannot  structure  a  problem,  we  probably  do  not  understand  the 
problem. the objective of the definitions is thus to abstract a structure from 
the complex and intimidating unstructured natural language text. they also 
serve as a common framework to unify various existing research directions, 
and  to  enable  researchers  to  design  more  robust  and  accurate  solution 
techniques by exploiting the inter-relationships of the sub-problems. from a 
practical application point of view, the definitions let practitioners see what 
sub-problems need to be solved in a practical system, how they are related, 
and what output should be produced.  

unlike  factual  information,  opinions  and  sentiments  have  an  important 
characteristic, namely, they are subjective. it is thus important to examine a 
collection of opinions from many people rather than only a single opinion 
from one person because such an opinion represents only the subjective view 
of that single person, which is usually not sufficient for application. due to a 
large collection of opinions on the web, some form of summary of opinions 
is  needed  (hu  and  liu,  2004).  the  problem  definitions  state  what kind  of 
summary  may  be  desired. along  with  the  problem  definitions,  the  chapter 
will also discuss several related concepts such as subjectivity and emotion. 

note  that  throughout  this  chapter  and  also  the  whole  book,  i  mainly  use 
reviews and sentences from reviews as examples to introduce ideas and to 
define key concepts, but the ideas and the resulting definitions are general 
and applicable to all forms of formal and informal opinion text such as news 
articles, tweets (twitter postings), forum discussions, blogs, and facebook 
postings.  since  product  reviews  are  highly  focused  and  opinion  rich,  they 
allow  us  to  see  different  issues  more  clearly  than  from  other  forms  of 
opinion  text.  conceptually,  there  is  no  difference  between  them.  the 
differences are mainly superficial and in the degree of difficulty in dealing 
with  them.  for  example,  twitter  postings  (tweets)  are  short  (at  most  140 
characters)  and  informal,  and  use  many  internet  slangs  and  emoticons. 
twitter postings are, in fact, easier to analyze due to the length limit because 

16 
 

id31 and opinion mining 

the authors are usually straight to the point. thus, it is often easier to achieve 
high id31 accuracy. reviews are also easier because they are 
highly  focused  with  little  irrelevant  information.  forum  discussions  are 
perhaps the hardest to deal with because the users there can discuss anything 
and also interact with one another. in terms of the degree of difficulty, there 
is  also  the  dimension  of  different  application  domains.  opinions  about 
products  and  services  are  usually  easier  to  analyze.  social  and  political 
discussions  are  much  harder  due 
topic  and  sentiment 
expressions, sarcasms and ironies.  

to  complex 

2.1 

problem definitions  

as  mentioned  at  the  beginning  of  chapter  1,  sentiment  analysis  mainly 
studies  opinions  which  express  or  imply  positive  or  negative  sentiments. 
this section thus defines the problem in this context.  

2.1.1 

opinion defintion 

we use the following review about a canon camera to introduce the problem 
(an id number is associated with each sentence for easy reference): 

date: september 10, 2011 
posted by: john smith 
   (1) i bought a canon g12 camera six months ago. (2) i simply love 
it. (3) the picture quality is amazing. (4) the battery life is also long. 
(5) however, my wife thinks it is too heavy for her.     

from this review, we notice a few important points: 

1.  the review has a number of opinions, both positive and negative, about 
canon g12 camera. sentence (2) expresses a positive opinion about the 
canon  camera  as  a  whole.  sentence  (3)  expresses  a  positive  opinion 
about  its  picture  equality.  sentence  (4)  expresses  a  positive  opinion 
about  its  battery  life.  sentence  (5)  expresses  a  negative  opinion  about 
the  weight  of  the  camera.  from  these  opinions,  we  can  make  the 
following important observation:  

observation:  an  opinion  consists  of  two  key  components:  a  target  g 

and a sentiment s on the target, i.e.,  

(g, s),  

where  g  can  be  any  entity  or  aspect  of  the  entity  about  which  an 
opinion has been expressed, and s is a positive, negative, or neutral 
sentiment, or a numeric rating score expressing the strength/intensity 

 

17 

 

  id31 and opinion mining 

of the sentiment (e.g., 1 to 5 stars). positive, negative and neutral are 
called sentiment (or opinion) orientations (or polarities).    

for example, the target of the opinion in sentence (2) is canon g12, and 
the target of the opinion in sentence (3) is the picture quality of canon 
g12. target is also called topic in the literature.  

2.  this  review  has  opinions  from  two  persons,  which  are  called  opinion 
sources or opinion holders (kim and hovy, 2004; wiebe, wilson and 
cardie, 2005). the holder of the opinions in sentences (2), (3), and (4) is 
the author of the review (   john smith   ), but for sentence (5), it is the 
wife of the author.  

3.  the date of the review is september 10, 2011. this date is important in 
practice  because  one  often  wants  to  know  how  opinions  change  with 
time and opinion trends.   

we are now ready to define opinion as a quadruple.  

definition (opinion): an opinion is a quadruple,  

 

(g, s, h, t), 

where g is the opinion (or sentiment) target, s is the sentiment about the 
target, h is the opinion holder and t is the time when the opinion was 
expressed.  

this definition, although quite concise, may not be easy to use in practice 
especially in the domain of online reviews of products, services, and brands 
because the full description of the target can be complex and may not even 
appear in the same sentence. for example, in sentence (3), the opinion target 
is actually    picture quality of canon g12   , but the sentence mentioned only 
   picture quality   . in this case, the opinion target is not just    picture quality    
because without knowing that the sentence is evaluating the picture quality of 
the canon g12 camera, the opinion in sentence (3) alone is of little use. in 
practice, the target can often be decomposed and described in a structured 
manner with multiple levels, which greatly facilitate both mining of opinions 
and later use of the mined opinion results. for example,    picture quality of 
canon g12    can be decomposed into an entity and an attribute of the entity 
and represented as a pair, 

(cannon-g12, picture-quality)   

let us use the term entity to denote the target object that has been evaluated. 
entity can be defined as follows (hu and liu, 2004; liu, 2006 and 2011). 

definition  (entity):  an  entity  e  is  a  product,  service,  topic,  issue,  person, 
organization, or event. it is described with a pair, e: (t, w), where t is a 
hierarchy of parts, sub-parts, and so on, and w is a set of attributes of e. 

18 
 

id31 and opinion mining 

each part or sub-part also has its own set of attributes.  

example 1: a particular model of camera is an entity, e.g., canon g12. it 
has a set of attributes, e.g., picture quality, size, and weight, and a set of 
parts, e.g., lens, viewfinder, and battery. battery also has its own set of 
attributes, e.g., battery life and battery weight. a topic can be an entity 
too,  e.g.,  tax  increase,  with  its  parts     tax  increase  for  the  poor,        tax 
increase for the middle class    and    tax increase for the rich.    

this  definition  essentially  describes  a  hierarchical  decomposition  of  entity 
based on the part-of relation. the root node is the name of the entity, e.g., 
canon g12 in the above review. all the other nodes are parts and sub-parts, 
etc. an opinion can be expressed on any node and any attribute of the node.  
example 2: in our example review above, sentence (2) expresses a positive 
opinion  about  the  entity  canon  g12  camera  as  a  whole.  sentence  (3) 
expresses  a  positive  opinion  on  the  attribute  of  picture  quality  of  the 
camera. clearly, one can also express opinions about parts or components 
of the camera.  

this entity as a hierarchy of any number of levels needs a nested relation to 
represent it, which is often too complex for applications. the main reason is 
that since nlp is a very difficult task, recognizing parts and attributes of an 
entity at different levels of details is extremely hard. most applications also 
do not need such a complex analysis. thus, we simplify the hierarchy to two 
levels  and  use  the  term  aspects  to  denote  both  parts  and  attributes.  in  the 
simplified  tree,  the  root  node  is  still  the  entity  itself,  but  the  second  level 
(also the leaf level) nodes are different aspects of the entity. this simplified 
framework is what is typically used in practical id31 systems.  

note  that  in  the  research  literature,  entities  are  also  called  objects,  and 
aspects are also called features (as in product features). however, features 
here  can  confuse  with  features  used  in  machine  learning,  where  a  feature 
means  a  data  attribute.  to  avoid  confusion,  aspects  have  become  more 
popular in recent years. note that some researchers also use the terms facets, 
attributes and topics, and in specific applications, entities and aspects may 
also be called other names based on the application domain conventions.  

after decomposing the opinion target, we can redefine an opinion (hu and 
liu, 2004; liu, 2010). 

definition (opinion): an opinion is a quintuple,  

(ei, aij, sijkl, hk, tl),  

where ei is the name of an entity, aij is an aspect of ei, sijkl is the sentiment 
on aspect aij of entity ei, hk is the opinion holder, and tl is the time when 
the opinion is expressed by hk. the sentiment sijkl is positive, negative, or 

 

19 

 

  id31 and opinion mining 

neutral, or expressed with different strength/intensity levels, e.g., 1 to 5 
stars as used by most review sits on the web. when an opinion is on the 
entity itself as a whole, the special aspect general is used to denote it. 
here, ei and aij together represent the opinion target.  

some important remarks about this definition are in order: 

1. 

in this definition, we purposely use subscripts to emphasize that the five 
pieces of information in the quintuple must correspond to one another. 
that is, the opinion sijkl must be given by opinion holder hk about aspect 
aij of entity ei at time tl. any mismatch is an error.  

2.  the five components are essential. missing any of them is problematic 
in general. for example, if we do not have the time component, we will 
not be able to analyze opinions on an entity according to time, which is 
often very important in practice because an opinion two years ago and 
an  opinion  yesterday  is  not  the  same.  without  opinion  holder  is  also 
problematic.  for  example,  in  the  sentence     the  mayor  is  loved  by  the 
people in the city, but he has been criticized by the state government,    
the two opinion holders,    people in the city    and    state government,    are 
clearly important for applications.  

3.  the  definition  covers  most  but  not  all  possible  facets  of  the  semantic 
meaning of an opinion, which can be arbitrarily complex. for example, 
it does not cover the situation in    the view finder and the lens are too 
close,    which expresses an opinion on the distance of two parts. it also 
does not cover the context of the opinion, e g.,    this car is too small for 
a  tall  person,     which  does  not  say  the  car  is  too  small  for  everyone. 
   tall person    is the context here. note also that in the original definition 
of entity, it is a hierarchy of parts, sub-parts, and so on. every part can 
have  its  set  of  attributes.  due  to  the  simplification,  the  quintuple 
representation  can  result  in  information  loss.  for  example,     ink     is  a 
part/component of a printer. in a printer review, one wrote    the ink of 
this printer is expensive.    this does not say that the printer is expensive 
(which  indicates  the  aspect  price).  if  one  does  not  care  about  any 
attribute of the ink, this sentence just gives a negative opinion to the ink, 
which is an aspect of the printer entity. however, if one also wants to 
study opinions about different aspects of the ink, e.g., price and quality, 
the  ink  needs  to  be  treated  as  a  separate  entity.  then,  the  quintuple 
representation  still  applies,  but  the  part-of  relationship  needs  to  be 
saved. of course, conceptually we can also expand the representation of 
opinion  target  using  a  nested  relation.  despite  the  limitations,  the 
definition  does  cover  the  essential  information  of  an  opinion  which  is 
sufficient for most applications. as we mentioned above, too complex a 
definition can make the problem extremely difficult to solve. 

20 
 

id31 and opinion mining 

4.  this definition provides a framework to transform unstructured text to 
structured  data.  the  quintuple  above  is  basically  a  database  schema, 
based on which the extracted opinions can be put into a database table. 
then  a  rich  set  of  qualitative,  quantitative,  and  trend  analyses  of 
opinions  can  be  performed  using  the  whole  suite  of  database 
management systems (dbms) and olap tools.  

5.  the  opinion  defined  here  is  just  one  type  of  opinion,  called  regular 
opinion.  another  type  is  comparative  opinion  (jindal  and  liu,  2006b; 
liu, 2006 and 2011), which needs a different definition. section 2.3 will 
discuss  different  types  of  opinions.  chapter  8  defines  and  analyzes 
comparative  opinions.  for  the  rest  of  this  section,  we  only  focus  on 
regular opinions. for simplicity, we just called them opinions.  

2.1.2 

id31 tasks 

with the definition, we can now present the objective and the key tasks of 
id31 (liu, 2010; liu, 2006 and 2011).  

objective of id31: given an opinion document d, discover all 

opinion quintuples (ei, aij, sijkl, hk, tl) in d. 

the key tasks are derived from the 5 components of the quintuple. the first 
component  is  the  entity.  that  is,  we  need  to  extract  entities.  the  task  is 
similar to id39 (ner) in information extraction (hobbs 
and riloff, 2010; mooney and bunescu, 2005; sarawagi, 2008). thus, the 
extraction itself is a problem. after extraction, we also need to categorize the 
extracted entities. in natural language text, people often write the same entity 
in different ways. for example, motorola may be written as mot, moto, and 
motorola. we need to recognize that they all refer to the same entity. 

definition  (entity  category  and  entity  expression):  an  entity  category 
represents a unique entity, while an entity expression is an actual word or 
phrase that appears in the text indicating an entity category.  

each  entity  category  (or  simply  entity)  should  have  a  unique  name  in  a 
particular application. the process of grouping entity expressions into entity 
categories is called entity categorization.  

now we look at aspects of entities. the problem is basically the same as for 
entities.  for  example,  picture,  image,  and  photo  are  the  same  aspect  for 
cameras. we thus need to extract aspect expressions and categorize them.   

definition (aspect category and aspect expression): an aspect category of 
an  entity  represents  a  unique  aspect  of  the  entity,  while  an  aspect 

 

21 

 

  id31 and opinion mining 

expression is an actual word or phrase that appears in the text indicating 
an aspect category.  

each aspect category (or simply aspect) should also have a unique name in a 
particular  application.  the  process  of  grouping  aspect  expressions  into 
aspect categories (aspects) is called aspect categorization.   

aspect  expressions  are  usually  nouns  and  noun  phrases  but  can  also  be 
verbs,  verb phrases, adjectives, and adverbs.  the following definitions  are 
useful (hu and liu, 2004).  

definition (explicit aspect expression): aspect expressions that are nouns 

and noun phrases are called explicit aspect expressions.  

for  example,     picture  quality     in     the  picture  quality  of  this  camera  is 
great    is an explicit aspect expression.  

definition  (implicit  aspect  expression):  aspect  expressions  that  are  not 

nouns or noun phrases are called implicit aspect expressions.  

for example,    expensive    is an implicit aspect expression in    this camera is 
expensive.    it implies the aspect price. many implicit aspect expressions are 
adjectives  and  adverbs  that  are  used  to  describe  or  qualify  some  specific 
aspects,  e.g., expensive  (price),  and  reliably  (reliability).  they  can  also  be 
verb  and  verb  phrases,  e.g.,     i  can  install  the  software  easily.        install    
indicates  the  aspect  installation.  implicit  aspect  expressions  are  not  just 
adjectives, adverbs, verbs and verb phrases; they can also be very complex, 
e.g.,    this camera will not easily fit in a coat pocket.    here,    fit in a coat 
pocket    indicates the aspect size (and/or shape). 

the  third  component  in  the  opinion  definition  is  the  sentiment.  this  task 
classifies whether the sentiment on the aspect is positive, negative or neutral. 
the  fourth  component  and  fifth  components  are  opinion  holder  and  time 
respectively. they also need to be extracted and categorized as for entities 
and aspects. note that an opinion holder (bethard et al., 2004; choi et al., 
2005; kim and hovy, 2004) (also called opinion source in (wiebe, wilson 
and  cardie,  2005))  can  be  a  person  or  organization  who  expressed  an 
opinion.  for  product  reviews  and  blogs,  opinion  holders  are  usually  the 
authors of the postings. opinion holders are more important for news articles 
as they often explicitly state the person or organization that holds an opinion. 
however, in some cases, identifying opinion holders can also be important 
in  social  media,  e.g.,  identifying  opinions  from  advertisers  or  people  who 
quote advertisements of companies.   

based on the above discussions, we can define a model of entity and a model 
of opinion document (liu, 2006 and 2011).  

22 
 

id31 and opinion mining 

model of entity: an entity ei is represented by itself as a whole and a finite 
set of aspects ai = {ai1, ai2,    , ain}. ei can be expressed with any one of a 
finite set of its entity expressions {eei1, eei2,    , eeis}. each aspect aij     ai 
of  entity  ei  can  be  expressed  with  any  one  of  its  finite  set  of  aspect 
expressions {aeij1, aeij2,    , aeijm}.  

model of opinion document: an opinion document d contains opinions on 
a set of entities {e1, e2,    , er} and a subset of their aspects from a set of 
opinion holders {h1, h2,    , hp} at some particular time point.  

finally,  to  summarize,  given  a  set  of  opinion  documents  d,  sentiment 
analysis consists of the following 6 main tasks.  

task 1 (entity extraction and categorization): extract all entity expressions 
in d, and categorize or group synonymous entity expressions into entity 
clusters (or categories). each entity expression cluster indicates a unique 
entity ei. 

task 2 (aspect extraction and categorization): extract all aspect expressions 
of the entities, and categorize these aspect expressions into clusters. each 
aspect expression cluster of entity ei represents a unique aspect aij.  

task  3  (opinion  holder  extraction  and  categorization):  extract  opinion 
holders  for  opinions  from  text  or  structured  data  and  categorize  them. 
the task is analogous to the above two tasks.  

task  4  (time  extraction  and  standardization):  extract  the  times  when 
opinions  are  given  and  standardize  different  time  formats.  the  task  is 
also analogous to the above tasks.  

task 5 (aspect sentiment classification): determine whether an opinion on 
an  aspect  aij  is  positive,  negative  or  neutral,  or  assign  a  numeric 
sentiment rating to the aspect.  

task  6  (opinion  quintuple  generation):  produce  all  opinion  quintuples  (ei, 
aij, sijkl, hk, tl) expressed in document d based on the results of the above 
tasks. this task is seemingly very simple but it is in fact very difficult in 
many cases as example 4 below shows.   

sentiment  analysis  (or  opinion  mining)  based  on  this  framework  is  often 
called aspect-based id31 (or opinion mining), or feature-based 
id31 (or opinion mining) as it was called in (hu and liu, 2004; 
liu, hu and cheng, 2005).  

we now use an example blog to illustrate the tasks (a sentence id is again 
associated with each sentence) and the analysis results.  

example 4:  

posted by: bigjohn  

date: sept. 15, 2011  

(1)  i  bought  a  samsung  camera  and  my  friends  brought  a  canon 
camera yesterday. (2) in the past week, we both used the cameras a 
lot. (3) the photos from my samy are not that great, and the battery 

 

23 

 

  id31 and opinion mining 

life is short too. (4) my friend was very happy with his camera and 
loves  its  picture  quality.  (5)  i  want  a  camera  that  can  take  good 
photos. (6) i am going to return it tomorrow. 

task  1  should  extract  the  entity  expressions,     samsung,        samy,     and 
   canon,    and group    samsung    and    samy    together as they represent the 
same entity. task 2 should extract aspect expressions    picture,       photo,    and 
   battery life,    and group    picture    and    photo    together as for cameras they 
are synonyms. task 3 should find the holder of the opinions in sentence (3) 
to be bigjohn (the blog author) and the holder of the opinions in sentence (4) 
to be bigjohn   s friend. task 4 should also find the time when the blog was 
posted is sept-15-2011. task 5 should find that sentence (3) gives a negative 
opinion  to  the  picture  quality  of  the  samsung  camera  and  also  a  negative 
opinion to its battery life. sentence (4) gives a positive opinion to the canon 
camera  as  a  whole  and  also  to  its  picture  quality.  sentence  (5)  seemingly 
expresses a positive opinion, but it does not. to generate opinion quintuples 
for sentence (4) we need to know what    his camera    and    its    refer to. task 
6 should finally generate the following four opinion quintuples: 
(samsung, picture_quality, negative, bigjohn, sept-15-2011) 
(samsung, battery_life, negative, bigjohn, sept-15-2011) 
(canon, general, positive, bigjohn   s_friend, sept-15-2011) 
(canon, picture_quality, positive, bigjohn   s_friend, sept-15-2011) 

2.2  opinion summarization  

unlike factual information, opinions are essentially subjective. one opinion 
from  a  single  opinion  holder  is  usually  not  sufficient  for  action.  in  most 
applications, one needs to analyze opinions from a large number of people. 
this indicates that some form of summary of opinions is desired. although 
an opinion summary can be in one of many forms, e.g., structured summary 
(see below) or short text summary, the key components of a summary should 
include  opinions  about  different  entities  and  their  aspects  and  should  also 
have  a  quantitative  perspective.  the  quantitative  perspective  is  especially 
important because 20% of the people being positive about a product is very 
different from 80% of the people being positive about the product. we will 
discuss this further in chapter 7.  

the  opinion  quintuple  defined  above  actually  provides  a  good  source  of 
information  and  also  a  framework  for  generating  both  qualitative  and 
quantitative  summaries.  a  common  form  of  summary  is  based  on  aspects 
and  is  called  aspect-based  opinion  summary  (or  feature-based  opinion 
summary)  (hu  and  liu,  2004;  liu,  hu  and  cheng,  2005).  in  the  past  few 

24 
 

id31 and opinion mining 

105 
12 

aspect: picture quality 

 
 

aspect: general 

positive:  
negative:  

  digital camera 1:  
 
 
 
 
 
 
 
 
 
 

positive:  
 negative:  

positive:  
negative:  

95 
10 
aspect: battery life 

    

 
 

 
 

<individual review sentences> 
<individual review sentences> 

<individual review sentences> 
<individual review sentences> 

50        <individual review sentences> 
9 
<individual review sentences> 

figure 2.1. an aspect-based opinion summary. 

years, a significant amount of research has been done on opinion summary. 
most of them are related to this framework (see chapter 7).  

let  us  use  an  example  to  illustrate  this  form  of  summary,  which  was 
proposed in (hu and liu, 2004; liu, hu and cheng, 2005) . we summarize a 
set  of  reviews  of  a  digital  camera,  called  digital  camera  1.  the  summary 
looks  like  that  in  figure  2.1,  which  is  called  a  structured  summary  in 
contrast to a traditional text summary of a short document generated from 
one  or  multiple  long  documents.  in  the  figure,  general  represents  the 
camera itself (the entity). 105 reviews expressed positive opinions about the 
camera and 12 expressed negative opinions. picture quality and battery life 
are  two  camera  aspects.  95  reviews  expressed  positive  opinions  about  the 
picture  quality,  and  10  expressed  negative  opinions.  <individual  review 
sentences> is a link pointing to the sentences and/or the whole reviews that 
give  the  opinions.  with  such  a  summary,  one  can  easily  see  how  existing 
customers feel about the camera. if one is interested in a particular aspect 
and additional  details,  he/she  can  drill  down  by  following the <individual 
review sentences> link to see the actual opinion sentences or reviews. 

2.3 

different types of opinions  

the type of opinions that we have discussed so far is called regular opinion 
(liu,  2006  and  2011).  another  type  is  called  comparative  opinion  (jindal 
and liu, 2006b). in fact, we can also classify opinions based on how they are 
expressed in text, explicit opinion and implicit (or implied) opinion.  

2.3.1 

regular and comparative opinions 

regular  opinion:  a  regular  opinion  is  often  referred  to  simply  as  an 

 

25 

 

  id31 and opinion mining 

opinion in the literature and it has two main sub-types (liu, 2006 and 2011):  
direct opinion: a direct opinion refers to an opinion expressed directly 

on an entity or an entity aspect, e.g.,    the picture quality is great.    

indirect  opinion:  an  indirect  opinion  is  an  opinion  that  is  expressed 
indirectly  on an  entity  or aspect  of  an entity  based on  its  effects  on 
some other entities. this sub-type often occurs in the medical domain. 
for example, the sentence    after injection of the drug, my joints felt 
worse     describes  an  undesirable  effect  of  the  drug  on     my  joints   , 
which indirectly gives a negative opinion or sentiment to the drug. in 
the case, the entity is the drug and the aspect is the effect on joints.   

much of the current research focuses on direct opinions. they are simpler 
to handle. indirect opinions are often harder to deal with. for example, in 
the  drug  domain,  one  needs  to  know  whether  some  desirable  and 
undesirable  state  is  before  or  after  using  the  drug.  for  example,  the 
sentence    since my joints were painful, my doctor put me on this drug    
does  not  express  a  sentiment  or  opinion  on  the  drug  because     painful 
joints    (which is negative) happened before using the drug.   

comparative  opinion:  a  comparative  opinion  expresses  a  relation  of 
similarities  or  differences  between  two  or  more  entities  and/or  a 
preference  of  the  opinion  holder  based  on  some  shared  aspects  of  the 
entities (jindal and liu, 2006a; jindal and liu, 2006b). for example, the 
sentences,     coke  tastes  better  than  pepsi     and     coke  tastes  the  best    
express  two  comparative  opinions.  a  comparative  opinion  is  usually 
expressed using the comparative or superlative  form  of  an adjective  or 
adverb,  although  not  always  (e.g.,  prefer).  comparative  opinions  also 
have many types. we will discuss and define them in chapter 8.  

2.3.2 

explicit and implicit opinions 

explicit opinion: an explicit opinion is a subjective statement that gives a 

regular or comparative opinion, e.g.,  
 
 

   coke tastes great,    and  
   coke tastes better than pepsi.     

 
 
implicit (or implied) opinion: an implicit opinion is an objective statement 
that  implies  a  regular  or  comparative  opinion.  such  an  objective 
statement usually expresses a desirable or undesirable fact, e.g., 
 
 

   i bought the mattress a week ago, and a valley has formed,    and 
   the battery life of nokia phones is longer than samsung phones.    

 
 

explicit opinions are easier to detect and to classify than implicit opinions. 
much  of  the  current  research  has  focused  on  explicit  opinions.  relatively 

26 
 

id31 and opinion mining 

less work has been done on implicit opinions (zhang and liu, 2011b).  in a 
slightly different direction, (greene and resnik, 2009) studied the influence 
of syntactic choices on perceptions of implicit sentiment. for example, for 
the same story, different headlines can imply different sentiments.  

2.4 

subjectivity and emotion  

there are two important concepts that are closely related to sentiment and 
opinion, i.e., subjectivity and emotion.  

definition  (sentence  subjectivity):  an  objective  sentence  presents  some 
factual  information  about  the  world,  while  a  subjective  sentence 
expresses some personal feelings, views, or beliefs.  

an example objective sentence is    iphone is an apple product.    an example 
subjective sentence is    i like iphone.    subjective expressions come in many 
forms,  e.g.,  opinions,  allegations,  desires,  beliefs,  suspicions,  and 
speculations (riloff, patwardhan and wiebe, 2006; wiebe, 2000). there is 
some confusion among researchers to equate subjectivity with opinionated. 
by opinionated, we mean that a document or sentence expresses or implies a 
positive  or  negative  sentiment.  the  two  concepts  are  not  equivalent, 
although they have a large intersection. the task of determining whether a 
sentence is subjective or objective is called subjectivity classification (wiebe 
and riloff, 2005) (see chapter 4). here, we should note the following:  

     a subjective sentence may not express any sentiment. for example,    i 
think that he went home    is a subjective sentence, but does not express 
any sentiment. sentence (5) in example 4 is also subjective but it does 
not give a positive or negative sentiment about anything.  

     objective sentences can imply opinions or sentiments due to desirable 
and  undesirable  facts  (zhang  and  liu,  2011b).  for  example,  the 
following two sentences which state some facts clearly imply negative 
sentiments (which are implicit opinions) about their respective products 
because the facts are undesirable:  

   the earphone broke in two days.    
   i brought the mattress a week ago and a valley has formed    

apart  from  explicit  opinion  bearing  subjective  expressions,  many  other 
types of subjectivity have also been studied although not as extensive, e.g., 
affect,  judgment,  appreciation,  speculation,  hedge,  perspective,  arguing, 
agreement  and  disagreement,  political  stances  (alm,  2008;  ganter  and 
strube, 2009; greene and resnik, 2009; hardisty, boyd-graber and resnik, 
2010;  lin  et  al.,  2006;  medlock  and  briscoe,  2007;  mukherjee  and  liu, 

 

27 

 

  id31 and opinion mining 

2012;  murakami  and  raymond,  2010;  neviarouskaya,  prendinger  and 
ishizuka, 2010; somasundaran and wiebe, 2009). many of them may also 
imply sentiments. 

definition (emotion): emotions are our subjective feelings and thoughts.  

emotions have been studied in multiple fields, e.g., psychology, philosophy, 
and  sociology.  the  studies  are  very  broad,  from  emotional  responses  of 
physiological  reactions  (e.g.,  heart  rate  changes,  blood  pressure,  sweating 
and  so  on),  facial  expressions,  gestures  and  postures  to  different  types  of 
subjective  experiences  of  an  individual   s  state  of  mind.  scientists  have 
categorized people   s emotions into some categories. however, there is still 
not  a  set  of  agreed  basic  emotions  among  researchers.  based  on  (parrott, 
2001),  people  have  six  primary  emotions,  i.e.,  love,  joy,  surprise,  anger, 
sadness,  and  fear,  which  can  be  sub-divided  into  many  secondary  and 
tertiary emotions. each emotion can also have different intensities.  

emotions  are closely  related  to  sentiments.  the  strength  of  a  sentiment  or 
opinion is typically linked to the intensity of certain emotions, e.g., joy and 
anger. opinions that we study in id31 are mostly evaluations 
(although  not  always).  according 
research, 
evaluations can be broadly categorized into two types: rational evaluations 
and emotional evaluations (chaudhuri, 2006).  

to  consumer  behavior 

rational evaluation: such evaluations are from rational reasoning, tangible 
beliefs,  and  utilitarian  attitudes.  for  example,  the  following  sentences 
express rational evaluations:    the voice of this phone is clear,       this car 
is worth the price,    and    i am happy with this car.     

emotional  evaluation:  such  evaluations  are  from  non-tangible  and 
emotional responses to entities which go deep into people   s state of mind. 
for  example,  the  following  sentences  express  emotional  evaluations:     i 
love iphone,       i am so angry with their service people    and    this is the 
best car ever built.    

to make use of these two types of evaluations in practice, we can design 5 
sentiment ratings, emotional negative (-2), rational negative (-1), neutral (0), 
rational positive (+1), and emotional positive (+2). in practice, neutral often 
means no opinion or sentiment expressed.  

finally, we need to note that the concepts of emotion and opinion are clearly 
not  equivalent.  rational  opinions  express  no  emotions,  e.g.,     the  voice  of 
this  phone 
is  clear   ,  and  many  emotional  sentences  express  no 
opinion/sentiment  on  anything,  e.g.,     i  am  so  surprised  to  see  you  here   . 
more importantly, emotions may not have targets, but just people   s internal 
feelings, e.g.,    i am so sad today.     

28 
 

id31 and opinion mining 

2.5 

author and reader standing point  

we can look at an opinion from two perspectives, i.e., the author (opinion 
holder)  who  expresses  the  opinion,  and  the  reader  who  reads  the  opinion. 
for example, one wrote    the housing price has gone down, which is bad for 
the  economy.     clearly,  this  author  talks  about  the  negative  impact  of  the 
dropping  housing  price  on  the  economy.  however,  this  sentence  can  be 
perceived in both ways by readers. for sellers, this is indeed negative, but 
for buyers, this could well be a piece of good news. as another example, one 
wrote    i am so happy that google share price shot up today.    if a reader 
sold his google shares yesterday at a loss, he will not be very happy, but if 
the reader bought a lot of google shares yesterday, he will almost certainly 
be as happy as the author of the sentence.  

i am not aware of any reported studies about this issue. in current research or 
applications, researchers either ignore the issue or assume a standing point in 
their analysis. usually, the opinion holders are assumed to be the consumers 
or  the  general  public  unless  otherwise  stated  (e.g.,  the  president  of  the 
united  states).  product  manufacturers  or  service  providers     opinions  are 
considered advertisements if they are marked explicitly or fake opinions if 
they are not marked explicitly (e.g., mixed with opinions from consumers).   

2.6 

summary  

this  chapter  defined  the  concept  of  opinion  in  the  context  of  sentiment 
analysis, the main tasks of id31, and the framework of opinion 
summarization.  along  with  them,  two  relevant  and  important  concepts  of 
subjectivity and emotion were also introduced, which are highly related to 
but  not  equivalent  to  opinion.  existing  studies  about  them  have  mostly 
focused on their intersections with opinion (although not always). however, 
we  should  realize  that  all  these  concepts  and  their  definitions  are  rather 
fuzzy and subjective. for example, there is still not a set of emotions that all 
researchers agree. opinion itself is a broad concept too. id31 
mainly deals with the evaluation type of opinions or opinions which imply 
positive  or  negative  sentiments.  i  will  not  be  surprised  if  you  do  not 
completely agree with everything in this chapter. the goal of this chapter is 
to give a reasonably precise definition of id31 and its related 
issues. i hope i have succeeded to some extent.  

 

29 

 

  id31 and opinion mining 

chapter 3 

document sentiment classification 

 
starting from this chapter, we discuss the current major research directions 
or topics and their core techniques. sentiment classification is perhaps the 
most extensively studied topic (also see the survey (pang and lee, 2008)). it 
aims  to  classify  an  opinion  document  as  expressing  a  positive  or  negative 
opinion or sentiment. the task is also commonly known as the document-
level sentiment classification because it considers the whole document as a 
basic  information  unit.  a  large  majority  of  research  papers  on  this  topic 
classifies  online  reviews.  we  thus  also  define  the  problem  in  the  review 
context, but the definition is also applicable to other similar contexts.  

problem  definition:  given  an  opinion  document  d  evaluating  an  entity, 
determine the overall sentiment s of the opinion holder about the entity, 
i.e., determine s expressed on aspect general in the quintuple  

(_, general, s, _, _),  

where the entity e, opinion holder h, and time of opinion t are assumed 
known or irrelevant (do not care).  

there are two formulations based on the type of value that s takes. if s takes 
categorical  values,  e.g.,  positive  and  negative,  then  it  is  a  classification 
problem. if s takes numeric values or ordinal scores within a given range, 
e.g., 1 to 5, the problem becomes regression.  

to ensure that the task is meaningful in practice, existing research makes the 
following implicit assumption (liu, 2010):  

assumption: sentiment classification or regression assumes that the opinion 
document d (e.g., a product review) expresses opinions on a single entity 
e and contains opinions from a single opinion holder h.  

in practice, if an opinion document evaluates more than one entity, then the 
sentiments on the entities can be different. for example, the opinion holder 
may be positive about some entities and negative about others. thus, it does 
not  make  practical  sense  to  assign  one  sentiment  orientation  to  the  entire 
document in this case. it also does not make much sense if multiple opinion 
holders express opinions in a single document because their opinions can be 
different too.   

this  assumption  holds  for  reviews  of  products  and  services  because  each 

30 
 

id31 and opinion mining 

review  usually  focuses  on  evaluating  a  single  product  or  service  and  is 
written by a single reviewer. however, the assumption may not hold for a 
forum and blog post because in such a post the author may express opinions 
on multiple entities and compare them using comparative sentences.  

below, we first discuss the classification problem to predict categorical class 
labels and then the regression problem to predict rating scores. most existing 
techniques  for  document-level  classification  use  supervised  learning, 
although  there  are  also  unsupervised  methods.  sentiment  regression  has 
been done mainly using supervised learning. recently, several extensions to 
this  research  have  also  appeared,  most  notably,  cross-domain  sentiment 
classification  (or  domain  adaptation)  and  cross-language  sentiment 
classification, which will also be discussed at length. 

3.1 

sentiment classification using 
supervised learning  

is  essentially  a 

sentiment  classification  is  usually  formulated  as  a  two-class  classification 
problem, positive and negative. training and testing data used are normally 
product reviews. since online reviews have rating scores assigned by their 
reviewers,  e.g.,  1-5  stars,  the  positive  and  negative  classes  are  determined 
using  the  ratings.  for  example,  a  review  with  4  or  5  stars  is  considered  a 
positive  review,  and  a  review  with  1  to  2  stars  is  considered  a  negative 
review. most research papers do not use the neutral class, which makes the 
classification  problem  considerably  easier,  but  it  is  possible  to  use  the 
neutral class, e.g., assigning all 3-star reviews the neutral class.  
sentiment  classification 
text  classification  problem. 
traditional  text  classification  mainly  classifies  documents  of  different 
topics,  e.g.,  politics,  sciences,  and  sports.  in  such  classifications,  topic-
related  words  are  the  key  features.  however,  in  sentiment  classification, 
sentiment  or  opinion  words  that  indicate  positive  or  negative  opinions  are 
more important, e.g., great, excellent, amazing, horrible, bad, worst, etc.  
since  it  is  a  text  classification  problem,  any  existing  supervised  learning 
method can be applied, e.g., na  ve bayes classification, and support vector 
machines  (id166)  (joachims,  1999;  shawe-taylor  and  cristianini,  2000). 
pang, lee and vaithyanathan (2002) was the first paper to take this approach 
to  classify  movie  reviews  into  two  classes,  positive  and  negative.  it  was 
shown  that  using  unigrams  (a  bag  of  words)  as  features  in  classification 
performed quite well with either na  ve bayes or id166, although the authors 
also tried a number of other feature options.  

 

31 

 

  id31 and opinion mining 

in  subsequent  research,  many  more  features  and  learning  algorithms  were 
tried  by  a  large  number  of  researchers.  like  other  supervised  machine 
learning applications, the key for sentiment classification is the engineering 
of a set of effective features. some of the example features are:   
terms  and  their  frequency.  these  features  are  individual  words  (unigram) 
and  their  id165s  with  associated  frequency  counts.  they  are  also  the 
most common features used in traditional topic-based text classification. 
in  some  cases,  word  positions  may  also  be  considered.  the  tf-idf 
weighting scheme from information retrieval may be applied too. as in 
traditional  text  classification,  these  features  have  been  shown  highly 
effective for sentiment classification as well.  

part of speech. the part-of-speech (pos) of each word can be important too. 
words of different parts of speech (pos) may be treated differently. for 
example,  it  was  shown  that  adjectives  are  important  indicators  of 
opinions.  thus,  some  researchers  treated  adjectives  as  special  features. 
however,  one  can  also  use  all  pos  tags  and  their  id165s  as  features. 
note that in this book, we use the standard id32 pos tags as 
shown  in  table  3.1  (santorini,  1990).  the  penn  treebank  site  is  at 
http://www.cis.upenn.edu/ ~treebank/home.html. 

sentiment words and phrases. sentiment words are words in a language that 
are used to express positive or negative sentiments. for example, good, 
wonderful, and amazing are positive sentiment words, and bad, poor, and 
terrible  are  negative  sentiment  words.  most  sentiment  words  are 
adjectives  and  adverbs,  but  nouns  (e.g.,  rubbish,  junk,  and  crap)  and 
verbs (e.g., hate and love) can also be used to express sentiments. apart 
from individual words, there are also sentiment phrases and idioms, e.g., 
cost someone an arm and a leg.  

rules of opinions. apart from sentiment words and phrases, there are also 
many  other  expressions  or  language  compositions  that  can  be  used  to 
express or imply sentiments and opinions. we will list and discuss some 
of such expressions in section 5.2.  

sentiment  shifters.  these  are  expressions  that  are  used  to  change  the 
sentiment  orientations,  e.g.,  from  positive  to  negative  or  vice  versa. 
negation  words  are  the  most  important  class  of  sentiment  shifters.  for 
example,  the sentence     i don   t like this camera     is negative.  there are 
also  several  other  types  of  sentiment  shifters.  we  will  discuss  them  in 
section 5.2 too. such shifters also need to be handled with care because 
not all occurrences of such words mean sentiment changes. for example, 
   not    in    not only     but also    does not change sentiment orientation.  

syntactic  dependency.  words  dependency-based  features  generated  from 

parsing or dependency trees are also tried by researchers. 

32 
 

id31 and opinion mining 

table 3.1. id32 part-of-speech (pos) tags  

tag  description 

tag  description 
cc  coordinating conjunction   prp$   possessive pronoun   
cd  cardinal number   
dt  determiner 
ex 
fw 
in 

rb  adverb   
rbr  adverb, comparative   
rbs  adverb, superlative   
rp    particle   
sym  symbol   

existential there   
foreign word   
preposition 
or 
subordinating conjunction 
adjective 

to   
interjection   

jj 
to 
jjr  adjective, comparative    uh   
jjs  adjective, superlative    vb    verb, base form   
vbd  verb, past tense   
ls 
md  modal   
vbg   verb, gerund or present participle   
nn  noun, singular or mass    vbn   verb, past participle   
nns  noun, plural   

vbp   verb,  non-3rd  person 

list item marker   

singular 

present   

nnp  proper noun, singular   
nnps proper noun, plural   
pdt  predeterminer   
pos  possessive ending   
prp  personal pronoun   

vbz   verb, 3rd person singular present   
wdt  wh-determiner   
wp  wh-pronoun   
wp$   possessive wh-pronoun   
wrb   wh-adverb   

 
instead of using a standard machine learning method, researchers have also 
proposed several custom techniques specifically for sentiment classification, 
e.g.,  the  score  function  in  (dave,  lawrence  and  pennock,  2003)  based  on 
words  in  positive  and  negative  reviews,  and  the  aggregation  method  in 
(tong, 2001) using manually compiled domain-specific words and phrases.  

a  large  number  of  papers  have  been  published  in  the  literature.  here,  we 
introduce them briefly. in (gamon, 2004), classification was performed on 
customer  feedback  data,  which  are  usually  short  and  noisy  compared  to 
reviews. in (pang and lee, 2004), the minimum cut algorithm working on a 
graph was employed to help sentiment classification. in (mullen and collier, 
2004;  xia  and  zong,  2010),  syntactic  relations  were  used  together  with 
traditional  features.  in  (kennedy  and  inkpen,  2006;  li  et  al.,  2010),  the 
contextual valence and sentiment shifters were employed for classification. 
in  (cui,  mittal  and  datar,  2006),  an  evaluation  was  reported  with  several 
sentiment classification algorithms available at that time. in (ng, dasgupta 
and  arifin,  2006),  the  classification  was  done  by  using  some  linguistic 
knowledge sources. in (abbasi, chen and salem, 2008), a genetic algorithm 
based feature selection was proposed for sentiment classification in different 
languages.  in  (li,  zhang  and  sindhwani,  2009),  a  non-negative  matrix 
factorization  method  was  proposed.  in  (dasgupta  and  ng,  2009;  li  et  al., 
2011; zhou, chen and wang, 2010), semi-supervised learning and/or active 

 

33 

 

  id31 and opinion mining 

learning were experimented. in (kim, li and lee, 2009) and (paltoglou and 
thelwall,  2010),  different  ir  term  weighting  schemes  were  studied  and 
compared for sentiment classification. in (martineau and finin, 2009), a new 
term  weighting  scheme  called  delta  tfidf  was  proposed.  in  (qiu  et  al., 
2009),  a  lexicon-based  and  self-supervision  approach  was  used.  in  (he, 
2010),  labeled  features  (rather  than  labeled  documents)  were  exploited  for 
classification. in (mejova and srinivasan, 2011) the authors explored various 
feature  definition  and  selection  strategies.  in  (nakagawa,  inui  and 
kurohashi,  2010),  a  dependency  tree-based  classification  method  was 
proposed, which used id49 (crf) (lafferty, mccallum 
and  pereira,  2001)  with  hidden  variables.  in  (bickerstaffe  and  zukerman, 
2010),  a  hierarchical  multi-classifier  considering  inter-class  similarity  was 
reported. in (li et al., 2010), personal (i, we) and impersonal (they, it, this 
product)  sentences  were  exploited  to  help  classification.  in  (yessenalina, 
choi  and  cardie,  2010),  automatically  generated  annotator  rationales  was 
used to help classification. in (yessenalina, yue and cardie, 2010), multi-
level structured models were proposed. in (wang et al., 2011), the authors 
proposed  a  graph-based  hashtag  approach  to  classifying  twitter  post 
sentiments,  and  in  (kouloumpis,  wilson  and  moore,  2011),  linguistic 
features  and  features  that  capture  information  about  the  informal  and 
creative  language  used  in  microblogs  were  also  utilized.  in  (maas  et  al., 
2011), the authors used word vectors which can capture some latent aspects 
of  the  words  to  help  classification.  in  (bespalov  et  al.,  2011),  sentiment 
classification was performed based on supervised latent id165 analysis. in 
(burfoot,  bird  and  baldwin,  2011),  congressional  floor  debates  were 
classified.  in  (becker  and  aharonson,  2010),  the  authors  showed  that 
sentiment classification should focus on the final portion of the text based on 
their psycholinguistic and psychophysical experiments. in (liu et al., 2010), 
different  linguistic  features  were  compared  for  both  blog  and  review 
sentiment classification. in (tokuhisa, inui and matsumoto, 2008), emotion 
classification  of  dialog  utterances  was  investigated.  it  first  performed 
sentiment classification of three classes (positive, negative and neutral) and 
then classified positive and negative utterances into 10 emotion categories.  

3.2 

sentiment classification using 
unsupervised learning  

since  sentiment  words  are  often  the  dominating  factor  for  sentiment 
classification, it is not hard to imagine that sentiment words and phrases may 
be used for sentiment classification in an unsupervised manner. the method 

34 
 

id31 and opinion mining 

in  (turney,  2002)  is  such  a  technique.  it  performs  classification  based  on 
some fixed syntactic patterns that are likely to be used to express opinions. 
the  syntactic  patterns  are  composed  based  on  part-of-speech  (pos)  tags. 
the algorithm given in (turney, 2002) consists of three steps:  

step 1: two consecutive words are extracted if their pos tags conform to 
any of the patterns in table 3.2. for example, pattern 2  means that two 
consecutive words are extracted if the first word is an adverb, the second 
word is an adjective, and the third word (not extracted) is not a noun. as 
an  example,  in  the  sentence     this  piano  produces  beautiful  sounds   , 
   beautiful sounds    is extracted as it satisfies the first pattern. the reason 
these patterns are used is that jj, rb, rbr and rbs words often express 
opinions.  the  nouns  or  verbs  act  as  the  contexts  because  in  different 
contexts a jj, rb, rbr and rbs word may express different sentiments. 
for  example,  the  adjective  (jj)     unpredictable     may  have  a  negative 
sentiment in a car review as in    unpredictable steering,    but it could have 
a positive sentiment in a movie review as in    unpredictable plot.     

step 2: it estimates the sentiment orientation (so) of the extracted phrases 

using the pointwise mutual information (pmi) measure:  

pmi

(

term
1

,

term

2

)

   

log

2

   
      
   

pr(
pr(

term
1
term
)
1

   
pr(

term
2
term

)

2

)

 

   
.
      
   

(1) 

pmi  measures  the  degree  of  statistical  dependence  between  two  terms. 
here,  pr(term1       term2)  is  the  actual  co-occurrence  id203  of  term1 
and term2, and pr(term1)pr(term2) is the co-occurrence id203 of the 
two terms if they are statistically independent. the sentiment orientation 
(so)  of  a  phrase  is  computed  based  on  its  association  with  the  positive 
reference word    excellent    and the negative reference word    poor   : 

so(phrase) = pmi(phrase,    excellent   )     pmi(phrase,    poor   ). 

(2) 

the probabilities are calculated by issuing queries to a search engine and 
collecting  the  number  of  hits.  for  each  search  query,  a  search  engine 
usually gives the number of relevant documents to the query, which is the 
number of hits. thus, by searching the two terms together and separately, 

table 3.2. patterns of pos tags for extracting two-word phrases  

 

first word 

second word 

jj 

nn or nns 

1 
2  rb, rbr, or rbs 
3 
4 
5  rb, rbr, or rbs  vb, vbd, vbn, or vbg

nn or nns 

jj 
jj 
jj 

jj 

third word 
(not extracted) 

anything 

not nn nor nns 
not nn nor nns 
not nn nor nns 

anything 

 

35 

 

  id31 and opinion mining 

the probabilities in equation (1) can be estimated. in (turney, 2002), the 
altavista  search  engine  was  used  because  it  has  a  near  operator  to 
constrain the search to documents that contain the words within ten words 
of  one  another  in  either  order.  let  hits(query)  be  the  number  of  hits 
returned. equation (2) can be rewritten as: 

so

(

phrase
)

   

log

   
      
   

2

hits
hits

(
(

 
phrase
 
phrase

near
" 
near
" 

hits
)
)
poor"
excellent"
hits
excellent"
poor
"(
)

)"

"(

(3) 

 

   
.
      
   

step  3:  given  a  review,  the  algorithm  computes  the  average  so  of  all 
phrases in the review and classifies the review as positive if the average 
so is positive and negative otherwise.  

final classification accuracies on reviews from various domains range from 
84% for automobile reviews to 66% for movie reviews.  

another unsupervised approach is the lexicon-based method, which uses a 
dictionary of sentiment words and phrases with their associated orientations 
and  strength,  and  incorporates  intensification  and  negation  to  compute  a 
sentiment score for each document (taboada et al., 2011). this method was 
originally used in sentence and aspect-level sentiment classification (ding, 
liu and yu, 2008; hu and liu, 2004; kim and hovy, 2004).  

3.3 

sentiment rating prediction  

apart  from  classification  of  positive  and  negative  sentiments,  researchers 
also studied the problem of predicting the rating scores (e.g., 1   5 stars) of 
reviews (pang and lee, 2005). in this case, the problem can be formulated as 
a  regression  problem  since  the  rating  scores  are  ordinal,  although  not  all 
researchers  solved  the  problem  using  regression  techniques.  pang  and  lee 
(2005)  experimented  with  id166  regression,  id166  multiclass  classification 
using  the  one-vs-all  (ova)  strategy,  and  a  meta-learning  method  called 
metric labeling. it was shown that ova based classification is significantly 
poorer  than  the  other  two  approaches,  which  performed  similarly.  this  is 
understandable as the numerical ratings are not categorical values. goldberg 
and zhu (2006) improved this approach by modeling rating prediction as a 
graph-based  semi-supervised  learning  problem,  which  used  both  labeled 
(with  ratings)  and  unlabeled  (without  ratings)  reviews.  the  unlabeled 
reviews were also the test reviews whose ratings need to be predicted. in the 
graph, each node is a document (review) and the link between two nodes is 
the similarity value between the two documents. a large similarity weight 
implies that the two documents tend to have the same sentiment rating. the 

36 
 

id31 and opinion mining 

paper experimented with several different similarity schemes. the algorithm 
also  assumes  that  initially  a  separate  learner  has  already  predicted  the 
numerical ratings of the unlabeled documents. the graph based method only 
improves  them  by  revising  the  ratings  through  solving  an  optimization 
problem to force ratings to be smooth throughout the graph with regard to 
both the ratings and the link weights. 

qu, ifrim and weikum (2010) introduced a bag-of-opinions representation 
of  documents  to  capture  the  strength  of  id165s  with  opinions,  which  is 
different  from  the  traditional  bag-of-words  representation.  each  of  the 
opinions  is  a  triple,  a  sentiment  word,  a  modifier,  and  a  negator.  for 
example,  in     not  very  good   ,     good     is  the  sentiment  word,     very     is  the 
modifier and    not    is the negator.  for sentiment classification of two classes 
(positive  and  negative),  the  opinion  modifier  is  not  crucial  but  for  rating 
prediction,  it  is  very  important  and  so  is  the  impact  of  negation.  a 
constrained  ridge  regression  method  was  developed  to  learn  the  sentiment 
score  or  strength  of  each  opinion  from  domain-independent  corpora  (of 
multiple domains) of rated reviews. the key idea of learning was to exploit 
an  available  opinion  lexicon  and  the  review  ratings.  to  transfer  the 
regression  model  to  a  newly  given  domain-dependent  application,  the 
algorithm  derives  a  set  of  statistics  over  the  opinion  scores  and  then  uses 
them  as  additional  features  together  with  the  standard  unigrams  for  rating 
prediction. prior to this work, (liu and seneff, 2009) proposed an approach 
to extracting adverb-adjective-noun phrases (e.g.,    very nice car   ) based on 
the  clause  structure  obtained  by  parsing  sentences  into  a  hierarchical 
representation. they assigned sentiment scores based on a heuristic method 
which computes the contribution of adjectives, adverbials and negations to 
the  sentiment  degree  based  on  the  ratings  of  reviews  where  these  words 
occurred.  unlike  the  above  work,  there  was  no  learning  involved  in  this 
work.   

instead of predicting the rating of each review, snyder and barzilay (2007) 
studied  the  problem  of  predicting  the  rating  for  each  aspect.  a  simple 
approach to this task would be to use a standard regression or classification 
technique.  however,  this  approach  does  not  exploit  the  dependencies 
between  users     judgments  across  different  aspects.  knowledge  of  these 
dependencies  is  useful  for  accurate  prediction.  thus,  this  paper  proposed 
two  models,  aspect  model  (which  works  on  individual  aspects)  and 
agreement model (which models the rating agreement among aspects). both 
models  were  combined  in  learning.  the  features  used  for  training  were 
lexical features such as unigram and bigrams from each review.  

long, zhang and zhu (2010) used a similar approach as that in (pang and 
lee,  2005)  but  with  a  baysian  network  classifier  for  rating  prediction  of 

 

37 

 

  id31 and opinion mining 

each aspect in a review. for good accuracy, instead of predicting for every 
review, they focused on predicting only aspect ratings for a selected subset 
of  reviews  which  comprehensively  evaluates  the  aspects.  clearly,  the 
estimations  from  these  reviews  should  be  more  accurate  than  for  those  of 
other  reviews  because 
these  other  reviews  do  not  have  sufficient 
information.  the  review  selection  method  used  an  information  measure 
based  on  kolmogorov  complexity.  the  aspect  rating  prediction  for  the 
selected reviews used machine learning. the features for training were only 
from  those  aspect  related  sentences.  the  aspect  extraction  was  done  in  a 
similar way to that in (hu and liu, 2004).  

3.4 

cross-domain sentiment 
classification  

it  has  been  shown  that  sentiment  classification  is  highly  sensitive  to  the 
domain from which the training data is extracted. a classifier trained using 
opinion documents from one domain often performs poorly on test data from 
another domain. the reason is that words and even language constructs used 
in different domains for expressing opinions can be quite different. to make 
matters  worse,  the  same  word  in  one  domain  may  mean  positive  but  in 
another  domain  may  mean  negative.  thus,  domain  adaptation  or  transfer 
learning  is  needed.  existing  researches  are  mainly  based  on  two  settings. 
the first setting needs a small amount of labeled training data for the new 
domain (aue and gamon, 2005). the second needs no labeled data for the 
new  domain  (blitzer,  dredze  and  pereira,  2007;  tan  et  al.,  2007).  the 
original domain with labeled training data is often called the source domain, 
and the new domain which is used for testing is called the target domain.  

in  (aue  and  gamon,  2005),  the  authors  proposed  to  transfer  sentiment 
classifiers to new domains in the absence of large amounts of labeled data in 
these  domains.  they  experimented  with  four  strategies:  (1)  training  on  a 
mixture of labeled reviews from other domains where such data are available 
and  testing  on  the  target  domain;  (2)  training  a  classifier  as  above,  but 
limiting the set of features to those only observed in the target domain; (3) 
using ensembles of classifiers from domains with available labeled data and 
testing on the target domain; (4) combining small amounts of labeled data 
with  large  amounts  of  unlabeled  data  in  the  target  domain  (this  is  the 
traditional  semi-supervised  learning  setting).  id166  was  used  for  the  first 
three strategies, and em for semi-supervised learning (nigam et al., 2000) 
was used for the fourth strategy. their experiments showed that the strategy 
(4) performed the best because it was able to make use of both the labeled 
and unlabeled data in the target domain.  

38 
 

id31 and opinion mining 

in (yang, si and callan, 2006), a simple strategy based on feature selection 
was  proposed  for  transfer  learning  for  sentence  level  classification.  their 
method first used two fully labeled training set from two domains to select 
features  that  were  highly  ranked  in  both  domains.  these  selected  features 
were  considered  domain  independent  features.  the  classifier  built  using 
these features was then applied to any target/test domains. another simple 
strategy  was  proposed  in  (tan  et  al.,  2007),  which  first  trains  a  base 
classifier using the labeled data from the source domain, and then uses the 
classifier to label some informative examples in the target domain. based on 
the selected examples in the target domain, a new classifier is learned, which 
is finally applied to classify the test cases in the target domain. 
in  (blitzer,  dredze  and  pereira,  2007),  the  authors  used  a  method  called 
structural correspondence learning (scl) for id20, which was 
proposed  earlier  in  (blitzer,  mcdonald  and  pereira,  2006).  given  labeled 
reviews from a source domain and unlabeled reviews from both the source 
and  target  domains,  scl  first  chooses  a  set  of  m  features  which  occur 
frequently in both domains and are also good predictors of the source label 
(the paper chose those features with highest mutual information to the source 
label). these features are called the pivot features which represent the shared 
feature space of the two domains. it then computes the correlations of each 
pivot feature with other non-pivot features in both domains. this produces a 
correlation matrix w where row i is a vector of correlation values of non-
pivot features with the ith pivot feature. intuitively, positive values indicate 
that  those  non-pivot  features  are  positively  correlated  with  the  ith  pivot 
feature in the source domain or in the new domain. this establishes a feature 
correspondence  between  the  two  domains.  after  that,  singular  value 
decomposition  (svd)  is  employed  to  compute  a  low-dimensional  linear 
approximation     (the top k left singular vectors, transposed) of w. the final 
set  of  features  for  training  and  for  testing  is  the  original  set  of  features  x 
combined with    x which produces k real-valued features. the classifier built 
using the combined features and labeled data in the source domain should 
work in both the source and the target domains.  
pan et al. (pan et al., 2010) proposed a method similar to scl at the high 
level.  the  algorithm  works  in  the  setting  where  there  are  only  labeled 
examples in the source domain and unlabeled examples in the target domain. 
it bridges the gap between the domains by using a spectral feature alignment 
(sfa) algorithm to align domain-specific words from different domains into 
unified clusters, with the help of domain independent words as the bridge. 
domain-independent  words  are  like  pivot  words  in  (blitzer,  dredze  and 
pereira, 2007) and can be selected similarly. sfa works by first constructing 
a bipartite graph with the domain-independent words as one set of nodes and 
the domain-specific words as the other set of nodes. a domain specific word 
is linked to a domain-independent word if they co-occur. the co-occurrence 
can be defined as co-occurring in the same document or within a window. 

 

39 

 

  id31 and opinion mining 

to 

find  a  common  semantic  space  based  on  domain 

the link weight is the frequency of their co-occurrence. a spectral id91 
algorithm is then applied on the bipartite graph to co-align domain-specific 
and domain-independent words into a set of feature clusters. the idea is that 
if  two  domain-specific  words  have  connections  to  more  common  domain-
independent words in the graph, they tend to be aligned or clustered together 
with a higher id203. similarly, if two domain-independent words have 
connections to more common domain-specific words in the graph, they tend 
to be aligned together with a higher id203. for the final cross-domain 
training and testing, all data examples are represented with the combination 
of these clusters and the original set of features. 
along the same line, he, lin and alani (2011) used joint id96 to 
identify  opinion  topics  (which  are  similar  to  clusters  in  the  above  work) 
from  both  domains  to  bridge  them.  the  resulting  topics  which  cover  both 
domains  are  used  as  additional  features  to  augment  the  original  set  of 
features for classification. in (gao and li, 2011), id96 was used 
too 
term 
correspondences and term co-occurrences in the two domains. this common 
semantic space was then used to learn a classifier which was applied to the 
target  domain.  bollegala,  weir  and  carroll  (2011)  proposed  a  method  to 
automatically create a sentiment sensitive thesaurus using both labeled and 
unlabeled data from multiple source domains to find the association between 
words  that  express  similar  sentiments  in  different  domains.  the  created 
thesaurus is then used to expand the original feature vectors to train a binary 
sentiment classifier. in (yoshida et al., 2011), the authors proposed a method 
for  transfer  from  multiple  source  domains  to  multiple  target  domains  by 
identifying  domain  dependent  and  independent  word  sentiments.  in 
(andreevskaia  and  bergler,  2008),  a  method  using  an  ensemble  of  two 
classifiers was proposed. the first classifier was  built using a dictionary and 
the second was built using a small amount of in-domain training data.  
in (wu, tan and cheng, 2009), a graph-based method was proposed, which 
uses  the  idea  of  label  propagation  on  a  similarity  graph  (zhu  and 
ghahramani, 2002) to perform the transfer. in the graph, each document is a 
node  and  each  link  between  two  nodes  is  a  weight  computed  using  the 
cosine similarity of the two documents. initially, every document in the old 
domain has a label score of +1 (positive) or -1 (negative) and each document 
in  the  new  domain  is  assigned  a  label  score  based  a  normal  sentiment 
classifier,  which  can  be  learned  from  the  old  domain.  the  algorithm  then 
iteratively updates the label score of each new domain document i by finding 
k  nearest  neighbors  in  the  old  domain  and k  nearest  neighbors  in  the  new 
domain. a linear combination of the neighbor label scores and link weights 
are used to assign a new score to node i. the iterative process stops when the 
label  scores  converge.  the  sentiment  orientations  of  the  new  domain 
documents are determined by their label scores.   

40 
 

id31 and opinion mining 

xia and zong (2011) found that across different domains, features of some 
types of part-of-speech (pos) tags are usually domain-dependent, while of 
some  others  are  domain-free.  based  on  this  observation,  they  proposed  a 
pos-based ensemble model to integrate features with different types of pos 
tags to improve the classification performance. 

3.5 

cross-language sentiment 
classification  

cross-language  sentiment  classification  means 
to  perform  sentiment 
classification  of  opinion  documents  in  multiple  languages.  there  are  two 
main  motivations  for  cross-language  classification.  first,  researchers  from 
different  countries  want  to  build  sentiment  analysis  systems  in  their  own 
languages. however, much of the research has been done in english. there 
are not many resources or tools in other languages that can be used to build 
good sentiment classifiers quickly in these languages. the natural question is 
whether  it  is  possible  to  leverage  the  automated  machine  translation 
capability  and  existing  sentiment  analysis  resources  and  tools  available  in 
english  to  help  build  sentiment  analysis  systems  in  other  languages.  the 
second motivation is that in many applications, companies want to know and 
compare  consumer  opinions  about  their  products  and  services  in  different 
countries. if they have a id31 system in english, they want to 
quickly  build  sentiment  analysis  systems  in  other  languages  through 
translation.  
several  researchers  have  studied  this  problem.  much  of  the  current  work 
focuses  on  sentiment  classification  at  the  document  level,  and  subjectivity 
and  sentiment  classification  at  the  sentence  level.  limited  work  has  been 
done at the aspect level except that in (guo et al., 2010). in this section, we 
focus on cross-language document-level sentiment classification. section 4.5 
in the next chapter focuses on the sentence level.  
in  (wan,  2008),  the  author  exploited  sentiment  resources  in  english  to 
perform  classification  of  chinese  reviews.  the  first  step  of  the  algorithm 
translates each chinese review into english using multiple translators, which 
produce different english versions. it then uses a lexicon-based approach to 
classify  each  translated  english  version.  the  lexicon  consists  of  a  set  of 
positive terms, a set of negative terms, a set of negation terms, and a set of 
intensifiers. the algorithm then sums up the sentiment scores of the terms in 
the  review  considering  negations  and  intensifiers.  if  the  final  score  is  less 
than 0, the review is negative, otherwise positive. for the final classification 
of each review, it combines the scores of different translated versions using 
various  ensemble  methods,  e.g.,  average,  max,  weighted  average,  voting, 

 

41 

 

  id31 and opinion mining 

etc. if a chinese lexicon is also available, the same technique can be applied 
to the chinese version. its result may also be combined with the results of 
those english translations. the results show that the ensemble technique is 
effective.  brooke,  tofiloski  and  taboada  (2009)  also  experimented  with 
translation (using only one translator) from the source language (english) to 
the  target  language  (spanish)  and  then  used  a  lexicon-based  approach  or 
machine learning for target language document sentiment classification. 

in (wan, 2009), a co-training method was proposed which made use of an 
annotated  english  corpus  for  classification  of  chinese  reviews  in  a 
supervised manner. no chinese resources were used. in training, the input 
consisted of a set of labeled english reviews and a set of unlabeled chinese 
reviews. the labeled english reviews were translated into labeled chinese 
reviews, and the unlabeled chinese reviews were translated into unlabeled 
english reviews. each review was thus associated with an english version 
and a chinese version. english features and chinese features for each review 
were considered as two independent and redundant views of the review. a 
co-training algorithm using id166 was then applied to learn two classifiers. 
finally,  the  two  classifiers  were  combined  into  a  single  classifier.  in  the 
classification  phase,  each  unlabeled  chinese  review  for  testing  was  first 
translated into an english review, and then the learned classifier was applied 
to classify the review into either positive or negative. 

wei  and  pal  (2010)  proposed  to  use  a  transfer  learning  method  for  cross-
language sentiment classification. due to the fact that machine translation is 
still far from  perfect, to minimize the noise introduced in translation, they 
proposed  to  use  the  structural  correspondence  learning  (scl)  method 
(blitzer, dredze and pereira, 2007) discussed in the previous section to find 
a small set of core features shared by both languages (english and chinese). 
to alleviate the problem of data and feature sparseness, they issued queries 
to a search engine to find other highly correlated features to those in the core 
feature  set,  and  then  used  the  newly  discovered  features  to  create  extra 
pseudo-examples for training.   

boyd-graber  and  resnik  (2010)  extended  the  topic  modeling  method 
supervised id44 (slda) (blei and mcauliffe, 2007) to 
work on reviews from multi-languages for review rating prediction. slda is 
able  to  consider  the  user-rating  of  each  review  in  topic  modeling.  the 
extended  model  mlslda  creates  topics  using  documents  from  multiple 
languages at the same time. the resulting multi-language topics are globally 
consistent across languages. to bridge topic terms in different languages in 
id96, the model used the aligned id138s of different languages 
or dictionaries.  

42 
 

id31 and opinion mining 

in (guo et al., 2010), a topic model based method was proposed to group a 
set  of  given  aspect  expressions  in  different  languages  into  aspect  clusters 
(categories)  for  aspect-based  sentiment  comparison  of  opinions  from 
different countries (see also section 5.3.4).  

in  (duh,  fujino  and  nagata,  2011),  the  authors  presented  their  opinions 
about the research of cross-language sentiment classification. based on their 
analysis,  they  claimed  that  domain  mismatch  was  not  caused  by  machine 
translation  (mt)  errors,  and  accuracy  degradation  would  occur  even  with 
perfect mt. it also argued that the cross-language adaptation problem was 
qualitatively  different  from  other  (monolingual)  adaptation  problems  in 
nlp; thus new adaptation algorithms should to be considered. 

3.6 

summary  

sentiment  classification  at  the  document  level  provides  an  overall  opinion 
on  an  entity,  topic  or  event.  it  has  been  studied  by  a  large  number  of 
researchers. however, this level of classification has some shortcomings for 
applications:  

     in many applications, the user needs to know additional details, e.g., what 
aspects of entities are liked and disliked by consumers. in typical opinion 
documents,  such  details  are  provided,  but  document  sentiment 
classification does not extract them for the user.  

     document sentiment classification is not easily applicable to non-reviews 
such as forum discussions, blogs, and news articles, because many such 
postings can evaluate multiple entities and compare them. in many cases, 
it  is  hard  to  determine  whether  a  posting  actually  evaluates  the  entities 
that  the  user  is  interested  in,  and  whether  the  posting  expresses  any 
opinion  at  all,  let  alone  to  determine  the  sentiment  about  them. 
document-level  sentiment  classification  does  not  perform  such  fine-
grained tasks, which require in-depth natural language processing. in fact, 
online  reviews  do  not  need  sentiment  classification  because  almost  all 
reviews already have user-assigned star ratings. in practice, it is the forum 
discussions  and  blogs  that  need  sentiment  classification  to  determine 
people   s opinions about different entities (e.g., products and services) and 
topics.  

 
 
 

 

43 

 

  id31 and opinion mining 

chapter 4 

sentence subjectivity and 
sentiment classification  

in 

as  discussed 
the  previous  chapter,  document-level  sentiment 
classification may be too crude for most applications. we now move to the 
sentence  level,  i.e.,  to  classify  sentiment  expressed  in  each  sentence. 
however,  there  is  no  fundamental  difference  between  document  and 
sentence  level  classifications  because  sentences  are  just  short  documents. 
one assumption that researchers often make about sentence-level analysis is 
that a sentence usually contains a single opinion (although not true in many 
cases).  a  document  typically  contains  multiple  opinions.  let  us  start  our 
discussion with an example review:  

    i  bought  a  motorola  phone  two  weeks  ago.  everything  was  good 
initially. the voice was clear and the battery life was long, although it 
is a bit bulky. then, it stopped working yesterday.     

the first sentence expresses no opinion as it simply states a fact. all other 
sentences express either explicit or implicit sentiments. note no opinion is 
usually regarded as neutral. 

problem definition:  given  a  sentence x,  determine  whether  x  expresses  a 

positive, negative, or neutral (or no) opinion.   

the quintuple (e, a, s, h, t) definition is not used here because sentence-level 
classification  is  an  intermediate  step.  in  most  applications,  one  needs  to 
know the opinion targets. knowing only that a sentence expresses a positive 
or negative opinion, but not what entities/aspects the opinion is about, is of 
limited use. however, sentence level classification is still useful because in 
many cases, if we know what entities and entity aspects are talked about in a 
sentence, this step can help determine whether the opinions about the entities 
and their aspects are positive or negative.   

sentence  sentiment  classification  can  be  solved  either  as  a  three-class 
classification  problem  or  as  two  separate  classification  problems.  in  the 
latter case, the first problem (also called the first step) is to classify whether 
a sentence expresses an opinion or not. the second problem (also called the 
second  step)  then  classifies  those  opinion  sentences  into  positive  and 
is  usually  called  subjectivity 
negative  classes.  the 
classification,  which  determines  whether  a  sentence  expresses  a  piece  of 
subjective  information  or  factual  (objective)  information  (hatzivassiloglou 

first  problem 

44 
 

id31 and opinion mining 

and wiebe, 2000; riloff, patwardhan and wiebe, 2006; riloff and wiebe, 
2003;  wiebe  et  al.,  2004;  wilson,  wiebe  and  hwa,  2004;  wilson,  wiebe 
and  hwa,  2006;  yu  and  hatzivassiloglou,  2003).  objective  sentences  are 
regarded as expressing no sentiment or opinion. this can be problematic as 
we  discussed  earlier  because  objective  sentences  can  also  imply  opinions. 
for example,    then, it stopped working yesterday    in the above review is an 
objective  sentence,  but  it  implies  a  negative  sentiment  about  the  phone 
because of the undesirable fact. thus, it is more appropriate for the first step 
to  classify  each  sentence  as  opinionated  or  not  opinionated,  regardless 
whether it is subjective or objective. however, due to the common practice, 
we  still  use  the  term  subjectivity  classification  in  this  chapter.  below,  we 
first discuss existing work on sentence-level subjectivity classification and 
then sentiment classification.  

4.1 

subectivity classification  

subjectivity  classification  classifies  sentences  into  two  classes,  subjective 
and  objective  (wiebe,  bruce  and  o'hara,  1999).  an  objective  sentence 
expresses  some  factual  information,  while  a  subjective  sentence  usually 
gives personal views and opinions. in fact, subjective sentences can express 
many  types  of  information,  e.g.,  opinions,  evaluations,  emotions,  beliefs, 
speculations, judgments, allegations, stances, etc. (quirk et al., 1985; wiebe, 
bruce  and  o'hara,  1999).  some  of  them  indicate  positive  or  negative 
sentiments  and  some  of  them  do  not.  early  research  solved  subjectivity 
classification as a standalone problem, i.e., not for the purpose of sentiment 
classification. in more recent research, some researchers treated it as the first 
step  of  sentiment  classification  by  using  it  to  remove  objective  sentences 
which are assumed to express or imply no opinion.  

most  existing  approaches  to  subjectivity  classification  are  based  on 
supervised learning. for example, the early work reported in (wiebe, bruce 
and  o'hara,  1999)  performed  subjectivity  classification  using  the  na  ve 
bayes  classifier  with  a  set  of  binary  features,  e.g.,  the  presence  in  the 
sentence of a pronoun, an adjective, a cardinal number, a modal other than 
will  and  an  adverb  other  than  not.  subsequent  researches  also  used  other 
learning algorithms and more sophisticated features.  

in (wiebe, 2000), wiebe proposed an unsupervised method for subjectivity 
classification, which simply used the presence of subjective expressions in a 
sentence to determine the subjectivity of a sentence. since there was not a 
complete  set  of  such  expressions,  it  provided  some  seeds  and  then  used 
distributional similarity (lin, 1998) to find similar words, which were also 

 

45 

 

  id31 and opinion mining 

in  (hatzivassiloglou  and  mckeown,  1997) 

likely to be subjectivity indicators. however, words found this way had low 
precision  and  high  recall.  then,  the  method  in  (hatzivassiloglou  and 
mckeown,  1997)  and  gradability  in  (hatzivassiloglou  and  wiebe,  2000) 
were applied to filter the wrong subjective expressions. we will discuss the 
method 
in  section  6.2. 
gradability  is  a  semantic  property  that  enables  a  word  to  appear  in  a 
comparative  construct  and  to  accept  modifying  expressions  that  act  as 
intensifiers or diminishers. gradable adjectives express properties in varying 
degrees  of  strength,  relative  to  a  norm  either  explicitly  mentioned  or 
implicitly  supplied  by  the  modified  noun  (for  example,  a  small  planet  is 
usually  much  larger  than  a  large  house).  gradable  adjectives  were  found 
using a seed list of manually compiled adverbs and noun phrases (such as a 
little, exceedingly, somewhat, and very) that are frequently used as grading 
modifiers. such gradable adjectives are good indicators of subjectivity.  

the  simfinder  system 

to  factual  sentences.  they  used 

in  (yu  and  hatzivassiloglou,  2003)  yu  and  hatzivassiloglou  performed 
subjectivity  classifications  using  sentence  similarity  and  a  na  ve  bayes 
classifier.  the  sentence  similarity  method  is  based  on  the  assumption  that 
subjective or opinion sentences are more similar to other opinion sentences 
than 
in 
(hatzivassiloglou  et  al.,  2001)  to  measure  sentence  similarity  based  on 
shared words, phrases, and id138 synsets. for na  ve bayes classification, 
they  used  features  such  as,  words  (unigram),  bigrams,  trigrams,  part  of 
speech,  the  presence  of  sentiment  words,  the  counts  of  the  polarities  (or 
orientations)  of  sequences  of  sentiment  words  (e.g.,     ++     for  two 
consecutive  positively  oriented  words),  and  the  counts  of  parts  of  speech 
combined with sentiment information (e.g.,    jj+    for positive adjective), as 
well as features encoding the sentiment (if any) of the head verb, the main 
subject,  and  their  immediate  modifiers.  this  work  also  does  sentiment 
classification  to  determine  whether  a  subjective  sentence  is  positive  or 
negative, which we will discuss in the next section.  

one of the bottlenecks in applying supervised learning is the manual effort 
involved  in  annotating  a  large  number  of  training  examples.  to  save  the 
manual  labeling  effort,  a  id64  approach  to  label  training  data 
automatically  was  proposed  in  (riloff  and  wiebe,  2003).  the  algorithm 
works by first using two high precision classifiers (hp-subj and hp-obj) to 
automatically  identify  some  subjective  and  objective  sentences.  the  high-
precision classifiers use lists of lexical items (single words or id165s) that 
are good subjectivity clues. hp-subj classifies a sentence as subjective if it 
contains two or more strong subjective clues. hp-obj classifies a sentence as 
objective if there are no strong subjective clues. these classifiers will give 
very high precision but low recall. the extracted sentences are then added to 

46 
 

id31 and opinion mining 

the training data to learn patterns. the patterns (which form the subjectivity 
classifiers in the next iteration) are then used to automatically identify more 
subjective and objective sentences, which are then added to the training set, 
and the next iteration of the algorithm begins.  

for pattern learning, a set of syntactic templates are provided to restrict the 
kinds  of  patterns  to  be  learned.  some  example  syntactic  templates  and 
example patterns are shown below.  

syntactic template 
<subj> passive-verb  
<subj> active-verb  
active-verb <dobj>  
noun aux <dobj>  
passive-verb prep <np>  

example pattern 
<subj> was satisfied 
<subj> complained 
endorsed <dobj> 
fact is <dobj> 
was worried about <np> 

wiebe and riloff (2005) used so discovered patterns to generate a rule-based 
method  to  produce  training  data  for  subjectivity  classification.  the  rule-
based  subjective  classifier  classifies  a  sentence  as  subjective  if  it  contains 
two  or  more  strong  subjective  clues  (otherwise,  it  does  not  label  the 
sentence).  in  contrast,  the  rule-based  objective  classifier  looks  for  the 
absence of clues: it classifies a sentence as objective if there are no strong 
subjective  clues  in  the  sentence,  and  several  other  conditions.  the  system 
also  learns  new  patterns  about  objective  sentences  using  the  information 
extraction system autoslog-ts (riloff, 1996), which finds patterns based on 
some  fixed  syntactic  templates.  the  data  produced  by  the  rule-based 
classifiers  was  used  to  train  a  na  ve  bayes  classifier.  a  related  study  was 
also reported in (wiebe et al., 2004), which used a more comprehensive set 
of features or subjectivity clues for subjectivity classification.  

riloff, patwardhan and wiebe (2006) studied relationships among different 
features. they defined subsumption relationships among unigrams, id165s 
and  lexico-syntactic  patterns.  if  a  feature  is  subsumed  by  another,  the 
subsumed feature is not needed. this can remove many redundant features. 

in (pang and lee, 2004), a mincut-based algorithm was proposed to classify 
each  sentence  as  being  subjective  or  objective.  the  algorithm  works  on  a 
sentence  graph  of  an  opinion  document,  e.g.,  a  review.  the  graph  is  first 
built  based  on  local  labeling  consistencies  (which  produces  an  association 
score of two sentences) and individual sentence subjectivity score computed 
based  on  the  id203  produced  by  a  traditional  classification  method 
(which  produces  a  score  for  each  sentence).  local  labeling  consistency 
means that sentences close to each other are more likely to have the same 
class label (subjective or objective). the mincut approach is able to improve 
individual  sentence  based  subjectivity  classification  because  of  the  local 

 

47 

 

  id31 and opinion mining 

labeling  consistencies.  the  purpose  of  this  work  was  actually  to  remove 
objective  sentences  from  reviews  to  improve  document  level  sentiment 
classification.  

barbosa and feng (2010) classified the subjectivity of tweets (postings on 
twitter)  based  on  traditional  features  with  the  inclusion  of  some  twitter 
specific clues such as retweets, hashtags, links, upper case words, emoticons, 
and  exclamation  and  question  marks.  for  sentiment  classification  of 
subjective tweets, the same set of features was also used.  

interestingly, in (raaijmakers and kraaij, 2008), it was found that character 
id165s of subwords rather than words id165s can also perform sentiment 
and subjectivity classification well. for example, for the sentence    this car 
rocks   ,  subword  character  bigrams  are  th,  hi,  is,  ca,  ar,  ro,  oc,  ck,  ks.  in 
(raaijmakers,  truong  and  wilson,  2008)  and  (wilson  and  raaijmakers, 
2008),  word  id165s,  character  id165  and  phoneme  id165s  were  all 
experimented  and  compared  for  subjectivity  classification.  boostexter 
(schapire  and  singer,  2000)  was  used  as 
learning  algorithm. 
surprisingly, their experiments showed that character id165s performed the 
best, and phoneme id165s performed similarly to word id165s. 

the 

wilson,  wiebe  and  hwa  (2004)  pointed  out  that  a  single  sentence  may 
contain both subjective and objective clauses. it is useful to pinpoint such 
clauses. it is also useful to identify the strength of subjectivity. a study of 
automatic  subjectivity  classification  was  presented  to  classify  clauses  of  a 
sentence  by  the  strength  of  subjectivity  expressed  in  individual  clauses, 
down to four levels deep (neutral, low, medium, and high). neutral indicates 
the absence of subjectivity. strength classification thus subsumes the task of 
classifying  a  sentence  as  subjective  or  objective.  the  authors  used 
supervised  learning.  their  features  included  subjectivity  indicating  words 
and phrases, and syntactic clues generated from the dependency parse tree.  

benamara  et  al.  (2011)  performed  subjectivity  classification  with  four 
classes, s, oo, o and sn, where s means subjective and evaluative (their 
sentiment  can  be  positive  or  negative),  oo  means  positive  or  negative 
opinion  implied  in  an  objective  sentence  or  sentence  segment,  o  means 
objective with no opinion, and sn means subjective but non-evaluative (no 
positive  or  negative  sentiment).  this  classification  is  more  complete  and 
conforms  to  our  discussion  earlier  and  also  in  section  2.4,  which  showed 
that a subjective sentence may not be evaluative (with positive or negative 
sentiment) and an objective sentence can imply sentiment too. 

additional  works  on  subjectivity  classification  of  sentences  has  also  been 
done  in  arabic  (abdul-mageed,  diab  and  korayem,  2011)  and  urdu 
languages (mukund and srihari, 2010) based on different machine learning 

48 
 

id31 and opinion mining 

algorithms using general and language specific features.  

4.2 

sentence sentiment classification  

if  a  sentence  is  classified  as  being  subjective,  we  determine  whether  it 
expresses a positive or negative opinion. supervised learning again can be 
applied just like that for document-level sentiment classification, and so can 
lexicon-based  methods.  before  discussing  existing  algorithms  (some 
algorithms do not use the subjectivity classification step), let us point out an 
implicit assumption made in much of the research on the subject. 

assumption  of  sentence-level  sentiment  classification:  a  sentence 

expresses a single sentiment from a single opinion holder.  

this assumption is appropriate for simple sentences with one sentiment, e.g., 
   the  picture  quality  of  this  camera  is  amazing.     however,  for  compound 
and  complex  sentences,  a  single  sentence  may  express  more  than  one 
sentiment. for example, the sentence,    the picture quality of this camera is 
amazing and so is the battery life, but the viewfinder is too small for such a 
great  camera,     expresses  both  positive  and  negative  sentiments  (or  it  has 
mixed sentiments). for    picture quality    and    battery life,    the sentence is 
positive,  but  for     viewfinder,     it  is  negative.  it  is  also  positive  about  the 
camera as a whole (which is the general aspect in section 2.1).   

sentiment 

subjective 

sentences,  yu 

classification  of 

for 
and 
hatzivassiloglou  (2003)  used  a  method  similar  to  that  in  (turney,  2002), 
which has been discussed in section 3.2. instead of using one seed word for 
positive and one for negative as in (turney, 2002), this work used a large set 
of  seed  adjectives.  furthermore,  instead  of  using  pmi,  this  work  used  a 
modified  log-likelihood  ratio  to  determine  the  positive  or  negative 
orientation  for  each  adjective,  adverb,  noun  and  verb.  to  assign  an 
orientation to each sentence, it used the average log-likelihood scores of its 
words. two thresholds were chosen using the training data and applied to 
determine  whether  the  sentence  has  a  positive,  negative,  or  neutral 
orientation.  the  same  problem  was  also  studied  in  (hatzivassiloglou  and 
wiebe, 2000) considering gradable adjectives.  

in (hu and liu, 2004), hu and liu proposed a lexicon-based algorithm for 
aspect  level  sentiment  classification,  but  the  method  can  determine  the 
sentiment  orientation  of  a  sentence  as  well.  it  was  based  on  a  sentiment 
lexicon  generated  using  a  id64  strategy  with  some  given  positive 
and  negative  sentiment  word  seeds  and  the  synonyms  and  antonyms 
relations  in  id138.  we  will  discuss  various  methods  for  generating 

 

49 

 

  id31 and opinion mining 

sentiment lexicons in chapter 6. the sentiment orientation of a sentence was 
determined by summing up the orientation scores of all sentiment words in 
the  sentence.  a  positive  word  was  given  the  sentiment  score  of  +1  and  a 
negative  word  was  given  the  sentiment  score  of  -1.  negation  words  and 
contrary words (e.g., but and however) were also considered. in (kim and 
hovy, 2004), a similar approach was also used. their method of compiling 
the  sentiment  lexicon  was  also  similar.  however,  they  determined  the 
sentiment  orientation  of  a  sentence  by  multiplying  the  scores  of  the 
sentiment  words  in  the  sentence.  again,  a  positive  word  was  given  the 
sentiment score of +1 and a negative word was given the sentiment score of 
-1.  the  authors  also  experimented  with  two  other  methods  of  aggregating 
sentiment scores but they were inferior. in (kim and hovy, 2007; kim and 
hovy,  2004;  kim  et  al.,  2006),  supervised  learning  was  used  to  identify 
several specific types of opinions. in (nigam and hurst, 2004), nigam and 
hurst  applied  a  domain  specific  lexicon  and  a  shallow  nlp  approach  to 
assessing the sentence sentiment orientation. 

in (gamon et al., 2005), a semi-supervised learning algorithm was used to 
learn  from  a  small  set  of  labeled  sentences  and  a  large  set  of  unlabeled 
sentences. the learning algorithm was based on expectation maximization 
(em) using the naive bayes as the base classifier (nigam et al., 2000). this 
work performed three-class classification, positive, negative, and    other" (no 
opinion or mixed opinion).  

in  (mcdonald  et  al.,  2007),  the  authors  presented  a  hierarchical  sequence 
learning  model  similar  to  conditional  random  fields  (crf)  (lafferty, 
mccallum and pereira, 2001) to jointly learn and infer sentiment at both the 
sentence-level  and  the  document-level.  in  the  training  data,  each  sentence 
was labeled with a sentiment, and each whole review was also labeled with a 
sentiment. they showed that learning both levels jointly improved accuracy 
for  both  levels  of  classification.  in  (t  ckstr  m  and  mcdonald,  2011),  a 
method was reported that learns from the document level labeling only but 
performs  both  sentence  and  document  level  sentiment  classification.  the 
method is thus partially supervised. in (t  ckstr  m and mcdonald, 2011), a 
fully supervised model and a partially supervised model were integrated to 
perform multi-level sentiment classification.  

in (hassan, qazvinian and radev, 2010), a method was proposed to identify 
attitudes about participants in online discussions. since the paper was only 
interested  in  the  discussion  recipient,  the  algorithm  only  used  sentence 
segments  with  second  person  pronouns.  its  first  step  finds  sentences  with 
attitudes  using  supervised  learning.  the  features  were  generated  using 
markov  models.  its  second  step  determines  the  orientation  (positive  or 
negative) of the attitudes, for which it used a lexicon-based method similar 

50 
 

id31 and opinion mining 

to  that  in  (ding,  liu  and  yu,  2008)  except  that  the  shortest  path  in  the 
dependence tree was utilized to determine the orientation when there were 
conflicting sentiment words in a sentence, while (ding, liu and yu, 2008) 
used words distance (see section 5.1).  

in (davidov, tsur and rappoport, 2010), sentiment classification of twitter 
postings (or tweets) was studied. each tweet is basically a single sentence. 
the authors took a supervised learning approach. apart from the traditional 
features,  the  method  also  used  hashtags,  smileys,  punctuations,  and  their 
frequent patterns. these features were shown to be quite effective.  

4.3  dealing with conditional sentences  

much of the existing research on sentence-level subjectivity classification or 
sentiment  classification  focused  on  solving  the  general  problem  without 
considering  that  different  types  of  sentences  may  need  very  different 
treatments. narayanan, liu and choudhary (2009) argued that it is unlikely 
to have a one-technique-fit-all solution because different types of sentences 
express  sentiments  in  very  different  ways.  a  divide-and-conquer  approach 
may be needed, i.e., focused studies on different types of sentences. their 
paper  focused  on  conditional  sentences,  which  have  some  unique 
characteristics  that  make  it  hard  for  a  system  to  determine  their  sentiment 
orientations. 

that  describe 

conditional  sentences  are  sentences 
implications  or 
hypothetical  situations  and  their  consequences.  such  a  sentence  typically 
contains  two  clauses:  the  condition  clause  and  the  consequent  clause,  that 
are  dependent  on  each  other.  their  relationship  has  significant  impact  on 
whether the sentence expresses a positive or negative sentiment. a simple 
observation is that sentiment words (e.g., great, beautiful, bad) alone cannot 
distinguish an opinion sentence from a non-opinion one, e.g.,     if someone 
makes a reliable car, i will buy it    and    if your nokia phone is not good, buy 
this  samsung  phone.   .  the  first  sentence  expresses  no  sentiment  towards 
any particular car, although    reliable    is a positive sentiment word, but the 
second sentence is positive about the samsung phone and it does not express 
an opinion about the nokia phone (although the owner of the nokia phone 
may be negative about it). hence, a method for determining sentiments in 
non-conditional  sentences  will  not  work  for  conditional  sentences.  a 
supervised learning approach was proposed to deal with the problem using a 
set of linguistic features, e.g., sentiment words/phrases and their locations, 
pos tags of sentiment words, tense patterns, conditional connectives, etc.  
another type of difficult sentences is the question sentences. for example, 

 

51 

 

  id31 and opinion mining 

   can anyone tell me where i can find a good nokia phone?    clearly has no 
opinion about any particular phone. however,    can anyone tell me how to 
fix this lousy nokia phone?    has a negative opinion about the nokia phone. 
to my knowledge, there is no study on this problem. i believe that for more 
accurate id31, we need to handle different types of sentences 
differently. much further research is needed in this direction.  

4.4 

dealing with sarcastic sentences  

sarcasm is a sophisticated form of speech act in which the speakers or the 
writers  say  or  write  the  opposite  of  what  they  mean.  sarcasm  has  been 
studied in linguistics, psychology and cognitive science (gibbs and colston, 
2007; gibbs, 1986; kreuz and caucci, 2007; kreuz and glucksberg, 1989; 
utsumi, 2000)). in the context of id31, it means that when one 
says  something  positive  he/she  actually  means  negative,  and  vice  versa. 
sarcasm is very difficult to deal with. some initial work has been done in 
(gonz  lez-ib    ez,  muresan  and  wacholder,  2011;  tsur,  davidov  and 
rappoport, 2010). based on my own experiences, sarcastic sentences are not 
very common in reviews of products and services, but they are very frequent 
in online discussions and commentaries about politics.   

in  (tsur,  davidov  and  rappoport,  2010),  a  semi-supervised  learning 
approach was proposed to identify sarcasms. it used a small set of labeled 
sentences (seeds), but did not use unlabeled examples. instead, it expanded 
the  seed  set  automatically  through  web  search.  the  authors  posited  that 
sarcastic  sentences  frequently  co-occur  in  texts  with  other  sarcastic 
sentences. an automated web search using each sentence in the seed training 
set  as  a  query  was  performed.  the  system  then  collected  up  to  50  search 
engine snippets for each seed example and added the collected sentences to 
the  training  set.  this  enriched  training  set  was  then  used  for  learning  and 
classification.  for  learning,  it  used  two  types  of  features,  pattern-based 
features  and  punctuation-based  features.  a  pattern  is  simply  an  ordered 
sequence  of  high  frequency  words.  two  criteria  were  also  designed  to 
remove too general and too specific patterns. these patterns are similar to 
sequential patterns in data mining (liu, 2006 and 2011). punctuation-based 
features  include  the  number  of     !   ,     ?     and  quotes,  and  the  number  of 
capitalized/all capital words in the sentence. for classification, a knn-based 
method  was  employed.  this  work,  however,  did  not  perform  sentiment 
classification. it only separated sarcastic and non-sarcastic sentences.  

the work of gonz  lez-ib    ez, muresan and wacholder (2011) studied the 
problem  in  the  context  of  sentiment  analysis  using  twitter  data,  i.e.,  to 

52 
 

id31 and opinion mining 

distinguish  sarcastic  tweets  and  non-sarcastic  tweets  that  directly  convey 
positive  or  negative  opinions  (neutral  utterances  were  not  considered). 
again,  a  supervised  learning  approach  was  taken  using  id166  and  logistic 
regression.  as  features,  they  used  unigrams  and  some  dictionary-based 
information.  the  dictionary-based  features  include  (i)  word  categories 
(pennebaker  et  al.,  2007);  ii)  id138  affect  (wna)  (strapparava  and 
valitutti,  2004);  and  iii)  a  list  of  interjections  (e.g.,  ah,  oh,  yeah),  and 
punctuations (e.g., !, ?). features like emoticons, and touser (which marks 
if a tweet is a reply to another tweet, signaled by <@user>) were also used. 
experimental  results  for  three-way  classification  (sarcastic,  positive  and 
negative)  showed  that  the  problem  is  very  challenging.  the  best  accuracy 
was  only  57%.  again,  this  work  did  not  classify  sarcastic  sentences  into 
positive and negative classes.  

4.5 

cross-language subjectivity and 
sentiment classification 

as  in  document-level  cross-language  sentiment  classification,  researchers 
have  also  studied  cross-language  subjectivity  classification  and  sentiment 
classification  at  the  sentence  level.  again,  the  research  focused  on  using 
extensive resources and tools available in english and automated translations 
to help build id31 systems in other languages which have few 
resources or tools. current research proposed three main strategies: 

(1)  translate test sentences in the target language into the source language 

and classify them using a source language classifier.  

(2).  translate a source language training corpus into the target language and 

build a corpus-based classifier in the target language. 

(3). translate a sentiment or subjectivity lexicon in the source language to 
the  target  language  and  build  a  lexicon-based  classifier  in  the  target 
language. 

kim and hovy (2006) experimented with (1) translating german emails to 
english  and  applied  english  sentiment  words  to  determine  sentiment 
orientation,  and  (2)  translating  english  sentiment  words  to  german,  and 
analyzing german emails using german sentiment words. mihalcea, banea 
and  wiebe  (2007)  also  experimented  with  translating  english  subjectivity 
words and phrases into the target language. in fact, they actually tried two 
translation  strategies  for  cross-language  subjectivity  classification.  first, 
they  derived  a  subjectivity  lexicon  for  the  new  language  (in  their  case, 
romanian) using an english subjectivity lexicon through translation. a rule-
based subjectivity classifier similar to that in (riloff and wiebe, 2003) was 

 

53 

 

  id31 and opinion mining 

then  applied  to  classify  romanian  sentences  into  subjective  and  objective 
classes.  the  precision  was  not  bad,  but  the  recall  was  poor.  second,  they 
derived  a  subjectivity-annotated  corpus  in  the  new  language  using  a 
manually  translated  parallel  corpus.  they  first  automatically  classified 
english sentences in the corpus into subjective and objective classes using 
some  existing  tools,  and  then  projected  the  subjectivity  class  labels  to  the 
romanian sentences in the parallel corpus using the available sentence-level 
alignment  in  the  parallel  corpus.  a  subjectivity  classifier  based  on 
supervised  learning  was  then  built  in  romanian  to  classify  romanian 
sentences.  in  this  case,  the  result  was  better  than  the  first  approach. 
however, it should be noted that the translation of the parallel corpus was 
done manually.  
in  (banea  et  al.,  2008),  three  sets  of  experiments  were  reported.  first,  a 
labeled corpus in the source language (english) was automatically translated 
into  the  target  language  (romanian).  the  subjectivity  labels  in  the  source 
language were then mapped to the translated version in the target language. 
second, the source language text was automatically labeled for subjectivity 
and  then  translated  into  the  target  language.  in  both  cases,  the  translated 
version  with  subjectivity  labels  in  the  target  language  was  used  to  train  a 
subjectivity classifier in the target language. third, the target language was 
translated into the source language, and then a subjectivity classification tool 
was used to classify the automatically translated source language text. after 
classification,  the  labels  were  mapped  back  into  the  target  language.  the 
resulting labeled corpus was then used to train a subjectivity classifier in the 
target  language.  the  final  classification  results  were  quite  similar  for  the 
three strategies.  
in  (banea,  mihalcea  and  wiebe,  2010),  extensive  experiments  for  cross-
language  sentence  level  subjectivity  classification  were  conducted  by 
translating from a labeled english corpus to 5 other languages. first, it was 
shown that using the translated corpus for training worked reasonably well 
consistently  for  all  5  languages.  combining  the  translated  versions  in 
different  languages  with  the  original  english  version  to  form  a  single 
training  corpus  can  also 
the  original  english  subjectivity 
classification itself. second, the paper demonstrated that by combining the 
predictions made by monolingual classifiers using majority vote, it was able 
to generate a high precision sentence-level subjectivity classifier.  

improve 

the  technique  in  (bautin,  vijayarenu  and  skiena,  2008)  also  translated 
documents  in  the  target  language  to  english  and  used  a  english  lexicon-
based  method  to  determine  the  sentiment  orientation  for  each  sentence 
containing  an  entity.  this  paper  actually  worked  at  the  aspect  level.  the 
sentiment classification method was similar to that in (hu and liu, 2004).  

in (kim, li and lee, 2010), a concept called the multi-lingual comparability 

54 
 

id31 and opinion mining 

was  introduced  to  evaluate  multi-lingual  subjectivity  analysis  systems.  by 
multilingual  comparability,  they  meant  the  level  of  agreement  in  the 
classification  results  of  a  pair  of  multilingual  texts  with  an  identical 
subjective  meaning.  using  a  parallel  corpus,  they  studied  the  agreement 
among  the  classification  results  of  the  source  language  and  the  target 
language  using  cohen   s  kappa.  for  the  target  language  classification, 
several  existing  translation  based  cross-language  subjectivity  classification 
methods were experimented. their results showed that classifiers trained on 
corpora translated from english to the target languages performed well for 
both subjectivity classification and multi-lingual comparability. 

in (lu et al., 2011), a slightly different problem was attempted. the paper 
assumed that there was a certain amount of sentiment labeled data available 
for  both  the  source  and  target  languages,  and  there  was  also  an  unlabeled 
parallel  corpus.  their  method  can  simultaneously  improve  sentiment 
classification for both languages. the method is a maximum id178-based 
em algorithm which jointly learns two monolingual sentiment classifiers by 
treating  the  sentiment  labels  in  the  unlabeled  parallel  text  as  unobserved 
latent  variables,  and  maximizing  the  regularized  joint  likelihood  of  the 
language-specific labeled data together with the inferred sentiment labels of 
the parallel text. in learning, it exploits the intuition that two sentences or 
documents that are parallel (i.e., translations of one another) should exhibit 
the same sentiment.  

4.6 

using discourse information for 
sentiment classification  

most  existing  works  on  both  the  document-level  and  the  sentence-level 
sentiment classification do not use the discourse information either among 
sentences  or  among  clauses  in  the  same  sentence.  sentiment  annotation  at 
the  discourse  level  was  studied  in  (asher,  benamara  and  mathieu,  2008; 
somasundaran,  ruppenhofer  and  wiebe,  2008).  asher,  benamara  and 
mathieu (2008) used five types of rhetorical relations: contrast, correction, 
support, result,  and continuation  with  attached  sentiment  information  for 
annotation.  somasundaran,  ruppenhofer  and  wiebe  (2008)  proposed  a 
concept  called  opinion  frame.  the  components  of  opinion  frames  are 
opinions and the relationships between their targets.  

in  (somasundaran  et  al.,  2009),  somasundaran  et  al.  performed  sentiment 
classification  based  on  the  opinion  frame  annotation.  the  classification 
algorithm  used  was  collective  classification  (bilgic,  namata  and  getoor, 

 

55 

 

  id31 and opinion mining 

2007), which performs classification on a graph. the nodes are sentences (or 
other expressions) that need to be classified, and the links are relations. in 
the discourse context, they are sentiments related discourse relations. these 
relations  can  be  used  to  generate  a  set  of  relational  features  for  learning. 
each node itself also generates a set of local features. the relational features 
allow the classification of one node to affect the classification of other nodes 
in the collective classification scheme. in (zhou et al., 2011), the discourse 
information  within  a  single  compound  sentence  was  used  to  perform 
sentiment  classification  of  the  sentence.  for  example,  the  sentence 
   although fujimori was criticized by the international community, he was 
loved by the domestic population because people hated the corrupted ruling 
class     is  a  positive  sentence  although  it  has  more  negative  opinion  words 
(see  also  section  4.7).  this  paper  used  pattern  mining  to  find  discourse 
patterns for classification.  

in (zirn et al., 2011), the authors proposed a  method to classify discourse 
segments.  each  segment  expresses  a  single  (positive  or  negative)  opinion. 
markov  logic  networks  were  used  for  classification  which  not  only  can 
utilize a sentiment lexicon but also the local/neighboring discourse context.  

4.7 

summary  

sentence  level  subjectivity  classification  and  sentiment  classification  goes 
further  than  document  level  sentiment  classification  as  it  moves  closer  to 
opinion  targets  and  sentiments  on  the  targets.  it  can  be  regarded  as  an 
intermediate step in the overall id31 task. however, it still has 
several shortcomings for many real-life applications:  

     in most applications, the user needs to know additional details, i.e., what 
entities  or  aspects  of  entities  are  liked  and  disliked.  as  the  document 
level, the sentence level analysis still does not do that.  

     although one may say that if we know the opinion targets (e.g., entities 
and  aspects,  or  topics),  we  can  assign  the  sentiment  orientation  of  a 
sentence to the targets in the sentence. however, this is insufficient:  
(1) many  complex  sentences  have  different  sentiments  on  different 
targets, e.g.,    trying out chrome because firefox keeps crashing    and 
   apple  is  doing  very  well  in  this  lousy  economy.     in  this  latter 
sentence, even the clause level classification is insufficient. we need 
to go to the opinion target or the aspect level.    

(2) although  a  sentence  may  have  an  overall  positive  or  negative  tone, 
some of its components may express opposite opinions. for example, 
some 
sentence  as  positive 

researchers 

follow 

regard 

the 

56 
 

id31 and opinion mining 

(neviarouskaya, prendinger and ishizuka, 2010; zhou et al., 2011): 

 

 

 

   despite the high unemployment rate, the economy is doing well.     

it is true that the overall tone of this sentence is positive or the author 
is trying to emphasize the positive side, but it does contain a negative 
sentiment on the unemployment rate, which we must not ignore. if we 
go to the aspect-level id31, the problem is solved. that 
is,  the  sentence  is  positive  about  the  overall  economy  but  negative 
about the unemployment rate.  

(3) sentence  level  sentiment  classification  cannot  deal  with  opinions  in 
comparative sentences, e.g.,    coke tastes better than pepsi.     in this 
case, we need different methods to extract and to analyze comparative 
opinions as they have quite different meanings from regular opinions. 
although this sentence clearly expresses an opinion, we cannot simply 
classify the sentence as being positive, negative or neutral.  

we  discuss  aspect-level  sentiment  analysis  in  the  next  chapter  and 
comparative opinion analysis in chapter 8.  

 

57 

 

  id31 and opinion mining 

chapter 5 

aspect-based id31  

 
following the natural progression of chapters, this chapter should focus on 
phrase and word-level sentiment classification as the last two chapters were 
about  document  and  sentence-level  classification.  however,  we  leave  that 
topic to the next chapter. in this chapter, we focus on aspect-based sentiment 
analysis as it is time to deal with the full problem defined in chapter 2 and 
many phrase and word sentiments depend on aspect contexts.  

as we discussed in the two previous chapters, classifying opinion texts at the 
document  level  or  the  sentence  level  is  often  insufficient  for  applications 
because  they  do  not  identify  opinion  targets  or  assign  sentiments  to  such 
targets. even if we assume that each document evaluates a single entity, a 
positive opinion document about the entity does not mean that the author has 
positive opinions about all aspects of the entity. likewise, a negative opinion 
document does not  mean that the author is negative about everything. for 
more  complete  analysis,  we  need  to  discover  the  aspects  and  determine 
whether the sentiment is positive or negative on each aspect.  

to extract such details, we go to the aspect level, which means that we need 
the full model of chapter 2, i.e., aspect-based id31 (or opinion 
mining), which was also called the feature-based opinion mining in (hu and 
liu,  2004).  note  that  as  discussed  in  chapter  2,  the  opinion  target  is 
decomposed  into  entity  and  its  aspects.  the  aspect  general  is  used  to 
represent the entity itself in the result. thus aspect-based id31 
covers both entities and aspects. it also introduces a suite of problems which 
require deeper nlp capabilities and produce a richer set of results.  

recall that, at the aspect level, the objective is to discover every quintuple 
(ei, aij, sijkl, hk, tl) in a given document d. to achieve this goal, six tasks have 
to  be performed.  this  chapter  mainly  focuses  on  the  two  core  tasks  listed 
below. they have been studied extensively by researchers. the other tasks 
will also be covered but relatively briefly.  

1.  aspect extraction: this task extracts aspects that have been evaluated. 
for  example,  in  the  sentence,     the  voice  quality  of  this  phone  is 
amazing,    the aspect is    voice quality    of the entity represented by    this 
phone.    note that    this phone    does not indicate the aspect general 
here because the evaluation is not about the phone as a whole, but only 
about  its  voice  quality.  however,  the  sentence     i  love  this  phone    

58 
 

id31 and opinion mining 

evaluates the phone as a whole, i.e., the general aspect of the entity 
represented  by     this  phone.     bear  in  mind  whenever  we  talk  about  an 
aspect, we must know which entity it belongs to. in our discussion below, 
we often omit the entity just for simplicity of presentation. 

2.  aspect  sentiment  classification:  this  task  determines  whether  the 
opinions on different aspects are positive, negative, or neutral. in the first 
example above, the opinion on the    voice quality    aspect is positive. in 
the second, the opinion on the aspect general is also positive.   

note that it is possible that in an application the opinion targets are given 
because the user is only interested in these particular targets (e.g., the bmw 
and ford brands). in that case, we do not need to perform entity or aspect 
extraction, but only to determine the sentiments on the targets.  

5.1 

aspect sentiment classification 

we study the second task first, i.e., determining the orientation of sentiment 
expressed on each aspect in a sentence. there are two main approaches, i.e., 
the supervised learning approach and the lexicon-based approach.  

for the supervised learning approach, the learning based methods used for 
sentence-level and clause-level sentiment classification discussed in chapter 
4  are  applicable.  in  (wei  and  gulla,  2010),  a  hierarchical  classification 
model was also proposed. however, the key issue is how to determine the 
scope  of  each  sentiment  expression,  i.e.,  whether  it  covers  the  aspect  of 
interest  in  the  sentence.  the  current  main  approach  is  to  use  parsing  to 
determine the dependency and the other relevant information. for example, 
in  (jiang  et  al.,  2011),  a  dependency  parser  was  used  to  generate  a  set  of 
aspect  dependent  features  for  classification.  a  related  approach  was  also 
used in (boiy and moens, 2009), which weights each feature based on the 
position  of  the  feature  relative  to  the  target  aspect  in  the  parse  tree.  for 
comparative sentences,    than    or other related words can be used to segment 
a sentence (ding, liu and zhang, 2009; ganapathibhotla and liu, 2008).  

supervised learning  is  dependent  on the  training  data.  as  we  discussed  in 
section 3.4, a model or classifier trained from labeled data in one domain 
often performs  poorly  in  another  domain.  although  domain  adaptation  (or 
transfer  learning)  has  been  studied  by  researchers  (section  3.4),  the 
technology is still far from mature, and the current methods are also mainly 
used for document level sentiment classification as documents are long and 
contain more features for classification than individual sentences or clauses. 
thus,  supervised  learning  has  difficulty  to  scale  up  to  a  large  number  of 
application domains.   

 

59 

 

  id31 and opinion mining 

the lexicon-based approach can avoid some of the issues (ding, liu and yu, 
2008;  hu  and  liu,  2004),  and  has  been  shown  to  perform  quite  well  in  a 
large  number  of  domains.  such  methods  are  typically  unsupervised.  they 
use a sentiment lexicon (which contains a list of sentiment words, phrases, 
and  idioms),  composite  expressions,  rules  of  opinions  (section  5.2),  and 
(possibly) the sentence parse tree to determine the sentiment orientation on 
each aspect in a sentence. they also consider sentiment shifters, but-clauses 
(see  below)  and  many  other  constructs  which  may  affect  sentiments.  of 
course, the lexicon-based approach also has its own shortcomings, which we 
will  discuss  later.  an  extension  of  this  method  to  handling  comparative 
sentences will be discussed in section 8.2. below, we introduce one simple 
lexicon-based method to give a flavor of this approach. the method is from 
(ding, liu and yu, 2008) and it has four steps. here, we assume that entities 
and aspects are known. their extraction will be discussed in section 5.3.  

1.  mark  sentiment  words  and  phrases:  for  each  sentence  that  contains 
one or more aspects, this step marks all sentiment words and phrases in 
the  sentence. each positive  word  is  assigned  the  sentiment  score  of  +1 
and  each  negative  word  is  assigned  the  sentiment  score  of     1.  for 
example, we have the sentence,    the voice quality of this phone is not 
good, but the battery life is long.    after this step, the sentence becomes 
   the voice quality of this phone is not good [+1], but the battery life is 
long     because     good     is  a  positive  sentiment  word  (the  aspects  in  the 
sentence are italicized). note that    long    here is not a sentiment word as 
it does not indicate a positive or negative sentiment by itself in general, 
but we can infer its sentiment in this context shortly. in fact,    long    can 
be  regarded  as  a  context-dependent  sentiment  word,  which  we  will 
discuss  in  chapter  6.  in  the  next  section,  we  will  see  some  other 
expressions that can give or imply positive or negative sentiments.  

2.  apply  sentiment  shifters:  sentiment  shifters  (also  called  valence 
shifters in (polanyi and zaenen, 2004)) are words and phrases that can 
change  sentiment  orientations.  there  are  several  types  of  such  shifters. 
negation  words  like  not,  never,  none,  nobody,  nowhere,  neither,  and 
cannot are the most common type. this step turns our sentence into    the 
voice quality of this phone is not good[-1], but the battery life is long    
due to the negation word    not.    we will discuss several other types of 
sentiment shifters in the next section. note that not every appearance of a 
sentiment shifter changes the sentiment orientation, e.g.,    not only     but 
also.    such cases need to be dealt with care. that is, such special uses 
and patterns need to be identified beforehand.  

3.  handle  but-clauses:  words  or  phrases  that  indicate  contrary  need 
special  handling  because  they  often  change  sentiment  orientations  too. 

60 
 

id31 and opinion mining 

the most commonly used contrary word in english is    but   . a sentence 
containing  a  contrary  word  or  phrase  is  handled  by  applying  the 
following rule: the sentiment orientations before the contrary word (e.g., 
but) and after the contrary word are opposite to each other if the opinion 
on  one  side  cannot  be  determined.  the  if-condition  in  the  rule  is  used 
because  contrary  words  and  phrases  do  not  always  indicate  an  opinion 
change,  e.g.,     car-x  is  great,  but  car-y  is  better.     after  this  step,  the 
above  sentence  is  turned  into     the  voice  quality  of  this  phone  is  not 
good[-1], but the battery life is long[+1]    due to    but    ([+1] is added at 
the  end  of  the  but-clause).  notice  here,  we  can  infer  that     long     is 
positive  for     battery  life   .  apart  from  but,  phrases  such  as     with  the 
exception of,       except that,    and    except for    also have the meaning of 
contrary and are handled in the same way. as in the case of negation, not 
every  but  means  contrary,  e.g.,     not  only       but  also.     such  non-but 
phrases containing    but    also need to be identified beforehand.  

4.  aggregate opinions: this step applies an opinion aggregation function 
to the resulting sentiment scores to determine the final orientation of the 
sentiment  on  each  aspect  in  the  sentence.  let  the  sentence  be s,  which 
contains  a  set  of  aspects  {a1,     ,  am}  and  a  set  of  sentiment  words  or 
phrases {sw1,    , swn} with their sentiment scores obtained from steps 1-
3. the sentiment orientation for each aspect ai in s is determined by the 
following aggregation function:  

 

score
(

sa
),
i

   

   

ow

s
   

j

sw
(

.
j
sw

so
a
,
i

j

dist

,

 

)

(5) 

where swj is an sentiment word/phrase in s, dist(swj, ai) is the distance 
between  aspect  ai  and  sentiment  word  swj  in  s.  swj.so  is  the  sentiment 
score of swi. the multiplicative inverse is used to give lower weights to 
sentiment  words  that  are  far  away  from  aspect  ai.  if  the  final  score  is 
positive, then the opinion on aspect ai in s is positive. if the final score is 
negative,  then  the  sentiment  on  the  aspect  is  negative.  it  is  neutral 
otherwise.  

this simple algorithm performs quite well in many cases. it is able to handle 
the  sentence     apple  is  doing  very  well  in  this  bad  economy     with  no 
problem. note that there are many other opinion aggregation methods. for 
example, (hu and liu, 2004) simply summed up the sentiment scores of all 
sentiment words in a sentence or sentence segment. kim, and hovy (2004) 
used multiplication of sentiment scores of words. similar methods were also 
employed by other researchers (wan, 2008; zhu et al., 2009). 

to  make  this  method  even  more  effective,  we  can  determine  the  scope of 
each individual sentiment word instead of using words distance as above. in 

 

61 

 

  id31 and opinion mining 

this  case,  parsing  is  needed  to  find  the  dependency  as  in  the  supervised 
method discussed above. we can also automatically discover the sentiment 
orientation of context dependent words such as    long    above. more details 
will  be  given  in  chapter  6.  in  fact,  the  above  simple  approach  can  be 
enhanced in many directions. for example, blair-goldensohn et al. (2008) 
integrated the  lexicon-based  method  with  supervised learning.  kessler  and 
nicolov  (2009)  experimented  with  four  different  strategies  of  determining 
the sentiment on each aspect/target (including a ranking method). they also 
showed several interesting statistics on why it is so hard to link sentiment 
words to their targets based on a large amount of manually annotated data.  

along with aspect sentiment classification research, researchers also studied 
the aspect sentiment rating prediction problem which has mostly been done 
together with aspect extraction in the context of id96, which we 
discuss in section 5.3.4.  

as indicated above, apart from sentiment words and phrases, there are many 
other  types  of  expressions  that  can  convey  or  imply  sentiments.  most  of 
them  are  also  harder  to  handle.  below,  we  list  some  of  them,  which  are 
called the basic rules of opinions (liu, 2010).  

5.2 

basic rules of opinions and 
id152 

an  opinion  rule  expresses  a  concept  that  implies  a  positive  or  negative 
sentiment.  it  can  be  as  simple  as  individual  sentiment  words  with  their 
implied sentiments or compound expressions that  may need commonsense 
or domain knowledge to determine their orientations. this section describes 
some of these rules. one way of representing these rules is to use the idea of 
id152 (dowty, wall and peters, 1981; montague, 1974), 
which states that the meaning of a compound expression is a function of the 
meaning  of  its  constituents  and  of  the  syntactic  rules  by  which  they  are 
combined. below, we first describe the rules at the conceptual level without 
considering how they may be expressed in actual sentences because many of 
these rules can be expressed in numerous ways and can also be domain and 
context dependent. after that, we go to the expression level to discuss the 
current  research  on  compositional  semantics  in  the  context  of  sentiment 
analysis, which aims to combine more than one input constituent expressions 
to derive an overall sentiment orientation for the composite expression.  
the  rules  are  presented  using  a  formalism  similar  to  the  bnf  form.  the 
rules are from (liu, 2010).  

62 
 

id31 and opinion mining 

1.  positive  
2. 
3. 
4 
5.  negative  
6. 
7. 
8. 

::=  p 

|  po 
|   sentiment_shifter  n  
|  sentiment_shifter  ne 

::=   n 
|  ne 
|   sentiment_shifter  p 
|  sentiment_shifter  po 

the  non-terminals  p  and  po  represent  two  types  of  positive  sentiment 
expressions. p indicates an atomic positive expression, a word or a phrase, 
while po represents a positive expression composed of multiple expressions. 
similarly, the non-terminals n and ne also represent two types of negative 
sentiment  expressions.     sentiment_shifter  n     and     sentiment_shifter  ne    
represent the negation of n and ne, respectively, and    sentiment_shifter p    
and    sentiment_shifter po    represent the negation of p and po, respectively. 
we need to note that these are not expressed in the actual bnf form but a 
pseudo language stating some abstract concepts. it is hard to specify them 
precisely  because  in  an  actual  sentence,  the  sentiment  shifter  may  be  in 
many different forms and can appear before or after n, ne, p, or po and 
there may be words between the sentiment shifter and positive (or negative) 
sentiment expressions. positive and negative are the final sentiments 
used to determine the opinions on the targets/aspects in a sentence. 

sentiment_shifters  (or  valence  shifters  (polanyi  and  zaenen,  2004)): 
negation  words  like  not,  never,  none,  nobody,  nowhere,  neither,  and 
cannot are the most common type of sentiment shifters. modal auxiliary 
verbs  (e.g.,  would,  should,  could,  might,  must,  and  ought)  are  another 
type, e.g.,    the brake could be improved,    which may change sentiment 
orientation, but not always. some presuppositional items are yet another 
type. this case is typical for adverbs like barely and hardly as shown by 
comparing    it works    with    it hardly works.       works    indicates positive, 
but     hardly  works     does  not:  it  presupposes  that  better  was  expected. 
words like fail, omit, neglect behave similarly, e.g.,    this camera fails to 
impress me.    furthermore, sarcasm often changes orientations too, e.g., 
   what a great car, it failed to start the first day.    although it may not be 
hard  to  recognize  such  shifters  manually,  spotting  them  and  handling 
them correctly in actual sentences by an automated system is challenging 
(see section 4.4). also, the rules 11-14 below can be seen as sentiment 
shifters  as  well.  we  present  them  separately  because  they  also  cover 
comparative  opinions.  note  that  several  researchers  also  studied  the 
application  scope  of  negations  (ikeda  et  al.,  2008;  jia,  yu  and  meng, 
2009;  li  et  al.,  2010;  morante,  schrauwen  and  daelemans,  2011).  we 

 

63 

 

  id31 and opinion mining 

will  discuss  more  about  sentiment  shifters  when  we  discuss  sentiment 
composition.   

we now define n, ne, p, and po, which contain no sentiment shifters. we 
group  these  expressions  into  six  conceptual  categories  based  on  their 
specific characteristics.  

1.  sentiment  word  or  phrase:  this  is  the  simplest  and  also  the  most 
commonly used category, in which sentiment words or phrases alone can 
imply positive or negative opinions on aspects, e.g.,    good    in    the voice 
quality is good.    these words or phrases are reduced to p and n.   

9. 
p  
10.  n  

::=  a_positive_sentiment_word_or_phrase  
::=  a_negative_sentiment_word_or_phrase 

again,  the  details  of  the  right-hand  sides  are  not  specified  (which  also 
apply to all the subsequent rules). much of the current research only uses 
words and phrases in this category.  

2.  decreased and increased quantity of an opinionated item (n and p): this 
set of rules is similar to the negation (or sentiment shifter) rules 3, 4, 7, 
and  8  above.  they  express  that  decreasing  or  increasing  the  quantity 
associated with an opinionated item (often nouns and noun phrases) can 
change  the  orientation  of  the  sentiment.  for  example,  in  the  sentence 
   this drug reduced my pain significantly,       pain    is a negative sentiment 
word, and the reduction of    pain    indicates a desirable effect of the drug. 
thus, decreased pain implies a positive opinion on the drug. the concept 
of decreasing also extends to removal and disappearance, e.g.,    my pain 
disappeared after taking the drug.    we then have the following rules: 

11.  po  
12. 
13.  ne  
14. 

::=  less_or_decreased  n  
|  more_or_increased  p 
::=  less_or_decreased  p 

|   more_or_increased  n  

note that rules 12 and 14 do not change of sentiment orientation, but they 
can  change  the  intensity  of  an  opinion.  the  actual  words  or  phrases 
representing the concepts of less_or_decreased and more_or_increased in 
a sentence may appear before or after n or p, e.g.,    my pain has subsided 
after taking the drug,    and    this drug has reduced my pain.    

3.  high,  low,  increased  and  decreased  quantity  of  a  positive  or  negative 
potential  item:  for  some  items,  a  small  value/quantity  of  them  is 
negative, and a large value/quantity of them is positive, e.g.,    the battery 
life is short    and    the battery life is long.    we call such items positive 
potential items (ppi). here    battery life    is a positive potential item. for 
some other aspects, a small value/quantity of them is positive, and a large 
value/quantity  of  them  is  negative,  e.g.,     this  phone  costs  a  lot     and 

64 
 

id31 and opinion mining 

   sony reduced the price of the camera.    such items are called negative 
potential  items  (npi).     cost     and     price     are  negative  potential  items. 
both positive and negative potential items themselves imply no opinions, 
i.e.,     battery  life     and     cost   ,  but  when  they  are  modified  by  quantity 
adjectives  or  quantity  change  words  or  phrases,  positive  or  negative 
sentiments may be implied. the following rules cover these cases:  

::=  no_low_less_or_decreased_quantity_of  npi  
|  large_larger_or_increased_quantity_of  ppi  
::=  no_low_less_or_decreased_quantity_of  ppi 
|  large_larger_or_increased_quantity_of  npi 

15.  po  
16. 
17.  ne  
18. 
19.  npi   ::=  a_negative_potential_item 
20.  ppi  
::=  a_positive_potential_item 

in (wen and wu, 2011), a id64 and classification method was 
proposed to discover ppi and npi in chinese.  

4.  desirable  or  undesirable  fact:  the  rules  above  all  contain  some 
subjective  expressions.  but  objective expressions  can  imply  positive  or 
negative  sentiments  too  as  they  can  describe  desirable  and  undesirable 
facts. such sentences often do not use any sentiment words. for example, 
the sentence    after my wife and i slept on the mattress for two weeks, i 
saw a mountain in the middle    clearly implies a negative opinion about 
the  mattress.  however,  the  word     mountain     itself  does  not  carry  any 
opinion. thus, we have the following two rules: 

21.  p  
22.  n  

::=  desirable_fact 
::=  undesirable_fact 

5.  deviation from the norm or a desired value range: in some application 
domains, the value of an item has a desired range or norm. if the value 
deviates  from  the  normal  range,  it  is  negative,  e.g.,     after  taking  the 
drug, my blood pressure went to 410.    such sentences are often objective 
sentences as well. we thus have the following rules: 

23.  p  
24.  n  

::=  within  the_desired_value_range  
::=  deviate_from  the_desired_value_range  

6.  produce and consume resource and waste: if an entity produces a large 
quantity of resources, it is desirable (or positive). if it consumes a large 
quantity  of  resources,  it  is  undesirable  (or  negative).  for  example, 
electricity  is  a  resource.  the  sentence,     this  computer  uses  a  lot  of 
electricity     gives  a  negative  opinion  about  power  consumption  of  the 
computer. likewise, if an entity produces a large quantity of wastes, it is 
negative. if it consumes a large quantity of wastes, it is positive. these 
give us the following rules:  

 

65 

 

  id31 and opinion mining 

25.  p  
26. 
27. 
28. 
29.  n  
30. 
31. 
32. 

::=  produce  a_large_quantity_of_or_more  resource 

|   produce  no,_little_or_less  waste 
|   consume  no,_little_or_less  resource 
|   consume  a_large_quantity_of_or_more  waste 

::=  produce  no,_little_or_less  resource  

|   produce  some_or_more  waste 
|   consume  a_large_quantity_of_or_more  resource 
|   consume  no,_little_or_less  waste 

these  conceptual  rules  can  appear  in  many  (seemly  unlimited  number  of) 
forms using different words and phrases in actual sentences, and in different 
domains they may also manifest in different ways. thus, they are very hard 
to recognize. without recognizing them, the rules cannot be applied.  

this  set  of  conceptual  rules  is  by  no  means  the  complete  set  that governs 
opinions or sentiments. in fact, there are others, and with further research, 
more rules may be discovered. it is also important to note that like individual 
sentiment  words  an  occurrence  of  any  of  the  rules  in  a  sentence  does  not 
always  imply  opinions.  for  example,     i  want  a  car  with  high  reliability    
does not express a positive or negative opinion on any specific car, although 
   high  reliability     satisfies  rule  16.  more  complex  rules  or  discourse  level 
analysis may be needed to deal with such sentences. 

we now discuss the existing work applying the principle of compositionality 
to express some of the above rules at the expression level. the most studied 
composition  rules  are  those  related  to  sentiment  reversal,  which  are 
combinations of sentiment shifters and positive or negative sentiment words, 
e.g.,    not    & pos(   good   ) => neg(   not good   ). we have discussed them 
at length above. another main type is represented by rules 11 to 14 above, 
e.g.,    reduced    & neg(   pain   ) => pos(   reduced pain   ).  

such  composition  rules  can  express  some  of  the  opinion  rules  and  also 
certain other expression level sentiment compositions. apart from the above 
two  composition  types,  moilanen  and  pulman  (2007)  also  introduced 
sentiment  conflict,  which  is  used  when  multiple  sentiment  words  occur 
together, e.g.,    terribly good   . conflict resolution is achieved by ranking the 
constituents on the basis of relative weights assigned to them dictating which 
constituent is more important with respect to sentiment.  

in (neviarouskaya, prendinger and ishizuka, 2010), six types of composition 
rules  were  introduced,  i.e.,  sentiment  reversal,  aggregation,    propagation, 
domination,  neutralization,  and  intensification.  sentiment  reversal  is  the 
same as what we have discussed above. aggregation is similar to sentiment 
conflict  above,  but  defined  differently.  if  the  sentiments  of  terms  in 
adjective-noun,  noun-noun,  adverb-adjective,  adverb-verb  phrases  have 

66 
 

id31 and opinion mining 

opposite directions, mixed polarity with dominant polarity of a pre-modifier 
is  assigned  to  the  phrase,  e.g.,  pos(   beautiful   )  &  neg(   fight   )  => 
posneg(   beautiful fight   ). the rule of propagation is applied when a verb of 
   propagation    or    transfer    type is used in a phrase/clause and the sentiment 
of an argument that has prior neutral polarity needs to be determined, e.g., 
prop-pos(   to  admire   )  &     his  behavior     =>  pos(   his  behavior   );     mr. 
x    & trans(   supports   ) & neg(   crime business   ) => neg(   mr. x   ). the 
rules of domination are: (1) if polarities of a verb and an object in a clause 
have  opposite  directions,  the  polarity  of  verb  is  prevailing  (e.g.,  neg(   to 
deceive   ) & pos(   hopes   ) => neg(   to deceive hopes   )); (2) if a compound 
sentence  joints  clauses  using  the  coordinate  connector     but   ,  the  attitude 
features  of  the  clause  following  after  the  connector  are  dominant  (e.g., 
   neg(   it  was  hard  to  climb  a  mountain  all  night  long   ),  but  pos(   a 
magnificent  view  rewarded the traveler  at  the  morning   ).     =>  pos(whole 
sentence)). the rule of neutralization is applied when a preposition-modifier 
or  condition  operator  relates  to  a  sentiment  statement,  e.g.,     despite     & 
neg(   worries   )  =>  neut(   despite  worries   ).  the  rule  of  intensification 
strengthens 
e.g., 
pos_score(   happy   )  <  pos_score(   extremely  happy   )).  additional  related 
works  can  be  found  in  (choi  and  cardie,  2008;  ganapathibhotla  and  liu, 
2008; min and park, 2011; nakagawa, inui and kurohashi, 2010; nasukawa 
and yi, 2003; neviarouskaya, prendinger and ishizuka, 2009; polanyi and 
zaenen, 2004; socher et al., 2011; yessenalina and cardie, 2011).  

or  weakens 

(intensity), 

sentiment 

score 

a 

as  we  can  see,  some  of  the  opinion  rules  have  not  been  expressed  with 
compositions,  e.g.,  those  involved  in  resource  usages  (rules  25   32). 
however,  it  is  possible  to  express  them  to  some  extent  using  triples  in 
(zhang and liu, 2011a). the desirable and undesirable facts or value ranges 
have not been included either (rules 21   24). they are, in fact, not directly 
related  to  composition  because  they  are  essentially  context  or  domain 
implicit sentiment terms, which need to be discovered in a domain corpus 
(zhang and liu, 2011b).  

5.3 

aspect extraction 

we now turn to aspect extraction, which can also be seen as an information 
extraction task. however, in the context of id31, some specific 
characteristics  of  the  problem  can  facilitate  the  extraction.  the  key 
characteristic is that an opinion always has a target. the target is often the 
aspect  or  topic  to  be  extracted  from  a  sentence.  thus,  it  is  important  to 
recognize each opinion expression and its target from a sentence. however, 

 

67 

 

  id31 and opinion mining 

we should also note that some opinion expressions can play two roles, i.e., 
indicating a positive or negative sentiment and implying an (implicit) aspect 
(target). for example, in    this car is expensive,       expensive    is a sentiment 
word and also indicates the aspect price. we will discuss implicit aspects in 
section  5.3.5.  here,  we  will  focus  on  explicit  aspect  extraction.  there  are 
four main approaches:  

1.  extraction based on frequent nouns and noun phrases  
2.  extraction by exploiting opinion and target relations 
3.  extraction using supervised learning 
4.  extraction using id96 

since  existing  research  on  aspect  extraction  (more  precisely,  aspect 
expression extraction) is mainly carried out in online reviews, we also use 
the  review  context  to  describe  these  techniques,  but  there  is  nothing  to 
prevent them being used on other forms of social media text.  
there are two common review formats on the web.  
format  1       pros,  cons,  and  the  detailed  review:  the  reviewer  first 
describes  some  brief  pros  and  cons  separately  and  then  writes  a 
detailed/full review. an example of such a review is given in figure 5.1.    
format 2     free format: the reviewer writes freely, i.e., no brief pros and 

cons. an example of such a review is given in figure 5.2. 

extracting  aspects  from  pros  and  cons  in  reviews  of  format  1  (not  the 
detailed review, which is the same as that in format 2) is a special case of 
extracting aspects from the full review and also relatively easy. in (liu, hu 
and cheng, 2005), a specific method based on a sequential learning method 
was proposed to extract aspects from pros and cons, which also exploited a 
key  characteristic  of  pros  and  cons,  i.e.,  they  are  usually  very  brief, 
consisting  of  short  phrases  or  sentence  segments.  each  segment  typically 
contains only one aspect. sentence segments can be separated by commas, 
periods, semi-colons, hyphens, &, and, but, etc. this observation helps the 
extraction algorithm to perform more accurately. 

since the same set of basic techniques can be applied to both pros and cons 
and full text, from now on we will not distinguish them, but will focus on 
different approaches.  

5.3.1 

finding frequent nouns and noun phrases  

this  method  finds  explicit  aspect  expressions  that  are  nouns  and  noun 
phrases  from  a  large  number  of  reviews  in  a  given  domain.  hu  and  liu 
(2004) used a data mining algorithm. nouns and noun phrases (or groups) 

68 
 

id31 and opinion mining 

my slr is on the shelf 
by camerafun4. aug 09    04 
pros: great photos, easy to use, very small 
cons: battery usage; included memory is stingy. 
i had never used a digital camera prior to purchasing this canon a70. 
i have always used a slr     read the full review 

figure 5.1. an example of a review of format 1. 

great camera., jun 3, 2004  
reviewer: jprice174 from atlanta, ga. 
i did a lot of research last year before i bought this camera... it kinda 
hurt to leave behind my beloved nikon 35mm slr, but i was going to 
italy, and i needed something smaller, and digital.  
the pictures coming out of this camera are amazing. the 'auto' feature 
takes  great  pictures  most  of  the  time.  and  with  digital,  you're  not 
wasting film if the picture doesn't come out.     

figure 5.2. an example of a review of format 2. 

were  identified  by  a  part-of-speech  (pos)  tagger.  their  occurrence 
frequencies are counted, and only the frequent ones are kept. a frequency 
threshold  can  be  decided  experimentally.  the  reason  that  this  approach 
works  is  that  when  people  comment  on  different  aspects  of  an  entity,  the 
vocabulary  that  they  use  usually  converges.  thus,  those  nouns  that  are 
frequently talked about are usually genuine and important aspects. irrelevant 
contents in reviews are often diverse, i.e., they are quite different in different 
reviews. hence, those infrequent nouns are likely to be non-aspects or less 
important aspects. although this method is very simple, it is actually quite 
effective. some commercial companies are using this  method with several 
improvements.  
the  precision  of  this  algorithm  was  improved  in  (popescu  and  etzioni, 
2005). their algorithm tried to remove those noun phrases that may not be 
aspects of entities. it evaluated each discovered noun phrase by computing a 
pointwise  mutual  information  (pmi)  score  between  the  phrase  and  some 
meronymy  discriminators  associated  with  the  entity  class,  e.g.,  a  camera 
class. the meronymy discriminators for the camera class are,    of camera,    
   camera  has,        camera  comes  with,     etc.,  which  were  used  to  find 
components  or  parts  of  cameras  by  searching  the  web.  the  pmi  measure 
was a simplified version of that in section 3.2: 

pmi

da
,(

)

   

hits
(
hits
a
)(

da
   
hits

)
d
)(

 

,

(4) 

where a is a candidate aspect identified using the frequency approach and d 
is  a  discriminator.  web  search  was  used  to  find  the  number  of  hits  of 

 

69 

 

  id31 and opinion mining 

individual terms and also their co-occurrences. the idea of this approach is 
clear.  if  the  pmi  value  of  a  candidate  aspect  is  too  low,  it  may  not  be  a 
component of the product because a and d do not co-occur frequently. the 
algorithm  also  distinguishes  components/parts  from  attributes  using 
id138   s is-a hierarchy (which enumerates different kinds of properties) 
and morphological cues (e.g.,    -iness,       -ity    suffixes).  

blair-goldensohn  et  al.  (2008)  refined  the  frequent  noun  and  noun  phrase 
approach  by  considering  mainly  those  noun  phrases  that  are  in  sentiment-
bearing  sentences  or in  some  syntactic patterns  which  indicate  sentiments. 
several  filters  were  applied  to  remove  unlikely  aspects,  e.g.,  dropping 
aspects which do not have sufficient mentions along-side known sentiment 
words. they also collapsed aspects at the word stem level, and ranked the 
discovered aspects by a manually tuned weighted sum of their frequency in 
sentiment-bearing sentences and the type of sentiment phrases/patterns, with 
appearances in phrases carrying a greater weight. using sentiment sentences 
is related to the approach in section 5.3.2.   

a frequency-based approach was also taken in (ku, liang and chen, 2006). 
the authors called the so discovered terms the major topics. their method 
also  made  use  of  the  tf-idf  scheme  considering  terms  at  the  document 
level and at the paragraph level. moghaddam and ester (2010) augmented 
the  frequency-based  approach  with  an  additional  pattern-based  filter  to 
remove  some  non-aspect  terms.  their  work  also  predicted  aspect  ratings. 
scaffidi  et  al.  (2007)  compared  the  frequency  of  extracted  frequent  nouns 
and noun phrases in a review corpus with their occurrence rates in a generic 
english corpus to identify true aspects.  

zhu  et  al.  (2009)  proposed  a  method  based  on  the  cvalue  measure  from 
(frantzi, ananiadou and mima, 2000) for extracting multi-word aspects. the 
cvalue method is also based on frequency, but it considers the frequency of 
multi-word  term  t,  the  length  of  t,  and  also  other  terms  that  contain  t. 
however, cvalue only helped find a set of candidates, which is then refined 
using a id64 technique with a set of given seed aspects. the idea of 
refinement is based on each candidate   s co-occurrence with the seeds.  

long, zhang and zhu (2010) extracted aspects (nouns) based on frequency 
and  information  distance.  their  method  first  finds  the  core  aspect  words 
using the frequency-based method. it then uses the information distance in 
(cilibrasi and vitanyi, 2007) to find other related words to an aspect, e.g., 
for aspect price, it may find    $    and    dollars   . all these words are then used 
to select reviews which discuss a particular aspect most.  

70 
 

id31 and opinion mining 

5.3.2 

using opinion and target relations  

since opinions have targets, they are obviously related. their relationships 
can  be  exploited  to  extract  aspects  which  are  opinion  targets  because 
sentiment  words  are  often  known.  this  method  was  used  in  (hu  and  liu, 
2004)  for  extracting  infrequent  aspects.  the  idea  is  as  follows:  the  same 
sentiment  word  can  be  used  to  describe  or  modify  different  aspects.  if  a 
sentence does not have a frequent aspect but has some sentiment words, the 
nearest noun or noun phrase to each sentiment word is extracted. since no 
parser was used in (hu and liu, 2004), the    nearest    function approximates 
the dependency relation between sentiment word and noun or noun phrase 
that  it  modifies,  which  usually  works  quite  well.  for  example,  in  the 
following sentence,  

   the software is amazing.    

if we know that    amazing    is a sentiment word, then    software    is extracted 
as an aspect. this idea turns out to be quite useful in practice even when it is 
applied  alone.  the  sentiment  patterns  method  in  (blair-goldensohn  et  al., 
2008) uses a similar idea. additionally, this relation-based method is also a 
useful method for discovering important or key aspects (or topics) in opinion 
documents because an aspect or topic is unlikely to be important if nobody 
expresses any opinion or sentiment about it.  

in (zhuang, jing and zhu, 2006), a dependency parser was used to identify 
such dependency relations for aspect extraction. somasundaran and wiebe 
(2009) employed a similar approach, and so did kobayashi et al. (kobayashi 
et al., 2006). the dependency idea was further generalized into the double-
propagation method for simultaneously extracting both sentiment words and 
aspects in (qiu et al., 2011) (to be discussed in section 5.5). in (wu et al., 
2009),  a  phrase  dependency  parser  was  used  rather  than  a  normal 
dependency parser for extracting noun phrases and verb phrases, which form 
candidate aspects. the system then employed a language model to filter out 
those  unlikely  aspects.  note  that  a  normal  dependency  parser  identifies 
dependency  of  individual  words  only,  but  a  phrase  dependency  parser 
identifies  dependency  of  phrases,  which  can  be  more  suitable  for  aspect 
extraction. the idea of using dependency relations has been used by many 
researchers for different purposes (kessler and nicolov, 2009).  

5.3.3 

using supervised learning  

aspect extraction can be seen as a special case of the general information 

 

71 

 

  id31 and opinion mining 

extraction  problem.  many  algorithms  based  on  supervised  learning  have 
been  proposed  in  the  past  for  information  extraction  (hobbs  and  riloff, 
2010;  mooney  and  bunescu,  2005;  sarawagi,  2008).  the  most  dominant 
methods  are  based  on  sequential  learning  (or  sequential  labeling).  since 
these  are  supervised  techniques,  they  need  manually  labeled  data  for 
training. that is, one needs to manually annotate aspects and non-aspects in 
a corpus. the current state-of-the-art sequential learning methods are hidden 
markov  models  (id48)  (rabiner,  1989)  and  conditional  random  fields 
(crf) (lafferty, mccallum and pereira, 2001). jin and ho (2009) applied a 
lexicalized  id48  model  to  learn  patterns  to  extract  aspects  and  opinion 
expressions.  jakob  and  gurevych  (jakob  and  gurevych,  2010)  used  crf. 
they trained crf on review sentences from different domains for a  more 
domain independent extraction. a set of domain independent features were 
also used, e.g. tokens, pos tags, syntactic dependency, word distance, and 
opinion sentences. li et al (2010) integrated two crf variations, i.e., skip-
crf and tree-crf, to extract aspects and also opinions. unlike the original 
crf, which can only use word sequences in learning, skip-crf and tree-
crf enable crf to exploit structure features. crf was also used in (choi 
and cardie, 2010). liu, hu and cheng (2005) and jindal and liu (2006b) 
used  sequential  pattern  rules.  these  rules  are  mined  based  on  sequential 
pattern mining considering labels (or classes).  

one  can  also  use  other  supervised  methods.  for  example,  the  method  in 
(kobayashi,  inui  and  matsumoto,  2007)  first  finds  candidate  aspect  and 
opinion  word  pairs  using  a  dependency  tree,  and  then  employs  a  tree-
structured classification method to learn and to classify the candidate pairs 
as being an aspect and evaluation relation or not. aspects are extracted from 
the  highest  scored  pairs.  the  features  used  in  learning  include  contextual 
clues, statistical co-occurrence clues, among others. yu et al. (2011) used a 
partially  supervised  learning  method  called  one-class  id166  (manevitz  and 
yousef, 2002) to extract aspects. using one-class id166, one only needs to 
label  some  positive  examples,  which  are  aspects,  but  not  non-aspects.  in 
their case, they only extracted aspects from pros and cons of review format 
2  as  in  (liu,  hu  and  cheng,  2005).  they  also  clustered  those  synonym 
aspects and ranked aspects based on their frequency and their contributions 
to  the  overall  review  rating  of  reviews.  ghani  et  al.  (2006)  used  both 
traditional  supervised  learning  and  semi-supervised  learning  for  aspect 
extraction.  kovelamudi  et  al.,  (2011)  used  a  supervised  method  but  also 
exploited some relevant information from wikipedia. 

72 
 

id31 and opinion mining 

5.3.4 

using topic models 

in recent years, statistical topic models have emerged as a principled method 
for  discovering  topics  from  a  large  collection  of  text  documents.  topic 
modeling is an unsupervised learning method that assumes each document 
consists  of  a  mixture  of  topics  and  each  topic  is  a  id203  distribution 
over words. a topic model is basically a document generative model which 
specifies  a  probabilistic  procedure  by  which  documents  can  be  generated. 
the output of id96 is a set of word clusters. each cluster forms a 
topic and is a id203 distribution over words in the document collection.  

there  were  two  main  basic  models,  plsa  (probabilistic  latent  semantic 
analysis) (hofmann, 1999) and lda (id44) (blei, ng 
and  jordan,  2003;  griffiths  and  steyvers,  2003;  steyvers  and  griffiths, 
2007).  technically,  topic  models  are  a  type  of  graphical  models  based  on 
bayesian  networks.  although  they  are  mainly  used  to  model  and  extract 
topics from text collections, they can be extended to model many other types 
of  information  simultaneously.  for  example,  in  the  sentiment  analysis 
context,  one can  design  a joint  model  to  model  both  sentiment  words  and 
topics  at  the  same  time,  due  to  the  observation  that  every  opinion  has  a 
target. for readers who are not familiar with topic models, id114 
or id110s, apart from reading the id96 literature, the 
   pattern recognition and machine learning    book by christopher m. bishop 
(bishop, 2006) is an excellent source of background knowledge.   

intuitively  topics  from  topic  models  are  aspects  in  the  sentiment  analysis 
context.  topic  modeling  can  thus  be  applied  to  extract  aspects.  however, 
there is also a difference. that is, topics can cover both aspect words and 
sentiment  words.  for  sentiment  analysis,  they  need  to  be  separated.  such 
separations  can  be  achieved  by  extending  the  basic  model  (e.g.,  lda)  to 
jointly model both aspects and sentiments. below, we give an overview of 
the  current  research  in  sentiment  analysis  that  has  used  topic  models  to 
extract aspects and to perform other tasks. note that topic models not only 
discover aspects but also group synonym aspects.  

mei et al (mei et al., 2007) proposed a joint model for id31. 
specifically, they built an aspect-sentiment mixture model, which was based 
on  an  aspect  (topic)  model,  a  positive  sentiment  model,  and  a  negative 
sentiment model learned with the help of some external training data. their 
model was based on plsa. most other models proposed by researchers are 
based on lda.  

in (titov and mcdonald, 2008), the authors showed that global topic models 
such as lda (blei, ng and jordan, 2003) might not be suitable for detecting 

 

73 

 

  id31 and opinion mining 

aspects.  the  reason  is  that  lda  depends  on  topic  distribution  differences 
and  word  co-occurrences  among  documents  to  identify  topics  and  word 
id203 distribution in each topic. however, opinion documents such as 
reviews about a particular type of products are quite homogenous, meaning 
that every document talks about the same aspects, which makes global topic 
models  ineffective  and  are  only  effective  for  discovering  entities  (e.g., 
different  brands  or  product  names).  the  authors  then  proposed  the 
multigrain topic models. the global model discovers entities while the local 
model discovers aspects using a few sentences (or a sliding text window) as 
a document. here, each discovered aspect is a unigram language model, i.e., 
a multinomial distribution over words. different words expressing the same 
or related facets are automatically grouped together under the same aspect. 
however, this technique does not separate aspects and sentiment words.  

branavan  et  al.  (2008)  proposed  a  method  which  made  use  of  the  aspect 
descriptions  as  keyphrases  in  pros  and  cons  of  review  format  1  to  help 
finding aspects in the detailed review text. their model consists of two parts. 
the  first  part  clusters  the  keyphrases  in  pros  and  cons  into  some  aspect 
categories based on distributional similarity. the second part builds a topic 
model modeling the topics or aspects in the review text. their final graphical 
model models these two parts simultaneously. the two parts are integrated 
based on the idea that the model biases the assignment of hidden topics in 
the review text to be similar to the topics represented by the keyphrases in 
pros and cons of the review, but it also permits some words in the document 
to  be  drawn  from  other  topics  not  represented  by  the  keyphrases.  this 
flexibility  in  the  coupling  allows  the  model  to  learn  effectively  in  the 
presence  of  incomplete  keyphrases,  while  still  encouraging  the  keyphrase 
id91 to cohere with the topics supported by the review text. however, 
this approach still does not separate aspects and sentiments.  

lin  and  he  (2009)  proposed  a  joint  topic-sentiment  model  by  extending 
lda,  where  aspect  words  and  sentiment  words  were  still  not  explicitly 
separated. brody and elhadad (2010) proposed to first identify aspects using 
topic  models  and  then  identify  aspect-specific  sentiment  words  by 
considering adjectives only. li, huang and zhu (2010) proposed two joint 
models,  sentiment-lda  and  dependency-sentiment-lda,  to  find  aspects 
with positive and negative sentiments. it does not find aspects independently 
and it does not separate aspect words and sentiment words. zhao et al. (zhao 
et  al.,  2010)  proposed  the  maxent-lda  (a  maximum  id178  and  lda 
combination) hybrid model to jointly discover both aspect words and aspect-
specific  opinion  words,  which  can  leverage  syntactic  features  to  help 
separate  aspects  and  sentiment  words.  the  joint  modeling  is  achieved 
through an indicator variable (also called a switch variable) which is drawn 

74 
 

id31 and opinion mining 

from  a  multinomial  distribution  governed  by  a  set  of  parameters.  the 
indicator variable determines whether a word in sentence is an aspect word, 
an  opinion  word  or  a  background  word.  maximum  id178  was  used  to 
learn the parameters of the variable using labeled training data.  

a joint model was also proposed in (sauper, haghighi and barzilay, 2011) 
which worked only on short snippets already extracted from reviews, e.g., 
   battery  life  is  the  best  i   ve  found.     it  combined  topic  modeling  with  a 
hidden  markov  model  (id48),  where  the  id48  models  the  sequence  of 
words with types (aspect word, sentiment word, or background word). their 
model is related to id48-lda proposed in (griffiths et al., 2005), which 
also  models  the  word  sequence.  variations  of  the  joint  topic  modeling 
approach were also taken in (liu et al., 2007), (lu and zhai, 2008) and (jo 
and oh, 2011).  

in (mukherjee and liu, 2012), a semi-supervised joint model was proposed, 
which  allows  the  user  to  provide  some  seed  aspect  terms  for  some 
topics/aspects in order to guide the id136 to produce aspect distributions 
that conform to the user   s need.  

another line of work using id96 aimed to associate aspects with 
opinion/sentiment  ratings,  i.e.,  to  predict  aspect  ratings  based  on  joint 
modeling  of  aspects  and  ratings.  titov  and  mcdonald  (2008)  proposed  a 
model to discover aspects from reviews and also to extract textual evidence 
from reviews supporting each aspect rating. lu, zhai and sundaresan (2009) 
defined the problem of rated aspect summarization of short comments from 
ebay.com.  their  aspect  extraction  was  based  on  a  topic  model  called 
structured plsa. this model can model the dependency structure of phrases 
in  short  comments.  to  predict  the  rating  for  each  aspect  in  a  comment,  it 
combined the overall rating of the comment and the classification result of a 
learned  classifier  for  the  aspect  based  on  all  the  comments.  wang  et  al. 
(2010) proposed a probabilistic rating regression model to assign ratings to 
aspects. their method first uses some given seed aspects to find more aspect 
words using a heuristic id64 method. it then predicts aspect ratings 
using  the  proposed  probabilistic  rating  regression  model,  which  is  also  a 
graphical model. the model makes use of review ratings and assumes that 
the overall rating of a review is a linear combination of its aspect ratings. 
the model parameters are estimated using the maximum likelihood (ml) 
estimator and an em style algorithm.  

a series of joint models were also proposed in (lakkaraju et al., 2011) based 
on  the  composite  topic  model  of  id48-lda  in  (griffiths  et  al.,  2005), 
which  considers  both  word  sequence  and  word-bag.  the  models  thus  can 
capture both syntactic structures and semantic dependencies similar to that 

 

75 

 

  id31 and opinion mining 

in (sauper, haghighi and barzilay, 2011). they are able to discover latent 
aspects  and  their  corresponding  sentiment  ratings.  moghaddam  and  ester 
(2011) also proposed a joint topic model to find and group aspects and to 
derive their ratings.  

although  topic  modeling  is  a  principled  approach  based  on  probabilistic 
inferencing and can be extended to model many types of information, it does 
have  some  weaknesses  which  limit  its  practical  use  in  real-life  sentiment 
analysis applications. one main issue is that it needs a large volume of data 
and a significant amount of tuning in order to achieve reasonable results. to 
make  matters  worse,  most  topic  modeling  methods  use  gibbs  sampling, 
which  produces  slightly  different  results  in  different  runs  due  to  mcmc 
(id115) sampling, which makes parameter tuning time 
consuming. while it is not hard for id96 to find those very general 
and  frequent  topics  or  aspects  from  a  large  document  collection,  it  is  not 
easy to find those locally frequent but globally not so frequent aspects. such 
locally  frequent  aspects  are  often  the  most  useful  ones  for  applications 
because they are likely to be most relevant to the specific entities that the 
user  is  interested  in.  those  very  general  and  frequent  aspects  can  also  be 
easily found by the methods discussed earlier. these methods can find less 
frequent aspects as well without the need of a large amount of data. in short, 
the results from current id96 methods are usually not granular or 
specific  enough  for  many  practical  sentiment  analysis  applications.  it  is 
more useful for the user to get some high level ideas about what a document 
collection is about.  

that being said, id96 is a powerful and flexible modeling tool. it 
is also very nice conceptually and mathematically. i expect that continued 
research  will  make  it  more  practically  useful.  one  promising  research 
direction  is  to  incorporate  more  existing  natural  language  and  domain 
knowledge  in  the  models.  there  are  already  some  initial  works  in  this 
direction  (andrzejewski  and  zhu,  2009;  andrzejewski,  zhu  and  craven, 
2009;  mukherjee  and  liu,  2012;  zhai  et  al.,  2011).  we  will  discuss  them 
section 5.6. however, i think they are still too statistics centric and come 
with their own limitations. it could be fruitful if we can shift more toward 
natural  language  and  knowledge  centric  for  a  more  balanced  approach. 
another  direction  would  be  to  integrate  topic  modeling  with  some  other 
techniques to overcome its shortcomings.  

apart  from  the  main  methods  discussed  above  and  in  the  previous  three 
sections, there are still other works on aspect extraction. for example, yi et 
al.  (2003)  used  a  mixture  language  model  and  likelihood  ratio  to  extract 
product  aspects.  ma  and  wan  (2010)  used  the  centering  theory  and 
supervised learning. meng and wang (2009) extracted aspects from product 

76 
 

id31 and opinion mining 

specifications,  which  are  structured  data.  kim  and  hovy  (2006)  used 
semantic  role  labeling.  stoyanov  and  cardie  (2008)  exploited  coreference 
resolution. toprak, jakob and gurevych (2010) designed a comprehensive 
annotation scheme for aspect-based opinion annotation. earlier annotations 
were partial and mainly for the special needs of individual papers. carvalho 
et  al.  (2011)  annotated  a  collection  of  political  debates  with  aspects  and 
other information.  

5.3.5  mapping implicit aspects  

in (hu and liu, 2004), two kinds of aspects were identified, explicit aspects 
and implicit aspects. however, it only dealt with explicit aspects. recall in 
section 2.1, we call aspects that are expressed as nouns and noun phrases the 
explicit aspects, e.g.,    picture quality    in    the picture quality of this camera 
is  great.     all  other  expressions  that  indicate  aspects  are  called  implicit 
aspects. there are many types of implicit aspect expressions. adjectives and 
adverbs  are  perhaps  the  most  common  types  because  most  adjectives 
describe  some  specific  attributes  or  properties  of  entities,  e.g.,  expensive 
describes    price,    and beautiful describes    appearance.    implicit aspects can 
be verbs too. in general, implicit aspect expressions can be very complex, 
e.g.,    this camera will not easily fit in a pocket.       fit in a pocket    indicates 
the aspect size.  

although  explicit  aspect  extraction  has  been  studied  extensively,  limited 
research has been done on mapping implicit aspects to their explicit aspects. 
in (su et al., 2008), a id91 method was proposed to map implicit aspect 
expressions,  which  were  assumed  to  be  sentiment  words,  to  their 
corresponding  explicit  aspects.  the  method  exploits 
the  mutual 
reinforcement relationship between an explicit aspect and a sentiment word 
forming a co-occurring pair in a sentence. such a pair may indicate that the 
sentiment  word  describes  the  aspect,  or  the  aspect  is  associated  with  the 
sentiment  word.  the  algorithm  finds  the  mapping  by  iteratively  id91 
the set of explicit aspects and the set of sentiment words separately. in each 
iteration, before id91 one set, the id91 results of the other set is 
used to update the pairwise similarity of the set. the pairwise similarity in a 
set is determined by a linear combination of intra-set similarity and inter-set 
similarity. the intra-set similarity of two items is the traditional similarity. 
the  inter-set  similarity  of  two  items  is  computed  based  on  the  degree  of 
association  between  aspects  and  sentiment  words.  the  association  (or 
mutual  reinforcement  relationship)  is  modeled  using  a  bipartite  graph.  an 
aspect  and  an  opinion  word  are  linked  if  they  have  co-occurred  in  a 

 

77 

 

  id31 and opinion mining 

sentence. the links are also weighted based on the co-occurrence frequency. 
after  the  iterative  id91,  the  strongest  n  links  between  aspects  and 
sentiment word groups form the mapping.  

in (hai, chang and kim, 2011), a two-phase co-occurrence association rule 
mining  approach  was  proposed  to  match  implicit  aspects  (which  are  also 
assumed to be sentiment words) with explicit aspects. in the first phase, the 
approach generates association rules involving each sentiment word as the 
condition  and  an  explicit  aspect  as  the  consequence,  which  co-occur 
frequently in sentences of a corpus. in the second phase, it clusters the rule 
consequents  (explicit  aspects)  to  generate  more  robust  rules  for  each 
sentiment  word  mentioned  above.  for  application  or  testing,  given  a 
sentiment word with no explicit aspect, it finds the best rule cluster and then 
assigns the representative word of the cluster as the final identified aspect.  

5.4 

identifying resource usage aspect  

as  discussed  in  section  4.3,  researchers  often  try  to  solve  a  problem  in  a 
general fashion and in many cases based on a simplistic view. in the context 
of aspect extraction and aspect sentiment classification, it is not always the 
sentiment  word  and  aspect  word  pairs  that  are  important.  as  indicated  in 
section  5.2,  the  real  world  is  much  more  complex  and  diverse  than  that. 
here,  we  use  resource  usage  as  an  example  to  show  that  a  divide  and 
conquer approach may be needed for aspect-based id31.   

in  many  applications,  resource  usage  is  an  important  aspect,  e.g.,     this 
washer uses a lot of water.    here the water usage is an aspect of the washer, 
and  this  sentence  indicates  a  negative  opinion  as  consuming  too  much 
resource  is  undesirable.  there  is  no  opinion  word  in  this  sentence. 
discovering  resource  words  and phrases,  which are  called resource terms, 
are thus important for id31. in section 5.2, we presented some 
opinion rules involving resources. we reproduce two of them below:  

1.  p  
2.  n  

::=  consume  no,_little_or_less  resource 
::=  consume  a_large_quantity_of_or_more  resource 

in (zhang and liu, 2011a), a method was proposed to extract resource terms. 
for  example,  in  the  above  example,     water     should  be  extracted  as  a 
resource term. the paper formulated the problem based on a bipartite graph 
and proposed an iterative algorithm to solve the problem. the algorithm was 
based on the following observation: 

observation:  the  sentiment  or  opinion  expressed  in  a  sentence  about 

resource usage is often determined by the following triple,  

78 
 

id31 and opinion mining 

(verb, quantifier, noun_term), 

where noun_term is a noun or a noun phrase 

for example, in    this washer uses a lot of water,       uses    is the main verb, 
   a  lot  of     is  a  quantifier  phrase,  and     water     is  the  noun  representing  a 
resource.  the  method  used  such  triples  to  help  identify  resources  in  a 
domain  corpus.  the  model  used  a  circular  definition  to  reflect  a  special 
reinforcement  relationship  between  resource  usage  verbs  (e.g.,  consume) 
and resource terms (e.g., water) based on the bipartite graph. the quantifier 
was not used in computation but was employed to identify candidate verbs 
and resource terms. the algorithm assumes that a list of quantifiers is given, 
which is not numerous and can be manually compiled. based on the circular 
definition, the problem is solved using an iterative algorithm similar to the 
hits  algorithm  in  (kleinberg,  1999).  to  start  the  iterative  computation, 
some global seed resources are employed to find and to score some strong 
resource usage verbs. these scores are then applied as the initialization for 
the  iterative  computation  for  any  application  domain.  when  the  algorithm 
converges, a ranked list of candidate resource terms is identified.  

5.5 

simutaneous opinion lexicon 
expansion and aspect extraction  
as mentioned in chapter 2, an opinion always has a target. this property has 
been  exploited  in  aspect  extraction  by  several  researchers  (see  section 
5.3.2).  in  (qiu  et  al.,  2009;  qiu  et  al.,  2011),  it  was  used  to  extract  both 
sentiment words and aspects at the same time by exploiting certain syntactic 
relations between sentiments and targets, and a small set of seed sentiment 
words (no seed aspects are required) for extraction. the method is based on 
id64.  note  that  sentiment  words  generation  is  an  important  task 
itself (see chapter 6). 

due  to  the  relationships  between  sentiments/opinions  and  their  targets  (or 
aspects),  sentiment  words  can  be  recognized  by  identified  aspects,  and 
aspects  can  be  identified  by  known  sentiment  words.  the  extracted 
sentiment  words  and  aspects  are  utilized  to  identify  new  sentiment  words 
and new aspects, which are used again to extract more sentiment words and 
aspects.  this  propagation  process  ends  when  no  more  sentiment  words  or 
aspects  can  be  found.  as  the  process  involves  propagation  through  both 
sentiment  words  and  aspects,  the  method  is  called  double  propagation. 
extraction rules were based on certain special dependency relations among 
sentiment  words  and  aspects.  the  dependency  grammar  (tesniere,  1959) 

 

79 

 

  id31 and opinion mining 

was  adopted  to  describe  the  relations.  the  dependency  parser  used  was 
minipar (lin, 2007).  

some constraints were also imposed. sentiment words were considered to be 
adjectives  and  aspects  nouns  or  noun  phrases.  the  dependency  relations 
between sentiment words and aspects include mod, pnmod, subj, s, obj, obj2, 
and  desc,  while  the  relations  for  sentiment  words  and  aspects  themselves 
contain  only  the  conjunction  relation  conj.  oa-rel  denotes  the  relations 
between  sentiment  words  and  aspects,  oo-rel  between  sentiment  words 
themselves, and aa-rel between aspects. each relation in oa-rel, oo-rel, 
or aa-rel is a triple    pos(wi), r, pos(wj)   , where pos(wi) is the pos tag 
of word wi and r is one the dependency relations above.   
the extraction process uses a rule-based approach. for example, in    canon 
g3 produces great pictures,    the adjective    great    is parsed as depending on 
the noun    pictures    through mod, formulated as an oa-rel    jj, mod, nns   . 
if we know    great    is a sentiment word and are given the rule    a noun on 
which a sentiment word directly depends through mod is taken as an aspect,    
we can extract    pictures    as an aspect. similarly, if we know    pictures    is an 
aspect, we can extract    great    as an opinion word using a similar rule. the 
propagation performs four subtasks:  

1.  extracting aspects using sentiment words  
2.  extracting aspects using extracted aspects  
3.  extracting sentiment words using extracted aspects  
4.  extracting  sentiment  words  using  both  given  and  extracted  opinion 

words 

oa-rels are used for tasks (1) and (3), aa-rels are used for task (2), and 
oo-rels  are  used  for  task  (4).  four  types  of  rules  are  defined  (shown  in 
table 5.1) respectively, for these four subtasks. in the table, o (or a) stands 
for the output (or extracted) sentiment word (or aspect). {o} (or {a}) is the 
set  of  known  sentiment  words  (or  aspects)  either  given  or  extracted.  h 
means any word. pos(o(or a)) and o(or a)-dep stand for the pos tag and 
dependency relation of the word o (or a) respectively. {jj} and {nn} are 
sets of pos tags of potential sentiment words and aspects respectively. {jj} 
contains  jj,  jjr  and  jjs;  {nn}  contains  nn  and  nns.  {mr}  consists  of 
dependency relations, which is the set {mod, pnmod, subj, s, obj, obj2, and 
desc}.  {conj}  contains  conj  only.  the  arrows  mean  dependency.  for 
example, o      o-dep      a  means  o  depends on a  through a  relation o-
dep.  specifically,  r1i  is  employed  to  extract  aspects  (a)  using  sentiment 
words (o), r2i to extract opinion words (o) using aspects (a), r3i to extract 
aspects (a) using extracted aspects (ai), and r4i to extract sentiment words 
(o) using known sentiment words (oi). 

80 
 

 
r11 

(oa-rel) 

(oa-rel) 

r12 

 
r21 

 
r22 

(oa-rel) 

(oa-rel) 

r31 

(aa-rel) 

r32 

(aa-rel) 

r41 

(oo-rel) 

r42 

(oo-rel) 

id31 and opinion mining 

observations 
o   o-dep   a 

s.t. o   {o}, o-dep   {mr}, 

pos(a)   {nn} 

o   o-dep   h   a-dep   a 
s.t. o   {o}, o/a-dep   {mr}, 

pos(a)   {nn} 
o   o-dep   a 

s.t. a   {a}, o-dep   {mr}, 

pos(o)   {jj} 

o   o-dep   h   a-dep   a 
s.t. a   {a}, o/a-dep   {mr}, 

pos(o)   {jj} 

ai(j)   ai(j)-dep   aj(i) 

s.t. aj(i)    {a}, ai(j)-dep   {conj}, 

pos(ai(j))   {nn} 

ai   ai-dep   h   aj-dep   aj 
s.t. ai   {a}, ai-dep=aj-dep or 

(ai-dep = subj and aj-dep = obj), 

pos(aj)   {nn} 

oi(j)   oi(j)-dep   oj(i) 

s.t. oj(i)   {o}, oi(j)-dep   {conj}, 

pos(oi(j))   {jj} 

output
a = a  the phone has a good    screen   . 

examples 

good   mod   screen 

a = a     ipod    is the best mp3 player. 

best   mod   player   subj   ipod 

o = o  same as r11 with screen as the 

known word and good as the 
extracted word 

o = o  same as r12 with ipod is the 
known word and best as the 
extract word. 

a = ai(j) does the player play dvd with 

audio and    video   ? 
video   conj   audio 

a = aj  canon    g3    has a great len. 

len   obj   has   subj   g3 

o = oi(j) the camera is amazing and 

   easy    to use. 
easy   conj   amazing 

oi   oi-dep   h   oj-dep   oj 
s.t. oi   {o}, oi-dep=oj-dep or 
(oi /oj-dep    {pnmod, mod}), 

pos(oj)   {jj} 

o = oj if you want to buy a sexy,    cool   , 
accessory-available mp3 player, 
you can choose ipod. 
sexy   mod   player   mod   cool 

table 5.1. rules for aspect and opinion word extraction. column 1 is the rule 
id,  column  2  is  the  observed  relation  (line  1)  and  the  constraints  that  it  must  satisfy 
(lines 2     4), column 3 is the output, and column 4 is an example. in each example, the 
underlined word is the known word and the word with double quotes is the extracted 
word. the corresponding instantiated relation is given right below the example.  

this method was originally designed for english, but it has also been used 
for chinese online discussions (zhai et al., 2011). this method can also be 
reduced  for  finding  aspects  only  using  a  large  sentiment  lexicon.  for 
practical use, the set of relations can be significantly expanded. also, instead 
of using word-based id33, a phrase level id33 
may be better as many aspects are phrases (wu et al., 2009). zhang et al. 
(2010) improved this method by adding more relations and by ranking the 
extracted aspects using a graph method.  

5.6  grouping aspects into categories  

after  aspect  extraction,  aspect  expressions  (actual  words  and  phrases 
indicating aspects) need to be grouped into synonymous aspect categories. 
each category represents a unique aspect. as in any writing, people often 

 

81 

 

  id31 and opinion mining 

use  different  words  and  phrases  to  describe  the  same  aspect.  for 
example,    call quality    and    voice quality    refer to the same aspect for 
phones.  grouping  such  aspect  expressions  from  the  same  aspect  is 
critical  for  opinion  analysis.  although  worldnet  and  other  thesaurus 
dictionaries can help to some extent, they are far from sufficient because 
many synonyms are domain dependent (liu, hu and cheng, 2005). for 
example,     movie     and     picture     are  synonyms  in  movie  reviews,  but 
they are not synonyms in camera reviews as    picture    is more likely to 
be  synonymous  to     photo     while     movie     to     video   .  many  aspect 
expressions  are  multi-word  phrases,  which  cannot  be  easily  handled  with 
dictionaries.  furthermore,  it  is  also  important  to  note  that  many  aspect 
expressions  describing  the  same  aspect  are  not  general  or  domain  specific 
synonyms.  for  example,     expensive     and     cheap     can  both  indicate  the 
aspect  price  but  they  are  not  synonyms  of  each  other  (but  antonyms)  or 
synonyms of price.  

carenini, ng and zwart (2005) proposed the first method to deal with this 
problem. their method was based on several similarity metrics defined using 
string similarity, synonyms, and lexical distances measured using id138. 
the  method  requires  a  taxonomy  of  aspects  to  be  given  for  a  particular 
domain. it merges each discovered aspect expression to an aspect node in the 
taxonomy  based  on  the  similarities.  experiments  based  on  digital  camera 
and  dvd  reviews  showed  promising  results.  in  (yu  et  al.,  2011),  a  more 
sophisticated  method  was  presented  to  also  use  publicly  available  aspect 
hierarchies/taxonomies  of  products  and  the  actual  product  reviews  to 
produce  the  final  aspect  hierarchies.  a  set  of  distance  measures  was  also 
used but was combined with an optimization strategy.  

in (zhai et al., 2010), a semi-supervised learning method was proposed to 
group  aspect  expressions  into  some  user-specified  aspect  categories.  to 
reflect the user needs, he/she first labels a small number of seeds for each 
category.  the  system  then  assigns  the  rest  of  the  aspect  expressions  to 
suitable  categories  using  a  semi-supervised  learning  method  working  with 
labeled  and  unlabeled  examples.  the  method  uses  the  expectation-
maximization  (em)  algorithm  in  (nigam  et  al.,  2000).  the  method  also 
employed two pieces of prior knowledge to provide a better initialization for 
em: (1) aspect expressions sharing some common words are likely to belong 
to the  same  group,  e.g.,     battery  life     and     battery  power,     and  (2)  aspect 
expressions  that  are  synonyms  in  a  dictionary  are  likely  to  belong  to  the 
same  group,  e.g.,     movie     and     picture.     these  two  pieces  of  knowledge 
help  em  produce  better  classification  results.  in  (zhai  et  al.,  2011),  soft 
constraints were used to help label some examples, i.e., sharing words and 
lexical similarity (jiang and conrath, 1997). the learning method also used 

82 
 

id31 and opinion mining 

em, but it eliminated the need of asking the user to provide seeds. note that 
the  general  nlp  research  on  concept  similarity  and  synonym  discovery  is 
also relevant here (mohammad and hirst, 2006; wang and hirst, 2011).  

in (guo et al., 2009), a method called multilevel latent semantic association 
was presented. at the first level, all the words in aspect expressions (each 
aspect expression can have more than one word) are grouped into a set of 
concepts/topics  using  lda.  the  results  are  used  to  build  latent  topic 
structures  for  aspect  expressions.  for  example,  we  have  four  aspect 
expressions     day  photos   ,     day  photo   ,     daytime  photos     and     daytime 
photo   .  if  lda  groups  the  individual  words     day     and     daytime     into 
topic10,  and     photo     and     photos     into  topic12,  the  system  will  group  all 
four  aspect  expressions  into  one  group,  call  it     topic10-topic12   ,  which  is 
called  a  latent  topic  structure.  at  the  second  level,  aspect  expressions  are 
grouped  by  lda  again  but  according  to  their  latent  topic  structures 
produced  at  level  1  and  their  context  snippets  in  reviews.  following  the 
above example,    day photos   ,    day photo   ,    daytime photos    and    daytime 
photo    in    topic10-topic12    combined with their surrounding words form a 
document. lda runs on such documents to produce the final result. in (guo 
et  al.,  2010),  a  similar  idea  was  also  used  to  group  aspects  from  different 
languages  into  aspect  categories,  which  can  be  used  to  compare  opinions 
along different aspects from different languages (or countries).  

topic  modeling  methods  discussed  in  section  5.3.4  actually  perform  both 
aspect  expression  discovery  and  categorization  at  the  same  time  in  an 
unsupervised  manner  as  topic  modeling  basically  clusters  terms  in  a 
document collection. recently, some algorithms have also been proposed to 
use  domain  knowledge  or  constraints  to  guide  topic  modeling  to  produce 
better topic clusters (andrzejewski, zhu and craven, 2009). the constraints 
are  in  the  form  of  must-links  and  cannot-links.  a  must-link  constraint  in 
id91 specifies that two data instances  must be in the same cluster. a 
cannot-link constraint specifies that two data instances cannot be in the same 
cluster.  however,  the  method  can  result  in  an  exponential  growth  in  the 
encoding of cannot-link constraints and thus have difficulty in processing a 
large number of constraints.  

constrained-lda  of  zhai  et  al.  (2011)  took  a  different  but  heuristic 
approach. instead of treating constraints as priors, the constraints were used 
in id150 to bias the id155 for topic assignment of 
a word. this method can handle a large number of must-link and cannot-link 
constraints. the constraints can also be relaxed, i.e., they are treated as soft 
(rather  than  hard)  constraints  and  may  not  be  satisfied.  for  aspect 
categorization, constrained-lda used the following constraints:  

 

83 

 

  id31 and opinion mining 

must-link: if two aspect expressions ai and aj share one or more words, they 
form a must-link, i.e., they are likely to be in the same topic or category, 
e.g.,    battery power    and    battery life.     

cannot-link: if two aspect expressions ai and aj in the same sentence, they 
form a cannot-link. the reason for this constraint is that people usually 
do  not  repeat  the  same  aspect  in  the  same  sentence,  e.g.,     i  like  the 
picture quality, battery life, and zoom of this camera.    

in (mukherjee and liu, 2012), the domain knowledge came in the form of 
some  user-provided  seed  aspect  words  to  some  topics  (or  aspects).  the 
resulting  model  is  thus  semi-supervised.  the  model  also  separates  aspect 
words and sentiment words. the model in (andrzejewski, zhu and craven, 
2009) or the constrained-lda method does not do that.   

5.7 

entity, opinion holder and time 
extraction  

entity, opinion holder and time extraction is the classic problem of named 
entity  recognition  (ner).  ner  has  been  studied  extensively  in  several 
fields, e.g., information retrieval, id111, data mining, machine learning 
and  natural  language  processing  under  the  name  of  information  extraction 
(hobbs  and  riloff,  2010;  mooney  and  bunescu,  2005;  sarawagi,  2008). 
there  are  two  main  approaches  to  information  extraction:  rule-based  and 
statistical. early extraction systems were mainly based on rules (e.g., (riloff, 
1993)). statistical methods were typically based on id48 
(id48) (rabiner, 1989) (jin and ho, 2009) and id49 
(crf)  (lafferty,  mccallum  and  pereira,  2001).  both  id48  and  crf  are 
supervised methods. due to the prior work in the area, specific works in the 
context of id31 and opinion mining is not extensive. thus, we 
will  not  discuss  it  further.  see  a  comprehensive  survey  of  information 
extraction tasks and algorithms in (sarawagi, 2008). here we only discuss 
some specific issues in id31 applications.  

in most applications that use social media, we do not need to extract opinion 
holders and the times of postings from the text as opinion holders are usually 
the authors of the reviews, blogs, or discussion postings, whose login ids are 
known although their true identities in the real world are unknown. the date 
and time when a posting was submitted are also known and displayed on the 
web  page.  they  can  be  scraped  from  the  page  using  structured  data 
extraction techniques (liu, 2006 and 2011). in some cases, opinion holders 
can be in the actual text and need to be extracted. we discuss it below.  

84 
 

id31 and opinion mining 

here  we  first  discuss  a  specific problem  of  named  entity  extraction  in  the 
sentiment  analysis  context.  in  a  typical  sentiment  analysis  application,  the 
user  usually  wants  to  find  opinions  about  some  competing  entities,  e.g., 
competing products or brands. however, he/she often can only provide a few 
names because there are so many different brands and models. even for the 
same  entity,  web  users  may  write  the  entity  in  many  different  ways.  for 
example,     motorola     may  be  written  as     moto     or     mot.     it  is  thus 
important for a system to automatically discover them from the corpus (e.g., 
reviews,  blogs  and  forum  discussions).  the  main  requirement  of  this 
extraction  is  that  the  extracted  entities  must  be  of  the  same  type  as  the 
entities provided by the user (e.g., phone brands and models).  

in  (li  et  al.,  2010),  li  et  al.  formulated  the  problem  as  a  set  expansion 
problem (ghahramani and heller, 2006; pantel et al., 2009). the problem is 
stated as follows: given a set q of seed entities of a particular class c, and a 
set d of candidate entities, we wish to determine which of the entities in d 
belong  to  c.  that  is,  we     grow     the  class  c  based  on  the  set  of  seed 
examples  q.  although  this  is  a  classification  problem,  in  practice,  the 
problem is often solved as a ranking problem, i.e., to rank the entities in d 
based on their likelihoods of belonging to c.  

the  classic  methods  for  solving  this  problem  in  nlp  are  based  on 
distributional similarity (lee, 1999; pantel et al., 2009). the approach works 
by comparing the similarity of the surround words of each candidate entity 
with those of the seed entities and then ranking the candidate entities based 
on the similarity values. in (li et al., 2010), it was shown that this approach 
was  inaccurate.  learning  from  positive  and  unlabeled  examples  (pu 
learning)  using  the  s-em  algorithm  (liu  et  al.,  2002)  was  considerably 
better.  to  apply  pu  learning,  the  given  seeds  were  used  to  automatically 
extract  sentences  that  contain  one  or  more  of  the  seeds.  the  surrounding 
words of each seed in these sentences served as the context of the seed. the 
rest  of  the  sentences  were  treated  as  unlabeled  examples.  experimental 
results  indicated  that  s-em  outperformed  the  machine  learning  technique 
bayesian sets (ghahramani and heller, 2006), which also outperformed the 
distributional similarity measure significantly.  

about opinion holder extraction in the context of id31, several 
researchers  have  investigated  it.  the  extraction  was  mainly  done  in  news 
articles.  kim  and  hovy  (2004)  considered  person  and  organization  as  the 
only  possible  opinion  holders,  and  used  a  named  entity  tagger  to  identify 
them. choi, breck and cardie (2006) used id49 (crf) 
for extraction. to train crf, they used features such as surrounding words, 
part-of-speech  of  surrounding  words,  grammatical  roles,  sentiment  words, 
etc. in (kim and hovy, 2006), the method first generates all possible holder 

 

85 

 

  id31 and opinion mining 

candidates  in  a  sentence,  i.e.,  all  noun  phrases,  including  common  noun 
phrases,  named  entities,  and  pronouns.  it  then  parses  the  sentence  and 
extracts a set of features from the parse tree. a learned maximum id178 
(me)  model  then  ranks  all  holder  candidates  according  to  the  scores 
obtained by the me model. the system picks the candidate with the highest 
score as the holder of the opinion in the sentence. johansson and moschitti 
(2010) used id166 with a set of features. wiegand and klakow (2010) used 
convolution kernels, and lu (2010) applied a dependency parser.  

in  (ruppenhofer,  somasundaran  and  wiebe,  2008),  the  authors  discussed 
the  issue  of  using  automatic  semantic  role  labeling  (asrl)  to  identify 
opinion holders. they argued that asrl is insufficient and other linguistic 
phenomena such as the discourse structure may need to be considered. kim 
and hovy (2006) earlier also used id14 for the purpose.  

5.8 

coreference resolution and word 
sense disambiguation 

although  we  discuss  only  coreference  resolution  and  word  sense 
disambiguation in this section, we really want to highlight nlp issues and 
problems  in  the  sentiment  analysis  context.  most  of  such  issues  have  not 
been studied in id31. 

coreference resolution has been studied extensively in the nlp community 
in general. it refers to the problem of determining multiple expressions in a 
sentence or document referring to the same thing, i.e., they have the same 
"referent." for example, in    i bought an iphone two days ago. it looks very 
nice. i made many calls in the past two days. they were great,       it    in the 
second sentence refers to iphone, which is an entity, and    they    in the fourth 
sentence refers to    calls   , which is an aspect. recognizing these coreference 
relationships is clearly very important for aspect-based id31. if 
we  do  not  resolve  them,  but  only  consider  opinion  in  each  sentence  in 
isolation,  we  lose  recall.  that  is,  although  we  know  that  the  second  and 
fourth sentences express opinions, we do not know about what. then, from 
this piece of text we will get no useful opinion, but in fact, it has a positive 
opinion on iphone itself and also a positive opinion on the call quality.  

ding and liu (2010) proposed the problem of entity and aspect coreference 
resolution.  the  task  aims  to  determine  which  mentions  of  entities  and/or 
aspects  that  pronouns  refer  to.  the  paper  took  a  supervised  learning 
approach.  the  key  interesting  points  were  the  design  and  testing  of  two 
opinion-related features, which showed that id31 was used for 

86 
 

id31 and opinion mining 

the purpose of coreference resolution. the first feature is based on sentiment 
analysis  of  regular  sentences  and  comparative  sentences,  and  the  idea  of 
sentiment consistency. consider these sentences,    the nokia phone is better 
than this motorola phone. it is cheap too.    our commonsense tells us that 
   it    means    nokia phone    because in the first sentence, the sentiment about 
   nokia  phone     is  positive  (comparative  positive),  but  it  is  negative 
(comparative  negative)  for     motorola  phone,     and  the  second  sentence  is 
positive. thus, we conclude that    it    refers to    nokia phone    because people 
usually express sentiments in a consistent way. it is unlikely that    it    refers 
to    motorola phone.    however, if we change    it is cheap too    to    it is also 
expensive   , then    it    should now refer to    motorola phone.    to obtain this 
feature,  the  system  needs  to  have  the  ability  to  determine  positive  and 
negative opinions expressed in both regular and comparative sentences.  

the second feature considers what entities and aspects are modified by what 
opinion  words.  consider  these  sentences,     i  bought  a  nokia  phone 
yesterday. the sound quality is good. it is cheap too.    the question is what 
   it    refers to,    sound quality    or the    nokia phone.    clearly, we know that 
   it    refers to    nokia phone    because    sound quality    cannot be cheap. to 
obtain  this  feature,  the  system  needs to  identify  what sentiment  words  are 
usually associated with what entities or aspects. such relationships have to 
be  mined  from  the  corpus.  these  two  features  are  semantic  features  that 
current general coreference resolution methods do not consider. these two 
features can help improve the coreference resolution accuracy.  

in (stoyanov and cardie, 2006), stoyanov and cardie proposed the problem 
of  source  coreference  resolution,  which  is  the  task  of  determining  which 
mentions of opinion holders (sources) refer to the same entity. the authors 
used  existing  coreference  resolution  features  in  (ng  and  cardie,  2002). 
however,  instead  of  simply  employing  supervised  learning,  they  used 
partially supervised id91.  

akkaya,  wiebe  and  mihalcea  (2009)  studied  subjectivity  word  sense 
disambiguation (swsd). the task is to automatically determine which word 
instances in a corpus are being used with subjective senses, and which are 
being used with objective senses. currently, most subjectivity or sentiment 
lexicons are compiled as lists of words, rather than word meanings (senses). 
however, many words have both subjective and objective senses. false hits 
    subjectivity clues used with objective senses     are a significant source of 
error in subjectivity and id31. the authors built a supervised 
swsd model to disambiguate members of a subjectivity lexicon as having a 
subjective  sense  or  an  objective  sense  in  a  corpus  context.  the  algorithm 
relied on common machine learning features for id51 
(wsd).  however,  the  performance  was  substantially  better  than  the 

 

87 

 

  id31 and opinion mining 

performance of full wsd on the same data, suggesting that the swsd task 
was  feasible,  and  that  subjectivity  provided  a  natural  coarse  grained 
grouping  of  senses.  they  also  showed  that  swsd  can  subsequently  help 
subjectivity and id31.  

5.9 

summary  

aspect-level  sentiment  analysis  is  usually  the  level  of  details  required  for 
practical  applications.  most  industrial  systems  are  so  based.  although  a 
great  deal  of  work  has  been  done  in  the  research  community  and  many 
systems  have  also  been  built,  the  problem  is  still  far  from  being  solved. 
every  sub-problem  remains  to  be  highly  challenging.  as  one  ceo  put  it, 
   our  sentiment  analysis  is  as  bad  as  everyone  else   s,     which  is  a  nice 
portrayal of the current situation and the difficulty of the problem.   

two most outstanding problems are aspect extraction and aspect sentiment 
classifications.  the  accuracies  for  both  problems  are  not  high  because 
existing  algorithms  are  still  unable  to  deal  with  complex  sentences  that 
requires more than sentiment words and simple parsing, or to handle factual 
sentences  that  imply  opinions.  we  discussed  some  of  these  problems  in 
basic rules of opinions in section 5.2. 

on the whole, we seem to have  met a long tail problem. while sentiment 
words can handle about 60% of the cases (more in some domains and less in 
others), the rest are highly diverse, numerous and infrequent, which make it 
hard  for  statistical  learning  algorithms  to  learn  patterns  because  there  are 
simply  not  enough  training  data  for  them.  in  fact,  there  seem  to  be  an 
unlimited number of ways that people can use to express positive or negative 
opinions.  every  domain  appears  to  have  something  special.  in  (wu  et  al., 
2011),  a  more  complex  graph-based  representation  of  opinions  was 
proposed, which requires even more sophisticated solution methods.   

so  far,  the  research  community  has  mainly  focused  on  opinions  about 
electronics  products,  hotels,  and  restaurants.  these  domains  are  easier 
(although not easy) and reasonably good accuracies can be achieved if one 
can  focus  on  each  domain  and  take  care  of  its  special  cases.  when  one 
moves  to  other  domains,  e.g.,  mattress  and  paint,  the  situations  get 
considerably harder because in these domains many factual statements imply 
opinions.  politics  is  another  can  of  warms.  here,  the  current  aspect 
extraction algorithms only had limited success because few political issues 
(aspects) can be described with one or two words. political sentiments are 
also  harder  to  determine  due  to  complex  mixture  of  factual  reporting  and 
subjective opinions, and heavy use of sarcastic sentences.  

88 
 

id31 and opinion mining 

in  term  of  the  type  of  social  media,  researchers  working  on  aspect-based 
sentiment  analysis  have  focused  mainly  on  product/service  reviews  and 
tweets from twitter. these forms of data are also easier (again, not easy) to 
handle  because  reviews  are  opinion  rich  and  have  little  irrelevant 
information  while  tweets  are  very  short  and  often  straight  to  the  point. 
however,  other  forms  of  opinion  text  such  as  forum  discussions  and 
commentaries are much harder to deal with because they are mixed with all 
kinds  of  non-opinion  contents  and  often  talk  about  multiple  entities  and 
involve user interactions. this leads us to another major issue that we have 
not discussed  so  far  as  there  is  limited  research on it.  it  is  the data  noise. 
almost all forms of social media are very noisy (except reviews) and full of 
all kinds of spelling, grammatical, and punctuation errors. most nlp tools 
such as pos taggers and parsers need clean data to perform accurately. thus 
a  significant  amount  of  pre-processing  is  needed  before  any  analysis.  see 
(dey and haque, 2008) for some pre-processing tasks and methods.  

to make a significant progress, we still need novel ideas and to study a wide 
range of domains. successful algorithms are likely to be a good integration 
of machine learning and domain and natural language knowledge.  

 

89 

 

  id31 and opinion mining 

chapter 6 

sentiment lexicon generation  

 
by now, it should be quite clear that words and phrases that convey positive 
or negative sentiments are instrumental for id31. this chapter 
discusses  how  to  compile  such  words  lists.  in  the  research  literature, 
sentiment  words  are  also  called  opinion  words,  polar  words,  or  opinion-
bearing words. positive sentiment words are used to express some desired 
states or qualities while negative sentiment words are used to express some 
undesired  states  or  qualities.  examples  of  positive  sentiment  words  are 
beautiful,  wonderful,  and  amazing.  examples  of  negative  sentiment  words 
are  bad,  awful,  and  poor.  apart  from  individual  words,  there  are  also 
sentiment  phrases  and  idioms,  e.g.,  cost  someone  an  arm  and  a  leg. 
collectively, they are called sentiment lexicon (or opinion lexicon). for easy 
presentation,  from  now  on  when  we  say  sentiment  words,  we  mean  both 
individual words and phrases.  

sentiment words can be divided into two types, base type and comparative 
type. all the example words above are of the base type. sentiment words of 
the comparative type (which include the superlative type) are used to express 
comparative  and  superlative  opinions.  examples  of  such  words  are  better, 
worse, best, worst, etc., which are comparative and superlative forms of their 
base adjectives or adverbs, e.g., good and bad. unlike sentiment words of 
the  base  type,  sentiment  words  of  the  comparative  type  do  not  express  a 
regular  opinion  on  an  entity  but  a  comparative  opinion  on  more  than  one 
entity, e.g.,    pepsi tastes better than coke.    this sentence does not express 
an opinion saying that any of the two drinks is good or bad. it just says that 
compared  to  coke,  pepsi  tastes  better.  we  will  discuss  comparative  and 
superlative sentiment words further in chapter 8. this chapter focuses only 
on sentiment words of the base type.  

researchers  have  proposed  many  approaches  to  compile  sentiment  words. 
three main approaches are: manual approach, dictionary-based approach, 
and  corpus-based  approach.  the  manual  approach  is  labor  intensive  and 
time  consuming,  and  is  thus  not  usually  used  alone  but  combined  with 
automated approaches as the final check, because automated methods make 
mistakes.  below,  we  discuss  the  two  automated  approaches.  along  with 
them, we will also discuss the issue of factual statements implying opinions, 
which has largely been overlooked by the research community.  

90 
 

id31 and opinion mining 

6.1 

dictionary-based approach  

using  a  dictionary  to  compile  sentiment  words  is  an  obvious  approach 
because most dictionaries (e.g., id138 (miller et al., 1990)) list synonyms 
and antonyms for each word. thus, a simple technique in this approach is to 
use  a  few  seed  sentiment  words  to  bootstrap  based  on  the  synonym  and 
antonym  structure  of  a  dictionary.  specifically,  this  method  works  as 
follows:  a  small  set  of  sentiment  words  (seeds)  with  known  positive  or 
negative  orientations  is  first  collected  manually,  which  is  very  easy.  the 
algorithm then grows this set by searching in the id138 or another online 
dictionary  for  their  synonyms  and  antonyms.  the  newly  found  words  are 
added to the seed list. the next iteration begins. the iterative process ends 
when no more new words can be found. this approach was used in (hu and 
liu, 2004). after the process completes, a manual inspection step was used 
to clean up the list. a similar method was also used by valitutti, strapparava 
and  stock  (2004).  kim  and  hovy  (2004)  tried  to  clean  up  the  resulting 
words  (to  remove  errors)  and  to  assign  a  sentiment  strength  to  each  word 
using  a  probabilistic  method.  mohammad,  dunne  and  dorr  (2009) 
additionally  exploited  many  antonym-generating  affix  patterns  like  x  and 
disx (e.g., honest   dishonest) to increase the coverage. 

a more sophisticated approach was proposed in (kamps et al., 2004), which 
used  a  id138  distance  based  method  to  determine  the  sentiment 
orientation of a given adjective. the distance d(t1, t2) between terms t1 and t2 
is  the  length  of  the  shortest  path  that  connects  t1  and  t2  in  id138.  the 
orientation of an adjective term t is determined by its relative distance from 
two  reference  (or  seed)  terms  good  and bad,  i.e., so(t)  =  (d(t, bad)      d(t, 
good))/d(good,  bad).  t  is  positive  iff  so(t)  >  0,  and  is  negative  otherwise. 
the  absolute  value  of  so(t)  gives  the  strength  of  the  sentiment.  along  a 
similar  line, williams  and  anand  (2009)  studied  the problem  of  assigning 
sentiment strength to each word.  

in  (blair-goldensohn  et  al.,  2008),  a  different  id64  method  was 
proposed,  which  used  a  positive  seed  set,  a  negative  seed  set,  and  also  a 
neutral seed set. the approach works based on a directed, weighted semantic 
graph  where  neighboring  nodes  are  synonyms  or  antonyms  of  words  in 
id138 and are not part of the seed neutral set. the neutral set is used to 
stop the propagation of sentiments through neutral words. the edge weights 
are pre-assigned based on a scaling parameter for different types of edges, 
i.e.,  synonym  or  antonym  edges.  each  word  is  then  scored  (giving  a 
sentiment value) using a modified version of the label propagation algorithm 
in (zhu and ghahramani, 2002). at the beginning, each positive seed word 
is given the score of +1, each negative seed is given the score of -1, and all 

 

91 

 

  id31 and opinion mining 

other  words  are  given  the  score  of  0.  the  scores  are  revised  during  the 
propagation  process.  when  the  propagation  stops  after  a  number  of 
iterations, the final scores after a logarithmic scaling are assigned to words 
as their degrees of being positive or negative.  

in  (rao  and  ravichandran,  2009),  three  graph-based  semi-supervised 
learning methods were tried to separate positive and negative words given a 
positive seed set, a negative seed set, and a synonym graph extracted from 
the id138. the three algorithms were mincut (blum and chawla, 2001), 
randomized  mincut  (blum  et  al.,  2004),  and  label  propagation  (zhu  and 
ghahramani,  2002).  it  was  shown  that  mincut  and  randomized  mincut 
produced  better  f  scores,  but  label  propagation  gave  significantly  higher 
precisions with low recalls.  

hassan and radev (2010) presented a markov random walk model over a 
word relatedness graph to produce a sentiment estimate for a given word. it 
first  uses  id138  synonyms  and  hypernyms  to  build  a  word  relatedness 
graph. a measure, called the mean hitting time h(i|s), was then defined and 
used to gauge the distance from a node i to a set of nodes (words) s, which is 
the average number of steps that a random walker, starting in state i     s, will 
take  to  enter  a  state  k       s  for  the  first  time.  given  a  set  of  positive  seed 
words  s+  and  a  set  of  negative  seed  words  s   ,  to  estimate  the  sentiment 
orientation  of  a  given  word  w,  it  computes  the  hitting  times  h(w|s+)  and 
h(w|s   ). if h(w|s+) is greater than h(w|s   ), the word is classified as negative, 
otherwise positive. in (hassan et al., 2011), this method was applied to find 
sentiment  orientations  of  foreign  words.  for  this  purpose,  a  multilingual 
word graph was created with both english words and foreign words. words 
in different languages are connected based on their meanings in dictionaries. 
other  methods  based  on  graphs  include  those  in  (takamura,  inui  and 
okumura, 2005) and (takamura, inui and okumura, 2007; takamura, inui 
and okumura, 2006).  

in (turney and littman, 2003), the same pmi based method as in (turney, 
2002)  was  used  to  compute  the  sentiment  orientation  of  a  given  word. 
specifically, it computes the orientation of the word from the strength of its 
association  with  a  set  of  positive  words  (good,  nice,  excellent,  positive, 
fortunate, correct, and superior), minus the strength of its association with a 
set of negative words (bad, nasty, poor, negative, unfortunate, wrong, and 
inferior). the association strength is measured using pmi.  
esuli and sebastiani (2005) used supervised learning to classify words into 
positive and negative classes. given a set p of positive seed words and a set 
n  of  negative  seed  words,  the  two  seed  sets  are  first  expanded  using 
synonym and antonym relations in an online dictionary (e.g., id138) to 

92 
 

id31 and opinion mining 

generate  the  expanded  sets  p     and  n   ,  which  form  the  training  set.  the 
algorithm then uses all the glosses in the dictionary for each term in p        n    
to generate a feature vector. a binary classifier is then built using different 
learning  algorithms.  the  process  can  also  be  run  iteratively.  that  is,  the 
newly  identified  positive  and  negative  terms  and  their  synonyms  and 
antonyms  are  added  to  the  training  set,  an  updated  classifier  can  be 
constructed  and  so  on.  in  (esuli  and  sebastiani,  2006),  the  authors  also 
included the category objective. to expand the objective seed set, hyponyms 
were used in addition to synonyms and antonyms. they then tried different 
strategies to do the three-class classification. in (esuli and sebastiani, 2006), 
a committee of classifiers based on the above method was utilized to build 
the  sentiid138,  a  lexical  resource  in  which  each  synset  of  id138  is 
associated with three numerical scores obj(s), pos(s) and neg(s), describing 
how objective, positive, and negative the terms contained in the synset are. 
the  method  of  kim  and  hovy  (2006)  also  started  with  three  seed  sets  of 
positive,  negative,  and  neutral  words.  it  then  finds  their  synonyms  in 
id138. the expanded sets, however, have many errors. the method then 
uses  a  bayesian  formula  to  compute  the  closeness  of  each  word  to  each 
category  (positive,  negative,  and  neutral)  to  determine  the  most  probable 
class for the word.  

andreevskaia  and  bergler 
(2006)  proposed  a  more  sophisticated 
id64 method with several techniques to expand the initial positive 
and  negative  seed  sets  and  to  clean  up  the  expanded  sets  (removing  non-
adjectives and words in both positive and negative sets). in addition, their 
algorithm  also  performs  multiple  runs  of  the  id64  process  using 
non-overlapping seed sub-sets. each run typically finds a slightly different 
set  of  sentiment  words.  a  net  overlapping  score  for  each  word  is  then 
computed based on how many times the word is discovered in the runs as a 
positive word and as a negative word. the score is then normalized to [0, 1] 
based on the fuzzy set theory.  

in  (kaji  and  kitsuregawa,  2006;  kaji  and  kitsuregawa,  2007),  many 
heuristics  were  used  to  build  a  sentiment  lexicon  from  html  documents 
based on web page layout structures. for example, a table in a web page 
may have a column clearly indicate positive or negative orientations (e.g., 
pros and cons) of the surround text. these clues can be exploited to extract 
a large number of candidate positive and negative opinion sentences from a 
large  set  of  web  pages.  adjective  phrases  are  then  extracted  from  these 
sentences and assigned sentiment orientations based on different statistics of 
their occurrences in the positive and negative sentence sets respectively. 

velikovich  et  al.  (2010)  also  proposed  a  method  to  construct  a  sentient 

 

93 

 

  id31 and opinion mining 

lexicon  using  web  pages.  it  was  based  on  a  graph  propagation  algorithm 
over  a  phrase  similarity  graph.  it  again assumed  as  input  a  set  of  positive 
seed  phrases  and  a  set  of  negative  seed  phrases.  the  nodes  in  the  phrase 
graph were the candidate phrases selected from all id165s up to length 10 
extracted from 4 billion web pages. only 20 million candidate phrases were 
selected using several heuristics, e.g., frequency and mutual information of 
word  boundaries.  a  context  vector  for  each  candidate  phrase  was  then 
constructed  based  on  a  word  window  of  size  six  aggregated  over  all 
mentions  of  the  phrase  in  the  4  billion  documents.  the  edge  set  was 
constructed through cosine similarity computation of the context vectors of 
the candidate phrases. all edges (vi, vj) were discarded if they were not one 
of the 25 highest weighted edges adjacent to either node vi or vj. the edge 
weight  was  set  to  the  corresponding  cosine  similarity  value.  a  graph-
propagation method was used to calculate the sentiment of each phrase as 
the aggregate of all the best paths to the seed words. 

in (dragut et al., 2010), yet another but very different id64 method 
was proposed using id138. given a set of seed words, instead of simply 
following  the  dictionary,  the  authors  proposed  a  set  of  sophisticated 
id136  rules  to  determine  other  words     sentiment  orientations  through a 
deductive process. that is, the algorithm takes words with known sentiment 
orientations  (the  seeds)  as  input  and  produces  synsets  (sets  of  synonyms) 
with  orientations.  the  synsets  with  the  deduced  orientations  can  then  be 
used to further deduce the polarities of other words.  

peng  and  park  (2011)  presented  a  sentiment  lexicon  generation  method 
using  constrained  symmetric  nonnegative  matrix  factorization  (csnmf). 
the  method  first  uses  id64  to  find  a  set  of  candidate  sentiment 
words in a dictionary and then uses a large corpus to assign polarity scores 
to each word. this method thus uses both dictionary and corpus. xu, meng 
and  wang  (2010)  presented  several  integrated  methods  as  well  using 
dictionaries  and  corpora to  find  emotion  words.  their  method  is  based  on 
label propagation in a similarity graph (zhu and ghahramani, 2002).   

in  summary,  we  note  that  the  advantage  of  using  a  dictionary-based 
approach is that one can easily and quickly find a large number of sentiment 
words  with  their  orientations.  although  the  resulting  list  can  have  many 
errors,  a  manual  checking  can  be  performed  to  clean  it  up,  which  is  time 
consuming  (not  as  bad  as  people  thought,  only  a  few  days  for  a  native 
speaker) but it is only a one-time effort. the main disadvantage is that the 
sentiment orientations of words collected this way are general or domain and 
context independent.  in other words, it is hard to use the dictionary-based 
approach  to  find  domain  or  context  dependent  orientations  of  sentiment 
words. as discussed before, many sentiment words have context dependent 

94 
 

id31 and opinion mining 

orientations.  for  example,  for  a  speaker  phone,  if  it  is  quiet,  it  is  usually 
negative.  however,  for  a  car,  if  it  is  quiet,  it  is  positive.  the  sentiment 
orientation  of  quiet  is  domain  or  context  dependent.  the  corpus-based 
approach below can help deal with this problem. 

6.2 

corpus-based approach  

the  corpus-based  approach  has  been  applied  to  two  main  scenarios:  (1) 
given a seed list of known (often general-purpose) sentiment words, discover 
other sentiment words and their orientations from a domain corpus, and (2) 
adapt  a  general-purpose  sentiment  lexicon  to  a  new  one  using  a  domain 
corpus for id31 applications in the domain. however, the issue 
is more complicated than just building a domain specific sentiment lexicon 
because in the same domain the same word can be positive in one context 
but negative in another. below, we discuss some of the existing works that 
tried  to  deal  with  these  problems.  note  that  although  the  corpus-based 
approach may also be used to build a general-purpose sentiment lexicon if a 
very  large  and  very  diverse  corpus  is  available,  the  dictionary-based 
approach  is  usually  more  effective  for  that  because  a  dictionary  has  all 
words.  

one of the key and also early ideas was proposed by hazivassiloglou and 
mckeown  (1997).  the  authors  used  a  corpus  and  some  seed  adjective 
sentiment words to find additional sentiment adjectives in the corpus. their 
technique exploited a set of linguistic rules or conventions on connectives to 
identify  more  adjective  sentiment  words  and  their  orientations  from  the 
corpus.  one  of  the  rules  is  about  the  conjunction  and,  which  says  that 
conjoined adjectives usually have the same orientation. for example, in the 
sentence,    this car is beautiful and spacious,    if    beautiful    is known to be 
positive, it can be inferred that    spacious    is also positive. this is so because 
people usually express the same sentiment on both sides of a conjunction. 
the following sentence is not likely,    this car is beautiful and difficult to 
drive.     it  is  more  acceptable  if  it  is  changed  to     this  car  is  beautiful  but 
difficult to drive.    rules were also designed for other connectives, i.e., or, 
but,  either   or,  and  neither   nor.  this  idea  is  called  sentiment 
consistency. in practice, it is not always consistent. thus, a learning step was 
also  applied  to  determine  if  two  conjoined  adjectives  have  the  same  or 
different  orientations.  first,  a graph  was  formed  with same-  and different-
orientation links between adjectives. id91 was then performed on the 
graph to produce two sets of words: positive and negative.  

kanayama and nasukawa (2006) extended the approach by introducing the 

 

95 

 

  id31 and opinion mining 

concepts of intra-sentential (within a sentence) and inter-sentential (between 
neighboring  sentences)  sentiment  consistency,  which  they  call  coherency. 
the intra-sentential consistency is similar to the idea above. inter-sentential 
consistency  simply  applies  the  idea  to  neighboring  sentences.  that  is,  the 
same  sentiment  orientation  is  usually  expressed  in  consecutive  sentences. 
sentiment changes are indicated by adversative expressions such as but and 
however.  some  criteria  were  also  proposed  to  determine  whether  to  add  a 
word to the positive or negative lexicon. this study was based on japanese 
text  and  was  used  to  find  domain  dependent  sentiment  words  and  their 
orientations.  other  related  work  includes  those  in  (kaji  and  kitsuregawa, 
2006; kaji and kitsuregawa, 2007).  

although finding domain specific sentiment words and their orientations are 
useful,  it  is  insufficient  in  practice. ding,  liu  and  yu  (2008)  showed  that 
many words in the same domain can have different orientations in different 
contexts. in fact, this phenomenon has been depicted by the basic rules of 
opinions in section 5.2. for example, in the camera domain, the word    long    
clearly  expresses  opposite  opinions  in  the  following  two  sentences:     the 
battery life is long    (positive) and    it takes a long time to focus    (negative). 
such situations often occur with quantifiers, e.g., long, short, large, small, 
etc. however, it is not always. for example, in a car review, the sentence 
   this car is very quiet    is positive, but the sentence    the audio system in the 
car  is  very  quiet     is  negative.  thus,  finding  domain-dependent  sentiment 
words and their orientations is insufficient. the authors found that both the 
aspect and the sentiment expressing words were both important. they then 
proposed to use the pair (aspect, sentiment_word) as an opinion context, e.g., 
(   battery life   ,    long   ). their method thus determines sentiment words and 
their orientations together with the aspects that they modify. in determining 
whether a pair is positive or negative, the above intra-sentential and inter-
sentential  sentiment  consistency  rules  about  connectives  are  still  applied. 
the  work  in  (ganapathibhotla  and  liu,  2008)  adopted  the  same  context 
definition  but  used  it  for  analyzing  comparative  sentences.  wu  and  wen 
(2010) dealt with a similar problem in chinese. however, they only focused 
on pairs in which the adjectives are quantifiers such as big, small, low and 
high. their method is based on syntactic patterns as in (turney, 2002), and 
also  use  the  web  search  hit  counts  to solve  the  problem.  lu  et  al.  (2011) 
used  the  same  context  definition  as  well.  like  that  in  (ding,  liu  and  yu, 
2008), they assumed that the set of aspects was given. they formulated the 
problem  of  assigning  each  pair  the  positive  or  negative  sentiment  as  an 
optimization problem with a number of constraints. the objective function 
and  constraints  were  designed  based  on  clues  such  as  a  general-purpose 
sentiment  lexicon,  the  overall  sentiment  rating  of  each  review,  synonyms 

96 
 

id31 and opinion mining 

and  antonyms,  as  well  as  conjunction     and     rules,     but     rules,  and 
   negation     rules.  to  some  extent,  the  methods  in  (takamura,  inui  and 
okumura, 2007; turney, 2002) can also be considered as an implicit method 
for  finding  context-specific  opinions,  but  they  did  not  use  the  sentiment 
consistency  idea.  instead,  they  used  the  web  to  find  their  orientations. 
however,  we  should  note  that  all  these  context  definitions  are  still  not 
sufficient for all cases as the basic rules of opinions discussed in section 5.2 
showed, i.e., many contexts can be more complex, e.g., consuming a large 
amount of resources.  

along  a  similar  line,  wilson,  wiebe,  and  hoffmann  (2005)  studied 
contextual  subjectivities  and  sentiments  at  the  phrase  or  expression  level. 
contextual sentiment means that although a word or phrase in a lexicon is 
marked positive or negative, but in the context of the sentence expression it 
may  have  no  sentiment  or  have  the  opposite  sentiment.  in  this  work,  the 
subjective expressions were first labeled in the corpus, i.e., those expressions 
that contain subjective words or phrases in a given subjectivity lexicon. note 
that a subjectivity lexicon is slightly different from a sentiment lexicon as 
subjectivity lexicon may contains words that indicate only subjectivity but 
no sentiment, e.g., feel, and think. the goal of the work was to classify the 
contextual  sentiment  of  the  given  expressions  that  contain  instances  of 
subjectivity  clues  in  the  subjectivity  lexicon.  the  paper  took  a  supervised 
learning approach with two steps. in the first step, it determines whether the 
expression  is  subjective  or  objective.  in  the  second  step,  it  determines 
whether the subjective expression is positive, negative, both, or neutral. both 
means  there  are  both  positive  and  negative  sentiments.  neutral  is  still 
included  because  the  first  step  can  make  mistakes  and  left  some  neutral 
expressions unidentified. for subjectivity classification, a large and rich set 
of  features  was  used,  which  included  word  features,  modification  features 
(dependency features), structure features (dependency tree based patterns), 
sentence features, and document features. for the second step of sentiment 
classification, it used features such as word tokens, word prior sentiments, 
negations,  modified  by  polarity,  conj  polarity,  etc.  for  both  steps,  the 
machine  learning  algorithm  boostexter  adaboost.hm  (schapire  and 
singer, 2000) was employed to build classifiers.  

a related work on expression level sentiment classification was also done in 
(choi  and  cardie,  2008),  where  the  authors  classified  the  expressions 
annotated  in  multi-perspective  question  answering  (mpqa)  corpus 
(wiebe,  wilson  and  cardie,  2005).  both  lexicon   based  classification  and 
supervised learning were experimented. in (breck, choi and cardie, 2007), 
the authors studied the problem of extracting sentiment expressions with any 
number  of  words  using  conditional  random  fields  (crf)  (lafferty, 

 

97 

 

  id31 and opinion mining 

mccallum and pereira, 2001).  

the problem of adapting a general lexicon to a new one for domain specific 
expression  level  sentiment  classification  was  studied  in  (choi  and  cardie, 
2009).  their  technique  adapted  the  word-level  polarities  of  a  general-
purpose  sentiment  lexicon  for  a  particular  domain  by  utilizing  the 
expression-level polarities in the domain, and in return, the adapted word-
level  polarities  were  used  to  improve  the  expression-level  polarities.  the 
word-level and the expression-level polarity relationships were modeled as a 
set  of  constraints  and  the  problem  was  solved  using  integer  linear 
programming.  this  work  assumed  that  there  was  a  given  general-purpose 
polarity  lexicon  l,  and  a  polarity  classification  algorithm  f(el,  l)  that  can 
determine the polarity of the opinion expression el based on the words in el 
and l. jijkoun, rijke and weerkamp (2010) proposed a related method to 
adapt a general sentiment lexicon to a topic specific one as well.  

du et al. (2010) studied the problem of adapting the sentiment lexicon from 
one domain (not a general-purpose lexicon) to another domain. as input, the 
algorithm  assumes  the  availability  of  a  set  of  in-domain  sentiment-labeled 
documents, a set of sentiment words from these in-domain documents, and a 
set  of  out-of-domain  documents.  the  task  was  to  make  the  in-domain 
sentiment lexicon adapted for the out-of-domain documents. two ideas were 
used  in  the  study.  first,  a  document  should  be  positive  (or  negative)  if  it 
contains many positive (or negative) words, and a word should be positive 
(or negative) if it appears in many positive (or negative) documents. these 
are  mutual  reinforcement  relationships.  second,  even  though  the  two 
domains  may  be  under  different  distributions,  it  is  possible  to  identify  a 
common part between them (e.g. the same word has the same orientation). 
the sentiment lexicon adaption was solved using the information bottleneck 
framework. the same problem was also solved in (du and tan, 2009).  

on  a  slightly  different  topic,  wiebe  and  mihalcea  (2006)  investigated  the 
possibility of assigning subjectivity labels to word senses based on a corpus. 
two  studies  were  conducted.  the  first  study  investigated  the  agreement 
between  annotators  who  manually  assigned  labels  subjective,  objective,  or 
both to id138 senses. the second study evaluated a method for automatic 
assignment  of  subjectivity  labels/scores  to  word  senses.  the  method  was 
based  on  distributional  similarity  (lin,  1998).  their  work  showed  that 
subjectivity is a property that can be associated with word senses, and word 
sense  disambiguation  can  directly  benefit  from  subjectivity  annotations.  a 
subsequent work was reported in (akkaya, wiebe and mihalcea, 2009). su 
and markert (2008) also studied the problem and performed a case study for 
subjectivity recognition. in (su and markert, 2010), they further investigated 
this problem and applied it in a cross-lingual environment.  

98 
 

id31 and opinion mining 

brody  and  diakopoulos  (2011)  studied  the  lengthening  of  words  (e.g., 
slooooow)  in  microblogs.  they  showed  that  lengthening  is  strongly 
associated with subjectivity and sentiment, and presented an automatic way 
to leverage this association to detect domain sentiment and emotion words. 

finally,  feng,  bose  and  choi  (2011)  studied  the  problem  of  producing  a 
connotation lexicon. a connotation lexicon differs from a sentiment lexicon 
in that the latter concerns words that express sentiment either explicitly or 
implicitly, while the former concerns words that are often associated with a 
specific  polarity  of  sentiment,  e.g.,  award  and  promotion  have  positive 
connotation and cancer and war have negative connotation. a graph-based 
method based on mutual reinforcement was proposed to solve the problem.  

6.3 

desirable and undesirable facts  

sentiment words and expressions that we have discussed so far are mainly 
subjective words and expressions that indicate positive or negative opinions. 
however, as mentioned earlier, many objective words and expressions can 
imply opinions too in certain domains or contexts because they can represent 
desirable or undesirable facts in these domains or contexts.  

in  (zhang  and  liu, 2011b),  a  method  was  proposed  to  identify  nouns  and 
noun  phrases  that  are  aspects  and  also  imply  sentiments  in  a  particular 
domain. these nouns and noun phrases alone indicate no sentiments, but in 
the  domain  context  they  may  represent  desirable  or  undesirable  facts.  for 
example,    valley    and    mountain    do not have any sentiment connotation in 
general, i.e., they are objective. however, in the domain of mattress reviews, 
they  often  imply  negative  opinions  as  in     within  a  month,  a  valley  has 
formed  in  the  middle  of  the  mattress.     here,     valley     implies  a  negative 
sentiment  on  the  aspect  of  mattress  quality.  identifying  the  sentiment 
orientations  of  such  aspects  is  very  challenging  but  critical  for  effective 
id31 in these domains.  

the algorithm in (zhang and liu, 2011b) was based on the following idea: 
although such sentences are usually objective with no explicit sentiments, in 
some  cases  the  authors/reviewers  may  also  give  explicit  sentiments,  e.g., 
   within a month, a valley has formed in the middle of the mattress, which is 
terrible.     the  context  of  this  sentence  indicates  that     valley     may  not  be 
desirable.  note  that  this  work  assumed  that  the  set  of  aspects  which  are 
nouns and noun phrases are given. however, the problem with this approach 
is  that  those  aspects  (nouns  and  noun  phrases)  with  no  implied  sentiment 
may  also  be  in  some  positive  or  negative  sentiment  contexts,  e.g.,     voice 
quality    in    the voice quality is poor.    to distinguish these two cases, the 

 

99 

 

  id31 and opinion mining 

following observation was used.  

observation: for normal aspects which themselves don   t have positive or 
negative  connotations,  people  can  express  different  opinions,  i.e.,  both 
positive  and  negative.  for  example,  for  aspect     voice  quality   ,  people 
can  say     good  voice  quality     and     bad  voice  quality   .  however,  for 
aspects  which  represent  desirable  or  undesirable  facts,  they  often  have 
only  a  single  sentiment,  either  positive  or  negative,  but  not  both.  for 
example, it is unlikely that both the following two sentences appear:    a 
bad valley has formed    and    a good valley has formed   .  

with this observation in mind, the approach consists of two steps: 
1.  candidate identification: this step determines the surrounding sentiment 
context of each noun aspect. if an aspect occurs in negative (respectively 
positive) sentiment contexts significantly more frequently than in positive 
(or negative) sentiment contexts, it is inferred that its polarity is negative 
(or  positive).  this  step  thus  produces  a  list  of  candidate  aspects  with 
positive opinions and a list of candidate aspects with negative opinions. 

2.  pruning: this step prunes the two lists based on the observation above. 
the idea is that when a noun aspect is directly modified by both positive 
and negative sentiment words, it is unlikely to be an opinionated aspect. 
two types of direct dependency relations were used.  
type 1: 

o      o-dep     f 

it means o depends on f through the relation o-dep, e.g.,    this tv 
has a good picture quality.    

type 2:  

o     o-dep     h     f-dep     f 

it means both o and f depend on h through relations o-dep and f-
dep respectively, e.g.,    the springs of the mattress are bad.    

where o is a sentiment word, o-dep / f-dep is a dependency relation. f 
is the noun aspect. h means any word. for the first example, given aspect 
   picture  quality   ,  we  can  identify  its  modification  sentiment  word 
   good.    for the second example, given aspect    springs   , we can get its 
modification sentiment word    bad   . here h is the word    are   . 

this work is just the first attempt to tackle the problem. its accuracy is still 
not high. much further research is needed.  

6.4 

summary  

due 
to  contributions  of  many  researchers,  several  general-purpose 
subjectivity,  sentiment,  and  emotion  lexicons  have  been  constructed,  and 
some of them are also publically available, e.g.,  

100 
 

id31 and opinion mining 

     general inquirer lexicon (stone, 1968): 

(http://www.wjh.harvard.edu/~inquirer/ spreadsheet_guide.htm)  

     sentiment lexicon (hu and liu, 2004): 

(http://www.cs.uic.edu/~liub/fbs/ sentiment-analysis.html)   

     mpqa subjectivity lexicon (wilson, wiebe and hoffmann, 2005): 

(http://www.cs.pitt.edu/mpqa/subj _lexicon .html)  

     sentiid138 (esuli and sebastiani, 2006): 

(http://sentiid138.isti.cnr.it/) 

     emotion lexicon (mohammad and turney, 2010): 

(http://www.purl.org/net/emolex) 

however,  domain  and  context  dependent  sentiments  remain  to  be  highly 
challenging even with so much research. recent work also used word vector 
and matrix to capture the contextual information of sentiment words (maas 
et al., 2011; yessenalina and cardie, 2011). factual words and expressions 
implying opinions have barely been studied (see section 6.3), but they are 
very important for many domains.  

finally, we note that having a sentiment lexicon (even with domain specific 
orientations) does not mean that a word in the lexicon always expresses an 
opinion/sentiment in a specific sentence. for example, in    i am looking for a 
good car to buy,       good    here does not express either a positive or negative 
opinion on any particular car.  

 

101 

 

  id31 and opinion mining 

chapter 7 

opinion summarization 

 
as  discussed  in  chapter  2,  in  most  sentiment  analysis  applications,  one 
needs  to  study  opinions  from  many  people  because  due  to  the  subjective 
nature  of  opinions,  looking  at  only  the  opinion  from  a  single  person  is 
usually insufficient. some form of summary is needed. chapter 2 indicated 
that  the  opinion  quintuple  provides  the  basic  information  for  an  opinion 
summary. such a summary is called an aspect-based summary (or feature-
based  summary)  and  was  proposed  in  (hu  and  liu,  2004;  liu,  hu  and 
cheng,  2005).  much  of  the  opinion  summarization  research  uses  related 
ideas. this framework is also widely applied in industry. for example, the 
sentiment  analysis  systems  of  microsoft  bing  and  google  product  search 
use this form of summary. the output summary can be either in a structured 
form (see section 7.1) or in an unstructured form as a short text document.   

in general, opinion summarization can be seen as a form of multi-document 
text  summarization.  text  summarization  has  been  studied  extensively  in 
nlp (das, 2007). however, an opinion summary is quite different from a 
traditional  single  document  or  multi-document  summary  (of  factual 
information) as an opinion summary is often centered on entities and aspects 
and sentiments about them, and also has a quantitative side, which are the 
essence  of  aspect-based  opinion  summary.  traditional  single  document 
summarization  produces  a  short  text  from  a  long  text  by  extracting  some 
   important     sentences.  traditional  multi-document  summarization  finds 
differences among documents and discards repeated information. neither of 
them explicitly captures different topics/entities and their aspects discussed 
in the document, nor do they have a quantitative side. the    importance    of a 
sentence  in  traditional  text  summarization  is  often  defined  operationally 
based on the summarization algorithms and measures used in each system. 
opinion summarization, on the other hand, can be conceptually defined. the 
summaries are thus structured. even for output summaries that are short text 
documents, there are still some explicit structures in them.  

aspect-based opinion 
summarization  

7.1 

102 
 

id31 and opinion mining 

aspect-based opinion summarization has two  main characteristics. first, it 
captures the essence of opinions: opinion targets (entities and their aspects) 
and  sentiments  about  them.  second,  it is  quantitative, which  means  that  it 
gives  the  number  or  percent  of  people  who  hold  positive  or  negative 
opinions  about  the  entities  and  aspects.  the  quantitative  side  is  crucial 
because of the subjective nature of opinions. the resulting opinion summary 
is  a  form  of  structured  summary  produced  from  the  opinion  quintuple  in 
section 2.1. we have described the summary in section 2.2. it is reproduced 
here  for  completeness.  figure  7.1  shows  an  aspect-based  summary  of 
opinions about a digital camera (hu and liu, 2004). the aspect general 
represents opinions on the camera as a whole, i.e., the entity. for each aspect 
(e.g., picture quality), it shows how many people have positive and negative 
opinions  respectively.  <individual  review  sentences>  links  to  the  actual 
sentences (or full reviews or blogs).  this structured summary can also be 
visualized  (liu,  hu  and  cheng,  2005).  figure  7.2(a)  uses  a  bar  chart  to 
visualize the summary in figure 7.1. in the figure, each bar above the x-axis 
shows the number of positive opinions about the aspect given at the top. the 
corresponding bar below the x-axis shows the number of negative opinions 
on  the  same  aspect.  clicking  on  each  bar,  we  can  see  the  individual 
sentences and full reviews. obviously, other visualizations are also possible. 
for  example,  the  bar  charts  of  both  microsoft  bing  search  and  google 
product  search  use  the  percent  of  positive  opinions  on  each  aspect. 
comparing  opinion  summaries  of  a  few  entities  is  even  more  interesting 
(liu,  hu  and  cheng,  2005).  figure  7.2(b)  shows  the  visual  opinion 
comparison of two cameras. we can see how consumers view each of them 
along different aspect dimensions including the entities themselves.  
the opinion quintuples in fact allows one to provide many more forms of 
structured summaries. for example, if time is extracted, one can show the 
trend of opinions on different aspects. even without using sentiments, one 
can see the buzz (frequency) of each aspect mentions, which gives the user 
an  idea  what  aspects  people  are  most  concerned  about.  in  fact,  with  the 
quintuple, a full range of database and olap tools can be used to slice and 
dice  the  data  for  all  kinds  of  qualitative  and  quantitative  analysis.  for 
example,  in  one  practical  sentiment  analysis  application  in  the  automobile 
domain,  opinion  quintuples  of  individual  cars  were  mined  first.  the  user 
then compared sentiments about small cars, medium sized cars, german cars 
and japanese cars, etc. in addition, the id31 results were also 
used as raw data for data mining. the user ran a id91 algorithm and 
found some interesting segments of the market. for example, it was found 
that  one  segment  of  the  customers  always  talked  about  how  beautiful  and 
slick the car looked and how fun it was to drive, etc, while another segment 
of the customers talked a lot about back seats and trunk space, etc. clearly, 

 

103 

 

  id31 and opinion mining 

aspect: picture quality 

aspect: general 

 
 

positive:  
negative:  

  digital camera 1:  
 
 
 
 
 
 
 
 
 
 

positive:  
 negative:  

positive:  
negative:  

    

 
 

 
 

95 
10 
aspect: battery life 

105 
12 

<individual review sentences> 
<individual review sentences> 

<individual review sentences> 
<individual review sentences> 

50        <individual review sentences> 
9 
<individual review sentences> 

figure 7.1. an aspect-based opinion summary. 

the  first  segment  consisted  of  mainly  young  people,  while  the  second 
segment  consisted  mainly  of  people  with  families  and  children.  such 
insights were extremely important. they enabled the user to see the opinions 
of different segments of customers.  

positive   general  picture 

battery 

lens 

weight 

size 

negative 

digital camera 1 

(a) visualization of aspect-based summary of opinions on a digital camera 

positive 

general   picture  

battery  

lens  

weight  

size 

negative 

digital camera 1 

digital camera 2

(b) visual opinion comparison of two digital cameras 
figure 7.2. visualization of aspect-based summaries of opinions 

 

this form of structured summary has also been adopted by other researchers 
to summarize movie reviews (zhuang, jing and zhu, 2006), to summarize 
chinese opinion text (ku, liang and chen, 2006), and to summarize service 

104 
 

id31 and opinion mining 

reviews  (blair-goldensohn  et  al.,  2008).  however,  we  should  note  that 
aspect-based summary does not have to be in this structured form. it can also 
be  in  the  form  of  a  text  document  based  on  the  same  idea.  in  the  next 
section, we discuss other related researches.  

7.2 

improvements to aspect-based 
opinion summarization 

several improvements and refinements have been proposed by researchers 
for  the  basic  aspect-based  summary.  carenini,  ng  and  pauls  (2006) 
proposed to integrate aspect-based summarization with two traditional text 
summarization approaches of factual documents, i.e., sentence selection (or 
extraction)  and  sentence  generation.  we  discuss  the  integration  with  the 
sentence  selection  approach  first.  their  system  first  identifies  aspect 
expressions  from  reviews  of  a  particular  entity  (e.g.,  a  product)  using  the 
method in (hu and liu, 2004). it then maps the aspect expressions to some 
given aspect categories organized as an ontology tree for the entity. these 
aspects in the tree are then scored based on their sentiment strength. those 
sentences  containing  aspect  expressions  are  also  extracted.  each  such 
sentence is then rated based on scores of aspects in the sentence. if multiple 
sentences  have  the  same  sentence  rating,  a  traditional  centroid  based 
sentence selection method is used to break the tie (radev et al., 2003). all 
relevant  sentences  are  attached  to  their  corresponding  aspects  in  the 
ontology.  the  sentences  for  each  aspect  are  then  selected  for  the  final 
summary based on sentence scores and aspect positions in the ontology tree. 
the integration with the sentence generation approach works similarly. first, 
a  measure  is  used  to  score  the  aspects  in  the  ontology  based  on  their 
occurrence  frequencies,  sentiment  strengths,  and  their  positions  in  the 
ontology. an algorithm is also applied to select aspects in the ontology tree. 
positive and negative sentiments are then computed for the aspects. based 
on the selected aspects and their sentiments, a language generator generates 
the  summary  sentences  which  can  be  qualitative  and  quantitative.  a  user 
evaluation was carried out to assess the effectiveness of the two integration 
approaches.  the  results  showed  that  they  performed  equally  well,  but  for 
different  reasons.  the  sentence  selection  method  gave  more  varied 
languages and more details, while the sentence generation approach gives a 
better sentiment overview of the reviews.  

in  (tata  and  di  eugenio,  2010),  tata  and  eugenio  produced  an  opinion 
summary of song reviews similar to that in (hu and liu, 2004), but for each 
aspect  and  each  sentiment  (postive  or  ngative)  they  first  selected  a 

 

105 

 

  id31 and opinion mining 

representative  sentence  for  the  group.  the  sentence  should  mention  the 
fewest  aspects  (thus  the  representative  sentence  is  focused).  they  then 
ordered the sentences using a given domain ontology by mapping sentences 
to  the  ontology  nodes.  the  ontology  basically  encodes  the  key  domain 
concepts and their relations. the sentences were ordered and organized into 
paragraphs  following  the  tree  such  that  they  appear  in  a  conceptually 
coherent fashion. 

lu  et  al.  (2010)  also  used  online  ontologies  of  entities  and  aspects  to 
organize and summarize opinions. their method is related to the above two, 
but  is  also  different.  their  system  first  selects  aspects  that  capture  major 
opinions.  the  selection  is  done  by  frequency,  opinion  coverage  (no 
redundancy),  or  conditional  id178.  it  then  orders  aspects  and  their 
corresponding  sentences  based  on  a  coherence  measure,  which  tries  to 
optimize  the  ordering  so  that  they  best  follow  the  sequences  of  aspect 
appearances in their original postings.  

ku,  liang,  and  chen  (2006)  performed  blog  opinion  summarization,  and 
produced two types of summaries: brief and detailed summaries, based on 
extracted  topics  (aspects)  and  sentiments  on  the  topics.  for  the  brief 
summary,  their  method  picks  up  the  document/article  with  the  largest 
number of positive or negative sentences and uses its headline to represent 
the  overall  summary  of  positive-topical  or  negative-topical  sentences.  for 
detailed  summary,  it  lists  positive-topical  and  negative-topical  sentences 
with high sentiment degrees. 

lerman,  blair-goldensohn  and  mcdonald 
(2009)  defined  opinion 
summarization in a slightly different way. given a set of documents d (e.g., 
reviews) that contains opinions about some entity of interest, the goal of an 
opinion summarization system is to generate a summary s of that entity that 
is representative of the average opinion and speaks to its important aspects. 
this  paper  proposed  three  different  models  to  perform  summarization  of 
reviews of a product. all these models choose some set of sentences from a 
review.  the  first  model  is  called  sentiment  match  (sm),  which  extracts 
sentences  so  that  the  average  sentiment  of  the  summary  is  as  close  as 
possible to the average sentiment rating of reviews of the entity. the second 
model,  called  sentiment  match  +  aspect  coverage  (smac),  builds  a 
summary that trades-off between maximally covering important aspects and 
matching  the  overall  sentiment  of  the  entity.  the  third  model,  called 
sentiment-aspect  match  (sam),  not  only  attempts  to  cover  important 
aspects,  but  cover  them  with  appropriate  sentiment.  a  comprehensive 
evaluation  of  human  users  was  conducted  to  compare  the  three  types  of 
summaries. it was found that although the sam model was the best, it is not 
significantly better than others.  

106 
 

id31 and opinion mining 

in (nishikawa et al., 2010b), a more sophisticated summarization technique 
was proposed, which generates a traditional text summary by selecting and 
ordering  sentences 
taken  from  multiple  reviews,  considering  both 
informativeness and readability of the final summary. the informativeness 
was  defined  as  the  sum  of  frequency  of  each  aspect-sentiment  pair. 
readability  was  defined  as  the  natural  sequence  of  sentences,  which  was 
measured  as  the  sum  of  the  connectivity  of  all  adjacent  sentences  in  the 
sequence. the problem was then solved through optimization. in (nishikawa 
et al., 2010a), the authors further studied this problem using an integer linear 
programming  formulation.  in  (ganesan,  zhai  and  han,  2010),  a  graphical 
model  based  method  was  used  to  generate  an  abstractive  summary  of 
opinions. in (yatani et al., 2011), adjective-noun pairs were extracted as a 
summary.  

7.3 

contrastive view summarization 

several  researchers  also  studied  the  problem  of  summarizing  opinions  by 
finding contrastive viewpoints. for example, a reviewer may give a positive 
opinion about  the voice  quality of  iphone  by  saying     the voice quality of 
iphone  is  really  good,     but  another  reviewer  may  say  the  opposite,     the 
voice quality of my iphone is lousy.    such pairs can give the reader a direct 
comparative view of different opinions.  

kim and zhai (2009) proposed and studied this problem. given a positive 
sentence  set  and  a  negative  sentence  set,  this  work  performed  contrastive 
opinion  summarization  by  extracting  a  set  of  k  contrastive  sentence  pairs 
from the sets. a pair of opinionated sentences (x, y) is called a contrastive 
sentence pair if sentence x and sentence y are about the same topic aspect, 
but have opposite sentiment orientations. the k chosen sentence pairs must 
also represent both the positive and negative sentence sets well. the authors 
formulated  the  summarization  as  an  optimization  problem  and  solved  it 
based on several similarity functions.  

paul, zhai and girju (2010) worked on this problem as well. their algorithm 
generates a macro multi-view summary and a micro multi-view summary. a 
macro  multi-view  summary  contains  multiple  sets  of  sentences,  each 
representing a different opinion. a micro multi-view summary contains a set 
of  pairs  of  contrastive  sentences  (each  pair  consists  of  two  sentences 
representing two different opinions). the algorithm works in two steps. in 
the first step, it uses a id96 approach to modeling and mining both 
topics  (aspects)  and  sentiments.  in  the  second  step,  a  random  walk 
formulation (similar to id95 (page et al., 1999)) was proposed to score 

 

107 

 

  id31 and opinion mining 

sentences  and  pairs  of  sentences  from  opposite  viewpoints  based  on  both 
their  representativeness  and  their  contrastiveness  with each other.  along  a 
similar  line,  park,  lee  and  song  (2011)  reported  another  method  for 
generating contrasting opposing views in news articles. 

in  (lerman  and  mcdonald,  2009),  lerman  and  mcdonald  formulated  a 
different  contrastive  summarization  problem.  they  wanted  to  produce 
contrastive summaries of opinions about two different products to highlight 
the differences of opinions about them. their approach is to jointly model 
the two summarization tasks and in optimization to explicitly consider the 
fact that it wants the two summaries to contrast.  

7.4 

traditional summarization 

several  researchers  have  also  studied  opinion  summarization  in  the 
traditional  fashion,  e.g.,  producing  a  short  text  summary  with  limited  or 
without  consideration  of  aspects  (or  topics)  and  sentiments  about  them.  a 
supervised learning method was proposed in (beineke et al., 2003) to select 
important  sentences  in  reviews.  a  paragraph-id91  algorithm  was 
proposed in (seki et al., 2006) to also select a set of important sentences.  

in  (wang  and  liu,  2011),  the  authors  studied  extractive  summarization 
(selection  of  important  sentences)  of  opinions  in  conversations.  they 
experimented  with  both  the  traditional  sentence  ranking  and  graph-based 
approaches, but also considered additional features such as topic relevance, 
sentiments, and the dialogue structure.  

a weakness of such traditional summaries is that they only have limited or 
no consideration of target entities and aspects, and sentiments about them. 
thus, they may select sentences which are not related to sentiments or any 
aspects. another issue is that there is no quantitative perspective, which is 
often important in practice because one out of ten people hating something is 
very different from 5 out of ten people hating something.  

7.5 

summary  

opinion  summarization  is  still  an  active  research  area.  most  opinion 
summarization  methods  which  produce  a  short  text  summary  have  not 
focused  on  the  quantitative  side  (proportions  of  positive  and  negative 
opinions). future research can deal with this problem while also producing 
human  readable  texts.  we  should  note  that  the  opinion  summarization 

108 
 

id31 and opinion mining 

research cannot progress alone because it critically depends on results and 
techniques from other areas of research in id31, e.g., aspect or 
topic  extraction  and  sentiment  classification.  all  these  research  directions 
will need to go hand-in-hand. finally, we should also note that based on the 
structured  summary  in  section  7.1  one  can  generate  natural  language 
sentences  as  well  based  on  what  are  shown  in  the  bar  charts  using  some 
predefined  sentence  templates.  for  instance,  the  first  bar  in  figure  7.2(b) 
can be summarized as    70% of the people are positive about digital camera 1 
in general.    however, this may not be the best sentence for people   s reading 
pleasure.  

 

109 

 

  id31 and opinion mining 

chapter 8 

analysis of comparative opinions  

 
apart from directly expressing positive or negative opinions about an entity 
and its aspects, one can also express opinions by comparing similar entities. 
such  opinions  are  called  comparative  opinions  (jindal  and  liu,  2006a; 
jindal  and  liu,  2006b).  comparative  opinions  are  related  to  but  are  also 
different  from  regular  opinions.  they  not  only  have  different  semantic 
meanings  but  also  have  different  syntactic  forms.  for  example,  a  typical 
regular opinion sentence is    the voice quality of this phone is amazing,    and 
a typical comparative opinion sentence is    the voice quality of nokia phones 
is better than that of iphones.    this comparative sentence does not say that 
any phone   s voice quality is good or bad, but simply compares them. due to 
this  difference,  comparative  opinions  require  different  analysis  techniques. 
like  regular  sentences,  comparative  sentences  can  be  opinionated  or  not-
opinionated.  the  comparative  sentence  above  is  opinionated  because  it 
explicitly expresses a comparative sentiment of its author, while the sentence 
   iphone is 1 inch wider than a normal nokia phone    expresses no sentiment. 
in this chapter, we first define the problem and then present some existing 
methods for solving it. we should also note that there are in fact two main 
types of opinions that are based on comparisons: comparative opinions and 
superlative  opinions.  in  english,  they  are  usually  expressed  using  the 
comparative or superlative forms of adjectives or adverbs, but not always. 
however,  in  this  chapter,  we  study  them  together  and  just  call  them 
comparative  opinions  in  general  because  their  semantic  meanings  and 
handling methods are similar.  

8.1 

problem definitions  

a  comparative  sentence  expresses  a  relation  based  on  similarities  or 
differences of more than one entity. there are several types of comparisons. 
they  can  be  grouped  into  two  main  categories:  gradable  comparison  and 
non-gradable comparison (jindal and liu, 2006a; kennedy, 2005).  

gradable  comparison:  such  a  comparison  expresses  an  ordering 
relationship of entities being compared. it has three sub-types:  
1.  non-equal  gradable  comparison:  it  expresses  a  relation  of  the  type 

110 
 

id31 and opinion mining 

greater or less than that ranks a set of entities over another set of entities 
based  on  some  of  their  shared  aspects,  e.g.,     coke  tastes  better  than 
pepsi.     this  type  also  includes  preference,  e.g.,     i  prefer  coke  to 
pepsi.     

2.  equative  comparison:  it  expresses  a  relation  of  the  type  equal  to  that 
states  two  or  more  entities  are  equal  based  on  some  of  their  shared 
aspects, e.g.,    coke and pepsi taste the same.    

3.  superlative  comparison:  it  expresses  a  relation  of  the  type  greater  or 
less  than  all  others  that  ranks  one  entity  over  all  others,  e.g.,     coke 
tastes the best among all soft drinks.    

non-gradable comparison: such a comparison expresses a relation of two 
or more entities but does not grade them. there are three main sub-types:  
1.  entity a is similar to or different from entity b based on some of their 

shared aspects, e.g.,    coke tastes differently from pepsi.    

2.  entity a has aspect a1, and entity b has aspect a2 (a1 and a2 are usually 
substitutable), e.g.,    desktop pcs use external speakers but laptops use 
internal speakers.    

3.  entity a has aspect a, but entity b does not have, e.g.,    nokia phones 

come with earphones, but iphones do not.    

we  only  focus  on  gradable  comparisons  in  this  chapter.  non-gradable 
comparisons may also express opinions but they are often more subtle and 
difficult to recognize.  

in  english,  comparisons  are  usually  expressed  using  comparative  words 
(also called comparatives) and superlative words (also called superlatives). 
comparatives  are  formed  by  adding  the  suffix  -er  and  superlatives  are 
formed by adding the suffix -est to their base adjectives and adverbs. for 
example,  in     the  battery  life  of  nokia  phones  is  longer  than  motorola 
phones,       longer    is the comparative form of the adjective    long.       longer    
(and    than   ) here also indicates that this is a comparative sentence. in    the 
battery life of nokia phones is the longest,       longest    is the superlative form 
of  the  adjective     long   ,  and  it indicates that  this  is a superlative  sentence. 
we call this type of comparatives and superlatives type 1 comparatives and 
superlatives.  note  that  for  simplicity,  we  often  use  comparative  to  mean 
both comparative and superlative if superlative is not explicitly stated.  

however, adjectives and adverbs with two syllables or more and not ending 
in y do not form comparatives or superlatives by adding -er or -est. instead, 
more, most, less, and least are used before such words, e.g., more beautiful. 
we call this type of comparatives and superlatives type 2 comparatives and 
superlatives. both type 1 and type 2 are called regular comparatives and 
superlatives.  

 

111 

 

  id31 and opinion mining 

english also has irregular comparatives and superlatives, i.e., more, most, 
less,  least,  better,  best,  worse,  worst,  further/farther  and  furthest/farthest, 
which  do  not  follow  the  above  rules.  however,  they  behave  similarly  to 
type 1 comparatives and are thus grouped under type 1.  

these  standard  comparatives  and  superlatives  are  only  some  of  the  words 
that indicate comparisons. in fact, there are many other words and phrases 
that  can  be  used  to  express  comparisons,  e.g.,  prefer  and  superior.  for 
example,  the  sentence     iphone   s  voice  quality  is  superior  to  that  of 
blackberry    says that iphone has a better voice quality and is preferred. in 
(jindal  and  liu,  2006a),  a  list  of  such  words  and  phrases  were  compiled 
(which  by  no  means  is  complete).  since  these  words  and  phrases  usually 
behave similarly to type 1 comparatives, they are also grouped under type 
1.  all  these  words  and  phrases  plus  the  above  standard  comparatives  and 
superlatives are collectively called comparative keywords.  

comparative  keywords  used  in  non-equal  gradable  comparisons  can  be 
further  grouped  into  two  categories  according  to  whether  they  express 
increased or decreased quantities, which are useful in id31.  

     increasing  comparative:  such  a  comparative  expresses  an  increased 

quantity, e.g., more and longer.  

     decreasing  comparative:  such  a  comparative  expresses  a  decreased 

quantity, e.g., less and fewer.  

objective  of  mining  comparative  opinions  (jindal  and  liu,  2006b;  liu, 
2010):  given  an  opinion  document  d,  discover  in  d  all  comparative 
opinion sextuples of the form:  

 

(e1, e2, a, pe, h, t),  

where e1 and e2 are the entity sets being compared based on their shared 
aspects a (entities in e1 appear before entities in e2 in the sentence), pe 
(    {e1, e2}) is the preferred entity set of the opinion holder h, and t is the 
time  when  the  comparative  opinion  is  expressed.  for  a  superlative 
comparison, if one entity set is implicit (not given in the text), we can use 
a special set u to denote it. for an equative comparison, we can use the 
special symbol equal as the value for pe.     

for example, consider the comparative sentence    canon   s picture quality is 
better  than  those  of  lg  and  sony,     written  by  jim  on  9-25-2011.  the 
extracted comparative opinion is: 

  ({canon}, {lg, sony}, {picture_quality}, {canon}, jim, 9-25-2011)  
the entity set e1 is {canon}, the entity set e2 is {lg, sony }, their shared 
aspect set a being compared is {picture_quality}, the preferred entity set is 

112 
 

id31 and opinion mining 

{canon}, the opinion holder h is jim, and the time t when this comparative 
opinion was written is 9-25-2011.  

note that the above representation may not be easily put in a database due to 
the use of sets, but it can be easily converted to multiple tuples with no sets, 
e.g., the above sets based sextuples can be expanded into two tuples:   

(canon, lg, picture_quality, canon, jim, dec-25-2010) 
(canon, sony, picture_quality, canon, jim, dec-25-2010) 

like mining regular opinions, mining comparative opinions needs to extract 
entities, aspects, opinion holders, and times. the techniques used are similar 
too. in fact, these tasks are often easier for comparative sentences because 
entities are usually on the two sides of the comparative keyword, and aspects 
are also near. however, for id31 to identify the preferred entity 
set, a different method is needed which we will discuss in section 8.3. we 
also  need  to  identify  comparative  sentences  themselves  because  not  all 
sentences containing comparative keywords express comparisons and many 
comparative  keywords  and  phrases  are  hard  to  identify  (jindal  and  liu, 
2006b).  below,  we  only  focus  on  studying  two  comparative  opinion 
id31 specific problems, i.e., identifying comparative sentences 
and determining the preferred entity set.   

8.2 

identify comparative sentences  

although most comparative sentences contain comparative and superlative 
keywords, e.g., better, superior, and best, many sentences that contain such 
words are not comparative sentences, e.g.,    i cannot agree with you more.     

in  (jindal  and  liu,  2006a),  it  was  shown  that  almost  every  comparative 
sentence has a keyword (a word or phrase) indicating comparison. using a 
set  of  keywords,  98%  of  comparative  sentences  (recall  =  98%)  were 
identified with a precision of 32% based on their data set. the keywords are: 

1. comparative adjectives (jjr) and comparative adverbs (rbr), e.g., more, 
less,  better,  and  words  ending  with  -er.  these  are  counted  as  only  two 
keywords. 

2. superlative  adjectives  (jjs)  and  superlative  adverbs  (rbs),  e.g.,  most, 
least,  best,  and  words  ending  with  -est.  these  are  also  counted  as  only 
two keywords. 

3. other non-standard indicative words and phrases such as favor, beat, win, 
exceed, outperform, prefer, ahead, than, superior, inferior, number one, 
up  against,  etc.  these  are  counted  individually  in  the  number  of 
keywords.  

 

113 

 

  id31 and opinion mining 

since keywords alone are able to achieve a high recall, they can be used to 
filter out those sentences that are unlikely to be comparative sentences. we 
just need to improve the precision on the remaining sentences.  

it was also observed in (jindal and liu, 2006a) that comparative sentences 
have  strong  patterns  involving  comparative  keywords,  which  is  not 
surprising. these patterns can be used as features in learning. to discover 
these patterns, class sequential rule (csr) mining was employed in (jindal 
and liu, 2006a). class sequential rule mining is a special kind of sequential 
pattern mining (liu, 2006 and 2011). each training example is a pair (si, yi), 
where si is a sequence and yi is a class label, i.e., yi     {comparison, non-
comparison}. the sequence is generated from a sentence. using the training 
data, csrs can be generated.  
for classification model building, the left-hand side sequence patterns of the 
csr rules with high conditional probabilities were used as features. na  ve 
bayes was employed for model building. in (yang and ko, 2011), the same 
problem  was  studied  but  in  the  context  of  korean  language.  the  learning 
algorithm used was the transformation-based learning, which produces rules. 

identification, 

classifying  comparative  sentences  into  four  types:  after  comparative 
sentences are identified, the algorithm also classifies them into four types, 
non-equal gradable, equative, superlative, and non-gradable. for this task, 
(jindal  and  liu,  2006a)  showed  that  keywords  and  keyphrases  as  features 
were already sufficient. id166 gave the best results.  
li et al. (2010) studied the problem of identifying comparative questions 
and the entities (which they call comparators) that are compared. unlike 
the works above, this paper did not decide the types of comparison. for 
comparative  sentences 
they  also  used  sequential 
patterns/rules.  however,  their  patterns  are  different.  they  decided 
whether  a  question  is  a  comparative  question  and  the  entities  being 
compared at the same time. for example, the question sentence    which 
city  is  better,  new  york  or  chicago?     satisfies  the  sequential  pattern 
<which  nn  is  better,  $c  or  $c  ?>,  where  $c  is  an  entity.  a  weakly 
supervised  learning  method  based  on  the  idea  in  (ravichandran  and 
hovy, 2002) was used to learn such patterns. the algorithm is based on 
id64, which starts with a user-given pattern. from this pattern, 
the algorithm extracts a set of initial seed entity (comparators) pairs. for 
each entity pair, all questions containing the pair are retrieved from the 
question  collection  and  regarded  as  comparative  questions.  from  the 
comparative  questions  and  entity  pairs,  all  possible  sequential  patterns 
are  learned  and  evaluated.  the  learning  process  is  the  traditional 

114 
 

id31 and opinion mining 

generalization  and  specialization  process.  any  words  or  phrases  which 
match  $c  in  a  sentence  are  entities.  both  (jindal  and  liu,  2006b)  and 
(yang  and  ko,  2011)  also  extract  compared  entities.  we  will  discuss 
them  in  section  8.4.  other  information  extraction  algorithms  are 
applicable here as well.  

8.3 

identifying preferred entities  

unlike regular opinions, it does not make much sense to perform sentiment 
classification to a comparative opinion sentence as a whole because such a 
sentence  does  not  express a  direct positive or  negative  opinion.  instead, it 
compares  multiple  entities  by  ranking  the  entities  based  on  their  shared 
aspects  to  give  a  comparative  opinion.  that  is,  it  expresses  a  preference 
order  of  the  entities  using  comparison.  since  most  comparative  sentences 
compare  two  sets  of  entities,  the  analysis  of  an  opinionated  comparative 
sentence means to identify the preferred entity set. however, for application 
purposes, one may assign positive opinions to the aspects of the entities in 
the preferred set, and negative opinions to the aspects of the entities in the 
not  preferred  set.  note  that  like  regular  sentences,  it  is  still  meaningful  to 
classify  whether  a  comparative  sentence  expresses  an  opinion  or  not,  but 
little research has been done on such classification. below we only describe 
a method for identifying the preferred entity set.  

in  (ding,  liu  and  zhang,  2009)  and 

the  method,  proposed 
in 
(ganapathibhotla  and  liu,  2008),  basically  extends  the  lexicon-based 
approach  to  aspect  based  sentiment  classification  of  regular  opinions  to 
comparative  opinions.  it  thus  needs  a  sentiment  lexicon  for  comparative 
opinions.  similar  to  opinion  words  of  the  base  type,  we  can  divide 
comparative opinion words into two categories:   

1.  general-purpose 

comparative 

sentiment  words:  for  type  1 
comparatives, this category includes words like better, worse, etc., which 
often  have  domain  independent  positive  or  negative  sentiments.  in 
sentences involving such words, it is often easy to determine which entity 
set is preferred. in the case of type 2 comparatives, formed by adding 
more, less, most, or least before adjectives/adverbs, the preferred entity 
sets are determined by both words. the following rules are applied:  

comparative negative 
   
comparative positive  
 

::=  increasing_comparative n 
  |  decreasing_comparative p   
::=  increasing_comparative p  
  |  decreasing_comparative n 

 

115 

 

  id31 and opinion mining 

  here, p (respectively n) denotes a positive (negative) sentiment word or 
phrase of the base type. the first rule above says that the combination of 
an  increasing  comparative  (e.g.,  more)  and  a  negative  sentiment  word 
(e.g.,  awful)  implies  a  negative  comparative  opinion  (on  the  left).  the 
other  rules  have  similar  meanings.  note  that  the  above  four  rules  have 
already been discussed as basic rules of opinions in section 5.2.  

2.  context-dependent comparative sentiment words: in the case of type 1 
comparatives,  such  words  include  higher,  lower,  etc.  for  example, 
   nokia phones have longer battery life than motorola phones    carries a 
comparative positive sentiment about    nokia phones    and a comparative 
negative  sentiment  about     motorola  phones,     i.e.,     nokia  phones     are 
preferred  with  respect  to  the  battery  life  aspect.  however,  without 
domain  knowledge  it  is  hard  to  know  whether     longer     is  positive  or 
negative for battery life. this issue is the same as for regular opinions, 
and  this  case  has  also  been  included  in  the  basic  rules  of  opinions  in 
section 5.2. here,    battery life    is a positive potential item (ppi).  

in the case of type 2 comparatives, the situation is similar. however, in 
this  case  the  comparative  word  (more,  most,  less  or  least),  the 
adjective/adverb,  and  the  aspect  are  all  important  in  determining  the 
preference. if we know whether the comparative word is an increasing or 
decreasing comparative (which is easy since there are only four of them), 
then the opinion can be determined by applying the four rules in (1).  

as  discussed  in  section  6.2,  the  pair  (aspect,  context_sentiment_word) 
forms  an  opinion  context.  to  determine  whether  a  pair  is  positive  or 
negative, the algorithm in (ganapathibhotla and liu, 2008) uses a large 
amount  of  external  data.  it  employed  a  large  corpus  of  pros  and  cons 
from product reviews. the idea is to determine whether the aspect and 
context_sentiment_word are more associated with each other in pros or in 
cons.  if  they  are  more  associated  in  pros,  context_sentiment_word  is 
most likely to be positive. otherwise, it is likely to be negative. however, 
since  pros  and  cons  seldom  use  comparative  opinions,  the  context 
opinion words in a comparative sentence have to be converted to its base 
form,  which  can  be  done  using  id138  with  the  help  of  english 
comparative  formation  rules.  this  conversion  is  useful  because  of  the 
following observation.  

observation:  if  an  adjective  or  adverb  of  the  base  form  is  positive  (or 
negative), then its comparative or superlative form is also positive (or 
negative), e.g., good, better, and best.  

after  the  conversion,  these  words  are  manually  categorized  into 
increasing and  decreasing comparatives.  for  context dependent opinion 

116 
 

id31 and opinion mining 

words, comparative words can also be converted to their base forms.  

their  orientations  are 

the  sentiment  words  and 

after 
identified, 
determining  which  entity  set  is  preferred  is  fairly  simple.  without 
negation,  if  the  comparative  is  positive  (or  negative),  then  the  entities 
before (or after) than is preferred. otherwise, the entities after (or before) 
than  are  preferred.  additional  details  can  be  found  in  (ding,  liu  and 
zhang, 2009; ganapathibhotla and liu, 2008).  

8.4 

summary  

although there have been some existing works, comparative sentences have 
not been studied as extensively as many other topics of id31. 
further  research  is  still  needed.  one  of  the  difficult  problems  is  how  to 
identify many types of non-standard or implicit comparative sentences, e.g., 
   i  am  very  happy  that  my  iphone  is  nothing  like  my  old  ugly  droid.    
without identifying them, further id31 is hard to perform.   

apart  from  identifying  comparative  sentences  and  their  types,  several 
researchers have also studied the extraction of compared entities, compared 
aspects,  and  comparative  words.  jindal  and  liu  (2006b)  used  label 
sequential  rule  mining,  which  is  a  supervised  learning  method  based  on 
sequential patterns. yang and ko (2011) applied the maximum id178 and 
id166  learning  algorithms  to  extract  compared  entities  and  comparative 
predicates,  which  are  aspects  that  are  compared.  as  noted  in  section  8.2, 
sequential patterns in (li et al., 2010) for identifying comparative questions 
can already identify compared entities. however, their work is limited in the 
sense that it only works with simple comparative questions. in (fiszman et 
al., 2007), the authors studied the problem of identifying which entity has 
more  of  certain  aspects  in  comparative  sentences  in  biomedical  texts,  but 
they did not analyze opinions in comparisons. 

 

117 

 

  id31 and opinion mining 

chapter 9 

opinion search and retrieval  

 
as web search has proven to be a valuable service on the web, it is not hard 
to imagine that opinion search will also be of great use. two typical kinds of 
opinion search queries are:  

1.  find public opinions about a particular entity or an aspect of the entity, 
e.g., find customer opinions about a digital camera or the picture quality 
of  the  camera,  and  find  public  opinions  about  a  political  issue  or 
candidate.  

2.  find opinions of a person or organization (i.e., opinion holder) about a 
particular entity or an aspect of the entity (or topic), e.g., find barack 
obama   s  opinion  about  abortion.  this  type  of  search  is  particularly 
relevant  to  news  articles,  where  individuals  or  organizations  who 
express opinions are explicitly stated.  

for the first type of queries, the user may simply give the name of the entity 
or  the  name  of  the  aspect  together  with  the  name  of  the  entity.  for  the 
second type of queries, the user may give the name of the opinion holder and 
the name of the entity or topic.  

9.1  web search vs. opinion search  

similar to traditional web search, opinion search also has two major tasks: 
1) retrieve relevant documents/sentences to the user query and 2) rank the 
retrieved documents or sentences. however, there are also major differences. 
on retrieval, opinion search needs to perform two sub-tasks:  

1.  find documents or sentences that are relevant to the query. this is the 

only task performed in the traditional web search or retrieval.  

2.  determine whether the documents or sentences express opinions on the 
query topic (entity and/or aspect) and whether the opinions are positive 
or  negative.  this  is  the  task  of  sentiment  analysis.  traditional  search 
does not perform this sub-task.  

as  for  ranking,  traditional  web  search  engines  rank  web  pages  based  on 
authority and relevance scores (liu, 2006 and 2011). the basic premise is 
that  the  top  ranked  pages  (ideally  the  first  page)  contain  sufficient 
information to satisfy the user   s information need. this paradigm is adequate 

118 
 

id31 and opinion mining 

for factual information search because one fact equals to any number of the 
same fact. that is, if the first page contains the required information, there is 
no  need  to  see  the  rest  of  the  relevant  pages.  for  opinion  search,  this 
paradigm  is  fine  only  for  the  second  type  of  queries  because  the  opinion 
holder usually has only one opinion about a particular entity or topic, and the 
opinion  is  contained  in  a  single  document  or  page.  however,  for  the  first 
type of opinion queries, this paradigm needs to be modified because ranking 
in opinion search has two objectives. first, it needs to rank those opinionated 
documents or sentences with high utilities or information contents at the top 
(see  chapter  11).  second,  it  needs  to  reflect  the  natural  distribution  of 
positive and negative opinions. this second objective is important because 
in most applications the actual proportions of positive and negative opinions 
are critical pieces of information. only reading the top ranked result as in the 
traditional  search  is problematic  because  the  top  result  only  represents  the 
opinion of a single opinion holder. thus, ranking in opinion search needs to 
capture  the  natural  distribution  of  positive  and  negative  sentiments  of  the 
whole population. one simple solution for this is to produce two rankings, 
one for positive opinions and one for negative opinions, and also to display 
the numbers of positive and negative opinions.   

providing  an  aspect-based  summary  for  each  opinion  search  will  be  even 
better.  however,  it  is  an  extremely  challenging  problem  because  aspect 
extraction, aspect categorization, and associating entities to its aspects are all 
very  challenging  problems.  without  effective  solutions  for  them,  such  a 
summary will not be possible.  

9.2 

existing opinion retrieval 
techniques  

current research in opinion retrieval typically treats the task as a two-stage 
process. in the first stage, documents are ranked by topical relevance only. 
in  the  second  stage,  candidate  relevant  documents  are  re-ranked  by  their 
opinion  scores.  the  opinion  scores  can  be  acquired  by  either  a  machine 
learning  based  sentiment  classifier,  such  as  id166,  or  a  lexicon-based 
sentiment  classifier  using  a  sentiment  lexicon  and  a  combination  of 
sentiment  word  scores  and  query  term   sentiment  word  proximity  scores. 
more  advanced  research  models  topic  relevance  and  opinion  at  the  same 
time, and produces rankings based on their integrated score.  

to give a flavor of opinion search, we present an example system (zhang 
and yu, 2007), which was the winner of the blog track in the 2007 trec 

 

119 

 

  id31 and opinion mining 

evaluation  (http://trec.nist.gov/).  the  task  was  exactly  opinion  search  (or 
retrieval).  this  system  has  two  components.  the  first  component  is  for 
retrieving relevant documents for each query. the second component is for 
classifying the retrieved documents as being opinionated or not-opinionated. 
the opinionated documents are further classified into positive, negative, or 
mixed (containing both positive and negative opinions).  

retrieval component: this component performs the traditional information 
retrieval  (ir) task.  it  considers  both keywords  and concepts.  concepts  are 
named entities (e.g., names of people or organizations) or various types of 
phrases  from  dictionaries  and  other  sources  (e.g.,  wikipedia  entries).  the 
strategy for processing a user query is as follows (zhang et al., 2008; zhang 
and yu, 2007): it first recognizes and disambiguates the concepts within the 
user query. it then broadens the search query with its synonyms. after that, 
it recognizes concepts in the retrieved documents and also performs pseudo-
feedback  to  automatically  extract  relevant  words  from  the  top-ranked 
documents  to  expand  the  query.  finally,  it  computes  a  similarity  (or 
relevance  score)  of  each  document  with  the  expanded  query  using  both 
concepts and keywords.  

opinion classification component: this component performs two tasks: (1) 
classifying each document into one of the two categories, opinionated and 
not-opinionated,  and  (2)  classifying  each  opinionated  document  as 
expressing a positive, negative, or mixed opinion. for both tasks, the system 
uses  supervised  learning.  for  the  first  task,  it  obtains  a  large  amount  of 
opinionated (subjective) training data from review sites such as rateitall.com 
and  epinions.com.  the  data  are  also  collected  from  different  domains 
involving consumer goods and services as well as government policies and 
political  viewpoints.  the  not-opinionated  training  data  are  obtained  from 
sites that give objective information such as wikipedia. from these training 
data, a id166 classifier is constructed.  

this  classifier  is  then  applied  to  each  retrieved  document  as  follows.  the 
document  is  first  partitioned  into  sentences.  the  id166  classifier  then 
classifies each sentence as opinionated or not-opinionated. if a sentence is 
classified  to  be  opinionated,  its  strength,  as  determined  by  id166,  is  also 
noted. a document is regarded opinionated if there is at least one sentence 
that is classified as opinionated. to ensure that the opinion of the sentence is 
directed  at  the  query  topic,  the  system  requires  that  enough  query 
concepts/words  are  found  in  its  vicinity.  the  totality  of  the  opinionated 
sentences  and  their  strengths  in  a  document  together  with  the  document   s 
similarity with the query is used to rank the document.  

120 
 

id31 and opinion mining 

to  determine  whether  an  opinionated  document  expresses  a  positive, 
negative  or  mixed opinion, a  second  classifier  is constructed.  the  training 
data  are  reviews  from  review  sites  containing  review  ratings  (e.g., 
rateitall.com). a low rating indicates a negative opinion while a high rating 
indicates a positive opinion. using positive and negative reviews as training 
data, a sentiment classifier is built to classify each document as expressing a 
positive, negative, or mixed opinion.  

there  are  also  other  approaches to  opinion  retrieval  in  trec  evaluations. 
the  readers  are  encouraged  to  read  the  papers  at  the  trec  web  site 
(http://trec.nist.gov/).  for a  summary  of  trec evaluations, please refer to 
the  overview  paper  of  2006  trec  blog  track  (ounis  et  al.,  2006),  the 
overview paper of 2007 trec blog track (macdonald, ounis and soboroff, 
2007), and the overview paper of 2008 trec blog track (ounis, macdonald 
and soboroff, 2008). below, we discuss research published in other forums.  

in (eguchi and lavrenko, 2006), eguchi and lavrenko proposed a sentiment 
retrieval  technique  based  on  generative  language  modeling.  in  their 
approach,  the  user  needs  to  provide  a  set  of  query  terms  representing  a 
particular topic of interest, and also sentiment polarity (orientation) interest, 
which is represented either as a set of seed sentiment words or a particular 
sentiment orientation (positive or negative). one main advance of their work 
is  that  they  combined  sentiment  relevance  models  and  topic  relevance 
models with model parameters estimated from the training data, considering 
the  topic  dependence  of  the  sentiment.  they  showed  that  the  explicit 
modeling  of  dependency  between  topic  and  sentiment  produced  better 
retrieval  results  than  treating  them  independently.  a  similar  approach  was 
also proposed by huang and croft (2009), which scored the relevance of a 
document  using  a  topic  reliance  model  and  an  opinion  relevance  model. 
both these works took a linear combination of topic relevance and sentiment 
relevance for final ranking. in (zhang and ye, 2008), the authors used the 
product of the two relevance scores. the relevance formulation is also based 
on id38.  

in  (na  et  al.,  2009),  a  lexicon-based  approach  was  proposed  for  opinion 
retrieval.  they  also  attempted  to  deal  with  the  domain  dependent  lexicon 
construction issue. a relevant feedback style learning for generating query-
specific  sentiment  lexicon  was  proposed,  which  made  use  of  a  set  of  top-
ranked documents in response to a query.  

liu, li and liu (2009) explored various lexical and sentiment features and 
different  learning  algorithms  for  identifying  opinionated  blogs.  they  also 
presented results for the strategy that combines both the opinion analysis and 
the retrieval components for retrieving relevant and opinionated blogs.  

 

121 

 

  id31 and opinion mining 

li et al. (2010) took a different approach. their algorithm first finds topic 
and sentiment word pairs from each sentence of a document, and then builds 
a bipartite graph to link such pairs with the documents that contain the pairs. 
the graph based ranking algorithm hits (kleinberg, 1999) was applied to 
rank  the  documents,  where  documents  were  considered  as  authorities  and 
pairs were considered as hubs. each link connecting a pair and a document 
is weighted based on the contribution of the pair to the document.  

in (pang and lee, 2008), a simple method was proposed for review search. it 
only re-ranks the top k topic-based search results by using an idiosyncrasy 
measure  defined  on  the  rarity  of  terms  appeared  in  the  initial  search 
results.  the  rationale  for  the  measure  was  explained  in  the  paper.  the 
assumption was that the search engine has already found good results and 
only  re-ranking  is  needed  to  put  reviews  at  the  top.  the  method  is 
unsupervised and does not use any pre-existing lexicon.  

9.3 

summary 

it will be really useful if a web search engine such as google or microsoft 
bing  can  provide  a  general  opinion  search  service.  although  both  google 
and  microsoft  bing  already  provide  opinion  summarization  services  for 
reviews of some products, their coverage is still very limited. for those not 
covered  entities  and  topics,  it  is  not  easy  to  find  opinions  about  them 
because their opinions are scattered all over the internet. there are also some 
large  and  well  known  review  hosting  sites  such  as  amazon.com  and 
yelp.com.  however,  they  do  not  cover  all  entities  and  topics  either.  for 
those not covered entities or topics, finding opinions about them remains to 
be  a  formidable  task  because  of  the  proliferation  of  diverse  sites  and  the 
difficulty of identifying relevant opinions. a lot of research is still needed 
before a breakthrough can be achieved.  

122 
 

id31 and opinion mining 

chapter 10 

opinion spam detection  

opinions  from  social  media  are  increasingly  used  by  individuals  and 
organizations for making purchase decisions and making choices at elections 
and for marketing and product design. positive opinions often mean profits 
and fames for businesses and individuals, which, unfortunately, give strong 
incentives  for  people  to  game  the  system  by  posting  fake  opinions  or 
reviews  to  promote  or  to  discredit  some  target  products,  services, 
organizations,  individuals,  and  even  ideas  without  disclosing  their  true 
intentions, or the person  or  organization that they are secretly working 
for. such individuals are called opinion spammers and their activities are 
called opinion spamming (jindal and liu, 2008; jindal and liu, 2007). 
opinion spamming about social and political issues can even be frightening 
as  they  can  warp  opinions  and  mobilize  masses  into  positions  counter  to 
legal or ethical mores. it is safe to say that as opinions in social media are 
increasingly used in practice, opinion spamming will become more and more 
rampant and also sophisticated, which presents a major challenge for their 
detection. however, they must be detected in order to ensure that the social 
media continues to be a trusted source of public opinions, rather than being 
full of fake opinions, lies, and deceptions.  

spam detection in general has been studied in many fields. web spam and 
email spam are the two most widely studied types of spam. opinion spam is, 
however, very different. there are two main types of web spam, i.e., link 
spam and content spam (castillo and davison, 2010; liu, 2006 and 2011). 
link spam is spam on hyperlinks, which hardly exist in reviews. although 
advertising  links  are  common  in  other  forms  of  social  media,  they  are 
relatively easy to detect. content spam adds popular (but irrelevant) words 
in target web pages in order to fool search engines to make them relevant to 
many search queries, but this hardly occurs in opinion postings. email spam 
refers to unsolicited advertisements, which are also rare in online opinions.  

challenge: the key challenge of opinion spam detection is that unlike other 
forms  of  spam,  it  is  very  hard,  if  not  impossible,  to  recognize  fake 
opinions  by  manually  reading  them,  which  makes  it  difficult  to  find 
opinion spam data to help design and evaluate detection algorithms. for 
other forms of spam, one can recognize them fairly easily.  

in fact, in the extreme case, it is logically impossible to recognize spam by 
simply reading it. for example, one can write a truthful review for a good 

 

123 

 

  id31 and opinion mining 

restaurant  and  post  it  as  a  fake  review  for  a  bad  restaurant  in  order  to 
promote it. there is no way to detect this fake review without considering 
information  beyond  the  review  text  itself  simply  because  the  same  review 
cannot be both truthful and fake at the same time. 

this  chapter  uses  consumer  reviews  as  an  example  to  study  the  problem. 
little research has been done in the context of other forms of social media. 

10.1  types of spam and spamming 

three types of spam reviews were identified in (jindal and liu, 2008):  

type  1  (fake  reviews):  these  are  untruthful  reviews  that  are  written  not 
based  on  the  reviewers     genuine  experiences  of  using  the  products  or 
services,  but  are  written  with  hidden  motives.  they  often  contain 
undeserving  positive  opinions  about  some  target  entities  (products  or 
services) in order to promote the entities and/or unjust or false negative 
opinions about some other entities in order to damage their reputations.  

type 2 (reviews about brands only): these reviews do not comment on the 
specific products or services that they are supposed to review, but only 
comment on the brands or the manufacturers of the products. although 
they may be genuine, they are considered as spam as they are not targeted 
at the specific products and are often biased. for example, a review for a 
specific hp printer says    i hate hp. i never buy any of their products   .  
type  3  (non-reviews):  these  are  not  reviews.  there  are  two  main  sub-
types:  (1)  advertisements  and  (2)  other  irrelevant  texts  containing  no 
opinions (e.g., questions, answers, and random texts). strictly speaking, 
they are not opinion spam as they do not give user opinions.  

it has been shown in (jindal and liu, 2008) that types 2 and 3 spam reviews 
are rare and relatively easy to detect using supervised learning. even if they 
are not detected, it is not a major problem because human readers can easily 
spot them during reading. this chapter thus focuses on type 1, fake reviews.  

fake  reviews  can  be  seen  as  a  special  form  of  deception  (hancock  et  al., 
2007; mihalcea and strapparava, 2009; newman et al., 2003; pennebaker et 
al.,  2007;  vrij,  2008;  zhou,  shi  and  zhang,  2008).  however,  traditional 
deceptions usually refer to lies about some facts or a person   s true feeling. 
researchers  have  identified  many  deception  signals  in  text.  for  example, 
studies have shown that when people lie they tend to detach themselves and 
like  to  use  words  such  as  you,  she,  he,  they,  rather  than  i,  myself,  mine,  etc. 
liars also use words related to certainty more frequently to hide    fake    or to 
emphasize     truth   .  fake  reviews  are  different  from  lies  in  many  aspects. 

124 
 

id31 and opinion mining 

table 10.1. fake reviews vs. product quality 
 

positive fake review  negative fake review 

good quality product 
average quality product 

bad quality product 

1 
3 
5 

2 
4 
6 

 
first, fake reviewers actually like to use i, myself, mine, etc., to give readers 
the impression that their reviews express their true experiences. second, fake 
reviews  are  not  necessarily  the  traditional  lies.  for  example,  one  wrote  a 
book and pretended to be a reader and wrote a review to promote the book. 
the review might be the true feeling of the author. furthermore, many fake 
reviewers might have never used the reviewed products/services, but simply 
tried to give positive or negative opinions about something that they do not 
know. they are not lying about any facts they know or their true feelings.  

10.1.1  harmful fake reviews 

not all fake reviews are equally harmful. table 10.1 gives a conceptual view 
of different kinds of fake reviews. here we assume we know the true quality 
of  a  product.  the  objective  of  fake  reviews  in  regions  1,  3  and  5  is  to 
promote the product. although opinions expressed in region 1 may be true, 
the  reviewers  do  not  disclose  their  conflict  of  interests  or  hidden  motives. 
the goal of fake reviews in regions 2, 4, and 6 is to damage the reputation of 
the product. although opinions in the reviews of region 6 may be true, the 
reviewers have malicious intensions. clearly, fake reviews in regions 1 and 
6 are not very damaging, but fake reviews in regions 2, 3, 4, and 5 are very 
harmful. thus, fake review detection algorithms should focus on identifying 
reviews  in  these  regions.  some  of  the  existing  detection  algorithms  are 
already  using  this  idea  by  employing  different  types  of  rating  deviation 
features. note that the good, bad, and average quality may be defined based 
on the average rating of the reviews given to the product. however, this can 
be invalid if there are many spammers or there are too few reviews. 

10.1.2 

individual and group spamming 

fake  reviews  may  be  written  by  many  types  of  people,  e.g.,  friends  and 
family,  company  employees,  competitors,  businesses  that  provide  fake 
review writing services, and even genuine customers  (some businesses give 
discounts and even full refunds to some of their customers on the condition 
that the customers write positive reviews for them). in other forms of social 

 

125 

 

  id31 and opinion mining 

media,  public  or  private  agencies  and  political  organizations  may  employ 
people to post messages to secretly influence social media conversations and 
to spread lies and disinformation.  

in general, a spammer may work individually, or knowingly or unknowingly 
work as a member of a group (these activities are often highly secretive).  

individual spammers: in this case, a spammer does not work with anyone. 
he/she just writes fake reviews him/herself using a single user-id, e.g., the 
author of a book.  

group  spammers:  there  are  two  main  sub-cases  (mukherjee,  liu  and 

glance, 2012; mukherjee et al., 2011).  
     a group of spammers (persons) works in collusion to promote a target 
entity  and/or  to  damage  the  reputation  of  another.  the  individual 
spammers in the group may or may not know each other.  

     a single person registers multiple user-ids and spam using these user-
ids. these multiple user-ids behave just like a group in collusion. this 
case is often called sock puppetting.  

group spamming is highly damaging because due to the sheer number of 
members in a group, it can take total control of the sentiment on a product 
and completely mislead potential customers, especially at the beginning of 
a  product  launch.  although  group  spammers  can  also  be  seen  as  many 
individual  spammers,  group  spamming  has  some  special  characteristics 
which can give them away as we will see in section 10.4.  

we should also note that a spammer may work individually sometimes and 
as a member of a group some other times. a spammer may also be a genuine 
reviewer sometimes because he/she also purchases products as a consumer 
and  may  write  reviews  about  them  based  on  his/her  true  experiences.  all 
these complicated situations make opinion spamming very difficult to detect.  

10.1.3  types of data, features and detection 

three main types of data have been used for review spam detection:  

review content: the actual text content of each review. from the content, 
we  can  extract  linguistic  features  such  as  word  and  pos  id165s  and 
other  syntactic  and  semantic  clues  for  deceptions  and  lies.  however, 
linguistic features may not be enough because one can fairly easily craft a 
fake review that is just like a genuine one. for example, one can write a 
fake positive review for a bad restaurant based on his true experience in a 
good restaurant.   

meta-data about the review: the data such as the star rating given to each 

126 
 

id31 and opinion mining 

review, user-id of the reviewer, the time when the review was posted, the 
time taken to write the review, the host ip address and mac address of 
the  reviewer   s  computer,  the  geo-location  of  the  reviewer,  and  the 
sequence of clicks at the review site. from such data, we can mine many 
types  of  abnormal  behavioral  patterns  of  reviewers  and  their  reviews. 
for  example,  from  review  ratings,  we  may  find  that  a  reviewer  wrote 
only  positive  reviews  for  a  brand  and  only  negative  reviews  for  a 
competing brand. along a similar line, if multiple user-ids from the same 
computer  posted  a  number  of  positive  reviews  about  a  product,  these 
reviews  are  suspicious.  also,  if  the  positive  reviews  for  a  hotel  are  all 
from the nearby area of the hotel, they are clearly not trustworthy.  

product information: information about the entity being reviewed, e.g., the 
product description and sales volume/rank. for example, a product is not 
selling well but has many positive reviews, which is hard to believe.  

these types of data have been used to produce many spam features. one can 
also classify the data into public data and site private data. by public data, 
we mean the data displayed on the review pages of the hosting site, e.g., the 
review  content,  the  reviewer   s  user-id  and  the  time  when  the  review  was 
posted.  by  private  data,  we  mean  the  data  that  the  site  collects  but  is  not 
displayed on their review pages for public viewing, e.g., the ip address and 
mac address from the reviewer   s computer, and the cookie information.  

opinion spam detection: the ultimate goal of opinion spam detection in 
the review context is to identify every fake review, fake reviewer, and fake 
reviewer group. the three concepts are clearly related as fake reviews are 
written  by  fake  reviewers  and  fake  reviewers  can  form  fake  reviewer 
groups.  the  detection  of  one  type  can  help  the  detection  of  others. 
however, each of them also has its own special characteristics, which can 
be exploited for detection.  

in the next two sections, we focus on detecting individual fake reviews and 
reviewers, and in section 10.4 we discuss the detection of spammer groups.   

10.2  supervised spam detection  

in  general,  opinion  spam  detection  can  be  formulated  as  a  classification 
problem  with  two  classes,  fake  and  non-fake.  supervised  learning  is 
naturally applicable. however, as we described above, a key difficulty is that 
it  is  very  hard,  if  not  impossible,  to  recognize  fake  reviews  reliably  by 
manually reading them because a spammer can carefully craft a fake review 
that  is  just  like  any  innocent  review  (jindal  and  liu,  2008).  due  to  this 
difficulty, there is no reliable fake review and non-fake review data available 

 

127 

 

  id31 and opinion mining 

to  train  a  machine  learning  algorithm  to  recognize  fake  reviews.  despite 
these  difficulties,  several  detection  algorithms  have  been  proposed  and 
evaluated in various ways. this section discusses three supervised learning 
methods. the next section describes some unsupervised methods.  

due to the fact that there is no labeled training data for learning, jindal and 
liu (2008) exploited duplicate reviews. in their study of 5.8 million reviews 
and 2.14 million reviewers from amazon.com, a large number of duplicate 
and  near-duplicate  reviews  were  found,  which  indicated  that  review  spam 
was widespread. since writing new reviews can be taxing, many spammers 
use  the  same  reviews  or  slightly  revised  reviews  for  different  products. 
these duplicates and near-duplicates can be divided into four categories: 

1.  duplicates from the same user-id on the same product  
2.  duplicates from different user-ids on the same product 
3.  duplicates from the same user-id on different products 
4.  duplicates from different user-ids on different products  

the  first  type  of  duplicates  can  be  the  results  of  reviewers  mistakenly 
clicking  the  review  submit  button  multiple  times  (which  can  be  easily 
checked  based  on  the  submission  dates).  however,  the  last  three  types  of 
duplicates are very likely to be fake. thus the last three types of duplicates 
were used as fake reviews and the rest of the reviews as non-fake reviews in 
the training data for machine learning. three sets of features were employed:  

review  centric  features:  these  are  features  about  each  review.  example 
features include the actual words and id165s of the review, the number 
of times that brand names are mentioned, the percent of opinion words, 
the review length, and the number of helpful feedbacks. in many review 
sites (e.g., amazon.com), the readers can provide feedback to each review 
by answering a question like    do you find this review helpful?    

reviewer centric features: these are features about each reviewer. example 
features include the average rating given by the reviewer, the mean and 
the standard deviation in rating, the ratio of the number of reviews that 
this reviewer wrote which were the first reviews of products to the total 
number of reviews that he/she has written, and the ratio of the number of 
cases in which he/she was the only reviewer. 

product  centric  features:  these  features  are  about  each  product.  example 
features  include  the  price  of  the  product,  the  sales  rank  of  the  product 
(amazon.com assigns a sales rank to each product according to its sales 
volume),  the  mean  and  the  standard  deviation  of  review  ratings  of  the 
product. 

logistic  regression  was  used  for  model  building.  experimental  results 
showed some tentative but interesting results.  

128 
 

id31 and opinion mining 

     negative  outlier  reviews  (ratings  with  significant  negative  deviations 
from  the  average  rating  of  a  product)  tend  to  be  heavily  spammed. 
positive outlier reviews are not badly spammed.   

     reviews that are the only reviews of some products are likely to be fake. 
this can be explained by the tendency of a seller promoting an unpopular 
product by writing a fake review.  

     top-ranked reviewers are more likely to be fake reviewers. amazon.com 
gives a rank to each reviewer based on its proprietary method. analysis 
showed  that  top-ranked  reviewers  generally  wrote  a  large  number  of 
reviews.  people  who  wrote  a  large  number  of  reviews  are  natural 
suspects. some top reviewers wrote thousands or even tens of thousands 
of reviews, which is unlikely for an ordinary consumer.  

     fake reviews can get good feedbacks and genuine reviews can get bad 
feedbacks. this shows that if the quality of a review is defined based on 
helpfulness  feedbacks,  people  can  be  fooled  by  fake  reviews  because 
spammers  can  easily  craft  a  sophisticated  review  that  can  get  many 
positive feedbacks.  

     products  of  lower  sales  ranks  are  more  likely  to  be  spammed.  this 
indicates that spam activities seem to be limited to low selling products, 
which is intuitive as it is difficult to damage the reputation of a popular 
product, and an unpopular product needs some promotion.  

it should be stressed again that these results are tentative because (1) it is not 
confirmed that the three types of duplicates are definitely fake reviews, and 
(2)  many  fake  reviews  are  not  duplicates  and  they  are  considered  as  non-
fake reviews in model building in (jindal and liu, 2008).  

in (li et al., 2011), another supervised learning approach was attempted to 
identify fake reviews. in their case, a manually labeled fake review corpus 
was built from epinions reviews. in epinions, after a review is posted, users 
can evaluate the review by giving it a helpfulness score. they can also write 
comments about the reviews. the authors manually labeled a set of fake or 
non-fake  reviews  by  reading  the  reviews  and  the  comments.  for  learning, 
several types of features were proposed, which are similar to those in (jindal 
and liu, 2008) with some additions, e.g., subjective and objectivity features, 
positive and negative features, reviewer   s profile, authority score computed 
using  id95  (page  et  al.,  1999),  etc.  for  learning,  they  used  na  ve 
bayesian  classification  which  gave  promising  results.  the  authors  also 
experimented  with  a  semi-supervised  learning  method  exploiting  the  idea 
that a spammer tends to write many fake reviews.  

in (ott et al., 2011),  supervised learning was also employed. in this case, the 
authors used amazon mechanical turk to crowdsource fake hotel reviews of 

 

129 

 

  id31 and opinion mining 

identification,  psycholinguistic  deception  detection,  and 

20  hotels.  several  provisions  were  made  to  ensure  the  quality  of  the  fake 
reviews.  for  example,  they  only  allowed  each  turker  to  make  a  single 
submission, turkers must be in the united states, etc. the turkers were 
also given the  scenario  that they  worked in the hotels and their bosses 
asked them to write fake reviews to promote the hotels. truthful reviews 
were  obtained  from  the  tripadvisor  web  site.  the  authors  tried  several 
classification  approaches  which  have  been  used  in  related  tasks  such  as 
genre 
text 
classification.  all  these  tasks  have  some  existing  features  proposed  by 
researchers. their experiments showed that text classification performed the 
best using only unigram and bigrams based on the 50/50 fake and non-fake 
class distribution. traditional features for deceptions (hancock et al., 2007; 
mihalcea  and  strapparava,  2009;  newman  et  al.,  2003;  pennebaker  et  al., 
2007;  vrij,  2008;  zhou,  shi  and  zhang,  2008)  did  not  do  well.  however, 
like the previous studies, the evaluation data used here is also not perfect. 
the  fake  reviews  from  amazon  mechanical  turk  may  not  be  true     fake 
reviews     as  the  turkers  do  not  know  the  hotels  being  reviewed  although 
they  were  asked  to  pretend  that  they  worked  for  the  hotels.  furthermore, 
using  50/50  fake  and  non-fake  data  for  testing  may  not  reflect  the  true 
distribution  of  the  real-life  situation.  the  class  distribution  can  have  a 
significant impact on the precision of the detected fake reviews.  

10.3  unsupervised spam detection  

due to the difficulty of manually labeling of training data, using supervised 
learning  alone  for  fake  review  detection  is  difficult.  in  this  section,  we 
discuss  two  unsupervised  approaches.  techniques  similar  to  these  are 
already in use in many review hosting sites.  

10.3.1  spam detection based on atypical behaviors  

this  sub-section  describes  some  techniques  that  try  to  discover  atypical 
behaviors  of  reviewers  for  spammer  detection.  for  example,  if  a  reviewer 
wrote all negative reviews for a brand but other reviewers were all positive 
about the brand, and wrote all positive reviews for a competing brand, then 
this reviewer is naturally suspicious. 

the  first  technique  is  from  (lim  et  al.,  2010),  which  identified  several 
unusual  reviewer  behavior  models  based  on  different  review  patterns  that 
suggest spamming. each model assigns a numeric spamming behavior score 

130 
 

id31 and opinion mining 

to  a  reviewer  by  measuring  the  extent  to  which  the  reviewer  practices 
spamming behavior of the type. all the scores are then combined to produce 
the final spam score. thus, this method focuses on finding spammers or fake 
reviewers rather than fake reviews. the spamming behavior models are:   
(a)  targeting products: to game a review system, it is hypothesized that a 
spammer will direct most of his efforts on promoting or victimizing a 
few target products. he is expected to monitor the products closely and 
mitigate the ratings by writing fake reviews when time is appropriate. 

(b)  targeting  groups:  this  spam  behavior  model  defines  the  pattern  of 
spammers  manipulating  ratings  of  a  set  of  products  sharing  some 
attribute(s)  within  a  short  span of  time.  for  example, a  spammer  may 
target  several  products  of  a  brand  within  a  few  hours.  this  pattern  of 
ratings saves the spammers    time as they do not need to log on to the 
review  system  many  times.  to  achieve  maximum  impact,  the  ratings 
given to these target groups of products are either very high or very low. 
(c)  general  rating  deviation:  a  genuine  reviewer  is  expected  to  give 
ratings similar to other raters of the same product. as spammers attempt 
to  promote  or  demote  some  products,  their  ratings  typically  deviate  a 
great deal from those of other reviewers.   

(d)  early  rating  deviation:  early  deviation  captures  the  behavior  of  a 
spammer  contributing  a  fake  review  soon  after  product  launch.  such 
reviews  are  likely  to  attract  attention  from  other  reviewers,  allowing 
spammers to affect the views of subsequent reviewers. 

the second technique also focused on finding fake reviewers or spammers 
(jindal,  liu  and  lim,  2010).  here  the  problem  was  formulated  as  a  data 
mining  task  of  discovering  unexpected  class  association  rules.  unlike 
conventional spam  detection  approaches such  as the  above  supervised and 
unsupervised  methods,  which  first  manually  identify  some  heuristic  spam 
features and then use them for spam detection. this technique is generic and 
can be applied to solve a class of problems due to its domain independence.  

class association rules are a special type of association rules (liu, hsu and 
ma, 1998) with a fixed class attribute. the data for mining class association 
rules (cars) consists of a set of data records, which are described by a set 
of normal attributes a = {a1,          , an}, and a class attribute c = {c1,          , cm} of 
m discrete values, called class labels. a car rule is of the form: x     ci, 
where x is a set of conditions from the attributes in a and ci is a class label 
in c. such a rule computes the id155 of pr(ci | x) (called 
confidence) and the joint id203 pr(x, ci) (called support).  
for the spammer detection application, the data for car mining is produced 
as  follows:  each  review  forms  a  data  record  with  a  set  of  attributes,  e.g., 

 

131 

 

  id31 and opinion mining 

reviewer-id,  brand-id,  product-id,  and  a  class.  the  class  represents  the 
sentiment of the reviewer on the product, positive, negative, or neutral based 
on the review rating. in most review sites (e.g., amazon.com), each review 
has a rating between 1 (lowest) and 5 (highest) assigned by its reviewer. the 
rating  of  4  or  5  is  assigned  positive,  3  neutral,  and  1  or  2  negative.  a 
discovered car rule could be that a reviewer gives all positive ratings to a 
particular  brand  of  products.  the  method  in  (jindal,  liu  and  lim,  2010) 
finds  four  types  of  unexpected  rules  based  on  four  unexpectedness 
definitions. the unexpected rules represent atypical behaviors of reviewers. 
below,  an  example  behavior  is  given  for  each  type  of  unexpectedness 
definition.  the  unexpectedness  definitions  are  quite  involved  and  can  be 
found in (jindal, liu and lim, 2010).     

     confidence unexpectedness: using this measure, one can find reviewers 
who give all high ratings to products of a brand, but most other reviewers 
are generally negative about the brand.  

     support  unexpectedness:  using  this  measure,  one  can  find  reviewers 
who  write  multiple  reviews  for  a  single  product,  while  other  reviewers 
only write one review.  

     attribute  distribution  unexpectedness:  using  this  measure,  one  can 
find  that  most  positive  reviews  for  a  brand  of  products  are  written  by 
only one  reviewer  although  there  are a  large number  of  reviewers who 
have reviewed the products of the brand.  

     attribute unexpectedness: using this measure, one can find reviewers 
who write only positive reviews to one brand and only negative reviews 
to another brand.  

the advantage of this approach is that all the unexpectedness measures are 
defined on cars rules, and are thus domain independent. the technique can 
thus be used in other domains to find unexpected patterns. the weakness is 
that some atypical behaviors cannot be detected, e.g., time-related behaviors, 
because class association rules do not consider time.  

it is important to note that the behaviors studied in published papers are all 
based on public data displayed on review pages of their respective review 
hosting  sites. as  mentioned  earlier,  review  hosting  sites  also  collect many 
other pieces of data about each reviewer and his/her activities at the sites. 
these  data  are  not  visible  to  the  general  public,  but  can  be  very  useful, 
perhaps  even  more  useful  than  the  public  data,  for  spam  detection.  for 
example, if multiple user-ids from the same ip address posted a number of 
positive reviews about a product, then these user-ids are suspicious. if the 
positive reviews for a hotel are all from the nearby area of the hotel, they are 
also doubtful. some review hosting sites are already using these and other 

132 
 

id31 and opinion mining 

pieces of their internal data to detect fake reviewers and reviews.  

finally,  wu  et  al.  (2010)  also  proposed  an  unsupervised  method  to  detect 
fake reviews based on a distortion criterion (not on reviewers    behaviors as 
the  above  methods).  the  idea  is  that  fake  reviews  will  distort  the  overall 
popularity  ranking  for  a  collection  of  entities.  that  is,  deleting  a  set  of 
reviews  chosen  at  random  should  not  overly  disrupt  the  ranked  list  of 
entities, while deleting fake reviews should significantly alter or distort the 
ranking  of  entities  to  reveal  the     true"  ranking.  this  distortion  can  be 
measured by comparing popularity rankings before and after deletion using 
rank correlation. 

10.3.2  spam detection using review graph  

in  (wang  et  al.,  2011),  a  graph-based  method  was  proposed  for  detecting 
spam  in  store  or  merchant  reviews.  such  reviews  describe  purchase 
experiences and evaluations of stores. this study was based on a snapshot of 
all reviews from resellerratings.com, which were crawled on oct. 6th, 2010. 
after  removing  stores  with  no  reviews,  there  were  343603  reviewers  who 
wrote 408470 reviews about 14561 stores. 

although  one  can  borrow  some  ideas  from  product  review  spammer 
detection,  their  clues  are  insufficient  for  the  store  review  context.  for 
example, it is suspicious for a person to post multiple reviews to the same 
product, but it can be normal for a person to post more than one review to 
the  same  store  due  to  multiple  purchasing  experiences.  also,  it  can  be 
normal to have near-duplicate reviews from one reviewer for multiple stores 
because unlike different products, different stores basically provide the same 
type  of  services.  therefore,  features  or  clues  proposed  in  existing 
approaches  to  detecting  fake  product  reviews  and  reviewers  are  not  all 
appropriate for detecting spammers of store reviews. thus, there is a need to 
look for a more sophisticated and complementary framework.  

this paper used a heterogeneous review graph with three types of nodes, i.e., 
reviewers,  reviews  and  stores,  to  capture  their  relationships  and  to  model 
spamming  clues.  a  reviewer  node  has  a  link  to  each  review  that  he/she 
wrote. a review node has an edge to a store node if the review is about that 
store. a store is connected to a reviewer via this reviewer   s review about the 
store. each node is also attached with a set of features. for example, a store 
node has features about its average rating, its number of reviews, etc. based 
on  the  review  graph,  three  concepts  are  defined  and  computed,  i.e.  the 
trustiness of reviewers, the honesty of reviews, and the reliability of stores. 
a reviewer is more trustworthy if he/she has written more honesty reviews; 

 

133 

 

  id31 and opinion mining 

a  store  is  more  reliable  if  it  has  more  positive  reviews  from  trustworthy 
reviewers;  and  a  review  is  more  honest  if  it  is  supported  by  many  other 
honest reviews. furthermore, if the honesty of a review goes down, it affects 
the reviewer   s trustiness, which has an impact on the store he/she reviewed. 
these  intertwined  relations  are  revealed  in  the  review  graph  and  defined 
mathematically. an iterative computation method was proposed to compute 
the three values, which are then used to rank reviewers, stores and reviews. 
those top ranked reviewers, stores and reviews are likely to be involved in 
review  spamming.  the  evaluation  was  done  using  human  judges  by 
comparing with scores of stores from better business bureaus (bbb), which 
is  a  well-known  corporation  in  usa  that  gathers  reports  on  business 
reliability and alerts the public to business or consumer scams.  

10.4  group spam detection  

an  initial  group  spam  detection  algorithm  was  proposed  in  (mukherjee  et 
al., 2011), which was improved in (mukherjee, liu and glance, 2012). the 
algorithm finds groups of spammers who might have worked in collusion in 
promoting or demoting some target entities. it works in two steps:  

1.  frequent  pattern  mining:  first,  it  pre-processes  the  review  data  to 
produce  a  set  of  transactions.  each  transaction  represents  a  unique 
product and consists of all reviewers (their ids) who have reviewed that 
product. using all the transactions, it performs frequent pattern mining to 
find  a  set  of  frequent  patterns.  each  pattern  is  basically  a  group  of 
reviewers  who  have  all  reviewed  a  set  of  products.  such  a  group  is 
regarded  as  a  candidate  spam  group.  the  reason  for  using  frequent 
pattern mining is as follows: if a group of reviewers who only worked 
together once to promote or to demote a single product, it can be hard to 
detect based on their collective behavior. however, these fake reviewers 
(especially those who get paid to write) cannot be just writing one review 
for  a  single  product  because  they  would  not  make  enough  money  that 
way.  instead,  they  work  on  many  products,  i.e.,  write  many  reviews 
about  many  products,  which  also  gives  them  away.  frequent  pattern 
mining can find them working together on multiple products. 

2.  rank  groups  based  on  a  set  of  group  spam  indicators:  the  groups 
discovered in step 1 may not all be true spammer groups. many of the 
reviewers are grouped together in pattern mining simply due to chance. 
then,  this  step  first  uses  a  set  of  indicators  to  catch  different  types  of 
unusual  group  and  individual  member  behaviors.  these  indicators 
include writing reviews together in a short time window, writing reviews 

134 
 

id31 and opinion mining 

right  after  the  product  launch,  group  review  content  similarity,  group 
rating  deviation,  etc  (mukherjee,  liu  and  glance,  2012).  a  relational 
model, called gsrank (group spam rank), was then proposed to exploit 
the relationships of groups, individual group members, and products that 
they  reviewed  to  rank  candidate  groups  based  on  their  likelihoods  for 
being spammer groups. an iterative algorithm was then used to solve the 
problem. a set of spammer groups was also manually labeled and used to 
evaluate  the  proposed  model,  which  showed  promising  results.  one 
weakness of this method is that due to the frequency threshold used in 
pattern mining, if a group has not worked together many times (three or 
more times), it will not be detected by this method.  

this method is unsupervised as it does not use any manually labeled data for 
training. clearly, with the labeled data supervised learning can be applied as 
well. indeed, (mukherjee, liu and glance, 2012) described experiments with 
several  state-of-the-art  supervised  classification,  regression and  learning  to 
rank algorithms but they were shown to be less effective.  

10.5  summary  

as  social  media  is  increasingly  used  for  critical  decision  making  by 
organizations and individuals, opinion spamming is also becoming more 
and  more  widespread.  for  many  businesses,  posting  fake  opinions 
themselves  or  employing  others  to  do  it  for  them  has  become  a  cheap 
way of marketing and brand promotion.  

although  current  research  on  opinion  spam  detection  is  still  in  its  early 
stage, several effective algorithms have already been proposed and used in 
practice.  spammers,  however,  are  also  getting  more  sophisticated  and 
careful in writing and posting fake opinions to avoid detection. in fact, we 
have  already  seen  an  arms  race  between  detection  algorithms  and 
spammers.  however,  i  am  optimistic  that  more  sophisticated  detection 
algorithms  will  be  designed  to  make  it  very  difficult  for  spammers  to 
post fake opinions. such algorithms are likely to be holistic approaches 
that integrate all possible features or clues in the detection process.  

finally, we should note that opinion spamming occurs not only in reviews, 
but  also  in  other  forms  of  social  media  such  as  blogs,  forum  discussions, 
commentaries, and twitter postings. however, so far little research has been 
done in these contexts.  

 

135 

 

  id31 and opinion mining 

chapter 11 

quality of reviews  

 
in  this  chapter,  we  discuss  the  quality  of  reviews.  the  topic  is  related  to 
opinion  spam  detection,  but  is  also  different  because  low  quality  reviews 
may not be spam or fake reviews, and fake reviews may not be perceived as 
low quality reviews by readers because as we discussed in the last chapter, 
by reading reviews it is very hard to spot fake reviews. for this reason, fake 
reviews may also be seen as helpful or high quality reviews if the imposters 
write their reviews early and craft them well.  

the objective of this task is to determine the quality, helpfulness, usefulness, 
or utility of each review (ghose and ipeirotis, 2007; kim et al., 2006; liu et 
al., 2007; zhang and varadarajan, 2006). this is a meaningful task because 
it is desirable to rank reviews based on quality or helpfulness when showing 
reviews to the user, with the most helpful reviews first. in fact, many review 
aggregation or hosting sites have been practicing this for years. they obtain 
the helpfulness or quality score of each review by asking readers to provide 
helpfulness  feedbacks  to  each  review.  for  example,  in  amazon.com,  the 
reader can indicate whether he/she finds a review helpful by responding to 
the question    was the review helpful to you?    just below each review. the 
feedback results from all those responded are then aggregated and displayed 
right before each review, e.g.,    15 of 16 people found the following review 
helpful.     although  most  review  hosting  sites  already  provide  the  service, 
automatically determining the quality of each review is still useful because a 
good number of user feedbacks may take a long time to accumulate. that is 
why many reviews have few or no feedbacks. this is especially true for new 
reviews.  

11.1  quality as regression problem  

determining  the  quality  of  reviews  is  usually  formulated  as  a  regression 
problem. the learned model assigns a quality score to each review, which 
can  be  used  in  review  ranking  or  review  recommendation.  in  this  area  of 
research, the ground truth data used for both training and testing are usually 
the user-helpfulness feedback given to each review, which as we discussed 
above is provided for each review at many review hosting sites. so, unlike 
fake  review  detection,  the  training  and  testing  data  here  is  not  an  issue. 

136 
 

id31 and opinion mining 

researchers have used many types of features for model building.  

in (kim et al., 2006), id166 regression was used to solve the problem. the 
feature sets included,  

structure  features:  review  length,  number  of  sentences,  percentages  of 
question sentences and exclamations, and the number of html bold tags 
<b> and line breaks <br>.  

lexical features: unigrams and bigrams with tf-idf weights.  
syntactic features: percentage of parsed tokens that are of open-class (i.e., 
nouns,  verbs,  adjectives  and  adverbs),  percentage  of  tokens  that  are 
nouns, percentage of tokens that are verbs, percentage of tokens that are 
verbs  conjugated  in  the  first  person,  and  percentage  of  tokens  that  are 
adjectives or adverbs. 

semantic features: product aspects, and sentiment words.   
meta-data features: review rating (number of stars).  

in (zhang and varadarajan, 2006), the authors also treated the problem as a 
regressions problem. they used similar features, e.g., review length, review 
rating, counts of some specific pos tags, sentiment words, tf-idf weighting 
scores,  wh-words,  product  aspect  mentions,  comparison  with  product 
specifications, comparison with editorial reviews, etc.  

unlike  the  above  approaches,  (liu  et  al.,  2008)  considered  three  main 
factors,  i.e.,  reviewers     expertise,  the  timeliness  of  reviews,  and  review 
styles  based  on  pos  tags.  a  nonlinear  regression  model  was  proposed  to 
integrate the factors. this work focused on movie reviews. 

in (ghose and ipeirotis, 2007; ghose and ipeirotis, 2010), three additional 
sets  of  features  were  used,  namely,  reviewer  profile  features  which  are 
available from the review site, reviewer history features which capture the 
helpfulness of his/her reviews in the past, and a set of readability features, 
i.e., spelling errors and readability indices from the readability research. for 
learning, the authors tried both regression and binary classification.   

lu  et  al.  (2010)  looked  at  the  problem  from  an  additional  angle.  they 
investigated  how  the  social  context  of  reviewers  can  help  enhance  the 
accuracy  of  a  text-based  review  quality  predictor.  they  argued  that  the 
social  context  can  reveal  a  great  deal  of  information  about  the  quality  of 
reviewers,  which  in  turn  affects  the  quality  of  their  reviews.  specifically, 
their approach was based on the following hypotheses:  

author consistency hypothesis: reviews from the same author are of similar 

quality. 

trust consistency hypothesis: a link from a reviewer r1 to a reviewer r2 is an 
explicit or implicit statement of trust. reviewer r1 trusts reviewer r2 only 

 

137 

 

  id31 and opinion mining 

if the quality of reviewer r2 is at least as high as that of reviewer r1.  

co-citation consistency hypothesis: people are consistent in how they trust 
other people. so if two reviewers r1 and r2 are trusted by the same third 
reviewer r3, then their quality should be similar. 

link  consistency  hypothesis:  if  two  people  are  connected  in  the  social 
network  (r1  trusts  r2,  or  r2  trusts  r1,  or  both),  then  their  review  quality 
should be similar.  

the  authors  used 

these hypotheses were enforced as regularizing constraints and added into 
the text-based id75 model to solve the review quality prediction 
problem.  for  experiments, 
the  data  from  ciao 
(www.ciao.co.uk), which is a community review web site. in ciao, people 
not only write reviews for products and services, but also rate the reviews 
written by others. furthermore, people can add members to their network of 
trusted members or    circle of trust   , if they find these members    reviews 
consistently  interesting  and  helpful.  clearly,  this  technique  will  not  be 
applicable to web sites which do not have a trust social network in place.  

11.2  other methods  

in (o'mahony and smyth, 2009), a classification approach was proposed to 
classify helpful and non-helpful reviews. many features were used:  

reputation features: the  mean  (r1)  and standard  deviation  (r2) of review 
helpfulness over all reviews authored by the reviewer, the percentage of 
reviews authored by the reviewer which have received a minimum of t 
feedbacks (r3), etc.    

content  features:  review  length  (c1),  the  ratio  of  uppercase  to  lowercase 

characters in the review text (c3), etc.  

social features: the number of reviews authored by the reviewer (sl1), the 
mean  (sl2)  and  standard  deviation  (sl3)  of  the  number  of  reviews 
authored by all reviewers, etc.  

sentiment features: the rating score of the review (st1), and the mean (st5) 
and standard deviation (st6) of the scores assigned by the reviewer over 
all reviews authored by the reviewer, etc.   

in  (liu  et  al.,  2007),  the  problem  was  also  formulated  as  a  two-class 
classification  problem.  however,  they  argued  that  using  the  helpfulness 
votes as the ground truth may not be appropriate because of three biases: (1) 
vote imbalance (a very large percentage of votes are helpful votes); (2) early 
bird  bias  (early  reviews  tend  to  get  more  votes);  (3)  winner  circle  bias 
(when some reviews get many votes they are ranked high at the review sites 

138 
 

id31 and opinion mining 

which help them get even more votes). those lowly ranked reviews get few 
votes, but they may not be of low quality. the authors then divided reviews 
into  4  categories,     best  review   ,     good  review   ,     fair  review   ,  and     bad 
review,     based  on  whether  reviews  discuss  many  aspects  of  the  product 
and  provide  convincing  opinions.  manual  labeling  was  carried  out  to 
produce  the  gold-standard  training  and  testing  data.  in  classification,  they 
used id166 to perform binary classification. only the    bad review    category 
was regarded as the low quality class and all the other three categories were 
regarded  as  belonging  to  the  high  quality  class.  the  features  for  learning 
were  informativeness,  subjectiveness,  and  readability.  each  of  them 
contained a set of individual features.  

tsur and rappoport (2009) studied the helpfulness of book reviews using an 
unsupervised  approach  which  is  quite  different  from  the  above  supervised 
methods. the method works in three steps. given a collection of reviews, it 
first identifies a set of important terms in the reviews. these terms together 
form  a  vector  representing  a  virtual  optimal  or  core  review.  then,  each 
actual review is mapped or converted to this vector representation based on 
occurrences of the discovered important terms in the review. after that, each 
review is assigned a rank score based on the distance of the review to the 
virtual review (both are represented as vectors).  

in  (moghaddam,  jamali  and  ester,  2012),  a  new  problem  of  personalized 
review  quality  prediction  for  recommendation  of  helpful  reviews  was 
proposed. all of the above methods assume that the helpfulness of a review 
is  the  same  for  all  users/readers,  which  the  authors  argued  is  not  true.  to 
solve  the  new  problem,  they  proposed  several  factorization  models.  these 
models are based on the assumption that the observed review ratings depend 
on some latent features of the reviews, reviewers, raters/users, and products. 
in essence, the paper treated the problem as a personalized recommendation 
problem.  the  proposed  technique  to  solve  the  problem  is  quite  involved. 
some  background  knowledge  about  this  form  of  recommendation  can  be 
found in chapter 12 of the book (liu, 2006 and 2011). 

all the above approaches rank reviews based on the computed helpfulness or 
quality  scores.  however,  tsaparas,  ntoulas  and  terzi  (2011)  argued  that 
these  approaches  do  not  consider  an  important  fact  that  the  top  few  high-
quality  reviews  may  be  highly  redundant  and  repeating 
the  same 
information.  in  their  work,  they  proposed  the  problem  of  selecting  a 
comprehensive and yet a small set of high-quality reviews that cover many 
different aspects of the reviewed entity and also different viewpoints of the 
reviews. they formulated the problem as a maximum coverage problem, and 
presented an algorithm to solve the problem. an earlier work in (lappas and 

 

139 

 

  id31 and opinion mining 

gunopulos, 2010) also studied the problem of finding a small set of reviews 
that cover all product aspects.  

11.3  summary  

in summary, determining review helpfulness is an important research topic. 
it is especially useful for products and services that have a large of number 
reviews. to help the reader get quality opinions quickly, review sites should 
provide  good  review  rankings.  however,  i  would  also  like  to  add  some 
cautionary notes. first, as we discussed in the chapter about opinion search 
and retrieval, we argued that the review ranking (rankings) must reflect the 
natural distribution of positive and negative opinions. it is not a good idea to 
rank  all  positive  (or  all  negative)  reviews  at  the  top  simply  because  they 
have high quality scores. the redundancy issue raised in (tsaparas, ntoulas 
and  terzi,  2011)  is  also  a  valid  concern.  in  my  opinion,  both  quality  and 
distribution  (in  terms  of  positive  and  negative  viewpoints)  are  important. 
second,  readers tend to determine whether a review is helpful or not based 
on  whether  the  review  expresses  opinions  on  many  aspects  of  the  product 
and  appear  to  be  genuine.  a  spammer  can  satisfy  this  requirement  by 
carefully crafting a review that is just like a normal helpful review. so, using 
the  number  of  helpfulness  feedbacks  to  define  review  quality  or  as  the 
ground truth alone can be problematic. furthermore, user feedbacks can be 
spammed  too.  feedback  spam  is  a  sub-problem  of  click  fraud  in  search 
advertising, where a person or robot clicks on some online advertisements to 
give  the  impression  of  real  customer  clicks.  here,  a  robot  or  a  human 
spammer  can  click  on  the  helpfulness  feedback  button  to  increase  the 
helpfulness of a review.  
 
 
 
 
 
 

140 
 

id31 and opinion mining 

chapter 12 

concluding remarks   

 
this book introduced the field of id31 and opinion mining and 
surveyed  the  current  state-of-the-art.  due  to  many  challenging  research 
problems  and  a  wide  variety  of  practical  applications,  the  research  in  the 
field  has  been  very  active  in  recent  years.  it  has  spread  from  computer 
science  to  management  science  (archak,  ghose  and  ipeirotis,  2007;  chen 
and  xie,  2008;  das  and  chen,  2007;  dellarocas,  zhang  and  awad,  2007; 
ghose, ipeirotis and sundararajan, 2007; hu, pavlou and zhang, 2006; park, 
lee and han, 2007) as opinions about products are closely related to profits.  

studied 

the  widely 

topic  of  document-level 

the  book  first  defined  the  sentiment  analysis  problem,  which  provided  a 
common framework to unify different research directions in the field. it then 
discussed 
sentiment 
classification, which aims to determine whether an opinion document (e.g., a 
review) expresses a positive or negative sentiment. this was followed by the 
sentence-level  subjectivity  and  sentiment  classification,  which  determines 
whether a sentence is opinionated, and if so, whether it carries a positive or 
negative opinion. the book then described aspect-based id31 
which  explored  the  full  power  of  the  problem  definition  and  showed  that 
sentiment  analysis  is  a  multi-faceted  problem  with  many  challenging  sub-
problems.  the  existing  techniques  for  dealing  with  them  were  discussed. 
after that, the book discussed the problem of sentiment lexicon generation. 
two dominant approaches were covered. this was followed by the chapter 
on  opinion  summarization,  which  is  a  special  form  of  multi-document 
summarization. however, it is also very different from the traditional multi-
document summarization because opinion summarization can be done in a 
structured  manner,  which  facilitates  both  qualitative  and  quantitative 
analysis, and visualization of opinions. chapter 8 discussed the problem of 
analyzing comparative and superlative sentences. such sentences represent a 
different type of evaluation from regular opinions which have been the focus 
of  the  current  research.  the  topic  of  opinion  search  or  retrieval  was 
introduced  in  chapter  9.  last  but  not  least,  we  discussed  opinion  spam 
detection in chapter 10 and assessing the quality of reviews in chapter 11. 
opinion spamming by writing fake reviews and posting bogus comments are 
increasingly  becoming  an  important  issue  as  more  and  more  people  are 
relying  on  the  opinions  on  the  web  for  decision  making.  to  ensure  the 
trustworthiness of such opinions, combating opinion spamming is an urgent 

 

141 

 

  id31 and opinion mining 

and critical task.  

by reading this book thus far, it is not hard to see that id31 is 
very  challenging  technically.  although  the  research  community  has 
attempted  so  many  sub-problems  from  many  different  angles  and  a  large 
number  of  research  papers  have  also  been  published,  none  of  the  sub-
problems has been solved satisfactorily. our understanding and knowledge 
about  the  whole  problem  and  its  solution  are  still  very  limited.  the  main 
reason is that this is a natural language processing task, and natural language 
processing has no easy problems. another reason may be due to our popular 
ways of doing research. we probably relied too much on machine learning. 
some of the most effective machine learning algorithms, e.g., support vector 
machines,  na  ve  bayes  and  conditional  random  fields,  produce  no  human 
understandable  results  such  that  although  they  may  help  us  achieve 
improved  accuracy,  we  know  little  about  how  and  why  apart  from  some 
superficial knowledge gained in the manual feature engineering process.  

that being said, we have indeed made significant progresses over the past 
decade.  this  is  evident  from  the  large  number  of  start-up  and  established 
companies  that  offer  sentiment  analysis  services.  there  is  a  real  and  huge 
need in the industry for such services because every business wants to know 
how  consumers  perceive  their  products  and  services  and  those  of  their 
competitors. the same can also be said about consumers because whenever 
one  wants  to  buy  something,  one  wants  to  know  the  opinions  of  existing 
users. these practical needs and the technical challenges will keep the field 
vibrant and lively for years to come. 

building on what has been done so far, i believe that we just need to conduct 
more in-depth investigations and to build integrated systems that try to deal 
with all the sub-problems together because their interactions can help solve 
each individual sub-problem. i am optimistic that the whole problem will be 
solved satisfactorily in the near future for widespread applications.  

for applications, a completely automated and accurate solution is nowhere 
in  sight.  however,  it  is  possible  to  devise  effective  semi-automated 
solutions.  the  key  is  to  fully  understand  the  whole  range  of  issues  and 
pitfalls,  cleverly  manage  them,  and  determine  what  portions  can  be  done 
automatically  and  what  portions  need  human  assistance.  in  the  continuum 
between the fully manual solution and the fully automated solution, as time 
goes  by  we  can  push  more  and  more  towards  automation.  i  do  not  see  a 
silver  bullet  solution  soon.  a  good  bet  would  be  to  work  hard  on  a  large 
number of diverse application domains, understand each of them, and design 
a general solution gradually.  

142 
 

id31 and opinion mining 

bibliography 

1.  abbasi,  ahmed,  hsinchun  chen,  and  arab  salem.  sentiment  analysis  in 
multiple  languages:  feature  selection  for  opinion  classification  in  web 
forums. acm transactions on information systems (tois), 2008. 26(3). 

2.  abdul-mageed,  muhammad,  mona  t.  diab,  and  mohammed  korayem. 
subjectivity  and  sentiment  analysis  of  modern  standard  arabic.  in 
proceedings  of 
for 
computational linguistics:shortpapers. 2011. 

the  49th  annual  meeting  of 

the  association 

3.  akkaya, cem, janyce wiebe, and rada mihalcea. subjectivity word sense 
disambiguation.  in  proceedings  of  the  2009  conference  on  empirical 
methods in natural language processing (emnlp-2009). 2009. 

4.  alm, ebba cecilia ovesdotter. affect in text and speech, 2008: proquest. 
5.  andreevskaia,  alina  and  sabine  bergler.  mining  id138  for  fuzzy 
sentiment: sentiment tag extraction from id138 glosses. in proceedings 
of  conference  of 
for 
computational linguistics (eacl-06). 2006. 

the  european  chapter  of 

the  association 

6.  andreevskaia, alina and sabine bergler. when specialists and generalists 
work  together:  overcoming  domain  dependence  in  sentiment  tagging.  in 
proceedings of the annual meeting of the association for computational 
linguistics (acl-2008). 2008. 

7.  andrzejewski,  david  and  xiaojin  zhu.  latent  dirichlet  allocation  with 

topic-in-set knowledge. in proceedings of naacl hlt. 2009. 

8.  andrzejewski,  david,  xiaojin  zhu,  and  mark  craven.  incorporating 
domain  knowledge  into  topic  modeling  via  dirichlet  forest  priors.  in 
proceedings of icml. 2009. 

9.  archak, nikolay, anindya ghose, and panagiotis g. ipeirotis. show me the 
money!:  deriving  the  pricing  power  of  product  features  by  mining 
consumer  reviews.  in  proceedings  of  the  acm  sigkdd  conference  on 
knowledge discovery and data mining (kdd-2007). 2007. 

10.  asher, nicholas, farah  benamara, and yvette yannick mathieu. distilling 
opinion  in  discourse:  a  preliminary  study.  in  proceedings  of  the 
international  conference  on  computational  linguistics  (coling-2008): 
companion volume: posters and demonstrations. 2008. 

11.  asur,  sitaram  and  bernardo  a.  huberman.  predicting  the  future  with 

social media. arxiv preprint arxiv:1003.5699, 2010. 

12.  aue,  anthony  and  michael  gamon.  customizing  sentiment  classifiers  to 
new domains: a case study. in proceedings of recent advances in natural 
language processing (ranlp-2005). 2005. 

13.  banea,  carmen,  rada  mihalcea,  and  janyce  wiebe.  multilingual 
subjectivity:  are  more 
the 
international  conference  on  computational  linguistics  (coling-2010). 
2010. 

in  proceedings  of 

languages  better? 

14.  banea,  carmen,  rada  mihalcea,  janyce  wiebe,  and  samer  hassan. 
multilingual 
in 
proceedings of the conference on empirical methods in natural language 
processing (emnlp-2008). 2008. 

subjectivity  analysis  using  machine 

translation. 

15.  bar-haim,  roy,  elad  dinur,  ronen  feldman,  moshe  fresko,  and  guy 
goldstein. identifying and following expert investors in stock microblogs. 
in  proceedings  of  the  conference  on  empirical  methods  in  natural 
language processing (emnlp-2011). 2011. 

 

143 

 

  id31 and opinion mining 

the 

16.  barbosa, luciano and junlan feng. robust sentiment detection on twitter 
from  biased  and  noisy  data.  in  proceedings  of  the  international 
conference on computational linguistics (coling-2010). 2010. 

17.  bautin,  mikhail,  lohit  vijayarenu,  and  steven  skiena.  international 
id31 for news and blogs. in proceedings of the international 
aaai conference on weblogs and social media (icwsm-2008). 2008. 

18.  becker, israela and vered aharonson. last but definitely not least: on the 
in 

role  of 
proceedings of the acl 2010 conference short papers. 2010. 

in  automatic  polarity-classification. 

last  sentence 

19.  beineke,  philip,  trevor  hastie,  christopher  manning,  and  shivakumar 
vaithyanathan. an exploration of sentiment summarization. in proceedings 
of  aaai  spring  symposium  on  exploring  attitude  and  affect  in  text: 
theories and applications. 2003. 

20.  benamara,  farah,  baptiste  chardon,  yannick  mathieu,  and  vladimir 
popescu. towards context-based subjectivity analysis. in proceedings of 
the  5th  international  joint  conference  on  natural  language  processing 
(ijcnlp-2011). 2011. 

21.  bespalov, dmitriy, bing bai, yanjun qi, and ali shokoufandeh. sentiment 
classification based on supervised latent id165 analysis. in proceeding of 
the acm conference on information and knowledge management (cikm-
2011). 2011. 

22.  bethard,  steven,  hong  yu,  ashley  thornton,  vasileios  hatzivassiloglou, 
and dan jurafsky. automatic extraction of opinion propositions and their 
holders.  in  proceedings  of  the  aaai  spring  symposium  on  exploring 
attitude and affect in text. 2004. 

23.  bickerstaffe,  a.  and  i.  zukerman.  a  hierarchical  classifier  applied  to 
multi-way  sentiment  detection.  in  proceedings  of  the  23rd  international 
conference on computational linguistics (coling 2010). 2010. 

24.  bilgic,  mustafa,  galileo  mark  namata,  and  lise  getoor.  combining 
collective  classification  and  link  prediction.  in  proceedings  of  workshop 
on mining graphs and complex structures. 2007. 

25.  bishop,  c.  m.  pattern  recognition  and  machine  learning.  vol.  4.  2006: 

springer new york. 

26.  blair-goldensohn, sasha, kerry hannan, ryan mcdonald, tyler neylon, 
george  a.  reis,  and  jeff  reynar.  building  a  sentiment  summarizer  for 
local service reviews. in proceedings of www-2008 workshop on nlp in 
the information explosion era. 2008. 

27.  blei,  david  m.  and  jon  d.  mcauliffe.  supervised  topic  models.  in 

proceedings of nips. 2007. 

28.  blei,  david  m.,  andrew  y.  ng,  and  michael  i.  jordan.  latent  dirichlet 
allocation.  the  journal  of  machine  learning  research,  2003.  3:  p.  993-
1022. 

29.  blitzer, john, mark dredze, and fernando pereira. biographies, bollywood, 
boom-boxes and blenders: id20 for sentiment classification. 
in  proceedings  of  annual  meeting  of  the  association  for  computational 
linguistics (acl-2007). 2007. 

30.  blitzer, john, ryan mcdonald, and fernando pereira. id20 
with structural correspondence learning. in proceedings of the conference 
on  empirical  methods  in  natural  language  processing  (emnlp-2006). 
2006. 

31.  blum,  avrim  and  shuchi  chawla.  learning  from  labeled  and  unlabeled 
data using graph mincuts. in proceedings of international conference on 
machine learning (icml-2001). 2001. 

32.  blum,  avrim,  john  lafferty,  mugizi  r.  rwebangira,  and  rajashekar 
in 

reddy.  semi-supervised 

randomized  mincuts. 

learning  using 

144 
 

id31 and opinion mining 

proceedings  of  international  conference  on  machine  learning  (icml-
2004). 2004. 

33.  boiy,  erik  and  marie-francine  moens.  a  machine  learning  approach  to 
sentiment  analysis  in multilingual  web  texts.  information  retrieval, 2009. 
12(5): p. 526-558. 

34.  bollegala,  danushka,  david  weir,  and  john  carroll.  using  multiple 
sources  to  construct  a  sentiment  sensitive  thesaurus  for  cross-domain 
sentiment classification. in proceedings of the 49th annual meeting of the 
association for computational linguistics (acl-2011). 2011. 

35.  bollen, johan, huina mao, and xiao-jun zeng. twitter mood predicts the 

stock market. journal of computational science, 2011. 

36.  boyd-graber, jordan and philip resnik. holistic id31 across 
languages:  multilingual  supervised 
in 
proceedings of the conference on empirical methods in natural language 
processing (emnlp-2010). 2010. 

latent  dirichlet  allocation. 

37.  branavan,  s.  r.  k.,  harr  chen,  jacob  eisenstein,  and  regina  barzilay. 
learning document-level semantic properties from free-text annotations. in 
proceedings of the annual meeting of the association for computational 
linguistics (acl-2008). 2008. 

38.  breck,  eric,  yejin  choi,  and  claire  cardie.  identifying  expressions  of 
opinion in context. in proceedings of the international joint conference on 
artificial intelligence (ijcai-2007). 2007. 

39.  brody, 

 

and 

samuel 

diakopoulos. 
cooooooooooooooollllllllllllll!!!!!!!!!!!!!!  using  word  lengthening  to 
detect  sentiment  in  microblogs.  in  proceedings  of  the  conference  on 
empirical  methods  in  natural  language  processing  (emnlp-2011). 
2011. 

nicholas 

 

40.  brody,  samuel  and  noemie  elhadad.  an  unsupervised  aspect-sentiment 
model for online reviews. in proceedings of the 2010 annual conference 
of the north american chapter of the acl. 2010. 

41.  brooke,  julian,  milan  tofiloski,  and  maite  taboada.  cross-linguistic 
sentiment  analysis:  from  english  to  spanish.  in  proceedings  of  ranlp. 
2009. 

42.  burfoot,  clinton,  steven  bird,  and  timothy  baldwin.  collective 
classification of congressional floor-debate transcripts. in proceedings of 
the 49th annual meeting of the association for computational linguistics 
(acl-2011). 2011. 

43.  carenini,  giuseppe,  raymond  ng,  and  adam  pauls.  multi-document 
summarization of evaluative text. in proceedings of the european chapter 
of the association for computational linguistics (eacl-2006). 2006. 

44.  carenini,  giuseppe,  raymond  ng,  and  ed  zwart.  extracting  knowledge 
from  evaluative  text.  in  proceedings  of  third  intl.  conf.  on  knowledge 
capture (k-cap-05). 2005. 

45.  carvalho, paula, lu  s sarmento, jorge teixeira, and m  rio j. silva. liars 
and  saviors  in  a  sentiment  annotated  corpus  of  comments  to  political 
debates. in proceedings of the 49th annual meeting of the association for 
computational linguistics:shortpapers. 2011. 

46.  castellanos,  malu,  umeshwar  dayal,  meichun  hsu,  riddhiman  ghosh, 
mohamed dekhil, yue lu, lei zhang, and mark schreiman. lci: a social 
channel analysis platform for live customer intelligence. in proceedings of 
the  2011  international  conference  on  management  of  data  (sigmod-
2011). 2011. 

47.  castillo,  carlos  and  brian  d.  davison.  adversarial  web  search. 

foundations and trends in information retrieval, 2010. 4(5): p. 377-486. 

 

145 

 

  id31 and opinion mining 

48.  chaudhuri,  arjun.  emotion  and  reason  in  consumer  behavior2006: 

elsevier butterworth-heinemann. 

49.  chen, bi, leilei zhu, daniel kifer, and dongwon lee. what is an opinion 
about?  exploring  political  standpoints  using  opinion  scoring  model.  in 
proceeedings of aaai conference on artificial intelligence (aaai-2010). 
2010. 

50.  chen, yubo and jinhong xie. online consumer review: word-of-mouth as 
a  new  element  of  marketing  communication  mix.  management  science, 
2008. 54(3): p. 477-491. 

51.  choi, yejin, eric breck, and claire cardie. joint extraction of entities and 
relations  for  opinion  recognition.  in  proceedings  of  the  conference  on 
empirical  methods  in  natural  language  processing  (emnlp-2006). 
2006. 

52.  choi, yejin  and  claire  cardie. adapting  a polarity  lexicon  using  integer 
linear  programming  for  domain-specific  sentiment  classification.  in 
proceedings  of  the  2009  conference  on  empirical  methods  in  natural 
language processing (emnlp-2009). 2009. 

53.  choi,  yejin  and  claire  cardie.  hierarchical  sequential  learning  for 
extracting opinions and their attributes. in proceedings of annual meeting 
of the association for computational linguistics (acl-2010). 2010. 

54.  choi, yejin and claire cardie. learning with id152 as 
structural id136 for subsentential id31. in proceedings of 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2008). 2008. 

55.  choi,  yejin,  claire  cardie,  ellen  riloff,  and  siddharth  patwardhan. 
identifying  sources  of  opinions  with  conditional  random  fields  and 
extraction  patterns.  in  proceedings  of  the  human  language  technology 
conference  and  the  conference  on  empirical  methods  in  natural 
language processing (hlt/emnlp-2005). 2005. 

56.  cilibrasi, rudi l. and paul m. b. vitanyi. the google similarity distance. 
ieee  transactions  on  knowledge  and  data  engineering,  2007. 19(3):  p. 
370-383. 

57.  cui, hang, vibhu mittal, and mayur datar. comparative experiments on 
sentiment  classification  for  online  product  reviews.  in  proceedings  of 
aaai-2006. 2006. 

58.  das,  dipanjan.  a  survey  on  automatic  text  summarization  single-

document summarization. language, 2007. 4: p. 1-31. 

59.  das,  sanjiv  and  mike  chen.  yahoo!  for  amazon:  extracting  market 
sentiment from stock message boards. in proceedings of apfa-2001. 2001. 
60.  das, sanjiv and mike chen. yahoo! for amazon: sentiment extraction from 

small talk on the web. management science, 2007. 53(9): p. 1375-1388. 

61.  dasgupta, sajib and vincent ng. mine the easy, classify the hard: a semi-
supervised approach to automatic sentiment classification. in proceedings 
of the 47th annual meeting of the acl and the 4th ijcnlp of the afnlp 
(acl-2009). 2009. 

62.  dave, kushal, steve lawrence, and david m. pennock. mining the peanut 
gallery: opinion extraction and semantic classification of product reviews. 
in proceedings of international conference on world wide web (www-
2003). 2003. 

63.  davidov,  dmitry,  oren  tsur,  and  ari  rappoport.  enhanced  sentiment 
learning  using  twitter  hashtags  and  smileys.  in  proceedings  of  coling-
2010. 2010. 

64.  dellarocas, c., x.m. zhang, and n.f. awad. exploring the value of online 
product reviews in forecasting sales: the case of motion pictures. journal 
of interactive marketing, 2007. 21(4): p. 23-45. 

146 
 

id31 and opinion mining 

65.  dey, lipika  and s k mirajul haque. opinion mining from noisy text data. 
for  noisy 

the  second  workshop  on  analytics 

in  proceedings  of 
unstructured text data (and-2008). 2008. 

66.  ding, xiaowen and bing liu. resolving object and attribute coreference 
in  opinion  mining.  in  proceedings  of  international  conference  on 
computational linguistics (coling-2010). 2010. 

67.  ding,  xiaowen,  bing  liu,  and  philip  s.  yu.  a  holistic  lexicon-based 
approach  to  opinion  mining.  in  proceedings  of  the  conference  on  web 
search and web data mining (wsdm-2008). 2008. 

68.  ding, xiaowen, bing liu, and lei zhang. entity discovery and assignment 
for  opinion  mining  applications.  in  proceedings  of  acm  sigkdd 
international  conference  on  knowledge  discovery  and  data  mining 
(kdd-2009). 2009. 

69.  dowty,  david  r.,  robert  e.  wall,  and  stanley  peters.  introduction  to 

montague semantics. vol. 11. 1981: springer. 

lexicon. 

70.  dragut,  eduard  c.,  clement  yu,  prasad  sistla,  and  weiyi  meng. 
construction  of  a  sentimental  word  dictionary.  in  proceedings  of  acm 
international  conference  on  information  and  knowledge  management 
(cikm-2010). 2010. 

71.  du, weifu and songbo tan. building domain-oriented sentiment lexicon by 
improved  information  bottleneck.  in  proceedings  of  acm  conference  on 
information and knowledge management (cikm-2009). 2009. acm. 

72.  du,  weifu,  songbo  tan,  xueqi  cheng,  and  xiaochun  yun.  adapting 
information  bottleneck  method  for  automatic  construction  of  domain-
oriented  sentiment 
in  proceedings  of  acm  international 
confernece on web search and data mining (wsdm-2010). 2010. 

73.  duh, kevin, akinori fujino, and masaaki nagata. is machine translation 
ripe for cross-lingual sentiment classification? in proceedings of the 49th 
annual  meeting 
computational 
linguistics:shortpapers (acl-2011). 2011. 

association 

74.  eguchi,  koji  and  victor  lavrenko.  sentiment  retrieval  using  generative 
models.  in  proceedings  of  conference  on  empirical  methods  in  natural 
language processing (emnlp-2006). 2006. 

75.  esuli, andrea and fabrizio sebastiani. determining term subjectivity and 
term  orientation  for  opinion  mining.  in  proceedings  of  conf.  of  the 
european  chapter  of  the  association  for  computational  linguistics 
(eacl-2006). 2006. 

76.  esuli,  andrea  and  fabrizio  sebastiani.  determining 

the  semantic 
orientation of terms through gloss classification. in proceedings of acm 
international  conference  on  information  and  knowledge  management 
(cikm-2005). 2005. 

77.  esuli, andrea and fabrizio sebastiani. sentiid138: a publicly available 
lexical  resource  for  opinion  mining.  in  proceedings  of  language 
resources and evaluation (lrec-2006). 2006. 

the 

for 

of 

78.  feldman,  ronen,  benjamin  rosenfeld,  roy    bar-haim,  and  moshe  
fresko. the stock sonar - id31 of stocks based on a hybrid 
approach. 
in  proceedings  of  23rd  iaai  conference  on  artificial 
intelligence (iaai-2011). 2011. 

79.  feng, song, ritwik bose, and yejin choi. learning general connotation of 
words  using  graph-based  algorithms.  in  proceedings  of  confernece  on 
empirical  methods  in  natural  language  processing  (emnlp-2011). 
2011. 

80.  fiszman,  marcelo,  dina  demner-fushman,  francois  m.  lang,  philip 
goetz, and thomas c. rindflesch. interpreting comparative constructions 
in biomedical text. in proceedings of bionlp. 2007. 

 

147 

 

  id31 and opinion mining 

terms:. 

81.  frantzi,  katerina,  sophia  ananiadou,  and  hideki    mima.  automatic 
the  c-value/nc-value  method. 

recognition  of  multi-word 
international journal on digital libraries, 2000. 3(2): p. 115-130. 

82.  gamon,  michael.  sentiment  classification  on  customer  feedback  data: 
noisy  data,  large  feature  vectors,  and  the  role  of  linguistic  analysis.  in 
proceedings  of  international  conference  on  computational  linguistics 
(coling-2004). 2004. 

83.  gamon, michael, anthony aue, simon corston-oliver, and eric ringger. 
pulse:  mining  customer  opinions  from  free  text.  advances  in  intelligent 
data analysis vi, 2005: p. 121-132. 

84.  ganapathibhotla,  murthy  and  bing  liu.  mining  opinions  in  comparative 
sentences.  in  proceedings  of  international  conference  on  computational 
linguistics (coling-2008). 2008. 

85.  ganesan,  kavita,  chengxiang  zhai,  and  jiawei  han.  opinosis:  a  graph-
based  approach  to  abstractive  summarization  of  highly  redundant 
opinions.  in  proceedings  of  the  23rd  international  conference  on 
computational linguistics (coling-2010). 2010. 

86.  ganter,  viola  and  michael  strube.  finding  hedges  by  chasing  weasels: 
hedge  detection  using  wikipedia  tags  and  shallow  linguistic  features.  in 
proceedings of the acl-ijcnlp 2009 conference, short papers. 2009. 

87.  gao,  sheng  and  haizhou  li.  a  cross-domain  adaptation  method  for 
sentiment classification using probabilistic latent analysis. in proceeding 
of  the  acm  conference  on  information  and  knowledge  management 
(cikm-2011). 2011. 

88.  ghahramani, zoubin and katherine a. heller. bayesian sets. advances in 

neural information processing systems, 2006. 18: p. 435. 

89.  ghani,  rayid,  katharina  probst,  yan  liu,  marko  krema,  and  andrew 
fano.  text  mining  for  product  attribute  extraction.  acm  sigkdd 
explorations newsletter, 2006. 8(1): p. 41-48. 

90.  ghose,  anindya  and  panagiotis  g.  ipeirotis.  designing  novel  review 
ranking  systems:  predicting  the  usefulness  and  impact  of  reviews.  in 
proceedings  of  the  international  conference  on  electronic  commerce. 
2007. 

91.  ghose, anindya and panagiotis g. ipeirotis. estimating the helpfulness and 
economic 
text  and  reviewer 
characteristics.  ieee  transactions  on  knowledge  and  data  engineering, 
2010. 

impact  of  product  reviews:  mining 

92.  ghose, anindya, panagiotis g. ipeirotis, and arun sundararajan. opinion 
mining  using  econometrics:  a  case  study  on  reputation  systems.  in 
proceedings of the association for computational linguistics (acl-2007). 
2007. 

93.  gibbs,  raymond  w    and  herbert  l.    colston.  irony  in  language  and 

thought: a cognitive science reader, 2007: lawrence erlbaum. 

94.  gibbs,  raymond  w.  on  the  psycholinguistics  of  sarcasm.  journal  of 

experimental psychology: general, 1986. 115(1): p. 3. 

95.  goldberg,  andrew  b.  and  xiaojin  zhu.  seeing  stars  when  there  aren't 
many  stars:  graph-based  semi-supervised 
for  sentiment 
categorization.  in  proceedings  of  hlt-naacl  2006  workshop  on 
textgraphs:  graph-based  algorithms  for  natural  language  processing. 
2006. 

96.  gonz  lez-ib    ez,  roberto,  smaranda  muresan,  and  nina  wacholder. 
identifying sarcasm in twitter: a closer look. in proceedings of the 49th 
annual  meeting 
computational 
linguistics:shortpapers (acl-2011). 2011. 

association 

learning 

the 

for 

of 

148 
 

id31 and opinion mining 

97.  greene, stephan and philip resnik. more than words: syntactic packaging 
and implicit sentiment. in proceedings of human language technologies: 
the 2009 annual conference of the north american chapter of the acl 
(naacl-2009). 2009. 

98.  griffiths,  thomas  l.  and  mark  steyvers.  prediction  and  semantic 

association. in neural information processing systems 15. 2003. 

99.  griffiths,  thomas  l.,  mark  steyvers,  david  m.  blei,  and  joshua  b. 
in  neural 

topics  and  syntax.  advances 

tenenbaum. 
information processing systems, 2005. 17: p. 537   544. 

integrating 

100. groh,  georg  and  jan  hauffa.  characterizing  social  relations  via  nlp-
based id31. in proceedings of the fifth international aaai 
conference on weblogs and social media (icwsm-2011). 2011. 

101. guo,  honglei  ,  huijia  zhu,  zhili  guo,  xiaoxun  zhang,  and  zhong  su. 
opinionit:  a  text  mining  system  for  cross-lingual  opinion  analysis.  in 
proceeding  of  the  acm  conference  on  information  and  knowledge 
management (cikm-2010). 2010. 

102. guo,  honglei  ,  huijia  zhu,  zhili  guo,  xiaoxun  zhang,  and  zhong  su. 
product feature categorization with multilevel latent semantic association. 
in  proceedings  of  acm  international  conference  on  information  and 
knowledge management (cikm-2009). 2009. 

103. hai, zhen, kuiyu chang, and jung-jae kim. implicit feature identification 
via co-occurrence association rule mining. computational linguistics and 
intelligent text processing, 2011: p. 393-404. 

104. hancock,  jeffrey  t.,  lauren  e.  curry,  saurabh  goorha,  and  michael 
woodworth. on lying and being lied to: a linguistic analysis of deception 
in computer-mediated communication. discourse processes, 2007. 45(1): p. 
1-23. 

105. hardisty,  eric  a.,  jordan  boyd-graber,  and  philip  resnik.  modeling 
perspective  using  adaptor  grammars.  in  proceedings  of  the  2010 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2010). 2010. 

106. hassan,  ahmed,  amjad  abu-jbara,  rahul  jha,  and  dragomir  radev. 
identifying the semantic orientation of foreign words. in proceedings of the 
49th  annual  meeting  of 
for  computational 
linguistics:shortpapers (acl-2011). 2011. 

the  association 

107. hassan, ahmed, vahed qazvinian, and dragomir radev. what's with the 
attitude?:  identifying  sentences  with  attitude  in  online  discussions.  in 
proceedings  of  the  2010  conference  on  empirical  methods  in  natural 
language processing (emnlp-2010). 2010. 

108. hassan,  ahmed  and  dragomir  radev.  identifying  text  polarity  using 
random  walks.  in  proceedings  of  annual  meeting  of  the  association  for 
computational linguistics (acl-2010). 2010. 

109. hatzivassiloglou,  vasileios,  judith  l.  klavans,  melissa  l.  holcombe, 
regina barzilay, min-yen kan, and kathleen r. mckeown. simfinder: a 
flexible  id91  tool  for  summarization.  in  in  proceedings  of  the 
workshop on summarization in naacl-01. 2001. 

110. hatzivassiloglou,  vasileios  and  kathleen  r.  mckeown.  predicting  the 
semantic orientation of adjectives. in proceedings of annual meeting of the 
association for computational linguistics (acl-1997). 1997. 

111. hatzivassiloglou,  vasileios  and  janyce  wiebe.  effects  of  adjective 
orientation  and  gradability  on  sentence  subjectivity.  in  proceedings  of 
interntional  conference  on  computational  linguistics  (coling-2000). 
2000. 

 

149 

 

  id31 and opinion mining 

112. he, yulan. learning sentiment classification model from labeled features. 
in  proceeding  of  the  acm  conference  on  information  and  knowledge 
management (cikm-2011). 2010. 

113. he,  yulan,  chenghua  lin,  and  harith  alani.  automatically  extracting 
polarity-bearing  topics  for  cross-domain  sentiment  classification.  in 
proceedings  of 
for 
computational linguistics (acl-2011). 2011. 

the  49th  annual  meeting  of 

the  association 

114. hearst, marti. direction-based text interpretation as an information access 
refinement,  in  text-based  intelligent  systems,  p.  jacobs,  editor  1992, 
lawrence erlbaum associates. p. 257-274. 

115. hobbs, jerry r. and ellen riloff. information extraction, in in handbook 
of  natural  language  processing,  2nd  edition,  n.  indurkhya  and  f.j. 
damerau, editors. 2010, chapman & hall/crc press. 

116. hofmann, thomas. probabilistic id45. in proceedings 

of conference on uncertainty in artificial intelligence (uai-1999). 1999. 

117. hong, yancheng  and steven  skiena. the wisdom of bookies? sentiment 
analysis  vs.  the  nfl  point  spread.  in  proceedings  of  the  international 
conference on weblogs and social media (icwsm-2010). 2010. 

118. hu, minqing and bing liu. mining and summarizing customer reviews. in 
proceedings  of  acm  sigkdd  international  conference  on  knowledge 
discovery and data mining (kdd-2004). 2004. 

119. hu, nan, paul a pavlou, and jennifer zhang. can online reviews reveal a 
product's  true  quality?:  empirical  findings  and  analytical  modeling  of 
online  word-of-mouth  communication.  in  proceedings  of  electronic 
commerce (ec-2006). 2006. 

120. huang,  xuanjing  and  w.  bruce  croft.  a  unified  relevance  model  for 
opinion retrieval. in proceedings of acm confernece on information and 
knowledge management (cikm-2009). 2009. 

121. ikeda,  daisuke,  hiroya  takamura,  lev-arie  ratinov,  and  manabu 
okumura.  learning 
for  sentiment 
classification. in proceedings of the 3rd international joint conference on 
natural language processing (ijcnlp-2008). 2008. 

the  polarity  of  words 

122. indurkhya,  nitin  and  fred  j.  damerau.  handbook  of  natural  language 

to  shift 

processing, 2010: second edition, chapman & hall. 

123. jakob, niklas and iryna gurevych. extracting opinion targets in a single-
and  cross-domain  setting  with  conditional  random  fields. 
in 
proceedings  of  conference  on  empirical  methods  in  natural  language 
processing (emnlp-2010). 2010. 

124. jia,  lifeng,  clement  yu,  and  weiyi  meng.  the  effect  of  negation  on 
sentiment  analysis  and  retrieval  effectiveness.  in  proceeding  of  the  18th 
acm  conference  on  information  and  knowledge  management  (cikm-
2009). 2009. 

125. jiang, jay j. and david w. conrath. semantic similarity based on corpus 
in 

in  proceedings  of  research 

taxonomy. 

statistics  and 
computational linguistics. 1997. 

lexical 

126. jiang, long, mo yu, ming zhou, xiaohua liu, and tiejun zhao. target-
dependent  twitter  sentiment  classification.  in  proceedings  of  the  49th 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
2011). 2011. 

127. jijkoun, valentin , maarten de  rijke, and wouter  weerkamp. generating 
focused  topic-speci   c  sentiment  lexicons.  in  proceedings  of  annual 
meeting  of  the  association  for  computational  linguistics  (acl-2010). 
2010. 

150 
 

id31 and opinion mining 

128. jin,  wei  and  hung  hay  ho.  a  novel  lexicalized  id48-based  learning 
framework  for  web  opinion  mining.  in  proceedings  of  international 
conference on machine learning (icml-2009). 2009. 

129. jindal,  nitin  and  bing  liu.  identifying  comparative  sentences  in  text 
documents.  in  proceedings  of  acm  sigir  conf.  on  research  and 
development in information retrieval (sigir-2006). 2006a. 

130. jindal, nitin and bing liu. mining comparative sentences and relations. in 
proceedings  of  national  conf.  on  artificial  intelligence  (aaai-2006). 
2006b. 

131. jindal, nitin and bing liu. opinion spam and analysis. in proceedings of 
the  conference  on  web  search  and  web  data  mining  (wsdm-2008). 
2008. 

132. jindal,  nitin  and  bing  liu.  review  spam  detection.  in  proceedings  of 

www (poster paper). 2007. 

133. jindal,  nitin,  bing  liu,  and  ee-peng  lim.  finding  unusual  review 
patterns  using  unexpected  rules.  in  proceedings  of  acm  international 
conference  on  information  and  knowledge  management  (cikm-2010). 
2010. 

134. jo, yohan and alice oh. aspect and sentiment unification model for online 
review analysis. in proceedings of acm conference on web search and 
data mining (wsdm-2011). 2011. 

135. joachims,  thorsten.  making  large-scale  id166  learning  practical,  in 
advances in kernel methods - support vector learning, b. sch  lkopf, c. 
burges, and a. smola, editors. 1999, mit press. 

136. johansson,  richard  and  alessandro  moschitti.  reranking  models  in  fine-
grained opinion analysis. in proceedings of the international conference 
on computational linguistics (coling-2010). 2010. 

137. joshi, mahesh, dipanjan das, kevin gimpel, and noah a. smith. movie 
reviews and revenues: an experiment in text regression. in proceedings of 
the  north  american  chapter  of  the  association  for  computational 
linguistics  human  language  technologies  conference  (naacl  2010). 
2010. 

138. kaji,  nobuhiro  and  masaru  kitsuregawa.  automatic  construction  of 
polarity-tagged  corpus 
in  proceedings  of 
coling/acl  2006  main  conference  poster  sessions  (coling-acl-
2006). 2006. 

from  html  documents. 

139. kaji,  nobuhiro  and  masaru  kitsuregawa.  building  lexicon  for  sentiment 
analysis  from  massive  collection  of  html  documents.  in  proceedings  of 
the  joint  conference  on  empirical  methods  in  natural  language 
processing  and  computational  natural  language  learning  (emnlp-
2007). 2007. 

140. kamps,  jaap,  maarten  marx,  robert  j.  mokken,  and  maarten  de  rijke. 
using id138 to measure semantic orientation of adjectives. in proc. of 
lrec-2004. 2004. 

141. kanayama,  hiroshi  and  tetsuya  nasukawa.  fully  automatic  lexicon 
expansion  for  domain-oriented  sentiment  analysis.  in  proceedings  of 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2006). 2006. 

142. kennedy,  alistair  and  diana  inkpen.  sentiment  classification  of  movie 
reviews  using  contextual  valence  shifters.  computational  intelligence, 
2006. 22(2): p. 110-125. 

143. kennedy,  christopher.  comparatives,  semantics  of,  in  encyclopedia  of 

language and linguistics, second edition, 2005, elsevier. 

144. kessler,  jason  s.  and  nicolas  nicolov.  targeting  sentiment  expressions 
through supervised ranking of linguistic configurations. in proceedings of 

 

151 

 

  id31 and opinion mining 

the  third  international  aaai  conference  on  weblogs  and  social  media 
(icwsm-2009). 2009. 

145. kim,  hyun  duk  and  chengxiang  zhai.  generating  comparative 
summaries  of  contradictory  opinions  in  text.  in  proceedings  of  acm 
conference  on  information  and  knowledge  management  (cikm-2009). 
2009. 
146. kim, 

jin-ji  li,  and 

jungi  kim, 

multilanguage-comparability  of 
proceedings  of 
computational linguistics (acl-2010). 2010. 

the  48th  annual  meeting  of 

subjectivity  analysis 

jong-hyeok  lee.  evaluating 
in 
for 

systems. 
the  association 

147. kim, jungi, jin-ji li, and jong-hyeok lee. discovering the discriminative 
views:  measuring  term  weights  for  sentiment  analysis.  in  proceedings  of 
the  47th  annual  meeting of the  acl and  the 4th ijcnlp  of  the  afnlp 
(acl-2009). 2009. 

148. kim, soo-min and eduard hovy. automatic identification of pro and con 
reasons  in  online  reviews.  in  proceedings  of  coling/acl  2006  main 
conference poster sessions (acl-2006). 2006. 

149. kim,  soo-min  and  eduard  hovy.  crystal:  analyzing  predictive  opinions 
on the web. in proceedings of the joint conference on empirical methods 
in  natural  language  processing  and  computational  natural  language 
learning (emnlp/conll-2007). 2007. 

150. kim, soo-min and eduard hovy. determining the sentiment of opinions. in 
proceedings  of  interntional  conference  on  computational  linguistics 
(coling-2004). 2004. 

151. kim,  soo-min  and  eduard  hovy.  extracting  opinions,  opinion  holders, 
and  topics  expressed  in  online  news  media  text.  in  proceedings  of  the 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2006). 2006. 

152. kim,  soo-min  and  eduard  hovy.  identifying  and  analyzing  judgment 
opinions.  in proceedings of human  language  technology  conference of 
the north american chapter of the acl. 2006. 

153. kim,  soo-min,  patrick  pantel,  tim  chklovski,  and  marco  pennacchiotti. 
automatically  assessing  review  helpfulness. 
the 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2006). 2006. 

in  proceedings  of 

154. kleinberg,  jon  m.  authoritative  sources  in  a  hyperlinked  environment. 

journal of the acm (jacm), 1999. 46(5): p. 604-632. 

155. kobayashi, nozomi, ryu iida, kentaro inui, and yuji matsumoto. opinion 
mining  on  the  web  by  extracting  subject-attribute-value  relations.  in 
proceedings of aaai-caaw'06. 2006. 

156. kobayashi, nozomi, kentaro inui, and yuji matsumoto. extracting aspect-
evaluation and aspect-of relations in opinion mining. in proceedings of the 
2007  joint  conference  on  empirical  methods  in  natural  language 
processing and computational natural language learning. 2007. 

157. kouloumpis,  efthymios,  theresa  wilson,  and  johanna  moore.  twitter 
sentiment  analysis:  the good  the  bad and  the omg!  in proceedings of 
the  fifth  international  aaai  conference  on  weblogs  and  social  media 
(icwsm-2011). 2011. 

158. kovelamudi,  sudheer,  sethu  ramalingam,  arpit  sood,  and  vasudeva 
varma. domain independent model for product attribute extraction from 
user  reviews  using  wikipedia.  in  proceedings  of  the  5th  international 
joint conference on natural language processing (ijcnlp-2010). 2011. 
159. kreuz, roger j  and gina m caucci. lexical influences on the perception 
of sarcasm. in proceedings of the workshop on computational approaches 
to figurative language. 2007. 

152 
 

id31 and opinion mining 

160. kreuz,  roger  j.  and  sam  glucksberg.  how  to  be  sarcastic:  the  echoic 
reminder  theory  of  verbal  irony.  journal  of  experimental  psychology: 
general, 1989. 118(4): p. 374. 

161. ku,  lun-wei,  yu-ting  liang,  and  hsin-hsi  chen.  opinion  extraction, 
summarization and tracking in news and blog corpora. in proceedings of 
aaai-caaw'06. 2006. 

162. lafferty,  john,  andrew  mccallum,  and  fernando  pereira.  conditional 
random fields: probabilistic models for segmenting and labeling sequence 
data.  in  proceedings  of  international  conference  on  machine  learning 
(icml-2001). 2001. 

163. lakkaraju, himabindu, chiranjib bhattacharyya, indrajit bhattacharya, and 
srujana merugu. exploiting coherence for the simultaneous discovery of 
latent  facets  and  associated  sentiments.  in  proceedings  of  siam 
conference on data mining (sdm-2011). 2011. 

164. lappas, theodoros and dimitrios gunopulos. efficient confident search in 

large review corpora. in proceedings of ecml-pkdd 2010. 2010. 

165. lee,  lillian.  measures  of  distributional  similarity.  in  proceedings  of 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
1999). 1999. 

166. lerman, kevin, sasha blair-goldensohn, and ryan mcdonald. sentiment 
summarization: evaluating and learning user preferences. in proceedings 
of  the  12th  conference  of  the  european  chapter  of  the  association  for 
computational linguistics (eacl-2009). 2009. 

167. lerman,  kevin  and  ryan  mcdonald.  contrastive  summarization:  an 
experiment with consumer reviews. in proceedings of naacl hlt 2009: 
short papers. 2009. 

168. li,  binyang,  lanjun    zhou,  shi  feng,  and  kam-fai  wong.  a  uni   ed 
graph  model  for  sentence-based  opinion  retrieval.  in  proceedings  of 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
2010). 2010. 

169. li,  fangtao,  chao  han,  minlie  huang,  xiaoyan  zhu,  ying-ju  xia,  shu 
zhang, and hao yu. structure-aware review mining and summarization. in 
proceedings  of  the  23rd  international  conference  on  computational 
linguistics (coling-2010). 2010. 

170. li,  fangtao,  minlie  huang,  yi  yang,  and  xiaoyan  zhu.  learning  to 
the  international  joint 

identify  review  spam. 
conference on artificial intelligence (ijcai-2011). 2011. 

in  proceedings  of 

171. li,  fangtao,  minlie  huang,  and  xiaoyan  zhu.  sentiment  analysis  with 
global topics and local dependency. in proceedings of the twenty-fourth 
aaai conference on artificial intelligence (aaai-2010). 2010. 

172. li, junhui, guodong zhou, hongling wang, and qiaoming zhu. learning 
the scope of negation via shallow id29. in proceedings of the 
23rd  international  conference  on  computational  linguistics  (coling-
2010). 2010. 

173. li, shasha, chin-yew lin, young-in song, and zhoujun li. comparable 
entity  mining  from  comparative  questions.  in  proceedings  of  annual 
meeting  of  the  association  for  computational  linguistics  (acl-2010). 
2010. 

174. li, shoushan, chu-ren  huang, guodong zhou, and sophia yat mei lee. 
employing personal/impersonal views in supervised and semi-supervised 
sentiment  classi   cation.  in  proceedings  of  annual  meeting  of  the 
association for computational linguistics (acl-2010). 2010. 

175. li,  shoushan,  sophia  yat  mei  lee,  ying  chen,  chu-ren  huang,  and 
in 

guodong  zhou.  sentiment  classification  and  polarity  shifting. 

 

153 

 

  id31 and opinion mining 

2007. 

proceedings  of  the  23rd  international  conference  on  computational 
linguistics (coling-2010). 2010. 

176. li, shoushan, zhongqing wang, guodong zhou, and sophia yat mei lee. 
semi-supervised  learning  for  imbalanced  sentiment  classification.  in 
proceedings  of  international  joint  conference  on  artificial  intelligence 
(ijcai-2011). 2011. 

177. li,  tao,  yi  zhang,  and  vikas  sindhwani.  a  non-negative  matrix  tri-
factorization  approach  to  sentiment  classification  with  lexical  prior 
knowledge.  in  proceedings  of  the  annual  meeting  of  the  association  for 
computational linguistics (acl-2009). 2009. 

178. li,  xiao-li,  lei  zhang,  bing  liu,  and  see-kiong  ng.  distributional 
similarity  vs.  pu  learning  for  entity  set  expansion.  in  proceedings  of 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
2010). 2010. 

179. lim,  ee-peng,  viet-an  nguyen,  nitin  jindal,  bing  liu,  and  hady  w. 
lauw.  detecting  product  review  spammers  using  rating  behaviors.  in 
proceedings  of  acm  international  conference  on  information  and 
knowledge management (cikm-2010). 2010. 

180. lin,  chenghua  and  yulan  he.  joint  sentiment/topic  model  for  sentiment 
analysis. in proceedings of acm international conference on information 
and knowledge management (cikm-2009). 2009. 

181. lin,  dekang.  automatic  retrieval  and  id91  of  similar  words.  in 
proccedings of 36th annual meeting of the association for computational 
linguistics  and  17th 
international  conference  on  computational 
linguistics (coling-acl-1998). 1998. 

182. lin,  dekang.  minipar.  http://webdocs.cs.ualberta.ca/lindek/minipar.htm. 

183. lin, wei-hao, theresa wilson, janyce wiebe, and alexander hauptmann. 
which  side  are  you  on?:  identifying  perspectives  at  the  document  and 
sentence  levels.  in  proceedings  of  the  conference  on  natural  language 
learning (conll-2006). 2006. 

184. liu,  bing.  sentiment  analysis  and  subjectivity,  in  handbook  of  natural 
language  processing,  second  edition,  n.  indurkhya  and  f.j.  damerau, 
editors. 2010. 

185. liu, bing. web data mining: exploring hyperlinks, contents, and usage 

data, 2006 and 2011: springer. 

186. liu,  bing,  wynne  hsu,  and  yiming  ma.  integrating  classification  and 
association  rule  mining.  in  proceedings  of  international  conference  on 
knowledge discovery and data mining (kdd-1998). 1998. 

187. liu, bing, minqing hu, and junsheng cheng. opinion observer: analyzing 
and  comparing  opinions  on  the  web.  in  proceedings  of  international 
conference on world wide web (www-2005). 2005. 

188. liu, bing, wee sun lee, philip s. yu, and xiao-li li. partially supervised 
in  proceedings  of  international 

classification  of 
conference on machine learning (icml-2002). 2002. 

text  documents. 

189. liu,  feifan,  bin  li,  and  yang  liu.  finding  opinionated  blogs  using 
statistical  classifiers  and  lexical  features.  in  proceedings  of  the  third 
international  aaai  conference  on  weblogs  and  social  media  (icwsm-
2009). 2009. 

190. liu, feifan, dong wang, bin li, and yang liu. improving blog polarity 
classification via topic analysis and adaptive methods. in proceedings of 
human language technologies: the 2010 annual conference of the north 
american chapter of the acl (hlt-naacl-2010). 2010. 

191. liu, jingjing, yunbo cao, chin-yew lin, yalou huang, and ming zhou. 
low-quality  product  review  detection  in  opinion  summarization.  in 

154 
 

id31 and opinion mining 

proceedings  of  the  joint  conference  on  empirical  methods  in  natural 
language  processing  and  computational  natural  language  learning 
(emnlp-conll-2007). 2007. 

192. liu, jingjing and stephanie seneff. review sentiment scoring via a parse-
and-paraphrase  paradigm.  in  proceedings  of  the  2009  conference  on 
empirical  methods  in  natural  language  processing  (emnlp-2009). 
2009. 

193. liu, yang, xiangji huang, aijun an, and xiaohui yu. arsa: a sentiment-
aware model for predicting sales performance using blogs. in proceedings 
of  acm  sigir  conf.  on  research  and  development  in  information 
retrieval (sigir-2007). 2007. 

194. liu,  yang,  xiangji  huang,  aijun  an,  and  xiaohui  yu.  modeling  and 
predicting  the  helpfulness  of  online  reviews.  in  proceedings  of  icdm-
2008. 2008. 

195. long, chong, jie zhang, and xiaoyan zhu. a review selection approach 
for  accurate  feature  rating  estimation.  in  proceedings  of  coling  2010: 
poster volume. 2010. 

196. lu, bin. identifying opinion holders and targets with dependency parser in 
chinese news texts. in proceedings of human language technologies: the 
2010 annual conference of the north american chapter of the acl (hlt-
naacl-2010). 2010. 

197. lu,  bin,  chenhao  tan,  claire  cardie,  and  benjamin  k.  tsou.  joint 
bilingual  sentiment  classification  with  unlabeled  parallel  corpora.  in 
proceedings  of 
for 
computational linguistics (acl-2011). 2011. 

the  49th  annual  meeting  of 

the  association 

198. lu,  yue,  malu  castellanos,  umeshwar  dayal,  and  chengxiang  zhai. 
automatic  construction  of  a  context-aware  sentiment 
lexicon:  an 
optimization approach. in proceedings of the 20th international conference 
on world wide web (www-2011). 2011. 

199. lu,  yue,  huizhong  duan,  hongning  wang,  and  chengxiang  zhai. 
exploiting structured ontology to organize scattered online opinions. in 
proceedings  of  interntional  conference  on  computational  linguistics 
(coling-2010). 2010. 

200. lu,  yue,  panayiotis  tsaparas,  alexandros  ntoulas,  and  livia  polanyi. 
exploiting social context for review quality prediction. in proceedings of 
international world wide web confernece (www-2010). 2010. 

201. lu,  yue  and  chengxiang  zhai.  opinion  integration  through  semi-
supervised id96. in proceedings of international conference on 
world wide web (www-2008). 2008. 

202. lu,  yue,  chengxiang  zhai,  and  neel  sundaresan.  rated  aspect 
summarization  of  short  comments.  in  proceedings  of  international 
conference on world wide web (www-2009). 2009. 

203. ma, tengfei and xiaojun wan. opinion target extraction in chinese news 
comments. in proceedings of coling 2010 poster volume (coling-2010). 
2010. 

204. maas, andrew l., raymond e. daly, peter t. pham, dan huang, andrew 
y.  ng,  and  christopher  potts.  learning  word  vectors  for  sentiment 
analysis. in proceedings of the 49th annual meeting of the association for 
computational linguistics (acl-2011). 2011. 

205. macdonald,  craig,  iadh ounis,  and ian  soboroff. overview  of  the  trec 

2007 blog track. 2007. 

206. manevitz,  larry  m.  and  malik  yousef.  one-class  id166s  for  document 
classification. the journal of machine learning research, 2002. 2: p. 139-
154. 

 

155 

 

  id31 and opinion mining 

207. manning,  christopher  d.,  prabhakar  raghavan,  and  hinrich  schutze. 
introduction to information retrieval. vol. 1. 2008: cambridge university 
press. 

208. manning,  christopher  d.  and  hinrich  schutze.  foundations  of  statistical 

natural language processing. vol. 999. 1999: mit press. 

209. martineau, justin and tim finin. delta tfidf: an improved feature space for 
sentiment  analysis.  in  proceedings  of  the  third  international  aaai 
conference on weblogs and social media (icwsm-2009). 2009. 

210. mcdonald,  ryan,  kerry  hannan,  tyler  neylon,  mike  wells,  and  jeff 
reynar.  structured  models  for  fine-to-coarse  sentiment  analysis.  in 
proceedings  of  annual  meeting  of  the  association  for  computational 
linguistics (acl-2007). 2007. 

211. mcglohon,  mary,  natalie  glance,  and  zach  reiter.  star  quality: 
aggregating  reviews  to  rank  products  and  merchants.  in  proceedings  of 
the  international  conference  on  weblogs  and  social  media  (icwsm-
2010). 2010. 

212. medlock,  ben  and  ted  briscoe.  weakly  supervised  learning  for  hedge 
classification  in  scientific  literature.  in  proceedings  of  the  45th  annual 
meeting of the association of computational linguistics. 2007. 

213. mei,  qiaozhu,  xu  ling,  matthew  wondra,  hang  su,  and  chengxiang 
zhai. topic sentiment mixture: modeling facets and opinions in weblogs. in 
proceedings  of  international  conference  on  world  wide  web  (www-
2007). 2007. 

214. mejova, yelena and padmini srinivasan. exploring feature definition and 
selection 
the  fifth 
international  aaai  conference  on  weblogs  and  social  media  (icwsm-
2011). 2011. 

for  sentiment  classifiers. 

in  proceedings  of 

215. meng, xinfan and houfeng wang. mining user reviews: from specification 
to  summarization.  in  proceedings  of  the  acl-ijcnlp  2009  conference 
short papers. 2009. 

216. mihalcea, rada, carmen banea, and janyce wiebe. learning multilingual 
subjective  language  via  cross-lingual  projections.  in  proceedings  of  the 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
2007). 2007. 

217. mihalcea,  rada  and  carlo  strapparava.  the  lie  detector:  explorations  in 
the  automatic  recognition  of  deceptive  language.  in  proceedings  of  the 
acl-ijcnlp 2009 conference short papers. 2009. 

218. miller, george a., richard beckwith, christiane fellbaum, derek gross, 
and katherine miller. id138: an on-line lexical database1990: oxford 
univ. press. 

219. miller,  mahalia,  conal  sathi,  daniel  wiesenthal,  jure  leskovec,  and 
christopher  potts.  sentiment  flow  through  hyperlink  networks.  in 
proceedings of the fifth international aaai conference on weblogs and 
social media (icwsm-2011). 2011. 

220. min, hye-jin and jong c. park. detecting and blocking false sentiment 
propagation. in proceedings of the 5th international joint conference on 
natural language processing (ijcnlp-2010). 2011. 
221. mitchell, tom. machine learning1997: mcgraw hill. 
222. moghaddam,  samaneh  and  martin  ester.  ilda:  interdependent  lda 
model  for  learning  latent  aspects  and  their  ratings  from  online  product 
reviews.  in  proceedings  of  the  annual  acm  sigir  international 
conference on research and development in information retrieval (sigir-
2011). 2011. 

223. moghaddam, samaneh and martin ester. opinion digger: an unsupervised 
opinion  miner  from  unstructured  product  reviews.  in  proceeding  of  the 

156 
 

id31 and opinion mining 

acm  conference  on  information  and  knowledge  management  (cikm-
2010). 2010. 

224. moghaddam, samaneh, mohsen jamali, and martin ester. etf: extended 
tensor 
for  personalizing  prediction  of  review 
helpfulness.  in  proceedings  of  acm  international  conference  on  web 
search and data mining (wsdm-2012). 2012. 

factorization  model 

225. mohammad,  saif.  from  once  upon  a  time  to  happily  ever  after: 
tracking emotions in novels and fairy tales. in proceedings of the acl 
2011  workshop  on  language  technology  for  cultural  heritage,  social 
sciences, and humanities (latech). 2011. 

226. mohammad,  saif    and  tony  yang.  tracking  sentiment  in  mail:  how 
genders differ on emotional axes. in proceedings of the acl workshop 
on acl 2011 workshop on computational approaches to subjectivity and 
id31 (wassa-2011). 2011. 

227. mohammad,  saif,  cody  dunne,  and  bonnie  dorr.  generating  high-
coverage semantic orientation lexicons from overtly marked words and a 
thesaurus. in proceedings of the 2009 conference on empirical methods in 
natural language processing (emnlp-2009). 2009. 

228. mohammad,  saif  and graeme  hirst. distributional  measures  of  concept-
distance: a task-oriented evaluation. in proceedings of the conference on 
empirical  methods  in  natural  language  processing  (emnlp-2006). 
2006. 

229. mohammad,  saif  m.  and  peter  d.  turney.  emotions  evoked  by  common 
words and phrases: using mechanical turk to create an emotion lexicon. in 
proceedings  of  the  naacl  hlt  2010  workshop  on  computational 
approaches to analysis and generation of emotion in text. 2010. 

230. moilanen,  karo  and  stephen  pulman.  sentiment  composition. 

in 
proceedings of recent advances in natural language processing (ranlp 
2007). 2007. 

231. montague,  richard.  formal  philosophy;  selected  papers  of  richard 

montague, 1974: yale university press. 

232. mooney,  raymond  j.  and  razvan  bunescu.  mining  knowledge  from  text 
using  information  extraction.  acm  sigkdd  explorations  newsletter, 
2005. 7(1): p. 3-10. 

233. morante, roser, sarah schrauwen, and walter daelemans. corpus-based 
approaches to processing the scope of negation cues: an evaluation of the 
state of the art. in proceedings of the ninth international conference on 
computational semantics (iwcs-2011). 2011. 

234. morinaga,  satoshi,  kenji  yamanishi,  kenji  tateishi,  and  toshikazu 
fukushima.  mining  product  reputations  on  the  web.  in  proceedings  of 
acm  sigkdd  international  conference  on  knowledge  discovery  and 
data mining (kdd-2002). 2002. 

235. mukherjee,  arjun  and  bing  liu.  aspect  extraction  through  semi-
supervised modeling. in roceedings of 50th anunal meeting of association 
for  computational  linguistics  (acl-2012)  (accepted  for  publication). 
2012. 

236. mukherjee,  arjun  and  bing  liu.  modeling  review  comments.  in 
proceedings  of  50th  anunal  meeting  of  association  for  computational 
linguistics (acl-2012) (accepted for publication). 2012. 

237. mukherjee, arjun, bing liu, and natalie glance. spotting fake reviewer 
groups in consumer reviews. in proceedings of international world web 
conference (www-2012). 2012. 

238. mukherjee,  arjun,  bing  liu,  junhui  wang,  natalie  glance,  and  nitin 
jindal.  detecting  group  review  spam.  in  proceedings  of  international 
conference on world wide web (www-2011, poster paper). 2011. 

 

157 

 

  id31 and opinion mining 

239. mukund,  smruthi  and  rohini  k.  srihari.  a  vector  space  model  for 
subjectivity classification in urdu aided by co-training. in proceedings of 
coling 2010: poster volume. 2010. 

240. mullen, tony and nigel collier. id31 using support vector 
machines  with  diverse  information  sources.  in  proceedings  of  emnlp-
2004. 2004. 

241. murakami,  akiko  and  rudy    raymond.  support  or  oppose?:  classifying 
positions in online debates from reply activities and opinion expressions. in 
proceedings of coling 2010: poster volume. 2010. 

242. na,  seung-hoon,  yeha  lee,  sang-hyob  nam,  and  jong-hyeok  lee. 
improving  opinion  retrieval  based  on  query-specific  sentiment  lexicon. 
advances in information retrieval, 2009: p. 734-738. 

243. nakagawa, tetsuji, kentaro inui, and sadao kurohashi. dependency tree-
based  sentiment  classification  using  crfs  with  hidden  variables.  in 
proceedings  of  human  language  technologies:  the  2010  annual 
conference  of  the  north  american  chapter  of  the  acl  (haacl-2010). 
2010. 

244. narayanan,  ramanathan,  bing  liu,  and  alok  choudhary.  sentiment 
analysis  of  conditional  sentences.  in  proceedings  of  conference  on 
empirical  methods  in  natural  language  processing  (emnlp-2009). 
2009. 

245. nasukawa,  tetsuya  and  jeonghee  yi.  sentiment  analysis:  capturing 
favorability  using natural  language  processing.  in proceedings of  the k-
cap-03, 2nd intl. conf. on knowledge capture. 2003. 

246. neviarouskaya,  alena,  helmut  prendinger,  and  mitsuru 

ishizuka. 
compositionality  principle  in  recognition  of  fine-grained  emotions  from 
text.  in  proceedings  of  third  international  conference  on  weblogs  and 
social media (icwsm-2009). 2009. 

247. neviarouskaya,  alena,  helmut  prendinger,  and  mitsuru 

ishizuka. 
recognition of affect, judgment, and appreciation in text. in proceedings of 
the  23rd 
international  conference  on  computational  linguistics 
(coling-2010). 2010. 

248. newman, matthew l., james w. pennebaker, diane s. berry, and jane m. 
richards.  lying  words:  predicting  deception  from  linguistic  styles. 
personality and social psychology bulletin, 2003. 29(5): p. 665. 

249. ng, vincent and claire cardie. improving machine learning approaches to 
coreference  resolution.  in  proceedings  of  the  annual  meeting  of  the 
association for computational linguistics (acl-2002). 2002. 

250. ng, vincent, sajib dasgupta, and s. m. niaz arifin. examining the role of 
linguistic  knowledge  sources 
identification  and 
classification  of  reviews.  in  proceedings  of  coling/acl  2006  main 
conference poster sessions (coling/acl-2006). 2006. 

the  automatic 

251. nigam, kamal and matthew hurst. towards a robust metric of opinion. in 
proceedings  of  aaai  spring  symp.  on  exploring  attitude  and  affect  in 
text. 2004. 

252. nigam,  kamal,  andrew  k.  mccallum,  sebastian  thrun,  and  tom 
mitchell. text classification from labeled and unlabeled documents using 
em. machine learning, 2000. 39(2): p. 103-134. 

253. nishikawa, hitoshi, takaaki hasegawa, yoshihiro matsuo, and genichiro 
kikui.  opinion  summarization  with 
linear  programming 
formulation for sentence extraction and ordering. in proceedings of coling 
2010: poster volume. 2010a. 

254. nishikawa, hitoshi, takaaki hasegawa, yoshihiro matsuo, and genichiro 
for  sentiment 

informativeness  and  readability 

kikui.  optimizing 

integer 

in 

158 
 

id31 and opinion mining 

summarization.  in  proceedings  of  annual  meeting  of  the  association  for 
computational linguistics (acl-2010). 2010b. 

255. o'connor, brendan, ramnath balasubramanyan, bryan r. routledge, and 
noah a.  smith. from tweets to polls: linking text sentiment to public 
opinion time series. in proceedings of the international aaai conference 
on weblogs and social media (icwsm 2010). 2010. 

256. o'mahony, michael p. and barry smyth. learning to recommend helpful 
third  acm  conference  on 

in  proceedings  of 

the 

hotel  reviews. 
recommender systems. 2009. 

257. ott,  myle,  yejin  choi,  claire  cardie,  and  jeffrey  t.  hancock.  finding 
deceptive opinion spam by any stretch of the imagination. in proceedings 
of  the  49th  annual  meeting  of  the  association  for  computational 
linguistics (acl-2011). 2011. 

258. ounis, iadh, craig macdonald, maarten de rijke, gilad mishne, and ian 
soboroff.  overview  of  the  trec-2006  blog  track.  in  proceedings  of  the 
fifteenth text retrieval conference (trec-2006). 2006. 

259. ounis, iadh, craig macdonald, and ian soboroff. overview of the trec-
2008 blog track. in in proceedings of the 16th text retrieval conference 
(trec-2008). 2008. 

260. page, lawrence, sergey brin, rajeev motwani, and terry winograd. the 

id95 citation ranking: bringing order to the web. 1999. 

261. paltoglou,  georgios  and  mike  thelwall.  a  study  of  information  retrieval 
weighting  schemes  for  sentiment  analysis.  in  proceedings  of  the  48th 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
2010). 2010. 

262. pan,  sinno  jialin,  xiaochuan  ni,  jian-tao  sun,  qiang  yang,  and  zheng 
chen.  cross-domain  sentiment  classification  via  spectral 
feature 
alignment. in proceedings of international conference on world wide web 
(www-2010). 2010. 

263. pang,  bo  and  lillian  lee.  opinion  mining  and  sentiment  analysis. 

foundations and trends in information retrieval, 2008. 2(1-2): p. 1-135. 

264. pang, bo and lillian lee. seeing stars: exploiting class relationships for 
sentiment  categorization  with  respect  to  rating  scales.  in  proceedings  of 
meeting  of  the  association  for  computational  linguistics  (acl-2005). 
2005. 

265. pang,  bo  and  lillian  lee.  a  sentimental  education:  sentiment  analysis 
using subjectivity summarization based on minimum cuts. in proceedings 
of meeting of the association for computational linguistics (acl-2004). 
2004. 

266. pang, bo and lillian lee. using very simple statistics for review search: 
international  conference  on 

an  exploration. 
computational linguistics, poster paper (coling-2008). 2008. 

in  proceedings  of 

267. pang,  bo,  lillian  lee,  and  shivakumar  vaithyanathan.  thumbs  up?: 
sentiment classification using machine learning techniques. in proceedings 
of  conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2002). 2002. 

268. pantel, patrick, eric crestan, arkady borkovsky, ana-maria popescu, and 
vishnu vyas. web-scale distributional similarity and entity set expansion. 
in proceedings of conference on empirical methods in natural language 
processing (emnlp-2009). 2009. 

269. park,  do-hyung,  jumin  lee,  and  ingoo  han.  the  effect  of  on-line 
consumer reviews on consumer purchasing intention: the moderating role 
of involvement. international journal of electronic commerce, 2007. 11(4): 
p. 125-148. 

 

159 

 

  id31 and opinion mining 

270. park, souneil, kyungsoon lee, and junehwa song. contrasting opposing 
views  of  news  articles  on  contentious  issues.  in  proceedings  of  the  49th 
annual  meeting  of  the  association  for  computational  linguistics  (acl-
2011). 2011. 

271. parrott,  w.  gerrod.  emotions 

in 

social  psychology:  essential 

readings2001: psychology pr. 

lexicon 

272. paul,  michael  j.,  chengxiang  zhai,  and  roxana  girju.  summarizing 
contrastive viewpoints in opinionated text. in proceedings of conference 
on  empirical  methods  in  natural  language  processing  (emnlp-2010). 
2010. 

273. peng, wei and dae hoon park. generate adjective sentiment dictionary 
for  social  media  sentiment  analysis  using  constrained  nonnegative 
matrix  factorization.  in  proceedings  of  the  fifth  international  aaai 
conference on weblogs and social media (icwsm-2011). 2011. 

274. pennebaker,  james  w.,  cindy  k.  chung,  molly  ireland,  amy  gonzales, 
and  roger  j.  booth.  the  development  and  psychometric  properties  of 
liwc2007. www.liwc.net, 2007. 

275. polanyi,  livia  and  annie  zaenen.  contextual  valence  shifters.  in 
proceedings  of  the  aaai  spring  symposium  on  exploring  attitude  and 
affect in text. 2004. 

276. popescu,  ana-maria  and  oren  etzioni.  extracting  product  features  and 
opinions  from  reviews.  in  proceedings  of  conference  on  empirical 
methods in natural language processing (emnlp-2005). 2005. 

277. qiu,  guang,  bing  liu,  jiajun  bu,  and  chun  chen.  expanding  domain 
sentiment 
in  proceedings  of 
international  joint  conference  on  artificial  intelligence  (ijcai-2009). 
2009. 

through  double  propagation. 

278. qiu,  guang,  bing  liu,  jiajun  bu,  and  chun  chen.  opinion  word 
through  double  propagation. 

expansion  and  target  extraction 
computational linguistics, vol. 37, no. 1: 9.27, 2011. 

279. qiu,  likun,  weish  zhang,  changjian  hu,  and  kai  zhao.  selc:  a  self-
supervised  model  for  sentiment  classification.  in  proceeding  of  the  18th 
acm  conference  on  information  and  knowledge  management  (cikm-
2009). 2009. 

280. qu, lizhen, georgiana ifrim, and gerhard weikum. the bag-of-opinions 
method  for  review  rating  prediction  from  sparse  text  patterns.  in 
proceedings of the international conference on computational linguistics 
(coling-2010). 2010. 

281. quirk, randolph, sidney greenbaum, geoffrey leech, and jan svartvik. a 
comprehensive  grammar  of  the  english  language.  vol.  397.  1985: 
cambridge univ press. 

282. raaijmakers,  stephan  and  wessel  kraaij.  a  shallow  approach  to 
subjectivity classification, in proceedings of icwsm-2008, 2008. p. 216-
217. 

283. raaijmakers,  stephan,  khiet  truong,  and  theresa  wilson.  multimodal 
subjectivity  analysis  of  multiparty  conversation.  in  proceedings  of 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2008). 2008. 

284. rabiner,  lawrence  r.  a  tutorial  on  hidden  markov  models  and  selected 
applications in id103. proceedings of the ieee, 1989. 77(2): 
p. 257-286. 

285. radev,  dragomir  r.,  simone  teufel,  horacio  saggion,  wai  lam,  john 
blitzer, hong qi, arda celebi, danyu liu, and elliott drabek. evaluation 
challenges in large-scale document summarization. in proceedings of the 

160 
 

id31 and opinion mining 

annual  meeting  of  the  association  for  computational  linguistics  (acl-
2003). 2003. 

286. rao,  delip  and  deepak  ravichandran.  semi-supervised  polarity  lexicon 
induction. in proceedings of the 12th conference of the european chapter 
of the acl (eacl-2009). 2009. 

287. ravichandran,  deepak  and  eduard  hovy.  learning  surface  text  patterns 
for a id53 system. in proceedings of the annual meeting of 
the association for computational linguistics (acl-2002). 2002. 

288. riloff,  ellen.  automatically  constructing  a  dictionary  for  information 

extraction tasks. in processing of aaai-2003. 1993. 

289. riloff, ellen. automatically generating extraction patterns from untagged 

text. in proceedings of aaai-1996. 1996. 

290. riloff,  ellen,  siddharth  patwardhan,  and  janyce  wiebe.  feature 
subsumption  for  opinion  analysis.  in  proceedings  of  the  conference  on 
empirical  methods  in  natural  language  processing  (emnlp-2006). 
2006. 

291. riloff, ellen and janyce wiebe. learning extraction patterns for subjective 
expressions.  in  proceedings  of  conference  on  empirical  methods  in 
natural language processing (emnlp-2003). 2003. 

292. ruppenhofer,  josef,  swapna  somasundaran,  and  janyce  wiebe.  finding 
the sources and targets of subjective expressions. in proceedings of lrec. 
2008. 

293. sadikov,  eldar,  aditya  parameswaran,  and  petros  venetis.  blogs  as 
predictors  of  movie  success.  in  proceedings  of  the  third  international 
conference on weblogs and social media (icwsm-2009). 2009. 

294. sakunkoo,  patty  and  nathan  sakunkoo.  analysis  of  social  influence  in 
online  book  reviews.  in  proceedings  of  third  international  aaai 
conference on weblogs and social media (icwsm-2009). 2009. 

295. santorini,  beatrice.  part-of-speech  tagging  guidelines  for  the  penn 
treebank  project,  1990:  university  of  pennsylvania,  school  of 
engineering  and  applied  science,  dept.  of  computer  and  information 
science. 

296. sarawagi,  sunita.  information  extraction.  foundations  and  trends  in 

databases, 2008. 1(3): p. 261-377. 

297. sauper,  christina,  aria  haghighi,  and  regina  barzilay.  content  models 
with attitude. in proceedings of the 49th annual meeting of the association 
for computational linguistics (acl-2011). 2011. 

298. scaffidi,  christopher,  kevin  bierhoff,  eric  chang,  mikhael  felker, 
herman  ng,  and  chun  jin.  red  opal:  product-feature  scoring  from 
reviews.  in  proceedings  of  twelfth  acm  conference  on  electronic 
commerce (ec-2007). 2007. 

299. schapire,  robert  e.  and  yoram  singer.  boostexter:  a  boosting-based 

system for text categorization. machine learning, 2000. 39(2): p. 135-168. 
300. seki,  yohei,  koji  eguchi,  noriko  kando,  and  masaki  aono.  opinion-
focused  summarization  and  its  analysis  at  duc  2006.  in  proceedings  of 
the document understanding conference (duc). 2006. 

301. shanahan, james g., yan qu, and janyce wiebe. computing attitude and 
affect in text: theory and applications. vol. 20. 2006: springer-verlag new 
york inc. 

302. shawe-taylor, john and nello cristianini. support vector machines, 2000, 

cambridge university press. 

303. snyder, benjamin and regina barzilay. multiple aspect ranking using the 
good  grief  algorithm.  in  proceedings  of  the  conference  of  the  north 
american  chapter  of  the  association  for  computational  linguistics: 
human language technologies (naacl/hlt-2007). 2007. 

 

161 

 

  id31 and opinion mining 

304. socher, r., j. pennington, e. h. huang, a.y. ng, and c.d. manning. semi-
supervised recursive autoencoders for predicting sentiment distributions. 
in  proceedings  of  the  conference  on  empirical  methods  in  natural 
language processing (emnlp-2011). 2011. 

305. somasundaran, s., j. ruppenhofer, and j. wiebe. discourse level opinion 
relations:  an  annotation  study.  in  proceedings  of  the  9th  sigdial 
workshop on discourse and dialogue. 2008. 

306. somasundaran, swapna, galileo namata, lise getoor, and janyce wiebe. 
opinion graphs for polarity and discourse classification. in proceedings of 
the  2009  workshop  on  graph-based  methods  for  natural  language 
processing. 2009. 

307. somasundaran, swapna and janyce wiebe. recognizing stances in online 
debates. in proceedings of the 47th annual meeting of the acl and the 4th 
ijcnlp of the afnlp (acl-ijcnlp-2009). 2009. 

308. steyvers,  mark    and  thomas  l.  griffiths.  probabilistic  topic  models. 

handbook of latent semantic analysis, 2007. 427(7): p. 424-440. 

309. stone,  philip.  the  general  inquirer:  a  computer  approach  to  content 

analysis. journal of regional science, 1968. 8(1). 

310. stoyanov,  veselin  and  claire  cardie.  partially  supervised  coreference 
resolution for opinion summarization through structured rule learning. in 
proceedings  of  conference  on  empirical  methods  in  natural  language 
processing (emnlp-2006). 2006. 

311. stoyanov, veselin and claire cardie. topic identification for fine-grained 
opinion  analysis.  in  proceedings  of  the  international  conference  on 
computational linguistics (coling-2008). 2008. 

312. strapparava, carlo and alessandro valitutti. id138-affect: an affective 
extension of id138. in proceedings of the international conference on 
language resources and evaluation. 2004. 

313. su, fangzhong and katja markert. from words to senses: a case study of 
the  22nd  international 

subjectivity  recognition. 
conference on computational linguistics (coling-2008). 2008. 

in  proceedings  of 

314. su,  fangzhong  and  katja  markert.  word  sense  subjectivity  for  cross-
lingual 
in  proceedings  of  human  language 
technologies:  the  2010  annual  conference  of  the  north  american 
chapter of the acl (hlt-naacl-2010). 2010. 

lexical  substitution. 

315. su, qi, xinying xu, honglei guo, zhili guo, xian wu, xiaoxun zhang, 
bin  swen,  and  zhong  su.  hidden  sentiment  association  in  chinese  web 
opinion  mining.  in  proceedings  of  international  conference  on  world 
wide web (www-2008). 2008. 

316. taboada,  maite,  julian  brooke,  milan  tofiloski,  kimberly  voll,  and 
sentiment  analysis. 

manfred  stede.  lexicon-based  methods 
for 
computational linguistics, 2011. 37(2): p. 267-307. 

317. t  ckstr  m,  oscar  and  ryan  mcdonald.  discovering 

fine-grained 
sentiment  with  latent  variable  structured  prediction  models.  advances  in 
information retrieval, 2011: p. 368-374. 

318. t  ckstr  m,  oscar  and  ryan  mcdonald.  semi-supervised  latent  variable 
models  for  sentence-level  sentiment  analysis.  in  proceedings  of  the  49th 
annual  meeting 
computational 
linguistics:shortpapers (acl-2011). 2011. 

association 

319. takamura,  hiroya,  takashi  inui,  and  manabu  okumura.  extracting 
semantic  orientations  of  phrases  from  dictionary.  in  proceedings  of  the 
joint  human language technology/north  american  chapter of  the  acl 
conference (hlt-naacl-2007). 2007. 

320. takamura,  hiroya,  takashi  inui,  and  manabu  okumura.  extracting 
semantic  orientations  of  words  using  spin  model.  in  proceedings  of  the 

the 

for 

of 

162 
 

id31 and opinion mining 

annual  meeting  of  the  association  for  computational  linguistics  (acl-
2005). 2005. 

321. takamura,  hiroya,  takashi  inui,  and  manabu  okumura.  latent  variable 
models  for  semantic  orientations  of  phrases.  in  proceedings  of  the 
conference of the european chapter of the association for computational 
linguistics (eacl-2006). 2006. 

322. tan,  songbo,  gaowei  wu,  huifeng  tang,  and  xueqi  cheng.  a  novel 
scheme for domain-transfer problem in the context of id31. in 
proceeding  of  the  acm  conference  on  information  and  knowledge 
management (cikm-2007). 2007. 

323. tata,  swati  and  barbara  di eugenio. generating  fine-grained  reviews  of 
songs  from  album  reviews.  in  proceedings  of  annual  meeting  of  the 
association for computational linguistics (acl-2010). 2010. 

324. tesniere, l.   lements de syntaxe structurale: pr  f. de jean fourquet1959: 

c. klincksieck. 

325. titov, ivan and ryan mcdonald. a joint model of text and aspect ratings 
for  sentiment  summarization.  in  proceedings  of  annual  meeting  of  the 
association for computational linguistics (acl-2008). 2008. 

326. titov, ivan and ryan mcdonald. modeling online reviews with multi-grain 
topic models. in proceedings of international conference on world wide 
web (www-2008). 2008. 

327. tokuhisa,  ryoko,  kentaro 

inui,  and  yuji  matsumoto.  emotion 
classification  using  massive  examples  extracted  from  the  web.  in 
proceedings  of  the  22nd  international  conference  on  computational 
linguistics (coling-2008). 2008. 

328. tong,  richard  m.  an  operational  system  for  detecting  and  tracking 
opinions  in  on-line  discussion.  in  proceedings  of  sigir  workshop  on 
operational text classification. 2001. 

329. toprak,  cigdem,  niklas  jakob,  and  iryna  gurevych.  sentence  and 
expression  level  annotation  of  opinions  in  user-generated  discourse.  in 
proceedings  of 
for 
computational linguistics (acl-2010). 2010. 

the  48th  annual  meeting  of 

the  association 

330. tsaparas, panayiotis, alexandros ntoulas, and evimaria terzi. selecting a 
comprehensive  set  of  reviews.  in  proceedings  of  the  acm  sigkdd 
conference on knowledge discovery and data mining (kdd-2011). 2011. 
331. tsur, oren, dmitry davidov, and ari rappoport. a great catchy name: 
semi-supervised  recognition  of  sarcastic  sentences  in  online  product 
reviews. in proceedings of the fourth international aaai conference on 
weblogs and social media (icwsm-2010). 2010. 

332. tsur, oren and ari rappoport. revrank: a fully unsupervised algorithm for 
selecting the most helpful book reviews. in proceedings of the international 
aaai conference on weblogs and social media (icwsm-2009). 2009. 

333. tumasjan, andranik, timm o. sprenger, philipp g. sandner, and isabell 
m. welpe.  predicting  elections  with  twitter:  what  140  characters  reveal 
about political sentiment. in roceedings of the international conference on 
weblogs and social media (icwsm-2010). 2010. 

334. turney,  peter  d.  thumbs  up  or  thumbs  down?:  semantic  orientation 
applied to unsupervised classification of reviews. in proceedings of annual 
meeting  of  the  association  for  computational  linguistics  (acl-2002). 
2002. 

335. turney, peter d. and micharel l. littman. measuring praise and criticism: 
id136 of semantic orientation from association. acm transactions on 
information systems, 2003. 

 

163 

 

  id31 and opinion mining 

336. utsumi,  akira.  verbal  irony  as  implicit  display  of  ironic  environment: 
distinguishing  ironic  utterances  from  nonirony.  journal  of  pragmatics, 
2000. 32(12): p. 1777-1806. 

337. valitutti,  alessandro,  carlo  strapparava,  and  oliviero  stock.  developing 

affective lexical resources. psychnology journal, 2004. 2(1): p. 61-83. 

338. velikovich,  leonid,  sasha  blair-goldensohn,  kerry  hannan,  and  ryan 
mcdonald. the viability of web-derived polarity lexicons. in proceedings 
of  annual  conference  of  the  north  american  chapter  of  the  association 
for computational linguistics (haacl-2010). 2010. 

339. vrij,  aldert.  detecting  lies  and  deceit:  pitfalls  and  opportunities,  2008: 

wiley-interscience. 

340. wan,  xiaojun.  co-training  for  cross-lingual  sentiment  classification.  in 
proceedings of the 47th annual meeting of the acl and the 4th ijcnlp of 
the afnlp (acl-ijcnlp-2009). 2009. 

341. wan,  xiaojun.  using  bilingual  knowledge  and  ensemble  techniques  for 
unsupervised chinese id31. in proceedings of conference on 
empirical  methods  in  natural  language  processing  (emnlp-2008). 
2008. 

342. wang,  dong  and  yang  liu.  a  pilot  study  of  opinion  summarization  in 
conversations.  in  proceedings  of  the  49th  annual  meeting  of  the 
association for computational linguistics (acl-2011). 2011. 

343. wang, guan, sihong xie, bing liu, and philip s. yu. identify online store 
review  spammers  via  social  review  graph.  acm  transactions  on 
intelligent systems and technology, accepted for publication, 2011. 

344. wang,  hongning,  yue  lu,  and  chengxiang  zhai.  latent  aspect  rating 
analysis on review text data: a rating regression approach. in proceedings 
of acm sigkdd international conference on knowledge discovery and 
data mining (kdd-2010). 2010. 

345. wang, tong and graeme hirst. refining the notions of depth and density 
in  id138-based  semantic  similarity  measures.  in  proceedings  of  the 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2011). 2011. 

346. wang,  xiaolong,  furu  wei,  xiaohua  liu,  ming  zhou,  and  ming  zhang. 
topic  sentiment  analysis  in  twitter:  a  graph-based  hashtag  sentiment 
classification  approach.  in  proceeding  of  the  acm  conference  on 
information and knowledge management (cikm-2011). 2011. 

347. wei, bin and christopher pal. cross lingual adaptation: an experiment on 
sentiment  classifications.  in  proceedings  of  the  acl  2010  conference 
short papers (acl-2010). 2010. 

348. wei, wei and jon atle gulla. sentiment learning on product reviews via 
sentiment  ontology  tree.  in  proceedings  of  annual  meeting  of  the 
association for computational linguistics (acl-2010). 2010. 

349. wen, miaomiao and yunfang  wu. mining the sentiment expectation of 
the  5th 
nouns  using  id64  method. 
international joint conference on natural language processing (ijcnlp-
2010). 2011. 

in  proceedings  of 

350. wiebe,  janyce.  identifying  subjective  characters 

in 
proceedings of the international conference on computational linguistics 
(coling-1990). 1990. 

in  narrative. 

351. wiebe,  janyce.  learning  subjective  adjectives 

in 
proceedings  of  national  conf.  on  artificial  intelligence  (aaai-2000). 
2000. 

from  corpora. 

352. wiebe,  janyce.  tracking  point  of  view  in  narrative.  computational 

linguistics, 1994. 20: p. 233   287. 

164 
 

id31 and opinion mining 

353. wiebe,  janyce    and  rada  mihalcea.  word  sense  and  subjectivity.  in 
proceedings of intl. conf. on computational linguistics and 44th annual 
meeting of the acl (coling/acl-2006). 2006. 

354. wiebe,  janyce,  rebecca  f.  bruce,  and  thomas  p.  o'hara.  development 
and  use  of  a  gold-standard  data  set  for  subjectivity  classifications.  in 
proceedings of the association for computational linguistics (acl-1999). 
1999. 

355. wiebe, janyce and ellen riloff. creating subjective and objective sentence 
texts.  computational  linguistics  and 

from  unannotated 

classifiers 
intelligent text processing, 2005: p. 486-497. 

356. wiebe,  janyce,  theresa  wilson,  rebecca  f.  bruce,  matthew  bell,  and 
melanie martin. learning subjective language. computational linguistics, 
2004. 30(3): p. 277-308. 

357. wiebe, janyce, theresa wilson, and claire cardie. annotating expressions 
language.  language  resources  and 

of  opinions  and  emotions 
evaluation, 2005. 39(2): p. 165-210. 

in 

358. wiegand,  m.  and  d.  klakow.  convolution  kernels  for  opinion  holder 
extraction.  in  proceedings  of  human  language  technologies:  the  2010 
annual conference of the north american chapter of the acl (haacl-
2010). 2010. 

359. williams, gbolahan k. and sarabjot singh anand. predicting the polarity 
strength  of  adjectives  using  id138.  in  proceedings  of  the  third 
international  aaai  conference  on  weblogs  and  social  media  (icwsm-
2009). 2009. 

360. wilson,  theresa  and  stephan  raaijmakers.  comparing  word,  character, 
and phoneme id165s for subjective utterance recognition. in proceedings 
of interspeech. 2008. 

361. wilson,  theresa,  janyce  wiebe,  and  paul  hoffmann.  recognizing 
contextual  polarity  in  phrase-level  sentiment  analysis.  in  proceedings  of 
the  human  language  technology  conference  and  the  conference  on 
empirical methods in natural language processing (hlt/emnlp-2005). 
2005. 

362. wilson, theresa, janyce wiebe, and rebecca hwa. just how mad are you? 
finding  strong  and  weak  opinion  clauses.  in  proceedings  of  national 
conference on artificial intelligence (aaai-2004). 2004. 

363. wilson,  theresa,  janyce  wiebe,  and  rebecca  hwa.  recognizing  strong 
and weak opinion clauses. computational intelligence, 2006. 22(2): p. 73-
99. 

364. wu,  guangyu,  derek  greene,  barry  smyth,  and  p  draig  cunningham. 
distortion  as  a  validation  criterion  in  the  identification  of  suspicious 
reviews. in proceedings of social media analytics. 2010. 

365. wu,  qion,  songbo  tan,  and  xueqi  cheng.  graph  ranking  for  sentiment 
transfer.  in  proceedings  of  the  acl-ijcnlp  2009  conference  short 
papers (acl-ijcnlp-2009). 2009. 

366. wu,  yuanbin,  qi  zhang,  xuanjing  huang,  and  lide  wu.  phrase 
id33 for opinion mining. in proceedings of conference on 
empirical  methods  in  natural  language  processing  (emnlp-2009). 
2009. 

367. wu,  yuanbin,  qi  zhang,  xuanjing  huang,  and  lide  wu.  structural 
opinion mining for graph-based sentiment representation. in proceedings 
of  the  2011  conference  on  empirical  methods  in  natural  language 
processing (emnlp-2011). 2011. 

368. wu,  yunfang  and  miaomiao  wen.  disambiguating  dynamic  sentiment 
ambiguous adjectives. in proceedings of the 23rd international conference 
on computational linguistics (coling 2010). 2010. 

 

165 

 

  id31 and opinion mining 

369. xia, rui and chengqing zong. exploring the use of word relation features 
for  sentiment  classification.  in  proceedings  of  coling  2010:  poster 
volume. 2010. 

370. xia,  rui  and  chengqing  zong.  a  pos-based  ensemble  model  for  cross-
domain  sentiment  classification.  in  proceedings  of  the  5th  international 
joint conference on natural language processing (ijcnlp-2010). 2011. 
371. xu, g., x. meng, and h. wang. build chinese emotion lexicons using a 
graph-based algorithm and multiple resources. in proceedings of the 23rd 
international  conference  on  computational  linguistics  (coling  2010). 
2010. 

372. yang,  hui,  luo  si,  and  jamie  callan.  knowledge  transfer  and  opinion 

detection in the trec2006 blog track. in proceedings of trec. 2006. 

373. yang,  seon  and  youngjoong  ko.  extracting  comparative  entities  and 
predicates from texts using comparative type classification. in proceedings 
of  the  49th  annual  meeting  of  the  association  for  computational 
linguistics (acl-2011). 2011. 

374. yano, tae and noah a. smith. what's worthy of comment? content and 
comment  volume  in  political  blogs.  in  proceedings  of  the  international 
aaai conference on weblogs and social media (icwsm 2010). 2010. 

375. yatani,  koji,  michael  novati,  andrew  trusty,  and  khai  n.  truong. 
analysis  of  adjective-noun  word  pair  extraction  methods  for  online 
review summarization. in proceedings of international joint conference 
on artificial intelligence (ijcai-2011). 2011. 

376. yessenalina, ainur and claire cardie. compositional matrix-space models 
for  sentiment  analysis.  in  proceedings  of  the  conference  on  empirical 
methods in natural language processing (emnlp-2011). 2011. 

377. yessenalina,  ainur,  yejin  choi,  and  claire  cardie.  automatically 
generating  annotator  rationales  to  improve  sentiment  classification.  in 
proceedings of the acl 2010 conference short papers. 2010. 

378. yessenalina, ainur, yison yue, and claire cardie. multi-level structured 
models  for  document-level  sentiment  classification.  in  proceedings  of 
conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2010). 2010. 

379. yi,  jeonghee,  tetsuya  nasukawa,  razvan  bunescu,  and  wayne  niblack. 
sentiment  analyzer:  extracting  sentiments  about  a  given  topic  using 
natural 
in  proceedings  of  ieee 
international conference on data mining (icdm-2003). 2003. 

language  processing 

techniques. 

380. yoshida, yasuhisa, tsutomu hirao, tomoharu iwata, masaaki nagata, and 
yuji  matsumoto.  transfer  learning  for  multiple-domain  sentiment 
analysis   identifying  domain  dependent/independent  word  polarity.  in 
proceedings of the twenty-fifth aaai conference on artificial intelligence 
(aaai-2011). 2011. 

381. yu,  hong  and  vasileios  hatzivassiloglou.  towards  answering  opinion 
questions:  separating  facts  from  opinions  and  identifying  the  polarity  of 
opinion sentences. in proceedings of conference on empirical methods in 
natural language processing (emnlp-2003). 2003. 

382. yu,  jianxing,  zheng-jun  zha,  meng  wang,  and  tat-seng  chua.  aspect 
ranking:  identifying  important  product  aspects  from  online  consumer 
reviews. in proceedings of the 49th annual meeting of the association for 
computational linguistics. 2011. 

383. yu, jianxing, zheng-jun zha, meng wang, kai wang, and tat-seng chua. 
domain-assisted  product  aspect  hierarchy  generation:  towards 
hierarchical  organization  of  unstructured  consumer  reviews. 
in 
proceedings of the conference on empirical methods in natural language 
processing (emnlp-2011). 2011. 

166 
 

id31 and opinion mining 

384. zhai,  zhongwu,  bing  liu,  hua  xu,  and  peifa  jia.  id91  product 
features  for  opinion  mining.  in  proceedings  of  acm  international 
conference on web search and data mining (wsdm-2011). 2011. 

385. zhai,  zhongwu,  bing  liu,  hua  xu,  and  peifa  jia.  constrained  lda  for 
grouping  product  features  in  opinion  mining.  in  proceedings  of 
pakdd-2011. 2011. 

386. zhai,  zhongwu,  bing  liu,  hua  xu,  and  peifa  jia.  grouping  product 
features  using  semi-supervised  learning  with  soft-constraints.  in 
proceedings  of  international  conference  on  computational  linguistics 
(coling-2010). 2010. 

387. zhai, zhongwu, bing liu, lei zhang, hua xu, and peifa jia. identifying 

evaluative opinions in online discussions. in proceedings of aaai. 2011. 

388. zhang,  lei  and  bing  liu.  extracting  resource  terms  for  sentiment 

analysis. in proceedings of ijcnlp-2011. 2011a. 

389. zhang,  lei  and  bing  liu.  identifying  noun  product  features  that  imply 
opinions.  in  proceedings  of    the  annual  meeting  of  the  association  for 
computational linguistics (short paper) (acl-2011). 2011b. 

390. zhang,  lei,  bing  liu,  suk  hwan  lim,  and  eamonn  o   brien-strain. 
extracting  and  ranking  product  features  in  opinion  documents.  in 
proceedings  of  international  conference  on  computational  linguistics 
(coling-2010). 2010. 

391. zhang, min and xingyao ye. a generation model to unify topic relevance 
and  lexicon-based  sentiment  for  opinion  retrieval.  in  proceedings  of  the 
annual  acm  sigir 
international  conference  on  research  and 
development in information retrieval (sigir-2008). 2008. 

392. zhang,  wei,  lifeng  jia,  clement  yu,  and  weiyi  meng.  improve  the 
effectiveness of the opinion retrieval and opinion polarity classification. in 
proceedings  of  acm  international  conference  on  information  and 
knowledge management (cikm-2008). 2008. 

393. zhang, wei and clement yu. uic at trec 2007 blog report, 2007. 
394. zhang, wenbin and steven skiena. trading strategies to exploit blog and 
news  sentiment.  in  proceedings  of  the  international  conference  on 
weblogs and social media (icwsm-2010). 2010. 

395. zhang, zhu and balaji varadarajan. utility scoring of product reviews. in 
proceedings  of  acm  international  conference  on  information  and 
knowledge management (cikm-2006). 2006. 

396. zhao,  wayne  xin,  jing  jiang,  hongfei  yan,  and  xiaoming  li.  jointly 
modeling aspects and opinions with a maxent-lda hybrid. in proceedings 
of  conference  on  empirical  methods  in  natural  language  processing 
(emnlp-2010). 2010. 

397. zhou, lanjun, binyang li, wei gao, zhongyu wei, and kam-fai wong. 
unsupervised  discovery  of  discourse  relations  for  eliminating  intra-
sentence  polarity  ambiguities.  in  proceedings  of  the  conference  on 
empirical  methods  in  natural  language  processing  (emnlp-2011). 
2011. 

398. zhou,  lina,  yongmei  shi,  and  dongsong  zhang.  a  statistical  language 
modeling approach to online deception detection. ieee transactions on 
knowledge and data engineering, 2008: p. 1077-1081. 

399. zhou,  shusen,  qingcai  chen,  and xiaolong wang. active  deep  networks 
for  semi-supervised  sentiment  classification.  in  proceedings  of  coling 
2010: poster volume. 2010. 

400. zhu, jingbo, huizhen wang, benjamin k. tsou, and muhua zhu. multi-
aspect  opinion  polling  from  textual  reviews.  in  proceedings  of  acm 
international  conference  on  information  and  knowledge  management 
(cikm-2009). 2009. 

 

167 

 

  id31 and opinion mining 

401. zhu,  xiaojin  and  zoubin  ghahramani.  learning  from  labeled  and 
unlabeled  data  with  label  propagation.  school  comput.  sci.,  carnegie 
mellon univ., pittsburgh, pa, tech. rep. cmu-cald-02-107, 2002. 

402. zhuang,  li,  feng  jing,  and  xiaoyan  zhu.  movie  review  mining  and 
summarization.  in  proceedings  of  acm  international  conference  on 
information and knowledge management (cikm-2006). 2006. 

403. zirn,  c  cilia,  mathias  niepert,  heiner  stuckenschmidt,  and  michael 
strube.  fine-grained  sentiment  analysis  with  structural  features.  in 
proceedings  of  the  5th  international  joint  conference  on  natural 
language processing (ijcnlp-2011). 2011. 

 
 

168 
 

