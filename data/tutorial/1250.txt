spectral algorithms for latent variable models 

part iii: latent tree models 

 
 
 
 
 
 
 

le song 

icml 2012 tutorial on spectral algorithms for latent 

variable models, edinburgh, uk 

 

joint work with mariya ishteva, ankur parikh, eric xing, byron boots , geoff gordon, alex 

smola and kenji fukumizu 

 

latent tree id114 

graphical model: nodes represent variables, edges represent 
conditional independence relation 
 
latent tree id114: latent and observed variables are 
arranged in a tree structure 
 
 
 
 
 
    1 
 
many real world applications, eg., time-series prediction, topic 
modeling 
 

hidden markov model 

observed variable 

latent variable 

latent tree 

    11 

    10 

    10 

    12 

    6 

    8 

    7 

    7 

    2 

    3 

    4 

    5 

    8 

    2 

    3 

    6 

    5 

    4 

    9 

    1 

    9 

2 

scope of this tutorial 

estimating marginal id203 of the observed variables 

spectral id48s (hsu et al. colt   09)  
kernel spectral id48s (song et al. icml   10) 
spectral latent tree (parikh et al. icml   11, song et al. nips   11) 
spectral dimensional reduction for id48s (foster et al. arxiv) 
more recent: cohen et al. acl   12, balle et al. icml   12  

 
estimating latent parameters 

pca approach (mossel & roch aoap   06) 
pca and svd approach, (anandkumar et al. colt   12, arxiv) 

 
estimating the structure of latent variable models 

recursive grouping (choi et al. jmlr   11) 
spectral short quartet (anandkumar et al. nips   11) 

3 

challenge of estimating marginal of observed variables  

exponential number of entries in          1,     2,     ,     6  

discrete variable taking      possible values,      has     (    6) entries! 
 

latent tree reduces the number of parameters  

    7,    8,    9,    10

    7,    8,    9,    10

 
         1,     2,     ,     6 =   
 
 
 
 
 
 
    2 

 

    10 

    4 

    5 

    8 

    3 

    6 

    9 

    7 

    1 

 

 

 

 

         1,     2,     ,     6,     7,     ,     10   

            
params 

         10  
         7     10          1     7          2     7  
         8     10          3     8     (    4|    8) 
         9     10          5     9     (    6|    9) 

     3    2   
params 

latent tree has      9    2  params 
significant saving! 

4 

em algorithm for parameter estimation 

    7 

    10 

do not observe latent variables, need to estimate the 
corresponding parameters, eg.,     (    7|    10) and          1     7  
 
 
 
 
 
 
 
expectation maximization: maximize likelihood of observations 

goal of spectral algorithm: 
estimate the marginal in 
local-minimum-free fashion 

     =      

     = 1 

     
    5

     
    6

     
    1

     
    3

     
    2

     
    4

1 
    5

1 
    1

1 
    3

1 
    6

1 
    2

1 
    4

   

   

   

    3 

    4 

    2 

    5 

    6 

    8 

    9 

    1 

 

 

 

max       (    1

    
    =1

     ,     ,     6
     )

  

drawback: local maxima, slow to converge, difficult to analyze 

5 

key features of spectral algorithms 

represent joint id203 table of observed variables with 
low rank factorization, without using the joint table in the 
computation! 
 
eg.      1,   ,     ;     +1 ,   ,2     =                            (         1,     ,     2     , 1,     ,      ) 
 

         

    represent it by low rank factors 

         

     1,   ,     ;     +1 ,   ,2      

to avoid exponential blowup 

    use clever decomposition 
technique to avoid directly 
using all entries from the table 

    use singular value decomposition  

6 

tensor view of marginal id203 

marginal id203 table      =          1,     2,     ,     6  

discrete variable taking      possible values  1,     ,       

6-way table, or 6th order tensor 
dimension labeled by the variable 

 
value of the variable is the index to the corresponding 
dimension, need 6 indexes to access a single entry 

    (    1 = 1,     2 = 4,     ,     6 = 3) is the entry     [1,4,     , 3] 
 

running examples: 

    10 

    8 

latent tree 

    9 

    7 

    1 

    2 

    3 

    4 

    5 

    6 

    7 

    8 

    9 

    10 

    11 

    12 

    1 

    2 

    3 

    4 

    5 

    6 

hidden  
markov  
model 

7 

reshaping tensor into matrices 

     =                                 ,      : multi-index      mapped into row index, and 
the remaining indexes into column index  

eg.      =          1,     2,     3 , a 3rd order tensor and      = 3 
     2 ;{1,3} =                                 , {2}  turns the dimension of     2 into row 

    3 

    1 

    2 

     

    1 

slice at dimension of     3      2 

    3 = 1 

    3 = 2 

    3 = 3 

    1 

       = 

    2 

8 

reshaping 6th order tensor 

     =      1,2,3 ;{4,5,6} =                            (         1,     ,     6 , 1,2,3 ) 

    6 
    5 
    4 

1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 

1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 

1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 

each entry is the 
id203  
of a unique 
assignment to  
    1,     ,     6 

    (2,3,1,2,1,2) 

9 

    1 

    2 

    3 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

2

 

3

 

1

 

1

 

1

 

2

 

2

 

2

 

3

 

3

 

3

 

1

 

1

 

1

 

2

 

2

 

2

 

3

 

3

 

3

 

1

 

1

 

1

 

2

 

2

 

2

 

3

 

3

 

3

 

1

 

1

 

1

 

1

 

1

 

1

 

1

 

1

 

1

 

2

 

2

 

2

 

2

 

2

 

2

 

2

 

2

 

2

 

3

 

3

 

3

 

3

 

3

 

3

 

3

 

3

 

3

 

reshaping according to latent tree structure  

for marginal      =          1,     2,     ,     6  of a latent tree model, 
reshape it according to the edges in the tree  
 
     1 ;{2,3,4,5,6} =                            (    , 1 ) 
 
     1,2 ;{3,4,5,6} =                            (    , 1,2 ) 
 
     1,2,3,4 ;{5,6} =                            (    , 1,2,3,4 ) 
 
 
 

    10 

    10 

    10 

    7 

    8 

    4 

    8 

    7 

    7 

    3 

    8 

    2 

    9 

    9 

    1 

     1 ;{2,3,4,5,6} 

    9 

    5 

    6 

    1 

    2 

    3 

    4 

    5 

    6 

    1 

    2 

    3 

    4 

    5 

    6 

     1,2 ;{3,4,5,6} 

     1,2,3,4 ;{5,6} 

10 

low rank structure after reshaping 

size of      1,2 ;{3,4,5,6} is     2        4, but its rank is just      
 
         1,     2,     ,     6 = 
 
           1,     2     7  
 
    7,    10
 
     1,2 ;{3,4,5,6} 
 
use id127s to express summation over     7,     10 

    (    3,     4,     5,     6|    10) 

         7,     10  

    10 

    3 

    4 

    7 

    8 

    2 

    5 

    1 

    9 

    6 

     1,2 ;{3,4,5,6} =      1,2 | 7       7 ;{10}      3,4,5,6 |{10}
     1,2 | 7                                (         1,     2     7 , 1,2 ) 
     3,4,5,6 | 10                                (         3,     4,     5,     6     10 , 3,4,5,6 ) 

 

   

 
 

    2 

    4 

     1,2 ;{3,4,5,6} 

= 

    2 

     

     

     

     

    4 

     7 ;{10} 

11 

low rank structure of latent tree model 

 

    4 

   

     

     

    4 

     3,4 ;{1,2,5,6} =      3,4 | 8       8 ;{10}      1,2,5,6 |{10}
 
    2 
 
 
     1 ;{2,3,4,5,6} =      1 | 7       7 ;{7}      2,3,4,5,6 |{7}
 
     
 

    2 

    5 

= 

= 

     

     

   

 

    10 

    8 

    7 

     

     

     

     

     

    9 

    1 

    2 

    3 

    4 

    5 

    6 

    5 

all these reshapings are low rank,  
and with rank      

12 

low rank structure of id48 

    7 

    8 

    9 

    10 

    11 

    12 

    1 

    2 

    3 

    4 

    5 

    6 

    2 

   

 

     

     

    4 

     1,2 ;{3,4,5,6} =      1,2 | 8       8 ;{9}      3,4,5,6 |{9}
 
 
 
     1,2,3 ;{4,5,6} =      1,2,3 | 9       9 ;{10}      4,5,6 |{10}
 
 

    2 

    3 

    3 

= 

     

     

     

     

     

     

   

= 

    3 

    4 

 

    3 

13 

key features of spectral algorithms 

represent joint id203 table of observed variables with 
low rank factorization, without using the joint table in the 
computation! 
 
eg.      1,   ,     ;     +1 ,   ,2     =                            (         1,     ,     2     , 1,     ,      ) 
 

         

    represent it by low rank factors 

         

     1,   ,     ;     +1 ,   ,2      

to avoid exponential blowup 

    use clever decomposition 
technique to avoid directly 
using all entries from the table 

    use singular value decomposition  

14 

key theorem 

theorem 1:  

    :                               ,                       
    :                               ,                       
    :                               ,                       

                                                               ,                      =                          1           

     will be the reshaped joint id203 table  
 
     and      will be marginalization operator  
 
theorem 1 will be applied recursively 
 
recover several existing spectral algorithms as special cases  

15 

marginalization operator a and b 

compute the marginal id203 of a subset of variables can 
be expressed as matrix product 
 
         1,     2,     3,     4 =  
 
     1,2,3 ; 4 =      1,2,3 ;{4,5,6}    ,   where      = 1         1                  

         1,     2,     3,     4,     5,     6

    5,    6

 

     

    3 

     

     

    3 

= 

    3 

    3 

      

    3 

      

= 

1 

     

1 

    

     

    

     

     
       

1 

    2 

16 

zoom into marginalization operation 

    6 
    5 
    4 

1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 

1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 

1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 

= 

     1,2,3 ;{4} 

 

     1,2,3 ;{4,5,6} 

 

13     13         3 
 
17 

apply theorem 1 to latent tree model 

let 

     =      1,2 ;{3,4,5,6} 
     = 1         1         1                  
     =              1    

    

 
then  

     1,2 ; 3,4,5,6      =      1,2 ;{3} 
         1,2 ; 3,4,5,6 =      2 ;{3,4,5,6} 
         1,2 ; 3,4,5,6      =      2 ;{3} 

    10 

    8 

    9 

    7 

    1 

    2 

    3 

    4 

    5 

    6 

     1,2 ;{3,4,5,6} 

 
finally use      =                          1           

     1,2 ;{3,4,5,6} =      1,2 ;{3}     2 ; 3

   1      2 ;{3,4,5,6} 

18 

latent tree decomposition 

     1,2 ;{3,4,5,6} =      1,2 ;{3}     2 ; 3

   1      2 ;{3,4,5,6} 

     1,2 ;{3,4,5,6} 

    7 

    10 

    8 

    9 

    1 

    2 

    3 

    4 

    5 

    6 

decompose: 

    10 

    8 

    7 

    10 

    7 

    8 

    7 

    10 

    8 

    9 

    1 

    2 

    3 

    2 

    3 

    2 

    3 

    4 

    5 

    6 

     1,2 ;{3} 

   1

     2 ; 3

 

     2 ;{3,4,5,6} 

19 

apply theorem 1 to id48 

    7 

    8 

    9 

    10 

    11 

    12 

    1 

    2 

    3 

    4 

    5 

    6 

     1,2,3 ;{4,5,6} 

let 

     =      1,2,3 ;{4,5,6} 
     = 1         1                  
     =              1         1    

    

 
then  

     1,2,3 ; 4,5,6      =      1,2,3 ;{4} 
         1,2,3 ; 4,5,6 =      3 ;{4,5,6} 
         1,2,3 ; 4,5,6      =      3 ;{4} 

 
finally use      =                          1           

     1,2,3 ;{4,5,6} =      1,2,3 ;{4}     3 ; 4

   1      3 ;{4,5,6} 

 
 

20 

hidden markov model decomposition 

     1,2,3 ;{4,5,6} =      1,2,3 ;{4}     3 ; 4

   1      3 ;{4,5,6} 

    7 

    8 

    9 

    10 

    11 

    12 

    1 

    2 

    3 

    4 

    5 

    6 

     1,2,3 ;{4,5,6} 

decompose: 

    7 

    8 

    9 

    10 

    9 

    10 

    9 

    10 

    11 

    12 

    1 

    2 

    3 

    4 

     1,2,3 ;{4} 

    3 

    4 

   1

     3 ; 4

 

    3 

    4 

    5 

    6 

     3 ;{4,5,6} 

21 

recursive decomposition of latent tree 

    10 

    8 

    9 

    7 

    1      2      3      4      5      6 

     1,2 ;{3,4,5,6} 

    10 

    8 

    7 

    10 

    8 

    7 

    10 

    8 

    9 

    7 

    1      2      3 

     1,2 ;{3} 

    2      3 
   1

     2 ; 3

 

    2      3      4      5      6 

     2 ;{3,4,5,6} 

reshape 

    7 

    10 

    8 

    9 

    2      3      4      5      6 

     2,3,4 ;{5,6} 

    10 

    8 

    7 

    9 

reshape 

    7 

    10 

    8 

    10 

    8 

    9 

    10 

    8 

    9 

    9 

    2      3      4      5 

     3,4 ;{2,5} 

    2      3      4      5 

    4      5 

    4      5      6 

     2,3,4 ;{5} 

   1

     4 ; 5

 

     4 ;{5,6} 

    10 

    8 

    7 

    2      3      4 
     3,4 ;{2} 

    7 

    2 

    10 

    8 

    10 

    8 

    9 

    7 

    4 
   1

     4 ; 2

    2 

    4      5 

 

     4 ;{2,5} 

22 

recursive decomposition of id48 

    7 

    8 

    9 

    10 

    11 

    12 

    1 

    2 

    4 

    5 

    6 

     1,2,3 ;{4,5,6} 

     1,2,3 ;{4,5,6} = 
     1,2,3 ;{4}     3 ; 4

   1      3 ;{4,5,6} 

    7 

    8 

    9 

    10 

    9 

    10 

    9 

    10 

    11 

    12 

    1 

    2 

    4 

     1,2,3 ;{4} 
reshape 

    7 

    8 

    9 

    10 

    1 

    2 

    4 

     1,2 ;{3,4} 

    4 

   1  

     3 ; 4

     1,2 ;{3,4} = 
     1,2 ;{3}     3 ; 4

   1      3 ;{4,5} 

    4 

    5 

    6 

     3 ;{4,5,6} 

reshape 

    9 

    10 

    11 

    12 

    4 

    5 

    6 

     3,4 ;{5,6} 

     1,2 ;{3,4} = 
     1,2 ;{3}     3 ; 4

   1      3 ;{4,5,6} 

    7 

    8 

    9 

    8 

    9 

    8 

    9 

    10 

    9 

    10 

    11 

    10 

    11 

    10 

    11 

    12 

    1 

    2 

    2 

    2 

    4 

     1,2 ;{3} 

   1  

     2 ; 3

     2 ;{3,4} 

    4 

    5 

    4 

    5 

     3,4 ;{5} 

   1  

     4 ; 5

    4 

    5 

    6 

     4 ;{5,6} 
23 

one entries in joint id203 table of id48 

fix some observations 

fix     3 =     3,      2 ;    3;{4}              2,     3,     4  is a matrix 
fix     2=     2,     2=     2,         1;    2;{3}              1,     2,     3  is a vector 

 
         1,     2,     3,     4,     5,     6  
=         1;    2;{3}      2 ; 3

   1      2 ;    3;{4}     3 ; 4

   1      3 ;    4;{5}     4 ; 5

   1     {4};    5;    6 

    7 

    8 

    9 

    8 

    9 

    8 

    9 

    10 

    9 

    10 

    4 

   1  

     3 ; 4

    9 

    10 

    11 

    10 

    11 

    10 

    11 

    12 

    1 

    2 

    2 

    2 

    4 

    4 

    5 

    4 

    5 

     1,2 ;{3} 

   1  

     2 ; 3

     2 ;{3,4} 

     3,4 ;{5} 

   1  

     4 ; 5

    4 

    5 

    6 

     4 ;{5,6} 

24 

connection to foster et al. 

         1,     2,     3,     4,     5,     6  
=         1;    2;{3}      2 ; 3

   1      2 ;    3;{3}     3 ; 4

   1      4 ;    4;{5}     4 ; 5

   1     {4};    5;    6 

 

introduce variable     0 into         1;    2;{3} 

   1     {1};    2;{3} 

= 1        0 ;    1;{1}     1 ; 2
= 1        0 ;{1}     0 ; 1
=      1
 

         0 ; 1

   1      0 ;    1;{1}     1 ; 2

   1     {1};    2;{3} 

   1      0 ;    1;{1}     1 ; 2

   1     {1};    2;{3} 

do similar things to     {6};    5;    6 
 
assume time homogeneous  

   1 =      1 ; 2
     0 ; 1

   1

,     {1,2};{3} =     {2,3};{4} 

 

    7 

    8 

    9 

    1 

    2 

    3 

     

    7 

    8 

    9 

    0 

    1 

    2 

    3 

25 

what if hidden state      < observed state      

   .  

let      =      1,2,3 ;{4,5,6},      = 1         1                 ,      =              1         1    
 
use      =                          1           

    10 

    9 

     1,2,3 ;{4,5,6} =      1,2,3 ;{4}     3 ; 4

   1      3 ;{4,5,6} 

 
     3 ; 4  of size              has rank      and not invertible!  
    
singular value decomposition of      3 ;{4} =                       

    4 

   1  

     3 ; 4

 
solution: use further projection such that (            ) is invertible 

let      = 1         1                          ,      =         

                 1         1    

    

     1,2,3 ;{4,5,6} =      1,2,3 ;{4}                 

        3 ; 4         

   1

        

        3 ;{4,5,6} 

 
 

26 

connection to hsu et al. 

two equivalent forms of applying further projection          and          

    
     3 ;{4} =                       

 

        

        3 ; 4         

   1

        

        3 ;{4,5,6} =      3 ; 4         

   

     3 ;{4,5,6} 

 
         1,     2,     3,     4,     5,     6  
=         1;    2; 3                    2 ; 3
           3 ; 4

   1         

   

   1         

   

     2 ;    3; 4          
   

     4 ;    4; 5                    4 ; 5

   1         

    {4};    5;    6 

 
           1             6        
    1
 

27 

proof of theorem 1 

theorem 1:  
                                                                                                                     ,  

                                                                                                        ,  
                                                                                                        ,  

                     =                          1           

    +        0       
    

svd:      =                       
 
assume  

     = (        ,        )

     = (        ,        )

    
    
    
    

,      of size              and invertible 

,      of size              and invertible 

 
plug the above      and      into                           1           
 

28 

finite sample estimator 

given      iid samples, estimate pairwise and triplet marginals 

one-of-     encoding, e.g.,           = 1 =

1
0
   
0

,           = 2 =

0
1
   
0

 

    
           1

    
    =1

             2

    
                  3

 

      1 ; 2 ;{3} =
 

1
    

      1 ;{2} =
 
      1 =
 
 

1
    

1
    

    
           1

    
    =1

        

         2

 

    10 

    8 

    9 

    7 

    
           1

    
    =1

 

    1 

    2 

    3 

    4 

    5 

    6 

     = 1 
   

 

     =      

1 
    1
   
     
    1

 

1 
    2

1 
    3

1 
    4

1 
    5

     
    2

     
    3

     
    4

     
    5

29 

1 
    6
   
     
    6

 

sample complexity analysis 

error in estimate       1 ; 2 ;{3},       1 ;{2} 
 
error propagation in the recursive decomposition  

         1,     2,     3,     4,     5,     6  

=         1;    2;{3}      2 ; 3

   1      2 ;    3;{3}     3 ; 4

   1      4 ;    4;{5}     4 ; 5

   1     {4};    5;    6 

depends on the smallest singular value of the invesion terms eg., 
     1 ;{2} 

 
spectral algorithms 

use svd for further projection  
error depends on singular value 

 

30 

synthetic data 

31 

stock trend prediction data 

59 stocks, 6800 samples, learn latent structure first and then 
estimate the marginal 
 
map prediction task          =                              (        |    1,     2,     ,            1) 
(query      variables) 
 
absolute error |                     
 
also compared with  

   | 

    chow-liu tree  
    (fully observed model) 

32 

non-discrete, non-gaussian case 

previous approach all about discrete variables 
 
real world data can be continuous, and have multimodal 
distribution and other rich statistical features 
 
 
 
 
replace discrete probabilities by kernel embedding of 
distributions 

          (    ,     ,     ) 

          (    ,     ) 

    (    ) 

     

     

     

     

         ,         =           ,     (       ) , eg., exp                          2  
expected feature of distribution          =                     (can be infinite 
dimensional feature) 
one-of-     feature of discrete case is a special case 

33 

kernel embedding and covariance operator 

    (    ) 

    (    ,     ) 

    (    ,     ,     ) 

discrete 

        1 

             

                     

    (    ) 

          (    ,     ) 

kernel 

embedding 

     

             

        [    (    )] 
 

 

     

                 

            [                  (    )] 

                [                                      ] 

 

          (    ,     ,     ) 

     

     
                     

       1 

           

                  

34 

kernel embedding with finite sample 

    (    ,     ) 

                                      

             

              

                                 

joint feature space 

             =                                                        =

1
    

    
                                 
    =1

 

use finite sample mean to approximate expectation, 
then apply the recursively low rank decomposition 
 

35 

how to deal with infinite features?  

kernel trick: never explicitly compute features, always turn it 
into inner product           ,         =           ,     (       )  
 
eg. kernel singular value decomposition 

1
    

    
    =1

=               

                                 

              =
 
run kernel principal component analysis on                           
     
 
eigenvector lies the span of data      =                        
 
solve a generalized eigenvalue problem  

    
    =1

 

                 =               
kernel matrix              =     (        ,         ) and              =     (        ,         ) 

 

36 

video and slot car senor prediction 

37 

demographic feature prediction 

50 variables, 1400 samples, learn the latent structure first and 
then run spectral algorithms 
 
compare to gaussian latent variable model and gaussian 
copula model (npn), absolute error |                | 

38 

summary and future direction (more) 

spectral algorithm is the consequence of low rank structure of 
latent variable model  

     =                          1           
recursively decomposition 
better low rank approximation?  

 
what if the latent variable model is the wrong model?  

estimating latent parameters 

pca approach (mossel & roch aoap   06), pca and svd approach, 
(anandkumar et al. colt   12, arxiv) 

estimating the structure of latent variable models 

recursive grouping (choi et al. jmlr   11), spectral short quartet 
(anandkumar et al. nips   11) 

39 

questions? 

thanks 

40 

