an idiot   s guide to support vector

machines (id166s)

r. berwick, village idiot

id166s:  a new

generation of learning algorithms

    pre 1980:

    almost all learning methods learned linear decision surfaces.
    linear learning methods have nice theoretical properties

    1980   s

    id90 and nns allowed efficient learning of non-

linear decision surfaces

    little theoretical basis and all suffer from local minima

    1990   s

    efficient learning algorithms for non-linear functions based

on computational learning theory developed

    nice theoretical properties.

1

key ideas

    two independent developments within last decade

    new efficient separability of non-linear regions that use

   id81s    : generalization of    similarity    to
new kinds of similarity measures based on dot products

    use of quadratic optimization problem to avoid    local

minimum    issues with neural nets

    the resulting learning algorithm is an optimization

algorithm rather than a greedy search

organization

    basic idea of support vector machines: just like 1-

layer or multi-layer neural nets
    optimal hyperplane for linearly separable

patterns

    extend to patterns that are not linearly

separable by transformations of original data to
map into new space     the id81

    id166 algorithm for pattern recognition

2

support vectors

    support vectors are the data points that lie closest

to the decision surface (or hyperplane)

    they are the data points most difficult to classify
    they have direct bearing on the optimum location

of the decision surface

    we can show that the optimal hyperplane stems

from the function class with the lowest
   capacity   = # of independent features/parameters
we can twiddle [note this is    extra    material not
covered in the lectures    you don   t have to know
this]

recall from 1-layer nets : which separating

hyperplane?

    in general, lots of possible

solutions for a,b,c (an
infinite number!)

    support vector machine
(id166) finds an optimal
solution

3

support vector machine (id166)
support vectors

    id166s maximize the margin

(winston terminology: the    street   )
around the separating hyperplane.

    the decision function is fully

specified by a (usually very small)
subset of training samples, the
support vectors.

    this becomes a quadratic

programming problem that is easy
to solve by standard methods

maximize
margin

separation by hyperplanes

    assume linear separability for now (we will relax this

   

later)
in 2 dimensions, can separate by a line
    in higher dimensions, need hyperplanes

4

general input/output for id166s just like for
neural nets, but for one important addition   

input: set of  (input, output) training pair samples; call the
input sample features x1, x2   xn, and the output result y.
typically, there can be lots of input features xi.

output: set of weights w  (or wi), one for each feature,
whose linear combination predicts the value of y.  (so far,
just like neural nets   )
important difference: we use the optimization of maximizing
the margin (   street width   ) to reduce the number of weights
that are nonzero to just a few that correspond to the
important features that    matter    in deciding the separating
line(hyperplane)   these nonzero weights correspond to the
support vectors (because they    support    the separating
hyperplane)

2-d case

find a,b,c, such that
ax + by     c for red points
ax + by     (or < ) c for green
points.

5

which hyperplane to pick?

    lots of possible solutions for a,b,c.
    some methods find a separating

hyperplane, but not the optimal one (e.g.,
neural net)

    but: which points should influence

optimality?
    all points?

    id75
    neural nets

    or only    difficult points    close to

decision boundary
    support vector machines

support vectors again for linearly separable case

    support vectors are the elements of the training set that
would change the position of the dividing hyperplane if
removed.

    support vectors are the critical elements of the training set
    the problem of finding the optimal hyper plane is an

optimization problem and can be solved by optimization
techniques (we use lagrange multipliers to get this
problem into a form that can be solved analytically).

6

support vectors: input vectors that just touch the boundary of the
margin (street)     circled below, there are 3 of them (or, rather, the
   tips    of the vectors
w0

tx + b0 = 1     or      w0

tx + b0 =    1

x

x

x

d

x

x

x

here, we have shown the actual support vectors, v1, v2, v3,  instead of
just the 3 circled points at the tail ends of the support vectors. d
denotes 1/2 of the street    width   

x

x

x

v2

v3

d

x

v1

x

x

7

definitions

h1

h0
h2

d+

d-

h

define the hyperplanes h such that:
w   xi+b     +1 when yi =+1 
w   xi+b     -1 when yi =    1
h1 and h2 are the planes:
h1: w   xi+b = +1
h2: w   xi+b =    1
the points on the planes h1 and
h2 are the tips of the support
vectors
the plane h0 is the median in
between, where w   xi+b =0

d+ = the shortest distance to the closest positive point
d- = the shortest distance to the closest negative point
the margin (gutter) of a separating hyperplane is d+ + d   .

moving a support vector
moves the decision
boundary

moving the other vectors
has no effect

the optimization algorithm to generate the weights proceeds in such a
way that only the support vectors determine the weights and thus the
boundary

8

defining the separating hyperplane

    form of equation defining the decision surface separating

the classes is a hyperplane of the form:

wtx + b = 0

    w is a weight vector
    x is input vector
    b is bias

    allows us to write

wtx + b     0 for di = +1
wtx + b < 0 for di =    1

some final definitions

    margin of separation (d):  the separation between the

hyperplane and the closest data point for a given weight
vector w and bias b.

    optimal hyperplane (maximal margin):  the particular

hyperplane for which the margin of separation d is
maximized.

9

maximizing the margin (aka street width)

we want a classifier (linear separator)
 with as big a margin as possible. 

h1

h0

h2

recall the distance from a point(x0,y0) to a line:
ax+by+c = 0 is: |ax0 +by0 +c|/sqrt(a2+b2), so,
the distance between h0 and h1 is then:
|w   x+b|/||w||=1/||w||, so
the total  distance between h1 and h2 is thus: 2/||w||

d+

d-

in order to maximize the margin, we thus need to minimize ||w||. with the 
condition that there are no datapoints between h1 and h2:
xi   w+b     +1 when yi =+1 
xi   w+b        1 when yi =   1        can be combined into: yi(xi   w)     1 

we now must solve a quadratic

programming problem

    problem is: minimize ||w||, s.t. discrimination boundary is
obeyed, i.e., min f(x) s.t. g(x)=0, which we can rewrite as:
 min f:    ||w||2   (note this is a quadratic function)
s.t.  g: yi(w   xi)   b  = 1 or [yi(w   xi)   b]     1 =0

this is a constrained optimization problem
it can be solved by the lagrangian multipler method
because it is quadratic, the surface is a paraboloid, with just a

single global minimum (thus avoiding a problem we had
with neural nets!)

10

flatten

example: paraboloid 2+x2+2y2 s.t. x+y=1

intuition: find intersection of two functions f, g at
a tangent point (intersection = both constraints
satisfied; tangent = derivative is 0); this will be a
min (or max) for f s.t. the contraint g is satisfied

flattened paraboloid f: 2x2+2y2=0 with superimposed
constraint   g: x +y = 1

minimize when the constraint line g (shown in green)
is tangent to the inner ellipse contour linez of f (shown in red)    
note direction of gradient arrows.

11

flattened paraboloid f: 2+x2+2y2=0 with superimposed constraint
g: x +y = 1;  at tangent solution p, gradient vectors of  f,g are
parallel (no possible move to increment f that also keeps you in
region g)

minimize when the constraint line g is tangent to the inner ellipse
contour line of f

two constraints

1. parallel normal constraint (= gradient constraint

on f, g  s.t. solution is a max, or a min)

2. g(x)=0 (solution is on the constraint line as well)

we now recast these by combining f, g as the new
lagrangian function by introducing new    slack
variables    denoted a or (more usually, denoted   
in the literature)

12

redescribing these conditions

    want to look for solution point p where

f p

(

)

"

= "

!

g p

(

)

g x

( ) 0

=

    or, combining these two as the langrangian l &

requiring derivative of l be zero:
l(x, a) = f (x) ! ag(x)

"(x, a) = 0
  

at a solution p

    the the constraint line g and the contour lines of f must

be tangent

    if they are tangent, their gradient vectors

(perpendiculars) are parallel

    gradient of g must be 0     i.e., steepest ascent & so

perpendicular to f

    gradient of f must also be in the same direction as g

13

how langrangian solves constrained

optimization

l(x, a) = f (x) ! ag(x) where

"(x, a) = 0
  

partial derivatives wrt x recover the parallel normal
 constraint
partial derivatives wrt     recover the g(x,y)=0

in general, 

l(x, a) = f (x) +
  

i! gi (x)

ai

in general

gradient min of f

constraint condition g

l(x, a) = f (x) +

i! gi (x) a function of n + m variables

ai

n for the x 's,  m for the a. differentiating gives n + m equations, each
 set to 0. the n eqns differentiated wrt each xi  give the gradient conditions; 
the m eqns differentiated wrt each ai  recover the constraints gi
  

in our case, f(x):   || w||2 ; g(x): yi(w   xi +b)   1=0 so lagrangian  is:

min l=   || w||2       ai[yi(w   xi +b)   1] wrt w, b

we expand the last to get the following l form:

min l=   || w||2       aiyi(w   xi +b) +  ai wrt w, b

14

lagrangian formulation
    so in the id166 problem the lagrangian is

min lp = 1

2

2

w

!

l

" yi x i # w + b

ai

(

i=1

) +

ai

"

i=1

l

s.t. $i, ai % 0 where l  is the # of training points
   

    from the property that the derivatives at min = 0

we get:

!lp
!w
!lp
!b

l

= w "

l

#

i =1

ai yixi = 0

=

l

"

i =1

ai yi = 0  so 

l

w =
   

! yi x i ,    

ai

i =1

! yi = 0

ai

i =1

what   s with this lp business?

    this indicates that this is the primal form of the

optimization problem

    we will actually solve the optimization problem

by now solving for the dual of this original
problem

    what is this dual formulation?

15

the lagrangian dual problem: instead of minimizing over w, b,
subject to constraints involving a   s, we can maximize over a (the
dual variable)  subject to the relations obtained previously for w

and b

our solution must satisfy these two relations:

l

l

w =
   

! yi x i ,    

ai

i =1

! yi = 0

ai

i =1

by substituting for w and b back in the original eqn we can get rid of the
dependence on w and b.
note first that we already now have our answer for what the weights w
must be: they are a linear combination of the training inputs and the
training outputs, xi and yi and the values of a.  we will now solve for the
a   s by differentiating the dual problem wrt a, and setting it to zero.  most
of the a   s will turn out to have the value zero.  the non-zero a   s will
correspond to the support vectors

primal problem:

min lp = 1

2

2

w

!

s.t. $i a i % 0
   

l

" yi x i # w + b

ai

(

i=1

l

) +

ai

"

i=1

l

l

! yi x i ,    

ai

w =
   
dual problem:

i =1

! yi = 0

ai

i =1

max ld (ai ) =

l

! "

ai

i=1

1

2

l

! yi y j x i # x j

ai a j

(

i=1

)

l

s.t.  
   

!

i=1

ai yi = 0

 & ai $ 0

(note that we have removed the dependence on w and b)

16

the dual problem

    kuhn-tucker theorem: the solution we find here will

be the same as the solution to the  original problem

    q: but why are we doing this???? (why not just

solve the original problem????)

    ans: because this will let us solve the problem by

computing the just the inner products of xi, xj  (which
will be very important later on when we want to
solve non-linearly separable classification problems)

the dual problem

dual problem:

l

max ld (ai ) =

l

! "

ai

i=1

1

2

l

! yi y j x i # x j

ai a j

(

i=1

)

i=1

!

ai yi = 0

 & ai $ 0

s.t.  
   
notice that all we have are the dot products of xi,xj
if we take the derivative wrt a and set it equal to zero,
we get the following solution, so we can solve for ai:

l

! = 0

ai yi

i =1
0 " ai " c

17

now knowing the ai we can find the
weights w for the maximal margin

separating hyperplane:

l

w =
   

! yi x i

ai

i =1

and now, after training and finding the w by this method,
given an unknown point u measured on features xi we
can classify it by looking at the sign of:

f (x) = wiu + b = (

l

!

i =1

ai yix i iu) + b

 

remember: most of the weights wi, i.e., the a   s, will be zero
only the support vectors (on the gutters or margin) will have nonzero
weights or a   s     this reduces the dimensionality of the solution

inner products, similarity, and id166s

why should inner product kernels be involved in pattern
recognition using id166s, or at all?
    intuition is that inner products provide some measure of
   similarity   
    inner product in 2d between 2 vectors of unit length returns the
cosine of the angle between them = how    far apart    they are
e.g. x = [1, 0]t ,  y = [0, 1]t
i.e. if they are parallel their inner product is 1 (completely similar)

 xt y = x   y = 1

if they are perpendicular (completely unlike) their inner product is
0 (so should not contribute to the correct classifier)

 xt    y = x   y = 0

18

insight into inner products

consider that we are trying to maximize the form:

l

! "

ai

i=1

1

2

ld (ai ) =

l

l

! yi y j x i # x j

ai a j

(

i=1

)

s.t.  
   

!

i=1

ai yi = 0

 & ai $ 0

the claim is that this function will be maximized if we give nonzero values to a   s that
correspond to the support vectors, ie, those that    matter    in fixing the maximum width
margin (   street   ).  well, consider what this looks like. note first from the constraint
condition that all the a   s are positive.  now let   s think about a few cases.
case 1. if two features xi , xj are completely dissimilar, their dot product is 0, and they don   t
contribute to l.
case 2. if two features xi,xj are completely alike, their dot product is 0. there are 2 subcases.
subcase 1: both xi,and xj predict the same output value yi (either +1 or    1). then yi

x yj is always 1, and the value of aiajyiyjxixj will be positive.  but this would  decrease the
value of l (since it would subtract from the first term sum).  so, the algorithm downgrades
similar feature vectors that make the same prediction.

subcase 2: xi,and xj make opposite predictions about the output value yi (ie, one is

+1, the other    1), but are otherwise very closely similar: then the product  aiajyiyjxix is
negative and we are subtracting it, so this adds to the sum, maximizing it. this is precisely
the examples we are looking for: the critical ones that tell the two classses apart.

insight into inner products, graphically: 2 very

very similar xi, xj vectors that predict difft
classes tend to maximize the margin width

xj

xi

19

2 vectors that are similar but predict the

same class are redundant

xi

xj

2 dissimilar (orthogonal) vectors don   t

count at all

xj

xi

20

but   are we done???

not linearly separable!

find a line that penalizes
points on    the wrong side   

21

transformation to separate

o
o

x

o

o

x
x

o

  

x

x

x

x

o

   (x)

   (x)

   (o)
   (o)

   (o)
   (o)

   (o)

   (x)

   (x)

   (x)

   (o)

   (x)

   (o)

   (x)

x

f

non   linear id166s

    the idea is to gain linearly separation by mapping the data to

a higher dimensional space
    the following set can   t be separated by a linear function,

but can be separated by a quadratic one

(

x a x b

!

!

)(

a

b

    so if we map 

we gain linear separation

x

!

{ }2 ,

x x

)

=

2

x

!

(
a b x ab

)

+

+

22

problems with linear id166

=-1
=+1

what if the decision function is not linear? what transform would separate these?

the kernel trick

ans: polar coordinates!

non-linear id166
imagine a function    that maps the data into another space:
                     =radial     

radial

  

=-1
=+1

  

=-1
=+1

remember the function we want to optimize: ld =    ai          ai ajyiyj (xi   xj) where (xi   xj) is the
dot product of the two feature vectors. if we now transform to   , instead of computing this 
dot product (xi   xj) we will have to compute (   (xi)       (xj)).  but how can we do this?  this is
expensive and time consuming (suppose    is a quartic polynomial    or worse, we don   t know the
function explicitly.  well, here is the neat thing:  
if there is a    id81    k such that k(xi,xj) =    (xi)       (xj), then we do not need to know
or compute    at all!!  that is, the id81 defines inner products in the transformed space.
or, it defines similarity in the transformed space.

23

non-linear id166s

so, the function we end up optimizing is:
ld =    ai          aiaj yiyjk(xi   xj),

kernel example: the polynomial kernel
k(xi,xj) = (xi   xj + 1)p, where p is a tunable parameter
note: evaluating k only requires one addition and one exponentiation
more than the original dot product

examples for non linear id166s

k

(
x y

,

)

=

(
x y

! +

k

(
x y

,

)

=

exp

x y

"

p

)
1
}2
)

22
!

"

"

{
(
!

k

(
x y

,

)

=

tanh

x y

# $

1st is polynomial (includes x   x as special case)
2nd is radial basis function (gaussians)
3rd is sigmoid (neural net activation function)

24

we   ve already seen such nonlinear

transforms   

    what is it???

   

   

tanh(  0xtxi +   1)

it   s the sigmoid
transform (for neural
nets)

    so, id166s subsume
neural nets! (but w/o
their problems   )

inner product kernels

type of support vector
machine

inner product kernel
k(x,xi), i = 1, 2,    , n

usual inner product

polynomial learning
machine

(xtxi + 1)p

power p is specified a
priori by the user

radial-basis function
(rbf)

exp(1/(2  2)||x-xi||2)

the width   2 is
specified a priori

two layer neural net

tanh(  0xtxi +   1)

actually works only for
some values of   0 and
  1

25

kernels generalize the notion of    inner

product similarity   

note that one can define kernels over more than just
vectors: strings, trees, structures,     in fact, just about
anything

a very powerful idea: used in comparing dna, protein
structure, sentence structures, etc.

examples for non linear id166s 2    

gaussian kernel

linear

gaussian

26

nonlinear rbf kernel

admiral   s delight w/ difft kernel

functions

27

overfitting by id166

every point is a support vector    too much freedom to bend to fit the
training data     no generalization.
in fact, id166s have an    automatic    way to avoid such issues, but we
won   t cover it here    see the book by vapnik, 1995. (we add a
penalty function for mistakes made after training by over-fitting: recall
that if one over-fits, then one will tend to make errors on new data.
this penalty fn can be put into the quadratic programming problem
directly.  you don   t need to know this for this course.)

28

