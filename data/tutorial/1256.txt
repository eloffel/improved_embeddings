machine learning methods in natural language processing

michael collins

mit csail

some nlp problems

  information extraction

    named entities
    relationships between entities

  finding linguistic structure

    part-of-speech tagging
    parsing

  machine translation

common themes

  need to learn mapping from one discrete structure to another

    strings to hidden state sequences

named-entity extraction, part-of-speech tagging

    strings to strings

machine translation

    strings to underlying trees

parsing

    strings to relational data structures

information extraction

  id103 is similar (and shares many techniques)

two fundamental problems

tagging: strings to tagged sequences

a b e e a f h j

a/c b/d e/c e/c a/d f/c h/d j/c

parsing: strings to trees

d e f g

d e f g

(a (b (d d) (e e)) (c (f f) (g g)))

a

b

c

d

d

e

e

f

f

g

g

information extraction

id39
input: pro   ts soared at boeing co., easily topping forecasts on wall street, as
their ceo alan mulally announced    rst quarter results.

output: pro   ts soared at
on
quarter results.

location wall street

company boeing co.

, as their ceo

person alan mulally

, easily topping forecasts
announced    rst

relationships between entities
input: boeing is located in seattle. alan mulally is the ceo.

output:

relationship = company-location
company
location

= boeing
= seattle

relationship = employer-employee
employer
employee

= boeing co.
= alan mulally

 
 
 
 
 
 
 
 
 
 
part-of-speech tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall
street, as their ceo alan mulally announced    rst quarter results.

output:
pro   ts /n soared/v at/p boeing/n co./n ,/, easily/adv topping/v
forecasts/n on/p wall/n street/n ,/, as/p their/poss ceo/n
alan/n mulally/n announced/v    rst /adj quarter/n results/n ./.

n
v
p
adv
adj

= noun
= verb
= preposition
= adverb
= adjective

 
 
 
named entity extraction as tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall
street, as their ceo alan mulally announced    rst quarter results.

output:
pro   ts /na soared/na at/na boeing/sc co./cc ,/na easily/na
topping/na forecasts/na on/na wall/sl street/cl ,/na as/na
their/na ceo/na alan/sp mulally/cp announced/na    rst /na
quarter/na results/na ./na

na
sc
cc
sl
cl

= no entity
= start company
= continue company
= start location
= continue location

 
 
 
parsing (syntactic structure)

boeing is located in seattle.

input:

output:

s

v

is

np

n

boeing

vp

vp

v

pp

located

p

in

np

n

seattle

machine translation

input:
boeing is located in seattle. alan mulally is the ceo.

output:
boeing ist in seattle. alan mulally ist der ceo.

techniques covered in this tutorial

  generative models for parsing

  log-linear (maximum-id178) taggers

  learning theory for nlp

data for parsing experiments

  penn wsj treebank = 50,000 sentences with associated trees

  usual set-up: 40,000 training sentences, 2400 test sentences

an example tree:

top

s

np

vp

nnp

nnps

vbd

np

pp

np

pp

advp

in

np

cd

nn

in

np

qp

$

cd

cd

punc,

rb

np

prp$

jj

nn

cc

jj

nn

nns

in

pp

np

np

sbar

nnp

punc,

whadvp

s

wrb

np

vp

dt

nn

vbz

np

qp

nns

punc.

rb

cd

canadian

utilities

had

1988

revenue

of

c$

1.16

billion

,

mainly

from

its

natural

gas

and

electric

utility

businesses

in

alberta

,

where

the

company

serves

about

800,000

customers

.

canadian utilities had 1988 revenue of c$ 1.16 billion , mainly from its
natural gas and electric utility businesses in alberta , where the company
serves about 800,000 customers .

the information conveyed by parse trees

1) part of speech for each word

(n = noun, v = verb, d = determiner)

s

np

d

n

vp

v

np

the

burglar

robbed

d

n

the

apartment

2) phrases

s

np

vp

dt

n

the

burglar

v

np

robbed

dt

n

the

apartment

noun phrases (np):    the burglar   ,    the apartment   

verb phrases (vp):    robbed the apartment   

sentences (s):

   the burglar robbed the apartment   

3) useful relationships

s

np

vp

subject

v

verb

s

np

vp

dt

n

the

burglar

v

np

robbed

dt

n

the

apartment

     the burglar    is the subject of    robbed   

an example application: machine translation

  english word order is

subject     verb     object

  japanese word order is

subject     object     verb

english:
japanese:

english:
japanese:

ibm bought lotus
ibm lotus bought

sources said that ibm bought lotus yesterday
sources yesterday ibm lotus bought that said

context-free grammars

[hopcroft and ullman 1979]
a id18 

 

 

	

	

where:

  is a set of non-terminal symbols
  is a set of terminal symbols
  is a set of rules of the form

 

for 

  ,

 

  , 

  is a distinguished start symbol

 
 
 
 
 
 
 
 



 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 


 
a context-free grammar for english

s, np, vp, pp, d, vi, vt, n, p

  =

 = s
  =

sleeps, saw, man, woman, telescope, the, with, in

  s

  np vp

vp   vi
vp   vt np
vp   vp pp
np   d
n
np   np pp
pp   p
np

vi   sleeps
vt   saw
n   man
n   woman
n   telescope
d   the
p   with
p   in

note: s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional phrase,
d=determiner, vi=intransitive verb, vt=transitive verb, n=noun, p=preposition

 
 
 
 
 
left-most derivations
a left-most derivation is a sequence of strings 

 , where


 , the start symbol

 , i.e. 

 is made up of terminal symbols only

  each 

 for 

most non-terminal in 

 is derived from 

 by picking the left-
 and replacing it by some  where

 

 

  is a rule in 

for example: [s], [np vp], [d n vp], [the n vp], [the man vp],
[the man vi], [the man sleeps]
representation of a derivation as a tree:

s

np

d

n

vp

vi

the

man

sleeps

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


the problem with parsing: ambiguity

input:
she announced a program to promote safety in trucks and vans

possible outputs:

s

vp

np

she

announced

np

np

a

program

vp

to

promote

s

s

np

she

vp

announced

safety

np

in

pp

np

trucks

and

vans

np

np

and

np

a

program

vp

to

promote

np

safety

pp

in

np

trucks

np

she

vp

announced

np

np

a

program

vp

to

promote

np

vans

np

and

np

vans

np

safety

pp

in

np

trucks

s

vp

np

she

announced

np

np

a

program

vp

to

promote

np

safety

pp

in

np

trucks

and

vans

s

s

np

she

vp

announced

np

np

a

program

vp

to

promote

np

pp

safety

in

np

trucks

np

she

vp

announced

np

np

and

np

vans

np

pp

np

vp

a

program

in

np

trucks

and

vans

to

promote

np

safety

and there are more...

 
an example tree

canadian utilities had 1988 revenue of c$ 1.16 billion ,
mainly from its natural gas and electric utility businesses
in alberta , where the company serves about 800,000
customers .

top

s

np

vp

nnp

nnps

vbd

np

pp

np

pp

advp

in

np

cd

nn

in

np

qp

$

cd

cd

punc,

rb

np

prp$

jj

nn

cc

jj

nn

nns

in

pp

np

np

sbar

nnp

punc,

whadvp

s

canadian

utilities

had

1988

revenue

of

c$

1.16

billion

,

mainly

from

its

natural

gas

and

electric

utility

businesses

in

alberta

,

where

the

company

serves

about

800,000

customers

.

wrb

np

vp

dt

nn

vbz

np

qp

nns

punc.

rb

cd

a probabilistic context-free grammar

  np vp

s
vp   vi
vp   vt np
vp   vp pp
np   d
n
np   np pp
np
pp   p

1.0
0.4
0.4
0.2
0.3
0.7
1.0

1.0
vi   sleeps
1.0
vt   saw
0.7
n   man
n   woman
0.2
n   telescope 0.1
1.0
d   the
p   with
0.5
0.5
p   in

  id203 of a tree with rules 

 is 

 

  id113

vp   v np

vp

vp   v np

vp

 

 
 
 
 
 
 

 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 

pid18s
showed that

[booth and thompson 73]
probabilities correctly de   nes a distribution over
derivations provided that:

a id18 with rule
the set of

1. the rule probabilities de   ne conditional distributions over the

different ways of rewriting each non-terminal.

2. a technical condition on the rule probabilities ensuring that
the id203 of the derivation terminating in a    nite number
of steps is 1. (this condition is not really a practical concern.)

top

s

np

n

vp

v

ibm

bought

np

n

lotus

prob = 

top
 s
s
 np vp
vp
 v np
np
 n
np
 n

n
v
n

 

 

 
 
 

 
 
 
 
 

 
 
 

 
 
 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 
 

 
 
 

the spatter parser: (magerman 95;jelinek et al 94)

  for each rule, identify the    head    child

s   np
vp   v
np   dt

vp
np
n

  add word to each non-terminal
s(questioned)

np(lawyer)

vp(questioned)

dt

n

the

lawyer

v

np(witness)

questioned

dt

n

the

witness

a lexicalized pid18

  np(lawyer)

s(questioned)
vp(questioned)   v(questioned) np(witness)
np(lawyer)
np(witness)

  d(the)
  d(the)

n(lawyer)
n(witness)

vp(questioned)

??
??
??
??

  the big question: how to estimate rule probabilities??

charniak (1997)

s(questioned)

s(questioned)

np vp(questioned)

s(questioned)

np(lawyer) vp(questioned)

np vp

s(questioned)

lawyer

s,vp,np, questioned)

 
 
 
 

 
 
 
 

smoothed estimation

np vp

s(questioned)

 


s(questioned) np vp 


s(questioned) 


s np vp 


s 

  where 

 , and 

 
 
 
 
 
 
 
 
	




 
	




 
 
 
 
 
	




 
	




 
 
 
 
 
 
 
 
 
 
 
smoothed estimation

lawyer

s,np,vp,questioned

 


lawyer  s,np,vp,questioned 


s,np,vp,questioned 


lawyer  s,np,vp 


s,np,vp 


lawyer  np 


np 

  where 

 , and 

 
 
 
 
 
 
 
 
	




 
	




 
 
 
 
 
	




 
	




 
 
 
 
 
	




 
	




 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
np(lawyer) vp(questioned)

s(questioned)

 

 

 s(questioned) np vp 

 s(questioned) 

 s np vp 

 s 

 lawyer
s,np,vp,questioned 

 s,np,vp,questioned 

 lawyer
s,np,vp 

 s,np,vp 

 lawyer
np 

 np 

 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 

 
 
 
 
 
 
 
 
 
 
 
 
lexicalized id140

  transformation to lexicalized rules

s
 np vp

vs. s(questioned)
 np(lawyer) vp(questioned)

  smoothed estimation techniques    blend    different counts

  search for most probable tree through id145

  perform vastly better than pid18s (88% vs. 73% accuracy)

  pid18s

independence assumptions

s

np

vp

dt

the

n

lawyer

v

np

questioned

dt

the

n

witness

  lexicalized pid18s

s(questioned)

np(lawyer)

vp(questioned)

dt

the

n

lawyer

v

np(witness)

questioned

dt

the

n

witness

results

method
pid18s (charniak 97)
conditional models     id90 (magerman 95)
lexical dependencies (collins 96)
conditional models     logistic (ratnaparkhi 97)
generative lexicalized model (charniak 97)
generative lexicalized model (collins 97)
logistic-inspired model (charniak 99)
boosting (collins 2000)

accuracy

73.0%
84.2%
85.5%
86.9%
86.7%
88.2%
89.6%
89.8%

  accuracy = average recall/precision

parsing for information extraction:

relationships between entities

input:
boeing is located in seattle.
output:

relationship = company-location
company
location

= boeing
= seattle

 
 
a generative model (miller et. al)

[miller et. al 2000] use non-terminals to carry lexical items and
semantic tags

sis
cl

np

boeing
company

vpis

clloc

boeing

v

is

vplocated
clloc

v

located

ppin

clloc

p

in

npseattle

location

seattle

ppin

clloc

lexical head
semantic tag

a generative model [miller et. al 2000]

we   re now left with an even more complicated estimation problem,

p(sis

cl   np

boeing
company vpis

clloc)

see [miller et. al 2000] for the details

  parsing algorithm recovers annotated trees

  simultaneously recovers syntactic structure and named
entity relationships

  accuracy (precision/recall) is greater than 80% in recovering

relations

techniques covered in this tutorial

  generative models for parsing

  log-linear (maximum-id178) taggers

  learning theory for nlp

tagging problems

tagging: strings to tagged sequences

a b e e a f h j

a/c b/d e/c e/c a/d f/c h/d j/c

example 1: part-of-speech tagging

pro   ts/n soared/v at/p boeing/n co./n ,/, easily/adv topping/v
forecasts/n on/p wall/n street/n ,/, as/p their/poss ceo/n alan/n
mulally/n announced/v    rst/adj quarter/n results/n ./.

example 2: id39

pro   ts/na soared/na at/na boeing/sc co./cc ,/na easily/na
topping/na forecasts/na on/na wall/sl street/cl ,/na as/na their/na
ceo/na alan/sp mulally/cp announced/na    rst/na quarter/na
results/na ./na

id148

  assume we have sets

and

  goal: de   ne

for any 

  , 

  .

 

  a feature vector representation is  

 

  parameters  

  de   ne

where



 






 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	



 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	



 
 
 
 
 
log-linear taggers: notation

  set of possible words = , possible tags =

  word sequence 

 

  tag sequence 

 

  training data is  tagged sentences,

where the    th sentence is of length 

for 

 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 
	
 
 
	
 
 
 



 
 
 
	
 
 
 



 
 
 
 
 
 
 
 
log-linear taggers: independence assumptions

  the basic idea

 

 

  two questions:

chain rule

independence
assumptions

1. how to parameterize
2. how to    nd

 

 




?

?

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
	
 
 
 
 
	
 
 
 
 
 
 

 
 

 

 
 

 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
the parameterization problem

hispaniola/nnp
important/jj base/??
its empire into the rest of the western hemisphere .

quickly/rb became/vb an/dt
from which spain expanded

  there are many possible tags in the position ??

  need to learn a function from (context, tag) pairs to a id203

 

 
 
 
 
 
 
 
 
 
 
 
 
 

representation: histories

  a history is a 4-tuple

 are the previous two tags.

are the  words in the input sentence.

 is the index of the word being tagged

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .

 dt, jj

 

 
 
	
 
 
 
	
 
 
 
 
 
 

 
 
 
 
 
 
	
 
 
 
	
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 
 
 
 
 


 
 
 
 
 
 
 
 


 
 
 
 
 

 
 
 

 
 
 

feature   vector representations

  take a history/tag pair

 

.

for  

  are features representing

tagging decision in context  .

example: id52 [ratnaparkhi 96]
  word/tag features

  if current word 

 is base and  = vb

  otherwise

  if current word 

 ends in ing and  = vbg

  otherwise

  contextual features

  if

  otherwise

dt, jj, vb

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 

part-of-speech (pos) tagging [ratnaparkhi 96]

  word/tag features

  if current word 

 is base and  = vb

  otherwise

  spelling features

  if current word 

 ends in ing and  = vbg

  otherwise

  if current word 

 starts with pre and  = nn

  otherwise

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
ratnaparkhi   s pos tagger

  contextual features

  if

  otherwise

dt, jj, vb

  if

jj, vb

  otherwise

  if

vb

  otherwise

  if previous word 

 = the and 

 vb

 

  otherwise

  if next word 

 = the and 

 vb

  otherwise

 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 

 
 

 
 
 
 
 
 
 
 

 
 
 

 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
log-linear (maximum-id178) models

  take a history/tag pair

 

.

for 

  are features

 for 

  are parameters

  parameters de   ne a conditional distribution

 

 

where

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 

 


 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
	
 

 


 
 
log-linear (maximum id178) models

  word sequence  
  tag sequence  
  histories

 

 

 

 

 

linear score

local id172

terms

 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
	
 
 
 
	
	
 
 
 
	
	
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

	
 
 
 
 
 
 
 
	
 
 
	
 
 

	
 
 
 
 
 
 
 
	
 
 
	
 
 
 
 
 
 

	
 
 
 
 
 
 
 
	
 
 
 
 
 
 
id148

  parameter estimation:

maximize likelihood of training data through id119,
iterative scaling

  search for

 

:

id145,  

 

complexity

  experimental results:

    almost 97% accuracy for id52 [ratnaparkhi 96]
    over
extraction

90% accuracy

named-entity

for

[borthwick et. al 98]

    better results than an id48 for faq segmentation

[mccallum et al. 2000]


 
 

 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 


techniques covered in this tutorial

  generative models for parsing

  log-linear (maximum-id178) taggers

  learning theory for nlp

linear models for classi   cation

  goal: learn a function 

 

  training examples

for 


,

  a representation 

  classi   er is de   ned as

  , parameter vector  

  .

 

	

  unifying framework for many results: support vector machines, boosting,
kernel methods, id28, margin-based generalization bounds,
online algorithms (id88, winnow), mistake bounds, etc.

how can these methods be generalized beyond classi   cation
problems?

 
 

 
 
 
 
	
 
 
 
 
 
 
	
 

 
 
 
 
 
 
 

 
 
 
 
 

 


 
 
 
 
 
 
 

 

linear models for parsing and tagging

  goal: learn a function 

  training examples

for 


,

  three components to the model:

    a function 
    a representation 
    a parameter vector  

  .

  .

enumerating candidates for 

  function is de   ned as

	

 

 
 

 
 
 
 
 
	
 

 
 
 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
	





 
 
 
 
 
 
 
	

 
component 1:

  enumerates a set of candidates for a sentence

she announced a program to promote safety in trucks and vans

s

vp

np

she

announced

np

np

a

program

vp

to

promote

s

s

np

she

vp

announced

safety

np

in

pp

np

trucks

and

vans

np

np

and

np

a

program

vp

to

promote

np

safety

pp

in

np

trucks

np

she

vp

announced

np

np

a

program

vp

to

promote

np

vans

np

and

np

vans

np

safety

pp

in

np

trucks

s

vp

np

she

announced

np

np

a

program

vp

to

promote

np

safety

pp

in

np

trucks

and

vans

s

s

np

she

vp

announced

np

np

a

program

vp

to

promote

np

pp

safety

in

np

trucks

np

she

vp

announced

np

np

and

np

vans

np

pp

np

vp

a

program

in

np

trucks

and

vans

to

promote

np

safety

 
 
 
 
 
 
 
examples of

  a context-free grammar

  a    nite-state machine

  top  most probable analyses from a probabilistic grammar

component 2:

  maps a candidate to a feature vector 

  de   nes the representation of a candidate

s

vp

np

she

announced

np

np

vp

a

program

to

vp

promote

np

safety

pp

in

np

np

and

trucks

np

vans

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

features

  a    feature    is a function on a structure, e.g.,

 number of times

a

is seen in 

  a

b

c

d

d

e

e

f

f

g

g

b c

a

b

c

d

d

e

e

f

h

a

b

b

c

c

 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
feature vectors

  a set of functions 

  de   ne a feature vector

  a

b

c

d

d

e

e

f

f

g

g

 

a

b

c

d

d

e

e

f

h

a

b

b

c

c

 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
component 3:

is a parameter vector 

  and

together map a candidate to a real-valued score

s

vp

np

she

announced

np

np

vp

a

program

to

vp

promote

np

safety

pp

in

np

np

and

trucks

np

vans

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
putting it all together

  is set of sentences,  is set of possible outputs (e.g. trees)

  need to learn a function 

 , ,

de   ne

choose the highest scoring tree as the most plausible structure



  given examples

, how to set

?

 
 
 

 
 
 
 
 
 
 
 


 
 

 
 
 
 
 
 


 
 
 
 
 
 
 
 
 
 
 
	
 

she announced a program to promote safety in trucks and vans

s

vp

np

she

announced

np

np

a

program

vp

to

promote

s

s

np

she

vp

announced

safety

np

in

pp

np

trucks

and

vans

np

np

and

np

a

program

vp

to

promote

np

safety

pp

in

np

trucks

np

she

vp

announced

np

np

a

program

vp

to

promote

np

vans

np

and

np

vans

np

safety

pp

in

np

trucks

s

vp

np

she

announced

np

np

a

program

vp

to

promote

np

safety

pp

in

np

trucks

and

vans

s

s

np

she

vp

announced

np

np

a

program

vp

to

promote

np

pp

safety

in

np

trucks

np

she

vp

announced

np

np

and

np

vans

np

pp

np

vp

a

program

in

np

trucks

and

vans

to

promote

np

safety

 

13.6

12.2

12.1

3.3

9.4

11.1

s

vp

np

she

announced

np

np

vp

a

program

to

vp

promote

np

safety

pp

in

np

np

and

trucks

np

vans

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	




	




	




	




	




	




 
 
 
 
 
 
 
markov random fields

[johnson et. al 1999, lafferty et al. 2001]

  parameters

de   ne a conditional distribution over candidates:



 





  gaussian prior: 

 

  map parameter estimates maximise



 





note: this is a    globallynormalised    model

 
 
	
 
 
	
 
 
 
 
 





 


 
 
 
 
 
 
 
 




 
 
 





 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
	
 
 
 
 
 





 


 
 
 
 
 
 
 
 




 
 
 





 
 
 
 
 
 
 
 
 
 
 
a variant of the id88 algorithm

inputs:

training set

for 

initialization:  

de   ne:

algorithm:

 

 , 

for 

if

output:

parameters

 
 
 
 
	
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
	





 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
	
 

 
	
 
 
 
 
 
	
 

 
 
 
 
 
 
 
 

theory underlying the algorithm

  de   nition: 

  de   nition: the training set is separable with margin ,

if there is a vector 

  with

 

 such that

theorem: for any training sequence
with margin , then for the id88 algorithm

which is separable

number of mistakes 

where is such that 







proof: direct modi   cation of the proof for the classi   cation case.
see [crammer and singer 2001b, collins 2002a]

 
 
 
 
 

 
 
 
 
 
 
 

 
 
	
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
	
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
	
 

 
 
 
 
	

 
 
 
 
 
 




 
 
 
 





 


 
	
 





 
 
 
 
 
 
results that carry over from classi   cation

  [freund and schapire 99] de   ne the voted id88, prove

results for the inseparable case.

  compression bounds [littlestone and warmuth, 1986]

say the id88 algorithm makes mistakes when
run to convergence over a training set of size
.
, with id203
then for all distributions 
 over the choice of training set of size

at least 
drawn from , if is the hypothesis at convergence,

	

nb.

 

 
 
 
	
 
 
 
 
 
 
 
 
 






 
 
 
 


 
 
 
 

 
 
 
 


 
 
 
 
 
 
large-margin (id166) hypothesis

an approach which is close to that of [crammer and singer 2001a]:

minimize

with respect to , 

 for 

	

, subject to the constraints

 

 

 

 

 

 see [altun, tsochantaridis, and hofmann, 2003]:

   hidden markov support vector machines   

 
 
 
 
 
 
 
 
 
 
 






 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


 
 
 

 
 
 
 
 
 
de   ne:

 

 

 

 

 

 

 

 

 

generalization theorem:
for all distributions 

 

generating examples, for all 

! with

"


, for all

$

 , with id203 at least


%over the choice of training set of

sizedrawn from  ,

)

 

where' is a constant such that

 

 

 

that

 

'. the variable+ is the smallest positive integer such

 

+ .

proof: see [collins 2002b]. based on [bartlett 1998, zhang, 2002]; closely
related to multiclass bounds in e.g., [schapire et al., 1998].

 
 
 
 
 
 
 
 
 
 
 
 
 
	





 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	





 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
#
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
&



'
 
 
(
*
 

 
(
*
 
+
 
 
 
 
(
*
 


%
 
 
,
 

 
 
 
 
 
 
 
 
 

-
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
-
 
 
 
 
 
 
,
 
 
 
 
 
 
 
 
 
 


 
id88 algorithm 1: tagging

going back to tagging:

 inputs are sentences 

 

 



 i.e. all tag sequences of length 

 global representations  are composed from local feature

vectors 

where 

 

 

 

 

 

 

 

 
 
 
 
 
 
 
	


 
 
 
 
 
 
 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
id88 algorithm 1: tagging

 typically, local features are indicator functions, e.g.,


 if current word 

  ends in ing and  = vbg

  otherwise

 and global features are then counts,

 number of times a word ending in ing is

 

 

tagged as vbg in

 

 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
id88 algorithm 1: tagging

 score for a

 

	
 



pair is

 

 

 

 

 viterbi algorithm for

 

	

log-linear taggers (see earlier part of the tutorial) are locally normalised models:

 

 

 

 

 

linear model

local id172

 
 
 


 
 
 
 
 
 


 
 
 



 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 


 

 
 
 
 
 
 
 



 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


 
 
 
 
 
 


 
 
 


(
*
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
!
 
 
"
 
 
 
 
 
#
$
%
&
 
 
 
 
 
(
*
 
'
 
"
 
 
 
 
#
$
%
&
training the parameters

inputs: training set

 

 

 

for 

 .

initialization: 

algorithm: for 

 



 

 

 

 

is output on    th sentence with current parameters

 

if 

 

 

then

 

 

 

 

 

 

correct tags   
feature value

incorrect tags   
feature value

output: parameter vector

.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 



 
 
 
 



 
 
 
 
 
 



 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
	




 
 
 
 

 


 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 



 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
#
$
%
&

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
#
$
%
&
an example

say the correct tags for    th sentence are

the/dt man/nn bit/vbd the/dt dog/nn

under current parameters, output is

the/dt man/nn bit/nn the/dt dog/nn

assume also that features track: (1) all bigrams; (2) word/tag pairs
parameters incremented:

nn, vbd

vbd, dt

vbd   bit

parameters decremented:

nn, nn

nn, dt

nn   bit

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
experiments

 wall street journal part-of-speech tagging data

id88 = 2.89%, max-ent = 3.28%
(11.9% relative error reduction)

 [ramshaw and marcus 95] np chunking data

id88 = 93.63%, max-ent = 93.29%
(5.1% relative error reduction)

see [collins 2002a]

id88 algorithm 2: reranking approaches


 is the top most probable candidates from a base model

    parsing: a lexicalized probabilistic context-free grammar
    tagging:    maximum id178    tagger
    id103: existing recogniser

 
 
	
parsing experiments


 id125 used to parse training and test sentences:

around 27 parses for each sentence

 

, where 

   rst-pass parser, 

  are 

 log-likelihood from
  indicator functions


 if contains

 

  otherwise

s

vp

np

she

announced

np

np

vp

a

program

to

vp

promote

np

safety

pp

in

np

np

and

trucks

np

vans

 
	
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
	


	
 
 



 
 

 
 
 
 
 
 


 


 
 
 


 
 
 
 
 


 
 
 
 
 


 


 
 
 
 
 


 


 
 
 
 
 
 
 
 
 
 
 
 


 
 
 
 
 
named entities


 top 20 segmentations from a    maximum-id178    tagger

,


 if contains a boundary =    [the

  otherwise

you   re

whether
[gen-xer],    [the day they shot john lennon],    playing at
[dougherty arts center], entertains the imagination.

   ower

aging

child

an

or

a

clueless
the

 
	
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 
 


 
 


 
 
 
 
 
 
 


 


 
 
 


 


 
 
 
 
 


 
 
 
 
 


 
 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 


 


 
you   re

whether
child
[gen-xer],
   [the day they shot john lennon],   
[dougherty arts center], entertains the imagination.

   ower

aging

an

whether
gen-xer,
[dougherty arts center], entertains the imagination.

you   re
   the day [they shot john lennon],   

   ower

aging

child

an

you   re

child
whether
   the day [they shot john lennon],   
[gen-xer],
[dougherty arts center], entertains the imagination.

   ower

aging

an

or
a
playing

clueless
at
the

a
or
playing

clueless
the
at

or
a
playing

clueless
at
the

 

 
 


 
 


 
 
 
 
 
 
 


 


 
 
 


 


 
 
 
 
 


 
 
 
 
 


 
 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 


 


 
 

 
 
 


 


 


 


 
 
 
 
 


 
 
 
 
 


 
 
 


 


 


 
 
 


 
 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 


 
 
 
 

 
 
 
 
 
 
 
 
 


 
 
 
 
 


 
 
 
 
 


 
 
 
 
 
 
 
 
 
 
 


 
 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 


 
 
 
experiments

parsing wall street journal treebank

training set = 40,000 sentences, test 2,416 sentences
state-of-the-art parser: 88.2% f-measure
reranked model: 89.5% f-measure (11% relative error reduction)
boosting: 89.7% f-measure (13% relative error reduction)

recovering named-entities in web data

training data 53,609 sentences (1,047,491 words),
test data 14,717 sentences (291,898 words)
state-of-the-art tagger: 85.3% f-measure
reranked model: 87.9% f-measure (17.7% relative error reduction)
boosting: 87.6% f-measure (15.6% relative error reduction)

id88 algorithm 3: kernel methods

(work with nigel duffy)

 it   s simple to derive a    dual form    of the id88 algorithm

if we can compute

 

ef   ciently

we can learn ef   ciently with the representation

 
 


 
 
 


   all subtrees   representation [bod 98]

 given: non-terminal symbols

 

terminal symbols

 an in   nite set of subtrees

 

 

 

a

b c

a

e

b

b

b

b

a

a

c

b

a

a b

c

b

b

 step 1:

choose an (arbitrary) mapping from subtrees to integers

 number of times subtree is seen in 



 
 
 
 
 
 
 
 
 
 
 
 
 
 
	
 
 
 
 



 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 


 
 
 
 
 
 
 
all subtrees representation

  is now huge

 but inner product

can be computed

ef   ciently using id145.
see [collins and duffy 2001, collins and duffy 2002]

 




 
 
 


computing the inner product

de   ne

    
 and 

 are sets of nodes in 

 and 

 respectively.

    


if    th subtree is rooted at .

  otherwise 

and 

follows that:



 

where 
subtrees at 

is the number of common

 
 
 
 

 
 
 
 
 
 

 


 
 


 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 


 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 


 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 


 
 


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
an example

  a

  a

b

c

d

d

e

e

f

f

g

g

b

c

d

d

e

e

f

h

g

i

 most of these terms are   (e.g. 

).

 some are non-zero, e.g. 

b

b

b

b

d e

d

d

e

d e

e

d

d

e

e

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 



 
 
 
 
 
 
 
 
 
 
 
 
 
 



 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
recursive de   nition of

 

 if the productions at 

 and 

 are different

 else if 

 are pre-terminals,





 else



 

 

is number of children of node 
 ;

is the    th child of 
 .

 
 


 
 
 


 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 



 
 
 
 
 
 
 
 

 
 
 
 


 
 
 
 
 


 
 
 
	
 
 
 
 
 
 
 
 
	
 
 
 
 
 
 
 
 
 
 
	
 
 
 
 
	
 
 
 
 
 
 
 
illustration of the recursion

a

a

b

c

b

c

d

d

e

e

f

f

g

g

d

d

e

e

f

h

g

i

how many subtrees do nodes  and  have in common? i.e., what is 

?

b

b

b

b

d e

d

d

e

d e

e

d

d

e

e

c

f g

 
 
 
 
 
 
 


 
 
 
 


 
 
 
 


 
 
 
 


 
 


 
 
 
 


 
 


 
 
 
a

a

a

a

a

b c

b

c

b

c

b

c

b

c

d e

d

d

a

e

d e

e

a

d

d

e

e

a

a

b

c

b

c

b

c

b

c

a

b

c

f g

d e

f g

d

d

e

f g

d e

f g

e

f g

d

d

e

e

similar kernels exist for tagged sequences

whether you   re an aging    ower
[gen-xer],
at the [dougherty arts center], entertains the imagination.

   [the day they shot john lennon],   

child or a clueless
playing

whether [gen-xer], day they john lennon],   playing

whether you   re an aging    ower child or a clueless [gen

 
 
 
experiments

parsing wall street journal treebank

training set = 40,000 sentences, test 2,416 sentences
state-of-the-art parser: 88.5% f-measure
reranked model: 89.1% f-measure
(5% relative error reduction)

recovering named-entities in web data

training data 53,609 sentences (1,047,491 words),
test data 14,717 sentences (291,898 words)
state-of-the-art tagger: 85.3% f-measure
reranked model: 87.6% f-measure
(15.6% relative error reduction)

open questions

 can

large-margin

the
ef   ciently ,
[altun, tsochantaridis, and hofmann, 2003])

hypothesis
is

even when  

be
huge?

trained
(see

 can the large-margin bound be improved, to remove the 

factor?

 which representations lead to good performance on parsing,

tagging etc.?

 can methods with    global    kernels be

implemented

ef   ciently?

	


 
 
 
 
 
 
conclusions

some other topics in statistical nlp:

 machine translation

 unsupervised/partially supervised methods

 finite state machines

 generation

 id53

 coreference

 id38 for id103

 lexical semantics

 id51

 summarization

references
[altun, tsochantaridis, and hofmann, 2003] altun, y., i. tsochantaridis, and t. hofmann. 2003.

hidden markov support vector machines. in proceedings of icml 2003.

[bartlett 1998] p. l. bartlett. 1998. the sample complexity of pattern classi   cation with neural
networks: the size of the weights is more important than the size of the network, ieee
transactions on id205, 44(2): 525-536, 1998.

[bod 98] bod, r. (1998). beyond grammar: an experience-based theory of language. csli

publications/cambridge university press.

[booth and thompson 73] booth, t., and thompson, r. 1973. applying id203 measures to

abstract languages. ieee transactions on computers, c-22(5), pages 442   450.

[borthwick et. al 98] borthwick, a., sterling, j., agichtein, e., and grishman, r. (1998). exploiting
diverse knowledge sources via maximum id178 in id39. proc.
of the sixth workshop on very large corpora.

[collins and duffy 2001] collins, m. and duffy, n. (2001). convolution kernels for natural

language. in proceedings of nips 14.

[collins and duffy 2002] collins, m. and duffy, n. (2002). new ranking algorithms for parsing
and tagging: kernels over discrete structures, and the voted id88. in proceedings
of acl 2002.

[collins 2002a] collins, m. (2002a). discriminative training methods for id48:
theory and experiments with the id88 algorithm. in proceedings of emnlp 2002.
[collins 2002b] collins, m. (2002b). parameter estimation for statistical parsing models: theory

and practice of distribution-free methods. to appear as a book chapter.

[crammer and singer 2001a] crammer, k.,

and singer, y. 2001a. on the algorithmic
implementation of multiclass kernel-based vector machines. in journal of machine
learning research, 2(dec):265-292.

[crammer and singer 2001b] koby crammer and yoram singer. 2001b. ultraconservative online

algorithms for multiclass problems in proceedings of colt 2001.

[freund and schapire 99] freund, y. and schapire, r. (1999). large margin classi   cation using the

id88 algorithm. in machine learning, 37(3):277   296.

[helmbold and warmuth 95] helmbold, d., and warmuth, m. on weak learning. journal of

computer and system sciences, 50(3):551-573, june 1995.

[hopcroft and ullman 1979] hopcroft, j. e., and ullman, j. d. 1979. introduction to automata

theory, languages, and computation. reading, mass.: addison   wesley.

[johnson et. al 1999] johnson, m., geman, s., canon, s., chi, s., & riezler, s. (1999). estimators
for stochastic    uni   cation-based    grammars. in proceedings of the 37th annual meeting
of the association for computational linguistics. san francisco: morgan kaufmann.

[lafferty et al. 2001] john lafferty, andrew mccallum, and fernando pereira. conditional random
   elds: probabilistic models for segmenting and labeling sequence data. in proceedings of
icml-01, pages 282-289, 2001.

[littlestone and warmuth, 1986] littlestone, n., and warmuth, m. 1986. relating data compression

and learnability. technical report, university of california, santa cruz.

[msm93] marcus, m., santorini, b., & marcinkiewicz, m. (1993). building a large annotated

corpus of english: the id32. computational linguistics, 19, 313-330.

[mccallum et al. 2000] mccallum, a., freitag, d., and pereira, f. (2000) maximum id178 markov

models for information extraction and segmentation. in proceedings of icml 2000.

[miller et. al 2000] miller, s., fox, h., ramshaw, l., and weischedel, r. 2000. a novel use of

statistical parsing to extract information from text. in proceedings of anlp 2000.

[ramshaw and marcus 95] ramshaw, l., and marcus, m. p. (1995). text chunking using
transformation-based learning. in proceedings of the third acl workshop on very large
corpora, association for computational linguistics, 1995.

[ratnaparkhi 96] a maximum id178 part-of-speech tagger. in proceedings of the empirical

methods in natural language processing conference.

[schapire et al., 1998] schapire r., freund y., bartlett p. and lee w. s. 1998. boosting the margin:
a new explanation for the effectiveness of voting methods. the annals of statistics,
26(5):1651-1686.

[zhang, 2002] zhang, t. 2002. covering number bounds of certain regularized linear function

classes. in journal of machine learning research, 2(mar):527-550, 2002.

