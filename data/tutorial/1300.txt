structured belief propagation for nlp
matthew r. gorid113y & jason eisner
acl    15 tutorial
july 26, 2015
1
for the latest version of these slides, please visit:
http://www.cs.jhu.edu/~mrg/bp-tutorial/ 
2

language has a lot going on at once  

outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
3
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
4
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
5
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
6
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
7
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
8
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
9
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
10
section 1:
introduction
modeling with factor graphs
11
sampling from a joint distribution
12
n
v
p
d
n
sample 6:
v
n
v
d
n
sample 5:
v
n
p
d
n
sample 4:
n
v
p
d
n
sample 3:
n
n
v
d
n
sample 2:
n
v
p
d
n
sample 1:
a joint distribution defines a id203 p(x) for each assignment of values x to variables x. 
this gives the proportion of samples that will equal x.
sampling from a joint distribution
13
a joint distribution defines a id203 p(x) for each assignment of values x to variables x. 
this gives the proportion of samples that will equal x.
sampling from a joint distribution
14
a joint distribution defines a id203 p(x) for each assignment of values x to variables x. 
this gives the proportion of samples that will equal x.




factors have local opinions (    0)
15
each black box looks at some of the tags xi and words wi
note: we chose to reuse the same factors at different positions in the sentence.




factors have local opinions (    0)
16
each black box looks at some of the tags xi and words wi
p(n, v, p, d, n, time, flies, like, an, arrow)     =      ?




global id203 = product of local opinions
17
each black box looks at some of the tags xi and words wi
p(n, v, p, d, n, time, flies, like, an, arrow)     =       (4 * 8 * 5 * 3 *    )
uh-oh! the probabilities of the various assignments sum up to z > 1.
so divide them all by z.




markov random field (mrf)
18
p(n, v, p, d, n, time, flies, like, an, arrow)     =       (4 * 8 * 5 * 3 *    )
joint distribution over tags xi and words wi
the individual factors aren   t necessarily probabilities.




hidden markov model
19
but sometimes we choose to make them probabilities.  
constrain each row of a factor to sum to one.  now z = 1.
p(n, v, p, d, n, time, flies, like, an, arrow)     =       (.3 * .8 * .2 * .5 *    )




markov random field (mrf)
20
p(n, v, p, d, n, time, flies, like, an, arrow)     =       (4 * 8 * 5 * 3 *    )
joint distribution over tags xi and words wi


conditional random field (crf)
21
time
flies
like
an
arrow
n
  2
v
  4
p
  6
d
  8
n
  1
  3
  5
  7
  9
  0
<start>




conditional distribution over tags xi given words wi.
the factors and z are now specific to the sentence w.
p(n, v, p, d, n, time, flies, like, an, arrow)     =       (4 * 8 * 5 * 3 *    )
how general are factor graphs?
factor graphs can be used to describe
markov random fields (undirected id114)
i.e., id148 over a tuple of variables
id49
id110s (directed id114)

id136 treats all of these interchangeably.
convert your model to a factor graph first.
pearl (1988) gave key strategies for exact id136:
belief propagation, for id136 on acyclic graphs
junction tree algorithm, for making any graph acyclic
(by merging variables and factors: blows up the runtime)
object-oriented analogy
what is a sample?  
a datum: an immutable object that describes a linguistic structure.
what is the sample space?
the class of all possible sample objects.






what is a random variable? 
an accessor method of the class, e.g., one that returns a certain field.
will give different values when called on different random samples.
23
random variable w5 takes value w5 ==    arrow    in this sample
object-oriented analogy
what is a sample?  
a datum: an immutable object that describes a linguistic structure.
what is the sample space?
the class of all possible sample objects.
what is a random variable?  
an accessor method of the class, e.g., one that returns a certain field.

a model is represented by a different object.  what is a factor of the model?
a method of the model that computes a number     0 from a sample, based on the sample   s values of a few random variables, and parameters stored in the model.





what id203 does the model assign to a sample?  
a product of its factors (rescaled).    e.g., uprob(tagging) / z().







how do you find the scaling factor?  
add up the probabilities of all possible samples.  if the result z != 1, divide the probabilities by that z.


24

modeling with factor graphs
factor graphs can be used to model many linguistic structures.

here we highlight a few example nlp tasks.
people have used bp for all of these.

we   ll describe how variables and factors were used to describe structures and the interactions among their parts.
25
annotating a tree
26
given: a sentence and unlabeled parse tree.
annotating a tree
27
given: a sentence and unlabeled parse tree.

construct a factor graph which mimics the tree structure, to predict the tags / nonterminals.
annotating a tree
28
given: a sentence and unlabeled parse tree.

construct a factor graph which mimics the tree structure, to predict the tags / nonterminals.
annotating a tree
29
given: a sentence and unlabeled parse tree.

construct a factor graph which mimics the tree structure, to predict the tags / nonterminals.
we could add a linear chain structure between tags.
(this creates cycles!)
constituency parsing
30
n
  1
  2
v
  3
  4
p
  5
  6
d
  7
  8
n
  9
vp
  12
pp
  11
s
  13
what if we needed to predict the tree structure too? 

use more variables: predict the nonterminal of each substring, or     if it   s not a constituent.
constituency parsing
31
n
  1
  2
v
  3
  4
p
  5
  6
d
  7
  8
n
  9
vp
  12
pp
  11
s
  13
what if we needed to predict the tree structure too? 

use more variables: predict the nonterminal of each substring, or     if it   s not a constituent.

but nothing prevents non-tree structures.
constituency parsing
32
n
  1
  2
v
  3
  4
p
  5
  6
d
  7
  8
n
  9
vp
  12
pp
  11
s
  13
what if we needed to predict the tree structure too? 

use more variables: predict the nonterminal of each substring, or     if it   s not a constituent.

but nothing prevents non-tree structures.



constituency parsing
33
n
  1
  2
v
  3
  4
p
  5
  6
d
  7
  8
n
  9
vp
  12
pp
  11
s
  13
what if we needed to predict the tree structure too? 

use more variables: predict the nonterminal of each substring, or     if it   s not a constituent.

but nothing prevents non-tree structures.

add a factor which multiplies in 1 if the variables form a tree and 0 otherwise.

constituency parsing
34
n
  1
  2
v
  3
  4
p
  5
  6
d
  7
  8
n
  9
vp
  12
pp
  11
s
  13
what if we needed to predict the tree structure too? 

use more variables: predict the nonterminal of each substring, or     if it   s not a constituent.

but nothing prevents non-tree structures.

add a factor which multiplies in 1 if the variables form a tree and 0 otherwise.

constituency parsing
variables: 
constituent type (or    ) for each of o(n2) substrings
interactions:
constituents must describe a binary tree
tag bigrams
nonterminal triples (parent, left-child, right-child)
        [these factors not shown]
35
example task:
(naradowsky, vieira, & smith, 2012)
id33
variables: 
pos tag for each word
syntactic label (or    ) for each of o(n2) possible directed arcs
interactions: 
arcs must form a tree
discourage (or forbid) crossing edges
features on edge pairs that share a vertex
36
(smith & eisner, 2008)
example task:
time
like
flies
an
arrow
learn to discourage a verb from having 2 objects, etc.
learn to encourage specific multi-arc constructions
joint id35 parsing and id55
variables:
spans
labels on non-terminals
supertags on pre-terminals
interactions:
spans must form a tree
triples of labels: parent, left-child, and right-child
adjacent tags
37
(auli & lopez, 2011)
example task:
38
figure thanks to markus dreyer
example task:
id68 or back-id68
variables (string):
english and japanese orthographic strings
english and japanese phonological strings
interactions:
all pairs of strings could be relevant
variables (string): 
inflected forms 
of the same verb
interactions: 
between pairs of entries in the table 
(e.g. infinitive form affects present-singular)
39
(dreyer & eisner, 2009)
example task:
morphological paradigms
word alignment / phrase extraction
variables (boolean):
for each (chinese phrase, english phrase) pair, 
are they linked?
interactions:
word fertilities
few    jumps    (discontinuities)
syntactic reorderings
   itg contraint    on alignment
phrases are disjoint (?)

40
(burkett & klein, 2012)
application:
congressional voting
41
(stoyanov & eisner, 2012)
application:
variables:
representative   s vote
text of all speeches of a representative 
local contexts of references between two representatives
interactions:
words used by representative and their vote
pairs of representatives and their local context



id14 with latent syntax
variables:
semantic predicate sense
semantic dependency arcs
labels of semantic arcs
latent syntactic dependency arcs
interactions:
pairs of syntactic and semantic dependencies
syntactic dependency arcs must form a tree
42
   (naradowsky, riedel, & smith, 2012)
   (gorid113y, mitchell, van durme, & dredze, 2014)
application:
time
like
flies
an
arrow
arg0
arg1
joint ner & id31
variables:
named entity spans
sentiment directed toward each entity
interactions:
words and entities
entities and sentiment
43
(mitchell, aguilar, wilson, & van durme, 2013)
application:

person
positive

44

variable-centric view of the world

when we deeply understand language, what representations 
(type and token) does that understanding comprise?  
45


entailment
correlation
inflection
cognates
id68
abbreviation
neologism
language evolution
translation
alignment
editing
quotation










speech
 misspellings,typos 
  formatting
   entanglement
    annotation




n
tokens

to recover variables, 
model and exploit
 their correlations
section 2:
belief propagation basics

46
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
47
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
48
factor graph notation
49
variables:

factors:

factors are tensors
50


factors:



s
vp
pp
id136
given a factor graph, two common tasks    
compute the most likely joint assignment,
 x* = argmaxx p(x=x)
compute the marginal distribution of variable xi:
p(xi=xi) for each value xi
	
both consider all joint assignments.
both are np-hard in general.

so, we turn to approximations.
51

marginals by sampling on factor graph
52
n
v
p
d
n
sample 6:
v
n
v
d
n
sample 5:
v
n
p
d
n
sample 4:
n
v
p
d
n
sample 3:
n
n
v
d
n
sample 2:
n
v
p
d
n
sample 1:
suppose we took many samples from the distribution over taggings:
marginals by sampling on factor graph
53
n
v
p
d
n
sample 6:
v
n
v
d
n
sample 5:
v
n
p
d
n
sample 4:
n
v
p
d
n
sample 3:
n
n
v
d
n
sample 2:
n
v
p
d
n
sample 1:
the marginal p(xi = xi) gives the id203 that variable xi takes value xi in a random sample 
marginals by sampling on factor graph
54
n
v
p
d
n
sample 6:
v
n
v
d
n
sample 5:
v
n
p
d
n
sample 4:
n
v
p
d
n
sample 3:
n
n
v
d
n
sample 2:
n
v
p
d
n
sample 1:
estimate the marginals as:







sampling one joint assignment is also np-hard in general.
in practice: use mcmc (e.g., id150) as an anytime algorithm.
so draw an approximate sample fast, or run longer for a    good    sample.

sampling finds the high-id203 values xi efficiently.
but it takes too many samples to see the low-id203 ones.
how do you find p(   the quick brown fox       ) under a language model?  
draw random sentences to see how often you get it?  takes a long time.
or multiply factors (trigram probabilities)?  that   s what bp would do.
55
how do we get marginals without sampling?

that   s what belief propagation is all about!


why not just sample?






 ____  ____ __      ______  ______
great ideas in ml: message passing
3 behind you
2 behind you
1 behind you
4 behind you
5 behind you
1 
before
you
2 before
you
3 before
you
4 before
you
5 before
you
count the soldiers
56
adapted from mackay (2003) textbook
great ideas in ml: message passing
3 behind you
2 before
you
belief:
must be
2 + 1 + 3 = 6 of us
only see
my incoming
messages
count the soldiers
57
adapted from mackay (2003) textbook
2 before
you

great ideas in ml: message passing
4 behind you
1 before
you

only see
my incoming
messages
count the soldiers
58
adapted from mackay (2003) textbook
belief:
must be
2 + 1 + 3 = 6 of us
belief:
must be
1 + 1 + 4 = 6 of us
great ideas in ml: message passing



7 here
3 here
11 here
(= 7+3+1)
each soldier receives reports from all branches of tree
59
adapted from mackay (2003) textbook
great ideas in ml: message passing



3 here
3 here
7 here
(= 3+3+1)
each soldier receives reports from all branches of tree
60
adapted from mackay (2003) textbook
great ideas in ml: message passing



7 here
3 here
11 here
(= 7+3+1)
each soldier receives reports from all branches of tree
61
adapted from mackay (2003) textbook
great ideas in ml: message passing


7 here
3 here

3 here
belief:
must be
14 of us
each soldier receives reports from all branches of tree
62
adapted from mackay (2003) textbook
great ideas in ml: message passing
each soldier receives reports from all branches of tree


7 here
3 here

3 here
belief:
must be
14 of us

wouldn't work correctly
with a 'loopy' (cyclic) graph
63
adapted from mackay (2003) textbook
message passing in belief propagation
64
x
 
  



   


 
   

   






both of these messages judge the possible values of variable x.
their product = belief at x = product of all 3 messages to x.
sum-product belief propagation
65


sum-product belief propagation
66

variable belief
sum-product belief propagation
67
variable message
sum-product belief propagation
68
factor belief
  1
x1
x3

sum-product belief propagation
69
factor belief



sum-product belief propagation
70
factor message
sum-product belief propagation
factor message

71
  1
x1
x3

matrix-vector product
(for a binary factor)
input: a factor graph with no cycles
output: exact marginals for each variable and factor

algorithm:
initialize the messages to the uniform distribution.

choose a root node.
send messages from the leaves to the root.
send messages from the root to the leaves.


compute the beliefs (unnormalized marginals).


normalize beliefs and return the exact marginals.

 
sum-product belief propagation
72
sum-product belief propagation
73



sum-product belief propagation
74



x2
x3
x1


crf tagging model
75





could be adjective or verb
could be noun or verb
could be verb or noun


76






   

   
find
preferred
tags







crf tagging by belief propagation
  
  
  
belief
message
message
forward-backward is a message passing algorithm.
it   s the simplest case of belief propagation.

  
forward algorithm =
message passing
(matrix-vector products)
backward algorithm =
message passing
(matrix-vector products)
x2
x3
x1


77





could be adjective or verb
could be noun or verb
could be verb or noun
so let   s review forward-backward    
x2
x3
x1


so let   s review forward-backward    
78
v
n
a
v
n
a
v
n
a
start
end
show the possible values for each variable






x2
x3
x1


79
v
n
a
v
n
a
v
n
a
start
end
let   s show the possible values for each variable
one possible assignment






so let   s review forward-backward    
x2
x3
x1


80
v
n
a
v
n
a
v
n
a
start
end
let   s show the possible values for each variable
one possible assignment
and what the 7 factors think of it    


























so let   s review forward-backward    
x2
x3
x1


viterbi algorithm: most probable assignment
81
v
n
a
v
n
a
v
n
a
start
end

























so p(v a n) = (1/z) * product of 7 numbers
numbers associated with edges and nodes of path
most probable assignment = path with highest product

  {0,1}(start,v)
  {1,2}(v,a)
  {2,3}(a,n)
  {3,4}(a,end)
  {1}(v)
  {2}(a)
  {3}(n)
x2
x3
x1


viterbi algorithm: most probable assignment
82
v
n
a
v
n
a
v
n
a
start
end

























so p(v a n) = (1/z) * product weight of one path
  {0,1}(start,v)
  {1,2}(v,a)
  {2,3}(a,n)
  {3,4}(a,end)
  {1}(v)
  {2}(a)
  {3}(n)
x2
x3
x1


forward-backward algorithm: finds marginals
83
v
n
a
v
n
a
v
n
a
start
end

























so p(v a n) = (1/z) * product weight of one path
marginal id203 p(x2 = a)
		 = (1/z) * total weight of all paths through
a
x2
x3
x1


forward-backward algorithm: finds marginals
84
v
n
a
v
n
a
v
n
a
start
end

























so p(v a n) = (1/z) * product weight of one path
marginal id203 p(x2 = a)
		 = (1/z) * total weight of all paths through
n
x2
x3
x1


forward-backward algorithm: finds marginals
85
v
n
a
v
n
a
v
n
a
start
end

























so p(v a n) = (1/z) * product weight of one path
marginal id203 p(x2 = a)
		 = (1/z) * total weight of all paths through
v
x2
x3
x1


forward-backward algorithm: finds marginals
86
v
n
a
v
n
a
v
n
a
start
end

























so p(v a n) = (1/z) * product weight of one path
marginal id203 p(x2 = a)
		 = (1/z) * total weight of all paths through
n
 (found by id145: matrix-vector products)
x2
x3
x1


forward-backward algorithm: finds marginals
87
v
n
a
v
n
a
v
n
a
start
end


























= total weight of these
   path suffixes

x2
x3
x1


forward-backward algorithm: finds marginals
88
v
n
a
v
n
a
v
n
a
start
end

























   2(n)
 (found by id145: matrix-vector products)

= total weight of these
   path suffixes

x2
x3
x1


forward-backward algorithm: finds marginals
89
v
n
a
v
n
a
v
n
a
start
end

























   2(n)
(a + b + c)
(x + y + z)
total weight of all paths through
	=                           
x2
x3
x1


forward-backward algorithm: finds marginals
90
v
n
a
v
n
a
v
n
a
start
end

























n


   belief that x2 = n   
oops! the weight of a path through a state also includes a weight at that state.
so   (n)     (n) isn   t enough.

the extra weight is the opinion of the unigram factor at this variable.
x2
x3
x1


forward-backward algorithm: finds marginals
91
v
n
a
n
a
v
n
a
start
end

























   belief that x2 = v   
v
   belief that x2 = n   


total weight of all paths through
	=                           
v
x2
x3
x1


forward-backward algorithm: finds marginals
92
v
n
a
v
n
a
v
n
a
start
end


























  {2}(a)
  2(a)
   2(a)
   belief that x2 = a   
   belief that x2 = v   
   belief that x2 = n   




divide 
by z=6 to get marginal probs
total weight of all paths through
	=                           
a



(acyclic) belief propagation
93
in a factor graph with no cycles: 
pick any node to serve as the root.
send messages from the leaves to the root.
send messages from the root to the leaves.
a node computes an outgoing message along an edge 
only after it has received incoming messages along all its other edges.





(acyclic) belief propagation
94
in a factor graph with no cycles: 
pick any node to serve as the root.
send messages from the leaves to the root.
send messages from the root to the leaves.
a node computes an outgoing message along an edge 
only after it has received incoming messages along all its other edges.


acyclic bp as id145
95
x1
  1
x2
  3
x3
  5
x4
  7
x5
  9
x6
  10

  12
xi
  14
x9
  13
  11
f
g
h
figure adapted from 
burkett & klein (2012)
acyclic bp as id145
96
x1
x2
x3
x4
  7
x5
  9
x6
  10

xi
x9
  11
h
subproblem:
id136 using just the factors in subgraph h

the marginal of xi  in 
that smaller model is the message sent to xi from subgraph h

message to
a variable
acyclic bp as id145
97
x1
x2
x3
  5
x4
x5
x6

xi
  14
x9
g
message to
a variable
acyclic bp as id145
98
x1
  1
x2
  3
x3
x4
x5
x6
x		8
  12
xi
x9
  13
f
message to
a variable




acyclic bp as id145
99
x1
  1
x2
  3
x3
  5
x4
  7
x5
  9
x6
  10

  12
xi
  14
x9
  13
  11
f
h
subproblem:
id136 using just the factors in subgraph f   h

the marginal of xi  in 
that smaller model is the message sent by xi 
out of subgraph f   h




message from
a variable
if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
100


if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
101



if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
102




if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
103



if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
104




if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
105




if you want the marginal pi(xi) where xi has degree k, you can think of that summation as a product of k marginals computed on smaller subgraphs.
each subgraph is obtained by cutting some edge of the tree.
the message-passing algorithm uses id145 to compute the marginals on all such subgraphs, working from smaller to bigger.  so you can compute all the marginals.
acyclic bp as id145
106







loopy belief propagation
107
messages from different subgraphs are 
no longer independent!
id145 can   t help.  it   s now #p-hard in general to compute the exact marginals. 
but we can still run bp -- it's a local algorithm so it doesn't "see the cycles."


  2
  4
  6
  8
what if our graph has cycles?


what can go wrong with loopy bp?
108
f

all 4 factors
on cycle 
enforce 
equality
f
f
f
what can go wrong with loopy bp?
109
t

all 4 factors
on cycle 
enforce 
equality
t
t
t
this factor says
upper variable
is twice as likely
to be true as false
(and that   s the true
marginal!)
this is an extreme example.  often in practice, the cyclic influences are weak.  (as cycles are long or include at least one weak correlation.)
bp incorrectly treats this message as separate evidence that the variable is t. 
multiplies these two messages as if they were independent.
but they don   t actually come from independent parts of the graph.
one influenced the other (via a cycle).  

what can go wrong with loopy bp?
110



all 4 factors
on cycle 
enforce 
equality




this factor says
upper variable
is twice as likely
to be t as f
(and that   s the true
marginal!)



messages loop around and around    
2, 4, 8, 16, 32, ... more and more convinced that these variables are t! 
so beliefs converge to marginal distribution (1, 0) rather than (2/3, 1/3).





kathy	 
says so
bob 
says so
charlie 
says so
alice 
says so
your prior doesn   t think obama owns it.
but everyone   s saying he does.  under a 
na  ve bayes model, you therefore believe it.

what can go wrong with loopy bp?
111
   
obama
 owns it





a lie told often enough becomes truth.
-- lenin 
a rumor is circulating that obama secretly owns an insurance company. (obamacare is actually designed to maximize his profit.)






kathy	 
says so
bob 
says so
charlie 
says so
alice 
says so
better model ... rush can influence conversation.
now there are 2 ways to explain why everyone   s repeating the story: it   s true, or rush said it was.
the model favors one solution (probably rush).
yet bp has 2 stable solutions.  each solution is self-reinforcing around cycles; no impetus to switch.

what can go wrong with loopy bp?
112
   
obama
 owns it








a lie told often enough becomes truth.
-- lenin 
if everyone blames obama, then no one has to blame rush.
but if no one blames rush, then everyone has to continue to blame obama (to explain the gossip). 
loopy belief propagation algorithm
run the bp update equations on a cyclic graph
hope it    works    anyway (good approximation)
though we multiply messages that aren   t independent
no interpretation as id145
if largest element of a message gets very big or small,
divide the message by a constant to prevent over/underflow
can update messages in any order
stop when the normalized messages converge
compute beliefs from final messages
return normalized beliefs as approximate marginals
113
e.g., murphy, weiss & jordan (1999)
input: a factor graph with cycles
output: approximate marginals for each variable and factor

algorithm:
initialize the messages to the uniform distribution.


send messages until convergence.
normalize them when they grow too large.


compute the beliefs (unnormalized marginals).


normalize beliefs and return the approximate marginals.

 
loopy belief propagation
114
section 2 appendix
tensor notation for bp
115
tensor notation for bp
in section 2, bp was introduced with a notation which defined messages and beliefs as functions.

this appendix includes an alternate (and very concise) notation for the belief propagation algorithm using tensors.


tensor notation
tensor multiplication:



tensor marginalization:
117
tensor notation
118
a real function with r keyword arguments
axis-labeled array with arbitrary indices
database with column headers

a rank-r tensor is    
=
=
tensor multiplication: (vector outer product)
tensor notation
119
a real function with r keyword arguments
axis-labeled array with arbitrary indices
database with column headers

a rank-r tensor is    
=
=
tensor multiplication: (vector pointwise product)
tensor notation
120
a real function with r keyword arguments
axis-labeled array with arbitrary indices
database with column headers

a rank-r tensor is    
=
=
tensor multiplication: (matrix-vector product)
tensor notation
121
a real function with r keyword arguments
axis-labeled array with arbitrary indices
database with column headers

a rank-r tensor is    
=
=
tensor marginalization:
input: a factor graph with no cycles
output: exact marginals for each variable and factor

algorithm:
initialize the messages to the uniform distribution.

choose a root node.
send messages from the leaves to the root.
send messages from the root to the leaves.


compute the beliefs (unnormalized marginals).


normalize beliefs and return the exact marginals.

 
sum-product belief propagation
122
sum-product belief propagation
123



sum-product belief propagation
124



sum-product belief propagation
125

variable belief
sum-product belief propagation
126
variable message
sum-product belief propagation
127
factor belief

sum-product belief propagation
128
factor message
input: a factor graph with cycles
output: approximate marginals for each variable and factor

algorithm:
initialize the messages to the uniform distribution.


send messages until convergence.
normalize them when they grow too large.


compute the beliefs (unnormalized marginals).


normalize beliefs and return the approximate marginals.

 
loopy belief propagation
129
section 3:
belief propagation q&a
methods like bp and in what sense they work
130
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
131
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
132
q&a
133
max-product belief propagation
sum-product bp can be used to 
compute the marginals, pi(xi)

max-product bp can be used to 
compute the most likely assignment,
 x* = argmaxx p(x)
134
max-product belief propagation
change the sum to a max:




max-product bp computes max-marginals 
the max-marginal bi(xi) is the (unnormalized) id203 of the map assignment under the constraint xi = xi.
for an acyclic graph, the map assignment (assuming there are no ties) is given by: 

135
max-product belief propagation
change the sum to a max:




max-product bp computes max-marginals 
the max-marginal bi(xi) is the (unnormalized) id203 of the map assignment under the constraint xi = xi.
for an acyclic graph, the map assignment (assuming there are no ties) is given by: 

136
deterministic annealing
motivation: smoothly transition from sum-product to max-product

incorporate inverse temperature parameter into each factor:



send messages as usual for sum-product bp
anneal t from 1 to 0:



take resulting beliefs to power t

137
q&a
138
=
   
from arc consistency to bp
goal: find a satisfying assignment
algorithm: arc consistency
pick a constraint
reduce domains to satisfy the constraint
repeat until convergence
139








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   








propagation completely solved the problem!  












note: these steps can occur in somewhat arbitrary order
slide thanks to rina dechter (modified)
=
   
from arc consistency to bp
goal: find a satisfying assignment
algorithm: arc consistency
pick a constraint
reduce domains to satisfy the constraint
repeat until convergence
140








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   








propagation completely solved the problem!  












note: these steps can occur in somewhat arbitrary order
slide thanks to rina dechter (modified)
arc consistency is a special case of belief propagation.
   
from arc consistency to bp
solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
141








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=




   
from arc consistency to bp
solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
142








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=





   
from arc consistency to bp
143








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=





solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
   
from arc consistency to bp
144








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=





solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
   
from arc consistency to bp
145








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=







solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
   
from arc consistency to bp
146








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=







solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
   
from arc consistency to bp
147








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=







solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence
   
from arc consistency to bp
148








3
2,
1,
3
2,
1,
3
2,
1,
 x, y, u, t     {1, 2, 3}
x     y
y = u
t     u
x < t
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=




solve the same problem with bp
constraints become    hard    factors with only 1   s or 0   s
send messages until convergence








loopy bp will converge to the equivalent solution!
   
from arc consistency to bp
149








3
2,
1,
3
2,
1,
3
2,
1,
x
y
t
u
3
2,
1,
   
   
slide thanks to rina dechter (modified)
=












loopy bp will converge to the equivalent solution!
takeaways:
arc consistency is a special case of belief propagation.
arc consistency will only rule out impossible values.
bp rules out those same values 
(belief = 0).

q&a
150
from id150 to particle bp to bp
message representation:
belief propagation: full distribution
id150: single particle
particle bp:
multiple particles
151

# of particles



from id150 to particle bp to bp
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
152
from id150 to particle bp to bp
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
153
approach 1: id150
for each variable, resample the value by conditioning on all the other variables 
called the    full conditional    distribution
computationally easy because we really only need to condition on the markov blanket 
we can view the computation of the full conditional in terms of message passing
message puts all its id203 mass on the current particle (i.e. current value)


from id150 to particle bp to bp
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
154
approach 1: id150
for each variable, resample the value by conditioning on all the other variables 
called the    full conditional    distribution
computationally easy because we really only need to condition on the markov blanket 
we can view the computation of the full conditional in terms of message passing
message puts all its id203 mass on the current particle (i.e. current value)


from id150 to particle bp to bp
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
155
approach 1: id150
for each variable, resample the value by conditioning on all the other variables 
called the    full conditional    distribution
computationally easy because we really only need to condition on the markov blanket 
we can view the computation of the full conditional in terms of message passing
message puts all its id203 mass on the current particle (i.e. current value)





from id150 to particle bp to bp
156
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
too
tight
to
to
from id150 to particle bp to bp
157
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
too
tight
to
to
approach 2: multiple gibbs samplers
run each gibbs sampler independently
full conditionals computed independently
k separate messages that are each a pointmass distribution
from id150 to particle bp to bp
158
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
too
tight
to
to
approach 3: id150 w/averaging
keep k samples for each variable
resample from the average of the full conditionals for each possible pair of variables
message is a uniform distribution over current particles


from id150 to particle bp to bp
159
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
too
tight
to
to
approach 3: id150 w/averaging
keep k samples for each variable
resample from the average of the full conditionals for each possible pair of variables
message is a uniform distribution over current particles


from id150 to particle bp to bp
160
meant
to
type
man
too
tight
meant
two
taipei
mean
to
type
too
tight
to
to
approach 3: id150 w/averaging
keep k samples for each variable
resample from the average of the full conditionals for each possible pair of variables
message is a uniform distribution over current particles




from id150 to particle bp to bp
161
meant
type
man
tight
meant
taipei
mean
type
approach 4: particle bp
similar in spirit to id150 w/averaging
messages are a weighted distribution over k particles

(ihler & mcallester, 2009) 
from id150 to particle bp to bp
162
approach 5: bp
in particle bp, as the number of particles goes to +   , the estimated messages approach the true bp messages 
belief propagation represents messages as the full distribution
this assumes we can store the whole distribution compactly

(ihler & mcallester, 2009) 
from id150 to particle bp to bp
message representation:
belief propagation: full distribution
id150: single particle
particle bp:
multiple particles
163

# of particles
from id150 to particle bp to bp
sampling values or combinations of values:
quickly get a good estimate of the frequent cases
may take a long time to estimate probabilities of infrequent cases
may take a long time to draw a sample (mixing time)
exact if you run forever
enumerating each value and computing its id203 exactly:
have to spend time on all values
but only spend o(1) time on each value (don   t sample frequent values over and over while waiting for infrequent ones)
runtime is more predictable
lets you tradeoff exactness for greater speed (brute force exactly enumerates exponentially many assignments, bp approximates this by enumerating local configurations)
164
tension between approaches   
background: convergence
when bp is run on a tree-shaped factor graph, the beliefs converge to the marginals of the distribution after two passes.
165
q&a
166

  1

  2


  1
  2
  2
  2






q&a
167

  1

  2


  1
  2
  2
  2

  1

  2


  1
  2
  2
  2
q&a
168







q&a
169
q&a
170
we   ve found the bottom!!
q&a
171
might not be globally consistent in the sense of all being views of the same elephant.
*cartoon by g. renee guzlas
q&a
172
x1
    
x2



a variable belief and a factor belief are locally consistent if the marginal of the factor   s belief equals the variable   s belief.
 
q&a
173
we   ve found the bottom!!
q&a
174




q&a
175
[*] though we can   t just minimize each function separately     we need message passing to keep the beliefs locally consistent.
on an acyclic factor graph, it measures kl divergence between beliefs and true marginals, and so is minimized when beliefs = marginals.  (for a loopy graph, we close our eyes and hope it still works.)
section 3: appendix
bp as an optimization algorithm
176
bp as an optimization algorithm
this appendix provides a more in-depth study of bp as an optimization algorithm. 

our focus is on the bethe free energy and its relation to kl divergence, gibbs free energy, and the helmholtz free energy.

we also include a discussion of the convergence properties of max-product bp.
177
kl and free energies
178
gibbs free energy
kullback   leibler (kl) divergence
helmholtz free energy
minimizing kl divergence
if we find the distribution b that minimizes the kl divergence, then b = p




also, true of the minimum of the gibbs free energy
but what if b is not (necessarily) a id203 distribution?
179
true distribution:
bp on a 2 variable chain
180
beliefs at the end of bp:
*where u(x) is the uniform distribution
true distribution:
bp on a 3 variable chain
181
kl decomposes over the marginals

define the joint belief to have the same form:
the true distribution can be expressed in terms of its marginals:
true distribution:
bp on a 3 variable chain
182
gibbs free energy
decomposes over the marginals

define the joint belief to have the same form:
the true distribution can be expressed in terms of its marginals:
true distribution:
bp on an acyclic graph
183
kl decomposes over the marginals

define the joint belief to have the same form:
the true distribution can be expressed in terms of its marginals:
true distribution:
bp on an acyclic graph
184
the true distribution can be expressed in terms of its marginals:
define the joint belief to have the same form:

gibbs free energy
decomposes over the marginals
true distribution:
bp on a loopy graph
185
construct the joint belief as before:
kl is no longer well defined, because the joint belief is not a proper distribution.
this might not be a distribution! 
the beliefs are distributions: are non-negative and sum-to-one.
the beliefs are locally consistent:
 
so add constraints   

true distribution:
bp on a loopy graph
186
construct the joint belief as before:
this is called the bethe free energy
 and decomposes over the marginals
but we can still optimize the same objective as before, subject to our belief constraints:
this might not be a distribution! 
the beliefs are distributions: are non-negative and sum-to-one.
the beliefs are locally consistent:
 
so add constraints   

bp as an optimization algorithm
the bethe free energy, a function of the beliefs:




bp minimizes a constrained version of the bethe free energy
bp is just one local optimization algorithm: fast but not guaranteed to converge
if bp converges, the beliefs are called fixed points
the stationary points of a function have a gradient of zero

187
the fixed points of bp are local stationary points of the bethe free energy (yedidia, freeman, & weiss, 2000)
bp as an optimization algorithm
the bethe free energy, a function of the beliefs:




bp minimizes a constrained version of the bethe free energy
bp is just one local optimization algorithm: fast but not guaranteed to converge
if bp converges, the beliefs are called fixed points
the stationary points of a function have a gradient of zero

188
the stable fixed points of bp are local minima of the bethe free energy (heskes, 2003)
bp as an optimization algorithm
for graphs with no cycles:
the minimizing beliefs = the true marginals
bp finds the global minimum of the bethe free energy
this global minimum is 
   log z (the    helmholtz free energy   )
for graphs with cycles:
the minimizing beliefs only approximate the true marginals
attempting to minimize may get stuck at local minimum or other critical point
even the global minimum only approximates    log z 

189
convergence of sum-product bp
the fixed point beliefs:
do not necessarily correspond to marginals of any joint distribution over all the variables (mackay, yedidia, freeman, & weiss, 2001; yedidia, freeman, & weiss, 2005)
unbelievable probabilities
conversely, the true marginals for many joint distributions cannot be reached by bp (pitkow, ahmadian, & miller, 2011) 
190
figure adapted from (pitkow, ahmadian, & miller, 2011) 
the figure shows a two-dimensional slice of the bethe free energy for a binary graphical model with pairwise interactions
convergence of max-product bp
191
if the max-marginals bi(xi) are a fixed point of bp, and x* is the corresponding assignment (assumed unique), then p(x*) > p(x) for every x     x* in a rather large neighborhood around x* (weiss & freeman, 2001).
figure from (weiss & freeman, 2001) 
informally: if you take the fixed-point solution x* and arbitrarily change the values of the dark nodes in the figure, the overall id203 of the configuration will decrease.
the neighbors of x* are constructed as follows: for any set of vars s of disconnected trees and single loops, set the variables in s to arbitrary values, and the rest to x*.
convergence of max-product bp
192
if the max-marginals bi(xi) are a fixed point of bp, and x* is the corresponding assignment (assumed unique), then p(x*) > p(x) for every x     x* in a rather large neighborhood around x* (weiss & freeman, 2001).
figure from (weiss & freeman, 2001) 
informally: if you take the fixed-point solution x* and arbitrarily change the values of the dark nodes in the figure, the overall id203 of the configuration will decrease.
the neighbors of x* are constructed as follows: for any set of vars s of disconnected trees and single loops, set the variables in s to arbitrary values, and the rest to x*.
convergence of max-product bp
193
if the max-marginals bi(xi) are a fixed point of bp, and x* is the corresponding assignment (assumed unique), then p(x*) > p(x) for every x     x* in a rather large neighborhood around x* (weiss & freeman, 2001).
figure from (weiss & freeman, 2001) 
informally: if you take the fixed-point solution x* and arbitrarily change the values of the dark nodes in the figure, the overall id203 of the configuration will decrease.
the neighbors of x* are constructed as follows: for any set of vars s of disconnected trees and single loops, set the variables in s to arbitrary values, and the rest to x*.
section 4:
incorporating structure into 
factors and variables

194
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
195
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
196
bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



197
t
t
t
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



198
t
t
t
t
t
t
t
t
t


aligner









sending messages:
computational complexity
199
o(d*kd)
d = # of neighboring variables
k = maximum # possible values for a neighboring variable
o(d*k) 
d = # of neighboring factors
k = # possible values for xi
sending messages:
computational complexity
200
o(d*kd)
d = # of neighboring variables
k = maximum # possible values for a neighboring variable
o(d*k) 
d = # of neighboring factors
k = # possible values for xi

incorporating structure into factors

201
unlabeled constituency parsing
202
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  10
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
f
  10
f
  10
f
  10
f
  10
f
  10
f
  10
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
203
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  10
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
f
  10
f
  10
f
  10
f
  10
f
  10
f
  10

(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
204
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  10
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
f
  10
f
  10
f
  10
f
  10
f
  10
t
  10

(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
205
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  10
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
f
  10
f
  10
f
  10
f
  10
f
  10
f
  10
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
206
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
207
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
f
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
208
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  10
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.
f
  10
f
  10
f
  10
f
  10
f
  10
f
  10
sending a messsage to a variable from its unary factors takes only o(d*kd) time where k=2 and d=1.
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
209
t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  12
t
  11
t
  13
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.

but nothing prevents non-tree structures.



sending a messsage to a variable from its unary factors takes only o(d*kd) time where k=2 and d=1.
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
210
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.

but nothing prevents non-tree structures.

add a ckytree factor which multiplies in 1 if the variables form a tree and 0 otherwise.

t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  12
t
  11
t
  13
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
211
given: a sentence.
predict: unlabeled parse.

we could predict whether each span is present t or not f.

but nothing prevents non-tree structures.

add a ckytree factor which multiplies in 1 if the variables form a tree and 0 otherwise.

t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  12
t
  11
t
  13
(naradowsky, vieira, & smith, 2012)
unlabeled constituency parsing
212
add a ckytree factor which multiplies in 1 if the variables form a tree and 0 otherwise.

t
  1
  2
t
  3
  4
t
  5
  6
t
  7
  8
t
  9
t
  12
t
  11
t
  13
how long does it take to send a message to a variable from the the ckytree factor?

for the given sentence, o(d*kd) time where k=2 and d=15.

for a length n sentence, this will be o(2n*n).

but we know an algorithm (inside-outside) to compute all the marginals in o(n3)   
so can   t we do better?
(naradowsky, vieira, & smith, 2012)
example: the exactly1 factor
213
(smith & eisner, 2008)
variables: d binary variables x1,    , xd
global factor:
example: the exactly1 factor
214
(smith & eisner, 2008)
variables: d binary variables x1,    , xd
global factor:
example: the exactly1 factor
215
(smith & eisner, 2008)
variables: d binary variables x1,    , xd
global factor:
example: the exactly1 factor
216
(smith & eisner, 2008)
variables: d binary variables x1,    , xd
global factor:
example: the exactly1 factor
217
(smith & eisner, 2008)
variables: d binary variables x1,    , xd
global factor:
example: the exactly1 factor
218
(smith & eisner, 2008)
variables: d binary variables x1,    , xd
global factor:
messages: the exactly1 factor
219
o(d*2d)
d = # of neighboring variables
o(d*2) 
d = # of neighboring factors
messages: the exactly1 factor
220
o(d*2d)
d = # of neighboring variables
o(d*2) 
d = # of neighboring factors
fast!
messages: the exactly1 factor
221
o(d*2d)
d = # of neighboring variables
messages: the exactly1 factor
222
but the outgoing messages from the exactly1 factor are defined as a sum over the 2d possible assignments to x1,    , xd. 
conveniently,   e1(xa) is 0 for all but d values     so the sum is sparse!

so we can compute all the outgoing messages from   e1 in o(d) time!
o(d*2d)
d = # of neighboring variables
messages: the exactly1 factor
223
but the outgoing messages from the exactly1 factor are defined as a sum over the 2d possible assignments to x1,    , xd. 
conveniently,   e1(xa) is 0 for all but d values     so the sum is sparse!

so we can compute all the outgoing messages from   e1 in o(d) time!
o(d*2d)
d = # of neighboring variables
fast!

messages: the exactly1 factor
224
(smith & eisner, 2008)
a factor has a belief about each of its variables.
an outgoing message from a factor is the factor's belief with the incoming message divided out.
we can compute the exactly1 factor   s beliefs about each of its variables efficiently. (each of the parenthesized terms needs to be computed only once for all the variables.)
example: the ckytree factor
variables: o(n2) binary variables sij
global factor:

225
(naradowsky, vieira, & smith, 2012)
messages: the ckytree factor
226
o(d*2d)
d = # of neighboring variables
o(d*2) 
d = # of neighboring factors
messages: the ckytree factor
227
o(d*2d)
d = # of neighboring variables
o(d*2) 
d = # of neighboring factors
fast!
messages: the ckytree factor
228
o(d*2d)
d = # of neighboring variables
but the outgoing messages from the ckytree factor are defined as a sum over the o(2n*n) possible assignments to {sij}. 
messages: the ckytree factor
229
  ckytree(xa) is 1 for exponentially many values in the sum     but they all correspond to trees!

with inside-outside we can compute all the outgoing messages from ckytree in o(n3) time!
o(d*2d)
d = # of neighboring variables
but the outgoing messages from the ckytree factor are defined as a sum over the o(2n*n) possible assignments to {sij}. 
messages: the ckytree factor
230
  ckytree(xa) is 1 for exponentially many values in the sum     but they all correspond to trees!

with inside-outside we can compute all the outgoing messages from ckytree in o(n3) time!
o(d*2d)
d = # of neighboring variables
fast!

example: the ckytree factor
231
(naradowsky, vieira, & smith, 2012)
for a length n sentence, define an anchored weighted id18 (wid18).
each span is weighted by the ratio of the incoming messages from the corresponding span variable.
run the inside-outside algorithm on the sentence a1, a1,    , an with the anchored wid18.
example: the trigramid48 factor
232
(smith & eisner, 2008)

factors can compactly encode the preferences of an entire sub-model.
consider the joint distribution of a trigram id48 over 5 variables:
it   s traditionally defined as a bayes network
but we can represent it as a (loopy) factor graph
we could even pack all those factors into a single trigramid48 factor (smith & eisner, 2008)


example: the trigramid48 factor
233
(smith & eisner, 2008)

factors can compactly encode the preferences of an entire sub-model.
consider the joint distribution of a trigram id48 over 5 variables:
it   s traditionally defined as a bayes network
but we can represent it as a (loopy) factor graph
we could even pack all those factors into a single trigramid48 factor (smith & eisner, 2008)


example: the trigramid48 factor
234
(smith & eisner, 2008)

x1
  1
  2
x2
  3
  4
x3
  5
  6
x4
  7
  8
x5
  9
  10
  11
  12
factors can compactly encode the preferences of an entire sub-model.
consider the joint distribution of a trigram id48 over 5 variables:
it   s traditionally defined as a bayes network
but we can represent it as a (loopy) factor graph
we could even pack all those factors into a single trigramid48 factor


example: the trigramid48 factor
factors can compactly encode the preferences of an entire sub-model.
consider the joint distribution of a trigram id48 over 5 variables:
it   s traditionally defined as a bayes network
but we can represent it as a (loopy) factor graph
we could even pack all those factors into a single trigramid48 factor


235
(smith & eisner, 2008)

x1
x2
x3
x4
x5

example: the trigramid48 factor
236
(smith & eisner, 2008)

x1
x2
x3
x4
x5

variables: d discrete variables x1,    , xd
global factor:

example: the trigramid48 factor
237
(smith & eisner, 2008)

x1
x2
x3
x4
x5

variables: d discrete variables x1,    , xd
global factor:

compute outgoing messages efficiently with the standard trigram id48 id145 algorithm (junction tree)!
combinatorial factors
usually, it takes o(kd) time to compute outgoing messages from a factor over d variables with k possible values each.  
but not always:
factors like exactly1 with only polynomially many nonzeroes in the potential table
factors like ckytree with exponentially many nonzeroes but in a special pattern
factors like trigramid48 with all nonzeroes but which factor further
238
example nlp constraint factors:
projective and non-projective dependency parse constraint (smith & eisner, 2008)
id35 parse constraint (auli & lopez, 2011)
labeled and unlabeled constituency parse constraint (naradowsky, vieira, & smith, 2012)
inversion transduction grammar (itg) constraint (burkett & klein, 2012)

combinatorial factors
factor graphs can encode structural constraints on many variables via constraint factors.
239
combinatorial optimization within max-product
max-product bp computes max-marginals.
the max-marginal bi(xi) is the (unnormalized) id203 of the map assignment under the constraint xi = xi.
duchi et al. (2006) define factors, over many variables, for which efficient combinatorial optimization algorithms exist.
bipartite matching: max-marginals can be computed with standard max-flow algorithm and the floyd-warshall all-pairs shortest-paths algorithm.
minimum cuts: max-marginals can be computed with a min-cut algorithm.
similar to sum-product case: the combinatorial algorithms are embedded within the standard loopy bp algorithm.
240
(duchi, tarlow, elidan, & koller, 2006)
structured bp vs. 
id209
241
(duchi, tarlow, elidan, & koller, 2006)
(koo et al., 2010; rush et al., 2010)
additional resources
see naacl 2012 / acl 2013 tutorial by burkett & klein    variational id136 in structured nlp models    for    
an alternative approach to efficient marginal id136 for nlp: structured mean field
also, includes structured bp
242
http://nlp.cs.berkeley.edu/tutorials/variational-tutorial-slides.pdf
sending messages:
computational complexity
243
o(d*kd)
d = # of neighboring variables
k = maximum # possible values for a neighboring variable
o(d*k) 
d = # of neighboring factors
k = # possible values for xi
sending messages:
computational complexity
244
o(d*kd)
d = # of neighboring variables
k = maximum # possible values for a neighboring variable
o(d*k) 
d = # of neighboring factors
k = # possible values for xi

incorporating structure into variables

245
bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



246
t
  2
t
  4
t
f
f
f
la
blanca
casa


t
t
t
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



247
t
  2
t
  4
t
f
f
f
la
casa


t
t
t
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
t



bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



248
t
  2
t
  4
t
f
f
f
la
casa


t
t
t
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
t



bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



249
t
  2
t
  4
t
f
f
f
la
casa


f
t
f
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
blanca





string-valued variables
consider two examples from section 1:
250
variables (string):
english and japanese orthographic strings
english and japanese phonological strings
interactions:
all pairs of strings could be relevant
variables (string): 
inflected forms 
of the same verb
interactions: 
between pairs of entries in the table 
(e.g. infinitive form affects present-singular)
id114 over strings
251
  1
x2
x1
(dreyer & eisner, 2009)

most of our problems so far:
used discrete variables
over a small finite set of string values
examples:
id52
labeled constituency parsing
id33
we use tensors (e.g. vectors, matrices) to represent the messages and factors


id114 over strings
252
  1
x2
x1
(dreyer & eisner, 2009)

  1
x2
x1

what happens as the # of possible values for a variable, k, increases?
 
we can still keep the computational complexity down by including only low arity factors (i.e. small d).
time complexity:
var.     fac. o(d*kd)
fac.     var. o(d*k)
id114 over strings
253
  1
x2
x1
(dreyer & eisner, 2009)

  1
x2
x1

but what if the domain of a variable is   *, the infinite set of all possible strings?

how can we represent a distribution over one or more infinite sets?
id114 over strings
254
  1
x2
x1
(dreyer & eisner, 2009)

  1
x2
x1

  1
x2
x1
finite state machines let us represent something infinite in finite space!
id114 over strings
255
(dreyer & eisner, 2009)
  1
x2
x1

  1
x2
x1
finite state machines let us represent something infinite in finite space!
messages and beliefs are weighted finite state acceptors (wfsa)

factors are weighted finite state transducers (wfst)


id114 over strings
256
(dreyer & eisner, 2009)
  1
x2
x1

  1
x2
x1
finite state machines let us represent something infinite in finite space!
messages and beliefs are weighted finite state acceptors (wfsa)

factors are weighted finite state transducers (wfst)


that solves the problem of representation.

but how do we manage the problem of computation? (we still need to compute messages and beliefs.)
id114 over strings
257
(dreyer & eisner, 2009)
all the message and belief computations simply reuse standard id122 id145 algorithms.
id114 over strings
258
(dreyer & eisner, 2009)
  1
x2







  1
  1
the pointwise product of two wfsas is   

   their intersection.

compute the product of (possibly many) messages        i  (each of which is a wsfa) via wfsa intersection


id114 over strings
259
(dreyer & eisner, 2009)
  1
x2
x1

compute marginalized product of wfsa message   k      and wfst factor     , with: domain(compose(    ,   k     ))
compose: produces a new wfst with a distribution over (xi, xj)
domain: marginalizes over xj to obtain a wfsa over xi only


id114 over strings
260
(dreyer & eisner, 2009)
all the message and belief computations simply reuse standard id122 id145 algorithms.
the usual nlp toolbox
261
wfsa: weighted finite state automata
wfst:  weighted finite state transducer
k-tape wid122: weighted finite state machine jointly mapping between k strings
they each assign a score to a set of strings.

we can interpret them as factors in a graphical model. 

the only difference is the arity of the factor.
wfsa: weighted finite state automata
wfst:  weighted finite state transducer
k-tape wid122: weighted finite state machine jointly mapping between k strings


wfsa as a factor graph
262
x1
  1
  1 =

  1(x1) = 4.25 

a wfsa is a function which maps a string to a score.

wfsa: weighted finite state automata
wfst:  weighted finite state transducer
k-tape wid122: weighted finite state machine jointly mapping between k strings
wfst as a factor graph
263
x2
x1
  1
(dreyer, smith, & eisner, 2008)
  1(x1, x2) = 13.26 

a wfst is a function that maps a pair of strings to a score.





k-tape wid122 as a factor graph
264
wfsa: weighted finite state automata
wfst:  weighted finite state transducer
k-tape wid122: weighted finite state machine jointly mapping between k strings

  1(x1, x2, x3, x4) = 13.26 

a k-tape wid122 is a function that maps k strings to a score.

what's wrong with a 100-tape wid122 for jointly modeling the 100 distinct forms of a polish verb?
each arc represents a 100-way edit operation
too many arcs!
factor graphs over multiple strings
p(x1, x2, x3, x4) = 1/z   1(x1, x2)   2(x1, x3)   3(x1, x4)   4(x2, x3)   5(x3, x4)
265
  1
  2
  3
  4
  5
x2
x1
x4
x3
(dreyer & eisner, 2009)
instead, just build factor graphs with wfst factors (i.e. factors of arity 2)
factor graphs over multiple strings
p(x1, x2, x3, x4) = 1/z   1(x1, x2)   2(x1, x3)   3(x1, x4)   4(x2, x3)   5(x3, x4)
266
  1
  2
  3
  4
  5
x2
x1
x4
x3
(dreyer & eisner, 2009)
instead, just build factor graphs with wfst factors (i.e. factors of arity 2)
bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



267
t
  2
t
  4
t
f
f
f
la
casa


f
t
f
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
blanca



bp for coordination of algorithms
each factor is tractable by id145
overall model is no longer tractable, but bp lets us pretend it is



268
t
  2
t
  4
t
f
f
f
la
casa


f
t
f
t
t
t
t
t
t


aligner
tagger
parser
tagger
parser
blanca












section 5:
what if even bp is slow?
computing fewer message updates
computing them faster
269
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
270
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
271
loopy belief propagation algorithm
for every directed edge, initialize its message to the uniform distribution.
repeat until all normalized beliefs converge:
pick a directed edge u     v.   
update its message: recompute u     v from its    parent    messages v        u for v        v.    


loopy belief propagation algorithm
for every directed edge, initialize its message to the uniform distribution.
repeat until all normalized beliefs converge:
pick a directed edge u     v.   
update its message: recompute u     v from its    parent    messages v        u for v        v.    



which edge do we pick and recompute?
a    stale    edge?



message passing in belief propagation
274
x
 
  



   


 
   

   






stale messages
275
x
 
  



   


 
   

   

we update this message from its antecedents.
now it   s    fresh.    don   t need to update it again.




antecedents

stale messages
276
x
 
  



   


 
   

   





antecedents
but it again becomes    stale        out of sync with its antecedents     if they change.
then we do need to revisit.
we update this message from its antecedents.
now it   s    fresh.    don   t need to update it again.



stale messages
277

 
  



   


 
   

   



for a high-degree node that likes to update all its outgoing messages at once    
we say that the whole node is very stale if its incoming messages have changed a lot.




stale messages
278

 
  



   


 
   

   



for a high-degree node that likes to update all its outgoing messages at once    
we say that the whole node is very stale if its incoming messages have changed a lot.




maintain an queue of stale messages to update
messages from factors are stale.
messages from variables
are actually fresh (in sync
with their uniform antecedents).
initially all messages are uniform.

maintain an queue of stale messages to update

maintain an queue of stale messages to update




maintain an queue of stale messages to update

a priority queue!  (heap)

residual bp:  always update the message that is most stale (would be most changed by an update).

maintain a priority queue of stale edges (& perhaps variables).
each step of residual bp:    pop and update.   
prioritize by degree of staleness.
when something becomes stale, put it on the queue.
if it becomes staler, move it earlier on the queue.
need a measure of staleness.
so, process biggest updates first.
dramatically improves speed of convergence. 
and chance of converging at all.    
(elidan et al., 2006)
but what about the topology?
283
in a graph with no cycles:
send messages from the leaves to the root.
send messages from the root to the leaves.
each outgoing message is sent only after all its incoming messages have been received.


x1

x2

x3

x4

x5

x6

x8

x7

x9

a bad update order for residual bp!
try updating an entire acyclic subgraph
285
tree-based reparameterization (wainwright et al. 2001); also see residual splash
try updating an entire acyclic subgraph
286
x1

x2

x3

x4

x5

x6

x8

x7

x9

tree-based reparameterization (wainwright et al. 2001); also see residual splash
pick this subgraph;
update leaves to root, 
then root to leaves
try updating an entire acyclic subgraph
287
x1

x2

x3

x4

x5

x6

x8

x7

x9



tree-based reparameterization (wainwright et al. 2001); also see residual splash
another subgraph;
update leaves to root, 
then root to leaves
try updating an entire acyclic subgraph
288
x1

x2

x3

x4

x5

x6
x8

x7

x9



tree-based reparameterization (wainwright et al. 2001); also see residual splash
another subgraph;
update leaves to root, 
then root to leaves
at every step, pick a spanning
tree (or spanning forest)
that covers many stale edges
as we update messages in the tree, it affects staleness of messages outside the tree
acyclic belief propagation
289
in a graph with no cycles:
send messages from the leaves to the root.
send messages from the root to the leaves.
each outgoing message is sent only after all its incoming messages have been received.


don   t update a message if its antecedents will get a big update.

otherwise, will have to re-update.
summary of update ordering
asynchronous
pick a directed edge: update its message
or, pick a vertex: update all its outgoing messages at once
290
in what order do we send messages for loopy bp?
size.  send big updates first.
forces other messages to wait for them.
topology.  use graph structure.
e.g., in an acyclic graph, a message can wait for all updates before sending.

wait for your antecedents
message scheduling
synchronous (bad  idea)
compute all the messages
send all the messages
asynchronous
pick an edge: compute and send that message
tree-based reparameterization 
successively update embedded spanning trees 
choose spanning trees such that each edge is included in at least one
residual bp 
pick the edge whose message would change the most if sent: compute and send that message
291


figure from (elidan, mcgraw, & koller, 2006)
the order in which messages are sent has a significant effect on convergence
message scheduling
292
(jiang, moon, daum   iii, & eisner, 2013)

even better dynamic scheduling may be possible by id23 of a problem-specific heuristic for choosing which edge to update next.


section 5:
what if even bp is slow?
computing fewer message updates
computing them faster
293
a variable has k possible values.
what if k is large or infinite?
computing variable beliefs
294

suppose   
xi is a discrete variable
each incoming messages is a multinomial

pointwise product is easy when the variable   s domain is small and discrete
computing variable beliefs
suppose   
xi is a real-valued variable
each incoming message is a gaussian

the pointwise product of n gaussians is   
   a gaussian!


295





computing variable beliefs
suppose   
xi is a real-valued variable
each incoming messages is a mixture of k gaussians

the pointwise product explodes!
296






p(x) = p1(x) p2(x)   pn(x)
(  0.3 q1,1(x)
+ 0.7 q1,2(x))
(  0.5 q2,1(x)
+ 0.5 q2,2(x))


computing variable beliefs
297

suppose   
xi is a string-valued variable (i.e. its domain is the set of all strings)
each incoming messages is a fsa

the pointwise product explodes!
example: string-valued variables
messages can grow larger when sent through a transducer factor
repeatedly sending messages through a transducer can cause them to grow to unbounded size!
298
x2
x1
  2


a


a
  
a
a
a
a
(dreyer & eisner, 2009)
  1
a
a

example: string-valued variables
messages can grow larger when sent through a transducer factor
repeatedly sending messages through a transducer can cause them to grow to unbounded size!
299
x2
x1
  2


a
  
a
a
a
a
(dreyer & eisner, 2009)
  1
a
a



a
a
example: string-valued variables
messages can grow larger when sent through a transducer factor
repeatedly sending messages through a transducer can cause them to grow to unbounded size!
300
x2
x1
  2
  
a
a
a
(dreyer & eisner, 2009)
  1
a
a


a
a



a
a
a
example: string-valued variables
messages can grow larger when sent through a transducer factor
repeatedly sending messages through a transducer can cause them to grow to unbounded size!
301
x2
x1
  2
  
a
a
a
(dreyer & eisner, 2009)
  1
a
a



a
a
a


a
a
a
example: string-valued variables
messages can grow larger when sent through a transducer factor
repeatedly sending messages through a transducer can cause them to grow to unbounded size!
302
x2
x1
  2
  
a
a
a
(dreyer & eisner, 2009)
  1
a
a



a
a
a
a
a
a
a
a
a
a
a
a
a



a
a
example: string-valued variables
messages can grow larger when sent through a transducer factor
repeatedly sending messages through a transducer can cause them to grow to unbounded size!
303
x2
x1
  2
  
a
a
a
(dreyer & eisner, 2009)
  1
a
a



a
a
a
a
a
a
a
a
a
a
a
a
a



a
a
the domain of these variables is infinite (i.e.   *); 
wsfa   s representation is finite     but the size of the representation can grow
in cases where the domain of each variable is small and finite this is not an issue
message approximations
three approaches to dealing with complex messages:
particle belief propagation (see section 3)
message pruning
expectation propagation
304
for real variables, try a mixture of k gaussians:
e.g., true product is a mixture of kd gaussians
prune back: randomly keep just k of them
chosen in proportion to weight in full mixture
id150 to efficiently choose them



what if incoming messages are not gaussian mixtures?
could be anything sent by the factors    
can extend technique to this case.
message pruning
problem: product of d messages = complex distribution.
solution: approximate with a simpler distribution.
for speed, compute approximation without computing full product.
305
(sudderth et al., 2002       nonparametric bp   )
problem: product of d messages = complex distribution.
solution: approximate with a simpler distribution.
for speed, compute approximation without computing full product.
for string variables, use a small finite set:
each message   i gives positive id203 to    
    every word in a 50,000 word vocabulary
    every string in    * (using a weighted fsa)



prune back to a list l of a few    good    strings
each message adds its own k best strings to l
for each x     l, let   (x) =       i   i(x)     each message scores x
for each x     l, let   (x) = 0
message pruning
306
(dreyer & eisner, 2009)
problem: product of d messages = complex distribution.
solution: approximate with a simpler distribution.
for speed, compute approximation without computing full product.
ep provides four special advantages over pruning:

general recipe that can be used in many settings.
efficient.  uses approximations that are very fast.
conservative.  unlike pruning, never forces b(x) to 0.
never kills off a value x that had been possible.
adaptive.  approximates   (x) more carefully 
if x is favored by the other messages.
tries to be accurate on the most    plausible    values.
expectation propagation (ep)
307
(minka, 2001; heskes & zoeter, 2002) 
expectation propagation (ep)
x1


x2


x3


x4

x7


x5

belief at x3 
will be simple!
messages to and from x3 will be simple!
key idea: approximate variable x   s incoming messages   .
we force them to have a simple parametric form:
 	    				  (x) = exp (       f(x))         log-linear model    (unnormalized)
where f(x) extracts a feature vector from the value x.
for each variable x, we   ll choose a feature function f.
expectation propagation (ep)
309
so by storing a few parameters   , we   ve defined   (x) for all x.
now the messages are super-easy to multiply:

          1(x)   2(x) = exp (       f(x)) exp (       f(x)) = exp ((  1+  2)     f(x))

represent a message by its parameter vector   .
to multiply messages, just add their    vectors!
so beliefs and outgoing messages also have this simple form.
expectation propagation
x1


x2


x3


x4

x7



exponential-family
approximations inside



x5

form of messages/beliefs at x3?
always   (x) = exp (       f(x))
if x is real:
gaussian: take f(x) = (x,x2)
if x is string:
globally normalized trigram model: take f(x) = (count of aaa, count of aab,     count of zzz)
if x is discrete:
arbitrary discrete distribution 
(can exactly represent original message, so we get ordinary bp)
coarsened discrete distribution, based on features of x 

can   t use mixture models, or other models that use latent variables 
to define   (x) =    y p(x, y)
expectation propagation
x1


x2


x3


x4

x7



exponential-family
approximations inside



x5

each message to x3 is
          (x) = exp (       f(x))
for some   .  we only store   .

to take a product of such messages, just add their   
easily compute belief at x3 (sum of incoming    vectors)
then easily compute each outgoing message
(belief minus one incoming   )
all very easy    
expectation propagation
x1


x2


x3


x4

x7







x5


but what about messages from factors?
like the message m4.
this is not exponential family!  uh-oh!
it   s just whatever the factor happens to send.

this is where we need to approximate, by   4 .
  4


x5
expectation propagation
x1


x2


x3


x4
x7






  1
  2
  3
  4
m4
blue    = arbitrary distribution, 
green = simple distribution exp (       f(x)) 

the belief at x    should    be
p(x) =   1(x)       2(x)       3 (x)     m4(x)
but we   ll be using
b(x) =   1(x)       2(x)       3 (x)       4(x)

choose the simple distribution b that minimizes kl(p || b).
then, work backward from belief b to message   4.  
take    vector of b and subtract 
off the    vectors of   1,   2,   3.  
chooses   4 to preserve belief well.

find b   s params    in closed 
form     or follow gradient:
ex~p[f(x)]     ex~b[f(x)]

fit model that predicts same counts
broadcast id165 counts 
ml estimation = moment matching
fsa approx. = moment matching
(can compute with forward-backward)







fit model that predicts same fractional counts

kl(       ||       )

finds parameters    that minimize kl    error    in belief
  
fsa approx. = moment matching
min  
?
kl(       ||       )
how to approximate a message?




finds message parameters    that minimize kl    error    
of resulting belief
  
wisely, kl doesn   t insist on good approximations for values that are low-id203 in the belief 
kl(       ||       )
analogy: scheduling by 
approximate email messages




wisely, kl doesn   t insist on good approximations for values that are low-id203 in the belief 
   i prefer tue/thu, and i   m away the last week of july.   
(this is an approximation to my true schedule.  i   m not actually free on all tue/thu, but the bad tue/thu dates have already been ruled out by messages from other folks.) 
expectation propagation
319
(hall & klein, 2012)
example: factored pid18s
task: constituency parsing, with factored annotations
lexical annotations
parent annotations
latent annotations
approach:
sentence specific approximation is an anchored grammar:
q(a     b c, i, j, k)
sending messages is equivalent to marginalizing out the annotations











adaptive
approximation
section 6:
approximation-aware training

320
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
321
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
322
modern nlp
323
nlp
machine learning for nlp
324
linguistics inspires the structures we want to predict
machine learning for nlp
325
our model defines a score for each structure
   
machine learning for nlp
326
it also tells us what to optimize
our model defines a score for each structure
   
machine learning for nlp
327
learning tunes the parameters of the model
given training instances{(x1, y1), (x2, y2),   , (xn, yn)}
find the best model parameters,   
machine learning for nlp
328
learning tunes the parameters of the model
given training instances{(x1, y1), (x2, y2),   , (xn, yn)}
find the best model parameters,   
machine learning for nlp
329
learning tunes the parameters of the model
given training instances{(x1, y1), (x2, y2),   , (xn, yn)}
find the best model parameters,   
machine learning for nlp
330
id136 finds the best structure for a new sentence
given a new sentence, xnew
search over the set of all possible structures (often exponential in size of xnew)
return the highest scoring structure, y*

(id136 is usually called as a subroutine in learning)
machine learning for nlp
331
id136 finds the best structure for a new sentence
given a new sentence, xnew
search over the set of all possible structures (often exponential in size of xnew)
return the minimum bayes risk (mbr) structure, y*

(id136 is usually called as a subroutine in learning)
machine learning for nlp
332
id136 finds the best structure for a new sentence
given a new sentence, xnew
search over the set of all possible structures (often exponential in size of xnew)
return the minimum bayes risk (mbr) structure, y*

(id136 is usually called as a subroutine in learning)
332
easy
polynomial time
np-hard
modern nlp
333
linguistics inspires the structures we want to predict
it also tells us what to optimize
our model defines a score for each structure
learning tunes the parameters of the model
id136 finds the best structure for a new sentence
(id136 is usually called as a subroutine in learning)
an abstraction for modeling
334







now we can work at this level of abstraction.
training
thus far, we   ve seen how to compute (approximate) marginals, given a factor graph   

   but where do the potential tables      come from?
some have a fixed structure (e.g. exactly1, ckytree)
others could be trained ahead of time (e.g. trigramid48)
for the rest, we define them parametrically and learn the parameters!
335
two ways to learn:

standard crf training
(very simple; often yields state-of-the-art results)
erma 
(less simple; but takes approximations and id168 into account)

standard crf parameterization
define each potential function in terms of a fixed set of feature functions:
336
observed
variables
predicted
variables
standard crf parameterization
define each potential function in terms of a fixed set of feature functions:
337


standard crf parameterization
define each potential function in terms of a fixed set of feature functions:
338


what is training?
that   s easy: 

	training = picking good model parameters!


but how do we know if the 
model parameters are any    good   ?
339
conditional log-likelihood training
choose model
such that derivative in #3 is ea
choose objective: 
assign high id203 to the things we observe and low id203 to everything else
340
compute derivative by hand using the chain rule
replace exact id136 by approximate id136
conditional log-likelihood training
choose model 
such that derivative in #3 is easy
choose objective: 
assign high id203 to the things we observe and low id203 to everything else
341
compute derivative by hand using the chain rule
replace exact id136 by approximate id136



we can approximate the 
factor marginals by the (normalized) 
factor beliefs from bp!

stochastic id119
input: 
training data, {(x(i), y(i)) : 1     i     n }
initial model parameters,   
output: 
trained model parameters,   .

algorithm:
   while not converged:
sample a training example (x(i), y(i))
compute the gradient of log(p  (y(i) | x(i))) with respect to our model parameters   . 
take a (small) step in the direction of the gradient.
342
what   s wrong with the usual approach?
if you add too many factors, your predictions might get worse!
the model might be richer, but we replace the true marginals with approximate marginals (e.g. beliefs computed by bp)
approximate id136 can cause gradients for structured learning to go awry! (kulesza & pereira, 2008).
343
what   s wrong with the usual approach?
mistakes made by standard crf training:
using bp (approximate)
not taking id168 into account
should be doing mbr decoding

big pile of approximations   
					   which has tunable parameters.

treat it like a neural net, and run backprop!
344
modern nlp
345
linguistics inspires the structures we want to predict
it also tells us what to optimize
our model defines a score for each structure
learning tunes the parameters of the model
id136 finds the best structure for a new sentence
(id136 is usually called as a subroutine in learning)




empirical risk minimization
1. given training data:
346
2. choose each of these:
decision function


id168




examples: id75, id28, neural network
examples: mean-squared error, cross id178




empirical risk minimization
1. given training data:
3. define goal:
347
2. choose each of these:
decision function


id168




4. train with sgd:
(take small steps opposite the gradient)




1. given training data:
3. define goal:
348
2. choose each of these:
decision function


id168




4. train with sgd:
(take small steps opposite the gradient)

empirical risk minimization
conditional log-likelihood training
choose model 
such that derivative in #3 is easy
choose objective: 
assign high id203 to the things we observe and low id203 to everything else
349
compute derivative by hand using the chain rule
replace true id136 by approximate id136



what went wrong?
how did we compute these approximate marginal probabilities anyway?
350
by belief propagation of course!
error back-propagation
351
slide from (stoyanov & eisner, 2012)
error back-propagation
352
slide from (stoyanov & eisner, 2012)
error back-propagation
353
slide from (stoyanov & eisner, 2012)
error back-propagation
354
slide from (stoyanov & eisner, 2012)
error back-propagation
355
slide from (stoyanov & eisner, 2012)
error back-propagation
356
slide from (stoyanov & eisner, 2012)
error back-propagation
357
slide from (stoyanov & eisner, 2012)
error back-propagation
358
slide from (stoyanov & eisner, 2012)
error back-propagation
359
slide from (stoyanov & eisner, 2012)
error back-propagation
360
  
slide from (stoyanov & eisner, 2012)
error back-propagation
applying the chain rule of differentiation over and over.
forward pass:
regular computation (id136 + decoding) in the model (+ remember intermediate quantities).
backward pass:
replay the forward pass in reverse, computing gradients.
361
background: backprop through time
recurrent neural network:

bptt: 
1. unroll the computation over time
362
(robinson & fallside, 1987)
(werbos, 1988)
(mozer, 1995)
a
xt
bt
xt+1
yt+1

2. run backprop through the resulting feed-forward network
what went wrong?
how did we compute these approximate marginal probabilities anyway?
363
by belief propagation of course!
erma
empirical risk minimization under approximations (erma)
apply backprop through time to loopy bp
unrolls the bp computation graph
includes id136, decoding, loss and all the approximations along the way
364
(stoyanov, ropson, & eisner, 2011)
erma
choose model to be the computation with all its approximations
choose objective
 to likewise include the approximations
compute derivative by id26 (treating the entire computation as if it were a neural network)
make no approximations!
(our gradient is exact)
365
key idea: open up the black box!
(stoyanov, ropson, & eisner, 2011)
erma
empirical risk minimization
366
key idea: open up the black box!
minimum bayes risk (mbr) decoder
(stoyanov, ropson, & eisner, 2011)
approximation-aware learning
what if we   re using structured bp instead of regular bp?
no problem, the same approach still applies!
the only difference is that we embed id145 algorithms inside our computation graph.


367
key idea: open up the black box!
(gorid113y, dredze, & eisner, 2015)
connection to deep learning
368

y
exp(  y   f(x))



(gorid113y, yu, & dredze, in submission)
empirical risk minimization under approximations (erma)
369
id113
figure from (stoyanov & eisner, 2012)
section 7:
software

370
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
371
outline
do you want to push past the simple nlp models (id28, pid18, etc.) that we've all been using for 20 years?
then this tutorial is extremely practical for you!
models: factor graphs can express interactions among linguistic structures.
algorithm: bp estimates the global effect of these interactions on each variable, using local computations.
intuitions: what   s going on here?  can we trust bp   s estimates?
fancier models: hide a whole grammar and id145 algorithm within a single factor.  bp coordinates multiple factors. 
tweaked algorithm: finish in fewer steps and make the steps faster.
learning: tune the parameters.  approximately improve the true predictions -- or truly improve the approximate predictions.
software: build the model you want!
372
pacaya
373
features: 
structured loopy bp over factor graphs with:
discrete variables
structured constraint factors 
(e.g. projective dependency tree constraint factor)
erma training with id26
backprop through structured factors 
(gorid113y, dredze, & eisner, 2015)
language: java
authors: gorid113y, mitchell, & wolfe
url: http://www.cs.jhu.edu/~mrg/software/ 
(gorid113y, mitchell, van durme, & dredze, 2014)
(gorid113y, dredze, & eisner, 2015)
erma
374
features: 
erma performs id136 and training on crfs and mrfs with arbitrary model structure over discrete variables. the training regime, empirical risk minimization under approximations is loss-aware and approximation-aware. erma can optimize several id168s such as accuracy, mse and f-score. 

language: java
authors: stoyanov
url: https://sites.google.com/site/ermasoftware/ 
(stoyanov, ropson, & eisner, 2011)
(stoyanov & eisner, 2012)
id114 libraries
factorie (mccallum, shultz, & singh, 2012) is a scala library allowing modular specification of id136, learning, and optimization methods.  id136 algorithms include belief propagation and mcmc. learning settings include maximum likelihood learning, maximum margin learning, learning with approximate id136, samplerank, pseudo-likelihood.
http://factorie.cs.umass.edu/ 

libdai (mooij, 2010) is a c++ library that supports id136, but not learning, via loopy bp, fractional bp, tree-reweighted bp, (double-loop) generalized bp, variants of loop corrected belief propagation, conditioned belief propagation, and tree expectation propagation.
http://www.libdai.org 

opengm2 (andres, beier, & kappes, 2012) provides a c++ template library for discrete factor graphs with support for learning and id136 (including tie-ins to all libdai id136 algorithms).
http://hci.iwr.uni-heidelberg.de/opengm2/

fastinf (jaimovich, meshi, mcgraw, elidan) is an efficient approximate id136 library in c++.
http://compbio.cs.huji.ac.il/fastinf/fastinf/fastinf_homepage.html 

infer.net (minka et al., 2012) is a .net language framework for id114 with support for expectation propagation and variational message passing.
http://research.microsoft.com/en-us/um/cambridge/projects/infernet

375
references

376

m. auli and a. lopez,    a comparison of loopy belief propagation and id209 for integrated id35 id55 and parsing,    in proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, portland, oregon, usa, 2011, pp. 470   480.
m. auli and a. lopez,    training a log-linear parser with id168s via softmax-margin,    in proceedings of the 2011 conference on empirical methods in natural language processing, edinburgh, scotland, uk., 2011, pp. 333   343.
y. bengio,    training a neural network with a financial criterion rather than a prediction criterion,    in decision technologies for financial engineering: proceedings of the fourth international conference on neural networks in the capital markets (nncm   96), world scientific publishing, 1997, pp. 36   48.
d. p. bertsekas and j. n. tsitsiklis, parallel and distributed computation: numerical methods. prentice-hall, inc., 1989.
d. p. bertsekas and j. n. tsitsiklis, parallel and distributed computation: numerical methods. athena scientific, 1997.
l. bottou and p. gallinari,    a framework for the cooperation of learning algorithms,    in advances in neural information processing systems, vol. 3, d. touretzky and r. lippmann, eds. denver: morgan kaufmann, 1991.
r. bunescu and r. j. mooney,    collective information extraction with relational markov networks,    2004, p. 438   es.
c. burfoot, s. bird, and t. baldwin,    collective classification of congressional floor-debate transcripts,    presented at the proceedings of the 49th annual meeting of the association for computational linguistics: human language techologies, 2011, pp. 1506   1515.
d. burkett and d. klein,    fast id136 in phrase extraction models with belief propagation,    presented at the proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: human language technologies, 2012, pp. 29   38.
t. cohn and p. blunsom,    semantic role labelling with tree id49,    presented at the proceedings of the ninth conference on computational natural language learning (conll-2005), 2005, pp. 169   172.
377

f. cromier  s and s. kurohashi,    an alignment algorithm using belief propagation and a structure-based distortion model,    in proceedings of the 12th conference of the european chapter of the acl (eacl 2009), athens, greece, 2009, pp. 166   174.
m. dreyer,    a non-parametric model for the discovery of inflectional paradigms from plain text using id114 over strings,    johns hopkins university, baltimore, md, usa, 2011.
m. dreyer and j. eisner,    id114 over multiple strings,    presented at the proceedings of the 2009 conference on empirical methods in natural language processing, 2009, pp. 101   110.
m. dreyer and j. eisner,    discovering morphological paradigms from plain text using a dirichlet process mixture model,    presented at the proceedings of the 2011 conference on empirical methods in natural language processing, 2011, pp. 616   627.
j. duchi, d. tarlow, g. elidan, and d. koller,    using combinatorial optimization within max-product belief propagation,    advances in neural information processing systems, 2006.
g. durrett, d. hall, and d. klein,    decentralized entity-level modeling for coreference resolution,    presented at the proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: long papers), 2013, pp. 114   124.
g. elidan, i. mcgraw, and d. koller,    residual belief propagation: informed scheduling for asynchronous message passing,    in proceedings of the twenty-second conference on uncertainty in ai (uai, 2006.
k. gimpel and n. a. smith,    softmax-margin crfs: training id148 with cost functions,    in human language technologies: the 2010 annual conference of the north american chapter of the association for computational linguistics, los angeles, california, 2010, pp. 733   736.
j. gonzalez, y. low, and c. guestrin,    residual splash for optimally parallelizing belief propagation,    in international conference on artificial intelligence and statistics, 2009, pp. 177   184.
d. hall and d. klein,    training factored pid18s with expectation propagation,    presented at the proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, 2012, pp. 1146   1156.

378

t. heskes,    stable fixed points of loopy belief propagation are minima of the bethe free energy,    advances in neural information processing systems, vol. 15, pp. 359   366, 2003.
t. heskes and o. zoeter,    expectation propagation for approximate id136 in dynamic id110s,    uncertainty in artificial intelligence, 2002, pp. 216-233.
a. t. ihler, j. w. fisher iii, a. s. willsky, and d. m. chickering,    loopy belief propagation: convergence and effects of message errors.,    journal of machine learning research, vol. 6, no. 5, 2005.
a. t. ihler and d. a. mcallester,    particle belief propagation,    in international conference on artificial intelligence and statistics, 2009, pp. 256   263.
j. jancsary, j. matiasek, and h. trost,    revealing the structure of medical dictations with id49,    presented at the proceedings of the 2008 conference on empirical methods in natural language processing, 2008, pp. 1   10.
j. jiang, t. moon, h. daum   iii, and j. eisner,    prioritized asynchronous belief propagation,    in icml workshop on inferning, 2013.
a. kazantseva and s. szpakowicz,    linear text segmentation using affinity propagation,    presented at the proceedings of the 2011 conference on empirical methods in natural language processing, 2011, pp. 284   293.
t. koo and m. collins,    hidden-variable models for discriminative reranking,    presented at the proceedings of human language technology conference and conference on empirical methods in natural language processing, 2005, pp. 507   514.
a. kulesza and f. pereira,    structured learning with approximate id136.,    in nips, 2007, vol. 20, pp. 785   792.
j. lee, j. naradowsky, and d. a. smith,    a discriminative model for joint id60 and id33,    in proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, portland, oregon, usa, 2011, pp. 885   894.
s. lee,    structured discriminative model for dialog state tracking,    presented at the proceedings of the sigdial 2013 conference, 2013, pp. 442   451.

379

x. liu, m. zhou, x. zhou, z. fu, and f. wei,    joint id136 of id39 and id172 for tweets,    presented at the proceedings of the 50th annual meeting of the association for computational linguistics (volume 1: long papers), 2012, pp. 526   535.
d. j. c. mackay, j. s. yedidia, w. t. freeman, and y. weiss,    a conversation about the bethe free energy and sum-product,    merl, tr2001-18, 2001.
a. martins, n. smith, e. xing, p. aguiar, and m. figueiredo,    turbo parsers: id33 by approximate variational id136,    presented at the proceedings of the 2010 conference on empirical methods in natural language processing, 2010, pp. 34   44.
d. mcallester, m. collins, and f. pereira,    case-factor diagrams for structured probabilistic modeling,    in in proceedings of the twentieth conference on uncertainty in artificial intelligence (uai   04), 2004.
t. minka,    divergence measures and message passing,    technical report, microsoft research, 2005.
t. p. minka,    expectation propagation for approximate bayesian id136,    in uncertainty in artificial intelligence, 2001, vol. 17, pp. 362   369.
m. mitchell, j. aguilar, t. wilson, and b. van durme,    open domain targeted sentiment,    presented at the proceedings of the 2013 conference on empirical methods in natural language processing, 2013, pp. 1643   1654.
k. p. murphy, y. weiss, and m. i. jordan,    loopy belief propagation for approximate id136: an empirical study,    in proceedings of the fifteenth conference on uncertainty in artificial intelligence, 1999, pp. 467   475.
t. nakagawa, k. inui, and s. kurohashi,    dependency tree-based sentiment classification using crfs with hidden variables,    presented at the human language technologies: the 2010 annual conference of the north american chapter of the association for computational linguistics, 2010, pp. 786   794.
j. naradowsky, s. riedel, and d. smith,    improving nlp through marginalization of hidden syntactic structure,    in proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, 2012, pp. 810   820.
j. naradowsky, t. vieira, and d. a. smith, grammarless parsing for joint id136. mumbai, india, 2012.
j. niehues and s. vogel,    discriminative word alignment via alignment matrix modeling,    presented at the proceedings of the third workshop on id151, 2008, pp. 18   25.

380

j. pearl, probabilistic reasoning in intelligent systems: networks of plausible id136. morgan kaufmann, 1988.
x. pitkow, y. ahmadian, and k. d. miller,    learning unbelievable probabilities,    in advances in neural information processing systems 24, j. shawe-taylor, r. s. zemel, p. l. bartlett, f. pereira, and k. q. weinberger, eds. curran associates, inc., 2011, pp. 738   746.
v. qazvinian and d. r. radev,    identifying non-explicit citing sentences for citation-based summarization.,    presented at the proceedings of the 48th annual meeting of the association for computational linguistics, 2010, pp. 555   564.
h. ren, w. xu, y. zhang, and y. yan,    dialog state tracking using id49,    presented at the proceedings of the sigdial 2013 conference, 2013, pp. 457   461.
d. roth and w. yih,    probabilistic reasoning for entity & relation recognition,    presented at the coling 2002: the 19th international conference on computational linguistics, 2002.
a. rudnick, c. liu, and m. gasser,    hltdi: cl-wsd using markov random fields for semeval-2013 task 10,    presented at the second joint conference on lexical and computational semantics (*sem), volume 2: proceedings of the seventh international workshop on semantic evaluation (semeval 2013), 2013, pp. 171   177.
t. sato,    inside-outside id203 computation for belief propagation.,    in ijcai, 2007, pp. 2605   2610.
d. a. smith and j. eisner,    id33 by belief propagation,    in proceedings of the conference on empirical methods in natural language processing (emnlp), honolulu, 2008, pp. 145   156.
v. stoyanov and j. eisner,    fast and accurate prediction via evidence-specific mrf structure,    in icml workshop on inferning: interactions between id136 and learning, edinburgh, 2012.
v. stoyanov and j. eisner,    minimum-risk training of approximate crf-based nlp systems,    in proceedings of naacl-hlt, 2012, pp. 120   130.

381

v. stoyanov, a. ropson, and j. eisner,    empirical risk minimization of graphical model parameters given approximate id136, decoding, and model structure,    in proceedings of the 14th international conference on artificial intelligence and statistics (aistats), fort lauderdale, 2011, vol. 15, pp. 725   733.
e. b. sudderth, a. t. ihler, w. t. freeman, and a. s. willsky,    nonparametric belief propagation,    mit, technical report 2551, 2002.
e. b. sudderth, a. t. ihler, w. t. freeman, and a. s. willsky,    nonparametric belief propagation,    in in proceedings of cvpr, 2003.
e. b. sudderth, a. t. ihler, m. isard, w. t. freeman, and a. s. willsky,    nonparametric belief propagation,    communications of the acm, vol. 53, no. 10, pp. 95   103, 2010.
c. sutton and a. mccallum,    collective segmentation and labeling of distant entities in information extraction,    in icml workshop on statistical relational learning and its connections to other fields, 2004.
c. sutton and a. mccallum,    piecewise training of undirected models,    in conference on uncertainty in artificial intelligence (uai), 2005.
c. sutton and a. mccallum,    improved dynamic schedules for belief propagation,    uai, 2007.
m. j. wainwright, t. jaakkola, and a. s. willsky,    tree-based reparameterization for approximate id136 on loopy graphs.,    in nips, 2001, pp. 1001   1008.
z. wang, s. li, f. kong, and g. zhou,    collective personal profile summarization with social networks,    presented at the proceedings of the 2013 conference on empirical methods in natural language processing, 2013, pp. 715   725.
y. watanabe, m. asahara, and y. matsumoto,    a graph-based approach to named entity categorization in wikipedia using id49,    presented at the proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (emnlp-conll), 2007, pp. 649   657.

382

y. weiss and w. t. freeman,    on the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs,    id205, ieee transactions on, vol. 47, no. 2, pp. 736   744, 2001.
j. s. yedidia, w. t. freeman, and y. weiss,    bethe free energy, kikuchi approximations, and belief propagation algorithms,    merl, tr2001-16, 2001.
j. s. yedidia, w. t. freeman, and y. weiss,    constructing free-energy approximations and generalized belief propagation algorithms,    ieee transactions on id205, vol. 51, no. 7, pp. 2282   2312, jul. 2005.
j. s. yedidia, w. t. freeman, and y. weiss,    generalized belief propagation,    in nips, 2000, vol. 13, pp. 689   695.
j. s. yedidia, w. t. freeman, and y. weiss,    understanding belief propagation and its generalizations,    exploring artificial intelligence in the new millennium, vol. 8, pp. 236   239, 2003.
j. s. yedidia, w. t. freeman, and y. weiss,    constructing free energy approximations and generalized belief propagation algorithms,    merl, tr-2004-040, 2004.
j. s. yedidia, w. t. freeman, and y. weiss,    constructing free-energy approximations and generalized belief propagation algorithms,    id205, ieee transactions on, vol. 51, no. 7, pp. 2282   2312, 2005.

383
