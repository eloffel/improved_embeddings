acl 2008:  semi-supervised learning tutorial
john blitzer and xiaojin zhu
http://ssl-acl08.wikidot.com
what is semi-supervised learning (ssl)?
labeled data (entity classification)


lots more unlabeled data
person 
location
organization



    , says mr. cooper, vice  
  president of    
    firing line inc., a 
  philadelphia gun shop.



    , yahoo   s own jerry yang is right    

    the details of obama   s san francisco mis-adventure    
labels

can we build a better model from both labeled and unlabeled data?


who else has worked on ssl?
canonical nlp problems
tagging    (haghighi and klein 2006)
chunking, ner (ando & zhang 2005)
parsing (mcclosky & charniak 2006)

outside the classic nlp canon
entity-attribute extraction (bellare et al. 2007)
id31 (goldberg & zhu 2006)
link spam detection (zhou et al. 2007)
your problem?

anti-ssl arguments: practice
if a problem is important, we   ll find the time / money / linguists to label more data
penn chinese treebank 
2 years to annotate 4000 sentences
ip-hln
vv
np-sbj
np
np
dt
nn
nn
nn
   
   
      
         
      
the national track & field championships concluded
i want to parse the baidu zhidao question-answer database. 
who   s going to annotate it for me?



anti-ssl arguments: theory
    but tom cover said   : (castelli & cover 1996)
under a specific generative model, labeled samples are exponentially more useful than unlabeled
the semi-supervised models in this tutorial make different assumptions than c&c (1996)
today we   ll also discuss new, positive theoretical results in semi-supervised learning
why semi-supervised learning?
i have a good idea, but i can   t afford to label lots of data! 
i have lots of labeled data, but i have even more unlabeled data
ssl:  it   s not just for small amounts of labeled data anymore!
id20: i have labeled data from 1 domain, but i want a model for a different domain
goals of this tutorial
cover the most common classes of semi-supervised learning algorithms
for each major class, give examples of where it has been used for nlp
give you the ability to know which type of algorithm is right for your problem
suggest advice for avoiding pitfalls in semi-supervised learning



overview
id64 (50 minutes)
co-training
latent variables with linguistic side information
graph-id173 (45 minutes)
structural learning (55 minutes)
entity recognition, id20, and theoretical analysis
some notation
id64: outline
the general id64 procedure
co-training and co-boosting
applications to entity classification and entity-attribute extraction
ssl with latent variables, prototype learning and applications



id64
on labeled data, minimize error
on unlabeled data, minimize a proxy for error derived from the current model

most semi-supervised learning models in nlp

train model on labeled data
repeat until converged
label unlabeled data with current model
retrain model on unlabeled data
back to named entities
na  ve bayes model


parameters estimated from counts 

features:  <left word =    mr.   >
label:     person   

id64 step

estimate parameters

label unlabeled data

retrain model
      data

says mr. cooper, vice president

mr. balmer has already faxed

mr. balmer has already faxed
update action



label balmer    person   


id64 folk wisdom
id64 works better for generative models than for discriminative models
discriminative models can overfit some features
generative models are forced to assign id203 mass to all features with some count 

id64 works better when the na  ve bayes assumption is stronger
   mr.    is not predictive of    balmer    if we know the entity is a person
two views and co-training
make id64 folk wisdom explicit
there are two views of a problem. 
assume each view is sufficient to do good classification
named entity classification (nec)
2 views:  context vs. content
says mr. cooper, a vice president of . . .



general co-training procedure
on labeled data, maximize accuracy 
on unlabeled data, constrain models from different views to agree with one another
with multiple views, any supervised learning algorithm can be co-trained
co-boosting for named entity classification
a brief review of supervised boosting
boosting runs for t=1   t rounds.
on round t, we choose a base model            and weight  
for nlp, the model at round t,
    identifies the presence of a particular 
    feature and guesses or abstains
final model:  


collins and singer (1999)
boosting objective
loss
0-1 loss
exp loss

current model, steps 1. . .t-1
co-boosting objective

view 2 loss

view 1 loss



superscript: view
subscript: round of boosting
unlabeled co-regularizer
scores of individual ensembles ( x- and y-axis ) vs.
co-regularizer term ( z-axis )





score magnitude not important for agreement
score magnitude important for disagreement


co-boosting updates
optimize each view separately.  
set hypothesis     ,     to minimize


similarly for view 1

each greedy update is guaranteed to decrease one view of the objective
basic co-boosting walk-through
labeled:  mr. balmer has already faxed 
unlabeled: 	says mr. smith, vice president of  
		adam smith wrote    the wealth of nations   
co-boosting step

update context view

label unlabeled data

update content view

label unlabeled data


update context view
 data

mr. balmer has already faxed

says mr. smith, vice president

says mr. smith, vice president

adam smith wrote    the wealth of nations   

adam smith wrote . . .
update action



label     person   



label     person   




co-boosting nec results
data:  90,000 unlabeled named entities
seeds:  location     new york, california, u.s
               person context     mr.
               organization name     i.b.m., microsoft	
               organization context     incorporated	
create labeled data using seeds as rules
whenever i see mr. ____, label it as a person
results
baseline (most frequent) 45%  co-boosting: 91%
entity-attribute extraction
entities: companies, countries, people
attributes: c.e.o., market capitalization, border, prime minister, age, employer, address
extracting entity-attribute pairs from sentences
the	population	of	china		exceeds
l			        x		m    	     y		      r
l,m,r = context  	x,y = content
bellare et al. (2008)



data and learning
input: seed list of entities and attributes
2 views: context and content
training: co-training decision lists and self-trained maxent classifier
problem: no negative instances
just treat all unlabeled instances as negative
re-label most confident positive instances
examples of learned attributes
countries and attributes
<malaysia, problems>, <nepal, districts>,  <colombia, important highway>
companies and attributes
<limited too, chief executive>,                                <intel, speediest chip>, <uniroyal, former chairman>

where can co-boosting go wrong?
co-boosting enforces agreement on unlabeled data
if only 1 view can classify correctly, this causes errors 
co-boosting step

update context view

label unlabeled data

update content view

label unlabeled data

 data

mr. balmer has already faxed

says mr. cooper, vice president

says mr. cooper, vice president

cooper tires spokesman john
update action



label     person   



label     person   

ssl with latent variables
maximize likelihood treating unlabeled labels as hidden



labeled data gives us basic label-feature structure. maximum likelihood (id113) via em fills in the gaps








y=<person>
y=<hidden>
mw=<cooper>
lw=<mr.>
rw=<president>
lw=<mr.>
mw=<balmer>
rw=<has>



where can id113 go wrong?
unclear when likelihood and error are related
collins & singer (1999) : co-boosting 92%, em: 83% 
mark johnson.  why doesn   t em find good id48 pos-taggers?  emnlp 2007.
how can we fix id113? 
good solutions are high likelihood, even if they   re not maximum likelihood
coming up: constrain solutions to be consistent with linguistic intuition












prototype-driven learning
standard ssl
haghighi and klein 2006






labeled data
unlabeled data
prototype learning (part of speech)











training data
prototypes
 each instance is partially labeled
 prototypes force representative instances to be consistent
using prototypes in id48s

em algorithm:  constrained forward-backward
haghighi and klein (2006) use markov random fields

says	   mr.	  cooper,    vice    president    of	. . .  




. . .
<y=vbd>

mw=president
lw = vice
suffix = dent
<y hidden>
  <y=nn>
incorporating distributional similarity
represent each word by bigram counts with most frequent words on left & right
k-dimensional representation via svd

president
lw=   vice   : 0.1
lw=   the   : 0.02
. . .
rw=   of   : 0.13
. . .
rw=   said   : 0.05
similarity between a word and prototype
we   ll see a similar idea when discussing structural learning
results: id52
 prototype examples (3 prototypes per tag)
 results
results: classified ads
goal: segment housing advertisements

location

terms

restrict

size
remodeled 2 bdrms/1 bath, spacious upper unit, located in hilltop mall area. walking distance to shopping, public transportation, and schools. paid water and garbage. no dogs allowed.  
 prototype examples
results
computed from bag-of-words in current sentence




comments on id64
easy to write down and optimize.  
hard to predict failure cases

co-training encodes assumptions as 2-view agreement
prototype learning enforces linguistically consistent constraints on unsupervised solutions

co-training doesn   t always succeed 
structural learning section
prototype learning needs good sim features to perform well
id178 and id64
haffari & sarkar 2007.  analysis of semi-supervised learning with the yarowsky algorithm.
variants of yarowsky algorithm minimize id178 of 
	p(y | x) on unlabeled data.
other empirical work has looked at minimizing id178 directly.
id178 is not error.
little recent theoretical work connecting id178 & error
more id64 work
mcclosky & charniak (2006).  effective self-training for parsing.  self-trained charniak parser on wsj & nanc.
aria haghighi   s prototype sequence toolkit.  http://code.google.com/p/prototype-sequence-toolkit/
mann & mccallum (2007). expectation id173. similar to prototype learning, but develops a id173 framework for id49.



graph-based semi-supervised learning
from items to graphs
basic graph-based algorithms
mincut
label propagation and harmonic function
manifold id173
advanced graphs
dissimilarities
directed graphs
texpoint fonts used in emf. 
read the texpoint manual before you delete this box.: aaaaaaaaaaa
text classification: easy example
two classes: astronomy vs. travel
document = 0-1 bag-of-word vector
cosine similarity
x1=   bright asteroid   , y1=astronomy
x2=   yellowstone denali   , y2=travel
x3=   asteroid comet   ?
x4=   camp yellowstone   ?
38
easy, by word overlap


hard example
x1=   bright asteroid   , y1=astronomy
x2=   yellowstone denali   , y2=travel
x3=   zodiac   ?
x4=   airport bike   ?
no word overlap
zero cosine similarity
pretend you don   t know english

39
hard example
40
unlabeled data comes to the rescue

41
intuition
some unlabeled documents are similar to the labeled documents     same label
some other unlabeled documents are similar to the above unlabeled documents     same label
ad infinitum

we will formalize this with graphs.
the graph
nodes
weighted, undirected edges
large weight     similar
known labels    
want to know
transduction:
induction:  
how to create a graph
empirically, the following works well:
compute distance between i, j
for each i, connect to its knn.  k very small but still connects the graph
optionally put weights on (only) those edges

4.  tune    



mincut (st-cut)
mincut example: subjectivity
task: classify each sentence in a document into objective/subjective. (pang,lee. acl 2004)
nb/id166 for isolated classification
subjective data (y=1): movie review snippets     bold, imaginative, and impossible to resist   
objective data (y=0): imdb
but there is more   



mincut example: subjectivity
key observation: sentences next to each other tend to have the same label

two special labeled nodes (source, sink)

every sentence connects to both:

mincut example: subjectivity







some issues with mincut
multiple equally min cuts, but different 



lacks classification confidence
these are addressed by id94 and label propagation
















harmonic function
an electric network interpretation


label propagation
the graph laplacian



closed-form solution
harmonic example 1: wsd
wsd from context, e.g.,    interest   ,    line    (niu,ji,tan acl 2005)
xi: context of the ambiguous word, features: pos, words, collocations
dij: cosine similarity or js-divergence
wij: knn graph
labeled data: a few xi   s are tagged with their word sense.
harmonic example 1: wsd
senseval-3, as percent labeled:
(niu,ji,tan acl 2005)
harmonic example 2: sentiment
rating (0-3) from movie reviews (goldberg,zhu. naacl06 workshop)
xi: movie reviews
wij: cosine similarity btw    positive sentence percentage    (psp) vectors of xi, xj
psp classifier trained on    snippet    data (pang,lee. acl 2005)

harmonic example 2: sentiment 
graph                          accuracy





some issues with harmonic function
it fixes the given labels yl
what if some labels are wrong?
it cannot easily handle new test items
transductive, not inductive
add test items to graph, recompute
manifold id173 addresses these issues
manifold id173
manifold example
text classification (sindhwani,niyogi,belkin.icml 2005)
xi: mac/windows.  tfidf.
wij: weighted knn graph




advanced topics
so far edges denote symmetric similarity
larger weights     similar labels
what if we have dissimilarity knowledge?
   two items probably have different labels   
what if the relation is asymmetric?
     related to      but      not always related to
dissimilarity
political view classification 
	(goldberg, zhu, wright. aistats 2007)





they disagree     different classes
indicators: quoting, !?, all caps (internet shouting), etc.
> deshrubinator:    you were the one who thought it should be investigated last week.   
dixie: no i didn   t, and i made it clear. you are insane! you are the one with no ****ing respect for democracy!
dissimilarity
recall to encode similarity between i,j:

wrong ways: small w = no preference; negative w nasty optimization
one solution (also see (tong,jin.aaai07))

overall 
depends on dissim, sim
directed graphs
spam vs. good webpage classification (zhou,burges,tao. airw 2007)
hyperlinks as graph edges, a few webpages manually labeled
directed graphs
directed hyperlink edges





can define an analogous    directed graph laplacian    + manifold id173
spam
x
spam
x
x more likely spam
x may be good
caution
advantages of graph-based methods:
clear intuition, elegant math 
performs well if the graph fits the task
disadvantages:
performs poorly if the graph is bad: sensitive to graph structure and edge weights
usually we do not know which will happen!
structural learning: outline
the structural learning algorithm
application to id39
id20 with structural correspondence learning 
relationship between structural and two-view learning



structural learning
ando and zhang (2005).  use unlabeled data to constrain structure of hypothesis space

given a target problem (entity classification)
design auxiliary problems 
look like target problem
can be trained using unlabeled data

regularize target problem hypothesis to be close to auxiliary problem hypothesis space
what are auxiliary problems?
2 criteria for auxiliary problems
look like target problem
can be trained from unlabeled data
named entity classification: predict presence or absence of left / middle / right words

left
mr.
president
middle
thursday
john
york
right
corp.
inc.
said
running with scissors: a memoir
title: horrible book, horrible.
this book was horrible.  i read half of it, suffering from a headache the entire time, and eventually i lit it on fire.  one less copy in the world...don't waste your money.  i wish i had the time spent reading this book back so i could use it for better purposes.  this book wasted my life

running with scissors: a memoir
title: horrible book, horrible.
this book was horrible.  i read half of it, suffering from a headache the entire time, and eventually i lit it on fire.  one less copy in the world...don't waste your money.  i wish i had the time spent reading this book back so i could use it for better purposes.  this book wasted my life
positive
negative
labels
auxiliary problems
presence or absence of frequent words and bigrams
auxiliary problems for sentiment classification
don   t_waste,  horrible,  suffering
auxiliary problem hypothesis space






consider linear, binary auxiliary predictors:
given a new hypothesis weight vector     , how far is it from                ?


weight vector for auxiliary problem i  
two steps of structural learning
step 1:  use unlabeled data and auxiliary problems to learn a representation     :  an approximation to 
1
0




0
.
.
.
1

0.3
0.7




-1.0
.
.
.

-2.1

features:  
<left word =    mr.   >


weights learned from labeled data
low-dimensional representation
step 2:  use labeled data to learn weights for the new representation
unlabeled step: train auxiliary predictors

for each unlabeled instance, create a binary presence / absence label
 mask and predict pivot features using other features
 train n linear predictors, one for each binary problem
 auxiliary weight vectors give us clues about feature conditional covariance structure

binary problem:  does    not buy    appear here?
(2) an excellent book.  once again, another wonderful novel from grisham
(1) the book is so repetitive that i found myself yelling    . i will definitely not buy another.
unlabeled step: id84


	      gives n new features
 value of ith feature is the propensity to see    not buy    in the same document
 we want a low-dimensional representation
 many pivot predictors give similar information 
    horrible   ,    terrible   ,    awful   
 compute svd & use top left singular vectors 





step 2:  labeled training
step 2:  use      to regularize labeled objective

original, high-dimensional weight vector

low-dimensional weight vector for learned features

only high-dimensional features have quadratic id173 term
step 2:  labeled training
  comparison to prototype similarity
 uses predictor (weight vector) space, rather than counts
 similarity is learned rather than fixed

results: id39
data: conll 2003 shared task
labeled: 204 thousand tokens of reuters news data
annotations:  person, location, organization, miscellaneous
unlabeled: 30 million words of reuters news data
a glance of some of the rows of 
numerical results (f-measure)
  large difference between co-training here and co-boosting (collins & singer 1999)
  this task is entity recognition, not classification
  we must improve over a supervised baseline
id20 with structural learning
running with scissors: a memoir
title: horrible book, horrible.
this book was horrible.  i read half of it, suffering from a headache the entire time, and eventually i lit it on fire.  one less copy in the world...don't waste your money.  i wish i had the time spent reading this book back so i could use it for better purposes.  this book wasted my life
avante deep fryer, chrome & black
title: lid does not work well...
i love the way the tefal deep fryer cooks, however, i am returning my second one due to a defective lid closure.  the lid may close initially, but after a few uses it no longer stays closed. i will not be purchasing this one again.


running with scissors: a memoir
title: horrible book, horrible.
this book was horrible.  i read half of it, suffering from a headache the entire time, and eventually i lit it on fire.  one less copy in the world...don't waste your money.  i wish i had the time spent reading this book back so i could use it for better purposes.  this book wasted my life

avante deep fryer, chrome & black
title: lid does not work well...
i love the way the tefal deep fryer cooks, however, i am returning my second one due to a defective lid closure.  the lid may close initially, but after a few uses it no longer stays closed. i will not be purchasing this one again.


error increase: 13%     26% 
blitzer et al. (2006): structural correspondence learning (scl)
blitzer et al. (2007):  for sentiment: books & kitchen appliances
pivot features
pivot features are features which are shared across domains
 do not buy the shark portable steamer    . trigger mechanism is defective. 
 the very nice lady assured me that i must have a defective set    . what a disappointment!
 maybe mine was defective    . the directions were unclear
unlabeled kitchen contexts
 the book is so repetitive that i found myself yelling    . i will definitely not buy another.
 a disappointment    . ender was talked about for <#> pages altogether.
 it   s unclear    . it   s repetitive and boring
unlabeled books contexts

use presence of pivot features as auxiliary problems
choosing pivot features: mutual information
pivot selection (scl):  select top features      by shared counts
pivot selection (scl-mi):  select top features in two passes
    (1) filter feature      if min count in both domains < k
    (2) select top filtered features by 
books-kitchen example
in scl-mi, not scl
in scl, not scl-mi
sentiment classification data
product reviews from amazon.com
books, dvds, kitchen appliances, electronics
2000 labeled reviews from each domain
3000     6000 unlabeled reviews
binary classification problem 
positive if 4 stars or more, negative if 2 or less
features: unigrams & bigrams
pivots: scl & scl-mi
at train time: minimize huberized hinge loss (zhang, 2004)






negative                    vs.                 positive
plot
<#>_pages
predictable









fascinating
engaging
must_read
grisham












the_plastic
poorly_designed
leaking
awkward_to













espresso
are_perfect
years_now





a_breeze

books
kitchen

visualizing 	  (books & kitchen)

empirical results: books & dvds
baseline loss due to adaptation: 7.6%
scl-mi loss due to adaptation: 0.7%

on average, scl-mi reduces error due to adaptation by 36%



structural learning: why does it work?
good auxiliary problems = good representation 
structural learning vs. co-training
structural learning separates unsupervised and supervised learning
leads to a more stable solution
structural learning vs. graph id173
use structural learning when auxiliary problems are obvious, but graph is not
understanding structural learning: goals
develop a relationship between structural learning and multi-view learning 
discuss assumptions under which structural learning can perform well
give a bound on the error of structural learning under these assumptions
structural and multi-view learning
lw=mr.
rw=said
rw=corp.
balmer
smith
yahoo
general electric
context pivots
orthography features






rw=expounded
lw=senator
rw=llc
lw=the

brown
microsoft
smith
context features
orthography pivots







canonical correlation analysis
canonical correlation analysis     cca (hotelling, 1936)
mr.
said

smith

microsoft








correlated features from different views are mapped to similar areas of space


structural learning and cca
some changes to structural learning
minimize squared loss for auxiliary predictors  
(2) block svd by view: train auxiliary predictors for view 1 using features from view 2 and vice versa

cca and semi-supervised learning
kakade and foster (2007).  multi-view regression via canonical correlation analysis.
assume:
contrast with co-training:  k&f don   t assume independence


semi-supervised learning procedure

training error using transformed inputs

regularize based on amount of correlation
a bound on squared error under cca
main theorem of kakade & foster (2007)

assumption:  how good is single view compared to joint model?

expected error of learned, transformed predictor

expected error of best model


number of training examples
amount of correlation


when can structural learning break?
hard-to-define auxiliary problems
id33:  how to define auxiliary problems for an edge?
mt alignment:  how to define auxiliary problems for a pair of words?

combining real-valued & binary features

scaling, optimization


high-dimensional, sparse
low-dimensional, dense



other work on structural learning
scott miller et al. (2004). name tagging with word clusters and discriminative training.
hierarchical id91, not structural learning.
representation easily combines with binary features
rie ando, mark dredze, and tong zhang (2005).  trec 2005 genomics track experiments at ibm watson.
applying structural learning to information retrieval
ariadna quattoni,  michael collins, and trevor darrel (cvpr 2007).  learning visual representations using images with captions.
ssl summary
id64
easy to write down.  hard to analyze.  
graph-based id173  
works best when graph encodes information not easily represented in normal feature vectors
structural learning
with good auxiliary problems, can improve even with lots of training data
difficult to combine with standard feature vectors


two take-away messages
semi-supervised learning yields good results for small amounts of labeled data
   i have lots of labeled data    is not an excuse not to use semi-supervised techniques
http://ssl-acl08.wikidot.com
semi-supervised learning at acl 2008
koo et al.  simple semi-supervised id33. 
mann & mccallum.  generalized expectation criteria for semi-supervised learning of id49.
pasca & van durme.  weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs.
wang et al.  semi-supervised convex training for id33.
suzuki & isozaki.  semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.
