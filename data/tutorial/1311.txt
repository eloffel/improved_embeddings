  

natural language understanding

john f. sowa & arun k. majumdar

kyndi, inc.

 

data analytics summit, december 2015
revised 15 june 2017

outline

1. why are natural languages so hard to analyze?

computers process syntax and logic very well.  difficulties arise from 
the many ways of thinking and acting in and on a complex world.

2. hybrid systems are necessary to support diversity

flexibility and generality are key to intelligence.  no single algorithm 
or paradigm can do everything or talk about everything.

3. cognitive computing

for any specific task, a computer simulation can often do as well or 
better than humans.  but people are superior to any computer system 
in relating, integrating, and talking about all possible tasks.

4. cycles of learning and reasoning

the cognitive cycle of induction, abduction, deduction, and testing.

for videos of the talks presented at the data analytics summit (including      
this one), see http://livestream.com/hulive/datasummit/videos/107034438

 

2

natural language processing

  

a classroom in 2000, as imagined in 1900 *
* http://publicdomainreview.org/collections/france-in-the-year-2000-1899-1910/

 

3

1. what makes language so hard

early hopes for artificial intelligence have not been realized. *
language understanding is more difficult than anyone thought.
a three-year-old child is better able to learn, understand, and 
speak a language than any current computer system.
tasks that are easy for many animals are impossible for the 
latest and greatest robots.
questions:

    have we been using the right theories, tools, and techniques?
    why haven   t these tools worked as well as we had hoped?
    what other methods might be more promising?
    what can research in neuroscience and psycholinguistics tell us?
    can it suggest better ways of designing intelligent systems?

* see the best ai still flunks 8th grade science, wired magazine.

 

4

early days of artificial intelligence 

  

1960:  hao wang   s theorem prover took 7 minutes to prove all 378
fol theorems of principia mathematica on an ibm 704         
much faster than two brilliant logicians, whitehead and russell. 

1960: emile delavenay, in a book on machine translation:

   while a great deal remains to be done, it can be stated without
hesitation that the essential has already been accomplished.    

1965:  irving john good, in speculations on the future of ai:

   it is more probable than not that, within the twentieth century,
an ultraintelligent machine will be built and that it will be the last
invention that man need make.    

1968:  marvin minsky, a technical adviser for the movie 2001:

   the hal 9000 is a conservative estimate of the level of artificial
intelligence in 2001.   

 

5

hal 9000 in 2001:  a space odyssey

  

      

the advisers made two incorrect predictions:

    hardware technology developed faster than they expected.
    but software, including ai, developed much slower.

predicting a future invention is almost as hard as inventing it.

 

the id88

  

 

one-layer neural network invented by frank rosenblatt (1957).
mark i:  a hardware version funded by the us navy:

    input:  400 photocells in a 20 x 20 array.
    weights represented by potentiometers updated by electric motors.

the new york times, after a press conference in 1958:

the id88 is    the embryo of an electronic computer that [the navy] 
expects will be able to walk, talk, see, write, reproduce itself and be 
conscious of its existence.    *

 

* http://query.nytimes.com/gst/abstract.html?res=9d03e4d91f3ae73abc4b52dfb1668383649ede 

7

a breakthrough in machine learning 

  

program for playing checkers by art samuel in 1959:

    ran on the ibm 704, later on the ibm 7090. 
    the ibm 7090 was comparable in speed to the original ibm pc (1981),     
  and its maximum ram was only 144k bytes.

samuel   s program was a hybrid:

    a id88-like algorithm for learning to evaluate game positions.
    the alpha-beta algorithm for searching game trees. 

won a game against the connecticut state checkers champion.

 

bird nest problem

  

robots can perform many tasks 
with great precision. 
but they don   t have the flexibility 
to handle unexpected shapes.
they can   t wash dishes the way 
people do     with an open-ended 
variety of shapes and sizes.
and they can   t build a nest in    
an irregular tree with irregular 
twigs, straw, and moss.
if a human guides a robot through a complex task with complex 
material, the robot can repeat the same task in the same way.
but it doesn   t have the flexibility of a bird, a beaver, or a human.
9

 

understanding language

the syntax is easy:  parse the question and the answer.
semantics is harder:  use background knowledge to 

    recognize the situation type and the roles of the two agents,
    relate the word 'thing' to the picture and to the concept car,
    relate the verbs 'take' and 'move' to the situation,
    apply the laws of physics to understand the answer.

pragmatics is the hardest:  explain the irony and the humor.
* search for 'moving' at http://www.shoecomics.com/

 

10

the ultimate understanding engine

sentences uttered by a child named laura before the age of 3. *

here   s a seat.  it must be mine if it   s a little one.
i went to the aquarium and saw the fish.
i want this doll because she   s big.
when i was a little girl, i could go    geek geek    like that,
but now i can go    this is a chair.    

no computer system today can learn and use language as fast, 
as accurately, and as flexibly as a child.
preschool children constantly ask    why?   
those questions get into the pragmatics.  they are the hardest 
for parents and computer systems to answer.

* john limber, the genesis of complex sentences.  
http://pubpages.unh.edu/~jel/jlimber/genesis_complex_sentences.pdf 

 

11

child reasoning

a mother talking with her son, about the same age as laura: *

mother:  which of your animal friends will come to school today?
son:  big bunny, because bear and platypus are eating. 

the mother looks in his room, where the stuffed bear and the 
platypus are sitting in a chair and    eating   .
the child relates the sentences to the situation:

    the bear and the platypus are eating.
    eating and going to school cannot be done at the same time.
    big bunny isn   t doing anything else.
    therefore, big bunny is available.

this reasoning is more    logical    than anything that siri says. 

* reported by the father, the psychologist gary marcus, in an interview with will knight (2015) 
http://www.technologyreview.com/featuredstory/544606/can-this-man-make-ai-more-human/#comments 

 

12

mental maps, images, and models 

quotation by the neuroscientist antonio damasio (2010):

   the distinctive feature of brains such as the one we own is their 
uncanny ability to create maps...  but when brains make maps, they  
are also creating images, the main currency of our minds.  ultimately 
consciousness allows us to experience maps as images, to  
manipulate those images, and to apply reasoning to them.   

the maps and images form mental models of the real world or  
of the imaginary worlds in our hopes, fears, plans, and desires.
words and phrases of language can be generated from them.
they provide a    model theoretic    semantics for language that 
uses perception and action for testing models against reality.
like tarski   s models, they determine the criteria for truth, but 
they are flexible, dynamic, and situated in the daily drama of life.

 

13

role of imagery in mathematics

paul halmos, mathematician: 
   mathematics         this may surprise or shock some         is never deductive in its 
creation.  the mathematician at work makes vague guesses, visualizes broad 
generalizations, and jumps to unwarranted conclusions.  he arranges and 
rearranges his ideas, and becomes convinced of their truth long before he 
can write down a logical proof...  the deductive stage, writing the results 
down, and writing its rigorous proof are relatively trivial once the real insight 
arrives;  it is more the draftsman   s work not the architect   s.     *
albert einstein, physicist: 
   the words or the language, as they are written or spoken, do not seem to 
play any role in my mechanism of thought.  the psychical entities which 
seem to serve as elements in thought are certain signs and more or less clear 
images which can be voluntarily reproduced and combined...  the above-
mentioned elements are, in my case, of visual and some of muscular type. 
conventional words or other signs have to be sought for laboriously only in  
a secondary stage, when the mentioned associative play is sufficiently 
established and can be reproduced at will.     **

* halmos (1968).  ** quoted by hadamard (1945).   see also lakoff & n    ez (2000). 

 

14

archimedes    eureka moment

 

insight:  a submerged body displaces an equal volume of water.

    it   s a mathematical principle, a property of euclidean space.
    scientists and engineers have used it ever since.
    they don   t prove it.  they use it to define incompressible fluid.

 

 

15

determining the value of    

 

archimedes had two creative insights, both inspired by images:

    the circumference of the circle is greater than the perimeter of        
  the inner polygon and less than that of the outer polygon.
    as the number of sides increases, the inner polygon expands,         
  and the outer polygon shrinks.  they converge to the circle.

given these insights, a good mathematician could compute       
to any desired precision.  archimedes used 96-agons.
 
16

 

euclid   s proposition 1

euclid   s statement, as translated by thomas heath:

    on a given finite straight line, to draw an equilateral triangle.

the creative insight is to draw two circles:

    the circle with center at a has radii ab and ac.
    the circle with center at b has radii ba and bc.
    since all radii of a circle have the same length, the three lines          
  ab, ac, and bc form an equilateral triangle.

 
for more details and discussion, see http://www.jfsowa.com/talks/natlog.pdf 

 

17

feelings and emotions

damasio and carvalho (2013),

       feelings are mental experiences of body states.   
       they signify physiological need, tissue injury, optimal function,            
    threats to the organism, or specific social interactions.   
       feelings constitute a crucial component of the mechanisms of life       
    regulation, from simple to complex.   
       their neural substrates can be found at all levels of the nervous           
     system, from individual neurons to subcortical nuclei and cortical       
    regions.   

damasio (2014), *

       i   m ready to give the very teeny brain of an insect     provided it has      
    the possibility of representing its body states     the possibility of          
    having feelings.   
       of course, what flies don   t have is all the intellect around those            
    feelings that could make use of them:  to found a religious order,         
    or develop an art form, or write a poem.   

* interview,  http://www.technologyreview.com/qa/528151/the-importance-of-feelings/ 

 

18

cognitive learning

areas of the cerebral cortex 
are highly specialized. 
a study with fmri scans 
shows which areas are active 
at various stages of cognitive 
learning. *
14 participants studied how four devices work:  bathroom scale, 
fire extinguisher, automobile braking system, and trumpet.
cognitive learning is more complex than dnn learning:
1.  the visual cortex is most active in the initial perceptual stage.
2.  parietal lobes are active while    imagining the components moving.   
3.  all lobes become active as participants are    generating causal                 
     hypotheses    about how the system works.
4.  finally, the frontal cortex is active while    the person (probably oneself)    
     imagines what it would be like to    interact with the system.   

 

* r. a. mason & m. a. just (2015) http://www.sciencedirect.com/science/article/pii/s1053811915000841 

19

 

the brain in language learning

language learning increases connectivity among brain regions.

    39 native english speakers studied chinese for 6 weeks. *
    fmri scans showed an increase in connectivity in successful learners 
  compared to less successful learners.
    those who learned the fastest had more connectivity at the start.

learning a new language, natural or artificial, rewires the brain.

* ping li (2014) http://medicalxpress.com/news/2014-11-languages-workout-brains-young.html

 

signatures of consciousness

stanislas dehaene on introspection:
    people can   t observe their own brains.
    but introspection is data about experience.
    at the right is an fmri scan of dehaene's own  
  brain while he was reading concrete words.
    he calls the patterns of neural activity               
  signatures of consciousness. *
       these signatures are remarkably stable and can be observed in a great   
   variety of visual, auditory, tactile, and cognitive stimulations.      (p. 13)

example:  signatures for numbers and computation.

    mathematicians treat numbers and arithmetic as a unified system.
    but people learn, use, and experience numbers in a variety of ways:           
  verbal, computational, spatial (diagrams), and temporal (counting).
    by examining the neural signature, the experimenters can distinguish        
  which version a subject is thinking about.

* stanislas dehaene (2014) consciousness and the brain, new york: viking. 

 

21

2. hybrid systems to support diversity

flexibility and generality are key to intelligence.

    the languages of our stone-age ancestors can be adapted to any            
   subject:  science, technology, business, law, finance, and the arts.
    when people invent anything, they find ways to describe it.
    when people in any culture adopt anything from another culture,             
  they borrow or adapt words to describe it in their native language.

minsky   s proposal:  a society of heterogeneous agents:

   what magical trick makes us intelligent?  the trick is that there is            
no trick.  the power of intelligence stems from our vast diversity,             
not from any single, perfect principle.  our species has evolved many 
effective although imperfect methods, and each of us individually 
develops more on our own.  eventually, very few of our actions and 
decisions come to depend on any single mechanism.  instead, they  
emerge from conflicts and negotiations among societies of processes  
that constantly challenge one another.    *

* marvin minsky (1986) the society of mind, new york: simon & schuster,    30.8.                    
see also push singh & marvin minsky (2004) an architecture for cognitive diversity.

 

22

machine learning applications

deep neural nets can be very useful, but they   re not magic. *

    observation by andrew ng:  current  ml methods automate tasks  
  that take less than one second of mental effort by humans.
    they only automate perception, not cognition (understanding).
    cognition requires the ability to explain what was learned.

* andrew ng (2016) https://hbr.org/2016/11/what-artificial-intelligence-can-and-cant-do-right-now 

 

slight perturbations

random noise or perturbations may cause a misclassification.
    six pairs of images show the originals and slightly modified versions.
    a dnn that correctly classified the first member of each pair made             
  serious mistakes in classifying the modified versions. *
    such mistakes in a self-driving car could cause a disaster.

people and other animals use cognition to correct perception.

    if they see something unexpected, they blink and look again. 

* huang et al. (2017) safety verification of deep neural networks 

 

24

take advantage of available tools 

sixty years of r & d in ai and machine translation.
tools and resources for a wide variety of paradigms:

    parsers and translators for natural and artificial languages.
    grammars, lexicons, ontologies, terminologies, corpora,                 
  wikipedia, dbpedia, linked open data, and the semantic web.
    theorem provers and id136 engines for formal logic and          
  many kinds of informal and fuzzy reasoning.
    qualitative, case-based, and analogical reasoning.
    statistical, connectionist, and neural network methods.
    pattern recognition, data mining, and graph data mining,
    id107 and machine-learning methods.
    thousands of implementations of all the above.  

but many systems are designed around a single paradigm     
they do not take advantage of all the available resources.

 

25

google translate

based on statistical methods for matching strings (id165s).
string matching is good for short sentences:

english source:  the electrician is working.
german:  der electriker arbeitet.
polish:  elektryk pracuje. 
english source:  the telephone is working.
german:  der telefon funtioniert.
polish:  telefon dzia  a.   

but it can   t keep track of long-distance connections: * 

english source:  the electrician that came to fix the telephone is working.
german:  der electriker, der das telefon zu beheben kam funktioniert.
polish:  elektryk, kt  ry przyszedl naprawi   telefon dzia  a. 
english source:  the telephone on the desk is working.
german:  das telefon auf dem schreibtisch arbeitet.
polish:  telefon na biurku pracuje.

 

26

* ernest davis & gary marcus (2015) commonsense reasoning and knowledge in ai.

translating latin to english

for latin, word inflections are more important than word order.
if a text is very common, google may find enough english-latin 
copies for high quality statistical translation:

    latin source:  e pluribus unum.
    english:  out of many, one.
    latin source:  credo in unum deum, patrem omnipotentem, factorem coeli   
  et terrae, visibilium omnium et invisibilium.
    english:  i believe in one god, the father, the almighty, maker of heaven       
  and earth, and of all things visible and invisible.

google sometimes uses, sometimes confuses punctuation:

    latin source:  gaudeamus, igitur, juvenes dum sumus.
    english:  let us, therefore, while we are young.
    if the second comma is deleted,    gaudeamus, igitur ...     is translated          
     us rejoice, therefore ...   
    these translations were made in november 2016.  the results may             
  change with updates to the algorithms or the language data.

27

 

translating unfamiliar texts

an epigram by martial (book viii, no. lxi) and a human translation: *

google produces unintelligible word hash:
livet health, bursting with rage, weep
he asks the high, which is bound up in the branches of:
and read that the world was no longer the whole,
neither the muscles as well, and cedar
i will scatter through them all at rome, which are bound to:
but the reason why we have the summer to the country under the city,
vehimurque mules no longer, as before, they were hired.
what imprecabor o severe, liventi?
this said, i would: mule, for it, and in the suburbs.
 

* by cora sowa, http://www.minervaclassics.com/quotat16.htm#jul16 

28

google id4 

google upgraded their mt system with neural nets (gid4). *

    in december 2016, they made a major improvement for 8 language pairs.
    for each pair of languages, neural nets match syntactic patterns better   
  than the old word strings (id165s).
    that upgrade improved the sentence-level translation.
    but the paragraph-level translation is just as bad.

limitation:  gid4 doesn   t use semantics or pragmatics:

    people relate language to imagery, purpose, and feelings.
    they show their understanding by explaining what they know.
    3-year-old children can explain what they say and why.
    but google   s neural networks cannot explain what they translate.
    they generate better syntax at the sentence level, but they cannot            
  recognize and use paragraph-level context and intentions.

* wu et al. (2016) google's id4 system.   
lewis-kraus (2016) the great ai awakening.
 

29

good syntax.  bad semantics.

a text translated by google (24 march 2017):

from a novel:     she spoke earnestly, excitedly; eagerly he hung upon her 
words.  then her voice broke.     satirical comment:     and down he went.    *
spanish:  ella habl   con seriedad, con entusiasmo; ansiosamente que 
colgaba  de sus palabras.  entonces su voz se quebr  .  y abajo que iba.
back to english:  she spoke earnestly, with enthusiasm; anxiously 
hanging from his words.  then his voice cracked.  and down i was going.
german:  sie sprach sehr aufgeregt; eifrig hing er auf ihre worte.                 
dann brach ihre stimme.  und er ging hinunter.
back to english:  she spoke very excitedly; eagerly he hung his words.       
then her voice broke.  and he went downstairs.
chinese:                               ;
                          
                                                
back to english:  she was so excited that he was eager to hang her words.
then, her voice choked.  he went up and down.

                                 

exercise:  try this example with other language pairs.
 
* original text and comment from the altoona tribune, 29 december 1919, p. 8.

 

30

multi-layer neural nets

deep neural nets (dnns) are much better than earlier nns.
but cognitive learning requires symbolic methods. *
* wermter & sun (2000) hybrid neural systems, http://www.cogsci.rpi.edu/~rsun/intro-w.pdf 

 

learning to play games

using dnns to learn how to play games for the atari 2600: *

    seven games:  pong, breakout, space invaders, seaquest, beamrider,       
  enduro, and q*bert.
    no prior knowledge about objects, actions, features, or game rules. 
    bottom layer starts with pixels:  210 x 160 video and the game score.
    each layer learns features, which represent the data at the next layer.
    top layer determines which move to make at each step of the game.
shows that dnns can be used to learn time-varying patterns.

* mnih et al. (2013) at deepmind technologies, http://www.cs.toronto.edu/~vmnih/docs/id25.pdf 

 

developments at deepmind

good for perceptual learning, not cognitive learning. 
results on the atari games:

    outperforms all other machine-learning methods on 6 of the 7 games.
    better than a human expert on breakout, enduro, and pong.  close to        
  human performance on beamrider.
    but far from human performance on q*bert, seaquest, and space               
  invaders     because those games require long-term strategy.

comparison with samuel   s checker-playing system:
    modern dnns are much more powerful than a id88.
    but lookahead methods are necessary for long-term strategy.
    samuel   s system was a hybrid that combined alpha-beta search with a      
  learning method that was much simpler than a dnn.
    for chess, a hybrid that combined a dnn to learn the evaluation function  
  with alpha-beta search reached the international master level. *

* michael lai (2015) http://arxiv.org/pdf/1509.01549.pdf 

 

33

games of go and go-moku

same syntax, but very different strategy:

syntax defines legal moves, but not meaningful moves.
the meaning of any move is determined by its purpose.
in go, the goal is to place stones that surround territory.
in go-moku, the goal is to place five stones in a row.
different goals change the strategy from the first move.

 

34

alphago by deepmind

major breakthrough in learning to play go.

    computationally, the game of go is far more                                                   
  challenging than chess.
    for both games, pattern recognition and tree                                                  
  search are important for high-level play.
    but go has about 250 options at each step, and                                             
  chess has about 37.
    the search strategies used for chess programs                                              
  are inadequate for playing a good game of go.   

alphago won 9 of 10 games with go masters. *

    a hybrid system with dnns and id169 (mtcs). 
    but the dnns were trained on millions of games, far more than any            
  human could play in a lifetime.
    and the 19x19 go board is much simpler than the scenes and imagery      
  that determine the semantics of natural languages. 

* david silver, et al. (2016) mastering the game of go with deep neural networks and tree search,  
nature, vol. 529, pp. 484   489.

 

cognitive learning

with perceptual learning, ml systems can outperform humans.

    they can beat the world champions in chess, go, and poker.
    they can recognize signs and traffic patterns faster than human drivers.
    but they need to be trained on millions of examples     more than any          
  human can experience in a lifetime. 
    and pattern recognition must be supplemented with reasoning methods,  
  such as search strategies (for chess and go) or statistics (for poker).

in cognitive learning, the agent understands what it learns.
    by analysis, people can learn new patterns from a single example.
    by analogy, they can relate patterns from different sensory modalities.
    cognition uses    common sense   :  it enables an agent to detect unusual   
  situations, to correct errors in perception, and to explain what it learns.

active ml is the first step toward cognitive learning. *
    the agent can choose the information from which it learns.
    next step:  explain why it made a choice and what it learned.

* http://burrsettles.com/pub/settles.activelearning.pdfi

 

36

the role of knowledge in perception

most animals are
camouflaged with
colors and features
that blend with their
native environment.
where is the cat?
prior knowledge
enables faster, more
accurate perception.
neural networks,
even dnns, use
bottom-up methods.

for better performance, they should be supplemented with top-down, 
knowledge-based methods. 

 

37

the role of knowledge in perception

the white oval directs
attention to the cat.
but prior knowledge
about cats is also
important.
in science, new
discoveries enable
observers to    see   
patterns they had
overlooked.
chromosomes,
for example, were
discovered and
named in the 1880s.  before then, no drawings of cells showed 
chromosomes.  afterwards, they all showed chromosomes.

 

38

stanford nlp group

developing statistical-symbolic hybrids. * 
    statistical methods for computing parse trees.
    dnns for recognizing images, parse trees for images, and parse trees 
  for language that describes the images.
    statistical methods for representing word meaning in vectors and        
  semantic graphs that relate the vectors.
    logical id136s to derive the implications.
    methods for translating texts to graphs, generating visual scenes         
  from the graphs, and using the scenes for retrieving images.

hybrid methods have produced promising results.
but more research is needed to generalize and systematize 
the methods for relating heterogeneous paradigms.

* chris manning (2015) computational linguistics and deep learning.

 

39

a hybrid with dnns and parse trees

from the stanford nlp group, http://nlp.stanford.edu/publications.shtml   

 

40

learning language as a child

syntagmatic-paradigmatic learner (spl). *

    learns to map language to and from situation descriptions.
    the box on the left represents the word ball, which was spoken in        
   the two situations described by the boxes on the right.

* designed by barend beekhuizen (2015), constructions emerging, phd dissertation.

 

learning to map language to situations

spl learns to relate strings of words to discourse situations:  

    the notation is based on langacker   s theory of cognitive grammar.
    spl is trained with strings of words paired with diagrams of relevant  
  situations (as in the diagram in the previous slide).
    in the early stages, spl learns to map single words to appropriate       
  nodes of a situation description.
    later, it maps two-word and multi-word constructions of words to       
  larger subgraphs of the situation descriptions.
    it also learns reverse mappings from situations to strings of words.
    appropriate rewards train spl to learn correct mappings.

but some human chose what features are significant and 
should be included in the description:

    that choice reflects human feelings, values, and intentions.
    current dnns do not recognize human intentions, but an infant very   
  quickly learns to recognize and respond to them.
    a truly deep learning system must also recognize them.

 

42

geometry problem solver (geos)

geos solves typical problems on the geometry sat exam. *
it   s a hybrid that relates imagery to language and mathematics.
* developed by the allen institute for ai and the university of washington.

 

43

cyc project

the most ambitious attempt to build the hal 9000:

    cyc project founded by doug lenat in 1984.
    starting goal:  implement the background knowledge of a typical    
  high-school graduate.
    ultimate goal:  learn new knowledge by reading textbooks.

after the first 25 years,

    100 million dollars and 1000 person-years of work,
    600,000 concepts,
    defined by 5,000,000 axioms,
    organized in 6,000 microtheories.

some good applications, but more needs to be done:

    cyc cannot yet learn by reading a textbook.
    cyc cannot understand language as well as a child.

 

44

cyc ontology

world   s largest formal ontology     with controlled english as an option.
see http://www.cyc.com and http://opencyc.org 
45

 

cyc system

a hybrid with a formal ontology, knowledge base, and nlp. *

    logical foundation for ontology and multiple reasoning modules.
    can use a variety of structured and unstructured data.

 

* see https://www.academia.edu/16911744/common_sense_reasoning_from_cyc_to_intelligent_assistant

case study:  cyc and ibm watson

why did ibm, not cyc, beat the jeopardy! champion?
short answer:  cyc was not designed for game shows.

    cyc was designed to be a general-purpose intelligent assistant.
    but ibm devoted a large research team to a single task.

longer answer:  watson had more diversity than cyc.

    as minsky said, the number of reasoning methods is unlimited.
    the first version of watson, which performed poorly on jeopardy!,      
   used 6 different reasoning algorithms.
    the version that won the jeopardy! challenge used about 100               
  algorithms optimized for different kinds of questions and data.
    the    dramatically different environment    led the watson team to          
  design a framework with novel methods of machine learning. *
    and it ran on a supercomputer with 2,880 parallel threads.

* gondek et al, a framework for merging and ranking answers in deepqa, 
http://researcher.watson.ibm.com/researcher/files/us-heq/w%2816%29%20answers%20merging_ranking%2006177810.pdf

 

ibm watson

multiple paradigms and a growing number of modules. *

    for jeopardy!, one api and about 100 reasoning methods.
    now, a few dozen apis and growing. 
    versions of all the major paradigms for natural language processing,         
  statistical, symbolic, semantic, pragmatic. **
    but in language learning, nothing can compete with a 3-year-old child.

* rob high, http://www.redbooks.ibm.com/redpapers/pdfs/redp4955.pdf 
** zadrozny, de paiva, & moss, https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/viewfile/9905/9684

 

finding associations

the kinds of associations are highly context dependent.
different algorithms for different kinds.

    indexes for finding co-occurrences in the data.
    searching a network to find shortest paths.
    deduction from definitions and axioms.

but watson chose climate instead of religion as the context:

       this  kind of meat should not be shipped to iraq.   
       what is reindeer?   

 

49

generating a response for jeopardy!

multiple steps that use a variety of algorithms:

1. parse the question and analyze the relationships among key phrases.
2. generate hypotheses, find evidence for them, and estimate their quality.
3. combine the best hypotheses in possible answers.
4. rank the answers by a confidence measure. 
5. select the best one and respond    who is edmund hillary?   
6. use feedback about success or failure for    dynamic learning.   
similar methods are used in the ibm engagement adviser.

50

 

ibm watson for applications

a hybrid with multiple paradigms.
scenario-based system for reasoning
and q/a about various applications.
the input scenario describes some
situation, e.g. a patient   s symptoms.
the first step translates the input to
an assertion graph.
instead of answering one question,
as in jeopardy!, watson paths does
extended reasoning to generate a
more complex assertion graph.
the reasoning may continue until
the assertion graph satisfies some
task-dependent criteria.
see lally et al. (2014) watson paths.
 

  

  

 

51

3. cognitive computing

support learning at a deeper, more human level than dnns.
cognitive memory    can use knowledge from any source:
    associative retrieval from background knowledge in log(n) time.
    approximate pattern matching for analogies and metaphors.
    precise pattern matching for logic and mathematics.
    hybrid systems that can use any or all methods, including dnns.

analogies support informal, case-based reasoning:

    long-term memory stores large numbers of previous experiences.
    any new case is matched to similar cases in long-term memory.
    close matches are ranked by a measure of semantic distance.

the cycle of pragmatism can explain each step of reasoning:

    induction:  observe similarities to derive generalizations.
    abduction:  use insights from any source to form hypotheses.
    deduction:  derive a conclusion by formal or informal methods.
    testing:  act upon the conclusion, evaluate the results, and repeat.

 

52

designing hybrid systems

in his society of mind and emotion engine, minsky proposed a 
society of heterogeneous, interacting modules or agents.
    could that society take advantage of recent developments in ai?
    how would the agents communicate and cooperate among themselves?
    what methods of learning and reasoning would they support?
    how could they relate symbols, images, and feelings to neural networks?
    how could a society of agents produce a unified personality?
    how could they agree on common goals, strategic plans, and tactical        
  moves that support the strategy?
    could they explain their results in a way that people could understand?

 requirements for supporting a society of agents:

    a system of communication and coordination. *
    methods for sharing and using information in long-term memory. **
    a cycle of perception, learning, reasoning, acting, and evaluating.

* j. f. sowa (2002) architectures for intelligent systems.  http://www.jfsowa.com/pubs/arch.pdf
** a. k. majumdar & j. f. sowa (2009) two paradigms are better than one, and multiple   
paradigms are even better.  http://www.jfsowa.com/pubs/paradigm.pdf

 

53

kyndi cognitive architecture

a framework for perception, learning, reasoning, and acting.
theory of signs (semiotic) by charles sanders peirce.

    natural language semantics represented in conceptual graphs.
    the full range of precision from iso common logic to fuzzy sensations,  
  vague tweets, and tentative guesses.
    cycle of observation, induction, abduction, deduction, and action.

cognitive memory    for associative retrieval of patterns.
    networks of anything     signs, symbols, images, words, or texts. 
    finding exact or approximate matches in logarithmic time. 

market-driven learning    (mdl) for dynamic evolution.

    agents are organized in a managerial hierarchy.
    patterns of activity can dynamically reorganize the hierarchy.
    the theoretical foundation is based on the    -calculus, minsky   s society    
  of mind, mccarthy   s elephant 2000, and gelernter   s linda.

 

54

cognitive memory    

for more detail, see us patents 8,526,321 b2 and 9,158,847 b1.

 

55

role of cm in cognitive computing

cognitive memory    is essential for human-like cognition:

    used in every stage of perception, language, reasoning, and action.
    perception by matching new sensations to prior percepts.
    simultaneous syntactic and semantic analysis of language.
    approximate pattern matching for analogies and metaphors.
    precise pattern matching for logic and mathematics.

analogies support informal, case-based reasoning:
    long-term memory (ltm) stores all previous experiences.
    any new case is matched to similar cases in ltm.
    close matches are ranked by a measure of semantic distance.

formal reasoning is based on a disciplined use of analogy:
    induction:  generalize multiple cases to create new rules or axioms.
    deduction:  match (unify) a new case with part of some rule or axiom.
    abduction:  form a hypothesis based on aspects of similar cases.

any or all kyndi modules can call upon cm at any step.

 

56

applications

general approach:

    the following examples used earlier versions of kyndi technology.
    for each one, clients specified requirements and paid for the results.
    the kyndi modular structure enables rapid design and development    
  by combining new modules with a library of base modules.

three projects:

1. extract information from research reports and map to a relational db.
2. legacy re-engineering:  analyze 40 years of legacy software and relate    
    it to the documentation     manuals, reports, memos, and comments.
3. oil and gas exploration:  extract information from research reports and   
    answer english queries by a geologist.

for more detail, see the presentation on cognitive memory   :

    using conceptual graphs (cgs) for finding analogies in nl texts.
    application:  evaluating student answers in free-form english.
    method:  translate the answers to cgs and use analogies.

 

57

information extraction project

the next slide shows a table derived from research reports.
to extend the semantics, an ontology for chemistry was added 
to the basic kyndi ontology.
then for each report,

    map each sentence to a conceptual graph (cg). *
    analyze anaphoric references to link pronouns to named entities.
    the result is a large cg that represents every sentence in the document.
    store that graph (including subgraphs) in cognitive memory.
    query cognitive memory for the data in each row of the table.
    store the answers in the table.

in a competition among twelve nlp systems,
    the kyndi system got 96% of the entries correct.
    the second best score was 73%.  most scores were below 50%.

* for an overview of cg methods, see http://www.jfsowa.com/pubs/template.pdf 

 

58

information extracted from documents

 

59

application to legacy re-engineering

analyze the software and documentation of a corporation.
programs in daily use, some of which were up to 40 years old.

    1.5 million lines of cobol programs.
    100 megabytes of english documentation     reports, manuals,

       e-mails, lotus notes, html, and program comments. 
goal:

    analyze the cobol programs.
    analyze the english documentation.
    compare the two to determine:

           data dictionary of all data used by all programs.
           english glossary of all terms with index to the software.
           evolution of terminology over the years.
           structure diagrams of the programs, files, and data.
           discrepancies between programs and documentation.

 

60

an important simplification
an extremely difficult and still unsolved problem:

    translate english specifications to executable programs.

much easier task:

    translate the cobol programs to conceptual graphs (cgs).
    those cgs provide the ontology and background knowledge.
    the cgs derived from english may have ambiguous options.
    in parsing english, use cgs from cobol to resolve ambiguities.
    the cobol cgs show the most likely options.
    they can also provide missing information or detect errors.
the cgs derived from cobol provide a formal semantics 
for the informal english texts.

 

61

excerpt from the documentation

the input file that is used to create this piece of the billing interface for 
the general ledger is an extract from the 61 byte file that is created by the 
cobol program billcrua in the billing history production run.  this 
file is used instead of the history file for time efficiency.  this file contains 
the billing transaction codes (types of records) that are to be interfaced to 
general ledger for the given month.
for this process the following transaction codes are used: 32     loss on 
unbilled, 72     gain on uncollected, and 85     loss on uncollected.  any of 
these records that are actually taxes are bypassed.  only client types 01     
mar, 05     internal non/billable, 06     internal billable, and 08     bas 
are selected.  this is determined by a getbdata call to the client file.
note that none of the files or cobol variables are named.
by matching graphs derived from english to graphs derived from 
cobol, all names of files and cobol variables were determined.

 

62

interpreting novel patterns  

many documents contain unusual or ungrammatical patterns.
they may be elliptical forms that could be stored in tables.
but some authors wrote them as phrases:

    32     loss on unbilled
    72     gain on uncollected
    85     loss on uncollected

the dashes were represented by a default relation (link):
     [number: 32]   (link)   [punctuation:          ]   (link)   [loss]   (on)   [unbilled] 
this cg, which was derived from an english document, matched 
cgs derived from cobol programs:

    the value 32 was stored as a constant in a cobol program.
    the phrase    loss on unbilled    was in a comment that followed                    
  the value 32 in that program.

 

63

results

job finished in 8 weeks by arun majumdar and andr   leclerc.

        four weeks for customization:
           design, ontology, and additional programming for i/o formats. 

        three weeks to adapt the software that used cognitive memory:
           matches with strong evidence (close semantic distance) were correct.   
           weak matches were confirmed or corrected by majumdar and leclerc. 

        one week to produce a cd-rom with the desired results:
           glossary, data dictionary, data flow diagrams, process architecture        
           diagrams, system context diagrams, and list of errors detected. 
a major consulting firm estimated that the job would take 40 people 
two years to analyze the documentation and find all cross references.
with cognitive memory, the task was completed in 15 person weeks.

 

64

discrepancy detected

a diagram of relationships among data types in the database:

question:  which location determines the market?

    according to the documentation:   business unit.
    according to the cobol programs:   client hq.

for many years, management had been making decisions based 
on incorrect assumptions.

 

65

contradiction detected

from the ontology used for interpreting english:

    every employee is a human being.
    no human being is a computer. 

from analyzing cobol programs:

    some employees are computers. 

what is the reason for this contradiction?

 

66

quick patch in 1979

a cobol programmer made a quick patch:

    two computers were used to assist human consultants.
    but there was no provision to bill for computer time.
    therefore, the programmer named the computers bob and         
  sally, and assigned them employee ids. 

for more than 20 years:

    bob and sally were issued payroll checks.
    but they never cashed them. 

the software discovered two computer    employees.   

 

67

relating formal and informal cgs

the legacy-reengineering task required two kinds of processing.
precise reasoning:

    analyzing the cobol programs and translating them to cgs.
    detecting discrepancies between different programs.
    detecting discrepancies between programs and documentation.

indexing and cross references:

    creating an index of english terms and names of programs.
    mapping english documents to the files and programs they mention.

conceptual graphs derived from cobol are precise.

    but cgs derived from english are informal and unreliable.
    informal cgs are adequate for cross-references between the english   
  documents and the cobol programs.
    all precise reasoning was performed on cgs from cobol or on cgs  
  from english that were corrected by cgs from cobol. 

 

68

application to oil and gas exploration

source material:

    79 documents, ranging in length from 1 page to 50 pages.
    some are reports about oil or gas fields, and others are chapters    
  from a textbook on geology used as background information.
    english, as written for human readers (no semantic annotations).
    additional data from relational dbs and other structured sources.
    lexical resources derived from id138, corelex, ibm-csli verb   
  ontology, roget   s thesaurus, and other sources. 
    an ontology for the oil and gas domain written in controlled             
  english by geologists from the university of utah.

queries:

    a paragraph that describes a potential oil or gas field.
    analogies compare the query to the documents.

 

69

answering questions

for the sources, either nl documents or structured data:

    translate the text or data to conceptual graphs.
    translate all cgs to cognitive signatures    in time proportional to       
  (n log n), where n is the total number of cgs.
    store each cognitive signature in cognitive memory    with a pointer   
  to the original cg and the source from which that cg was derived.
    use previously translated cgs to help interpret new sentences.

for a query stated as an english sentence or paragraph,

    translate the query to conceptual graphs.
    find matching patterns in the source data and rank them in order of    
  semantic distance.  the time is proportional to (log n).
    for each match within a given threshold, use structure mapping to      
  verify which parts of the query cg match the source cg.
    as answer, return the english sentences or paragraphs in the source   
  document that had the closest match to the query.

 

70

a query written by a geologist

turbiditic sandstones and mudstones deposited as a passive 
margin lowstand fan in an intraslope basin setting.   hydrocarbons 
are trapped by a combination of structural and stratigraphic onlap 
with a large gas cap.  low relief basin consists of two narrow 
feeder corridors that open into a large low-relief basin 
approximately 32 km wide and 32 km long.

 

71

details of the closest matching hydrocarbon fields

 

72

linking the query to the paragraphs that contain the answer

 

what the screen shots show

information shown in the previous screen shot:
    the query in the green box describes some oil or gas field.
    the data in the small yellow box describes the vautreuil field.
    the large yellow box shows the paragraphs in a report by mccarthy           
   and kneller from which that data was extracted. 

the next screen shot shows how the answer was found:

    many terms in the query were not defined in the ontology:  lowstand          
  fan, passive margin, turbiditic sandstones, narrow feeder cables,                
  stratigraphic onlap, intraslope basin.
    generate tentative cgs for these phrases and look in cognitive memory   
  to find similar cgs derived from other sources.
    chapters 44 and 45 of the textbook on geology contained those cgs         
  as subgraphs of larger graphs that had related information.
    patterns found in the larger graphs helped relate the cgs derived               
  from the query to cgs derived from the report that had the answer. 

 

74

using background knowledge from a textbook to find the answer

 

emergent knowledge

when reading the 79 documents,

    translate the sentences and paragraphs to cgs.
    but do not do any further analysis of the documents.

when a geologist asks a question,

    look for related phrases in cognitive memory.
    to connect those phrases, further searches may be needed.
    some sources may be textbooks with background knowledge         
   that may help interpret the research reports.
    the result consists of cgs that relate the query to paragraphs         
   in research reports that contain the answer.
    the new cgs can be added to cognitive memory for future use.

by a    socratic    dialog, a geologist can lead the system to 
explore novel paths and discover unexpected patterns.

 

76

knowledge discovery

observation by immanuel kant:

socrates said he was the midwife to his listeners, i.e., he made    
them reflect better concerning that which they already knew, and 
become better conscious of it.  if we always knew what we know, 
namely, in the use of certain words and concepts that are so 
subtle in application, we would be astonished at the treasures 
contained in our knowledge...
platonic or socratic questions drag out of the other person   s 
cognitions what lay within them, in that one brings the other to 
consciousness of what he actually thought.
                                                                             from his vienna logic

we need tools that can play the role of socrates.
they should help us discover the implicit knowledge    
and use it to process the huge volumes of digital data.

 

77

4. cycles of learning and reasoning

children learn language by starting with words and patterns of 
words that are grounded in perception and purposive action.
by trial and error, children and adults revise, extend, and adjust 
their beliefs to make better predictions about the world:

    observations generate low-level facts.
    induction derives general axioms from multiple facts.
    a mixture of facts and axioms is an unstructured knowledge soup.
    abduction selects facts and axioms to form a hypothesis (theory).
    analogies may relabel a theory of one topic and apply it to another.
    deductions from a theory generate predictions about the world.
    actions test the predictions against reality.
    the effects of the actions lead to new observations.

cycles within cycles may be traversed at any speed     from 
seconds to minutes to research projects that take years.

 

78

observing, learning, reasoning, acting

the cognitive cycle, as described by charles sanders peirce.
similar cycles occur in every aspect of life, including science.

 

knowledge soup

a heterogeneous, loosely linked mixture:

    fluid, lumpy, and dynamically changing.
    many lumps are or can be structured in a computable form.
    but they may be inconsistent or incompatible with one another.

in anybody   s head, knowledge soup is

    the totality of everything in memory.
in the internet, knowledge soup is 

    the totality of everything people downloaded from their heads,          
  recorded automatically, or derived by any computable method.

linked open data is good for finding and classifying anything 
in the soup     whether loose items or structured lumps.
but understanding the contents of the lod poses the same 
challenge as understanding natural language. 

 

80

human learning requires language

people use language to express every aspect of life.
the cognitive cycle integrates all aspects, including language:

    new data (experiences) accumulate from observations in life.
    statistical methods are useful for finding generalizations.
    but those generalizations must be integrated with previous knowledge.
    routine abduction may use statistics to select patterns from the soup.
    but creative abduction is necessary to invent new patterns.
    belief revision integrates various patterns into larger, better structured     
  patterns called hypotheses or theories.
    deduction generates predictions from the theories.
    actions in and on the world test the predictions.
    new observations provide supervision (rewards and punishments). 
language is essential for expressing novel patterns and for 
learning the novel patterns discovered by other people.

 

81

relating language to the world

model-theoretic semantics relates symbols to symbols:

    it determines truth values by relating logic to a tarski-style model.
    but the symbols of a model are, at best, approximations to the world.
    as engineers say,    all models are wrong, but some are useful.   

peirce   s theory of symbol grounding:

    the signs of perception and purposive action are primary.
    the symbols of language are interpretations of the primary signs.
    logic is an abstraction from language, not a foundation for language.

 

boyd   s ooda loop 

john boyd drew a four-step diagram for training    
fighter pilots to observe and respond rapidly.
the first two steps     observe and orient      involve      
the occipital, parietal, and temporal lobes.
the next two steps     decide and act     involve the    
frontal lobes for reasoning and motor control.
the four steps and the associated brain areas:

1. observe:  visual input goes to the primary visual cortex (occipital lobes),       
    but object recognition and naming involve the temporal lobes.
2. orient:  parietal lobes relate vision, touch, and sound in    cognitive maps.   
3. decide:  reasoning is under the control of the frontal lobes, but other            
    areas store the    knowledge soup    and the    mental models.   
4. act:     action schemata    are patterns in the premotor cortex of the                   
    frontal lobes.  signals from the motor cortex go to the muscles.
each step must be traversed in milliseconds for rapid response.
the time constraints require high-speed matching of overlearned patterns.

 

extended ooda loop  

over the years, boyd added more detail to the ooda loop.
he applied it to decision-making processes of any kind.
both versions are consistent with peirce   s cycle.

diagram adapted from http://en.wikipedia.org/wiki/ooda_loop 

 

84

the h-cogaff architecture

the hierarchical cognition affect architecture by aaron sloman 
includes a cycle similar to peirce   s or boyd   s. *

* from an architecture of  diversity for commonsense reasoning by mccarthy, minsky, sloman, et al. (2002)

 

85

ohlsson   s deep learning cycle

deep learning is non-monotonic cognitive change: *

    create novel structures that are incompatible with previous versions.
    adapt cognitive skills to changing circumstances.
    test those skills by action upon the environment.

* stellan ohlsson (2011) deep learning:  how the mind overrides experience, cambridge: university press.

 

albus cognitive architecture

a cycle that resembles those by peirce, boyd, sloman, and ohlsson.
see albus (2010), http://www.james-albus.org/docs/modelofcomputation.pdf 
87

 

real-time control system (rcs)

designed by albus and colleagues:  http://en.wikipedia.org/wiki/real-time_control_system 

 

levels of ai computation

sheth, anantharam, and henson distinguished three levels: *

    semantic:  a semantic network for representing knowledge.
    perception:  using background knowledge to interpret sensory data.
    cognitive:  understanding the knowledge in context and acting upon it.
    but more research is necessary on all three levels.

* diagram adapted from http://arxiv.org/ftp/arxiv/papers/1510/1510.05963.pdf

 

89

implementing the cycles

an open-ended variety of methods for learning and reasoning.

 

creative abduction

creativity, by definition, introduces something totally new.
observation and abduction are the sources of novelty:

    observation is the ultimate source of all information.
    routine observations classify new information in familiar patterns.
    induction generalizes multiple observations by simplifying patterns.
    routine abduction makes selections from familiar patterns.
    belief revision modifies a theory by adding and deleting patterns.
    deduction uses systematic rules for combining and relating patterns.
    but creative abduction (invention) introduces novel patterns. 

for young children, almost everything is unfamiliar.

    they are the most creative people on earth. 
for most adults, most things are familiar.

    they seldom feel the need to create radically new patterns.
    but they can learn new patterns created by other people. 

 

91

references

research that established the foundations for kyndi technology:
majumdar, arun k. (2013) relativistic concept measuring system for data id91, us patent 8,526,321 b2. 
majumdar, arun k. (2015) cognitive memory encoding networks for fast semantic indexing, storage, and 
retrieval, us patent 9,158,847 b1.
majumdar, arun k.,  john f. sowa, & john stewart (2008) pursuing the goal of language understanding,  
http://www.jfsowa.com/pubs/pursuing.pdf
majumdar, arun k., & john f. sowa (2009) two paradigms are better than one and multiple paradigms are 
even better, http://www.jfsowa.com/pubs/paradigm.pdf   
majumdar, arun k., & john f. sowa (2014) quantum cognition, http://www.jfsowa.com/pubs/qcog.pdf
sowa, john f. (2002) architectures for intelligent systems, http://www.jfsowa.com/pubs/arch.pdf 
sowa, john f., & arun k. majumdar (2003) analogical reasoning,  http://www.jfsowa.com/pubs/analog.htm  
sowa, john f. (2005) the challenge of knowledge soup, http://www.jfsowa.com/pubs/challenge.pdf 
sowa, john f. (2006) worlds, models, and descriptions,  http://www.jfsowa.com/pubs/worlds.pdf   
sowa, john f. (2008) conceptual graphs, http://www.jfsowa.com/cg/cg_hbook.pdf
sowa, john f. (2010) role of logic and ontology in language and reasoning,
http://www.jfsowa.com/pubs/rolelog.pdf 
sowa, john f. (2011) cognitive architectures for conceptual structures,  
http://www.jfsowa.com/pubs/ca4cs.pdf 
sowa, john f. (2013) from existential graphs to conceptual graphs,    
http://www.jfsowa.com/pubs/eg2cg.pdf 
iso/iec standard 24707 for common logic,                                                                                                           
http://standards.iso.org/ittf/publiclyavailablestandards/c039175_iso_iec_24707_2007(e).zip 

 

92

