   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 2

   [21]24th january 2015
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   in [32]part 1 of my series on machine learning in python, we covered
   the first part of exercise 1 in andrew ng's [33]machine learning class.
   in this post we'll wrap up exercise 1 by completing part 2 of the
   exercise. if you recall, in part 1 we implemented id75 to
   predict the profits of a new food truck based on the population of the
   city that the truck would be placed in. for part 2 we've got a new task
   - predict the price that a house will sell for. the difference this
   time around is we have more than one dependent variable. we're given
   both the size of the house in square feet, and the number of bedrooms
   in the house. can we easily extend our previous code to handle multiple
   id75? let's find out!

   first let's take a look at the data.
path = os.getcwd() + '\data\ex1data2.txt'
data2 = pd.read_csv(path, header=none, names=['size', 'bedrooms', 'price'])
data2.head()

     size bedrooms price
   0 2104 3        399900
   1 1600 3        329900
   2 2400 3        369000
   3 1416 2        232000
   4 3000 4        539900

   notice that the scale of the values for each variable is vastly
   different. a house will typically have 2-5 bedrooms but may have
   anywhere from hundreds to thousands of square feet. if we were to run
   our regression algorithm on this data as-is, the "size" variable would
   be weighted too heavily and would end up dwarfing any contributions
   from the "number of bedrooms" feature. to fix this, we need to do
   something called "feature id172". that is, we need to adjust
   the scale of the features to level the playing field. one way to do
   this is by subtracting from each value in a feature the mean of that
   feature, and then dividing by the standard deviation. fortunately this
   is one line of code using pandas.
data2 = (data2 - data2.mean()) / data2.std()
data2.head()

       size    bedrooms    price
   0 0.130010  -0.223675 0.475747
   1 -0.504190 -0.223675 -0.084074
   2 0.502476  -0.223675 0.228626
   3 -0.735723 -1.537767 -0.867025
   4 1.257476  1.090417  1.595389

   next, we need to modify our implementation of id75 from
   part 1 to handle more than 1 dependent variable. or do we? let's take a
   look at the code for the id119 function again.
def gradientdescent(x, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)

    for i in range(iters):
        error = (x * theta.t) - y

        for j in range(parameters):
            term = np.multiply(error, x[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(x)) * np.sum(term))

        theta = temp
        cost[i] = computecost(x, y, theta)

    return theta, cost

   look closely at the line of code calculating the error term: error = (x
   * theta.t) - y. it might not be obvious at first but we're using all
   matrix operations! this is the power of id202 at work. this
   code will work correctly no matter how many variables (columns) are in
   x, as long as the number of parameters in theta agree. similarly, it
   will compute the error term for every row in x as long as the number of
   rows in y agree. on top of that, it's a very efficient calculation.
   this is a powerful way to apply any expression to a large number of
   instances at once.

   since both our id119 and cost function are using matrix
   operations, there is in fact no change to the code necessary to handle
   multiple id75. let's test it out. we first need to perform
   a few initializations to create the appropriate matrices to pass to our
   functions.
# add ones column
data2.insert(0, 'ones', 1)

# set x (training data) and y (target variable)
cols = data2.shape[1]
x2 = data2.iloc[:,0:cols-1]
y2 = data2.iloc[:,cols-1:cols]

# convert to matrices and initialize theta
x2 = np.matrix(x2.values)
y2 = np.matrix(y2.values)
theta2 = np.matrix(np.array([0,0,0]))

   now we're ready to give it a try. let's see what happens.
# perform id75 on the data set
g2, cost2 = gradientdescent(x2, y2, theta2, alpha, iters)

# get the cost (error) of the model
computecost(x2, y2, g2)

   0.13070336960771897

   looks promising! we can also plot the training progress to confirm that
   the error was in fact decreasing with each iteration of gradient
   descent.
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost2, 'r')
ax.set_xlabel('iterations')
ax.set_ylabel('cost')
ax.set_title('error vs. training epoch')

   error vs. training epoch

   the cost, or error of the solution, dropped with each sucessive
   iteration until it bottomed out. this is exactly what we would expect
   to happen. it looks like our algorithm worked.

   it's worth noting that we don't have to implement any algorithms from
   scratch to solve this problem. the great thing about python is its huge
   developer community and abundance of open-source software. in the
   machine learning realm, the top python library is [34]scikit-learn.
   let's see how we could have handled our simple id75 task
   from part 1 using scikit-learn's id75 class.
from sklearn import linear_model
model = linear_model.linearregression()
model.fit(x, y)

   it doesn't get much easier than that. there are lots of parameters to
   the "fit" method that we could have tweaked depending on how we want
   the algorithm to function, but the defaults are sensible enough for our
   problem that i left them alone. let's try plotting the fitted
   parameters to see how it compares to our earlier results.
x = np.array(x[:, 1].a1)
f = model.predict(x).flatten()

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='prediction')
ax.scatter(data.population, data.profit, label='traning data')
ax.legend(loc=2)
ax.set_xlabel('population')
ax.set_ylabel('profit')
ax.set_title('predicted profit vs. population size')

   predicted profit vs. population size

   notice i'm using the "predict" function to get the predicted y values
   in order to draw the line. this is much easier than trying to do it
   manually. scikit-learn has a great api with lots of convenience
   functions for typical machine learning workflows. we'll explore some of
   these in more detail in future posts.

   that's it for today. in part 3, we'll take a look at exercise 2 and
   dive into some classification tasks using id28.

   follow me on twitter to get new post updates.
   [35]follow @jdwittenauer
   [36]machine learning[37]data science[38]data visualization
   author

[39]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[40]curious insight

   author

[41]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [42]twitter [43]facebook [44]google+ [45]reddit [46]linkedin
   [47]pinterest

   copyright    curious insight. 2015     all rights reserved.

   proudly published with [48]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  33. https://www.coursera.org/course/ml
  34. http://scikit-learn.org/stable/
  35. https://twitter.com/jdwittenauer
  36. https://www.johnwittenauer.net/tag/machine-learning/
  37. https://www.johnwittenauer.net/tag/data-science/
  38. https://www.johnwittenauer.net/tag/data-visualization/
  39. https://www.johnwittenauer.net/author/john/
  40. https://www.johnwittenauer.net/
  41. https://www.johnwittenauer.net/author/john/
  42. http://twitter.com/share?text=machine learning exercises in python, part 2&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  43. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  44. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  45. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/&title=machine learning exercises in python, part 2
  46. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/&title=machine learning exercises in python, part 2
  47. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/&description=machine learning exercises in python, part 2
  48. https://ghost.org/

   hidden links:
  50. mailto:jdwittenauer@gmail.com
  51. https://twitter.com/jdwittenauer
  52. http://www.linkedin.com/in/jdwittenauer
  53. https://github.com/jdwittenauer
  54. http://www.kaggle.com/jdwittenauer
  55. https://www.johnwittenauer.net/rss/
  56. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
