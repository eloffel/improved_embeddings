continuous vector spaces 
for cross-language  
nlp applications 

rafael e. banchs 
human language technology department,  
institute for infocomm research, singapore 
 
 november 1, 2016  

austin, texas, usa.  emnlp2016 

tutorial outline 

part i 

    basic concepts and theoretical framework (   45 mins) 

    vector spaces in monolingual nlp (   45 mins) 

 

part ii 

    vector spaces in cross-language nlp (   70 mins) 

    future research and applications (   20 mins) 

 

motivation 

    the mathematical metaphor offered by the geometric 

concept of distance in vector spaces with respect to 
semantics and meaning has been proven to be useful in 
monolingual nlp applications.  

    there is some recent evidence that this paradigm can 

also be useful for cross-language nlp applications. 

objectives 

the main objectives of this tutorial are as follows: 

    to introduce the basic concepts related to distributional 

and cognitive semantics 

    to review some classical examples on the use of vector 

space models in monolingual nlp applications 

    to present some novel examples on the use of vector 

space models in cross-language nlp applications 

section 1 

basic concepts and theoretical framework 

    the distributional hypothesis  

    vector space models and the term-document matrix 

    association scores and similarity metrics 

    the curse of dimensionality and dimensionality 

reduction 

    semantic cognition, conceptualization and abstraction 

distributional hypothesis 

   a word is characterized for the company it keeps    * 

 

(meaning is mainly determined by the context rather than  
from individual language units) 

 
 
 
 
 

    please cash the cheque at the bank 

 

 

 

    please check for rocks along the bank  

* firth, j.r. (1957) a synopsis of linguistic theory 1930-1955, in studies in linguistic analysis, 51: 1-31 

distributional structure 

meaning as a result of language   s distributional 
structure     or vice versa ? 
 

 

       if we consider words or morphemes a and b to be 
more different in meaning than a and c, then we will 
often find that the distributions of a and b are more 
different than the distributions of a and c.    * 
 
   in the language itself, there are only differences    ** 

 
 

* harris, z. (1970) distributional structure, in papers in structural and transformational linguistics 
 
**  saussure, f. (1916) course in general linguistics 

not everyone is happy        

argument against     

    meaning involves more than language: 

    images and experiences that are beyond language 

    objects, ideas and concepts in the minds of the speaker and 

the listener  

counterargument     

       if extralingusitc factors do influence linguistic events, 

there will always be a distributional correlate to the 
event that will suffice as explanatory principle    * 

* sahlgren, m. (2006) the distributional hypothesis 

not everyone is happy        

argument against     

    the concept of semantic difference (or similarity) 

is too broad to be useful !!!  

 

counterargument      

    semantic relations    are not axiomatic, and the broad 

notion of semantic similarity seems perfectly plausible    * 

* sahlgren, m. (2006) the distributional hypothesis 

functional differences 

    functional differences across words are 

fundamental for defining the notion of meaning 

    two different types of functional differences 

between words can be distinguished: * 

    syntagmatic relations:  
  explain how words are combined (co-occurrences) 

    paradigmatic relations: 
  explain how words exclude each other (substitutions) 

 
 
*  saussure, f. (1916) course in general linguistics 

 

orthogonal dimensions 

 
 

c
i
t
a
m
g
i
d
a
r
a
p

some 

scientists 

look 

smart 

few 

people 

feel 

dumb 

most 

citizens 

seem 

gifted 

many 

lawyers 

are 

savvy 

syntagmatic  

the term-context matrix 

d1: dogs are animals 

 
s
l
a
m
n
a

i

 
 
 

 

e
r
a

 
s
t
a
c

 
s
g
o
d

 
s
d
i
h
c
r
o

 
s
t
n
a
l
p

 
s
e
s
o
r

d2: cats are animals 

animals 

x  x  x 

d3: orchids are plants 

d4: roses are plants 

  

 

are 

cats 

dogs 

orchids 

plants 

roses 

x 

x  x  x  x  x 

x  x 

x  x 

x 

x 

x 

x 

x 

x 

x 

paradigmatic relation matrix 

top paradigmatic pairs  

(dogs, cats) 

(orchids, roses) 

 

 

 
s
l
a
m
n
a

i

 
 
 

 

e
r
a

 
s
t
a
c

 
s
g
o
d

 
s
d
i
h
c
r
o

 
s
t
n
a
l
p

 
s
e
s
o
r

animals 

x  x  x 

are 

cats 

dogs 

orchids 

plants 

roses 

x 

x  x  x  x  x 

x  x 

x  x 

x 

x 

x 

x 

x 

x 

x 

the term-document matrix 

d1: dogs are animals 

d2: cats are animals 

d3: orchids are plants 

d4: roses are plants 

animals 

are 

cats 

dogs 

orchids 

plants 

roses 

d1  d2  d3  d4 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

  

 

syntagmatic relation matrix 

top syntagmatic pairs  

d1  d2  d3  d4 

(animals, cats) 

(animals, dogs) 

(orchids, plants) 

(plants, roses) 

 

 

animals 

are 

cats 

dogs 

orchids 

plants 

roses 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

section 1 

basic concepts and theoretical framework 

    the distributional hypothesis  

    vector space models and the term-document matrix 

    association scores and similarity metrics 

    the curse of dimensionality and dimensionality 

reduction 

    semantic cognition, conceptualization and abstraction 

vector space models (vsms) 

    vector space models have been extensively used 

in artificial intelligence and machine learning 
applications  

    vector space models for language applications 
were introduced by gerard salton* within the 
context of information retrieval 

    vector spaces allow for simultaneously modeling 

words and the contexts in which they occur 

* salton g. (1971) the smart retrieval system: experiments in automatic document processing 

three main vsm constructs*  

    the term-document matrix 

    similarity of documents 

    similarity of words (syntagmatic relations) 

    the word-context matrix 

    similarity of words (paradigmatic relations) 

    the pair-pattern matrix 

    similarity of relations 

* turney p.d., pantel p. (2010) from frequency to meaning: vector space models of semantics,  
journal of artificial intelligence research, 37: 141-188 

the term-document matrix 

    a model representing joint distributions 

between words and documents 

d1  d2  d3  d4  d5  d6  d7  d8       dn 

non-zero row values for those 

documents containing a given word  

vij 

non-zero column values for those words 

occurring within a given document  

. 

t1 

t2 

t3 

t4 

t5 

t6 

    

the term-document matrix 

    each row of the matrix represents a unique 

vocabulary word in the data collection 

    each column of the matrix represents a unique 

document in the data collection  

    represents joint distributions between words 

and documents 

    it is a bag-of-words kind of representation 

    a real-valued weighting strategy is typically 
used to improve discriminative capabilities  

a bag-of-words type of model 

document collection 

document x 

response 

said 

covering 

candidate 

picture 

document z 

animals 

rain 

feeding  environment 

response 

    relative word orderings within the documents 

are not taken into account 

weighting strategies 

    more discriminative words are more important ! 

. 

very frequent words 
(function words) 

frequent and infrequent words 
(content words) 

very rare words 
(content words) 

zipf   s law 
for languages 

. 

tf-idf weighting scheme* 

we want to favor words that are: 

    common within documents 

    term-frequency weight (tf): it counts how many 

times a word occurs within a document 

    uncommon across documents 

    inverse-document-frequency (idf): it inversely 

accounts for the number of documents that contain 
a given word   

* sp  rck jones, k. (1972), a statistical interpretation of term specificity and its application in 
retrieval, journal of documentation, 28(1), 11-21 

tf-idf weighting effects  

higher weights are given to those words that are 
frequent within but infrequent across documents  

term  
frequencies 

tf-idf 

inverse 
document 
frequencies 

rank 

very common words 

very rare words 

tf-idf weighting computation 

    term-frequency (tf): 

 

tf(wi,dj) = |wi    dj|  

    inverse-document-frequency (idf): 

idf(wi) = log   

 
    tf-idf with document length id172: 

(             ) 

1+|d    d : wi    d| 

|d| 

tf-ifd(wi,dj) = 

tf(wi,dj)  ifd(wi) 

   i|wi    dj| 

pmi weighting scheme* 

    point-wise mutual information (pmi) 
pmi(wi,dj) = log    p(wi,dj) 
p(wi) p(dj) 

(        ) 

 

    positive pmi (ppmi) 

 

ppmi(wi,dj) = {         

pmi(wi,dj)   if  > 0 
0    otherwise 

    discounted pmi (compensates the tendency of pmi 
to increase the importance of infrequent events)  

dpmi(wi,dj) = d ij pmi(wi,dj)   

* church, k., hanks, p. (1989), word association norms, mutual information, and id69, in 
proceedings of the 27th annual conference of the association of computational linguistics, pp. 76-83 

section 1 

basic concepts and theoretical framework 

    the distributional hypothesis  

    vector space models and the term-document matrix 

    association scores and similarity metrics 

    the curse of dimensionality and dimensionality 

reduction 

    semantic cognition, conceptualization and abstraction 

document vector spaces 

pay attention to the columns of the term-document matrix 

document 
vector 

observations 

d1  d2  d3  d4  d5  d6  d7  d8       dn 

. 

l

 
s
e
b
a
i
r
a
v

t1 
t2 
t3 
t4 
t5 
t6 
    

tm 

document vector spaces 

association scores and similarity metrics can be 
used to assess the degree of semantic relatedness 
among documents  

. 
dissimilar documents 

similar documents 

word vector spaces 

pay attention to the rows of the term-document matrix 

variables 
d1  d2  d3  d4  d5  d6  d7  d8       dn 

term 
vector 

. 

 
s
n
o

i
t

a
v
r
e
s
b
o

t1 
t2 
t3 
t4 
t5 
t6 
    

tm 

word vector spaces 

association scores and similarity metrics can be 
used to assess the degree of semantic relatedness 
among words  

. 
dissimilar terms 

similar terms 

assessing vector similarities 

    association scores provide a means for 

measuring vector similarity 

    distances, on the other hand, provide a 

means for measuring vector dissimilarities   

    similarities and dissimilarities are in 

essence opposite measurements, and can 
be easily converted from one to another 

association scores 

    dice: 

dice(v1,v2) = 

 

    jaccard: 

jacc(v1,v2) = 

 

    cosine: 

cos(v1,v2) = 

u 

2 |n1    n2|  
|n1|+|n2|  

  |n1    n2|  

u 

|n1    n2|  

u 

  <v1,v2> 
||v1|| || v2||  

distance metrics 

    hamming: 

 

hm(v1,v2) =|n1   z2|+|z1   n2| 

u 

u 

    euclidean: 

d(v1,v2) = ||v1     v2||  

 

    citiblock: 

cb(v1,v2) = ||v1     v2||1  

 

    cosine: 

dcos(v1,v2) = 1     cos(v1,v2) 

section 1 

basic concepts and theoretical framework 

    the distributional hypothesis  

    vector space models and the term-document matrix 

    association scores and similarity metrics 

    the curse of dimensionality and dimensionality 

reduction 

    semantic cognition, conceptualization and abstraction 

the curse of dimensionality* 

    refers to the data sparseness problem that is 

intrinsic to high-dimensional spaces 

    the problem results from the disproportionate 

increase of space volume with respect to the 
amount of available data 

    if the statistical significance of results are to be 

maintained, then the amount of required data 
will grow exponentially with dimensionality 

. 
*  bellman, r.e. (1957), id145, princeton university press 

id84 

    deals with the    curse of dimensionality    

problem 

    intends to explain the observations with less 

variables 

    attempts to find (or construct) the most 

informative variables  

. 
provides a mathematical metaphor to the cognitive 
processes of generalization and abstraction ! 

types of id84 

linear projections 
are like shadows 

non-linear projections 

preserve structure 

example of a linear projection 

c 

b 

a 

a        b        c 

xa  xb  xc 

ya 

za 

yb 

zb 

yc 

zc 

a        b        c 

wa  wb  wc 

a 

b  c 

example of a non-linear projection 

c 

b 

a 

a        b        c 

xa  xb  xc 

ya 

za 

yb 

zb 

yc 

zc 

a        b        c 

wa  wb  wc 

a 

b 

c 

the case of categorical data 

set of observations 

dissimilarity matrix 

leaps 

swims  eggs 

frog    dolp.  kang.  shark 

frog 

dolphin 

kangaroo 

shark 

frog 

dolphin 

kangaroo 

shark 

0 

2 

2 

1 

2 

0 

2 

1 

2 

2 

0 

3 

1 

1 

3 

0 

1 

frog 

2 

shark 

2 

3 

kangaroo 

1 

dolphin 

2 

low-dimensional  
embedding 

some popular methods 

    variable merging and pruning: 

    combine correlated variables (merging) 

    eliminate uninformative variables (pruning) 

    principal component analysis (pca) 

    maximizes data variance in reduced space  

    multidimensional scaling (mds) 

    preserves data structure as much as possible 

    autoencoders 

    neural network approach to id84 

 

variable merging and pruning 

    lemmatization and id30 (merging)  

    stop-word-list (pruning)  

. 

a 
colony 
for 
never 
table 
table 
tables 
the 
 

   

 
 

   

 
 

   

 
 

   

 
 

term-document matrix 
after vocabulary merging 
and pruning  

principal component analysis (pca) 

    eigenvalue decomposition of data covariance or 

correlation matrix (real symmetric matrix) 

 
mn  n = qn  n   n  n qn  n  
 

t 

 

diagonal matrix 
(eigenvalues) 

orthonormal matrix 
(eigenvectors) 

    singular value decomposition (svd)                   

of data matrix  

. 

mm  n = um  m sm  n vn  n  

t 

diagonal matrix 
(singular values) 

unitary matrices 

latent semantic analysis (lsa)* 

    based on the singular value decomposition (svd) 

of a term-document matrix+ 

mm  n 

= 

um  m 

sm  n 

 
s

m
r
e
t
 

m

term-document 

matrix 

= 

 

e
c
a
p
s
 

m
r
e
t

n documents 

k dimensions 

t 

vn  n  

document space 

 
s
n
o
i
s
n
e
m
i
d
k

 

^ 
mm  n 

    

um  k 

sk  k 

t 

vk  n  

* deerwester, s., dumais, s.t., furnas, g.w., landauer, t.k. and harshman, r. (1990), indexing by 
latent semantic analysis, journal  of the american society for information science, 41, pp.391-407 

multidimensional scaling (mds) 

    computes a low dimensional embedding by 

minimizing a    stress    function 

monotonic transformation 
 

stress function = 

 

 

input data dissimilarities 

s s ( f(xij)     dij ) 
2 

scaling factor 

distances among points 
in the embedding 

    metric mds: directly minimizes stress function 

    non-metric mds: relaxes the optimization problem by 

using a monotonic transformation 

autoencoders* 

    symmetric feed-forward non-recurrent neural network 

    restricted id82 (pre-training) 

    id26 (fine-tuning)    

encoder 

decoder 

input 

    

input 

bottleneck layer 

* g. hinton, r. salakhutdinov "reducing the dimensionality of data with neural networks", 
science, 313(5786):504-507, 2006 

section 1 

basic concepts and theoretical framework 

    the distributional hypothesis  

    vector space models and the term-document matrix 

    association scores and similarity metrics 

    the curse of dimensionality and dimensionality 

reduction 

    semantic cognition, conceptualization and abstraction 

what is cognition? 

    cognition is the process by which a sensory 
input is transformed, reduced, elaborated, 
stored, recovered, and used* 

    etymology:  

    latin verb cognosco (   with   +   know   )  
    greek verb gn  sko (   knowledge   ) 

    it is a faculty that allows for processing 

information, reasoning and decision making 

* neisser, u (1967) cognitive psychology, appleton-century-crofts, new york 

three important concepts 

    memory: is the process in which information is 

encoded, stored, and retrieved 

    id136: is the process of deriving logical 

conclusions from premises known or assumed to 
be true (deduction, induction, abduction)  

    abstraction: is a generalization process by 

which concepts and rules are derived from a 
multiplicity of observations 

approaches to semantic cognition 

    the hierarchical propositional approach* 

    concepts are organized in a hierarchical fashion 

 

    the parallel distributed processing approach** 

    concept are stored in a distributed fashion and 

reconstructed by pattern completion mechanisms 

* quillian m.r. (1968) semantic memory, in semantic information processing (ed. minsky, m.) 
pp.227-270, mit press 
 
** mcclelland, j.l. and rogers, t.t. (2003) the parallel distributed processing approach to semantic 
cognition, nature reviews, 4, pp.310-322 

hierarchical propositional model 

example domain 
of living things 

general 

 
y
m
o
n
o
x
a
t
 
l
a
c
i
h
c
r
a
r
e
i
h

specific 

image taken from: mcclelland, j.l. and rogers, t.t. (2003) the parallel distributed processing 
approach to semantic cognition, nature reviews, 4, pp.310-322 

advantages of hierarchical model 

    economy of storage 

    immediate generalization of 

    known propositions to new members 

    new propositions to known members  

    explains cognitive processes of * 

    general-to-specific progression in children 

    progressive deterioration in semantic dementia 

patients   

 

* warringtong, e.k. (1975) the selective impairment of semantic memory, the quarterly of journal 
experimental psychology, 27, pp.635-657 

hierarchical model drawback! 

there is strong experimental evidence of a 
graded category membership in human cognition 

    humans are faster verifying the statement * 

       chicken is an animal    than    chicken is a bird    

       robin is a bird    than    chicken is a bird    

    this is better explained when the verification 

process is approached by means of assessing 
similarities across categories and elements   

* rips, l.j., shoben, e.j. and smith, e.e. (1973) semantic distance and the verification of semantic 
relations, journal of verbal learning and verbal behaviour, 12, pp.1-20 

parallel distributed processing* 

    semantic information is stored in a 

distributed manner across the system  

    semantic information is    reconstructed    

by means of a pattern completion 
mechanism 

    the reconstruction process is activated as 

the response to a given stimulus  

* mcclelland, j.l. and rogers, t.t. (2003) the parallel distributed processing approach to semantic 
cognition, nature reviews, 4, pp.310-322 

rumelhart connectionist network* 

two-dimensional projection 
of the representation layer 

* rumelhart, d.e. and abrahamsonm a.a. (1973) a model of analogical 
reasoning, cognitive psychology, 5, pp.1-28 

image taken from: mcclelland, j.l. and rogers, t.t. (2003) the parallel distributed 
processing approach to semantic cognition, nature reviews, 4, pp.310-322 
 

advantages of the pdp model* 

    also explains both cognitive processes of 

development and degradation  

    additionally, it can explain the phenomenon 

of graded category membership: 

    use of intermediate level categories (basic level**) 

    over-generalization of more frequent items 

 

* mcclelland, j.l. and rogers, t.t. (2003) the parallel distributed processing approach to semantic 
cognition, nature reviews, 4, pp.310-322  
 
** rosch e., mervis c.b., gray w., johnson d. and boyes-braem, p. (1976) basic objects in natural 
categories, cognitive psychology, 8, pp.382-439 

pdp, dh and vector spaces 

    the parallel distributed processing (pdp) 
model explains a good amount of observed 
cognitive semantic phenomena 

    in addition, the connectionist approach has a 

strong foundation on neurophysiology 

    both pdp and distributional hypothesis (dh) 

use differences/similarities over a feature 
space to model the semantic phenomenon 

    vector spaces constitute a great mathematical 

framework for this endeavor !!!     

section 1 

main references for this section 

    m. sahlgren, 2006,    the distributional hypothesis     

    p. d. turney and p. pantel, 2010,     from frequency to meaning: 

vector space models of semantics     

    s. deerwester, s. t. dumais, g. w. furnas, t. k. landauer, and  

r. harshman, 1990,    indexing by latent semantic analysis     

    g. hinton and r. salakhutdinov, 2006,    reducing the 

dimensionality of data with neural networks    

    j. l. mcclelland and t. t. rogers, 2003,    the parallel distributed 

processing approach to semantic cognition    

section 1 

additional references for this section 

    firth, j.r. (1957) a synopsis of linguistic theory 1930-1955, in studies in linguistic 

analysis, 51: 1-31 

    harris, z. (1970) distributional structure, in papers in structural and transformational 

linguistics 

    saussure, f. (1916) course in general linguistics 

    salton g. (1971) the smart retrieval system: experiments in automatic document 

processing 

    sp  rck jones, k. (1972), a statistical interpretation of term specificity and its 

application in retrieval, journal of documentation, 28(1), 11-21 

    church, k., hanks, p. (1989), word association norms, mutual information, and 

id69, in proceedings of the 27th annual conference of the association of 
computational linguistics, pp. 76-83 

section 1 

additional references for this section 

    bellman, r.e. (1957), id145, princeton university press 

    neisser, u (1967) cognitive psychology, appleton-century-crofts, new york 

    quillian m.r. (1968) semantic memory, in semantic information processing (ed. minsky, 

m.) pp.227-270, mit press 

    warringtong, e.k. (1975) the selective impairment of semantic memory, the quarterly 

of journal experimental psychology, 27, pp.635-657 

    rips, l.j., shoben, e.j. and smith, e.e. (1973) semantic distance and the verification of 

semantic relations, journal of verbal learning and verbal behaviour, 12, pp.1-20 

    rumelhart, d.e. and abrahamsonm a.a. (1973) a model of analogical reasoning, 

cognitive psychology, 5, pp.1-28 

    rosch e., mervis c.b., gray w., johnson d. and boyes-braem, p. (1976) basic objects in 

natural categories, cognitive psychology, 8, pp.382-439 

section 2 

vector spaces in monolingual nlp 

    the semantic nature of vector spaces  

    information retrieval and relevance ranking 

    word spaces and related word identification 

    semantic compositionality in vector spaces 

constructing semantic maps 

tf-idf 
weighting 

document collection 

dimensionality 

reduction 

   semantic map    of 
words or documents 

vector space of 

words or documents 

document collection 

    the holy bible 

    66 books       1189 chapters       31103 verses 

       700k running words           12k vocabulary terms 

 

distribution of verses per book within the collection 

pentateuch 

wisdom 
books 

historical books 

major 
prophets 

minor prophets 

gospels 

acts 

revelation 

epistles 
(paul) 

epistles 

(others) 

old testament 

new testament 

semantic maps of documents 

document collection 

tf-idf 

vector space of documents 

mds 

0 

0 

cosine 
distance 

0 

0 

0 

0 

0 

   semantic map    of documents 

dissimilarity matrix 

semantic maps of documents 

minor prophets 

wisdom 
books 

new testament 

epistles 
(paul) 

major prophets 

revelation 

pentateuch 

historical books 

old testament 

gospels 

acts 

epistles 

(others) 

semantic maps of words 

document collection 

tf-idf 

vector space of words 

mds 

0 

0 

cosine 
distance 

0 

0 

0 

0 

0 

   semantic map    of words 

dissimilarity matrix 

semantic maps of words 

sky 

bird 

sky 

lightning 

rain 

field 

thunder 

cloud 

flock 

goat 

mountain 

sea 

sheep 

wind 

storm 

fish 

river 

non-living things 

living things 

land 

water 

discriminating meta-categories 

opinionated content from rating website (spanish) 

    positive and negative comments gathered from financial 

and automotive domains: 

    2 topic categories: automotive and financial 

    2 polarity categories: positive and negative 

    term-document matrix was constructed using full 

comments as documents 

    a two-dimensional map was obtained by applying mds to 

the vector space of documents 

discriminating meta-categories 

positive 

negative 

automotive 

financial 

section 2 

vector spaces in monolingual nlp 

    the semantic nature of vector spaces  

    information retrieval and relevance ranking 

    word spaces and related word identification 

    semantic compositionality in vector spaces 

document search: the ir problem 

    given an informational need (   search query   ) 

    and a very large collection of documents,  

    find those documents that are relevant to it 

query 

   find my docs    

document collection 

precision and recall 

how good a retrieval system is? 

selected 
documents 

relevant 
documents 

tp = rd     sd 

u 

tn =   rd       sd 

u 

fp =   rd     sd 

u 

fn = rd       sd 

u 

precision =  

tp 

tp + fp 

recall =  

tp 

tp + fn 

f-score = 2 

precision    recall 
precision + recall 

binary search* 

    keyword based (query = list of keywords)  

    and-search: selects documents containing all 

keywords in the query 

    or-search: selects documents containing at 

least one of the keywords in the query   

    documents are either relevant or not relevant 

(binary relevance criterion)  

* lee, w.c. and fox, e.a. (1988) experimental comparison of schemes for interpreting boolean queries. 
technical report tr-88-27, computer science, virginia polytechnic institute and state university 

vector space search* 

    keyword based (query = list of keywords)  

    uses vector similarity scores to assess document 

relevance (a graded relevance criterion)  

query 

most relevant documents 

most irrelevant documents 

vector space representation 
of the document collection 

* salton g., wong a. and  
yang c.s. (1975) a vector space for automatic indexing. communications of the acm, 18(11), pp. 613-620 

precision/recall trade-off 

score 

100% 

|rd|-1 

|rd| 
|nd| 

    0% 

recall 

f-score 

precision 

top-1 

top-n (optimal) 

all documents 

(documents ranked according to vector similarity with the query)  

number of selected documents 

illustrative example* 

consider a collection of 2349 paragraphs extracted 
from three different books: 

    oliver twist by charles dickens 

    840 paragraphs from 53 chapters  

    don quixote by miguel de cervantes 

    843 paragraphs from 126 chapters  

    pride and prejudice by jane austen 

    666 paragraphs from 61 chapters  

* banchs r.e. (2013) id111 with matlab, springer , chap. 11, pp. 277-311 

illustrative example 

distribution of paragraphs per book and chapter 

oliver 
twist 

don 
quixote 

pride & 
prejudice 

image taken from banchs r.e. (2013) id111 with matlab, springer , chap. 11, pp. 277-311 

illustrative example 

consider a set of 8 search queries: 

query 

oliver, twist, board 

london, road 

relevant book and chapter 

oliver twist, chapter 2 

oliver twist, chapter 8 

brownlow, grimwig, oliver 

oliver twist, chapter 14 

curate, barber, niece 

don quixote, chapter 53 

courage, lions 

don quixote, chapter 69 

arrival, clavileno, adventure 

don quixote, chapter 93 

darcy, dance 

pride & prejudice, chapter 18 

gardiner, housekeeper, elizabeth 

pride & prejudice, chapter 43 

experimental results 

recall 
bias 

precision 
bias 

precision 
recall 
f-score 

60% 
 
50% 
 
40% 
 
30% 
 
20% 
 
10% 

binary or-search 

binary and-search 

vector@10 search 

automatic relevance feedback* 

use first search results to improve the search!  

query 

the most relevant documents 
should contain words that are 
good additional query keywords 

the most irrelevant documents 
should contain words that are to 
be avoided as query keywords 

newquery  =  originalquery  +  a                     dr       b                        dnr  

1 

|dr| 

s 

1 

|dnr| 

s 

* rocchio j.j. (1971) relevance feedback in information retrieval. in salton g. (ed.) the smart retrieval 
system     experiments in automatic document processing, pp.313-323 

experimental results 

1.25% 
absolute gain 

baseline        with arf  

0.55% abs gain 

0.14% abs gain 

30% 

 

 
 
20% 
 
  
 
10% 
 

mean precision @10  mean recall @10 

mean f-score @10 

section 2 

vector spaces in monolingual nlp 

    the semantic nature of vector spaces  

    information retrieval and relevance ranking 

    word spaces and related word identification 

    semantic compositionality in vector spaces 

latent semantic analysis (lsa) 

tf-idf 
weighting 

document collection 

lsa 

better semantic 

properties 

reduced-dimensionality space 

vector space model 

latent semantic analysis (lsa)* 

svd:  

mm  n = um  m  sm  n  vn  n  

t 

t 

um  m  mm  n = dm  n 

documents projected into 
word space 

mm  n vn  n = wm  n 

words projected into 
document space 

t 

uk  m = 

u11 ...u1k ...um1 
u21 ...u2k ...um2 
 
 .
 
.
.
um1 ...umk ...umm 
 
 

 .
.
.
 

.
.
.
 

t 

vn  k = 

 

v11 ...v1k  ...vn1 
v21 ...v2k  ...vn2 
 
 .
.
.
vn1 ...vnk  ...vnn 
 
 

 .
.
.
 

.
.
.
 

t 

uk  m  mm  n = dk  n 

mm  n vn  k = wm  k 

documents projected into 
reduced word space 

words projected into 
reduced document space 

* deerwester, s., dumais, s.t., furnas, g.w., landauer, t.k. and harshman, r. (1990), indexing by 
latent semantic analysis, journal  of the american society for information science, 41, pp.391-407 

dataset under consideration* 

term definitions from spanish dictionary used as documents 

 

 

 

 

collection 

terms 

definitions  aver. length 

verbs 

adjectives 

nouns 

others 

complete 

4,800 

5,390 

20,592 

5,273 

36,055 

12,414 

6.05 words 

8,596 

6.05 words 

38,689 

9.56 words 

9,835 

8.01 words 

69,534 

8.32 words 

    a document vector space for    verbs    is constructed 

    lsa is used to project into a latent semantic space 

    mds is used to create a 2d map for visualization purposes   

* banchs, r.e. (2009), semantic mapping for related term identification, in conference on intelligent 
text processing and computational linguistics, cicling 2009, lns 5449, pp 111-124 

differentiating semantic categories 

two semantic categories of verbs are considered 

 

 
group a 
 
ayudar (to help) 
compartir (to share) 
 
beneficiar (to benefit) 

 

group b 

agredir (to threaten) 

destruir (to destroy) 

aniquilar (to eliminate) 

colaborar (to collaborate) 

atacar (to attack) 

salvar (to save) 

arruinar (to ruin) 

apoyar (to support) 

matar (to kill) 

cooperar (to cooperate) 

perjudicar (to perjudice) 

favorecer (to favour) 

    

differentiating semantic categories 

no lsa applied: original dimensionality maintained 

 

 

 

group a 

non separable 

group b 

differentiating semantic categories 

lsa used to project into latent space of 800 dimensions 

 

 

 

group a 

separable 

group b 

differentiating semantic categories 

lsa used to project into latent space of 400 dimensions 

 

 

 

group a 

separable 

group b 

differentiating semantic categories 

lsa used to project into latent space of 100 dimensions 

 

 

 

group a 

non separable 

group b 

semantic similarity of words 

the totality of the 12,414 entries for verbs were considered 

    an 800-dimensional latent space representation was 

generated by applying lsa 

    id116 was applied to group the 12,414 entries into 

1,000 clusters (minimum size 2, maximum size 36, mean 

size 12.4, variance 4.7)  

    finally, non-linear id84 (mds) was 

applied to generate a map 

semantic similarity of words 

to put under the sun 

to laugh 

to study 

to write 

to read 

to sail 

to swim 

to jump 

to walk 

to cry 

to water 

to raise crops 

to rain 

regularities in vector spaces* 

recurrent neural network language model 

    after study internal word representations 

generated by the model  

    syntactic and semantic regularities were 

discovered to be mapped into the form of 

constant vector offsets   

* mikolov t., yih w.t. and zweig g. (2013), linguistic regularities in continuous space word 
representations, naacl-hlt 2013 

recurrent neural network (id56) 

1-of-n 
word 
encoding 

x(t) 

h(t) 

y(t) 

w 

v 

word 
id203 
distribution 

   

 

r 

z-1 

   

 

h(t) = sigmoid(w x(t) + r h(t-1)) 

y(t) = softmax(v h(t)) 

regularities as vector offsets 

queens 

gender offset 

singular/plural offset 

kings 

queen 

king 

kings     king        queens     queen 

queens       kings     king + queen 

image taken from mikolov t., yih w.t. and zweig g. (2013), linguistic regularities in continuous 
space word representations, naacl-hlt 2013 

comparative evaluations* 
propositions formulated as analogy questions: 

   x is to y as m is to ___    

id56-320 

lsa-320 

17% 

29% 

id56-320 

lsa-320 

40% 

36% 

syntactic evaluation 
(8000 propositions)* 

semantic evaluation 

(79 propositions from semeval 2012)** 

* mikolov t., yih w.t. and zweig g. (2013), linguistic regularities in continuous space word 
representations, naacl-hlt 2013 
 
** jurgens d., mohammad s., turney p. and holyoak k. (2012), semeval-2012 task: measuring degrees 
of relational similarity, in semeval 2012, pp. 356-364 

section 2 

vector spaces in monolingual nlp 

    the semantic nature of vector spaces  

    information retrieval and relevance ranking 

    word spaces and related word identification 

    semantic compositionality in vector spaces 

semantic compositionality 

    the principle of compositionality states that 

the meaning of a complex expression depends on: 

    the meaning of its constituent expressions 

    the rules used to combine them  

    some idiomatic expressions and named entities 

constitute typical exceptions to the principle of 

compositionality in natural language  

compositionality and exceptions 

consider the adjective-noun constructions 

 

 

red car 

 

 

white house 

 

??? 

compositionality in vector space 

    can this principle be modeled in vector space 

representations of language? 

    two basic mechanisms can be used to model 

compositionality in the vector space model framework*  

    intersection of properties (multiplicative approach) 

    combination of properties (additive approach) 

 

* mitchell j. and lapata m. (2008), vector-based models of semantic composition, in proceedings of 
acl-hlt 2008, pp. 236-244 

compositionality models 

    given two word vector representations x and y 

    a composition vector z can be computed as: 

multiplicative models  

tensor 
product 

linear 
combination 

additive models 

z = c x y 

z = a x + b y 

zi =      xj yi-j 

sj 

circular convolution 

zi = xi + yi 
simple additive 

zi = xi yi 

simple multiplicative 

zi = a xi + b yi + g xi yi 

combined model 

zi = a xi + b yi 
weighted additive 

additive compositionality* 

    use unigram and bigram counts to identify phrases 

    uses skip-gram model to compute word representations 

    compute element-wise additions of word vectors to 

retrieve associated words: 

    czech + currency           koruna, check crown,     

    german + airline            airline lufthansa, lufthansa,     

    russian + river                moscow, volga river,     

* mikolov t., sutskever i., chen k., corrado g. and dean j. (2013), distributed representations of 
words and phrases and their compositionality, arxiv:1310.4546v1 

adjectives as linear maps* 

    an adjective-noun composition vector is:  z = a n  

    the rows of a are estimated by id75s 

    some examples of predicted nearest neighbors:  

    general question              general issue 

    recent request                 recent enquiry 

    current dimension            current element 

    special something             special thing 

* baroni m. and zamparelli r. (2010), nous are vectors, adjectives are matrices: representing 
adjective-noun constructions in semantic space, in emnlp 2010 

section 2 

main references for this section 

    g. salton, a. wong and c. s. yang, 1975,    a vector space 

for automatic indexing      

    r. e. banchs, 2013,    id111 with matlab    

    r. e. banchs, 2009,    semantic mapping for related term 

identification     

    t. mikolov, w. t. yih and g. zweig, 2013,    linguistic 

regularities in continuous space word representations     

    j. mitchell and m. lapata, 2008,    vector-based models of 

semantic composition    

section 2 

additional references for this section 

    lee, w.c. and fox, e.a. (1988) experimental comparison of schemes for interpreting 
boolean queries. technical report tr-88-27, computer science, virginia polytechnic 
institute and state university 

    rocchio j.j. (1971) relevance feedback in information retrieval. in salton g. (ed.) the 
smart retrieval system     experiments in automatic document processing, pp.313-323 

    deerwester, s., dumais, s.t., furnas, g.w., landauer, t.k. and harshman, r. (1990), 

indexing by latent semantic analysis, journal  of the american society for information 
science, 41, pp.391-407 

    jurgens d., mohammad s., turney p. and holyoak k. (2012), semeval-2012 task: 

measuring degrees of relational similarity, in semeval 2012, pp. 356-364 

    mikolov t., sutskever i., chen k., corrado g. and dean j. (2013), distributed 

representations of words and phrases and their compositionality, arxiv:1310.4546v1 

    baroni m. and zamparelli r. (2010), nous are vectors, adjectives are matrices: 

representing adjective-noun constructions in semantic space, in emnlp 2010 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation 

    evaluating machine translation in vector space  

semantic maps revisited 

document collection 

tf-idf 

vector space of documents 

mds 

0 

0 

cosine 
distance 

0 

0 

0 

0 

0 

   semantic map    of documents 

dissimilarity matrix 

multilingual document collection 

66 books from the holy bible: english version 

 

 

 

 

 

  

(vocabulary size: 8121 words) 

multilingual document collection 

66 books from the holy bible: chinese version 

 

 

 

 

 

  

(vocabulary size: 12952 words) 

multilingual document collection 

66 books from the holy bible: spanish version 

 

 

 

 

 

  

(vocabulary size: 25385 words) 

cross-language similarities 

    each language map has been obtained 

independently from each other language 
(monolingual context)   

    the similarities among the maps are 

remarkable 

    could we exploit these similarities for 
performing cross-language information 
retrieval tasks? 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation  

    evaluating machine translation in vector space 

semantic maps for clir 

query 

english 

results 

spanish 

chinese 

clir by using mds projections* 

    start from a multilingual collection of    anchor 

documents    and construct the retrieval map 

    project new documents and queries from any 

source language into the retrieval language map  

    retrieve documents over retrieval language map 

by using a distance metric 

* banchs r.e. and kaltenbrunner a. (2008), exploring mds projections for cross-language 
information retrieval, in proceedings of the 31st annual international acm sigir 2008 

clir by using mds projections 

source language vector space 

retrieval language vector space 

anchor 
documents 

query  
   placement 

new document 
placement 

mds 

retrieval map 

computing a projection matrix 

a linear transformation from the original high dimensional 

space into the lower dimensionality map can be inferred 

from anchor documents 

coordinates of anchor documents 
in the projected space (kxn) 

m = t d 

distances among anchor documents  
in the original space (nxn) 

transformation 
matrix (kxn) 

t = m d-1 

projecting documents and queries 

a probe document or query can be placed into the 

retrieval map by using the transformation matrix 

m = t d 

transformation 
matrix (kxn) 

coordinates of probe document 
(or query) in the projected space 
of retrieval language 

distances between probe document 
(or query) and anchor documents 
in the original language space 

computing a projection matrix 

two different variants of the linear projection matrix t 

can be computed: 

    a monolingual projection matrix: * 

 

    m and d are computed on the retrieval language 

    a cross-language projection matrix: ** 

    m is computed on the retrieval language, and 

    d is computed on the source language 

 * banchs r.e. and kaltenbrunner a. (2008), exploring mds projections for cross-language 
information retrieval, in proceedings of the 31st annual international acm sigir 2008 
 
** banchs r.e. and costa-juss    m.r. (2013), cross-language document retrieval by using nonlinear 
semantic mapping, international journal of applied artificial intelligence, 27(9), pp. 781-802 

monolingual projection method 

source 

language  m = (md-1) d 

monolingual projection matrix 

m 

d 

mds 

retrieval language 

retrieval map 

cross-language projection method 

source 

language  m = (md-1) d 

d 

cross-language projection matrix 

m 

retrieval language 

retrieval map 

mds 

clir by using cross-language lsi* 

    in monolingual lsi, the term-document matrix is 

decomposed into a set of k orthogonal factors by means 

of singular value decomposition (svd) 

    in cross-language lsi, a multilingual term-document 

matrix is constructed from a multilingual parallel 

collection and lsi is applied by considering multilingual 

   extended    representations of query and documents 

* dumais s.t., letsche t.a., littman m.l. and landauer t.k. (1997), automatic cross-language 
retrieval using id45, in aaai-97 spring symposium series: cross-language 
text and speech retrieval, pp. 18-24 

the cross-language lsi method 

multilingual 
term-document matrix 

x = 

xa 
xb 

term-document matrix 
in language a 

term-document matrix 
in language b 

t 

svd:   x = u  s  v 
retrieval is based on 
internal product of the form: 
 
 
 
with: 

d =         or  

0 

da 
0 

db 

<u 

t d , u 

t q> 

document in language a 

query in language a 

q =         or  

qa 
0 

0 

qb 

document in language b 

query in language b 

comparative evaluations 

we performed a comparative evaluation of the three 

methods described over the trilingual dataset: 

    task 1: retrieve a book using the same book in a 

different language as query: 

    subtask 1.a: dimensionality of the retrieval space is varied 

    subtask 1.b: anchor document set size is varied 

    task 2: retrieve a chapter using the same chapter in a 

different language as a query 

task 1.a: dimensionality of space 

 
 
y
c
a
r
u
c
c
a
 
1
-
p
o
t

retrieval carried out over  
chinese map 

english map 

english to chinese 

spanish map 

task 1.b: anchor document set 

 
 
y
c
a
r
u
c
c
a
 
1
-
p
o
t

retrieval carried out over  
chinese map 

english map 

english to chinese 
(dimensionality of retrieval space  
is equal to anchor set size) 

spanish map 

task 2: chapter retrieval 

 
 
y
c
a
r
u
c
c
a
 
1
-
p
o
t

retrieval carried out over  
chinese map 

english map 

english to chinese 
(dimensionality of retrieval space  
is equal to anchor set size) 

spanish map 

some conclusions* 

    semantic maps, and more specifically mds 

projections, can be exploited for clir tasks 

    the cross-language projection matrix variant 

performs better than the monolingual projection 

matrix variant 

    mds maps perform better than lsi for the 

considered clir tasks  

* banchs r.e. and costa-juss    m.r. (2013), cross-language document retrieval by using nonlinear 
semantic mapping, international journal of applied artificial intelligence, 27(9), pp. 781-802 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation 

    evaluating machine translation in vector space  

main scripts used around the world 

id68 and romanization 

    the process of phonetically representing the 

words of one language in a non-native script 

    due to socio-cultural and technical reasons, 

most languages using non latin native scripts 

commonly implement latin script writing rules: 

   romanization    

       

 n   h  o 

the multi-script ir (msir) problem* 

    there are many languages that use non latin 

scripts (japanese, chinese, arabic, hindi, etc.) 

    there is a lot of text for these languages in the 

web that is represented into the latin script 

    for some of these languages, no standard rules 

exist for id68   

* gupta p., bali k., banchs r.e. choudhury m. and rosso p. (2014), id183 for multi-script 
information retrieval, in proceedings of the 37st annual international acm sigir 2014 

the main challenge of msir 

    mixed script queries and documents 

    extensive spelling variations 

native script 

non-native script 

mixed script 

teri 

galiyan 

spelling variations 

significance of msir 

    only 6% of the queries issued in india to bing 

contain hindi words in latin script 

    from a total number of 13.78 billion queries!!! 

800 million queries!!! 

others (25%) 

websites (22%) 

songs & lyrics (18%) 

people (6%) 

organizations (14%) 

locations (8%) 

movies (7%) 

proposed method for msir* 

    use characters and bigram of characters as terms 

(features) and words as documents (observations) 

    build a cross-script semantic space by means of a 

deep autoencoder  

    use the cross-script semantic space for finding 

   equivalent words    within and across scripts 

    use    equivalent words    for id183  

* gupta p., bali k., banchs r.e. choudhury m. and rosso p. (2014), id183 for multi-script 
information retrieval, in proceedings of the 37st annual international acm sigir 2014 

training the deep autoencoder 

20 

. . .  

250 

500 

3252 

(50 + 50x50) 
native script 

(26 + 26x26) 
latin script 

30k pairs (training data)  

images taken from gupta p., bali k., banchs r.e. choudhury m. and rosso p. (2014), id183 
for multi-script information retrieval, in proc. of the 37st annual international acm sigir 2014 

building the semantic space 

[semantic codes] 

20 

. . .  

2d visualization of the 
constructed cross-script 
semantic space 

250 

500 

3252 

[native script  | 000000   0] 

 

[0000000   0  | latin script] 

 all available words used 

images taken from gupta p., bali k., banchs r.e. choudhury m. and rosso p. (2014), id183 
for multi-script information retrieval, in proc. of the 37st annual international acm sigir 2014 

cross-script id183 

baseline systems 

the proposed method is compared to: 

    na  ve system: no id183 used  

    lsi: uses cross-language lsi to find the word 

equivalents 

    cca: uses canonical correlation analysis* to find 

the word equivalents   

* kumar s. and udupa r. (2011), learning hash functions for cross-view similarity search, in 
proceedings of ijcai, pp.1360-1365 

comparative evaluation results 

method 

mean average 

precision 

similarity 
threshold 

na  ve  

lsi 

cca 

29.10% 

35.22% 

38.91% 

autoencoder 

50.39% 

na 

0.920 

0.997 

0.960 

number of    word equivalents    

image taken from gupta p., bali k., banchs r.e. choudhury m. and rosso p. (2014), id183 
for multi-script information retrieval, in proc. of the 37st annual international acm sigir 2014 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation  

    evaluating machine translation in vector space 

cross-language sentence matching 

    focuses on the specific problem of text matching at the 

sentence level 

    a segment of text in a given language is used as a query 

for retrieving a similar segment of text in a different 

language 

    this task is useful to some specific applications: 

    parallel corpora compilation 

    cross-language plagiarism detection  

parallel corpora compilation* 

    deals with the problem of extracting parallel 

sentence from comparable corpora  

english 

1. singapore, officially the republic of singapore 

2.

is a sovereign city-state and island country in 

southeast asia 

3. and from indonesia's riau islands by the 

singapore strait to the south 

4.     

spanish 

1. singapur, oficialmente la rep  blica de singapur 

2. es un pa  s soberano insular de asia 

3. y al norte de las islas riau de indonesia, separada 

de estas por el estrecho de singapur 

4.     

* utiyama m. and tanimura m. (2007), automatic construction technology for parallel corpora, 
journal of the national institute of information and communications technology, 54(3), pp.25-31 

cl plagiarism detection* 

    deals with the problem of identifying copied 

documents or fragments across languages 

95% 

(english) 
source document 

83% 

67% 

60% 

document collection (spanish) 

* potthast m., stein b., eiselt a., barr  n a. and rosso p. (2009), overview of the 1st international 
competition on plagiarism detection, workshop on uncovering plagiarism, authorship, and social 
software misuse 

proposed method 

    the previously described mds-based semantic 

map approach to clir is used  

    cross-language projection matrix variant* 

    additionally, a majority voting strategy over 

different semantic retrieval maps is implemented 

and tested  

* banchs r.e. and costa-juss   m.r. (2010), a non-linear semantic mapping technique for cross-
language sentence matching, in proceedings of the 7th international conference on advances in natural 
language processing (icetal'10), pp. 57-66. 

majority voting strategy 

retrieval map 1 

retrieval map 2 

retrieval map k 

q 

d1 

d2 

d3 

d2 

d3 

q 

d1 

     

q 

d1 

d2 

d3 

global 
ranking 

d2 
d1 
d3 

ranking 1 

d1 
d2 
d3 

ranking 2 

     

ranking k 

d2 
d1 
d3 

d2 
d1 
d3 

penta-lingual data collection  

extracted from the spanish constitution 

english  spanish  catal    euskera  galego 

number of sentences 

611 

611 

611 

611 

611 

number of words 

15285 

14807 

15423 

10483 

13760 

vocabulary size 

2080 

2516 

2523 

average sentence length 

25.01 

24.23 

25.24 

3633 

17.16 

2667 

22.52 

language  sample sentence 

english  this right may not be restricted for political or ideological reasons 

spanish  este derecho no podr   ser limitado por motivos pol  ticos o ideol  gicos 

catal    aquest dret no podr   ser limitat por motius pol  tics o ideol  gics 

euskera  eskubide hau arrazoi politiko edo idiologikoek ezin dute mugatu 

galego  este dereito non poder   ser limitado por motivos pol  ticos ou ideol  xicos 

task description 

    to retrieve a sentence from the english version of the 

spanish constitution using the same sentence in any of 

the other four languages as a query 

    performance quality is evaluated by means of top-1 and 

top-5 accuracies measured over a 200-sentence test set 

    one retrieval map is constructed for each language 

available in the collection (400 anchor documents)  

    retrieval map dimensionality for all languages: 350 

evaluation results 

spanish 

catal   

euskera 

galego 

retrieval map 

top-1 

top-5 

top-1 

top-5 

top-1 

top-5 

top-1 

top-5 

english 

97.0 

100 

96.0 

99.0 

69.5 

91.0 

95.0 

98.5 

spanish 

95.5 

99.0 

94.5 

99.5 

77.0  93.0 

94.0 

99.5 

catal   

95.0 

100 

94.5 

99.5 

74.5 

90.5 

93.0 

99.0 

euskera 

96.5 

99.0 

95.0 

99.5 

70.0 

86.5 

95.0 

98.5 

galego 

96.5 

100 

94.5 

100 

73.0 

91.5 

93.0 

98.0 

majority voting  97.5 

100 

96.5 

99.5 

76.0 

92.5 

94.5 

99.5 

comparative evaluation 

    the proposed method (majority voting result) is 

compared to other two methods: 

    cross-language lsi* (previously described) 

    query translation** (a cascade combination of machine 

translation and monolingual information retrieval) 

* dumais s.t., letsche t.a., littman m.l. and landauer t.k. (1997), automatic cross-language 
retrieval using id45, in aaai-97 spring symposium series: cross-language 
text and speech retrieval, pp. 18-24 
 
** chen j. and bao y. (2009), cross-language search: the case of google language tools, first 
monday, 14(3-2) 

comparative evaluation results 

spanish 

catal   

euskera 

galego 

clir method  top-1  top-5  top-1  top-5  top-1  top-5 

top-1 

top-5 

lsi based  96.0 

99.0 

95.5 

98.5 

75.5 

90.5 

93.5 

97.5 

query transl.  96.0 

99.0 

95.5 

99.5 

* 

* 

93.5 

98.0 

semantic maps  97.5 

100 

96.5  99.5 

76.0  92.5 

94.5  99.5 

* euskera-to-english translations were not available 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation  

    evaluating machine translation in vector space 

id151 

developing context-awareness in smt systems 

    original noisy channel formulation: 
^ 
t = argmax p(t|s) = argmax p(s|t) p(t) 
 

t 

t 

 

    proposed model reformulation*: 
^ 
t = argmax p(t|s,c) = argmax p(c|s,t) p(s|t) p(t) 

context awareness model 

t 

t 

* banchs r.e. (2014), a principled approach to context-aware machine translation, in proceedings of 
the eacl 2014 third workshop on hybrid approaches to translation 

unit selection depends on context 

an actual example    

   wine    sense of    vino    

sc1:  

no hab  is comido pan ni tomado vino ni licor...  

 

ye have not eaten bread, neither have ye drunk wine or strong drink    

sc2:  

   dieron muchas primicias de grano, vino nuevo, aceite, miel y de todos     

 

    brought in abundance the first fruits of corn, wine, oil, honey, and of all     

   came    sense of    vino    

sc3:  

al tercer d  a vino jeroboam con todo el pueblo a roboam      

 

so jeroboam and all the people came to rehoboam the third day     

sc4:  

ella vino y ha estado desde la ma  ana hasta ahora     

 

she came , and hath continued even from the morning until now     

in1:  

in2:  

 

    una tierra como la vuestra, tierra de grano y de vino, tierra de pan y de vi  as     

(wine) 

cuando amanec  a, la mujer vino y cay   delante de la puerta de la casa de aquel     

(came) 

translation probabilities 

    translation probabilities: 

 

 

 

phrase 

f(f|e) 

lex(f|e) 

f(e|f) 

lex(e|f) 

{vino|||wine} 

0.665198  0.721612  0.273551 

0.329431 

{vino|||came} 

0.253568  0.131398  0.418478 

0.446488 

    proposed context-awareness model: 

sc1 

sc2 

sc3 

sc4 

sense 

in1 

in2 

{vino|||wine} 

{vino|||came} 

0.0636 

0.0023 

0.2666 

0.0513 

0.0351 

0.0888 

0.0310 

0.0774 

comparative evaluation* 

development 

baseline system 

vector space model 

statistical class model 

id44 

id45 

39.92 

40.61 

40.62 

40.63 

40.80 

test 

38.92 

39.43 

39.72 

39.82 

39.86 

* banchs r.e. and costa-juss   m.r. (2011), a semantic feature for id151, in 
fifth workshop on syntax, semantics and structure in statistical translation, acl 2011, pp. 126   134 

neural network models for mt* 

    the neural network framework can be used to 

incorporate source context information in both: 

    the target language model:  

 

neural network joint model (nnjm) 

    the translation model:  

 

neural network lexical translation model (nnltm) 

* devlin j., zbib r., huang z., lamar t., schwartz r. and makhoul j. (2014), fast and robust neural 
network joint models for id151, in proceedings of the 52 annual meeting of 
the association for computational linguistics, pp. 1370-1380 

joint model (nnjm) 

    estimates the id203 of a target word given 

its previous word history and a source context 

window 

| t | 

target history 

source context window 

p(t|s)           p( ti  | ti-1 , ti-2     ti-n , sj+m , sj+m-1     sj     sj-m+1 , sj-m ) 

i = 1 

target word 

with j = fa(i) 

lexical translation model (nnltm) 

    estimates the id203 of a target word 

given a source context window 

source context window 

| s | 

p(t|s)           p( ti  | sj+m , sj+m-1     sj     sj-m+1 , sj-m ) 

j = 1 

target word 

with i = fa(j) 

neural network architecture 

    feed-forward neural network language model* 

 
wt-1               wt-2                      wt-3                           wt-n 

 . . .  

1-of-n word 
encoding 

y = v f( b + w [c wt-1, c wt-2     c wt-n ] ) 

word 
representation layer 

hidden layer 

output layer 

yi = p(wt =i | context) 

* bengio j., ducharme r., vincent p. and jauvin c. (2003), a neural probabilistic language model, 
journal of machine learning research, 3, pp.1137-1155 

experimental results* 

arabic to english 

chinese to english 

48.9 

49.8 

51.2 

52.0 

33.0 

33.4 

34.2 

34.2 

baseline 

+id56lm 

+nnjm 

+nnltm 

* devlin j., zbib r., huang z., lamar t., schwartz r. and makhoul j. (2014), fast and robust neural 
network joint models for id151, in proceedings of the 52 annual meeting of 
the association for computational linguistics, pp. 1370-1380 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation 

    evaluating machine translation in vector space 

word translations in vector space 

    semantic similarities across languages can be 

exploited to    discover    word translation pairs 

from parallel data collections by:   

    either operating in the term-document matrix space*  

    or learning transformations across reduced spaces**  

* banchs r.e. (2013), id111 with matlab, springer , chap. 11, pp. 277-311 
 
** mikolov t., le q.v. and sutskever i. (2013), exploiting similarities among languages for machine 
translation, arxiv:1309.4168v1 

operating in term-document space* 

parallel corpus (aligned at sentence level)  

spanish 

term-document  
matrix (spanish) 

term w 

0  0  x  0  0  x  x  0  0  0 

english 

term-document  
matrix (english) 

vectors of parallel documents  
associated to term w 

vectors of parallel documents 

dissociated to term w 

* banchs r.e. (2013), id111 with matlab, springer , chap. 11, pp. 277-311 

obtaining the translation terms* 

    compute v+, the average vector of parallel 

documents associated to term w 

    compute v   , the average vector of parallel 

documents dissociated to term w 

    obtain the most relevant terms (with largest 

weights) for the difference vector v+     v    

* banchs r.e. (2013), id111 with matlab, springer , chap. 11, pp. 277-311 

some sample translations 

    english translations to spanish terms:  

    casa: house, home 

    ladr  n: thief, sure, fool  

    caballo: horse, horseback 

    spanish translations to english terms: 

    city: ciudad, fortaleza 

    fields: campo, vida 

    heart: coraz  n,   nimo, alma 

learning projections* 

    construct projection spaces by means of 

 

wt-2 

wt-1 

wt+1 

wt+2 

input 

    either cbow model 

(continuous bag-of-words) 

 

    or skip-gram model 

projection layer 

wt 

wt 

output 

input 

projection layer 

wt-2 

wt-1 

wt+1 

wt+2 

output 

* mikolov t., le q.v. and sutskever i. (2013), exploiting similarities among languages for machine 
translation, arxiv:1309.4168v1 

some sample projections 

english semantic map for animals 

spanish semantic map for animals 

horse 

cow 

dog 

pig 

caballo 

vaca 

perro 

cerdo 

cat 

gato 

images taken from mikolov t., le q.v. and sutskever i. (2013), exploiting similarities among 
languages for machine translation, arxiv:1309.4168v1 

obtaining the translation terms 

    use some bilingual word pairs {si, ti} to train a 

   translation matrix    w such that: 

 

 

ti     w si 

    use w for projecting a new term sj into the 

target space 

    collect the terms in target space that are closest 

to the obtained projection 

some sample translations* 

    english translations to spanish terms:  

    emociones: emotions, emotion, feeling 

    imperio: dictatorship, imperialism, tyranny  

    preparada: prepared, ready, prepare 

    millas: kilometers, kilometres, miles 

    hablamos: talking, talked, talk 

* mikolov t., le q.v. and sutskever i. (2013), exploiting similarities among languages for machine 
translation, arxiv:1309.4168v1 

the bi-cvm model* 

compositional sentence model 

| a | 

aroot =      ai s 

i = 0 

objective function 

minimizes: 
edist(a,b) = || aroot     broot ||2  
 
maximizes: 
edist(a,n) = || aroot     nroot ||2  
 

non parallel sentences 
(randomly selected) 

* hermann k.m., blunsom p. (2014), multilingual distributed representations without word alignment, 
arxiv:1312.6173v4 

some sample projections 

days of the week 

months of the year 

english        french        german 

french        german 

images taken from hermann k.m., blunsom p. (2014), multilingual distributed representations without 
word alignment, arxiv:1312.6173v4 

section 3 

vector spaces in cross-language nlp 

    semantic map similarities across languages  

    cross-language information retrieval in vector spaces 

    cross-script information retrieval and id68 

    cross-language sentence matching and its applications 

    semantic context modelling for machine translation 

    bilingual dictionary and translation-table generation 

    evaluating machine translation in vector space 

automatic evaluation of mt 

asr 

mt 

output 

unique 
transcription 

non 

unique 
reference 

? 

output 

human evaluation of mt* 

mt 

output 

p(t|s)     p(s|t) p(t) 

adequacy 
how much of the source information is preserved?  

 

fluency 
how good is the generated target language quality? 

* white j.s., o   cornell t. and nava f.o. (1994), the arpa mt evaluation methodologies: evolution, 
lessons and future approaches, in proc. of the assoc. for machine translation in the americas, pp. 193-205 

proposed evaluation framework* 

    approximate adequacy and fluency by means of 

independent models: 

    use a    semantic approach    for adequacy  

    use a    syntactic approach    for fluency  

    combine both id74 into a single 

evaluation score     

* banchs r.e., d'haro l.f., li h. (2015) "adequacy - fluency metrics: evaluating mt in the 
continuous space model framework", ieee/acm transactions on audio, speech and language 
processing, special issue on continuous space and related methods in nlp, vol.23, no.3, pp.472-482 

am: adequacy-oriented metric 

    compare sentences in a semantic space 

    monolingual am (mam): compare output vs. reference 

    cross-language am (xam): compare output vs. input 

mt 

output 

cl-lsi 

input 

lsi 

reference 

fm: fluency-oriented metric 

    measures the quality of the target language with a 

language model 

    uses a compensation factor to avoid effects derived from 

differences in sentence lengths 

mt 

output 

id165 lm 

input 

reference 

am-fm combined score 

both components can be combined into a single metric 

according to different criteria 

    weighted harmonic mean:  

 

h-am-fm  =  

am  fm 

a am + (1   a) fm 

     weighted mean: 

m-am-fm  =  (1   a) am + a fm 

 

     weighted l2-norm: 

n-am-fm  =     (1   a) am2 + a fm2 

wmt-2007 dataset* 

    fourteen tasks:  

    five european languages (en, es, de, fr, cz) and  

    two different domains (news and epps). 

    systems outputs available for fourteen of the fifteen 

systems that participated in the evaluation. 

    86 system outputs for a total of 172,315 individual 

sentence translations, from which 10,754 were rated for 

both adequacy and fluency by human judges. 

* callison-burch c., fordyce c., koehn p., monz c. and schroeder j. (2007), (meta-) evaluation of 
machine translation, in proceedings of id151 workshop, pp. 136-158 

dimensionality selection 

pearson   s correlation coefficients between the mam (left) 
and xam (right) components and human-generated scores 
for adequacy 

mam-fm and adequacy 

mam-fm and fluency 

xam-fm and adequacy 

xam-fm and fluency 

section 3 

main references for this section 

    r. e. banchs and a. kaltenbrunner, 2008,    exploring mds 

projections for cross-language information retrieval    

    p. gupta, k. bali, r. e. banchs, m. choudhury and p. rosso, 

2014,    id183 for multi-script information 
retrieval     

    r. e. banchs and m. r. costa-juss  , 2010,    a non-linear 

semantic mapping technique for cross-language sentence 
matching    

    r. e. banchs and m. r. costa-juss  , 2011,    a semantic 

feature for id151    

section 3 

main references for this section 

    j. devlin, r. zbib, z. huang, t. lamar, r. schwartz and j. 

makhoul,2014,    fast and robust neural network joint 
models for id151    

    t. mikolov, q. v. le and i. sutskever, 2013,    exploiting 
similarities among languages for machine translation    

    k.m. hermann k.m. and p. blunsom, 2014, multilingual 

distributed representations without word alignment 

    r.e. banchs, l.f. d'haro and h. li, 2015, "adequacy - 

fluency metrics: evaluating mt in the continuous space 
model framework" 

section 3 

additional references for this section 

    banchs r.e. and costa-juss    m.r. (2013), cross-language document retrieval by using 

nonlinear semantic mapping, international journal of applied artificial intelligence, 
27(9), pp. 781-802 

    dumais s.t., letsche t.a., littman m.l. and landauer t.k. (1997), automatic cross-

language retrieval using id45, in aaai-97 spring symposium series: 
cross-language text and speech retrieval, pp. 18-24 

    kumar s. and udupa r. (2011), learning hash functions for cross-view similarity search, 

in proceedings of ijcai, pp.1360-1365 

    utiyama m. and tanimura m. (2007), automatic construction technology for parallel 

corpora, journal of the national institute of information and communications 
technology, 54(3), pp.25-31 

    potthast m., stein b., eiselt a., barr  n a. and rosso p. (2009), overview of the 1st 

international competition on plagiarism detection, workshop on uncovering plagiarism, 
authorship, and social software misuse 

section 3 

additional references for this section 

    chen j. and bao y. (2009), cross-language search: the case of google language tools, 

first monday, 14(3-2) 

    banchs r.e. (2014), a principled approach to context-aware machine translation, in 

proceedings of the eacl 2014 third workshop on hybrid approaches to translation 

    bengio j., ducharme r., vincent p. and jauvin c. (2003), a neural probabilistic language 

model, journal of machine learning research, 3, pp.1137-1155 

    banchs r.e. (2013), id111 with matlab, springer , chap. 11, pp. 277-311 

    white j.s., o   cornell t. and nava f.o. (1994), the arpa mt evaluation methodologies: 

evolution, lessons and future approaches, in proc. of the assoc. for machine translation 
in the americas, pp. 193-205 

    callison-burch c., fordyce c., koehn p., monz c. and schroeder j. (2007), (meta-) 

evaluation of machine translation, in proceedings of id151 
workshop, pp. 136-158 

section 4 

future research and applications 

    current limitations of vector space models 

    encoding word position information into vectors 

    from vectors and matrices to tensors 

    final remarks and conclusions 

conceptual vs. functional 

    vector space models are very good to capture the 

conceptual aspect of meaning 

    {dog, cow, fish, bird} vs. {chair, table, sofa, bed}  

    however, they still fail to properly model the 

functional aspect of meaning 

       give me a pencil    vs.    give me that pencil      

word order information ignored 

    differently from formal semantics*, vsm lacks 

of a clean interconnection between the syntax 

and semantic phenomena 

    in part, a consequence of the bag-of-words 

nature of vsm 

 

vsms completely ignore word order information 
 
 * montague r. (1970), universal grammar, theoria, 36, pp. 373-398 

non-unique representations 

    consider the two following sentences* 

       that day the office manager, who was drinking, hit the 

problem sales worker with a bottle, but it was not serious    

       it was not the sales manager, who hit the bottle that day, but 

the office worker with a serious drinking problem    

    although they are completely different, they contain 

exactly the same set of words, so they will produce 

exactly the same vsm representation!  

* landauer t.k. and dumais s.t. (1997), a solution to plato   s problem: the latent semantic analysis theory 
of acquisition, induction and representation of knowledge, psychological review, 104(2), pp. 211-240 

 

other limitations 

additionally    

    vsms are strongly data-dependent  

    vsms noisy in nature (spurious events) 

    uncertainty or confidence estimation becomes 

an important issue 

    multiplicity of parameters with not clear 

relation to the outcomes 

 

section 4 

future research and applications 

    current limitations of vector space models 

    encoding word position information into vectors 

    from vectors and matrices to tensors 

    final remarks and conclusions 

semantics and word order 

    it is estimated that the meaning of english 

comes from* 

    word choice          80% 

    word order            20% 

* landauer t.k. (2002), on the computational basis of learning and cognition: arguments from lsa, 
in ross b.h. (ed.) the psychology of learning and motivation: advances in research and theory, 41, 
pp. 43-84 

word order in additive models 

    additive composition can be sensitive to word 

order by weighting the word contributions* 

p = x + y 

 

 

 

p = a x + b y 

p (a > b ) 

p (a =b ) 

x 

p (a < b ) 

y 

* mitchell j. and lapata m. (2008), vector-based models of semantic composition, in proceedings of 
acl    hlt 2008, pp. 236-244 

circular convolution model 

    word order encoded into a vector by collapsing 

outer-product matrix of word vectors* 

pi =         x j   y (i-j) mod_n 

sj 

pi = ( p0 , p1 , p2 ) 

x0 y0  x0 y1  x0 y2 

x1 y0  x1 y1 

x1 y2 

x2 y0  x2 y1  x2 y2 

* jones m.n. and mewhort d.j.k (2007), representing word meaning and order information in a 
composite holographic lexicon, psychological review, 114, pp. 1-37 

the random permutation model 

    use permutation functions to randomly shuffle 

the vectors to be composed* 

p = m x + m2y 

x 

m2 y 

m x 

random permutation  
operator  

y 

* sahlgren m., holst a. and kanerva p. (2008), permutations as a means to encode order in word 
space, in proceedings of the 30th annual conference of the cognitive science society, pp. 1300-1305 

recursive matrix vector spaces 

    each word and phrase is represented by a vector 

and a matrix* 

p1 = fv(zpo,poz) 
p1 = fm(po,z) 

(p1 , p1 ) 

p0 = fv(yx,xy) 
p0 = fm(x,y) 

(p0 , p0 ) 

(x , x) 

(y , y) 

(z , z) 

* socher r., huval b., manning c.d., ng a.y. (2012), semantic compositionality through recursive 
matrix-vector spaces, in proceedings of joint conference on empirical methods in natural language 
processing and computational natural language learning, pp. 1201-1211 

section 4 

future research and applications 

    current limitations of vector space models 

    encoding word position information into vectors 

    from vectors and matrices to tensors 

    final remarks and conclusions 

union/intersection limited binding  

    multiplicative operations limit vector interaction to 

those common non-zero components only 

 

 

[1, 0, 3, 0, 1, 0]    [0, 2, 1, 0, 4, 0] = [0, 0, 3, 0, 4, 0]  
x 

x 

    additive operations limit vector interaction to both 

common and non-common non-zero components 

 
[1, 0, 3, 0, 1, 0] + [0, 2, 1, 0, 4, 0] = [1, 2, 3, 0, 4, 0]  

? 

? 

 

    can we define operations to model richer interactions 

across vector components?   

 

vector binding with tensor product* 

    the tensor product of two vectors 

 
a    b = { ai bj }  for i= 1, 2     na and  j = 1, 2     nb  

 

 

    all possible interactions across components are 

taken into account 

    but, the resulting vector representation is of 

higher dimensionality!  

 * smolensky p. (1990), tensor product variable binding and the representation of symbolic structures 
in connectionist systems, artificial intelligence, 46, pp.159-216 

compressing tensor products 

    compress the result to produce a composed 

representations with the same dimensionality of 

the original vector space 

    one representative example of this is the 

circular convolution model 

    can tensor representations be exploited at 

high dimensional space?  

 

section 4 

future research and applications 

    current limitations of vector space models 

    encoding word position information into vectors 

    from vectors and matrices to tensors 

    final remarks and conclusions 

vsms in monolingual applications 

vector space models have been proven useful 

for many monolingual nlp applications, such as:  

    id91 

    id147 

    classification 

    role labeling 

    information retrieval 

    sense disambiguation 

    id53 

    information extraction 

    essay grading  

    and so on    

 

 

vsms in cross-language applications 

vector space models are also starting to be 

proven useful for cross-language nlp applications:  

    cross-language information retrieval  

    cross-script information retrieval 

    parallel corpus extraction and generation 

    automated bilingual dictionary generation 

    machine translation (decoding and evaluation) 

    cross-language plagiarism detection  

 

future research 

seems to be moving in two main directions:  

    improving the representation capability of 

current vsm approaches by: 

    using neural network architectures 

    incorporating word order information 

    leveraging on more complex operators  

    developing a more comprehensive framework by 
combining formal and distributional approaches 

 

section 4 

main references for this section 

    t. k. landauer s. t. and dumais s.t. , 1997,    a solution to 

plato   s problem: the latent semantic analysis theory of 
acquisition, induction and representation of knowledge     

    j. mitchell and m. lapata, 2008,    vector-based models of 

semantic composition    

    m. n. jones and d. j. k. mewhort, 2007,    representing 

word meaning and order information in a composite 
holographic lexicon     

    m. sahlgren, a. holst and p. kanerva, 2008,    permutations 

as a means to encode order in word space    

section 4 

additional references for this section 

    montague r. (1970), universal grammar, theoria, 36, pp. 373-398 

    landauer t.k. (2002), on the computational basis of learning and cognition: arguments 

from lsa, in ross b.h. (ed.) the psychology of learning and motivation: advances in 
research and theory, 41, pp. 43-84 

    socher r., huval b., manning c.d., ng a.y. (2012), semantic compositionality through 

recursive matrix-vector spaces, in proceedings of joint conference on empirical 
methods in natural language processing and computational natural language learning, 
pp. 1201-1211 

    smolensky p. (1990), tensor product variable binding and the representation of symbolic 

structures in connectionist systems, artificial intelligence, 46, pp.159-216 

vector spaces 
for cross-language  
nlp applications 

rafael e. banchs 
human language technology department,  
institute for infocomm research, singapore 
 
 november 1, 2016  

austin, texas, usa.  emnlp2016 

