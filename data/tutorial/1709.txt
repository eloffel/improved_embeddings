practical neural 
networks for nlp 

(part 2)

chris dyer, yoav goldberg, graham neubig

previous part

    dynet 

    feed forward networks 

    id56s 

    all pretty standard, can do very similar in tf / theano / keras.

this part

    where dynet shines -- dynamically structured networks. 
    things that are cumbersome / hard / ugly in other   

frameworks.

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

tag

tag

mlp
    this is by now a very common model 

mlp

mlp

concat

concat

    shown to be effective in many works 
concat
    let's see how to implement it in dynet 
lstm_f
    ... and we'll complicate it a bit later 
lstm_b

lstm_b

lstm_b

lstm_f

lstm_f

lstm_f

lstm_b

tag

mlp

concat

tag

mlp

concat

lstm_f

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1,  128,  50, model) 

layers

in-dim

out-dim

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = [] 
    s = f_init 
    for we in wembs: 
        s = s.add_input(we) 
        fw_exps.append(s.output()) 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1,  128,  50, model) 

layers

in-dim

out-dim

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = [] 
    s = f_init 
    for we in wembs: 
        s = s.add_input(we) 
        fw_exps.append(s.output()) 

           

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1,  128,  50, model) 

layers

in-dim

out-dim

def word_rep(w): 
    w_index = vw.w2i[w] 
    return words_lookup[w_index] 

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = [] 
    s = f_init 
    for we in wembs: 
        s = s.add_input(we) 
        fw_exps.append(s.output()) 

           

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1,  128,  50, model) 

layers

in-dim

out-dim

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = [] 
    s = f_init 
    for we in wembs: 
        s = s.add_input(we) 
        fw_exps.append(s.output()) 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1,  128,  50, model) 

layers

in-dim

out-dim

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = [] 
    fw_exps = f_init.transduce(wembs) 
    s = f_init 
    for we in wembs: 
        s = s.add_input(we) 
        fw_exps.append(s.output()) 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1,  128,  50, model) 

layers

in-dim

out-dim

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = [] 
    fw_exps = f_init.transduce(wembs) 
    s = f_init 
    for we in wembs: 
        s = s.add_input(we) 
        fw_exps.append(s.output()) 

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    b_init = bwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = f_init.transduce(wembs) 
    bw_exps = b_init.transduce(reversed(wembs)) 

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    b_init = bwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = f_init.transduce(wembs) 
    bw_exps = b_init.transduce(reversed(wembs)) 

    # bilstm states 
    bi = [dy.concatenate([f,b]) for f,b in zip(fw_exps,  
                                               reversed(bw_exps))] 

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 
ph = model.add_parameters((32, 50*2)) 
po = model.add_parameters((ntags, 32)) 

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    b_init = bwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = f_init.transduce(wembs) 
    bw_exps = b_init.transduce(reversed(wembs)
    # bilstm states 
    bi = [dy.concatenate([f,b]) for f,b in zip(fw_exps,  
                                               reversed(bw_exps))] 
    # mlps 
    h = dy.parameter(ph) 
    o = dy.parameter(po) 
    outs = [o*(dy.tanh(h * x)) for x in bi] 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 
ph = model.add_parameters((32, 50*2)) 
po = model.add_parameters((ntags, 32)) 

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    b_init = bwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = f_init.transduce(wembs) 
    bw_exps = b_init.transduce(reversed(wembs)
    # bilstm states 
    bi = [dy.concatenate([f,b]) for f,b in zip(fw_exps,  
                                               reversed(bw_exps))] 
    # mlps 
    h = dy.parameter(ph) 
    o = dy.parameter(po) 
    outs = [o*(dy.tanh(h * x)) for x in bi] 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
def word_rep(w): 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 
    w_index = vw.w2i[w] 
    return words_lookup[w_index] 

    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    b_init = bwdid56.initial_state() 
    wembs = [word_rep(w) for w in words] 
    fw_exps = f_init.transduce(wembs) 
    bw_exps = b_init.transduce(reversed(wembs)
    # bilstm states 
    bi = [dy.concatenate([f,b]) for f,b in zip(fw_exps,  
                                               reversed(bw_exps))] 
    # mlps 
    h = dy.parameter(ph) 
    o = dy.parameter(po) 
    outs = [o*(dy.tanh(h * x)) for x in bi] 

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

back off to char-lstm 

for rare words

concat

c_f c_f c_f c_f c_f c_f c_f c_f

c_b c_b c_b c_b c_b c_b c_b c_b

e

n

g

u

l

f

e

d

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

engulfed

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

the

bilstm tagger

tag

mlp

tag

mlp

tag

mlp

tag

mlp

tag

mlp

concat

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

the

words_lookup = model.add_lookup_parameters((nwords, 128)) 
chars_lookup = model.add_lookup_parameters((nchars, 20)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
cfwdid56 = dy.lstmbuilder(1, 20, 64, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 
cbwdid56 = dy.lstmbuilder(1, 20, 64, model) 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
chars_lookup = model.add_lookup_parameters((nchars, 20)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
cfwdid56 = dy.lstmbuilder(1, 20, 64, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 
cbwdid56 = dy.lstmbuilder(1, 20, 64, model) 

def word_rep(w): 
    w_index = vw.w2i[w] 
    return words_lookup[w_index] 

words_lookup = model.add_lookup_parameters((nwords, 128)) 
chars_lookup = model.add_lookup_parameters((nchars, 20)) 
fwdid56 = dy.lstmbuilder(1, 128, 50, model) 
cfwdid56 = dy.lstmbuilder(1, 20, 64, model) 
bwdid56 = dy.lstmbuilder(1, 128, 50, model) 
cbwdid56 = dy.lstmbuilder(1, 20, 64, model) 

def word_rep(w): 
    w_index = vw.w2i[w] 
    return words_lookup[w_index] 

def word_rep(w, cf_init, cb_init): 
    if wc[w] > 5: 
        w_index = vw.w2i[w] 
        return words_lookup[w_index] 
    else: 
        char_ids = [vc.w2i[c] for c in w] 
        char_embs = [chars_lookup[cid] for cid in char_ids] 
        fw_exps = cf_init.transduce(char_embs) 
        bw_exps = cb_init.transduce(reversed(char_embs)) 
        return dy.concatenate([ fw_exps[-1], bw_exps[-1] ]) 

def build_tagging_graph(words): 
    dy.renew_cg() 
    # initialize the id56s 
    f_init = fwdid56.initial_state() 
    b_init = bwdid56.initial_state() 
    cf_init = cfwdid56.initial_state() 
    cb_init = cbwdid56.initial_state() 
    wembs = [word_rep(w, cf_init, cb_init) for w in words] 
    fws = f_init.transduce(wembs) 
    bws = b_init.transduce(reversed(wembs)) 
    # bilstm states 
    bi = [dy.concatenate([f,b]) for f,b in zip(fws, reversed(bws))] 
    # mlps 
    h = dy.parameter(ph) 
    o = dy.parameter(po) 
    outs = [o*(dy.tanh(h * x)) for x in bi] 
    return outs 

def tag_sent(words): 
    vecs = build_tagging_graph(words) 
    vecs = [dy.softmax(v) for v in vecs] 
    probs = [v.npvalue() for v in vecs] 
    tags = [] 
    for prb in probs: 
        tag = np.argmax(prb) 
        tags.append(vt.i2w[tag]) 
    return zip(words, tags) 

def sent_loss(words, tags): 
    vecs = build_tagging_graph(words) 
    losses = [] 
    for v,t in zip(vecs,tags): 
        tid = vt.w2i[t] 
        loss = dy.pickneglogsoftmax(v, tid) 
        losses.append(loss) 
    return dy.esum(losses) 

num_tagged = cum_loss = 0 
for iter in xrange(50): 
    random.shuffle(train) 
    for i,s in enumerate(train,1): 
        if i > 0 and i % 500 == 0:   # print status 
            trainer.status() 
            print cum_loss / num_tagged 
            cum_loss = num_tagged = 0 
        if i % 10000 == 0:           # eval on dev 
            good = bad = 0.0 
            for sent in dev: 
                words = [w for w,t in sent] 
                golds = [t for w,t in sent] 
                tags = [t for w,t in tag_sent(words)] 
                for go,gu in zip(golds,tags): 
                    if go == gu: good +=1 
                    else: bad+=1 
            print good/(good+bad) 
        # train on sent 
        words = [w for w,t in s] 
        golds = [t for w,t in s] 
        loss_exp =  sent_loss(words, golds) 
        cum_loss += loss_exp.scalar_value() 
        num_tagged += len(golds) 
        loss_exp.backward() 
        trainer.update() 

num_tagged = cum_loss = 0 
for iter in xrange(50): 
    random.shuffle(train) 
    for i,s in enumerate(train,1): 
        if i > 0 and i % 500 == 0:   # print status 
            trainer.status() 
            print cum_loss / num_tagged 
            cum_loss = num_tagged = 0 
        if i % 10000 == 0:           # eval on dev 
            good = bad = 0.0 
            for sent in dev: 
                words = [w for w,t in sent] 
                golds = [t for w,t in sent] 
                tags = [t for w,t in tag_sent(words)] 
                for go,gu in zip(golds,tags): 
                    if go == gu: good +=1 
                    else: bad+=1 
            print good/(good+bad) 
        # train on sent 
        words = [w for w,t in s] 
        golds = [t for w,t in s] 
        loss_exp =  sent_loss(words, golds) 
        cum_loss += loss_exp.scalar_value() 
        num_tagged += len(golds) 
        loss_exp.backward() 
        trainer.update() 

progress
reports

training

to summarize this part

    we've seen an implementation of a bilstm tagger 

    ... where some words are represented as char-level lstms 

    ... and other words are represented as word-embedding 

vectors 

    ... and the representation choice is determined at run time 

    this is a rather dynamic graph structure.

up next

    even more dynamic graph structure (id132) 

    extending the bilstm tagger to use global id136.

transition-based 

parsing

stack

i

buffer
i saw her duck
saw her duck

i saw her duck
her duck
i saw

i saw her duck

i saw her duck
i saw her duck
i saw her duck

action
shift
shift
reduce-l
shift
shift
reduce-l
reduce-r

transition-based parsing   

    build trees by pushing words (   shift   ) onto a stack 
and combing elements at the top of the stack into a 
syntactic constituent (   reduce   ) 

    given current stack and buffer of unprocessed 
words, what action should the algorithm take?

let   s use a neural network!

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

tokens is the sentence to be parsed. 
oracle_actions is a list of {shift, reduce_l, reduce_r}.

transition-based parsing   

    this is a good problem for dynamic networks! 

    different sentences trigger different parsing 

states 

    the state that needs to be embedded is complex 

(sequences, trees, sequences of trees) 

    the parsing algorithm has fairly complicated    ow 

control and data structures

transition-based parsing   
challenges

unbounded depth
i saw

unbounded length
her duck

arbitrarily complex trees

reading and   

forgetting(

i saw her duck

i saw her duck

transition-based parsing   
state embeddings

    we can embed words 

    assume we can embed tree fragments 

    the contents of the buffer are just a sequence 

    which we periodically    shift    from 

    the contents of the stack is just a sequence 

    which we periodically pop from and push to 

    sequences -> use id56s to get an encoding! 
    but running an id56 for each state will be expensive. can we do better?

transition-based parsing   
stack id56s
    augment id56 with a stack pointer 
    three constant-time operations 

    push - read input, add to top of stack 
    pop - move stack pointer back 

    embedding - return the id56 state at the location 

of the stack pointer (which summarizes its 
current contents)

transition-based parsing   
stack id56s

y0

;

dynet:
s=[id56.inital_state()]
s.append[s[-1].add_input(x1)
s.pop()
s.append[s[-1].add_input(x2)
s.pop()
s.append[s[-1].add_input(x3)

transition-based parsing   
stack id56s

y0

y1

;

x1

dynet:
s=[id56.inital_state()]
s.append[s[-1].add_input(x1)
s.pop()
s.append[s[-1].add_input(x2)
s.pop()
s.append[s[-1].add_input(x3)

transition-based parsing   
stack id56s

y0

y1

;

x1

dynet:
s=[id56.inital_state()]
s.append[s[-1].add_input(x1)
s.pop()
s.append[s[-1].add_input(x2)
s.pop()
s.append[s[-1].add_input(x3)

transition-based parsing   
stack id56s

y0

y1

y2

;

x1

x2

dynet:
s=[id56.inital_state()]
s.append[s[-1].add_input(x1)
s.pop()
s.append[s[-1].add_input(x2)
s.pop()
s.append[s[-1].add_input(x3)

transition-based parsing   
stack id56s

y0

y1

y2

;

x1

x2

dynet:
s=[id56.inital_state()]
s.append[s[-1].add_input(x1)
s.pop()
s.append[s[-1].add_input(x2)
s.pop()
s.append[s[-1].add_input(x3)

transition-based parsing   
stack id56s

y0

y1

y2

;

x1

x2

dynet:
s=[id56.inital_state()]
s.append[s[-1].add_input(x1)
s.pop()
s.append[s[-1].add_input(x2)
s.pop()
s.append[s[-1].add_input(x3)

y3

x3

transition-based parsing   

dynet wrapper implementation:

transition-based parsing   
representing the state

red-l(amod)
shift
   
reduce_l reduce_r

shift

pt

transition-based parsing   
representing the state

red-l(amod)
shift
   
reduce_l reduce_r

shift

pt

}

{z

s

|

p

t

o

;

amod

an

decision

overhasty

transition-based parsing   
representing the state

}

{z

s

|

p

t

o

red-l(amod)
shift
   
reduce_l reduce_r

shift

pt

}

top

{z

b

;

amod

an

decision

overhasty

was

made

root

transition-based parsing   
syntactic compositions

head

h

transition-based parsing   
syntactic compositions

modifier

m

head

h

transition-based parsing   
syntactic compositions

c = tanh(w[h; m] + b)

modifier

m

head

h

transition-based parsing   
syntactic compositions

it is very easy to experiment with different   
composition functions.

code tour

transition-based parsing   
representing the state

}

{z

s

|

p

t

o

red-l(amod)
shift
   
reduce_l reduce_r

shift

pt

}

top

{z

b

;

amod

an

decision

overhasty

was

made

root

transition-based parsing   
representing the state

}

{z

s

|

p

t

o

red-l(amod)
shift
   
reduce_l reduce_r

shift

pt

}

top

{z

b

;

amod

an

decision

overhasty

t
|

o

p

{
z

a

}

was

made

root

reduce-left(amod)

shift

transition-based parsing   
pop quiz

    how should we add this functionality?

structured training

what do we know so far?

    how to create relatively complicated models 

    how to optimize them given an oracle action 

sequence

local vs. global id136

decisions?   

    what if optimizing local decisions doesn   t lead to good global 

time    ies like an arrow
p(                                     ) = 0.4
nn vbz prpdet nn
p(                                     ) = 0.3
nn nnp vb det nn
p(                                     ) = 0.3
vb nnp prpdet nn
nn nnp prpdet nn
    simple solution: input last label (e.g. id56lm)   

    modeling search is dif   cult, can lead down garden paths 

    better solutions: 

    local consistency parameters (e.g. crf: lample et al. 2016) 
    global training (e.g. globally normalized nns: andor et al. 2016)

   
   
   
   
   
   
bilstm tagger w/ tag 
bigram parameters

<s>

tag

tag

tag

tag

tag

<s>

mlp

concat

mlp

mlp

mlp

mlp

concat

concat

concat

concat

lstm_f

lstm_f

lstm_f

lstm_f

lstm_f

lstm_b

lstm_b

lstm_b

lstm_b

lstm_b

the

brown

fox

the

from local to global

    standard bilstm id168:

log p (y|x) =xi
zxi

    with transition features:
1

log p (y, x) =

log p (yi|x)

log emission   
probs as scores

(se(yi, x) + st(yi 1, yi))

global id172

transition scores

how do we train?

    cannot simply enumerate all possibilities and do backprop 

    in easily decomposable cases, can use dp to calculate 

gradients (crf) 

    more generally applicable solutions: structured id88, 

margin-based methods

structured id88 

overview

time    ies like an arrow

reference
nn vbz prpdet nn

   

hypothesis
nn nnp vb det nn

  y = argmax

y

score(y|x;    )

update!

id88 loss
`percep(x, y,    ) = max(score(   y|x;    )   score(y|x;    ), 0)

structured id88 in 

dynet

def viterbi_sent_loss(words, tags): 
    vecs = build_tagging_graph(words) 
    vit_tags, vit_score = viterbi_decoding(vecs, tags) 
    if vit_tags != tags: 
        ref_score = forced_decoding(vecs, tags) 
        return vit_score - ref_score 
    else: 
        return dy.scalarinput(0) 

viterbi algorithm

time

   ies

like

an

arrow

<s>

viterbi algorithm

   ies

like

an

arrow

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

viterbi algorithm

like

an

arrow

   ies

nn

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

viterbi algorithm

like

an

arrow

   ies
s2,nn
nn

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

viterbi algorithm

like

an

arrow

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

   ies
s2,nn
nn
s2,nnp
nnp
s2,vb
vb
s2,vbz
vbz
s2,det
det
s2,prp
prp
   

s6,<s>
<s>

viterbi algorithm

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

   ies
s2,nn
nn
s2,nnp
nnp
s2,vb
vb
s2,vbz
vbz
s2,det
det
s2,prp
prp
   

like
s3,nn
nn
s3,nnp
nnp
s3,vb
vb
s3,vbz
vbz
s3,det
det
s3,prp
prp
   

an
s4,nn
nn
s4,nnp
nnp
s4,vb
vb
s4,vbz
vbz
s4,det
det
s4,prp
prp
   

arrow
s5,nn
nn
s5,nnp
nnp
s5,vb
vb
s5,vbz
vbz
s5,det
det
s5,prp
prp
   

s6,<s>
<s>

viterbi algorithm

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

   ies
s2,nn
nn
s2,nnp
nnp
s2,vb
vb
s2,vbz
vbz
s2,det
det
s2,prp
prp
   

like
s3,nn
nn
s3,nnp
nnp
s3,vb
vb
s3,vbz
vbz
s3,det
det
s3,prp
prp
   

an
s4,nn
nn
s4,nnp
nnp
s4,vb
vb
s4,vbz
vbz
s4,det
det
s4,prp
prp
   

arrow
s5,nn
nn
s5,nnp
nnp
s5,vb
vb
s5,vbz
vbz
s5,det
det
s5,prp
prp
   

code

viterbi initialization code
time

arrow

   ies

like

an

s0,<s> = 0

s0 = [0, 1, 1, . . .]t

    init_score = [small_number] * ntags 
    init_score[s_t] = 0 
    for_expr = dy.inputvector(init_score)

<s>

s0,nn = -   

nn

s0,nnp = -   

nnp

s0,vb = -   

vb

s0,vbz = -   

vbz

s0,det = -   

det
   

viterbi forward step

   ies

<s>

log p (y, x) =

nn

s2,nnp,nn

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

1

zxi

(se(yi, x) + st(yi 1, yi))

sf,i,j,k = sf,i 1,j + se,i,k + st,j,k

forward

transition

emission

i = 2 (time step)
j = nnp (previous pos)
k = nn (next pos)

<s>

viterbi forward step

   ies

nn

sf,i,j,k = sf,i 1,j + se,i,k + st,j,k

s2,nn,nn
s2,nnp,nn
s2,vb,nn

s2,vbz,nn

s2,det,nn

s2,prp,nn

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

<s>

viterbi forward step

   ies

nn

sf,i,j,k = sf,i 1,j + se,i,k + st,j,k

vectorize

sf,i,k = sf,i 1 + se,i,k + st,k

s2,nn,nn
s2,nnp,nn
s2,vb,nn

s2,vbz,nn

s2,det,nn

s2,prp,nn

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

viterbi forward step

   ies
s2,nn
nn

sf,i,j,k = sf,i 1,j + se,i,k + st,j,k

vectorize

sf,i,k = sf,i 1 + se,i,k + st,k

max
sf,i,k = max(sf,i,k)

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

viterbi forward step

sf,i,j,k = sf,i 1,j + se,i,k + st,j,k

vectorize

sf,i,k = sf,i 1 + se,i,k + st,k

max
sf,i,k = max(sf,i,k)
recurse
concat

sf,i = concat(sf,i,1, sf,i,2, . . .)

<s>

time
s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

   ies
s2,nn
nn
s2,nnp
nnp
s2,vb
vb
s2,vbz
vbz
s2,det
det
s2,prp
prp
   

transition matrix in dynet

add additional parameters
trans_lookup = model.add_lookup_parameters((ntags, ntags))
initialize at sentence start
trans_exprs = [trans_lookup[tid] for tid in range(ntags)]

viterbi forward in dynet

# perform the forward pass through the sentence 
for i, vec in enumerate(vecs): 
    my_best_ids = [] 
    my_best_exprs = [] 
    for next_tag in range(ntags): 
        # calculate vector for single next tag 
        next_single_expr = for_expr + trans_exprs[next_tag] 
        next_single = next_single_expr.npvalue() 
        # find and save the best score 
        my_best_id = np.argmax(next_single) 
        my_best_ids.append(my_best_id) 
        my_best_exprs.append(dy.pick(next_single_expr, my_best_id)) 
    # concatenate vectors and add emission probs 
    for_expr = dy.concatenate(my_best_exprs) + vec 
    # save the best ids 
    best_ids.append(my_best_ids)
and do similar for    nal    <s>    tag

viterbi backward in dynet

# perform the reverse pass 
best_path = [vt.i2w[my_best_id]] 
for my_best_ids in reversed(best_ids): 
    my_best_id = my_best_ids[my_best_id] 
    best_path.append(vt.i2w[my_best_id]) 
best_path.pop() # remove final <s> 
best_path.reverse() 
# return the best path and best score as an expression 
return best_path, best_expr 

forced decoding in dynet

def forced_decoding(vecs, tags): 
    # initialize 
    for_expr = dy.scalarinput(0) 
    for_tag = s_t 
    # perform the forward pass through the sentence 
    for i, vec in enumerate(vecs):  
        my_tag = vt.w2i[tags[i]] 
        my_trans = dy.pick(trans_lookup[my_tag], for_tag) 
        for_expr = for_expr + my_trans + vec[my_tag] 
        for_tag = my_tag 
    for_expr = for_expr + dy.pick(trans_lookup[s_t], for_tag) 
    return for_expr

caveat: downsides of 
structured training

    structured training allows for richer models 

    but, it has disadvantages 

    speed: requires more complicated algorithms 

    stability: often can   t enumerate whole hypothesis space 

    one solution: initialize with ml, continue with structured 

training

bonus: margin methods

    idea: we want the model to be really sure about the best path 
    during search, give bonus to all but correct tag

<s>

s1,nn
nn
s1,nnp
nnp
s1,vb
vb
s1,vbz
vbz
s1,det
det
s1,prp
prp
   

+1

+1

+1

+1

+1

s2,nn
nn
s2,nnp
nnp
s2,vb
vb
s2,vbz
vbz
s2,det
det
s2,prp
prp
   

+1

+1

+1

+1

+1

margins in dynet

def viterbi_decoding(vecs, gold_tags = []): 
    ... 
    for i, vec in enumerate(vecs): 
        ... 
        for_expr = dy.concatenate(my_best_exprs) + vec 
        if margin != 0 and len(gold_tags) != 0: 
            adjust = [margin] * ntags 
            adjust[vt.w2i[gold_tags[i]]] = 0 
            for_expr = for_expr + dy.inputvector(adjust) 

conclusion

training nns for nlp

    we want the    exibility to handle the structures we like 

    we want to write code the way that we think about models 

    dynet gives you the tools to do so! 

    we welcome contributors to make it even better

