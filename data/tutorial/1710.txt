a tutorial on

graph-based semi-supervised 
learning algorithms for nlp

amarnag subramanya
(google research)

partha pratim talukdar

(carnegie mellon university)

acl 2012, south korea

1

http://graph-ssl.wikidot.com/

supervised learning

learning 
algorithm

model

2

labeled datasupervised learning

learning 
algorithm

model

examples:

id90

  support vector machine (id166)
  maximum id178 (maxent)

2

labeled datasemi-supervised learning (ssl)

learning 
algorithm

model

3

labeled dataa lot of unlabeled datasemi-supervised learning (ssl)

learning 
algorithm

model

examples:

self-training
co-training

3

labeled dataa lot of unlabeled datawhy ssl?

how can unlabeled data be helpful?

4

why ssl?

how can unlabeled data be helpful?

without unlabeled data

4

why ssl?

how can unlabeled data be helpful?

labeled 
instances

without unlabeled data

4

why ssl?

how can unlabeled data be helpful?

labeled 
instances

decision
boundary

without unlabeled data

4

why ssl?

how can unlabeled data be helpful?

labeled 
instances

decision
boundary

without unlabeled data

with unlabeled data

4

why ssl?

how can unlabeled data be helpful?

labeled 
instances

decision
boundary

without unlabeled data

with unlabeled data

4

unlabeled
instances

why ssl?

how can unlabeled data be helpful?

labeled 
instances

decision
boundary

more accurate 

decision boundary 
in the presence of 
unlabeled instances

unlabeled
instances

without unlabeled data

with unlabeled data

example from [belkin et al., jmlr 2006]
4

inductive vs transductive

5

inductive vs transductive

supervised
(labeled)

semi-supervised
(labeled + unlabeled)

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

semi-supervised
(labeled + unlabeled)

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

most graph ssl algorithms are non-parametric 

(i.e., # parameters grows with data size)

5

inductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

most graph ssl algorithms are non-parametric 

(i.e., # parameters grows with data size)

5

focus of this tutorialinductive vs transductive

inductive
(generalize to
unseen data)

transductive

(doesn   t generalize to

unseen data)

supervised
(labeled)

id166, 

maximum id178

x

semi-supervised
(labeled + unlabeled)

manifold 

id173

label propagation 
(lp), mad, mp, ...

most graph ssl algorithms are non-parametric 

(i.e., # parameters grows with data size)

see chapter 25 of ssl book: http://olivier.chapelle.cc/ssl-book/discussion.pdf

5

focus of this tutorialwhy graph-based ssl?

6

why graph-based ssl?

    some datasets are naturally represented by a graph
    web, citation network, social network, ...

6

why graph-based ssl?

    some datasets are naturally represented by a graph
    web, citation network, social network, ...
    uniform representation for heterogeneous data

6

why graph-based ssl?

    some datasets are naturally represented by a graph
    web, citation network, social network, ...
    uniform representation for heterogeneous data
    easily parallelizable, scalable to large data

6

why graph-based ssl?

    some datasets are naturally represented by a graph
    web, citation network, social network, ...
    uniform representation for heterogeneous data
    easily parallelizable, scalable to large data
    effective in practice

6

why graph-based ssl?

    some datasets are naturally represented by a graph
    web, citation network, social network, ...
    uniform representation for heterogeneous data
    easily parallelizable, scalable to large data
    effective in practice

text classi   cation

6

graph sslsupervisednon-graph sslgraph-based ssl

7

graph-based ssl

7

graph-based ssl

similarity

0.8

0.2

0.6

7

graph-based ssl

similarity

0.8

0.2

0.6

   business   

   politics   

7

graph-based ssl

similarity

   business   

   politics   

0.8

0.2

0.6

   business   

   politics   

7

graph-based ssl

8

graph-based ssl

smoothness assumption 

if two instances are similar 
according to the graph, then 
output labels should be similar

8

graph-based ssl

smoothness assumption 

if two instances are similar 
according to the graph, then 
output labels should be similar

8

graph-based ssl

smoothness assumption 

if two instances are similar 
according to the graph, then 
output labels should be similar

    two stages
    graph construction (if not already present)
    label id136

8

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

9

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

10

graph construction

    neighborhood methods
    id92 graph construction (id92g)
    e-neighborhood method
    metric learning
    other approaches

11

neighborhood methods

12

neighborhood methods

    k-nearest neighbor graph (id92g)

    add edges between an instance and its       

k-nearest neighbors

12

neighborhood methods

    k-nearest neighbor graph (id92g)

    add edges between an instance and its       

k-nearest neighbors

k = 3

12

neighborhood methods

    k-nearest neighbor graph (id92g)

    add edges between an instance and its       

k-nearest neighbors

k = 3

    e-neighborhood
    add edges to all instances inside a ball of 

radius e

12

neighborhood methods

    k-nearest neighbor graph (id92g)

    add edges between an instance and its       

k-nearest neighbors

k = 3

    e-neighborhood
    add edges to all instances inside a ball of 

radius e

e

12

issues with id92g

13

issues with id92g

    not scalable (quadratic)

13

issues with id92g

    not scalable (quadratic)
    results in an asymmetric graph

13

issues with id92g

    not scalable (quadratic)
    results in an asymmetric graph
    b is the closest neighbor of a, but not 

the other way

a

b

c

13

issues with id92g

a

b

c

    not scalable (quadratic)
    results in an asymmetric graph
    b is the closest neighbor of a, but not 

the other way

    results in irregular graphs
    some nodes may end up with 

higher degree than other nodes

13

issues with id92g

a

b

c

    not scalable (quadratic)
    results in an asymmetric graph
    b is the closest neighbor of a, but not 

the other way

    results in irregular graphs
    some nodes may end up with 

higher degree than other nodes

13

node of degree 4 inthe id92g (k = 1)issues with e-neighborhood

14

issues with e-neighborhood

    not scalable

14

issues with e-neighborhood

    not scalable
    sensitive to value of e : not invariant to scaling 

14

issues with e-neighborhood

    not scalable
    sensitive to value of e : not invariant to scaling 
    fragmented graph: disconnected components

14

issues with e-neighborhood

    not scalable
    sensitive to value of e : not invariant to scaling 
    fragmented graph: disconnected components

data

e-neighborhood

disconnected

figure from [jebara et al., icml 2009]

14

graph construction using 

metric learning

15

graph construction using 

metric learning

xi

wij     exp(   da(xi, xj))

xj

15

graph construction using 

metric learning

xi

wij     exp(   da(xi, xj))

xj

da(xi, xj) = (xi     xj)t a(xi     xj)

15

estimated using mahalanobis metric learning algorithmsgraph construction using 

metric learning

xi

wij     exp(   da(xi, xj))

xj

da(xi, xj) = (xi     xj)t a(xi     xj)

    supervised metric learning
    itml [kulis et al., icml 2007]
    lmnn [weinberger and saul, jmlr 2009]
    semi-supervised metric learning
    idml [dhillon et al., upenn tr 2010]

15

estimated using mahalanobis metric learning algorithmsbene   ts of metric learning for

graph construction

16

bene   ts of metric learning for

graph construction

0.5

0.375

0.25

0.125

0

r
o
r
r
e

original

rp

pca

itml

idml

newsgroups

amazon
100 seed and1400 test instances, all id136s using lp

enron a

reuters

text

16

bene   ts of metric learning for

graph construction

0.5

0.375

0.25

0.125

0

r
o
r
r
e

original

rp

pca

itml

idml

newsgroups

amazon
100 seed and1400 test instances, all id136s using lp

enron a

reuters

text

16

graph constructed using supervised metric learningbene   ts of metric learning for

graph construction

0.5

0.375

0.25

0.125

0

r
o
r
r
e

original

rp

pca

itml

idml

newsgroups

amazon
100 seed and1400 test instances, all id136s using lp

enron a

reuters

text

16

[dhillon et al., upenn tr 2010]

graph constructed using supervised metric learninggraph constructed using semi-supervised metric learning[dhillon et al., 2010]bene   ts of metric learning for

graph construction

0.5

0.375

0.25

0.125

0

r
o
r
r
e

original

rp

pca

itml

idml

newsgroups

amazon
100 seed and1400 test instances, all id136s using lp

enron a

reuters

text

careful graph construction is critical!

16

[dhillon et al., upenn tr 2010]

graph constructed using supervised metric learninggraph constructed using semi-supervised metric learning[dhillon et al., 2010]other graph construction 

approaches

    local reconstruction
    linear neighborhood [wang and zhang, icml 2005]
    regular graph: b-matching [jebara et al., icml 2008]
    fitting graph to vector data [daitch et al., icml 2009]
    graph kernels
    [zhu et al., nips 2005]

17

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- label propagation
- modi   ed adsorption
- measure propagation
- sparse label propagation
- manifold id173
- spectral graph transduction

18

graph laplacian

19

graph laplacian

    laplacian (un-normalized) of a graph:

l = d     w, where dii =   j

wij, dij(   =i) = 0

19

graph laplacian

    laplacian (un-normalized) of a graph:

l = d     w, where dii =   j

wij, dij(   =i) = 0

b

3

2

c

1

a

1

d

19

graph laplacian

    laplacian (un-normalized) of a graph:

l = d     w, where dii =   j

wij, dij(   =i) = 0

c

b

a
d
 3    -1   -2    0
-1     4   -3    0
-2    -3    6   -1
 0     0   -1     1  

a
b
c
d

b

3

2

c

1

a

1

d

19

graph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

1

20

graph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

wij(fi     fj)2

1

20

graph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

wij(fi     fj)2

1

20

measure of non-smoothnessgraph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

wij(fi     fj)2

1

20

measure of non-smoothnessvector of scores for single label on nodesgraph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

wij(fi     fj)2

10

b

1

1

a

3

2

c

5

1

25

d

1

f t = [1 10 5 25]

20

measure of non-smoothnessvector of scores for single label on nodesgraph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

wij(fi     fj)2

10

b

1

1

a

3

2

c

5

1

25

d

1

f t = [1 10 5 25]
f t lf = 588

20

measure of non-smoothnessvector of scores for single label on nodesnot smoothgraph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

c

1

1

3

d

1

b

3

2

1

a

1

f t = [1113]
f t lf = 4

wij(fi     fj)2

10

b

1

1

a

3

2

c

5

1

25

d

1

f t = [1 10 5 25]
f t lf = 588

20

measure of non-smoothnessvector of scores for single label on nodesnot smoothgraph laplacian (contd.)

    l is positive semi-de   nite (assuming non-negative weights)
    smoothness of prediction f over the graph in 

terms of the laplacian:

f t lf =   i,j

c

1

1

3

d

1

b

3

2

1

a

1

f t = [1113]
f t lf = 4

wij(fi     fj)2

10

b

1

1

a

3

2

c

5

1

25

d

1

f t = [1 10 5 25]
f t lf = 588

20

measure of non-smoothnessvector of scores for single label on nodesnot smoothsmoothrelationship between eigenvalues of 

the laplacian and smoothness

lg =   g

gt lg =   gt g

gt lg =   

21

relationship between eigenvalues of 

the laplacian and smoothness

lg =   g

gt lg =   gt g

gt lg =   

21

eigenvalue of leigenvector of lrelationship between eigenvalues of 

the laplacian and smoothness

lg =   g

gt lg =   gt g

gt lg =   

21

eigenvalue of leigenvector of l= 1, as eigenvectors are are orthonormalrelationship between eigenvalues of 

the laplacian and smoothness

lg =   g

gt lg =   gt g

gt lg =   

21

eigenvalue of leigenvector of l= 1, as eigenvectors are are orthonormalmeasure of non-smoothness(previous slide)relationship between eigenvalues of 

the laplacian and smoothness

lg =   g

gt lg =   gt g

gt lg =   

21

eigenvalue of leigenvector of l= 1, as eigenvectors are are orthonormalif an eigenvector is used to classify nodes, then the corresponding eigenvalue gives the measure of non-smoothnessmeasure of non-smoothness(previous slide)spectrum of the graph laplacian

constant within

component

number of
connected

components = 
number of 0 
eigenvalues

higher eigenvalue,
irregular eigenvector,

less smoothness

22

figure from [zhu et al., 2005]

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- label propagation
- modi   ed adsorption
- measure propagation
- sparse label propagation
- manifold id173

23

notations

  yv,l : score of estimated label l on node v 

yv,l : score of seed label l on node v 

v

rv,l : id173 target for label l on node v 

s : seed node indicator (diagonal matrix) 

wuv : weight of edge (u, v) in the graph

24

seed scores

label 

id173
estimated 

scores

lp-zgl [zhu et al., icml 2003]

arg min

  y

m

!

l=1

wuv(   yul       yvl)2

=

m

!

l=1

l l   yl
  y t

such that

yul =   yul,    suu = 1

25

graphlaplacianlp-zgl [zhu et al., icml 2003]

smooth

arg min

  y

m

!

l=1

wuv(   yul       yvl)2

=

m

!

l=1

l l   yl
  y t

such that

yul =   yul,    suu = 1

25

graphlaplacianlp-zgl [zhu et al., icml 2003]

smooth

arg min

  y

m

!

l=1

wuv(   yul       yvl)2

=

m

!

l=1

l l   yl
  y t

such that

yul =   yul,    suu = 1

match seeds 

(hard)

25

graphlaplacianlp-zgl [zhu et al., icml 2003]

smooth

arg min

  y

m

!

l=1

wuv(   yul       yvl)2

=

m

!

l=1

l l   yl
  y t

such that

yul =   yul,    suu = 1

match seeds 

(hard)

    smoothness

     two nodes connected by 
an edge with high weight 
should be assigned similar 
labels

25

graphlaplacianlp-zgl [zhu et al., icml 2003]

smooth

arg min

  y

m

!

l=1

wuv(   yul       yvl)2

=

m

!

l=1

l l   yl
  y t

such that

yul =   yul,    suu = 1

match seeds 

(hard)

    smoothness

     two nodes connected by 
an edge with high weight 
should be assigned similar 
labels

25

    solution satis   es harmonic 

property

graphlaplacianoutline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- label propagation
- modi   ed adsorption
- manifold id173
- spectral graph transduction
- measure propagation

26

two related views

label diffusion

27

ullabeled (seeded) nodeunlabeled nodetwo related views

label diffusion

random walk

27

ullulabeled (seeded) nodeunlabeled noderandom walk view

28

random walk view

what next?

v

starting node

u

28

random walk view

what next?

v

starting node

u

    continue walk with id203

cont
v

p

    assign v   s seed label to u with id203

inj
v

p

    abandon random walk with id203 

abnd
v

p

    assign u a dummy label 

28

discounting nodes

29

discounting nodes

    certain nodes can be unreliable (e.g., high degree nodes)

    do not allow propagation/walk through them

29

discounting nodes

    certain nodes can be unreliable (e.g., high degree nodes)
    solution: increase abandon id203 on such 

    do not allow propagation/walk through them

nodes:

29

discounting nodes

    certain nodes can be unreliable (e.g., high degree nodes)
    solution: increase abandon id203 on such 

    do not allow propagation/walk through them

nodes:

abnd
v

p

    degree(v)

29

rede   ning matrices

new	
   edge
weight

   wuv

!

w

u

uv = pcont
suu = !pinj
ru! = pabnd

u

u

, and 0 for non-dummy labels

dummy	
   label

30

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

31

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

m+1   l=1       s   y l     sy l   2 +   1   u,v

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

!

    m labels, +1 dummy label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

match seeds (soft)

m+1   l=1       s   y l     sy l   2 +   1   u,v

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

!

    m labels, +1 dummy label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

match seeds (soft)

m+1   l=1       s   y l     sy l   2 +   1   u,v

smooth

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

!

    m labels, +1 dummy label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

match seeds (soft)

m+1   l=1       s   y l     sy l   2 +   1   u,v

match priors
(regularizer)

smooth

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

!

    m labels, +1 dummy label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

match seeds (soft)

m+1   l=1       s   y l     sy l   2 +   1   u,v

match priors
(regularizer)

smooth

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

    m labels, +1 dummy label
!for none-of-the-above label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

match seeds (soft)

m+1   l=1       s   y l     sy l   2 +   1   u,v

match priors
(regularizer)

smooth

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

    m labels, +1 dummy label
!for none-of-the-above label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

mad has extra id173 compared to lp-zgl [zhu et al, icml 03]; similar to qc [bengio et al, 2006]modi   ed adsorption (mad)

[talukdar and crammer, ecml 2009]

arg min

  y

match seeds (soft)

m+1   l=1       s   y l     sy l   2 +   1   u,v

match priors
(regularizer)

smooth

m uv(   y ul       y vl)2 +   2      y l     rl   2   

!

    m labels, +1 dummy label
!for none-of-the-above label
    m = w     + w is the symmetrized weight matrix
      y vl: weight of label l on node v
v
    y vl: seed weight for label l on node v
    s: diagonal matrix, nonzero for seed nodes
    rvl: id173 target for label l on node v

31

seed scores
label priors
estimated
scores

mad has extra id173 compared to lp-zgl [zhu et al, icml 03]; similar to qc [bengio et al, 2006]mad   s objective is convexsolving mad objective

32

solving mad objective

    can be solved using matrix inversion (like in lp)
    but matrix inversion is expensive

32

solving mad objective

    can be solved using matrix inversion (like in lp)
    but matrix inversion is expensive
    instead solved exactly using a system of linear 

equations (ax = b)
    solved using jacobi iterations
    results in iterative updates
    guaranteed convergence
    see [bengio et al., 2006] and                                          

[talukdar and crammer, ecml 2009] for details

32

solving mad using iterative updates

!

!

inputs y , r : |v |   (|l| + 1), w : |v |  | v |, s : |v |  | v | diagonal
  y     y
m = w + w    
zv     svv +   1   u   =v m vu +   2    v     v
  y +   2rv   

zv   (sy )v +   1m v  

for all v     v do
  y v     1
end for

repeat

0.60

a

until convergence

b

0.75

seed

prior

0.05

v

c

33

current label estimate on bsolving mad using iterative updates

!

!

inputs y , r : |v |   (|l| + 1), w : |v |  | v |, s : |v |  | v | diagonal
  y     y
m = w + w    
zv     svv +   1   u   =v m vu +   2    v     v
  y +   2rv   

zv   (sy )v +   1m v  

for all v     v do
  y v     1
end for

repeat

0.60

a

until convergence

b

0.75

seed

prior

0.05

v

c

33

new label estimate on vsolving mad using iterative updates

!

!

inputs y , r : |v |   (|l| + 1), w : |v |  | v |, s : |v |  | v | diagonal
  y     y
m = w + w    
zv     svv +   1   u   =v m vu +   2    v     v
  y +   2rv   

zv   (sy )v +   1m v  

for all v     v do
  y v     1
end for

repeat

0.60

a

until convergence

b

0.75

v

seed

prior

    importance of a node can be discounted
    easily parallelizable: scalable (more later)

0.05

c

33

new label estimate on vwhen is mad most effective?

34

when is mad most effective?

34

00.10.20.30.407.51522.530relative increase in mrr by mad over lp-zglaverage degreewhen is mad most effective?

mad is particularly effective in denser graphs, where 

there is greater need for id173.

34

00.10.20.30.407.51522.530relative increase in mrr by mad over lp-zglaverage degreeextension to dependent labels

35

extension to dependent labels

labels are not always mutually exclusive

35

extension to dependent labels

labels are not always mutually exclusive

1.0

1.0

label similarity in sentiment classi   cation

35

extension to dependent labels

labels are not always mutually exclusive

1.0

1.0

label similarity in sentiment classi   cation

modi   ed adsorption with dependent labels 

(maddl) [talukdar and crammer, ecml 2009]

35

extension to dependent labels

labels are not always mutually exclusive

1.0

1.0

label similarity in sentiment classi   cation

modi   ed adsorption with dependent labels 

(maddl) [talukdar and crammer, ecml 2009]
    can take label similarities into account

35

extension to dependent labels

labels are not always mutually exclusive

1.0

1.0

label similarity in sentiment classi   cation

modi   ed adsorption with dependent labels 

(maddl) [talukdar and crammer, ecml 2009]
    can take label similarities into account
    convex objective

35

extension to dependent labels

labels are not always mutually exclusive

1.0

1.0

label similarity in sentiment classi   cation

modi   ed adsorption with dependent labels 

(maddl) [talukdar and crammer, ecml 2009]
    can take label similarities into account
    convex objective
    ef   cient iterative/parallelizable updates as in mad

35

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- label propagation
- modi   ed adsorption
- measure propagation
- sparse label propagation
- manifold id173

36

measure propagation (mp)

[subramanya and bilmes, emnlp 2008, nips 2009, jmlr 2011]

ckl

arg min
{pi}

l   i=1

dkl(ri||pi) +      i,j

s.t.    y

wijdkl(pi||pj)       

pi(y) = 1, pi(y)     0,    y, i

h(pi)

n   i=1

37

measure propagation (mp)

[subramanya and bilmes, emnlp 2008, nips 2009, jmlr 2011]

ckl

arg min
{pi}

l   i=1

divergence on
seed nodes

dkl(ri||pi) +      i,j

wijdkl(pi||pj)       

pi(y) = 1, pi(y)     0,    y, i

s.t.    y

h(pi)

n   i=1

37

seed and estimated label distributions (normalized) on node imeasure propagation (mp)

[subramanya and bilmes, emnlp 2008, nips 2009, jmlr 2011]

ckl

arg min
{pi}

l   i=1

divergence on
seed nodes

dkl(ri||pi) +      i,j

smoothness

(divergence across edge)

wijdkl(pi||pj)       

h(pi)

n   i=1

s.t.    y

pi(y) = 1, pi(y)     0,    y, i

dkl(pi||pj) =   y

pi(y) log

pi(y)
pj(y)

37

kl divergenceseed and estimated label distributions (normalized) on node imeasure propagation (mp)

[subramanya and bilmes, emnlp 2008, nips 2009, jmlr 2011]

smoothness

(divergence across edge)

entropic regularizer

ckl

arg min
{pi}

l   i=1

divergence on
seed nodes

dkl(ri||pi) +      i,j

wijdkl(pi||pj)       

pi(y) = 1, pi(y)     0,    y, i

s.t.    y

h(pi)

n   i=1

dkl(pi||pj) =   y

pi(y) log

pi(y)
pj(y)

h(pi) =       y

pi(y) log pi(y)

37

kl divergenceid178seed and estimated label distributions (normalized) on node imeasure propagation (mp)

[subramanya and bilmes, emnlp 2008, nips 2009, jmlr 2011]

smoothness

(divergence across edge)

entropic regularizer

ckl

arg min
{pi}

l   i=1

divergence on
seed nodes

dkl(ri||pi) +      i,j

wijdkl(pi||pj)       

pi(y) = 1, pi(y)     0,    y, i

s.t.    y

h(pi)

n   i=1

dkl(pi||pj) =   y

pi(y) log

pi(y)
pj(y)

h(pi) =       y

pi(y) log pi(y)

37

kl divergenceid178seed and estimated label distributions (normalized) on node iid172 constraintmeasure propagation (mp)

[subramanya and bilmes, emnlp 2008, nips 2009, jmlr 2011]

smoothness

(divergence across edge)

entropic regularizer

ckl

arg min
{pi}

l   i=1

divergence on
seed nodes

dkl(ri||pi) +      i,j

wijdkl(pi||pj)       

pi(y) = 1, pi(y)     0,    y, i

s.t.    y

h(pi)

n   i=1

dkl(pi||pj) =   y

pi(y) log

pi(y)
pj(y)

h(pi) =       y

pi(y) log pi(y)

ckl is convex (with non-negative edge weights and hyper-parameters)

mp is related to information id173 [corduneanu and jaakkola, 2003]

37

kl divergenceid178seed and estimated label distributions (normalized) on node iid172 constraintsolving mp objective

    for ease of optimization, reformulate mp objective:
cmp
n   i=1

dkl(ri||qi) +      i,j

w   
ijdkl(pi||qj)       

arg min
{pi,qi}

l   i=1

h(pi)

38

solving mp objective

    for ease of optimization, reformulate mp objective:
cmp
n   i=1

dkl(ri||qi) +      i,j

w   
ijdkl(pi||qj)       

arg min
{pi,qi}

l   i=1

h(pi)

38

new id203 measure, one for each vertex, similar to pisolving mp objective

    for ease of optimization, reformulate mp objective:
cmp
n   i=1

dkl(ri||qi) +      i,j

w   
ijdkl(pi||qj)       

arg min
{pi,qi}

l   i=1

h(pi)

w   
ij = wij +         (i, j)

38

   new id203 measure, one for each vertex, similar to pisolving mp objective

    for ease of optimization, reformulate mp objective:
cmp
n   i=1

dkl(ri||qi) +      i,j

w   
ijdkl(pi||qj)       

arg min
{pi,qi}

l   i=1

h(pi)

w   
ij = wij +         (i, j)

38

   new id203 measure, one for each vertex, similar to piencourages agreement between pi and qi    solving mp objective

    for ease of optimization, reformulate mp objective:
cmp
n   i=1

dkl(ri||qi) +      i,j

w   
ijdkl(pi||qj)       

arg min
{pi,qi}

l   i=1

h(pi)

w   
ij = wij +         (i, j)

cmp is also convex

(with non-negative edge weights and hyper-parameters)  

38

   new id203 measure, one for each vertex, similar to piencourages agreement between pi and qi    solving mp objective

    for ease of optimization, reformulate mp objective:
cmp
n   i=1

dkl(ri||qi) +      i,j

w   
ijdkl(pi||qj)       

arg min
{pi,qi}

l   i=1

h(pi)

w   
ij = wij +         (i, j)

cmp is also convex

(with non-negative edge weights and hyper-parameters)  

cmp can be solved using alternating minimization (am)

38

   new id203 measure, one for each vertex, similar to piencourages agreement between pi and qi    alternating minimization

39

alternating minimization

q0

39

alternating minimization

p1

q0

39

alternating minimization

p1

q1

q0

39

alternating minimization

p2

p1

q1

q0

39

alternating minimization

p2

p1

q2

q1

q0

39

alternating minimization

p3

p2

p1

q2

q1

q0

39

alternating minimization

p3

p2

p1

q2

q1

q0

39

alternating minimization

p3

p2

p1

q2

q1

q0

39

alternating minimization

p3

p2

p1

q2

q1

cmp satis   es the necessary conditions for am to 

converge [subramanya and bilmes, jmlr 2011]

q0

39

why am?

40

why am?

40

why am?

40

performance of ssl algorithms

comparison of accuracies for different number of labeled

samples across coil (6 classes) and opt (10 classes) datasets

41

performance of ssl algorithms

comparison of accuracies for different number of labeled

samples across coil (6 classes) and opt (10 classes) datasets

41

performance of ssl algorithms

comparison of accuracies for different number of labeled

samples across coil (6 classes) and opt (10 classes) datasets

graph ssl can be effective when the data satis   es manifold 
assumption. more results and discussion in chapter 21 of 

the ssl book (chapelle et al.)

41

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- label propagation
- modi   ed adsorption
- measure propagation
- sparse label propagation
- manifold id173

42

background: factor graphs

[kschischang et al., 2001]

factor graph
    bipartite graph
    variable nodes (e.g., label distribution on a node)
    factor nodes:    tness function over variable assignment

distribution over all variables    values

43

variable nodes (v)factor nodes (f)variables connected to factor ffactor graph interpretation of 

graph ssl [zhu et al., icml 2003] [das and smith, naacl 2012]

min

+

+

44

edge smoothness lossid173 loss seed matching loss (if any)3-term graph ssl objective (common to many algorithms)factor graph interpretation of 

graph ssl [zhu et al., icml 2003] [das and smith, naacl 2012]

+

q2

min

+

w1,2 ||q1     q2||2

q1

44

edge smoothness lossid173 loss seed matching loss (if any)3-term graph ssl objective (common to many algorithms)factor graph interpretation of 

graph ssl [zhu et al., icml 2003] [das and smith, naacl 2012]

min

+

q1

q1

+

q2

q2

w1,2 ||q1     q2||2

  (q1, q2)

44

edge smoothness lossid173 loss seed matching loss (if any)3-term graph ssl objective (common to many algorithms)factor graph interpretation of 

graph ssl [zhu et al., icml 2003] [das and smith, naacl 2012]

min

+

q1

q1

w1,2 ||q1     q2||2

  (q1, q2)

+

q2

q2

  (q1, q2)     w1,2 ||q1     q2||2

44

edge smoothness lossid173 loss seed matching loss (if any)3-term graph ssl objective (common to many algorithms)smoothness factorfactor graph interpretation of 

graph ssl [zhu et al., icml 2003] [das and smith, naacl 2012]

min

+

q1

q1

w1,2 ||q1     q2||2

  (q1, q2)

+

q2

q2

  (q1, q2)     w1,2 ||q1     q2||2

44

edge smoothness lossid173 loss seed matching loss (if any)3-term graph ssl objective (common to many algorithms)smoothness factorseed matchingfactor (unary)factor graph interpretation of 

graph ssl [zhu et al., icml 2003] [das and smith, naacl 2012]

min

+

q1

q1

w1,2 ||q1     q2||2

  (q1, q2)

+

q2

q2

  (q1, q2)     w1,2 ||q1     q2||2

44

edge smoothness lossid173 loss seed matching loss (if any)3-term graph ssl objective (common to many algorithms)id173 factor (unary)smoothness factorseed matchingfactor (unary)factor graph interpretation

[zhu et al., icml 2003][das and smith, naacl 2012]

q9264 

q9265 

q9266 

r1 

q1 

r2 

q2 

log   t(qt)

r3 

q3 

q4 

r4 

q9267 

q9268 

q9269 

q9270 

45

1. factor encouraging agreement on seed labels2. smoothnessfactor3. unary factor for id173label propagation with sparsity

46

label propagation with sparsity

enforce through sparsity inducing unary factor

46

label propagation with sparsity

enforce through sparsity inducing unary factor

lasso (tibshirani, 1996) 

log   t(qt) =

elitist lasso (kowalski and torr  sani, 2009)

log   t(qt) =

46

label propagation with sparsity

enforce through sparsity inducing unary factor

lasso (tibshirani, 1996) 

log   t(qt) =

elitist lasso (kowalski and torr  sani, 2009)

log   t(qt) =

46

for more details, see [das and smith, naacl 2012]outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- label propagation
- modi   ed adsorption
- measure propagation
- sparse label propagation
- manifold id173

47

manifold id173

[belkin et al., jmlr 2006]

f    = arg min

f

1
l

l   i=1

v (yi, f (xi)) +   f t lf +   ||f||2

k

48

manifold id173

[belkin et al., jmlr 2006]

f    = arg min

f

1
l

l   i=1

training data

loss

v (yi, f (xi)) +   f t lf +   ||f||2

k

48

id168(e.g., soft margin)manifold id173

[belkin et al., jmlr 2006]

f    = arg min

f

1
l

l   i=1

training data

loss

smoothness 
regularizer

v (yi, f (xi)) +   f t lf +   ||f||2

k

48

id168(e.g., soft margin)laplacian of graph  over labeled and unlabeled datamanifold id173

[belkin et al., jmlr 2006]

f    = arg min

f

1
l

l   i=1

training data

loss

smoothness 
regularizer

regularizer
(e.g., l2)

v (yi, f (xi)) +   f t lf +   ||f||2

k

48

id168(e.g., soft margin)laplacian of graph  over labeled and unlabeled datamanifold id173

[belkin et al., jmlr 2006]

f    = arg min

f

1
l

l   i=1

training data

loss

smoothness 
regularizer

regularizer
(e.g., l2)

v (yi, f (xi)) +   f t lf +   ||f||2

k

trains an inductive classi   er which can generalize 

to unseen instances 

48

id168(e.g., soft margin)laplacian of graph  over labeled and unlabeled dataother graph-based ssl methods
    ssl on directed graphs
    [zhou et al, nips 2005], [zhou et al., icml 2005]
    learning with dissimilarity edges
    [goldberg et al., aistats 2007]
    spectral graph transduction [joachims, icml 2003]
    graph transduction using alternating minimization
    [wang et al., icml 2008]
    graph as regularizer for multi-layered id88
    [karlen et al., icml 2008], [malkin et al., interspeech 2009]

49

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- scalability issues
- node reordering
- mapreduce parallelization

50

more (unlabeled) data is better data

51

[subramanya & bilmes, jmlr 2011]

more (unlabeled) data is better data

51

[subramanya & bilmes, jmlr 2011]

graph with 120m verticesmore (unlabeled) data is better data

 challenges with large unlabeled data:

    constructing graph from large data
    scalable id136 over large graphs

51

[subramanya & bilmes, jmlr 2011]

graph with 120m verticesoutline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- scalability issues
- node reordering
- mapreduce parallelization

52

scalability issues (i)

graph construction

53

scalability issues (i)

graph construction

    brute force (exact) id92g too expensive 

(quadratic)

53

scalability issues (i)

graph construction

    brute force (exact) id92g too expensive 

(quadratic)

    approximate nearest neighbor using kd-
tree [friedman et al., 1977, also see http://
www.cs.umd.edu/  mount/]

53

scalability issues (ii)

label id136

    sub-sample the data

data [delalleau et al., aistats 2005]

    construct graph over a subset of a unlabeled 
    sparse grids [garcke & griebel, kdd 2001]

54

scalability issues (ii)

label id136

    sub-sample the data

data [delalleau et al., aistats 2005]

    construct graph over a subset of a unlabeled 
    sparse grids [garcke & griebel, kdd 2001]

    how about using more computation? (next section)

    symmetric multi-processor (smp)
    distributed computer

54

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- scalability issues
- node reordering
      [subramanya & bilmes, jmlr 2011;
       bilmes & subramanya, 2011]
- mapreduce parallelization

55

parallel computation on a smp

1

2

.....

k

k+1

.....

smp with k 
processors

graph nodes (neighbors not shown)parallel computation on a smp

processor 1

smp with k 
processors

1

2

.....

k

k+1

.....

graph nodes (neighbors not shown)parallel computation on a smp

processor 1

processor 2

smp with k 
processors

1

2

.....

k

k+1

.....

graph nodes (neighbors not shown)parallel computation on a smp

processor 1
processor 1

processor 2

processor k

smp with k 
processors

1

2

.....

k

k+1

.....

graph nodes (neighbors not shown)parallel computation on a smp

processor 1

processor 2

processor k

1

2

.....

k

processor 1

k+1

.....

smp with k 
processors

graph nodes (neighbors not shown)label update using message passing

processor 1

processor 2

processor k

processor 1

smp with k 
processors

1

2

.....

k

k+1

.....

a

b

0.60

0.75

v

seed

prior

0.05

c

57

current label estimate on agraph nodes (neighbors not shown)label update using message passing

processor 1

processor 2

processor k

processor 1

smp with k 
processors

1

2

.....

k

k+1

.....

a

b

0.60

0.75

v

seed

prior

0.05

c

57

new label estimate on v   graph nodes (neighbors not shown)speed-up on smp

    graph with 1.4m nodes
    smp with 16 cores and 
128gb of ram

58

[subramanya & bilmes, jmlr, 2011]

speed-up on smp

    graph with 1.4m nodes
    smp with 16 cores and 
128gb of ram

58

[subramanya & bilmes, jmlr, 2011]

ratio of time spent when using 1 processor to time spent using n processorsspeed-up on smp

    graph with 1.4m nodes
    smp with 16 cores and 
128gb of ram

58

[subramanya & bilmes, jmlr, 2011]

ratio of time spent when using 1 processor to time spent using n processorscache miss?node reordering algorithm

input: graph g = (v, e)
result: node ordered graph
1. select an arbitrary node v
2. while unselected nodes remain do

2.1. select an unselected node v` from among the 

neighbors    neighbors of v that has maximum 
overlap with v` neighbors

2.2. mark v` as selected
2.3. set v to v`

59

[subramanya & bilmes, jmlr, 2011]

node reordering algorithm

input: graph g = (v, e)
result: node ordered graph
1. select an arbitrary node v
2. while unselected nodes remain do

2.1. select an unselected node v` from among the 

neighbors    neighbors of v that has maximum 
overlap with v` neighbors

2.2. mark v` as selected
2.3. set v to v`

59

[subramanya & bilmes, jmlr, 2011]

exhaustive for sparse (e.g., id92) graphsnode reordering algorithm : intuition 

k

a

c

b

60

node reordering algorithm : intuition 

k

a

c

b

 which node should 
be placed after k to 

optimize cache 
performance?

60

node reordering algorithm : intuition 

a

e

c

j

k

a

c

b

 which node should 
be placed after k to 

optimize cache 
performance?

60

node reordering algorithm : intuition 

a

b

c

e

c
a

c

l

n

j

d

m

k

a

c

b

 which node should 
be placed after k to 

optimize cache 
performance?

60

node reordering algorithm : intuition 

|n (k)     n (a)| = 1

k

a

c

b

 which node should 
be placed after k to 

optimize cache 
performance?

60

a

b

c

e

c
a

c

l

n

j

d

m

node reordering algorithm : intuition 

|n (k)     n (a)| = 1

k

a

c

b

 which node should 
be placed after k to 

optimize cache 
performance?

60

a

b

c

e

c
a

c

l

n

j

d

m

cardinality of intersectionnode reordering algorithm : intuition 

k

a

c

|n (k)     n (a)| = 1

b

|n (k)     n (b)| = 2

|n (k)     n (c)| = 0

 which node should 
be placed after k to 

optimize cache 
performance?

60

a

b

c

e

c
a

c

l

n

j

d

m

cardinality of intersectionnode reordering algorithm : intuition 

k

a

c

|n (k)     n (a)| = 1

b

|n (k)     n (b)| = 2

|n (k)     n (c)| = 0

 which node should 
be placed after k to 

optimize cache 
performance?

60

a

b

c

e

c
a

c

l

n

j

d

m

best nodecardinality of intersectionspeed-up on smp after node ordering

61

[subramanya & bilmes, jmlr, 2011]

distributed processing

within the same machine

    maximize overlap between consecutive nodes 
    minimize overlap across machines (reduce inter 

machine communication)

62

distributed processing

63

[bilmes & subramanya, 2011]

distributed processing

63

[bilmes & subramanya, 2011]

maximize intra-processor overlapdistributed processing

63

[bilmes & subramanya, 2011]

maximize intra-processor overlapminimizeinter-processor overlapnode reordering for distributed computer

processor #i

processor #j

......

......

......

......

64

node reordering for distributed computer

processor #i

processor #j

......

......

......

......

64

minimize overlapnode reordering for distributed computer

processor #i

processor #j

......

......

......

......

64

minimize overlapmaximize overlapdistributed processing results

65

[bilmes & subramanya, 2011]

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- scalability issues
- node reordering
- mapreduce parallelization

66

mapreduce implementation of mad

a

b

0.60

0.75

seed

prior

0.05

v

c

67

current label estimate on bmapreduce implementation of mad
    map
    each node send its current 

b

a

0.60

0.75

label assignments to its 
neighbors

seed

prior

0.05

v

c

67

current label estimate on bmapreduce implementation of mad
    map
    each node send its current 

b

a

0.60

0.75

label assignments to its 
neighbors

seed

prior

0.05

v

c

67

mapreduce implementation of mad
    map
    each node send its current 

b

a

0.60

0.75

label assignments to its 
neighbors

    reduce
    each node updates its own label 
assignment using messages 
received from neighbors, and its 
own information (e.g., seed 
labels, reg. penalties etc.)

    repeat until convergence

67

reduce

v

seed

prior

0.05

c

mapreduce implementation of mad
    map
    each node send its current 

b

a

0.60

0.75

label assignments to its 
neighbors

    reduce
    each node updates its own label 
assignment using messages 
received from neighbors, and its 
own information (e.g., seed 
labels, reg. penalties etc.)

    repeat until convergence

67

seed

prior

0.05

v

c

new label estimate on vmapreduce implementation of mad
    map
    each node send its current 

b

a

0.60

0.75

label assignments to its 
neighbors

v

seed

prior

    reduce
    each node updates its own label 
assignment using messages 
received from neighbors, and its 
own information (e.g., seed 
labels, reg. penalties etc.)

    repeat until convergence

code in junto label propagation toolkit
0.05
(includes hadoop-based implementation)

c
http://code.google.com/p/junto/

67

new label estimate on vmapreduce implementation of mad
    map
    each node send its current 

b

a

0.60

0.75

label assignments to its 
neighbors

v
amenable to distributed processing

graph-based algorithms are 
seed

prior

    reduce
    each node updates its own label 
assignment using messages 
received from neighbors, and its 
own information (e.g., seed 
labels, reg. penalties etc.)

    repeat until convergence

code in junto label propagation toolkit
0.05
(includes hadoop-based implementation)

c
http://code.google.com/p/junto/

67

new label estimate on voutline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
- multilingual id52
- id29

68

graph-ssl : how is it used?

graph ssl

use case 1: transductive classification

labeled dataunlabeled datalabeled unlabeled datagraph-ssl : how is it used?

graph ssl

use case 1: transductive classification

69

labeled dataunlabeled datalabeled unlabeled datagraph-ssl : how is it used?

graph ssl

use case 1: transductive classification

graph ssl

inductive learning

algorithm

use case 2: training better inductive model

69

labeled dataunlabeled datalabeled unlabeled datalabeled dataunlabeled datalarge resourcelabeled dataoutline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
- multilingual id52
- id29

70

problem description & motivation

71

problem description & motivation

    given a document (e.g., web page, news 
article), assign it to a    xed number of 
semantic categories (e.g., sports, politics, 
entertainment)

71

problem description & motivation

    given a document (e.g., web page, news 
article), assign it to a    xed number of 
semantic categories (e.g., sports, politics, 
entertainment)
    multi-label problem

71

problem description & motivation

    given a document (e.g., web page, news 
article), assign it to a    xed number of 
semantic categories (e.g., sports, politics, 
entertainment)
    multi-label problem
    training supervised models requires large 
amounts of labeled data [dumais et al., 1998]

71

corpora

    reuters [lewis, et al., 1978]
    newswire
    about 20k document with 135 categories. use 
top 10 categories (e.g.,    earnings   ,    acquistions   , 
   wheat   ,    interest   ) and label the remaining as 
   other   

72

corpora

    reuters [lewis, et al., 1978]
    newswire
    about 20k document with 135 categories. use 
top 10 categories (e.g.,    earnings   ,    acquistions   , 
   wheat   ,    interest   ) and label the remaining as 
   other   

    webkb [bekkerman, et al., 2003]
    8k webpages from 4 academic domains
    categories include    course   ,    department   , 

   faculty    and    project   

72

feature extraction

document
[lewis, et al., 1978]

73

showers continued throughout the week inthe bahia cocoa zone, alleviating the drought since early january and improving prospects for the coming temporao, ...feature extraction

document
[lewis, et al., 1978]

stop-word 
removal

73

showers continued throughout the week inthe bahia cocoa zone, alleviating the drought since early january and improving prospects for the coming temporao, ...showers continued week bahia cocoa zone alleviating drought early january improving prospects coming temporao, ...feature extraction

document
[lewis, et al., 1978]

stop-word 
removal

id30

73

showers continued throughout the week inthe bahia cocoa zone, alleviating the drought since early january and improving prospects for the coming temporao, ...showers continued week bahia cocoa zone alleviating drought early january improving prospects coming temporao, ...shower continu week bahia cocoa zone allevi drought earli januari improv prospect come temporao, ...feature extraction

shower
bahia
cocoa

.
.
.
.

0.01
0.34
0.33

.
.
.
.

document
[lewis, et al., 1978]

stop-word 
removal

id30

tfidf weighted 
bag-of-words

73

showers continued throughout the week inthe bahia cocoa zone, alleviating the drought since early january and improving prospects for the coming temporao, ...showers continued week bahia cocoa zone alleviating drought early january improving prospects coming temporao, ...shower continu week bahia cocoa zone allevi drought earli januari improv prospect come temporao, ...results

average 
prbep

id166 tid166 sgt

lp

mp

mad

reuters 48.9

59.3

60.3

59.7

66.3

-

webkb 23.0

29.2

36.8

41.2

51.9

53.7

precision-recall break even point (prbep)

74

results

average 
prbep

id166 tid166 sgt

lp

mp

mad

reuters 48.9

59.3

60.3

59.7

66.3

-

webkb 23.0

29.2

36.8

41.2

51.9

53.7

precision-recall break even point (prbep)

74

support vector machine (supervised)results

average 
prbep

id166 tid166 sgt

lp

mp

mad

reuters 48.9

59.3

60.3

59.7

66.3

-

webkb 23.0

29.2

36.8

41.2

51.9

53.7

precision-recall break even point (prbep)

74

support vector machine (supervised)transductive id166 [joachims 1999]results

average 
prbep

id166 tid166 sgt

lp

mp

mad

reuters 48.9

59.3

60.3

59.7

66.3

-

webkb 23.0

29.2

36.8

41.2

51.9

53.7

precision-recall break even point (prbep)

74

support vector machine (supervised)transductive id166 [joachims 1999]spectral graph transduction (sgt) [joachims 2003]label propagation [zhu & ghahramani 2002]measure propagation [subramanya & bilmes 2008]modi   ed adsorption[talukdar & crammer 2009]results on webkb

75

[subramanya & bilmes, emnlp 2008]

results on webkb

75

[subramanya & bilmes, emnlp 2008]

modi   ed adsorption (mad) [talukdar & crammer 2009]results on webkb

    more labeled data => better performance
    unnormalized distributions (scores) more 
suitable for multi-label problems (mad 
outperforms other approaches)

75

[subramanya & bilmes, emnlp 2008]

modi   ed adsorption (mad) [talukdar & crammer 2009]big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

76

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

76

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
- multilingual id52
- id29

77

problem description

78

problem description

    given a document either
    classify it as expressing a positive or 
negative sentiment or
    assign a star rating

78

problem description

    given a document either
    classify it as expressing a positive or 
negative sentiment or
    assign a star rating
    similar to text categorization
    can be solved using standard machine 
learning approaches [pang, lee & vaidyanathan, emnlp 2002]

78

problem description

movie review dataset [pang et al. emnlp 2002] 

79

   fortunately, they managed to do it in an interesting and funny way.   he is one of the most exciting martial artists on the big screen.   the romance was enchanting.problem description

movie review dataset [pang et al. emnlp 2002] 

79

   fortunately, they managed to do it in an interesting and funny way.   he is one of the most exciting martial artists on the big screen.   the romance was enchanting.   a woman in peril.  a confrontation.  an explosion. the end. yawn. yawn. yawn.   don   t go see this moviepolarity lexicons (i)

80

polarity lexicons (i)

    large lists of phrases that encode the polarity 
(positive or negative) of each phrase

80

polarity lexicons (i)

    large lists of phrases that encode the polarity 
(positive or negative) of each phrase
    positive polarity:     enjoyable   ,    breathtakingly   , 
   once in a life time   

80

polarity lexicons (i)

    large lists of phrases that encode the polarity 
(positive or negative) of each phrase
    positive polarity:     enjoyable   ,    breathtakingly   , 
   once in a life time   
    negative polarity:    bad   ,    humorless   , 
   unbearable   ,    out of touch   ,    bumps in the 
road     

80

polarity lexicons (i)

    large lists of phrases that encode the polarity 
(positive or negative) of each phrase
    positive polarity:     enjoyable   ,    breathtakingly   , 
   once in a life time   
    negative polarity:    bad   ,    humorless   , 
   unbearable   ,    out of touch   ,    bumps in the 
road     

    best results obtained by combining with machine 
learning approaches [wilson et al., hlt-emnlp 05; blair-
goldensohn et al. 08; choi & cardie emnlp 09]

80

polarity lexicons (ii)

    common strategy: start with two small seed sets 
    p: positive phrases, e.g.,    great       fantastic   
    n: negative phrases, e.g.,    awful   ,    dreadful   
    grow lexicons with graph propagation algorithms

81

polarity lexicons (ii)

    common strategy: start with two small seed sets 
    p: positive phrases, e.g.,    great       fantastic   
    n: negative phrases, e.g.,    awful   ,    dreadful   
    grow lexicons with graph propagation algorithms

love it

fantastic

excellent

good

great

pretty

nice

dreadful

ugly

irritating

bad

awful

slightly off

painful

...

81

polarity lexicons (ii)

    common strategy: start with two small seed sets 
    p: positive phrases, e.g.,    great       fantastic   
    n: negative phrases, e.g.,    awful   ,    dreadful   
    grow lexicons with graph propagation algorithms

love it

fantastic

excellent

good

great

pretty

nice

dreadful

ugly

irritating

bad

awful

slightly off

painful

...

81

polarity lexicons (ii)

    common strategy: start with two small seed sets 
    p: positive phrases, e.g.,    great       fantastic   
    n: negative phrases, e.g.,    awful   ,    dreadful   
    grow lexicons with graph propagation algorithms

love it

fantastic

excellent

good

great

pretty

nice

dreadful

ugly

irritating

bad

awful

slightly off

painful

...

81

polarity lexicons (ii)

    common strategy: start with two small seed sets 
    p: positive phrases, e.g.,    great       fantastic   
    n: negative phrases, e.g.,    awful   ,    dreadful   
    grow lexicons with graph propagation algorithms

love it

fantastic

excellent

good

great

pretty

nice

dreadful

ugly

irritating

bad

awful

slightly off

painful

...

81

graph construction (i)

goldensohn 08; rao & ravichandran eacl 09]

    id138 [hu & liu, kdd 04; kim & hovy,  iccl 04; blair-
    de   nes synonyms, antonyms, hypernyms, etc.
    make edges between synonyms
    enforce constraints between antonyms
    issues
    coverage
    hard to    nd resources for all languages

82

graph construction (ii)

    use web data!
    all id165s (phrases) up to length 10 from 4 
billion web pages 
    pruned down to 20 million candidate 
phrases
    feature vector obtained by aggregating 
words that occurred in local context

83

[velikovich, et al., naacl 2010]

graph propagation (i)

great

0.5

nice

0.5

good

0.5

0.5

0.5

ok

so-so

84

[zhu & ghahramani 2002]

graph propagation (i)

1.0

great

0.5

nice

0.5

good

0.5

0.5

0.5

ok

-0.1

so-so

84

[zhu & ghahramani 2002]

graph propagation (i)

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.69

84

[zhu & ghahramani 2002]

graph propagation (i)

0.53

1.0

great

0.5

nice

0.5

good

0.5

0.5

0.5

ok

-0.1

so-so

0.84

0.69

84

[zhu & ghahramani 2002]

graph propagation (i)

0.53

1.0

great

0.5

nice

0.5

good

0.5

0.84

0.69

0.5

0.5

-0.1

so-so

0.5

ok

0.5

satisfying

0.5
appealing

0.5

0.5

excellent

84

[zhu & ghahramani 2002]

graph propagation (i)

0.53

1.0

great

0.5

nice

0.5

good

0.5

0.84

0.69

0.5

0.5

-0.1

so-so

0.5

ok

0.5

satisfying

0.5
appealing

0.5

0.5

excellent

1.0

84

[zhu & ghahramani 2002]

graph propagation (i)

0.53

0.53
nice

0.5

0.5

1.0

great

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.69

0.84

0.69

0.5

0.5
appealing

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

84

[zhu & ghahramani 2002]

graph propagation (i)

0.53

0.53
nice

0.5

0.5

1.0

great

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.69

0.84

0.69

0.5

0.5
appealing

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

84

[zhu & ghahramani 2002]

equal?graph propagation (ii)

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.5

0.69

0.5
appealing

0.84

85

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

[zhu & ghahramani 2002]

graph propagation (ii)
1.0
-0.1

0.5

0.5

0.53
nice

so-so

great

0.5

0.5

good

0.5

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

ok

0.5

0.5
appealing

0.84

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.5

0.69

0.5
appealing

0.84

85

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

[zhu & ghahramani 2002]

graph propagation (ii)
1.0
-0.1

0.5

0.5

0.53
nice

so-so

great

0.5

0.5

good

0.5

0.84

ok

0.5

0.5
appealing

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

0.5

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.5

0.69

0.5
appealing

0.84

85

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

[zhu & ghahramani 2002]

graph propagation (ii)
1.0
-0.1

0.5

0.5

0.53
nice

so-so

great

0.5

good

0.5

0.84

0.5

ok
0.51

0.5

0.5
appealing

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

0.5

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.5

0.69

0.5
appealing

0.84

85

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

[zhu & ghahramani 2002]

graph propagation (ii)
1.0
-0.1

0.5

0.5

0.53
nice

so-so

great

0.5

good

0.5

0.84

0.5

ok
0.51

0.5

0.5
appealing

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

0.5

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.5

0.69

0.5
appealing

0.84

85

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

[zhu & ghahramani 2002]

graph propagation (ii)
1.0
-0.1

0.5

0.5

0.53
nice

so-so

great

changing the 
seed effects 
the score

0.5

good

0.5

0.84

0.5

ok
0.51

0.5

0.5
appealing

0.84

0.5

0.53
satisfying

0.5

0.5

excellent

0.5

1.0

great

0.5

0.5

0.53
nice

0.5

0.5

-0.1

so-so

good

0.5

0.84

ok

0.5

0.69

0.5
appealing

0.84

85

0.5

0.53
satisfying

0.5

0.5

excellent

1.0

[zhu & ghahramani 2002]

graph propagation (iii)

-0.1

so-so

1.0

great

0.5

nice

0.5

0.84

good

0.5

0.5

0.5

ok

86

[zhu & ghahramani 2002]

graph propagation (iii)

1.0

great

0.5

nice

0.5

-0.1

so-so

0.5

0.9

0.5

good

0.5

0.84

ok

86

[zhu & ghahramani 2002]

graph propagation (iii)

1.0

great

0.5

nice

0.5

-0.1

so-so

0.5

0.9

0.5

good
0.71

0.84

0.5

ok

86

[zhu & ghahramani 2002]

graph propagation (iii)

1.0

great

0.5

nice

0.5

-0.1

so-so

0.5

0.9

0.5

good
0.71

0.84

0.5

ok

single edge difference causes a change in the score

86

[zhu & ghahramani 2002]

   best path to seed    propagation

-0.1

so-so

0.5

1.0

great

0.5

0.5

0.9

nice

0.5

good

0.5

ok

87

[velikovich, et al., naacl 2010]

   best path to seed    propagation

-0.1

so-so

0.5

1.0

great

0.5

0.5

0.9

nice

0.5

good

0.5

ok

87

[velikovich, et al., naacl 2010]

   best path to seed    propagation

-0.1

so-so

0.5

1.0

great

0.5

0.5

0.9

nice

0.5

good

0.5

ok

87

[velikovich, et al., naacl 2010]

   best path to seed    propagation

-0.1

so-so

0.5

1.0

great

0.5

0.5

0.9

nice

0.5

good

0.5

ok

87

[velikovich, et al., naacl 2010]

   best path to seed    propagation

-0.1

so-so

0.5

1.0

great

0.5

0.5

0.9

nice

0.5

good

0.5

ok

87

[velikovich, et al., naacl 2010]

best path   best path to seed    propagation

-0.1

so-so

0.5

1.0

great

0.5

0.5

0.9

nice

0.5

good

0.5

ok

key observation: sentiment phrases are those that have 

short highly weighted paths to seed nodes

87

[velikovich, et al., naacl 2010]

best pathresults

lexicon

phrases

positive

negative

wilson et al. 2005

id138 lp 

[blair-goldensohn et al. 07]

web gp 

[velikovich et al. 2010]

7,618

12,310

2,718

5,705

4,900

6,605

178,104

90,337

87,767

size of the output lexicon

88

i

i

n
o
s
c
e
r
p

results

positive

wilson et al.

id138 lp

web gp

1

0.9

0.8

0.7

0.6

0.5

0.4

0

0.2

0.4

0.6

0.8

1

recall

89

[velikovich, et al., naacl 2010]

results

positive

wilson et al.

id138 lp

web gp

wilson et al.

id138 lp

web gp

negative

i

i

n
o
s
c
e
r
p

1

0.9

0.8

0.7

0.6

0.5

0.4

0

0.2

0.4

0.6

0.8

1

recall

0

0.2

0.4

recall

0.6

89

0.8

1

[velikovich, et al., naacl 2010]

i

i

n
o
s
c
e
r
p

1

0.9

0.8

0.7

0.6

0.5

0.4

resulting lexicon is 
larger in size and has 
much better precision

1

results

positive

wilson et al.

id138 lp

web gp

i

i

n
o
s
c
e
r
p

1

0.9

0.8

0.7

0.6

0.5

0.4

wilson et al.

id138 lp

web gp

negative

i

i

n
o
s
c
e
r
p

0.9

0.8

0.7

0.6

0.5

0.4

0

0.2

0.4

0.6

0.8

1

recall

0

0.2

0.4

recall

0.6

89

0.8

1

[velikovich, et al., naacl 2010]

results

90

[velikovich, et al., naacl 2010]

bad, awful, terrible, dirty, $#%! face, $#%!ed up, shut your $#%!ing mouth, run of the mill, out of touch, over the hillexcellent, fabulous, beautiful, inspiring,loveable, nicee, niice, cooool, coooool, once in a life time, state-of-the-art, fail-safe operation, just what you need, just what the doctor orderedresults

90

[velikovich, et al., naacl 2010]

bad, awful, terrible, dirty, $#%! face, $#%!ed up, shut your $#%!ing mouth, run of the mill, out of touch, over the hillexcellent, fabulous, beautiful, inspiring,loveable, nicee, niice, cooool, coooool, once in a life time, state-of-the-art, fail-safe operation, just what you need, just what the doctor orderedspelling variationsresults

90

[velikovich, et al., naacl 2010]

bad, awful, terrible, dirty, $#%! face, $#%!ed up, shut your $#%!ing mouth, run of the mill, out of touch, over the hillexcellent, fabulous, beautiful, inspiring,loveable, nicee, niice, cooool, coooool, once in a life time, state-of-the-art, fail-safe operation, just what you need, just what the doctor orderedspelling variationsmulti-word expressionsbig picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

91

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

91

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

91

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
- multilingual id52
- id29

[talukdar et al., emnlp 2008]

92

problem description

    given an entity, assign human readable 
descriptors to it
    toyota is a car manufacturer, japanese 
company, multinational company
    african countries such as uganda and angola
    large scale, open domain (1000   s of classes)
    applications

    web search,  advertising, etc.

93

extraction techniques

94

extraction techniques

....
what other musicians would fans of the 
album listen to:
storytelling musicians come to mind. musicians 
such as johnny cash, and woodie guthrie.
what is distinctive about this release?:
every song on the album has its own unique sound. 
from the fast paced that texas girl to the acoustic ....

 [van durme and pasca, aaai 2008]

    uses    <class> such as <instance>    
patterns
    extracts both class (musician) and 
instance (johnny cash)

94

extraction techniques

....
what other musicians would fans of the 
album listen to:
storytelling musicians come to mind. musicians 
such as johnny cash, and woodie guthrie.
what is distinctive about this release?:
every song on the album has its own unique sound. 
from the fast paced that texas girl to the acoustic ....

 [van durme and pasca, aaai 2008]

    uses    <class> such as <instance>    
patterns
    extracts both class (musician) and 
instance (johnny cash)

extractions from html lists and 
tables

    [wang and cohen, icdm 2007]
     webtables [cafarella et al., vldb 
2008], 154 million html tables

94

extraction techniques

....
what other musicians would fans of the 
album listen to:
storytelling musicians come to mind. musicians 
such as johnny cash, and woodie guthrie.
what is distinctive about this release?:
every song on the album has its own unique sound. 
from the fast paced that texas girl to the acoustic ....

 [van durme and pasca, aaai 2008]

    uses    <class> such as <instance>    
patterns
    extracts both class (musician) and 
instance (johnny cash)

pattern-based methods are usually tuned for          

high-precision, resulting in low coverage

can we combine extractions from all methods         

extractions from html lists and 
tables

(and sources) to improve coverage? 

    [wang and cohen, icdm 2007]
     webtables [cafarella et al., vldb 
2008], 154 million html tables

94

graph construction

pattern

table 
mining

95

graph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

95

graph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

95

extractioncon   dencegraph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

95

set 1set 2extractioncon   dencegraph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

bob dylan

johnny cash

billy joel

95

set 1set 2extractioncon   dencegraph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

bob dylan

johnny cash

billy joel

95

set 1set 2extractioncon   dencegraph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

bob dylan

johnny cash

billy joel

95

set 1set 2extractioncon   dencegraph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

0.95

bob dylan

johnny cash

billy joel

95

set 1set 2extractioncon   dencegraph construction

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

bob dylan

johnny cash

billy joel

0.95

0.87

0.73

0.82

0.72

95

set 1set 2extractioncon   dencegraph construction

0.95

0.87

0.73

0.82

0.72

bob dylan

johnny cash

billy joel

    bi-partite graph (not a id92g)
       set    nodes encourage members of the set to have 
similar labels
    natural way to represent extractions from many 
sources and methods

96

set 1set 2goal

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

0.95

0.87

0.73

0.82

0.72

bob dylan

johnny cash

billy joel

97

set 1set 2goal

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

musician

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

0.95

0.87

0.73

0.82

0.72

bob dylan

johnny cash

billy joel

97

set 1set 2goal

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

musician

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

0.95

0.87

0.73

0.82

0.72

bob dylan

johnny cash

musician

billy joel

97

set 1set 2goal

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

musician

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

0.95

0.87

0.73

0.82

0.72

bob dylan

johnny cash

musician

billy joel

musician

97

set 1set 2goal

pattern

set 1

bob dylan (0.95)

johnny cash (0.87)

billy joel (0.82)

musician

set 2

table 
mining

billy joel (0.72)

johnny cash (0.73)

0.95

0.87

0.73

0.82

0.72

bob dylan

johnny cash

musician

billy joel

musician

97

can we infer 

that bob 

dylan is also 

a musician?

set 1set 2graph propagation

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

0.95

0.87

0.73

0.82

0.72

98

set 1set 2graph propagation

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

0.95

0.87

0.73

0.82

0.72

98

set 1set 2seedgraph propagation

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

0.95

0.87

0.73

0.82

0.72

98

set 1set 2musician,1.0musician,1.0seedpredictiongraph propagation

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

0.95

0.87

0.73

0.82

0.72

98

set 1set 2musician,1.0musician,1.0musician,1.0musician,1.0seedpredictiongraph propagation

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

0.95

0.87

0.73

0.82

0.72

98

set 1set 2musician,1.0musician,1.0musician,1.0musician,1.0musician,0.8seedpredictiongraph propagation

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

0.95

0.87

0.73

0.82

0.72

98

set 1set 2musician,1.0musician,1.0musician,1.0musician,1.0musician,0.6musician,0.8seedpredictiongraph propagation

0.95

0.87

0.73

0.82

0.72

easily extendible to multiple 
seeds (classes) for each node

98

bob dylan

johnny cash

musician,1.0

billy joel

musician,1.0

set 1set 2musician,1.0musician,1.0musician,1.0musician,1.0musician,0.6musician,0.8seedpredictionevaluation metric
mean reciprocal rank

mrr =

1

|test-set|    v   test-set

1

rankv(class(v))

99

evaluation metric
mean reciprocal rank

mrr =

1

|test-set|    v   test-set

1

rankv(class(v))

billy joel

99

linguist, 0.6musician,0.4evaluation metric
mean reciprocal rank

mrr =

1

|test-set|    v   test-set

1

rankv(class(v))

billy joel

musician

99

linguist, 0.6musician,0.4gold labelevaluation metric
mean reciprocal rank

mrr =

1

|test-set|    v   test-set

1

rankv(class(v))

mrr

0.5

billy joel

musician

99

linguist, 0.6musician,0.4gold labelextraction for known instances

evaluation against id138 dataset (38 classes, 8910 instances)

)
r
r
m

(
 
k
n
a
r

i

 
l
a
c
o
r
p
c
e
r
 
n
a
e
m

patterns
adsorption
webtables

0.6

0.5

0.4

0.3

0.2

0.18

0.31

0.44

recall
100

0.57

0.70

graph with1.4m nodes,75m edges used.74m (class, instance) pairs extracted from webtables dataset924k (class, instance) pairs extracted from 100m web documentsextraction for known instances

adsorption is able to assign better 
class labels to more instances.

evaluation against id138 dataset (38 classes, 8910 instances)

)
r
r
m

(
 
k
n
a
r

i

 
l
a
c
o
r
p
c
e
r
 
n
a
e
m

patterns
adsorption
webtables

0.6

0.5

0.4

0.3

0.2

0.18

0.31

0.44

recall
100

0.57

0.70

graph with1.4m nodes,75m edges used.74m (class, instance) pairs extracted from webtables dataset924k (class, instance) pairs extracted from 100m web documentsextracted pairs

total classes: 9081

class

scienti   c journals

some non-seed instances found by 
adsorption
journal of physics, nature, structural and molecular 
biology, sciences sociales et sante, kidney and blood 
pressure research, american journal of physiology-cell 
physiology,    

nfl players

tony gonzales, thabiti davis, taylor stubble   eld, ron 
dixon, rodney hannan,    

book publishers

small night shade books, house of ansari press, 
highwater books, distributed art publishers, cooper 
canyon press,    

101

extracted pairs

total classes: 9081

class

scienti   c journals

some non-seed instances found by 
adsorption
journal of physics, nature, structural and molecular 
biology, sciences sociales et sante, kidney and blood 
pressure research, american journal of physiology-cell 
graph-based methods can easily handle large 
physiology,    

number of classes

tony gonzales, thabiti davis, taylor stubble   eld, ron 
dixon, rodney hannan,    

nfl players

book publishers

small night shade books, house of ansari press, 
highwater books, distributed art publishers, cooper 
canyon press,    

101

results

data available @ http://www.talukdar.net/datasets/class_inst/

102

0.150.20.250.30.35170 x 2170 x 10textrunner graph, 170 id138 classesmean reciprocal rank (mrr)amount of supervisionlp-zgladsorptionmadgraph with175k nodes529k edgesresults

data available @ http://www.talukdar.net/datasets/class_inst/

103

0.250.2850.320.3550.39192 x 2192 x 10freebase-2 graph, 192 id138 classesmean reciprocal rank (mrr)amount of supervisionlp-zgladsorptionmadgraph with303k nodes2.3m edges semantic constraints

isaac 
newton

johnny cash

billy joel

104

set 1set 2semantic constraints

isaac 
newton

johnny cash

billy joel

suppose we knew that both    johnny 
cash    and    billy joel    have albums.

how do we encode this constraint?

104

set 1set 2solution (i)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

105

set 1set 2solution (i)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

105

set 1set 2solution (i)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

    graph is no longer bi-partite (not necessarily bad)
    can lead to cliques of size of number of instances in 
the constraint (bad)

105

set 1set 2solution (ii)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

106

[talukdar & periera, acl 2010]

set 1set 2solution (ii)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

106

[talukdar & periera, acl 2010]

set 1set 2have_albumsolution (ii)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

106

[talukdar & periera, acl 2010]

set 1set 2have_albumsolution (ii)

both    johnny cash    and 
   billy joel    have albums.

isaac 
newton

johnny cash

billy joel

semantic constraints may be easily encoded

106

[talukdar & periera, acl 2010]

set 1set 2have_albumresults with semantic constraints

107

[talukdar & periera, acl 2010]

0.30.3380.3750.4130.45170 id138 classes, 10 seeds per class, using adsorption      mean reciprocal rank (mrr)textrunner graphyago graphtextrunner + yago graphresults with semantic constraints

107

[talukdar & periera, acl 2010]

0.30.3380.3750.4130.45170 id138 classes, 10 seeds per class, using adsorption      mean reciprocal rank (mrr)textrunner graphyago graphtextrunner + yago graphwith semantic constraintsresults with semantic constraints

additional semantic 

constraints help 

improve performance 

signi   cantly.

107

[talukdar & periera, acl 2010]

0.30.3380.3750.4130.45170 id138 classes, 10 seeds per class, using adsorption      mean reciprocal rank (mrr)textrunner graphyago graphtextrunner + yago graphwith semantic constraintsbig picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

108

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

108

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
     [subramanya et. al., emnlp 2008]
- multilingual id52
- id29

109

motivation

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datamotivation

domain

adaptation

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datamotivation

domain

adaptation

...    vbd     dt  nn       vbg        dt   ...
... bought a book detailing the ...

...      vbd     to    vb    dt  nn   to  ...
... wanted to book a    ight to ...

...   dt    nn  vbz   pp      dt   ...
... the book is about the ...

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datamotivation

domain

adaptation

...    vbd     dt  nn       vbg        dt   ...
... bought a book detailing the ...

...      vbd     to    vb    dt  nn   to  ...
... wanted to book a    ight to ...

...   dt    nn  vbz   pp      dt   ...
... the book is about the ...

... how to book a band ...

can you book a day room ...

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datamotivation

domain

adaptation

...    vbd     dt  nn       vbg        dt   ...
... bought a book detailing the ...

...      vbd     to    vb    dt  nn   to  ...
... wanted to book a    ight to ...

...   dt    nn  vbz   pp      dt   ...
... the book is about the ...

... how to book a band ...

can you book a day room ...

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datamotivation

domain

adaptation

...    vbd     dt  nn       vbg        dt   ...
... bought a book detailing the ...

...      vbd     to    vb    dt  nn   to  ...
... wanted to book a    ight to ...

...   dt    nn  vbz   pp      dt   ...
... the book is about the ...

... how to book a band ...

can you book a day room ...

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datamotivation

domain

adaptation

...    vbd     dt  nn       vbg        dt   ...
... bought a book detailing the ...

...      vbd     to    vb    dt  nn   to  ...
... wanted to book a    ight to ...

...   dt    nn  vbz   pp      dt   ...
... the book is about the ...

... how to book a band ...

can you book a day room ...

... how to unrar a    le ...

110

unlabeled datasmall amounts of labeled sourcedomain datalarge amounts of unlabeled targetdomain datagraph construction (i)

       when do you book plane tickets?   

   do you read a book on the plane?   

111

graph construction (i)

       when do you book plane tickets?   

similarity

   do you read a book on the plane?   

111

graph construction (i)

wrb   vbp  prp    vb      nn       nns

       when do you book plane tickets?   

similarity

   do you read a book on the plane?   
vbp   prp   vb   dt  nn    in  dt    nn

111

graph construction (ii)

can you book a day room at hilton hawaiian village ?

what was the book that has no letter e ?

how much does it cost to book a band ?

how to get a book agent ?

112

graph construction (ii)

can you book a day room at hilton hawaiian village ?

what was the book that has no letter e ?

how much does it cost to book a band ?

how to get a book agent ?

112

graph construction (ii)

can you book a day room at hilton hawaiian village ?

what was the book that has no letter e ?

how much does it cost to book a band ?

how to get a book agent ?

112

graph construction (ii)

you book a

to book a

the book that

a book agent

112

graph construction (ii)

you book a

k-nearest
neighbors?

the book that

to book a

a book agent

112

graph construction (iii)

you start a

the book that

you book a

the city that

a book agent

a movie agent

to book a

to run a

to become a

113

graph construction (iii)

you start a

the book that

you book a

the city that

a book agent

a movie agent

to book a

to run a

to become a

113

graph construction (iii)

you start a

the book that

you book a

you unrar a

the city that

a book agent

a movie agent

to book a

to run a

to become a

113

graph construction - features

how much does it cost to book a band ?

cost to book a band

to book a

114

graph construction - features

how much does it cost to book a band ?

cost to book a band

to book a

114

graph construction - features

how much does it cost to book a band ?

to book a

cost to book a band

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word  cost to ____ a
suffix

            book
        to ____ a
        to ____ a band

            none

114

graph construction - features

how much does it cost to book a band ?

to book a

cost to book a band

cost to

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word  cost to ____ a
suffix

            book
        to ____ a
        to ____ a band

            none

114

graph construction - features

how much does it cost to book a band ?

to book a

cost to book a band

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word  cost to ____ a
suffix

cost to
a band
            book
        to ____ a
        to ____ a band

            none

114

graph construction - features

how much does it cost to book a band ?

to book a

cost to book a band

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word  cost to ____ a
suffix

cost to
a band
            book
        to ____ a
        to ____ a band

            none

114

graph construction - features

how much does it cost to book a band ?

to book a

cost to book a band

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word  cost to ____ a
suffix

cost to
a band
            book
        to ____ a
        to ____ a band

            none

114

graph construction - features

how much to book a    ight to paris?

how much does it cost to book a band ?

115

graph construction - features

how much to book a    ight to paris?

how much does it cost to book a band ?

to book a

115

graph construction - features

how much to book a    ight to paris?

how much does it cost to book a band ?

to book a

115

graph construction - features

how much to book a    ight to paris?

how much does it cost to book a band ?

to book a

115

graph construction - features

to book a
to book a

115

graph construction - features

to book a

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word
suffix

115

graph construction - features

n

t i o

r m a

a l  i n f o

u

t

  m u

e

- w i s

t

o i n

p

to book a

0.1

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word
suffix

115

graph construction - features

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word
suffix

point-wise mutual information

to book a

0.1
0.4

115

graph construction - features

to book a

0.1
0.4
.
.
.

trigram + context
left context
right context
center word
trigram - center word
left word + right context
left context + right word
suffix

115

similarity function

to book a

0.1
0.4
.
.
.

cosine similarity (        ,        )

= 0.56

116

similarity function

to book a

0.1
0.4
.
.
.

you unrar a

cosine similarity (        ,        )

= 0.56

116

similarity function

0.56

you unrar a

to book a

cosine similarity (        ,        )

0.2
0.3
.
.
.

= 0.56

0.1
0.4
.
.
.

116

approach (i)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf

117

approach (i)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf

can you book a day room at hilton hawaiian village ?

how to unrar a zipped    le ?

how to  get   a   book  agent ?

how do you book a    ight to multiple cities ?

117

approach (i)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf

crf

can you book a day room at hilton hawaiian village ?

how to unrar a zipped    le ?

how to  get   a   book  agent ?

how do you book a    ight to multiple cities ?

117

approach (i)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf

can you book a day room at hilton hawaiian village ?

how to unrar a zipped    le ?

how to  get   a   book  agent ?

how do you book a    ight to multiple cities ?

117

approach (ii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)

118

approach (ii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)

can you book a day room at hilton hawaiian village ?

how do you book a    ight to multiple cities ?

118

approach (ii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)

can you book a day room at hilton hawaiian village ?

you book a

how do you book a    ight to multiple cities ?

118

approach (ii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)

can you book a day room at hilton hawaiian village ?

1

n   

you book a

how do you book a    ight to multiple cities ?

118

approach (iii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation

you book a

you start a

you unrar a

119

approach (iii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation

you book a
vb
0.7

.......
.......

nn
0.2

you start a
nn
0.3

vb
0.5

.......
.......

you unrar a
nn
0.2

vb
0.2

.......
.......

119

approach (iii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation

you book a
vb
0.7

.......
.......

nn
0.2

you start a
nn
0.3

vb
0.5

.......
.......

you unrar a
nn
0.2

vb
0.2

.......
.......

119

approach (iii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation

you book a
vb
0.7

.......
.......

nn
0.2

you start a
nn
0.3

vb
0.5

.......
.......

you unrar a
nn
0.1

vb
0.6

.......
.......

119

approach (iii)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation

you book a
vb
0.7

.......
.......

nn
0.2

you start a
nn
0.3

vb
0.5

.......
.......

you unrar a
nn
0.1

vb
0.6

.......
.......

if two id165s are similar according to the graph 
then their output distributions should be similar

119

approach (iv)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode

120

approach (iv)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode

can you unrar a zipped    le?

120

approach (iv)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode

can you unrar a zipped    le?

120

approach (iv)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode

can you unrar a zipped    le?

nn
0.2

vb
0.2

.......
.......

120

posterior decodeapproach (iv)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode

can you unrar a zipped    le?

nn
0.2

vb
0.2

.......
.......

nn
0.1

vb
0.6

120

posterior decodegraph propagationapproach (iv)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode

can you unrar a zipped    le?
can you unrar a zipped    le?

nn
0.2

vb
0.2

.......
.......

nn
0.1

vb
0.6

convex

combination

+

nn
0.15

vb
120
0.45

posterior decodegraph propagationapproach (v)

1. train a crf on labeled data
2. while not converged do:

2.1. posterior decode unlabeled data using crf
2.2. aggregate posteriors (token-to-type mapping)   
2.3. graph propagation
2.4. viterbi decode
2.5. retrain crf on labeled & automatically 
labeled unlabeled data

121

viterbi decoding : intuition

p0(y|x)

122

space of all distributions realizable using a crfcurrent estimateviterbi decoding : intuition

p0(y|x)

122

space of all distributions realizable using a crfcurrent estimateviterbi decoding : intuition

q(y|x)

p0(y|x)

122

space of all distributions realizable using a crfconverged graph posteriorcurrent estimateviterbi decoding : intuition

q(y|x)

kl projection

  q(y|x)

p0(y|x)

122

space of all distributions realizable using a crfconverged graph posteriorcurrent estimateviterbi decoding : intuition

q(y|x)

kl projection

  q(y|x)

p0(y|x)

122

space of all distributions realizable using a crfconverged graph posteriorcurrent estimateviterbi decoding : intuition

q(y|x)

kl projection

  q(y|x)

p   (y|x)

p0(y|x)

122

space of all distributions realizable using a crfconverged graph posteriorcurrent estimatecorpora

    source domain (labeled): wall street journal 

(wsj) section of the id32.

     target domain:
    questionbank: 4000 labeled sentences
    penn biotreebank: 1061 labeled sentences

123

graph construction: question bank

124

wsjunlabeled datagraph construction: question bank

pmi 

statistics

124

wsjunlabeled datagraph construction: question bank

pmi 

statistics

similarity graph 
construction

124

wsjunlabeled datagraph construction: question bank

pmi 

statistics

similarity graph 
construction

124

wsjunlabeled datalabels are not used during graph constructiongraph construction: question bank

pmi 

statistics

similarity graph 
construction

124

wsjunlabeled datalabels are not used during graph construction10 million questions collected from anonymized internet search queriesgraph construction: bio

pmi 

statistics

similarity graph 
construction

125

wsjunlabeled datalabels are not used during graph construction100,000 sentences chosen by searching medline (blitzer et al. 2006)baseline (supervised)

    features: word identity, suf   xes, pre   xes & 
special character detectors (dashes, digits, 
etc.).
    achieves 97.17% accuracy on wsj 
development set.

126

not the same as features used using graph constructionresults

questions

83.8

84.0

86.8

127

bio

86.2

87.1

87.6

baseline

self-training

semi-supervised 

crf

analysis

questions

bio

percentage of unlabeled trigrams not 
connected to and any labeled trigram 

12.4

46.8

average path length between an unlabeled 
trigram  and its nearest labeled trigram

9.4

22.4

128

analysis

questions

bio

percentage of unlabeled trigrams not 
connected to and any labeled trigram 

12.4

46.8

average path length between an unlabeled 
trigram  and its nearest labeled trigram

9.4

22.4

128

sparse graphanalysis

    pros
    inductive
    produces a crf (standard crf id136 
    issues
    graph construction
    graph is not integrated with crf training

infrastructure may be used)

129

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

130

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

130

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
- multilingual id52
     [das & petrov,  acl 2011]
- id29

131

motivation

132

motivation

    supervised pos taggers for english have accuracies 
in the high 90   s for most domains

132

motivation

    supervised pos taggers for english have accuracies 
in the high 90   s for most domains
    by comparison taggers in other languages are not as 
accurate

132

motivation

    supervised pos taggers for english have accuracies 
in the high 90   s for most domains
    by comparison taggers in other languages are not as 
accurate
    performance ranges from between 60 - 80%

132

motivation

    supervised pos taggers for english have accuracies 
in the high 90   s for most domains
    by comparison taggers in other languages are not as 
accurate
    performance ranges from between 60 - 80%

model in 

resource-rich 
language (e.g., 

english)

transfer 
knowledge

132

model in 

resource-poor 

language

cross-lingual projection

the

food at google

is

good

.

133

cross-lingual projection

det noun adp noun verb adj
the
good

food at google

is

.
.

133

96% accuracycross-lingual projection

det noun adp noun verb adj
the
good

food at google

is

.
.

das essen

ist

gut

bei google .

133

96% accuracycross-lingual projection

det noun adp noun verb adj
the
good

food at google

is

.
.

das essen

ist

gut

bei google .

automatic alignments from translation data

(available for more than 50 languages)

133

96% accuracycross-lingual projection

det
the

noun
food

adp
at

noun
google

verb

is

adj
good

.
.

das essen

ist

gut

bei google.

134

cross-lingual projection

noun
food

det
the

essen

das

adj
good

gut

bei

google

adp
at

noun
google

134

verb

is

ist

.

.
.

cross-lingual projection

noun
food

det
the

essen

das

adj
good

gut

bag of alignments

bei

google

adp
at

noun
google

135

verb

is

ist

.

.
.

cross-lingual projection

noun
food

det
the

essen

das

noun
verb
adj
adp
det

bag of alignments

ist

noun
verb
adj
adp
det

gut

adj
good

noun
verb
adj
adp
det

noun
verb
adj
adp
det

noun
verb
adj
adp
det

adp
at

bei

google

noun
verb
adj
adp
det

135

noun
google

verb

is

noun
verb
adj
adp
det
.

.

.
.

adv
nicely
adj
fine

adj
good

noun
noun
verb
verb
adj
adp
adj
det
.
adp
pron
det
adv

cross-lingual projection

noun
food

pron

it

det
the

essen

das

more alignments!

noun
verb
adj
adp
det

noun
verb
adj
adp
det

gut

adp
at

noun
noun
verb
verb
adj
adj
adp
det
adp
.
det
pron

ist

noun
verb
adj
adp
det

noun
verb
adj
adp
det
.

.

bei

google

noun
noun
verb
verb
adj
adj
adp
adp
det
det

135

noun
google

verb
google

verb

is

.
.

cross-lingual projection results

danish dutch german greek

italian portuguese spanish swedish average

feature-
id48

69.1

65.1

81.3

71.8

68.1

78.4

80.2

70.1

73.0

136

feature-id48 [berg-kirkpatrick, naacl 2010]

cross-lingual projection results

danish dutch german greek

italian portuguese spanish swedish average

feature-
id48

69.1

65.1

81.3

71.8

68.1

78.4

80.2

70.1

73.0

direct 
projection 73.6

77.0

83.2

79.3

79.7

82.6

80.1

74.7

78.8

136

feature-id48 [berg-kirkpatrick, naacl 2010]

graph id173

gutem essen zugetan

ist gut bei

ist wichtig bei

ist fein bei

ist lebhafter bei

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

zu realisieren ,

zu essen ,

zu stecken ,

zu erreichen ,

137

graph id173

ist gut bei

ist wichtig bei

ist fein bei

ist lebhafter bei

gutem essen zugetan

noun

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

zu realisieren ,

separation!

zu essen ,

zu stecken ,

zu erreichen ,

138

verb

graph id173

adj
good
adj
fine

adv

nicely

adj

important

ist wichtig bei

ist gut bei

ist fein bei

ist lebhafter bei

gutem essen zugetan

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

zu realisieren ,

zu stecken ,

zu erreichen ,

zu essen ,

noun
food

139

eat
verb

verb
eating
eat
verb

graph id173

adj
good
adj
fine

adv

nicely

adj

important

ist wichtig bei

ist gut bei

ist fein bei

ist lebhafter bei

gutem essen zugetan

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

zu realisieren ,

zu stecken ,

zu erreichen ,

zu essen ,

noun
food

139

eat
verb

verb
eating
eat
verb

graph id173

adj
good
adj
fine

adv

nicely

adj

important

ist wichtig bei

ist gut bei

adj
adv

adj
adv

ist fein bei

ist lebhafter bei

gutem essen zugetan

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

zu realisieren ,

zu stecken ,

zu erreichen ,

zu essen ,

noun
food

verb
noun
139

eat
verb

verb
eating
eat
verb

graph id173

adv

nicely

adj

important

gutem essen zugetan

adj
good
adj
fine

ist gut bei

adj
adv

ist wichtig bei

adj
adv

ist fein bei
adj
adv

ist lebhafter bei

adj
adv

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

zu realisieren ,

verb
noun

zu stecken ,

zu erreichen ,

zu essen ,

noun
food

verb
eating
eat
verb

verb
noun
140

eat
verb

graph id173

adv

nicely

adj

important

gutem essen zugetan

adj
good
adj
fine

ist gut bei

adj
adv

ist wichtig bei

adj
adv

ist fein bei
adj
adv

ist lebhafter bei

adj
adv

zum essen niederlassen

fuers essen drauf

1000 essen pro

schlechtes essen und

verb
noun

zu stecken ,

verb
noun

zu realisieren ,

verb
noun
zu essen ,

zu erreichen ,

verb
noun
141

noun
food

verb
eating
eat
verb

eat
verb

graph id173

adv

nicely

adj

important

gutem essen zugetan

adj
good
adj
fine

ist gut bei

adj
adv

ist wichtig bei

adj
adv

ist fein bei
adj
adv

ist lebhafter bei

adj
adv

zum essen niederlassen
continues till convergence...
fuers essen drauf

1000 essen pro

schlechtes essen und

verb
noun

zu stecken ,

verb
noun

zu realisieren ,

verb
noun
zu essen ,

zu erreichen ,

verb
noun
141

noun
food

verb
eating
eat
verb

eat
verb

results

danish dutch german greek

italian

portugese spanish swedish average

feature-
id48

direct 
projection

69.1

65.1

81.3

71.8

68.1

78.4

80.2

70.1

73.0

73.6

77.0

83.2

79.3

79.7

82.6

80.1

74.7

78.8

142

results

danish dutch german greek

italian

portugese spanish swedish average

feature-
id48

direct 
projection

graph-
based 

projection

69.1

65.1

81.3

71.8

68.1

78.4

80.2

70.1

73.0

73.6

77.0

83.2

79.3

79.7

82.6

80.1

74.7

78.8

83.2

79.5

82.8

82.5

86.8

87.9

84.2

80.5

83.4

142

results

danish dutch german greek

italian

portugese spanish swedish average

69.1

65.1

81.3

71.8

68.1

78.4

80.2

70.1

73.0

73.6

77.0

83.2

79.3

79.7

82.6

80.1

74.7

78.8

83.2

79.5

82.8

82.5

86.8

87.9

84.2

80.5

83.4

96.9

94.9

98.2

97.8

95.8

97.2

96.8

94.8

96.6

feature-
id48

direct 
projection

graph-
based 

projection

oracle 

(supervised)

142

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

multilingual id52

143

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

multilingual id52

143

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

multilingual id52

143

outline

    motivation
    graph construction
    id136 methods
    scalability
    applications
    conclusion & future work

- text categorization
- id31
- class instance acquisition
- id52
- multilingual id52
- id29
       [das & smith,  acl 2011]

144

problem description

    extract shallow semantic structure: frames and 

roles

i want to go to jeju island on sunday

145

problem description

    extract shallow semantic structure: frames and 

roles

i want to go to jeju island on sunday
i want to go to jeju island on sunday

145

predicateproblem description

    extract shallow semantic structure: frames and 

roles

travel

i want to go to jeju island on sunday
i want to go to jeju island on sunday

145

predicateframeproblem description

    extract shallow semantic structure: frames and 

roles

travel

i want to go to jeju island on sunday
i want to go to jeju island on sunday

145

predicateframeproblem description

    extract shallow semantic structure: frames and 

roles

travel

traveller

time

goal

i want to go to jeju island on sunday
i want to go to jeju island on sunday

145

predicateframeroleargumentproblem description

    predicate identi   cation
    most approaches assume this is given
    frame identi   cation
    argument identi   cation

146

motivation

f-     measure

f-     measure

95.0

81.3

67.5

53.8

40.0
seen+unseen predicates

95.0

81.3

67.5

53.8

40.0

unseen predicates

147

90.546.6frame identi   cationmotivation

f-     measure

f-     measure

95.0

81.3

67.5

53.8

40.0
seen+unseen predicates

f-     measure

70.0

58.8

47.5

36.3

25.0
seen+unseen predicates

95.0

81.3

67.5

53.8

40.0

70.0

58.8

47.5

36.3

25.0

unseen predicates

f-     measure

unseen predicates

147

90.546.668.530.2frame identi   cationfull parsingsparse label data

    labeled data has only about 9,263 labeled 
predicates (targets)
    english on the other hand has a lot more 
potential predicates (~65,000 in newswire)

148

sparse label data

    labeled data has only about 9,263 labeled 
predicates (targets)
    english on the other hand has a lot more 
potential predicates (~65,000 in newswire)

vertices

    construct a graph with potential predicates as 
    expand the lexicon by using graph-based ssl

148

graph propagation (i)

seed predicates

149

graph propagation (ii)

seed predicates

unseen predicates

150

graph propagation (iii)

151

graph propagation (iv)

152

results on unseen predicates

f-     measure

70.0

62.5

55.0

47.5

40.0

supervised

self-training

graph-based

153

65.342.746.6frame identi   cationresults on unseen predicates

f-     measure

70.0

62.5

55.0

47.5

40.0

50.0

43.8

37.5

31.3

25.0

supervised

self-training

graph-based

f-     measure

supervised

self-training

153

graph-based

65.342.746.646.726.630.2frame identi   cationfull parsingresults on unseen predicates

f-     measure

70.0

62.5

55.0

47.5

40.0

50.0

43.8

37.5

31.3

25.0

supervised

self-training

graph-based

f-     measure

supervised

self-training

153

graph-based

65.342.746.646.726.630.2frame identi   cationfull parsinggains from graph-based sslbig picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

multilingual id52

id29

154

big picture

use case 1: transductive classification
  use case 2: training better inductive model

use case 1 use case 2

text categorization

id31

class instance acquisition

id52

multilingual id52

id29

154

when to use graph-based ssl 

and which method?

155

when to use graph-based ssl 

and which method?

    when input data itself is a graph (relational data)
    or, when the data is expected to lie on a manifold

155

when to use graph-based ssl 

and which method?

    when input data itself is a graph (relational data)
    or, when the data is expected to lie on a manifold
    mad, quadratic criteria (qc)
    when labels are not mutually exclusive
    maddl: when label similarities are known

155

when to use graph-based ssl 

and which method?

    when input data itself is a graph (relational data)
    or, when the data is expected to lie on a manifold
    mad, quadratic criteria (qc)
    when labels are not mutually exclusive
    maddl: when label similarities are known
    measure propagation (mp)
    for probabilistic interpretation

155

when to use graph-based ssl 

and which method?

    when input data itself is a graph (relational data)
    or, when the data is expected to lie on a manifold
    mad, quadratic criteria (qc)
    when labels are not mutually exclusive
    maddl: when label similarities are known
    measure propagation (mp)
    for probabilistic interpretation
    manifold id173
    for generalization to unseen data (induction)

155

graph-based ssl: summary

156

graph-based ssl: summary

    provide    exible representation
    for both iid and relational data

156

graph-based ssl: summary

    provide    exible representation
    for both iid and relational data
    graph construction can be key

156

graph-based ssl: summary

    provide    exible representation
    for both iid and relational data
    graph construction can be key
    scalable: node reordering and mapreduce

156

graph-based ssl: summary

    provide    exible representation
    for both iid and relational data
    graph construction can be key
    scalable: node reordering and mapreduce
    can handle labeled as well as unlabeled data

156

graph-based ssl: summary

    provide    exible representation
    for both iid and relational data
    graph construction can be key
    scalable: node reordering and mapreduce
    can handle labeled as well as unlabeled data
    can handle multi class, multi label settings

156

graph-based ssl: summary

    provide    exible representation
    for both iid and relational data
    graph construction can be key
    scalable: node reordering and mapreduce
    can handle labeled as well as unlabeled data
    can handle multi class, multi label settings
    effective in practice

156

open challenges

157

open challenges

    graph-based ssl for id170
    algorithms: combining inductive and graph-based methods
    applications: constituency and id33, coreference

157

open challenges

    graph-based ssl for id170
    algorithms: combining inductive and graph-based methods
    applications: constituency and id33, coreference
    scalable graph construction, especially with      

multi-modal data

157

open challenges

    graph-based ssl for id170
    algorithms: combining inductive and graph-based methods
    applications: constituency and id33, coreference
    scalable graph construction, especially with      
    extensions with other id168s, sparsity, etc.

multi-modal data

157

open challenges

    graph-based ssl for id170
    algorithms: combining inductive and graph-based methods
    applications: constituency and id33, coreference
    scalable graph construction, especially with      
    extensions with other id168s, sparsity, etc.
    using side information

multi-modal data

157

acknowledgments

    national science foundation (nsf) iis-0447972
    darpa hro1107-1-0029, fa8750-09-c-0179
    google research award
    dipanjan das (google), ryan mcdonald (google), 
fernando pereira (google), slav petrov (google), 
noah smith (cmu)

158

references (i)

[1] a. alexandrescu and k. kirchhoff. data-driven graph construction for semi-supervised graph-based learning in nlp. in naacl hlt, 
2007.
[2] y. altun, d. mcallester, and m. belkin. maximum margin semi-supervised learn- ing for structured variables. nips, 2006.
[3] s. baluja, r. seth, d. sivakumar, y. jing, j. yagnik, s. kumar, d. ravichandran, and m. aly. video suggestion and discovery for youtube: 
taking id93 through the view graph. in www, 2008.
[4] r. bekkerman, r. el-yaniv, n. tishby, and y. winter. distributional word clusters vs. words for text categorization. j. mach. learn. 
res., 3:1183   1208, 2003.
[5] m. belkin, p. niyogi, and v. sindhwani. manifold id173: a geometric framework for learning from labeled and unlabeled 
examples. journal of machine learning research, 7:2399   2434, 2006.
[6] y. bengio, o. delalleau, and n. le roux. label propagation and quadratic criterion. semi-supervised learning, 2006.
[7] t. berg-kirkpatrick, a. bouchard-co  t   e, j. denero, and d. klein. painless unsupervised learning with features. in hlt-naacl, 
2010.
[8] j. bilmes and a. subramanya. scaling up machine learning: parallel and distributed approaches, chapter parallel graph-based semi-
supervised learning. 2011.
[9] s. blair-goldensohn, t. neylon, k. hannan, g. a. reis, r. mcdonald, and j. reynar. building a sentiment summarizer for local service 
reviews. in in nlp in the information explosion era, 2008.
[10] m. cafarella, a. halevy, d. wang, e. wu, and y. zhang. webtables: exploring the power of tables on the web. vldb, 2008.
[11] o. chapelle, b. scho   lkopf, a. zien, et al. semi-supervised learning. mit press cambridge, ma:, 2006.
[12] y. choi and c. cardie. adapting a polarity lexicon using integer linear program- ming for domain speci   c sentiment classi   cation. 
in emnlp, 2009.
[13] s. daitch, j. kelner, and d. spielman. fitting a graph to vector data. in icml, 2009.
[14] d. das and s. petrov. unsupervised part-of-speech tagging with bilingual graph- based projections. in acl, 2011.
[15] d. das, n. schneider, d. chen, and n. a. smith. probabilistic frame-id29. in naacl-hlt, 2010.
[16] d. das and n. smith. graph-based lexicon expansion with sparsity-inducing penalties. naacl-hlt, 2012.
[17] d. das and n. a. smith. semi-supervised frame-id29 for unknown predicates. in acl, 2011.
[18] j. davis, b. kulis, p. jain, s. sra, and i. dhillon. information-theoretic metric learning. in icml, 2007.
[19] o. delalleau, y. bengio, and n. l. roux. ef   cient non-parametric function induction in semi-supervised learning. in aistats, 2005.
[20] p. dhillon, p. talukdar, and k. crammer. id136-driven metric learning for graph construction. technical report, ms-cis-10-18, 
university of pennsylvania, 2010.

159

references (ii)

[21] s. dumais, j. platt, d. heckerman, and m. sahami. inductive learning algorithms and representations for text categorization. in 
cikm, 1998.
[22] j. friedman, j. bentley, and r. finkel. an algorithm for    nding best matches in logarithmic expected time. acm transaction on 
mathematical software, 3, 1977.
[23] j. garcke and m. griebel. data mining with sparse grids using simplicial basis functions. in kdd, 2001.
[24] a. goldberg and x. zhu. seeing stars when there aren   t many stars: graph-based semi-supervised learning for sentiment 
categorization. in proceedings of the first workshop on graph based methods for natural language processing, 2006.
[25] a. goldberg, x. zhu, and s. wright. dissimilarity in graph-based semi-supervised classi   cation. aistats, 2007.
[26] m. hu and b. liu. mining and summarizing customer reviews. in kdd, 2004. 
[27] t. jebara, j. wang, and s. chang. graph construction and b-matching for semi-supervised learning. in icml, 2009. 
[28] t. joachims. transductive id136 for text classi   cation using support vector machines. in icml, 1999.
[29] t. joachims. transductive learning via spectral graph partitioning. in icml, 2003.
[30] m. karlen, j. weston, a. erkan, and r. collobert. large scale manifold transduction. in icml, 2008. 
[31] s.-m. kim and e. hovy. determining the sentiment of opinions. in proceedings of the 20th international conference on 
computational linguistics, 2004. 
[32] f. kschischang, b. frey, and h. loeliger. factor graphs and the sum-product algorithm. id205, ieee transactions on, 
47(2):498   519, 2001. 
[33] k. lerman, s. blair-goldensohn, and r. mcdonald. sentiment summarization: evaluating and learning user preferences. in eacl, 
2009. 
[34] d.lewisetal.reuters-21578.http://www.daviddlewis.com/resources/testcollections/reuters21578, 1987. 
[35] j. malkin, a. subramanya, and j. bilmes. on the semi-supervised learning of multi-layered id88s. in interspeech, 2009. 
[36] b. pang, l. lee, and s. vaithyanathan. thumbs up?: sentiment classi   cation using machine learning techniques. in emnlp, 2002. [37] 
d. rao and d. ravichandran. semi-supervised polarity lexicon induction. in eacl, 2009. 
[38] a. subramanya and j. bilmes. soft-supervised learning for text classi   cation. in emnlp, 2008. 
[39] a. subramanya and j. bilmes. entropic graph id173 in non-parametric semi-supervised classi   cation. nips, 2009.
[40] a. subramanya and j. bilmes. semi-supervised learning with measure propagation. jmlr, 2011.

160

references (iii)

[41] a. subramanya, s. petrov, and f. pereira. ef   cient graph-based semi-supervised learning of structured tagging models. in emnlp, 
2010.
[42] p. talukdar. topics in graph construction for semi-supervised learning. technical report, ms-cis-09-13, university of 
pennsylvania, 2009.
[43] p. talukdar and k. crammer. new regularized algorithms for transductive learning. ecml, 2009.
[44] p. talukdar and f. pereira. experiments in graph-based semi-supervised learning methods for class-instance acquisition. in acl, 
2010.
[45] p. talukdar, j. reisinger, m. pa   sca, d. ravichandran, r. bhagat, and f. pereira. weakly-supervised acquisition of labeled class 
instances using graph id93. in emnlp, 2008.
[46] b. van durme and m. pasca. finding cars, goddesses and enzymes: parametrizable acquisition of labeled instances for open-
domain information extraction. in aaai, 2008.
[47] l. velikovich, s. blair-goldensohn, k. hannan, and r. mcdonald. the viability of web-derived polarity lexicons. in hlt-naacl, 
2010.
[48] f. wang and c. zhang. label propagation through linear neighborhoods. in icml, 2006.
[49] j. wang, t. jebara, and s. chang. graph transduction via alternating minimization. in icml, 2008.
[50] r. wang and w. cohen. language-independent set expansion of named entities using the web. in icdm, 2007.
[51] k. weinberger and l. saul. distance metric learning for large margin nearest neighbor classi   cation. the journal of machine 
learning research, 10:207   244, 2009.
[52] t. wilson, j. wiebe, and p. hoffmann. recognizing contextual polarity in phrase- level id31. in hlt-emnlp, 2005.
[53] d. zhou, o. bousquet, t. lal, j. weston, and b. scho   lkopf. learning with local and global consistency. nips, 2004.
[54] d. zhou, j. huang, and b. scho   lkopf. learning from labeled and un- labeled data on a directed graph. in icml, 2005.
[55] d. zhou, b. scho   lkopf, and t. hofmann. semi-supervised learning on directed graphs. in nips, 2005.
[56] x. zhu and z. ghahramani. learning from labeled and unlabeled data with label propagation. technical report, cmu-
cald-02-107, carnegie mellon university, 2002.
[57] x. zhu and z. ghahramani. learning from labeled and unlabeled data with label propagation. technical report, carnegie mellon 
university, 2002.
[58] x. zhu, z. ghahramani, and j. lafferty. semi-supervised learning using gaussian    elds and id94. in icml, 2003.
[59] x. zhu and j. lafferty. harmonic mixtures: combining mixture models and graph- based methods for inductive and scalable 
semi-supervised learning. in icml, 2005.

161

thanks!

web: http://graph-ssl.wikidot.com/

162

