id202

lecture slides for chapter 2 of deep learning 

ian goodfellow 

2016-06-24

about this chapter

    not a comprehensive survey of all of id202 

    focused on the subset most relevant to deep 

learning 

    larger subset: e.g., id202 by georgi shilov

(goodfellow 2016)

scalars

    a scalar is a single number 

    integers, real numbers, rational numbers, etc. 

    we denote it with italic font:

a, n, x

(goodfellow 2016)

vectors

    a vector is a 1-d array of numbers: 

    vectors: a vector is an array of numbers. the numbers are arranged in
order. we can identify each individual number by its index in that ordering.
typically we give vectors lower case names written in bold typeface, such
as x. the elements of the vector are identi   ed by writing its name in italic
typeface, with a subscript. the    rst element of x is x1, the second element
is x2 and so on. we also need to say what kind of numbers are stored in
the vector. if each element is in r, and the vector has n elements, then the
vector lies in the set formed by taking the cartesian product of r n times,
denoted as rn. when we need to explicitly identify the elements of a vector,
we write them as a column enclosed in square brackets:

    vectors: a vector is an array of numbers. the numbers are arranged in
order. we can identify each individual number by its index in that ordering.
typically we give vectors lower case names written in bold typeface, such
as x. the elements of the vector are identi   ed by writing its name in italic
typeface, with a subscript. the    rst element of x is x1, the second element
is x2 and so on. we also need to say what kind of numbers are stored in
the vector. if each element is in r, and the vector has n elements, then the
vector lies in the set formed by taking the cartesian product of r n times,
denoted as rn. when we need to explicitly identify the elements of a vector,
we write them as a column enclosed in square brackets:

we can think of vectors as identifying points in space, with each element
giving the coordinate along a di   erent axis.
sometimes we need to index a set of elements of a vector. in this case, we
de   ne a set containing the indices and write the set as a subscript. for
example, to access x1, x3 and x6, we de   ne the set s = {1, 3, 6} and write
xs. we use the   sign to index the complement of a set. for example x 1 is
the vector containing all elements of x except for x1, and x s is the vector
containing all of the elements of x except for x1, x3 and x6.

    matrices: a matrix is a 2-d array of numbers, so each element is identi   ed by
two indices instead of just one. we usually give matrices upper-case variable
names with bold typeface, such as a. if a real-valued matrix a has a height

    example notation for type and size:

    can be real, binary, integer, etc. 

x =26664

x1
x2
...
xn

37775

(goodfellow 2016)

(2.1)

.

de   ne a set containing the indices and write the set as a subscript. for
example, to access x1, x3 and x6, we de   ne the set s = {1, 3, 6} and write
xs. we use the   sign to index the complement of a set. for example x 1 is
the vector containing all elements of x except for x1, and x s is the vector
containing all of the elements of x except for x1, x3 and x6.

matrices
35 ) a> =    a1,1 a2,1 a3,1
a1,2 a2,2 a3,2  

figure 2.1: the transpose of the matrix can be thought of as a mirror image across the
main diagonal.
    a matrix is a 2-d array of numbers: 
the i-th column of a. when we need to explicitly identify the elements of a
matrix, we write them as an array enclosed in square brackets:

a1,1 a1,2
a2,1 a2,2
a3,1 a3,2

a =24

row

    matrices: a matrix is a 2-d array of numbers, so each element is identi   ed by
two indices instead of just one. we usually give matrices upper-case variable
names with bold typeface, such as a. if a real-valued matrix a has a height
of m and a width of n, then we say that a 2 rm   n. we usually identify
the elements of a matrix using its name in italic but not bold font, and the

sometimes we may need to index matrix-valued expressions that are not just
a single letter. in this case, we use subscripts after the expression, but do
not convert anything to lower case. for example, f (a)i,j gives element (i, j)
    example notation for type and shape:
of the matrix computed by applying the function f to a.
    tensors: in some cases we will need an array with more than two axes. in
the general case, an array of numbers arranged on a regular grid with a
variable number of axes is known as a tensor. we denote a tensor named    a   
with this typeface: a. we identify the element of a at coordinates (i, j, k)
by writing ai,j,k.

    a1,1 a1,2
a2,1 a2,2   .
column

(goodfellow 2016)

(2.2)

tensors

    a tensor is an array of numbers, that may have 

    zero dimensions, and be a scalar 

    one dimension, and be a vector 

    two dimensions, and be a matrix 

    or more dimensions.

(goodfellow 2016)

by writing ai,j,k.

matrix product operations have many useful properties that make mathematical
analysis of matrices more convenient. for example, id127 is

one important operation on matrices is the transpose. the transpose of a
matrix is the mirror image of the matrix across a diagonal line, called the main
diagonal, running down and to the right, starting from its upper left corner. see
fig. 2.1 for a graphical depiction of this operation. we denote the transpose of a
matrix a as a>, and it is de   ned such that

matrix transpose

a(b + c) = ab + ac.

chapter 2. id202

(2.6)

(a>)i,j = aj,i.

(2.3)

(2.7)
id127 is not commutative (the condition ab = ba does not
always hold), unlike scalar multiplication. however, the dot product between two
vectors is commutative:

vectors can be thought of as matrices that contain only one column. the
transpose of a vector is therefore a matrix with only one row. sometimes we

35 ) a> =    a1,1 a2,1 a3,1
a1,2 a2,2 a3,2  

a(bc) = (ab)c.

a1,1 a1,2
a2,1 a2,2
a3,1 a3,2

a =24

33

(2.8)
figure 2.1: the transpose of the matrix can be thought of as a mirror image across the
main diagonal.

x>y = y>x.

the transpose of a matrix product has a simple form:

the i-th column of a. when we need to explicitly identify the elements of a
(2.9)
matrix, we write them as an array enclosed in square brackets:

(ab)> = b>a>.

(2.2)
this allows us to demonstrate eq. 2.8, by exploiting the fact that the value of
(goodfellow 2016)
such a product is a scalar and therefore equal to its own transpose:

    a1,1 a1,2
a2,1 a2,2   .

for this product to be de   ned, a must have the same number of columns as b has
matrices. the matrix product of matrices a and b is a third matrix c. in order
rows. if a is of shape m     n and b is of shape n     p, then c is of shape m     p.
for this product to be de   ned, a must have the same number of columns as b has
we can write the matrix product just by placing two or more matrices together,
matrix (dot) product
rows. if a is of shape m     n and b is of shape n     p, then c is of shape m     p.
we can write the matrix product just by placing two or more matrices together,
(2.4)
(2.4)

the product operation is de   ned by

c = ab.

c = ab.

the product operation is de   ned by

ai,kbk,j.

(2.5)

ai,kbk,j.

(2.5)
note that the standard product of two matrices is not just a matrix containing
the product of the individual elements. such an operation exists and is called the
element-wise product or hadamard product, and is denoted as a   b.
note that the standard product of two matrices is not just a matrix containing
   
n
the product of the individual elements. such an operation exists and is called the
the dot product between two vectors x and y of the same dimensionality is the
matrix product x>y. we can think of the matrix product c = ab as computing
element-wise product or hadamard product, and is denoted as a   b.
p
ci,j as the dot product between row i of a and column j of b.
the dot product between two vectors x and y of the same dimensionality is the
n
matrix product x>y. we can think of the matrix product c = ab as computing
ci,j as the dot product between row i of a and column j of b.

must 
match

=

m

m

p

34

ci,j =xk
ci,j =xk

34

(goodfellow 2016)

2.3 identity and inverse matrices

chapter 2. id202

identity matrix

id202 o   ers a powerful tool called matrix inversion that allows us to
analytically solve eq. 2.11 for many values of a.

to describe matrix inversion, we    rst need to de   ne the concept of an identity
matrix. an identity matrix is a matrix that does not change any vector when we
multiply that vector by that matrix. we denote the identity matrix that preserves
n-dimensional vectors as in. formally, in 2 rn   n, and

figure 2.2: example identity matrix: this is i3.

a2,1x1 + a2,2x2 +        + a2,nxn = b2

24

1 0 0
0 1 0
0 0 1

35

8x 2 rn, inx = x.

. . .

am,1x1 + am,2x2 +        + am,nxn = bm.

the structure of the identity matrix is simple: all of the entries along the main
diagonal are 1, while all of the other entries are zero. see fig. 2.2 for an example.
the matrix inverse of a is denoted as a 1, and it is de   ned as the matrix

matrix-vector product notation provides a more compact representation for

equations of this form.

(goodfellow 2016)

(2.20)

(2.17)
(2.18)
(2.19)

the reader should be aware that many more exist.

we now know enough id202 notation to write down a system of linear

develop a comprehensive list of useful properties of the matrix product here, but
the reader should be aware that many more exist.

we now know enough id202 notation to write down a system of linear

systems of equations
(2.11)
where a 2 rm   n is a known matrix, b 2 rm is a known vector, and x 2 rn is a
vector of unknown variables we would like to solve for. each element xi of x is one
of these unknown variables. each row of a and each element of b provide another
constraint. we can rewrite eq. 2.11 as:

(2.11)
where a 2 rm   n is a known matrix, b 2 rm is a known vector, and x 2 rn is a
vector of unknown variables we would like to solve for. each element xi of x is one
of these unknown variables. each row of a and each element of b provide another
constraint. we can rewrite eq. 2.11 as:

expands to

ax = b

a1,:x = b1

a2,:x = b2

ax = b

(2.12)

. . .
a1,:x = b1

am,:x = bm

or, even more explicitly, as:

a2,:x = b2

a1,1x1 + a1,2x2 +        + a1,nxn = b1

. . .

am,:x = bm

35

(2.13)
(2.14)
(2.15)

(2.16)

(2.12)

(2.13)
(2.14)
(2.15)

(goodfellow 2016)

solving systems of equations

    a linear system of equations can have: 

    no solution 

    many solutions 

    exactly one solution: this means multiplication by 

the matrix is an invertible function

(goodfellow 2016)

8x 2 rn, inx = x.

(2.20)
the structure of the identity matrix is simple: all of the entries along the main
the structure of the identity matrix is simple: all of the entries along the main
diagonal are 1, while all of the other entries are zero. see fig. 2.2 for an example.
diagonal are 1, while all of the other entries are zero. see fig. 2.2 for an example.
the matrix inverse of a is denoted as a 1, and it is de   ned as the matrix
the matrix inverse of a is denoted as a 1, and it is de   ned as the matrix
(2.21)
(2.21)

8x 2 rn, inx = x.
matrix inversion

    matrix inverse: 

a 1a = in.

a 1a = in.

we can now solve eq. 2.11 by the following steps:

    solving a system using an inverse: 

we can now solve eq. 2.11 by the following steps:

ax = b

ax = b
a 1ax = a 1b

a 1ax = a 1b
inx = a 1b

    numerically unstable, but useful for abstract 

inx = a 1b

36

analysis

36

(2.22)
(2.22)
(2.23)
(2.23)
(2.24)
(2.24)

(goodfellow 2016)

invertibility

    matrix can   t be inverted if    

    more rows than columns 

    more columns than rows 

    redundant rows/columns (   linearly dependent   , 

   low rank   )

(goodfellow 2016)

measure the size of vectors using a function called a norm. formally, the lp norm
is given by

norms

||x||p = xi

p

|xi|p! 1

for p 2 r, p   1.
    functions that measure how    large    a vector is 
norms, including the lp norm, are functions mapping vectors to non-negative
values. on an intuitive level, the norm of a vector x measures the distance from
    similar to a distance between zero and the point 
the origin to the point x. more rigorously, a norm is any function f that satis   es
represented by the vector
the following properties:

    f (x) = 0 ) x = 0
    f (x + y)     f (x) + f (y) (the triangle inequality)
    8    2 r, f (   x) = |   |f (x)
the l2 norm, with p = 2, is known as the euclidean norm. it is simply the
euclidean distance from the origin to the point identi   ed by x. the l2 norm is
used so frequently in machine learning that it is often denoted simply as ||x||, with

(goodfellow 2016)

|xi|.

norms
||x||1 =xi
|xi|p! 1

||x||p = xi

sometimes we need to measure the size of a vector. in machine learning, we usually
measure the size of vectors using a function called a norm. formally, the lp norm

(2.31)

zero and elements that are small but nonzero. in these cases, we turn to a function
that grows at the same rate in all locations, but retains mathematical simplicity:
the l1 norm. the l1 norm may be simpli   ed to

chapter 2. id202

    lp norm 
the l1 norm is commonly used in machine learning when the di   erence between
zero and nonzero elements is very important. every time an element of x moves
away from 0 by    , the l1 norm increases by    .

p

in several machine learning
because it increases very slowly near the origin.
applications, it is important to discriminate between elements that are exactly
zero and elements that are small but nonzero. in these cases, we turn to a function
that grows at the same rate in all locations, but retains mathematical simplicity:
the l1 norm. the l1 norm may be simpli   ed to

we sometimes measure the size of the vector by counting its number of nonzero
elements. some authors refer to this function as the    l0 norm,    but this is incorrect
terminology. the number of non-zero entries in a vector is not a norm, because
scaling the vector by     does not change the number of nonzero entries. the l1
    most popular norm: l2 norm, p=2 
norm is often used as a substitute for the number of nonzero entries.
one other norm that commonly arises in machine learning is the l1 norm,
    l1 norm, p=1:   
also known as the max norm. this norm simpli   es to the absolute value of the
element with the largest magnitude in the vector,
    max norm, in   nite p:

norms, including the lp norm, are functions mapping vectors to non-negative
values. on an intuitive level, the norm of a vector x measures the distance from
the origin to the point x. more rigorously, a norm is any function f that satis   es
the following properties:

the l1 norm is commonly used in machine learning when the di   erence between
zero and nonzero elements is very important. every time an element of x moves
away from 0 by    , the l1 norm increases by    .

||x||1 =xi

||x||1 = max

(2.32)

(2.31)

|xi|.

|xi|.

i

sometimes we may also wish to measure the size of a matrix. in the context
of deep learning, the most common way to do this is with the otherwise obscure
frobenius norm

we sometimes measure the size of the vector by counting its number of nonzero
elements. some authors refer to this function as the    l0 norm,    but this is incorrect
terminology. the number of non-zero entries in a vector is not a norm, because

(goodfellow 2016)

(2.33)

    f (x) = 0 ) x = 0

||a||f =sxi,j

a2

i,j,

a = a>.

a unit vector is a vector with unit norm:
||x||2 = 1.

special matrices and vectors
    unit vector: 
a unit vector is a vector with unit norm:
||x||2 = 1.

and in that case, diag(v) 1 = diag([1/v1, . . . , 1/vn]>). in many cases, we may
(2.35)
derive some very general machine learning algorithm in terms of arbitrary matrices,
symmetric matrices often arise when the entries are generated by some function of
two arguments that does not depend on the order of the arguments. for example,
but obtain a less expensive (and less descriptive) algorithm by restricting some
symmetric matrices often arise when the entries are generated by some function of
if a is a matrix of distance measurements, with ai,j giving the distance from point
two arguments that does not depend on the order of the arguments. for example,
i to point j, then ai,j = aj,i because distance functions are symmetric.
if a is a matrix of distance measurements, with ai,j giving the distance from point
not all diagonal matrices need be square. it is possible to construct a rectangular
i to point j, then ai,j = aj,i because distance functions are symmetric.
diagonal matrix. non-square diagonal matrices do not have inverses but it is still
possible to multiply by them cheaply. for a non-square diagonal matrix d, the
(2.36)
product dx will involve scaling each element of x, and either concatenating some
(2.36)
zeros to the result if d is taller than it is wide, or discarding some of the last
a vector x and a vector y are orthogonal to each other if x>y = 0. if both
elements of the vector if d is wider than it is tall.
    symmetric matrix: 
vectors have nonzero norm, this means that they are at a 90 degree angle to each
a vector x and a vector y are orthogonal to each other if x>y = 0. if both
other. in rn, at most n vectors may be mutually orthogonal with nonzero norm.
vectors have nonzero norm, this means that they are at a 90 degree angle to each
if the vectors are not only orthogonal but also have unit norm, we call them
(2.35)
other. in rn, at most n vectors may be mutually orthogonal with nonzero norm.
chapter 2. id202
if the vectors are not only orthogonal but also have unit norm, we call them
symmetric matrices often arise when the entries are generated by some function of
an orthogonal matrix is a square matrix whose rows are mutually orthonormal
two arguments that does not depend on the order of the arguments. for example,
if a is a matrix of distance measurements, with ai,j giving the distance from point
an orthogonal matrix is a square matrix whose rows are mutually orthonormal
(2.37)
i to point j, then ai,j = aj,i because distance functions are symmetric.
(2.38)
(2.37)
so orthogonal matrices are of interest because their inverse is very cheap to compute.

a>a = aa> = i.
and whose columns are mutually orthonormal:
a 1 = a>,

a symmetric matrix is any matrix that is equal to its own transpose:

41
a unit vector is a vector with unit norm:

    orthogonal matrix:
and whose columns are mutually orthonormal:

a>a = aa> = i.

a = a>.

(goodfellow 2016)

show us information about their functional properties that is not obvious from the
representation of the matrix as an array of elements.

one of the most widely used kinds of matrix decomposition is called eigen-
decomposition, in which we decompose a matrix into a set of eigenvectors and

eigendecomposition

figure 2.3: an example of the e   ect of eigenvectors and eigenvalues. here, we have
a matrix a with two orthonormal eigenvectors, v(1) with eigenvalue  1 and v(2) with
eigenvalue  2. (left) we plot the set of all unit vectors u 2 r2 as a unit circle. (right)
we plot the set of all points au. by observing the way that a distorts the unit circle, we
can see that it scales space in direction v(i) by  i.

an eigenvector of a square matrix a is a non-zero vector v such that multipli-
chapter 2. id202

cation by a alters only the scale of v:

    eigenvector and eigenvalue: 

av =  v.

a = v diag( )v  1.

    eigendecomposition of a diagonalizable matrix: 

eigenvectors to form a matrix v with one eigenvector per column: v = [v(1), . . . ,
v(n)]. likewise, we can concatenate the eigenvalues to form a vector   = [ 1, . . . ,
 n]>. the eigendecomposition of a is then given by

(2.39)
cases, the decomposition exists, but may involve complex rather than real numbers.
fortunately, in this book, we usually need to decompose only a speci   c class of
the scalar   is known as the eigenvalue corresponding to this eigenvector. (one
matrices that have a simple decomposition. speci   cally, every real symmetric
    every real symmetric matrix has a real, orthogonal 
can also    nd a left eigenvector such that v>a =  v>, but we are usually concerned
matrix can be decomposed into an expression using only real-valued eigenvectors
with right eigenvectors).

we have seen that constructing matrices with speci   c eigenvalues and eigenvec-
tors allows us to stretch space in desired directions. however, we often want to
decompose matrices into their eigenvalues and eigenvectors. doing so can help us
to analyze certain properties of the matrix, much as decomposing an integer into
its prime factors can help us understand the behavior of that integer.

(2.41)
if v is an eigenvector of a, then so is any rescaled vector sv for s 2 r, s 6= 0.
moreover, sv still has the same eigenvalue. for this reason, we usually only look
where q is an orthogonal matrix composed of eigenvectors of a, and     is a
diagonal matrix. the eigenvalue    i,i is associated with the eigenvector in column i
of q, denoted as q:,i. because q is an orthogonal matrix, we can think of a as
suppose that a matrix a has n linearly independent eigenvectors, {v(1), . . . ,

not every matrix can be decomposed into eigenvalues and eigenvectors. in some

eigendecomposition:

a = q   q>,

(2.40)

(goodfellow 2016)

e   ect of eigenvalues

(goodfellow 2016)

   3   2   10123x(   3   2   10123x1v(1)v(1)before multiplication   3   2   10123x   (   3   2   10123x   1v(1)  1v(1)v(1)  1v(1)after multiplicationchapter 2. id202

singular value decomposition

more generally applicable. every real matrix has a singular value decomposition,
but the same is not true of the eigenvalue decomposition. for example, if a matrix
is not square, the eigendecomposition is not de   ned, and we must use a singular
value decomposition instead.

recall that the eigendecomposition involves analyzing a matrix a to discover
a matrix v of eigenvectors and a vector of eigenvalues   such that we can rewrite

    similar to eigendecomposition 

a = v diag( )v  1.

(2.42)

the singular value decomposition is similar, except this time we will write a

    more general; matrix need not be square

as a product of three matrices:

a = u dv >.

(2.43)

d to be an m     n matrix, and v to be an n     n matrix.

suppose that a is an m    n matrix. then u is de   ned to be an m    m matrix,
each of these matrices is de   ned to have a special structure. the matrices u

(goodfellow 2016)

(2.47)

a+ = v d+u>,

practical algorithms for computing the pseudoinverse are not based on this de   ni-
tion, but rather the formula

where u, d and v are the singular value decomposition of a, and the pseudoinverse
d+ of a diagonal matrix d is obtained by taking the reciprocal of its non-zero
elements then taking the transpose of the resulting matrix.

when a has more columns than rows, then solving a linear equation using the
moore-penrose pseudoinverse
pseudoinverse provides one of the many possible solutions. speci   cally, it provides
the solution x = a+y with minimal euclidean norm ||x||2 among all possible
solutions.
    if the equation has: 
when a has more rows than columns, it is possible for there to be no solution.
in this case, using the pseudoinverse gives us the x for which ax is as close as
possible to y in terms of euclidean norm ||ax   y||2.

when a has more columns than rows, then solving a linear equation using the
pseudoinverse provides one of the many possible solutions. speci   cally, it provides
the solution x = a+y with minimal euclidean norm ||x||2 among all possible
when a has more rows than columns, it is possible for there to be no solution.
in this case, using the pseudoinverse gives us the x for which ax is as close as
possible to y in terms of euclidean norm ||ax   y||2.

    exactly one solution: this is the same as the inverse. 

    many solutions: this gives us the solution with the 

    no solution: this gives us the solution with the 

smallest error 

2.10 the trace operator

smallest norm of x.

the trace operator gives the sum of all of the diagonal entries of a matrix:

2.10 the trace operator

(goodfellow 2016)

tr(a) =xi

cases. the pseudoinverse of a is de   ned as a matrix

the moore-penrose pseudoinverse allows us to make some headway in these
computing the pseudoinverse
(2.46)

(a>a +    i) 1a>.

a+ = lim
   &0

practical algorithms for computing the pseudoinverse are not based on this de   ni-
the svd allows the computation of the pseudoinverse:
tion, but rather the formula

a+ = v d+u>,

(2.47)

where u, d and v are the singular value decomposition of a, and the pseudoinverse
d+ of a diagonal matrix d is obtained by taking the reciprocal of its non-zero
elements then taking the transpose of the resulting matrix.

take reciprocal of non-zero entries

when a has more columns than rows, then solving a linear equation using the
pseudoinverse provides one of the many possible solutions. speci   cally, it provides
the solution x = a+y with minimal euclidean norm ||x||2 among all possible

(goodfellow 2016)

2.10 the trace operator

manipulate the expression using many useful identities. for example, the trace
operator is invariant to the transpose operator:

the trace operator gives the sum of all of the diagonal entries of a matrix:

tr(a) = tr(a>).

(2.50)

the trace of a square matrix composed of many factors is also invariant to
(2.48)
moving the last factor into the    rst position, if the shapes of the corresponding
matrices allow the resulting product to be de   ned:

ai,i.

tr(a) =xi

the trace operator is useful for a variety of reasons. some operations that are
(2.51)
di   cult to specify without resorting to summation notation can be speci   ed using

tr(abc) = tr(cab) = tr(bca)

trace

tr(

nyi=1

46
f (i)) = tr(f (n)

n 1yi=1

f (i)).

(2.52)

this invariance to cyclic permutation holds even if the resulting product has a
di   erent shape. for example, for a 2 rm   n and b 2 rn   m, we have

(goodfellow 2016)

tr(ab) = tr(ba)

(2.53)

learning id202

    do a lot of practice problems 

    start out with lots of summation signs and indexing 

into individual entries 

    eventually you will be able to mostly use matrix 

and vector product notation quickly and easily

(goodfellow 2016)

