   #[1]wildml    feed [2]wildml    comments feed [3]wildml    implementing a
   neural network from scratch in python     an introduction comments feed
   [4]speeding up your neural network with theano and the gpu [5]alternate
   [6]alternate

   [7]skip to content

   [8]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [9]home
     * [10]ai newsletter
     * [11]deep learning glossary
     * [12]contact
     * [13]about

   posted on [14]september 3, 2015january 10, 2016 by [15]denny britz

implementing a neural network from scratch in python     an introduction

   [16]get the code: to follow along, all the code is also available as an
   ipython notebook on github.

   in this post we will implement a simple 3-layer neural network from
   scratch. we won   t derive all the math that   s required, but i will try
   to give an intuitive explanation of what we are doing. i will also
   point to resources for you read up on the details.

   here i   m assuming that you are familiar with basic calculus and machine
   learning concepts, e.g. you know what classification and id173
   is. ideally you also know a bit about how optimization techniques like
   id119 work. but even if you   re not familiar with any of the
   above this post could still turn out to be interesting ;)

   but why implement a neural network from scratch at all? even if you
   plan on using neural network libraries like [17]pybrain in the future,
   implementing a network from scratch at least once is an extremely
   valuable exercise. it helps you gain an understanding of how neural
   networks work, and that is essential for designing effective models.

   one thing to note is that the code examples here aren   t terribly
   efficient. they are meant to be easy to understand. in an upcoming post
   i will explore how to write an efficient neural network implementation
   using [18]theano. (update: [19]now available)

generating a dataset

   let   s start by generating a dataset we can play with. fortunately,
   [20]scikit-learn has some useful dataset generators, so we don   t need
   to write the code ourselves. we will go with the [21]make_moons
   function.

   1
   2
   3
   4
   # generate a dataset and plot it
   np.random.seed(0)
   x, y = sklearn.datasets.make_moons(200, noise=0.20)
   plt.scatter(x[:,0], x[:,1], s=40, c=y, cmap=plt.cm.spectral)

   [22]a moon-shaped dataset with two classes.

   the dataset we generated has two classes, plotted as red and blue
   points. you can think of the blue dots as male patients and the red
   dots as female patients, with the x- and y- axis being medical
   measurements.

   our goal is to train a machine learning classifier that predicts the
   correct class (male of female) given the x- and y- coordinates. note
   that the data is not linearly separable, we can   t draw a straight line
   that separates the two classes. this means that linear classifiers,
   such as id28, won   t be able to fit the data unless you
   hand-engineer non-linear features (such as polynomials) that work well
   for the given dataset.

   in fact, that   s one of the major advantages of neural networks. you
   don   t need to worry about [23]feature engineering. the hidden layer of
   a neural network will learn features for you.

id28

   to demonstrate the point let   s train a id28 classifier.
   it   s input will be the x- and y-values and the output the predicted
   class (0 or 1). to make our life easy we use the id28
   class from scikit-learn.
   1
   2
   3
   4
   5
   6
   7
   # train the logistic rgeression classifier
   clf = sklearn.linear_model.logisticregressioncv()
   clf.fit(x, y)

   # plot the decision boundary
   plot_decision_boundary(lambda x: clf.predict(x))
   plt.title(&quot;id28&quot;)

   [24]id75 decision boundary

   the graph shows the decision boundary learned by our logistic
   regression classifier. it separates the data as good as it can using a
   straight line, but it   s unable to capture the    moon shape    of our data.

training a neural network

   let   s now build a 3-layer neural network with one input layer, one
   hidden layer, and one output layer. the number of nodes in the input
   layer is determined by the dimensionality of our data, 2. similarly,
   the number of nodes in the output layer is determined by the number of
   classes we have, also 2. (because we only have 2 classes we could
   actually get away with only one output node predicting 0 or 1, but
   having 2 makes it easier to extend the network to more classes later
   on). the input to the network will be x- and y- coordinates and its
   output will be two probabilities, one for class 0 (   female   ) and one
   for class 1 (   male   ). it looks something like this:

   [25]3-layer neural network diagram

   we can choose the dimensionality (the number of nodes) of the hidden
   layer. the more nodes we put into the hidden layer the more complex
   functions we will be able fit. but higher dimensionality comes at a
   cost. first, more computation is required to make predictions and learn
   the network parameters. a bigger number of parameters also means we
   become more prone to overfitting our data.

   how to choose the size of the hidden layer? while there are some
   general guidelines and recommendations, it always depends on your
   specific problem and is more of an art than a science. we will play
   with the number of nodes in the hidden later later on and see how it
   affects our output.

   we also need to pick an activation function for our hidden layer. the
   activation function transforms the inputs of the layer into its
   outputs. a nonlinear activation function is what allows us to fit
   nonlinear hypotheses. common chocies for id180 are
   [26]tanh, the [27]sigmoid function, or [28]relus. we will use tanh,
   which performs quite well in many scenarios. a nice property of these
   functions is that their derivate can be computed using the original
   function value. for example, the derivative of  \tanh x is 1-\tanh^2 x
   . this is useful because it allows us to compute \tanh x once and
   re-use its value later on to get the derivative.

   because we want our network to output probabilities the activation
   function for the output layer will be the [29]softmax, which is simply
   a way to convert raw scores to probabilities. if you   re familiar with
   the logistic function you can think of softmax as its generalization to
   multiple classes.

how our network makes predictions

   our network makes predictions using forward propagation, which is just
   a bunch of id127s and the application of the activation
   function(s) we defined above. if x is the 2-dimensional input to our
   network then we calculate our prediction \hat{y} (also two-dimensional)
   as follows:

   \begin{aligned} z_1 & = xw_1 + b_1 \\ a_1 & = \tanh(z_1) \\ z_2 & =
   a_1w_2 + b_2 \\ a_2 & = \hat{y} = \mathrm{softmax}(z_2) \end{aligned}

   z_i is the input of layer i and a_i is the output of layer i after
   applying the activation function. w_1, b_1, w_2, b_2 are parameters of
   our network, which we need to learn from our training data. you can
   think of them as matrices transforming data between layers of the
   network. looking at the id127s above we can figure out
   the dimensionality of these matrices. if we use 500 nodes for our
   hidden layer then w_1 \in \mathbb{r}^{2\times500} , b_1 \in
   \mathbb{r}^{500} , w_2 \in \mathbb{r}^{500\times2} , b_2 \in
   \mathbb{r}^{2} . now you see why we have more parameters if we increase
   the size of the hidden layer.

learning the parameters

   learning the parameters for our networid116 finding parameters ( w_1,
   b_1, w_2, b_2 ) that minimize the error on our training data. but how
   do we define the error? we call the function that measures our error
   the id168. a common choice with the softmax output is the
   categorical [30]cross-id178 loss (also known as negative log
   likelihood). if we have n training examples and c classes then the loss
   for our prediction \hat{y} with respect to the true labels y is given
   by:

   \begin{aligned} l(y,\hat{y}) = - \frac{1}{n} \sum_{n \in n} \sum_{i \in
   c} y_{n,i} \log\hat{y}_{n,i} \end{aligned}

   the formula looks complicated, but all it really does is sum over our
   training examples and add to the loss if we predicted the incorrect
   class. the further away the two id203 distributions y (the
   correct labels) and \hat{y} (our predictions) are, the greater our loss
   will be. by finding parameters that minimize the loss we maximize the
   likelihood of our training data.

   we can use [31]id119 to find the minimum and i will
   implement the most vanilla version of id119, also called
   batch id119 with a fixed learning rate. variations such as
   sgd (stochastic id119) or minibatch id119
   typically perform better in practice. so if you are serious you   ll want
   to use one of these, and ideally you would also [32]decay the learning
   rate over time.

   as an input, id119 needs the gradients (vector of
   derivatives) of the id168 with respect to our parameters:
   \frac{\partial{l}}{\partial{w_1}} , \frac{\partial{l}}{\partial{b_1}} ,
   \frac{\partial{l}}{\partial{w_2}} , \frac{\partial{l}}{\partial{b_2}} .
   to calculate these gradients we use the famous id26
   algorithm, which is a way to efficiently calculate the gradients
   starting from the output. i won   t go into detail how id26
   works, but there are many excellent explanations ([33]here or [34]here)
   floating around the web.

   applying the id26 formula we find the following (trust me on
   this):

   \begin{aligned} & \delta_3 = \hat{y} - y \\ & \delta_2 = (1 - \tanh^2
   z_1) \circ \delta_3w_2^t \\ & \frac{\partial{l}}{\partial{w_2}} = a_1^t
   \delta_3 \\ & \frac{\partial{l}}{\partial{b_2}} = \delta_3\\ &
   \frac{\partial{l}}{\partial{w_1}} = x^t \delta2\\ &
   \frac{\partial{l}}{\partial{b_1}} = \delta2 \\ \end{aligned}

implementation

   now we are ready for our implementation. we start by defining some
   useful variables and parameters for id119:
   1
   2
   3
   4
   5
   6
   7
   num_examples = len(x) # training set size
   nn_input_dim = 2 # input layer dimensionality
   nn_output_dim = 2 # output layer dimensionality

   # id119 parameters (i picked these by hand)
   epsilon = 0.01 # learning rate for id119
   reg_lambda = 0.01 # id173 strength

   first let   s implement the id168 we defined above. we use this
   to evaluate how well our model is doing:
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   # helper function to evaluate the total loss on the dataset
   def calculate_loss(model):
       w1, b1, w2, b2 = model['w1'], model['b1'], model['w2'], model['b2']
       # forward propagation to calculate our predictions
       z1 = x.dot(w1) + b1
       a1 = np.tanh(z1)
       z2 = a1.dot(w2) + b2
       exp_scores = np.exp(z2)
       probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)
       # calculating the loss
       corect_logprobs = -np.log(probs[range(num_examples), y])
       data_loss = np.sum(corect_logprobs)
       # add regulatization term to loss (optional)
       data_loss += reg_lambda/2 * (np.sum(np.square(w1)) +
   np.sum(np.square(w2)))
       return 1./num_examples * data_loss

   we also implement a helper function to calculate the output of the
   network. it does forward propagation as defined above and returns the
   class with the highest id203.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   # helper function to predict an output (0 or 1)
   def predict(model, x):
       w1, b1, w2, b2 = model['w1'], model['b1'], model['w2'], model['b2']
       # forward propagation
       z1 = x.dot(w1) + b1
       a1 = np.tanh(z1)
       z2 = a1.dot(w2) + b2
       exp_scores = np.exp(z2)
       probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)
       return np.argmax(probs, axis=1)

   finally, here comes the function to train our neural network. it
   implements batch id119 using the id26 derivates
   we found above.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   37
   38
   39
   40
   41
   42
   43
   44
   45
   46
   47
   48
   49
   50
   51
   52
   53
   54
   # this function learns parameters for the neural network and returns
   the model.
   # - nn_hdim: number of nodes in the hidden layer
   # - num_passes: number of passes through the training data for gradient
   descent
   # - print_loss: if true, print the loss every 1000 iterations
   def build_model(nn_hdim, num_passes=20000, print_loss=false):

       # initialize the parameters to random values. we need to learn
   these.
       np.random.seed(0)
       w1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)
       b1 = np.zeros((1, nn_hdim))
       w2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)
       b2 = np.zeros((1, nn_output_dim))

       # this is what we return at the end
       model = {}

       # id119. for each batch...
       for i in xrange(0, num_passes):

           # forward propagation
           z1 = x.dot(w1) + b1
           a1 = np.tanh(z1)
           z2 = a1.dot(w2) + b2
           exp_scores = np.exp(z2)
           probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)

           # id26
           delta3 = probs
           delta3[range(num_examples), y] -= 1
           dw2 = (a1.t).dot(delta3)
           db2 = np.sum(delta3, axis=0, keepdims=true)
           delta2 = delta3.dot(w2.t) * (1 - np.power(a1, 2))
           dw1 = np.dot(x.t, delta2)
           db1 = np.sum(delta2, axis=0)

           # add id173 terms (b1 and b2 don't have id173
   terms)
           dw2 += reg_lambda * w2
           dw1 += reg_lambda * w1

           # id119 parameter update
           w1 += -epsilon * dw1
           b1 += -epsilon * db1
           w2 += -epsilon * dw2
           b2 += -epsilon * db2

           # assign new parameters to the model
           model = { 'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}

           # optionally print the loss.
           # this is expensive because it uses the whole dataset, so we
   don't want to do it too often.
           if print_loss and i % 1000 == 0:
             print &quot;loss after iteration %i: %f&quot; %(i,
   calculate_loss(model))

       return model

a network with a hidden layer of size 3

   let   s see what happens if we train a network with a hidden layer size
   of 3.
   1
   2
   3
   4
   5
   6
   # build a model with a 3-dimensional hidden layer
   model = build_model(3, print_loss=true)

   # plot the decision boundary
   plot_decision_boundary(lambda x: predict(model, x))
   plt.title(&quot;decision boundary for hidden layer size 3&quot;)

   [35]neural network decision boundary with hidden layer size 3

   yay! this looks pretty good. our neural networks was able to find a
   decision boundary that successfully separates the classes.

varying the hidden layer size

   in the example above we picked a hidden layer size of 3. let   s now get
   a sense of how varying the hidden layer size affects the result.
   1
   2
   3
   4
   5
   6
   7
   8
   plt.figure(figsize=(16, 32))
   hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]
   for i, nn_hdim in enumerate(hidden_layer_dimensions):
       plt.subplot(5, 2, i+1)
       plt.title('hidden layer size %d' % nn_hdim)
       model = build_model(nn_hdim)
       plot_decision_boundary(lambda x: predict(model, x))
   plt.show()

   [36]neural network decision boundaries with varying hidden layer size

   we can see that a hidden layer of low dimensionality nicely captures
   the general trend of our data. higher dimensionalities are prone to
   overfitting. they are    memorizing    the data as opposed to fitting the
   general shape. if we were to evaluate our model on a separate test set
   (and you should!) the model with a smaller hidden layer size would
   likely perform better due to better generalization. we could counteract
   overfitting with stronger id173, but picking the a correct
   size for hidden layer is a much more    economical    solution.

exercises

   here are some things you can try to become more familiar with the code:
    1. instead of batch id119, use minibatch id119
       ([37]more info) to train the network. minibatch id119
       typically performs better in practice.
    2. we used a fixed learning rate \epsilon for id119.
       implement an annealing schedule for the id119 learning
       rate ([38]more info).
    3. we used a \tanh activation function for our hidden layer.
       experiment with other id180 (some are mentioned
       above). note that changing the activation function also means
       changing the id26 derivative.
    4. extend the network from two to three classes. you will need to
       generate an appropriate dataset for this.
    5. extend the network to four layers. experiment with the layer size.
       adding another hidden layer means you will need to adjust both the
       forward propagation as well as the id26 code.

   [39]all of the code is available as an ipython notebook on
   github. please leave questions or feedback in the comments!


   categories[40]deep learning, [41]neural networks

post navigation

   [42]next postnext speeding up your neural network with theano and the
   gpu

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [43]introduction to learning to trade with id23
     * [44]ai and deep learning in 2017     a year in review
     * [45]hype or not? some perspective on openai   s dota 2 bot
     * [46]learning id23 (with code, exercises and
       solutions)
     * [47]id56s in tensorflow, a practical guide and undocumented features
     * [48]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [49]deep learning for chatbots, part 1     introduction
     * [50]attention and memory in deep learning and nlp

archives

     * [51]february 2018
     * [52]december 2017
     * [53]august 2017
     * [54]october 2016
     * [55]august 2016
     * [56]july 2016
     * [57]april 2016
     * [58]january 2016
     * [59]december 2015
     * [60]november 2015
     * [61]october 2015
     * [62]september 2015

categories

     * [63]conversational agents
     * [64]convolutional neural networks
     * [65]deep learning
     * [66]gpu
     * [67]id38
     * [68]memory
     * [69]neural networks
     * [70]news
     * [71]nlp
     * [72]recurrent neural networks
     * [73]id23
     * [74]id56s
     * [75]tensorflow
     * [76]trading
     * [77]uncategorized

meta

     * [78]log in
     * [79]entries rss
     * [80]comments rss
     * [81]wordpress.org

   [82]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/feed/
   4. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
   5. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/&format=xml
   7. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/#content
   8. http://www.wildml.com/
   9. http://www.wildml.com/
  10. https://www.getrevue.co/profile/wildml
  11. http://www.wildml.com/deep-learning-glossary/
  12. mailto:dennybritz@gmail.com
  13. http://www.wildml.com/about/
  14. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  15. http://www.wildml.com/author/dennybritz/
  16. https://github.com/dennybritz/nn-from-scratch
  17. http://pybrain.org/
  18. http://deeplearning.net/software/theano/
  19. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
  20. http://scikit-learn.org/
  21. http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html
  22. http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-dataset.png
  23. http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/
  24. http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-lr-decision-boundary.png
  25. http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-3-layer-network.png
  26. https://reference.wolfram.com/language/ref/tanh.html
  27. https://en.wikipedia.org/wiki/sigmoid_function
  28. https://en.wikipedia.org/wiki/rectifier_(neural_networks
  29. https://en.wikipedia.org/wiki/softmax_function
  30. https://en.wikipedia.org/wiki/cross_id178#cross-id178_error_function_and_logistic_regression
  31. http://cs231n.github.io/optimization-1/
  32. http://cs231n.github.io/neural-networks-3/#anneal
  33. http://colah.github.io/posts/2015-08-backprop/
  34. http://cs231n.github.io/optimization-2/
  35. http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-h3.png
  36. http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-hidden-layer-varying.png
  37. http://cs231n.github.io/optimization-1/#gd
  38. http://cs231n.github.io/neural-networks-3/#anneal
  39. https://github.com/dennybritz/nn-from-scratch
  40. http://www.wildml.com/category/deep-learning/
  41. http://www.wildml.com/category/neural-networks/
  42. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
  43. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  44. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  45. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  46. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  47. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  48. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  49. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  50. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  51. http://www.wildml.com/2018/02/
  52. http://www.wildml.com/2017/12/
  53. http://www.wildml.com/2017/08/
  54. http://www.wildml.com/2016/10/
  55. http://www.wildml.com/2016/08/
  56. http://www.wildml.com/2016/07/
  57. http://www.wildml.com/2016/04/
  58. http://www.wildml.com/2016/01/
  59. http://www.wildml.com/2015/12/
  60. http://www.wildml.com/2015/11/
  61. http://www.wildml.com/2015/10/
  62. http://www.wildml.com/2015/09/
  63. http://www.wildml.com/category/conversational-agents/
  64. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  65. http://www.wildml.com/category/deep-learning/
  66. http://www.wildml.com/category/gpu/
  67. http://www.wildml.com/category/language-modeling/
  68. http://www.wildml.com/category/memory/
  69. http://www.wildml.com/category/neural-networks/
  70. http://www.wildml.com/category/news/
  71. http://www.wildml.com/category/nlp/
  72. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  73. http://www.wildml.com/category/reinforcement-learning/
  74. http://www.wildml.com/category/id56s/
  75. http://www.wildml.com/category/tensorflow/
  76. http://www.wildml.com/category/trading/
  77. http://www.wildml.com/category/uncategorized/
  78. http://www.wildml.com/wp-login.php
  79. http://www.wildml.com/feed/
  80. http://www.wildml.com/comments/feed/
  81. https://wordpress.org/
  82. https://wordpress.org/
