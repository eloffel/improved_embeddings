   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

a neural network in 11 lines of python (part 1)

a bare bones neural network implementation to describe the inner workings of
id26.

   posted by iamtrask on july 12, 2015

   summary: i learn best with toy code that i can play with. this tutorial
   teaches id26 via a very simple toy example, a short python
   implementation.

   edit: some folks have asked about a followup article, and i'm planning
   to write one. i'll tweet it out when it's complete at [5]@iamtrask.
   feel free to follow if you'd be interested in reading it and thanks for
   all the feedback!

just give me the code:

x = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).t
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1
for j in xrange(60000):
    l1 = 1/(1+np.exp(-(np.dot(x,syn0))))
    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))
    l2_delta = (y - l2)*(l2*(1-l2))
    l1_delta = l2_delta.dot(syn1.t) * (l1 * (1-l1))
    syn1 += l1.t.dot(l2_delta)
    syn0 += x.t.dot(l1_delta)

   other languages: [6]d, [7]c++ [8]cuda

   however, this is a bit terse   . let   s break it apart into a few simple
   parts.

part 1: a tiny toy network

   a neural network trained with id26 is attempting to use
   input to predict output.

                                inputs  output
                               0 0 1    0
                               1 1 1    1
                               1 0 1    1
                               0 1 1    0

   consider trying to predict the output column given the three input
   columns. we could solve this problem by simply measuring statistics
   between the input values and the output values. if we did so, we would
   see that the leftmost input column is perfectly correlated with the
   output. id26, in its simplest form, measures statistics like
   this to make a model. let's jump right in and use it to do this.

2 layer neural network:

import numpy as np

# sigmoid function
def nonlin(x,deriv=false):
    if(deriv==true):
        return x*(1-x)
    return 1/(1+np.exp(-x))

# input dataset
x = np.array([  [0,0,1],
                [0,1,1],
                [1,0,1],
                [1,1,1] ])

# output dataset
y = np.array([[0,0,1,1]]).t

# seed random numbers to make calculation
# deterministic (just a good practice)
np.random.seed(1)

# initialize weights randomly with mean 0
syn0 = 2*np.random.random((3,1)) - 1

for iter in xrange(10000):

    # forward propagation
    l0 = x
    l1 = nonlin(np.dot(l0,syn0))

    # how much did we miss?
    l1_error = y - l1

    # multiply how much we missed by the
    # slope of the sigmoid at the values in l1
    l1_delta = l1_error * nonlin(l1,true)

    # update weights
    syn0 += np.dot(l0.t,l1_delta)

print "output after training:"
print l1


output after training:
[[ 0.00966449]
 [ 0.00786506]
 [ 0.99358898]
 [ 0.99211957]]

   variable definition
   x input dataset matrix where each row is a training example
   y output dataset matrix where each row is a training example
   l0 first layer of the network, specified by the input data
   l1 second layer of the network, otherwise known as the hidden layer
   syn0 first layer of weights, synapse 0, connecting l0 to l1.
   * elementwise multiplication, so two vectors of equal size are
   multiplying corresponding values 1-to-1 to generate a final vector of
   identical size.
   </tr>
   - elementwise subtraction, so two vectors of equal size are subtracting
   corresponding values 1-to-1 to generate a final vector of identical
   size.
   x.dot(y) if x and y are vectors, this is a dot product. if both are
   matrices, it's a matrix-id127. if only one is a matrix,
   then it's vector id127.

   as you can see in the "output after training", it works!!! before i
   describe processes, i recommend playing around with the code to get an
   intuitive feel for how it works. you should be able to run it "as is"
   in an [9]ipython notebook (or a script if you must, but i highly
   recommend the notebook). here are some good places to look in the code:
       compare l1 after the first iteration and after the last iteration.
       check out the "nonlin" function. this is what gives us a id203
   as output.
       check out how l1_error changes as you iterate.
       take apart line 36. most of the secret sauce is here.
       check out line 39. everything in the network prepares for this
   operation.

   let's walk through the code line by line.

   recommendation: open this blog in two screens so you can see the code
   while you read it. that's kinda what i did while i wrote it. :)

   line 01: this imports numpy, which is a id202 library. this is
   our only dependency.

   line 04: this is our "nonlinearity". while it can be several kinds of
   functions, this nonlinearity maps a function called a "sigmoid". a
   [10]sigmoid function maps any value to a value between 0 and 1. we use
   it to convert numbers to probabilities. it also has several other
   desirable properties for training neural networks.

   line 05: notice that this function can also generate the derivative of
   a sigmoid (when deriv=true). one of the desirable properties of a
   sigmoid function is that its output can be used to create its
   derivative. if the sigmoid's output is a variable "out", then the
   derivative is simply out * (1-out). this is very efficient.
   if you're unfamililar with derivatives, just think about it as the
   slope of the sigmoid function at a given point (as you can see above,
   different points have different slopes). for more on derivatives, check
   out this [11]derivatives tutorial from khan academy.

   line 10: this initializes our input dataset as a numpy matrix. each row
   is a single "training example". each column corresponds to one of our
   input nodes. thus, we have 3 input nodes to the network and 4 training
   examples.

   line 16: this initializes our output dataset. in this case, i generated
   the dataset horizontally (with a single row and 4 columns) for space.
   ".t" is the transpose function. after the transpose, this y matrix has
   4 rows with one column. just like our input, each row is a training
   example, and each column (only one) is an output node. so, our network
   has 3 inputs and 1 output.

   line 20: it's good practice to seed your random numbers. your numbers
   will still be randomly distributed, but they'll be randomly distributed
   in exactly the same way each time you train. this makes it easier to
   see how your changes affect the network.

   line 23: this is our weight matrix for this neural network. it's called
   "syn0" to imply "synapse zero". since we only have 2 layers (input and
   output), we only need one matrix of weights to connect them. its
   dimension is (3,1) because we have 3 inputs and 1 output. another way
   of looking at it is that l0 is of size 3 and l1 is of size 1. thus, we
   want to connect every node in l0 to every node in l1, which requires a
   matrix of dimensionality (3,1). :)
   also notice that it is initialized randomly with a mean of zero. there
   is quite a bit of theory that goes into weight initialization. for now,
   just take it as a best practice that it's a good idea to have a mean of
   zero in weight initialization.
   another note is that the "neural network" is really just this matrix.
   we have "layers" l0 and l1 but they are transient values based on the
   dataset. we don't save them. all of the learning is stored in the syn0
   matrix.

   line 25: this begins our actual network training code. this for loop
   "iterates" multiple times over the training code to optimize our
   network to the dataset.

   line 28: since our first layer, l0, is simply our data. we explicitly
   describe it as such at this point. remember that x contains 4 training
   examples (rows). we're going to process all of them at the same time in
   this implementation. this is known as "full batch" training. thus, we
   have 4 different l0 rows, but you can think of it as a single training
   example if you want. it makes no difference at this point. (we could
   load in 1000 or 10,000 if we wanted to without changing any of the
   code).

   line 29: this is our prediction step. basically, we first let the
   network "try" to predict the output given the input. we will then study
   how it performs so that we can adjust it to do a bit better for each
   iteration.
   this line contains 2 steps. the first matrix multiplies l0 by syn0. the
   second passes our output through the sigmoid function. consider the
   dimensions of each:
   (4 x 3) dot (3 x 1) = (4 x 1)
   id127 is ordered, such the dimensions in the middle of
   the equation must be the same. the final matrix generated is thus the
   number of rows of the first matrix and the number of columns of the
   second matrix.
   since we loaded in 4 training examples, we ended up with 4 guesses for
   the correct answer, a (4 x 1) matrix. each output corresponds with the
   network's guess for a given input. perhaps it becomes intuitive why we
   could have "loaded in" an arbitrary number of training examples. the
   id127 would still work out. :)

   line 32: so, given that l1 had a "guess" for each input. we can now
   compare how well it did by subtracting the true answer (y) from the
   guess (l1). l1_error is just a vector of positive and negative numbers
   reflecting how much the network missed.

   line 36: now we're getting to the good stuff! this is the secret sauce!
   there's a lot going on in this line, so let's further break it into two
   parts.

first part: the derivative

nonlin(l1,true)

   if l1 represents these three dots, the code above generates the slopes
   of the lines below. notice that very high values such as x=2.0 (green
   dot) and very low values such as x=-1.0 (purple dot) have rather
   shallow slopes. the highest slope you can have is at x=0 (blue dot).
   this plays an important role. also notice that all derivatives are
   between 0 and 1.

entire statement: the error weighted derivative

l1_delta = l1_error * nonlin(l1,true)

   there are more "mathematically precise" ways than "the error weighted
   derivative" but i think that this captures the intuition. l1_error is a
   (4,1) matrix. nonlin(l1,true) returns a (4,1) matrix. what we're doing
   is multiplying them [12]"elementwise". this returns a (4,1) matrix
   l1_delta with the multiplied values.
   when we multiply the "slopes" by the error, we are reducing the error
   of high confidence predictions. look at the sigmoid picture again! if
   the slope was really shallow (close to 0), then the network either had
   a very high value, or a very low value. this means that the network was
   quite confident one way or the other. however, if the network guessed
   something close to (x=0, y=0.5) then it isn't very confident. we update
   these "wishy-washy" predictions most heavily, and we tend to leave the
   confident ones alone by multiplying them by a number close to 0.

   line 39: we are now ready to update our network! let's take a look at a
   single training example. in this training example, we're all setup to
   update our weights. let's update the far left weight (9.5).
   weight_update = input_value * l1_delta
   for the far left weight, this would multiply 1.0 * the l1_delta.
   presumably, this would increment 9.5 ever so slightly. why only a small
   ammount? well, the prediction was already very confident, and the
   prediction was largely correct. a small error and a small slope means a
   very small update. consider all the weights. it would ever so slightly
   increase all three.

   however, because we're using a "full batch" configuration, we're doing
   the above step on all four training examples. so, it looks a lot more
   like the image above. so, what does line 39 do? it computes the weight
   updates for each weight for each training example, sums them, and
   updates the weights, all in a simple line. play around with the matrix
   multiplication and you'll see it do this!

takeaways:

   so, now that we've looked at how the network updates, let's look back
   at our training data and reflect. when both an input and a output are
   1, we increase the weight between them. when an input is 1 and an
   output is 0, we decrease the weight between them.

                                inputs  output
                               0 0 1    0
                               1 1 1    1
                               1 0 1    1
                               0 1 1    0

   thus, in our four training examples below, the weight from the first
   input to the output would consistently increment or remain unchanged,
   whereas the other two weights would find themselves both increasing and
   decreasing across training examples (cancelling out progress). this
   phenomenon is what causes our network to learn based on correlations
   between the input and output.

part 2: a slightly harder problem

                                inputs  output
                               0 0 1    0
                               0 1 1    1
                               1 0 1    1
                               1 1 1    0

   consider trying to predict the output column given the two input
   columns. a key takeway should be that neither columns have any
   correlation to the output. each column has a 50% chance of predicting a
   1 and a 50% chance of predicting a 0.

   so, what's the pattern? it appears to be completely unrelated to column
   three, which is always 1. however, columns 1 and 2 give more clarity.
   if either column 1 or 2 are a 1 (but not both!) then the output is a 1.
   this is our pattern.

   this is considered a "nonlinear" pattern because there isn't a direct
   one-to-one relationship between the input and output. instead, there is
   a one-to-one relationship between a combination of inputs, namely
   columns 1 and 2.

   believe it or not, image recognition is a similar problem. if one had
   100 identically sized images of pipes and bicycles, no individual pixel
   position would directly correlate with the presence of a bicycle or
   pipe. the pixels might as well be random from a purely statistical
   point of view. however, certain combinations of pixels are not random,
   namely the combination that forms the image of a bicycle or a person.

our strategy

   in order to first combine pixels into something that can then have a
   one-to-one relationship with the output, we need to add another layer.
   our first layer will combine the inputs, and our second layer will then
   map them to the output using the output of the first layer as input.
   before we jump into an implementation though, take a look at this
   table.

               inputs (l0)   hidden weights (l1)   output (l2)
              0 0 1         0.1 0.2 0.5 0.2        0
              0 1 1         0.2 0.6 0.7 0.1        1
              1 0 1         0.3 0.2 0.3 0.9        1
              1 1 1         0.2 0.1 0.3 0.8        0

   if we randomly initialize our weights, we will get hidden state values
   for layer 1. notice anything? the second column (second hidden node),
   has a slight correlation with the output already! it's not perfect, but
   it's there. believe it or not, this is a huge part of how neural
   networks train. (arguably, it's the only way that neural networks
   train.) what the training below is going to do is amplify that
   correlation. it's both going to update syn1 to map it to the output,
   and update syn0 to be better at producing it from the input!

   note: the field of adding more layers to model more combinations of
   relationships such as this is known as [13]"deep learning" because of
   the increasingly deep layers being modeled.

3 layer neural network:

import numpy as np

def nonlin(x,deriv=false):
        if(deriv==true):
            return x*(1-x)

        return 1/(1+np.exp(-x))

x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

y = np.array([[0],
                        [1],
                        [1],
                        [0]])

np.random.seed(1)

# randomly initialize our weights with mean 0
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1

for j in xrange(60000):

        # feed forward through layers 0, 1, and 2
    l0 = x
    l1 = nonlin(np.dot(l0,syn0))
    l2 = nonlin(np.dot(l1,syn1))

    # how much did we miss the target value?
    l2_error = y - l2

    if (j% 10000) == 0:
        print "error:" + str(np.mean(np.abs(l2_error)))

    # in what direction is the target value?
    # were we really sure? if so, don't change too much.
    l2_delta = l2_error*nonlin(l2,deriv=true)

    # how much did each l1 value contribute to the l2 error (according to the we
ights)?
    l1_error = l2_delta.dot(syn1.t)

    # in what direction is the target l1?
    # were we really sure? if so, don't change too much.
    l1_delta = l1_error * nonlin(l1,deriv=true)

    syn1 += l1.t.dot(l2_delta)
    syn0 += l0.t.dot(l1_delta)


error:0.496410031903
error:0.00858452565325
error:0.00578945986251
error:0.00462917677677
error:0.00395876528027
error:0.00351012256786

   variable definition
   x input dataset matrix where each row is a training example
   y output dataset matrix where each row is a training example
   l0 first layer of the network, specified by the input data
   l1 second layer of the network, otherwise known as the hidden layer
   l2 final layer of the network, which is our hypothesis, and should
   approximate the correct answer as we train.
   syn0 first layer of weights, synapse 0, connecting l0 to l1.
   syn1 second layer of weights, synapse 1 connecting l1 to l2.
   l2_error this is the amount that the neural network "missed".
   l2_delta this is the error of the network scaled by the confidence.
   it's almost identical to the error except that very confident errors
   are muted.
   l1_error weighting l2_delta by the weights in syn1, we can calculate
   the error in the middle/hidden layer.
   l1_delta this is the l1 error of the network scaled by the confidence.
   again, it's almost identical to the l1_error except that confident
   errors are muted.

   recommendation: open this blog in two screens so you can see the code
   while you read it. that's kinda what i did while i wrote it. :)

   everything should look very familiar! it's really just 2 of the
   previous implementation stacked on top of each other. the output of the
   first layer (l1) is the input to the second layer. the only new thing
   happening here is on line 43.

   line 43: uses the "confidence weighted error" from l2 to establish an
   error for l1. to do this, it simply sends the error across the weights
   from l2 to l1. this gives what you could call a "contribution weighted
   error" because we learn how much each node value in l1 "contributed" to
   the error in l2. this step is called "backpropagating" and is the
   namesake of the algorithm. we then update syn0 using the same steps we
   did in the 2 layer implementation.

part 3: conclusion and future work

my recommendation:

   if you're serious about neural networks, i have one recommendation. try
   to rebuild this network from memory. i know that might sound a bit
   crazy, but it seriously helps. if you want to be able to create
   arbitrary architectures based on new academic papers or read and
   understand sample code for these different architectures, i think that
   it's a killer exercise. i think it's useful even if you're using
   frameworks like [14]torch, [15]caffe, or [16]theano. i worked with
   neural networks for a couple years before performing this exercise, and
   it was the best investment of time i've made in the field (and it
   didn't take long).

future work

   this toy example still needs quite a few bells and whistles to really
   approach the state-of-the-art architectures. here's a few things you
   can look into if you want to further improve your network. (perhaps i
   will in a followup post.)

       alpha
       [17]bias units
       [18]mini-batches
       delta trimming
       [19]parameterized layer sizes
       [20]id173
       [21]dropout
       [22]momentum
       [23]batch id172
       gpu compatability
       other awesomeness you implement
     __________________________________________________________________

want to work in machine learning?

   one of the best things you can do to learn machine learning is to have
   a job where you're practicing machine learning professionally. i'd
   encourage you to check out the [24]positions at digital reasoning in
   your job hunt. if you have questions about any of the positions or
   about life at digital reasoning, feel free to send me a message on
   [25]my linkedin. i'm happy to hear about where you want to go in life,
   and help you evaluate whether digital reasoning could be a good fit.

   if none of the positions above feel like a good fit. continue your
   search! machine learning expertise is one of the most valuable skills
   in the job market today, and there are many firms looking for
   practitioners. perhaps some of these services below will help you in
   your hunt.
   machine learning jobs
   what: title, keywords_____
   where: city, state, or zip_
   find jobs
   [26]jobs by indeed
   [27]view more job search results
     __________________________________________________________________

     * [28]    previous post
     * [29]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. http://iamtrask.github.io/
   2. http://iamtrask.github.io/
   3. http://iamtrask.github.io/about/
   4. http://iamtrask.github.io/contact/
   5. https://twitter.com/iamtrask
   6. https://github.com/marenz/neural_net_examples
   7. https://cognitivedemons.wordpress.com/2017/07/06/a-neural-network-in-10-lines-of-c-code/
   8. https://cognitivedemons.wordpress.com/2017/09/02/a-neural-network-in-10-lines-of-cuda-c-code
   9. http://ipython.org/notebook.html
  10. https://en.wikipedia.org/wiki/sigmoid_function
  11. https://www.khanacademy.org/math/differential-calculus/taking-derivatives/derivative_intro/v/calculus-derivatives-1
  12. http://nl.mathworks.com/help/matlab/ref/times.html
  13. https://en.wikipedia.org/wiki/deep_learning
  14. http://torch.ch/
  15. http://caffe.berkeleyvision.org/
  16. http://deeplearning.net/software/theano/
  17. http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
  18. https://class.coursera.org/ml-003/lecture/106
  19. https://www.youtube.com/watch?v=xqruheeiycs
  20. https://class.coursera.org/ml-003/lecture/63
  21. http://videolectures.net/nips2012_hinton_networks/
  22. https://www.youtube.com/watch?v=xqruheeiycs
  23. http://arxiv.org/abs/1502.03167
  24. https://www.linkedin.com/vsearch/j?page_num=1&locationtype=y&f_c=74158&trk=jobs_biz_prem_all_header
  25. https://www.linkedin.com/profile/view?id=226572677&trk=nav_responsive_tab_profile
  26. http://www.indeed.com/
  27. http://jobsearch.monster.com/jobs/?q=machine-learning
  28. http://2014/11/23/harry-potter/
  29. http://2015/07/27/python-network-part2/

   hidden links:
  31. http://iamtrask.github.io/feed.xml
  32. https://twitter.com/iamtrask
  33. https://github.com/iamtrask
