   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

a neural network in 13 lines of python (part 2 - id119)

improving our neural network by optimizing id119

   posted by iamtrask on july 27, 2015

   summary: i learn best with toy code that i can play with. this tutorial
   teaches id119 via a very simple toy example, a short python
   implementation.

   followup post: i intend to write a followup post to this one adding
   popular features leveraged by [5]state-of-the-art approaches (likely
   dropout, dropconnect, and momentum). i'll tweet it out when it's
   complete [6]@iamtrask. feel free to follow if you'd be interested in
   reading more and thanks for all the feedback!

just give me the code:

import numpy as np
x = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).t
alpha,hidden_dim = (0.5,4)
synapse_0 = 2*np.random.random((3,hidden_dim)) - 1
synapse_1 = 2*np.random.random((hidden_dim,1)) - 1
for j in xrange(60000):
    layer_1 = 1/(1+np.exp(-(np.dot(x,synapse_0))))
    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))
    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))
    layer_1_delta = layer_2_delta.dot(synapse_1.t) * (layer_1 * (1-layer_1))
    synapse_1 -= (alpha * layer_1.t.dot(layer_2_delta))
    synapse_0 -= (alpha * x.t.dot(layer_1_delta))

part 1: optimization

   in [7]part 1, i laid out the basis for id26 in a simple
   neural network. id26 allowed us to measure how each weight
   in the network contributed to the overall error. this ultimately
   allowed us to change these weights using a different algorithm,
   id119.

   the takeaway here is that id26 doesn't optimize! it moves
   the error information from the end of the network to all the weights
   inside the network so that a different algorithm can optimize those
   weights to fit our data. we actually have a plethora of different
   nonlinear optimization methods that we could use with id26:

   a few optimization methods:
       [8]annealing
       [9]stochastic id119
       [10]aw-sgd (new!)
       [11]momentum (sgd)
       [12]nesterov momentum (sgd)
       [13]adagrad
       [14]adadelta
       [15]adam
       [16]bfgs
       [17]lbfgs
   visualizing the difference:
       [18]convnet.js
       [19]robertsdionne

   many of these optimizations are good for different purposes, and in
   some cases several can be used together. in this tutorial, we will walk
   through id119, which is arguably the simplest and most
   widely used neural network optimization algorithm. by learning about
   id119, we will then be able to improve our toy neural
   network through parameterization and tuning, and ultimately make it a
   lot more powerful.

part 2: id119

   imagine that you had a red ball inside of a rounded bucket like in the
   picture below. imagine further that the red ball is trying to find the
   bottom of the bucket. this is optimization. in our case, the ball is
   optimizing it's position (from left to right) to find the lowest point
   in the bucket.

   (pause here.... make sure you got that last sentence.... got it?)

   so, to gamify this a bit. the ball has two options, left or right. it
   has one goal, get as low as possible. so, it needs to press the left
   and right buttons correctly to find the lowest spot

   so, what information does the ball use to adjust its position to find
   the lowest point? the only information it has is the slope of the side
   of the bucket at its current position, pictured below with the blue
   line. notice that when the slope is negative (downward from left to
   right), the ball should move to the right. however, when the slope is
   positive, the ball should move to the left. as you can see, this is
   more than enough information to find the bottom of the bucket in a few
   iterations. this is a sub-field of optimization called gradient
   optimization. (gradient is just a fancy word for slope or steepness).

oversimplified id119:

     * calculate slope at current position
     * if slope is negative, move right
     * if slope is positive, move left
     * (repeat until slope == 0)

   the question is, however, how much should the ball move at each time
   step? look at the bucket again. the steeper the slope, the farther the
   ball is from the bottom. that's helpful! let's improve our algorithm to
   leverage this new information. also, let's assume that the bucket is on
   an (x,y) plane. so, it's location is x (along the bottom). increasing
   the ball's "x" position moves it to the right. decreasing the ball's
   "x" position moves it to the left.

naive id119:

     * calculate "slope" at current "x" position
     * change x by the negative of the slope. (x = x - slope)
     * (repeat until slope == 0)

   make sure you can picture this process in your head before moving on.
   this is a considerable improvement to our algorithm. for very positive
   slopes, we move left by a lot. for only slightly positive slopes, we
   move left by only a little. as it gets closer and closer to the bottom,
   it takes smaller and smaller steps until the slope equals zero, at
   which point it stops. this stopping point is called convergence.

part 3: sometimes it breaks

   id119 isn't perfect. let's take a look at its issues and how
   people get around them. this will allow us to improve our network to
   overcome these issues.

problem 1: when slopes are too big

   how big is too big? remember our step size is based on the steepness of
   the slope. sometimes the slope is so steep that we overshoot by a lot.
   overshooting by a little is ok, but sometimes we overshoot by so much
   that we're even farther away than we started! see below.

   what makes this problem so destructive is that overshooting this far
   means we land at an even steeper slope in the opposite direction. this
   causes us to overshoot again even farther. this viscious cycle of
   overshooting leading to more overshooting is called divergence.

solution 1: make slopes smaller

   lol. this may seem too simple to be true, but it's used in pretty much
   every neural network. if our gradients are too big, we make them
   smaller! we do this by multiplying them (all of them) by a single
   number between 0 and 1 (such as 0.01). this fraction is typically a
   single float called alpha. when we do this, we don't overshoot and our
   network converges.

improved id119:

   alpha = 0.1 (or some number between 0 and 1)
     * calculate "slope" at current "x" position
     * x = x - (alpha*slope)
     * (repeat until slope == 0)

problem 2: local minimums

   sometimes your bucket has a funny shape, and following the slope
   doesn't take you to the absolute lowest point. consider the picture
   below.

   this is by far the most difficult problem with id119. there
   are a myriad of options to try to overcome this. generally speaking,
   they all involve an element of random searching to try lots of
   different parts of the bucket.

solution 2: multiple random starting states

   there are a myriad of ways in which randomness is used to overcome
   getting stuck in a local minimum. it begs the question, if we have to
   use randomness to find the global minimum, why are we still optimizing
   in the first place? why not just try randomly? the answer lies in the
   graph below.

   imagine that we randomly placed 100 balls on this line and started
   optimizing all of them. if we did so, they would all end up in only 5
   positions, mapped out by the five colored balls above. the colored
   regions represent the domain of each local minimum. for example, if a
   ball randomly falls within the blue domain, it will converge to the
   blue minimum. this means that to search the entire space, we only have
   to randomly find 5 spaces! this is far better than pure random
   searching, which has to randomly try every space (which could easily be
   millions of places on this black line depending on the granularity).

   in neural networks: one way that neural networks accomplish this is by
   having very large hidden layers. you see, each hidden node in a layer
   starts out in a different random starting state. this allows each
   hidden node to converge to different patterns in the network.
   parameterizing this size allows the neural network user to potentially
   try thousands [20](or tens of billions) of different local minima in a
   single neural network.

   sidenote 1: this is why neural networks are so powerful! they have the
   ability to search far more of the space than they actually compute! we
   can search the entire black line above with (in theory) only 5 balls
   and a handful of iterations. searching that same space in a brute force
   fashion could easily take orders of magnitude more computation.

   sidenote 2: a close eye might ask, "well, why would we allow a lot of
   nodes to converge to the same spot? that's actually wasting
   computational power!" that's an excellent point. the current
   state-of-the-art approaches to avoiding hidden nodes coming up with the
   same answer (by searching the same space) are dropout and drop-connect,
   which i intend to cover in a later post.

problem 3: when slopes are too small

   neural networks sometimes suffer from the slopes being too small. the
   answer is also obvious but i wanted to mention it here to expand on its
   symptoms. consider the following graph.

   our little red ball up there is just stuck! if your alpha is too small,
   this can happen. the ball just drops right into an instant local
   minimum and ignores the big picture. it doesn't have the umph to get
   out of the rut.

   and perhaps the more obvious symptom of deltas that are too small is
   that the convergence will just take a very, very long time.

solution 3: increase the alpha

   as you might expect, the solution to both of these symptoms is to
   increase the alpha. we might even multiply our deltas by a weight
   higher than 1. this is very rare, but it does sometimes happen.

part 4: sgd in neural networks

   so at this point you might be wondering, how does this relate to neural
   networks and id26? this is the hardest part, so get ready to
   hold on tight and take things slow. it's also quite important.

   that big nasty curve? in a neural network, we're trying to minimize the
   error with respect to the weights. so, what that curve represents is
   the network's error relative to the position of a single weight. so, if
   we computed the network's error for every possible value of a single
   weight, it would generate the curve you see above. we would then pick
   the value of the single weight that has the lowest error (the lowest
   part of the curve). i say single weight because it's a two-dimensional
   plot. thus, the x dimension is the value of the weight and the y
   dimension is the neural network's error when the weight is at that
   position.

          stop and make sure you got that last paragraph. it's key.

   let's take a look at what this process looks like in a simple 2 layer
   neural network.

2 layer neural network:

import numpy as np

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)

# input dataset
x = np.array([  [0,1],
                [0,1],
                [1,0],
                [1,0] ])

# output dataset
y = np.array([[0,0,1,1]]).t

# seed random numbers to make calculation
# deterministic (just a good practice)
np.random.seed(1)

# initialize weights randomly with mean 0
synapse_0 = 2*np.random.random((2,1)) - 1

for iter in xrange(10000):

    # forward propagation
    layer_0 = x
    layer_1 = sigmoid(np.dot(layer_0,synapse_0))

    # how much did we miss?
    layer_1_error = layer_1 - y

    # multiply how much we missed by the
    # slope of the sigmoid at the values in l1
    layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)
    synapse_0_derivative = np.dot(layer_0.t,layer_1_delta)

    # update weights
    synapse_0 -= synapse_0_derivative

print "output after training:"
print layer_1



   so, in this case, we have a single error at the output (single value),
   which is computed on line 35. since we have 2 weights, the output
   "error plane" is a 3 dimensional space. we can think of this as an
   (x,y,z) plane, where vertical is the error, and x and y are the values
   of our two weights in syn0.

   let's try to plot what the error plane looks like for the
   network/dataset above. so, how do we compute the error for a given set
   of weights? lines 31,32,and 35 show us that. if we take that logic and
   plot the overall error (a single scalar representing the network error
   over the entire dataset) for every possible set of weights (from -10 to
   10 for x and y), it looks something like this.

   don't be intimidated by this. it really is as simple as computing every
   possible set of weights, and the error that the network generates at
   each set. x is the first synapse_0 weight and y is the second synapse_0
   weight. z is the overall error. as you can see, our output data is
   positively correlated with the first input data. thus, the error is
   minimized when x (the first synapse_0 weight) is high. what about the
   second synapse_0 weight? how is it optimal?

how our 2 layer neural network optimizes

   so, given that lines 31,32,and 35 end up computing the error. it can be
   natural to see that lines 39, 40, and 43 optimize to reduce the error.
   this is where id119 is happening! remember our pseudocode?

naive id119:

     * lines 39 and 40: calculate "slope" at current "x" position
     * line 43: change x by the negative of the slope. (x = x - slope)
     * line 28: (repeat until slope == 0)

   it's exactly the same thing! the only thing that has changed is that we
   have 2 weights that we're optimizing instead of just 1. the logic,
   however, is identical.

part 5: improving our neural network

   remember that id119 had some weaknesses. now that we have
   seen how our neural network leverages id119, we can improve
   our network to overcome these weaknesses in the same way that we
   improved id119 in part 3 (the 3 problems and solutions).

improvement 1: adding and tuning the alpha parameter

   what is alpha? as described above, the alpha parameter reduces the size
   of each iteration's update in the simplest way possible. at the very
   last minute, right before we update the weights, we multiply the weight
   update by alpha (usually between 0 and 1, thus reducing the size of the
   weight update). this tiny change to the code has absolutely massive
   impact on its ability to train.

   we're going to jump back to our 3 layer neural network from the first
   post and add in an alpha parameter at the appropriate place. then,
   we're going to run a series of experiments to align all the intuition
   we developed around alpha with its behavior in live code.

improved id119:

     * calculate "slope" at current "x" position
     * lines 56 and 57: change x by the negative of the slope scaled by
       alpha. (x = x - (alpha*slope) )
     * (repeat until slope == 0)

import numpy as np

alphas = [0.001,0.01,0.1,1,10,100,1000]

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)

x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

y = np.array([[0],
                        [1],
                        [1],
                        [0]])

for alpha in alphas:
    print "\ntraining with alpha:" + str(alpha)
    np.random.seed(1)

    # randomly initialize our weights with mean 0
    synapse_0 = 2*np.random.random((3,4)) - 1
    synapse_1 = 2*np.random.random((4,1)) - 1

    for j in xrange(60000):

        # feed forward through layers 0, 1, and 2
        layer_0 = x
        layer_1 = sigmoid(np.dot(layer_0,synapse_0))
        layer_2 = sigmoid(np.dot(layer_1,synapse_1))

        # how much did we miss the target value?
        layer_2_error = layer_2 - y

        if (j% 10000) == 0:
            print "error after "+str(j)+" iterations:" + str(np.mean(np.abs(laye
r_2_error)))

        # in what direction is the target value?
        # were we really sure? if so, don't change too much.
        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)

        # how much did each l1 value contribute to the l2 error (according to th
e weights)?
        layer_1_error = layer_2_delta.dot(synapse_1.t)

        # in what direction is the target l1?
        # were we really sure? if so, don't change too much.
        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)

        synapse_1 -= alpha * (layer_1.t.dot(layer_2_delta))
        synapse_0 -= alpha * (layer_0.t.dot(layer_1_delta))




training with alpha:0.001
error after 0 iterations:0.496410031903
error after 10000 iterations:0.495164025493
error after 20000 iterations:0.493596043188
error after 30000 iterations:0.491606358559
error after 40000 iterations:0.489100166544
error after 50000 iterations:0.485977857846

training with alpha:0.01
error after 0 iterations:0.496410031903
error after 10000 iterations:0.457431074442
error after 20000 iterations:0.359097202563
error after 30000 iterations:0.239358137159
error after 40000 iterations:0.143070659013
error after 50000 iterations:0.0985964298089

training with alpha:0.1
error after 0 iterations:0.496410031903
error after 10000 iterations:0.0428880170001
error after 20000 iterations:0.0240989942285
error after 30000 iterations:0.0181106521468
error after 40000 iterations:0.0149876162722
error after 50000 iterations:0.0130144905381

training with alpha:1
error after 0 iterations:0.496410031903
error after 10000 iterations:0.00858452565325
error after 20000 iterations:0.00578945986251
error after 30000 iterations:0.00462917677677
error after 40000 iterations:0.00395876528027
error after 50000 iterations:0.00351012256786

training with alpha:10
error after 0 iterations:0.496410031903
error after 10000 iterations:0.00312938876301
error after 20000 iterations:0.00214459557985
error after 30000 iterations:0.00172397549956
error after 40000 iterations:0.00147821451229
error after 50000 iterations:0.00131274062834

training with alpha:100
error after 0 iterations:0.496410031903
error after 10000 iterations:0.125476983855
error after 20000 iterations:0.125330333528
error after 30000 iterations:0.125267728765
error after 40000 iterations:0.12523107366
error after 50000 iterations:0.125206352756

training with alpha:1000
error after 0 iterations:0.496410031903
error after 10000 iterations:0.5
error after 20000 iterations:0.5
error after 30000 iterations:0.5
error after 40000 iterations:0.5
error after 50000 iterations:0.5

   so, what did we observe with the different alpha sizes?

   alpha = 0.001
   the network with a crazy small alpha didn't hardly converge! this is
   because we made the weight updates so small that they hardly changed
   anything, even after 60,000 iterations! this is textbook problem 3:when
   slopes are too small.

   alpha = 0.01
   this alpha made a rather pretty convergence. it was quite smooth over
   the course of the 60,000 iterations but ultimately didn't converge as
   far as some of the others. this still is textbook problem 3:when slopes
   are too small.

   alpha = 0.1
   this alpha made some of progress very quickly but then slowed down a
   bit. this is still problem 3. we need to increase alpha some more.

   alpha = 1
   as a clever eye might suspect, this had the exact convergence as if we
   had no alpha at all! multiplying our weight updates by 1 doesn't change
   anything. :)

   alpha = 10
   perhaps you were surprised that an alpha that was greater than 1
   achieved the best score after only 10,000 iterations! this tells us
   that our weight updates were being too conservative with smaller
   alphas. this means that in the smaller alpha parameters (less than 10),
   the network's weights were generally headed in the right direction,
   they just needed to hurry up and get there!

   alpha = 100
   now we can see that taking steps that are too large can be very
   counterproductive. the network's steps are so large that it can't find
   a reasonable lowpoint in the error plane. this is textbook problem 1.
   the alpha is too big so it just jumps around on the error plane and
   never "settles" into a local minimum.

   alpha = 1000
   and with an extremely large alpha, we see a textbook example of
   divergence, with the error increasing instead of decreasing...
   hardlining at 0.5. this is a more extreme version of problem 3 where it
   overcorrectly whildly and ends up very far away from any local
   minimums.

let's take a closer look

import numpy as np

alphas = [0.001,0.01,0.1,1,10,100,1000]

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)

x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

y = np.array([[0],
                        [1],
                        [1],
                        [0]])



for alpha in alphas:
    print "\ntraining with alpha:" + str(alpha)
    np.random.seed(1)

    # randomly initialize our weights with mean 0
    synapse_0 = 2*np.random.random((3,4)) - 1
    synapse_1 = 2*np.random.random((4,1)) - 1

    prev_synapse_0_weight_update = np.zeros_like(synapse_0)
    prev_synapse_1_weight_update = np.zeros_like(synapse_1)

    synapse_0_direction_count = np.zeros_like(synapse_0)
    synapse_1_direction_count = np.zeros_like(synapse_1)

    for j in xrange(60000):

        # feed forward through layers 0, 1, and 2
        layer_0 = x
        layer_1 = sigmoid(np.dot(layer_0,synapse_0))
        layer_2 = sigmoid(np.dot(layer_1,synapse_1))

        # how much did we miss the target value?
        layer_2_error = y - layer_2

        if (j% 10000) == 0:
            print "error:" + str(np.mean(np.abs(layer_2_error)))

        # in what direction is the target value?
        # were we really sure? if so, don't change too much.
        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)

        # how much did each l1 value contribute to the l2 error (according to th
e weights)?
        layer_1_error = layer_2_delta.dot(synapse_1.t)

        # in what direction is the target l1?
        # were we really sure? if so, don't change too much.
        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)

        synapse_1_weight_update = (layer_1.t.dot(layer_2_delta))
        synapse_0_weight_update = (layer_0.t.dot(layer_1_delta))

        if(j > 0):
            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0
) - ((prev_synapse_0_weight_update > 0) + 0))
            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0
) - ((prev_synapse_1_weight_update > 0) + 0))

        synapse_1 += alpha * synapse_1_weight_update
        synapse_0 += alpha * synapse_0_weight_update

        prev_synapse_0_weight_update = synapse_0_weight_update
        prev_synapse_1_weight_update = synapse_1_weight_update

    print "synapse 0"
    print synapse_0

    print "synapse 0 update direction changes"
    print synapse_0_direction_count

    print "synapse 1"
    print synapse_1

    print "synapse 1 update direction changes"
    print synapse_1_direction_count



training with alpha:0.001
error:0.496410031903
error:0.495164025493
error:0.493596043188
error:0.491606358559
error:0.489100166544
error:0.485977857846
synapse 0
[[-0.28448441  0.32471214 -1.53496167 -0.47594822]
 [-0.7550616  -1.04593014 -1.45446052 -0.32606771]
 [-0.2594825  -0.13487028 -0.29722666  0.40028038]]
synapse 0 update direction changes
[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 1.  0.  1.  1.]]
synapse 1
[[-0.61957526]
 [ 0.76414675]
 [-1.49797046]
 [ 0.40734574]]
synapse 1 update direction changes
[[ 1.]
 [ 1.]
 [ 0.]
 [ 1.]]

training with alpha:0.01
error:0.496410031903
error:0.457431074442
error:0.359097202563
error:0.239358137159
error:0.143070659013
error:0.0985964298089
synapse 0
[[ 2.39225985  2.56885428 -5.38289334 -3.29231397]
 [-0.35379718 -4.6509363  -5.67005693 -1.74287864]
 [-0.15431323 -1.17147894  1.97979367  3.44633281]]
synapse 0 update direction changes
[[ 1.  1.  0.  0.]
 [ 2.  0.  0.  2.]
 [ 4.  2.  1.  1.]]
synapse 1
[[-3.70045078]
 [ 4.57578637]
 [-7.63362462]
 [ 4.73787613]]
synapse 1 update direction changes
[[ 2.]
 [ 1.]
 [ 0.]
 [ 1.]]

training with alpha:0.1
error:0.496410031903
error:0.0428880170001
error:0.0240989942285
error:0.0181106521468
error:0.0149876162722
error:0.0130144905381
synapse 0
[[ 3.88035459  3.6391263  -5.99509098 -3.8224267 ]
 [-1.72462557 -5.41496387 -6.30737281 -3.03987763]
 [ 0.45953952 -1.77301389  2.37235987  5.04309824]]
synapse 0 update direction changes
[[ 1.  1.  0.  0.]
 [ 2.  0.  0.  2.]
 [ 4.  2.  1.  1.]]
synapse 1
[[-5.72386389]
 [ 6.15041318]
 [-9.40272079]
 [ 6.61461026]]
synapse 1 update direction changes
[[ 2.]
 [ 1.]
 [ 0.]
 [ 1.]]

training with alpha:1
error:0.496410031903
error:0.00858452565325
error:0.00578945986251
error:0.00462917677677
error:0.00395876528027
error:0.00351012256786
synapse 0
[[ 4.6013571   4.17197193 -6.30956245 -4.19745118]
 [-2.58413484 -5.81447929 -6.60793435 -3.68396123]
 [ 0.97538679 -2.02685775  2.52949751  5.84371739]]
synapse 0 update direction changes
[[ 1.  1.  0.  0.]
 [ 2.  0.  0.  2.]
 [ 4.  2.  1.  1.]]
synapse 1
[[ -6.96765763]
 [  7.14101949]
 [-10.31917382]
 [  7.86128405]]
synapse 1 update direction changes
[[ 2.]
 [ 1.]
 [ 0.]
 [ 1.]]

training with alpha:10
error:0.496410031903
error:0.00312938876301
error:0.00214459557985
error:0.00172397549956
error:0.00147821451229
error:0.00131274062834
synapse 0
[[ 4.52597806  5.77663165 -7.34266481 -5.29379829]
 [ 1.66715206 -7.16447274 -7.99779235 -1.81881849]
 [-4.27032921 -3.35838279  3.44594007  4.88852208]]
synapse 0 update direction changes
[[  7.  19.   2.   6.]
 [  7.   2.   0.  22.]
 [ 19.  26.   9.  17.]]
synapse 1
[[ -8.58485788]
 [ 10.1786297 ]
 [-14.87601886]
 [  7.57026121]]
synapse 1 update direction changes
[[ 22.]
 [ 15.]
 [  4.]
 [ 15.]]

training with alpha:100
error:0.496410031903
error:0.125476983855
error:0.125330333528
error:0.125267728765
error:0.12523107366
error:0.125206352756
synapse 0
[[-17.20515374   1.89881432 -16.95533155  -8.23482697]
 [  5.70240659 -17.23785161  -9.48052574  -7.92972576]
 [ -4.18781704  -0.3388181    2.82024759  -8.40059859]]
synapse 0 update direction changes
[[  8.   7.   3.   2.]
 [ 13.   8.   2.   4.]
 [ 16.  13.  12.   8.]]
synapse 1
[[  9.68285369]
 [  9.55731916]
 [-16.0390702 ]
 [  6.27326973]]
synapse 1 update direction changes
[[ 13.]
 [ 11.]
 [ 12.]
 [ 10.]]

training with alpha:1000
error:0.496410031903
error:0.5
error:0.5
error:0.5
error:0.5
error:0.5
synapse 0
[[-56.06177241  -4.66409623  -5.65196179 -23.05868769]
 [ -4.52271708  -4.78184499 -10.88770202 -15.85879101]
 [-89.56678495  10.81119741  37.02351518 -48.33299795]]
synapse 0 update direction changes
[[ 3.  2.  4.  1.]
 [ 1.  2.  2.  1.]
 [ 6.  6.  4.  1.]]
synapse 1
[[  25.16188889]
 [  -8.68235535]
 [-116.60053379]
 [  11.41582458]]
synapse 1 update direction changes
[[ 7.]
 [ 7.]
 [ 7.]
 [ 3.]]

   what i did in the above code was count the number of times a derivative
   changed direction. that's the "update direction changes" readout at the
   end of training. if a slope (derivative) changes direction, it means
   that it passed over the local minimum and needs to go back. if it never
   changes direction, it means that it probably didn't go far enough.

a few takeaways:

     * when the alpha was tiny, the derivatives almost never changed
       direction.
     * when the alpha was optimal, the derivative changed directions a
       ton.
     * when the alpha was huge, the derivative changed directions a medium
       amount.
     * when the alpha was tiny, the weights ended up being reasonably
       small too
     * when the alpha was huge, the weights got huge too!

improvement 2: parameterizing the size of the hidden layer

   being able to increase the size of the hidden layer increases the
   amount of search space that we converge to in each iteration. consider
   the network and output
import numpy as np

alphas = [0.001,0.01,0.1,1,10,100,1000]
hiddensize = 32

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)

x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

y = np.array([[0],
                        [1],
                        [1],
                        [0]])

for alpha in alphas:
    print "\ntraining with alpha:" + str(alpha)
    np.random.seed(1)

    # randomly initialize our weights with mean 0
    synapse_0 = 2*np.random.random((3,hiddensize)) - 1
    synapse_1 = 2*np.random.random((hiddensize,1)) - 1

    for j in xrange(60000):

        # feed forward through layers 0, 1, and 2
        layer_0 = x
        layer_1 = sigmoid(np.dot(layer_0,synapse_0))
        layer_2 = sigmoid(np.dot(layer_1,synapse_1))

        # how much did we miss the target value?
        layer_2_error = layer_2 - y

        if (j% 10000) == 0:
            print "error after "+str(j)+" iterations:" + str(np.mean(np.abs(laye
r_2_error)))

        # in what direction is the target value?
        # were we really sure? if so, don't change too much.
        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)

        # how much did each l1 value contribute to the l2 error (according to th
e weights)?
        layer_1_error = layer_2_delta.dot(synapse_1.t)

        # in what direction is the target l1?
        # were we really sure? if so, don't change too much.
        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)

        synapse_1 -= alpha * (layer_1.t.dot(layer_2_delta))
        synapse_0 -= alpha * (layer_0.t.dot(layer_1_delta))


training with alpha:0.001
error after 0 iterations:0.496439922501
error after 10000 iterations:0.491049468129
error after 20000 iterations:0.484976307027
error after 30000 iterations:0.477830678793
error after 40000 iterations:0.46903846539
error after 50000 iterations:0.458029258565

training with alpha:0.01
error after 0 iterations:0.496439922501
error after 10000 iterations:0.356379061648
error after 20000 iterations:0.146939845465
error after 30000 iterations:0.0880156127416
error after 40000 iterations:0.065147819275
error after 50000 iterations:0.0529658087026

training with alpha:0.1
error after 0 iterations:0.496439922501
error after 10000 iterations:0.0305404908386
error after 20000 iterations:0.0190638725334
error after 30000 iterations:0.0147643907296
error after 40000 iterations:0.0123892429905
error after 50000 iterations:0.0108421669738

training with alpha:1
error after 0 iterations:0.496439922501
error after 10000 iterations:0.00736052234249
error after 20000 iterations:0.00497251705039
error after 30000 iterations:0.00396863978159
error after 40000 iterations:0.00338641021983
error after 50000 iterations:0.00299625684932

training with alpha:10
error after 0 iterations:0.496439922501
error after 10000 iterations:0.00224922117381
error after 20000 iterations:0.00153852153014
error after 30000 iterations:0.00123717718456
error after 40000 iterations:0.00106119569132
error after 50000 iterations:0.000942641990774

training with alpha:100
error after 0 iterations:0.496439922501
error after 10000 iterations:0.5
error after 20000 iterations:0.5
error after 30000 iterations:0.5
error after 40000 iterations:0.5
error after 50000 iterations:0.5

training with alpha:1000
error after 0 iterations:0.496439922501
error after 10000 iterations:0.5
error after 20000 iterations:0.5
error after 30000 iterations:0.5
error after 40000 iterations:0.5
error after 50000 iterations:0.5

   notice that the best error with 32 nodes is 0.0009 whereas the best
   error with 4 hidden nodes was only 0.0013. this might not seem like
   much, but it's an important lesson. we do not need any more than 3
   nodes to represent this dataset. however, because we had more nodes
   when we started, we searched more of the space in each iteration and
   ultimately converged faster. even though this is very marginal in this
   toy problem, this affect plays a huge role when modeling very complex
   datasets.

part 6: conclusion and future work

my recommendation:

   if you're serious about neural networks, i have one recommendation. try
   to rebuild this network from memory. i know that might sound a bit
   crazy, but it seriously helps. if you want to be able to create
   arbitrary architectures based on new academic papers or read and
   understand sample code for these different architectures, i think that
   it's a killer exercise. i think it's useful even if you're using
   frameworks like [21]torch, [22]caffe, or [23]theano. i worked with
   neural networks for a couple years before performing this exercise, and
   it was the best investment of time i've made in the field (and it
   didn't take long).

future work

   this toy example still needs quite a few bells and whistles to really
   approach the state-of-the-art architectures. here's a few things you
   can look into if you want to further improve your network. (perhaps i
   will in a followup post.)

       [24]bias units
       [25]mini-batches
       delta trimming
       [26]parameterized layer sizes
       [27]id173
       [28]dropout
       [29]momentum
       [30]batch id172
       gpu compatability
       other awesomeness you implement
     __________________________________________________________________

want to work in machine learning?

   one of the best things you can do to learn machine learning is to have
   a job where you're practicing machine learning professionally. i'd
   encourage you to check out the [31]positions at digital reasoning in
   your job hunt. if you have questions about any of the positions or
   about life at digital reasoning, feel free to send me a message on
   [32]my linkedin. i'm happy to hear about where you want to go in life,
   and help you evaluate whether digital reasoning could be a good fit.

   if none of the positions above feel like a good fit. continue your
   search! machine learning expertise is one of the most valuable skills
   in the job market today, and there are many firms looking for
   practitioners. perhaps some of these services below will help you in
   your hunt.
   machine learning jobs
   what: title, keywords_____
   where: city, state, or zip_
   find jobs
   [33]jobs by indeed
   [34]view more job search results
     __________________________________________________________________

     * [35]    previous post
     * [36]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. https://iamtrask.github.io/
   2. https://iamtrask.github.io/
   3. https://iamtrask.github.io/about/
   4. https://iamtrask.github.io/contact/
   5. http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
   6. https://twitter.com/iamtrask
   7. http://iamtrask.github.io/2015/07/12/basic-python-network/
   8. http://www.heatonresearch.com/articles/9
   9. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  10. http://arxiv.org/pdf/1506.09016v1.pdf
  11. http://jmlr.org/proceedings/papers/v28/sutskever13.pdf
  12. http://jmlr.org/proceedings/papers/v28/sutskever13.pdf
  13. http://www.magicbroom.info/papers/duchihasi10.pdf
  14. http://arxiv.org/abs/1212.5701
  15. http://arxiv.org/abs/1412.6980
  16. https://en.wikipedia.org/wiki/broyden   fletcher   goldfarb   shanno_algorithm
  17. https://en.wikipedia.org/wiki/limited-memory_bfgs
  18. http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html
  19. http://www.robertsdionne.com/bouncingball/
  20. http://www.digitalreasoning.com/buzz/digital-reasoning-trains-worlds-largest-neural-network-shatters-record-previously-set-by-google.1672770
  21. http://torch.ch/
  22. http://caffe.berkeleyvision.org/
  23. http://deeplearning.net/software/theano/
  24. http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
  25. https://class.coursera.org/ml-003/lecture/106
  26. https://www.youtube.com/watch?v=xqruheeiycs
  27. https://class.coursera.org/ml-003/lecture/63
  28. http://videolectures.net/nips2012_hinton_networks/
  29. https://www.youtube.com/watch?v=xqruheeiycs
  30. http://arxiv.org/abs/1502.03167
  31. https://www.linkedin.com/vsearch/j?page_num=1&locationtype=y&f_c=74158&trk=jobs_biz_prem_all_header
  32. https://www.linkedin.com/profile/view?id=226572677&trk=nav_responsive_tab_profile
  33. http://www.indeed.com/
  34. http://jobsearch.monster.com/jobs/?q=machine-learning
  35. https://2015/07/12/basic-python-network/
  36. https://2015/07/28/dropout/

   hidden links:
  38. https://iamtrask.github.io/feed.xml
  39. https://twitter.com/iamtrask
  40. https://github.com/iamtrask
