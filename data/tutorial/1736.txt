    #[1]big data made simple    feed [2]big data made simple    comments
   feed [3]big data made simple    decoding id84, pca
   and svd comments feed [4]alternate

   iframe: [5]https://www.googletagmanager.com/ns.html?id=gtm-mltth6d

   [6]big data made simple
   [7]crayon data
   (button)
   ____________________ (button)
   (button)

     * [8]home
     * [9]sectors
          + [10]banking / finance
          + [11]retail / ecom
          + [12]travel / hospitality
          + [13]privacy / security
          + [14]marketing
          + [15]media & entertainment
          + [16]telecommunications
          + [17]crime / law
          + [18]sports
          + [19]health / pharma
          + [20]education
          + [21]human resource
     * [22]tech & tools
          + [23]analytics
          + [24]data science
          + [25]business intelligence
          + [26]digital personalization
          + [27]machine learning
          + [28]artificial intelligence
          + [29]visualization
          + [30]hadoop
          + [31]data mining
          + [32]sql
          + [33]nosql
     * [34]resources
          + [35]interviews
          + [36]tutorials
          + [37]infographics
          + [38]videos
     * [39]products
     * [40]events
     * [41]write for us
     * [42]get in touch
          + [43]advertise
          + [44]partnerships
          + [45]contact us

   (button)
   ____________________ (button)
   (button)

   [46]machine learning

decoding id84, pca and svd

   posted on [47]may 20, 2015oct 22, 2018 author [48]manu jeevan

   every day ibm creates 2.5 quintillion bytes of [49]data and most of
   the  data generated are high dimensional. so it is necessary to reduce
   the dimensions of the data to work efficiently.
   one of the most common id84 technique is filtering,
   in which you leave most of the dimensions and concentrate only on
   certain dimensions. but that doesn   t always work, when you are dealing
   with image data, the number of pixels represents the number of
   dimensions in the image. now you have lot of dimensions and you don   t
   want to throw out dimensions inorder to make sense of your overall data
   set.
   as the dimensionality of your data increases, the volume of the space
   increases, in a sense the data you have becomes more and more
   sparse(scattered). one way to think about it is a very high data set
   might live in some kind of high dimensional manifold and as you are
   increasing the number of dimensions, that manifold becomes bigger and
   bigger.
   to do any statistical modelling, you have to increase the number of
   data points and samples you have, unfortunately that grows
   exponentially with the number of dimensions you have. the higher
   dimension you have, the more data you need to perform statistical
   id136. the basic idea is n (the number of data items) should be
   more than the number of dimensions.
   the image(shown in fig below) has 64  64 pixels(4096 dimensions). to
   reduce the dimensions, you want to project the high-dimensional data
   onto a lower dimensional subspace using linear or non-linear
   transformations (or projections).
   id84 in machine learning
   the most widely used methods are linear projections and the main linear
   projection method is principal component analysis(pca).
   principal component analysis (pca)
   you have data points that live in a 2d plane(x1 and x2)  and you want
   to approximate them  with a lower dimensional embedding. here it is
   obvious that the vector v(as shown in the image) is a pretty good
   approximation of your data. instead of storing 2 coordinates for each
   data points, you will store one scalar value plus the vector v, which
   is common across all of the data points, so you have to store it only
   once. so for each data point you have to store only this scalar value
   s, which gives the distance along this vector v.
   image 2
   in the figure below, you are projecting all of your data points on to
   v. what you are trying to do is to minimize in least square sense(the
   difference between your original data and your projections). you
   should  choose v in a way that you have to minimize the residual
   variance.
   *residuals are the projections of these data points on to the vector v
   min x1 and x2 vectors
   in this case the projection is orthogonal to vector v.  you  have to
   minimize the  overall sum of all of the residuals of your data points,
   by choosing the vector in such a way that the overall sum is minimized.
   it turns out that it is also the vector that closely allows you to
   reconstruct the original data using the least squared errors.  in this
   case it makes intuitive sense, you are picking v in the direction of
   biggest spread of your data. you can extend this to multiple
   components. here is the main component which we call the principal
   component, that   s the vector v that you are using to project the data
   on. and then you can repeat the process and then find the second
   component that has second biggest variance of the data, in this case is
   the direction of principal comp2 (see the image below).
   pc vectors orthogonal
   it is very important to understand the difference between principal
   component analysis (pca) and ordinary least squares (ols). i encourage
   you to go through this discussion on [50]stack exchange.
   to summarize, you are projecting your data on to these sub spaces(on to
   the principal components) to maximize the variance of the projected
   data, thats the basic idea.
   understanding singular value decomposition(svd):
   the pca algorithm is implemented as follows:
   1) subtract the mean from the data.
   2)  scale each dimensions by its variance.
   3) compute the covariance matrix s. here x is a data matrix.
   x transpose
   4) compute k largest eigen vectors of s.  these eigen vectors are the
   principal components of the data set.
   note: eigenvectors and values exist in pairs: every eigen vector has a
   corresponding eigenvalue.  eigen vector is the direction of the line
   (vertical, horizontal, 45 degrees etc.).  an eigenvalue is a number,
   telling you how much variance there is in the data in that direction,
   eigenvalue is a number telling you how spread out the data is on the
   line.
   but the operation(steps to implement pca) is expensive when x is very
   large or when x is very small. so the best way to compute principal
   components is by using svd. singular value decomposition (svd) is one
   of the best linear transformation methods  out there.
   you can take any matrix x, it doesn   t matter if it is square, singular
   or diagonal, you can decompose it into a product of three matrices(as
   shown in the figure below); two orthogonal matrices u and v and
   diagonal matrix d. the orthogonal matrix has same dimensions as your
   data matrix and then your diagonal matrix is square and it has
   dimensions kxk (k is the number of variables you have), v is again a
   square matrix.
   square matrix
   i am going compute svd on s(co-variance matrix) to obtain it   s eigen
   vectors.
   x transpose
   vector transpose
   some interesting notes on the result of factorization:
       the columns of matrix u form the eigen vectors of s.
       the d matrix is a diagonal matrix. the values of the diagonal are
   called eigen values and are in descending order.
   this is much more equivalent to calculate principal components
   analysis, but in a  much more robust way. you just need to take your
   matrix and feed it into svd.
   what does svd has to do with id84 ?
   the image below shows how i reduce the number of dimensions from k to
   q(k<q).  if you reduce the number of column vectors to q , then you
   have obtained the q-dimensional hyper-plane in this example. the values
   of d gives you the amount of variance retained by this reduction.
   svd and id84
   the best way to figure out how much variance does your dimensions
   capture is to plot a scree plot.
   variance explained
   it turns out that the variance of each principal component is related
   to d square(diagonal elements of d matrix) . in the scree plot, the
   first principal component explains 37% of the variance and then it
   quickly drops. it is better to figure out principal components that
   explain 80-90% variance of your data set.
   image recognition example
   lets say your task is to recognize faces of people, you take photo
   graph of people and you make small pictures and then you center all the
   faces such that they are roughly aligned.
   image recognition example svd
   lets say the image is 64 x64 pixels. so you have 4096 dimensions, you
   are rolling out each of the images into a vector, and then stack them
   up into your data matrix. each pixel is dimensional and each row is a
   different person.
   high dimensional data
   so you have this data matrix(as shown in the above image) and you apply
   pca on that one. here is what you get (as shown in the image below),
   these are called eigen faces. the image on the left(this is the mean
   sort of average face). the first image on the right hand side sort of
   explains the variance in left right dimensions, the 2nd image on seems
   to explain the variance in front to back.
   principal components
   in the image below, you can actually look at the variance of the
   components, it turns out that you need 50 eigen vectors to explain 90 %
   of the variance of a image.
   eigen values for faces
   as you can see after 50 eigen vectors this is a pretty good
   reconstruction. so you went from 4096 dimensions to 50 that   s a nice
   reduction in dimensions without too much reduction in quality. i hope i
   have given a broad idea of  what is id84, pca and
   svd without getting into too much mathematical details.

[51]manu jeevan

   manu jeevan is a self-taught data scientist and loves to explain data
   science concepts in simple terms. you can connect with him
   on [52]linkedin, or email him at [53]manu@bigdataexaminer.com.
   [54]preview post

   how to get a $50k+ job as a data analyst almost for free without a
   college degree
   [55]next post

   should your business adopt hadoop?

leave a comment [56]cancel reply

   your email address will not be published.
   ____________________
   ____________________
   ____________________

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   post comment

you may also like

   [57]post-image

[58]machine learning: best examples and ideas for mobile apps

   posted on [59]dec 21, 2018 author [60]premjith bpk
   every year, the world is swiftly moving closer and closer towards an
   era of digital reality. nearly 20 years ago, we had massive
   computers...
   [61]read more

   [62]post-image

[63]three mistakes to avoid if you are new to machine learning

   posted on [64]oct 9, 2014nov 7, 2018 author [65]guest
   machine learning (ml) is one of the hottest fields in data science. as
   soon as ml entered the mainstream through amazon, netflix, and
   facebook...
   [66]read more

   [67]post-image

[68]why every small business should use machine learning?

   posted on [69]jan 21, 2019jan 17, 2019 author [70]jaren nichols
   small businesses and large enterprises all want to jump on the machine
   learning bandwagon. big names like google and amazon use technology
   like this...
   [71]read more

   [72]post-image

[73]ai driven forex trading robot     first of its kind!

   posted on [74]jul 23, 2018dec 25, 2018 author [75]guest
   a lot of emotions come into a person   s mind upon hearing the phrase
      forex trading.    a significant percentage of people consider it a
   way...
   [76]read more

   [77]post-image

[78]6 revolutionary things you must know about machine learning

   posted on [79]jul 13, 2018dec 2, 2018 author [80]james warner
   we are stepping into an avant-garde period, powered by advances in
   robotics, the adoption of smart home appliances, intelligent retail
   stores, self-driving car technology...
   [81]read more

   [82]post-image

[83]how to get started with machine learning in python

   posted on [84]apr 23, 2014nov 7, 2018 author [85]guest
   the python conference pycon2014 has held recently and the videos for
   the conference are online. i have been working my way through the
   interesting...
   [86]read more
     __________________________________________________________________

   our channels
     * [87]about us
     * [88]events
     * [89]newsletter
     * [90]advertise with us
     * [91]write for us
     * [92]author guidelines
     * [93]contact us
     * [94]partnerships
     * [95]privacy policy
     * [96]rss feed

     *
     *

   testimonials

one stop resource

   great effort from team bdms and crayon data to put up a portal like
   this. great work.
   diana bhathena
   contributor

something unique

   big data made simple is one of the best big data content portals that i
   know. great resource. keep it up.
   manu jeevan prakash
   big data blogger
   our event partners

   subscribe
   subscribe to our newsletter to get regular updates on latest tech
   trends, news etc...
   name
   ____________________
   email *
   ____________________
   subscribe

   copyright    2018 crayon data. all rights reserved.

pin it on pinterest

     * [97]   
     * [98]   
     * [99]   

references

   visible links
   1. https://bigdata-madesimple.com/feed/
   2. https://bigdata-madesimple.com/comments/feed/
   3. https://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/feed/
   4. https://bigdata-madesimple.com/wp-json/oembed/1.0/embed?url=https://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/
   5. https://www.googletagmanager.com/ns.html?id=gtm-mltth6d
   6. https://bigdata-madesimple.com/
   7. http://www.crayondata.com/
   8. https://bigdata-madesimple.com/
   9. https://bigdata-madesimple.com/sectors/
  10. https://bigdata-madesimple.com/sectors/banking-finance/
  11. https://bigdata-madesimple.com/sectors/retail-ecom/
  12. https://bigdata-madesimple.com/sectors/travel-hospitality/
  13. https://bigdata-madesimple.com/sectors/privacy-security/
  14. https://bigdata-madesimple.com/sectors/marketing/
  15. https://bigdata-madesimple.com/sectors/media-and-entertainment/
  16. https://bigdata-madesimple.com/sectors/telecommunications/
  17. https://bigdata-madesimple.com/sectors/crime-law/
  18. https://bigdata-madesimple.com/sectors/sports/
  19. https://bigdata-madesimple.com/sectors/health-pharma/
  20. https://bigdata-madesimple.com/sectors/education/
  21. https://bigdata-madesimple.com/sectors/human-resource/
  22. https://bigdata-madesimple.com/tech-and-tools/
  23. https://bigdata-madesimple.com/tech-and-tools/analytics/
  24. https://bigdata-madesimple.com/tech-and-tools/data-science/
  25. https://bigdata-madesimple.com/tech-and-tools/business-intelligence/
  26. https://bigdata-madesimple.com/tech-and-tools/digital-personalization/
  27. https://bigdata-madesimple.com/tech-and-tools/machine-learning/
  28. https://bigdata-madesimple.com/tech-and-tools/artificial-intelligence/
  29. https://bigdata-madesimple.com/tech-and-tools/visualization/
  30. https://bigdata-madesimple.com/tech-and-tools/hadoop/
  31. https://bigdata-madesimple.com/tech-and-tools/data-mining/
  32. https://bigdata-madesimple.com/tech-and-tools/sql/
  33. https://bigdata-madesimple.com/tech-and-tools/nosql/
  34. https://bigdata-madesimple.com/resources/
  35. https://bigdata-madesimple.com/resources/interviews/
  36. https://bigdata-madesimple.com/resources/tutorials/
  37. https://bigdata-madesimple.com/resources/infographics/
  38. https://bigdata-madesimple.com/resources/video/
  39. https://bigdata-madesimple.com/products/
  40. https://bigdata-madesimple.com/events-all/
  41. https://bigdata-madesimple.com/write-for-us/
  42. https://bigdata-madesimple.com/contact-us/
  43. https://bigdata-madesimple.com/advertise/
  44. https://bigdata-madesimple.com/partnerships/
  45. https://bigdata-madesimple.com/contact-us/
  46. https://bigdata-madesimple.com/tech-and-tools/machine-learning/
  47. https://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/
  48. https://bigdata-madesimple.com/author/manu-jeevan/
  49. http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html
  50. http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues
  51. https://bigdata-madesimple.com/author/manu-jeevan/
  52. https://www.linkedin.com/in/manujeevan/
  53. mailto:manu@bigdataexaminer.com
  54. https://bigdata-madesimple.com/how-to-get-a-50k-job-as-a-data-analyst-almost-for-free-without-a-college-degree/
  55. https://bigdata-madesimple.com/should-your-business-adopt-hadoop/
  56. http://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/#respond
  57. https://bigdata-madesimple.com/machine-learning-best-examples-and-ideas-for-mobile-apps/
  58. https://bigdata-madesimple.com/machine-learning-best-examples-and-ideas-for-mobile-apps/
  59. https://bigdata-madesimple.com/machine-learning-best-examples-and-ideas-for-mobile-apps/
  60. https://bigdata-madesimple.com/author/premjith-bpk/
  61. https://bigdata-madesimple.com/machine-learning-best-examples-and-ideas-for-mobile-apps/
  62. https://bigdata-madesimple.com/three-mistakes-to-avoid-if-you-are-new-to-machine-learning/
  63. https://bigdata-madesimple.com/three-mistakes-to-avoid-if-you-are-new-to-machine-learning/
  64. https://bigdata-madesimple.com/three-mistakes-to-avoid-if-you-are-new-to-machine-learning/
  65. https://bigdata-madesimple.com/author/guest/
  66. https://bigdata-madesimple.com/three-mistakes-to-avoid-if-you-are-new-to-machine-learning/
  67. https://bigdata-madesimple.com/why-every-small-business-should-use-machine-learning/
  68. https://bigdata-madesimple.com/why-every-small-business-should-use-machine-learning/
  69. https://bigdata-madesimple.com/why-every-small-business-should-use-machine-learning/
  70. https://bigdata-madesimple.com/author/jaren-nichols/
  71. https://bigdata-madesimple.com/why-every-small-business-should-use-machine-learning/
  72. https://bigdata-madesimple.com/ai-driven-forex-trading-robot-first-kind/
  73. https://bigdata-madesimple.com/ai-driven-forex-trading-robot-first-kind/
  74. https://bigdata-madesimple.com/ai-driven-forex-trading-robot-first-kind/
  75. https://bigdata-madesimple.com/author/guest/
  76. https://bigdata-madesimple.com/ai-driven-forex-trading-robot-first-kind/
  77. https://bigdata-madesimple.com/6-revolutionary-things-to-know-about-machine-learning/
  78. https://bigdata-madesimple.com/6-revolutionary-things-to-know-about-machine-learning/
  79. https://bigdata-madesimple.com/6-revolutionary-things-to-know-about-machine-learning/
  80. https://bigdata-madesimple.com/author/jameswarner/
  81. https://bigdata-madesimple.com/6-revolutionary-things-to-know-about-machine-learning/
  82. https://bigdata-madesimple.com/how-to-get-started-with-machine-learning-in-python/
  83. https://bigdata-madesimple.com/how-to-get-started-with-machine-learning-in-python/
  84. https://bigdata-madesimple.com/how-to-get-started-with-machine-learning-in-python/
  85. https://bigdata-madesimple.com/author/guest/
  86. https://bigdata-madesimple.com/how-to-get-started-with-machine-learning-in-python/
  87. https://bigdata-madesimple.com/about-us/
  88. https://bigdata-madesimple.com/events-all/
  89. https://bigdata-madesimple.com/newsletter/
  90. https://bigdata-madesimple.com/advertise/
  91. https://bigdata-madesimple.com/write-for-us/
  92. https://bigdata-madesimple.com/guidelines-for-contributors/
  93. https://bigdata-madesimple.com/contact-us/
  94. https://bigdata-madesimple.com/partnerships/
  95. https://bigdata-madesimple.com/privacy-policy/
  96. https://bigdata-madesimple.com/feed/
  97. http://www.facebook.com/sharer.php?u=https://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/&t=decoding id84, pca and svd
  98. http://twitter.com/share?text=decoding id84, pca and svd&url=https://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/&via=big data made simple
  99. http://www.linkedin.com/sharearticle?mini=true&url=https://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/&title=decoding id84, pca and svd

   hidden links:
 101. https://www.sisense.com/whitepapers/compare-top-embedded-bi-vendors-using-g2-crowd/?utm_source=bigdatamadesimple&utm_medium=cpc&utm_campaign=mediabuy&ad=g2crowdembedded_leaderboard
 102. http://facebook.com/https://www.facebook.com/manujeevan
 103. http://twitter.com/https://twitter.com/manujeevaan
 104. https://www.sisense.com/whitepapers/compare-top-embedded-bi-vendors-using-g2-crowd/?utm_source=bigdatamadesimple&utm_medium=cpc&utm_campaign=mediabuy&ad=g2crowdembedded_leaderboard
 105. https://www.sisense.com/whitepapers/compare-top-embedded-bi-vendors-using-g2-crowd/?utm_source=bigdatamadesimple&utm_medium=cpc&utm_campaign=mediabuy&ad=g2crowdembedded_sidebar
 106. https://twitter.com/datamadesimple
 107. https://www.facebook.com/bigdatamadesimple/
 108. https://www.mindtheproduct.com/mtpcon/singapore/
