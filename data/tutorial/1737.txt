   #[1]joseph wilk

   joseph wilk

                                 [2]joseph wilk

things with code, creativity and computation.

     * [3]rss

     * [4]home
     * [5]about
     * [6]speaking
     * [7]art
     * [8]contact
     * [9]archives

latent semantic analysis in python

   dec 19th, 2007

   latent semantic analysis (lsa) is a mathematical method that tries to
   bring out latent relationships within a collection of documents. rather
   than looking at each document isolated from the others it looks at all
   the documents as a whole and the terms within them to identify
   relationships.

   an example of lsa: using a search engine search for    sand   .

   documents are returned which do not contain the search term    sand    but
   contains terms like    beach   .

   lsa has identified a latent relationship,    sand    is semantically close
   to    beach   .

   there are some very good papers which describing lsa in detail:
     * an introduction to lsa:
       [10]http://lsa.colorado.edu/papers/dp1.lsaintro.pdf
     * creating your own lsa space:
       [11]http://www.andrew.cmu.edu/user/jquesada/pdf/bookspacesrev1.pdf
     * latent semantic analysis:
       [12]http://en.wikipedia.org/wiki/latent_semantic_indexing

   this is an implementation of lsa in python (2.4+). thanks to scipy its
   rather simple!

1 create the term-document matrix

   we use the previous work in [13]vector space search to build this
   matrix.

2 tf-idf transform

   apply the tf-idf transform to the term-document matrix. this generally
   tends to help improve results with lsa.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

def tfidftransform(self,):
        """ apply termfrequency(tf)*inversedocumentfrequency(idf) for each matri
x element.
            this evaluates how important a word is to a document in a corpus

            with a document-term matrix: matrix[x][y]
                tf[x][y] = frequency of term y in document x / frequency of all
terms in document x
                idf[x][y] = log( abs(total number of documents in corpus) / abs(
number of documents with term y)  )
            note: this is not the only way to calculate tf*idf
        """

        documenttotal = len(self.matrix)
        rows,cols = self.matrix.shape

        for row in xrange(0, rows): #for each document

                wordtotal= reduce(lambda x, y: x+y, self.matrix[row] )

                for col in xrange(0,cols): #for each term

                        #for consistency ensure all self.matrix values are float
s
                        self.matrix[row][col] = float(self.matrix[row][col])

                        if self.matrix[row][col]!=0:

                                termdocumentoccurences = self.__gettermdocumento
ccurences(col)

                                termfrequency = self.matrix[row][col] / float(wo
rdtotal)
                                inversedocumentfrequency = log(abs(documenttotal
 / float(termdocumentoccurences)))
                                self.matrix[row][col]=termfrequency*inversedocum
entfrequency

3 singular value decomposition

   svd: [14]http://en.wikipedia.org/wiki/singular_value_decomposition

   determine u, sigma, vt from our matrix from previous steps.
 u . sigma . vt = matrix

   we use the [15]scipy svd implementation here. note that numpy (version:
   1.0.4 ) also has an implementation of svd but i had lots of problems
   with it. i found it did not work with anything larger than a 2
   dimensional matrix.
1
2
3

#sigma comes out as a list rather than a matrix
 u,sigma,vt = linalg.svd(self.matrix)


4 reduce the dimensions of sigma

   we generally delete the smallest coefficients in the diagonal matrix
   sigma to produce sigma   . the reduction of the dimensions of sigma
   combines some dimensions such that they are on more than one term. the
   number of coefficients deleted can depend of the corpus used. it should
   be large enough to fit the real structure in the data, but small enough
   such that noise or unimportant details are not modelled.

   the real difficulty and weakness of lsa is knowing how many dimensions
   to remove. there is no exact method of finding the right dimensions.
   generally [16]l2-norm or [17]frobenius norm are used.

5 calculate the product with new sigma   

   finally we calculate:
u . sigma' . vt = matrix'

1
2

#reconstruct matrix'
reconstructedmatrix= dot(dot(u,linalg.diagsvd(sigma,len(self.matrix),len(vt))),v
t)

   giving use our final lsa matrix. we can now apply the same
   functionality used in vector space search: searching and finding
   related scores for documents.

lsa in action     matrices

   we start with out model-term frequency matrix with is generated from
   creating a [18]vector space search with four documents (d1-d4): d1:   the
   cat in the hat disabled    d2:   a cat is a fine pet ponies.    d3:   dogs and
   cats make good pets    d4:   i haven   t got a hat.   
    <strong>good  pet   hat   make  dog   cat   poni  fine  disabl</strong>
d1 [+0.00 +0.00 +1.00 +0.00 +0.00 +1.00 +0.00 +0.00 +1.00 ]
d2 [+0.00 +1.00 +0.00 +0.00 +0.00 +1.00 +1.00 +1.00 +0.00 ]
d3 [+1.00 +1.00 +0.00 +1.00 +1.00 +1.00 +0.00 +0.00 +0.00 ]
d4 [+0.00 +0.00 +1.00 +0.00 +0.00 +0.00 +0.00 +0.00 +0.00 ]

   apply tf-idf transform:
d1 [+0.00 +0.00 +0.23 +0.00 +0.00 +0.10 +0.00 +0.00 +0.46 ]
d2 [+0.00 +0.17 +0.00 +0.00 +0.00 +0.07 +0.35 +0.35 +0.00 ]
d3 [+0.28 +0.14 +0.00 +0.28 +0.28 +0.06 +0.00 +0.00 +0.00 ]
d4 [+0.00 +0.00 +0.69 +0.00 +0.00 +0.00 +0.00 +0.00 <span style="color: #ff0000;
"><strong>+0.00</strong></span> ]

   perform svd     reduce sigmas dimensions(removing the 2 smallest
   coefficients)
d1 [+0.01 +0.01 +0.34 +0.01 +0.01 +0.03 +0.02 +0.02 +0.11 ]
d2 [-0.00 +0.17 -0.01 -0.00 -0.00 +0.08 +0.35 +0.35 +0.02 ]
d3 [+0.28 +0.14 -0.01 +0.28 +0.28 +0.06 -0.00 -0.00 +0.02 ]
d4 [-0.01 -0.01 +0.63 -0.01 -0.01 +0.04 -0.01 -0.01 <strong><span style="color:
#ff0000;">+0.19</span></strong> ]

   note the word    disabl    despite not being in d4 now has a weighting in
   that document.

dependencies

   [19]http://www.scipy.org/

problems

   lsa assumes the [20]normal distribution where the [21]poisson
   distribution has actually been observed.

source code

   available at github:
git clone git://github.com/josephwilk/semanticpy.git

   posted by admin dec 19th, 2007 [22]projects
   [23]tweet

   [24]   building a vector space search engine in python [25]automatic
   admin systems - semantics with rails & django   

comments

   please enable javascript to view the [26]comments powered by disqus.

recent posts

     * [27]emacs as a musical instrument
     * [28]functions explained through patterns
     * [29]audio fingerprint smudges
     * [30]visuals with overtone and shadertone
     * [31]animations with emacs
     * [32]clojure and kinesis at scale
     * [33]clojure and overtone driving minecraft
     * [34]live coding - repl electric
     * [35]creative machines
     * [36]sounds of the human brain
     * [37]building clojure services at scale

github repos

     * status updating...

   [38]@josephwilk on github

   copyright    2017 - joseph wilk - powered by [39]octopress

references

   1. http://blog.josephwilk.net/atom.xml
   2. http://blog.josephwilk.net/
   3. http://blog.josephwilk.net/atom.xml
   4. http://blog.josephwilk.net/
   5. http://blog.josephwilk.net/about
   6. http://blog.josephwilk.net/speaking
   7. http://art.josephwilk.net/
   8. http://blog.josephwilk.net/contact
   9. http://blog.josephwilk.net/archives
  10. http://lsa.colorado.edu/papers/dp1.lsaintro.pdf
  11. http://www.andrew.cmu.edu/user/jquesada/pdf/bookspacesrev1.pdf
  12. http://en.wikipedia.org/wiki/latent_semantic_indexing
  13. http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html
  14. http://en.wikipedia.org/wiki/singular_value_decomposition
  15. http://www.scipy.org/
  16. http://mathworld.wolfram.com/l2-norm.html
  17. http://en.wikipedia.org/wiki/frobenius_norm#frobenius_norm
  18. http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html
  19. http://www.scipy.org/
  20. http://en.wikipedia.org/wiki/normal_distribution
  21. http://en.wikipedia.org/wiki/poisson_distribution
  22. http://blog.josephwilk.net/projects
  23. http://twitter.com/share
  24. http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html
  25. http://blog.josephwilk.net/critique/automatic-admin-systems-semantics-with-rails-and-django.html
  26. http://disqus.com/?ref_noscript
  27. http://blog.josephwilk.net/art/emacs-as-a-musical-instrument.html
  28. http://blog.josephwilk.net/clojure/functions-explained-through-patterns.html
  29. http://blog.josephwilk.net/art/audio-fingerprint-smudges.html
  30. http://blog.josephwilk.net/art/overtone-shader-visuals.html
  31. http://blog.josephwilk.net/art/emacs-animation.html
  32. http://blog.josephwilk.net/clojure/clojure-and-kinesis-at-scale.html
  33. http://blog.josephwilk.net/clojure/overtone-driving-minecraft.html
  34. http://blog.josephwilk.net/art/live-coding-repl-electric.html
  35. http://blog.josephwilk.net/clojure/creative_machines.html
  36. http://blog.josephwilk.net/clojure/sounds-of-the-human-brain.html
  37. http://blog.josephwilk.net/clojure/building-clojure-services-at-scale.html
  38. https://github.com/josephwilk
  39. http://octopress.org/
