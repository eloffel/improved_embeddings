   #[1]deniz yuret's homepage - atom [2]deniz yuret's homepage - rss
   [3]deniz yuret's homepage - atom

[4]deniz yuret's homepage

   [5]ai.ku   [6]bibtex   [7]courses   [8]cv   [9]downloads   [10]github
   [11]publications   [12]projects   [13]scholar   [14]students

august 26, 2005

[15]singular value decomposition notes

   this post is dedicated to figuring out why svd works when it does. the
   image below is a java applet that demonstrates the use of svd for image
   compression. click on the image and use the arrow keys on your keyboard
   to change the rank of the pixel matrix. up/down keys change the rank by
   a factor of two, left/right keys increase and decrease the rank by one.
   the original image is 256x256. it is still recognizable when we reduce
   it to 16x16. the comments below are notes i took while i was trying to
   understand svd.
   labels: [16]machinelearning

15 comments:

   [17]deniz said...
          id45
          id45 is an idea to improve document
          retrieval performance by making the system less sensitive to
          word choice. "lsi explicitly represents terms and documents in a
          rich, high-dimensional space, allowing the underlying
          (``latent''), semantic relationships between terms and documents
          to be exploited during searching." a large incidence matrix is
          constructed where rows correspond to words, columns to
          documents, and entries to the number of times the words occurs
          in the documents. the entries are rescaled according to some
          weighting scheme. then (this is the important step) the matrix
          is reduced in dimensionality using singular value decomposition.
          now why should this work?
          documents, terms, and queries are represented as points in the
          same reduced dimension term-document space. queries are just
          mini documents. what about terms? and what does it mean for a
          document to be a "point" in the reduced term-document space
          while it was a whole column in the original incidence matrix? we
          need to understand svd first to figure the rest out. the oft
          cited reference seems to be:
          http://www.cs.utk.edu/~library/techreports/1994/ut-cs-94-270.ps.
          z
          using id202 for intelligent information retrieval.
          michael w. berry, susan t. dumais, and gavin w. o'brien,
          december 1994. published in siam review 37:4 (1995), pp.
          573-595.
          link: http://www.cs.utk.edu/~berry/lsi++/node8.html

          [18]august 26, 2005 [19][icon_delete13.gif]

   [20]deniz said...
          latent semantic analysis
          latent semantic analysis (lsa) is a mathematical/statistical
          technique for extracting and representing the similarity of
          meaning of words and passages by analysis of large bodies of
          text. it uses singular value decomposition, a general form of
          factor analysis, to condense a very large matrix of
          word-by-context data into a much smaller, but still
          large-typically 100-500 dimensional-representation (deerwester,
          dumais, furnas, landauer & harshman, 1990). the right number of
          dimensions appears to be crucial; the best values yield up to
          four times as accurate simulation of human judgments as ordinary
          co-occurence measures.
          link: [21]
          http://lsa.colorado.edu

          [22]august 26, 2005 [23][icon_delete13.gif]

   [24]deniz said...
          singular value decomposition i
          from wikipedia: "the geometric content of the svd theorem can
          thus be summarized as follows: for every linear map t: kn     km
          one can find orthonormal bases of kn and km such that t maps the
          i-th basis vector of kn to a non-negative multiple of the i-th
          basis vector of km, and sends the left-over basis vectors to
          zero. with respect to these bases, the map t is therefore
          represented by a diagonal matrix with non-negative real diagonal
          entries."
          what does this mean when the matrix is an incidence matrix of
          words and documents?
          here are a bunch of statements about svd from various sources:
          - the svd represents an expansion of the original data in a
          coordinate system where the covariance matrix is diagonal.
          - performing pca is the equivalent of performing singular value
          decomposition (svd) on the covariance matrix of the data.
          - the ith singular value indicates the amount of variation along
          the ith axis.
          link: [25]
          http://en.wikipedia.org/wiki/singular_value_decomposition

          [26]august 26, 2005 [27][icon_delete13.gif]

   [28]deniz said...
          trefethen book
          this book seems to have a good explanation and chapters 4 and 5
          are available online. basically, a matrix can be identified with
          the hyper-ellipse it takes the unit sphere to. the unit vectors
          along the principle axes of the ellipse form the columns of the
          u matrix (left singular vectors), their magnitudes form the
          diagonal s matrix (singular values), and their pre-images on the
          unit sphere form the columns of the v matrix (right singular
          vectors). the action of any matrix can be summarized as
          rotate-stretch-rotate. a=usv* means first rotate the pre-images
          to align with the axes (v* is the inverse of v. v, like any
          matrix, takes the unit vectors to its columns), then stretch the
          axes by amounts specified in s, then rotate to the target
          orientation using u. both u and v are unitary matrices (their
          rows and columns form orthonormal bases). u is defined to be
          that way, but i am not sure why v has to be.
          one confusion is there is more than one way to define the
          dimensions of u, v, s. standard is to define u and v as square
          and s has the same dimensions as a. another is to define s as
          square and adjust u and v accordingly. there are only rank(a)
          non-zero entries on s, so the rest of the values are padded with
          zeros. we can further reduce s by throwing away the smallest
          values, in which case we have a lower rank object which is the
          closest approximation (by several measures) to a.
          svd is different than eigen decomposition since eigen-vectors do
          not have to be orthogonal. (a matrix takes eigen-vectors to
          their own multiples). when they are orthogonal, eigens are just
          a special case of svd.
          now that we got the geometric interpretation pat, all that is
          left is to figure out what goes on when we have a covariance
          matrix or a data matrix. these matrices are not very intuitive
          to think about as geometric transforms.
          link: [29]
          http://web.comlab.ox.ac.uk/oucl/work/nick.trefethen/text.html

          [30]august 26, 2005 [31][icon_delete13.gif]

   [32]deniz said...
          data matrices
          coming back to the question of why svd makes sense for data
          matrices (matrices whose columns are high dimensional data
          points, e.g. documents expressed as word frequency vectors, or
          experiments expressed as gene expression vectors). at first
          visualizing the geometric interpretation of the svd (the unit
          sphere being transformed into a hyper-ellipsoid) on an arbitrary
          data matrix proved difficult. what is the action of a data
          matrix? according to the standard transform concept, a matrix
          transforms vectors in its row space into vectors in its column
          space. this meant a term-document matrix transforms a term into
          a document, or a gene matrix transforms a gene into an
          experiment, which made no sense.
          the great trefethen book starts with one simple observation: ax
          is a linear combination of columns of a. this is an alternative
          viewpoint to the usual: ax is a transformation of x into a new
          vector. but it makes certain things easier to understand. for
          example a matrix transforms unit vectors into its column
          vectors. the alternative is that a unit vector selects a column
          of the matrix. this may sound like hair splitting but let's look
          at its consequence for the data matrices. each unit vector (do
          not interpret these as entities in the row space like a new gene
          or a new term, but just as a selector), picks a column (i.e. one
          of the data points). thus the images of these unit vectors give
          us the cloud of data points (documents or gene experiments) in
          their multidimensional space. back to the svd concept of the
          unit sphere going into the ellipsoid. the cloud of data points
          all lie at the edges of our ellipsoid (because all the unit
          vectors they come from lie at the edges of the unit sphere).
          they determine its outline. we would expect the extent of the
          ellipsoid along a certain direction to be related to the spread
          of our data along that direction.
          what does reduced svd do? well imagine an ellipsoid shaped like
          a convex lens. i.e. one of its dimensions is dwarfed by the
          others. reducing svd will get rid of that dimension and turn the
          thing into a circle. why would you have a dwarf dimension to
          start with? well, maybe one of the genes is expressed very
          little compared to others. one of the document terms is seen
          very rarely. in that case you may not want the elimination, so
          maybe you should rescale that dimension. another reason may be
          several terms or genes that are correlated. in the perfect
          correlation case our ellipse would lie on the 45 degree
          hyperplane between those two axes, thus both terms expressed as
          a single dimension. i imagine this is the more relevant effect
          everybody is after. manning's document term example has two docs
          that have no words in common, so originally they look orthogonal
          with 0 similarity. but words from both documents co-occur
          frequently in other documents, so svd collapses them and in the
          modified space the two docs look pretty similar.

          [33]august 26, 2005 [34][icon_delete13.gif]

   [35]deniz said...
          svd on images
          svd can also be applied to images (represented as a matrix of
          pixels) to compress them. this is very different from data
          matrices where each column represents an individual object of
          interest. the reason svd works in the image application is that
          when one eliminates the smallest singular values and computes
          the reduced rank matrix, the result is the closest possible to
          the original for that rank. it is closest in various definitions
          of close, but the relevant one for images is the least squares
          of individual pixel value differences. here is an applet to
          demonstrate:
          use the arrow keys of your keyboard to change the rank of the
          matrix, left/right goes by one, up/down goes by factors of two.
          you may have to click on the image first.
          [36]
          http://home.ku.edu.tr/~dyuret/java/svd.html

          [37]august 26, 2005 [38][icon_delete13.gif]

   [39]deniz said...
          next step is to understand covariance matrices and the statement
          "pca is equivalent to performing svd on the covariance matrix".
          there is also the relation with id91.
          http://crd.lbl.gov/~cding/spectral talks about the equivalence
          of id116 id91 and the pca.

          [40]september 08, 2005 [41][icon_delete13.gif]

   [42]deniz said...
          here is another excerpt that tries to explain the relation of
          svd, pca and eigen decomposition:
          formally: there is a direct relation between pca and svd in the
          case where principal components are calculated from the
          covariance matrix. if one conditions the data matrix x by
          centering each column, then x'x = sum gi gi' is proportional to
          the covariance matrix of the variables of gi. by diagonalization
          of x'x yields v', which also yields the principal components of
          {gi}. so, the right singular vectors {vk} are the same as the
          principal components of {gi}. the eigenvalues of x'x are
          equivalent to s_k^2, which are proportional to the variances of
          the principal components. the matrix us then contains the
          principal component scores, which are the coordinates of the
          activations in the space of principal components.

          [43]september 09, 2005 [44][icon_delete13.gif]

   [45]deniz said...
          some thoughts from last night:
          1. i need to go over the proofs but basically both ripley and
          trefethen books seem to suggest svd and pca based on covariance
          matrix give the same results, and the svd solution is more
          stable
          2. i have been thinking about irrelevant and redundant features.
          here are two simple observations: if you add extra dimensions to
          a data set (a) distances always increase, (b) separable points
          stay separable, non-separable ones may become separable (x^2 id166
          example). the opposite is true for decreasing dimensions
          naturally. sets that are not separable will not become separable
          when you get rid of dimensions. this argues that discriminant
          approaches like id166s will probably do well in the presence of
          extra dimensions, whereas nearest neighbor, id91 type
          approaches will be misled. (id166 will find a separation along the
          correct dimensions if one exists but the random dims may prevent
          it from finding the largest margin one).
          3. why would you ever want to decrease the number of dimensions?
          it obviously doesn't help separability. some ideas:
          - to get better similarity: irrelevant dims may make similar
          points look different (but then how do you know svd gives the
          right solution, maybe the highest variation directions are the
          noise)
          - to uncover correlations: the astronaut text example from
          manning.
          - fight with sparse data: if the data is the tip of a histogram,
          sparseness prevents the real distribution difficult to estimate.
          (again astronaut text example). i think in that case dimensions
          of svd correspond to whole distributions, and the mapping to
          lower dimensions allows the tip of histograms to be classified
          as the right histogram learnt from more frequent words. (i
          should look at the dims of the svd in peter's data to see what
          sort of distributions they give)
          4. this may argue that svd actually performs id91. imagine
          n word distributions which have no intersection (orthogonal).
          wouldn't svd have the first n vectors point to the centers of
          these clusters?
          5. svd is just one way to project data. if we have a quality
          metric (like how well similar pairs can be distinguished from
          dissimilar pairs), one can numerically search for dimensions
          along which this distinction can be made better. see projection
          pursuit.
          6. finally our data may be such that each semantic relation is
          represented along distinct dimensions, rather than by some
          linear combination of common dimensions. will any kind of
          projection work in such a problem?

          [46]september 11, 2005 [47][icon_delete13.gif]

   [48]deniz said...
          some observations from looking at the spectral id91 slides
          by ding with alkan yesterday:
          1. there is incredible symmetry in the problem. no wonder why i
          am so confused. take the data matrix. find cross products
          between features. that is the covariance matrix. find cross
          products between observations. that is the similarity matrix.
          they are duals of each other. (guess you have to make the means
          zero first.)
          2. the eigens of the covariance matrix are the same as the
          singular vectors of the data matrix. i bet the eigens of the
          similarity matrix are the other singular vectors. spectral
          id91 works basically by finding the eigen vectors of the
          laplacian matrix of a graph. laplacian matrix is the adjacency
          matrix with (negative) degree information on the diagonal. a
          similarity matrix is sort of a continuous analogue of the
          adjacency matrix. (at least i am guessing that's how people use
          spectral id91.) interestingly it is the vectors with
          smallest eigen values that is of interest in spectral
          id91.

          [49]september 14, 2005 [50][icon_delete13.gif]

   [51]deniz said...
          svd vs pca:
          a = usv'
          a'a = vsu'usv' = v(s^2)v'
          a'a is the covariance matrix and v(s^2)v' is its eigen
          decomposition. still would be nice to have a geometric
          interpretation.

          [52]october 21, 2005 [53][icon_delete13.gif]

   [54]deniz said...
          to stop confusing rows and columns this note will summarize what
          to do to get compressed vectors:
          1. a is mxn, columns are objects, rows are attributes. so there
          are n objects with m attributes. a transforms rn -> rm. svd
          produces a=usv'. columns of u are orthonormal vectors in r^m.
          when reduced, u has r column vectors in r^m, s is rxr, and v has
          r column vectors in r^n (if not reduced => r=n). to reduce an
          object v from m dimensions to r dimensions we use u'v. to reduce
          all n objects to the new r dimensional coordinate system use
          u'a. we can just use transpose because the columns of u are
          orthonormal.
          2. b is nxm, rows are objects, columns are attributes. so there
          are n objects with m attributes. b transforms rm -> rn. svd
          produces b=usv'. columns of u are orthonormal vectors in r^n.
          when reduced, u has r column vectors in r^n, s is rxr, and v has
          r column vectors in r^m (if not reduced => r=n). in fact, b=a'
          since (usv')' = vsu', we obtain the same three matrices for a
          and b, except u and v have changed roles. to reduce an object v
          (row vector) from m dimensions to r dimensions we can use vv. to
          reduce all objects to r dimensions we can use bv (which is just
          the transpose of u'a from last example.

          [55]november 25, 2005 [56][icon_delete13.gif]

   [57]eric thomson said...
          i wrote up a little primer on svd that includes link to matlab
          code i wrote for visualizing the svd that can be found [58]here.
          your site provided some useful references.

          [59]february 22, 2008 [60][icon_delete13.gif]

   [61]chetan j said...
          nice posts !
          informative ..
          i have a doubt ..
          assume a ,is a matrix which is the sum of i(identity) and a*b'
          for some vectors a and b.
          what can u say about (a'*a)'s eigen vectors, values
          and also svd's

          [62]august 08, 2008 [63][icon_delete13.gif]

   [64]deniz said...
          in my comment of aug 26, 2005 i wrongly claimed that a matrix
          takes the edges of the unit sphere to the edges of the
          hyper-ellipse. that is not true in general, the whole unit
          sphere goes to the whole hyper-ellipse, but edges are not
          necessarily mapped to one another. this does not invalidate the
          rest of my observations about the svd of the data matrix.
          in fact edges of the unit sphere correspond to unit length
          vectors in the row space. when the matrix is multiplied with a
          unit length vector, what comes out is a linear combination of
          its columns where the squares of the coefficients add up to one.

          [65]september 29, 2008 [66][icon_delete13.gif]

   [67]post a comment
   [68]newer post [69]older post [70]home
   subscribe to: [71]post comments (atom)
   [72]my photo

   deniz yuret
          ko   university
            stanbul turkey
          +90-212-338-1724
          dyuret@ku.edu.tr

labels

     * [73]books (28)
     * [74]language (48)
     * [75]links (16)
     * [76]machinelearning (52)
     * [77]math (12)
     * [78]notes (17)
     * [79]t  rk  e (55)
     * [80]videos (13)

popular posts

     * [81]alec radford's animations for optimization algorithms
     * [82]courses
     * [83]turkish language resources
     * [84]machine learning in 10 pictures
     * [85]introducing knet8: beginning deep learning with 100 lines of
       julia
     * [86]ergun's english-turkish machine translation notes
     * [87]mehmet ali yatbaz, ph.d. 2014
     * [88]naive bayes is a joint maximum id178 model
     * [89]emacs turkish mode
     * [90]some starting points for deep learning and id56s

archive

     * [91]     [92]2019 (1)
          + [93]     [94]jan 2019 (1)

     * [95]     [96]2018 (26)
          + [97]     [98]dec 2018 (3)
          + [99]     [100]nov 2018 (2)
          + [101]     [102]oct 2018 (2)
          + [103]     [104]sep 2018 (1)
          + [105]     [106]aug 2018 (1)
          + [107]     [108]jul 2018 (3)
          + [109]     [110]jun 2018 (2)
          + [111]     [112]may 2018 (10)
          + [113]     [114]apr 2018 (2)

     * [115]     [116]2017 (12)
          + [117]     [118]dec 2017 (1)
          + [119]     [120]sep 2017 (3)
          + [121]     [122]aug 2017 (2)
          + [123]     [124]jul 2017 (1)
          + [125]     [126]may 2017 (2)
          + [127]     [128]apr 2017 (2)
          + [129]     [130]feb 2017 (1)

     * [131]     [132]2016 (13)
          + [133]     [134]dec 2016 (4)
          + [135]     [136]nov 2016 (2)
          + [137]     [138]sep 2016 (1)
          + [139]     [140]aug 2016 (2)
          + [141]     [142]jun 2016 (2)
          + [143]     [144]mar 2016 (1)
          + [145]     [146]feb 2016 (1)

     * [147]     [148]2015 (14)
          + [149]     [150]dec 2015 (1)
          + [151]     [152]nov 2015 (1)
          + [153]     [154]jul 2015 (1)
          + [155]     [156]jun 2015 (1)
          + [157]     [158]may 2015 (3)
          + [159]     [160]apr 2015 (2)
          + [161]     [162]mar 2015 (2)
          + [163]     [164]feb 2015 (1)
          + [165]     [166]jan 2015 (2)

     * [167]     [168]2014 (21)
          + [169]     [170]nov 2014 (1)
          + [171]     [172]sep 2014 (1)
          + [173]     [174]aug 2014 (2)
          + [175]     [176]jun 2014 (3)
          + [177]     [178]may 2014 (3)
          + [179]     [180]apr 2014 (3)
          + [181]     [182]mar 2014 (1)
          + [183]     [184]feb 2014 (2)
          + [185]     [186]jan 2014 (5)

     * [187]     [188]2013 (11)
          + [189]     [190]dec 2013 (1)
          + [191]     [192]nov 2013 (2)
          + [193]     [194]oct 2013 (1)
          + [195]     [196]sep 2013 (1)
          + [197]     [198]jun 2013 (1)
          + [199]     [200]may 2013 (2)
          + [201]     [202]feb 2013 (2)
          + [203]     [204]jan 2013 (1)

     * [205]     [206]2012 (12)
          + [207]     [208]nov 2012 (1)
          + [209]     [210]oct 2012 (2)
          + [211]     [212]aug 2012 (1)
          + [213]     [214]jul 2012 (3)
          + [215]     [216]jun 2012 (2)
          + [217]     [218]apr 2012 (2)
          + [219]     [220]mar 2012 (1)

     * [221]     [222]2011 (11)
          + [223]     [224]oct 2011 (1)
          + [225]     [226]aug 2011 (1)
          + [227]     [228]jul 2011 (2)
          + [229]     [230]jun 2011 (1)
          + [231]     [232]may 2011 (2)
          + [233]     [234]apr 2011 (1)
          + [235]     [236]mar 2011 (2)
          + [237]     [238]jan 2011 (1)

     * [239]     [240]2010 (20)
          + [241]     [242]dec 2010 (1)
          + [243]     [244]nov 2010 (4)
          + [245]     [246]oct 2010 (2)
          + [247]     [248]sep 2010 (2)
          + [249]     [250]aug 2010 (1)
          + [251]     [252]jul 2010 (2)
          + [253]     [254]apr 2010 (1)
          + [255]     [256]feb 2010 (7)

     * [257]     [258]2009 (15)
          + [259]     [260]dec 2009 (1)
          + [261]     [262]nov 2009 (1)
          + [263]     [264]aug 2009 (3)
          + [265]     [266]jul 2009 (1)
          + [267]     [268]apr 2009 (2)
          + [269]     [270]mar 2009 (3)
          + [271]     [272]feb 2009 (3)
          + [273]     [274]jan 2009 (1)

     * [275]     [276]2008 (6)
          + [277]     [278]dec 2008 (1)
          + [279]     [280]oct 2008 (1)
          + [281]     [282]sep 2008 (1)
          + [283]     [284]aug 2008 (2)
          + [285]     [286]apr 2008 (1)

     * [287]     [288]2007 (34)
          + [289]     [290]dec 2007 (2)
          + [291]     [292]nov 2007 (1)
          + [293]     [294]oct 2007 (5)
          + [295]     [296]sep 2007 (2)
          + [297]     [298]jul 2007 (1)
          + [299]     [300]jun 2007 (7)
          + [301]     [302]may 2007 (2)
          + [303]     [304]apr 2007 (1)
          + [305]     [306]mar 2007 (2)
          + [307]     [308]feb 2007 (8)
          + [309]     [310]jan 2007 (3)

     * [311]     [312]2006 (58)
          + [313]     [314]dec 2006 (9)
          + [315]     [316]nov 2006 (6)
          + [317]     [318]oct 2006 (16)
          + [319]     [320]sep 2006 (10)
          + [321]     [322]jul 2006 (2)
          + [323]     [324]jun 2006 (7)
          + [325]     [326]may 2006 (7)
          + [327]     [328]apr 2006 (1)

     * [329]     [330]2005 (19)
          + [331]     [332]oct 2005 (4)
          + [333]     [334]sep 2005 (2)
          + [335]     [336]aug 2005 (2)
               o [337]singular value decomposition notes
               o [338]semantic relations
          + [339]     [340]jun 2005 (7)
          + [341]     [342]may 2005 (2)
          + [343]     [344]apr 2005 (2)

     * [345]     [346]2004 (3)
          + [347]     [348]oct 2004 (1)
          + [349]     [350]jul 2004 (1)
          + [351]     [352]apr 2004 (1)

     * [353]     [354]2002 (2)
          + [355]     [356]jun 2002 (1)
          + [357]     [358]mar 2002 (1)

     * [359]     [360]1998 (1)
          + [361]     [362]may 1998 (1)

     * [363]     [364]1994 (1)
          + [365]     [366]may 1994 (1)

my blog list

     * [icon18_wrench_allbkg.png]
       [367]slashdot
       [368]the nations of the amazon want the name back
       25 minutes ago
     * [icon18_wrench_allbkg.png]
       [369]shtetl-optimized
       [370]congratulations!
       2 hours ago
     * [icon18_wrench_allbkg.png]
       [371]less wrong
       [372]ssc sacramento meetup
       4 hours ago
     * [icon18_wrench_allbkg.png]
       [373]slate star codex
       [374]classified thread 7
       1 day ago
     * [icon18_wrench_allbkg.png]
       [375]ask a mathematician
       [376]q: will time travel ever be invented?
       4 days ago
     * [icon18_wrench_allbkg.png]
       [377]sean carroll
       [378]true facts about cosmology (or, misconceptions skewered)
       2 months ago
     * [icon18_wrench_allbkg.png]
       [379]cognitive medium
       [380]using spaced repetition systems to see through a piece of
       mathematics
       2 months ago
     * [icon18_wrench_allbkg.png]
       [381]andrej karpathy
       [382]cool! :)
       1 year ago

references

   visible links
   1. http://www.denizyuret.com/feeds/posts/default
   2. http://www.denizyuret.com/feeds/posts/default?alt=rss
   3. http://www.denizyuret.com/feeds/112506257981598112/comments/default
   4. http://www.denizyuret.com/
   5. http://ai.ku.edu.tr/
   6. http://goo.gl/hq0sz3
   7. http://goo.gl/5yftro
   8. http://goo.gl/wfy0ns
   9. http://goo.gl/ydyll9
  10. http://goo.gl/elujdc
  11. http://goo.gl/us4sf6
  12. http://goo.gl/vbk9jm
  13. http://goo.gl/lmgqrc
  14. http://goo.gl/vnpejp
  15. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html
  16. http://www.denizyuret.com/search/label/machinelearning
  17. https://www.blogger.com/profile/00578023665603100985
  18. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1125062700000#c112506270721829266
  19. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112506270721829266
  20. https://www.blogger.com/profile/00578023665603100985
  21. http://lsa.colorado.edu/
  22. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1125062820000#c112506286346668051
  23. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112506286346668051
  24. https://www.blogger.com/profile/00578023665603100985
  25. http://en.wikipedia.org/wiki/singular_value_decomposition
  26. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1125062940000#c112506297209041552
  27. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112506297209041552
  28. https://www.blogger.com/profile/00578023665603100985
  29. http://web.comlab.ox.ac.uk/oucl/work/nick.trefethen/text.html
  30. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1125063000000#c112506303767925665
  31. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112506303767925665
  32. https://www.blogger.com/profile/00578023665603100985
  33. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1125063060000#c112506308231627643
  34. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112506308231627643
  35. https://www.blogger.com/profile/00578023665603100985
  36. http://home.ku.edu.tr/~dyuret/java/svd.html
  37. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1125063300000#c112506333181983528
  38. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112506333181983528
  39. https://www.blogger.com/profile/00578023665603100985
  40. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1126185780000#c112618579897327277
  41. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112618579897327277
  42. https://www.blogger.com/profile/00578023665603100985
  43. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1126273260000#c112627329634272679
  44. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112627329634272679
  45. https://www.blogger.com/profile/00578023665603100985
  46. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1126419960000#c112642000624443959
  47. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112642000624443959
  48. https://www.blogger.com/profile/00578023665603100985
  49. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1126711200000#c112671125422054478
  50. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112671125422054478
  51. https://www.blogger.com/profile/00578023665603100985
  52. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1129906380000#c112990643459141764
  53. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=112990643459141764
  54. https://www.blogger.com/profile/00578023665603100985
  55. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1132951260000#c113295131974580038
  56. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=113295131974580038
  57. https://www.blogger.com/profile/06847717704454032165
  58. http://neurochannels.blogspot.com/2008/02/visualizing-svd.html
  59. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1203642900000#c7645378545902558735
  60. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=7645378545902558735
  61. https://www.blogger.com/profile/10012161057511130758
  62. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1218216060000#c3408259238413988675
  63. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=3408259238413988675
  64. https://www.blogger.com/profile/00578023665603100985
  65. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html?showcomment=1222686660000#c1264434009861987116
  66. https://www.blogger.com/delete-comment.g?blogid=8540876&postid=1264434009861987116
  67. https://www.blogger.com/comment.g?blogid=8540876&postid=112506257981598112
  68. http://www.denizyuret.com/2005/09/basak-mutlum-ms-2005.html
  69. http://www.denizyuret.com/2005/08/semantic-relations.html
  70. http://www.denizyuret.com/
  71. http://www.denizyuret.com/feeds/112506257981598112/comments/default
  72. http://www.blogger.com/profile/00578023665603100985
  73. http://www.denizyuret.com/search/label/books
  74. http://www.denizyuret.com/search/label/language
  75. http://www.denizyuret.com/search/label/links
  76. http://www.denizyuret.com/search/label/machinelearning
  77. http://www.denizyuret.com/search/label/math
  78. http://www.denizyuret.com/search/label/notes
  79. http://www.denizyuret.com/search/label/t  rk  e
  80. http://www.denizyuret.com/search/label/videos
  81. http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html
  82. http://www.denizyuret.com/2009/01/classes.html
  83. http://www.denizyuret.com/2006/11/turkish-resources.html
  84. http://www.denizyuret.com/2014/02/machine-learning-in-5-pictures.html
  85. http://www.denizyuret.com/2015/02/beginning-deep-learning-with-500-lines.html
  86. http://www.denizyuret.com/2009/02/erguns-english-turkish-machine.html
  87. http://www.denizyuret.com/2007/10/mehmet-ali-yatbaz-ms-2007.html
  88. http://www.denizyuret.com/2010/11/naive-bayes-is-joint-maximum-id178.html
  89. http://www.denizyuret.com/2006/11/emacs-turkish-mode.html
  90. http://www.denizyuret.com/2014/11/some-starting-points-for-deep-learning.html
  91. javascript:void(0)
  92. http://www.denizyuret.com/2019/
  93. javascript:void(0)
  94. http://www.denizyuret.com/2019/01/
  95. javascript:void(0)
  96. http://www.denizyuret.com/2018/
  97. javascript:void(0)
  98. http://www.denizyuret.com/2018/12/
  99. javascript:void(0)
 100. http://www.denizyuret.com/2018/11/
 101. javascript:void(0)
 102. http://www.denizyuret.com/2018/10/
 103. javascript:void(0)
 104. http://www.denizyuret.com/2018/09/
 105. javascript:void(0)
 106. http://www.denizyuret.com/2018/08/
 107. javascript:void(0)
 108. http://www.denizyuret.com/2018/07/
 109. javascript:void(0)
 110. http://www.denizyuret.com/2018/06/
 111. javascript:void(0)
 112. http://www.denizyuret.com/2018/05/
 113. javascript:void(0)
 114. http://www.denizyuret.com/2018/04/
 115. javascript:void(0)
 116. http://www.denizyuret.com/2017/
 117. javascript:void(0)
 118. http://www.denizyuret.com/2017/12/
 119. javascript:void(0)
 120. http://www.denizyuret.com/2017/09/
 121. javascript:void(0)
 122. http://www.denizyuret.com/2017/08/
 123. javascript:void(0)
 124. http://www.denizyuret.com/2017/07/
 125. javascript:void(0)
 126. http://www.denizyuret.com/2017/05/
 127. javascript:void(0)
 128. http://www.denizyuret.com/2017/04/
 129. javascript:void(0)
 130. http://www.denizyuret.com/2017/02/
 131. javascript:void(0)
 132. http://www.denizyuret.com/2016/
 133. javascript:void(0)
 134. http://www.denizyuret.com/2016/12/
 135. javascript:void(0)
 136. http://www.denizyuret.com/2016/11/
 137. javascript:void(0)
 138. http://www.denizyuret.com/2016/09/
 139. javascript:void(0)
 140. http://www.denizyuret.com/2016/08/
 141. javascript:void(0)
 142. http://www.denizyuret.com/2016/06/
 143. javascript:void(0)
 144. http://www.denizyuret.com/2016/03/
 145. javascript:void(0)
 146. http://www.denizyuret.com/2016/02/
 147. javascript:void(0)
 148. http://www.denizyuret.com/2015/
 149. javascript:void(0)
 150. http://www.denizyuret.com/2015/12/
 151. javascript:void(0)
 152. http://www.denizyuret.com/2015/11/
 153. javascript:void(0)
 154. http://www.denizyuret.com/2015/07/
 155. javascript:void(0)
 156. http://www.denizyuret.com/2015/06/
 157. javascript:void(0)
 158. http://www.denizyuret.com/2015/05/
 159. javascript:void(0)
 160. http://www.denizyuret.com/2015/04/
 161. javascript:void(0)
 162. http://www.denizyuret.com/2015/03/
 163. javascript:void(0)
 164. http://www.denizyuret.com/2015/02/
 165. javascript:void(0)
 166. http://www.denizyuret.com/2015/01/
 167. javascript:void(0)
 168. http://www.denizyuret.com/2014/
 169. javascript:void(0)
 170. http://www.denizyuret.com/2014/11/
 171. javascript:void(0)
 172. http://www.denizyuret.com/2014/09/
 173. javascript:void(0)
 174. http://www.denizyuret.com/2014/08/
 175. javascript:void(0)
 176. http://www.denizyuret.com/2014/06/
 177. javascript:void(0)
 178. http://www.denizyuret.com/2014/05/
 179. javascript:void(0)
 180. http://www.denizyuret.com/2014/04/
 181. javascript:void(0)
 182. http://www.denizyuret.com/2014/03/
 183. javascript:void(0)
 184. http://www.denizyuret.com/2014/02/
 185. javascript:void(0)
 186. http://www.denizyuret.com/2014/01/
 187. javascript:void(0)
 188. http://www.denizyuret.com/2013/
 189. javascript:void(0)
 190. http://www.denizyuret.com/2013/12/
 191. javascript:void(0)
 192. http://www.denizyuret.com/2013/11/
 193. javascript:void(0)
 194. http://www.denizyuret.com/2013/10/
 195. javascript:void(0)
 196. http://www.denizyuret.com/2013/09/
 197. javascript:void(0)
 198. http://www.denizyuret.com/2013/06/
 199. javascript:void(0)
 200. http://www.denizyuret.com/2013/05/
 201. javascript:void(0)
 202. http://www.denizyuret.com/2013/02/
 203. javascript:void(0)
 204. http://www.denizyuret.com/2013/01/
 205. javascript:void(0)
 206. http://www.denizyuret.com/2012/
 207. javascript:void(0)
 208. http://www.denizyuret.com/2012/11/
 209. javascript:void(0)
 210. http://www.denizyuret.com/2012/10/
 211. javascript:void(0)
 212. http://www.denizyuret.com/2012/08/
 213. javascript:void(0)
 214. http://www.denizyuret.com/2012/07/
 215. javascript:void(0)
 216. http://www.denizyuret.com/2012/06/
 217. javascript:void(0)
 218. http://www.denizyuret.com/2012/04/
 219. javascript:void(0)
 220. http://www.denizyuret.com/2012/03/
 221. javascript:void(0)
 222. http://www.denizyuret.com/2011/
 223. javascript:void(0)
 224. http://www.denizyuret.com/2011/10/
 225. javascript:void(0)
 226. http://www.denizyuret.com/2011/08/
 227. javascript:void(0)
 228. http://www.denizyuret.com/2011/07/
 229. javascript:void(0)
 230. http://www.denizyuret.com/2011/06/
 231. javascript:void(0)
 232. http://www.denizyuret.com/2011/05/
 233. javascript:void(0)
 234. http://www.denizyuret.com/2011/04/
 235. javascript:void(0)
 236. http://www.denizyuret.com/2011/03/
 237. javascript:void(0)
 238. http://www.denizyuret.com/2011/01/
 239. javascript:void(0)
 240. http://www.denizyuret.com/2010/
 241. javascript:void(0)
 242. http://www.denizyuret.com/2010/12/
 243. javascript:void(0)
 244. http://www.denizyuret.com/2010/11/
 245. javascript:void(0)
 246. http://www.denizyuret.com/2010/10/
 247. javascript:void(0)
 248. http://www.denizyuret.com/2010/09/
 249. javascript:void(0)
 250. http://www.denizyuret.com/2010/08/
 251. javascript:void(0)
 252. http://www.denizyuret.com/2010/07/
 253. javascript:void(0)
 254. http://www.denizyuret.com/2010/04/
 255. javascript:void(0)
 256. http://www.denizyuret.com/2010/02/
 257. javascript:void(0)
 258. http://www.denizyuret.com/2009/
 259. javascript:void(0)
 260. http://www.denizyuret.com/2009/12/
 261. javascript:void(0)
 262. http://www.denizyuret.com/2009/11/
 263. javascript:void(0)
 264. http://www.denizyuret.com/2009/08/
 265. javascript:void(0)
 266. http://www.denizyuret.com/2009/07/
 267. javascript:void(0)
 268. http://www.denizyuret.com/2009/04/
 269. javascript:void(0)
 270. http://www.denizyuret.com/2009/03/
 271. javascript:void(0)
 272. http://www.denizyuret.com/2009/02/
 273. javascript:void(0)
 274. http://www.denizyuret.com/2009/01/
 275. javascript:void(0)
 276. http://www.denizyuret.com/2008/
 277. javascript:void(0)
 278. http://www.denizyuret.com/2008/12/
 279. javascript:void(0)
 280. http://www.denizyuret.com/2008/10/
 281. javascript:void(0)
 282. http://www.denizyuret.com/2008/09/
 283. javascript:void(0)
 284. http://www.denizyuret.com/2008/08/
 285. javascript:void(0)
 286. http://www.denizyuret.com/2008/04/
 287. javascript:void(0)
 288. http://www.denizyuret.com/2007/
 289. javascript:void(0)
 290. http://www.denizyuret.com/2007/12/
 291. javascript:void(0)
 292. http://www.denizyuret.com/2007/11/
 293. javascript:void(0)
 294. http://www.denizyuret.com/2007/10/
 295. javascript:void(0)
 296. http://www.denizyuret.com/2007/09/
 297. javascript:void(0)
 298. http://www.denizyuret.com/2007/07/
 299. javascript:void(0)
 300. http://www.denizyuret.com/2007/06/
 301. javascript:void(0)
 302. http://www.denizyuret.com/2007/05/
 303. javascript:void(0)
 304. http://www.denizyuret.com/2007/04/
 305. javascript:void(0)
 306. http://www.denizyuret.com/2007/03/
 307. javascript:void(0)
 308. http://www.denizyuret.com/2007/02/
 309. javascript:void(0)
 310. http://www.denizyuret.com/2007/01/
 311. javascript:void(0)
 312. http://www.denizyuret.com/2006/
 313. javascript:void(0)
 314. http://www.denizyuret.com/2006/12/
 315. javascript:void(0)
 316. http://www.denizyuret.com/2006/11/
 317. javascript:void(0)
 318. http://www.denizyuret.com/2006/10/
 319. javascript:void(0)
 320. http://www.denizyuret.com/2006/09/
 321. javascript:void(0)
 322. http://www.denizyuret.com/2006/07/
 323. javascript:void(0)
 324. http://www.denizyuret.com/2006/06/
 325. javascript:void(0)
 326. http://www.denizyuret.com/2006/05/
 327. javascript:void(0)
 328. http://www.denizyuret.com/2006/04/
 329. javascript:void(0)
 330. http://www.denizyuret.com/2005/
 331. javascript:void(0)
 332. http://www.denizyuret.com/2005/10/
 333. javascript:void(0)
 334. http://www.denizyuret.com/2005/09/
 335. javascript:void(0)
 336. http://www.denizyuret.com/2005/08/
 337. http://www.denizyuret.com/2005/08/singular-value-decomposition-notes.html
 338. http://www.denizyuret.com/2005/08/semantic-relations.html
 339. javascript:void(0)
 340. http://www.denizyuret.com/2005/06/
 341. javascript:void(0)
 342. http://www.denizyuret.com/2005/05/
 343. javascript:void(0)
 344. http://www.denizyuret.com/2005/04/
 345. javascript:void(0)
 346. http://www.denizyuret.com/2004/
 347. javascript:void(0)
 348. http://www.denizyuret.com/2004/10/
 349. javascript:void(0)
 350. http://www.denizyuret.com/2004/07/
 351. javascript:void(0)
 352. http://www.denizyuret.com/2004/04/
 353. javascript:void(0)
 354. http://www.denizyuret.com/2002/
 355. javascript:void(0)
 356. http://www.denizyuret.com/2002/06/
 357. javascript:void(0)
 358. http://www.denizyuret.com/2002/03/
 359. javascript:void(0)
 360. http://www.denizyuret.com/1998/
 361. javascript:void(0)
 362. http://www.denizyuret.com/1998/05/
 363. javascript:void(0)
 364. http://www.denizyuret.com/1994/
 365. javascript:void(0)
 366. http://www.denizyuret.com/1994/05/
 367. https://slashdot.org/
 368. https://tech.slashdot.org/story/19/04/05/1716206/the-nations-of-the-amazon-want-the-name-back?utm_source=rss1.0mainlinkanon&utm_medium=feed
 369. https://www.scottaaronson.com/blog
 370. https://www.scottaaronson.com/blog/?p=4156
 371. https://www.lesswrong.com/
 372. https://www.lesswrong.com/events/qetyqhopbaucfz6ri/ssc-sacramento-meetup
 373. https://slatestarcodex.com/
 374. https://slatestarcodex.com/2019/04/03/classified-thread-7/
 375. https://www.askamathematician.com/
 376. https://www.askamathematician.com/2019/03/q-will-time-travel-ever-be-invented/
 377. http://www.preposterousuniverse.com/blog
 378. http://www.preposterousuniverse.com/blog/2019/01/12/true-facts-about-cosmology-or-misconceptions-skewered/
 379. http://cognitivemedium.com/
 380. http://cognitivemedium.com/srs-mathematics
 381. https://medium.com/@karpathy?source=rss-ac9d9a35533e------2
 382. https://medium.com/@karpathy/cool-135489a407cc?source=rss-ac9d9a35533e------2

   hidden links:
 384. https://www.blogger.com/email-post.g?blogid=8540876&postid=112506257981598112
 385. https://www.blogger.com/post-edit.g?blogid=8540876&postid=112506257981598112&from=pencil
 386. https://www.blogger.com/profile/00578023665603100985
 387. https://www.blogger.com/profile/00578023665603100985
 388. https://www.blogger.com/profile/00578023665603100985
 389. https://www.blogger.com/profile/00578023665603100985
 390. https://www.blogger.com/profile/00578023665603100985
 391. https://www.blogger.com/profile/00578023665603100985
 392. https://www.blogger.com/profile/00578023665603100985
 393. https://www.blogger.com/profile/00578023665603100985
 394. https://www.blogger.com/profile/00578023665603100985
 395. https://www.blogger.com/profile/00578023665603100985
 396. https://www.blogger.com/profile/00578023665603100985
 397. https://www.blogger.com/profile/00578023665603100985
 398. https://www.blogger.com/profile/06847717704454032165
 399. https://www.blogger.com/profile/10012161057511130758
 400. https://www.blogger.com/profile/00578023665603100985
 401. http://www.blogger.com/rearrange?blogid=8540876&widgettype=html&widgetid=html4&action=editwidget&sectionid=sidebartop
 402. http://www.blogger.com/rearrange?blogid=8540876&widgettype=label&widgetid=label2&action=editwidget&sectionid=sidebartop
 403. http://www.blogger.com/rearrange?blogid=8540876&widgettype=popularposts&widgetid=popularposts1&action=editwidget&sectionid=sidebar
 404. http://www.blogger.com/rearrange?blogid=8540876&widgettype=blogarchive&widgetid=blogarchive1&action=editwidget&sectionid=sidebar
 405. http://www.blogger.com/rearrange?blogid=8540876&widgettype=bloglist&widgetid=bloglist2&action=editwidget&sectionid=sidebar
 406. http://www.blogger.com/rearrange?blogid=8540876&widgettype=html&widgetid=html5&action=editwidget&sectionid=sidebar
 407. http://www.blogger.com/rearrange?blogid=8540876&widgettype=html&widgetid=html2&action=editwidget&sectionid=footer
