   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    id28 comments feed [4]linear
   regression: the code [5]creating dummy variables in pandas [6]alternate
   [7]alternate [8]triangleinequality [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]maths    [16]statistics    [17]id28   
   id28

id28

   welcome back. [18]last time we showed how to use id75 to
   predict a numeric response. today we are going to talk about what to do
   if you have a categorical response, like the titanic data. we will only
   talk about binary responses, but this method can be extended to cover k
   possible outcomes, and perhaps i will cover that in the future.

   math

   we can encode our outcomes as 1 or 0, and we could just run linear
   regression on this treated as numeric, the problem is that we could get
   any real  number as a prediction, but we could round this to the
   nearest 1 or 0. and this sometimes works. but what we might like is for
   the prediction to always lie in [0,1] and interpret this as the
   id203 of outcome 1.

   to do this we will need to do something nonlinear.  as before let our
   data be x \in \mathbb{r}^{n \times m} , and now let our binary outcomes
   be the vector y \in \{0,1\}^{n} .  let p: \mathbb{r} \to [0,1]. . then
   what we can try is to choose a \beta \in \mathbb{r}^{m} so that px\beta
   is a good fit for y (note i am abusing notation and having p act on the
   vector row-wise).

   but what to choose for p ? one candidate is called the logistic
   function, defined p(x) = \frac{1}{1+e^{-x}} , this looks like

   [19](image courtesy of wikipedia).

   notice how large positive values asymptote to 1, whilst large negative
   values asymptote to 0. so if these are interpreted as probabilities,
   the larger the input from the linear combination x \beta for some data
   point x , the more certain, one way or the other, our model predicts it
   is 1 or 0. the decision boundary or cutoff is at zero where the
   intercept is 0.5. hence our decision boundary is given by the
   hyperplane satisfying \beta x =0 .

   note that if p(x) = \frac{1}{1+e^{-x\beta}} then this can be rearranged
   to give \log \left ( \frac{p(x)}{1-p(x)} \right ) = x \beta , where the
   left-hand side is called the log-odds in statistics, where    odds    is in
   the sense of betting odds.

   now previously we just tried to minimise the euclidean norm of the
   error, hence the name    least-squares regression   . in our case we have
   predicted probabilities versus binary outcomes, so it is not completely
   obvious what notion of error to use. we could again use the euclidean
   norm, but since we have probabilities, why not choose \beta so that the
   data we have observed is says \beta maximally probable? this is called
   id113.  if we assume that the observations are
   independent of one another, then we get

   p(x|\beta) =\prod \limits _{i=1}^n p(x_i)^{y_i}(1-p(x_i)^{1-y_i} ,

   and since products are annoying we look at the log-likelihood

   \log p(x|\beta) =l (\beta)=\sum \limits _{i=1}^n y_i \log p(x_i) +
   (1-y_i)\log(1-p(x_i)) \ \ \ \ \ (1)

   which we can rewrite as

   \sum \limits _{i=1}^n y_i \log \frac{p(x_i)}{1-p(x_i)} +\sum \limits
   _{i=1}^n \log(1-p(x_i)) ,

   plugging in for the log-odds on the left, and p(x_i) on the right gives

   \sum \limits _{i=1}^n y_i x_i\beta - \sum \limits _{i=1}^n
   \log(1+e^{x_i\beta})

   which simplifies finally to

   x\beta \cdot y - \sum \limits _{i=1}^n \log(1+e^{x_i\beta}) ,

   that you might notice is a concave function, and so there are only
   global maxima, if any.

   now to maximise the log-likelihood we can try differentiate and set to
   zero, observing that

   \frac{\partial l }{\partial \beta_i}= x e_i \beta - \sum \limits
   _{j=1}^n \frac{e^{x_j\beta}}{1+e^{x_j\beta}} x_{ji} = \sum \limits
   _{j=1}^n(y_j - \frac{e^{x_j\beta}}{1+e^{x_j\beta}})x_{ji}=\sum \limits
   _{j=1}^n(y_j - p(x_j))x_{ji} ,

   or letting p act on a matrix we can write the gradient as

   \nabla l = \left( y-p(x) \right ) x, \ \ \ \ \ (2)

   but this will not have a closed form! so we must resort to numerical
   methods. since that is not the focus of this post, we will not
   implement this ourselves, but if you want to do it yourself, you could
   look at [20]newton   s method or [21]id119 since these are
   very straightforward to code. i will use something called the truncated
   id77, that is found in the scipy library.

   python implementation

   first let   s make up some data to test with. we proceed in a similar
   fashion to id75, but this time instead of trying to
   predict the y-coordinate from the x, we take both coordinates as our
   input, and try to predict if the data point is in outcome 1 or 0.
   outcome 1 corresponds to points generated by adding a gaussian noise
   term to a linear function, and outcome 0 corresponds to points
   generated by subtracting a gaussian noise term from the same linear
   function.
import numpy as np
from scipy import linalg
from scipy import optimize
import random as rd
import matplotlib.pyplot as plt
def test_set(a,b, no_samples, intercept, slope, variance, offset):
    #first we grab some random x-values in the range [a,b].
    x1 = [rd.uniform(a,b) for i in range(no_samples)]
    #sorting them seems to make the plots work better later on.
    x1.sort()
    #now we define our linear function that will underly our synthetic dataset:
the true decision   boundary.
    def g(x):
        return intercept + slope*x
    #we create two classes of points r,b. r points are given by g(x) plus a gaus
sian noise term with positive mean,
    #b points are given by g(x) minus a gaussian norm term with positive mean.
    r_turn=true
    x2=[] #this will hold the y-coordinates of our data.
    y=[]  #this will hold the classification of our data.
    for x in x1:
        x2=g(x)
        if r_turn:
            x2 += rd.gauss(offset, variance)
            y.append(1)
        else:
            x2 -= rd.gauss(offset, variance)
            y.append(0)
        r_turn=not r_turn
        x2.append(x2)
    #now we combine the input data into a single matrix.
    x1 = np.array([x1]).t
    x2 = np.array([x2]).t
    x = np.hstack((x1, x2))
    #now we return the input, output, and true decision boundary.
    return [ x,np.array(y).t, map(g, x1)]

   fairly straightforward stuff i hope! if not then it should make sense
   when we plot it.
x,y,g = test_set(a=0, b=5, no_samples=200, intercept=10, slope=1, variance=2, of
fset=1)
fig, ax = plt.subplots()
r =  x[::2] #even terms
b =  x[1::2] #odd terms

ax.scatter(r[:,0],r[:,1],marker='o', facecolor='red', label="outcome 1")
ax.scatter(b[:,0],b[:,1],marker='o', facecolor='blue', label="outcome 0")
ax.plot(x[:,0], g, color='green', label="true decision boundary")
ax.legend(loc="upper left", prop={'size':8})
plt.show()

   this looks like

   [22]logistic_regression1

   so hopefully our decision boundary generated by id28
   will be close to the true one in green. note that there could be a
   better boundary for the random data set shown, but the green line is
   the one that will generalise best to new data points from the same
   distribution.

   now that we have some idea what we are doing, we need to code up the
   functions we want to numerically optimise.

   first we make the sigmoid function, which i have previously called p(x)
   , notice that i have coded it in a slightly strange way so as to avoid
   computing huge numbers ( this idea i got from [23]here).
def sigmoid(x):
    if x>0:
        r = 1/(1. + np.exp(-x))
        return r
    else:
        r = np.exp(x)/(1+np.exp(x))
        return r
vsigmoid= np.vectorize(sigmoid)

   the last line there is something i just discovered, vectorisation of
   functions in numpy! remember when i abused notation before by having by
   function act on matrices row wise, or vectors elementwise? well numpy
   gives you an extremely convienent way to implement that abuse. super
   cool.

   next we write in the log-likelihood or    cost   , since the algorithm i
   want to use is for minimising, we will take the negative of (1):
def cost(beta,x,y):
    #function to be minimised.
    p = vsigmoid(x.dot(beta))
    ret = -y*np.log(p) -(1-y)*np.log(1-p)
    return ret.sum()

   similarly we will take the negative of the gradient as in (2):
def grad(beta,x,y):
    p = vsigmoid(x.dot(beta))
    r =  p-y
    return r.t.dot(x)

   and that is really all of the hardwork we have to do! all that remains
   to write a wrapper to some numerical optimisation function from the
   scipy library. note that it took some experimentation to realise that
   this function wants its numpy vectors    flattened   , that is with shape =
   (n,), not (n,1)! my thanks to [24]this postfor helping me figure this
   out!
def logistic_regression(x, y):
    #initialise beta to something random, that way if it fails to converge
    #you can rerun and get a better result.
    beta = 0.1* numpy.random.randn(x.shape[1])
    #the args argument tells fmin_tnc what to pass to cost and fprime in additio
n to beta.
    return optimize.fmin_tnc(cost, beta, fprime=grad, args=(x,y))

   and we are done! now let   s see how it fares on our data set. remember
   that once we have our \beta , the decision boundary is given by the
   points (x,y): (x,y,1) \beta=0 .
x1 = np.hstack((x, np.ones((x.shape[0], 1), dtype=x.dtype)))
beta=logistic_regression(x1, y)
beta=beta[0] #the rest of the tupe has convergence information.
def decision_boundary(x):
    return -x*beta[0]/beta[1] - beta[2]/beta[1]

   where the decision boundary function comes from solving xb_0 + yb_1 +
   b_2 = 0 for y .

   now to display the information i will create two plots side by side.
   the first will show our decision boundary, the true decision boundary
   and all the data with its true classification shown by colour.
   the other one will show the fitted decision boundary along with the
   data points, but this time the colors will indicate the predicted
   probabilities of their classifications: brighter reds and blues
   correspond to confident predictions of outcomes 1 and 0 respectively,
   and the colours get lighter as we approach the decision boundary.
fig, (ax1,ax2) = plt.subplots(1,2, sharey=true, sharex=true)
d=[decision_boundary(x) for x in x1[:,0]] #predicted decision boundary

#recall we defined r, b earlier as the odd and even data points which correspond
 to the 2 outcomes.
ax1.scatter(r[:,0],r[:,1],marker='o',facecolor='red',s=50, label="outcome 1")
ax1.scatter(b[:,0],b[:,1],marker='o', facecolor='blue',s=50, label="outcome 0")
ax1.plot(x1[:,0], g, '-', color='yellow', linewidth=3, label="true decision boun
dary")
ax1.plot(x1[:,0], d,linewidth=3, color='black',label="fitted decision boundary")

#predicted id203 of being red given by model
p = x1.dot(beta)
p = vsigmoid(p)
colors = plt.cm.coolwarm(p)
ax2.scatter(x1[:,0],x1[:,1],marker='o',s=50,facecolors=colors, label="predicted
id203" )

#ax2.plot(x1[:,0], g, '-', color='yellow', linewidth=3, label="true decision bou
ndary")
ax2.plot(x1[:,0], d,linewidth=3, color='black',label="fitted decision boundary")
legend = ax1.legend(loc='lower right', shadow=true, prop={'size':10})
legend = ax2.legend(loc='lower right', shadow=true, prop={'size':12})
fig.set_size_inches(15,7)

   and this will look like:

   [25]logistic_regression2 i hope you have learned something, the
   [26]ipython notebook for this post is here, and the python code is
   [27]here.
   advertisements

share this:

     * [28]twitter
     * [29]facebook
     *

like this:

   like loading...

related

   tags: [30]id28, [31]logit, [32]machine learning,
   [33]python, [34]statistics
   by [35]triangleinequality in [36]id28, [37]machine
   learning, [38]maths, [39]python, [40]statistics on [41]december 2,
   2013.
   [42]    id75: the code [43]creating dummy variables
   in pandas    
     __________________________________________________________________

3 comments

    1. [44]creating dummy variables in pandas    triangleinequality says:
       [45]december 4, 2013 at 2:20 pm
       [   ] last time we implemented id28, where the data is
       in the form of a numpy array. but what if your data is not of that
       form? what if it is a pandas dataframe like the kaggle titanic
       data? [   ]
       [46]reply
    2. [47]enter the id88    triangleinequality says:
       [48]february 24, 2014 at 5:07 pm
       [   ] ago in my post on id28, i talked about the
       problem where you have a matrix representing data points with
       features, and [   ]
       [49]reply
    3. [50]neural networks: part 1    triangleinequality says:
       [51]march 27, 2014 at 3:53 pm
       [   ] time we looked at the id88 model for classification,
       which is similar to id28, in that it uses a
       nonlinear transformation of a linear combination of the input
       features to give [   ]
       [52]reply

leave a reply [53]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [54]googleplus-sign-in

     *
     *

   [55]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [56]log out /
   [57]change )
   google photo

   you are commenting using your google account. ( [58]log out /
   [59]change )
   twitter picture

   you are commenting using your twitter account. ( [60]log out /
   [61]change )
   facebook photo

   you are commenting using your facebook account. ( [62]log out /
   [63]change )
   [64]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [65]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [66]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [67]cookie policy

   iframe: [68]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/feed/
   4. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
   5. https://triangleinequality.wordpress.com/2013/12/04/creating-dummy-variables-in-pandas/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/maths/
  16. https://triangleinequality.wordpress.com/category/maths/statistics/
  17. https://triangleinequality.wordpress.com/category/maths/statistics/logistic-regression/
  18. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  19. http://en.wikipedia.org/wiki/logistic_function
  20. http://en.wikipedia.org/wiki/newton's_method
  21. http://en.wikipedia.org/wiki/gradient_descent
  22. https://triangleinequality.files.wordpress.com/2013/12/logistic_regression11.png
  23. http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/
  24. http://aimotion.blogspot.co.uk/2011/11/machine-learning-with-python-logistic.html
  25. https://triangleinequality.files.wordpress.com/2013/12/logistic_regression2.png
  26. http://sourceforge.net/projects/triangleinequal/files/machine learning/id28.ipynb/download
  27. http://sourceforge.net/projects/triangleinequal/files/machine learning/id28.py/download
  28. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/?share=twitter
  29. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/?share=facebook
  30. https://triangleinequality.wordpress.com/tag/logistic-regression/
  31. https://triangleinequality.wordpress.com/tag/logit/
  32. https://triangleinequality.wordpress.com/tag/machine-learning/
  33. https://triangleinequality.wordpress.com/tag/python/
  34. https://triangleinequality.wordpress.com/tag/statistics/
  35. https://triangleinequality.wordpress.com/author/triangleinequality/
  36. https://triangleinequality.wordpress.com/tag/logistic-regression/
  37. https://triangleinequality.wordpress.com/tag/machine-learning/
  38. https://triangleinequality.wordpress.com/category/maths/
  39. https://triangleinequality.wordpress.com/tag/python/
  40. https://triangleinequality.wordpress.com/tag/statistics/
  41. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  42. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  43. https://triangleinequality.wordpress.com/2013/12/04/creating-dummy-variables-in-pandas/
  44. https://triangleinequality.wordpress.com/2013/12/04/creating-dummy-variables-in-pandas/
  45. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-59
  46. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/?replytocom=59#respond
  47. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  48. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-73
  49. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/?replytocom=73#respond
  50. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  51. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-83
  52. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/?replytocom=83#respond
  53. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#respond
  54. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  55. https://gravatar.com/site/signup/
  56. javascript:highlandercomments.doexternallogout( 'wordpress' );
  57. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  58. javascript:highlandercomments.doexternallogout( 'googleplus' );
  59. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  60. javascript:highlandercomments.doexternallogout( 'twitter' );
  61. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  62. javascript:highlandercomments.doexternallogout( 'facebook' );
  63. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  64. javascript:highlandercomments.cancelexternalwindow();
  65. https://wordpress.com/?ref=footer_blog
  66. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  67. https://automattic.com/cookies
  68. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  70. http://en.wikipedia.org/wiki/file:logistic-curve.svg
  71. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-form-guest
  72. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-form-load-service:wordpress.com
  73. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-form-load-service:twitter
  74. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/#comment-form-load-service:facebook
