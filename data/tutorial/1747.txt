    #[1]index [2]search [3]theano 1.0.0 documentation [4]tutorial
   [5]derivatives in theano [6]baby steps - algebra

   [7]theano [theano_logo_allwhite_210x70.png]
   1.0
   ____________________
     * [8]release notes
     * [9]theano at a glance
     * [10]requirements
     * [11]installing theano
     * [12]updating theano
     * [13]tutorial
          + [14]prerequisites
          + [15]basics
               o [16]baby steps - algebra
               o [17]more examples
                    # [18]logistic function
                    # [19]computing more than one thing at the same time
                    # [20]setting a default value for an argument
                    # [21]using shared variables
                    # [22]copying functions
                    # [23]using random numbers
                    # [24]a real example: id28
               o [25]derivatives in theano
               o [26]conditions
               o [27]loop
               o [28]how shape information is handled by theano
               o [29]broadcasting
          + [30]advanced
          + [31]advanced configuration and debugging
          + [32]further readings
     * [33]extending theano
     * [34]developer start guide
     * [35]optimizations
     * [36]api documentation
     * [37]troubleshooting
     * [38]glossary
     * [39]links
     * [40]internal documentation
     * [41]acknowledgements
     * [42]license

   [43]theano
     * [44]docs   
     * [45]tutorial   
     * more examples
     * [46]view page source
     __________________________________________________________________

more examples[47]  

   at this point it would be wise to begin familiarizing yourself more
   systematically with theano   s fundamental objects and operations by
   browsing this section of the library: [48]basic tensor functionality.

   as the tutorial unfolds, you should also gradually acquaint yourself
   with the other relevant areas of the library and with the relevant
   subjects of the documentation entrance page.

logistic function[49]  

   here   s another straightforward example, though a bit more elaborate
   than adding two numbers together. let   s say that you want to compute
   the logistic curve, which is given by:

   s(x) = \frac{1}{1 + e^{-x}}
   ../_images/logistic.png

   a plot of the logistic function, with x on the x-axis and s(x) on the
   y-axis.

   you want to compute the function [50]elementwise on matrices of
   doubles, which means that you want to apply this function to each
   individual element of the matrix.

   well, what you do is this:
>>> import theano
>>> import theano.tensor as t
>>> x = t.dmatrix('x')
>>> s = 1 / (1 + t.exp(-x))
>>> logistic = theano.function([x], s)
>>> logistic([[0, 1], [-1, -2]])
array([[ 0.5       ,  0.73105858],
       [ 0.26894142,  0.11920292]])

   the reason logistic is performed elementwise is because all of its
   operations   division, addition, exponentiation, and division   are
   themselves elementwise operations.

   it is also the case that:

   s(x) = \frac{1}{1 + e^{-x}} = \frac{1 + \tanh(x/2)}{2}

   we can verify that this alternate form produces the same values:
>>> s2 = (1 + t.tanh(x / 2)) / 2
>>> logistic2 = theano.function([x], s2)
>>> logistic2([[0, 1], [-1, -2]])
array([[ 0.5       ,  0.73105858],
       [ 0.26894142,  0.11920292]])

computing more than one thing at the same time[51]  

   theano supports functions with multiple outputs. for example, we can
   compute the [52]elementwise difference, absolute difference, and
   squared difference between two matrices a and b at the same time:
>>> a, b = t.dmatrices('a', 'b')
>>> diff = a - b
>>> abs_diff = abs(diff)
>>> diff_squared = diff**2
>>> f = theano.function([a, b], [diff, abs_diff, diff_squared])

   note

   dmatrices produces as many outputs as names that you provide. it is a
   shortcut for allocating symbolic variables that we will often use in
   the tutorials.

   when we use the function f, it returns the three variables (the
   printing was reformatted for readability):
>>> f([[1, 1], [1, 1]], [[0, 1], [2, 3]])
[array([[ 1.,  0.],
       [-1., -2.]]), array([[ 1.,  0.],
       [ 1.,  2.]]), array([[ 1.,  0.],
       [ 1.,  4.]])]

setting a default value for an argument[53]  

   let   s say you want to define a function that adds two numbers, except
   that if you only provide one number, the other input is assumed to be
   one. you can do it like this:
>>> from theano import in
>>> from theano import function
>>> x, y = t.dscalars('x', 'y')
>>> z = x + y
>>> f = function([x, in(y, value=1)], z)
>>> f(33)
array(34.0)
>>> f(33, 2)
array(35.0)

   this makes use of the [54]in class which allows you to specify
   properties of your function   s parameters with greater detail. here we
   give a default value of 1 for y by creating a in instance with its
   value field set to 1.

   inputs with default values must follow inputs without default values
   (like python   s functions). there can be multiple inputs with default
   values. these parameters can be set positionally or by name, as in
   standard python:
>>> x, y, w = t.dscalars('x', 'y', 'w')
>>> z = (x + y) * w
>>> f = function([x, in(y, value=1), in(w, value=2, name='w_by_name')], z)
>>> f(33)
array(68.0)
>>> f(33, 2)
array(70.0)
>>> f(33, 0, 1)
array(33.0)
>>> f(33, w_by_name=1)
array(34.0)
>>> f(33, w_by_name=1, y=0)
array(33.0)

   note

   in does not know the name of the local variables y and w that are
   passed as arguments. the symbolic variable objects have name attributes
   (set by dscalars in the example above) and these are the names of the
   keyword parameters in the functions that we build. this is the
   mechanism at work in in(y, value=1). in the case of in(w, value=2,
   name='w_by_name'). we override the symbolic variable   s name attribute
   with a name to be used for this function.

   you may like to see [55]function in the library for more detail.

using shared variables[56]  

   it is also possible to make a function with an internal state. for
   example, let   s say we want to make an accumulator: at the beginning,
   the state is initialized to zero. then, on each function call, the
   state is incremented by the function   s argument.

   first let   s define the accumulator function. it adds its argument to
   the internal state, and returns the old state value.
>>> from theano import shared
>>> state = shared(0)
>>> inc = t.iscalar('inc')
>>> accumulator = function([inc], state, updates=[(state, state+inc)])

   this code introduces a few new concepts. the shared function constructs
   so-called [57]shared variables. these are hybrid symbolic and
   non-symbolic variables whose value may be shared between multiple
   functions. shared variables can be used in symbolic expressions just
   like the objects returned by dmatrices(...) but they also have an
   internal value that defines the value taken by this symbolic variable
   in all the functions that use it. it is called a shared variable
   because its value is shared between many functions. the value can be
   accessed and modified by the .get_value() and .set_value() methods. we
   will come back to this soon.

   the other new thing in this code is the updates parameter of function.
   updates must be supplied with a list of pairs of the form
   (shared-variable, new expression). it can also be a dictionary whose
   keys are shared-variables and values are the new expressions. either
   way, it means    whenever this function runs, it will replace the .value
   of each shared variable with the result of the corresponding
   expression   . above, our accumulator replaces the state   s value with the
   sum of the state and the increment amount.

   let   s try it out!
>>> print(state.get_value())
0
>>> accumulator(1)
array(0)
>>> print(state.get_value())
1
>>> accumulator(300)
array(1)
>>> print(state.get_value())
301

   it is possible to reset the state. just use the .set_value() method:
>>> state.set_value(-1)
>>> accumulator(3)
array(-1)
>>> print(state.get_value())
2

   as we mentioned above, you can define more than one function to use the
   same shared variable. these functions can all update the value.
>>> decrementor = function([inc], state, updates=[(state, state-inc)])
>>> decrementor(2)
array(2)
>>> print(state.get_value())
0

   you might be wondering why the updates mechanism exists. you can always
   achieve a similar result by returning the new expressions, and working
   with them in numpy as usual. the updates mechanism can be a syntactic
   convenience, but it is mainly there for efficiency. updates to shared
   variables can sometimes be done more quickly using in-place algorithms
   (e.g. low-rank matrix updates). also, theano has more control over
   where and how shared variables are allocated, which is one of the
   important elements of getting good performance on the [58]gpu.

   it may happen that you expressed some formula using a shared variable,
   but you do not want to use its value. in this case, you can use the
   givens parameter of function which replaces a particular node in a
   graph for the purpose of one particular function.
>>> fn_of_state = state * 2 + inc
>>> # the type of foo must match the shared variable we are replacing
>>> # with the ``givens``
>>> foo = t.scalar(dtype=state.dtype)
>>> skip_shared = function([inc, foo], fn_of_state, givens=[(state, foo)])
>>> skip_shared(1, 3)  # we're using 3 for the state, not state.value
array(7)
>>> print(state.get_value())  # old state still there, but we didn't use it
0

   the givens parameter can be used to replace any symbolic variable, not
   just a shared variable. you can replace constants, and expressions, in
   general. be careful though, not to allow the expressions introduced by
   a givens substitution to be co-dependent, the order of substitution is
   not defined, so the substitutions have to work in any order.

   in practice, a good way of thinking about the givens is as a mechanism
   that allows you to replace any part of your formula with a different
   expression that evaluates to a tensor of same shape and dtype.

   note

   theano shared variable broadcast pattern default to false for each
   dimensions. shared variable size can change over time, so we can   t use
   the shape to find the broadcastable pattern. if you want a different
   pattern, just pass it as a parameter theano.shared(...,
   broadcastable=(true, false))

copying functions[59]  

   theano functions can be copied, which can be useful for creating
   similar functions but with different shared variables or updates. this
   is done using the [60]copy() method of function objects. the optimized
   graph of the original function is copied, so compilation only needs to
   be performed once.

   let   s start from the accumulator defined above:
>>> import theano
>>> import theano.tensor as t
>>> state = theano.shared(0)
>>> inc = t.iscalar('inc')
>>> accumulator = theano.function([inc], state, updates=[(state, state+inc)])

   we can use it to increment the state as usual:
>>> accumulator(10)
array(0)
>>> print(state.get_value())
10

   we can use copy() to create a similar accumulator but with its own
   internal state using the swap parameter, which is a dictionary of
   shared variables to exchange:
>>> new_state = theano.shared(0)
>>> new_accumulator = accumulator.copy(swap={state:new_state})
>>> new_accumulator(100)
[array(0)]
>>> print(new_state.get_value())
100

   the state of the first function is left untouched:
>>> print(state.get_value())
10

   we now create a copy with updates removed using the delete_updates
   parameter, which is set to false by default:
>>> null_accumulator = accumulator.copy(delete_updates=true)

   as expected, the shared state is no longer updated:
>>> null_accumulator(9000)
[array(10)]
>>> print(state.get_value())
10

using random numbers[61]  

   because in theano you first express everything symbolically and
   afterwards compile this expression to get functions, using
   pseudo-random numbers is not as straightforward as it is in numpy,
   though also not too complicated.

   the way to think about putting randomness into theano   s computations is
   to put random variables in your graph. theano will allocate a numpy
   randomstream object (a random number generator) for each such variable,
   and draw from it as necessary. we will call this sort of sequence of
   random numbers a random stream. random streams are at their core shared
   variables, so the observations on shared variables hold here as well.
   theano   s random objects are defined and implemented in
   [62]randomstreams and, at a lower level, in [63]randomstreamsbase.

brief example[64]  

   here   s a brief example. the setup code is:
from theano.tensor.shared_randomstreams import randomstreams
from theano import function
srng = randomstreams(seed=234)
rv_u = srng.uniform((2,2))
rv_n = srng.normal((2,2))
f = function([], rv_u)
g = function([], rv_n, no_default_updates=true)    #not updating rv_n.rng
nearly_zeros = function([], rv_u + rv_u - 2 * rv_u)

   here,    rv_u    represents a random stream of 2x2 matrices of draws from a
   uniform distribution. likewise,    rv_n    represents a random stream of
   2x2 matrices of draws from a normal distribution. the distributions
   that are implemented are defined in randomstreams and, at a lower
   level, in [65]raw_random. they only work on cpu. see [66]other
   implementations for gpu version.

   now let   s use these objects. if we call f(), we get random uniform
   numbers. the internal state of the random number generator is
   automatically updated, so we get different random numbers every time.
>>> f_val0 = f()
>>> f_val1 = f()  #different numbers from f_val0

   when we add the extra argument no_default_updates=true to function (as
   in g), then the random number generator state is not affected by
   calling the returned function. so, for example, calling g multiple
   times will return the same numbers.
>>> g_val0 = g()  # different numbers from f_val0 and f_val1
>>> g_val1 = g()  # same numbers as g_val0!

   an important remark is that a random variable is drawn at most once
   during any single function execution. so the nearly_zeros function is
   guaranteed to return approximately 0 (except for rounding error) even
   though the rv_u random variable appears three times in the output
   expression.
>>> nearly_zeros = function([], rv_u + rv_u - 2 * rv_u)

seeding streams[67]  

   random variables can be seeded individually or collectively.

   you can seed just one random variable by seeding or assigning to the
   .rng attribute, using .rng.set_value().
>>> rng_val = rv_u.rng.get_value(borrow=true)   # get the rng for rv_u
>>> rng_val.seed(89234)                         # seeds the generator
>>> rv_u.rng.set_value(rng_val, borrow=true)    # assign back seeded rng

   you can also seed all of the random variables allocated by a
   randomstreams object by that object   s seed method. this seed will be
   used to seed a temporary random number generator, that will in turn
   generate seeds for each of the random variables.
>>> srng.seed(902340)  # seeds rv_u and rv_n with different seeds each

sharing streams between functions[68]  

   as usual for shared variables, the random number generators used for
   random variables are common between functions. so our nearly_zeros
   function will update the state of the generators used in function f
   above.

   for example:
>>> state_after_v0 = rv_u.rng.get_value().get_state()
>>> nearly_zeros()       # this affects rv_u's generator
array([[ 0.,  0.],
       [ 0.,  0.]])
>>> v1 = f()
>>> rng = rv_u.rng.get_value(borrow=true)
>>> rng.set_state(state_after_v0)
>>> rv_u.rng.set_value(rng, borrow=true)
>>> v2 = f()             # v2 != v1
>>> v3 = f()             # v3 == v1

copying random state between theano graphs[69]  

   in some use cases, a user might want to transfer the    state    of all
   random number generators associated with a given theano graph (e.g. g1,
   with compiled function f1 below) to a second graph (e.g. g2, with
   function f2). this might arise for example if you are trying to
   initialize the state of a model, from the parameters of a pickled
   version of a previous model. for
   [70]theano.tensor.shared_randomstreams.randomstreams and
   [71]theano.sandbox.rng_mrg.mrg_randomstreams this can be achieved by
   copying elements of the state_updates parameter.

   each time a random variable is drawn from a randomstreams object, a
   tuple is added to the state_updates list. the first element is a shared
   variable, which represents the state of the random number generator
   associated with this particular variable, while the second represents
   the theano graph corresponding to the random number generation process
   (i.e. randomfunction{uniform}.0).

   an example of how    random states    can be transferred from one theano
   function to another is shown below.
>>> from __future__ import print_function
>>> import theano
>>> import numpy
>>> import theano.tensor as t
>>> from theano.sandbox.rng_mrg import mrg_randomstreams
>>> from theano.tensor.shared_randomstreams import randomstreams

>>> class graph():
...     def __init__(self, seed=123):
...         self.rng = randomstreams(seed)
...         self.y = self.rng.uniform(size=(1,))

>>> g1 = graph(seed=123)
>>> f1 = theano.function([], g1.y)

>>> g2 = graph(seed=987)
>>> f2 = theano.function([], g2.y)

>>> # by default, the two functions are out of sync.
>>> f1()
array([ 0.72803009])
>>> f2()
array([ 0.55056769])

>>> def copy_random_state(g1, g2):
...     if isinstance(g1.rng, mrg_randomstreams):
...         g2.rng.rstate = g1.rng.rstate
...     for (su1, su2) in zip(g1.rng.state_updates, g2.rng.state_updates):
...         su2[0].set_value(su1[0].get_value())

>>> # we now copy the state of the theano random number generators.
>>> copy_random_state(g1, g2)
>>> f1()
array([ 0.59044123])
>>> f2()
array([ 0.59044123])

other random distributions[72]  

   there are [73]other distributions implemented.

other implementations[74]  

   there is another implementations based on [75]mrg31k3p. the
   randomstream only work on the cpu, mrg31k3p work on the cpu and gpu.

   note

   to use you the mrg version easily, you can just change the import to:

from theano.sandbox.rng_mrg import mrg_randomstreams as randomstreams

a real example: id28[76]  

   the preceding elements are featured in this more realistic example. it
   will be used repeatedly.
import numpy
import theano
import theano.tensor as t
rng = numpy.random

n = 400                                   # training sample size
feats = 784                               # number of input variables

# generate a dataset: d = (input_values, target_class)
d = (rng.randn(n, feats), rng.randint(size=n, low=0, high=2))
training_steps = 10000

# declare theano symbolic variables
x = t.dmatrix("x")
y = t.dvector("y")

# initialize the weight vector w randomly
#
# this and the following bias variable b
# are shared so they keep their values
# between training iterations (updates)
w = theano.shared(rng.randn(feats), name="w")

# initialize the bias term
b = theano.shared(0., name="b")

print("initial model:")
print(w.get_value())
print(b.get_value())

# construct theano expression graph
p_1 = 1 / (1 + t.exp(-t.dot(x, w) - b))   # id203 that target = 1
prediction = p_1 > 0.5                    # the prediction thresholded
xent = -y * t.log(p_1) - (1-y) * t.log(1-p_1) # cross-id178 id168
cost = xent.mean() + 0.01 * (w ** 2).sum()# the cost to minimize
gw, gb = t.grad(cost, [w, b])             # compute the gradient of the cost
                                          # w.r.t weight vector w and
                                          # bias term b
                                          # (we shall return to this in a
                                          # following section of this tutorial)

# compile
train = theano.function(
          inputs=[x,y],
          outputs=[prediction, xent],
          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))
predict = theano.function(inputs=[x], outputs=prediction)

# train
for i in range(training_steps):
    pred, err = train(d[0], d[1])

print("final model:")
print(w.get_value())
print(b.get_value())
print("target values for d:")
print(d[1])
print("prediction on d:")
print(predict(d[0]))

   [77]next [78]previous
     __________________________________________________________________

      copyright 2008--2017, lisa lab. last updated on nov 21, 2017.
   built with [79]sphinx using a [80]theme provided by [81]read the docs.

references

   1. http://deeplearning.net/software/theano/genindex.html
   2. http://deeplearning.net/software/theano/search.html
   3. http://deeplearning.net/software/theano/index.html
   4. http://deeplearning.net/software/theano/tutorial/index.html
   5. http://deeplearning.net/software/theano/tutorial/gradients.html
   6. http://deeplearning.net/software/theano/tutorial/adding.html
   7. http://deeplearning.net/software/theano/index.html
   8. http://deeplearning.net/software/theano/news.html
   9. http://deeplearning.net/software/theano/introduction.html
  10. http://deeplearning.net/software/theano/requirements.html
  11. http://deeplearning.net/software/theano/install.html
  12. http://deeplearning.net/software/theano/updating.html
  13. http://deeplearning.net/software/theano/tutorial/index.html
  14. http://deeplearning.net/software/theano/tutorial/index.html#prerequisites
  15. http://deeplearning.net/software/theano/tutorial/index.html#basics
  16. http://deeplearning.net/software/theano/tutorial/adding.html
  17. http://deeplearning.net/software/theano/tutorial/examples.html
  18. http://deeplearning.net/software/theano/tutorial/examples.html#logistic-function
  19. http://deeplearning.net/software/theano/tutorial/examples.html#computing-more-than-one-thing-at-the-same-time
  20. http://deeplearning.net/software/theano/tutorial/examples.html#setting-a-default-value-for-an-argument
  21. http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables
  22. http://deeplearning.net/software/theano/tutorial/examples.html#copying-functions
  23. http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers
  24. http://deeplearning.net/software/theano/tutorial/examples.html#a-real-example-logistic-regression
  25. http://deeplearning.net/software/theano/tutorial/gradients.html
  26. http://deeplearning.net/software/theano/tutorial/conditions.html
  27. http://deeplearning.net/software/theano/tutorial/loop.html
  28. http://deeplearning.net/software/theano/tutorial/shape_info.html
  29. http://deeplearning.net/software/theano/tutorial/broadcasting.html
  30. http://deeplearning.net/software/theano/tutorial/index.html#advanced
  31. http://deeplearning.net/software/theano/tutorial/index.html#advanced-configuration-and-debugging
  32. http://deeplearning.net/software/theano/tutorial/index.html#further-readings
  33. http://deeplearning.net/software/theano/extending/index.html
  34. http://deeplearning.net/software/theano/dev_start_guide.html
  35. http://deeplearning.net/software/theano/optimizations.html
  36. http://deeplearning.net/software/theano/library/index.html
  37. http://deeplearning.net/software/theano/troubleshooting.html
  38. http://deeplearning.net/software/theano/glossary.html
  39. http://deeplearning.net/software/theano/links.html
  40. http://deeplearning.net/software/theano/internal/index.html
  41. http://deeplearning.net/software/theano/acknowledgement.html
  42. http://deeplearning.net/software/theano/license.html
  43. http://deeplearning.net/software/theano/index.html
  44. http://deeplearning.net/software/theano/index.html
  45. http://deeplearning.net/software/theano/tutorial/index.html
  46. http://deeplearning.net/software/theano/_sources/tutorial/examples.txt
  47. http://deeplearning.net/software/theano/tutorial/examples.html#more-examples
  48. http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-basic-tensor
  49. http://deeplearning.net/software/theano/tutorial/examples.html#logistic-function
  50. http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-tensor-elementwise
  51. http://deeplearning.net/software/theano/tutorial/examples.html#computing-more-than-one-thing-at-the-same-time
  52. http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-tensor-elementwise
  53. http://deeplearning.net/software/theano/tutorial/examples.html#setting-a-default-value-for-an-argument
  54. http://deeplearning.net/software/theano/library/compile/io.html#function-inputs
  55. http://deeplearning.net/software/theano/library/compile/function.html#usingfunction
  56. http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables
  57. http://deeplearning.net/software/theano/library/compile/shared.html#libdoc-compile-shared
  58. http://deeplearning.net/software/theano/tutorial/using_gpu.html#using-gpu
  59. http://deeplearning.net/software/theano/tutorial/examples.html#copying-functions
  60. http://deeplearning.net/software/theano/library/compile/function.html#theano.compile.function_module.function.copy
  61. http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers
  62. http://deeplearning.net/software/theano/library/tensor/shared_randomstreams.html#libdoc-tensor-shared-randomstreams
  63. http://deeplearning.net/software/theano/library/tensor/raw_random.html#libdoc-tensor-raw-random
  64. http://deeplearning.net/software/theano/tutorial/examples.html#brief-example
  65. http://deeplearning.net/software/theano/library/tensor/raw_random.html#libdoc-tensor-raw-random
  66. http://deeplearning.net/software/theano/tutorial/examples.html#other-implementations
  67. http://deeplearning.net/software/theano/tutorial/examples.html#seeding-streams
  68. http://deeplearning.net/software/theano/tutorial/examples.html#sharing-streams-between-functions
  69. http://deeplearning.net/software/theano/tutorial/examples.html#copying-random-state-between-theano-graphs
  70. http://deeplearning.net/software/theano/library/tensor/shared_randomstreams.html#theano.tensor.shared_randomstreams.randomstreams
  71. http://deeplearning.net/software/theano/library/sandbox/rng_mrg.html#theano.sandbox.rng_mrg.mrg_randomstreams
  72. http://deeplearning.net/software/theano/tutorial/examples.html#other-random-distributions
  73. http://deeplearning.net/software/theano/library/tensor/raw_random.html#libdoc-tensor-raw-random
  74. http://deeplearning.net/software/theano/tutorial/examples.html#other-implementations
  75. http://deeplearning.net/software/theano/library/sandbox/rng_mrg.html#libdoc-rng-mrg
  76. http://deeplearning.net/software/theano/tutorial/examples.html#a-real-example-logistic-regression
  77. http://deeplearning.net/software/theano/tutorial/gradients.html
  78. http://deeplearning.net/software/theano/tutorial/adding.html
  79. http://sphinx-doc.org/
  80. https://github.com/snide/sphinx_rtd_theme
  81. https://readthedocs.org/
