   #[1]index [2]search [3]multilayer id88 [4]getting started

navigation

     * [5]index
     * [6]next |
     * [7]previous |
     * [8]deeplearning 0.1 documentation   

[9]table of contents

     * [10]classifying mnist digits using id28
          + [11]the model
          + [12]defining a id168
          + [13]creating a logisticregression class
          + [14]learning the model
          + [15]testing the model
          + [16]putting it all together
          + [17]prediction using a trained model

previous topic

   [18]getting started

next topic

   [19]multilayer id88

this page

     * [20]show source

quick search

   ____________________
   go

classifying mnist digits using id28[21]  

   note

   this sections assumes familiarity with the following theano concepts:
   [22]shared variables , [23]basic arithmetic ops , [24]t.grad ,
   [25]floatx. if you intend to run the code on gpu also read [26]gpu.

   note

   the code for this section is available for download [27]here.

   in this section, we show how theano can be used to implement the most
   basic classifier: the id28. we start off with a quick
   primer of the model, which serves both as a refresher but also to
   anchor the notation and show how mathematical expressions are mapped
   onto theano graphs.

   in the deepest of machine learning traditions, this tutorial will
   tackle the exciting problem of mnist digit classification.

the model[28]  

   id28 is a probabilistic, linear classifier. it is
   parametrized by a weight matrix w and a bias vector b . classification
   is done by projecting an input vector onto a set of hyperplanes, each
   of which corresponds to a class. the distance from the input to a
   hyperplane reflects the id203 that the input is a member of the
   corresponding class.

   mathematically, the id203 that an input vector x is a member of a
   class i , a value of a stochastic variable y , can be written as:

   p(y=i|x, w,b) &= softmax_i(w x + b) \\ &= \frac {e^{w_i x + b_i}}
   {\sum_j e^{w_j x + b_j}}

   the model   s prediction y_{pred} is the class whose id203 is
   maximal, specifically:

   y_{pred} = {\rm argmax}_i p(y=i|x,w,b)

   the code to do this in theano is the following:
        # initialize with 0 the weights w as a matrix of shape (n_in, n_out)
        self.w = theano.shared(
            value=numpy.zeros(
                (n_in, n_out),
                dtype=theano.config.floatx
            ),
            name='w',
            borrow=true
        )
        # initialize the biases b as a vector of n_out 0s
        self.b = theano.shared(
            value=numpy.zeros(
                (n_out,),
                dtype=theano.config.floatx
            ),
            name='b',
            borrow=true
        )

        # symbolic expression for computing the matrix of class-membership
        # probabilities
        # where:
        # w is a matrix where column-k represent the separation hyperplane for
        # class-k
        # x is a matrix where row-j  represents input training sample-j
        # b is a vector where element-k represent the free parameter of
        # hyperplane-k
        self.p_y_given_x = t.nnet.softmax(t.dot(input, self.w) + self.b)

        # symbolic description of how to compute prediction as class whose
        # id203 is maximal
        self.y_pred = t.argmax(self.p_y_given_x, axis=1)

   since the parameters of the model must maintain a persistent state
   throughout training, we allocate shared variables for w,b . this
   declares them both as being symbolic theano variables, but also
   initializes their contents. the dot and softmax operators are then used
   to compute the vector p(y|x, w,b) . the result p_y_given_x is a
   symbolic variable of vector-type.

   to get the actual model prediction, we can use the t.argmax operator,
   which will return the index at which p_y_given_x is maximal (i.e. the
   class with maximum id203).

   now of course, the model we have defined so far does not do anything
   useful yet, since its parameters are still in their initial state. the
   following section will thus cover how to learn the optimal parameters.

   note

   for a complete list of theano ops, see: [29]list of ops

defining a id168[30]  

   learning optimal model parameters involves minimizing a id168.
   in the case of multi-class id28, it is very common to
   use the negative log-likelihood as the loss. this is equivalent to
   maximizing the likelihood of the data set \cal{d} under the model
   parameterized by \theta . let us first start by defining the likelihood
   \cal{l} and loss \ell :

   \mathcal{l} (\theta=\{w,b\}, \mathcal{d}) = \sum_{i=0}^{|\mathcal{d}|}
   \log(p(y=y^{(i)}|x^{(i)}, w,b)) \\ \ell (\theta=\{w,b\}, \mathcal{d}) =
   - \mathcal{l} (\theta=\{w,b\}, \mathcal{d})

   while entire books are dedicated to the topic of minimization, gradient
   descent is by far the simplest method for minimizing arbitrary
   non-linear functions. this tutorial will use the method of stochastic
   gradient method with mini-batches (msgd). see [31]stochastic gradient
   descent for more details.

   the following theano code defines the (symbolic) loss for a given
   minibatch:
        # y.shape[0] is (symbolically) the number of rows in y, i.e.,
        # number of examples (call it n) in the minibatch
        # t.arange(y.shape[0]) is a symbolic vector which will contain
        # [0,1,2,... n-1] t.log(self.p_y_given_x) is a matrix of
        # log-probabilities (call it lp) with one row per example and
        # one column per class lp[t.arange(y.shape[0]),y] is a vector
        # v containing [lp[0,y[0]], lp[1,y[1]], lp[2,y[2]], ...,
        # lp[n-1,y[n-1]]] and t.mean(lp[t.arange(y.shape[0]),y]) is
        # the mean (across minibatch examples) of the elements in v,
        # i.e., the mean log-likelihood across the minibatch.
        return -t.mean(t.log(self.p_y_given_x)[t.arange(y.shape[0]), y])

   note

   even though the loss is formally defined as the sum, over the data set,
   of individual error terms, in practice, we use the mean (t.mean) in the
   code. this allows for the learning rate choice to be less dependent of
   the minibatch size.

creating a logisticregression class[32]  

   we now have all the tools we need to define a logisticregression class,
   which encapsulates the basic behaviour of id28. the code
   is very similar to what we have covered so far, and should be self
   explanatory.
class logisticregression(object):
    """multi-class id28 class

    the id28 is fully described by a weight matrix :math:`w`
    and bias vector :math:`b`. classification is done by projecting data
    points onto a set of hyperplanes, the distance to which is used to
    determine a class membership id203.
    """

    def __init__(self, input, n_in, n_out):
        """ initialize the parameters of the id28

        :type input: theano.tensor.tensortype
        :param input: symbolic variable that describes the input of the
                      architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
                     which the datapoints lie

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
                      which the labels lie

        """
        # start-snippet-1
        # initialize with 0 the weights w as a matrix of shape (n_in, n_out)
        self.w = theano.shared(
            value=numpy.zeros(
                (n_in, n_out),
                dtype=theano.config.floatx
            ),
            name='w',
            borrow=true
        )
        # initialize the biases b as a vector of n_out 0s
        self.b = theano.shared(
            value=numpy.zeros(
                (n_out,),
                dtype=theano.config.floatx
            ),
            name='b',
            borrow=true
        )

        # symbolic expression for computing the matrix of class-membership
        # probabilities
        # where:
        # w is a matrix where column-k represent the separation hyperplane for
        # class-k
        # x is a matrix where row-j  represents input training sample-j
        # b is a vector where element-k represent the free parameter of
        # hyperplane-k
        self.p_y_given_x = t.nnet.softmax(t.dot(input, self.w) + self.b)

        # symbolic description of how to compute prediction as class whose
        # id203 is maximal
        self.y_pred = t.argmax(self.p_y_given_x, axis=1)
        # end-snippet-1

        # parameters of the model
        self.params = [self.w, self.b]

        # keep track of model input
        self.input = input

    def negative_log_likelihood(self, y):
        """return the mean of the negative log-likelihood of the prediction
        of this model under a given target distribution.

        .. math::

            \frac{1}{|\mathcal{d}|} \mathcal{l} (\theta=\{w,b\}, \mathcal{d}) =
            \frac{1}{|\mathcal{d}|} \sum_{i=0}^{|\mathcal{d}|}
                \log(p(y=y^{(i)}|x^{(i)}, w,b)) \\
            \ell (\theta=\{w,b\}, \mathcal{d})

        :type y: theano.tensor.tensortype
        :param y: corresponds to a vector that gives for each example the
                  correct label

        note: we use the mean instead of the sum so that
              the learning rate is less dependent on the batch size
        """
        # start-snippet-2
        # y.shape[0] is (symbolically) the number of rows in y, i.e.,
        # number of examples (call it n) in the minibatch
        # t.arange(y.shape[0]) is a symbolic vector which will contain
        # [0,1,2,... n-1] t.log(self.p_y_given_x) is a matrix of
        # log-probabilities (call it lp) with one row per example and
        # one column per class lp[t.arange(y.shape[0]),y] is a vector
        # v containing [lp[0,y[0]], lp[1,y[1]], lp[2,y[2]], ...,
        # lp[n-1,y[n-1]]] and t.mean(lp[t.arange(y.shape[0]),y]) is
        # the mean (across minibatch examples) of the elements in v,
        # i.e., the mean log-likelihood across the minibatch.
        return -t.mean(t.log(self.p_y_given_x)[t.arange(y.shape[0]), y])
        # end-snippet-2

    def errors(self, y):
        """return a float representing the number of errors in the minibatch
        over the total number of examples of the minibatch ; zero one
        loss over the size of the minibatch

        :type y: theano.tensor.tensortype
        :param y: corresponds to a vector that gives for each example the
                  correct label
        """

        # check if y has same dimension of y_pred
        if y.ndim != self.y_pred.ndim:
            raise typeerror(
                'y should have the same shape as self.y_pred',
                ('y', y.type, 'y_pred', self.y_pred.type)
            )
        # check if y is of the correct datatype
        if y.dtype.startswith('int'):
            # the t.neq operator returns a vector of 0s and 1s, where 1
            # represents a mistake in prediction
            return t.mean(t.neq(self.y_pred, y))
        else:
            raise notimplementederror()

   we instantiate this class as follows:
    # generate symbolic variables for input (x and y represent a
    # minibatch)
    x = t.matrix('x')  # data, presented as rasterized images
    y = t.ivector('y')  # labels, presented as 1d vector of [int] labels

    # construct the id28 class
    # each mnist image has size 28*28
    classifier = logisticregression(input=x, n_in=28 * 28, n_out=10)


   we start by allocating symbolic variables for the training inputs x and
   their corresponding classes y . note that x and y are defined outside
   the scope of the logisticregression object. since the class requires
   the input to build its graph, it is passed as a parameter of the
   __init__ function. this is useful in case you want to connect instances
   of such classes to form a deep network. the output of one layer can be
   passed as the input of the layer above. (this tutorial does not build a
   multi-layer network, but this code will be reused in future tutorials
   that do.)

   finally, we define a (symbolic) cost variable to minimize, using the
   instance method classifier.negative_log_likelihood.
    # the cost we minimize during training is the negative log likelihood of
    # the model in symbolic format
    cost = classifier.negative_log_likelihood(y)


   note that x is an implicit symbolic input to the definition of cost,
   because the symbolic variables of classifier were defined in terms of x
   at initialization.

learning the model[33]  

   to implement msgd in most programming languages (c/c++, matlab,
   python), one would start by manually deriving the expressions for the
   gradient of the loss with respect to the parameters: in this case
   \partial{\ell}/\partial{w} , and \partial{\ell}/\partial{b} , this can
   get pretty tricky for complex models, as expressions for
   \partial{\ell}/\partial{\theta} can get fairly complex, especially when
   taking into account problems of numerical stability.

   with theano, this work is greatly simplified. it performs automatic
   differentiation and applies certain math transforms to improve
   numerical stability.

   to get the gradients \partial{\ell}/\partial{w} and
   \partial{\ell}/\partial{b} in theano, simply do the following:
    g_w = t.grad(cost=cost, wrt=classifier.w)
    g_b = t.grad(cost=cost, wrt=classifier.b)


   g_w and g_b are symbolic variables, which can be used as part of a
   computation graph. the function train_model, which performs one step of
   id119, can then be defined as follows:
    # specify how to update the parameters of the model as a list of
    # (variable, update expression) pairs.
    updates = [(classifier.w, classifier.w - learning_rate * g_w),
               (classifier.b, classifier.b - learning_rate * g_b)]

    # compiling a theano function `train_model` that returns the cost, but in
    # the same time updates the parameter of the model based on the rules
    # defined in `updates`
    train_model = theano.function(
        inputs=[index],
        outputs=cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + 1) * batch_size],
            y: train_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

   updates is a list of pairs. in each pair, the first element is the
   symbolic variable to be updated in the step, and the second element is
   the symbolic function for calculating its new value. similarly, givens
   is a dictionary whose keys are symbolic variables and whose values
   specify their replacements during the step. the function train_model is
   then defined such that:
     * the input is the mini-batch index index that, together with the
       batch size (which is not an input since it is fixed) defines x with
       corresponding labels y
     * the return value is the cost/loss associated with the x, y defined
       by the index
     * on every function call, it will first replace x and y with the
       slices from the training set specified by index. then, it will
       evaluate the cost associated with that minibatch and apply the
       operations defined by the updates list.

   each time train_model(index) is called, it will thus compute and return
   the cost of a minibatch, while also performing a step of msgd. the
   entire learning algorithm thus consists in looping over all examples in
   the dataset, considering all the examples in one minibatch at a time,
   and repeatedly calling the train_model function.

testing the model[34]  

   as explained in [35]learning a classifier, when testing the model we
   are interested in the number of misclassified examples (and not only in
   the likelihood). the logisticregression class therefore has an extra
   instance method, which builds the symbolic graph for retrieving the
   number of misclassified examples in each minibatch.

   the code is as follows:
    def errors(self, y):
        """return a float representing the number of errors in the minibatch
        over the total number of examples of the minibatch ; zero one
        loss over the size of the minibatch

        :type y: theano.tensor.tensortype
        :param y: corresponds to a vector that gives for each example the
                  correct label
        """

        # check if y has same dimension of y_pred
        if y.ndim != self.y_pred.ndim:
            raise typeerror(
                'y should have the same shape as self.y_pred',
                ('y', y.type, 'y_pred', self.y_pred.type)
            )
        # check if y is of the correct datatype
        if y.dtype.startswith('int'):
            # the t.neq operator returns a vector of 0s and 1s, where 1
            # represents a mistake in prediction
            return t.mean(t.neq(self.y_pred, y))
        else:
            raise notimplementederror()

   we then create a function test_model and a function validate_model,
   which we can call to retrieve this value. as you will see shortly,
   validate_model is key to our early-stopping implementation (see
   [36]early-stopping). these functions take a minibatch index and
   compute, for the examples in that minibatch, the number that were
   misclassified by the model. the only difference between them is that
   test_model draws its minibatches from the testing set, while
   validate_model draws its from the validation set.
    # compiling a theano function that computes the mistakes that are made by
    # the model on a minibatch
    test_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: test_set_x[index * batch_size: (index + 1) * batch_size],
            y: test_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

    validate_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: valid_set_x[index * batch_size: (index + 1) * batch_size],
            y: valid_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )


putting it all together[37]  

   the finished product is as follows.
"""
this tutorial introduces id28 using theano and stochastic
id119.

id28 is a probabilistic, linear classifier. it is parametrized
by a weight matrix :math:`w` and a bias vector :math:`b`. classification is
done by projecting data points onto a set of hyperplanes, the distance to
which is used to determine a class membership id203.

mathematically, this can be written as:

.. math::
  p(y=i|x, w,b) &= softmax_i(w x + b) \\
                &= \frac {e^{w_i x + b_i}} {\sum_j e^{w_j x + b_j}}


the output of the model or prediction is then done by taking the argmax of
the vector whose i'th element is p(y=i|x).

.. math::

  y_{pred} = argmax_i p(y=i|x,w,b)


this tutorial presents a stochastic id119 optimization method
suitable for large datasets.


references:

    - textbooks: "pattern recognition and machine learning" -
                 christopher m. bishop, section 4.3.2

"""

from __future__ import print_function

__docformat__ = 'restructedtext en'

import six.moves.cpickle as pickle
import gzip
import os
import sys
import timeit

import numpy

import theano
import theano.tensor as t


class logisticregression(object):
    """multi-class id28 class

    the id28 is fully described by a weight matrix :math:`w`
    and bias vector :math:`b`. classification is done by projecting data
    points onto a set of hyperplanes, the distance to which is used to
    determine a class membership id203.
    """

    def __init__(self, input, n_in, n_out):
        """ initialize the parameters of the id28

        :type input: theano.tensor.tensortype
        :param input: symbolic variable that describes the input of the
                      architecture (one minibatch)

        :type n_in: int
        :param n_in: number of input units, the dimension of the space in
                     which the datapoints lie

        :type n_out: int
        :param n_out: number of output units, the dimension of the space in
                      which the labels lie

        """
        # start-snippet-1
        # initialize with 0 the weights w as a matrix of shape (n_in, n_out)
        self.w = theano.shared(
            value=numpy.zeros(
                (n_in, n_out),
                dtype=theano.config.floatx
            ),
            name='w',
            borrow=true
        )
        # initialize the biases b as a vector of n_out 0s
        self.b = theano.shared(
            value=numpy.zeros(
                (n_out,),
                dtype=theano.config.floatx
            ),
            name='b',
            borrow=true
        )

        # symbolic expression for computing the matrix of class-membership
        # probabilities
        # where:
        # w is a matrix where column-k represent the separation hyperplane for
        # class-k
        # x is a matrix where row-j  represents input training sample-j
        # b is a vector where element-k represent the free parameter of
        # hyperplane-k
        self.p_y_given_x = t.nnet.softmax(t.dot(input, self.w) + self.b)

        # symbolic description of how to compute prediction as class whose
        # id203 is maximal
        self.y_pred = t.argmax(self.p_y_given_x, axis=1)
        # end-snippet-1

        # parameters of the model
        self.params = [self.w, self.b]

        # keep track of model input
        self.input = input

    def negative_log_likelihood(self, y):
        """return the mean of the negative log-likelihood of the prediction
        of this model under a given target distribution.

        .. math::

            \frac{1}{|\mathcal{d}|} \mathcal{l} (\theta=\{w,b\}, \mathcal{d}) =
            \frac{1}{|\mathcal{d}|} \sum_{i=0}^{|\mathcal{d}|}
                \log(p(y=y^{(i)}|x^{(i)}, w,b)) \\
            \ell (\theta=\{w,b\}, \mathcal{d})

        :type y: theano.tensor.tensortype
        :param y: corresponds to a vector that gives for each example the
                  correct label

        note: we use the mean instead of the sum so that
              the learning rate is less dependent on the batch size
        """
        # start-snippet-2
        # y.shape[0] is (symbolically) the number of rows in y, i.e.,
        # number of examples (call it n) in the minibatch
        # t.arange(y.shape[0]) is a symbolic vector which will contain
        # [0,1,2,... n-1] t.log(self.p_y_given_x) is a matrix of
        # log-probabilities (call it lp) with one row per example and
        # one column per class lp[t.arange(y.shape[0]),y] is a vector
        # v containing [lp[0,y[0]], lp[1,y[1]], lp[2,y[2]], ...,
        # lp[n-1,y[n-1]]] and t.mean(lp[t.arange(y.shape[0]),y]) is
        # the mean (across minibatch examples) of the elements in v,
        # i.e., the mean log-likelihood across the minibatch.
        return -t.mean(t.log(self.p_y_given_x)[t.arange(y.shape[0]), y])
        # end-snippet-2

    def errors(self, y):
        """return a float representing the number of errors in the minibatch
        over the total number of examples of the minibatch ; zero one
        loss over the size of the minibatch

        :type y: theano.tensor.tensortype
        :param y: corresponds to a vector that gives for each example the
                  correct label
        """

        # check if y has same dimension of y_pred
        if y.ndim != self.y_pred.ndim:
            raise typeerror(
                'y should have the same shape as self.y_pred',
                ('y', y.type, 'y_pred', self.y_pred.type)
            )
        # check if y is of the correct datatype
        if y.dtype.startswith('int'):
            # the t.neq operator returns a vector of 0s and 1s, where 1
            # represents a mistake in prediction
            return t.mean(t.neq(self.y_pred, y))
        else:
            raise notimplementederror()


def load_data(dataset):
    ''' loads the dataset

    :type dataset: string
    :param dataset: the path to the dataset (here mnist)
    '''

    #############
    # load data #
    #############

    # download the mnist dataset if it is not present
    data_dir, data_file = os.path.split(dataset)
    if data_dir == "" and not os.path.isfile(dataset):
        # check if dataset is in the data directory.
        new_path = os.path.join(
            os.path.split(__file__)[0],
            "..",
            "data",
            dataset
        )
        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':
            dataset = new_path

    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':
        from six.moves import urllib
        origin = (
            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'
        )
        print('downloading data from %s' % origin)
        urllib.request.urlretrieve(origin, dataset)

    print('... loading data')

    # load the dataset
    with gzip.open(dataset, 'rb') as f:
        try:
            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')
        except:
            train_set, valid_set, test_set = pickle.load(f)
    # train_set, valid_set, test_set format: tuple(input, target)
    # input is a numpy.ndarray of 2 dimensions (a matrix)
    # where each row corresponds to an example. target is a
    # numpy.ndarray of 1 dimension (vector) that has the same length as
    # the number of rows in the input. it should give the target
    # to the example with the same index in the input.

    def shared_dataset(data_xy, borrow=true):
        """ function that loads the dataset into shared variables

        the reason we store our dataset in shared variables is to allow
        theano to copy it into the gpu memory (when code is run on gpu).
        since copying data into the gpu is slow, copying a minibatch everytime
        is needed (the default behaviour if the data is not in a shared
        variable) would lead to a large decrease in performance.
        """
        data_x, data_y = data_xy
        shared_x = theano.shared(numpy.asarray(data_x,
                                               dtype=theano.config.floatx),
                                 borrow=borrow)
        shared_y = theano.shared(numpy.asarray(data_y,
                                               dtype=theano.config.floatx),
                                 borrow=borrow)
        # when storing data on the gpu it has to be stored as floats
        # therefore we will store the labels as ``floatx`` as well
        # (``shared_y`` does exactly that). but during our computations
        # we need them as ints (we use labels as index, and if they are
        # floats it doesn't make sense) therefore instead of returning
        # ``shared_y`` we will have to cast it to int. this little hack
        # lets ous get around this issue
        return shared_x, t.cast(shared_y, 'int32')

    test_set_x, test_set_y = shared_dataset(test_set)
    valid_set_x, valid_set_y = shared_dataset(valid_set)
    train_set_x, train_set_y = shared_dataset(train_set)

    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),
            (test_set_x, test_set_y)]
    return rval


def sgd_optimization_mnist(learning_rate=0.13, n_epochs=1000,
                           dataset='mnist.pkl.gz',
                           batch_size=600):
    """
    demonstrate stochastic id119 optimization of a log-linear
    model

    this is demonstrated on mnist.

    :type learning_rate: float
    :param learning_rate: learning rate used (factor for the stochastic
                          gradient)

    :type n_epochs: int
    :param n_epochs: maximal number of epochs to run the optimizer

    :type dataset: string
    :param dataset: the path of the mnist dataset file from
                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz

    """
    datasets = load_data(dataset)

    train_set_x, train_set_y = datasets[0]
    valid_set_x, valid_set_y = datasets[1]
    test_set_x, test_set_y = datasets[2]

    # compute number of minibatches for training, validation and testing
    n_train_batches = train_set_x.get_value(borrow=true).shape[0] // batch_size
    n_valid_batches = valid_set_x.get_value(borrow=true).shape[0] // batch_size
    n_test_batches = test_set_x.get_value(borrow=true).shape[0] // batch_size

    ######################
    # build actual model #
    ######################
    print('... building the model')

    # allocate symbolic variables for the data
    index = t.lscalar()  # index to a [mini]batch

    # generate symbolic variables for input (x and y represent a
    # minibatch)
    x = t.matrix('x')  # data, presented as rasterized images
    y = t.ivector('y')  # labels, presented as 1d vector of [int] labels

    # construct the id28 class
    # each mnist image has size 28*28
    classifier = logisticregression(input=x, n_in=28 * 28, n_out=10)

    # the cost we minimize during training is the negative log likelihood of
    # the model in symbolic format
    cost = classifier.negative_log_likelihood(y)

    # compiling a theano function that computes the mistakes that are made by
    # the model on a minibatch
    test_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: test_set_x[index * batch_size: (index + 1) * batch_size],
            y: test_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

    validate_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: valid_set_x[index * batch_size: (index + 1) * batch_size],
            y: valid_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

    # compute the gradient of cost with respect to theta = (w,b)
    g_w = t.grad(cost=cost, wrt=classifier.w)
    g_b = t.grad(cost=cost, wrt=classifier.b)

    # start-snippet-3
    # specify how to update the parameters of the model as a list of
    # (variable, update expression) pairs.
    updates = [(classifier.w, classifier.w - learning_rate * g_w),
               (classifier.b, classifier.b - learning_rate * g_b)]

    # compiling a theano function `train_model` that returns the cost, but in
    # the same time updates the parameter of the model based on the rules
    # defined in `updates`
    train_model = theano.function(
        inputs=[index],
        outputs=cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + 1) * batch_size],
            y: train_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )
    # end-snippet-3

    ###############
    # train model #
    ###############
    print('... training the model')
    # early-stopping parameters
    patience = 5000  # look as this many examples regardless
    patience_increase = 2  # wait this much longer when a new best is
                                  # found
    improvement_threshold = 0.995  # a relative improvement of this much is
                                  # considered significant
    validation_frequency = min(n_train_batches, patience // 2)
                                  # go through this many
                                  # minibatche before checking the network
                                  # on the validation set; in this case we
                                  # check every epoch

    best_validation_loss = numpy.inf
    test_score = 0.
    start_time = timeit.default_timer()

    done_looping = false
    epoch = 0
    while (epoch < n_epochs) and (not done_looping):
        epoch = epoch + 1
        for minibatch_index in range(n_train_batches):

            minibatch_avg_cost = train_model(minibatch_index)
            # iteration number
            iter = (epoch - 1) * n_train_batches + minibatch_index

            if (iter + 1) % validation_frequency == 0:
                # compute zero-one loss on validation set
                validation_losses = [validate_model(i)
                                     for i in range(n_valid_batches)]
                this_validation_loss = numpy.mean(validation_losses)

                print(
                    'epoch %i, minibatch %i/%i, validation error %f %%' %
                    (
                        epoch,
                        minibatch_index + 1,
                        n_train_batches,
                        this_validation_loss * 100.
                    )
                )

                # if we got the best validation score until now
                if this_validation_loss < best_validation_loss:
                    #improve patience if loss improvement is good enough
                    if this_validation_loss < best_validation_loss *  \
                       improvement_threshold:
                        patience = max(patience, iter * patience_increase)

                    best_validation_loss = this_validation_loss
                    # test it on the test set

                    test_losses = [test_model(i)
                                   for i in range(n_test_batches)]
                    test_score = numpy.mean(test_losses)

                    print(
                        (
                            '     epoch %i, minibatch %i/%i, test error of'
                            ' best model %f %%'
                        ) %
                        (
                            epoch,
                            minibatch_index + 1,
                            n_train_batches,
                            test_score * 100.
                        )
                    )

                    # save the best model
                    with open('best_model.pkl', 'wb') as f:
                        pickle.dump(classifier, f)

            if patience <= iter:
                done_looping = true
                break

    end_time = timeit.default_timer()
    print(
        (
            'optimization complete with best validation score of %f %%,'
            'with test performance %f %%'
        )
        % (best_validation_loss * 100., test_score * 100.)
    )
    print('the code run for %d epochs, with %f epochs/sec' % (
        epoch, 1. * epoch / (end_time - start_time)))
    print(('the code for file ' +
           os.path.split(__file__)[1] +
           ' ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)


def predict():
    """
    an example of how to load a trained model and use it
    to predict labels.
    """

    # load the saved model
    classifier = pickle.load(open('best_model.pkl'))

    # compile a predictor function
    predict_model = theano.function(
        inputs=[classifier.input],
        outputs=classifier.y_pred)

    # we can test it on some examples from test test
    dataset='mnist.pkl.gz'
    datasets = load_data(dataset)
    test_set_x, test_set_y = datasets[2]
    test_set_x = test_set_x.get_value()

    predicted_values = predict_model(test_set_x[:10])
    print("predicted values for the first 10 examples in test set:")
    print(predicted_values)


if __name__ == '__main__':
    sgd_optimization_mnist()

   the user can learn to classify mnist digits with sgd logistic
   regression, by typing, from within the deeplearningtutorials folder:
python code/logistic_sgd.py

   the output one should expect is of the form:
...
epoch 72, minibatch 83/83, validation error 7.510417 %
     epoch 72, minibatch 83/83, test error of best model 7.510417 %
epoch 73, minibatch 83/83, validation error 7.500000 %
     epoch 73, minibatch 83/83, test error of best model 7.489583 %
optimization complete with best validation score of 7.500000 %,with test perform
ance 7.489583 %
the code run for 74 epochs, with 1.936983 epochs/sec

   on an intel(r) core(tm)2 duo cpu e8400 @ 3.00 ghz the code runs with
   approximately 1.936 epochs/sec and it took 75 epochs to reach a test
   error of 7.489%. on the gpu the code does almost 10.0 epochs/sec. for
   this instance we used a batch size of 600.

prediction using a trained model[38]  

   sgd_optimization_mnist serialize and pickle the model each time new
   lowest validation error is reached. we can reload this model and
   predict labels of new data. predict function shows an example of how
   this could be done.
def predict():
    """
    an example of how to load a trained model and use it
    to predict labels.
    """

    # load the saved model
    classifier = pickle.load(open('best_model.pkl'))

    # compile a predictor function
    predict_model = theano.function(
        inputs=[classifier.input],
        outputs=classifier.y_pred)

    # we can test it on some examples from test test
    dataset='mnist.pkl.gz'
    datasets = load_data(dataset)
    test_set_x, test_set_y = datasets[2]
    test_set_x = test_set_x.get_value()

    predicted_values = predict_model(test_set_x[:10])
    print("predicted values for the first 10 examples in test set:")
    print(predicted_values)

   footnotes
   [1] for smaller datasets and simpler models, more sophisticated descent
   algorithms can be more effective. the sample code [39]logistic_cg.py
   demonstrates how to use scipy   s conjugate gradient solver with theano
   on the id28 task.

navigation

     * [40]index
     * [41]next |
     * [42]previous |
     * [43]deeplearning 0.1 documentation   

      copyright 2008--2010, lisa lab. last updated on jun 15, 2018. created
   using [44]sphinx 1.5.

references

   1. http://deeplearning.net/tutorial/genindex.html
   2. http://deeplearning.net/tutorial/search.html
   3. http://deeplearning.net/tutorial/mlp.html
   4. http://deeplearning.net/tutorial/gettingstarted.html
   5. http://deeplearning.net/tutorial/genindex.html
   6. http://deeplearning.net/tutorial/mlp.html
   7. http://deeplearning.net/tutorial/gettingstarted.html
   8. http://deeplearning.net/tutorial/contents.html
   9. http://deeplearning.net/tutorial/contents.html
  10. http://deeplearning.net/tutorial/logreg.html
  11. http://deeplearning.net/tutorial/logreg.html#the-model
  12. http://deeplearning.net/tutorial/logreg.html#defining-a-loss-function
  13. http://deeplearning.net/tutorial/logreg.html#creating-a-logisticregression-class
  14. http://deeplearning.net/tutorial/logreg.html#learning-the-model
  15. http://deeplearning.net/tutorial/logreg.html#testing-the-model
  16. http://deeplearning.net/tutorial/logreg.html#putting-it-all-together
  17. http://deeplearning.net/tutorial/logreg.html#prediction-using-a-trained-model
  18. http://deeplearning.net/tutorial/gettingstarted.html
  19. http://deeplearning.net/tutorial/mlp.html
  20. http://deeplearning.net/tutorial/_sources/logreg.txt
  21. http://deeplearning.net/tutorial/logreg.html#classifying-mnist-digits-using-logistic-regression
  22. http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables
  23. http://deeplearning.net/software/theano/tutorial/adding.html#adding-two-scalars
  24. http://deeplearning.net/software/theano/tutorial/examples.html#computing-gradients
  25. http://deeplearning.net/software/theano/library/config.html#config.floatx
  26. http://deeplearning.net/software/theano/tutorial/using_gpu.html
  27. http://deeplearning.net/tutorial/code/logistic_sgd.py
  28. http://deeplearning.net/tutorial/logreg.html#the-model
  29. http://deeplearning.net/software/theano/library/tensor/basic.html#basic-tensor-functionality
  30. http://deeplearning.net/tutorial/logreg.html#defining-a-loss-function
  31. http://deeplearning.net/tutorial/gettingstarted.html#opt-sgd
  32. http://deeplearning.net/tutorial/logreg.html#creating-a-logisticregression-class
  33. http://deeplearning.net/tutorial/logreg.html#learning-the-model
  34. http://deeplearning.net/tutorial/logreg.html#testing-the-model
  35. http://deeplearning.net/tutorial/gettingstarted.html#opt-learn-classifier
  36. http://deeplearning.net/tutorial/gettingstarted.html#opt-early-stopping
  37. http://deeplearning.net/tutorial/logreg.html#putting-it-all-together
  38. http://deeplearning.net/tutorial/logreg.html#prediction-using-a-trained-model
  39. http://deeplearning.net/tutorial/code/logistic_cg.py
  40. http://deeplearning.net/tutorial/genindex.html
  41. http://deeplearning.net/tutorial/mlp.html
  42. http://deeplearning.net/tutorial/gettingstarted.html
  43. http://deeplearning.net/tutorial/contents.html
  44. http://sphinx-doc.org/
