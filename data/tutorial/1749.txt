
   [1]florian hartl

id28     geometric intuition

   search ____________________ start searching
     * [2]home
     * [3]machine learning
     * [4]entrepreneurship
     * [5]information technology
     * [6]sports
     * [7]unstructured

id28     geometric intuition

   october 5, 2015

   everybody who has taken a machine learning course probably knows the
   geometric intuition behind a support vector machine (id166, [8]great
   book): a id166 is a large margin classifier. in other words, it maximizes
   the geometric distance between the decision boundary and the classes of
   samples. often you   ll find plots similar to this one:
   [9][157.png]

   but what about [10]id28? what is the geometric intuition
   behind it and how does it compare to linear id166s? let   s find out.

geometric intuition behind id28

   first, a quick reminder about the definition of the [11]logistic
   function, given n features:

       \[\begin{aligned} & g(z) = {{1} \over {1+e^{-z}}}\qquad
   \text{with}\\ & z = f(x) = w_0 + w_1x_1 + ... + w_nx_n \end{aligned}\]

   with that out of the way, let   s dive into the geometric aspects of
   id28, starting with a toy example of only one feature:
   [12][140.png]

   the blue samples of our feature data belong to class \text{y}=1 , while
   the red dots are supposed to get assigned to \text{y}=0 . as we can see
   in the figure above, our data is fully separable and the logistic
   regression is able to demonstrate this property. from a geometric
   perspective, the algorithm essentially adds a new dimension for
   our dependent variable \boldsymbol{\text{y}} and fits a logistic
   function  g(z) to the arising two-dimensional data in such a way that
   it separates the samples as good as possible within the range of
   0<=g(z)<=1 . once we have such a fitted curve, the usual method of
   prediction is to assign \text{y}=1 to everything g(z(sample))>=0.5 and
   vice versa.

   when thinking about id28, i usually have its tight
   connection to id75 somewhere in the back of my head (no
   surprise, both have    regression    as part of their names). therefore, i
   was wondering what the line \boldsymbol{\text{z}} would look like and
   added it to the plot. as we can easily see, \boldsymbol{\text{z}}
   differs significantly from the potential result of a id75
   for the given input data. while that was to be expected, i think it   s
   still nice to confirm it visually.

   note two details of the plot: for one, the logistic curve is not as
   steep as you might expect at first. this is due to the use of
   id173, which id28 basically can   t live without.
   second, g(z)=0.5 for z=0 and not, as my intuition initially suggested,
   for z=0.5 . not sure if this is particularly helpful, but i find it
   interesting nonetheless.

   so far, so good. but what happens when we introduce an additional
   dimension? let   s have a look at such a two feature scenario (feel free
   to pinch and zoom the plot):
   [13][149.png]

   what we see is the surface of the fitted logistic function  g(z) for a
   classification task with two features. this time, instead of
   transforming a line to a curve, the equivalent happened for a plane. to
   make it easier to understand how to carry out predictions given such a
   model, the following plot visualizes the data points projected onto the
   surface of the id28 function:
   [14][151.png]

   now we can again make decisions for our class \boldsymbol{\text{y}}
   based on a threshold \boldsymbol{\text{t}} , which usually is 0.5 :

       \[\text{y}=\\ \begin{cases} 1 & \text{for }g(z(sample)) >= t\\ 0 &
   \text{for }g(z(sample)) < t\\ \end{cases}\]

   so much about the geometric intuition behind id28. let   s
   see how that compares to linear id166s.

comparison of id28 to linear id166

   first of all, notice that with id28 we can use the g(z)
   value of a sample as a id203 estimate for  \text{y}(sample)=1 .
   the responsive 3d-plots above created with the help
   of [15]plotly   s fantastic library illustrate this property of logistic
   regression very well. i have yet to come across a real-world
   classification task where such id203 estimates are not at least
   somewhat useful. alas, id166s (in their original version) aren   t able to
   provide them. due to the way the model works, all it can tell you is
   whether a sample belongs to class  \text{y}=1 or not.

   i think at this point the most effective way of comparing logistic
   regression to linear id166 geometrically, is to add the decision boundary
   of id28 to the initial figure of the post. to make this
   happen, we need to project the decision boundary g(z)=0.5 of the
   three-dimensional plot above onto the two-dimensional feature space.
   doing so, we end up with the following figure:
   [16][153.png]

   as we can see, the id28 does not follow the same large
   margin characteristic as a linear id166. i   ll leave it to you to decide
   which is the better fit for the given data.

   if you   d like to know further differences between id28
   and linear id166s aside from the geometrically motivated ones mentioned
   here, have a look at the excellent answers [17]on quora.

   the code for generating the plots of this post can be found
   on [18]github.

tell the world

     * [19]twitter
     * [20]facebook
     * [21]linkedin
     * [22]reddit
     *

   id28     geometric intuition was posted in [23]machine
   learning by florian hartl.

9 comments on id28     geometric intuition

    1. [24]cast42 on [25]october 6, 2015 at 23:38 said:
       great tutorial !. thx for sharing.
    2. ezequiel on [26]october 8, 2015 at 07:18 said:
       thanks for this incredible article
    3. nicot on [27]october 8, 2015 at 09:00 said:
       great post! and nice viz!! thanks for posting!
    4. ivo p on [28]november 20, 2016 at 03:47 said:
       amazing! thanks
    5. andrej l. on [29]july 16, 2017 at 06:49 said:
       very helpful. great work! thanks
    6. trung nguyen on [30]february 28, 2018 at 02:03 said:
       the plots are very helpful, thanks a lot!
    7. jeetendra dhall on [31]august 5, 2018 at 22:10 said:
       it just clicks! thank you so much!
    8. jesper on [32]november 2, 2018 at 07:51 said:
       the id75 line doesn   t seem to be correct. i don   t know
       if your way (through machine learning) gives another result that
       just ordinary id75. either way, the slope is way to
       steep.
    9. florian hartl on [33]november 2, 2018 at 15:14 said:
       jesper, thanks for bringing this up! looking at the code again, i
       first fit the id28 g(z) to the data and then plotted
       the linear function z, i.e. the linear combination of weights and
       features before the logistic transformation. the function z looks
       just like id75 but is solved with a different loss
       function (e.g. log loss vs. mse) and hence, you   re right, z is not
       an actual line of a standalone id75. i should have
       been more clear about this in the post.

feedback

   your email address will not be published. required fields are marked *

   name * ______________________________

   email ______________________________

   website ______________________________

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   post comment

about me

   [34]florian hartl

   hi, i'm florian hartl. my main interests are data science, software
   engineering, health, and meaning. originally from bavaria, germany, i
   currently live in santa monica where i work as a data scientist.
   [35]follow me on rss [36]follow me on github [37]follow me on linkedin
   [38]follow me on meetup [39]follow me on speaker deck

recent articles

     * [40]large scale ctr prediction     lessons learned
     * [41]id28     geometric intuition
     * [42]thoughts on machine learning     dealing with skewed classes
     * [43]entrepreneurship: silicon valley vs. munich     part 1
     * [44]nutch     plugin tutorial

categories

     * [45]entrepreneurship (3)
     * [46]information technology (2)
     * [47]machine learning (10)
     * [48]sports (1)
     * [49]unstructured (1)

      2019 [50]florian hartl | [51]imprint | [52]sitemap | 107q 0.470s
   [53]    scotty   

references

   1. https://florianhartl.com/
   2. https://florianhartl.com/
   3. https://florianhartl.com/category/machine-learning.html
   4. https://florianhartl.com/category/entrepreneurship.html
   5. https://florianhartl.com/category/information-technology.html
   6. https://florianhartl.com/category/sports.html
   7. https://florianhartl.com/category/unstructured.html
   8. http://www.amazon.com/gp/product/0470371927/ref=as_li_qf_sp_asin_il_tl?ie=utf8&camp=1789&creative=9325&creativeasin=0470371927&linkcode=as2&tag=florhart-20&linkid=uhiwkyiwstsrpnve
   9. https://plot.ly/~florianh/157
  10. https://en.wikipedia.org/wiki/logistic_regression
  11. https://en.wikipedia.org/wiki/logistic_function
  12. https://plot.ly/~florianh/140
  13. https://plot.ly/~florianh/149
  14. https://plot.ly/~florianh/151
  15. https://plot.ly/
  16. https://plot.ly/~florianh/153
  17. https://www.quora.com/support-vector-machines/what-is-the-difference-between-linear-id166s-and-logistic-regression
  18. https://github.com/hafl/florianhartl.com
  19. https://florianhartl.com/logistic-regression-geometric-intuition.html?share=twitter
  20. https://florianhartl.com/logistic-regression-geometric-intuition.html?share=facebook
  21. https://florianhartl.com/logistic-regression-geometric-intuition.html?share=linkedin
  22. https://florianhartl.com/logistic-regression-geometric-intuition.html?share=reddit
  23. https://florianhartl.com/category/machine-learning.html
  24. https://castfortwo.blogspot.be/
  25. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-13247
  26. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-13249
  27. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-13250
  28. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-14584
  29. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-15336
  30. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-15778
  31. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-15890
  32. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-15967
  33. https://florianhartl.com/logistic-regression-geometric-intuition.html#comment-15969
  34. https://florianhartl.com/wp-content/uploads/florian-hartl.jpg
  35. https://florianhartl.com/feed/
  36. https://github.com/hafl
  37. https://www.linkedin.com/in/florianhartl
  38. https://www.meetup.com/members/30108772/
  39. https://speakerdeck.com/hafl
  40. https://florianhartl.com/large-scale-ctr-prediction-lessons-learned.html
  41. https://florianhartl.com/logistic-regression-geometric-intuition.html
  42. https://florianhartl.com/thoughts-on-machine-learning-dealing-with-skewed-classes.html
  43. https://florianhartl.com/entrepreneurship-silicon-valley-vs-munich-part-1.html
  44. https://florianhartl.com/nutch-plugin-tutorial.html
  45. https://florianhartl.com/category/entrepreneurship.html
  46. https://florianhartl.com/category/information-technology.html
  47. https://florianhartl.com/category/machine-learning.html
  48. https://florianhartl.com/category/sports.html
  49. https://florianhartl.com/category/unstructured.html
  50. https://florianhartl.com/
  51. https://florianhartl.com/imprint.html
  52. https://florianhartl.com/sitemap.html
  53. https://florianhartl.com/logistic-regression-geometric-intuition.html#page
