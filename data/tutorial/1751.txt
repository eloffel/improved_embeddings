mining

of

massive
datasets

jure leskovec
stanford univ.

anand rajaraman

milliway labs

je   rey d. ullman

stanford univ.

copyright c(cid:13) 2010, 2011, 2012, 2013, 2014 anand rajaraman, jure leskovec,
and je   rey d. ullman

ii

preface

this book evolved from material developed over several years by anand raja-
raman and je    ullman for a one-quarter course at stanford. the course
cs345a, titled    web mining,    was designed as an advanced graduate course,
although it has become accessible and interesting to advanced undergraduates.
when jure leskovec joined the stanford faculty, we reorganized the material
considerably. he introduced a new course cs224w on network analysis and
added material to cs345a, which was renumbered cs246. the three authors
also introduced a large-scale data-mining project course, cs341. the book now
contains material taught in all three courses.

what the book is about

at the highest level of description, this book is about data mining. however,
it focuses on data mining of very large amounts of data, that is, data so large
it does not    t in main memory. because of the emphasis on size, many of our
examples are about the web or data derived from the web. further, the book
takes an algorithmic point of view: data mining is about applying algorithms
to data, rather than using data to    train    a machine-learning engine of some
sort. the principal topics covered are:

1. distributed    le systems and map-reduce as a tool for creating parallel

algorithms that succeed on very large amounts of data.

2. similarity search, including the key techniques of minhashing and locality-

sensitive hashing.

3. data-stream processing and specialized algorithms for dealing with data

that arrives so fast it must be processed immediately or lost.

4. the technology of search engines, including google   s id95, link-spam

detection, and the hubs-and-authorities approach.

5. frequent-itemset mining, including association rules, market-baskets, the

a-priori algorithm and its improvements.

6. algorithms for id91 very large, high-dimensional datasets.

iii

iv

preface

7. two key problems for web applications: managing advertising and rec-

ommendation systems.

8. algorithms for analyzing and mining the structure of very large graphs,

especially social-network graphs.

9. techniques for obtaining the important properties of a large dataset by
id84, including id166 and la-
tent semantic indexing.

10. machine-learning algorithms that can be applied to very large data, such

as id88s, support-vector machines, and id119.

prerequisites

to appreciate fully the material in this book, we recommend the following
prerequisites:

1. an introduction to database systems, covering sql and related program-

ming systems.

2. a sophomore-level course in data structures, algorithms, and discrete

math.

3. a sophomore-level course in software systems, software engineering, and

programming languages.

exercises

the book contains extensive exercises, with some for almost every section. we
indicate harder exercises or parts of exercises with an exclamation point. the
hardest exercises have a double exclamation point.

support on the web

go to http://www.mmds.org for slides, homework assignments, project require-
ments, and exams from courses related to this book.

gradiance automated homework

there are automated exercises based on this book, using the gradiance root-
question technology, available at www.gradiance.com/services. students may
enter a public class by creating an account at that site and entering the class
with code 1edd8a1d. instructors may use the site by making an account there

preface

v

and then emailing support at gradiance dot com with their login name, the
name of their school, and a request to use the mmds materials.

acknowledgements

cover art is by scott ullman.

we would like to thank foto afrati, arun marathe, and rok sosic for critical

readings of a draft of this manuscript.

errors were also reported by rajiv abraham, ruslan aduk, apoorv agar-
wal, aris anagnostopoulos, yokila arora, stefanie anna baby, atilla soner
balkir, arnaud belletoile, robin bennett, susan biancani, amitabh chaud-
hary, leland chen, hua feng, marcus gemeinder, anastasios gounaris, clark
grubb, shrey gupta, waleed hameid, saman haratizadeh, julien hoachuck,
przemyslaw horban, hsiu-hsuan huang, je    hwang, ra    kamal, lachlan
kang, ed knorr, haewoon kwak, ellis lau, greg lee, david z. liu, ethan
lozano, yunan luo, michael mahoney, justin meyer, bryant moscon, brad
peno   , john phillips, philips kokoh prasetyo, qi ge, harizo rajaona, ti-
mon ruban, rich seiter, hitesh shetty, angad singh, sandeep sripada, dennis
sidharta, krzysztof stencel, mark storus, roshan sumbaly, zack taylor, tim
triche jr., wang bin, weng zhen-bin, robert west, steven euijong whang,
oscar wu, xie ke, christopher t.-r. yeh, nicolas zhao, and zhou jingbo, the
remaining errors are ours, of course.

j. l.
a. r.
j. d. u.
palo alto, ca
march, 2014

vi

preface

contents

1 data mining

1
1.1 what is data mining? . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
statistical modeling . . . . . . . . . . . . . . . . . . . . .
1
1.1.2 machine learning . . . . . . . . . . . . . . . . . . . . . .
2
1.1.3 computational approaches to modeling . . . . . . . . . .
2
1.1.4
summarization . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.5 feature extraction . . . . . . . . . . . . . . . . . . . . . .
4
1.2 statistical limits on data mining . . . . . . . . . . . . . . . . . .
4
1.2.1 total information awareness . . . . . . . . . . . . . . . .
5
1.2.2 bonferroni   s principle . . . . . . . . . . . . . . . . . . . .
5
1.2.3 an example of bonferroni   s principle . . . . . . . . . . .
6
1.2.4 exercises for section 1.2 . . . . . . . . . . . . . . . . . . .
7
1.3 things useful to know . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.1
. . . . . . . . . . . .
8
1.3.2 hash functions . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.3
indexes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.4
secondary storage . . . . . . . . . . . . . . . . . . . . . . 11
1.3.5 the base of natural logarithms . . . . . . . . . . . . . . 12
1.3.6 power laws . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3.7 exercises for section 1.3 . . . . . . . . . . . . . . . . . . . 15
1.4 outline of the book . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.5 summary of chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . 17
1.6 references for chapter 1 . . . . . . . . . . . . . . . . . . . . . . . 18

importance of words in documents

2 mapreduce and the new software stack

21
2.1 distributed file systems . . . . . . . . . . . . . . . . . . . . . . . 22
2.1.1 physical organization of compute nodes
. . . . . . . . . 22
2.1.2 large-scale file-system organization . . . . . . . . . . . 23
2.2 mapreduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.2.1 the map tasks . . . . . . . . . . . . . . . . . . . . . . . . 25
2.2.2 grouping by key . . . . . . . . . . . . . . . . . . . . . . . 26
2.2.3 the reduce tasks . . . . . . . . . . . . . . . . . . . . . . 27
2.2.4 combiners . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

vii

viii

contents

2.2.5 details of mapreduce execution . . . . . . . . . . . . . . 28
2.2.6 coping with node failures . . . . . . . . . . . . . . . . . 29
2.2.7 exercises for section 2.2 . . . . . . . . . . . . . . . . . . . 30
2.3 algorithms using mapreduce . . . . . . . . . . . . . . . . . . . . 30
. . . . . . . 31
2.3.1 matrix-vector multiplication by mapreduce
if the vector v cannot fit in main memory . . . . . . . . 31
2.3.2
. . . . . . . . . . . . . . . 32
2.3.3 relational-algebra operations
. . . . . . . . . . . 35
2.3.4 computing selections by mapreduce
2.3.5 computing projections by mapreduce . . . . . . . . . . . 36
2.3.6 union, intersection, and di   erence by mapreduce . . . . 36
2.3.7 computing natural join by mapreduce . . . . . . . . . . 37
2.3.8 grouping and aggregation by mapreduce . . . . . . . . . 37
2.3.9 id127 . . . . . . . . . . . . . . . . . . . . 38
2.3.10 id127 with one mapreduce step . . . . . 39
2.3.11 exercises for section 2.3 . . . . . . . . . . . . . . . . . . . 40
2.4 extensions to mapreduce . . . . . . . . . . . . . . . . . . . . . . 41
2.4.1 work   ow systems
. . . . . . . . . . . . . . . . . . . . . . 41
2.4.2 recursive extensions to mapreduce . . . . . . . . . . . . 42
2.4.3 pregel
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.4.4 exercises for section 2.4 . . . . . . . . . . . . . . . . . . . 46
. . . . . . . . . . . . . . . . . . 46
2.5.1 communication-cost for task networks . . . . . . . . . . 47
2.5.2 wall-clock time . . . . . . . . . . . . . . . . . . . . . . . 49
2.5.3 multiway joins . . . . . . . . . . . . . . . . . . . . . . . . 49
2.5.4 exercises for section 2.5 . . . . . . . . . . . . . . . . . . . 52
2.6 complexity theory for mapreduce . . . . . . . . . . . . . . . . . 54
2.6.1 reducer size and replication rate . . . . . . . . . . . . . 54
2.6.2 an example: similarity joins . . . . . . . . . . . . . . . . 55
2.6.3 a graph model for mapreduce problems . . . . . . . . . 57
. . . . . . . . . . . . . . . . . . . . . . 58
2.6.4 mapping schemas
2.6.5 when not all inputs are present
. . . . . . . . . . . . . 60
2.6.6 lower bounds on replication rate . . . . . . . . . . . . . 61
2.6.7 case study: id127 . . . . . . . . . . . . . 62
2.6.8 exercises for section 2.6 . . . . . . . . . . . . . . . . . . . 66
2.7 summary of chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . 67
2.8 references for chapter 2 . . . . . . . . . . . . . . . . . . . . . . . 69

2.5 the communication cost model

3 finding similar items

73
3.1 applications of near-neighbor search . . . . . . . . . . . . . . . 73
jaccard similarity of sets . . . . . . . . . . . . . . . . . . 74
3.1.1
similarity of documents . . . . . . . . . . . . . . . . . . . 74
3.1.2
3.1.3 id185 as a similar-sets problem . . . . . 75
3.1.4 exercises for section 3.1 . . . . . . . . . . . . . . . . . . . 77
. . . . . . . . . . . . . . . . . . . . . . . 77
k-shingles . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

3.2 shingling of documents

3.2.1

contents

ix

3.2.2 choosing the shingle size . . . . . . . . . . . . . . . . . . 78
3.2.3 hashing shingles . . . . . . . . . . . . . . . . . . . . . . . 79
3.2.4
shingles built from words . . . . . . . . . . . . . . . . . . 79
3.2.5 exercises for section 3.2 . . . . . . . . . . . . . . . . . . . 80
3.3 similarity-preserving summaries of sets . . . . . . . . . . . . . . 80
3.3.1 matrix representation of sets . . . . . . . . . . . . . . . . 81
3.3.2 minhashing . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3.3.3 minhashing and jaccard similarity . . . . . . . . . . . . . 82
3.3.4 minhash signatures
. . . . . . . . . . . . . . . . . . . . . 83
3.3.5 computing minhash signatures . . . . . . . . . . . . . . . 83
3.3.6 exercises for section 3.3 . . . . . . . . . . . . . . . . . . . 86
3.4 locality-sensitive hashing for documents . . . . . . . . . . . . . 87
3.4.1 lsh for minhash signatures
. . . . . . . . . . . . . . . . 88
3.4.2 analysis of the banding technique . . . . . . . . . . . . . 89
3.4.3 combining the techniques . . . . . . . . . . . . . . . . . . 91
3.4.4 exercises for section 3.4 . . . . . . . . . . . . . . . . . . . 91
3.5 distance measures . . . . . . . . . . . . . . . . . . . . . . . . . . 92
3.5.1 de   nition of a distance measure . . . . . . . . . . . . . . 92
3.5.2 euclidean distances
. . . . . . . . . . . . . . . . . . . . . 93
3.5.3
jaccard distance . . . . . . . . . . . . . . . . . . . . . . . 94
3.5.4 cosine distance . . . . . . . . . . . . . . . . . . . . . . . . 95
3.5.5 id153 . . . . . . . . . . . . . . . . . . . . . . . . . 95
3.5.6 hamming distance . . . . . . . . . . . . . . . . . . . . . . 96
3.5.7 exercises for section 3.5 . . . . . . . . . . . . . . . . . . . 97
. . . . . . . . . . . . 99
3.6.1 locality-sensitive functions . . . . . . . . . . . . . . . . . 99
3.6.2 locality-sensitive families for jaccard distance . . . . . . 100
3.6.3 amplifying a locality-sensitive family . . . . . . . . . . . 101
3.6.4 exercises for section 3.6 . . . . . . . . . . . . . . . . . . . 103
3.7 lsh families for other distance measures . . . . . . . . . . . . . 104
3.7.1 lsh families for hamming distance . . . . . . . . . . . . 104
3.7.2 random hyperplanes and the cosine distance . . . . . . 105
3.7.3
sketches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
3.7.4 lsh families for euclidean distance . . . . . . . . . . . . 107
3.7.5 more lsh families for euclidean spaces . . . . . . . . . . 108
3.7.6 exercises for section 3.7 . . . . . . . . . . . . . . . . . . . 109
3.8 applications of locality-sensitive hashing . . . . . . . . . . . . . 110
3.8.1 entity resolution . . . . . . . . . . . . . . . . . . . . . . . 110
3.8.2 an entity-resolution example . . . . . . . . . . . . . . . 111
3.8.3 validating record matches
. . . . . . . . . . . . . . . . . 112
3.8.4 matching fingerprints . . . . . . . . . . . . . . . . . . . . 113
3.8.5 a lsh family for fingerprint matching . . . . . . . . . . 114
3.8.6
. . . . . . . . . . . . . . . . . . . . 115
3.8.7 exercises for section 3.8 . . . . . . . . . . . . . . . . . . . 117
3.9 methods for high degrees of similarity . . . . . . . . . . . . . . 118

3.6 the theory of locality-sensitive functions

similar news articles

x

contents

3.9.1 finding identical items
. . . . . . . . . . . . . . . . . . . 118
3.9.2 representing sets as strings . . . . . . . . . . . . . . . . . 118
3.9.3 length-based filtering . . . . . . . . . . . . . . . . . . . . 119
3.9.4 pre   x indexing . . . . . . . . . . . . . . . . . . . . . . . . 119
3.9.5 using position information . . . . . . . . . . . . . . . . . 121
3.9.6 using position and length in indexes
. . . . . . . . . . . 122
3.9.7 exercises for section 3.9 . . . . . . . . . . . . . . . . . . . 125
3.10 summary of chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . 126
3.11 references for chapter 3 . . . . . . . . . . . . . . . . . . . . . . . 128

4 mining data streams

131
4.1 the stream data model . . . . . . . . . . . . . . . . . . . . . . . 131
4.1.1 a data-stream-management system . . . . . . . . . . . . 132
4.1.2 examples of stream sources . . . . . . . . . . . . . . . . . 133
stream queries . . . . . . . . . . . . . . . . . . . . . . . . 134
4.1.3
4.1.4
issues in stream processing . . . . . . . . . . . . . . . . . 135
4.2 sampling data in a stream . . . . . . . . . . . . . . . . . . . . . 136
4.2.1 a motivating example . . . . . . . . . . . . . . . . . . . . 136
4.2.2 obtaining a representative sample . . . . . . . . . . . . . 137
4.2.3 the general sampling problem . . . . . . . . . . . . . . . 137
4.2.4 varying the sample size . . . . . . . . . . . . . . . . . . . 138
4.2.5 exercises for section 4.2 . . . . . . . . . . . . . . . . . . . 138
4.3 filtering streams . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
4.3.1 a motivating example . . . . . . . . . . . . . . . . . . . . 139
4.3.2 the bloom filter . . . . . . . . . . . . . . . . . . . . . . . 140
4.3.3 analysis of bloom filtering . . . . . . . . . . . . . . . . . 140
4.3.4 exercises for section 4.3 . . . . . . . . . . . . . . . . . . . 141
4.4 counting distinct elements in a stream . . . . . . . . . . . . . . 142
4.4.1 the count-distinct problem . . . . . . . . . . . . . . . . 142
4.4.2 the flajolet-martin algorithm . . . . . . . . . . . . . . . 143
4.4.3 combining estimates
. . . . . . . . . . . . . . . . . . . . 144
4.4.4
space requirements . . . . . . . . . . . . . . . . . . . . . 144
4.4.5 exercises for section 4.4 . . . . . . . . . . . . . . . . . . . 145
4.5 estimating moments . . . . . . . . . . . . . . . . . . . . . . . . . 145
4.5.1 de   nition of moments . . . . . . . . . . . . . . . . . . . . 145
4.5.2 the alon-matias-szegedy algorithm for second

moments

. . . . . . . . . . . . . . . . . . . . . . . . . . . 146
4.5.3 why the alon-matias-szegedy algorithm works . . . . . 147
4.5.4 higher-order moments
. . . . . . . . . . . . . . . . . . . 148
4.5.5 dealing with in   nite streams . . . . . . . . . . . . . . . . 148
4.5.6 exercises for section 4.5 . . . . . . . . . . . . . . . . . . . 149
4.6 counting ones in a window . . . . . . . . . . . . . . . . . . . . . 150
4.6.1 the cost of exact counts . . . . . . . . . . . . . . . . . . 151
4.6.2 the datar-gionis-indyk-motwani algorithm . . . . . . . 151
4.6.3
storage requirements for the dgim algorithm . . . . . . 153

contents

xi

4.6.4 query answering in the dgim algorithm . . . . . . . . . 153
4.6.5 maintaining the dgim conditions . . . . . . . . . . . . . 154
4.6.6 reducing the error . . . . . . . . . . . . . . . . . . . . . . 155
4.6.7 extensions to the counting of ones
. . . . . . . . . . . . 156
4.6.8 exercises for section 4.6 . . . . . . . . . . . . . . . . . . . 157
4.7 decaying windows . . . . . . . . . . . . . . . . . . . . . . . . . . 157
4.7.1 the problem of most-common elements
. . . . . . . . . 157
4.7.2 de   nition of the decaying window . . . . . . . . . . . . . 158
. . . . . . . . . . . . 159
4.7.3 finding the most popular elements
4.8 summary of chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . 160
4.9 references for chapter 4 . . . . . . . . . . . . . . . . . . . . . . . 161

5 link analysis

163
5.1 id95 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
5.1.1 early search engines and term spam . . . . . . . . . . . 164
5.1.2 de   nition of id95 . . . . . . . . . . . . . . . . . . . 165
5.1.3
structure of the web . . . . . . . . . . . . . . . . . . . . . 169
5.1.4 avoiding dead ends . . . . . . . . . . . . . . . . . . . . . 170
5.1.5
spider traps and taxation . . . . . . . . . . . . . . . . . 173
5.1.6 using id95 in a search engine . . . . . . . . . . . . 175
5.1.7 exercises for section 5.1 . . . . . . . . . . . . . . . . . . . 175
5.2 e   cient computation of id95 . . . . . . . . . . . . . . . . . 177
5.2.1 representing transition matrices . . . . . . . . . . . . . . 178
5.2.2 id95 iteration using mapreduce . . . . . . . . . . . 179
5.2.3 use of combiners to consolidate the result vector . . . . 179
5.2.4 representing blocks of the transition matrix . . . . . . . 180
5.2.5 other e   cient approaches to id95 iteration . . . . 181
5.2.6 exercises for section 5.2 . . . . . . . . . . . . . . . . . . . 183
5.3 topic-sensitive id95 . . . . . . . . . . . . . . . . . . . . . . 183
5.3.1 motivation for topic-sensitive page rank . . . . . . . . . 183
5.3.2 biased id93 . . . . . . . . . . . . . . . . . . . . 184
5.3.3 using topic-sensitive id95 . . . . . . . . . . . . . . 185
5.3.4
inferring topics from words . . . . . . . . . . . . . . . . . 186
5.3.5 exercises for section 5.3 . . . . . . . . . . . . . . . . . . . 187
5.4 link spam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
5.4.1 architecture of a spam farm . . . . . . . . . . . . . . . . 187
5.4.2 analysis of a spam farm . . . . . . . . . . . . . . . . . . 189
5.4.3 combating link spam . . . . . . . . . . . . . . . . . . . . 190
5.4.4 trustrank . . . . . . . . . . . . . . . . . . . . . . . . . . 190
5.4.5
spam mass . . . . . . . . . . . . . . . . . . . . . . . . . . 191
5.4.6 exercises for section 5.4 . . . . . . . . . . . . . . . . . . . 191
. . . . . . . . . . . . . . . . . . . . . . . . 192
5.5.1 the intuition behind hits . . . . . . . . . . . . . . . . . 192
5.5.2 formalizing hubbiness and authority . . . . . . . . . . . 193
5.5.3 exercises for section 5.5 . . . . . . . . . . . . . . . . . . . 196

5.5 hubs and authorities

xii

contents

5.6 summary of chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . 196
5.7 references for chapter 5 . . . . . . . . . . . . . . . . . . . . . . . 200

6 frequent itemsets

201
6.1 the market-basket model . . . . . . . . . . . . . . . . . . . . . . 202
6.1.1 de   nition of frequent itemsets . . . . . . . . . . . . . . . 202
6.1.2 applications of frequent itemsets
. . . . . . . . . . . . . 204
6.1.3 association rules . . . . . . . . . . . . . . . . . . . . . . . 205
6.1.4 finding association rules with high con   dence . . . . . 207
6.1.5 exercises for section 6.1 . . . . . . . . . . . . . . . . . . . 207
6.2 market baskets and the a-priori algorithm . . . . . . . . . . . . 209
6.2.1 representation of market-basket data . . . . . . . . . . . 209
6.2.2 use of main memory for itemset counting . . . . . . . . 210
. . . . . . . . . . . . . . . . . . 212
6.2.3 monotonicity of itemsets
6.2.4 tyranny of counting pairs
. . . . . . . . . . . . . . . . . 213
6.2.5 the a-priori algorithm . . . . . . . . . . . . . . . . . . . 213
6.2.6 a-priori for all frequent itemsets
. . . . . . . . . . . . . 214
6.2.7 exercises for section 6.2 . . . . . . . . . . . . . . . . . . . 217
6.3 handling larger datasets in main memory . . . . . . . . . . . . 218
6.3.1 the algorithm of park, chen, and yu . . . . . . . . . . . 218
6.3.2 the multistage algorithm . . . . . . . . . . . . . . . . . . 220
6.3.3 the multihash algorithm . . . . . . . . . . . . . . . . . . 222
6.3.4 exercises for section 6.3 . . . . . . . . . . . . . . . . . . . 224
6.4 limited-pass algorithms . . . . . . . . . . . . . . . . . . . . . . . 226
6.4.1 the simple, randomized algorithm . . . . . . . . . . . . 226
6.4.2 avoiding errors in sampling algorithms . . . . . . . . . . 227
6.4.3 the algorithm of savasere, omiecinski, and

navathe . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.4.4 the son algorithm and mapreduce
. . . . . . . . . . . 229
6.4.5 toivonen   s algorithm . . . . . . . . . . . . . . . . . . . . 230
6.4.6 why toivonen   s algorithm works
. . . . . . . . . . . . . 231
6.4.7 exercises for section 6.4 . . . . . . . . . . . . . . . . . . . 232
6.5 counting frequent items in a stream . . . . . . . . . . . . . . . . 232
6.5.1
sampling methods for streams . . . . . . . . . . . . . . . 233
6.5.2 frequent itemsets in decaying windows . . . . . . . . . . 234
6.5.3 hybrid methods
. . . . . . . . . . . . . . . . . . . . . . . 235
6.5.4 exercises for section 6.5 . . . . . . . . . . . . . . . . . . . 235
6.6 summary of chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . 236
6.7 references for chapter 6 . . . . . . . . . . . . . . . . . . . . . . . 238

7 id91

7.1

241
introduction to id91 techniques . . . . . . . . . . . . . . . 241
7.1.1 points, spaces, and distances . . . . . . . . . . . . . . . . 241
7.1.2 id91 strategies . . . . . . . . . . . . . . . . . . . . . 243
7.1.3 the curse of dimensionality . . . . . . . . . . . . . . . . 244

contents

xiii

7.1.4 exercises for section 7.1 . . . . . . . . . . . . . . . . . . . 245
7.2 hierarchical id91 . . . . . . . . . . . . . . . . . . . . . . . . 245
7.2.1 hierarchical id91 in a euclidean space
. . . . . . . 246
7.2.2 e   ciency of hierarchical id91 . . . . . . . . . . . . 248
7.2.3 alternative rules for controlling hierarchical

7.5 id91 in non-euclidean spaces

id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
7.2.4 hierarchical id91 in non-euclidean spaces
. . . . . 252
7.2.5 exercises for section 7.2 . . . . . . . . . . . . . . . . . . . 253
7.3 id116 algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 254
7.3.1 id116 basics . . . . . . . . . . . . . . . . . . . . . . . . 255
7.3.2
initializing clusters for id116 . . . . . . . . . . . . . . 255
7.3.3 picking the right value of k . . . . . . . . . . . . . . . . 256
7.3.4 the algorithm of bradley, fayyad, and reina . . . . . . . 257
7.3.5 processing data in the bfr algorithm . . . . . . . . . . 259
7.3.6 exercises for section 7.3 . . . . . . . . . . . . . . . . . . . 262
7.4 the cure algorithm . . . . . . . . . . . . . . . . . . . . . . . . 262
7.4.1
initialization in cure . . . . . . . . . . . . . . . . . . . . 263
7.4.2 completion of the cure algorithm . . . . . . . . . . . . 264
7.4.3 exercises for section 7.4 . . . . . . . . . . . . . . . . . . . 265
. . . . . . . . . . . . . . . . 266
7.5.1 representing clusters in the grgpf algorithm . . . . . 266
7.5.2
initializing the cluster tree . . . . . . . . . . . . . . . . . 267
7.5.3 adding points in the grgpf algorithm . . . . . . . . . 268
7.5.4
splitting and merging clusters . . . . . . . . . . . . . . . 269
7.5.5 exercises for section 7.5 . . . . . . . . . . . . . . . . . . . 270
7.6 id91 for streams and parallelism . . . . . . . . . . . . . . . 270
7.6.1 the stream-computing model
. . . . . . . . . . . . . . . 271
7.6.2 a stream-id91 algorithm . . . . . . . . . . . . . . . 271
7.6.3
initializing buckets . . . . . . . . . . . . . . . . . . . . . . 272
7.6.4 merging buckets . . . . . . . . . . . . . . . . . . . . . . . 272
7.6.5 answering queries . . . . . . . . . . . . . . . . . . . . . . 275
7.6.6 id91 in a parallel environment . . . . . . . . . . . . 275
7.6.7 exercises for section 7.6 . . . . . . . . . . . . . . . . . . . 276
7.7 summary of chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . 276
7.8 references for chapter 7 . . . . . . . . . . . . . . . . . . . . . . . 280

8 advertising on the web

8.1

281
issues in on-line advertising . . . . . . . . . . . . . . . . . . . . 281
8.1.1 advertising opportunities . . . . . . . . . . . . . . . . . . 281
8.1.2 direct placement of ads . . . . . . . . . . . . . . . . . . . 282
8.1.3
issues for display ads . . . . . . . . . . . . . . . . . . . . 283
. . . . . . . . . . . . . . . . . . . . . . . . . 284
8.2.1 on-line and o   -line algorithms . . . . . . . . . . . . . . 284
8.2.2 id192 . . . . . . . . . . . . . . . . . . . . . . 285
8.2.3 the competitive ratio . . . . . . . . . . . . . . . . . . . 286

8.2 on-line algorithms

xiv

contents

8.2.4 exercises for section 8.2 . . . . . . . . . . . . . . . . . . . 286
8.3 the matching problem . . . . . . . . . . . . . . . . . . . . . . . . 287
8.3.1 matches and perfect matches . . . . . . . . . . . . . . . . 287
8.3.2 the greedy algorithm for maximal matching . . . . . . . 288
8.3.3 competitive ratio for greedy matching . . . . . . . . . . 289
8.3.4 exercises for section 8.3 . . . . . . . . . . . . . . . . . . . 290
8.4 the adwords problem . . . . . . . . . . . . . . . . . . . . . . . . 290
8.4.1 history of search advertising . . . . . . . . . . . . . . . . 291
8.4.2 de   nition of the adwords problem . . . . . . . . . . . . . 291
8.4.3 the greedy approach to the adwords problem . . . . . . 292
8.4.4 the balance algorithm . . . . . . . . . . . . . . . . . . . 293
8.4.5 a lower bound on competitive ratio for balance . . . . 294
8.4.6 the balance algorithm with many bidders . . . . . . . . 296
8.4.7 the generalized balance algorithm . . . . . . . . . . . . 297
8.4.8 final observations about the adwords problem . . . . . 298
8.4.9 exercises for section 8.4 . . . . . . . . . . . . . . . . . . . 299
8.5 adwords implementation . . . . . . . . . . . . . . . . . . . . . . 299
8.5.1 matching bids and search queries . . . . . . . . . . . . . 300
8.5.2 more complex matching problems . . . . . . . . . . . . . 300
8.5.3 a matching algorithm for documents and bids . . . . . . 301
8.6 summary of chapter 8 . . . . . . . . . . . . . . . . . . . . . . . . 303
8.7 references for chapter 8 . . . . . . . . . . . . . . . . . . . . . . . 305

9 id126s

item pro   les

307
9.1 a model for id126s . . . . . . . . . . . . . . . 307
9.1.1 the utility matrix . . . . . . . . . . . . . . . . . . . . . . 308
9.1.2 the long tail
. . . . . . . . . . . . . . . . . . . . . . . . 309
9.1.3 applications of id126s . . . . . . . . . 309
9.1.4 populating the utility matrix . . . . . . . . . . . . . . . . 311
9.2 content-based recommendations . . . . . . . . . . . . . . . . . . 312
9.2.1
. . . . . . . . . . . . . . . . . . . . . . . . . 312
9.2.2 discovering features of documents . . . . . . . . . . . . . 313
9.2.3 obtaining item features from tags
. . . . . . . . . . . . 314
9.2.4 representing item pro   les . . . . . . . . . . . . . . . . . . 315
9.2.5 user pro   les
. . . . . . . . . . . . . . . . . . . . . . . . . 316
9.2.6 recommending items to users based on content . . . . . 317
9.2.7 classi   cation algorithms
. . . . . . . . . . . . . . . . . . 318
9.2.8 exercises for section 9.2 . . . . . . . . . . . . . . . . . . . 320
9.3 id185 . . . . . . . . . . . . . . . . . . . . . . . . 321
9.3.1 measuring similarity . . . . . . . . . . . . . . . . . . . . . 322
9.3.2 the duality of similarity . . . . . . . . . . . . . . . . . . 324
9.3.3 id91 users and items . . . . . . . . . . . . . . . . . 325
9.3.4 exercises for section 9.3 . . . . . . . . . . . . . . . . . . . 327
9.4 id84 . . . . . . . . . . . . . . . . . . . . . . 328
9.4.1 uv-decomposition . . . . . . . . . . . . . . . . . . . . . . 328

contents

xv

. . . . . . . . . . . . . . . . . . 329
9.4.2 root-mean-square error
9.4.3
incremental computation of a uv-decomposition . . . . 330
9.4.4 optimizing an arbitrary element . . . . . . . . . . . . . . 332
9.4.5 building a complete uv-decomposition algorithm . . . . 334
9.4.6 exercises for section 9.4 . . . . . . . . . . . . . . . . . . . 336
9.5 the net   ix challenge . . . . . . . . . . . . . . . . . . . . . . . . 337
9.6 summary of chapter 9 . . . . . . . . . . . . . . . . . . . . . . . . 338
9.7 references for chapter 9 . . . . . . . . . . . . . . . . . . . . . . . 340

10 mining social-network graphs

343
10.1 social networks as graphs . . . . . . . . . . . . . . . . . . . . . . 343
. . . . . . . . . . . . . . . . . 344
10.1.1 what is a social network?
10.1.2 social networks as graphs
. . . . . . . . . . . . . . . . . 344
10.1.3 varieties of social networks . . . . . . . . . . . . . . . . . 346
10.1.4 graphs with several node types
. . . . . . . . . . . . . 347
10.1.5 exercises for section 10.1 . . . . . . . . . . . . . . . . . . 348
10.2 id91 of social-network graphs . . . . . . . . . . . . . . . . 349
10.2.1 distance measures for social-network graphs . . . . . . . 349
10.2.2 applying standard id91 methods
. . . . . . . . . . 349
10.2.3 betweenness . . . . . . . . . . . . . . . . . . . . . . . . . . 351
10.2.4 the girvan-newman algorithm . . . . . . . . . . . . . . . 351
10.2.5 using betweenness to find communities
. . . . . . . . . 354
10.2.6 exercises for section 10.2 . . . . . . . . . . . . . . . . . . 356
10.3 direct discovery of communities . . . . . . . . . . . . . . . . . . 357
10.3.1 finding cliques . . . . . . . . . . . . . . . . . . . . . . . . 357
10.3.2 complete bipartite graphs . . . . . . . . . . . . . . . . . 357
10.3.3 finding complete bipartite subgraphs . . . . . . . . . . . 358
10.3.4 why complete bipartite graphs must exist
. . . . . . . 359
10.3.5 exercises for section 10.3 . . . . . . . . . . . . . . . . . . 361
10.4 partitioning of graphs . . . . . . . . . . . . . . . . . . . . . . . . 361
10.4.1 what makes a good partition? . . . . . . . . . . . . . . . 362
10.4.2 normalized cuts . . . . . . . . . . . . . . . . . . . . . . . 362
10.4.3 some matrices that describe graphs
. . . . . . . . . . . 363
10.4.4 eigenvalues of the laplacian matrix . . . . . . . . . . . . 364
10.4.5 alternative partitioning methods . . . . . . . . . . . . . . 367
10.4.6 exercises for section 10.4 . . . . . . . . . . . . . . . . . . 368
10.5 finding overlapping communities . . . . . . . . . . . . . . . . . 369
10.5.1 the nature of communities . . . . . . . . . . . . . . . . . 369
10.5.2 maximum-likelihood estimation . . . . . . . . . . . . . . 369
10.5.3 the a   liation-graph model
. . . . . . . . . . . . . . . . 371
10.5.4 avoiding the use of discrete membership changes . . . . 374
10.5.5 exercises for section 10.5 . . . . . . . . . . . . . . . . . . 375
10.6 simrank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
10.6.1 random walkers on a social graph . . . . . . . . . . . . 376
10.6.2 id93 with restart
. . . . . . . . . . . . . . . . 377

xvi

contents

10.6.3 exercises for section 10.6 . . . . . . . . . . . . . . . . . . 380
10.7 counting triangles . . . . . . . . . . . . . . . . . . . . . . . . . . 380
10.7.1 why count triangles? . . . . . . . . . . . . . . . . . . . . 380
10.7.2 an algorithm for finding triangles
. . . . . . . . . . . . 381
10.7.3 optimality of the triangle-finding algorithm . . . . . . . 382
10.7.4 finding triangles using mapreduce . . . . . . . . . . . . 383
10.7.5 using fewer reduce tasks . . . . . . . . . . . . . . . . . . 384
10.7.6 exercises for section 10.7 . . . . . . . . . . . . . . . . . . 385
10.8 neighborhood properties of graphs . . . . . . . . . . . . . . . . . 386
10.8.1 directed graphs and neighborhoods . . . . . . . . . . . . 386
10.8.2 the diameter of a graph . . . . . . . . . . . . . . . . . . 388
10.8.3 transitive closure and reachability . . . . . . . . . . . . 389
10.8.4 transitive closure via mapreduce . . . . . . . . . . . . . 390
10.8.5 smart transitive closure
. . . . . . . . . . . . . . . . . . 392
10.8.6 transitive closure by graph reduction . . . . . . . . . . 393
10.8.7 approximating the sizes of neighborhoods
. . . . . . . . 395
10.8.8 exercises for section 10.8 . . . . . . . . . . . . . . . . . . 397
10.9 summary of chapter 10 . . . . . . . . . . . . . . . . . . . . . . . 398
10.10references for chapter 10 . . . . . . . . . . . . . . . . . . . . . . 402

11 id84

11.2 principal-component analysis

405
11.1 eigenvalues and eigenvectors of symmetric matrices . . . . . . . 406
. . . . . . . . . . . . . . . . . . . . . . . . . . 406
11.1.1 de   nitions
11.1.2 computing eigenvalues and eigenvectors
. . . . . . . . . 407
11.1.3 finding eigenpairs by power iteration . . . . . . . . . . . 408
11.1.4 the matrix of eigenvectors . . . . . . . . . . . . . . . . . 411
11.1.5 exercises for section 11.1 . . . . . . . . . . . . . . . . . . 411
. . . . . . . . . . . . . . . . . . . 412
11.2.1 an illustrative example . . . . . . . . . . . . . . . . . . . 413
11.2.2 using eigenvectors for id84 . . . . . 416
11.2.3 the matrix of distances . . . . . . . . . . . . . . . . . . . 417
11.2.4 exercises for section 11.2 . . . . . . . . . . . . . . . . . . 418
11.3 id166 . . . . . . . . . . . . . . . . . . . . 418
11.3.1 de   nition of svd . . . . . . . . . . . . . . . . . . . . . . 418
11.3.2 interpretation of svd . . . . . . . . . . . . . . . . . . . . 420
11.3.3 id84 using svd . . . . . . . . . . . 422
11.3.4 why zeroing low singular values works
. . . . . . . . . 423
11.3.5 querying using concepts . . . . . . . . . . . . . . . . . . 425
11.3.6 computing the svd of a matrix . . . . . . . . . . . . . . 426
11.3.7 exercises for section 11.3 . . . . . . . . . . . . . . . . . . 427
11.4 cur decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 428
11.4.1 de   nition of cur . . . . . . . . . . . . . . . . . . . . . . 429
11.4.2 choosing rows and columns properly . . . . . . . . . . . 430
11.4.3 constructing the middle matrix . . . . . . . . . . . . . . 431
11.4.4 the complete cur decomposition . . . . . . . . . . . . 432

contents

xvii

11.4.5 eliminating duplicate rows and columns . . . . . . . . . 433
11.4.6 exercises for section 11.4 . . . . . . . . . . . . . . . . . . 434
11.5 summary of chapter 11 . . . . . . . . . . . . . . . . . . . . . . . 434
11.6 references for chapter 11 . . . . . . . . . . . . . . . . . . . . . . 436

12 large-scale machine learning
12.1 the machine-learning model

12.3 support-vector machines

439
. . . . . . . . . . . . . . . . . . . . 440
12.1.1 training sets . . . . . . . . . . . . . . . . . . . . . . . . . 440
12.1.2 some illustrative examples . . . . . . . . . . . . . . . . . 440
12.1.3 approaches to machine learning . . . . . . . . . . . . . . 443
12.1.4 machine-learning architecture . . . . . . . . . . . . . . . 444
12.1.5 exercises for section 12.1 . . . . . . . . . . . . . . . . . . 447
12.2 id88s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
12.2.1 training a id88 with zero threshold . . . . . . . . 447
12.2.2 convergence of id88s . . . . . . . . . . . . . . . . . 451
12.2.3 the winnow algorithm . . . . . . . . . . . . . . . . . . . 451
12.2.4 allowing the threshold to vary . . . . . . . . . . . . . . . 453
12.2.5 multiclass id88s . . . . . . . . . . . . . . . . . . . . 455
12.2.6 transforming the training set
. . . . . . . . . . . . . . . 456
12.2.7 problems with id88s . . . . . . . . . . . . . . . . . 457
12.2.8 parallel implementation of id88s
. . . . . . . . . . 458
12.2.9 exercises for section 12.2 . . . . . . . . . . . . . . . . . . 459
. . . . . . . . . . . . . . . . . . . . . . 461
12.3.1 the mechanics of an id166 . . . . . . . . . . . . . . . . . . 461
12.3.2 normalizing the hyperplane . . . . . . . . . . . . . . . . . 462
12.3.3 finding optimal approximate separators . . . . . . . . . 464
12.3.4 id166 solutions by id119 . . . . . . . . . . . . 467
12.3.5 stochastic id119 . . . . . . . . . . . . . . . . . 470
12.3.6 parallel implementation of id166 . . . . . . . . . . . . . . 471
12.3.7 exercises for section 12.3 . . . . . . . . . . . . . . . . . . 472
12.4 learning from nearest neighbors . . . . . . . . . . . . . . . . . . 472
12.4.1 the framework for nearest-neighbor calculations . . . . 473
12.4.2 learning with one nearest neighbor . . . . . . . . . . . . 473
12.4.3 learning one-dimensional functions . . . . . . . . . . . . 474
12.4.4 kernel regression . . . . . . . . . . . . . . . . . . . . . . 477
12.4.5 dealing with high-dimensional euclidean data . . . . . . 477
12.4.6 dealing with non-euclidean distances . . . . . . . . . . . 479
12.4.7 exercises for section 12.4 . . . . . . . . . . . . . . . . . . 479
12.5 comparison of learning methods . . . . . . . . . . . . . . . . . . 480
12.6 summary of chapter 12 . . . . . . . . . . . . . . . . . . . . . . . 481
12.7 references for chapter 12 . . . . . . . . . . . . . . . . . . . . . . 483

xviii

contents

chapter 1

data mining

in this intoductory chapter we begin with the essence of data mining and a dis-
cussion of how data mining is treated by the various disciplines that contribute
to this    eld. we cover    bonferroni   s principle,    which is really a warning about
overusing the ability to mine data. this chapter is also the place where we
summarize a few useful ideas that are not data mining but are useful in un-
derstanding some important data-mining concepts. these include the tf.idf
measure of word importance, behavior of hash functions and indexes, and iden-
tities involving e, the base of natural logarithms. finally, we give an outline of
the topics covered in the balance of the book.

1.1 what is data mining?

the most commonly accepted de   nition of    data mining    is the discovery of
   models    for data. a    model,    however, can be one of several things. we
mention below the most important directions in modeling.

1.1.1 statistical modeling

statisticians were the    rst to use the term    data mining.    originally,    data
mining    or    data dredging    was a derogatory term referring to attempts to
extract information that was not supported by the data. section 1.2 illustrates
the sort of errors one can make by trying to extract what really isn   t in the data.
today,    data mining    has taken on a positive meaning. now, statisticians view
data mining as the construction of a statistical model, that is, an underlying
distribution from which the visible data is drawn.

example 1.1 : suppose our data is a set of numbers. this data is much
simpler than data that would be data-mined, but it will serve as an example. a
statistician might decide that the data comes from a gaussian distribution and
use a formula to compute the most likely parameters of this gaussian. the mean

1

2

chapter 1. data mining

and standard deviation of this gaussian distribution completely characterize the
distribution and would become the model of the data.    

1.1.2 machine learning

there are some who regard data mining as synonymous with machine learning.
there is no question that some data mining appropriately uses algorithms from
machine learning. machine-learning practitioners use the data as a training set,
to train an algorithm of one of the many types used by machine-learning prac-
titioners, such as bayes nets, support-vector machines, id90, hidden
markov models, and many others.

there are situations where using data in this way makes sense. the typical
case where machine learning is a good approach is when we have little idea of
what we are looking for in the data. for example, it is rather unclear what
it is about movies that makes certain movie-goers like or dislike it. thus,
in answering the    net   ix challenge    to devise an algorithm that predicts the
ratings of movies by users, based on a sample of their responses, machine-
learning algorithms have proved quite successful. we shall discuss a simple
form of this type of algorithm in section 9.4.

on the other hand, machine learning has not proved successful in situations
where we can describe the goals of the mining more directly. an interesting
case in point is the attempt by whizbang! labs1 to use machine learning to
locate people   s resumes on the web. it was not able to do better than algorithms
designed by hand to look for some of the obvious words and phrases that appear
in the typical resume. since everyone who has looked at or written a resume has
a pretty good idea of what resumes contain, there was no mystery about what
makes a web page a resume. thus, there was no advantage to machine-learning
over the direct design of an algorithm to discover resumes.

1.1.3 computational approaches to modeling

more recently, computer scientists have looked at data mining as an algorithmic
problem. in this case, the model of the data is simply the answer to a complex
query about it. for instance, given the set of numbers of example 1.1, we might
compute their average and standard deviation. note that these values might
not be the parameters of the gaussian that best    ts the data, although they
will almost certainly be very close if the size of the data is large.

there are many di   erent approaches to modeling data. we have already
mentioned the possibility of constructing a statistical process whereby the data
could have been generated. most other approaches to modeling can be described
as either

1. summarizing the data succinctly and approximately, or

1this startup attempted to use machine learning to mine large-scale data, and hired many

of the top machine-learning people to do so. unfortunately, it was not able to survive.

1.1. what is data mining?

3

2. extracting the most prominent features of the data and ignoring the rest.

we shall explore these two approaches in the following sections.

1.1.4 summarization

one of the most interesting forms of summarization is the id95 idea, which
made google successful and which we shall cover in chapter 5. in this form
of web mining, the entire complex structure of the web is summarized by a
single number for each page. this number, the    id95    of the page, is
(oversimplifying somewhat) the id203 that a random walker on the graph
would be at that page at any given time. the remarkable property this ranking
has is that it re   ects very well the    importance    of the page     the degree to
which typical searchers would like that page returned as an answer to their
search query.

another important form of summary     id91     will be covered in chap-
ter 7. here, data is viewed as points in a multidimensional space. points
that are    close    in this space are assigned to the same cluster. the clusters
themselves are summarized, perhaps by giving the centroid of the cluster and
the average distance from the centroid of points in the cluster. these cluster
summaries become the summary of the entire data set.

example 1.2 : a famous instance of id91 to solve a problem took place
long ago in london, and it was done entirely without computers.2 the physician
john snow, dealing with a cholera outbreak plotted the cases on a map of the
city. a small illustration suggesting the process is shown in fig. 1.1.

figure 1.1: plotting cholera cases on a map of london

2see http://en.wikipedia.org/wiki/1854 broad street cholera outbreak.

4

chapter 1. data mining

the cases clustered around some of the intersections of roads. these inter-
sections were the locations of wells that had become contaminated; people who
lived nearest these wells got sick, while people who lived nearer to wells that
had not been contaminated did not get sick. without the ability to cluster the
data, the cause of cholera would not have been discovered.    

1.1.5 feature extraction

the typical feature-based model looks for the most extreme examples of a phe-
nomenon and represents the data by these examples. if you are familiar with
bayes nets, a branch of machine learning and a topic we do not cover in this
book, you know how a complex relationship between objects is represented by
   nding the strongest statistical dependencies among these objects and using
only those in representing all statistical connections. some of the important
kinds of feature extraction from large-scale data that we shall study are:

1. frequent itemsets. this model makes sense for data that consists of    bas-
kets    of small sets of items, as in the market-basket problem that we shall
discuss in chapter 6. we look for small sets of items that appear together
in many baskets, and these    frequent itemsets    are the characterization of
the data that we seek. the original application of this sort of mining was
true market baskets: the sets of items, such as hamburger and ketchup,
that people tend to buy together when checking out at the cash register
of a store or super market.

2. similar items. often, your data looks like a collection of sets, and the
objective is to    nd pairs of sets that have a relatively large fraction of
their elements in common. an example is treating customers at an on-
line store like amazon as the set of items they have bought.
in order
for amazon to recommend something else they might like, amazon can
look for    similar    customers and recommend something many of these
customers have bought. this process is called    collaborative    ltering.   
if customers were single-minded, that is, they bought only one kind of
thing, then id91 customers might work. however, since customers
tend to have interests in many di   erent things, it is more useful to    nd,
for each customer, a small number of other customers who are similar
in their tastes, and represent the data by these connections. we discuss
similarity in chapter 3.

1.2 statistical limits on data mining

a common sort of data-mining problem involves discovering unusual events
hidden within massive amounts of data. this section is a discussion of the
problem, including    bonferroni   s principle,    a warning against overzealous use
of data mining.

1.2. statistical limits on data mining

5

1.2.1 total information awareness

following the terrorist attack of sept. 11, 2001, it was noticed that there were
four people enrolled in di   erent    ight schools, learning how to pilot commercial
aircraft, although they were not a   liated with any airline. it was conjectured
that the information needed to predict and foil the attack was available in
data, but that there was then no way to examine the data and detect suspi-
cious events. the response was a program called tia, or total information
awareness, which was intended to mine all the data it could    nd, including
credit-card receipts, hotel records, travel data, and many other kinds of infor-
mation in order to track terrorist activity. tia naturally caused great concern
among privacy advocates, and the project was eventually killed by congress.
it is not the purpose of this book to discuss the di   cult issue of the privacy-
security tradeo   . however, the prospect of tia or a system like it does raise
many technical questions about its feasibility.

the concern raised by many is that if you look at so much data, and you try
to    nd within it activities that look like terrorist behavior, are you not going to
   nd many innocent activities     or even illicit activities that are not terrorism    
that will result in visits from the police and maybe worse than just a visit? the
answer is that it all depends on how narrowly you de   ne the activities that you
look for. statisticians have seen this problem in many guises and have a theory,
which we introduce in the next section.

1.2.2 bonferroni   s principle

suppose you have a certain amount of data, and you look for events of a cer-
tain type within that data. you can expect events of this type to occur, even if
the data is completely random, and the number of occurrences of these events
will grow as the size of the data grows. these occurrences are    bogus,    in the
sense that they have no cause other than that random data will always have
some number of unusual features that look signi   cant but aren   t. a theorem
of statistics, known as the bonferroni correction gives a statistically sound way
to avoid most of these bogus positive responses to a search through the data.
without going into the statistical details, we o   er an informal version, bon-
ferroni   s principle, that helps us avoid treating random occurrences as if they
were real. calculate the expected number of occurrences of the events you are
looking for, on the assumption that data is random. if this number is signi   -
cantly larger than the number of real instances you hope to    nd, then you must
expect almost anything you    nd to be bogus, i.e., a statistical artifact rather
than evidence of what you are looking for. this observation is the informal
statement of bonferroni   s principle.

in a situation like searching for terrorists, where we expect that there are
few terrorists operating at any one time, bonferroni   s principle says that we
may only detect terrorists by looking for events that are so rare that they are
unlikely to occur in random data. we shall give an extended example in the

6

next section.

chapter 1. data mining

1.2.3 an example of bonferroni   s principle

suppose there are believed to be some    evil-doers    out there, and we want
to detect them. suppose further that we have reason to believe that periodi-
cally, evil-doers gather at a hotel to plot their evil. let us make the following
assumptions about the size of the problem:

1. there are one billion people who might be evil-doers.

2. everyone goes to a hotel one day in 100.

3. a hotel holds 100 people. hence, there are 100,000 hotels     enough to

hold the 1% of a billion people who visit a hotel on any given day.

4. we shall examine hotel records for 1000 days.

to    nd evil-doers in this data, we shall look for people who, on two di   erent
days, were both at the same hotel. suppose, however, that there really are no
evil-doers. that is, everyone behaves at random, deciding with id203 0.01
to visit a hotel on any given day, and if so, choosing one of the 105 hotels at
random. would we    nd any pairs of people who appear to be evil-doers?

we can do a simple approximate calculation as follows. the id203 of
any two people both deciding to visit a hotel on any given day is .0001. the
chance that they will visit the same hotel is this id203 divided by 105,
the number of hotels. thus, the chance that they will visit the same hotel on
one given day is 10   9. the chance that they will visit the same hotel on two
di   erent given days is the square of this number, 10   18. note that the hotels
can be di   erent on the two days.

now, we must consider how many events will indicate evil-doing. an    event   
in this sense is a pair of people and a pair of days, such that the two people
were at the same hotel on each of the two days. to simplify the arithmetic, note

that for large n, (cid:0)n
follows. thus, the number of pairs of people is (cid:0)109
of pairs of days is (cid:0)1000

2(cid:1) is about n2/2. we shall use this approximation in what
2 (cid:1) = 5    1017. the number
2 (cid:1) = 5    105. the expected number of events that look

like evil-doing is the product of the number of pairs of people, the number of
pairs of days, and the id203 that any one pair of people and pair of days
is an instance of the behavior we are looking for. that number is

5    1017    5    105    10   18 = 250, 000

that is, there will be a quarter of a million pairs of people who look like evil-
doers, even though they are not.

now, suppose there really are 10 pairs of evil-doers out there. the police
will need to investigate a quarter of a million other pairs in order to    nd the real
evil-doers. in addition to the intrusion on the lives of half a million innocent

1.3. things useful to know

7

people, the work involved is su   ciently great that this approach to    nding
evil-doers is probably not feasible.

1.2.4 exercises for section 1.2

exercise 1.2.1 : using the information from section 1.2.3, what would be the
number of suspected pairs if the following changes were made to the data (and
all other numbers remained as they were in that section)?

(a) the number of days of observation was raised to 2000.

(b) the number of people observed was raised to 2 billion (and there were

therefore 200,000 hotels).

(c) we only reported a pair as suspect if they were at the same hotel at the

same time on three di   erent days.

! exercise 1.2.2 : suppose we have information about the supermarket pur-
chases of 100 million people. each person goes to the supermarket 100 times
in a year and buys 10 of the 1000 items that the supermarket sells. we believe
that a pair of terrorists will buy exactly the same set of 10 items (perhaps the
ingredients for a bomb?) at some time during the year. if we search for pairs of
people who have bought the same set of items, would we expect that any such
people found were truly terrorists?3

1.3 things useful to know

in this section, we o   er brief introductions to subjects that you may or may
not have seen in your study of other courses. each will be useful in the study
of data mining. they include:

1. the tf.idf measure of word importance.

2. hash functions and their use.

3. secondary storage (disk) and its e   ect on running time of algorithms.

4. the base e of natural logarithms and identities involving that constant.

5. power laws.

3that is, assume our hypothesis that terrorists will surely buy a set of 10 items in common
at some time during the year. we don   t want to address the matter of whether or not terrorists
would necessarily do so.

8

chapter 1. data mining

1.3.1

importance of words in documents

in several applications of data mining, we shall be faced with the problem of
categorizing documents (sequences of words) by their topic. typically, topics
are identi   ed by    nding the special words that characterize documents about
that topic. for instance, articles about baseball would tend to have many
occurrences of words like    ball,       bat,       pitch,   ,    run,    and so on. once we
have classi   ed documents to determine they are about baseball, it is not hard
to notice that words such as these appear unusually frequently. however, until
we have made the classi   cation, it is not possible to identify these words as
characteristic.

thus, classi   cation often starts by looking at documents, and    nding the
signi   cant words in those documents. our    rst guess might be that the words
appearing most frequently in a document are the most signi   cant. however,
that intuition is exactly opposite of the truth. the most frequent words will
most surely be the common words such as    the    or    and,    which help build
ideas but do not carry any signi   cance themselves. in fact, the several hundred
most common words in english (called stop words) are often removed from
documents before any attempt to classify them.

in fact, the indicators of the topic are relatively rare words. however, not
all rare words are equally useful as indicators. there are certain words, for
example    notwithstanding    or    albeit,    that appear rarely in a collection of
documents, yet do not tell us anything useful. on the other hand, a word like
   chukker    is probably equally rare, but tips us o    that the document is about
the sport of polo. the di   erence between rare words that tell us something and
those that do not has to do with the concentration of the useful words in just a
few documents. that is, the presence of a word like    albeit    in a document does
not make it terribly more likely that it will appear multiple times. however,
if an article mentions    chukker    once, it is likely to tell us what happened in
the       rst chukker,    then the    second chukker,    and so on. that is, the word is
likely to be repeated if it appears at all.

the formal measure of how concentrated into relatively few documents are
the occurrences of a given word is called tf.idf (term frequency times in-
verse document frequency). it is normally computed as follows. suppose we
have a collection of n documents. de   ne fij to be the frequency (number of
occurrences) of term (word) i in document j. then, de   ne the term frequency
tf ij to be:

tf ij =

fij

maxk fkj

that is, the term frequency of term i in document j is fij normalized by dividing
it by the maximum number of occurrences of any term (perhaps excluding stop
words) in the same document. thus, the most frequent term in document j
gets a tf of 1, and other terms get fractions as their term frequency for this
document.

the idf for a term is de   ned as follows. suppose term i appears in ni

1.3. things useful to know

9

of the n documents in the collection. then idf i = log2(n/ni). the tf.idf
score for term i in document j is then de   ned to be tf ij    idf i. the terms
with the highest tf.idf score are often the terms that best characterize the
topic of the document.

example 1.3 : suppose our repository consists of 220 = 1,048,576 documents.
suppose word w appears in 210 = 1024 of these documents. then idf w =
log2(220/210) = log 2(210) = 10. consider a document j in which w appears 20
times, and that is the maximum number of times in which any word appears
(perhaps after eliminating stop words). then tf wj = 1, and the tf.idf score
for w in document j is 10.

suppose that in document k, word w appears once, while the maximum
number of occurrences of any word in this document is 20. then tf wk = 1/20,
and the tf.idf score for w in document k is 1/2.    

1.3.2 hash functions

the reader has probably heard of hash tables, and perhaps used them in java
classes or similar packages. the hash functions that make hash tables feasible
are also essential components in a number of data-mining algorithms, where
the hash table takes an unfamiliar form. we shall review the basics here.

first, a hash function h takes a hash-key value as an argument and produces
a bucket number as a result. the bucket number is an integer, normally in the
range 0 to b     1, where b is the number of buckets. hash-keys can be of any
type. there is an intuitive property of hash functions that they    randomize   
hash-keys. to be precise, if hash-keys are drawn randomly from a reasonable
population of possible hash-keys, then h will send approximately equal numbers
of hash-keys to each of the b buckets. it would be impossible to do so if, for
example, the population of possible hash-keys were smaller than b. such a
population would not be    reasonable.    however, there can be more subtle rea-
sons why a hash function fails to achieve an approximately uniform distribution
into buckets.

example 1.4 : suppose hash-keys are positive integers. a common and simple
hash function is to pick h(x) = x mod b, that is, the remainder when x is
divided by b. that choice works    ne if our population of hash-keys is all
positive integers. 1/bth of the integers will be assigned to each of the buckets.
however, suppose our population is the even integers, and b = 10. then only
buckets 0, 2, 4, 6, and 8 can be the value of h(x), and the hash function is
distinctly nonrandom in its behavior. on the other hand, if we picked b = 11,
then we would    nd that 1/11th of the even integers get sent to each of the 11
buckets, so the hash function would work very well.    

the generalization of example 1.4 is that when hash-keys are integers, chos-
ing b so it has any common factor with all (or even most of) the possible hash-
keys will result in nonrandom distribution into buckets. thus, it is normally

10

chapter 1. data mining

preferred that we choose b to be a prime. that choice reduces the chance of
nonrandom behavior, although we still have to consider the possibility that all
hash-keys have b as a factor. of course there are many other types of hash
functions not based on modular arithmetic. we shall not try to summarize
the options here, but some sources of information will be mentioned in the
bibliographic notes.

what if hash-keys are not integers? in a sense, all data types have values
that are composed of bits, and sequences of bits can always be interpreted as in-
tegers. however, there are some simple rules that enable us to convert common
types to integers. for example, if hash-keys are strings, convert each character
to its ascii or unicode equivalent, which can be interpreted as a small inte-
ger. sum the integers before dividing by b. as long as b is smaller than the
typical sum of character codes for the population of strings, the distribution
into buckets will be relatively uniform. if b is larger, then we can partition the
characters of a string into groups of several characters each. treat the concate-
nation of the codes for the characters of a group as a single integer. sum the
integers associated with all the groups of a string, and divide by b as before.
for instance, if b is around a billion, or 230, then grouping characters four at
a time will give us 32-bit integers. the sum of several of these will distribute
fairly evenly into a billion buckets.

for more complex data types, we can extend the idea used for converting

strings to integers, recursively.

    for a type that is a record, each of whose components has its own type,
recursively convert the value of each component to an integer, using the
algorithm appropriate for the type of that component. sum the integers
for the components, and convert the integer sum to buckets by dividing
by b.

    for a type that is an array, set, or bag of elements of some one type,
convert the values of the elements    type to integers, sum the integers, and
divide by b.

1.3.3

indexes

an index is a data structure that makes it e   cient to retrieve objects given the
value of one or more elements of those objects. the most common situation
is one where the objects are records, and the index is on one of the    elds
of that record. given a value v for that    eld, the index lets us retrieve all
the records with value v in that    eld. for example, we could have a    le of
(name, address, phone) triples, and an index on the phone    eld. given a phone
number, the index allows us to    nd quickly the record or records with that
phone number.

there are many ways to implement indexes, and we shall not attempt to
survey the matter here. the bibliographic notes give suggestions for further
reading. however, a hash table is one simple way to build an index. the    eld

1.3. things useful to know

11

or    elds on which the index is based form the hash-key for a hash function.
records have the hash function applied to value of the hash-key, and the record
itself is placed in the bucket whose number is determined by the hash function.
the bucket could be a list of records in main-memory, or a disk block, for
example.

then, given a hash-key value, we can hash it,    nd the bucket, and need to
search only that bucket to    nd the records with that value for the hash-key. if
we choose the number of buckets b to be comparable to the number of records
in the    le, then there will be relatively few records in any bucket, and the search
of a bucket takes little time.

sally jones   maple st   800   555   1212

records with h(phone) = 17

h (800   555   1212)

0

17

.
.
.

.
.
.

   1b

array of
bucket
headers

figure 1.2: a hash table used as an index; phone numbers are hashed to buckets,
and the entire record is placed in the bucket whose number is the hash value of
the phone

example 1.5 : figure 1.2 suggests what a main-memory index of records with
name, address, and phone    elds might look like. here, the index is on the phone
   eld, and buckets are linked lists. we show the phone 800-555-1212 hashed to
bucket number 17. there is an array of bucket headers, whose ith element is
the head of a linked list for the bucket numbered i. we show expanded one of
the elements of the linked list. it contains a record with name, address, and
phone    elds. this record is in fact one with the phone number 800-555-1212.
other records in that bucket may or may not have this phone number. we only
know that whatever phone number they have is a phone that hashes to 17.    

1.3.4 secondary storage

it is important, when dealing with large-scale data, that we have a good un-
derstanding of the di   erence in time taken to perform computations when the
data is initially on disk, as opposed to the time needed if the data is initially in

12

chapter 1. data mining

main memory. the physical characteristics of disks is another subject on which
we could say much, but shall say only a little and leave the interested reader to
follow the bibliographic notes.

disks are organized into blocks, which are the minimum units that the oper-
ating system uses to move data between main memory and disk. for example,
the windows operating system uses blocks of 64k bytes (i.e., 216 = 65,536 bytes
to be exact). it takes approximately ten milliseconds to access (move the disk
head to the track of the block and wait for the block to rotate under the head)
and read a disk block. that delay is at least    ve orders of magnitude (a factor
of 105) slower than the time taken to read a word from main memory, so if all
we want to do is access a few bytes, there is an overwhelming bene   t to having
data in main memory. in fact, if we want to do something simple to every byte
of a disk block, e.g., treat the block as a bucket of a hash table and search for
a particular value of the hash-key among all the records in that bucket, then
the time taken to move the block from disk to main memory will be far larger
than the time taken to do the computation.

by organizing our data so that related data is on a single cylinder (the
collection of blocks reachable at a    xed radius from the center of the disk, and
therefore accessible without moving the disk head), we can read all the blocks
on the cylinder into main memory in considerably less than 10 milliseconds
per block. you can assume that a disk cannot transfer data to main memory
at more than a hundred million bytes per second, no matter how that data is
organized. that is not a problem when your dataset is a megabyte. but a
dataset of a hundred gigabytes or a terabyte presents problems just accessing
it, let alone doing anything useful with it.

1.3.5 the base of natural logarithms

the constant e = 2.7182818        has a number of useful special properties. in
particular, e is the limit of (1 + 1
x )x as x goes to in   nity. the values of this
expression for x = 1, 2, 3, 4 are approximately 2, 2.25, 2.37, 2.44, so you should
   nd it easy to believe that the limit of this series is around 2.72.

some algebra lets us obtain approximations to many seemingly complex
expressions. consider (1 + a)b, where a is small. we can rewrite the expression
as (1+a)(1/a)(ab). then substitute a = 1/x and 1/a = x, so we have (1+ 1
x )x(ab),
which is

(cid:16)(cid:0)1 +

1

x(cid:1)x(cid:17)ab

since a is assumed small, x is large, so the subexpression (1 + 1
to the limiting value of e. we can thus approximate (1 + a)b as eab.

x )x will be close

similar identities hold when a is negative. that is, the limit as x goes to
in   nity of (1     1
x )x is 1/e. it follows that the approximation (1 + a)b = eab
holds even when a is a small negative number. put another way, (1     a)b is
approximately e   ab when a is small and b is large.

1.3. things useful to know

13

that is, ex = p   

some other useful approximations follow from the taylor expansion of ex.
i=0 xi/i!, or ex = 1 + x + x2/2 + x3/6 + x4/24 +        . when
x is large, the above series converges slowly, although it does converge because
n! grows faster than xn for any constant x. however, when x is small, either
positive or negative, the series converges rapidly, and only a few terms are
necessary to get a good approximation.

example 1.6 : let x = 1/2. then

e1/2 = 1 +

1
2

+

1
8

+

1
48

+

1

384

+       

or approximately e1/2 = 1.64844.

let x =    1. then

e   1 = 1     1 +

1
2    

1
6

+

1
24    

1

120

+

1
720    

1

5040

+       

or approximately e   1 = 0.36786.    

1.3.6 power laws

there are many phenomena that relate two variables by a power law, that is, a
linear relationship between the logarithms of the variables. figure 1.3 suggests
such a relationship. if x is the horizontal axis and y is the vertical axis, then
the relationship is log10 y = 6     2 log10 x.

10,000,000

1,000,000

100,000

10,000

1000

100

10

1

1

10

100 1000 10,000

figure 1.3: a power law with a slope of    2

14

chapter 1. data mining

the matthew e   ect

often, the existence of power laws with values of the exponent higher than
1 are explained by the matthew e   ect. in the biblical book of matthew,
there is a verse about    the rich get richer.    many phenomena exhibit this
behavior, where getting a high value of some property causes that very
property to increase. for example, if a web page has many links in, then
people are more likely to    nd the page and may choose to link to it from
one of their pages as well. as another example, if a book is selling well
on amazon, then it is likely to be advertised when customers go to the
amazon site. some of these people will choose to buy the book as well,
thus increasing the sales of this book.

example 1.7 : we might examine book sales at amazon.com, and let x rep-
resent the rank of books by sales. then y is the number of sales of the xth
best-selling book over some period. the implication of the graph of fig. 1.3
would be that the best-selling book sold 1,000,000 copies, the 10th best-selling
book sold 10,000 copies, the 100th best-selling book sold 100 copies, and so on
for all ranks between these numbers and beyond. the implication that above
rank 1000 the sales are a fraction of a book is too extreme, and we would in
fact expect the line to    atten out for ranks much higher than 1000.    

the general form of a power law relating x and y is log y = b + a log x. if we
raise the base of the logarithm (which doesn   t actually matter), say e, to the
values on both sides of this equation, we get y = ebea log x = ebxa. since eb is
just    some constant,    let us replace it by constant c. thus, a power law can be
written as y = cxa for some constants a and c.

example 1.8 : in fig. 1.3 we see that when x = 1, y = 106, and when x =
1000, y = 1. making the    rst substitution, we see 106 = c. the second
substitution gives us 1 = c(1000)a. since we now know c = 106, the second
equation gives us 1 = 106(1000)a, from which we see a =    2. that is, the law
expressed by fig. 1.3 is y = 106x   2, or y = 106/x2.    

we shall meet in this book many ways that power laws govern phenomena.

here are some examples:

1. node degrees in the web graph: order all pages by the number of in-
links to that page. let x be the position of a page in this ordering, and
let y be the number of in-links to the xth page. then y as a function of x
looks very much like fig. 1.3. the exponent a is slightly larger than the
   2 shown there; it has been found closer to 2.1.

1.4. outline of the book

15

2. sales of products: order products, say books at amazon.com, by their
sales over the past year. let y be the number of sales of the xth most pop-
ular book. again, the function y(x) will look something like fig. 1.3. we
shall discuss the consequences of this distribution of sales in section 9.1.2,
where we take up the matter of the    long tail.   

3. sizes of web sites: count the number of pages at web sites, and order
sites by the number of their pages. let y be the number of pages at the
xth site. again, the function y(x) follows a power law.

4. zipf    s law : this power law originally referred to the frequency of words
in a collection of documents. if you order words by frequency, and let y
be the number of times the xth word in the order appears, then you get
a power law, although with a much shallower slope than that of fig. 1.3.
zipf   s observation was that y = cx   1/2. interestingly, a number of other
kinds of data follow this particular power law. for example, if we order
states in the us by population and let y be the population of the xth
most populous state, then x and y obey zipf   s law approximately.

1.3.7 exercises for section 1.3

exercise 1.3.1 : suppose there is a repository of ten million documents. what
(to the nearest integer) is the idf for a word that appears in (a) 40 documents
(b) 10,000 documents?

exercise 1.3.2 : suppose there is a repository of ten million documents, and
word w appears in 320 of them.
in a particular document d, the maximum
number of occurrences of a word is 15. approximately what is the tf.idf
score for w if that word appears (a) once (b)    ve times?

! exercise 1.3.3 : suppose hash-keys are drawn from the population of all non-
negative integers that are multiples of some constant c, and hash function h(x)
is x mod 15. for what values of c will h be a suitable hash function, i.e., a
large random choice of hash-keys will be divided roughly equally into buckets?

exercise 1.3.4 : in terms of e, give approximations to

(a) (1.01)500 (b) (1.05)1000 (c) (0.9)40

exercise 1.3.5 : use the taylor expansion of ex to compute, to three decimal
places: (a) e1/10 (b) e   1/10 (c) e2.

1.4 outline of the book

this section gives brief summaries of the remaining chapters of the book.

chapter 2 is not about data mining per se. rather, it introduces us to the
mapreduce methodology for exploiting parallelism in computing clouds (racks

16

chapter 1. data mining

of interconnected processors). there is reason to believe that cloud computing,
and mapreduce in particular, will become the normal way to compute when
analysis of very large amounts of data is involved. a pervasive issue in later
chapters will be the exploitation of the mapreduce methodology to implement
the algorithms we cover.

chapter 3 is about    nding similar items. our starting point is that items
can be represented by sets of elements, and similar sets are those that have a
large fraction of their elements in common. the key techniques of minhashing
and locality-sensitive hashing are explained. these techniques have numerous
applications and often give surprisingly e   cient solutions to problems that ap-
pear impossible for massive data sets.

in chapter 4, we consider data in the form of a stream. the di   erence
between a stream and a database is that the data in a stream is lost if you do
not do something about it immediately. important examples of streams are the
streams of search queries at a search engine or clicks at a popular web site. in
this chapter, we see several of the surprising applications of hashing that make
management of stream data feasible.

chapter 5 is devoted to a single application: the computation of id95.
this computation is the idea that made google stand out from other search
engines, and it is still an essential part of how search engines know what pages
the user is likely to want to see. extensions of id95 are also essential in the
   ght against spam (euphemistically called    search engine optimization   ), and
we shall examine the latest extensions of the idea for the purpose of combating
spam.

then, chapter 6 introduces the market-basket model of data, and its canon-
ical problems of association rules and    nding frequent itemsets. in the market-
basket model, data consists of a large collection of baskets, each of which con-
tains a small set of items. we give a sequence of algorithms capable of    nding
all frequent pairs of items, that is pairs of items that appear together in many
baskets. another sequence of algorithms are useful for    nding most of the
frequent itemsets larger than pairs, with high e   ciency.

chapter 7 examines the problem of id91. we assume a set of items
with a distance measure de   ning how close or far one item is from another.
the goal is to examine a large amount of data and partition it into subsets
(clusters), each cluster consisting of items that are all close to one another, yet
far from items in the other clusters.

chapter 8 is devoted to on-line advertising and the computational problems
it engenders. we introduce the notion of an on-line algorithm     one where a
good response must be given immediately, rather than waiting until we have
seen the entire dataset. the idea of competitive ratio is another important
concept covered in this chapter; it is the ratio of the guaranteed performance of
an on-line algorithm compared with the performance of the optimal algorithm
that is allowed to see all the data before making any decisions. these ideas are
used to give good algorithms that match bids by advertisers for the right to
display their ad in response to a query against the search queries arriving at a

1.5. summary of chapter 1

17

search engine.

chapter 9 is devoted to id126s. many web applications
involve advising users on what they might like. the net   ix challenge is one
example, where it is desired to predict what movies a user would like, or ama-
zon   s problem of pitching a product to a customer based on information about
what they might be interested in buying. there are two basic approaches to
recommendation. we can characterize items by features, e.g., the stars of a
movie, and recommend items with the same features as those the user is known
to like. or, we can look at other users with preferences similar to that of the
user in question, and see what they liked (a technique known as collaborative
   ltering).

in chapter 10, we study social networks and algorithms for their analysis.
the canonical example of a social network is the graph of facebook friends,
where the nodes are people, and edges connect two people if they are friends.
directed graphs, such as followers on twitter, can also be viewed as social
networks. a common example of a problem to be addressed is identifying
   communities,    that is, small sets of nodes with an unusually large number of
edges among them. other questions about social networks are general questions
about graphs, such as computing the transitive closure or diameter of a graph,
but are made more di   cult by the size of typical networks.

chapter 11 looks at id84. we are given a very large
matrix, typically sparse. think of the matrix as representing a relationship
between two kinds of entities, e.g., ratings of movies by viewers. intuitively,
there are a small number of concepts, many fewer concepts than there are
movies or viewers, that explain why certain viewers like certain movies. we
o   er several algorithms that simplify matrices by decomposing them into a
product of matrices that are much smaller in one of the two dimensions. one
matrix relates entities of one kind to the small number of concepts and another
relates the concepts to the other kind of entity. if done correctly, the product
of the smaller matrices will be very close to the original matrix.

finally, chapter 12 discusses algorithms for machine learning from very
large datasets. techniques covered include id88s, support-vector ma-
chines,    nding models by id119, nearest-neighbor models, and deci-
sion trees.

1.5 summary of chapter 1

    data mining: this term refers to the process of extracting useful models
of data. sometimes, a model can be a summary of the data, or it can be
the set of most extreme features of the data.

    bonferroni   s principle: if we are willing to view as an interesting fea-
ture of data something of which many instances can be expected to exist
in random data, then we cannot rely on such features being signi   cant.

18

chapter 1. data mining

this observation limits our ability to mine data for features that are not
su   ciently rare in practice.

    tf.idf : the measure called tf.idf lets us identify words in a collection
of documents that are useful for determining the topic of each document.
a word has high tf.idf score in a document if it appears in relatively few
documents, but appears in this one, and when it appears in a document
it tends to appear many times.

    hash functions: a hash function maps hash-keys of some data type to
integer bucket numbers. a good hash function distributes the possible
hash-key values approximately evenly among buckets. any data type can
be the domain of a hash function.

    indexes: an index is a data structure that allows us to store and retrieve
data records e   ciently, given the value in one or more of the    elds of the
record. hashing is one way to build an index.

    storage on disk : when data must be stored on disk (secondary memory),
it takes very much more time to access a desired data item than if the same
data were stored in main memory. when data is large, it is important
that algorithms strive to keep needed data in main memory.

    power laws: many phenomena obey a law that can be expressed as
y = cxa for some power a, often around    2. such phenomena include the
sales of the xth most popular book, or the number of in-links to the xth
most popular page.

1.6 references for chapter 1

[7] is a clear introduction to the basics of data mining. [2] covers data mining
principally from the point of view of machine learning and statistics.

for construction of hash functions and hash tables, see [4]. details of the
tf.idf measure and other matters regarding document processing can be
found in [5]. see [3] for more on managing indexes, hash tables, and data
on disk.

power laws pertaining to the web were explored by [1]. the matthew e   ect

was    rst observed in [6].

1. a. broder, r. kumar, f. maghoul, p. raghavan, s. rajagopalan, r.
stata, a. tomkins, and j. weiner,    graph structure in the web,    com-
puter networks 33:1   6, pp. 309   320, 2000.

2. m.m. gaber, scienti   c data mining and knowledge discovery     prin-

ciples and foundations, springer, new york, 2010.

1.6. references for chapter 1

19

3. h. garcia-molina, j.d. ullman, and j. widom, database systems: the
complete book second edition, prentice-hall, upper saddle river, nj,
2009.

4. d.e. knuth, the art of computer programming vol. 3 (sorting and
searching), second edition, addison-wesley, upper saddle river, nj,
1998.

5. c.p. manning, p. raghavan, and h. sch  utze, introduction to information

retrieval, cambridge univ. press, 2008.

6. r.k. merton,    the matthew e   ect in science,    science 159:3810, pp. 56   

63, jan. 5, 1968.

7. p.-n. tan, m. steinbach, and v. kumar, introduction to data mining,

addison-wesley, upper saddle river, nj, 2005.

20

chapter 1. data mining

chapter 2

mapreduce and the new
software stack

modern data-mining applications, often called    big-data    analysis, require us
to manage immense amounts of data quickly. in many of these applications, the
data is extremely regular, and there is ample opportunity to exploit parallelism.
important examples are:

1. the ranking of web pages by importance, which involves an iterated

matrix-vector multiplication where the dimension is many billions.

2. searches in    friends    networks at social-networking sites, which involve

graphs with hundreds of millions of nodes and many billions of edges.

to deal with applications such as these, a new software stack has evolved. these
programming systems are designed to get their parallelism not from a    super-
computer,    but from    computing clusters        large collections of commodity
hardware, including conventional processors (   compute nodes   ) connected by
ethernet cables or inexpensive switches. the software stack begins with a new
form of    le system, called a    distributed    le system,    which features much larger
units than the disk blocks in a conventional operating system. distributed    le
systems also provide replication of data or redundancy to protect against the
frequent media failures that occur when data is distributed over thousands of
low-cost compute nodes.

on top of these    le systems, many di   erent higher-level programming sys-
tems have been developed. central to the new software stack is a programming
system called mapreduce. implementations of mapreduce enable many of the
most common calculations on large-scale data to be performed on computing
clusters e   ciently and in a way that is tolerant of hardware failures during the
computation.

mapreduce systems are evolving and extending rapidly. today, it is com-
mon for mapreduce programs to be created from still higher-level programming

21

22

chapter 2. mapreduce and the new software stack

systems, often an implementation of sql. further, mapreduce turns out to be
a useful, but simple, case of more general and powerful ideas. we include
in this chapter a discussion of generalizations of mapreduce,    rst to systems
that support acyclic work   ows and then to systems that implement recursive
algorithms.

our last topic for this chapter is the design of good mapreduce algorithms,
a subject that often di   ers signi   cantly from the matter of designing good
parallel algorithms to be run on a supercomputer. when designing mapreduce
algorithms, we often    nd that the greatest cost is in the communication. we
thus investigate communication cost and what it tells us about the most e   cient
mapreduce algorithms. for several common applications of mapreduce we are
able to give families of algorithms that optimally trade the communication cost
against the degree of parallelism.

2.1 distributed file systems

most computing is done on a single processor, with its main memory, cache, and
local disk (a compute node). in the past, applications that called for parallel
processing, such as large scienti   c calculations, were done on special-purpose
parallel computers with many processors and specialized hardware. however,
the prevalence of large-scale web services has caused more and more computing
to be done on installations with thousands of compute nodes operating more
or less independently. in these installations, the compute nodes are commodity
hardware, which greatly reduces the cost compared with special-purpose parallel
machines.

these new computing facilities have given rise to a new generation of pro-
gramming systems. these systems take advantage of the power of parallelism
and at the same time avoid the reliability problems that arise when the comput-
ing hardware consists of thousands of independent components, any of which
could fail at any time. in this section, we discuss both the characteristics of
these computing installations and the specialized    le systems that have been
developed to take advantage of them.

2.1.1 physical organization of compute nodes

the new parallel-computing architecture, sometimes called cluster computing,
is organized as follows. compute nodes are stored on racks, perhaps 8   64
on a rack. the nodes on a single rack are connected by a network, typically
gigabit ethernet. there can be many racks of compute nodes, and racks are
connected by another level of network or a switch. the bandwidth of inter-rack
communication is somewhat greater than the intrarack ethernet, but given the
number of pairs of nodes that might need to communicate between racks, this
bandwidth may be essential. figure 2.1 suggests the architecture of a large-
scale computing system. however, there may be many more racks and many
more compute nodes per rack.

2.1. distributed file systems

23

switch

racks of compute nodes

figure 2.1: compute nodes are organized into racks, and racks are intercon-
nected by a switch

it is a fact of life that components fail, and the more components, such as
compute nodes and interconnection networks, a system has, the more frequently
something in the system will not be working at any given time. for systems
such as fig. 2.1, the principal failure modes are the loss of a single node (e.g.,
the disk at that node crashes) and the loss of an entire rack (e.g., the network
connecting its nodes to each other and to the outside world fails).

some important calculations take minutes or even hours on thousands of
compute nodes.
if we had to abort and restart the computation every time
one component failed, then the computation might never complete successfully.
the solution to this problem takes two forms:

1. files must be stored redundantly. if we did not duplicate the    le at several
compute nodes, then if one node failed, all its    les would be unavailable
until the node is replaced. if we did not back up the    les at all, and the
disk crashes, the    les would be lost forever. we discuss    le management
in section 2.1.2.

2. computations must be divided into tasks, such that if any one task fails
to execute to completion, it can be restarted without a   ecting other tasks.
this strategy is followed by the mapreduce programming system that we
introduce in section 2.2.

2.1.2 large-scale file-system organization

to exploit cluster computing,    les must look and behave somewhat di   erently
from the conventional    le systems found on single computers. this new    le
system, often called a distributed    le system or dfs (although this term has
had other meanings in the past), is typically used as follows.

24

chapter 2. mapreduce and the new software stack

dfs implementations

there are several distributed    le systems of the type we have described
that are used in practice. among these:

1. the google file system (gfs), the original of the class.

2. hadoop distributed file system (hdfs), an open-source dfs used
with hadoop, an implementation of mapreduce (see section 2.2)
and distributed by the apache software foundation.

3. cloudstore, an open-source dfs originally developed by kosmix.

    files can be enormous, possibly a terabyte in size. if you have only small

   les, there is no point using a dfs for them.

    files are rarely updated. rather, they are read as data for some calcula-
tion, and possibly additional data is appended to    les from time to time.
for example, an airline reservation system would not be suitable for a
dfs, even if the data were very large, because the data is changed so
frequently.

files are divided into chunks, which are typically 64 megabytes in size.
chunks are replicated, perhaps three times, at three di   erent compute nodes.
moreover, the nodes holding copies of one chunk should be located on di   erent
racks, so we don   t lose all copies due to a rack failure. normally, both the chunk
size and the degree of replication can be decided by the user.

to    nd the chunks of a    le, there is another small    le called the master node
or name node for that    le. the master node is itself replicated, and a directory
for the    le system as a whole knows where to    nd its copies. the directory itself
can be replicated, and all participants using the dfs know where the directory
copies are.

2.2 mapreduce

mapreduce is a style of computing that has been implemented in several sys-
tems, including google   s internal implementation (simply called mapreduce)
and the popular open-source implementation hadoop which can be obtained,
along with the hdfs    le system from the apache foundation. you can use
an implementation of mapreduce to manage many large-scale computations
in a way that is tolerant of hardware faults. all you need to write are two
functions, called map and reduce, while the system manages the parallel exe-
cution, coordination of tasks that execute map or reduce, and also deals with

2.2. mapreduce

25

the possibility that one of these tasks will fail to execute. in brief, a mapreduce
computation executes as follows:

1. some number of map tasks each are given one or more chunks from a
distributed    le system. these map tasks turn the chunk into a sequence
of key-value pairs. the way key-value pairs are produced from the input
data is determined by the code written by the user for the map function.

2. the key-value pairs from each map task are collected by a master con-
troller and sorted by key. the keys are divided among all the reduce
tasks, so all key-value pairs with the same key wind up at the same re-
duce task.

3. the reduce tasks work on one key at a time, and combine all the val-
ues associated with that key in some way. the manner of combination
of values is determined by the code written by the user for the reduce
function.

figure 2.2 suggests this computation.

keys with all
their values
(k, [v, w,...])

key   value

pairs
(k,v)

input
chunks

combined
output

map
tasks

group
by keys

reduce
tasks

figure 2.2: schematic of a mapreduce computation

2.2.1 the map tasks

we view input    les for a map task as consisting of elements, which can be
any type: a tuple or a document, for example. a chunk is a collection of
elements, and no element is stored across two chunks. technically, all inputs

26

chapter 2. mapreduce and the new software stack

to map tasks and outputs from reduce tasks are of the key-value-pair form,
but normally the keys of input elements are not relevant and we shall tend to
ignore them. insisting on this form for inputs and outputs is motivated by the
desire to allow composition of several mapreduce processes.

the map function takes an input element as its argument and produces
zero or more key-value pairs. the types of keys and values are each arbitrary.
further, keys are not    keys    in the usual sense; they do not have to be unique.
rather a map task can produce several key-value pairs with the same key, even
from the same element.

example 2.1 : we shall illustrate a mapreduce computation with what has
become the standard example application: counting the number of occurrences
for each word in a collection of documents. in this example, the input    le is a
repository of documents, and each document is an element. the map function
for this example uses keys that are of type string (the words) and values that
are integers. the map task reads a document and breaks it into its sequence
of words w1, w2, . . . , wn. it then emits a sequence of key-value pairs where the
value is always 1. that is, the output of the map task for this document is the
sequence of key-value pairs:

(w1, 1), (w2, 1), . . . , (wn, 1)

note that a single map task will typically process many documents     all
the documents in one or more chunks. thus, its output will be more than the
sequence for the one document suggested above. note also that if a word w
appears m times among all the documents assigned to that process, then there
will be m key-value pairs (w, 1) among its output. an option, which we discuss
in section 2.2.4, is to combine these m pairs into a single pair (w, m), but we
can only do that because, as we shall see, the reduce tasks apply an associative
and commutative operation, addition, to the values.    

2.2.2 grouping by key

as soon as the map tasks have all completed successfully, the key-value pairs are
grouped by key, and the values associated with each key are formed into a list of
values. the grouping is performed by the system, regardless of what the map
and reduce tasks do. the master controller process knows how many reduce
tasks there will be, say r such tasks. the user typically tells the mapreduce
system what r should be. then the master controller picks a hash function that
applies to keys and produces a bucket number from 0 to r     1. each key that
is output by a map task is hashed and its key-value pair is put in one of r local
   les. each    le is destined for one of the reduce tasks.1

1optionally, users can specify their own hash function or other method for assigning keys
to reduce tasks. however, whatever algorithm is used, each key is assigned to one and only
one reduce task.

2.2. mapreduce

27

to perform the grouping by key and distribution to the reduce tasks, the
master controller merges the    les from each map task that are destined for
a particular reduce task and feeds the merged    le to that process as a se-
quence of key-list-of-value pairs. that is, for each key k, the input to the
reduce task that handles key k is a pair of the form (k, [v1, v2, . . . , vn]), where
(k, v1), (k, v2), . . . , (k, vn) are all the key-value pairs with key k coming from
all the map tasks.

2.2.3 the reduce tasks

the reduce function   s argument is a pair consisting of a key and its list of
associated values. the output of the reduce function is a sequence of zero or
more key-value pairs. these key-value pairs can be of a type di   erent from
those sent from map tasks to reduce tasks, but often they are the same type.
we shall refer to the application of the reduce function to a single key and its
associated list of values as a reducer.

a reduce task receives one or more keys and their associated value lists.
that is, a reduce task executes one or more reducers. the outputs from all the
reduce tasks are merged into a single    le. reducers may be partitioned among
a smaller number of reduce tasks is by hashing the keys and associating each
reduce task with one of the buckets of the hash function.

example 2.2 : let us continue with the word-count example of example 2.1.
the reduce function simply adds up all the values. the output of a reducer
consists of the word and the sum. thus, the output of all the reduce tasks is a
sequence of (w, m) pairs, where w is a word that appears at least once among
all the input documents and m is the total number of occurrences of w among
all those documents.    

2.2.4 combiners

sometimes, a reduce function is associative and commutative. that is, the
values to be combined can be combined in any order, with the same result.
the addition performed in example 2.2 is an example of an associative and
commutative operation.
it doesn   t matter how we group a list of numbers
v1, v2, . . . , vn; the sum will be the same.

when the reduce function is associative and commutative, we can push
some of what the reducers do to the map tasks. for example, instead of the
map tasks in example 2.1 producing many pairs (w, 1), (w, 1), . . ., we could
apply the reduce function within the map task, before the output of the map
tasks is subject to grouping and aggregation. these key-value pairs would thus
be replaced by one pair with key w and value equal to the sum of all the 1   s in
all those pairs. that is, the pairs with key w generated by a single map task
would be replaced by a pair (w, m), where m is the number of times that w
appears among the documents handled by this map task. note that it is still
necessary to do grouping and aggregation and to pass the result to the reduce

28

chapter 2. mapreduce and the new software stack

reducers, reduce tasks, compute nodes, and skew

if we want maximum parallelism, then we could use one reduce task
to execute each reducer, i.e., a single key and its associated value list.
further, we could execute each reduce task at a di   erent compute node,
so they would all execute in parallel. this plan is not usually the best. one
problem is that there is overhead associated with each task we create, so
we might want to keep the number of reduce tasks lower than the number
of di   erent keys. moreover, often there are far more keys than there are
compute nodes available, so we would get no bene   t from a huge number
of reduce tasks.

second, there is often signi   cant variation in the lengths of the value
lists for di   erent keys, so di   erent reducers take di   erent amounts of time.
if we make each reducer a separate reduce task, then the tasks themselves
will exhibit skew     a signi   cant di   erence in the amount of time each
takes. we can reduce the impact of skew by using fewer reduce tasks
than there are reducers. if keys are sent randomly to reduce tasks, we
can expect that there will be some averaging of the total time required by
the di   erent reduce tasks. we can further reduce the skew by using more
reduce tasks than there are compute nodes.
in that way, long reduce
tasks might occupy a compute node fully, while several shorter reduce
tasks might run sequentially at a single compute node.

tasks, since there will typically be one key-value pair with key w coming from
each of the map tasks.

2.2.5 details of mapreduce execution

let us now consider in more detail how a program using mapreduce is executed.
figure 2.3 o   ers an outline of how processes, tasks, and    les interact. taking
advantage of a library provided by a mapreduce system such as hadoop, the
user program forks a master controller process and some number of worker
processes at di   erent compute nodes. normally, a worker handles either map
tasks (a map worker) or reduce tasks (a reduce worker), but not both.

the master has many responsibilities. one is to create some number of
map tasks and some number of reduce tasks, these numbers being selected
by the user program. these tasks will be assigned to worker processes by the
master. it is reasonable to create one map task for every chunk of the input
   le(s), but we may wish to create fewer reduce tasks. the reason for limiting
the number of reduce tasks is that it is necessary for each map task to create
an intermediate    le for each reduce task, and if there are too many reduce
tasks the number of intermediate    les explodes.

the master keeps track of the status of each map and reduce task (idle,

2.2. mapreduce

29

user

program

fork

master

fork

fork

assign
map

assign

reduce

worker

worker

worker

input
data

worker

worker

intermediate

files

output
file

figure 2.3: overview of the execution of a mapreduce program

executing at a particular worker, or completed). a worker process reports to
the master when it    nishes a task, and a new task is scheduled by the master
for that worker process.

each map task is assigned one or more chunks of the input    le(s) and
executes on it the code written by the user. the map task creates a    le for
each reduce task on the local disk of the worker that executes the map task.
the master is informed of the location and sizes of each of these    les, and the
reduce task for which each is destined. when a reduce task is assigned by the
master to a worker process, that task is given all the    les that form its input.
the reduce task executes code written by the user and writes its output to a
   le that is part of the surrounding distributed    le system.

2.2.6 coping with node failures

the worst thing that can happen is that the compute node at which the master
is executing fails. in this case, the entire mapreduce job must be restarted.
but only this one node can bring the entire process down; other failures will be
managed by the master, and the mapreduce job will complete eventually.

suppose the compute node at which a map worker resides fails. this fail-
ure will be detected by the master, because it periodically pings the worker
processes. all the map tasks that were assigned to this worker will have to
be redone, even if they had completed. the reason for redoing completed map

30

chapter 2. mapreduce and the new software stack

tasks is that their output destined for the reduce tasks resides at that compute
node, and is now unavailable to the reduce tasks. the master sets the status
of each of these map tasks to idle and will schedule them on a worker when
one becomes available. the master must also inform each reduce task that the
location of its input from that map task has changed.

dealing with a failure at the node of a reduce worker is simpler. the master
simply sets the status of its currently executing reduce tasks to idle. these
will be rescheduled on another reduce worker later.

2.2.7 exercises for section 2.2

exercise 2.2.1 : suppose we execute the word-count mapreduce program de-
scribed in this section on a large repository such as a copy of the web. we shall
use 100 map tasks and some number of reduce tasks.

(a) suppose we do not use a combiner at the map tasks. do you expect there
to be signi   cant skew in the times taken by the various reducers to process
their value list? why or why not?

(b) if we combine the reducers into a small number of reduce tasks, say 10
tasks, at random, do you expect the skew to be signi   cant? what if we
instead combine the reducers into 10,000 reduce tasks?

! (c) suppose we do use a combiner at the 100 map tasks. do you expect skew

to be signi   cant? why or why not?

2.3 algorithms using mapreduce

mapreduce is not a solution to every problem, not even every problem that
pro   tably can use many compute nodes operating in parallel. as we mentioned
in section 2.1.2, the entire distributed-   le-system milieu makes sense only when
   les are very large and are rarely updated in place. thus, we would not expect
to use either a dfs or an implementation of mapreduce for managing on-
line retail sales, even though a large on-line retailer such as amazon.com uses
thousands of compute nodes when processing requests over the web. the reason
is that the principal operations on amazon data involve responding to searches
for products, recording sales, and so on, processes that involve relatively little
calculation and that change the database.2 on the other hand, amazon might
use mapreduce to perform certain analytic queries on large amounts of data,
such as    nding for each user those users whose buying patterns were most
similar.

the original purpose for which the google implementation of mapreduce
was created was to execute very large matrix-vector multiplications as are

2remember that even looking at a product you don   t buy causes amazon to remember

that you looked at it.

2.3. algorithms using mapreduce

31

needed in the calculation of id95 (see chapter 5). we shall see that
matrix-vector and matrix-matrix calculations    t nicely into the mapreduce
style of computing. another important class of operations that can use mapre-
duce e   ectively are the relational-algebra operations. we shall examine the
mapreduce execution of these operations as well.

2.3.1 matrix-vector multiplication by mapreduce
suppose we have an n   n matrix m , whose element in row i and column j will
be denoted mij. suppose we also have a vector v of length n, whose jth element
is vj. then the matrix-vector product is the vector x of length n, whose ith
element xi is given by

n

xi =

xj=1

mij vj

if n = 100, we do not want to use a dfs or mapreduce for this calculation.
but this sort of calculation is at the heart of the ranking of web pages that
goes on at search engines, and there, n is in the tens of billions.3 let us    rst
assume that n is large, but not so large that vector v cannot    t in main memory
and thus be available to every map task.

the matrix m and the vector v each will be stored in a    le of the dfs. we
assume that the row-column coordinates of each matrix element will be discov-
erable, either from its position in the    le, or because it is stored with explicit
coordinates, as a triple (i, j, mij). we also assume the position of element vj in
the vector v will be discoverable in the analogous way.

the map function: the map function is written to apply to one element of
m . however, if v is not already read into main memory at the compute node
executing a map task, then v is    rst read, in its entirety, and subsequently will
be available to all applications of the map function performed at this map task.
each map task will operate on a chunk of the matrix m . from each matrix
element mij it produces the key-value pair (i, mijvj). thus, all terms of the
sum that make up the component xi of the matrix-vector product will get the
same key, i.

the reduce function: the reduce function simply sums all the values as-
sociated with a given key i. the result will be a pair (i, xi).

2.3.2

if the vector v cannot fit in main memory

however, it is possible that the vector v is so large that it will not    t in its
entirety in main memory. it is not required that v    t in main memory at a
compute node, but if it does not then there will be a very large number of

3the matrix is sparse, with on the average of 10 to 15 nonzero elements per row, since the
matrix represents the links in the web, with mij nonzero if and only if there is a link from
page j to page i. note that there is no way we could store a dense matrix whose side was
1010, since it would have 1020 elements.

32

chapter 2. mapreduce and the new software stack

disk accesses as we move pieces of the vector into main memory to multiply
components by elements of the matrix. thus, as an alternative, we can divide
the matrix into vertical stripes of equal width and divide the vector into an equal
number of horizontal stripes, of the same height. our goal is to use enough
stripes so that the portion of the vector in one stripe can    t conveniently into
main memory at a compute node. figure 2.4 suggests what the partition looks
like if the matrix and vector are each divided into    ve stripes.

matrix 

m

vector

v

figure 2.4: division of a matrix and vector into    ve stripes

the ith stripe of the matrix multiplies only components from the ith stripe
of the vector. thus, we can divide the matrix into one    le for each stripe, and
do the same for the vector. each map task is assigned a chunk from one of
the stripes of the matrix and gets the entire corresponding stripe of the vector.
the map and reduce tasks can then act exactly as was described above for the
case where map tasks get the entire vector.

we shall take up matrix-vector multiplication using mapreduce again in
section 5.2. there, because of the particular application (id95 calcula-
tion), we have an additional constraint that the result vector should be part-
itioned in the same way as the input vector, so the output may become the
input for another iteration of the matrix-vector multiplication. we shall see
there that the best strategy involves partitioning the matrix m into square
blocks, rather than stripes.

2.3.3 relational-algebra operations

there are a number of operations on large-scale data that are used in database
queries. many traditional database applications involve retrieval of small am-
ounts of data, even though the database itself may be large. for example, a
query may ask for the bank balance of one particular account. such queries are
not useful applications of mapreduce.

however, there are many operations on data that can be described easily in
terms of the common database-query primitives, even if the queries themselves

2.3. algorithms using mapreduce

33

are not executed within a database management system. thus, a good starting
point for exploring applications of mapreduce is by considering the standard
operations on relations. we assume you are familiar with database systems,
the query language sql, and the relational model, but to review, a relation is
a table with column headers called attributes. rows of the relation are called
tuples. the set of attributes of a relation is called its schema. we often write
an expression like r(a1, a2, . . . , an) to say that the relation name is r and its
attributes are a1, a2, . . . , an.

from to

url1
url1
url2
url2
      

url2
url3
url3
url4
      

figure 2.5: relation links consists of the set of pairs of url   s, such that the
   rst has one or more links to the second

example 2.3 : in fig. 2.5 we see part of the relation links that describes the
structure of the web. there are two attributes, from and to. a row, or tuple,
of the relation is a pair of url   s, such that there is at least one link from
the    rst url to the second. for instance, the    rst row of fig. 2.5 is the pair
(url1, url2) that says the web page url1 has a link to page url2. while we
have shown only four tuples, the real relation of the web, or the portion of it
that would be stored by a typical search engine, has billions of tuples.    

a relation, however large, can be stored as a    le in a distributed    le system.

the elements of this    le are the tuples of the relation.

there are several standard operations on relations, often referred to as re-
lational algebra, that are used to implement queries. the queries themselves
usually are written in sql. the relational-algebra operations we shall discuss
are:

1. selection: apply a condition c to each tuple in the relation and produce
as output only those tuples that satisfy c. the result of this selection is
denoted   c (r).

2. projection: for some subset s of the attributes of the relation, produce
from each tuple only the components for the attributes in s. the result
of this projection is denoted   s(r).

3. union, intersection, and di   erence: these well-known set operations
apply to the sets of tuples in two relations that have the same schema.
there are also bag (multiset) versions of the operations in sql, with

34

chapter 2. mapreduce and the new software stack

somewhat unintuitive de   nitions, but we shall not go into the bag versions
of these operations here.

4. natural join: given two relations, compare each pair of tuples, one from
each relation. if the tuples agree on all the attributes that are common
to the two schemas, then produce a tuple that has components for each
of the attributes in either schema and agrees with the two tuples on each
attribute. if the tuples disagree on one or more shared attributes, then
produce nothing from this pair of tuples. the natural join of relations r
and s is denoted r        s. while we shall discuss executing only the nat-
ural join with mapreduce, all equijoins (joins where the tuple-agreement
condition involves equality of attributes from the two relations that do not
necessarily have the same name) can be executed in the same manner. we
shall give an illustration in example 2.4.

5. grouping and aggregation:4 given a relation r, partition its tuples
according to their values in one set of attributes g, called the grouping
attributes. then, for each group, aggregate the values in certain other at-
tributes. the normally permitted aggregations are sum, count, avg,
min, and max, with the obvious meanings. note that min and max
require that the aggregated attributes have a type that can be compared,
e.g., numbers or strings, while sum and avg require that the type allow
arithmetic operations. we denote a grouping-and-aggregation operation
on a relation r by   x (r), where x is a list of elements that are either

(a) a grouping attribute, or

(b) an expression   (a), where    is one of the    ve aggregation opera-
tions such as sum, and a is an attribute not among the grouping
attributes.

the result of this operation is one tuple for each group. that tuple has
a component for each of the grouping attributes, with the value common
to tuples of that group. it also has a component for each aggregation,
with the aggregated value for that group. we shall see an illustration in
example 2.5.

example 2.4 : let us try to    nd the paths of length two in the web, using
the relation links of fig. 2.5. that is, we want to    nd the triples of url   s
(u, v, w) such that there is a link from u to v and a link from v to w. we
essentially want to take the natural join of links with itself, but we    rst need
to imagine that it is two relations, with di   erent schemas, so we can describe the
desired connection as a natural join. thus, imagine that there are two copies
of links, namely l1(u 1, u 2) and l2(u 2, u 3). now, if we compute l1        l2,

4some descriptions of relational algebra do not include these operations, and indeed they
were not part of the original de   nition of this algebra. however, these operations are so
important in sql, that modern treatments of relational algebra include them.

2.3. algorithms using mapreduce

35

we shall have exactly what we want. that is, for each tuple t1 of l1 (i.e.,
each tuple of links) and each tuple t2 of l2 (another tuple of links, possibly
even the same tuple), see if their u 2 components are the same. note that
these components are the second component of t1 and the    rst component of
t2. if these two components agree, then produce a tuple for the result, with
schema (u 1, u 2, u 3). this tuple consists of the    rst component of t1, the
second component of t1 (which must equal the    rst component of t2), and the
second component of t2.

we may not want the entire path of length two, but only want the pairs
(u, w) of url   s such that there is at least one path from u to w of length two. if
so, we can project out the middle components by computing   u1,u3(l1        l2).
   

example 2.5 : imagine that a social-networking site has a relation

friends(user, friend)

this relation has tuples that are pairs (a, b) such that b is a friend of a. the site
might want to develop statistics about the number of friends members have.
their    rst step would be to compute a count of the number of friends of each
user. this operation can be done by grouping and aggregation, speci   cally

  user,count(friend)(friends)

this operation groups all the tuples by the value in their    rst component, so
there is one group for each user. then, for each group the count of the number
of friends of that user is made. the result will be one tuple for each group, and
a typical tuple would look like (sally, 300), if user    sally    has 300 friends.    

2.3.4 computing selections by mapreduce

selections really do not need the full power of mapreduce. they can be done
most conveniently in the map portion alone, although they could also be done
in the reduce portion alone. here is a mapreduce implementation of selection
  c (r).

the map function: for each tuple t in r, test if it satis   es c. if so, produce
the key-value pair (t, t). that is, both the key and value are t.

the reduce function: the reduce function is the identity. it simply passes
each key-value pair to the output.

note that the output is not exactly a relation, because it has key-value pairs.
however, a relation can be obtained by using only the value components (or
only the key components) of the output.

36

chapter 2. mapreduce and the new software stack

2.3.5 computing projections by mapreduce

projection is performed similarly to selection, because projection may cause
the same tuple to appear several times, the reduce function must eliminate
duplicates. we may compute   s(r) as follows.
the map function: for each tuple t in r, construct a tuple t    by eliminating
from t those components whose attributes are not in s. output the key-value
pair (t   , t   ).
the reduce function: for each key t    produced by any of the map tasks,
there will be one or more key-value pairs (t   , t   ). the reduce function turns
(t   , [t   , t   , . . . , t   ]) into (t   , t   ), so it produces exactly one pair (t   , t   ) for this key
t   .

observe that the reduce operation is duplicate elimination. this operation
is associative and commutative, so a combiner associated with each map task
can eliminate whatever duplicates are produced locally. however, the reduce
tasks are still needed to eliminate two identical tuples coming from di   erent
map tasks.

2.3.6 union, intersection, and di   erence by mapreduce

first, consider the union of two relations. suppose relations r and s have the
same schema. map tasks will be assigned chunks from either r or s; it doesn   t
matter which. the map tasks don   t really do anything except pass their input
tuples as key-value pairs to the reduce tasks. the latter need only eliminate
duplicates as for projection.

the map function: turn each input tuple t into a key-value pair (t, t).

the reduce function: associated with each key t there will be either one or
two values. produce output (t, t) in either case.

to compute the intersection, we can use the same map function. however,
the reduce function must produce a tuple only if both relations have the tuple.
if the key t has a list of two values [t, t] associated with it, then the reduce
task for t should produce (t, t). however, if the value-list associated with key
t is just [t], then one of r and s is missing t, so we don   t want to produce a
tuple for the intersection.

the map function: turn each tuple t into a key-value pair (t, t).

the reduce function: if key t has value list [t, t], then produce (t, t). oth-
erwise, produce nothing.

the di   erence r     s requires a bit more thought. the only way a tuple
t can appear in the output is if it is in r but not in s. the map function
can pass tuples from r and s through, but must inform the reduce function
whether the tuple came from r or s. we shall thus use the relation as the
value associated with the key t. here is a speci   cation for the two functions.

2.3. algorithms using mapreduce

37

the map function: for a tuple t in r, produce key-value pair (t, r), and
for a tuple t in s, produce key-value pair (t, s). note that the intent is that
the value is the name of r or s (or better, a single bit indicating whether the
relation is r or s), not the entire relation.

the reduce function: for each key t, if the associated value list is [r], then
produce (t, t). otherwise, produce nothing.

2.3.7 computing natural join by mapreduce

the idea behind implementing natural join via mapreduce can be seen if we
look at the speci   c case of joining r(a, b) with s(b, c). we must    nd tuples
that agree on their b components, that is the second component from tuples
of r and the    rst component of tuples of s. we shall use the b-value of tuples
from either relation as the key. the value will be the other component and the
name of the relation, so the reduce function can know where each tuple came
from.

the map function: for each tuple (a, b) of r, produce the key-value pair

(cid:0)b, (r, a)(cid:1). for each tuple (b, c) of s, produce the key-value pair (cid:0)b, (s, c)(cid:1).

the reduce function: each key value b will be associated with a list of pairs
that are either of the form (r, a) or (s, c). construct all pairs consisting of one
with    rst component r and the other with    rst component s, say (r, a) and
(s, c). the output from this key and value list is a sequence of key-value pairs.
the key is irrelevant. each value is one of the triples (a, b, c) such that (r, a)
and (s, c) are on the input list of values.

the same algorithm works if the relations have more than two attributes.
you can think of a as representing all those attributes in the schema of r but
not s. b represents the attributes in both schemas, and c represents attributes
only in the schema of s. the key for a tuple of r or s is the list of values in all
the attributes that are in the schemas of both r and s. the value for a tuple
of r is the name r together with the values of all the attributes belonging to
r but not to s, and the value for a tuple of s is the name s together with the
values of the attributes belonging to s but not r.

the reduce function looks at all the key-value pairs with a given key and
combines those values from r with those values of s in all possible ways. from
each pairing, the tuple produced has the values from r, the key values, and the
values from s.

2.3.8 grouping and aggregation by mapreduce

as with the join, we shall discuss the minimal example of grouping and aggrega-
tion, where there is one grouping attribute and one aggregation. let r(a, b, c)
be a relation to which we apply the operator   a,  (b)(r). map will perform the
grouping, while reduce does the aggregation.

the map function: for each tuple (a, b, c) produce the key-value pair (a, b).

38

chapter 2. mapreduce and the new software stack

the reduce function: each key a represents a group. apply the aggregation
operator    to the list [b1, b2, . . . , bn] of b-values associated with key a. the
output is the pair (a, x), where x is the result of applying    to the list. for
example, if    is sum, then x = b1 + b2 +        + bn, and if    is max, then x is
the largest of b1, b2, . . . , bn.

if there are several grouping attributes, then the key is the list of the values
of a tuple for all these attributes. if there is more than one aggregation, then
the reduce function applies each of them to the list of values associated with
a given key and produces a tuple consisting of the key, including components
for all grouping attributes if there is more than one, followed by the results of
each of the aggregations.

2.3.9 id127

if m is a matrix with element mij in row i and column j, and n is a matrix
with element njk in row j and column k, then the product p = m n is the
matrix p with element pik in row i and column k, where

pik = xj

mijnjk

it is required that the number of columns of m equals the number of rows of
n , so the sum over j makes sense.

we can think of a matrix as a relation with three attributes: the row number,
the column number, and the value in that row and column. thus, we could view
matrix m as a relation m (i, j, v ), with tuples (i, j, mij), and we could view
matrix n as a relation n (j, k, w ), with tuples (j, k, njk). as large matrices are
often sparse (mostly 0   s), and since we can omit the tuples for matrix elements
that are 0, this relational representation is often a very good one for a large
matrix. however, it is possible that i, j, and k are implicit in the position of a
matrix element in the    le that represents it, rather than written explicitly with
the element itself. in that case, the map function will have to be designed to
construct the i, j, and k components of tuples from the position of the data.
the product m n is almost a natural join followed by grouping and ag-
gregation. that is, the natural join of m (i, j, v ) and n (j, k, w ), having
only attribute j in common, would produce tuples (i, j, k, v, w) from each tuple
(i, j, v) in m and tuple (j, k, w) in n . this    ve-component tuple represents the
pair of matrix elements (mij, njk). what we want instead is the product of
these elements, that is, the four-component tuple (i, j, k, v    w), because that
represents the product mij njk. once we have this relation as the result of one
mapreduce operation, we can perform grouping and aggregation, with i and
k as the grouping attributes and the sum of v    w as the aggregation. that
is, we can implement id127 as the cascade of two mapreduce
operations, as follows. first:
the map function: for each matrix element mij, produce the key value pair

(cid:0)j, (m, i, mij)(cid:1). likewise, for each matrix element njk, produce the key value

2.3. algorithms using mapreduce

39

pair (cid:0)j, (n, k, njk)(cid:1). note that m and n in the values are not the matrices

themselves. rather they are names of the matrices or (as we mentioned for the
similar map function used for natural join) better, a bit indicating whether the
element comes from m or n .

the reduce function: for each key j, examine its list of associated values.
for each value that comes from m , say (m, i, mij), and each value that comes
from n , say (n, k, njk), produce a key-value pair with key equal to (i, k) and
value equal to the product of these elements, mij njk.

now, we perform a grouping and aggregation by another mapreduce operation.

the map function: this function is just the identity. that is, for every input
element with key (i, k) and value v, produce exactly this key-value pair.

the reduce function: for each key (i, k), produce the sum of the list of

values associated with this key. the result is a pair (cid:0)(i, k), v(cid:1), where v is the

value of the element in row i and column k of the matrix p = m n .

2.3.10 id127 with one mapreduce step

there often is more than one way to use mapreduce to solve a problem. you
may wish to use only a single mapreduce pass to perform id127
p = m n . 5 it is possible to do so if we put more work into the two functions.
start by using the map function to create the sets of matrix elements that are
needed to compute each element of the answer p . notice that an element of
m or n contributes to many elements of the result, so one input element will
be turned into many key-value pairs. the keys will be pairs (i, k), where i is a
row of m and k is a column of n . here is a synopsis of the map and reduce
functions.

the map function: for each element mij of m , produce all the key-value

n . similarly, for each element njk of n , produce all the key-value pairs

pairs (cid:0)(i, k), (m, j, mij )(cid:1) for k = 1, 2, . . ., up to the number of columns of
(cid:0)(i, k), (n, j, njk)(cid:1) for i = 1, 2, . . ., up to the number of rows of m . as be-

fore, m and n are really bits to tell which of the two matrices a value comes
from.

the reduce function: each key (i, k) will have an associated list with all
the values (m, j, mij ) and (n, j, njk), for all possible values of j. the reduce
function needs to connect the two values on the list that have the same value of
j, for each j. an easy way to do this step is to sort by j the values that begin
with m and sort by j the values that begin with n , in separate lists. the jth
values on each list must have their third components, mij and njk extracted
and multiplied. then, these products are summed and the result is paired with
(i, k) in the output of the reduce function.

5however, we show in section 2.6.7 that two passes of mapreduce are usually better than

one for id127.

40

chapter 2. mapreduce and the new software stack

you may notice that if a row of the matrix m or a column of the matrix n
is so large that it will not    t in main memory, then the reduce tasks will be
forced to use an external sort to order the values associated with a given key
(i, k). however, in that case, the matrices themselves are so large, perhaps 1020
elements, that it is unlikely we would attempt this calculation if the matrices
were dense. if they are sparse, then we would expect many fewer values to be
associated with any one key, and it would be feasible to do the sum of products
in main memory.

2.3.11 exercises for section 2.3

exercise 2.3.1 : design mapreduce algorithms to take a very large    le of
integers and produce as output:

(a) the largest integer.

(b) the average of all the integers.

(c) the same set of integers, but with each integer appearing only once.

(d) the count of the number of distinct integers in the input.

exercise 2.3.2 : our formulation of matrix-vector multiplication assumed that
the matrix m was square. generalize the algorithm to the case where m is an
r-by-c matrix for some number of rows r and columns c.

! exercise 2.3.3 : in the form of relational algebra implemented in sql, rela-
tions are not sets, but bags; that is, tuples are allowed to appear more than
once. there are extended de   nitions of union, intersection, and di   erence for
bags, which we shall de   ne below. write mapreduce algorithms for computing
the following operations on bags r and s:

(a) bag union, de   ned to be the bag of tuples in which tuple t appears the

sum of the numbers of times it appears in r and s.

(b) bag intersection, de   ned to be the bag of tuples in which tuple t appears

the minimum of the numbers of times it appears in r and s.

(c) bag di   erence, de   ned to be the bag of tuples in which the number of
times a tuple t appears is equal to the number of times it appears in r
minus the number of times it appears in s. a tuple that appears more
times in s than in r does not appear in the di   erence.

! exercise 2.3.4 : selection can also be performed on bags. give a mapreduce
implementation that produces the proper number of copies of each tuple t that
passes the selection condition. that is, produce key-value pairs from which the
correct result of the selection can be obtained easily from the values.

2.4. extensions to mapreduce

41

exercise 2.3.5 : the relational-algebra operation r(a, b)        b<c s(c, d)
produces all tuples (a, b, c, d) such that tuple (a, b) is in relation r, tuple (c, d) is
in s, and b < c. give a mapreduce implementation of this operation, assuming
r and s are sets.

2.4 extensions to mapreduce

mapreduce has proved so in   uential that it has spawned a number of extensions
and modi   cations. these systems typically share a number of characteristics
with mapreduce systems:

1. they are built on a distributed    le system.

2. they manage very large numbers of tasks that are instantiations of a

small number of user-written functions.

3. they incorporate a method for dealing with most of the failures that
occur during the execution of a large job, without having to restart that
job from the beginning.

in this section, we shall mention some of the interesting directions being ex-
plored. references to the details of the systems mentioned can be found in the
bibliographic notes for this chapter.

2.4.1 work   ow systems

two experimental systems called clustera from the university of wisconsin and
hyracks from the university of california at irvine extend mapreduce from the
simple two-step work   ow (the map function feeds the reduce function) to any
collection of functions, with an acyclic graph representing work   ow among the
functions. that is, there is an acyclic    ow graph whose arcs a     b represent
the fact that function a   s output is input to function b. a suggestion of what a
work   ow might look like is in fig. 2.6. there,    ve functions, f through j, pass
data from left to right in speci   c ways, so the    ow of data is acyclic and no task
needs to provide data out before its input is available. for instance, function h
takes its input from a preexisting    le of the distributed    le system. each of h   s
output elements is passed to at least one of the functions i and j.

in analogy to map and reduce functions, each function of a work   ow can
be executed by many tasks, each of which is assigned a portion of the input to
the function. a master controller is responsible for dividing the work among
the tasks that implement a function, usually by hashing the input elements to
decide on the proper task to receive an element. thus, like map tasks, each task
implementing a function f has an output    le of data destined for each of the
tasks that implement the successor function(s) of f . these    les are delivered
by the master at the appropriate time     after the task has completed its work.

42

chapter 2. mapreduce and the new software stack

f

h

g

i

j

figure 2.6: an example of a work   ow that is more complex than map feeding
reduce

the functions of a work   ow, and therefore the tasks, share with mapreduce
tasks the important property that they only deliver output after they complete.
as a result, if a task fails, it has not delivered output to any of its successors
in the    ow graph. a master controller can therefore restart the failed task at
another compute node, without worrying that the output of the restarted task
will duplicate output that previously was passed to some other task.

many applications of work   ow systems such as clustera or hyracks are
cascades of mapreduce jobs. an example would be the join of three relations,
where one mapreduce job joins the    rst two relations, and a second mapreduce
job joins the third relation with the result of joining the    rst two relations. both
jobs would use an algorithm like that of section 2.3.7.

there is an advantage to implementing such cascades as a single work   ow.
for example, the    ow of data among tasks, and its replication, can be managed
by the master controller, without need to store the temporary    le that is out-
put of one mapreduce job in the distributed    le system. by locating tasks at
compute nodes that have a copy of their input, we can avoid much of the com-
munication that would be necessary if we stored the result of one mapreduce
job and then initiated a second mapreduce job (although hadoop and other
mapreduce systems also try to locate map tasks where a copy of their input is
already present).

2.4.2 recursive extensions to mapreduce

many large-scale computations are really recursions. an important example is
id95, which is the subject of chapter 5. that computation is, in sim-
ple terms, the computation of the    xedpoint of a matrix-vector multiplication.
it is computed under mapreduce systems by the iterated application of the
matrix-vector multiplication algorithm described in section 2.3.1, or by a more
complex strategy that we shall introduce in section 5.2. the iteration typi-
cally continues for an unknown number of steps, each step being a mapreduce
job, until the results of two consecutive iterations are su   ciently close that we
believe convergence has occurred.

the reason recursions are normally implemented by iterated mapreduce

2.4. extensions to mapreduce

43

jobs is that a true recursive task does not have the property necessary for
independent restart of failed tasks. it is impossible for a collection of mutually
recursive tasks, each of which has an output that is input to at least some of
the other tasks, to produce output only at the end of the task.
if they all
followed that policy, no task would ever receive any input, and nothing could
be accomplished. as a result, some mechanism other than simple restart of
failed tasks must be implemented in a system that handles recursive work   ows
(   ow graphs that are not acyclic). we shall start by studying an example of a
recursion implemented as a work   ow, and then discuss approaches to dealing
with task failures.

example 2.6 : suppose we have a directed graph whose arcs are represented
by the relation e(x, y ), meaning that there is an arc from node x to node y .
we wish to compute the paths relation p (x, y ), meaning that there is a path
of length 1 or more from node x to node y . that is, p is the transitive closure
of e. a simple recursive algorithm to do so is:

1. start with p (x, y ) = e(x, y ).

2. while changes to the relation p occur, add to p all tuples in

  x,y(cid:0)p (x, z)        p (z, y )(cid:1)

that is,    nd pairs of nodes x and y such that for some node z there is
known to be a path from x to z and also a path from z to y .

figure 2.7 suggests how we could organize recursive tasks to perform this
computation. there are two kinds of tasks: join tasks and dup-elim tasks.
there are n join tasks, for some n, and each corresponds to a bucket of a hash
function h. a path tuple p (a, b), when it is discovered, becomes input to two
join tasks: those numbered h(a) and h(b). the job of the ith join task, when
it receives input tuple p (a, b), is to    nd certain other tuples seen previously
(and stored locally by that task).

1. store p (a, b) locally.

2. if h(a) = i then look for tuples p (x, a) and produce output tuple p (x, b).

3. if h(b) = i then look for tuples p (b, y) and produce output tuple p (a, y).

note that in rare cases, we have h(a) = h(b), so both (2) and (3) are executed.
but generally, only one of these needs to be executed for a given tuple.

there are also m dup-elim tasks, and each corresponds to a bucket of a hash
function g that takes two arguments. if p (c, d) is an output of some join task,
then it is sent to dup-elim task j = g(c, d). on receiving this tuple, the jth
dup-elim task checks that it had not received it before, since its job is duplicate
elimination. if previously received, the tuple is ignored. but if this tuple is new,
it is stored locally and sent to two join tasks, those numbered h(c) and h(d).

44

chapter 2. mapreduce and the new software stack

join
task
0

join
task
1

.
.
.

join
task
i

.
.
.

p(c,d) if
g(c,d) = j

dup   elim

task
0

dup   elim

task
1

.
.
.

dup   elim

task
j

.
.
.

to join task h(c)

p(c,d) if never
seen before

to join task h(d)

p(a,b) if
h(a) = i or
h(b) = i

figure 2.7: implementation of transitive closure by a collection of recursive
tasks

every join task has m output    les     one for each dup-elim task     and every
dup-elim task has n output    les     one for each join task. these    les may be
distributed according to any of several strategies. initially, the e(a, b) tuples
representing the arcs of the graph are distributed to the dup-elim tasks, with
e(a, b) being sent as p (a, b) to dup-elim task g(a, b). the master can wait until
each join task has processed its entire input for a round. then, all output    les
are distributed to the dup-elim tasks, which create their own output. that
output is distributed to the join tasks and becomes their input for the next
round. alternatively, each task can wait until it has produced enough output
to justify transmitting its output    les to their destination, even if the task has
not consumed all its input.    

in example 2.6 it is not essential to have two kinds of tasks. rather, join
tasks could eliminate duplicates as they are received, since they must store
their previously received inputs anyway. however, this arrangement has an
advantage when we must recover from a task failure.
if each task stores all
the output    les it has ever created, and we place join tasks on di   erent racks
from the dup-elim tasks, then we can deal with any single compute node or

2.4. extensions to mapreduce

45

pregel and giraph

like mapreduce, pregel was developed originally at google. also like
mapreduce, there is an apache, open-source equivalent, called giraph.

single rack failure. that is, a join task needing to be restarted can get all the
previously generated inputs that it needs from the dup-elim tasks, and vice
versa.

in the particular case of computing transitive closure, it is not necessary to
prevent a restarted task from generating outputs that the original task gener-
ated previously. in the computation of the transitive closure, the rediscovery of
a path does not in   uence the eventual answer. however, many computations
cannot tolerate a situation where both the original and restarted versions of a
task pass the same output to another task. for example, if the    nal step of the
computation were an aggregation, say a count of the number of nodes reached
by each node in the graph, then we would get the wrong answer if we counted
a path twice. in such a case, the master controller can record what    les each
task generated and passed to other tasks. it can then restart a failed task and
ignore those    les when the restarted version produces them a second time.

2.4.3 pregel

another approach to managing failures when implementing recursive algorithms
on a computing cluster is represented by the pregel system. this system views
its data as a graph. each node of the graph corresponds roughly to a task
(although in practice many nodes of a large graph would be bundled into a
single task, as in the join tasks of example 2.6). each graph node generates
output messages that are destined for other nodes of the graph, and each graph
node processes the inputs it receives from other nodes.

example 2.7 : suppose our data is a collection of weighted arcs of a graph,
and we want to    nd, for each node of the graph, the length of the shortest
path to each of the other nodes. initially, each graph node a stores the set of
pairs (b, w) such that there is an arc from a to b of weight w. these facts are
initially sent to all other nodes, as triples (a, b, w).6 when the node a receives
a triple (c, d, w), it looks up its current distance to c; that is, it    nds the pair
(c, v) stored locally, if there is one. it also    nds the pair (d, u) if there is one.
if w + v < u, then the pair (d, u) is replaced by (d, w + v), and if there was
no pair (d, u), then the pair (d, w + v) is stored at the node a. also, the other
nodes are sent the message (a, d, w + v) in either of these two cases.    

6this algorithm uses much too much communication, but it will serve as a simple example

of the pregel computation model.

46

chapter 2. mapreduce and the new software stack

computations in pregel are organized into supersteps. in one superstep, all
the messages that were received by any of the nodes at the previous superstep
(or initially, if it is the    rst superstep) are processed, and then all the messages
generated by those nodes are sent to their destination.

in case of a compute-node failure, there is no attempt to restart the failed
tasks at that compute node. rather, pregel checkpoints its entire computation
after some of the supersteps. a checkpoint consists of making a copy of the
entire state of each task, so it can be restarted from that point if necessary.
if any compute node fails, the entire job is restarted from the most recent
checkpoint.

although this recovery strategy causes many tasks that have not failed to
redo their work, it is satisfactory in many situations. recall that the reason
mapreduce systems support restart of only the failed tasks is that we want
assurance that the expected time to complete the entire job in the face of fail-
ures is not too much greater than the time to run the job with no failures.
any failure-management system will have that property as long as the time
to recover from a failure is much less than the average time between failures.
thus, it is only necessary that pregel checkpoints its computation after a num-
ber of supersteps such that the id203 of a failure during that number of
supersteps is low.

2.4.4 exercises for section 2.4

! exercise 2.4.1 : suppose a job consists of n tasks, each of which takes time t
seconds. thus, if there are no failures, the sum over all compute nodes of the
time taken to execute tasks at that node is nt. suppose also that the id203
of a task failing is p per job per second, and when a task fails, the overhead of
management of the restart is such that it adds 10t seconds to the total execution
time of the job. what is the total expected execution time of the job?

! exercise 2.4.2 : suppose a pregel job has a id203 p of a failure during
any superstep. suppose also that the execution time (summed over all compute
nodes) of taking a checkpoint is c times the time it takes to execute a superstep.
to minimize the expected execution time of the job, how many supersteps
should elapse between checkpoints?

2.5 the communication cost model

in this section we shall introduce a model for measuring the quality of algorithms
implemented on a computing cluster of the type so far discussed in this chapter.
we assume the computation is described by an acyclic work   ow, as discussed
in section 2.4.1. for many applications, the bottleneck is moving data among
tasks, such as transporting the outputs of map tasks to their proper reduce
tasks. as an example, we explore the computation of multiway joins as single

2.5. the communication cost model

47

mapreduce jobs, and we see that in some situations, this approach is more
e   cient than the straightforward cascade of 2-way joins.

2.5.1 communication-cost for task networks

imagine that an algorithm is implemented by an acyclic network of tasks. these
tasks could be map tasks feeding reduce tasks, as in a standard mapreduce
algorithm, or they could be several mapreduce jobs cascaded, or a more general
work   ow structure, such as a collection of tasks each of which implements the
work   ow of fig. 2.6.7 the communication cost of a task is the size of the input
to the task. this size can be measured in bytes. however, since we shall be
using relational database operations as examples, we shall often use the number
of tuples as a measure of size.

the communication cost of an algorithm is the sum of the communication
cost of all the tasks implementing that algorithm. we shall focus on the commu-
nication cost as the way to measure the e   ciency of an algorithm. in particular,
we do not consider the amount of time it takes each task to execute when es-
timating the running time of an algorithm. while there are exceptions, where
execution time of tasks dominates, these exceptions are rare in practice. we
can explain and justify the importance of communication cost as follows.

    the algorithm executed by each task tends to be very simple, often linear

in the size of its input.

    the typical interconnect speed for a computing cluster is one gigabit per
second. that may seem like a lot, but it is slow compared with the speed
at which a processor executes instructions. moreover, in many cluster
architectures, there is competition for the interconnect when several com-
pute nodes need to communicate at the same time. as a result, the
compute node can do a lot of work on a received input element in the
time it takes to deliver that element.

    even if a task executes at a compute node that has a copy of the chunk(s)
on which the task operates, that chunk normally will be stored on disk,
and the time taken to move the data into main memory may exceed the
time needed to operate on the data once it is available in memory.

assuming that communication cost is the dominant cost, we might still ask
why we count only input size, and not output size. the answer to this question
involves two points:

1. if the output of one task    is input to another task, then the size of       s
output will be accounted for when measuring the input size for the receiv-
ing task. thus, there is no reason to count the size of any output except
for those tasks whose output forms the result of the entire algorithm.

7recall that this    gure represented functions, not tasks. as a network of tasks, there
would be, for example, many tasks implementing function f , each of which feeds data to each
of the tasks for function g and each of the tasks for function i.

48

chapter 2. mapreduce and the new software stack

2. but in practice, the algorithm output is rarely large compared with the
input or the intermediate data produced by the algorithm. the reason
is that massive outputs cannot be used unless they are summarized or
aggregated in some way. for example, although we talked in example 2.6
of computing the entire transitive closure of a graph, in practice we would
want something much simpler, such as the count of the number of nodes
reachable from each node, or the set of nodes reachable from a single
node.

example 2.8 : let us evaluate the communication cost for the join algorithm
from section 2.3.7. suppose we are joining r(a, b)        s(b, c), and the sizes
of relations r and s are r and s, respectively. each chunk of the    les holding
r and s is fed to one map task, so the sum of the communication costs for all
the map tasks is r + s. note that in a typical execution, the map tasks will
each be executed at a compute node holding a copy of the chunk to which it
applies. thus, no internode communication is needed for the map tasks, but
they still must read their data from disk. since all the map tasks do is make a
simple transformation of each input tuple into a key-value pair, we expect that
the computation cost will be small compared with the communication cost,
regardless of whether the input is local to the task or must be transported to
its compute node.

the sum of the outputs of the map tasks is roughly as large as their in-
puts. each output key-value pair is sent to exactly one reduce task, and it is
unlikely that this reduce task will execute at the same compute node. there-
fore, communication from map tasks to reduce tasks is likely to be across the
interconnect of the cluster, rather than memory-to-disk. this communication
is o(r + s), so the communication cost of the join algorithm is o(r + s).

the reduce tasks execute the reducer (application of the reduce function
to a key and its associated value list) for one or more values of attribute b.
each reducer takes the inputs it receives and divides them between tuples that
came from r and those that came from s. each tuple from r pairs with each
tuple from s to produce one output. the output size for the join can be either
larger or smaller than r + s, depending on how likely it is that a given r-tuple
joins with a given s-tuple. for example, if there are many di   erent b-values,
we would expect the output to be small, while if there are few b-values, a large
output is likely.

if the output is large, then the computation cost of generating all the outputs
from a reducer could be much larger than o(r+s). however, we shall rely on our
supposition that if the output of the join is large, then there is probably some
aggregation being done to reduce the size of the output. it will be necessary to
communicate the result of the join to another collection of tasks that perform
this aggregation, and thus the communication cost will be at least proportional
to the computation needed to produce the output of the join.    

2.5. the communication cost model

49

2.5.2 wall-clock time

while communication cost often in   uences our choice of algorithm to use in
a cluster-computing environment, we must also be aware of the importance of
wall-clock time, the time it takes a parallel algorithm to    nish. using careless
reasoning, one could minimize total communication cost by assigning all the
work to one task, and thereby minimize total communication. however, the
wall-clock time of such an algorithm would be quite high. the algorithms we
suggest, or have suggested so far, have the property that the work is divided
fairly among the tasks. therefore, the wall-clock time would be approximately
as small as it could be, given the number of compute nodes available.

2.5.3 multiway joins

to see how analyzing the communication cost can help us choose an algorithm
in the cluster-computing environment, we shall examine carefully the case of a
multiway join. there is a general theory in which we:

1. select certain attributes of the relations involved in the natural join of
three or more relations to have their values hashed, each to some number
of buckets.

2. select the number of buckets for each of these attributes, subject to the
constraint that the product of the numbers of buckets for each attribute
is k, the number of reducers that will be used.

3. identify each of the k reducers with a vector of bucket numbers. these
vectors have one component for each of the attributes selected at step (1).

4. send tuples of each relation to all those reducers where it might    nd tuples
to join with. that is, the given tuple t will have values for some of the
attributes selected at step (1), so we can apply the hash function(s) to
those values to determine certain components of the vector that identi   es
the reducers. other components of the vector are unknown, so t must
be sent to reducers for all vectors having any value in these unknown
components.

some examples of this general technique appear in the exercises.

here, we shall look only at the join r(a, b)        s(b, c)        t (c, d) as
an example. suppose that the relations r, s, and t have sizes r, s, and t,
respectively, and for simplicity, suppose p is the id203 that

1. an r-tuple and and s-tuple agree on b, and also the id203 that

2. an s-tuple and a t -tuple agree on c.

if we join r and s    rst, using the mapreduce algorithm of section 2.3.7,
then the communication cost is o(r + s), and the size of the intermediate join

50

chapter 2. mapreduce and the new software stack

r        s is prs. when we join this result with t , the communication of this
second mapreduce job is o(t + prs). thus, the entire communication cost of
the algorithm consisting of two 2-way joins is o(r + s + t + prs). if we instead
join s and t    rst, and then join r with the result, we get another algorithm
whose communication cost is o(r + s + t + pst).

a third way to take this join is to use a single mapreduce job that joins
the three relations at once. suppose that we plan to use k reducers for this
job. pick numbers b and c representing the number of buckets into which we
shall hash b- and c-values, respectively. let h be a hash function that sends
b-values into b buckets, and let g be another hash function that sends c-values
into c buckets. we require that bc = k; that is, each reducer corresponds to
a pair of buckets, one for the b-value and one for the c-value. the reducer
corresponding to bucket pair (i, j) is responsible for joining the tuples r(u, v),
s(v, w), and t (w, x) whenever h(v) = i and g(w) = j.

as a result, the map tasks that send tuples of r, s, and t to the reducers
that need them must send r- and t -tuples to more than one reducer. for an
s-tuple s(v, w), we know the b- and c-values, so we can send this tuple only to

the reducer for (cid:0)h(v), g(w)(cid:1). however, consider an r-tuple r(u, v). we know
it only needs to go to reducers that correspond to (cid:0)h(v), y(cid:1), for some y. but

we don   t know y; the value of c could be anything as far as we know. thus,
we must send r(u, v) to c reducers, since y could be any of the c buckets for
c-values. similarly, we must send the t -tuple t (w, x) to each of the reducers

(cid:0)z, g(w)(cid:1) for any z. there are b such reducers.

g(t.c) = 1

g(c) =

h(s.b) = 2 and g(s.c) = 1

0

1

2

3

0

1

2

3

h(b) =

h(r.b) = 2

figure 2.8: sixteen reducers together perform a 3-way join

example 2.9 : suppose that b = c = 4, so k = 16. the sixteen reducers can
be thought of as arranged in a rectangle, as suggested by fig. 2.8. there, we
see a hypothetical s-tuple s(v, w) for which h(v) = 2 and g(w) = 1. this
tuple is sent by its map task only to the reducer for key (2, 1). we also see

2.5. the communication cost model

51

computation cost of the 3-way join

each of the reducers must join of parts of the three relations, and it is
reasonable to ask whether this join can be taken in time that is linear
in the size of the input to that reduce task. while more complex joins
might not be computable in linear time, the join of our running example
can be executed at each reduce process e   ciently. first, create an index
on r.b, to organize the r-tuples received. likewise, create an index on
t.c for the t -tuples. then, consider each received s-tuple, s(v, w). use
the index on r.b to    nd all r-tuples with r.b = v and use the index on
t.c to    nd all t -tuples with t.c = w.

an r-tuple r(u, v). since h(v) = 2, this tuple is sent to all reducers (2, y), for
y = 1, 2, 3, 4. finally, we see a t -tuple t (w, x). since g(w) = 1, this tuple is
sent to all reducers (z, 1) for z = 1, 2, 3, 4. notice that these three tuples join,
and they meet at exactly one reducer, the reducer for key (2, 1).    

now, suppose that the sizes of r, s, and t are di   erent; recall we use r,
s, and t, respectively, for those sizes.
if we hash b-values to b buckets and
c-values to c buckets, where bc = k, then the total communication cost for
moving the tuples to the proper reducers is the sum of:

1. s to move each tuple s(v, w) once to the reducer (cid:0)h(v), g(w)(cid:1).
2. cr to move each tuple r(u, v) to the c reducers (cid:0)h(v), y(cid:1) for each of the c
3. bt to move each tuple t (w, x) to the b reducers (cid:0)z, g(w)(cid:1) for each of the

b possible values of z.

possible values of y.

there is also a cost r + s + t to make each tuple of each relation be input to
one of the map tasks. this cost is    xed, independent of b, c, and k.

we must select b and c, subject to the constraint bc = k, to minimize
s + cr + bt. we shall use the technique of lagrangean multipliers to    nd the
place where the function s + cr + bt       (bc     k) has its derivatives with respect
to b and c equal to 0. that is, we must solve the equations r       b = 0 and
t       c = 0. since r =   b and t =   c, we may multiply corresponding sides of
these equations to get rt =   2bc. since bc = k, we get rt =   2k, or    = prt/k.
thus, the minimum communication cost is obtained when c = t/   = pkt/r,
and b = r/   = pkr/t.
if we substitute these values into the formula s + cr + bt, we get s + 2   krt.
that is the communication cost for the reduce tasks, to which we must add
the cost s + r + t for the communication cost of the map tasks. the total

chapter 2. mapreduce and the new software stack

52
communication cost is thus r + 2s + t + 2   krt. in most circumstances, we can
neglect r + t, because it will be less than 2   krt, usually by a factor of o(   k).

example 2.10 : let us see under what circumstances the 3-way join has lower
communication cost than the cascade of two 2-way joins. to make matters
simple, let us assume that r, s, and t are all the same relation r, which
represents the    friends    relation in a social network like facebook. there are
roughly a billion subscribers on facebook, with an average of 300 friends each, so
relation r has r = 3    1011 tuples. suppose we want to compute r        r        r,
perhaps as part of a calculation to    nd the number of friends of friends of
friends each subscriber has, or perhaps just the person with the largest number
of friends of friends of friends.8 the cost of the 3-way join of r with itself is
4r + 2r   k; 3r represents the cost of the map tasks, and r + 2   kr2 is the cost
of the reduce tasks. since we assume r = 3   1011, this cost is 1, 2   1012 + 6  
1011   k.
now consider the communication cost of joining r with itself, and then
joining the result with r again. the map and reduce tasks for the    rst join each
have a cost of 2r, so the    rst join only has communication cost 4r = 1.2   1012.
but the size of r        r is large. we cannot say exactly how large, since friends
tend to fall into cliques, and therefore a person with 300 friends will have many
fewer than the maximum possible number of friends of friends, which is 90,000.
let us estimate conservatively that the size of r        r is not 300r, but only
30r, or 9    1012. the communication cost for the second join of (r        r)        r
is thus 1.8    1013 + 6    1011. the total cost of the two joins is therefore
1.2    1012 + 1.8    1013 + 6    1011 = 1.98    1013.
1.2    1012 + 6    1011   k

we must ask whether the cost of the 3-way join, which is

is less than 1.98    1013. that is so, provided 6    1011   k < 1.86    1013, or
   k < 31. that is, the 3-way join will be preferable provided we use no more
than 312 = 961 reducers.    

2.5.4 exercises for section 2.5

exercise 2.5.1 : what is the communication cost of each of the following
algorithms, as a function of the size of the relations, matrices, or vectors to
which they are applied?

(a) the matrix-vector multiplication algorithm of section 2.3.2.

(b) the union algorithm of section 2.3.6.

(c) the aggregation algorithm of section 2.3.8.

8this person, or more generally, people with large extended circles of friends, are good

people to use to start a marketing campaign by giving them free samples.

2.5. the communication cost model

53

star joins

a common structure for data mining of commercial data is the star join.
for example, a chain store like walmart keeps a fact table whose tu-
ples each represent a single sale. this relation looks like f (a1, a2, . . .),
where each attribute ai is a key representing one of the important com-
ponents of the sale, such as the purchaser, the item purchased, the store
branch, or the date. for each key attribute there is a dimension table
giving information about the participant. for instance, the dimension ta-
ble d(a1, b11, b12, . . .) might represent purchasers. a1 is the purchaser
id, the key for this relation. the b1i   s might give the purchaser   s name,
address, phone, and so on. typically, the fact table is much larger than
the dimension tables. for instance, there might be a fact table of a billion
tuples and ten dimension tables of a million tuples each.

analysts mine this data by asking analytic queries that typically join
the fact table with several of the dimension tables (a    star join   ) and then
aggregate the result into a useful form. for instance, an analyst might ask
   give me a table of sales of pants, broken down by region and color, for
each month of 2012.    under the communication-cost model of this section,
joining the fact table and dimension tables by a multiway join is almost
certain to be more e   cient than joining the relations in pairs. in fact, it
may make sense to store the fact table over however many compute nodes
are available, and replicate the dimension tables permanently in exactly
the same way as we would replicate them should we take the join of the
fact table and all the dimension tables.
in this special case, only the
key attributes (the a   s above) are hashed to buckets, and the number of
buckets for each key attribute is proportional to the size of its dimension
table.

(d) the matrix-multiplication algorithm of section 2.3.10.

! exercise 2.5.2 : suppose relations r, s, and t have sizes r, s, and t, respec-
tively, and we want to take the 3-way join r(a, b)        s(b, c)        t (a, c),
using k reducers. we shall hash values of attributes a, b, and c to a, b, and c
buckets, respectively, where abc = k. each reducer is associated with a vector
of buckets, one for each of the three hash functions. find, as a function of r, s,
t, and k, the values of a, b, and c that minimize the communication cost of the
algorithm.

! exercise 2.5.3 : suppose we take a star join of a fact table f (a1, a2, . . . , am)
with dimension tables di(ai, bi) for i = 1, 2, . . . , m. let there be k reducers,
each associated with a vector of buckets, one for each of the key attributes
a1, a2, . . . , am. suppose the number of buckets into which we hash ai is ai.

54

chapter 2. mapreduce and the new software stack

naturally, a1a2        am = k. finally, suppose each dimension table di has size
di, and the size of the fact table is much larger than any of these sizes. find
the values of the ai   s that minimize the cost of taking the star join as one
mapreduce operation.

2.6 complexity theory for mapreduce

now, we shall explore the design of mapreduce algorithms in more detail. sec-
tion 2.5 introduced the idea that communication between the map and reduce
tasks often accounts for the largest fraction of the time spent by these tasks.
here, we shall look at how the communication cost relates to other desiderata
for mapreduce algorithms, in particular our desire to shrink the wall-clock time
and to execute each reducer in main memory. recall that a    reducer    is the
execution of the reduce function on a single key and its associated value list.
the point of the exploration in this section is that for many problems there is a
spectrum of mapreduce algorithms requiring di   erent amounts of communica-
tion. moreover, the less communication an algorithm uses, the worse it may be
in other respects, including wall-clock time and the amount of main memory it
requires.

2.6.1 reducer size and replication rate

let us now introduce the two parameters that characterize families of mapre-
duce algorithms. the    rst is the reducer size, which we denote by q. this
parameter is the upper bound on the number of values that are allowed to ap-
pear in the list associated with a single key. reducer size can be selected with
at least two goals in mind.

1. by making the reducer size small, we can force there to be many reducers,
i.e., many di   erent keys according to which the problem input is divided
by the map tasks. if we also create many reduce tasks     even one for
each reducer     then there will be a high degree of parallelism, and we can
look forward to a low wall-clock time.

2. we can choose a reducer size su   ciently small that we are certain the
computation associated with a single reducer can be executed entirely in
the main memory of the compute node where its reduce task is located.
regardless of the computation done by the reducers, the running time
will be greatly reduced if we can avoid having to move data repeatedly
between main memory and disk.

the second parameter is the replication rate, denoted r. we de   ne r to
be the number of key-value pairs produced by all the map tasks on all the
inputs, divided by the number of inputs. that is, the replication rate is the
average communication from map tasks to reduce tasks (measured by counting
key-value pairs) per input.

2.6. complexity theory for mapreduce

55

example 2.11 : let us consider the one-pass matrix-multiplication algorithm
of section 2.3.10. suppose that all the matrices involved are n    n matrices.
then the replication rate r is equal to n. that fact is easy to see, since for
each element mij, there are n key-value pairs produced; these have all keys of
the form (i, k), for 1     k     n. likewise, for each element of the other matrix,
say njk, we produce n key-value pairs, each having one of the keys (i, k), for
1     i     n. in this case, not only is n the average number of key-value pairs
produced for an input element, but each input produces exactly this number of
pairs.

we also see that q, the required reducer size, is 2n. that is, for each key
(i, k), there are n key-value pairs representing elements mij of the    rst matrix
and another n key-value pairs derived from the elements njk of the second
matrix. while this pair of values represents only one particular algorithm for
one-pass id127, we shall see that it is part of a spectrum of
algorithms, and in fact represents an extreme point, where q is as small as can
be, and r is at its maximum. more generally, there is a tradeo    between r and
q, that can be expressed as qr     2n2.    

2.6.2 an example: similarity joins

to see the tradeo    between r and q in a realistic situation, we shall examine a
problem known as similarity join. in this problem, we are given a large set of
elements x and a similarity measure s(x, y) that tells how similar two elements
x and y of set x are. in chapter 3 we shall learn about the most important
notions of similarity and also learn some tricks that let us    nd similar pairs
quickly. but here, we shall consider only the raw form of the problem, where
we have to look at each pair of elements of x and determine their similarity by
applying the function s. we assume that s is symmetric, so s(x, y) = s(y, x),
but we assume nothing else about s. the output of the algorithm is those pairs
whose similarity exceeds a given threshold t.

for example, let us suppose we have a collection of one million images, each
of size one megabyte. thus, the dataset has size one terabyte. we shall not
try to describe the similarity function s, but it might, say, involve giving higher
values when images have roughly the same distribution of colors or when images
have corresponding regions with the same distribution of colors. the goal would
be to discover pairs of images that show the same type of object or scene. this
problem is extremely hard, but classifying by color distribution is generally of
some help toward that goal.

let us look at how we might do the computation using mapreduce to exploit
the natural parallelism found in this problem. the input is key-value pairs
(i, pi), where i is an id for the picture and pi is the picture itself. we want
to compare each pair of pictures, so let us use one key for each set of two id   s
{i, j}. there are approximately 5    1011 pairs of two id   s. we want each
key {i, j} to be associated with the two values pi and pj, so the input to the
corresponding reducer will be ({i, j}, [pi, pj]). then, the reduce function can

56

chapter 2. mapreduce and the new software stack

simply apply the similarity function s to the two pictures on its value list, that
is, compute s(pi, pj), and decide whether the similarity of the two pictures is
above threshold. the pair would be output if so.

alas, this algorithm will fail completely. the reducer size is small, since no
list has more than two values, or a total of 2mb of input. although we don   t
know exactly how the similarity function s operates, we can reasonably expect
that it will not require more than the available main memory. however, the
replication rate is 999,999, since for each picture we generate that number of
key-value pairs, one for each of the other pictures in the dataset. the total
number of bytes communicated from map tasks to reduce tasks is 1,000,000
(for the pictures) times 999,999 (for the replication), times 1,000,000 (for the
size of each picture). that   s 1018 bytes, or one exabyte. to communicate this
amount of data over gigabit ethernet would take 1010 seconds, or about 300
years.9

fortunately, this algorithm is only the extreme point in a spectrum of possi-
ble algorithms. we can characterize these algorithms by grouping pictures into
g groups, each of 106/g pictures.
the map function: take an input element (i, pi) and generate g     1 key-
value pairs. for each, the key is one of the sets {u, v}, where u is the group to
which picture i belongs, and v is one of the other groups. the associated value
is the pair (i, pi).
the reduce function: consider the key {u, v}. the associated value list
will have the 2    106/g elements (j, pj ), where j belongs to either group u or
group v. the reduce function takes each (i, pi) and (j, pj ) on this list, where
i and j belong to di   erent groups, and applies the similarity function s(pi, pj ).
in addition, we need to compare the pictures that belong to the same group,
but we don   t want to do the same comparison at each of the g     1 reducers
whose key contains a given group number. there are many ways to handle this
problem, but one way is as follows. compare the members of group u at the
reducer {u, u + 1}, where the    +1    is taken in the end-around sense. that is,
if u = g (i.e., u is the last group), then u + 1 is group 1. otherwise, u + 1 is the
group whose number is one greater than u.

we can compute the replication rate and reducer size as a function of the
number of groups g. each input element is turned into g     1 key-value pairs.
that is, the replication rate is g     1, or approximately r = g, since we suppose
that the number of groups is still fairly large. the reducer size is 2  106/g, since
that is the number of values on the list for each reducer. each value is about a
megabyte, so the number of bytes needed to store the input is 2    1012/g.
example 2.12 : if g is 1000, then the input consumes about 2gb. that   s
enough to hold everything in a typical main memory. moreover, the total

9in a typical cluster, there are many switches connecting subsets of the compute nodes, so
all the data does not need to go across a single gigabit switch. however, the total available
communication is still small enough that it is not feasible to implement this algorithm for the
scale of data we have hypothesized.

2.6. complexity theory for mapreduce

57

number of bytes communicated is now 106    999    106, or about 1015 bytes.
while that is still a huge amount of data to communicate, it is 1000 times
less than that of the obvious algorithm. moreover, there are still about half a
million reducers. since we are unlikely to have available that many compute
nodes, we can divide all the reducers into a smaller number of reduce tasks
and still keep all the compute nodes busy; i.e., we can get as much parallelism
as our computing cluster o   ers us.    

the computation cost for algorithms in this family is independent of the
number of groups g, as long as the input to each reducer    ts in main memory.
the reason is that the bulk of the computation is the application of function s
to the pairs of pictures. no matter what value g has, s is applied to each pair
once and only once. thus, although the work of algorithms in the family may
be divided among reducers in widely di   erent ways, all members of the family
do the same computation.

2.6.3 a graph model for mapreduce problems

in this section, we begin the study of a technique that will enable us to prove
lower bounds on the replication rate, as a function of reducer size for a number
of problems. our    rst step is to introduce a graph model of problems. for each
problem solvable by a mapreduce algorithm there is:

1. a set of inputs.

2. a set of outputs.

3. a many-many relationship between the inputs and outputs, which de-

scribes which inputs are necessary to produce which outputs.

example 2.13 : figure 2.9 shows the graph for the similarity-join problem
discussed in section 2.6.2, if there were four pictures rather than a million. the
inputs are the pictures, and the outputs are the six possible pairs of pictures.
each output is related to the two inputs that are members of its pair.    

example 2.14 : id127 presents a more complex graph. if we
multiply n    n matrices m and n to get matrix p , then there are 2n2 inputs,
mij and njk, and there are n2 outputs pik. each output pik is related to 2n
inputs: mi1, mi2, . . . , min and n1k, n2k, . . . , nnk. moreover, each input is related
to n outputs. for example, mij is related to pi1, pi2, . . . , pin. figure 2.10 shows
the input-output relationship for id127 for the simple case of
2    2 matrices, speci   cally

(cid:20) a b
c d (cid:21)(cid:20) e

g h (cid:21) = (cid:20) i

f

k

j

l (cid:21)

   

58

chapter 2. mapreduce and the new software stack

p 1

p 2

p 3

p 4

{      ,       }
p 1

p 2

{      ,       }
p 1

p 3

{      ,       }
p 1

p 4

{      ,       }
p 2

p 3

{      ,       }
p 2

p 4

{      ,       }
p 3 p 4

figure 2.9: input-output relationship for a similarity join

in the problems of examples 2.13 and 2.14, the inputs and outputs were
clearly all present. however, there are other problems where the inputs and/or
outputs may not all be present in any instance of the problem. an example
of such a problem is the natural join of r(a, b) and s(b, c) discussed in
section 2.3.7. we assume the attributes a, b, and c each have a    nite domain,
so there are only a    nite number of possible inputs and outputs. the inputs are
all possible r-tuples, those consisting of a value from the domain of a paired
with a value from the domain of b, and all possible s-tuples     pairs from the
domains of b and c. the outputs are all possible triples, with components from
the domains of a, b, and c in that order. the output (a, b, c) is connected to
two inputs, namely r(a, b) and s(b, c).

but in an instance of the join computation, only some of the possible inputs
will be present, and therefore only some of the possible outputs will be produced.
that fact does not in   uence the graph for the problem. we still need to know
how every possible output relates to inputs, whether or not that output is
produced in a given instance.

2.6.4 mapping schemas

now that we see how to represent problems addressable by mapreduce as
graphs, we can de   ne the requirements for a mapreduce algorithm to solve
a given problem. each such algorithm must have a mapping schema, which
expresses how outputs are produced by the various reducers used by the algo-
rithm. that is, a mapping schema for a given problem with a given reducer
size q is an assignment of inputs to one or more reducers, such that:

1. no reducer is assigned more than q inputs.

2.6. complexity theory for mapreduce

59

a

b

c

d

e

f

g

h

i

j

k

l

figure 2.10: input-output relationship for id127

2. for every output of the problem, there is at least one reducer that is
assigned all the inputs that are related to that output. we say this reducer
covers the output.

it can be argued that the existence of a mapping schema for any reducer size
is what distinguishes problems that can be solved by a single mapreduce job
from those that cannot.

example 2.15 : let us reconsider the    grouping    strategy we discussed in
connection with the similarity join in section 2.6.2. to generalize the problem,
suppose the input is p pictures, which we place in g equal-sized groups of p/g

inputs each. the number of outputs is (cid:0)p

2(cid:1), or approximately p2/2 outputs. a

reducer will get the inputs from two groups     that is 2p/g inputs     so the reducer
size we need is q = 2p/g. each picture is sent to the reducers corresponding to
the pairs consisting of its group and any of the g     1 other groups. thus, the
replication rate is g     1, or approximately g. if we replace g by the replication
rate r in q = 2p/g, we conclude that r = 2p/q. that is, the replication rate
is inversely proportional to the reducer size. that relationship is common; the
smaller the reducer size, the larger the replication rate, and therefore the higher
the communication.

this family of algorithms is described by a family of mapping schemas, one

in the mapping schema for q = 2p/g, there are (cid:0)g

for each possible q.
approximately g2/2 reducers. each reducer corresponds to a pair of groups,
and an input p is assigned to all the reducers whose pair includes the group of
p . thus, no reducer is assigned more than 2p/g inputs; in fact each reducer
is assigned exactly that number. moreover, every output is covered by some
reducer. speci   cally, if the output is a pair from two di   erent groups u and v,
then this output is covered by the reducer for the pair of groups {u, v}. if the

2(cid:1), or

60

chapter 2. mapreduce and the new software stack

output corresponds to inputs from only one group u, then the output is covered
by several reducers     those corresponding to the set of groups {u, v} for any
v 6= u. note that the algorithm we described has only one of these reducers
computing the output, but any of them could compute it.    

the fact that an output depends on a certain input means that when that
input is processed at the map task, there will be at least one key-value pair
generated to be used when computing that output. the value might not be
exactly the input (as was the case in example 2.15), but it is derived from
that input. what is important is that for every related input and output there
is a unique key-value pair that must be communicated. note that there is
technically never a need for more than one key-value pair for a given input and
output, because the input could be transmitted to the reducer as itself, and
whatever transformations on the input were applied by the map function could
instead be applied by the reduce function at the reducer for that output.

2.6.5 when not all inputs are present

example 2.15 describes a problem where we know every possible input is pre-
sent, because we can de   ne the input set to be those pictures that actually
exist in the dataset. however, as discussed at the end of section 2.6.3, there
are problems like computing the join, where the graph of inputs and outputs
describes inputs that might exist, and outputs that are only made when at least
one of the inputs exists in the dataset. in fact, for the join, both inputs related
to an output must exist if we are to make that output.

an algorithm for a problem where outputs can be missing still needs a
mapping schema. the justi   cation is that all inputs, or any subset of them,
might be present, so an algorithm without a mapping schema would not be
able to produce every possible output if all the inputs related to that output
happened to be present, and yet no reducer covered that output.

the only way the absence of some inputs makes a di   erence is that we
may wish to rethink the desired value of the reducer size q when we select an
algorithm from the family of possible algorithms. especially, if the value of q
we select is that number such that we can be sure the input will just    t in main
memory, then we may wish to increase q to take into account that some fraction
of the inputs are not really there.

example 2.16 : suppose that we know we can execute the reduce function
in main memory on a key and its associated list of q values. however, we also
know that only 5% of the possible inputs are really present in the data set.
then a mapping schema for reducer size q will really send about q/20 of the
inputs that exist to each reducer. put another way, we could use the algorithm
for reducer size 20q and expect that an average of q inputs will actually appear
on the list for each reducer. we can thus choose 20q as the reducer size, or since
there will be some randomness in the number of inputs actually appearing at

2.6. complexity theory for mapreduce

61

each reducer, we might wish to pick a slightly smaller value of reducer size, such
as 18q.    

2.6.6 lower bounds on replication rate

the family of similarity-join algorithms described in example 2.15 lets us trade
o    communication against the reducer size, and through reducer size to trade
communication against parallelism or against the ability to execute the reduce
function in main memory. how do we know we are getting the best possible
tradeo   ? we can only know we have the minimum possible communication if
we can prove a matching lower bound. using existence of a mapping schema as
the starting point, we can often prove such a lower bound. here is an outline
of the technique.

1. prove an upper bound on how many outputs a reducer with q inputs can
cover. call this bound g(q). this step can be di   cult, but for examples
like similarity join, it is actually quite simple.

2. determine the total number of outputs produced by the problem.

computed in step (2).

3. suppose that there are k reducers, and the ith reducer has qi < q inputs.
i=1 g(qi) must be no less than the number of outputs

observe that pk
4. manipulate the inequality from (3) to get a lower bound on pk

i=1 qi.
often, the trick used at this step is to replace some factors of qi by their
upper bound q, but leave a single factor of qi in the term for i.

5. since pk

i=1 qi is the total communication from map tasks to reduce tasks,
divide the lower bound from (4) on this quantity by the number of inputs.
the result is a lower bound on the replication rate.

example 2.17 : this sequence of steps may seem mysterious, but let us con-
sider the similarity join as an example that we hope will make things clear.
recall that in example 2.15 we gave an upper bound on the replication rate
r of r     2p/q, where p was the number of inputs and q was the reducer size.
we shall show a lower bound on r that is half that amount, which implies that,
although improvements to the algorithm might be possible, any reduction in
communication for a given reducer size will be by a factor of 2 at most.

for step (1), observe that if a reducer gets q inputs, it cannot cover more

than (cid:0)q
total of (cid:0)p

2(cid:1), or approximately q2/2 outputs. for step (2), we know there are a
2(cid:1), or approximately p2/2 outputs that each must be covered. the

inequality constructed at step (3) is thus

k

xi=1

i /2     p2/2
q2

62

chapter 2. mapreduce and the new software stack

or, multiplying both sides by 2,

k

xi=1

q2
i     p2

(2.1)

now, we must do the manipulation of step (4). following the hint, we note
that there are two factors of qi in each term on the left of equation (2.1), so
we replace one factor by q and leave the other as qi. since q     qi, we can only
increase the left side by doing so, and thus the inequality continues to hold:

or, dividing by q:

q

k

xi=1

qi     p2

k

xi=1

qi     p2/q

(2.2)

the    nal step, which is step (5), is to divide both sides of equation 2.2 by
i=1 qi)/p is equal
to the replication rate, and the right side becomes p/q. that is, we have proved
the lower bound on r:

p, the number of inputs. as a result, the left side, which is (pk

as claimed, this shows that the family of algorithms from example 2.15 all have
a replication rate that is at most twice the lowest possible replication rate.    

r     p/q

2.6.7 case study: id127

in this section we shall apply the lower-bound technique to one-pass matrix-
multiplication algorithms. we saw one such algorithm in section 2.3.10, but
that is only an extreme case of a family of possible algorithms. in particular,
for that algorithm, a reducer corresponds to a single element of the output
matrix. just as we grouped inputs in the similarity-join problem to reduce the
communication at the expense of a larger reducer size, we can group rows and
columns of the two input matrices into bands. each pair consisting of a band of
rows of the    rst matrix and a band of columns of the second matrix is used by
one reducer to produce a square of elements of the output matrix. an example
is suggested by fig. 2.11.

in more detail, suppose we want to compute m n = p , and all three matrices
are n    n. group the rows of m into g bands of n/g rows each, and group the
columns of n into g bands of n/g columns each. this grouping is as suggested
by fig. 2.11. keys correspond to two groups (bands), one from m and one
from n .
the map function: for each element of m , the map function generates g
key-value pairs. the value in each case is the element itself, together with its

2.6. complexity theory for mapreduce

63

=

figure 2.11: dividing matrices into bands to reduce communication

row and column number so it can be identi   ed by the reduce function. the
key is the group to which the element belongs, paired with any of the groups
of the matrix n . similarly, for each element of n , the map function generates
g key-value pairs. the key is the group of that element paired with any of the
groups of m , and the value is the element itself plus its row and column.
the reduce function: the reducer corresponding to the key (i, j), where i
is a group of m and j is a group of n , gets a value list consisting of all the
elements in the ith band of m and the jth band of n .
it thus has all the
values it needs to compute the elements of p whose row is one of those rows
comprising the ith band of m and whose column is one of those comprising the
jth band of n . for instance, fig. 2.11 suggests the third group of m and the
fourth group of n , combining to compute a square of p at the reducer (3, 4).

each reducer gets n(n/g) elements from each of the two matrices, so q =
2n2/g. the replication rate is g, since each element of each matrix is sent to
g reducers. that is, r = g. combining r = g with q = 2n2/g we can conclude
that r = 2n2/q. that is, just as for similarity join, the replication rate varies
inversely with the reducer size.

it turns out that this upper bound on replication rate is also a lower bound.
that is, we cannot do better than the family of algorithms we described above
in a single round of mapreduce. interestingly, we shall see that we can get a
lower total communication for the same reducer size, if we use two passes of
mapreduce as we discussed in section 2.3.9. we shall not give the complete
proof of the lower bound, but will suggest the important elements.

for step (1) we need to get an upper bound on how many outputs a reducer
of size q can cover. first, notice that if a reducer gets some of the elements in
a row of m , but not all of them, then the elements of that row are useless; the
reducer cannot produce any output in that row of p . similarly, if a reducer
receives some but not all of a column of n , these inputs are also useless. thus,
we may assume that the best mapping schema will send to each reducer some
number of full rows of m and some number of full columns of n . this reducer

64

chapter 2. mapreduce and the new software stack

is then capable of producing output element pik if and only if it has received
the entire ith row of m and the entire kth column of n . the remainder of the
argument for step (1) is to prove that the largest number of outputs are covered
when the reducer receives the same number of rows as columns. we leave this
part as an exercise.

however, assuming a reducer receives k rows of m and k columns of n ,
then q = 2nk, and k2 outputs are covered. that is, g(q), the maximum number
of outputs covered by a reducer that receives q inputs, is q2/4n2.

for step (2), we know the number of outputs is n2. in step (3) we observe
that if there are k reducers, with the ith reducer receiving qi     q inputs, then

or

k

xi=1

i /4n2     n2
q2

k

xi=1

i     4n4
q2

from this inequality, you can derive

r     2n2/q

we leave the algebraic manipulation, which is similar to that in example 2.17,
as an exercise.

now, let us consider the generalization of the two-pass matrix-multiplication
algorithm that we described in section 2.3.9. first, notice that we could have
designed the    rst pass to use one reducer for each triple (i, j, k). this reducer
would get only the two elements mij and njk. we can generalize this idea to
use reducers that get larger sets of elements from each matrix; these sets of
elements form squares within their respective matrices. the idea is suggested
by fig. 2.12. we may divide the rows and columns of both input matrices m
and n into g groups of n/g rows or columns each. the intersections of the
groups partition each matrix into g2 squares of n2/g2 elements each.

the square of m corresponding to set of rows i and set of columns j com-
bines with the square of n corresponding to set of rows j and set of columns
k. these two squares compute some of the terms that are needed to produce
the square of the output matrix p that has set of rows i and set of columns k.
however, these two squares do not compute the full value of these elements of
p ; rather they produce a contribution to the sum. other pairs of squares, one
from m and one from n , contribute to the same square of p . these contribu-
tions are suggested in fig. 2.12. there, we see how all the squares of m with
a    xed value for set of rows i pair with all the squares of n that have a    xed
value for the set of columns k by letting the set j vary.

so in the    rst pass, we compute the products of the square (i, j) of m with
the square (j, k) of n , for all i, j, and k. then, in the second pass, for each

2.6. complexity theory for mapreduce

65

=

figure 2.12: partitioning matrices into squares for a two-pass mapreduce al-
gorithm

i and k we sum the products over all possible sets j. in more detail, the    rst
mapreduce job does the following.
the map function: the keys are triples of sets of rows and/or column num-
bers (i, j, k). suppose the element mij belongs to group of rows i and group
of columns j. then from mij we generate g key-value pairs with value equal to
mij, together with its row and column numbers, i and j, to identify the matrix
element. there is one key-value pair for each key (i, j, k), where k can be any
of the g groups of columns of n . similarly, from element njk of n , if j belongs
to group j and k to group k, the map function generates g key-value pairs
with value consisting of njk, j, and k, and with keys (i, j, k) for any group i.
the reduce function: the reducer corresponding to (i, j, k) receives as
input all the elements mij where i is in i and j is in j, and it also receives all
the elements njk, where j is in j and k is in k. it computes

xijk = xj in j

mijnjk

for all i in i and k in k.

notice that the replication rate for the    rst mapreduce job is g, and the to-
tal communication is therefore 2gn2. also notice that each reducer gets 2n2/g2
inputs, so q = 2n2/g2. equivalently, g = np2/q. thus, the total communica-
tion 2gn2 can be written in terms of q as 2   2n3/   q.

66

chapter 2. mapreduce and the new software stack

the second mapreduce job is simple; it sums up the xijk   s over all sets j.
the map function: we assume that the map tasks execute at whatever
compute nodes executed the reduce tasks of the previous job. thus, no com-
munication is needed between the jobs. the map function takes as input one
element xijk, which we assume the previous reducers have left labeled with i
and k so we know to what element of matrix p this term contributes. one
key-value pair is generated. the key is (i, k) and the value is xijk.
the reduce function: the reduce function simply sums the values associ-
ated with key (i, k) to compute the output element pik.

the communication between the map and reduce tasks of the second job is
gn2, since there are n possible values of i, n possible values of k, and g possible
values of the set j, and each xijk is communicated only once. if we recall from
our analysis of the    rst mapreduce job that g = np2/q, we can write the
communication for the second job as n2g =    2n3/   q. this amount is exactly
half the communication for the    rst job, so the total communication for the
two-pass algorithm is 3   2n3/   q. although we shall not examine this point
here, it turns out that we can do slightly better if we divide the matrices m
and n not into squares but into rectangles that are twice as long on one side
as on the other. in that case, we get the slightly smaller constant 4 in place
of 3   2 = 4.24, and we get a two-pass algorithm with communication equal to
4n3/   q.

now, recall that the communication cost we computed for the one-pass
algorithm is 4n4/q. we may as well assume q is less than n2, or else we can
just use a serial algorithm at one compute node and not use mapreduce at all.
thus, n3/   q is smaller than n4/q, and if q is close to its minimum possible
value of 2n,10 then the two-pass algorithm beats the one-pass algorithm by a
factor of o(   n) in communication. moreover, we can expect the di   erence
in communication to be the signi   cant cost di   erence. both algorithms do
the same o(n3) arithmetic operations. the two-pass method naturally has
more overhead managing tasks than does the one-job method. on the other
hand, the second pass of the two-pass algorithm applies a reduce function
that is associative and commutative. thus, it might be possible to save some
communication cost by using a combiner on that pass.

2.6.8 exercises for section 2.6

exercise 2.6.1 : describe the graphs that model the following problems.

(a) the multiplication of an n    n matrix by a vector of length n.
(b) the natural join of r(a, b) and s(b, c), where a, b, and c have do-

mains of sizes a, b, and c, respectively.

10if q is less than 2n, then a reducer cannot get even one row and one column, and therefore

cannot compute any outputs at all.

2.7. summary of chapter 2

67

(c) the grouping and aggregation on the relation r(a, b), where a is the
grouping attribute and b is aggregated by the max operation. assume
a and b have domains of size a and b, respectively.

! exercise 2.6.2 : provide the details of the proof that a one-pass matrix-
multiplication algorithm requires replication rate at least r     2n2/q, including:
(a) the proof that, for a    xed reducer size, the maximum number of outputs
are covered by a reducer when that reducer receives an equal number of
rows of m and columns of n .

(b) the algebraic manipulation needed, starting with pk

i=1 q2

i     4n4.

!! exercise 2.6.3 : suppose our inputs are bit strings of length b, and the outputs

correspond to pairs of strings at hamming distance 1.11

(a) prove that a reducer of size q can cover at most (q/2) log2 q outputs.
(b) use part (a) to show the lower bound on replication rate: r     b/ log2 q.
(c) show that there are algorithms with replication rate as given by part (b)

for the cases q = 2, q = 2b, and q = 2b/2.

2.7 summary of chapter 2

    cluster computing: a common architecture for very large-scale applica-
tions is a cluster of compute nodes (processor chip, main memory, and
disk). compute nodes are mounted in racks, and the nodes on a rack are
connected, typically by gigabit ethernet. racks are also connected by a
high-speed network or switch.

    distributed file systems: an architecture for very large-scale    le sys-
tems has developed recently. files are composed of chunks of about 64
megabytes, and each chunk is replicated several times, on di   erent com-
pute nodes or racks.

    mapreduce: this programming system allows one to exploit parallelism
inherent in cluster computing, and manages the hardware failures that
can occur during a long computation on many nodes. many map tasks
and many reduce tasks are managed by a master process. tasks on a
failed compute node are rerun by the master.

    the map function: this function is written by the user.

it takes a
collection of input objects and turns each into zero or more key-value
pairs. keys are not necessarily unique.

11bit strings have hamming distance 1 if they di   er in exactly one bit position. you may

look ahead to section 3.5.6 for the general de   nition.

68

chapter 2. mapreduce and the new software stack

    the reduce function: a mapreduce programming system sorts all the
key-value pairs produced by all the map tasks, forms all the values asso-
ciated with a given key into a list and distributes key-list pairs to reduce
tasks. each reduce task combines the elements on each list, by applying
the function written by the user. the results produced by all the reduce
tasks form the output of the mapreduce process.

    reducers: it is often convenient to refer to the application of the reduce

function to a single key and its associated value list as a    reducer.   

    hadoop: this programming system is an open-source implementation of a
distributed    le system (hdfs, the hadoop distributed file system) and
mapreduce (hadoop itself). it is available through the apache founda-
tion.

    managing compute-node failures: mapreduce systems support restart
of tasks that fail because their compute node, or the rack containing
that node, fail. because map and reduce tasks deliver their output only
after they    nish, it is possible to restart a failed task without concern for
possible repetition of the e   ects of that task. it is necessary to restart the
entire job only if the node at which the master executes fails.

    applications of mapreduce: while not all parallel algorithms are suitable
for implementation in the mapreduce framework, there are simple im-
plementations of matrix-vector and matrix-id127. also,
the principal operators of relational algebra are easily implemented in
mapreduce.

    work   ow systems: mapreduce has been generalized to systems that sup-
port any acyclic collection of functions, each of which can be instantiated
by any number of tasks, each responsible for executing that function on
a portion of the data.

    recursive work   ows: when implementing a recursive collection of func-
tions, it is not always possible to preserve the ability to restart any failed
task, because recursive tasks may have produced output that was con-
sumed by another task before the failure. a number of schemes for check-
pointing parts of the computation to allow restart of single tasks, or restart
all tasks from a recent point, have been proposed.

    communication-cost : many applications of mapreduce or similar sys-
tems do very simple things for each task. then, the dominant cost is
usually the cost of transporting data from where it is created to where
it is used.
in these cases, e   ciency of a mapreduce algorithm can be
estimated by calculating the sum of the sizes of the inputs to all the
tasks.

2.8. references for chapter 2

69

    multiway joins: it is sometimes more e   cient to replicate tuples of the
relations involved in a join and have the join of three or more relations
computed as a single mapreduce job. the technique of lagrangean mul-
tipliers can be used to optimize the degree of replication for each of the
participating relations.

    star joins: analytic queries often involve a very large fact table joined
with smaller dimension tables. these joins can always be done e   ciently
by the multiway-join technique. an alternative is to distribute the fact
table and replicate the dimension tables permanently, using the same
strategy as would be used if we were taking the multiway join of the fact
table and every dimension table.

    replication rate and reducer size:

it is often convenient to measure
communication by the replication rate, which is the communication per
input. also, the reducer size is the maximum number of inputs associated
with any reducer. for many problems, it is possible to derive a lower
bound on replication rate as a function of the reducer size.

    representing problems as graphs: it is possible to represent many prob-
lems that are amenable to mapreduce computation by a graph in which
nodes represent inputs and outputs. an output is connected to all the
inputs that are needed to compute that output.

    mapping schemas: given the graph of a problem, and given a reducer size,
a mapping schema is an assignment of the inputs to one or more reducers
so that no reducer is assigned more inputs than the reducer size permits,
and yet for every output there is some reducer that gets all the inputs
needed to compute that output. the requirement that there be a mapping
schema for any mapreduce algorithm is a good expression of what makes
mapreduce algorithms di   erent from general parallel computations.

    id127 by mapreduce: there is a family of one-pass map-
reduce algorithms that performs multiplication of n    n matrices with
the minimum possible replication rate r = 2n2/q, where q is the reducer
size. on the other hand, a two-pass mapreduce algorithm for the same
problem with the same reducer size can use up to a factor of n less com-
munication.

2.8 references for chapter 2

gfs, the google file system, was described in [10]. the paper on google   s
mapreduce is [8]. information about hadoop and hdfs can be found at [11].
more detail on relations and relational algebra can be found in [16].

clustera is covered in [9]. hyracks (previously called hyrax) is from [4].
the dryad system [13] has similar capabilities, but requires user creation of

70

chapter 2. mapreduce and the new software stack

parallel tasks. that responsibility was automated through the introduction of
dryadlinq [17]. for a discussion of cluster implementation of recursion, see
[1]. pregel is from [14].

a di   erent approach to recursion was taken in haloop [5]. there, recursion
is seen as an iteration, with the output of one round being input to the next
round. e   ciency is obtained by managing the location of the intermediate data
and the tasks that implement each round.

there are a number of other systems built on a distributed    le system and/or
mapreduce, which have not been covered here, but may be worth knowing
about.
[6] describes bigtable, a google implementation of an object store of
very large size. a somewhat di   erent direction was taken at yahoo! with pnuts
[7]. the latter supports a limited form of transaction processing, for example.
pig [15] is an implementation of relational algebra on top of hadoop. sim-

ilarly, hive [12] implements a restricted form of sql on top of hadoop.

the communication-cost model for mapreduce algorithms and the optimal
implementations of multiway joins is from [3]. the material on replication rate,
reducer size, and their relationship is from [2]. solutions to exercises 2.6.2 and
2.6.3 can be found there.

1. f.n. afrati, v. borkar, m. carey, a. polyzotis, and j.d. ullman,    clus-
ter computing, recursion, and datalog,    to appear in proc. datalog 2.0
workshop, elsevier, 2011.

2. f.n. afrati, a. das sarma, s. salihoglu, and j.d. ullman,    upper and
lower bounds on the cost of a mapreduce computation.    to appear in
proc. intl. conf. on very large databases, 2013. also available as corr,
abs/1206.4377.

3. f.n. afrati and j.d. ullman,    optimizing joins in a mapreduce environ-
ment,    proc. thirteenth intl. conf. on extending database technology,
2010.

4. v. borkar and m. carey,    hyrax: demonstrating a new foundation for

data-parallel computation,   

http://asterix.ics.uci.edu/pub/hyraxdemo.pdf

univ. of california, irvine, 2010.

5. y. bu, b. howe, m. balazinska, and m. ernst,    haloop: e   cient iter-
ative data processing on large clusters,    proc. intl. conf. on very large
databases, 2010.

6. f. chang, j. dean, s. ghemawat, w.c. hsieh, d.a. wallach, m. burrows,
t. chandra, a. fikes, and r.e. gruber,    bigtable: a distributed storage
system for structured data,    acm transactions on computer systems
26:2, pp. 1   26, 2008.

2.8. references for chapter 2

71

7. b.f. cooper, r. ramakrishnan, u. srivastava, a. silberstein, p. bohan-
non, h.-a. jacobsen, n. puz, d. weaver, and r. yerneni,    pnuts: ya-
hoo!   s hosted data serving platform,    pvldb 1:2, pp. 1277   1288, 2008.

8. j. dean and s. ghemawat,    mapreduce: simpli   ed data processing on

large clusters,    comm. acm 51:1, pp. 107   113, 2008.

9. d.j. dewitt, e. paulson, e. robinson, j.f. naughton, j. royalty, s.
shankar, and a. krioukov,    clustera: an integrated computation and
data management system,    pvldb 1:1, pp. 28   41, 2008.

10. s. ghemawat, h. gobio   , and s.-t. leung,    the google    le system,   

19th acm symposium on operating systems principles, 2003.

11. hadoop.apache.org, apache foundation.

12. hadoop.apache.org/hive, apache foundation.

13. m. isard, m. budiu, y. yu, a. birrell, and d. fetterly.    dryad: dis-
tributed data-parallel programs from sequential building blocks,    proceed-
ings of the 2nd acm sigops/eurosys european conference on com-
puter systems, pp. 59   72, acm, 2007.

14. g. malewicz, m.n. austern, a.j.c. sik, j.c. denhert, h. horn, n. leiser,
and g. czajkowski,    pregel: a system for large-scale graph processing,   
proc. acm sigmod conference, 2010.

15. c. olston, b. reed, u. srivastava, r. kumar, and a. tomkins,    pig latin:
a not-so-foreign language for data processing,    proc. acm sigmod con-
ference, pp. 1099   1110, 2008.

16. j.d. ullman and j. widom, a first course in database systems, third

edition, prentice-hall, upper saddle river, nj, 2008.

17. y. yu, m. isard, d. fetterly, m. budiu, i. erlingsson, p.k. gunda, and
j. currey,    dryadlinq: a system for general-purpose distributed data-
parallel computing using a high-level language,    osdi, pp. 1   14, usenix
association, 2008.

72

chapter 2. mapreduce and the new software stack

chapter 3

finding similar items

a fundamental data-mining problem is to examine data for    similar    items. we
shall take up applications in section 3.1, but an example would be looking at a
collection of web pages and    nding near-duplicate pages. these pages could be
plagiarisms, for example, or they could be mirrors that have almost the same
content but di   er in information about the host and about other mirrors.

we begin by phrasing the problem of similarity as one of    nding sets with
a relatively large intersection. we show how the problem of    nding textually
similar documents can be turned into such a set problem by the technique known
as    shingling.    then, we introduce a technique called    minhashing,    which
compresses large sets in such a way that we can still deduce the similarity of
the underlying sets from their compressed versions. other techniques that work
when the required degree of similarity is very high are covered in section 3.9.
another important problem that arises when we search for similar items of
any kind is that there may be far too many pairs of items to test each pair for
their degree of similarity, even if computing the similarity of any one pair can be
made very easy. that concern motivates a technique called    locality-sensitive
hashing,    for focusing our search on pairs that are most likely to be similar.

finally, we explore notions of    similarity    that are not expressible as inter-
section of sets. this study leads us to consider the theory of distance measures
in arbitrary spaces. it also motivates a general framework for locality-sensitive
hashing that applies for other de   nitions of    similarity.   

3.1 applications of near-neighbor search

we shall focus initially on a particular notion of    similarity   : the similarity of
sets by looking at the relative size of their intersection. this notion of similarity
is called    jaccard similarity,    and will be introduced in section 3.1.1. we then
examine some of the uses of    nding similar sets. these include    nding textually
similar documents and collaborative    ltering by    nding similar customers and
similar products. in order to turn the problem of textual similarity of documents

73

74

chapter 3. finding similar items

into one of set intersection, we use a technique called    shingling,    which is
introduced in section 3.2.

3.1.1 jaccard similarity of sets

the jaccard similarity of sets s and t is |s     t|/|s     t|, that is, the ratio
of the size of the intersection of s and t to the size of their union. we shall
denote the jaccard similarity of s and t by sim(s, t ).

example 3.1 : in fig. 3.1 we see two sets s and t . there are three elements
in their intersection and a total of eight elements that appear in s or t or both.
thus, sim(s, t ) = 3/8.    

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

s

 
 
 
 

t

figure 3.1: two sets with jaccard similarity 3/8

3.1.2 similarity of documents

an important class of problems that jaccard similarity addresses well is that
of    nding textually similar documents in a large corpus such as the web or a
collection of news articles. we should understand that the aspect of similarity
we are looking at here is character-level similarity, not    similar meaning,    which
requires us to examine the words in the documents and their uses. that problem
is also interesting but is addressed by other techniques, which we hinted at in
section 1.3.1. however, textual similarity also has important uses. many of
these involve    nding duplicates or near duplicates. first, let us observe that
testing whether two documents are exact duplicates is easy; just compare the
two documents character-by-character, and if they ever di   er then they are not
the same. however, in many applications, the documents are not identical, yet
they share large portions of their text. here are some examples:

3.1. applications of near-neighbor search

75

plagiarism

finding plagiarized documents tests our ability to    nd textual similarity. the
plagiarizer may extract only some parts of a document for his own. he may alter
a few words and may alter the order in which sentences of the original appear.
yet the resulting document may still contain 50% or more of the original. no
simple process of comparing documents character by character will detect a
sophisticated plagiarism.

mirror pages

it is common for important or popular web sites to be duplicated at a number
of hosts, in order to share the load. the pages of these mirror sites will be
quite similar, but are rarely identical. for instance, they might each contain
information associated with their particular host, and they might each have
links to the other mirror sites but not to themselves. a related phenomenon
is the appropriation of pages from one class to another. these pages might
include class notes, assignments, and lecture slides. similar pages might change
the name of the course, year, and make small changes from year to year. it
is important to be able to detect similar pages of these kinds, because search
engines produce better results if they avoid showing two pages that are nearly
identical within the    rst page of results.

articles from the same source

it is common for one reporter to write a news article that gets distributed,
say through the associated press, to many newspapers, which then publish
the article on their web sites. each newspaper changes the article somewhat.
they may cut out paragraphs, or even add material of their own. they most
likely will surround the article by their own logo, ads, and links to other articles
at their site. however, the core of each newspaper   s page will be the original
article. news aggregators, such as google news, try to    nd all versions of such
an article, in order to show only one, and that task requires    nding when two
web pages are textually similar, although not identical.1

3.1.3 id185 as a similar-sets problem

another class of applications where similarity of sets is very important is called
collaborative    ltering, a process whereby we recommend to users items that were
liked by other users who have exhibited similar tastes. we shall investigate
collaborative    ltering in detail in section 9.3, but for the moment let us see
some common examples.

1news aggregation also involves    nding articles that are about the same topic, even though
not textually similar. this problem too can yield to a similarity search, but it requires
techniques other than jaccard similarity of sets.

76

chapter 3. finding similar items

on-line purchases

amazon.com has millions of customers and sells millions of items. its database
records which items have been bought by which customers. we can say two cus-
tomers are similar if their sets of purchased items have a high jaccard similarity.
likewise, two items that have sets of purchasers with high jaccard similarity
will be deemed similar. note that, while we might expect mirror sites to have
jaccard similarity above 90%, it is unlikely that any two customers have jac-
card similarity that high (unless they have purchased only one item). even a
jaccard similarity like 20% might be unusual enough to identify customers with
similar tastes. the same observation holds for items; jaccard similarities need
not be very high to be signi   cant.

collaborative    ltering requires several tools, in addition to    nding similar
customers or items, as we discuss in chapter 9. for example, two amazon
customers who like science-   ction might each buy many science-   ction books,
but only a few of these will be in common. however, by combining similarity-
   nding with id91 (chapter 7), we might be able to discover that science-
   ction books are mutually similar and put them in one group. then, we can
get a more powerful notion of customer-similarity by asking whether they made
purchases within many of the same groups.

movie ratings

net   ix records which movies each of its customers rented, and also the ratings
assigned to those movies by the customers. we can see movies as similar if they
were rented or rated highly by many of the same customers, and see customers
as similar if they rented or rated highly many of the same movies. the same
observations that we made for amazon above apply in this situation: similarities
need not be high to be signi   cant, and id91 movies by genre will make
things easier.

when our data consists of ratings rather than binary decisions (bought/did
not buy or liked/disliked), we cannot rely simply on sets as representations of
customers or items. some options are:

1. ignore low-rated customer/movie pairs; that is, treat these events as if

the customer never watched the movie.

2. when comparing customers, imagine two set elements for each movie,
   liked    and    hated.    if a customer rated a movie highly, put the    liked   
for that movie in the customer   s set. if they gave a low rating to a movie,
put    hated    for that movie in their set. then, we can look for high jaccard
similarity among these sets. we can do a similar trick when comparing
movies.

3. if ratings are 1-to-5-stars, put a movie in a customer   s set n times if
they rated the movie n-stars. then, use jaccard similarity for bags when
measuring the similarity of customers. the jaccard similarity for bags

3.2. shingling of documents

77

b and c is de   ned by counting an element n times in the intersection if
n is the minimum of the number of times the element appears in b and
c. in the union, we count the element the sum of the number of times it
appears in b and in c.2

example 3.2 : the bag-similarity of bags {a, a, a, b} and {a, a, b, b, c} is 1/3.
the intersection counts a twice and b once, so its size is 3. the size of the
union of two bags is always the sum of the sizes of the two bags, or 9 in this
case. since the highest possible jaccard similarity for bags is 1/2, the score
of 1/3 indicates the two bags are quite similar, as should be apparent from an
examination of their contents.    

3.1.4 exercises for section 3.1

exercise 3.1.1 : compute the jaccard similarities of each pair of the following
three sets: {1, 2, 3, 4}, {2, 3, 5, 7}, and {2, 4, 6}.
exercise 3.1.2 : compute the jaccard bag similarity of each pair of the fol-
lowing three bags: {1, 1, 1, 2}, {1, 1, 2, 2, 3}, and {1, 2, 3, 4}.

!! exercise 3.1.3 : suppose we have a universal set u of n elements, and we
choose two subsets s and t at random, each with m of the n elements. what
is the expected value of the jaccard similarity of s and t ?

3.2 shingling of documents

the most e   ective way to represent documents as sets, for the purpose of iden-
tifying lexically similar documents is to construct from the document the set
of short strings that appear within it. if we do so, then documents that share
pieces as short as sentences or even phrases will have many common elements
in their sets, even if those sentences appear in di   erent orders in the two docu-
ments. in this section, we introduce the simplest and most common approach,
shingling, as well as an interesting variation.

3.2.1

k-shingles

a document is a string of characters. de   ne a k-shingle for a document to be
any substring of length k found within the document. then, we may associate

2although the union for bags is normally (e.g., in the sql standard) de   ned to have the
sum of the number of copies in the two bags, this de   nition causes some inconsistency with
the jaccard similarity for sets. under this de   nition of bag union, the maximum jaccard
similarity is 1/2, not 1, since the union of a set with itself has twice as many elements as the
intersection of the same set with itself. if we prefer to have the jaccard similarity of a set
with itself be 1, we can rede   ne the union of bags to have each element appear the maximum
number of times it appears in either of the two bags. this change does not simply double the
similarity in each case, but it also gives a reasonable measure of bag similarity.

78

chapter 3. finding similar items

with each document the set of k-shingles that appear one or more times within
that document.

example 3.3 : suppose our document d is the string abcdabd, and we pick
k = 2. then the set of 2-shingles for d is {ab, bc, cd, da, bd}.
note that the substring ab appears twice within d, but appears only once
as a shingle. a variation of shingling produces a bag, rather than a set, so each
shingle would appear in the result as many times as it appears in the document.
however, we shall not use bags of shingles here.    

there are several options regarding how white space (blank, tab, newline,
etc.) is treated. it probably makes sense to replace any sequence of one or more
white-space characters by a single blank. that way, we distinguish shingles that
cover two or more words from those that do not.

example 3.4 : if we use k = 9, but eliminate whitespace altogether, then we
would see some lexical similarity in the sentences    the plane was ready for
touch down   . and    the quarterback scored a touchdown   . however, if we
retain the blanks, then the    rst has shingles touch dow and ouch down, while
the second has touchdown. if we eliminated the blanks, then both would have
touchdown.    

3.2.2 choosing the shingle size

we can pick k to be any constant we like. however, if we pick k too small, then
we would expect most sequences of k characters to appear in most documents.
if so, then we could have documents whose shingle-sets had high jaccard simi-
larity, yet the documents had none of the same sentences or even phrases. as
an extreme example, if we use k = 1, most web pages will have most of the
common characters and few other characters, so almost all web pages will have
high similarity.

how large k should be depends on how long typical documents are and how

large the set of typical characters is. the important thing to remember is:

    k should be picked large enough that the id203 of any given shingle

appearing in any given document is low.

thus, if our corpus of documents is emails, picking k = 5 should be    ne.
to see why, suppose that only letters and a general white-space character ap-
pear in emails (although in practice, most of the printable ascii characters
if so, then there would be 275 =
can be expected to appear occasionally).
14,348,907 possible shingles. since the typical email is much smaller than 14
million characters long, we would expect k = 5 to work well, and indeed it does.
however, the calculation is a bit more subtle. surely, more than 27 charac-
ters appear in emails, however, all characters do not appear with equal proba-
bility. common letters and blanks dominate, while    z    and other letters that

3.2. shingling of documents

79

have high point-value in scrabble are rare. thus, even short emails will have
many 5-shingles consisting of common letters, and the chances of unrelated
emails sharing these common shingles is greater than would be implied by the
calculation in the paragraph above. a good rule of thumb is to imagine that
there are only 20 characters and estimate the number of k-shingles as 20k. for
large documents, such as research articles, choice k = 9 is considered safe.

3.2.3 hashing shingles

instead of using substrings directly as shingles, we can pick a hash function
that maps strings of length k to some number of buckets and treat the resulting
bucket number as the shingle. the set representing a document is then the
set of integers that are bucket numbers of one or more k-shingles that appear
in the document. for instance, we could construct the set of 9-shingles for a
document and then map each of those 9-shingles to a bucket number in the
range 0 to 232     1. thus, each shingle is represented by four bytes instead
of nine. not only has the data been compacted, but we can now manipulate
(hashed) shingles by single-word machine operations.

notice that we can di   erentiate documents better if we use 9-shingles and
hash them down to four bytes than to use 4-shingles, even though the space used
to represent a shingle is the same. the reason was touched upon in section 3.2.2.
if we use 4-shingles, most sequences of four bytes are unlikely or impossible to
   nd in typical documents. thus, the e   ective number of di   erent shingles is
much less than 232     1. if, as in section 3.2.2, we assume only 20 characters are
frequent in english text, then the number of di   erent 4-shingles that are likely
to occur is only (20)4 = 160,000. however, if we use 9-shingles, there are many
more than 232 likely shingles. when we hash them down to four bytes, we can
expect almost any sequence of four bytes to be possible, as was discussed in
section 1.3.2.

3.2.4 shingles built from words

an alternative form of shingle has proved e   ective for the problem of identifying
similar news articles, mentioned in section 3.1.2. the exploitable distinction for
this problem is that the news articles are written in a rather di   erent style than
are other elements that typically appear on the page with the article. news
articles, and most prose, have a lot of stop words (see section 1.3.1), the most
common words such as    and,       you,       to,    and so on. in many applications,
we want to ignore stop words, since they don   t tell us anything useful about
the article, such as its topic.

however, for the problem of    nding similar news articles, it was found that
de   ning a shingle to be a stop word followed by the next two words, regardless
of whether or not they were stop words, formed a useful set of shingles. the
advantage of this approach is that the news article would then contribute more
shingles to the set representing the web page than would the surrounding ele-

80

chapter 3. finding similar items

ments. recall that the goal of the exercise is to    nd pages that had the same
articles, regardless of the surrounding elements. by biasing the set of shingles
in favor of the article, pages with the same article and di   erent surrounding
material have higher jaccard similarity than pages with the same surrounding
material but with a di   erent article.

example 3.5 : an ad might have the simple text    buy sudzo.    however, a
news article with the same idea might read something like    a spokesperson
for the sudzo corporation revealed today that studies have shown it is
good for people to buy sudzo products.    here, we have italicized all the
likely stop words, although there is no set number of the most frequent words
that should be considered stop words. the    rst three shingles made from a
stop word and the next two following are:

a spokesperson for
for the sudzo
the sudzo corporation

there are nine shingles from the sentence, but none from the    ad.       

3.2.5 exercises for section 3.2

exercise 3.2.1 : what are the    rst ten 3-shingles in the    rst sentence of sec-
tion 3.2?

exercise 3.2.2 : if we use the stop-word-based shingles of section 3.2.4, and
we take the stop words to be all the words of three or fewer letters, then what
are the shingles in the    rst sentence of section 3.2?

exercise 3.2.3 : what is the largest number of k-shingles a document of n
bytes can have? you may assume that the size of the alphabet is large enough
that the number of possible strings of length k is at least as n.

3.3 similarity-preserving summaries of sets

sets of shingles are large. even if we hash them to four bytes each, the space
needed to store a set is still roughly four times the space taken by the document.
if we have millions of documents, it may well not be possible to store all the
shingle-sets in main memory.3

our goal in this section is to replace large sets by much smaller represen-
tations called    signatures.    the important property we need for signatures is
that we can compare the signatures of two sets and estimate the jaccard sim-
ilarity of the underlying sets from the signatures alone. it is not possible that

3there is another serious concern: even if the sets    t in main memory, the number of pairs
may be too great for us to evaluate the similarity of each pair. we take up the solution to
this problem in section 3.4.

3.3. similarity-preserving summaries of sets

81

the signatures give the exact similarity of the sets they represent, but the esti-
mates they provide are close, and the larger the signatures the more accurate
the estimates. for example, if we replace the 200,000-byte hashed-shingle sets
that derive from 50,000-byte documents by signatures of 1000 bytes, we can
usually get within a few percent.

3.3.1 matrix representation of sets

before explaining how it is possible to construct small signatures from large
sets, it is helpful to visualize a collection of sets as their characteristic matrix.
the columns of the matrix correspond to the sets, and the rows correspond to
elements of the universal set from which elements of the sets are drawn. there
is a 1 in row r and column c if the element for row r is a member of the set for
column c. otherwise the value in position (r, c) is 0.

element s1
1
0
0
1
0

a
b
c
d
e

s2
0
0
1
0
0

s3
0
1
0
1
1

s4
1
0
1
1
0

figure 3.2: a matrix representing four sets

example 3.6 : in fig. 3.2 is an example of a matrix representing sets chosen
from the universal set {a, b, c, d, e}. here, s1 = {a, d}, s2 = {c}, s3 = {b, d, e},
and s4 = {a, c, d}. the top row and leftmost columns are not part of the matrix,
but are present only to remind us what the rows and columns represent.    

it is important to remember that the characteristic matrix is unlikely to be
the way the data is stored, but it is useful as a way to visualize the data. for one
reason not to store data as a matrix, these matrices are almost always sparse
(they have many more 0   s than 1   s) in practice. it saves space to represent a
sparse matrix of 0   s and 1   s by the positions in which the 1   s appear. for another
reason, the data is usually stored in some other format for other purposes.

as an example, if rows are products, and columns are customers, represented
by the set of products they bought, then this data would really appear in a
database table of purchases. a tuple in this table would list the item, the
purchaser, and probably other details about the purchase, such as the date and
the credit card used.

3.3.2 minhashing

the signatures we desire to construct for sets are composed of the results of a
large number of calculations, say several hundred, each of which is a    minhash   

82

chapter 3. finding similar items

of the characteristic matrix. in this section, we shall learn how a minhash is
computed in principle, and in later sections we shall see how a good approxi-
mation to the minhash is computed in practice.

to minhash a set represented by a column of the characteristic matrix, pick
a permutation of the rows. the minhash value of any column is the number of
the    rst row, in the permuted order, in which the column has a 1.

example 3.7 : let us suppose we pick the order of rows beadc for the matrix
of fig. 3.2. this permutation de   nes a minhash function h that maps sets to
rows. let us compute the minhash value of set s1 according to h. the    rst
column, which is the column for set s1, has 0 in row b, so we proceed to row e,
the second in the permuted order. there is again a 0 in the column for s1, so
we proceed to row a, where we    nd a 1. thus. h(s1) = a.

element s1
0
0
1
1
0

b
e
a
d
c

s2
0
0
0
0
1

s3
1
1
0
1
0

s4
0
0
1
1
1

figure 3.3: a permutation of the rows of fig. 3.2

although it is not physically possible to permute very large characteristic
matrices, the minhash function h implicitly reorders the rows of the matrix of
fig. 3.2 so it becomes the matrix of fig. 3.3. in this matrix, we can read o   
the values of h by scanning from the top until we come to a 1. thus, we see
that h(s2) = c, h(s3) = b, and h(s4) = a.    

3.3.3 minhashing and jaccard similarity

there is a remarkable connection between minhashing and jaccard similarity
of the sets that are minhashed.

    the id203 that the minhash function for a random permutation of
rows produces the same value for two sets equals the jaccard similarity
of those sets.

to see why, we need to picture the columns for those two sets. if we restrict
ourselves to the columns for sets s1 and s2, then rows can be divided into three
classes:

1. type x rows have 1 in both columns.

2. type y rows have 1 in one of the columns and 0 in the other.

3.3. similarity-preserving summaries of sets

83

3. type z rows have 0 in both columns.

since the matrix is sparse, most rows are of type z. however, it is the ratio
of the numbers of type x and type y rows that determine both sim(s1, s2)
and the id203 that h(s1) = h(s2). let there be x rows of type x and y
rows of type y . then sim(s1, s2) = x/(x + y). the reason is that x is the size
of s1     s2 and x + y is the size of s1     s2.
now, consider the id203 that h(s1) = h(s2). if we imagine the rows
permuted randomly, and we proceed from the top, the id203 that we shall
meet a type x row before we meet a type y row is x/(x + y). but if the
   rst row from the top other than type z rows is a type x row, then surely
h(s1) = h(s2). on the other hand, if the    rst row other than a type z row
that we meet is a type y row, then the set with a 1 gets that row as its minhash
value. however the set with a 0 in that row surely gets some row further down
the permuted list. thus, we know h(s1) 6= h(s2) if we    rst meet a type y row.
we conclude the id203 that h(s1) = h(s2) is x/(x + y), which is also the
jaccard similarity of s1 and s2.

3.3.4 minhash signatures

again think of a collection of sets represented by their characteristic matrix m .
to represent sets, we pick at random some number n of permutations of the
rows of m . perhaps 100 permutations or several hundred permutations will do.
call the minhash functions determined by these permutations h1, h2, . . . , hn.
from the column representing set s, construct the minhash signature for s, the
vector [h1(s), h2(s), . . . , hn(s)]. we normally represent this list of hash-values
as a column. thus, we can form from matrix m a signature matrix, in which
the ith column of m is replaced by the minhash signature for (the set of) the
ith column.

note that the signature matrix has the same number of columns as m but
only n rows. even if m is not represented explicitly, but in some compressed
form suitable for a sparse matrix (e.g., by the locations of its 1   s), it is normal
for the signature matrix to be much smaller than m .

3.3.5 computing minhash signatures

it is not feasible to permute a large characteristic matrix explicitly. even picking
a random permutation of millions or billions of rows is time-consuming, and
the necessary sorting of the rows would take even more time. thus, permuted
matrices like that suggested by fig. 3.3, while conceptually appealing, are not
implementable.

fortunately, it is possible to simulate the e   ect of a random permutation by
a random hash function that maps row numbers to as many buckets as there
are rows. a hash function that maps integers 0, 1, . . . , k     1 to bucket numbers
0 through k    1 typically will map some pairs of integers to the same bucket and
leave other buckets un   lled. however, the di   erence is unimportant as long as

84

chapter 3. finding similar items

k is large and there are not too many collisions. we can maintain the    ction
that our hash function h    permutes    row r to position h(r) in the permuted
order.

thus, instead of picking n random permutations of rows, we pick n randomly
chosen hash functions h1, h2, . . . , hn on the rows. we construct the signature
matrix by considering each row in their given order. let sig(i, c) be the element
of the signature matrix for the ith hash function and column c. initially, set
sig(i, c) to     for all i and c. we handle row r by doing the following:

1. compute h1(r), h2(r), . . . , hn(r).

2. for each column c do the following:

(a) if c has 0 in row r, do nothing.

(b) however, if c has 1 in row r, then for each i = 1, 2, . . . , n set sig(i, c)

to the smaller of the current value of sig(i, c) and hi(r).

row

0
1
2
3
4

s1
1
0
0
1
0

s2
0
0
1
0
0

s3
0
1
0
1
1

s4
1
0
1
1
0

x + 1 mod 5

3x + 1 mod 5

1
2
3
4
0

1
4
2
0
3

figure 3.4: hash functions computed for the matrix of fig. 3.2

example 3.8 : let us reconsider the characteristic matrix of fig. 3.2, which
we reproduce with some additional data as fig. 3.4. we have replaced the
letters naming the rows by integers 0 through 4. we have also chosen two hash
functions: h1(x) = x+1 mod 5 and h2(x) = 3x+1 mod 5. the values of these
two functions applied to the row numbers are given in the last two columns of
fig. 3.4. notice that these simple hash functions are true permutations of the
rows, but a true permutation is only possible because the number of rows, 5, is
a prime. in general, there will be collisions, where two rows get the same hash
value.

now, let us simulate the algorithm for computing the signature matrix.

initially, this matrix consists of all       s:
s2

s3

s1

s4
h1                
h2                

first, we consider row 0 of fig. 3.4. we see that the values of h1(0) and
h2(0) are both 1. the row numbered 0 has 1   s in the columns for sets s1 and

3.3. similarity-preserving summaries of sets

85

s4, so only these columns of the signature matrix can change. as 1 is less than
   , we do in fact change both values in the columns for s1 and s4. the current
estimate of the signature matrix is thus:

h1
h2

s3

s2

s1
s4
1         1
1         1

now, we move to the row numbered 1 in fig. 3.4. this row has 1 only in
s3, and its hash values are h1(1) = 2 and h2(1) = 4. thus, we set sig(1, 3) to 2
and sig(2, 3) to 4. all other signature entries remain as they are because their
columns have 0 in the row numbered 1. the new signature matrix:

h1
h2

s2

s1
s3
1     2
1     4

s4
1
1

the row of fig. 3.4 numbered 2 has 1   s in the columns for s2 and s4, and
its hash values are h1(2) = 3 and h2(2) = 2. we could change the values in the
signature for s4, but the values in this column of the signature matrix, [1, 1], are
each less than the corresponding hash values [3, 2]. however, since the column
for s2 still has       s, we replace it by [3, 2], resulting in:

s1
1
1

s2
3
2

s3
2
4

s4
1
1

h1
h2

next comes the row numbered 3 in fig. 3.4. here, all columns but s2 have
1, and the hash values are h1(3) = 4 and h2(3) = 0. the value 4 for h1 exceeds
what is already in the signature matrix for all the columns, so we shall not
change any values in the    rst row of the signature matrix. however, the value
0 for h2 is less than what is already present, so we lower sig(2, 1), sig(2, 3) and
sig(2, 4) to 0. note that we cannot lower sig(2, 2) because the column for s2 in
fig. 3.4 has 0 in the row we are currently considering. the resulting signature
matrix:

s1
1
0

s2
3
2

s3
2
0

s4
1
0

h1
h2

finally, consider the row of fig. 3.4 numbered 4. h1(4) = 0 and h2(4) = 3.
since row 4 has 1 only in the column for s3, we only compare the current
signature column for that set, [2, 0] with the hash values [0, 3]. since 0 < 2, we
change sig(1, 3) to 0, but since 3 > 0 we do not change sig(2, 3). the    nal
signature matrix is:

s1
1
0

s2
3
2

s3
0
0

s4
1
0

h1
h2

86

chapter 3. finding similar items

we can estimate the jaccard similarities of the underlying sets from this
signature matrix. notice that columns 1 and 4 are identical, so we guess that
sim(s1, s4) = 1.0. if we look at fig. 3.4, we see that the true jaccard similarity
of s1 and s4 is 2/3. remember that the fraction of rows that agree in the
signature matrix is only an estimate of the true jaccard similarity, and this
example is much too small for the law of large numbers to assure that the
estimates are close. for additional examples, the signature columns for s1 and
s3 agree in half the rows (true similarity 1/4), while the signatures of s1 and
s2 estimate 0 as their jaccard similarity (the correct value).    

3.3.6 exercises for section 3.3

exercise 3.3.1 : verify the theorem from section 3.3.3, which relates the jac-
card similarity to the id203 of minhashing to equal values, for the partic-
ular case of fig. 3.2.

(a) compute the jaccard similarity of each of the pairs of columns in fig. 3.2.

! (b) compute, for each pair of columns of that    gure, the fraction of the 120
permutations of the rows that make the two columns hash to the same
value.

exercise 3.3.2 : using the data from fig. 3.4, add to the signatures of the
columns the values of the following hash functions:

(a) h3(x) = 2x + 4 mod 5.

(b) h4(x) = 3x     1 mod 5.

element s1
0
0
1
0
0
1

0
1
2
3
4
5

s2
1
1
0
0
0
0

s3
0
0
0
1
1
0

s4
1
0
1
0
1
0

figure 3.5: matrix for exercise 3.3.3

exercise 3.3.3 : in fig. 3.5 is a matrix with six rows.

(a) compute the minhash signature for each column if we use the following
three hash functions: h1(x) = 2x + 1 mod 6; h2(x) = 3x + 2 mod 6;
h3(x) = 5x + 2 mod 6.

3.4. locality-sensitive hashing for documents

87

(b) which of these hash functions are true permutations?

(c) how close are the estimated jaccard similarities for the six pairs of columns

to the true jaccard similarities?

! exercise 3.3.4 : now that we know jaccard similarity is related to the proba-
bility that two sets minhash to the same value, reconsider exercise 3.1.3. can
you use this relationship to simplify the problem of computing the expected
jaccard similarity of randomly chosen sets?

! exercise 3.3.5 : prove that if the jaccard similarity of two columns is 0, then

minhashing always gives a correct estimate of the jaccard similarity.

!! exercise 3.3.6 : one might expect that we could estimate the jaccard simi-
larity of columns without using all possible permutations of rows. for example,
we could only allow cyclic permutations; i.e., start at a randomly chosen row
r, which becomes the    rst in the order, followed by rows r + 1, r + 2, and so
on, down to the last row, and then continuing with the    rst row, second row,
and so on, down to row r     1. there are only n such permutations if there are
n rows. however, these permutations are not su   cient to estimate the jaccard
similarity correctly. give an example of a two-column matrix where averaging
over all the cyclic permutations does not give the jaccard similarity.

! exercise 3.3.7 : suppose we want to use a mapreduce framework to compute
minhash signatures. if the matrix is stored in chunks that correspond to some
columns, then it is quite easy to exploit parallelism. each map task gets some
of the columns and all the hash functions, and computes the minhash signatures
of its given columns. however, suppose the matrix were chunked by rows, so
that a map task is given the hash functions and a set of rows to work on. design
map and reduce functions to exploit mapreduce with data in this form.

3.4 locality-sensitive hashing for documents

even though we can use minhashing to compress large documents into small
signatures and preserve the expected similarity of any pair of documents, it
still may be impossible to    nd the pairs with greatest similarity e   ciently. the
reason is that the number of pairs of documents may be too large, even if there
are not too many documents.

example 3.9 : suppose we have a million documents, and we use signatures
of length 250. then we use 1000 bytes per document for the signatures, and
the entire data    ts in a gigabyte     less than a typical main memory of a laptop.

however, there are (cid:0)1,000,000

microsecond to compute the similarity of two signatures, then it takes almost
six days to compute all the similarities on that laptop.    

(cid:1) or half a trillion pairs of documents. if it takes a

2

88

chapter 3. finding similar items

if our goal is to compute the similarity of every pair, there is nothing we
can do to reduce the work, although parallelism can reduce the elapsed time.
however, often we want only the most similar pairs or all pairs that are above
some lower bound in similarity. if so, then we need to focus our attention only
on pairs that are likely to be similar, without investigating every pair. there is
a general theory of how to provide such focus, called locality-sensitive hashing
(lsh) or near-neighbor search.
in this section we shall consider a speci   c form
of lsh, designed for the particular problem we have been studying: documents,
represented by shingle-sets, then minhashed to short signatures. in section 3.6
we present the general theory of locality-sensitive hashing and a number of
applications and related techniques.

3.4.1 lsh for minhash signatures

one general approach to lsh is to    hash    items several times, in such a way that
similar items are more likely to be hashed to the same bucket than dissimilar
items are. we then consider any pair that hashed to the same bucket for any
of the hashings to be a candidate pair. we check only the candidate pairs for
similarity. the hope is that most of the dissimilar pairs will never hash to the
same bucket, and therefore will never be checked. those dissimilar pairs that
do hash to the same bucket are false positives; we hope these will be only a
small fraction of all pairs. we also hope that most of the truly similar pairs
will hash to the same bucket under at least one of the hash functions. those
that do not are false negatives; we hope these will be only a small fraction of
the truly similar pairs.

if we have minhash signatures for the items, an e   ective way to choose the
hashings is to divide the signature matrix into b bands consisting of r rows
each. for each band, there is a hash function that takes vectors of r integers
(the portion of one column within that band) and hashes them to some large
number of buckets. we can use the same hash function for all the bands, but
we use a separate bucket array for each band, so columns with the same vector
in di   erent bands will not hash to the same bucket.

example 3.10 : figure 3.6 shows part of a signature matrix of 12 rows divided
into four bands of three rows each. the second and fourth of the explicitly
shown columns each have the column vector [0, 2, 1] in the    rst band, so they
will de   nitely hash to the same bucket in the hashing for the    rst band. thus,
regardless of what those columns look like in the other three bands, this pair
of columns will be a candidate pair. it is possible that other columns, such as
the    rst two shown explicitly, will also hash to the same bucket according to
the hashing of the    rst band. however, since their column vectors are di   erent,
[1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the
chances of an accidental collision to be very small. we shall normally assume
that two vectors hash to the same bucket if and only if they are identical.

two columns that do not agree in band 1 have three other chances to become
a candidate pair; they might be identical in any one of these other bands.

3.4. locality-sensitive hashing for documents

89

. . .

1 0 0 0 2
3 2 1 2 2
0 1 3 1 1

. . .

band 1

band 2

band 3

band 4

figure 3.6: dividing a signature matrix into four bands of three rows per band

however, observe that the more similar two columns are, the more likely it is
that they will be identical in some band. thus, intuitively the banding strategy
makes similar columns much more likely to be candidate pairs than dissimilar
pairs.    

3.4.2 analysis of the banding technique

suppose we use b bands of r rows each, and suppose that a particular pair of
documents have jaccard similarity s. recall from section 3.3.3 that the prob-
ability the minhash signatures for these documents agree in any one particular
row of the signature matrix is s. we can calculate the id203 that these
documents (or rather their signatures) become a candidate pair as follows:

1. the id203 that the signatures agree in all rows of one particular

band is sr.

2. the id203 that the signatures disagree in at least one row of a par-

ticular band is 1     sr.

3. the id203 that the signatures disagree in at least one row of each

of the bands is (1     sr)b.

4. the id203 that the signatures agree in all the rows of at least one

band, and therefore become a candidate pair, is 1     (1     sr)b.

it may not be obvious, but regardless of the chosen constants b and r, this
function has the form of an s-curve, as suggested in fig. 3.7. the threshold, that
is, the value of similarity s at which the id203 of becoming a candidate
is 1/2, is a function of b and r. the threshold is roughly where the rise is
the steepest, and for large b and r there we    nd that pairs with similarity
above the threshold are very likely to become candidates, while those below the
threshold are unlikely to become candidates     exactly the situation we want.

90

chapter 3. finding similar items

id203
of becoming
a candidate

0

jaccard similarity
 of documents

1

figure 3.7: the s-curve

an approximation to the threshold is (1/b)1/r. for example, if b = 16 and
r = 4, then the threshold is approximately at s = 1/2, since the 4th root of
1/16 is 1/2.

example 3.11 : let us consider the case b = 20 and r = 5. that is, we suppose
we have signatures of length 100, divided into twenty bands of    ve rows each.
figure 3.8 tabulates some of the values of the function 1     (1     s5)20. notice
that the threshold, the value of s at which the curve has risen halfway, is just
slightly more than 0.5. also notice that the curve is not exactly the ideal step
function that jumps from 0 to 1 at the threshold, but the slope of the curve
in the middle is signi   cant. for example, it rises by more than 0.6 going from
s = 0.4 to s = 0.6, so the slope in the middle is greater than 3.

s
.2
.3
.4
.5
.6
.7
.8

1     (1     sr)b
.006
.047
.186
.470
.802
.975
.9996

figure 3.8: values of the s-curve for b = 20 and r = 5

for example, at s = 0.8, 1     (0.8)5 is about 0.672. if you raise this number
to the 20th power, you get about 0.00035. subtracting this fraction from 1

3.4. locality-sensitive hashing for documents

91

yields 0.99965. that is, if we consider two documents with 80% similarity, then
in any one band, they have only about a 33% chance of agreeing in all    ve rows
and thus becoming a candidate pair. however, there are 20 bands and thus 20
chances to become a candidate. only roughly one in 3000 pairs that are as high
as 80% similar will fail to become a candidate pair and thus be a false negative.
   

3.4.3 combining the techniques

we can now give an approach to    nding the set of candidate pairs for similar
documents and then discovering the truly similar documents among them. it
must be emphasized that this approach can produce false negatives     pairs of
similar documents that are not identi   ed as such because they never become
a candidate pair. there will also be false positives     candidate pairs that are
evaluated, but are found not to be su   ciently similar.

1. pick a value of k and construct from each document the set of k-shingles.

optionally, hash the k-shingles to shorter bucket numbers.

2. sort the document-shingle pairs to order them by shingle.

3. pick a length n for the minhash signatures. feed the sorted list to the
algorithm of section 3.3.5 to compute the minhash signatures for all the
documents.

4. choose a threshold t that de   nes how similar documents have to be in
order for them to be regarded as a desired    similar pair.    pick a number
of bands b and a number of rows r such that br = n, and the threshold
t is approximately (1/b)1/r. if avoidance of false negatives is important,
you may wish to select b and r to produce a threshold lower than t; if
speed is important and you wish to limit false positives, select b and r to
produce a higher threshold.

5. construct candidate pairs by applying the lsh technique of section 3.4.1.

6. examine each candidate pair   s signatures and determine whether the frac-

tion of components in which they agree is at least t.

7. optionally, if the signatures are su   ciently similar, go to the documents
themselves and check that they are truly similar, rather than documents
that, by luck, had similar signatures.

3.4.4 exercises for section 3.4
exercise 3.4.1 : evaluate the s-curve 1    (1    sr)b for s = 0.1, 0.2, . . . , 0.9, for
the following values of r and b:

    r = 3 and b = 10.

92

chapter 3. finding similar items

    r = 6 and b = 20.
    r = 5 and b = 50.

! exercise 3.4.2 : for each of the (r, b) pairs in exercise 3.4.1, compute the
threshold, that is, the value of s for which the value of 1   (1   sr)b is exactly 1/2.
how does this value compare with the estimate of (1/b)1/r that was suggested
in section 3.4.2?

! exercise 3.4.3 : use the techniques explained in section 1.3.5 to approximate
the s-curve 1     (1     sr)b when sr is very small.
! exercise 3.4.4 : suppose we wish to implement lsh by mapreduce. speci   -
cally, assume chunks of the signature matrix consist of columns, and elements
are key-value pairs where the key is the column number and the value is the
signature itself (i.e., a vector of values).

(a) show how to produce the buckets for all the bands as output of a single
mapreduce process. hint : remember that a map function can produce
several key-value pairs from a single element.

(b) show how another mapreduce process can convert the output of (a) to
a list of pairs that need to be compared. speci   cally, for each column i,
there should be a list of those columns j > i with which i needs to be
compared.

3.5 distance measures

we now take a short detour to study the general notion of distance measures.
the jaccard similarity is a measure of how close sets are, although it is not
really a distance measure. that is, the closer sets are, the higher the jaccard
similarity. rather, 1 minus the jaccard similarity is a distance measure, as we
shall see; it is called the jaccard distance.

however, jaccard distance is not the only measure of closeness that makes
sense. we shall examine in this section some other distance measures that have
applications. then, in section 3.6 we see how some of these distance measures
also have an lsh technique that allows us to focus on nearby points without
comparing all points. other applications of distance measures will appear when
we study id91 in chapter 7.

3.5.1 de   nition of a distance measure

suppose we have a set of points, called a space. a distance measure on this
space is a function d(x, y) that takes two points in the space as arguments and
produces a real number, and satis   es the following axioms:

1. d(x, y)     0 (no negative distances).

3.5. distance measures

93

2. d(x, y) = 0 if and only if x = y (distances are positive, except for the

distance from a point to itself).

3. d(x, y) = d(y, x) (distance is symmetric).

4. d(x, y)     d(x, z) + d(z, y) (the triangle inequality).

the triangle inequality is the most complex condition. it says, intuitively, that
to travel from x to y, we cannot obtain any bene   t if we are forced to travel via
some particular third point z. the triangle-inequality axiom is what makes all
distance measures behave as if distance describes the length of a shortest path
from one point to another.

3.5.2 euclidean distances

the most familiar distance measure is the one we normally think of as    dis-
tance.    an n-dimensional euclidean space is one where points are vectors of n
real numbers. the conventional distance measure in this space, which we shall
refer to as the l2-norm, is de   ned:

d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = vuut
xi=1

n

(xi     yi)2

that is, we square the distance in each dimension, sum the squares, and take
the positive square root.

it is easy to verify the    rst three requirements for a distance measure are
satis   ed. the euclidean distance between two points cannot be negative, be-
cause the positive square root is intended. since all squares of real numbers are
nonnegative, any i such that xi 6= yi forces the distance to be strictly positive.
on the other hand, if xi = yi for all i, then the distance is clearly 0. symmetry
follows because (xi     yi)2 = (yi     xi)2. the triangle inequality requires a good
deal of algebra to verify. however, it is well understood to be a property of
euclidean space: the sum of the lengths of any two sides of a triangle is no less
than the length of the third side.

there are other distance measures that have been used for euclidean spaces.
for any constant r, we can de   ne the lr-norm to be the distance measure d
de   ned by:

d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = (

xi=1

|xi     yi|r)1/r

n

the case r = 2 is the usual l2-norm just mentioned. another common distance
measure is the l1-norm, or manhattan distance. there, the distance between
two points is the sum of the magnitudes of the di   erences in each dimension.
it is called    manhattan distance    because it is the distance one would have to

94

chapter 3. finding similar items

travel between points if one were constrained to travel along grid lines, as on
the streets of a city such as manhattan.

another interesting distance measure is the l   -norm, which is the limit
as r approaches in   nity of the lr-norm. as r gets larger, only the dimension
with the largest di   erence matters, so formally, the l   -norm is de   ned as the
maximum of |xi     yi| over all dimensions i.
example 3.12 : consider the two-dimensional euclidean space (the custom-
ary plane) and the points (2, 7) and (6, 4). the l2-norm gives a distance

of p(2     6)2 + (7     4)2 =    42 + 32 = 5. the l1-norm gives a distance of
|2     6| + |7     4| = 4 + 3 = 7. the l   -norm gives a distance of

max(|2     6|,|7     4|) = max(4, 3) = 4

   

3.5.3 jaccard distance

as mentioned at the beginning of the section, we de   ne the jaccard distance
of sets by d(x, y) = 1     sim(x, y). that is, the jaccard distance is 1 minus the
ratio of the sizes of the intersection and union of sets x and y. we must verify
that this function is a distance measure.

1. d(x, y) is nonnegative because the size of the intersection cannot exceed

the size of the union.

2. d(x, y) = 0 if x = y, because x     x = x     x = x. however, if x 6= y, then
the size of x     y is strictly less than the size of x     y, so d(x, y) is strictly
positive.

3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e.,

x     y = y     x and x     y = y     x.

4. for the triangle inequality, recall from section 3.3.3 that sim(x, y) is the
id203 a random minhash function maps x and y to the same value.
thus, the jaccard distance d(x, y) is the id203 that a random min-
hash function does not send x and y to the same value. we can therefore
translate the condition d(x, y)     d(x, z) + d(z, y) to the statement that if
h is a random minhash function, then the id203 that h(x) 6= h(y)
is no greater than the sum of the id203 that h(x) 6= h(z) and the
id203 that h(z) 6= h(y). however, this statement is true because
whenever h(x) 6= h(y), at least one of h(x) and h(y) must be di   erent
from h(z). they could not both be h(z), because then h(x) and h(y)
would be the same.

.

3.5. distance measures

95

3.5.4 cosine distance

the cosine distance makes sense in spaces that have dimensions, including eu-
clidean spaces and discrete versions of euclidean spaces, such as spaces where
points are vectors with integer components or boolean (0 or 1) components. in
such a space, points may be thought of as directions. we do not distinguish be-
tween a vector and a multiple of that vector. then the cosine distance between
two points is the angle that the vectors to those points make. this angle will
be in the range 0 to 180 degrees, regardless of how many dimensions the space
has.

we can calculate the cosine distance by    rst computing the cosine of the
angle, and then applying the arc-cosine function to translate to an angle in the
0-180 degree range. given two vectors x and y, the cosine of the angle between
them is the dot product x.y divided by the l2-norms of x and y (i.e., their
euclidean distances from the origin). recall that the dot product of vectors

i=1 xiyi.

[x1, x2, . . . , xn].[y1, y2, . . . , yn] is pn
example 3.13 : let our two vectors be x = [1, 2,   1] and = [2, 1, 1]. the dot
product x.y is 1    2 + 2    1 + (   1)    1 = 3. the l2-norm of both vectors is
   6. for example, x has l2-norm p12 + 22 + (   1)2 =    6. thus, the cosine of
the angle between x and y is 3/(   6   6) or 1/2. the angle whose cosine is 1/2
is 60 degrees, so that is the cosine distance between x and y.    

we must show that the cosine distance is indeed a distance measure. we
have de   ned it so the values are in the range 0 to 180, so no negative distances
are possible. two vectors have angle 0 if and only if they are the same direction.4
symmetry is obvious: the angle between x and y is the same as the angle
between y and x. the triangle inequality is best argued by physical reasoning.
one way to rotate from x to y is to rotate to z and thence to y. the sum of
those two rotations cannot be less than the rotation directly from x to y.

3.5.5 id153

this distance makes sense when points are strings. the distance between two
strings x = x1x2        xn and y = y1y2        ym is the smallest number of insertions
and deletions of single characters that will convert x to y.

example 3.14 : the id153 between the strings x = abcde and y =
acfdeg is 3. to convert x to y:

1. delete b.

2. insert f after c.

4notice that to satisfy the second axiom, we have to treat vectors that are multiples of
one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. if we regarded these
as di   erent vectors, we would give them distance 0 and thus violate the condition that only
d(x, x) is 0.

96

chapter 3. finding similar items

3. insert g after e.

no sequence of fewer than three insertions and/or deletions will convert x to y.
thus, d(x, y) = 3.    

another way to de   ne and calculate the id153 d(x, y) is to compute
a longest common subsequence (lcs) of x and y. an lcs of x and y is a
string that is constructed by deleting positions from x and y, and that is as
long as any string that can be constructed that way. the id153 d(x, y)
can be calculated as the length of x plus the length of y minus twice the length
of their lcs.

example 3.15 : the strings x = abcde and y = acfdeg from example 3.14
have a unique lcs, which is acde. we can be sure it is the longest possible,
because it contains every symbol appearing in both x and y. fortunately, these
common symbols appear in the same order in both strings, so we are able to
use them all in an lcs. note that the length of x is 5, the length of y is 6, and
the length of their lcs is 4. the id153 is thus 5 + 6     2    4 = 3, which
agrees with the direct calculation in example 3.14.
for another example, consider x = aba and y = bab. their id153 is
2. for example, we can convert x to y by deleting the    rst a and then inserting
b at the end. there are two lcs   s: ab and ba. each can be obtained by
deleting one symbol from each string. as must be the case for multiple lcs   s
of the same pair of strings, both lcs   s have the same length. therefore, we
may compute the id153 as 3 + 3     2    2 = 2.    

id153 is a distance measure. surely no id153 can be negative,
and only two identical strings have an id153 of 0. to see that edit
distance is symmetric, note that a sequence of insertions and deletions can be
reversed, with each insertion becoming a deletion, and vice versa. the triangle
inequality is also straightforward. one way to turn a string s into a string t
is to turn s into some string u and then turn u into t. thus, the number of
edits made going from s to u, plus the number of edits made going from u to t
cannot be less than the smallest number of edits that will turn s into t.

3.5.6 hamming distance

given a space of vectors, we de   ne the hamming distance between two vectors
to be the number of components in which they di   er.
it should be obvious
that hamming distance is a distance measure. clearly the hamming distance
cannot be negative, and if it is zero, then the vectors are identical. the dis-
tance does not depend on which of two vectors we consider    rst. the triangle
inequality should also be evident. if x and z di   er in m components, and z
and y di   er in n components, then x and y cannot di   er in more than m + n
components. most commonly, hamming distance is used when the vectors are
boolean; they consist of 0   s and 1   s only. however, in principle, the vectors can
have components from any set.

3.5. distance measures

97

non-euclidean spaces

notice that several of the distance measures introduced in this section are
not euclidean spaces. a property of euclidean spaces that we shall    nd
important when we take up id91 in chapter 7 is that the average
of points in a euclidean space always exists and is a point in the space.
however, consider the space of sets for which we de   ned the jaccard dis-
tance. the notion of the    average    of two sets makes no sense. likewise,
the space of strings, where we can use the id153, does not let us
take the    average    of strings.

vector spaces, for which we suggested the cosine distance, may or may
not be euclidean. if the components of the vectors can be any real num-
bers, then the space is euclidean. however, if we restrict components to
be integers, then the space is not euclidean. notice that, for instance, we
cannot    nd an average of the vectors [1, 2] and [3, 1] in the space of vectors
with two integer components, although if we treated them as members of
the two-dimensional euclidean space, then we could say that their average
was [2.0, 1.5].

example 3.16 : the hamming distance between the vectors 10101 and 11110
is 3. that is, these vectors di   er in the second, fourth, and    fth components,
while they agree in the    rst and third components.    

3.5.7 exercises for section 3.5

! exercise 3.5.1 : on the space of nonnegative integers, which of the following
functions are distance measures? if so, prove it; if not, prove that it fails to
satisfy one or more of the axioms.

(a) max(x, y) = the larger of x and y.

(b) di   (x, y) = |x     y| (the absolute magnitude of the di   erence between x

and y).

(c) sum(x, y) = x + y.

exercise 3.5.2 : find the l1 and l2 distances between the points (5, 6, 7) and
(8, 2, 4).

!! exercise 3.5.3 : prove that if i and j are any positive integers, and i < j,
then the li norm between any two points is greater than the lj norm between
those same two points.

exercise 3.5.4 : find the jaccard distances between the following pairs of
sets:

98

chapter 3. finding similar items

(a) {1, 2, 3, 4} and {2, 3, 4, 5}.
(b) {1, 2, 3} and {4, 5, 6}.
exercise 3.5.5 : compute the cosines of the angles between each of the fol-
lowing pairs of vectors.5

(a) (3,   1, 2) and (   2, 3, 1).
(b) (1, 2, 3) and (2, 4, 6).

(c) (5, 0,   4) and (   1,   6, 2).
(d) (0, 1, 1, 0, 1, 1) and (0, 0, 1, 0, 0, 0).

! exercise 3.5.6 : prove that the cosine distance between any two vectors of 0   s

and 1   s, of the same length, is at most 90 degrees.

exercise 3.5.7 : find the id153s (using only insertions and deletions)
between the following pairs of strings.

(a) abcdef and bdaefc.

(b) abccdabc and acbdcab.

(c) abcdef and baedfc.

! exercise 3.5.8 : there are a number of other notions of id153 available.
for instance, we can allow, in addition to insertions and deletions, the following
operations:

i. mutation, where one symbol is replaced by another symbol. note that a
mutation can always be performed by an insertion followed by a deletion,
but if we allow mutations, then this change counts for only 1, not 2, when
computing the id153.

ii. transposition, where two adjacent symbols have their positions swapped.
like a mutation, we can simulate a transposition by one insertion followed
by one deletion, but here we count only 1 for these two steps.

repeat exercise 3.5.7 if id153 is de   ned to be the number of insertions,
deletions, mutations, and transpositions needed to transform one string into
another.

! exercise 3.5.9 : prove that the id153 discussed in exercise 3.5.8 is

indeed a distance measure.

exercise 3.5.10 : find the hamming distances between each pair of the fol-
lowing vectors: 000000, 110011, 010101, and 011100.

5note that what we are asking for is not precisely the cosine distance, but from the cosine
of an angle, you can compute the angle itself, perhaps with the aid of a table or library
function.

3.6. the theory of locality-sensitive functions

99

3.6 the theory of locality-sensitive functions

the lsh technique developed in section 3.4 is one example of a family of func-
tions (the minhash functions) that can be combined (by the banding technique)
to distinguish strongly between pairs at a low distance from pairs at a high dis-
tance. the steepness of the s-curve in fig. 3.7 re   ects how e   ectively we can
avoid false positives and false negatives among the candidate pairs.

now, we shall explore other families of functions, besides the minhash func-
tions, that can serve to produce candidate pairs e   ciently. these functions can
apply to the space of sets and the jaccard distance, or to another space and/or
another distance measure. there are three conditions that we need for a family
of functions:

1. they must be more likely to make close pairs be candidate pairs than

distant pairs. we make this notion precise in section 3.6.1.

2. they must be statistically independent, in the sense that it is possible to
estimate the id203 that two or more functions will all give a certain
response by the product rule for independent events.

3. they must be e   cient, in two ways:

(a) they must be able to identify candidate pairs in time much less
than the time it takes to look at all pairs. for example, minhash
functions have this capability, since we can hash sets to minhash
values in time proportional to the size of the data, rather than the
square of the number of sets in the data. since sets with common
values are colocated in a bucket, we have implicitly produced the
candidate pairs for a single minhash function in time much less than
the number of pairs of sets.

(b) they must be combinable to build functions that are better at avoid-
ing false positives and negatives, and the combined functions must
also take time that is much less than the number of pairs. for ex-
ample, the banding technique of section 3.4.1 takes single minhash
functions, which satisfy condition 3a but do not, by themselves have
the s-curve behavior we want, and produces from a number of min-
hash functions a combined function that has the s-curve shape.

our    rst step is to de   ne    locality-sensitive functions    generally. we then
see how the idea can be applied in several applications. finally, we discuss
how to apply the theory to arbitrary data with either a cosine distance or a
euclidean distance measure.

3.6.1 locality-sensitive functions

for the purposes of this section, we shall consider functions that take two items
and render a decision about whether these items should be a candidate pair.

100

chapter 3. finding similar items

in many cases, the function f will    hash    items, and the decision will be based
on whether or not the result is equal. because it is convenient to use the
notation f (x) = f (y) to mean that f (x, y) is    yes; make x and y a candidate
pair,    we shall use f (x) = f (y) as a shorthand with this meaning. we also use
f (x) 6= f (y) to mean    do not make x and y a candidate pair unless some other
function concludes we should do so.   
a collection of functions of this form will be called a family of functions.
for example, the family of minhash functions, each based on one of the possible
permutations of rows of a characteristic matrix, form a family.

let d1 < d2 be two distances according to some distance measure d. a

family f of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in f:

1. if d(x, y)     d1, then the id203 that f (x) = f (y) is at least p1.
2. if d(x, y)     d2, then the id203 that f (x) = f (y) is at most p2.

p

1

p

2

probabilty
of being
declared a
candidate

d

1

d

2

distance

figure 3.9: behavior of a (d1, d2, p1, p2)-sensitive function

figure 3.9 illustrates what we expect about the id203 that a given
function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a can-
didate pair. notice that we say nothing about what happens when the distance
between the items is strictly between d1 and d2, but we can make d1 and d2 as
close as we wish. the penalty is that typically p1 and p2 are then close as well.
as we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2
   xed.

3.6.2 locality-sensitive families for jaccard distance

for the moment, we have only one way to    nd a family of locality-sensitive
functions: use the family of minhash functions, and assume that the distance

3.6. the theory of locality-sensitive functions

101

measure is the jaccard distance. as before, we interpret a minhash function h
to make x and y a candidate pair if and only if h(x) = h(y).

    the family of minhash functions is a (d1, d2, 1   d1, 1   d2)-sensitive family

for any d1 and d2, where 0     d1 < d2     1.

the reason is that if d(x, y)     d1, where d is the jaccard distance, then
sim(x, y) = 1     d(x, y)     1     d1. but we know that the jaccard similarity
of x and y is equal to the id203 that a minhash function will hash x and
y to the same value. a similar argument applies to d2 or any distance.

example 3.17 : we could let d1 = 0.3 and d2 = 0.6. then we can assert that
the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. that is,
if the jaccard distance between x and y is at most 0.3 (i.e., sim(x, y)     0.7)
then there is at least a 0.7 chance that a minhash function will send x and y to
the same value, and if the jaccard distance between x and y is at least 0.6 (i.e.,
sim(x, y)     0.4), then there is at most a 0.4 chance that x and y will be sent
to the same value. note that we could make the same assertion with another
choice of d1 and d2; only d1 < d2 is required.    

3.6.3 amplifying a locality-sensitive family

suppose we are given a (d1, d2, p1, p2)-sensitive family f. we can construct a
new family f    by the and-construction on f, which is de   ned as follows. each
member of f    consists of r members of f for some    xed r. if f is in f   , and f is
constructed from the set {f1, f2, . . . , fr} of members of f, we say f (x) = f (y)
if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. notice that this construction
mirrors the e   ect of the r rows in a single band: the band makes x and y a
candidate pair if every one of the r rows in the band say that x and y are equal
(and therefore a candidate pair according to that row).

since the members of f are independently chosen to make a member of f   ,

we can assert that f    is a (cid:0)d1, d2, (p1)r, (p2)r(cid:1)-sensitive family. that is, for any

p, if p is the id203 that a member of f will declare (x, y) to be a candidate
pair, then the id203 that a member of f    will so declare is pr.

there is another construction, which we call the or-construction, that turns

a (d1, d2, p1, p2)-sensitive family f into a (cid:0)d1, d2, 1     (1     p1)b, 1     (1     p2)b(cid:1)-

sensitive family f   . each member f of f    is constructed from b members of f,
say f1, f2, . . . , fb. we de   ne f (x) = f (y) if and only if fi(x) = fi(y) for one or
more values of i. the or-construction mirrors the e   ect of combining several
bands: x and y become a candidate pair if any band makes them a candidate
pair.

if p is the id203 that a member of f will declare (x, y) to be a candidate
pair, then 1   p is the id203 it will not so declare. (1   p)b is the id203
that none of f1, f2, . . . , fb will declare (x, y) a candidate pair, and 1     (1     p)b
is the id203 that at least one fi will declare (x, y) a candidate pair, and
therefore that f will declare (x, y) to be a candidate pair.

102

chapter 3. finding similar items

notice that the and-construction lowers all probabilities, but if we choose f
and r judiciously, we can make the small id203 p2 get very close to 0, while
the higher id203 p1 stays signi   cantly away from 0. similarly, the or-
construction makes all probabilities rise, but by choosing f and b judiciously,
we can make the larger id203 approach 1 while the smaller id203
remains bounded away from 1. we can cascade and- and or-constructions in
any order to make the low id203 close to 0 and the high id203 close
to 1. of course the more constructions we use, and the higher the values of r
and b that we pick, the larger the number of functions from the original family
that we are forced to use. thus, the better the    nal family of functions is, the
longer it takes to apply the functions from this family.

example 3.18 : suppose we start with a family f. we use the and-construc-
tion with r = 4 to produce a family f1. we then apply the or-construction
to f1 with b = 4 to produce a third family f2. note that the members of f2
each are built from 16 members of f, and the situation is analogous to starting
with 16 minhash functions and treating them as four bands of four rows each.

p
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

1     (1     p4)4

0.0064
0.0320
0.0985
0.2275
0.4260
0.6666
0.8785
0.9860

figure 3.10: e   ect of the 4-way and-construction followed by the 4-way or-
construction

the 4-way and-function converts any id203 p into p4. when we
follow it by the 4-way or-construction, that id203 is further converted
into 1    (1    p4)4. some values of this transformation are indicated in fig. 3.10.
this function is an s-curve, staying low for a while, then rising steeply (although
not too steeply; the slope never gets much higher than 2), and then leveling
o    at high values. like any s-curve, it has a    xedpoint, the value of p that is
left unchanged when we apply the function of the s-curve. in this case, the
   xedpoint is the value of p for which p = 1     (1     p4)4. we can see that the
   xedpoint is somewhere between 0.7 and 0.8. below that value, probabilities are
decreased, and above it they are increased. thus, if we pick a high id203
above the    xedpoint and a low id203 below it, we shall have the desired
e   ect that the low id203 is decreased and the high id203 is increased.
suppose f is the minhash functions, regarded as a (0.2, 0.6, 0.8, 0.4)-sens-
itive family. then f2, the family constructed by a 4-way and followed by a

3.6. the theory of locality-sensitive functions

103

4-way or, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the
rows for 0.8 and 0.4 in fig. 3.10. by replacing f by f2, we have reduced both
the false-negative and false-positive rates, at the cost of making application of
the functions take 16 times as long.    

p
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8

(cid:0)1     (1     p)4(cid:1)4

0.0140
0.1215
0.3334
0.5740
0.7725
0.9015
0.9680
0.9936

figure 3.11: e   ect of the 4-way or-construction followed by the 4-way and-
construction

example 3.19 : for the same cost, we can apply a 4-way or-construction
followed by a 4-way and-construction. figure 3.11 gives the transformation
on probabilities implied by this construction. for instance, suppose that f is a
(0.2, 0.6, 0.8, 0.4)-sensitive family. then the constructed family is a

(0.2, 0.6, 0.9936, 0.5740)-sensitive

family. this choice is not necessarily the best. although the higher id203
has moved much closer to 1, the lower id203 has also raised, increasing
the number of false positives.    

example 3.20 : we can cascade constructions as much as we like. for exam-
ple, we could use the construction of example 3.18 on the family of minhash
functions and then use the construction of example 3.19 on the resulting family.
the constructed family would then have functions each built from 256 minhash
functions. it would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family
into a (0.2, 0.8, 0.9991285, 0.0000004)-sensitive family.    

3.6.4 exercises for section 3.6

exercise 3.6.1 : what is the e   ect on id203 of starting with the family
of minhash functions and applying:

(a) a 2-way and construction followed by a 3-way or construction.

(b) a 3-way or construction followed by a 2-way and construction.

104

chapter 3. finding similar items

(c) a 2-way and construction followed by a 2-way or construction, followed

by a 2-way and construction.

(d) a 2-way or construction followed by a 2-way and construction, followed

by a 2-way or construction followed by a 2-way and construction.

exercise 3.6.2 : find the    xedpoints for each of the functions constructed in
exercise 3.6.1.

! exercise 3.6.3 : any function of id203 p, such as that of fig. 3.10, has
a slope given by the derivative of the function. the maximum slope is where
that derivative is a maximum. find the value of p that gives a maximum slope
for the s-curves given by fig. 3.10 and fig. 3.11. what are the values of these
maximum slopes?

!! exercise 3.6.4 : generalize exercise 3.6.3 to give, as a function of r and b, the
point of maximum slope and the value of that slope, for families of functions
de   ned from the minhash functions by:

(a) an r-way and construction followed by a b-way or construction.

(b) a b-way or construction followed by an r-way and construction.

3.7 lsh families for other distance measures

there is no guarantee that a distance measure has a locality-sensitive family of
hash functions. so far, we have only seen such families for the jaccard distance.
in this section, we shall show how to construct locality-sensitive families for
hamming distance, the cosine distance and for the normal euclidean distance.

3.7.1 lsh families for hamming distance

it is quite simple to build a locality-sensitive family of functions for the ham-
ming distance. suppose we have a space of d-dimensional vectors, and h(x, y)
denotes the hamming distance between vectors x and y. if we take any one
position of the vectors, say the ith position, we can de   ne the function fi(x)
to be the ith bit of vector x. then fi(x) = fi(y) if and only if vectors x and
y agree in the ith position. then the id203 that fi(x) = fi(y) for a ran-
domly chosen i is exactly 1     h(x, y)/d; i.e., it is the fraction of positions in
which x and y agree.
this situation is almost exactly like the one we encountered for minhashing.

thus, the family f consisting of the functions {f1, f2, . . . , fd} is a

(d1, d2, 1     d1/d, 1     d2/d)-sensitive

family of hash functions, for any d1 < d2. there are only two di   erences
between this family and the family of minhash functions.

3.7. lsh families for other distance measures

105

1. while jaccard distance runs from 0 to 1, the hamming distance on a
vector space of dimension d runs from 0 to d. it is therefore necessary to
scale the distances by dividing by d, to turn them into probabilities.

2. while there is essentially an unlimited supply of minhash functions, the

size of the family f for hamming distance is only d.

the    rst point is of no consequence; it only requires that we divide by d at
appropriate times. the second point is more serious. if d is relatively small,
then we are limited in the number of functions that can be composed using
the and and or constructions, thereby limiting how steep we can make the
s-curve be.

3.7.2 random hyperplanes and the cosine distance

recall from section 3.5.4 that the cosine distance between two vectors is the
angle between the vectors. for instance, we see in fig. 3.12 two vectors x
and y that make an angle    between them. note that these vectors may be
in a space of many dimensions, but they always de   ne a plane, and the angle
between them is measured in this plane. figure 3.12 is a    top-view    of the
plane containing x and y.

  

x

y

figure 3.12: two vectors make an angle   

suppose we pick a hyperplane through the origin. this hyperplane intersects
the plane of x and y in a line. figure 3.12 suggests two possible hyperplanes,
one whose intersection is the dashed line and the other   s intersection is the
dotted line. to pick a random hyperplane, we actually pick the normal vector
to the hyperplane, say v. the hyperplane is then the set of points whose dot
product with v is 0.

106

chapter 3. finding similar items

first, consider a vector v that is normal to the hyperplane whose projection
is represented by the dashed line in fig. 3.12; that is, x and y are on di   erent
sides of the hyperplane. then the dot products v.x and v.y will have di   erent
signs. if we assume, for instance, that v is a vector whose projection onto the
plane of x and y is above the dashed line in fig. 3.12, then v.x is positive,
while v.y is negative. the normal vector v instead might extend in the opposite
direction, below the dashed line. in that case v.x is negative and v.y is positive,
but the signs are still di   erent.

on the other hand, the randomly chosen vector v could be normal to a
hyperplane like the dotted line in fig. 3.12.
in that case, both v.x and v.y
have the same sign. if the projection of v extends to the right, then both dot
products are positive, while if v extends to the left, then both are negative.

what is the id203 that the randomly chosen vector is normal to a
hyperplane that looks like the dashed line rather than the dotted line? all
angles for the line that is the intersection of the random hyperplane and the
plane of x and y are equally likely. thus, the hyperplane will look like the
dashed line with id203   /180 and will look like the dotted line otherwise.
thus, each hash function f in our locality-sensitive family f is built from
a randomly chosen vector vf . given two vectors x and y, say f (x) = f (y) if
and only if the dot products vf .x and vf .y have the same sign. then f is a
locality-sensitive family for the cosine distance. the parameters are essentially
the same as for the jaccard-distance family described in section 3.6.2, except
the scale of distances is 0   180 rather than 0   1. that is, f is a

(d1, d2, (180     d1)/180, (180     d2)/180)-sensitive

family of hash functions. from this basis, we can amplify the family as we wish,
just as for the minhash-based family.

3.7.3 sketches

instead of chosing a random vector from all possible vectors, it turns out to be
su   ciently random if we restrict our choice to vectors whose components are
+1 and    1. the dot product of any vector x with a vector v of +1   s and    1   s
is formed by adding the components of x where v is +1 and then subtracting
the other components of x     those where v is    1.
if we pick a collection of random vectors, say v1, v2, . . . , vn, then we can
apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , vn.x and then
replacing any positive value by +1 and any negative value by    1. the result is
called the sketch of x. you can handle 0   s arbitrarily, e.g., by chosing a result +1
or    1 at random. since there is only a tiny id203 of a zero dot product,
the choice has essentially no e   ect.

example 3.21 : suppose our space consists of 4-dimensional vectors, and we
pick three random vectors: v1 = [+1,   1, +1, +1], v2 = [   1, +1,   1, +1], and
v3 = [+1, +1,   1,   1]. for the vector x = [3, 4, 5, 6], the sketch is [+1, +1,   1].

3.7. lsh families for other distance measures

107

that is, v1.x = 3   4+5+6 = 10. since the result is positive, the    rst component
of the sketch is +1. similarly, v2.x = 2 and v3.x =    4, so the second component
of the sketch is +1 and the third component is    1.
consider the vector y = [4, 3, 2, 1]. we can similarly compute its sketch to
be [+1,   1, +1]. since the sketches for x and y agree in 1/3 of the positions,
we estimate that the angle between them is 120 degrees. that is, a randomly
chosen hyperplane is twice as likely to look like the dashed line in fig. 3.12 than
like the dotted line.

the above conclusion turns out to be quite wrong. we can calculate the

cosine of the angle between x and y to be x.y, which is

6    1 + 5    2 + 4    3 + 3    4 = 40

divided by the magnitudes of the two vectors. these magnitudes are

p62 + 52 + 42 + 32 = 9.274

and    12 + 22 + 32 + 42 = 5.477. thus, the cosine of the angle between x and
y is 0.7875, and this angle is about 38 degrees. however, if you look at all
16 di   erent vectors v of length 4 that have +1 and    1 as components, you
   nd that there are only four of these whose dot products with x and y have
a di   erent sign, namely v2, v3, and their complements [+1,   1, +1,   1] and
[   1,   1, +1, +1]. thus, had we picked all sixteen of these vectors to form a
sketch, the estimate of the angle would have been 180/4 = 45 degrees.    

3.7.4 lsh families for euclidean distance

now, let us turn to the euclidean distance (section 3.5.2), and see if we can
develop a locality-sensitive family of hash functions for this distance. we shall
start with a 2-dimensional euclidean space. each hash function f in our family
f will be associated with a randomly chosen line in this space. pick a constant
a and divide the line into segments of length a, as suggested by fig. 3.13, where
the    random    line has been oriented to be horizontal.

the segments of the line are the buckets into which function f hashes points.
a point is hashed to the bucket in which its projection onto the line lies. if the
distance d between two points is small compared with a, then there is a good
chance the two points hash to the same bucket, and thus the hash function f
will declare the two points equal. for example, if d = a/2, then there is at least
a 50% chance the two points will fall in the same bucket. in fact, if the angle
   between the randomly chosen line and the line connecting the points is large,
then there is an even greater chance that the two points will fall in the same
bucket. for instance, if    is 90 degrees, then the two points are certain to fall
in the same bucket.

however, suppose d is larger than a. in order for there to be any chance of
the two points falling in the same bucket, we need d cos        a. the diagram of
fig. 3.13 suggests why this requirement holds. note that even if d cos        a it

108

chapter 3. finding similar items

points at
distance
  

d

bucket
width a

figure 3.13: two points at distance d     a have a small chance of being hashed
to the same bucket

is still not certain that the two points will fall in the same bucket. however,
we can guarantee the following. if d     2a, then there is no more than a 1/3
chance the two points fall in the same bucket. the reason is that for cos    to
be less than 1/2, we need to have    in the range 60 to 90 degrees. if    is in the
range 0 to 60 degrees, then cos    is more than 1/2. but since    is the smaller
angle between two randomly chosen lines in the plane,    is twice as likely to be
between 0 and 60 as it is to be between 60 and 90.

we conclude that the family f just described forms a (a/2, 2a, 1/2, 1/3)-
sensitive family of hash functions. that is, for distances up to a/2 the proba-
bility is at least 1/2 that two points at that distance will fall in the same bucket,
while for distances at least 2a the id203 points at that distance will fall in
the same bucket is at most 1/3. we can amplify this family as we like, just as
for the other examples of locality-sensitive hash functions we have discussed.

3.7.5 more lsh families for euclidean spaces

there is something unsatisfying about the family of hash functions developed
in section 3.7.4. first, the technique was only described for two-dimensional
euclidean spaces. what happens if our data is points in a space with many
dimensions? second, for jaccard and cosine distances, we were able to develop
locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2.
in section 3.7.4 we appear to need the stronger condition d1 < 4d2.

however, we claim that there is a locality-sensitive family of hash func-
tions for any d1 < d2 and for any number of dimensions. the family   s hash
functions still derive from random lines through the space and a bucket size
a that partitions the line. we still hash points by projecting them onto the
line. given that d1 < d2, we may not know what the id203 p1 is that two

3.7. lsh families for other distance measures

109

points at distance d1 hash to the same bucket, but we can be certain that it
is greater than p2, the id203 that two points at distance d2 hash to the
same bucket. the reason is that this id203 surely grows as the distance
shrinks. thus, even if we cannot calculate p1 and p2 easily, we know that there
is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any
given number of dimensions.

using the ampli   cation techniques of section 3.6.3, we can then adjust the
two probabilities to surround any particular value we like, and to be as far apart
as we like. of course, the further apart we want the probabilities to be, the
larger the number of basic hash functions in f we must use.

3.7.6 exercises for section 3.7

exercise 3.7.1 : suppose we construct the basic family of six locality-sensitive
functions for vectors of length six. for each pair of the vectors 000000, 110011,
010101, and 011100, which of the six functions makes them candidates?

exercise 3.7.2 : let us compute sketches using the following four    random   
vectors:

v1 = [+1, +1, +1,   1]
v3 = [+1,   1, +1, +1]

v2 = [+1, +1,   1, +1]
v4 = [   1, +1, +1, +1]

compute the sketches of the following vectors.

(a) [2, 3, 4, 5].

(b) [   2, 3,   4, 5].
(c) [2,   3, 4,   5].
for each pair, what is the estimated angle between them, according to the
sketches? what are the true angles?

exercise 3.7.3 : suppose we form sketches by using all sixteen of the vectors
of length 4, whose components are each +1 or    1. compute the sketches of
the three vectors in exercise 3.7.2. how do the estimates of the angles between
each pair compare with the true angles?

exercise 3.7.4 : suppose we form sketches using the four vectors from exer-
cise 3.7.2.

! (a) what are the constraints on a, b, c, and d that will cause the sketch of

the vector [a, b, c, d] to be [+1, +1, +1, +1]?

!! (b) consider two vectors [a, b, c, d] and [e, f, g, h]. what are the conditions on
a, b, . . . , h that will make the sketches of these two vectors be the same?

110

chapter 3. finding similar items

exercise 3.7.5 : suppose we have points in a 3-dimensional euclidean space:
p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). consider the three hash functions
de   ned by the three axes (to make our calculations very easy). let buckets be
of length a, with one bucket the interval [0, a) (i.e., the set of points x such that
0     x < a), the next [a, 2a), the previous one [   a, 0), and so on.
(a) for each of the three lines, assign each of the points to buckets, assuming

a = 1.

(b) repeat part (a), assuming a = 2.

(c) what are the candidate pairs for the cases a = 1 and a = 2?

! (d) for each pair of points, for what values of a will that pair be a candidate

pair?

3.8 applications of locality-sensitive hashing

in this section, we shall explore three examples of how lsh is used in practice.
in each case, the techniques we have learned must be modi   ed to meet certain
constraints of the problem. the three subjects we cover are:

1. entity resolution: this term refers to matching data records that refer to
the same real-world entity, e.g., the same person. the principal problem
addressed here is that the similarity of records does not match exactly
either the similar-sets or similar-vectors models of similarity on which the
theory is built.

2. matching fingerprints:

it is possible to represent    ngerprints as sets.
however, we shall explore a di   erent family of locality-sensitive hash func-
tions from the one we get by minhashing.

3. matching newspaper articles: here, we consider a di   erent notion of
shingling that focuses attention on the core article in an on-line news-
paper   s web page, ignoring all the extraneous material such as ads and
newspaper-speci   c material.

3.8.1 entity resolution

it is common to have several data sets available, and to know that they refer to
some of the same entities. for example, several di   erent bibliographic sources
provide information about many of the same books or papers. in the general
case, we have records describing entities of some type, such as people or books.
the records may all have the same format, or they may have di   erent formats,
with di   erent kinds of information.

there are many reasons why information about an entity may vary, even if
the    eld in question is supposed to be the same. for example, names may be

3.8. applications of locality-sensitive hashing

111

expressed di   erently in di   erent records because of misspellings, absence of a
middle initial, use of a nickname, and many other reasons. for example,    bob
s. jomes    and    robert jones jr.    may or may not be the same person.
if
records come from di   erent sources, the    elds may di   er as well. one source   s
records may have an    age       eld, while another does not. the second source
might have a    date of birth       eld, or it may have no information at all about
when a person was born.

3.8.2 an entity-resolution example

we shall examine a real example of how lsh was used to deal with an entity-
resolution problem. company a was engaged by company b to solicit cus-
tomers for b. company b would pay a a yearly fee, as long as the customer
maintained their subscription. they later quarreled and disagreed over how
many customers a had provided to b. each had about 1,000,000 records, some
of which described the same people; those were the customers a had provided
to b. the records had di   erent data    elds, but unfortunately none of those
   elds was    this is a customer that a had provided to b.    thus, the problem
was to match records from the two sets to see if a pair represented the same
person.

each record had    elds for the name, address, and phone number of the
person. however, the values in these    elds could di   er for many reasons. not
only were there the misspellings and other naming di   erences mentioned in
section 3.8.1, but there were other opportunities to disagree as well. a customer
might give their home phone to a and their cell phone to b. or they might
move, and tell b but not a (because they no longer had need for a relationship
with a). area codes of phones sometimes change.

the strategy for identifying records involved scoring the di   erences in three
   elds: name, address, and phone. to create a score describing the likelihood
that two records, one from a and the other from b, described the same per-
son, 100 points was assigned to each of the three    elds, so records with exact
matches in all three    elds got a score of 300. however, there were deductions for
mismatches in each of the three    elds. as a    rst approximation, edit-distance
(section 3.5.5) was used, but the penalty grew quadratically with the distance.
then, certain publicly available tables were used to reduce the penalty in ap-
propriate situations. for example,    bill    and    william    were treated as if they
di   ered in only one letter, even though their edit-distance is 5.

however, it is not feasible to score all one trillion pairs of records. thus,
a simple lsh was used to focus on likely candidates. three    hash functions   
were used. the    rst sent records to the same bucket only if they had identical
names; the second did the same but for identical addresses, and the third did
the same for phone numbers.
in practice, there was no hashing; rather the
records were sorted by name, so records with identical names would appear
consecutively and get scored for overall similarity of the name, address, and
phone. then the records were sorted by address, and those with the same

112

chapter 3. finding similar items

when are record matches good enough?

while every case will be di   erent, it may be of interest to know how the
experiment of section 3.8.3 turned out on the data of section 3.8.2. for
scores down to 185, the value of x was very close to 10; i.e., these scores
indicated that the likelihood of the records representing the same person
was essentially 1. note that a score of 185 in this example represents a
situation where one    eld is the same (as would have to be the case, or the
records would never even be scored), one    eld was completely di   erent,
and the third    eld had a small discrepancy. moreover, for scores as low as
115, the value of x was noticeably less than 45, meaning that some of these
pairs did represent the same person. note that a score of 115 represents
a case where one    eld is the same, but there is only a slight similarity in
the other two    elds.

address were scored. finally, the records were sorted a third time by phone,
and records with identical phones were scored.

this approach missed a record pair that truly represented the same person
but none of the three    elds matched exactly. since the goal was to prove in
a court of law that the persons were the same, it is unlikely that such a pair
would have been accepted by a judge as su   ciently similar anyway.

3.8.3 validating record matches

what remains is to determine how high a score indicates that two records truly
represent the same individual.
in the example at hand, there was an easy
way to make that decision, and the technique can be applied in many similar
situations. it was decided to look at the creation-dates for the records at hand,
and to assume that 90 days was an absolute maximum delay between the time
the service was bought at company a and registered at b. thus, a proposed
match between two records that were chosen at random, subject only to the
constraint that the date on the b-record was between 0 and 90 days after the
date on the a-record, would have an average delay of 45 days.

it was found that of the pairs with a perfect 300 score, the average delay was
10 days. if you assume that 300-score pairs are surely correct matches, then you
can look at the pool of pairs with any given score s, and compute the average
delay of those pairs. suppose that the average delay is x, and the fraction of
true matches among those pairs with score s is f . then x = 10f + 45(1     f ),
or x = 45    35f . solving for f , we    nd that the fraction of the pairs with score
s that are truly matches is (45     x)/35.

the same trick can be used whenever:

1. there is a scoring system used to evaluate the likelihood that two records

3.8. applications of locality-sensitive hashing

113

represent the same entity, and

2. there is some    eld, not used in the scoring, from which we can derive a

measure that di   ers, on average, for true pairs and false pairs.

for instance, suppose there were a    height       eld recorded by both companies
a and b in our running example. we can compute the average di   erence in
height for pairs of random records, and we can compute the average di   erence in
height for records that have a perfect score (and thus surely represent the same
entities). for a given score s, we can evaluate the average height di   erence of the
pairs with that score and estimate the id203 of the records representing
the same entity. that is, if h0 is the average height di   erence for the perfect
matches, h1 is the average height di   erence for random pairs, and h is the
average height di   erence for pairs of score s, then the fraction of good pairs
with score s is (h1     h)/(h1     h0).

3.8.4 matching fingerprints

when    ngerprints are matched by computer, the usual representation is not
an image, but a set of locations in which minutiae are located. a minutia,
in the context of    ngerprint descriptions, is a place where something unusual
happens, such as two ridges merging or a ridge ending. if we place a grid over a
   ngerprint, we can represent the    ngerprint by the set of grid squares in which
minutiae are located.

ideally, before overlaying the grid,    ngerprints are normalized for size and
orientation, so that if we took two images of the same    nger, we would    nd
minutiae lying in exactly the same grid squares. we shall not consider here
the best ways to normalize images. let us assume that some combination of
techniques, including choice of grid size and placing a minutia in several adjacent
grid squares if it lies close to the border of the squares enables us to assume
that grid squares from two images have a signi   cantly higher id203 of
agreeing in the presence or absence of a minutia than if they were from images
of di   erent    ngers.

thus,    ngerprints can be represented by sets of grid squares     those where
their minutiae are located     and compared like any sets, using the jaccard sim-
ilarity or distance. there are two versions of    ngerprint comparison, however.

    the many-one problem is the one we typically expect. a    ngerprint has
been found on a gun, and we want to compare it with all the    ngerprints
in a large database, to see which one matches.

    the many-many version of the problem is to take the entire database, and

see if there are any pairs that represent the same individual.

while the many-many version matches the model that we have been following
for    nding similar items, the same technology can be used to speed up the
many-one problem.

114

chapter 3. finding similar items

3.8.5 a lsh family for fingerprint matching

we could minhash the sets that represent a    ngerprint, and use the standard
lsh technique from section 3.4. however, since the sets are chosen from a
relatively small set of grid points (perhaps 1000), the need to minhash them
into more succinct signatures is not clear. we shall study here another form of
locality-sensitive hashing that works well for data of the type we are discussing.
suppose for an example that the id203 of    nding a minutia in a random
grid square of a random    ngerprint is 20%. also, assume that if two    ngerprints
come from the same    nger, and one has a minutia in a given grid square, then
the id203 that the other does too is 80%. we can de   ne a locality-sensitive
family of hash functions as follows. each function f in this family f is de   ned
by three grid squares. function f says    yes    for two    ngerprints if both have
minutiae in all three grid squares, and otherwise f says    no.    put another
way, we may imagine that f sends to a single bucket all    ngerprints that have
minutiae in all three of f    s grid points, and sends each other    ngerprint to a
bucket of its own. in what follows, we shall refer to the    rst of these buckets as
   the    bucket for f and ignore the buckets that are required to be singletons.

if we want to solve the many-one problem, we can use many functions from
the family f and precompute their buckets of    ngerprints to which they answer
   yes.    then, given a new    ngerprint that we want to match, we determine
which of these buckets it belongs to and compare it with all the    ngerprints
found in any of those buckets. to solve the many-many problem, we compute
the buckets for each of the functions and compare all    ngerprints in each of the
buckets.

let us consider how many functions we need to get a reasonable id203
of catching a match, without having to compare the    ngerprint on the gun with
each of the millions of    ngerprints in the database. first, the id203 that
two    ngerprints from di   erent    ngers would be in the bucket for a function f
in f is (0.2)6 = 0.000064. the reason is that they will both go into the bucket
only if they each have a minutia in each of the three grid points associated with
f , and the id203 of each of those six independent events is 0.2.

now, consider the id203 that two    ngerprints from the same    nger
wind up in the bucket for f . the id203 that the    rst    ngerprint has
minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. however,
if it does, then the id203 is (0.8)3 = 0.512 that the other    ngerprint
will as well. thus, if the    ngerprints are from the same    nger, there is a
0.008    0.512 = 0.004096 id203 that they will both be in the bucket of f .
that is not much; it is about one in 200. however, if we use many functions
from f, but not too many, then we can get a good id203 of matching
   ngerprints from the same    nger while not having too many false positives    
   ngerprints that must be considered but do not match.

example 3.22 : for a speci   c example,
let us suppose that we use 1024
functions chosen randomly from f. next, we shall construct a new fam-
ily f1 by performing a 1024-way or on f. then the id203 that f1

3.8. applications of locality-sensitive hashing

115

will put    ngerprints from the same    nger together in at least one bucket is
1     (1     0.004096)1024 = 0.985. on the other hand, the id203 that
two    ngerprints from di   erent    ngers will be placed in the same bucket is
(1     (1     0.000064)1024 = 0.063. that is, we get about 1.5% false negatives
and about 6.3% false positives.    

the result of example 3.22 is not the best we can do. while it o   ers only a
1.5% chance that we shall fail to identify the    ngerprint on the gun, it does force
us to look at 6.3% of the entire database. increasing the number of functions
from f will increase the number of false positives, with only a small bene   t
of reducing the number of false negatives below 1.5%. on the other hand, we
can also use the and construction, and in so doing, we can greatly reduce
the id203 of a false positive, while making only a small increase in the
false-negative rate. for instance, we could take 2048 functions from f in two
groups of 1024. construct the buckets for each of the functions. however, given
a    ngerprint p on the gun:

1. find the buckets from the    rst group in which p belongs, and take the

union of these buckets.

2. do the same for the second group.

3. take the intersection of the two unions.

4. compare p only with those    ngerprints in the intersection.

note that we still have to take unions and intersections of large sets of    nger-
prints, but we compare only a small fraction of those. it is the comparison of
   ngerprints that takes the bulk of the time; in steps (1) and (2)    ngerprints
can be represented by their integer indices in the database.

if we use this scheme, the id203 of detecting a matching    ngerprint
is (0.985)2 = 0.970; that is, we get about 3% false negatives. however, the
id203 of a false positive is (0.063)2 = 0.00397. that is, we only have to
examine about 1/250th of the database.

3.8.6 similar news articles

our last case study concerns the problem of organizing a large repository of
on-line news articles by grouping together web pages that were derived from
the same basic text. it is common for organizations like the associated press
to produce a news item and distribute it to many newspapers. each newspaper
puts the story in its on-line edition, but surrounds it by information that is
special to that newspaper, such as the name and address of the newspaper,
links to related articles, and links to ads.
in addition, it is common for the
newspaper to modify the article, perhaps by leaving o    the last few paragraphs
or even deleting text from the middle. as a result, the same news article can
appear quite di   erent at the web sites of di   erent newspapers.

116

chapter 3. finding similar items

the problem looks very much like the one that was suggested in section 3.4:
   nd documents whose shingles have a high jaccard similarity. note that this
problem is di   erent from the problem of    nding news articles that tell about the
same events. the latter problem requires other techniques, typically examining
the set of important words in the documents (a concept we discussed brie   y
in section 1.3.1) and id91 them to group together di   erent articles about
the same topic.

however, an interesting variation on the theme of shingling was found to be
more e   ective for data of the type described. the problem is that shingling as
we described it in section 3.2 treats all parts of a document equally. however,
we wish to ignore parts of the document, such as ads or the headlines of other
articles to which the newspaper added a link, that are not part of the news
article.
it turns out that there is a noticeable di   erence between text that
appears in prose and text that appears in ads or headlines. prose has a much
greater frequency of stop words, the very frequent words such as    the    or    and.   
the total number of words that are considered stop words varies with the
application, but it is common to use a list of several hundred of the most
frequent words.

example 3.23 : a typical ad might say simply    buy sudzo.    on the other
hand, a prose version of the same thought that might appear in an article is
   i recommend that you buy sudzo for your laundry.    in the latter sentence, it
would be normal to treat    i,       that,       you,       for,    and    your    as stop words.
   

suppose we de   ne a shingle to be a stop word followed by the next two
words. then the ad    buy sudzo    from example 3.23 has no shingles and
would not be re   ected in the representation of the web page containing that
ad. on the other hand, the sentence from example 3.23 would be represented
by    ve shingles:    i recommend that,       that you buy,       you buy sudzo,       for
your laundry,    and    your laundry x,    where x is whatever word follows that
sentence.

suppose we have two web pages, each of which consists of half news text
and half ads or other material that has a low density of stop words. if the news
text is the same but the surrounding material is di   erent, then we would expect
that a large fraction of the shingles of the two pages would be the same. they
might have a jaccard similarity of 75%. however, if the surrounding material
is the same but the news content is di   erent, then the number of common
shingles would be small, perhaps 25%.
if we were to use the conventional
shingling, where shingles are (say) sequences of 10 consecutive characters, we
would expect the two documents to share half their shingles (i.e., a jaccard
similarity of 1/3), regardless of whether it was the news or the surrounding
material that they shared.

3.8. applications of locality-sensitive hashing

117

3.8.7 exercises for section 3.8

exercise 3.8.1 : suppose we are trying to perform entity resolution among
bibliographic references, and we score pairs of references based on the similar-
ities of their titles, list of authors, and place of publication. suppose also that
all references include a year of publication, and this year is equally likely to be
any of the ten most recent years. further, suppose that we discover that among
the pairs of references with a perfect score, there is an average di   erence in the
publication year of 0.1.6 suppose that the pairs of references with a certain
score s are found to have an average di   erence in their publication dates of 2.
what is the fraction of pairs with score s that truly represent the same pub-
lication? note: do not make the mistake of assuming the average di   erence
in publication date between random pairs is 5 or 5.5. you need to calculate it
exactly, and you have enough information to do so.

exercise 3.8.2 : suppose we use the family f of functions described in sec-
tion 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80%
chance of a second copy of a    ngerprint having a minutia in a grid square where
the    rst copy does, and each function in f being formed from three grid squares.
in example 3.22, we constructed family f1 by using the or construction on
1024 members of f. suppose we instead used family f2 that is a 2048-way or
of members of f.

(a) compute the rates of false positives and false negatives for f2.

(b) how do these rates compare with what we get if we organize the same
2048 functions into a 2-way and of members of f1, as was discussed at
the end of section 3.8.5?

exercise 3.8.3 : suppose    ngerprints have the same statistics outlined in ex-
ercise 3.8.2, but we use a base family of functions f    de   ned like f, but using
only two randomly chosen grid squares. construct another set of functions f   
1
from f    by taking the n-way or of functions from f   . what, as a function of
n, are the false positive and false negative rates for f   

1?

exercise 3.8.4 : suppose we use the functions f1 from example 3.22, but we
want to solve the many-many problem.

(a) if two    ngerprints are from the same    nger, what is the id203 that

they will not be compared (i.e., what is the false negative rate)?

(b) what fraction of the    ngerprints from di   erent    ngers will be compared

(i.e., what is the false positive rate)?

! exercise 3.8.5 : assume we have the set of functions f as in exercise 3.8.2,
and we construct a new set of functions f3 by an n-way or of functions in
f. for what value of n is the sum of the false positive and false negative rates
minimized?

6we might expect the average to be 0, but in practice, errors in publication year do occur.

118

chapter 3. finding similar items

3.9 methods for high degrees of similarity

lsh-based methods appear most e   ective when the degree of similarity we
accept is relatively low. when we want to    nd sets that are almost identical,
there are other methods that can be faster. moreover, these methods are exact,
in that they    nd every pair of items with the desired degree of similarity. there
are no false negatives, as there can be with lsh.

3.9.1 finding identical items

the extreme case is    nding identical items, for example, web pages that are
identical, character-for-character. it is straightforward to compare two docu-
ments and tell whether they are identical, but we still must avoid having to
compare every pair of documents. our    rst thought would be to hash docu-
ments based on their    rst few characters, and compare only those documents
that fell into the same bucket. that scheme should work well, unless all the
documents begin with the same characters, such as an html header.

our second thought would be to use a hash function that examines the
entire document. that would work, and if we use enough buckets, it would be
very rare that two documents went into the same bucket, yet were not identical.
the downside of this approach is that we must examine every character of every
document. if we limit our examination to a small number of characters, then
we never have to examine a document that is unique and falls into a bucket of
its own.

a better approach is to pick some    xed random positions for all documents,
and make the hash function depend only on these. this way, we can avoid
a problem where there is a common pre   x for all or most documents, yet we
need not examine entire documents unless they fall into a bucket with another
document. one problem with selecting    xed positions is that if some documents
are short, they may not have some of the selected positions. however, if we are
looking for highly similar documents, we never need to compare two documents
that di   er signi   cantly in their length. we exploit this idea in section 3.9.3.

3.9.2 representing sets as strings

now, let us focus on the harder problem of    nding, in a large collection of sets,
all pairs that have a high jaccard similarity, say at least 0.9. we can represent
a set by sorting the elements of the universal set in some    xed order, and
representing any set by listing its elements in this order. the list is essentially
a string of    characters,    where the characters are the elements of the universal
set. these strings are unusual, however, in that:

1. no character appears more than once in a string, and

2. if two characters appear in two di   erent strings, then they appear in the

same order in both strings.

3.9. methods for high degrees of similarity

119

example 3.24 : suppose the universal set consists of the 26 lower-case letters,
and we use the normal alphabetical order. then the set {d, a, b} is represented
by the string abd.    

in what follows, we shall assume all strings represent sets in the manner just
described. thus, we shall talk about the jaccard similarity of strings, when
strictly speaking we mean the similarity of the sets that the strings represent.
also, we shall talk of the length of a string, as a surrogate for the number of
elements in the set that the string represents.

note that the documents discussed in section 3.9.1 do not exactly match
this model, even though we can see documents as strings. to    t the model,
we would shingle the documents, assign an order to the shingles, and represent
each document by its list of shingles in the selected order.

3.9.3 length-based filtering

the simplest way to exploit the string representation of section 3.9.2 is to sort
the strings by length. then, each string s is compared with those strings t that
follow s in the list, but are not too long. suppose the lower bound on jaccard
similarity between two strings is j. for any string x, denote its length by lx.
note that ls     lt. the intersection of the sets represented by s and t cannot
have more than ls members, while their union has at least lt members. thus,
the jaccard similarity of s and t, which we denote sim(s, t), is at most ls/lt.
that is, in order for s and t to require comparison, it must be that j     ls/lt,
or equivalently, lt     ls/j.
example 3.25 : suppose that s is a string of length 9, and we are looking for
strings with at least 0.9 jaccard similarity. then we have only to compare s
with strings following it in the length-based sorted order that have length at
most 9/0.9 = 10. that is, we compare s with those strings of length 9 that
follow it in order, and all strings of length 10. we have no need to compare s
with any other string.

suppose the length of s were 8 instead. then s would be compared with
following strings of length up to 8/0.9 = 8.89. that is, a string of length 9
would be too long to have a jaccard similarity of 0.9 with s, so we only have to
compare s with the strings that have length 8 but follow it in the sorted order.
   

3.9.4 pre   x indexing

in addition to length, there are several other features of strings that can be
exploited to limit the number of comparisons that must be made to identify
all pairs of similar strings. the simplest of these options is to create an index
for each symbol; recall a symbol of a string is any one of the elements of the
universal set. for each string s, we select a pre   x of s consisting of the    rst p

120

chapter 3. finding similar items

a better ordering for symbols

instead of using the obvious order for elements of the universal set, e.g.,
lexicographic order for shingles, we can order symbols rarest    rst. that
is, determine how many times each element appears in the collection of
sets, and order them by this count, lowest    rst. the advantage of doing
so is that the symbols in pre   xes will tend to be rare. thus, they will
cause that string to be placed in index buckets that have relatively few
members. then, when we need to examine a string for possible matches,
we shall    nd few other strings that are candidates for comparison.

symbols of s. how large p must be depends on ls and j, the lower bound on
jaccard similarity. we add string s to the index for each of its    rst p symbols.
in e   ect, the index for each symbol becomes a bucket of strings that must be
compared. we must be certain that any other string t such that sim(s, t)     j
will have at least one symbol in its pre   x that also appears in the pre   x of s.
suppose not; rather sim(s, t)     j, but t has none of the    rst p symbols of
s. then the highest jaccard similarity that s and t can have occurs when t is
a su   x of s, consisting of everything but the    rst p symbols of s. the jaccard
similarity of s and t would then be (ls     p)/ls. to be sure that we do not
have to compare s with t, we must be certain that j > (ls     p)/ls. that
is, p must be at least    (1     j)ls    + 1. of course we want p to be as small as
possible, so we do not index string s in more buckets than we need to. thus,
we shall hereafter take p =    (1     j)ls    + 1 to be the length of the pre   x that
gets indexed.

if ls = 9, then p =    0.1    9    + 1 =
example 3.26 : suppose j = 0.9.
   0.9    + 1 = 1. that is, we need to index s under only its    rst symbol. any
string t that does not have the    rst symbol of s in a position such that t is
indexed by that symbol will have jaccard similarity with s that is less than 0.9.
suppose s is bcdefghij. then s is indexed under b only. suppose t does not
begin with b. there are two cases to consider.

1. if t begins with a, and sim(s, t)     0.9, then it can only be that t is
abcdefghij. but if that is the case, t will be indexed under both a and
b. the reason is that lt = 10, so t will be indexed under the symbols of
its pre   x of length    0.1    10    + 1 = 2.

2. if t begins with c or a later letter, then the maximum value of sim(s, t)

occurs when t is cdefghij. but then sim(s, t) = 8/9 < 0.9.

in general, with j = 0.9, strings of length up to 9 are indexed by their    rst
symbol, strings of lengths 10   19 are indexed under their    rst two symbols,

3.9. methods for high degrees of similarity

121

strings of length 20   29 are indexed under their    rst three symbols, and so on.
   

we can use the indexing scheme in two ways, depending on whether we
are trying to solve the many-many problem or a many-one problem; recall the
distinction was introduced in section 3.8.4. for the many-one problem, we
create the index for the entire database. to query for matches to a new set
s, we convert that set to a string s, which we call the probe string. determine
the length of the pre   x that must be considered, that is,    (1     j)ls    + 1. for
each symbol appearing in one of the pre   x positions of s, we look in the index
bucket for that symbol, and we compare s with all the strings appearing in that
bucket.

if we want to solve the many-many problem, start with an empty database
of strings and indexes. for each set s, we treat s as a new set for the many-one
problem. we convert s to a string s, which we treat as a probe string in the
many-one problem. however, after we examine an index bucket, we also add s
to that bucket, so s will be compared with later strings that could be matches.

3.9.5 using position information

consider the strings s = acdefghijk and t = bcdefghijk, and assume j = 0.9.
since both strings are of length 10, they are indexed under their    rst two
symbols. thus, s is indexed under a and c, while t is indexed under b and c.
whichever is added last will    nd the other in the bucket for c, and they will be
compared. however, since c is the second symbol of both, we know there will
be two symbols, a and b in this case, that are in the union of the two sets but
not in the intersection. indeed, even though s and t are identical from c to the
end, their intersection is 9 symbols and their union is 11; thus sim(s, t) = 9/11,
which is less than 0.9.

if we build our index based not only on the symbol, but on the position of
the symbol within the string, we could avoid comparing s and t above. that
is, let our index have a bucket for each pair (x, i), containing the strings that
have symbol x in position i of their pre   x. given a string s, and assuming j is
the minimum desired jaccard similarity, we look at the pre   x of s, that is, the
positions 1 through    (1     j)ls    + 1. if the symbol in position i of the pre   x is
x, add s to the index bucket for (x, i).
now consider s as a probe string. with what buckets must it be compared?
we shall visit the symbols of the pre   x of s from the left, and we shall take
advantage of the fact that we only need to    nd a possible matching string t if
none of the previous buckets we have examined for matches held t. that is, we
only need to    nd a candidate match once. thus, if we    nd that the ith symbol
of s is x, then we need look in the bucket (x, j) for certain small values of j.

to compute the upper bound on j, suppose t is a string none of whose    rst
j     1 symbols matched anything in s, but the ith symbol of s is the same as the
jth symbol of t. the highest value of sim(s, t) occurs if s and t are identical

122

chapter 3. finding similar items

symbols definitely

appearing in
only one string

s

t

i

j

figure 3.14: strings s and t begin with i     1 and j     1 unique symbols, respec-
tively, and then agree beyond that

beyond their ith and jth symbols, respectively, as suggested by fig. 3.14. if
that is the case, the size of their intersection is ls     i + 1, since that is the
number of symbols of s that could possibly be in t. the size of their union is
at least ls + j     1. that is, s surely contributes ls symbols to the union, and
there are also at least j     1 symbols of t that are not in s. the ratio of the sizes
of the intersection and union must be at least j, so we must have:

ls     i + 1
ls + j     1     j

if we isolate j in this inequality, we have j     (cid:0)ls(1     j)     i + 1 + j(cid:1)/j.

example 3.27 : consider the string s = acdefghijk with j = 0.9 discussed
at the beginning of this section. suppose s is now a probe string. we already
established that we need to consider the    rst two positions; that is, i can be 1
or 2. suppose i = 1. then j     (10    0.1     1 + 1 + 0.9)/0.9. that is, we only
have to compare the symbol a with strings in the bucket for (a, j) if j     2.11.
thus, j can be 1 or 2, but nothing higher.
now suppose i = 2. then we require j     (10    0.1     2 + 1 + 0.9)/0.9, or
j     1. we conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1),
but in no other bucket. in comparison, using the buckets of section 3.9.4, we
would look into the buckets for a and c, which is equivalent to looking to all
buckets (a, j) and (c, j) for any j.    

3.9.6 using position and length in indexes

when we considered the upper limit on j in the previous section, we assumed
that what follows positions i and j were as in fig. 3.14, where what followed
these positions in strings s and t matched exactly. we do not want to build an
index that involves every symbol in the strings, because that makes the total
work excessive. however, we can add to our index a summary of what follows
the positions being indexed. doing so expands the number of buckets, but not
beyond reasonable bounds, and yet enables us to eliminate many candidate

3.9. methods for high degrees of similarity

123

matches without comparing entire strings. the idea is to use index buckets
corresponding to a symbol, a position, and the su   x length, that is, the number
of symbols following the position in question.

example 3.28 : the string s = acdefghijk, with j = 0.9, would be indexed
in the buckets for (a, 1, 9) and (c, 2, 8). that is, the    rst position of s has symbol
a, and its su   x is of length 9. the second position has symbol c and its su   x
is of length 8.    

figure 3.14 assumes that the su   xes for position i of s and position j of t
have the same length. if not, then we can either get a smaller upper bound on
the size of the intersection of s and t (if t is shorter) or a larger lower bound
on the size of the union (if t is longer). suppose s has su   x length p and t has
su   x length q.

case 1: p     q. here, the maximum size of the intersection is

ls     i + 1     (p     q)

since ls = i + p, we can write the above expression for the intersection size as
q + 1. the minimum size of the union is ls + j     1, as it was when we did not
take su   x length into account. thus, we require

q + 1

ls + j     1     j

whenever p     q.

case 2: p < q. here, the maximum size of the intersection is ls     i + 1, as
when su   x length was not considered. however, the minimum size of the union
is now ls + j     1 + q     p. if we again use the relationship ls = i + p, we can
replace ls     p by i and get the formula i + j     1 + q for the size of the union.
if the jaccard similarity is at least j, then

ls     i + 1
i + j     1 + q     j

whenever p < q.

example 3.29 : let us again consider the string s = acdefghijk, but to make
the example show some details, let us choose j = 0.8 instead of 0.9. we know
that ls = 10. since    (1     j)ls    + 1 = 3, we must consider pre   x positions
i = 1, 2, and 3 in what follows. as before, let p be the su   x length of s and q
the su   x length of t.

first, consider the case p     q. the additional constraint we have on q and
j is (q + 1)/(9 + j)     0.8. we can enumerate the pairs of values of j and q for
each i between 1 and 3, as follows.

124

chapter 3. finding similar items

i = 1: here, p = 9, so q     9. let us consider the possible values of q:

q = 9: we must have 10/(9 + j)     0.8. thus, we can have j = 1, j = 2,

or j = 3. note that for j = 4, 10/13 > 0.8.

q = 8: we must have 9/(9 + j)     0.8. thus, we can have j = 1 or j = 2.

for j = 3, 9/12 > 0.8.

q = 7: we must have 8/(9 + j)     0.8. only j = 1 satis   es this inequality.
q = 6: there are no possible values of j, since 7/(9 + j) > 0.8 for every

positive integer j. the same holds for every smaller value of q.

i = 2: here, p = 8, so we require q     8. since the constraint (q+1)/(9+j)     0.8
does not depend on i,7 we can use the analysis from the above case, but
exclude the case q = 9. thus, the only possible values of j and q when
i = 2 are

1. q = 8; j = 1.
2. q = 8; j = 2.
3. q = 7; j = 1.

i = 3: now, p = 7 and the constraints are q     7 and (q + 1)/(9 + j)     0.8. the

only option is q = 7 and j = 1.

next, we must consider the case p < q. the additional constraint is

11     i

i + j + q     1     0.8

again, consider each possible value of i.
i = 1: then p = 9, so we require q     10 and 10/(q + j)     0.8. the possible

values of q and j are

1. q = 10; j = 1.
2. q = 10; j = 2.
3. q = 11; j = 1.

i = 2: now, p = 8, so we require q     9 and 9/(q + j + 1)     0.8. since j must
be a positive integer, the only solution is q = 9 and j = 1, a possibility
that we already knew about.

i = 3: here, p = 7, so we require q     8 and 8/(q + j + 2)     0.8. there are no

solutions.

when we accumulate the possible combinations of i, j, and q, we see that
the set of index buckets in which we must look forms a pyramid. figure 3.15
shows the buckets in which we must search. that is, we must look in those
buckets (x, j, q) such that the ith symbol of the string s is x, j is the position
associated with the bucket and q the su   x length.    

7note that i does in   uence the value of p, and through p, puts a limit on q.

3.9. methods for high degrees of similarity

125

q
7
8
9
10
11
7
8
9
7

i = 1

i = 2

i = 3

j = 1

j = 2

j = 3

x

x
x
x

x

x
x
x
x
x
x
x
x
x

figure 3.15: the buckets that must be examined to    nd possible matches for
the string s = acdefghijk with j = 0.8 are marked with an x

3.9.7 exercises for section 3.9

exercise 3.9.1 : suppose our universal set is the lower-case letters, and the
order of elements is taken to be the vowels, in alphabetic order, followed by the
consonants in reverse alphabetic order. represent the following sets as strings.

a {q, w, e, r, t, y}.
(b) {a, s, d, f, g, h, j, u, i}.
exercise 3.9.2 : suppose we    lter candidate pairs based only on length, as in
section 3.9.3. if s is a string of length 20, with what strings is s compared when
j, the lower bound on jaccard similarity has the following values: (a) j = 0.85
(b) j = 0.95 (c) j = 0.98?

exercise 3.9.3 : suppose we have a string s of length 15, and we wish to index
its pre   x as in section 3.9.4.

(a) how many positions are in the pre   x if j = 0.85?

(b) how many positions are in the pre   x if j = 0.95?

! (c) for what range of values of j will s be indexed under its    rst four symbols,

but no more?

exercise 3.9.4 : suppose s is a string of length 12. with what symbol-position
pairs will s be compared with if we use the indexing approach of section 3.9.5,
and (a) j = 0.75 (b) j = 0.95?

! exercise 3.9.5 : suppose we use position information in our index, as in sec-
tion 3.9.5. strings s and t are both chosen at random from a universal set of
100 elements. assume j = 0.9. what is the id203 that s and t will be
compared if

126

chapter 3. finding similar items

(a) s and t are both of length 9.

(b) s and t are both of length 10.

exercise 3.9.6 : suppose we use indexes based on both position and su   x
length, as in section 3.9.6.
if s is a string of length 20, with what symbol-
position-length triples will s be compared with, if (a) j = 0.8 (b) j = 0.9?

3.10 summary of chapter 3

    jaccard similarity: the jaccard similarity of sets is the ratio of the size
of the intersection of the sets to the size of the union. this measure of
similarity is suitable for many applications, including textual similarity of
documents and similarity of buying habits of customers.

    shingling: a k-shingle is any k characters that appear consecutively in
a document.
if we represent a document by its set of k-shingles, then
the jaccard similarity of the shingle sets measures the textual similarity
of documents. sometimes, it is useful to hash shingles to bit strings of
shorter length, and use sets of hash values to represent documents.

    minhashing: a minhash function on sets is based on a permutation of the
universal set. given any such permutation, the minhash value for a set is
that element of the set that appears    rst in the permuted order.

    minhash signatures: we may represent sets by picking some list of per-
mutations and computing for each set its minhash signature, which is the
sequence of minhash values obtained by applying each permutation on the
list to that set. given two sets, the expected fraction of the permutations
that will yield the same minhash value is exactly the jaccard similarity
of the sets.

    e   cient minhashing: since it is not really possible to generate random
permutations, it is normal to simulate a permutation by picking a random
hash function and taking the minhash value for a set to be the least hash
value of any of the set   s members.

    locality-sensitive hashing for signatures: this technique allows us to
avoid computing the similarity of every pair of sets or their minhash sig-
natures. if we are given signatures for the sets, we may divide them into
bands, and only measure the similarity of a pair of sets if they are identi-
cal in at least one band. by choosing the size of bands appropriately, we
can eliminate from consideration most of the pairs that do not meet our
threshold of similarity.

    distance measures: a distance measure is a function on pairs of points in
a space that satisfy certain axioms. the distance between two points is 0 if

3.10. summary of chapter 3

127

the points are the same, but greater than 0 if the points are di   erent. the
distance is symmetric; it does not matter in which order we consider the
two points. a distance measure must satisfy the triangle inequality: the
distance between two points is never more than the sum of the distances
between those points and some third point.

    euclidean distance: the most common notion of distance is the euclidean
distance in an n-dimensional space. this distance, sometimes called the
l2-norm, is the square root of the sum of the squares of the di   erences
between the points in each dimension. another distance suitable for eu-
clidean spaces, called manhattan distance or the l1-norm is the sum of
the magnitudes of the di   erences between the points in each dimension.

    jaccard distance: one minus the jaccard similarity is a distance measure,

called the jaccard distance.

    cosine distance: the angle between vectors in a vector space is the cosine
distance measure. we can compute the cosine of that angle by taking the
dot product of the vectors and dividing by the lengths of the vectors.

    id153: this distance measure applies to a space of strings, and
is the number of insertions and/or deletions needed to convert one string
into the other. the id153 can also be computed as the sum of
the lengths of the strings minus twice the length of the longest common
subsequence of the strings.

    hamming distance: this distance measure applies to a space of vectors.
the hamming distance between two vectors is the number of positions in
which the vectors di   er.

    generalized locality-sensitive hashing: we may start with any collection
of functions, such as the minhash functions, that can render a decision
as to whether or not a pair of items should be candidates for similarity
checking. the only constraint on these functions is that they provide a
lower bound on the id203 of saying    yes    if the distance (according
to some distance measure) is below a given limit, and an upper bound on
the id203 of saying    yes    if the distance is above another given limit.
we can then increase the id203 of saying    yes    for nearby items and
at the same time decrease the id203 of saying    yes    for distant items
to as great an extent as we wish, by applying an and construction and
an or construction.

    random hyperplanes and lsh for cosine distance: we can get a set of
basis functions to start a generalized lsh for the cosine distance measure
by identifying each function with a list of randomly chosen vectors. we
apply a function to a given vector v by taking the dot product of v with
each vector on the list. the result is a sketch consisting of the signs (+1 or
   1) of the dot products. the fraction of positions in which the sketches of

128

chapter 3. finding similar items

two vectors agree, multiplied by 180, is an estimate of the angle between
the two vectors.

    lsh for euclidean distance: a set of basis functions to start lsh for
euclidean distance can be obtained by choosing random lines and project-
ing points onto those lines. each line is broken into    xed-length intervals,
and the function answers    yes    to a pair of points that fall into the same
interval.

    high-similarity detection by string comparison: an alternative approach
to    nding similar items, when the threshold of jaccard similarity is close to
1, avoids using minhashing and lsh. rather, the universal set is ordered,
and sets are represented by strings, consisting their elements in order.
the simplest way to avoid comparing all pairs of sets or their strings is to
note that highly similar sets will have strings of approximately the same
length. if we sort the strings, we can compare each string with only a
small number of the immediately following strings.

    character indexes:

if we represent sets by strings, and the similarity
threshold is close to 1, we can index all strings by their    rst few characters.
the pre   x whose characters must be indexed is approximately the length
of the string times the maximum jaccard distance (1 minus the minimum
jaccard similarity).

    position indexes: we can index strings not only on the characters in
their pre   xes, but on the position of that character within the pre   x. we
reduce the number of pairs of strings that must be compared, because
if two strings share a character that is not in the    rst position in both
strings, then we know that either there are some preceding characters that
are in the union but not the intersection, or there is an earlier symbol that
appears in both strings.

    su   x indexes: we can also index strings based not only on the characters
in their pre   xes and the positions of those characters, but on the length
of the character   s su   x     the number of positions that follow it in the
string. this structure further reduces the number of pairs that must be
compared, because a common symbol with di   erent su   x lengths implies
additional characters that must be in the union but not in the intersection.

3.11 references for chapter 3

the technique we called shingling is attributed to [10]. the use in the manner
we discussed here is from [2]. minhashing comes from [3]. the original works
on locality-sensitive hashing were [9] and [7].
[1] is a useful summary of ideas
in this    eld.

3.11. references for chapter 3

129

[4] introduces the idea of using random-hyperplanes to summarize items in
a way that respects the cosine distance. [8] suggests that random hyperplanes
plus lsh can be more accurate at detecting similar documents than minhashing
plus lsh.

techniques for summarizing points in a euclidean space are covered in [6].

[11] presented the shingling technique based on stop words.

the length and pre   x-based indexing schemes for high-similarity matching

comes from [5]. the technique involving su   x length is from [12].

1. a. andoni and p. indyk,    near-optimal hashing algorithms for approxi-
mate nearest neighbor in high dimensions,    comm. acm 51:1, pp. 117   
122, 2008.

2. a.z. broder,    on the resemblance and containment of documents,    proc.
compression and complexity of sequences, pp. 21   29, positano italy,
1997.

3. a.z. broder, m. charikar, a.m. frieze, and m. mitzenmacher,    min-wise
independent permutations,    acm symposium on theory of computing,
pp. 327   336, 1998.

4. m.s. charikar,    similarity estimation techniques from rounding algo-
rithms,    acm symposium on theory of computing, pp. 380   388, 2002.

5. s. chaudhuri, v. ganti, and r. kaushik,    a primitive operator for sim-
ilarity joins in data cleaning,    proc. intl. conf. on data engineering,
2006.

6. m. datar, n. immorlica, p. indyk, and v.s. mirrokni,    locality-sensitive
hashing scheme based on p-stable distributions,    symposium on compu-
tational geometry pp. 253   262, 2004.

7. a. gionis, p. indyk, and r. motwani,    similarity search in high dimen-
sions via hashing,    proc. intl. conf. on very large databases, pp. 518   
529, 1999.

8. m. henzinger,    finding near-duplicate web pages: a large-scale evaluation

of algorithms,    proc. 29th sigir conf., pp. 284   291, 2006.

9. p. indyk and r. motwani.    approximate nearest neighbor: towards re-
moving the curse of dimensionality,    acm symposium on theory of com-
puting, pp. 604   613, 1998.

10. u. manber,    finding similar    les in a large    le system,    proc. usenix

conference, pp. 1   10, 1994.

11. m. theobald, j. siddharth, and a. paepcke,    spotsigs: robust and e   -
cient near duplicate detection in large web collections,    31st annual acm
sigir conference, july, 2008, singapore.

130

chapter 3. finding similar items

12. c. xiao, w. wang, x. lin, and j.x. yu,    e   cient similarity joins for

near duplicate detection,    proc. www conference, pp. 131-140, 2008.

chapter 4

mining data streams

most of the algorithms described in this book assume that we are mining a
database. that is, all our data is available when and if we want it.
in this
chapter, we shall make another assumption: data arrives in a stream or streams,
and if it is not processed immediately or stored, then it is lost forever. moreover,
we shall assume that the data arrives so rapidly that it is not feasible to store
it all in active storage (i.e., in a conventional database), and then interact with
it at the time of our choosing.

the algorithms for processing streams each involve summarization of the
stream in some way. we shall start by considering how to make a useful sample
of a stream and how to    lter a stream to eliminate most of the    undesirable   
elements. we then show how to estimate the number of di   erent elements in
a stream using much less storage than would be required if we listed all the
elements we have seen.

another approach to summarizing a stream is to look at only a    xed-length
   window    consisting of the last n elements for some (typically large) n. we
then query the window as if it were a relation in a database.
if there are
many streams and/or n is large, we may not be able to store the entire window
for every stream, so we need to summarize even the windows. we address the
fundamental problem of maintaining an approximate count on the number of 1   s
in the window of a bit stream, while using much less space than would be needed
to store the entire window itself. this technique generalizes to approximating
various kinds of sums.

4.1 the stream data model

let us begin by discussing the elements of streams and stream processing. we
explain the di   erence between streams and databases and the special problems
that arise when dealing with streams. some typical applications where the
stream model applies will be examined.

131

132

chapter 4. mining data streams

streams entering

1, 5, 2, 7, 4, 0, 3, 5
q, w, e, r, t, y, u, i, o

0, 1, 1, 0, 1, 0, 0, 0

. . .

time

ad   hoc
queries

standing
queries

stream
processor

output streams

limited
working
storage

archival

storage

figure 4.1: a data-stream-management system

4.1.1 a data-stream-management system

in analogy to a database-management system, we can view a stream processor
as a kind of data-management system, the high-level organization of which is
suggested in fig. 4.1. any number of streams can enter the system. each
stream can provide elements at its own schedule; they need not have the same
data rates or data types, and the time between elements of one stream need not
be uniform. the fact that the rate of arrival of stream elements is not under
the control of the system distinguishes stream processing from the processing
of data that goes on within a database-management system. the latter system
controls the rate at which data is read from the disk, and therefore never has
to worry about data getting lost as it attempts to execute queries.

streams may be archived in a large archival store, but we assume it is not
possible to answer queries from the archival store. it could be examined only
under special circumstances using time-consuming retrieval processes. there is
also a working store, into which summaries or parts of streams may be placed,
and which can be used for answering queries. the working store might be disk,
or it might be main memory, depending on how fast we need to process queries.
but either way, it is of su   ciently limited capacity that it cannot store all the
data from all the streams.

4.1. the stream data model

133

4.1.2 examples of stream sources

before proceeding, let us consider some of the ways in which stream data arises
naturally.

sensor data

imagine a temperature sensor bobbing about in the ocean, sending back to a
base station a reading of the surface temperature each hour. the data produced
by this sensor is a stream of real numbers. it is not a very interesting stream,
since the data rate is so low. it would not stress modern technology, and the
entire stream could be kept in main memory, essentially forever.

now, give the sensor a gps unit, and let it report surface height instead of
temperature. the surface height varies quite rapidly compared with tempera-
ture, so we might have the sensor send back a reading every tenth of a second.
if it sends a 4-byte real number each time, then it produces 3.5 megabytes per
day. it will still take some time to    ll up main memory, let alone a single disk.
but one sensor might not be that interesting. to learn something about
ocean behavior, we might want to deploy a million sensors, each sending back a
stream, at the rate of ten per second. a million sensors isn   t very many; there
would be one for every 150 square miles of ocean. now we have 3.5 terabytes
arriving every day, and we de   nitely need to think about what can be kept in
working storage and what can only be archived.

image data

satellites often send down to earth streams consisting of many terabytes of
images per day. surveillance cameras produce images with lower resolution
than satellites, but there can be many of them, each producing a stream of
images at intervals like one second. london is said to have six million such
cameras, each producing a stream.

internet and web tra   c

a switching node in the middle of the internet receives streams of ip packets
from many inputs and routes them to its outputs. normally, the job of the
switch is to transmit data and not to retain it or query it. but there is a
tendency to put more capability into the switch, e.g., the ability to detect
denial-of-service attacks or the ability to reroute packets based on information
about congestion in the network.

web sites receive streams of various types. for example, google receives sev-
eral hundred million search queries per day. yahoo! accepts billions of    clicks   
per day on its various sites. many interesting things can be learned from these
streams. for example, an increase in queries like    sore throat    enables us to
track the spread of viruses. a sudden increase in the click rate for a link could

134

chapter 4. mining data streams

indicate some news connected to that page, or it could mean that the link is
broken and needs to be repaired.

4.1.3 stream queries

there are two ways that queries get asked about streams. we show in fig. 4.1 a
place within the processor where standing queries are stored. these queries are,
in a sense, permanently executing, and produce outputs at appropriate times.

example 4.1 : the stream produced by the ocean-surface-temperature sen-
sor mentioned at the beginning of section 4.1.2 might have a standing query
to output an alert whenever the temperature exceeds 25 degrees centigrade.
this query is easily answered, since it depends only on the most recent stream
element.

alternatively, we might have a standing query that, each time a new reading
arrives, produces the average of the 24 most recent readings. that query also
can be answered easily, if we store the 24 most recent stream elements. when a
new stream element arrives, we can drop from the working store the 25th most
recent element, since it will never again be needed (unless there is some other
standing query that requires it).

another query we might ask is the maximum temperature ever recorded by
that sensor. we can answer this query by retaining a simple summary: the
maximum of all stream elements ever seen. it is not necessary to record the
entire stream. when a new stream element arrives, we compare it with the
stored maximum, and set the maximum to whichever is larger. we can then
answer the query by producing the current value of the maximum. similarly,
if we want the average temperature over all time, we have only to record two
values: the number of readings ever sent in the stream and the sum of those
readings. we can adjust these values easily each time a new reading arrives,
and we can produce their quotient as the answer to the query.    

the other form of query is ad-hoc, a question asked once about the current
state of a stream or streams. if we do not store all streams in their entirety, as
normally we can not, then we cannot expect to answer arbitrary queries about
streams. if we have some idea what kind of queries will be asked through the
ad-hoc query interface, then we can prepare for them by storing appropriate
parts or summaries of streams as in example 4.1.

if we want the facility to ask a wide variety of ad-hoc queries, a common
approach is to store a sliding window of each stream in the working store. a
sliding window can be the most recent n elements of a stream, for some n, or
it can be all the elements that arrived within the last t time units, e.g., one
day. if we regard each stream element as a tuple, we can treat the window as a
relation and query it with any sql query. of course the stream-management
system must keep the window fresh, deleting the oldest elements as new ones
come in.

4.1. the stream data model

135

example 4.2 : web sites often like to report the number of unique users over
the past month. if we think of each login as a stream element, we can maintain
a window that is all logins in the most recent month. we must associate the
arrival time with each login, so we know when it no longer belongs to the
window. if we think of the window as a relation logins(name, time), then
it is simple to get the number of unique users over the past month. the sql
query is:

select count(distinct(name))
from logins
where time >= t;

here, t is a constant that represents the time one month before the current
time.

note that we must be able to maintain the entire stream of logins for the
past month in working storage. however, for even the largest sites, that data
is not more than a few terabytes, and so surely can be stored on disk.    

4.1.4

issues in stream processing

before proceeding to discuss algorithms, let us consider the constraints under
which we work when dealing with streams. first, streams often deliver elements
very rapidly. we must process elements in real time, or we lose the opportunity
to process them at all, without accessing the archival storage. thus, it often is
important that the stream-processing algorithm is executed in main memory,
without access to secondary storage or with only rare accesses to secondary
storage. moreover, even when streams are    slow,    as in the sensor-data example
of section 4.1.2, there may be many such streams. even if each stream by itself
can be processed using a small amount of main memory, the requirements of all
the streams together can easily exceed the amount of available main memory.
thus, many problems about streaming data would be easy to solve if we
had enough memory, but become rather hard and require the invention of new
techniques in order to execute them at a realistic rate on a machine of realistic
size. here are two generalizations about stream algorithms worth bearing in
mind as you read through this chapter:

    often, it is much more e   cient to get an approximate answer to our

problem than an exact solution.

    as in chapter 3, a variety of techniques related to hashing turn out to be
useful. generally, these techniques introduce useful randomness into the
algorithm   s behavior, in order to produce an approximate answer that is
very close to the true result.

136

chapter 4. mining data streams

4.2 sampling data in a stream

as our    rst example of managing streaming data, we shall look at extracting
reliable samples from a stream. as with many stream algorithms, the    trick   
involves using hashing in a somewhat unusual way.

4.2.1 a motivating example

the general problem we shall address is selecting a subset of a stream so that we
can ask queries about the selected subset and have the answers be statistically
representative of the stream as a whole.
if we know what queries are to be
asked, then there are a number of methods that might work, but we are looking
for a technique that will allow ad-hoc queries on the sample. we shall look at
a particular problem, from which the general idea will emerge.

our running example is the following. a search engine receives a stream of
queries, and it would like to study the behavior of typical users.1 we assume the
stream consists of tuples (user, query, time). suppose that we want to answer
queries such as    what fraction of the typical user   s queries were repeated over
the past month?    assume also that we wish to store only 1/10th of the stream
elements.

the obvious approach would be to generate a random number, say an integer
from 0 to 9, in response to each search query. store the tuple if and only if the
random number is 0. if we do so, each user has, on average, 1/10th of their
queries stored. statistical    uctuations will introduce some noise into the data,
but if users issue many queries, the law of large numbers will assure us that
most users will have a fraction quite close to 1/10th of their queries stored.

however, this scheme gives us the wrong answer to the query asking for
the average number of duplicate queries for a user. suppose a user has issued
s search queries one time in the past month, d search queries twice, and no
search queries more than twice. if we have a 1/10th sample, of queries, we shall
see in the sample for that user an expected s/10 of the search queries issued
once. of the d search queries issued twice, only d/100 will appear twice in the
sample; that fraction is d times the id203 that both occurrences of the
query will be in the 1/10th sample. of the queries that appear twice in the full
stream, 18d/100 will appear exactly once. to see why, note that 18/100 is the
id203 that one of the two occurrences will be in the 1/10th of the stream
that is selected, while the other is in the 9/10th that is not selected.

the correct answer to the query about the fraction of repeated searches is
d/(s+d). however, the answer we shall obtain from the sample is d/(10s+19d).
to derive the latter formula, note that d/100 appear twice, while s/10+18d/100
appear once. thus, the fraction appearing twice in the sample is d/100 divided

1while we shall refer to    users,    the search engine really receives ip addresses from which
the search query was issued. we shall assume that these ip addresses identify unique users,
which is approximately true, but not exactly true.

4.2. sampling data in a stream

137

by d/100 + s/10 + 18d/100. this ratio is d/(10s + 19d). for no positive values
of s and d is d/(s + d) = d/(10s + 19d).

4.2.2 obtaining a representative sample

the query of section 4.2.1, like many queries about the statistics of typical
users, cannot be answered by taking a sample of each user   s search queries.
thus, we must strive to pick 1/10th of the users, and take all their searches for
the sample, while taking none of the searches from other users. if we can store
a list of all users, and whether or not they are in the sample, then we could
do the following. each time a search query arrives in the stream, we look up
the user to see whether or not they are in the sample. if so, we add this search
query to the sample, and if not, then not. however, if we have no record of
ever having seen this user before, then we generate a random integer between
0 and 9. if the number is 0, we add this user to our list with value    in,    and if
the number is other than 0, we add the user with the value    out.   

that method works as long as we can a   ord to keep the list of all users and
their in/out decision in main memory, because there isn   t time to go to disk for
every search that arrives. by using a hash function, one can avoid keeping the
list of users. that is, we hash each user name to one of ten buckets, 0 through
9. if the user hashes to bucket 0, then accept this search query for the sample,
and if not, then not.

note we do not actually store the user in the bucket; in fact, there is no
data in the buckets at all. e   ectively, we use the hash function as a random-
number generator, with the important property that, when applied to the same
user several times, we always get the same    random    number. that is, without
storing the in/out decision for any user, we can reconstruct that decision any
time a search query by that user arrives.

more generally, we can obtain a sample consisting of any rational fraction
a/b of the users by hashing user names to b buckets, 0 through b     1. add the
search query to the sample if the hash value is less than a.

4.2.3 the general sampling problem

the running example is typical of the following general problem. our stream
consists of tuples with n components. a subset of the components are the key
components, on which the selection of the sample will be based. in our running
example, there are three components     user, query, and time     of which only
user is in the key. however, we could also take a sample of queries by making
query be the key, or even take a sample of user-query pairs by making both
those components form the key.

to take a sample of size a/b, we hash the key value for each tuple to b
buckets, and accept the tuple for the sample if the hash value is less than a.
if the key consists of more than one component, the hash function needs to
combine the values for those components to make a single hash-value. the

138

chapter 4. mining data streams

result will be a sample consisting of all tuples with certain key values. the
selected key values will be approximately a/b of all the key values appearing in
the stream.

4.2.4 varying the sample size

often, the sample will grow as more of the stream enters the system. in our
running example, we retain all the search queries of the selected 1/10th of
the users, forever. as time goes on, more searches for the same users will be
accumulated, and new users that are selected for the sample will appear in the
stream.

if we have a budget for how many tuples from the stream can be stored as
the sample, then the fraction of key values must vary, lowering as time goes
on. in order to assure that at all times, the sample consists of all tuples from a
subset of the key values, we choose a hash function h from key values to a very
large number of values 0, 1, . . . , b   1. we maintain a threshold t, which initially
can be the largest bucket number, b     1. at all times, the sample consists of
those tuples whose key k satis   es h(k)     t. new tuples from the stream are
added to the sample if and only if they satisfy the same condition.
if the number of stored tuples of the sample exceeds the allotted space, we
lower t to t    1 and remove from the sample all those tuples whose key k hashes
to t. for e   ciency, we can lower t by more than 1, and remove the tuples with
several of the highest hash values, whenever we need to throw some key values
out of the sample. further e   ciency is obtained by maintaining an index on
the hash value, so we can    nd all those tuples whose keys hash to a particular
value quickly.

4.2.5 exercises for section 4.2

exercise 4.2.1 : suppose we have a stream of tuples with the schema

grades(university, courseid, studentid, grade)

assume universities are unique, but a courseid is unique only within a uni-
versity (i.e., di   erent universities may have di   erent courses with the same id,
e.g.,    cs101   ) and likewise, studentid   s are unique only within a university
(di   erent universities may assign the same id to di   erent students). suppose
we want to answer certain queries approximately from a 1/20th sample of the
data. for each of the queries below, indicate how you would construct the
sample. that is, tell what the key attributes should be.

(a) for each university, estimate the average number of students in a course.

(b) estimate the fraction of students who have a gpa of 3.5 or more.

(c) estimate the fraction of courses where at least half the students got    a.   

4.3. filtering streams

139

4.3 filtering streams

another common process on streams is selection, or    ltering. we want to
accept those tuples in the stream that meet a criterion. accepted tuples are
passed to another process as a stream, while other tuples are dropped. if the
selection criterion is a property of the tuple that can be calculated (e.g., the
   rst component is less than 10), then the selection is easy to do. the problem
becomes harder when the criterion involves lookup for membership in a set. it
is especially hard, when that set is too large to store in main memory. in this
section, we shall discuss the technique known as    bloom    ltering    as a way to
eliminate most of the tuples that do not meet the criterion.

4.3.1 a motivating example

again let us start with a running example that illustrates the problem and
what we can do about it. suppose we have a set s of one billion allowed email
addresses     those that we will allow through because we believe them not to
be spam. the stream consists of pairs: an email address and the email itself.
since the typical email address is 20 bytes or more, it is not reasonable to store
s in main memory. thus, we can either use disk accesses to determine whether
or not to let through any given stream element, or we can devise a method that
requires no more main memory than we have available, and yet will    lter most
of the undesired stream elements.

suppose for argument   s sake that we have one gigabyte of available main
memory. in the technique known as bloom    ltering, we use that main memory
as a bit array. in this case, we have room for eight billion bits, since one byte
equals eight bits. devise a hash function h from email addresses to eight billion
buckets. hash each member of s to a bit, and set that bit to 1. all other bits
of the array remain 0.

since there are one billion members of s, approximately 1/8th of the bits
will be 1. the exact fraction of bits set to 1 will be slightly less than 1/8th,
because it is possible that two members of s hash to the same bit. we shall
discuss the exact fraction of 1   s in section 4.3.3. when a stream element arrives,
we hash its email address. if the bit to which that email address hashes is 1,
then we let the email through. but if the email address hashes to a 0, we are
certain that the address is not in s, so we can drop this stream element.

unfortunately, some spam email will get through. approximately 1/8th of
the stream elements whose email address is not in s will happen to hash to a
bit whose value is 1 and will be let through. nevertheless, since the majority of
emails are spam (about 80% according to some reports), eliminating 7/8th of
the spam is a signi   cant bene   t. moreover, if we want to eliminate every spam,
we need only check for membership in s those good and bad emails that get
through the    lter. those checks will require the use of secondary memory to
access s itself. there are also other options, as we shall see when we study the
general bloom-   ltering technique. as a simple example, we could use a cascade

140

chapter 4. mining data streams

of    lters, each of which would eliminate 7/8th of the remaining spam.

4.3.2 the bloom filter

a bloom    lter consists of:

1. an array of n bits, initially all 0   s.

2. a collection of hash functions h1, h2, . . . , hk. each hash function maps

   key    values to n buckets, corresponding to the n bits of the bit-array.

3. a set s of m key values.

the purpose of the bloom    lter is to allow through all stream elements whose
keys are in s, while rejecting most of the stream elements whose keys are not
in s.

to initialize the bit array, begin with all bits 0. take each key value in s
and hash it using each of the k hash functions. set to 1 each bit that is hi(k)
for some hash function hi and some key value k in s.

to test a key k that arrives in the stream, check that all of

h1(k), h2(k), . . . , hk(k)

are 1   s in the bit-array. if all are 1   s, then let the stream element through. if
one or more of these bits are 0, then k could not be in s, so reject the stream
element.

4.3.3 analysis of bloom filtering

if a key value is in s, then the element will surely pass through the bloom
   lter. however, if the key value is not in s, it might still pass. we need to
understand how to calculate the id203 of a false positive, as a function of
n, the bit-array length, m the number of members of s, and k, the number of
hash functions.

the model to use is throwing darts at targets. suppose we have x targets
and y darts. any dart is equally likely to hit any target. after throwing the
darts, how many targets can we expect to be hit at least once? the analysis is
similar to the analysis in section 3.4.2, and goes as follows:

    the id203 that a given dart will not hit a given target is (x     1)/x.
x (cid:1)y
    the id203 that none of the y darts will hit a given target is (cid:0) x   1
.
we can write this expression as (1     1
    using the approximation (1      )1/   = 1/e for small    (recall section 1.3.5),
we conclude that the id203 that none of the y darts hit a given target
is e   y/x.

x )x( y

x ).

4.3. filtering streams

141

example 4.3 : consider the running example of section 4.3.1. we can use
the above calculation to get the true expected number of 1   s in the bit array.
think of each bit as a target, and each member of s as a dart. then the
id203 that a given bit will be 1 is the id203 that the corresponding
target will be hit by one or more darts. since there are one billion members of
s, we have y = 109 darts. as there are eight billion bits, there are x = 8    109
targets. thus, the id203 that a given target is not hit is e   y/x = e   1/8
and the id203 that it is hit is 1     e   1/8. that quantity is about 0.1175.
in section 4.3.1 we suggested that 1/8 = 0.125 is a good approximation, which
it is, but now we have the exact calculation.    

we can apply the rule to the more general situation, where set s has m
members, the array has n bits, and there are k hash functions. the number
of targets is x = n, and the number of darts is y = km. thus, the id203
that a bit remains 0 is e   km/n. we want the fraction of 0 bits to be fairly
large, or else the id203 that a nonmember of s will hash at least once to
a 0 becomes too small, and there are too many false positives. for example,
we might choose k, the number of hash functions to be n/m or less. then the
id203 of a 0 is at least e   1 or 37%. in general, the id203 of a false
positive is the id203 of a 1 bit, which is 1     e   km/n, raised to the kth
power, i.e., (1     e   km/n)k.
example 4.4 : in example 4.3 we found that the fraction of 1   s in the array of
our running example is 0.1175, and this fraction is also the id203 of a false
positive. that is, a nonmember of s will pass through the    lter if it hashes to
a 1, and the id203 of it doing so is 0.1175.

suppose we used the same s and the same array, but used two di   erent
hash functions. this situation corresponds to throwing two billion darts at
eight billion targets, and the id203 that a bit remains 0 is e   1/4. in order
to be a false positive, a nonmember of s must hash twice to bits that are 1,
and this id203 is (1     e   1/4)2, or approximately 0.0493. thus, adding a
second hash function for our running example is an improvement, reducing the
false-positive rate from 0.1175 to 0.0493.    

4.3.4 exercises for section 4.3

exercise 4.3.1 : for the situation of our running example (8 billion bits, 1
billion members of the set s), calculate the false-positive rate if we use three
hash functions? what if we use four hash functions?

! exercise 4.3.2 : suppose we have n bits of memory available, and our set s
has m members. instead of using k hash functions, we could divide the n bits
into k arrays, and hash once to each array. as a function of n, m, and k, what
is the id203 of a false positive? how does it compare with using k hash
functions into a single array?

142

chapter 4. mining data streams

!! exercise 4.3.3 : as a function of n, the number of bits and m the number
of members in the set s, what number of hash functions minimizes the false-
positive rate?

4.4 counting distinct elements in a stream

in this section we look at a third simple kind of processing we might want to
do on a stream. as with the previous examples     sampling and    ltering     it is
somewhat tricky to do what we want in a reasonable amount of main memory,
so we use a variety of hashing and a randomized algorithm to get approximately
what we want with little space needed per stream.

4.4.1 the count-distinct problem

suppose stream elements are chosen from some universal set. we would like
to know how many di   erent elements have appeared in the stream, counting
either from the beginning of the stream or from some known time in the past.

example 4.5 : as a useful example of this problem, consider a web site gath-
ering statistics on how many unique users it has seen in each given month. the
universal set is the set of logins for that site, and a stream element is generated
each time someone logs in. this measure is appropriate for a site like amazon,
where the typical user logs in with their unique login name.

a similar problem is a web site like google that does not require login to
issue a search query, and may be able to identify users only by the ip address
from which they send the query. there are about 4 billion ip addresses,2
sequences of four 8-bit bytes will serve as the universal set in this case.    

the obvious way to solve the problem is to keep in main memory a list of all
the elements seen so far in the stream. keep them in an e   cient search structure
such as a hash table or search tree, so one can quickly add new elements and
check whether or not the element that just arrived on the stream was already
seen. as long as the number of distinct elements is not too great, this structure
can    t in main memory and there is little problem obtaining an exact answer
to the question how many distinct elements appear in the stream.

however, if the number of distinct elements is too great, or if there are too
many streams that need to be processed at once (e.g., yahoo! wants to count
the number of unique users viewing each of its pages in a month), then we
cannot store the needed data in main memory. there are several options. we
could use more machines, each machine handling only one or several of the
streams. we could store most of the data structure in secondary memory and
batch stream elements so whenever we brought a disk block to main memory
there would be many tests and updates to be performed on the data in that
block. or we could use the strategy to be discussed in this section, where we

2at least that will be the case until ipv6 becomes the norm.

4.4. counting distinct elements in a stream

143

only estimate the number of distinct elements but use much less memory than
the number of distinct elements.

4.4.2 the flajolet-martin algorithm

it is possible to estimate the number of distinct elements by hashing the ele-
ments of the universal set to a bit-string that is su   ciently long. the length of
the bit-string must be su   cient that there are more possible results of the hash
function than there are elements of the universal set. for example, 64 bits is
su   cient to hash url   s. we shall pick many di   erent hash functions and hash
each element of the stream using these hash functions. the important property
of a hash function is that when applied to the same element, it always produces
the same result. notice that this property was also essential for the sampling
technique of section 4.2.

the idea behind the flajolet-martin algorithm is that the more di   erent
elements we see in the stream, the more di   erent hash-values we shall see. as
we see more di   erent hash-values, it becomes more likely that one of these
values will be    unusual.    the particular unusual property we shall exploit is
that the value ends in many 0   s, although many other options exist.

whenever we apply a hash function h to a stream element a, the bit string
h(a) will end in some number of 0   s, possibly none. call this number the tail
length for a and h. let r be the maximum tail length of any a seen so far in
the stream. then we shall use estimate 2r for the number of distinct elements
seen in the stream.

this estimate makes intuitive sense. the id203 that a given stream
element a has h(a) ending in at least r 0   s is 2   r. suppose there are m distinct
elements in the stream. then the id203 that none of them has tail length
at least r is (1     2   r)m. this sort of expression should be familiar by now.
we can rewrite it as (cid:0)(1     2   r)2r(cid:1)m2   r
. assuming r is reasonably large, the
inner expression is of the form (1       )1/  , which is approximately 1/e. thus,
the id203 of not    nding a stream element with as many as r 0   s at the
end of its hash value is e   m2   r

. we can conclude:

1. if m is much larger than 2r, then the id203 that we shall    nd a tail

of length at least r approaches 1.

2. if m is much less than 2r, then the id203 of    nding a tail length at

least r approaches 0.

we conclude from these two points that the proposed estimate of m, which is
2r (recall r is the largest tail length for any stream element) is unlikely to be
either much too high or much too low.

144

chapter 4. mining data streams

4.4.3 combining estimates

unfortunately, there is a trap regarding the strategy for combining the estimates
of m, the number of distinct elements, that we obtain by using many di   erent
hash functions. our    rst assumption would be that if we take the average of
the values 2r that we get from each hash function, we shall get a value that
approaches the true m, the more hash functions we use. however, that is not
the case, and the reason has to do with the in   uence an overestimate has on
the average.

consider a value of r such that 2r is much larger than m. there is some
id203 p that we shall discover r to be the largest number of 0   s at the end
of the hash value for any of the m stream elements. then the id203 of
   nding r + 1 to be the largest number of 0   s instead is at least p/2. however, if
we do increase by 1 the number of 0   s at the end of a hash value, the value of
2r doubles. consequently, the contribution from each possible large r to the
expected value of 2r grows as r grows, and the expected value of 2r is actually
in   nite.3

another way to combine estimates is to take the median of all estimates.
the median is not a   ected by the occasional outsized value of 2r, so the worry
described above for the average should not carry over to the median. unfortu-
nately, the median su   ers from another defect: it is always a power of 2. thus,
no matter how many hash functions we use, should the correct value of m be
between two powers of 2, say 400, then it will be impossible to obtain a close
estimate.

there is a solution to the problem, however. we can combine the two
methods. first, group the hash functions into small groups, and take their
average. then, take the median of the averages. it is true that an occasional
outsized 2r will bias some of the groups and make them too large. however,
taking the median of group averages will reduce the in   uence of this e   ect
almost to nothing. moreover, if the groups themselves are large enough, then
the averages can be essentially any number, which enables us to approach the
true value m as long as we use enough hash functions. in order to guarantee
that any possible average can be obtained, groups should be of size at least a
small multiple of log2 m.

4.4.4 space requirements

observe that as we read the stream it is not necessary to store the elements
seen. the only thing we need to keep in main memory is one integer per hash
function; this integer records the largest tail length seen so far for that hash
function and any stream element.
if we are processing only one stream, we
could use millions of hash functions, which is far more than we need to get a

3technically, since the hash value is a bit-string of    nite length, there is no contribution
to 2r for r   s that are larger than the length of the hash value. however, this e   ect is not
enough to avoid the conclusion that the expected value of 2r is much too large.

4.5. estimating moments

145

close estimate. only if we are trying to process many streams at the same time
would main memory constrain the number of hash functions we could associate
with any one stream. in practice, the time it takes to compute hash values for
each stream element would be the more signi   cant limitation on the number of
hash functions we use.

4.4.5 exercises for section 4.4

exercise 4.4.1 : suppose our stream consists of the integers 3, 1, 4, 1, 5, 9, 2,
6, 5. our hash functions will all be of the form h(x) = ax + b mod 32 for some
a and b. you should treat the result as a 5-bit binary integer. determine the
tail length for each stream element and the resulting estimate of the number of
distinct elements if the hash function is:

(a) h(x) = 2x + 1 mod 32.

(b) h(x) = 3x + 7 mod 32.

(c) h(x) = 4x mod 32.

! exercise 4.4.2 : do you see any problems with the choice of hash functions in
exercise 4.4.1? what advice could you give someone who was going to use a
hash function of the form h(x) = ax + b mod 2k?

4.5 estimating moments

in this section we consider a generalization of the problem of counting distinct
elements in a stream. the problem, called computing    moments,    involves the
distribution of frequencies of di   erent elements in the stream. we shall de   ne
moments of all orders and concentrate on computing second moments, from
which the general algorithm for all moments is a simple extension.

4.5.1 de   nition of moments

suppose a stream consists of elements chosen from a universal set. assume the
universal set is ordered so we can speak of the ith element for any i. let mi
be the number of occurrences of the ith element for any i. then the kth-order
moment (or just kth moment) of the stream is the sum over all i of (mi)k.

example 4.6 : the 0th moment is the sum of 1 for each mi that is greater than
0.4 that is, the 0th moment is a count of the number of distinct elements in
the stream. we can use the method of section 4.4 to estimate the 0th moment
of a stream.

4technically, since mi could be 0 for some elements in the universal set, we need to make
explicit in the de   nition of    moment    that 00 is taken to be 0. for moments 1 and above,
the contribution of mi   s that are 0 is surely 0.

146

chapter 4. mining data streams

the 1st moment is the sum of the mi   s, which must be the length of the
stream. thus,    rst moments are especially easy to compute; just count the
length of the stream seen so far.

the second moment is the sum of the squares of the mi   s.

it is some-
times called the surprise number, since it measures how uneven the distribu-
tion of elements in the stream is. to see the distinction, suppose we have a
stream of length 100, in which eleven di   erent elements appear. the most
even distribution of these eleven elements would have one appearing 10 times
and the other ten appearing 9 times each. in this case, the surprise number is
102 + 10    92 = 910. at the other extreme, one of the eleven elements could
appear 90 times and the other ten appear 1 time each. then, the surprise
number would be 902 + 10    12 = 8110.    

as in section 4.4, there is no problem computing moments of any order if we
can a   ord to keep in main memory a count for each element that appears in the
stream. however, also as in that section, if we cannot a   ord to use that much
memory, then we need to estimate the kth moment by keeping a limited number
of values in main memory and computing an estimate from these values. for
the case of distinct elements, each of these values were counts of the longest tail
produced by a single hash function. we shall see another form of value that is
useful for second and higher moments.

4.5.2 the alon-matias-szegedy algorithm for second

moments

for now, let us assume that a stream has a particular length n. we shall show
how to deal with growing streams in the next section. suppose we do not have
enough space to count all the mi   s for all the elements of the stream. we can
still estimate the second moment of the stream using a limited amount of space;
the more space we use, the more accurate the estimate will be. we compute
some number of variables. for each variable x, we store:

1. a particular element of the universal set, which we refer to as x.element ,

and

2. an integer x.value, which is the value of the variable. to determine the
value of a variable x, we choose a position in the stream between 1 and n,
uniformly and at random. set x.element to be the element found there,
and initialize x.value to 1. as we read the stream, add 1 to x.value each
time we encounter another occurrence of x.element .

example 4.7 : suppose the stream is a, b, c, b, d, a, c, d, a, b, d, c, a, a, b. the
length of the stream is n = 15. since a appears 5 times, b appears 4 times,
and c and d appear three times each, the second moment for the stream is
52 + 42 + 32 + 32 = 59. suppose we keep three variables, x1, x2, and x3. also,

4.5. estimating moments

147

assume that at    random    we pick the 3rd, 8th, and 13th positions to de   ne
these three variables.

when we reach position 3, we    nd element c, so we set x1.element = c
and x1.value = 1. position 4 holds b, so we do not change x1. likewise,
nothing happens at positions 5 or 6. at position 7, we see c again, so we set
x1.value = 2.

at position 8 we    nd d, and so set x2.element = d and x2.value = 1.
positions 9 and 10 hold a and b, so they do not a   ect x1 or x2. position 11
holds d so we set x2.value = 2, and position 12 holds c so we set x1.value = 3.
at position 13, we    nd element a, and so set x3.element = a and x3.value = 1.
then, at position 14 we see another a and so set x3.value = 2. position 15,
with element b does not a   ect any of the variables, so we are done, with    nal
values x1.value = 3 and x2.value = x3.value = 2.    

we can derive an estimate of the second moment from any variable x. this

estimate is n(2x.value     1).
example 4.8 : consider the three variables from example 4.7. from x1 we
derive the estimate n(2x1.value     1) = 15    (2    3     1) = 75. the other
two variables, x2 and x3, each have value 2 at the end, so their estimates are
15    (2    2     1) = 45. recall that the true value of the second moment for this
stream is 59. on the other hand, the average of the three estimates is 55, a
fairly close approximation.    

4.5.3 why the alon-matias-szegedy algorithm works

we can prove that the expected value of any variable constructed as in sec-
tion 4.5.2 is the second moment of the stream from which it is constructed.
some notation will make the argument easier to follow. let e(i) be the stream
element that appears at position i in the stream, and let c(i) be the number of
times element e(i) appears in the stream among positions i, i + 1, . . . , n.

example 4.9 : consider the stream of example 4.7. e(6) = a, since the 6th
position holds a. also, c(6) = 4, since a appears at positions 9, 13, and 14, as
well as at position 6. note that a also appears at position 1, but that fact does
not contribute to c(6).    

the expected value of n(2x.value     1) is the average over all positions i

between 1 and n of n(2c(i)     1), that is
e(cid:0)n(2x.value     1)(cid:1) =

1
n

n

xi=1

n(2c(i)     1)

we can simplify the above by canceling factors 1/n and n, to get

e(cid:0)n(2x.value     1)(cid:1) =

xi=1(cid:0)2c(i)     1(cid:1)

n

148

chapter 4. mining data streams

however, to make sense of the formula, we need to change the order of
summation by grouping all those positions that have the same element. for
instance, concentrate on some element a that appears ma times in the stream.
the term for the last position in which a appears must be 2    1     1 = 1. the
term for the next-to-last position in which a appears is 2    2     1 = 3. the
positions with a before that yield terms 5, 7, and so on, up to 2ma     1, which
is the term for the    rst position in which a appears. that is, the formula for
the expected value of 2x.value     1 can be written:

e(cid:0)n(2x.value     1)(cid:1) = xa

1 + 3 + 5 +        + (2ma     1)

note that 1 + 3 + 5 +      + (2ma    1) = (ma)2. the proof is an easy induction
on the number of terms in the sum. thus, e(cid:0)n(2x.value     1)(cid:1) = pa(ma)2,

which is the de   nition of the second moment.

4.5.4 higher-order moments

we estimate kth moments, for k > 2, in essentially the same way as we estimate
second moments. the only thing that changes is the way we derive an estimate
from a variable. in section 4.5.2 we used the formula n(2v     1) to turn a value
v, the count of the number of occurrences of some particular stream element
a, into an estimate of the second moment. then, in section 4.5.3 we saw why
this formula works: the terms 2v     1, for v = 1, 2, . . . , m sum to m2, where m
is the number of times a appears in the stream.
notice that 2v     1 is the di   erence between v2 and (v     1)2. suppose we
wanted the third moment rather than the second. then all we have to do is
replace 2v   1 by v3   (v   1)3 = 3v2   3v+1. then pm
v=1 3v2   3v+1 = m3, so we
can use as our estimate of the third moment the formula n(3v2     3v + 1), where
v = x.value is the value associated with some variable x. more generally, we
can estimate kth moments for any k     2 by turning value v = x.value into
n(cid:0)vk     (v     1)k(cid:1).

4.5.5 dealing with in   nite streams

technically, the estimate we used for second and higher moments assumes that
n, the stream length, is a constant. in practice, n grows with time. that fact,
by itself, doesn   t cause problems, since we store only the values of variables
and multiply some function of that value by n when it is time to estimate the
moment. if we count the number of stream elements seen and store this value,
which only requires log n bits, then we have n available whenever we need it.

a more serious problem is that we must be careful how we select the positions
for the variables. if we do this selection once and for all, then as the stream gets
longer, we are biased in favor of early positions, and the estimate of the moment
will be too large. on the other hand, if we wait too long to pick positions, then

4.5. estimating moments

149

early in the stream we do not have many variables and so will get an unreliable
estimate.

the proper technique is to maintain as many variables as we can store at
all times, and to throw some out as the stream grows. the discarded variables
are replaced by new ones, in such a way that at all times, the id203 of
picking any one position for a variable is the same as that of picking any other
position. suppose we have space to store s variables. then the    rst s positions
of the stream are each picked as the position of one of the s variables.

inductively, suppose we have seen n stream elements, and the id203 of
any particular position being the position of a variable is uniform, that is s/n.
when the (n+1)st element arrives, pick that position with id203 s/(n+1).
if not picked, then the s variables keep their same positions. however, if the
(n + 1)st position is picked, then throw out one of the current s variables, with
equal id203. replace the one discarded by a new variable whose element
is the one at position n + 1 and whose value is 1.

surely, the id203 that position n + 1 is selected for a variable is what
it should be: s/(n + 1). however, the id203 of every other position also
is s/(n + 1), as we can prove by induction on n. by the inductive hypothesis,
before the arrival of the (n + 1)st stream element, this id203 was s/n.
with id203 1     s/(n + 1) the (n + 1)st position will not be selected, and
the id203 of each of the    rst n positions remains s/n. however, with
id203 s/(n + 1), the (n + 1)st position is picked, and the id203 for
each of the    rst n positions is reduced by factor (s    1)/s. considering the two
cases, the id203 of selecting each of the    rst n positions is

(cid:0)1    

s

n + 1(cid:1)(cid:0)

s

n(cid:1) +(cid:0)

s

n + 1(cid:1)(cid:0)

s     1
s

s

n(cid:1)

(cid:1)(cid:0)

this expression simpli   es to

(cid:0)1    
(cid:16)(cid:0)1    

s

s

n + 1(cid:1)(cid:0)

n(cid:1) +(cid:0)

s

n + 1(cid:1) +(cid:0)

s     1
n + 1(cid:1)(cid:0)
s     1
n + 1(cid:1)(cid:17)(cid:0)

s

n(cid:1)

s

n(cid:1)

and then to

which in turn simpli   es to

n

n + 1(cid:1)(cid:0)

s

n(cid:1) =

(cid:0)

s

n + 1

thus, we have shown by induction on the stream length n that all positions
have equal id203 s/n of being chosen as the position of a variable.

4.5.6 exercises for section 4.5

exercise 4.5.1 : compute the surprise number (second moment) for the stream
3, 1, 4, 1, 3, 4, 2, 1, 2. what is the third moment of this stream?

150

chapter 4. mining data streams

a general stream-sampling problem

notice that the technique described in section 4.5.5 actually solves a more
general problem.
it gives us a way to maintain a sample of s stream
elements so that at all times, all stream elements are equally likely to be
selected for the sample.

as an example of where this technique can be useful, recall that in
section 4.2 we arranged to select all the tuples of a stream having key
value in a randomly selected subset. suppose that, as time goes on, there
are too many tuples associated with any one key. we can arrange to limit
the number of tuples for any key k to a    xed constant s by using the
technique of section 4.5.5 whenever a new tuple for key k arrives.

! exercise 4.5.2 : if a stream has n elements, of which m are distinct, what are
the minimum and maximum possible surprise number, as a function of m and
n?

exercise 4.5.3 : suppose we are given the stream of exercise 4.5.1, to which
we apply the alon-matias-szegedy algorithm to estimate the surprise number.
for each possible value of i, if xi is a variable starting position i, what is the
value of xi.value?

exercise 4.5.4 : repeat exercise 4.5.3 if the intent of the variables is to com-
pute third moments. what is the value of each variable at the end? what
estimate of the third moment do you get from each variable? how does the
average of these estimates compare with the true value of the third moment?

exercise 4.5.5 : prove by induction on m that 1 + 3 + 5 +       + (2m    1) = m2.

exercise 4.5.6 :
convert x.value to an estimate of the fourth moment?

if we wanted to compute fourth moments, how would we

4.6 counting ones in a window

we now turn our attention to counting problems for streams. suppose we have
a window of length n on a binary stream. we want at all times to be able to
answer queries of the form    how many 1   s are there in the last k bits?    for any
k     n . as in previous sections, we focus on the situation where we cannot
a   ord to store the entire window. after showing an approximate algorithm for
the binary case, we discuss how this idea can be extended to summing numbers.

4.6. counting ones in a window

151

4.6.1 the cost of exact counts

to begin, suppose we want to be able to count exactly the number of 1   s in
the last k bits for any k     n . then we claim it is necessary to store all n
bits of the window, as any representation that used fewer than n bits could
not work. in proof, suppose we have a representation that uses fewer than n
bits to represent the n bits in the window. since there are 2n sequences of n
bits, but fewer than 2n representations, there must be two di   erent bit strings
w and x that have the same representation. since w 6= x, they must di   er in
at least one bit. let the last k     1 bits of w and x agree, but let them di   er on
the kth bit from the right end.

example 4.10 : if w = 0101 and x = 1010, then k = 1, since scanning from
the right, they    rst disagree at position 1. if w = 1001 and x = 0101, then
k = 3, because they    rst disagree at the third position from the right.    

suppose the data representing the contents of the window is whatever se-
quence of bits represents both w and x. ask the query    how many 1   s are in
the last k bits?    the query-answering algorithm will produce the same an-
swer, whether the window contains w or x, because the algorithm can only see
their representation. but the correct answers are surely di   erent for these two
bit-strings. thus, we have proved that we must use at least n bits to answer
queries about the last k bits for any k.

in fact, we need n bits, even if the only query we can ask is    how many
1   s are in the entire window of length n ?    the argument is similar to that
used above. suppose we use fewer than n bits to represent the window, and
therefore we can    nd w, x, and k as above. it might be that w and x have
the same number of 1   s, as they did in both cases of example 4.10. however,
if we follow the current window by any n     k bits, we will have a situation
where the true window contents resulting from w and x are identical except for
the leftmost bit, and therefore, their counts of 1   s are unequal. however, since
the representations of w and x are the same, the representation of the window
must still be the same if we feed the same bit sequence to these representations.
thus, we can force the answer to the query    how many 1   s in the window?    to
be incorrect for one of the two possible window contents.

4.6.2 the datar-gionis-indyk-motwani algorithm

we shall present the simplest case of an algorithm called dgim. this version of
the algorithm uses o(log2 n ) bits to represent a window of n bits, and allows
us to estimate the number of 1   s in the window with an error of no more than
50%. later, we shall discuss an improvement of the method that limits the
error to any fraction    > 0, and still uses only o(log2 n ) bits (although with a
constant factor that grows as    shrinks).

to begin, each bit of the stream has a timestamp, the position in which it
arrives. the    rst bit has timestamp 1, the second has timestamp 2, and so on.

152

chapter 4. mining data streams

since we only need to distinguish positions within the window of length n , we
shall represent timestamps modulo n , so they can be represented by log2 n
bits. if we also store the total number of bits ever seen in the stream (i.e., the
most recent timestamp) modulo n , then we can determine from a timestamp
modulo n where in the current window the bit with that timestamp is.

we divide the window into buckets,5 consisting of:

1. the timestamp of its right (most recent) end.

2. the number of 1   s in the bucket. this number must be a power of 2, and

we refer to the number of 1   s as the size of the bucket.

to represent a bucket, we need log2 n bits to represent the timestamp (modulo
n ) of its right end. to represent the number of 1   s we only need log2 log2 n
bits. the reason is that we know this number i is a power of 2, say 2j, so we
can represent i by coding j in binary. since j is at most log2 n , it requires
log2 log2 n bits. thus, o(log n ) bits su   ce to represent a bucket.

there are six rules that must be followed when representing a stream by

buckets.

    the right end of a bucket is always a position with a 1.
    every position with a 1 is in some bucket.
    no position is in more than one bucket.
    there are one or two buckets of any given size, up to some maximum size.
    all sizes must be a power of 2.
    buckets cannot decrease in size as we move to the left (back in time).

. . 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0

. . . 1 0 1

1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1

0 1 1 0

at least one
of size 8

two of size 4

one of
size 2

two of
size 1

figure 4.2: a bit-stream divided into buckets following the dgim rules

5do not confuse these    buckets    with the    buckets    discussed in connection with hashing.

4.6. counting ones in a window

153

example 4.11 : figure 4.2 shows a bit stream divided into buckets in a way
that satis   es the dgim rules. at the right (most recent) end we see two buckets
of size 1. to its left we see one bucket of size 2. note that this bucket covers
four positions, but only two of them are 1. proceeding left, we see two buckets
of size 4, and we suggest that a bucket of size 8 exists further left.

notice that it is ok for some 0   s to lie between buckets. also, observe from
fig. 4.2 that the buckets do not overlap; there are one or two of each size up to
the largest size, and sizes only increase moving left.    

in the next sections, we shall explain the following about the dgim algo-

rithm:

1. why the number of buckets representing a window must be small.

2. how to estimate the number of 1   s in the last k bits for any k, with an

error no greater than 50%.

3. how to maintain the dgim conditions as new bits enter the stream.

4.6.3 storage requirements for the dgim algorithm

if the
we observed that each bucket can be represented by o(log n ) bits.
window has length n , then there are no more than n 1   s, surely. suppose the
largest bucket is of size 2j. then j cannot exceed log2 n , or else there are more
1   s in this bucket than there are 1   s in the entire window. thus, there are at
most two buckets of all sizes from log2 n down to 1, and no buckets of larger
sizes.

we conclude that there are o(log n ) buckets. since each bucket can be
represented in o(log n ) bits, the total space required for all the buckets repre-
senting a window of size n is o(log2 n ).

4.6.4 query answering in the dgim algorithm

suppose we are asked how many 1   s there are in the last k bits of the window,
for some 1     k     n . find the bucket b with the earliest timestamp that
includes at least some of the k most recent bits. estimate the number of 1   s to
be the sum of the sizes of all the buckets to the right (more recent) than bucket
b, plus half the size of b itself.

example 4.12 : suppose the stream is that of fig. 4.2, and k = 10. then the
query asks for the number of 1   s in the ten rightmost bits, which happen to be
0110010110. let the current timestamp (time of the rightmost bit) be t. then
the two buckets with one 1, having timestamps t     1 and t     2 are completely
included in the answer. the bucket of size 2, with timestamp t     4, is also
completely included. however, the rightmost bucket of size 4, with timestamp
t    8 is only partly included. we know it is the last bucket to contribute to the
answer, because the next bucket to its left has timestamp less than t     9 and

154

chapter 4. mining data streams

thus is completely out of the window. on the other hand, we know the buckets
to its right are completely inside the range of the query because of the existence
of a bucket to their left with timestamp t     9 or greater.
our estimate of the number of 1   s in the last ten positions is thus 6. this
number is the two buckets of size 1, the bucket of size 2, and half the bucket of
size 4 that is partially within range. of course the correct answer is 5.    

suppose the above estimate of the answer to a query involves a bucket b
of size 2j that is partially within the range of the query. let us consider how
far from the correct answer c our estimate could be. there are two cases: the
estimate could be larger or smaller than c.

case 1 : the estimate is less than c. in the worst case, all the 1   s of b are
actually within the range of the query, so the estimate misses half bucket b, or
2j   1 1   s. but in this case, c is at least 2j; in fact it is at least 2j+1     1, since
there is at least one bucket of each of the sizes 2j   1, 2j   2, . . . , 1. we conclude
that our estimate is at least 50% of c.

case 2 : the estimate is greater than c. in the worst case, only the rightmost
bit of bucket b is within range, and there is only one bucket of each of the sizes
smaller than b. then c = 1 + 2j   1 + 2j   2 +        + 1 = 2j and the estimate we
give is 2j   1 + 2j   1 + 2j   2 +        + 1 = 2j + 2j   1     1. we see that the estimate
is no more than 50% greater than c.

4.6.5 maintaining the dgim conditions

suppose we have a window of length n properly represented by buckets that
satisfy the dgim conditions. when a new bit comes in, we may need to modify
the buckets, so they continue to represent the window and continue to satisfy
the dgim conditions. first, whenever a new bit enters:

    check the leftmost (earliest) bucket. if its timestamp has now reached
the current timestamp minus n , then this bucket no longer has any of its
1   s in the window. therefore, drop it from the list of buckets.

now, we must consider whether the new bit is 0 or 1. if it is 0, then no
further change to the buckets is needed. if the new bit is a 1, however, we may
need to make several changes. first:

    create a new bucket with the current timestamp and size 1.
if there was only one bucket of size 1, then nothing more needs to be done.
however, if there are now three buckets of size 1, that is one too many. we    x
this problem by combining the leftmost (earliest) two buckets of size 1.

    to combine any two adjacent buckets of the same size, replace them by
one bucket of twice the size. the timestamp of the new bucket is the
timestamp of the rightmost (later in time) of the two buckets.

4.6. counting ones in a window

155

combining two buckets of size 1 may create a third bucket of size 2. if so,
we combine the leftmost two buckets of size 2 into a bucket of size 4. that, in
turn, may create a third bucket of size 4, and if so we combine the leftmost two
into a bucket of size 8. this process may ripple through the bucket sizes, but
there are at most log2 n di   erent sizes, and the combination of two adjacent
buckets of the same size only requires constant time. as a result, any new bit
can be processed in o(log n ) time.

example 4.13 : suppose we start with the buckets of fig. 4.2 and a 1 enters.
first, the leftmost bucket evidently has not fallen out of the window, so we
do not drop any buckets. we create a new bucket of size 1 with the current
timestamp, say t. there are now three buckets of size 1, so we combine the
leftmost two. they are replaced with a single bucket of size 2. its timestamp is
t     2, the timestamp of the bucket on the right (i.e., the rightmost bucket that
actually appears in fig. 4.2.

. . 1 0 1

1 0 1 1 0 0 0 1

0

1 1 1 0 1

1 0 0 1

0

1 1

10

at least one
of size 8

two of size 4

two of
size 2

one of
size 1

figure 4.3: modi   ed buckets after a new 1 arrives in the stream

there are now two buckets of size 2, but that is allowed by the dgim rules.
thus, the    nal sequence of buckets after the addition of the 1 is as shown in
fig. 4.3.    

4.6.6 reducing the error

instead of allowing either one or two of each size bucket, suppose we allow either
r     1 or r of each of the exponentially growing sizes 1, 2, 4, . . ., for some integer
r > 2. in order to represent any possible number of 1   s, we must relax this
condition for the buckets of size 1 and buckets of the largest size present; there
may be any number, from 1 to r, of buckets of these sizes.

the rule for combining buckets is essentially the same as in section 4.6.5. if
we get r + 1 buckets of size 2j, combine the leftmost two into a bucket of size
2j+1. that may, in turn, cause there to be r + 1 buckets of size 2j+1, and if so
we continue combining buckets of larger sizes.

the argument used in section 4.6.4 can also be used here. however, because
there are more buckets of smaller sizes, we can get a stronger bound on the error.
we saw there that the largest relative error occurs when only one 1 from the
leftmost bucket b is within the query range, and we therefore overestimate the
true count. suppose bucket b is of size 2j. then the true count is at least

156

chapter 4. mining data streams

bucket sizes and ripple-carry adders

there is a pattern to the distribution of bucket sizes as we execute the
basic algorithm of section 4.6.5. think of two buckets of size 2j as a    1   
in position j and one bucket of size 2j as a    0    in that position. then
as 1   s arrive in the stream, the bucket sizes after each 1 form consecutive
binary integers. the occasional long sequences of bucket combinations
are analogous to the occasional long rippling of carries as we go from an
integer like 101111 to 110000.

1 + (r     1)(2j   1 + 2j   2 +        + 1) = 1 + (r     1)(2j     1). the overestimate is
2j   1     1. thus, the fractional error is

2j   1     1

1 + (r     1)(2j     1)

no matter what j is, this fraction is upper bounded by 1/(r     1). thus, by
picking r su   ciently large, we can limit the error to any desired    > 0.

4.6.7 extensions to the counting of ones

it is natural to ask whether we can extend the technique of this section to
handle aggregations more general than counting 1   s in a binary stream. an
obvious direction to look is to consider streams of integers and ask if we can
estimate the sum of the last k integers for any 1     k     n , where n , as usual,
is the window size.
it is unlikely that we can use the dgim approach to streams containing
both positive and negative integers. we could have a stream containing both
very large positive integers and very large negative integers, but with a sum in
the window that is very close to 0. any imprecision in estimating the values of
these large integers would have a huge e   ect on the estimate of the sum, and
so the fractional error could be unbounded.

for example, suppose we broke the stream into buckets as we have done, but
represented the bucket by the sum of the integers therein, rather than the count
of 1   s. if b is the bucket that is partially within the query range, it could be that
b has, in its    rst half, very large negative integers and in its second half, equally
large positive integers, with a sum of 0. if we estimate the contribution of b by
half its sum, that contribution is essentially 0. but the actual contribution of
that part of bucket b that is in the query range could be anything from 0 to the
sum of all the positive integers. this di   erence could be far greater than the
actual query answer, and so the estimate would be meaningless.

on the other hand, some other extensions involving integers do work. sup-
pose that the stream consists of only positive integers in the range 1 to 2m for

4.7. decaying windows

157

some m. we can treat each of the m bits of each integer as if it were a separate
stream. we then use the dgim method to count the 1   s in each bit. suppose
the count of the ith bit (assuming bits count from the low-order end, starting
at 0) is ci. then the sum of the integers is

m   1

xi=0

ci2i

if we use the technique of section 4.6.6 to estimate each ci with fractional error
at most   , then the estimate of the true sum has error at most   . the worst
case occurs when all the ci   s are overestimated or all are underestimated by the
same fraction.

4.6.8 exercises for section 4.6

exercise 4.6.1 : suppose the window is as shown in fig. 4.2. estimate the
number of 1   s the the last k positions, for k = (a) 5 (b) 15. in each case, how
far o    the correct value is your estimate?

! exercise 4.6.2 : there are several ways that the bit-stream 1001011011101

could be partitioned into buckets. find all of them.

exercise 4.6.3 : describe what happens to the buckets if three more 1   s enter
the window represented by fig. 4.3. you may assume none of the 1   s shown
leave the window.

4.7 decaying windows

we have assumed that a sliding window held a certain tail of the stream, either
the most recent n elements for    xed n , or all the elements that arrived after
some time in the past. sometimes we do not want to make a sharp distinction
between recent elements and those in the distant past, but want to weight
the recent elements more heavily. in this section, we consider    exponentially
decaying windows,    and an application where they are quite useful:    nding the
most common    recent    elements.

4.7.1 the problem of most-common elements

suppose we have a stream whose elements are the movie tickets purchased all
over the world, with the name of the movie as part of the element. we want
to keep a summary of the stream that is the most popular movies    currently.   
while the notion of    currently    is imprecise, intuitively, we want to discount
the popularity of a movie like star wars   episode 4, which sold many tickets,
but most of these were sold decades ago. on the other hand, a movie that sold

158

chapter 4. mining data streams

n tickets in each of the last 10 weeks is probably more popular than a movie
that sold 2n tickets last week but nothing in previous weeks.

one solution would be to imagine a bit stream for each movie. the ith bit
has value 1 if the ith ticket is for that movie, and 0 otherwise. pick a window
size n , which is the number of most recent tickets that would be considered
in evaluating popularity. then, use the method of section 4.6 to estimate the
number of tickets for each movie, and rank movies by their estimated counts.
this technique might work for movies, because there are only thousands of
movies, but it would fail if we were instead recording the popularity of items
sold at amazon, or the rate at which di   erent twitter-users tweet, because
there are too many amazon products and too many tweeters. further, it only
o   ers approximate answers.

4.7.2 de   nition of the decaying window

an alternative approach is to rede   ne the question so that we are not asking
for a count of 1   s in a window. rather, let us compute a smooth aggregation of
all the 1   s ever seen in the stream, with decaying weights, so the further back
in the stream, the less weight is given. formally, let a stream currently consist
of the elements a1, a2, . . . , at, where a1 is the    rst element to arrive and at is
the current element. let c be a small constant, such as 10   6 or 10   9. de   ne
the exponentially decaying window for this stream to be the sum

t   1

xi=0

at   i(1     c)i

the e   ect of this de   nition is to spread out the weights of the stream el-
ements as far back in time as the stream goes.
in contrast, a    xed window
with the same sum of the weights, 1/c, would put equal weight 1 on each of the
most recent 1/c elements to arrive and weight 0 on all previous elements. the
distinction is suggested by fig. 4.4.

window of
length 1/c

figure 4.4: a decaying window and a    xed-length window of equal weight

it is much easier to adjust the sum in an exponentially decaying window
than in a sliding window of    xed length.
in the sliding window, we have to
worry about the element that falls out of the window each time a new element
arrives. that forces us to keep the exact elements along with the sum, or to use

4.7. decaying windows

159

an approximation scheme such as dgim. however, when a new element at+1
arrives at the stream input, all we need to do is:

1. multiply the current sum by 1     c.
2. add at+1.

the reason this method works is that each of the previous elements has now
moved one position further from the current element, so its weight is multiplied
by 1     c. further, the weight on the current element is (1     c)0 = 1, so adding
at+1 is the correct way to include the new element   s contribution.

4.7.3 finding the most popular elements

let us return to the problem of    nding the most popular movies in a stream of
ticket sales.6 we shall use an exponentially decaying window with a constant
c, which you might think of as 10   9. that is, we approximate a sliding window
holding the last one billion ticket sales. for each movie, we imagine a separate
stream with a 1 each time a ticket for that movie appears in the stream, and a
0 each time a ticket for some other movie arrives. the decaying sum of the 1   s
measures the current popularity of the movie.

we imagine that the number of possible movies in the stream is huge, so we
do not want to record values for the unpopular movies. therefore, we establish
a threshold, say 1/2, so that if the popularity score for a movie goes below this
number, its score is dropped from the counting. for reasons that will become
obvious, the threshold must be less than 1, although it can be any number less
than 1. when a new ticket arrives on the stream, do the following:

1. for each movie whose score we are currently maintaining, multiply its

score by (1     c).

2. suppose the new ticket is for movie m . if there is currently a score for m ,
add 1 to that score. if there is no score for m , create one and initialize it
to 1.

3. if any score is below the threshold 1/2, drop that score.

it may not be obvious that the number of movies whose scores are main-
tained at any time is limited. however, note that the sum of all scores is 1/c.
there cannot be more than 2/c movies with score of 1/2 or more, or else the
sum of the scores would exceed 1/c. thus, 2/c is a limit on the number of
movies being counted at any time. of course in practice, the ticket sales would
be concentrated on only a small number of movies at any time, so the number
of actively counted movies would be much less than 2/c.

6this example should be taken with a grain of salt, because, as we pointed out, there
aren   t enough di   erent movies for this technique to be essential. imagine, if you will, that
the number of movies is extremely large, so counting ticket sales of each one separately is not
feasible.

160

chapter 4. mining data streams

4.8 summary of chapter 4

    the stream data model : this model assumes data arrives at a processing
engine at a rate that makes it infeasible to store everything in active
storage. one strategy to dealing with streams is to maintain summaries
of the streams, su   cient to answer the expected queries about the data.
a second approach is to maintain a sliding window of the most recently
arrived data.

    sampling of streams: to create a sample of a stream that is usable for
a class of queries, we identify a set of key attributes for the stream. by
hashing the key of any arriving stream element, we can use the hash value
to decide consistently whether all or none of the elements with that key
will become part of the sample.

    bloom filters: this technique allows us to    lter streams so elements that
belong to a particular set are allowed through, while most nonmembers
are deleted. we use a large bit array, and several hash functions. members
of the selected set are hashed to buckets, which are bits in the array, and
those bits are set to 1. to test a stream element for membership, we hash
the element to a set of bits using each of the hash functions, and only
accept the element if all these bits are 1.

    counting distinct elements: to estimate the number of di   erent elements
appearing in a stream, we can hash elements to integers, interpreted as
binary numbers. 2 raised to the power that is the longest sequence of 0   s
seen in the hash value of any stream element is an estimate of the number
of di   erent elements. by using many hash functions and combining these
estimates,    rst by taking averages within groups, and then taking the
median of the averages, we get a reliable estimate.

    moments of streams: the kth moment of a stream is the sum of the kth
powers of the counts of each element that appears at least once in the
stream. the 0th moment is the number of distinct elements, and the 1st
moment is the length of the stream.

    estimating second moments: a good estimate for the second moment, or
surprise number, is obtained by choosing a random position in the stream,
taking twice the number of times this element appears in the stream from
that position onward, subtracting 1, and multiplying by the length of
the stream. many random variables of this type can be combined like
the estimates for counting the number of distinct elements, to produce a
reliable estimate of the second moment.

    estimating higher moments: the technique for second moments works
for kth moments as well, as long as we replace the formula 2x     1 (where
x is the number of times the element appears at or after the selected
position) by xk     (x     1)k.

4.9. references for chapter 4

161

    estimating the number of 1   s in a window : we can estimate the number
of 1   s in a window of 0   s and 1   s by grouping the 1   s into buckets. each
bucket has a number of 1   s that is a power of 2; there are one or two
buckets of each size, and sizes never decrease as we go back in time. if
we record only the position and size of the buckets, we can represent the
contents of a window of size n with o(log2 n ) space.

    answering queries about numbers of 1   s: if we want to know the approx-
imate numbers of 1   s in the most recent k elements of a binary stream,
we    nd the earliest bucket b that is at least partially within the last k
positions of the window and estimate the number of 1   s to be the sum of
the sizes of each of the more recent buckets plus half the size of b. this
estimate can never be o    by more that 50% of the true count of 1   s.

    closer approximations to the number of 1   s: by changing the rule for
how many buckets of a given size can exist in the representation of a
binary window, so that either r or r     1 of a given size may exist, we can
assure that the approximation to the true number of 1   s is never o    by
more than 1/r.

    exponentially decaying windows: rather than    xing a window size, we
can imagine that the window consists of all the elements that ever arrived
in the stream, but with the element that arrived t time units ago weighted
by e   ct for some time-constant c. doing so allows us to maintain certain
summaries of an exponentially decaying window easily. for instance, the
weighted sum of elements can be recomputed, when a new element arrives,
by multiplying the old sum by 1     c and then adding the new element.
    maintaining frequent elements in an exponentially decaying window :
we can imagine that each item is represented by a binary stream, where
0 means the item was not the element arriving at a given time, and 1
means that it was. we can    nd the elements whose sum of their binary
stream is at least 1/2. when a new element arrives, multiply all recorded
sums by 1 minus the time constant, add 1 to the count of the item that
just arrived, and delete from the record any item whose sum has fallen
below 1/2.

4.9 references for chapter 4

many ideas associated with stream management appear in the    chronicle data
model    of [8]. an early survey of research in stream-management systems is
[2]. also, [6] is a recent book on the subject of stream management.

the sampling technique of section 4.2 is from [7]. the bloom filter is
generally attributed to [3], although essentially the same technique appeared as
   superimposed codes    in [9].

162

chapter 4. mining data streams

the algorithm for counting distinct elements is essentially that of [5], al-
though the particular method we described appears in [1]. the latter is also
the source for the algorithm for calculating the surprise number and higher
moments. however, the technique for maintaining a uniformly chosen sample
of positions in the stream is called    reservoir sampling    and comes from [10].

the technique for approximately counting 1   s in a window is from [4].

1. n. alon, y. matias, and m. szegedy,    the space complexity of approxi-
mating frequency moments,    28th acm symposium on theory of com-
puting, pp. 20   29, 1996.

2. b. babcock, s. babu, m. datar, r. motwani, and j. widom,    models
and issues in data stream systems,    symposium on principles of database
systems, pp. 1   16, 2002.

3. b.h. bloom,    space/time trade-o   s in hash coding with allowable errors,   

comm. acm 13:7, pp. 422   426, 1970.

4. m. datar, a. gionis, p. indyk, and r. motwani,    maintaining stream
statistics over sliding windows,    siam j. computing 31, pp. 1794   1813,
2002.

5. p. flajolet and g.n. martin,    probabilistic counting for database applica-
tions,    24th symposium on foundations of computer science, pp. 76   82,
1983.

6. m. garofalakis, j. gehrke, and r. rastogi (editors), data stream man-

agement, springer, 2009.

7. p.b. gibbons,    distinct sampling for highly-accurate answers to distinct
values queries and event reports,    intl. conf. on very large databases,
pp. 541   550, 2001.

8. h.v. jagadish, i.s. mumick, and a. silberschatz,    view maintenance
issues for the chronicle data model,    proc. acm symp. on principles of
database systems, pp. 113   124, 1995.

9. w.h. kautz and r.c. singleton,    nonadaptive binary superimposed codes,   

ieee transactions on id205 10, pp. 363   377, 1964.

10. j. vitter,    random sampling with a reservoir,    acm transactions on

mathematical software 11:1, pp. 37   57, 1985.

chapter 5

link analysis

one of the biggest changes in our lives in the decade following the turn of
the century was the availability of e   cient and accurate web search, through
search engines such as google. while google was not the    rst search engine, it
was the    rst able to defeat the spammers who had made search almost useless.
moreover, the innovation provided by google was a nontrivial technological
advance, called    id95.    we shall begin the chapter by explaining what
id95 is and how it is computed e   ciently.

yet the war between those who want to make the web useful and those
who would exploit it for their own purposes is never over. when id95 was
established as an essential technique for a search engine, spammers invented
ways to manipulate the id95 of a web page, often called link spam.1
that development led to the response of trustrank and other techniques for
preventing spammers from attacking id95. we shall discuss trustrank
and other approaches to detecting link spam.

finally, this chapter also covers some variations on id95. these tech-
niques include topic-sensitive id95 (which can also be adapted for combat-
ing link spam) and the hits, or    hubs and authorities    approach to evaluating
pages on the web.

5.1 id95

we begin with a portion of the history of search engines, in order to motivate
the de   nition of id95,2 a tool for evaluating the importance of web pages
in a way that it is not easy to fool. we introduce the idea of    random surfers,   
to explain why id95 is e   ective. we then introduce the technique of    tax-
ation    or recycling of random surfers, in order to avoid certain web structures

1link spammers sometimes try to make their unethicality less apparent by referring to

what they do as    search-engine optimization.   

2the term id95 comes from larry page, the inventor of the idea and a founder of

google.

163

164

chapter 5. link analysis

that present problems for the simple version of id95.

5.1.1 early search engines and term spam

there were many search engines before google. largely, they worked by crawl-
ing the web and listing the terms (words or other strings of characters other
than white space) found in each page, in an inverted index. an inverted index
is a data structure that makes it easy, given a term, to    nd (pointers to) all the
places where that term occurs.

when a search query (list of terms) was issued, the pages with those terms
were extracted from the inverted index and ranked in a way that re   ected the
use of the terms within the page. thus, presence of a term in a header of
the page made the page more relevant than would the presence of the term in
ordinary text, and large numbers of occurrences of the term would add to the
assumed relevance of the page for the search query.

as people began to use search engines to    nd their way around the web,
unethical people saw the opportunity to fool search engines into leading people
to their page. thus, if you were selling shirts on the web, all you cared about
was that people would see your page, regardless of what they were looking for.
thus, you could add a term like    movie    to your page, and do it thousands of
times, so a search engine would think you were a terribly important page about
movies. when a user issued a search query with the term    movie,    the search
engine would list your page    rst. to prevent the thousands of occurrences of
   movie    from appearing on your page, you could give it the same color as the
background. and if simply adding    movie    to your page didn   t do the trick,
then you could go to the search engine, give it the query    movie,    and see what
page did come back as the    rst choice. then, copy that page into your own,
again using the background color to make it invisible.

techniques for fooling search engines into believing your page is about some-
thing it is not, are called term spam. the ability of term spammers to operate
so easily rendered early search engines almost useless. to combat term spam,
google introduced two innovations:

1. id95 was used to simulate where web surfers, starting at a random
page, would tend to congregate if they followed randomly chosen outlinks
from the page at which they were currently located, and this process were
allowed to iterate many times. pages that would have a large number of
surfers were considered more    important    than pages that would rarely
be visited. google prefers important pages to unimportant pages when
deciding which pages to show    rst in response to a search query.

2. the content of a page was judged not only by the terms appearing on that
page, but by the terms used in or near the links to that page. note that
while it is easy for a spammer to add false terms to a page they control,
they cannot as easily get false terms added to the pages that link to their
own page, if they do not control those pages.

5.1. id95

165

simpli   ed id95 doesn   t work

as we shall see, computing id95 by simulating random surfers is
a time-consuming process. one might think that simply counting the
number of in-links for each page would be a good approximation to where
random surfers would wind up. however, if that is all we did, then the
hypothetical shirt-seller could simply create a    spam farm    of a million
pages, each of which linked to his shirt page. then, the shirt page looks
very important indeed, and a search engine would be fooled.

these two techniques together make it very hard for the hypothetical shirt
vendor to fool google. while the shirt-seller can still add    movie    to his page,
the fact that google believed what other pages say about him, over what he says
about himself would negate the use of false terms. the obvious countermeasure
is for the shirt seller to create many pages of his own, and link to his shirt-
selling page with a link that says    movie.    but those pages would not be given
much importance by id95, since other pages would not link to them. the
shirt-seller could create many links among his own pages, but none of these
pages would get much importance according to the id95 algorithm, and
therefore, he still would not be able to fool google into thinking his page was
about movies.

it is reasonable to ask why simulation of random surfers should allow us to
approximate the intuitive notion of the    importance    of pages. there are two
related motivations that inspired this approach.

    users of the web    vote with their feet.    they tend to place links to pages
they think are good or useful pages to look at, rather than bad or useless
pages.

    the behavior of a random surfer indicates which pages users of the web
are likely to visit. users are more likely to visit useful pages than useless
pages.

but regardless of the reason, the id95 measure has been proved empirically
to work, and so we shall study in detail how it is computed.

5.1.2 de   nition of id95

id95 is a function that assigns a real number to each page in the web
(or at least to that portion of the web that has been crawled and its links
discovered). the intent is that the higher the id95 of a page, the more
   important    it is. there is not one    xed algorithm for assignment of id95,
and in fact variations on the basic idea can alter the relative id95 of any
two pages. we begin by de   ning the basic, idealized id95, and follow it

166

chapter 5. link analysis

by modi   cations that are necessary for dealing with some real-world problems
concerning the structure of the web.

think of the web as a directed graph, where pages are the nodes, and there
is an arc from page p1 to page p2 if there are one or more links from p1 to p2.
figure 5.1 is an example of a tiny version of the web, where there are only four
pages. page a has links to each of the other three pages; page b has links to
a and d only; page c has a link only to a, and page d has links to b and c
only.

a

c

b

d

figure 5.1: a hypothetical example of the web

suppose a random surfer starts at page a in fig. 5.1. there are links to b,
c, and d, so this surfer will next be at each of those pages with id203
1/3, and has zero id203 of being at a. a random surfer at b has, at the
next step, id203 1/2 of being at a, 1/2 of being at d, and 0 of being at
b or c.

in general, we can de   ne the transition matrix of the web to describe what
happens to random surfers after one step. this matrix m has n rows and
columns, if there are n pages. the element mij in row i and column j has value
1/k if page j has k arcs out, and one of them is to page i. otherwise, mij = 0.

example 5.1 : the transition matrix for the web of fig. 5.1 is

m =

0

1/2 1
0
0
1/3
1/3
0
0
1/3 1/2 0

   
         

0
1/2
1/2
0

   
         

in this matrix, the order of the pages is the natural one, a, b, c, and d. thus,
the    rst column expresses the fact, already discussed, that a surfer at a has a
1/3 id203 of next being at each of the other pages. the second column
expresses the fact that a surfer at b has a 1/2 id203 of being next at a
and the same of being at d. the third column says a surfer at c is certain to
be at a next. the last column says a surfer at d has a 1/2 id203 of being
next at b and the same at c.    

5.1. id95

167

the id203 distribution for the location of a random surfer can be
described by a column vector whose jth component is the id203 that the
surfer is at page j. this id203 is the (idealized) id95 function.

suppose we start a random surfer at any of the n pages of the web with
equal id203. then the initial vector v0 will have 1/n for each component.
if m is the transition matrix of the web, then after one step, the distribution
of the surfer will be m v0, after two steps it will be m (m v0) = m 2v0, and so
on. in general, multiplying the initial vector v0 by m a total of i times will
give us the distribution of the surfer after i steps.

to see why multiplying a distribution vector v by m gives the distribution
x = m v at the next step, we reason as follows. the id203 xi that a

random surfer will be at node i at the next step, is pj mij vj. here, mij is the

id203 that a surfer at node j will move to node i at the next step (often
0 because there is no link from j to i), and vj is the id203 that the surfer
was at node j at the previous step.

this sort of behavior is an example of the ancient theory of markov processes.
it is known that the distribution of the surfer approaches a limiting distribution
v that satis   es v = m v, provided two conditions are met:

1. the graph is strongly connected; that is, it is possible to get from any

node to any other node.

2. there are no dead ends: nodes that have no arcs out.

note that fig. 5.1 satis   es both these conditions.

the limit is reached when multiplying the distribution by m another time
does not change the distribution. in other terms, the limiting v is an eigenvec-
tor of m (an eigenvector of a matrix m is a vector v that satis   es v =   m v for
some constant eigenvalue   ). in fact, because m is stochastic, meaning that its
columns each add up to 1, v is the principal eigenvector (its associated eigen-
value is the largest of all eigenvalues). note also that, because m is stochastic,
the eigenvalue associated with the principal eigenvector is 1.

the principal eigenvector of m tells us where the surfer is most likely to
be after a long time. recall that the intuition behind id95 is that the
more likely a surfer is to be at a page, the more important the page is. we
can compute the principal eigenvector of m by starting with the initial vector
v0 and multiplying by m some number of times, until the vector we get shows
little change at each round. in practice, for the web itself, 50   75 iterations are
su   cient to converge to within the error limits of double-precision arithmetic.

example 5.2 : suppose we apply the process described above to the matrix
m from example 5.1. since there are four nodes, the initial vector v0 has four
components, each 1/4. the sequence of approximations to the limit that we

168

chapter 5. link analysis

solving linear equations

if you look at the 4-node    web    of example 5.2, you might think that the
way to solve the equation v = m v is by gaussian elimination. indeed,
in that example, we argued what the limit would be essentially by doing
so. however, in realistic examples, where there are tens or hundreds of
billions of nodes, gaussian elimination is not feasible. the reason is that
gaussian elimination takes time that is cubic in the number of equations.
thus, the only way to solve equations on this scale is to iterate as we
have suggested. even that iteration is quadratic at each round, but we
can speed it up by taking advantage of the fact that the matrix m is very
sparse; there are on average about ten links per page, i.e., ten nonzero
entries per column.

moreover, there is another di   erence between id95 calculation
and solving linear equations. the equation v = m v has an in   nite number
of solutions, since we can take any solution v, multiply its components by
any    xed constant c, and get another solution to the same equation. when
we include the constraint that the sum of the components is 1, as we have
done, then we get a unique solution.

get by multiplying at each step by m is:

   
         

1/4
1/4
1/4
1/4

,

   
         

   
         

9/24
5/24
5/24
5/24

,

   
         

   
         

15/48
11/48
11/48
11/48

,

   
         

   
         

11/32
7/32
7/32
7/32

   
         

, . . . ,

   
         

3/9
2/9
2/9
2/9

   
         

notice that in this example, the probabilities for b, c, and d remain the
same. it is easy to see that b and c must always have the same values at any
iteration, because their rows in m are identical. to show that their values are
also the same as the value for d, an inductive proof works, and we leave it as
an exercise. given that the last three values of the limiting vector must be the
same, it is easy to discover the limit of the above sequence. the    rst row of
m tells us that the id203 of a must be 3/2 the other probabilities, so the
limit has the id203 of a equal to 3/9, or 1/3, while the id203 for the
other three nodes is 2/9.

this di   erence in id203 is not great. but in the real web, with billions
of nodes of greatly varying importance, the true id203 of being at a node
like www.amazon.com is orders of magnitude greater than the id203 of
typical nodes.    

5.1. id95

169

5.1.3 structure of the web

it would be nice if the web were strongly connected like fig. 5.1. however, it
is not, in practice. an early study of the web found it to have the structure
shown in fig. 5.2. there was a large strongly connected component (scc), but
there were several other portions that were almost as large.

1. the in-component, consisting of pages that could reach the scc by fol-

lowing links, but were not reachable from the scc.

2. the out-component, consisting of pages reachable from the scc but un-

able to reach the scc.

3. tendrils, which are of two types. some tendrils consist of pages reachable
from the in-component but not able to reach the in-component. the
other tendrils can reach the out-component, but are not reachable from
the out-component.

tendrils

out

tendrils

in

in

component

strongly
connected
component

out

component

tubes

disconnected
components

figure 5.2: the    bowtie    picture of the web

in addition, there were small numbers of pages found either in

170

chapter 5. link analysis

(a) tubes, which are pages reachable from the in-component and able to reach
the out-component, but unable to reach the scc or be reached from the
scc.

(b) isolated components that are unreachable from the large components (the

scc, in- and out-components) and unable to reach those components.

several of these structures violate the assumptions needed for the markov-
process iteration to converge to a limit. for example, when a random surfer
enters the out-component, they can never leave. as a result, surfers starting
in either the scc or in-component are going to wind up in either the out-
component or a tendril o    the in-component. thus, no page in the scc or in-
component winds up with any id203 of a surfer being there. if we interpret
this id203 as measuring the importance of a page, then we conclude falsely
that nothing in the scc or in-component is of any importance.

as a result, id95 is usually modi   ed to prevent such anomalies. there
are really two problems we need to avoid. first is the dead end, a page that
has no links out. surfers reaching such a page disappear, and the result is that
in the limit no page that can reach a dead end can have any id95 at all.
the second problem is groups of pages that all have outlinks but they never
link to any other pages. these structures are called spider traps.3 both these
problems are solved by a method called    taxation,    where we assume a random
surfer has a    nite id203 of leaving the web at any step, and new surfers
are started at each page. we shall illustrate this process as we study each of
the two problem cases.

5.1.4 avoiding dead ends

recall that a page with no link out is called a dead end.
if we allow dead
ends, the transition matrix of the web is no longer stochastic, since some of
the columns will sum to 0 rather than 1. a matrix whose column sums are at
if we compute m iv for increasing powers of a
most 1 is called substochastic.
substochastic matrix m , then some or all of the components of the vector go
to 0. that is, importance    drains out    of the web, and we get no information
about the relative importance of pages.

example 5.3 : in fig. 5.3 we have modi   ed fig. 5.1 by removing the arc from
c to a. thus, c becomes a dead end.
in terms of random surfers, when
a surfer reaches c they disappear at the next round. the matrix m that
describes fig. 5.3 is

m =

0

1/2 0
1/3
0
0
1/3
0
0
1/3 1/2 0

   
         

0
1/2
1/2
0

   
         

3they are so called because the programs that crawl the web, recording pages and links,

are often referred to as    spiders.    once a spider enters a spider trap, it can never leave.

5.1. id95

171

a

c

b

d

figure 5.3: c is now a dead end

note that it is substochastic, but not stochastic, because the sum of the third
column, for c, is 0, not 1. here is the sequence of vectors that result by starting
with the vector with each component 1/4, and repeatedly multiplying the vector
by m :

   
         

1/4
1/4
1/4
1/4

,

   
         

   
         

3/24
5/24
5/24
5/24

,

   
         

   
         

5/48
7/48
7/48
7/48

,

   
         

   
         

21/288
31/288
31/288
31/288

   
         

, . . . ,

   
         

0
0
0
0

   
         

as we see, the id203 of a surfer being anywhere goes to 0, as the number
of steps increase.    

there are two approaches to dealing with dead ends.

1. we can drop the dead ends from the graph, and also drop their incoming
arcs. doing so may create more dead ends, which also have to be dropped,
recursively. however, eventually we wind up with a strongly-connected
component, none of whose nodes are dead ends.
in terms of fig. 5.2,
recursive deletion of dead ends will remove parts of the out-component,
tendrils, and tubes, but leave the scc and the in-component, as well as
parts of any small isolated components.4

2. we can modify the process by which random surfers are assumed to move
about the web. this method, which we refer to as    taxation,    also solves
the problem of spider traps, so we shall defer it to section 5.1.5.

if we use the    rst approach, recursive deletion of dead ends, then we solve the
remaining graph g by whatever means are appropriate, including the taxation
method if there might be spider traps in g. then, we restore the graph, but keep

4you might suppose that the entire out-component and all the tendrils will be removed, but
remember that they can have within them smaller strongly connected components, including
spider traps, which cannot be deleted.

172

chapter 5. link analysis

the id95 values for the nodes of g. nodes not in g, but with predecessors
all in g can have their id95 computed by summing, over all predecessors
p, the id95 of p divided by the number of successors of p in the full graph.
now there may be other nodes, not in g, that have the id95 of all their
predecessors computed. these may have their own id95 computed by
the same process. eventually, all nodes outside g will have their id95
computed; they can surely be computed in the order opposite to that in which
they were deleted.

a

c

b

d

e

figure 5.4: a graph with two levels of dead ends

example 5.4 : figure 5.4 is a variation on fig. 5.3, where we have introduced
a successor e for c. but e is a dead end, and when we remove it, and the
arc entering from c, we    nd that c is now a dead end. after removing c, no
more nodes can be removed, since each of a, b, and d have arcs leaving. the
resulting graph is shown in fig. 5.5.

the matrix for the graph of fig. 5.5 is

m =    
   

1/2 0
0
1/2
1
1/2 1/2 0

0

   
   

the rows and columns correspond to a, b, and d in that order. to get the
id95s for this matrix, we start with a vector with all components equal
to 1/3, and repeatedly multiply by m . the sequence of vectors we get is

   
   

1/3
1/3
1/3

   
    ,   
   

1/6
3/6
2/6

   
    ,   
   

3/12
5/12
4/12

   
    ,   
   

5/24
11/24
8/24

   
    , . . . ,   
   

2/9
4/9
3/9

   
   

we now know that the id95 of a is 2/9, the id95 of b is 4/9,
and the id95 of d is 3/9. we still need to compute id95s for c

5.1. id95

173

a

b

d

figure 5.5: the reduced graph with no dead ends

and e, and we do so in the order opposite to that in which they were deleted.
since c was last to be deleted, we know all its predecessors have id95s
computed. these predecessors are a and d. in fig. 5.4, a has three successors,
so it contributes 1/3 of its id95 to c. page d has two successors in
fig. 5.4, so it contributes half its id95 to c. thus, the id95 of c is
1
3    2
9 + 1
now we can compute the id95 for e. that node has only one pre-
decessor, c, and c has only one successor. thus, the id95 of e is the
same as that of c. note that the sums of the id95s exceed 1, and they
no longer represent the distribution of a random surfer. yet they do represent
decent estimates of the relative importance of the pages.    

9 = 13/54.

2    3

5.1.5 spider traps and taxation

as we mentioned, a spider trap is a set of nodes with no dead ends but no arcs
out. these structures can appear intentionally or unintentionally on the web,
and they cause the id95 calculation to place all the id95 within the
spider traps.

example 5.5 : consider fig. 5.6, which is fig. 5.1 with the arc out of c
changed to point to c itself. that change makes c a simple spider trap of one
node. note that in general spider traps can have many nodes, and as we shall
see in section 5.4, there are spider traps with millions of nodes that spammers
construct intentionally.

the transition matrix for fig. 5.6 is

m =

0

1/2 0
0
0
1/3
1/3
1
0
1/3 1/2 0

   
         

0
1/2
1/2
0

   
         

if we perform the usual iteration to compute the id95 of the nodes, we

174

chapter 5. link analysis

a

c

b

d

figure 5.6: a graph with a one-node spider trap

get

   
         

1/4
1/4
1/4
1/4

   
         

,   
         

3/24
5/24
11/24
5/24

   
         

,   
         

5/48
7/48
29/48
7/48

   
         

,   
         

21/288
31/288
205/288
31/288

   
         

, . . . ,   
         

0
0
1
0

   
         

as predicted, all the id95 is at c, since once there a random surfer can
never leave.    

to avoid the problem illustrated by example 5.5, we modify the calculation
of id95 by allowing each random surfer a small id203 of teleporting
to a random page, rather than following an out-link from their current page.
the iterative step, where we compute a new vector estimate of id95s v   
from the current id95 estimate v and the transition matrix m is

v    =   m v + (1       )e/n

where    is a chosen constant, usually in the range 0.8 to 0.9, e is a vector of all
1   s with the appropriate number of components, and n is the number of nodes
in the web graph. the term   m v represents the case where, with id203
  , the random surfer decides to follow an out-link from their present page. the
term (1       )e/n is a vector each of whose components has value (1       )/n and
represents the introduction, with id203 1       , of a new random surfer at
a random page.
note that if the graph has no dead ends, then the id203 of introducing a
new random surfer is exactly equal to the id203 that the random surfer will
decide not to follow a link from their current page. in this case, it is reasonable
to visualize the surfer as deciding either to follow a link or teleport to a random
page. however, if there are dead ends, then there is a third possibility, which
is that the surfer goes nowhere. since the term (1       )e/n does not depend on
the sum of the components of the vector v, there will always be some fraction

5.1. id95

175

of a surfer operating on the web. that is, when there are dead ends, the sum
of the components of v may be less than 1, but it will never reach 0.

example 5.6 : let us see how the new approach to computing id95
fares on the graph of fig. 5.6. we shall use    = 0.8 in this example. thus, the
equation for the iteration becomes

v    =    
         

0

2/5

0
0

4/15
4/15
4/15 2/5

0

0
0
2/5
4/5 2/5
0

0

   
         

v +   
         

1/20
1/20
1/20
1/20

   
         

notice that we have incorporated the factor    into m by multiplying each of
its elements by 4/5. the components of the vector (1       )e/n are each 1/20,
since 1        = 1/5 and n = 4. here are the    rst few iterations:

   
         

1/4
1/4
1/4
1/4

,

   
         

   
         

9/60
13/60
25/60
13/60

,

   
         

   
         

41/300
53/300
153/300
53/300

,

   
         

   
         

543/4500
707/4500
2543/4500
707/4500

   
         

, . . . ,

   
         

15/148
19/148
95/148
19/148

   
         

by being a spider trap, c has managed to get more than half of the id95
for itself. however, the e   ect has been limited, and each of the nodes gets some
of the id95.    

5.1.6 using id95 in a search engine

having seen how to calculate the id95 vector for the portion of the web
that a search engine has crawled, we should examine how this information is
used. each search engine has a secret formula that decides the order in which
to show pages to the user in response to a search query consisting of one or
more search terms (words). google is said to use over 250 di   erent properties
of pages, from which a linear order of pages is decided.

first, in order to be considered for the ranking at all, a page has to have at
least one of the search terms in the query. normally, the weighting of properties
is such that unless all the search terms are present, a page has very little chance
of being in the top ten that are normally shown    rst to the user. among the
quali   ed pages, a score is computed for each, and an important component of
this score is the id95 of the page. other components include the presence
or absence of search terms in prominent places, such as headers or the links to
the page itself.

5.1.7 exercises for section 5.1

exercise 5.1.1 : compute the id95 of each page in fig. 5.7, assuming
no taxation.

176

chapter 5. link analysis

a

b

c

figure 5.7: an example graph for exercises

exercise 5.1.2 : compute the id95 of each page in fig. 5.7, assuming
   = 0.8.

! exercise 5.1.3 : suppose the web consists of a clique (set of nodes with all
possible arcs from one to another) of n nodes and a single additional node that
is the successor of each of the n nodes in the clique. figure 5.8 shows this graph
for the case n = 4. determine the id95 of each page, as a function of n
and   .

figure 5.8: example of graphs discussed in exercise 5.1.3

!! exercise 5.1.4 : construct, for any integer n, a web such that, depending on
it is

  , any of the n nodes can have the highest id95 among those n.
allowed for there to be other nodes in the web besides these n.

! exercise 5.1.5 : show by induction on n that if the second, third, and fourth
components of a vector v are equal, and m is the transition matrix of exam-
ple 5.1, then the second, third, and fourth components are also equal in m nv
for any n     0.

5.2. efficient computation of id95

177

. . .

figure 5.9: a chain of dead ends

exercise 5.1.6 : suppose we recursively eliminate dead ends from the graph,
solve the remaining graph, and estimate the id95 for the dead-end pages
as described in section 5.1.4. suppose the graph is a chain of dead ends, headed
by a node with a self-loop, as suggested in fig. 5.9. what would be the page-
rank assigned to each of the nodes?

exercise 5.1.7 : repeat exercise 5.1.6 for the tree of dead ends suggested by
fig. 5.10. that is, there is a single node with a self-loop, which is also the root
of a complete binary tree of n levels.

. . .

. . .

. . .

. . .

figure 5.10: a tree of dead ends

5.2 e   cient computation of id95

to compute the id95 for a large graph representing the web, we have
to perform a matrix   vector multiplication on the order of 50 times, until the
vector is close to unchanged at one iteration. to a    rst approximation, the
mapreduce method given in section 2.3.1 is suitable. however, we must deal
with two issues:

1. the transition matrix of the web m is very sparse. thus, representing
it by all its elements is highly ine   cient. rather, we want to represent
the matrix by its nonzero elements.

2. we may not be using mapreduce, or for e   ciency reasons we may wish
to use a combiner (see section 2.2.4) with the map tasks to reduce the
amount of data that must be passed from map tasks to reduce tasks. in
this case, the striping approach discussed in section 2.3.1 is not su   cient
to avoid heavy use of disk (thrashing).

we discuss the solution to these two problems in this section.

178

chapter 5. link analysis

5.2.1 representing transition matrices

the transition matrix is very sparse, since the average web page has about 10
out-links. if, say, we are analyzing a graph of ten billion pages, then only one
in a billion entries is not 0. the proper way to represent any sparse matrix is
to list the locations of the nonzero entries and their values. if we use 4-byte
integers for coordinates of an element and an 8-byte double-precision number
for the value, then we need 16 bytes per nonzero entry. that is, the space
needed is linear in the number of nonzero entries, rather than quadratic in the
side of the matrix.

however, for a transition matrix of the web, there is one further compression
that we can do. if we list the nonzero entries by column, then we know what
each nonzero entry is; it is 1 divided by the out-degree of the page. we can
thus represent a column by one integer for the out-degree, and one integer
per nonzero entry in that column, giving the row number where that entry
is located. thus, we need slightly more than 4 bytes per nonzero entry to
represent a transition matrix.

example 5.7 : let us reprise the example web graph from fig. 5.1, whose
transition matrix is

m =

   
         

0

1/2 1
0
0
1/3
1/3
0
0
1/3 1/2 0

0
1/2
1/2
0

   
         

recall that the rows and columns represent nodes a, b, c, and d, in that
order. in fig. 5.11 is a compact representation of this matrix.5

source degree destinations

a
b
c
d

3
2
1
2

b, c, d
a, d
a
b, c

figure 5.11: represent a transition matrix by the out-degree of each node and
the list of its successors

for instance, the entry for a has degree 3 and a list of three successors.
from that row of fig. 5.11 we can deduce that the column for a in matrix m
has 0 in the row for a (since it is not on the list of destinations) and 1/3 in
the rows for b, c, and d. we know that the value is 1/3 because the degree
column in fig. 5.11 tells us there are three links out of a.    

5because m is not sparse, this representation is not very useful for m . however, the
example illustrates the process of representing matrices in general, and the sparser the matrix
is, the more this representation will save.

5.2. efficient computation of id95

179

5.2.2 id95 iteration using mapreduce

one iteration of the id95 algorithm involves taking an estimated page-
rank vector v and computing the next estimate v    by

v    =   m v + (1       )e/n

recall    is a constant slightly less than 1, e is a vector of all 1   s, and n is the
number of nodes in the graph that transition matrix m represents.

if n is small enough that each map task can store the full vector v in main
memory and also have room in main memory for the result vector v   , then there
is little more here than a matrix   vector multiplication. the additional steps
are to multiply each component of m v by constant    and to add (1       )/n to
each component.
however, it is likely, given the size of the web today, that v is much too
large to    t in main memory. as we discussed in section 2.3.1, the method
of striping, where we break m into vertical stripes (see fig. 2.4) and break v
into corresponding horizontal stripes, will allow us to execute the mapreduce
process e   ciently, with no more of v at any one map task than can conveniently
   t in main memory.

5.2.3 use of combiners to consolidate the result vector

there are two reasons the method of section 5.2.2 might not be adequate.

1. we might wish to add terms for v   

i, the ith component of the result vector
v, at the map tasks. this improvement is the same as using a combiner,
since the reduce function simply adds terms with a common key. recall
that for a mapreduce implementation of matrix   vector multiplication,
the key is the value of i for which a term mij vj is intended.

2. we might not be using mapreduce at all, but rather executing the iter-

ation step at a single machine or a collection of machines.

we shall assume that we are trying to implement a combiner in conjunction
with a map task; the second case uses essentially the same idea.

suppose that we are using the stripe method to partition a matrix and
vector that do not    t in main memory. then a vertical stripe from the matrix
m and a horizontal stripe from the vector v will contribute to all components
of the result vector v   . since that vector is the same length as v, it will not
   t in main memory either. moreover, as m is stored column-by-column for
e   ciency reasons, a column can a   ect any of the components of v   . as a result,
it is unlikely that when we need to add a term to some component v   
i, that
component will already be in main memory. thus, most terms will require
that a page be brought into main memory to add it to the proper component.
that situation, called thrashing, takes orders of magnitude too much time to
be feasible.

180

chapter 5. link analysis

an alternative strategy is based on partitioning the matrix into k2 blocks,
while the vectors are still partitioned into k stripes. a picture, showing the
division for k = 4, is in fig. 5.12. note that we have not shown the multiplica-
tion of the matrix by    or the addition of (1       )e/n, because these steps are
straightforward, regardless of the strategy we use.

v

   
1

v

   
2

v

   
3

v

   
4

=

m
11 m12

m

13

m

14

m

21

m

22

m

23

m

24

m

31

m

32

m

33

m

34

m

41

m

42

m

43

m

44

v

1

v

2

v

3

v

4

figure 5.12: partitioning a matrix into square blocks

in this method, we use k2 map tasks. each task gets one square of the
matrix m , say mij, and one stripe of the vector v, which must be vj. notice
that each stripe of the vector is sent to k di   erent map tasks; vj is sent to the
task handling mij for each of the k possible values of i. thus, v is transmitted
over the network k times. however, each piece of the matrix is sent only once.
since the size of the matrix, properly encoded as described in section 5.2.1, can
be expected to be several times the size of the vector, the transmission cost is
not too much greater than the minimum possible. and because we are doing
considerable combining at the map tasks, we save as data is passed from the
map tasks to the reduce tasks.

the advantage of this approach is that we can keep both the jth stripe of
v and the ith stripe of v    in main memory as we process mij. note that all
terms generated from mij and vj contribute to v   

i and no other stripe of v   .

5.2.4 representing blocks of the transition matrix

since we are representing transition matrices in the special way described in
section 5.2.1, we need to consider how the blocks of fig. 5.12 are represented.
unfortunately, the space required for a column of blocks (a    stripe    as we called
it earlier) is greater than the space needed for the stripe as a whole, but not
too much greater.

for each block, we need data about all those columns that have at least one
nonzero entry within the block. if k, the number of stripes in each dimension,
is large, then most columns will have nothing in most blocks of its stripe. for
a given block, we not only have to list those rows that have a nonzero entry for
that column, but we must repeat the out-degree for the node represented by
the column. consequently, it is possible that the out-degree will be repeated as
many times as the out-degree itself. that observation bounds from above the

5.2. efficient computation of id95

181

space needed to store the blocks of a stripe at twice the space needed to store
the stripe as a whole.

a    b     c     d

a

b

c

d

figure 5.13: a four-node graph is divided into four 2-by-2 blocks

example 5.8 : let us suppose the matrix from example 5.7 is partitioned into
blocks, with k = 2. that is, the upper-left quadrant represents links from a or
b to a or b, the upper-right quadrant represents links from c or d to a or
b, and so on. it turns out that in this small example, the only entry that we
can avoid is the entry for c in m22, because c has no arcs to either c or d.
the tables representing each of the four blocks are shown in fig. 5.14.

if we examine fig. 5.14(a), we see the representation of the upper-left quad-
rant. notice that the degrees for a and b are the same as in fig. 5.11, because
we need to know the entire number of successors, not the number of successors
within the relevant block. however, each successor of a or b is represented
in fig. 5.14(a) or fig. 5.14(c), but not both. notice also that in fig. 5.14(d),
there is no entry for c, because there are no successors of c within the lower
half of the matrix (rows c and d).    

5.2.5 other e   cient approaches to id95 iteration

the algorithm discussed in section 5.2.3 is not the only option. we shall discuss
several other approaches that use fewer processors. these algorithms share with
the algorithm of section 5.2.3 the good property that the matrix m is read only
once, although the vector v is read k times, where the parameter k is chosen
so that 1/kth of the vectors v and v    can be held in main memory. recall that
the algorithm of section 5.2.3 uses k2 processors, assuming all map tasks are
executed in parallel at di   erent processors.

we can assign all the blocks in one row of blocks to a single map task, and
thus reduce the number of map tasks to k. for instance, in fig. 5.12, m11,
m12, m13, and m14 would be assigned to a single map task. if we represent the
blocks as in fig. 5.14, we can read the blocks in a row of blocks one-at-a-time,
so the matrix does not consume a signi   cant amount of main-memory. at the
same time that we read mij, we must read the vector stripe vj. as a result,
each of the k map tasks reads the entire vector v, along with 1/kth of the
matrix.

182

chapter 5. link analysis

source degree destinations

a
b

3
2

b
a

(a) representation of m11 connecting a and b to a and b

source degree destinations

c
d

1
2

a
b

(b) representation of m12 connecting c and d to a and b

source degree destinations

a
b

3
2

c, d
d

(c) representation of m21 connecting a and b to c and d

source degree destinations

d

2

c

(d) representation of m22 connecting c and d to c and d

figure 5.14: sparse representation of the blocks of a matrix

the work reading m and v is thus the same as for the algorithm of sec-
tion 5.2.3, but the advantage of this approach is that each map task can combine
all the terms for the portion v   
i for which it is exclusively responsible. in other
words, the reduce tasks have nothing to do but to concatenate the pieces of v   
received from the k map tasks.

we can extend this idea to an environment in which mapreduce is not used.
suppose we have a single processor, with m and v stored on its disk, using the
same sparse representation for m that we have discussed. we can    rst simulate
the    rst map task, the one that uses blocks m11 through m1k and all of v to
compute v   
1. then we simulate the second map task, reading m21 through m2k
and all of v to compute v   
2, and so on. as for the previous algorithms, we thus
read m once and v k times. we can make k as small as possible, subject to the
constraint that there is enough main memory to store 1/kth of v and 1/kth of
v   , along with as small a portion of m as we can read from disk (typically, one
disk block).

5.3. topic-sensitive id95

183

5.2.6 exercises for section 5.2
exercise 5.2.1 : suppose we wish to store an n    n boolean matrix (0 and
1 elements only). we could represent it by the bits themselves, or we could
represent the matrix by listing the positions of the 1   s as pairs of integers, each
integer requiring    log2 n    bits. the former is suitable for dense matrices; the
latter is suitable for sparse matrices. how sparse must the matrix be (i.e., what
fraction of the elements should be 1   s) for the sparse representation to save
space?

exercise 5.2.2 : using the method of section 5.2.1, represent the transition
matrices of the following graphs:

(a) figure 5.4.

(b) figure 5.7.

exercise 5.2.3 : using the method of section 5.2.4, represent the transition
matrices of the graph of fig. 5.3, assuming blocks have side 2.

exercise 5.2.4 : consider a web graph that is a chain, like fig. 5.9, with
n nodes. as a function of k, which you may assume divides n, describe the
representation of the transition matrix for this graph, using the method of
section 5.2.4

5.3 topic-sensitive id95

there are several improvements we can make to id95. one, to be studied
in this section, is that we can weight certain pages more heavily because of their
topic. the mechanism for enforcing this weighting is to alter the way random
surfers behave, having them prefer to land on a page that is known to cover the
chosen topic. in the next section, we shall see how the topic-sensitive idea can
also be applied to negate the e   ects of a new kind of spam, called       link spam,   
that has developed to try to fool the id95 algorithm.

5.3.1 motivation for topic-sensitive page rank

di   erent people have di   erent interests, and sometimes distinct interests are
expressed using the same term in a query. the canonical example is the search
query jaguar, which might refer to the animal, the automobile, a version of the
mac operating system, or even an ancient game console. if a search engine
can deduce that the user is interested in automobiles, for example, then it can
do a better job of returning relevant pages to the user.

ideally, each user would have a private id95 vector that gives the
importance of each page to that user.
it is not feasible to store a vector of
length many billions for each of a billion users, so we need to do something

184

chapter 5. link analysis

simpler. the topic-sensitive id95 approach creates one vector for each of
some small number of topics, biasing the id95 to favor pages of that topic.
we then endeavour to classify users according to the degree of their interest in
each of the selected topics. while we surely lose some accuracy, the bene   t is
that we store only a short vector for each user, rather than an enormous vector
for each user.

example 5.9 : one useful topic set is the 16 top-level categories (sports, med-
icine, etc.) of the open directory (dmoz).6 we could create 16 id95
vectors, one for each topic. if we could determine that the user is interested
in one of these topics, perhaps by the content of the pages they have recently
viewed, then we could use the id95 vector for that topic when deciding
on the ranking of pages.    

5.3.2 biased id93

suppose we have identi   ed some pages that represent a topic such as    sports.   
to create a topic-sensitive id95 for sports, we can arrange that the random
surfers are introduced only to a random sports page, rather than to a random
page of any kind. the consequence of this choice is that random surfers are
likely to be at an identi   ed sports page, or a page reachable along a short path
from one of these known sports pages. our intuition is that pages linked to
by sports pages are themselves likely to be about sports. the pages they link
to are also likely to be about sports, although the id203 of being about
sports surely decreases as the distance from an identi   ed sports page increases.
the mathematical formulation for the iteration that yields topic-sensitive
id95 is similar to the equation we used for general id95. the only
di   erence is how we add the new surfers. suppose s is a set of integers consisting
of the row/column numbers for the pages we have identi   ed as belonging to a
certain topic (called the teleport set). let es be a vector that has 1 in the
components in s and 0 in other components. then the topic-sensitive page-
rank for s is the limit of the iteration

v    =   m v + (1       )es/|s|

here, as usual, m is the transition matrix of the web, and |s| is the size of set
s.

example 5.10 : let us reconsider the original web graph we used in fig. 5.1,
which we reproduce as fig. 5.15. suppose we use    = 0.8. then the transition
matrix for this graph, multiplied by   , is

  m =

   
         

0

2/5 4/5
0
0

0
0
0

4/15
4/15
4/15 2/5

0
2/5
2/5
0

   
         

6this directory, found at www.dmoz.org, is a collection of human-classi   ed web pages.

5.3. topic-sensitive id95

185

a

c

b

d

figure 5.15: repeat of example web graph

suppose that our topic is represented by the teleport set s = {b, d}. then
the vector (1       )es/|s| has 1/10 for its second and fourth components and 0
for the other two components. the reason is that 1        = 1/5, the size of s is
2, and es has 1 in the components for b and d and 0 in the components for a
and c. thus, the equation that must be iterated is

v    =

0

2/5 4/5

0
0

4/15
4/15
4/15 2/5

0
0
0

   
         

0

2/5
2/5

0

   
         

v +

   
         

0

1/10

0

1/10

   
         

here are the    rst few iterations of this equation. we have also started with
the surfers only at the pages in the teleport set. although the initial distribution
has no e   ect on the limit, it may help the computation to converge faster.

   
         

0/2
1/2
0/2
1/2

   
         

,

   
         

2/10
3/10
2/10
3/10

,

   
         

   
         

42/150
41/150
26/150
41/150

   
         

,

   
         

62/250
71/250
46/250
71/250

   
         

, . . . ,

   
         

54/210
59/210
38/210
59/210

   
         

notice that because of the concentration of surfers at b and d, these nodes get
a higher id95 than they did in example 5.2. in that example, a was the
node of highest id95.    

5.3.3 using topic-sensitive id95

in order to integrate topic-sensitive id95 into a search engine, we must:

1. decide on the topics for which we shall create specialized id95 vec-

tors.

2. pick a teleport set for each of these topics, and use that set to compute

the topic-sensitive id95 vector for that topic.

186

chapter 5. link analysis

3. find a way of determining the topic or set of topics that are most relevant

for a particular search query.

4. use the id95 vectors for that topic or topics in the ordering of the

responses to the search query.

we have mentioned one way of selecting the topic set: use the top-level topics
of the open directory. other approaches are possible, but there is probably a
need for human classi   cation of at least some pages.

the third step is probably the trickiest, and several methods have been

proposed. some possibilities:

(a) allow the user to select a topic from a menu.

(b) infer the topic(s) by the words that appear in the web pages recently
searched by the user, or recent queries issued by the user. we need to
discuss how one goes from a collection of words to a topic, and we shall
do so in section 5.3.4

(c) infer the topic(s) by information about the user, e.g., their bookmarks or

their stated interests on facebook.

5.3.4

inferring topics from words

the question of classifying documents by topic is a subject that has been studied
for decades, and we shall not go into great detail here. su   ce it to say that
topics are characterized by words that appear surprisingly often in documents
on that topic. for example, neither fullback nor measles appear very often in
documents on the web. but fullback will appear far more often than average
in pages about sports, and measles will appear far more often than average in
pages about medicine.

if we examine the entire web, or a large, random sample of the web, we
can get the background frequency of each word. suppose we then go to a large
sample of pages known to be about a certain topic, say the pages classi   ed
under sports by the open directory. examine the frequencies of words in the
sports sample, and identify the words that appear signi   cantly more frequently
in the sports sample than in the background.
in making this judgment, we
must be careful to avoid some extremely rare word that appears in the sports
sample with relatively higher frequency. this word is probably a misspelling
that happened to appear only in one or a few of the sports pages. thus, we
probably want to put a    oor on the number of times a word appears, before it
can be considered characteristic of a topic.

once we have identi   ed a large collection of words that appear much more
frequently in the sports sample than in the background, and we do the same
for all the topics on our list, we can examine other pages and classify them by
topic. here is a simple approach. suppose that s1, s2, . . . , sk are the sets of
words that have been determined to be characteristic of each of the topics on

5.4. link spam

187

our list. let p be the set of words that appear in a given page p . compute
the jaccard similarity (recall section 3.1.1) between p and each of the si   s.
classify the page as that topic with the highest jaccard similarity. note that
all jaccard similarities may be very low, especially if the sizes of the sets si are
small. thus, it is important to pick reasonably large sets si to make sure that
we cover all aspects of the topic represented by the set.

we can use this method, or a number of variants, to classify the pages the
user has most recently retrieved. we could say the user is interested in the topic
into which the largest number of these pages fall. or we could blend the topic-
sensitive id95 vectors in proportion to the fraction of these pages that
fall into each topic, thus constructing a single id95 vector that re   ects
the user   s current blend of interests. we could also use the same procedure on
the pages that the user currently has bookmarked, or combine the bookmarked
pages with the recently viewed pages.

5.3.5 exercises for section 5.3

exercise 5.3.1 : compute the topic-sensitive id95 for the graph of fig.
5.15, assuming the teleport set is:

(a) a only.

(b) a and c.

5.4 link spam

when it became apparent that id95 and other techniques used by google
made term spam ine   ective, spammers turned to methods designed to fool
the id95 algorithm into overvaluing certain pages. the techniques for
arti   cially increasing the id95 of a page are collectively called link spam.
in this section we shall    rst examine how spammers create link spam, and
then see several methods for decreasing the e   ectiveness of these spamming
techniques, including trustrank and measurement of spam mass.

5.4.1 architecture of a spam farm

a collection of pages whose purpose is to increase the id95 of a certain
page or pages is called a spam farm. figure 5.16 shows the simplest form of
spam farm. from the point of view of the spammer, the web is divided into
three parts:

1. inaccessible pages: the pages that the spammer cannot a   ect. most of

the web is in this part.

2. accessible pages: those pages that, while they are not controlled by the

spammer, can be a   ected by the spammer.

188

chapter 5. link analysis

3. own pages: the pages that the spammer owns and controls.

target
page

accessible

pages

inaccessible

pages

own
pages

figure 5.16: the web from the point of view of the link spammer

the spam farm consists of the spammer   s own pages, organized in a special
way as seen on the right, and some links from the accessible pages to the
spammer   s pages. without some links from the outside, the spam farm would
be useless, since it would not even be crawled by a typical search engine.

concerning the accessible pages, it might seem surprising that one can af-
fect a page without owning it. however, today there are many sites, such as
blogs or newspapers that invite others to post their comments on the site. in
order to get as much id95    owing to his own pages from outside, the
spammer posts many comments such as    i agree. please see my article at
www.myspamfarm.com.   

in the spam farm, there is one page t, the target page, at which the spammer
attempts to place as much id95 as possible. there are a large number
m of supporting pages, that accumulate the portion of the id95 that is
distributed equally to all pages (the fraction 1        of the id95 that repre-
sents surfers going to a random page). the supporting pages also prevent the
id95 of t from being lost, to the extent possible, since some will be taxed
away at each round. notice that t has a link to every supporting page, and
every supporting page links only to t.

5.4. link spam

189

5.4.2 analysis of a spam farm

suppose that id95 is computed using a taxation parameter   , typically
around 0.85. that is,    is the fraction of a page   s id95 that gets dis-
tributed to its successors at the next round. let there be n pages on the web
in total, and let some of them be a spam farm of the form suggested in fig. 5.16,
with a target page t and m supporting pages. let x be the amount of id95
contributed by the accessible pages. that is, x is the sum, over all accessible
pages p with a link to t, of the id95 of p times   , divided by the number
of successors of p. finally, let y be the unknown id95 of t. we shall solve
for y.

first, the id95 of each supporting page is

  y/m + (1       )/n

the    rst term represents the contribution from t. the id95 y of t is taxed,
so only   y is distributed to t   s successors. that id95 is divided equally
among the m supporting pages. the second term is the supporting page   s share
of the fraction 1        of the id95 that is divided equally among all pages
on the web.
now, let us compute the id95 y of target page t. its id95 comes

from three sources:

1. contribution x from outside, as we have assumed.

2.    times the id95 of every supporting page; that is,

  (cid:0)  y/m + (1       )/n(cid:1)

3. (1      )/n, the share of the fraction 1       of the id95 that belongs to
t. this amount is negligible and will be dropped to simplify the analysis.

thus, from (1) and (2) above, we can write

y = x +   m(cid:16)   y

m

+

1       
n (cid:17) = x +   2y +   (1       )

m
n

we may solve the above equation for y, yielding

y =

x

1       2 + c

m
n

where c =   (1       )/(1       2) =   /(1 +   ).

if we choose    = 0.85, then 1/(1       2) = 3.6, and c =
example 5.11 :
  /(1 +   ) = 0.46. that is, the structure has ampli   ed the external id95
contribution by 360%, and also obtained an amount of id95 that is 46%
of the fraction of the web, m/n, that is in the spam farm.    

190

chapter 5. link analysis

5.4.3 combating link spam

it has become essential for search engines to detect and eliminate link spam,
just as it was necessary in the previous decade to eliminate term spam. there
are two approaches to link spam. one is to look for structures such as the
spam farm in fig. 5.16, where one page links to a very large number of pages,
each of which links back to it. search engines surely search for such structures
and eliminate those pages from their index. that causes spammers to develop
di   erent structures that have essentially the same e   ect of capturing id95
for a target page or pages. there is essentially no end to variations of fig. 5.16,
so this war between the spammers and the search engines will likely go on for
a long time.

however, there is another approach to eliminating link spam that doesn   t
rely on locating the spam farms. rather, a search engine can modify its de   ni-
tion of id95 to lower the rank of link-spam pages automatically. we shall
consider two di   erent formulas:

1. trustrank, a variation of topic-sensitive id95 designed to lower the

score of spam pages.

2. spam mass, a calculation that identi   es the pages that are likely to be
spam and allows the search engine to eliminate those pages or to lower
their id95 strongly.

5.4.4 trustrank

trustrank is topic-sensitive id95, where the    topic    is a set of pages be-
lieved to be trustworthy (not spam). the theory is that while a spam page
might easily be made to link to a trustworthy page, it is unlikely that a trust-
worthy page would link to a spam page. the borderline area is a site with
blogs or other opportunities for spammers to create links, as was discussed in
section 5.4.1. these pages cannot be considered trustworthy, even if their own
content is highly reliable, as would be the case for a reputable newspaper that
allowed readers to post comments.

to implement trustrank, we need to develop a suitable teleport set of

trustworthy pages. two approaches that have been tried are:

1. let humans examine a set of pages and decide which of them are trust-
worthy. for example, we might pick the pages of highest id95 to
examine, on the theory that, while link spam can raise a page   s rank from
the bottom to the middle of the pack, it is essentially impossible to give
a spam page a id95 near the top of the list.

2. pick a domain whose membership is controlled, on the assumption that it
is hard for a spammer to get their pages into these domains. for example,
we could pick the .edu domain, since university pages are unlikely to be
spam farms. we could likewise pick .mil, or .gov. however, the problem

5.4. link spam

191

with these speci   c choices is that they are almost exclusively us sites. to
get a good distribution of trustworthy web pages, we should include the
analogous sites from foreign countries, e.g., ac.il, or edu.sg.

it is likely that search engines today implement a strategy of the second type
routinely, so that what we think of as id95 really is a form of trustrank.

5.4.5 spam mass

the idea behind spam mass is that we measure for each page the fraction of its
id95 that comes from spam. we do so by computing both the ordinary
id95 and the trustrank based on some teleport set of trustworthy pages.
suppose page p has id95 r and trustrank t. then the spam mass of p is
(r     t)/r. a negative or small positive spam mass means that p is probably not
a spam page, while a spam mass close to 1 suggests that the page probably is
spam. it is possible to eliminate pages with a high spam mass from the index
of web pages used by a search engine, thus eliminating a great deal of the link
spam without having to identify particular structures that spam farmers use.

example 5.12 : let us consider both the id95 and topic-sensitive page-
rank that were computed for the graph of fig. 5.1 in examples 5.2 and 5.10,
respectively.
in the latter case, the teleport set was nodes b and d, so let
us assume those are the trusted pages. figure 5.17 tabulates the id95,
trustrank, and spam mass for each of the four nodes.

node id95 trustrank

spam mass

a
b
c
d

3/9
2/9
2/9
2/9

54/210
59/210
38/210
59/210

0.229
-0.264
0.186
-0.264

figure 5.17: calculation of spam mass

in this simple example, the only conclusion is that the nodes b and d, which
were a priori determined not to be spam, have negative spam mass and are
therefore not spam. the other two nodes, a and c, each have a positive spam
mass, since their id95s are higher than their trustranks. for instance,
the spam mass of a is computed by taking the di   erence 3/9    54/210 = 8/105
and dividing 8/105 by the id95 3/9 to get 8/35 or about 0.229. however,
their spam mass is still closer to 0 than to 1, so it is probable that they are not
spam.    

5.4.6 exercises for section 5.4

exercise 5.4.1 : in section 5.4.2 we analyzed the spam farm of fig. 5.16, where
every supporting page links back to the target page. repeat the analysis for a

192

chapter 5. link analysis

spam farm in which:

(a) each supporting page links to itself instead of to the target page.

(b) each supporting page links nowhere.

(c) each supporting page links both to itself and to the target page.

exercise 5.4.2 : for the original web graph of fig. 5.1, assuming only b is a
trusted page:

(a) compute the trustrank of each page.

(b) compute the spam mass of each page.

! exercise 5.4.3 : suppose two spam farmers agree to link their spam farms.
how would you link the pages in order to increase as much as possible the
id95 of each spam farm   s target page? is there an advantage to linking
spam farms?

5.5 hubs and authorities

an idea called    hubs and authorities    was proposed shortly after id95 was
   rst implemented. the algorithm for computing hubs and authorities bears
some resemblance to the computation of id95, since it also deals with the
iterative computation of a    xedpoint involving repeated matrix   vector multi-
plication. however, there are also signi   cant di   erences between the two ideas,
and neither can substitute for the other.

this hubs-and-authorities algorithm, sometimes called hits (hyperlink-
induced topic search), was originally intended not as a preprocessing step before
handling search queries, as id95 is, but as a step to be done along with
the processing of a search query, to rank only the responses to that query. we
shall, however, describe it as a technique for analyzing the entire web, or the
portion crawled by a search engine. there is reason to believe that something
like this approach is, in fact, used by the ask search engine.

5.5.1 the intuition behind hits

while id95 assumes a one-dimensional notion of importance for pages,
hits views important pages as having two    avors of importance.

1. certain pages are valuable because they provide information about a

topic. these pages are called authorities.

2. other pages are valuable not because they provide information about any
topic, but because they tell you where to go to    nd out about that topic.
these pages are called hubs.

5.5. hubs and authorities

193

example 5.13 : a typical department at a university maintains a web page
listing all the courses o   ered by the department, with links to a page for each
course, telling about the course     the instructor, the text, an outline of the
course content, and so on. if you want to know about a certain course, you
need the page for that course; the departmental course list will not do. the
course page is an authority for that course. however, if you want to    nd out
what courses the department is o   ering, it is not helpful to search for each
courses    page; you need the page with the course list    rst. this page is a hub
for information about courses.    

just as id95 uses the recursive de   nition of importance that    a page
is important if important pages link to it,    hits uses a mutually recursive
de   nition of two concepts:    a page is a good hub if it links to good authorities,
and a page is a good authority if it is linked to by good hubs.   

5.5.2 formalizing hubbiness and authority

to formalize the above intuition, we shall assign two scores to each web page.
one score represents the hubbiness of a page     that is, the degree to which it
is a good hub, and the second score represents the degree to which the page
is a good authority. assuming that pages are enumerated, we represent these
scores by vectors h and a. the ith component of h gives the hubbiness of the
ith page, and the ith component of a gives the authority of the same page.

while importance is divided among the successors of a page, as expressed by
the transition matrix of the web, the normal way to describe the computation
of hubbiness and authority is to add the authority of successors to estimate
hubbiness and to add hubbiness of predecessors to estimate authority. if that
is all we did, then the hubbiness and authority values would typically grow
beyond bounds. thus, we normally scale the values of the vectors h and a so
that the largest component is 1. an alternative is to scale so that the sum of
components is 1.

to describe the iterative computation of h and a formally, we use the link
matrix of the web, l. if we have n pages, then l is an n  n matrix, and lij = 1
if there is a link from page i to page j, and lij = 0 if not. we shall also have
need for lt, the transpose of l. that is, lt
ij = 1 if there is a link from page
ij = 0 otherwise. notice that lt is similar to the matrix m
j to page i, and lt
that we used for id95, but where lt has 1, m has a fraction     1 divided
by the number of out-links from the page represented by that column.

example 5.14 : for a running example, we shall use the web of fig. 5.4,
which we reproduce here as fig. 5.18. an important observation is that dead
ends or spider traps do not prevent the hits iteration from converging to a
meaningful pair of vectors. thus, we can work with fig. 5.18 directly, with
no    taxation    or alteration of the graph needed. the link matrix l and its
transpose are shown in fig. 5.19.    

194

chapter 5. link analysis

a

c

e

b

d

figure 5.18: sample data used for hits examples

l =

0
1
0
0
0

   

               

1
0
0
1
0

1 1
0 1
0 0
1 0
0 0

0
0
1
0
0

   

               

lt =

0
1
1
1
0

   

               

1 0
0 0
0 0
1 0
0 1

0 0
1 0
1 0
0 0
0 0

   

               

figure 5.19: the link matrix for the web of fig. 5.18 and its transpose

the fact that the hubbiness of a page is proportional to the sum of the
authority of its successors is expressed by the equation h =   la, where    is
an unknown constant representing the scaling factor needed. likewise, the fact
that the authority of a page is proportional to the sum of the hubbinesses of
its predecessors is expressed by a =   lth, where    is another scaling constant.
these equations allow us to compute the hubbiness and authority indepen-
dently, by substituting one equation in the other, as:

    h =     llth.
    a =     ltla.
however, since llt and ltl are not as sparse as l and lt, we are usually
better o    computing h and a in a true mutual recursion. that is, start with h
a vector of all 1   s.

1. compute a = lth and then scale so the largest component is 1.

2. next, compute h = la and scale again.

5.5. hubs and authorities

195

now, we have a new h and can repeat steps (1) and (2) until at some iteration
the changes to the two vectors are su   ciently small that we can stop and accept
the current values as the limit.

   

               

   

               

1
1
1
1
1

h

   

               

1
2
2
2
1

   

               

lth

   

               

   

               

1/2
1
1
1
1/2

a

   

               

   

               

3
3/2
1/2
2
0

la

   

               

   

               

1

1/2
1/6
2/3

0

h

   

               

1/2
5/3
5/3
3/2
1/6

   

               

lth

   

               

   

               

3/10

1
1

9/10
1/10

a

   

               

   

               

29/10

6/5
1/10

2
0

la

   

               

   

               

1

12/29
1/29
20/29

0

h

figure 5.20: first two iterations of the hits algorithm

example 5.15 : let us perform the    rst two iterations of the hits algorithm
on the web of fig. 5.18. in fig. 5.20 we see the succession of vectors computed.
the    rst column is the initial h, all 1   s. in the second column, we have estimated
the relative authority of pages by computing lth, thus giving each page the
sum of the hubbinesses of its predecessors. the third column gives us the    rst
estimate of a. it is computed by scaling the second column; in this case we
have divided each component by 2, since that is the largest value in the second
column.

the fourth column is la. that is, we have estimated the hubbiness of each
page by summing the estimate of the authorities of each of its successors. then,
the    fth column scales the fourth column. in this case, we divide by 3, since
that is the largest value in the fourth column. columns six through nine repeat
the process outlined in our explanations for columns two through    ve, but with
the better estimate of hubbiness given by the    fth column.

the limit of this process may not be obvious, but it can be computed by a

simple program. the limits are:

h =

   

               

1

0.3583

0

0.7165

0

   

               

a =

   

               

0.2087

1
1

0.7913

0

   

               

196

chapter 5. link analysis

this result makes sense. first, we notice that the hubbiness of e is surely 0,
since it leads nowhere. the hubbiness of c depends only on the authority of e
and vice versa, so it should not surprise us that both are 0. a is the greatest
hub, since it links to the three biggest authorities, b, c, and d. also, b and
c are the greatest authorities, since they are linked to by the two biggest hubs,
a and d.

for web-sized graphs, the only way of computing the solution to the hubs-
and-authorities equations is iteratively. however, for this tiny example, we
can compute the solution by solving equations. we shall use the equations
h =     llth. first, llt is

llt =

3 1
1 2
0 0
2 0
0 0

   

               

0 2
0 0
1 0
0 2
0 0

0
0
0
0
0

   

               

let    = 1/(    ) and let the components of h for nodes a through e be a through
e, respectively. then the equations for h can be written

  a = 3a + b + 2d
  c = c
  e = 0

  b = a + 2b
  d = 2a + 2d

the equation for b tells us b = a/(       2) and the equation for d tells us d =
2a/(       2). if we substitute these expressions for b and d in the equation for a,
we get   a = a(cid:0)3+5/(     2)(cid:1). from this equation, since a is a factor of both sides,
we are left with a quadratic equation for    which simpli   es to   2     5   + 1 = 0.
the positive root is    = (5 +    21)/2 = 4.791. now that we know    is neither
0 or 1, the equations for c and e tell us immediately that c = e = 0.

finally, if we recognize that a is the largest component of h and set a = 1,
we get b = 0.3583 and d = 0.7165. along with c = e = 0, these values give us
the limiting value of h. the value of a can be computed from h by multiplying
by lt and scaling.    

5.5.3 exercises for section 5.5

exercise 5.5.1 : compute the hubbiness and authority of each of the nodes in
our original web graph of fig. 5.1.

! exercise 5.5.2 : suppose our graph is a chain of n nodes, as was suggested by

fig. 5.9. compute the hubs and authorities vectors, as a function of n.

5.6 summary of chapter 5

    term spam: early search engines were unable to deliver relevant results
because they were vulnerable to term spam     the introduction into web
pages of words that misrepresented what the page was about.

5.6. summary of chapter 5

197

    the google solution to term spam: google was able to counteract term
spam by two techniques. first was the id95 algorithm for deter-
mining the relative importance of pages on the web. the second was a
strategy of believing what other pages said about a given page, in or near
their links to that page, rather than believing only what the page said
about itself.

    id95 : id95 is an algorithm that assigns a real number, called
its id95, to each page on the web. the id95 of a page is a
measure of how important the page is, or how likely it is to be a good
response to a search query. in its simplest form, id95 is a solution
to the recursive equation    a page is important if important pages link to
it.   

    transition matrix of the web: we represent links in the web by a matrix
whose ith row and ith column represent the ith page of the web. if there
are one or more links from page j to page i, then the entry in row i and
column j is 1/k, where k is the number of pages to which page j links.
other entries of the transition matrix are 0.

    computing id95 on strongly connected web graphs: for strongly
connected web graphs (those where any node can reach any other node),
id95 is the principal eigenvector of the transition matrix. we can
compute id95 by starting with any nonzero vector and repeatedly
multiplying the current vector by the transition matrix, to get a better
estimate.7 after about 50 iterations, the estimate will be very close to
the limit, which is the true id95.

    the random surfer model : calculation of id95 can be thought of
as simulating the behavior of many random surfers, who each start at a
random page and at any step move, at random, to one of the pages to
which their current page links. the limiting id203 of a surfer being
at a given page is the id95 of that page. the intuition is that people
tend to create links to the pages they think are useful, so random surfers
will tend to be at a useful page.

    dead ends: a dead end is a web page with no links out. the presence of
dead ends will cause the id95 of some or all of the pages to go to 0
in the iterative computation, including pages that are not dead ends. we
can eliminate all dead ends before undertaking a id95 calculation
by recursively dropping nodes with no arcs out. note that dropping one
node can cause another, which linked only to it, to become a dead end,
so the process must be recursive.

7technically, the condition for this method to work is more restricted than simply    strongly
connected.    however, the other necessary conditions will surely be met by any large strongly
connected component of the web that was not arti   cially constructed.

198

chapter 5. link analysis

    spider traps: a spider trap is a set of nodes that, while they may link to
each other, have no links out to other nodes. in an iterative calculation
of id95, the presence of spider traps cause all the id95 to be
captured within that set of nodes.

    taxation schemes: to counter the e   ect of spider traps (and of dead ends,
if we do not eliminate them), id95 is normally computed in a way
that modi   es the simple iterative multiplication by the transition matrix.
a parameter    is chosen, typically around 0.85. given an estimate of the
id95, the next estimate is computed by multiplying the estimate by
   times the transition matrix, and then adding (1       )/n to the estimate
for each page, where n is the total number of pages.

    taxation and random surfers: the calculation of id95 using taxa-
tion parameter    can be thought of as giving each random surfer a prob-
ability 1        of leaving the web, and introducing an equivalent number
of surfers randomly throughout the web.

    e   cient representation of transition matrices: since a transition matrix
is very sparse (almost all entries are 0), it saves both time and space
to represent it by listing its nonzero entries. however, in addition to
being sparse, the nonzero entries have a special property: they are all the
same in any given column; the value of each nonzero entry is the inverse
of the number of nonzero entries in that column. thus, the preferred
representation is column-by-column, where the representation of a column
is the number of nonzero entries, followed by a list of the rows where those
entries occur.

    very large-scale matrix   vector multiplication: for web-sized graphs, it
may not be feasible to store the entire id95 estimate vector in the
main memory of one machine. thus, we can break the vector into k
segments and break the transition matrix into k2 squares, called blocks,
assigning each square to one machine. the vector segments are each sent
to k machines, so there is a small additional cost in replicating the vector.

    representing blocks of a transition matrix : when we divide a transition
matrix into square blocks, the columns are divided into k segments. to
represent a segment of a column, nothing is needed if there are no nonzero
entries in that segment. however, if there are one or more nonzero entries,
then we need to represent the segment of the column by the total number
of nonzero entries in the column (so we can tell what value the nonzero
entries have) followed by a list of the rows with nonzero entries.

    topic-sensitive id95 : if we know the queryer is interested in a cer-
tain topic, then it makes sense to bias the id95 in favor of pages
on that topic. to compute this form of id95, we identify a set of
pages known to be on that topic, and we use it as a    teleport set.    the

5.6. summary of chapter 5

199

id95 calculation is modi   ed so that only the pages in the teleport
set are given a share of the tax, rather than distributing the tax among
all pages on the web.

    creating teleport sets: for topic-sensitive id95 to work, we need to
identify pages that are very likely to be about a given topic. one approach
is to start with the pages that the open directory (dmoz) identi   es with
that topic. another is to identify words known to be associated with the
topic, and select for the teleport set those pages that have an unusually
high number of occurrences of such words.

    link spam: to fool the id95 algorithm, unscrupulous actors have
created spam farms. these are collections of pages whose purpose is to
concentrate high id95 on a particular target page.

    structure of a spam farm: typically, a spam farm consists of a target
page and very many supporting pages. the target page links to all the
supporting pages, and the supporting pages link only to the target page.
in addition, it is essential that some links from outside the spam farm be
created. for example, the spammer might introduce links to their target
page by writing comments in other people   s blogs or discussion groups.

    trustrank : one way to ameliorate the e   ect of link spam is to compute
a topic-sensitive id95 called trustrank, where the teleport set is a
collection of trusted pages. for example, the home pages of universities
could serve as the trusted set. this technique avoids sharing the tax in
the id95 calculation with the large numbers of supporting pages in
spam farms and thus preferentially reduces their id95.

    spam mass: to identify spam farms, we can compute both the conven-
tional id95 and the trustrank for all pages. those pages that have
much lower trustrank than id95 are likely to be part of a spam
farm.

    hubs and authorities: while id95 gives a one-dimensional view
of the importance of pages, an algorithm called hits tries to measure
two di   erent aspects of importance. authorities are those pages that
contain valuable information. hubs are pages that, while they do not
themselves contain the information, link to places where the information
can be found.

    recursive formulation of the hits algorithm: calculation of the hubs
and authorities scores for pages depends on solving the recursive equa-
tions:    a hub links to many authorities, and an authority is linked to
by many hubs.    the solution to these equations is essentially an iter-
ated matrix   vector multiplication, just like id95   s. however, the
existence of dead ends or spider traps does not a   ect the solution to the

200

chapter 5. link analysis

hits equations in the way they do for id95, so no taxation scheme
is necessary.

5.7 references for chapter 5

the id95 algorithm was    rst expressed in [1]. the experiments on the
structure of the web, which we used to justify the existence of dead ends and
spider traps, were described in [2]. the block-stripe method for performing the
id95 iteration is taken from [5].

topic-sensitive id95 is taken from [6]. trustrank is described in [4],

and the idea of spam mass is taken from [3].

the hits (hubs and authorities) idea was described in [7].

1. s. brin and l. page,    anatomy of a large-scale hypertextual web search
engine,    proc. 7th intl. world-wide-web conference, pp. 107   117, 1998.

2. a. broder, r. kumar, f. maghoul, p. raghavan, s. rajagopalan, r.
stata, a. tomkins, and j. weiner,    graph structure in the web,    com-
puter networks 33:1   6, pp. 309   320, 2000.

3. z. gy  ongi, p. berkhin, h. garcia-molina, and j. pedersen,    link spam
detection based on mass estimation,    proc. 32nd intl. conf. on very large
databases, pp. 439   450, 2006.

4. z. gy  ongi, h. garcia-molina, and j. pedersen,    combating link spam
with trustrank,    proc. 30th intl. conf. on very large databases, pp. 576   
587, 2004.

5. t.h. haveliwala,    e   cient computation of id95,    stanford univ.

dept. of computer science technical report, sept., 1999. available as

http://infolab.stanford.edu/~taherh/papers/efficient-pr.pdf

6. t.h. haveliwala,    topic-sensitive id95,    proc. 11th intl. world-

wide-web conference, pp. 517   526, 2002

7. j.m. kleinberg,    authoritative sources in a hyperlinked environment,    j.

acm 46:5, pp. 604   632, 1999.

chapter 6

frequent itemsets

we turn in this chapter to one of the major families of techniques for character-
izing data: the discovery of frequent itemsets. this problem is often viewed as
the discovery of    association rules,    although the latter is a more complex char-
acterization of data, whose discovery depends fundamentally on the discovery
of frequent itemsets.

to begin, we introduce the    market-basket    model of data, which is essen-
tially a many-many relationship between two kinds of elements, called    items   
and    baskets,    but with some assumptions about the shape of the data. the
frequent-itemsets problem is that of    nding sets of items that appear in (are
related to) many of the same baskets.

the problem of    nding frequent itemsets di   ers from the similarity search
discussed in chapter 3. here we are interested in the absolute number of baskets
that contain a particular set of items. in chapter 3 we wanted items that have
a large fraction of their baskets in common, even if the absolute number of
baskets is small.

the di   erence leads to a new class of algorithms for    nding frequent item-
sets. we begin with the a-priori algorithm, which works by eliminating most
large sets as candidates by looking    rst at smaller sets and recognizing that a
large set cannot be frequent unless all its subsets are. we then consider various
improvements to the basic a-priori idea, concentrating on very large data sets
that stress the available main memory.

next, we consider approximate algorithms that work faster but are not
guaranteed to    nd all frequent itemsets. also in this class of algorithms are
those that exploit parallelism, including the parallelism we can obtain through
a mapreduce formulation. finally, we discuss brie   y how to    nd frequent
itemsets in a data stream.

201

202

chapter 6. frequent itemsets

6.1 the market-basket model

the market-basket model of data is used to describe a common form of many-
many relationship between two kinds of objects. on the one hand, we have
items, and on the other we have baskets, sometimes called    transactions.   
each basket consists of a set of items (an itemset), and usually we assume that
the number of items in a basket is small     much smaller than the total number
of items. the number of baskets is usually assumed to be very large, bigger
than what can    t in main memory. the data is assumed to be represented in a
   le consisting of a sequence of baskets. in terms of the distributed    le system
described in section 2.1, the baskets are the objects of the    le, and each basket
is of type    set of items.   

6.1.1 de   nition of frequent itemsets

intuitively, a set of items that appears in many baskets is said to be    frequent.   
to be formal, we assume there is a number s, called the support threshold. if
i is a set of items, the support for i is the number of baskets for which i is a
subset. we say i is frequent if its support is s or more.

example 6.1 : in fig. 6.1 are sets of words. each set is a basket, and the
words are items. we took these sets by googling cat dog and taking snippets
from the highest-ranked pages. do not be concerned if a word appears twice
in a basket, as baskets are sets, and in principle items can appear only once.
also, ignore capitalization.

1. {cat, and, dog, bites}
2. {yahoo, news, claims, a, cat, mated, with, a, dog, and, produced, viable,

o   spring}

3. {cat, killer, likely, is, a, big, dog}
4. {professional, free, advice, on, dog, training, puppy, training}
5. {cat, and, kitten, training, and, behavior}
6. {dog, &, cat, provides, dog, training, in, eugene, oregon}
7. {   dog, and, cat   , is, a, slang, term, used, by, police, o   cers, for, a, male   

female, relationship}

8. {shop, for, your, show, dog, grooming, and, pet, supplies}

figure 6.1: here are eight baskets, each consisting of items that are words

since the empty set is a subset of any set, the support for     is 8. however,
we shall not generally concern ourselves with the empty set, since it tells us

6.1. the market-basket model

203

nothing.

among the singleton sets, obviously {cat} and {dog} are quite frequent.
   dog    appears in all but basket (5), so its support is 7, while    cat    appears in
all but (4) and (8), so its support is 6. the word    and    is also quite frequent;
it appears in (1), (2), (5), (7), and (8), so its support is 5. the words    a    and
   training    appear in three sets, while    for    and    is    appear in two each. no
other word appears more than once.

suppose that we set our threshold at s = 3. then there are    ve frequent

singleton itemsets: {dog}, {cat}, {and}, {a}, and {training}.

now, let us look at the doubletons. a doubleton cannot be frequent unless
both items in the set are frequent by themselves. thus, there are only ten
possible frequent doubletons. fig. 6.2 is a table indicating which baskets contain
which doubletons.

training
4, 6
5, 6
5
none

a
2, 3, 7
2, 3, 7
2, 7

dog
cat
and
a

and
1, 2, 7, 8
1, 2, 5, 7

cat
1, 2, 3, 6, 7

figure 6.2: occurrences of doubletons

for example, we see from the table of fig. 6.2 that doubleton {dog, training}
appears only in baskets (4) and (6). therefore, its support is 2, and it is not
frequent. there are    ve frequent doubletons if s = 3; they are

{dog, a}
{cat, a}

{dog, and}
{cat, and}

{dog, cat}

each appears at least three times; for instance, {dog, cat} appears    ve times.
next, let us see if there are frequent triples. in order to be a frequent triple,
each pair of elements in the set must be a frequent doubleton. for example,
{dog, a, and} cannot be a frequent itemset, because if it were, then surely {a,
and} would be frequent, but it is not. the triple {dog, cat, and} might be
frequent, because each of its doubleton subsets is frequent. unfortunately, the
three words appear together only in baskets (1) and (2), so there are in fact no
frequent triples. the triple {dog, cat, a} might be frequent, since its doubletons
are all frequent. in fact, all three words do appear in baskets (2), (3), and (7),
so it is a frequent triple. no other triple of words is even a candidate for
being a frequent triple, since for no other triple of words are its three doubleton
subsets frequent. as there is only one frequent triple, there can be no frequent
quadruples or larger sets.    

204

chapter 6. frequent itemsets

on-line versus brick-and-mortar retailing

we suggested in section 3.1.3 that an on-line retailer would use similarity
measures for items to    nd pairs of items that, while they might not be
bought by many customers, had a signi   cant fraction of their customers
in common. an on-line retailer could then advertise one item of the pair
to the few customers who had bought the other item of the pair. this
methodology makes no sense for a bricks-and-mortar retailer, because un-
less lots of people buy an item, it cannot be cost e   ective to advertise a
sale on the item. thus, the techniques of chapter 3 are not often useful
for brick-and-mortar retailers.

conversely, the on-line retailer has little need for the analysis we dis-
cuss in this chapter, since it is designed to search for itemsets that appear
frequently. if the on-line retailer was limited to frequent itemsets, they
would miss all the opportunities that are present in the    long tail    to
select advertisements for each customer individually.

6.1.2 applications of frequent itemsets

the original application of the market-basket model was in the analysis of true
market baskets. that is, supermarkets and chain stores record the contents
of every market basket (physical shopping cart) brought to the register for
checkout. here the    items    are the di   erent products that the store sells, and
the    baskets    are the sets of items in a single market basket. a major chain
might sell 100,000 di   erent items and collect data about millions of market
baskets.

by    nding frequent itemsets, a retailer can learn what is commonly bought
together. especially important are pairs or larger sets of items that occur much
more frequently than would be expected were the items bought independently.
we shall discuss this aspect of the problem in section 6.1.3, but for the moment
let us simply consider the search for frequent itemsets. we will discover by this
analysis that many people buy bread and milk together, but that is of little
interest, since we already knew that these were popular items individually. we
might discover that many people buy hot dogs and mustard together. that,
again, should be no surprise to people who like hot dogs, but it o   ers the
supermarket an opportunity to do some clever marketing. they can advertise
a sale on hot dogs and raise the price of mustard. when people come to the
store for the cheap hot dogs, they often will remember that they need mustard,
and buy that too. either they will not notice the price is high, or they reason
that it is not worth the trouble to go somewhere else for cheaper mustard.

the famous example of this type is    diapers and beer.    one would hardly
expect these two items to be related, but through data analysis one chain store
discovered that people who buy diapers are unusually likely to buy beer. the

6.1. the market-basket model

205

theory is that if you buy diapers, you probably have a baby at home, and if you
have a baby, then you are unlikely to be drinking at a bar; hence you are more
likely to bring beer home. the same sort of marketing ploy that we suggested
for hot dogs and mustard could be used for diapers and beer.

however, applications of frequent-itemset analysis is not limited to market
baskets. the same model can be used to mine many other kinds of data. some
examples are:

1. related concepts: let items be words, and let baskets be documents
(e.g., web pages, blogs, tweets). a basket/document contains those
items/words that are present in the document.
if we look for sets of
words that appear together in many documents, the sets will be domi-
nated by the most common words (stop words), as we saw in example 6.1.
there, even though the intent was to    nd snippets that talked about cats
and dogs, the stop words    and    and    a    were prominent among the fre-
quent itemsets. however, if we ignore all the most common words, then
we would hope to    nd among the frequent pairs some pairs of words
that represent a joint concept. for example, we would expect a pair like
{brad, angelina} to appear with surprising frequency.

2. plagiarism: let the items be documents and the baskets be sentences.
an item/document is    in    a basket/sentence if the sentence is in the
document. this arrangement appears backwards, but it is exactly what
we need, and we should remember that the relationship between items
and baskets is an arbitrary many-many relationship. that is,    in    need
not have its conventional meaning:    part of.    in this application, we
look for pairs of items that appear together in several baskets. if we    nd
such a pair, then we have two documents that share several sentences in
common.
in practice, even one or two sentences in common is a good
indicator of plagiarism.

3. biomarkers: let the items be of two types     biomarkers such as genes
or blood proteins, and diseases. each basket is the set of data about
a patient: their genome and blood-chemistry analysis, as well as their
medical history of disease. a frequent itemset that consists of one disease
and one or more biomarkers suggests a test for the disease.

6.1.3 association rules

while the subject of this chapter is extracting frequent sets of items from data,
this information is often presented as a collection of if   then rules, called associ-
ation rules. the form of an association rule is i     j, where i is a set of items
and j is an item. the implication of this association rule is that if all of the
items in i appear in some basket, then j is    likely    to appear in that basket as
well.

206

chapter 6. frequent itemsets

we formalize the notion of    likely    by de   ning the con   dence of the rule
i     j to be the ratio of the support for i     {j} to the support for i. that is,
the con   dence of the rule is the fraction of the baskets with all of i that also
contain j.

example 6.2 : consider the baskets of fig. 6.1. the con   dence of the rule
{cat, dog}     and is 3/5. the words    cat    and    dog    appear in    ve baskets:
(1), (2), (3), (6), and (7). of these,    and    appears in (1), (2), and (7), or 3/5
of the baskets.

for another illustration, the con   dence of {cat}     kitten is 1/6. the word
   cat    appears in six baskets, (1), (2), (3), (5), (6), and (7). of these, only (5)
has the word    kitten.       

con   dence alone can be useful, provided the support for the left side of
the rule is fairly large. for example, we don   t need to know that people are
unusually likely to buy mustard when they buy hot dogs, as long as we know
that many people buy hot dogs, and many people buy both hot dogs and
mustard. we can still use the sale-on-hot-dogs trick discussed in section 6.1.2.
however, there is often more value to an association rule if it re   ects a true
relationship, where the item or items on the left somehow a   ect the item on
the right.

thus, we de   ne the interest of an association rule i     j to be the di   erence
between its con   dence and the fraction of baskets that contain j. that is,
if i has no in   uence on j, then we would expect that the fraction of baskets
including i that contain j would be exactly the same as the fraction of all
baskets that contain j. such a rule has interest 0. however, it is interesting, in
both the informal and technical sense, if a rule has either high interest, meaning
that the presence of i in a basket somehow causes the presence of j, or highly
negative interest, meaning that the presence of i discourages the presence of j.

example 6.3 : the story about beer and diapers is really a claim that the asso-
ciation rule {diapers}     beer has high interest. that is, the fraction of diaper-
buyers who buy beer is signi   cantly greater than the fraction of all customers
that buy beer. an example of a rule with negative interest is {coke}     pepsi.
that is, people who buy coke are unlikely to buy pepsi as well, even though
a good fraction of all people buy pepsi     people typically prefer one or the
other, but not both. similarly, the rule {pepsi}     coke can be expected to
have negative interest.
for some numerical calculations, let us return to the data of fig. 6.1. the
rule {dog}     cat has con   dence 5/7, since    dog    appears in seven baskets, of
which    ve have    cat.    however,    cat    appears in six out of the eight baskets,
so we would expect that 75% of the seven baskets with    dog    would have    cat   
as well. thus, the interest of the rule is 5/7    3/4 =    0.036, which is essentially
0. the rule {cat}     kitten has interest 1/6     1/8 = 0.042. the justi   cation is
that one out of the six baskets with    cat    have    kitten    as well, while    kitten   

6.1. the market-basket model

207

appears in only one of the eight baskets. this interest, while positive, is close
to 0 and therefore indicates the association rule is not very    interesting.       

6.1.4 finding association rules with high con   dence

identifying useful association rules is not much harder than    nding frequent
itemsets. we shall take up the problem of    nding frequent itemsets in the
balance of this chapter, but for the moment, assume it is possible to    nd those
frequent itemsets whose support is at or above a support threshold s.

if we are looking for association rules i     j that apply to a reasonable
fraction of the baskets, then the support of i must be reasonably high.
in
practice, such as for marketing in brick-and-mortar stores,    reasonably high   
is often around 1% of the baskets. we also want the con   dence of the rule to
be reasonably high, perhaps 50%, or else the rule has little practical e   ect. as
a result, the set i     {j} will also have fairly high support.
suppose we have found all itemsets that meet a threshold of support, and
that we have the exact support calculated for each of these itemsets. we can
   nd within them all the association rules that have both high support and high
con   dence. that is, if j is a set of n items that is found to be frequent, there are
only n possible association rules involving this set of items, namely j    {j}     j
for each j in j. if j is frequent, j     {j} must be at least as frequent. thus, it
too is a frequent itemset, and we have already computed the support of both j
and j     {j}. their ratio is the con   dence of the rule j     {j}     j.
it must be assumed that there are not too many frequent itemsets and thus
not too many candidates for high-support, high-con   dence association rules.
the reason is that each one found must be acted upon. if we give the store
manager a million association rules that meet our thresholds for support and
con   dence, they cannot even read them, let alone act on them. likewise, if we
produce a million candidates for biomarkers, we cannot a   ord to run the ex-
periments needed to check them out. thus, it is normal to adjust the support
threshold so that we do not get too many frequent itemsets. this assump-
tion leads, in later sections, to important consequences about the e   ciency of
algorithms for    nding frequent itemsets.

6.1.5 exercises for section 6.1

exercise 6.1.1 : suppose there are 100 items, numbered 1 to 100, and also 100
baskets, also numbered 1 to 100. item i is in basket b if and only if i divides b
with no remainder. thus, item 1 is in all the baskets, item 2 is in all    fty of the
even-numbered baskets, and so on. basket 12 consists of items {1, 2, 3, 4, 6, 12},
since these are all the integers that divide 12. answer the following questions:

(a) if the support threshold is 5, which items are frequent?

! (b) if the support threshold is 5, which pairs of items are frequent?

208

chapter 6. frequent itemsets

! (c) what is the sum of the sizes of all the baskets?

! exercise 6.1.2 : for the item-basket data of exercise 6.1.1, which basket is

the largest?

exercise 6.1.3 : suppose there are 100 items, numbered 1 to 100, and also 100
baskets, also numbered 1 to 100. item i is in basket b if and only if b divides i
with no remainder. for example, basket 12 consists of items

{12, 24, 36, 48, 60, 72, 84, 96}

repeat exercise 6.1.1 for this data.

! exercise 6.1.4 : this question involves data from which nothing interesting
can be learned about frequent itemsets, because there are no sets of items that
are correlated. suppose the items are numbered 1 to 10, and each basket is
constructed by including item i with id203 1/i, each decision being made
independently of all other decisions. that is, all the baskets contain item 1,
half contain item 2, a third contain item 3, and so on. assume the number of
baskets is su   ciently large that the baskets collectively behave as one would
expect statistically. let the support threshold be 1% of the baskets. find the
frequent itemsets.

exercise 6.1.5 : for the data of exercise 6.1.1, what is the con   dence of the
following association rules?

(a) {5, 7}     2.
(b) {2, 3, 4}     5.
exercise 6.1.6 : for the data of exercise 6.1.3, what is the con   dence of the
following association rules?

(a) {24, 60}     8.
(b) {2, 3, 4}     5.

!! exercise 6.1.7 : describe all the association rules that have 100% con   dence

for the market-basket data of:

(a) exercise 6.1.1.

(b) exercise 6.1.3.

! exercise 6.1.8 : prove that in the data of exercise 6.1.4 there are no interest-

ing association rules; i.e., the interest of every association rule is 0.

6.2. market baskets and the a-priori algorithm

209

6.2 market baskets and the a-priori algorithm

we shall now begin a discussion of how to    nd frequent itemsets or information
derived from them, such as association rules with high support and con   dence.
the original improvement on the obvious algorithms, known as    a-priori,    from
which many variants have been developed, will be covered here. the next two
sections will discuss certain further improvements. before discussing the a-
priori algorithm itself, we begin the section with an outline of the assumptions
about how data is stored and manipulated when searching for frequent itemsets.

6.2.1 representation of market-basket data

as we mentioned, we assume that market-basket data is stored in a    le basket-
by-basket. possibly, the data is in a distributed    le system as in section 2.1,
and the baskets are the objects the    le contains. or the data may be stored
in a conventional    le, with a character code to represent the baskets and their
items.

example 6.4 : we could imagine that such a    le begins:

{23,456,1001}{3,18,92,145}{...

here, the character { begins a basket and the character } ends it. the items in
a basket are represented by integers and are separated by commas. thus, the
   rst basket contains items 23, 456, and 1001; the second basket contains items
3, 18, 92, and 145.    

it may be that one machine receives the entire    le. or we could be using
mapreduce or a similar tool to divide the work among many processors, in
which case each processor receives only a part of the    le.
it turns out that
combining the work of parallel processors to get the exact collection of itemsets
that meet a global support threshold is hard, and we shall address this question
only in section 6.4.4.

we also assume that the size of the    le of baskets is su   ciently large that it
does not    t in main memory. thus, a major cost of any algorithm is the time
it takes to read the baskets from disk. once a disk block full of baskets is in
main memory, we can expand it, generating all the subsets of size k. since one
of the assumptions of our model is that the average size of a basket is small,
generating all the pairs in main memory should take time that is much less
than the time it took to read the basket from disk. for example, if there are 20

items in a basket, then there are (cid:0)20

these can be generated easily in a pair of nested for-loops.

2(cid:1) = 190 pairs of items in the basket, and

as the size of the subsets we want to generate gets larger, the time required
grows larger; in fact takes approximately time nk/k! to generate all the subsets
of size k for a basket with n items. eventually, this time dominates the time
needed to transfer the data from disk. however:

210

chapter 6. frequent itemsets

1. often, we need only small frequent itemsets, so k never grows beyond 2

or 3.

2. and when we do need the itemsets for a large size k, it is usually possible
to eliminate many of the items in each basket as not able to participate
in a frequent itemset, so the value of n drops as k increases.

the conclusion we would like to draw is that the work of examining each of
the baskets can usually be assumed proportional to the size of the    le. we can
thus measure the running time of a frequent-itemset algorithm by the number
of times that each disk block of the data    le is read.

moreover, all the algorithms we discuss have the property that they read the
basket    le sequentially. thus, algorithms can be characterized by the number
of passes through the basket    le that they make, and their running time is
proportional to the product of the number of passes they make through the
basket    le times the size of that    le. since we cannot control the amount of
data, only the number of passes taken by the algorithm matters, and it is that
aspect of the algorithm that we shall focus upon when measuring the running
time of a frequent-itemset algorithm.

6.2.2 use of main memory for itemset counting

there is a second data-related issue that we must examine, however. all
frequent-itemset algorithms require us to maintain many di   erent counts as
we make a pass through the data. for example, we might need to count the
number of times that each pair of items occurs in baskets. if we do not have
enough main memory to store each of the counts, then adding 1 to a random
count will most likely require us to load a page from disk. in that case, the
algorithm will thrash and run many orders of magnitude slower than if we were
certain to    nd each count in main memory. the conclusion is that we cannot
count anything that doesn   t    t in main memory. thus, each algorithm has a
limit on how many items it can deal with.

example 6.5 : suppose a certain algorithm has to count all pairs of items,

and there are n items. we thus need space to store (cid:0)n
2(cid:1) integers, or about
n2/2 integers. if integers take 4 bytes, we require 2n2 bytes. if our machine
has 2 gigabytes, or 231 bytes of main memory, then we require n     215, or
approximately n < 33,000.    

it is not trivial to store the (cid:0)n

2(cid:1) counts in a way that makes it easy to    nd
the count for a pair {i, j}. first, we have not assumed anything about how
items are represented. they might, for instance, be strings like    bread.    it
is more space-e   cient to represent items by consecutive integers from 1 to n,
where n is the number of distinct items. unless items are already represented
this way, we need a hash table that translates items as they appear in the    le
to integers. that is, each time we see an item in the    le, we hash it. if it is

6.2. market baskets and the a-priori algorithm

211

already in the hash table, we can obtain its integer code from its entry in the
table. if the item is not there, we assign it the next available number (from a
count of the number of distinct items seen so far) and enter the item and its
code into the table.

the triangular-matrix method

even after coding items as integers, we still have the problem that we must
count a pair {i, j} in only one place. for example, we could order the pair so
that i < j, and only use the entry a[i, j] in a two-dimensional array a. that
strategy would make half the array useless. a more space-e   cient way is to
use a one-dimensional triangular array. we store in a[k] the count for the pair
{i, j}, with 1     i < j     n, where

k = (i     1)(cid:16)n    

i

2(cid:17) + j     i

the result of this layout is that the pairs are stored in lexicographic order, that
is    rst {1, 2}, {1, 3}, . . . ,{1, n}, then {2, 3}, {2, 4}, . . . ,{2, n}, and so on, down
to {n     2, n     1}, {n     2, n}, and    nally {n     1, n}.

the triples method

there is another approach to storing counts that may be more appropriate,
depending on the fraction of the possible pairs of items that actually appear in
some basket. we can store counts as triples [i, j, c], meaning that the count of
pair {i, j}, with i < j, is c. a data structure, such as a hash table with i and
j as the search key, is used so we can tell if there is a triple for a given i and j
and, if so, to    nd it quickly. we call this approach the triples method of storing
counts.

unlike the triangular matrix, the triples method does not require us to store
anything if the count for a pair is 0. on the other hand, the triples method
requires us to store three integers, rather than one, for every pair that does
appear in some basket.
in addition, there is the space needed for the hash
table or other data structure used to support e   cient retrieval. the conclusion

is that the triangular matrix will be better if at least 1/3 of the (cid:0)n

pairs actually appear in some basket, while if signi   cantly fewer than 1/3 of the
possible pairs occur, we should consider using the triples method.

2(cid:1) possible

example 6.6 : suppose there are 100,000 items, and 10,000,000 baskets of 10

(approximately) integer counts.1 on the other hand, the total number of pairs

items each. then the triangular-matrix method requires (cid:0)100000
(cid:1) = 5    109
among all the baskets is 107(cid:0)10
2(cid:1) = 4.5    108. even in the extreme case that
every pair of items appeared only once, there could be only 4.5   108 pairs with
2(cid:1) = n2/2 for

1here, and throughout the chapter, we shall use the approximation that (cid:0)n

2

large n.

212

chapter 6. frequent itemsets

nonzero counts. if we used the triples method to store counts, we would need
only three times that number of integers, or 1.35    109 integers. thus, in this
case the triples method will surely take much less space than the triangular
matrix.

however, even if there were ten or a hundred times as many baskets, it
would be normal for there to be a su   ciently uneven distribution of items that
we might still be better o    using the triples method. that is, some pairs would
have very high counts, and the number of di   erent pairs that occurred in one
or more baskets would be much less than the theoretical maximum number of
such pairs.    

6.2.3 monotonicity of itemsets

much of the e   ectiveness of the algorithms we shall discuss is driven by a single
observation, called monotonicity for itemsets:

    if a set i of items is frequent, then so is every subset of i.

the reason is simple. let j     i. then every basket that contains all the items
in i surely contains all the items in j. thus, the count for j must be at least
as great as the count for i, and if the count for i is at least s, then the count
for j is at least s. since j may be contained in some baskets that are missing
one or more elements of i     j, it is entirely possible that the count for j is
strictly greater than the count for i.
in addition to making the a-priori algorithm work, monotonicity o   ers us
a way to compact the information about frequent itemsets.
if we are given
a support threshold s, then we say an itemset is maximal if no superset is
frequent.
if we list only the maximal itemsets, then we know that all subsets
of a maximal itemset are frequent, and no set that is not a subset of some
maximal itemset can be frequent.

example 6.7 : let us reconsider the data of example 6.1 with support thresh-
old s = 3. we found that there were    ve frequent singletons, those with words
   cat,       dog,       a,       and,    and    training.    each of these is contained in a
frequent doubleton, except for    training,    so one maximal frequent itemset is
{training}. there are also    ve frequent doubletons with s = 3, namely

{dog, a}
{cat, a}

{dog, and}
{cat, and}

{dog, cat}

we also found one frequent triple, {dog, cat, a}, and there are no larger frequent
itemsets. thus, this triple is maximal, but the three frequent doubletons it
contains are not maximal. the other frequent doubletons, {dog, and} and {cat,
and}, are maximal. notice that we can deduce from the frequent doubletons
that singletons like {dog} are frequent.    

6.2. market baskets and the a-priori algorithm

213

6.2.4 tyranny of counting pairs

as you may have noticed, we have focused on the matter of counting pairs in
the discussion so far. there is a good reason to do so: in practice the most main
memory is required for determining the frequent pairs. the number of items,
while possibly very large, is rarely so large we cannot count all the singleton
sets in main memory at the same time.

what about larger sets     triples, quadruples, and so on? recall that in order
for frequent-itemset analysis to make sense, the result has to be a small number
of sets, or we cannot even read them all, let alone consider their signi   cance.
thus, in practice the support threshold is set high enough that it is only a rare
set that is frequent. monotonicity tells us that if there is a frequent triple, then
there are three frequent pairs contained within it. and of course there may be
frequent pairs contained in no frequent triple as well. thus, we expect to    nd
more frequent pairs than frequent triples, more frequent triples than frequent
quadruples, and so on.

that argument would not be enough were it impossible to avoid counting
all the triples, since there are many more triples than pairs. it is the job of the
a-priori algorithm and related algorithms to avoid counting many triples or
larger sets, and they are, as we shall see, e   ective in doing so. thus, in what
follows, we concentrate on algorithms for computing frequent pairs.

6.2.5 the a-priori algorithm

for the moment, let us concentrate on    nding the frequent pairs only. if we have
enough main memory to count all pairs, using either of the methods discussed
in section 6.2.2 (triangular matrix or triples), then it is a simple matter to read
the    le of baskets in a single pass. for each basket, we use a double loop to
generate all the pairs. each time we generate a pair, we add 1 to its count.
at the end, we examine all pairs to see which have counts that are equal to or
greater than the support threshold s; these are the frequent pairs.

however, this simple approach fails if there are too many pairs of items to
count them all in main memory. the a-priori algorithm is designed to reduce
the number of pairs that must be counted, at the expense of performing two
passes over data, rather than one pass.

the first pass of a-priori

in the    rst pass, we create two tables. the    rst table, if necessary, translates
item names into integers from 1 to n, as described in section 6.2.2. the other
table is an array of counts; the ith array element counts the occurrences of the
item numbered i. initially, the counts for all the items are 0.

as we read baskets, we look at each item in the basket and translate its
name into an integer. next, we use that integer to index into the array of
counts, and we add 1 to the integer found there.

214

chapter 6. frequent itemsets

between the passes of a-priori

after the    rst pass, we examine the counts of the items to determine which of
them are frequent as singletons. it might appear surprising that many singletons
are not frequent. but remember that we set the threshold s su   ciently high
that we do not get too many frequent sets; a typical s would be 1% of the
baskets.
if we think about our own visits to a supermarket, we surely buy
certain things more than 1% of the time: perhaps milk, bread, coke or pepsi,
and so on. we can even believe that 1% of the customers buy diapers, even
though we may not do so. however, many of the items on the shelves are surely
not bought by 1% of the customers: creamy caesar salad dressing for example.
for the second pass of a-priori, we create a new numbering from 1 to m for
just the frequent items. this table is an array indexed 1 to n, and the entry
for i is either 0, if item i is not frequent, or a unique integer in the range 1 to
m if item i is frequent. we shall refer to this table as the frequent-items table.

the second pass of a-priori

during the second pass, we count all the pairs that consist of two frequent
items. recall from section 6.2.3 that a pair cannot be frequent unless both its
members are frequent. thus, we miss no frequent pairs. the space required on
the second pass is 2m2 bytes, rather than 2n2 bytes, if we use the triangular-
matrix method for counting. notice that the renumbering of just the frequent
items is necessary if we are to use a triangular matrix of the right size. the
complete set of main-memory structures used in the    rst and second passes is
shown in fig. 6.3.

also notice that the bene   t of eliminating infrequent items is ampli   ed; if
only half the items are frequent we need one quarter of the space to count.
likewise, if we use the triples method, we need to count only those pairs of two
frequent items that occur in at least one basket.

the mechanics of the second pass are as follows.

1. for each basket, look in the frequent-items table to see which of its items

are frequent.

2. in a double loop, generate all pairs of frequent items in that basket.

3. for each such pair, add one to its count in the data structure used to

store counts.

finally, at the end of the second pass, examine the structure of counts to
determine which pairs are frequent.

6.2.6 a-priori for all frequent itemsets

the same technique used for    nding frequent pairs without counting all pairs
lets us    nd larger frequent itemsets without an exhaustive count of all sets. in

6.2. market baskets and the a-priori algorithm

215

item
names

to

integers

1
2

n

item
counts

item
names

to

integers

1
2

n

fre   
quent
items

unused

data structure

for counts
of pairs

pass 1

pass 2

figure 6.3: schematic of main-memory use during the two passes of the a-priori
algorithm

the a-priori algorithm, one pass is taken for each set-size k. if no frequent
itemsets of a certain size are found, then monotonicity tells us there can be no
larger frequent itemsets, so we can stop.

the pattern of moving from one size k to the next size k + 1 can be sum-

marized as follows. for each size k, there are two sets of itemsets:

1. ck is the set of candidate itemsets of size k     the itemsets that we must

count in order to determine whether they are in fact frequent.

2. lk is the set of truly frequent itemsets of size k.

the pattern of moving from one set to the next and one size to the next is
suggested by fig. 6.4.

c

1

l

1

filter

construct

c

2

l

2

filter

construct

c

3

l

3

filter

construct

. . .

all
items

frequent
items

pairs of
frequent
items

frequent

pairs

figure 6.4: the a-priori algorithm alternates between constructing candidate
sets and    ltering to    nd those that are truly frequent

216

chapter 6. frequent itemsets

we start with c1, which is all singleton itemsets, i.e., the items themselves.
that is, before we examine the data, any item could be frequent as far as we
know. the    rst    lter step is to count all items, and those whose counts are at
least the support threshold s form the set l1 of frequent items.

the set c2 of candidate pairs is the set of pairs both of whose items are
in l1; that is, they are frequent items. note that we do not construct c2
explicitly. rather we use the de   nition of c2, and we test membership in c2 by
testing whether both of its members are in l1. the second pass of the a-priori
algorithm counts all the candidate pairs and determines which appear at least
s times. these pairs form l2, the frequent pairs.

we can follow this pattern as far as we wish. the set c3 of candidate
triples is constructed (implicitly) as the set of triples, any two of which is a
pair in l2. our assumption about the sparsity of frequent itemsets, outlined
in section 6.2.4 implies that there will not be too many frequent pairs, so they
can be listed in a main-memory table. likewise, there will not be too many
candidate triples, so these can all be counted by a generalization of the triples
method. that is, while triples are used to count pairs, we would use quadruples,
consisting of the three item codes and the associated count, when we want to
count triples. similarly, we can count sets of size k using tuples with k + 1
components, the last of which is the count, and the    rst k of which are the item
codes, in sorted order.

to    nd l3 we make a third pass through the basket    le. for each basket,
we need only look at those items that are in l1. from these items, we can
examine each pair and determine whether or not that pair is in l2. any item
of the basket that does not appear in at least two frequent pairs, both of which
consist of items in the basket, cannot be part of a frequent triple that the
basket contains. thus, we have a fairly limited search for triples that are both
contained in the basket and are candidates in c3. any such triples found have
1 added to their count.

example 6.8 : suppose our basket consists of items 1 through 10. of these, 1
through 5 have been found to be frequent items, and the following pairs have
been found frequent: {1, 2}, {2, 3}, {3, 4}, and {4, 5}. at    rst, we eliminate the
nonfrequent items, leaving only 1 through 5. however, 1 and 5 appear in only
one frequent pair in the itemset, and therefore cannot contribute to a frequent
triple contained in the basket. thus, we must consider adding to the count of
triples that are contained in {2, 3, 4}. there is only one such triple, of course.
however, we shall not    nd it in c3, because {2, 4} evidently is not frequent.    
the construction of the collections of larger frequent itemsets and candidates
proceeds in essentially the same manner, until at some pass we    nd no new
frequent itemsets and stop. that is:

1. de   ne ck to be all those itemsets of size k, every k     1 of which is an

itemset in lk   1.

6.2. market baskets and the a-priori algorithm

217

2. find lk by making a pass through the baskets and counting all and only
the itemsets of size k that are in ck. those itemsets that have count at
least s are in lk.

6.2.7 exercises for section 6.2

exercise 6.2.1 : if we use a triangular matrix to count pairs, and n, the num-
ber of items, is 20, what pair   s count is in a[100]?

! exercise 6.2.2 : in our description of the triangular-matrix method in sec-
tion 6.2.2, the formula for k involves dividing an arbitrary integer i by 2. yet
we need to have k always be an integer. prove that k will, in fact, be an integer.

! exercise 6.2.3 : let there be i items in a market-basket data set of b baskets.
suppose that every basket contains exactly k items. as a function of i, b,
and k:

(a) how much space does the triangular-matrix method take to store the

counts of all pairs of items, assuming four bytes per array element?

(b) what is the largest possible number of pairs with a nonzero count?

(c) under what circumstances can we be certain that the triples method will

use less space than the triangular array?

!! exercise 6.2.4 : how would you count all itemsets of size 3 by a generalization
of the triangular-matrix method? that is, arrange that in a one-dimensional
array there is exactly one element for each set of three items.

! exercise 6.2.5 : suppose the support threshold is 5. find the maximal fre-

quent itemsets for the data of:

(a) exercise 6.1.1.

(b) exercise 6.1.3.

exercise 6.2.6 : apply the a-priori algorithm with support threshold 5 to
the data of:

(a) exercise 6.1.1.

(b) exercise 6.1.3.

! exercise 6.2.7 : suppose we have market baskets that satisfy the following

assumptions:

1. the support threshold is 10,000.

2. there are one million items, represented by the integers 0, 1, . . . , 999999.

218

chapter 6. frequent itemsets

3. there are n frequent items, that is, items that occur 10,000 times or

more.

4. there are one million pairs that occur 10,000 times or more.

5. there are 2m pairs that occur exactly once. of these pairs, m consist of
two frequent items; the other m each have at least one nonfrequent item.

6. no other pairs occur at all.

7. integers are always represented by 4 bytes.

suppose we run the a-priori algorithm and can choose on the second pass
between the triangular-matrix method for counting candidate pairs and a hash
table of item-item-count triples. neglect in the    rst case the space needed to
translate between original item numbers and numbers for the frequent items,
and in the second case neglect the space needed for the hash table. as a function
of n and m , what is the minimum number of bytes of main memory needed to
execute the a-priori algorithm on this data?

6.3 handling larger datasets in main memory

the a-priori algorithm is    ne as long as the step with the greatest requirement
for main memory     typically the counting of the candidate pairs c2     has enough
memory that it can be accomplished without thrashing (repeated moving of
data between disk and main memory). several algorithms have been proposed
to cut down on the size of candidate set c2. here, we consider the pcy
algorithm, which takes advantage of the fact that in the    rst pass of a-priori
there is typically lots of main memory not needed for the counting of single
items. then we look at the multistage algorithm, which uses the pcy trick
and also inserts extra passes to further reduce the size of c2.

6.3.1 the algorithm of park, chen, and yu

this algorithm, which we call pcy after its authors, exploits the observation
that there may be much unused space in main memory on the    rst pass. if there
are a million items and gigabytes of main memory, we do not need more than
10% of the main memory for the two tables suggested in fig. 6.3     a translation
table from item names to small integers and an array to count those integers.
the pcy algorithm uses that space for an array of integers that generalizes
the idea of a bloom    lter (see section 4.3). the idea is shown schematically in
fig. 6.5.

think of this array as a hash table, whose buckets hold integers rather than
sets of keys (as in an ordinary hash table) or bits (as in a bloom    lter). pairs of
items are hashed to buckets of this hash table. as we examine a basket during
the    rst pass, we not only add 1 to the count for each item in the basket, but

6.3. handling larger datasets in main memory

219

item
names

to

integers

1
2

n

item
counts

item
names

to

integers

1
2

n

fre   
quent
items

bitmap

hash table
for bucket

counts

data structure

for counts
of pairs

pass 1

pass 2

figure 6.5: organization of main memory for the    rst two passes of the pcy
algorithm

we generate all the pairs, using a double loop. we hash each pair, and we add
1 to the bucket into which that pair hashes. note that the pair itself doesn   t
go into the bucket; the pair only a   ects the single integer in the bucket.

at the end of the    rst pass, each bucket has a count, which is the sum of
the counts of all the pairs that hash to that bucket. if the count of a bucket
is at least as great as the support threshold s, it is called a frequent bucket.
we can say nothing about the pairs that hash to a frequent bucket; they could
all be frequent pairs from the information available to us. but if the count of
the bucket is less than s (an infrequent bucket), we know no pair that hashes
to this bucket can be frequent, even if the pair consists of two frequent items.
that fact gives us an advantage on the second pass. we can de   ne the set of
candidate pairs c2 to be those pairs {i, j} such that:

1. i and j are frequent items.

2. {i, j} hashes to a frequent bucket.

it is the second condition that distinguishes pcy from a-priori.

example 6.9 : depending on the data and the amount of available main mem-
ory, there may or may not be a bene   t to using the hash table on pass 1. in
the worst case, all buckets are frequent, and the pcy algorithm counts exactly
the same pairs as a-priori does on the second pass. however, sometimes, we
can expect most of the buckets to be infrequent. in that case, pcy reduces the
memory requirements of the second pass.

220

chapter 6. frequent itemsets

suppose we have a gigabyte of main memory available for the hash table
on the    rst pass. suppose also that the data    le has a billion baskets, each
with ten items. a bucket is an integer, typically 4 bytes, so we can maintain a

quarter of a billion buckets. the number of pairs in all the baskets is 109   (cid:0)10
2(cid:1)
or 4.5    1010 pairs; this number is also the sum of the counts in the buckets.
thus, the average count is 4.5   1010/2.5   108, or 180. if the support threshold
s is around 180 or less, we might expect few buckets to be infrequent. however,
if s is much larger, say 1000, then it must be that the great majority of the
buckets are infrequent. the greatest possible number of frequent buckets is
4.5    1010/1000, or 45 million out of the 250 million buckets.    

between the passes of pcy, the hash table is summarized as a bitmap, with
one bit for each bucket. the bit is 1 if the bucket is frequent and 0 if not. thus
integers of 32 bits are replaced by single bits, and the bitmap shown in the
second pass in fig. 6.5 takes up only 1/32 of the space that would otherwise be
available to store counts. however, if most buckets are infrequent, we expect
that the number of pairs being counted on the second pass will be much smaller
than the total number of pairs of frequent items. thus, pcy can handle some
data sets without thrashing during the second pass, while a-priori would run
out of main memory and thrash.

there is another subtlety regarding the second pass of pcy that a   ects
the amount of space needed. while we were able to use the triangular-matrix
method on the second pass of a-priori if we wished, because the frequent items
could be renumbered from 1 to some m, we cannot do so for pcy. the reason
is that the pairs of frequent items that pcy lets us avoid counting are placed
randomly within the triangular matrix; they are the pairs that happen to hash
to an infrequent bucket on the    rst pass. there is no known way of compacting
the matrix to avoid leaving space for the uncounted pairs.

consequently, we are forced to use the triples method in pcy. that re-
striction may not matter if the fraction of pairs of frequent items that actually
appear in buckets were small; we would then want to use triples for a-priori
anyway. however, if most pairs of frequent items appear together in at least
one bucket, then we are forced in pcy to use triples, while a-priori can use a
triangular matrix. thus, unless pcy lets us avoid counting at least 2/3 of the
pairs of frequent items, we cannot gain by using pcy instead of a-priori.

while the discovery of frequent pairs by pcy di   ers signi   cantly from a-
priori, the later stages, where we    nd frequent triples and larger sets if desired,
are essentially the same as a-priori. this statement holds as well for each of
the improvements to a-priori that we cover in this section. as a result, we
shall cover only the construction of the frequent pairs from here on.

6.3.2 the multistage algorithm

the multistage algorithm improves upon pcy by using several successive hash
tables to reduce further the number of candidate pairs. the tradeo    is that

6.3. handling larger datasets in main memory

221

multistage takes more than two passes to    nd the frequent pairs. an outline of
the multistage algorithm is shown in fig. 6.6.

item
names

to

integers

1
2

n

item
counts

item
names

to

integers

1
2

n

fre   
quent
items

bitmap 1

hash table
for bucket

counts

second
hash table
for bucket

counts

item
names

to

integers

1
2

n

fre   
quent
items

bitmap 1

bitmap 2

data structure

for counts
of pairs

pass 1

pass 2

pass 3

figure 6.6: the multistage algorithm uses additional hash tables to reduce the
number of candidate pairs

the    rst pass of multistage is the same as the    rst pass of pcy. after that
pass, the frequent buckets are identi   ed and summarized by a bitmap, again
the same as in pcy. but the second pass of multistage does not count the
candidate pairs. rather, it uses the available main memory for another hash
table, using another hash function. since the bitmap from the    rst hash table
takes up 1/32 of the available main memory, the second hash table has almost
as many buckets as the    rst.

on the second pass of multistage, we again go through the    le of baskets.
there is no need to count the items again, since we have those counts from
the    rst pass. however, we must retain the information about which items are
frequent, since we need it on both the second and third passes. during the
second pass, we hash certain pairs of items to buckets of the second hash table.
a pair is hashed only if it meets the two criteria for being counted in the second
pass of pcy; that is, we hash {i, j} if and only if i and j are both frequent,
and the pair hashed to a frequent bucket on the    rst pass. as a result, the sum
of the counts in the second hash table should be signi   cantly less than the sum
for the    rst pass. the result is that, even though the second hash table has
only 31/32 of the number of buckets that the    rst table has, we expect there
to be many fewer frequent buckets in the second hash table than in the    rst.

after the second pass, the second hash table is also summarized as a bitmap,
and that bitmap is stored in main memory. the two bitmaps together take up

222

chapter 6. frequent itemsets

a subtle error in multistage

occasionally, an implementation tries to eliminate the second requirement
for {i, j} to be a candidate     that it hashes to a frequent bucket on the    rst
pass. the (false) reasoning is that if it didn   t hash to a frequent bucket
on the    rst pass, it wouldn   t have been hashed at all on the second pass,
and thus would not contribute to the count of its bucket on the second
pass. while it is true that the pair is not counted on the second pass, that
doesn   t mean it wouldn   t have hashed to a frequent bucket had it been
hashed. thus, it is entirely possible that {i, j} consists of two frequent
items and hashes to a frequent bucket on the second pass, yet it did not
hash to a frequent bucket on the    rst pass. therefore, all three conditions
must be checked on the counting pass of multistage.

slightly less than 1/16th of the available main memory, so there is still plenty
of space to count the candidate pairs on the third pass. a pair {i, j} is in c2 if
and only if:

1. i and j are both frequent items.
2. {i, j} hashed to a frequent bucket in the    rst hash table.
3. {i, j} hashed to a frequent bucket in the second hash table.

the third condition is the distinction between multistage and pcy.

it might be obvious that it is possible to insert any number of passes between
the    rst and last in the multistage algorithm. there is a limiting factor that
each pass must store the bitmaps from each of the previous passes. eventually,
there is not enough space left in main memory to do the counts. no matter
how many passes we use, the truly frequent pairs will always hash to a frequent
bucket, so there is no way to avoid counting them.

6.3.3 the multihash algorithm

sometimes, we can get most of the bene   t of the extra passes of the multistage
algorithm in a single pass. this variation of pcy is called the multihash
algorithm. instead of using two di   erent hash tables on two successive passes,
use two hash functions and two separate hash tables that share main memory
on the    rst pass, as suggested by fig. 6.7.

the danger of using two hash tables on one pass is that each hash table has
half as many buckets as the one large hash table of pcy. as long as the average
count of a bucket for pcy is much lower than the support threshold, we can
operate two half-sized hash tables and still expect most of the buckets of both
hash tables to be infrequent. thus, in this situation we might well choose the
multihash approach.

6.3. handling larger datasets in main memory

223

item
names

to

integers

1
2

n

item
counts

hash table 1

hash table 2

item
names

to

integers

1
2

n

fre   
quent
items

bitmap 1 bitmap 2

data structure

for counts
of pairs

pass 1

pass 2

figure 6.7: the multihash algorithm uses several hash tables in one pass

example 6.10 : suppose that if we run pcy, the average bucket will have a
count s/10, where s is the support threshold. then if we used the multihash
approach with two half-sized hash tables, the average count would be s/5. as
a result, at most 1/5th of the buckets in either hash table could be frequent,
and a random infrequent pair has at most id203 (1/5)2 = 0.04 of being in
a frequent bucket in both hash tables.

by the same reasoning, the upper bound on the infrequent pair being in a
frequent bucket in the one pcy hash table is at most 1/10. that is, we might
expect to have to count 2.5 times as many infrequent pairs in pcy as in the
version of multihash suggested above. we would therefore expect multihash to
have a smaller memory requirement for the second pass than would pcy.

but these upper bounds do not tell the complete story. there may be
many fewer frequent buckets than the maximum for either algorithm, since the
presence of some very frequent pairs will skew the distribution of bucket counts.
however, this analysis is suggestive of the possibility that for some data and
support thresholds, we can do better by running several hash functions in main
memory at once.    

for the second pass of multihash, each hash table is converted to a bitmap,
as usual. note that the two bitmaps for the two hash functions in fig. 6.7
occupy exactly as much space as a single bitmap would for the second pass of
the pcy algorithm. the conditions for a pair {i, j} to be in c2, and thus
to require a count on the second pass, are the same as for the third pass of
multistage: i and j must both be frequent, and the pair must have hashed to
a frequent bucket according to both hash tables.

224

chapter 6. frequent itemsets

just as multistage is not limited to two hash tables, we can divide the
available main memory into as many hash tables as we like on the    rst pass of
multihash. the risk is that should we use too many hash tables, the average
count for a bucket will exceed the support threshold. at that point, there may
be very few infrequent buckets in any of the hash tables. even though a pair
must hash to a frequent bucket in every hash table to be counted, we may    nd
that the id203 an infrequent pair will be a candidate rises, rather than
falls, if we add another hash table.

6.3.4 exercises for section 6.3

exercise 6.3.1 : here is a collection of twelve baskets. each contains three of
the six items 1 through 6.

{1, 2, 3}
{1, 3, 5}
{3, 5, 6}

{2, 3, 4}
{2, 4, 6}
{1, 2, 4}

{3, 4, 5}
{1, 3, 4}
{2, 3, 5}

{4, 5, 6}
{2, 4, 5}
{3, 4, 6}

suppose the support threshold is 4. on the    rst pass of the pcy algorithm
we use a hash table with 11 buckets, and the set {i, j} is hashed to bucket i   j
mod 11.

(a) by any method, compute the support for each item and each pair of items.

(b) which pairs hash to which buckets?

(c) which buckets are frequent?

(d) which pairs are counted on the second pass of the pcy algorithm?

exercise 6.3.2 : suppose we run the multistage algorithm on the data of
exercise 6.3.1, with the same support threshold of 4. the    rst pass is the same
as in that exercise, and for the second pass, we hash pairs to nine buckets,
using the hash function that hashes {i, j} to bucket i + j mod 9. determine
the counts of the buckets on the second pass. does the second pass reduce the
set of candidate pairs? note that all items are frequent, so the only reason a
pair would not be hashed on the second pass is if it hashed to an infrequent
bucket on the    rst pass.

exercise 6.3.3 : suppose we run the multihash algorithm on the data of
exercise 6.3.1. we shall use two hash tables with    ve buckets each. for one,
the set {i, j}, is hashed to bucket 2i+3j +4 mod 5, and for the other, the set is
hashed to i + 4j mod 5. since these hash functions are not symmetric in i and
j, order the items so that i < j when evaluating each hash function. determine
the counts of each of the 10 buckets. how large does the support threshold
have to be for the multistage algorithm to eliminate more pairs than the pcy
algorithm would, using the hash table and function described in exercise 6.3.1?

6.3. handling larger datasets in main memory

225

! exercise 6.3.4 : suppose we perform the pcy algorithm to    nd frequent

pairs, with market-basket data meeting the following speci   cations:

1. the support threshold is 10,000.

2. there are one million items, represented by the integers 0, 1, . . . , 999999.

3. there are 250,000 frequent items, that is, items that occur 10,000 times

or more.

4. there are one million pairs that occur 10,000 times or more.

5. there are p pairs that occur exactly once and consist of two frequent

items.

6. no other pairs occur at all.

7. integers are always represented by 4 bytes.

8. when we hash pairs, they distribute among buckets randomly, but as
evenly as possible; i.e., you may assume that each bucket gets exactly its
fair share of the p pairs that occur once.

suppose there are s bytes of main memory. in order to run the pcy algorithm
successfully, the number of buckets must be su   ciently large that most buckets
are not frequent. in addition, on the second pass, there must be enough room
to count all the candidate pairs. as a function of s, what is the largest value
of p for which we can successfully run the pcy algorithm on this data?

! exercise 6.3.5 : under the assumptions given in exercise 6.3.4, will the mul-
tihash algorithm reduce the main-memory requirements for the second pass?
as a function of s and p , what is the optimum number of hash tables to use
on the    rst pass?

! exercise 6.3.6 : suppose we perform the 3-pass multistage algorithm to    nd

frequent pairs, with market-basket data meeting the following speci   cations:

1. the support threshold is 10,000.

2. there are one million items, represented by the integers 0, 1, . . . , 999999.

all items are frequent; that is, they occur at least 10,000 times.

3. there are one million pairs that occur 10,000 times or more.

4. there are p pairs that occur exactly once.

5. no other pairs occur at all.

6. integers are always represented by 4 bytes.

226

chapter 6. frequent itemsets

7. when we hash pairs, they distribute among buckets randomly, but as
evenly as possible; i.e., you may assume that each bucket gets exactly its
fair share of the p pairs that occur once.

8. the hash functions used on the    rst two passes are completely indepen-

dent.

suppose there are s bytes of main memory. as a function of s and p , what
is the expected number of candidate pairs on the third pass of the multistage
algorithm?

6.4 limited-pass algorithms

the algorithms for frequent itemsets discussed so far use one pass for each size
of itemset we investigate. if main memory is too small to hold the data and the
space needed to count frequent itemsets of one size, there does not seem to be
any way to avoid k passes to compute the exact collection of frequent itemsets.
however, there are many applications where it is not essential to discover every
frequent itemset. for instance, if we are looking for items purchased together at
a supermarket, we are not going to run a sale based on every frequent itemset
we    nd, so it is quite su   cient to    nd most but not all of the frequent itemsets.
in this section we explore some algorithms that have been proposed to    nd
all or most frequent itemsets using at most two passes. we begin with the
obvious approach of using a sample of the data rather than the entire dataset.
an algorithm called son uses two passes, gets the exact answer, and lends
itself to implementation by mapreduce or another parallel computing regime.
finally, toivonen   s algorithm uses two passes on average, gets an exact answer,
but may, rarely, not terminate in any given amount of time.

6.4.1 the simple, randomized algorithm

instead of using the entire    le of baskets, we could pick a random subset of
the baskets and pretend it is the entire dataset. we must adjust the support
threshold to re   ect the smaller number of baskets. for instance, if the support
threshold for the full dataset is s, and we choose a sample of 1% of the baskets,
then we should examine the sample for itemsets that appear in at least s/100
of the baskets.

the safest way to pick the sample is to read the entire dataset, and for each
basket, select that basket for the sample with some    xed id203 p. suppose
there are m baskets in the entire    le. at the end, we shall have a sample whose
size is very close to pm baskets. however, if we have reason to believe that the
baskets appear in random order in the    le already, then we do not even have
to read the entire    le. we can select the    rst pm baskets for our sample. or, if
the    le is part of a distributed    le system, we can pick some chunks at random
to serve as the sample.

6.4. limited-pass algorithms

227

why not just pick the first part of the file?

the risk in selecting a sample from one portion of a large    le is that the
data is not uniformly distributed in the    le. for example, suppose the    le
were a list of true market-basket contents at a department store, organized
by date of sale. if you took only the    rst baskets in the    le, you would
have old data. for example, there would be no ipods in these baskets,
even though ipods might have become a popular item later.

as another example, consider a    le of medical tests performed at
di   erent hospitals.
if each chunk comes from a di   erent hospital, then
picking chunks at random will give us a sample drawn from only a small
subset of the hospitals. if hospitals perform di   erent tests or perform them
in di   erent ways, the data may be highly biased.

having selected our sample of the baskets, we use part of main memory
to store these baskets. the balance of the main memory is used to execute
one of the algorithms we have discussed, such as a-priori, pcy, multistage,
or multihash. however, the algorithm must run passes over the main-memory
sample for each itemset size, until we    nd a size with no frequent items. there
are no disk accesses needed to read the sample, since it resides in main memory.
as frequent itemsets of each size are discovered, they can be written out to disk;
this operation and the initial reading of the sample from disk are the only disk
i/o   s the algorithm does.

of course the algorithm will fail if whichever method from section 6.2 or 6.3
we choose cannot be run in the amount of main memory left after storing the
sample. if we need more main memory, then an option is to read the sample
from disk for each pass. since the sample is much smaller than the full dataset,
we still avoid most of the disk i/o   s that the algorithms discussed previously
would use.

6.4.2 avoiding errors in sampling algorithms

we should be mindful of the problem with the simple algorithm of section 6.4.1:
it cannot be relied upon either to produce all the itemsets that are frequent in
the whole dataset, nor will it produce only itemsets that are frequent in the
whole. an itemset that is frequent in the whole but not in the sample is a false
negative, while an itemset that is frequent in the sample but not the whole is a
false positive.

if the sample is large enough, there are unlikely to be serious errors. that is,
an itemset whose support is much larger than the threshold will almost surely
be identi   ed from a random sample, and an itemset whose support is much less
than the threshold is unlikely to appear frequent in the sample. however, an
itemset whose support in the whole is very close to the threshold is as likely to

228

chapter 6. frequent itemsets

be frequent in the sample as not.

we can eliminate false positives by making a pass through the full dataset
and counting all the itemsets that were identi   ed as frequent in the sample.
retain as frequent only those itemsets that were frequent in the sample and
also frequent in the whole. note that this improvement will eliminate all false
positives, but a false negative is not counted and therefore remains undiscovered.
to accomplish this task in a single pass, we need to be able to count all
frequent itemsets of all sizes at once, within main memory. if we were able to
run the simple algorithm successfully with the main memory available, then
there is a good chance we shall be able to count all the frequent itemsets at
once, because:

(a) the frequent singletons and pairs are likely to dominate the collection of
all frequent itemsets, and we had to count them all in one pass already.

(b) we now have all of main memory available, since we do not have to store

the sample in main memory.

we cannot eliminate false negatives completely, but we can reduce their
number if the amount of main memory allows it. we have assumed that if s is
the support threshold, and the sample is fraction p of the entire dataset, then we
use ps as the support threshold for the sample. however, we can use something
smaller than that as the threshold for the sample, such a 0.9ps. having a lower
threshold means that more itemsets of each size will have to be counted, so
the main-memory requirement rises. on the other hand, if there is enough
main memory, then we shall identify as having support at least 0.9ps in the
sample almost all those itemsets that have support at least s is the whole. if
we then make a complete pass to eliminate those itemsets that were identi   ed
as frequent in the sample but are not frequent in the whole, we have no false
positives and hopefully have none or very few false negatives.

6.4.3 the algorithm of savasere, omiecinski, and

navathe

our next improvement avoids both false negatives and false positives, at the
cost of making two full passes. it is called the son algorithm after the authors.
the idea is to divide the input    le into chunks (which may be    chunks    in the
sense of a distributed    le system, or simply a piece of the    le). treat each
chunk as a sample, and run the algorithm of section 6.4.1 on that chunk. we
use ps as the threshold, if each chunk is fraction p of the whole    le, and s is
the support threshold. store on disk all the frequent itemsets found for each
chunk.

once all the chunks have been processed in that way, take the union of
all the itemsets that have been found frequent for one or more chunks. these
are the candidate itemsets. notice that if an itemset is not frequent in any
chunk, then its support is less than ps in each chunk. since the number of

6.4. limited-pass algorithms

229

chunks is 1/p, we conclude that the total support for that itemset is less than
(1/p)ps = s. thus, every itemset that is frequent in the whole is frequent in
at least one chunk, and we can be sure that all the truly frequent itemsets are
among the candidates; i.e., there are no false negatives.

we have made a total of one pass through the data as we read each chunk
in a second pass, we count all the candidate itemsets and

and processed it.
select those that have support at least s as the frequent itemsets.

6.4.4 the son algorithm and mapreduce

the son algorithm lends itself well to a parallel-computing environment. each
of the chunks can be processed in parallel, and the frequent itemsets from each
chunk combined to form the candidates. we can distribute the candidates to
many processors, have each processor count the support for each candidate
in a subset of the baskets, and    nally sum those supports to get the support
for each candidate itemset in the whole dataset. this process does not have
to be implemented in mapreduce, but there is a natural way of expressing
each of the two passes as a mapreduce operation. we shall summarize this
mapreduce-mapreduce sequence below.

first map function: take the assigned subset of the baskets and    nd the
itemsets frequent in the subset using the algorithm of section 6.4.1. as de-
scribed there, lower the support threshold from s to ps if each map task gets
fraction p of the total input    le. the output is a set of key-value pairs (f, 1),
where f is a frequent itemset from the sample. the value is always 1 and is
irrelevant.

first reduce function: each reduce task is assigned a set of keys, which
are itemsets. the value is ignored, and the reduce task simply produces those
keys (itemsets) that appear one or more times. thus, the output of the    rst
reduce function is the candidate itemsets.

second map function: the map tasks for the second map function take
all the output from the    rst reduce function (the candidate itemsets) and a
portion of the input data    le. each map task counts the number of occurrences
of each of the candidate itemsets among the baskets in the portion of the dataset
that it was assigned. the output is a set of key-value pairs (c, v), where c is one
of the candidate sets and v is the support for that itemset among the baskets
that were input to this map task.

second reduce function: the reduce tasks take the itemsets they are
given as keys and sum the associated values. the result is the total support
for each of the itemsets that the reduce task was assigned to handle. those
itemsets whose sum of values is at least s are frequent in the whole dataset, so

230

chapter 6. frequent itemsets

the reduce task outputs these itemsets with their counts. itemsets that do not
have total support at least s are not transmitted to the output of the reduce
task.2

6.4.5 toivonen   s algorithm

this algorithm uses randomness in a di   erent way from the simple sampling
algorithm of section 6.4.1. toivonen   s algorithm, given su   cient main mem-
ory, will use one pass over a small sample and one full pass over the data. it
will give neither false negatives nor positives, but there is a small yet nonzero
id203 that it will fail to produce any answer at all. in that case it needs
to be repeated until it gives an answer. however, the average number of passes
needed before it produces all and only the frequent itemsets is a small constant.
toivonen   s algorithm begins by selecting a small sample of the input dataset,
and    nding from it the candidate frequent itemsets. the process is exactly that
of section 6.4.1, except that it is essential the threshold be set to something
less than its proportional value. that is, if the support threshold for the whole
dataset is s, and the sample size is fraction p, then when looking for frequent
itemsets in the sample, use a threshold such as 0.9ps or 0.8ps. the smaller we
make the threshold, the more main memory we need for computing all itemsets
that are frequent in the sample, but the more likely we are to avoid the situation
where the algorithm fails to provide an answer.

having constructed the collection of frequent itemsets for the sample, we
next construct the negative border. this is the collection of itemsets that are not
frequent in the sample, but all of their immediate subsets (subsets constructed
by deleting exactly one item) are frequent in the sample.

example 6.11 : suppose the items are {a, b, c, d, e} and we have found the
following itemsets to be frequent in the sample: {a}, {b}, {c}, {d}, {b, c},
{c, d}. note that     is also frequent, as long as there are at least as many
baskets as the support threshold, although technically the algorithms we have
described omit this obvious fact. first, {e} is in the negative border, because
it is not frequent in the sample, but its only immediate subset,    , is frequent.
the sets {a, b}, {a, c}, {a, d} and {b, d} are in the negative border.
none of these sets are frequent, and each has two immediate subsets, both of
which are frequent. for instance, {a, b} has immediate subsets {a} and {b}.
of the other six doubletons, none are in the negative border. the sets {b, c}
and {c, d} are not in the negative border, because they are frequent. the
remaining four pairs are each e together with another item, and those are not
in the negative border because they have an immediate subset {e} that is not
frequent.
none of the triples or larger sets are in the negative border. for instance,
{b, c, d} is not in the negative border because it has an immediate subset
2strictly speaking, the reduce function has to produce a value for each key. it can produce

1 as the value for itemsets found frequent and 0 for those not frequent.

6.4. limited-pass algorithms

231

{b, d} that is not frequent. thus, the negative border consists of    ve sets:
{e}, {a, b}, {a, c}, {a, d} and {b, d}.    

to complete toivonen   s algorithm, we make a pass through the entire data-
set, counting all the itemsets that are frequent in the sample or are in the
negative border. there are two possible outcomes.

1. no member of the negative border is frequent in the whole dataset. in
this case, the correct set of frequent itemsets is exactly those itemsets
from the sample that were found to be frequent in the whole.

2. some member of the negative border is frequent in the whole. then we
cannot be sure that there are not some even larger sets, in neither the
negative border nor the collection of frequent itemsets for the sample,
that are also frequent in the whole. thus, we can give no answer at this
time and must repeat the algorithm with a new random sample.

6.4.6 why toivonen   s algorithm works

clearly toivonen   s algorithm never produces a false positive, since it only re-
ports as frequent those itemsets that have been counted and found to be frequent
in the whole. to argue that it never produces a false negative, we must show
that when no member of the negative border is frequent in the whole, then
there can be no itemset whatsoever that is:

1. frequent in the whole, but

2. in neither the negative border nor the collection of frequent itemsets for

the sample.

suppose the contrary. that is, there is a set s that is frequent in the
whole, but not in the negative border and not frequent in the sample. also,
this round of toivonen   s algorithm produced an answer, which would certainly
not include s among the frequent itemsets. by monotonicity, all subsets of s
are also frequent in the whole. let t be a subset of s that is of the smallest
possible size among all subsets of s that are not frequent in the sample.

we claim that t must be in the negative border. surely t meets one of the
conditions for being in the negative border:
it is not frequent in the sample.
it also meets the other condition for being in the negative border: each of its
immediate subsets is frequent in the sample. for if some immediate subset of
t were not frequent in the sample, then there would be a subset of s that is
smaller than t and not frequent in the sample, contradicting our selection of t
as a subset of s that was not frequent in the sample, yet as small as any such
set.

now we see that t is both in the negative border and frequent in the whole
dataset. consequently, this round of toivonen   s algorithm did not produce an
answer.

232

chapter 6. frequent itemsets

6.4.7 exercises for section 6.4

exercise 6.4.1 : suppose there are eight items, a, b, . . . , h, and the following
are the maximal frequent itemsets: {a, b}, {b, c}, {a, c}, {a, d}, {e}, and
{f}. find the negative border.
exercise 6.4.2 : apply toivonen   s algorithm to the data of exercise 6.3.1,
with a support threshold of 4. take as the sample the    rst row of baskets:
{1, 2, 3}, {2, 3, 4}, {3, 4, 5}, and {4, 5, 6}, i.e., one-third of the    le. our scaled-
down support theshold will be 1.

(a) what are the itemsets frequent in the sample?

(b) what is the negative border?

(c) what is the outcome of the pass through the full dataset? are any of the

itemsets in the negative border frequent in the whole?

!! exercise 6.4.3 : suppose item i appears exactly s times in a    le of n baskets,
where s is the support threshold. if we take a sample of n/100 baskets, and
lower the support threshold for the sample to s/100, what is the id203
that i will be found to be frequent? you may assume that both s and n are
divisible by 100.

6.5 counting frequent items in a stream

suppose that instead of a    le of baskets we have a stream of baskets, from
which we want to mine the frequent itemsets. recall from chapter 4 that the
di   erence between a stream and a data    le is that stream elements are only
available when they arrive, and typically the arrival rate is so great that we
cannot store the entire stream in a way that allows easy querying. further, it
is common that streams evolve over time, so the itemsets that are frequent in
today   s stream may not be frequent tomorrow.

a clear distinction between streams and    les, when frequent itemsets are
considered, is that there is no end to a stream, so eventually an itemset is going
to exceed the support threshold, as long as it appears repeatedly in the stream.
as a result, for streams, we must think of the support threshold s as a fraction of
the baskets in which an itemset must appear in order to be considered frequent.
even with this adjustment, we still have several options regarding the portion
of the stream over which that fraction is measured.

in this section, we shall discuss several ways that we might extract frequent
itemsets from a stream. first, we consider ways to use the sampling tech-
niques of the previous section. then, we consider the decaying-window model
from section 4.7, and extend the method described in section 4.7.3 for    nding
   popular    items.

6.5. counting frequent items in a stream

233

6.5.1 sampling methods for streams

in what follows, we shall assume that stream elements are baskets of items.
perhaps the simplest approach to maintaining a current estimate of the frequent
itemsets in a stream is to collect some number of baskets and store it as a    le.
run one of the frequent-itemset algorithms discussed in this chapter, meanwhile
ignoring the stream elements that arrive, or storing them as another    le to be
analyzed later. when the frequent-itemsets algorithm    nishes, we have an
estimate of the frequent itemsets in the stream. we then have several options.

1. we can use this collection of frequent itemsets for whatever application is
at hand, but start running another iteration of the chosen frequent-itemset
algorithm immediately. this algorithm can either:

(a) use the    le that was collected while the    rst iteration of the algo-
rithm was running. at the same time, collect yet another    le to be
used at another iteration of the algorithm, when this current itera-
tion    nishes.

(b) start collecting another    le of baskets now, and run the algorithm

when an adequate number of baskets has been collected.

2. we can continue to count the numbers of occurrences of each of these
frequent itemsets, along with the total number of baskets seen in the
stream, since the counting started. if any itemset is discovered to occur
in a fraction of the baskets that is signi   cantly below the threshold fraction
s, then this set can be dropped from the collection of frequent itemsets.
when computing the fraction, it is important to include the occurrences
from the original    le of baskets, from which the frequent itemsets were
derived. if not, we run the risk that we shall encounter a short period in
which a truly frequent itemset does not appear su   ciently frequently and
throw it out. we should also allow some way for new frequent itemsets
to be added to the current collection. possibilities include:

(a) periodically gather a new segment of the baskets in the stream and
use it as the data    le for another iteration of the chosen frequent-
itemsets algorithm. the new collection of frequent items is formed
from the result of this iteration and the frequent itemsets from the
previous collection that have survived the possibility of having been
deleted for becoming infrequent.

(b) add some random itemsets to the current collection, and count their
fraction of occurrences for a while, until one has a good idea of
whether or not they are currently frequent. rather than choosing
new itemsets completely at random, one might focus on sets with
items that appear in many itemsets already known to be frequent.
for example, a good choice is to pick new itemsets from the negative
border (section 6.4.5) of the current set of frequent itemsets.

234

chapter 6. frequent itemsets

6.5.2 frequent itemsets in decaying windows

recall from section 4.7 that a decaying window on a stream is formed by picking
a small constant c and giving the ith element prior to the most recent element
the weight (1     c)i, or approximately e   ci. section 4.7.3 actually presented a
method for computing the frequent items, provided the support threshold is
de   ned in a somewhat di   erent way. that is, we considered, for each item, a
stream that had 1 if the item appeared at a certain stream element and 0 if
not. we de   ned the    score    for that item to be the sum of the weights where
its stream element was 1. we were constrained to record all items whose score
was at least 1/2. we can not use a score threshold above 1, because we do not
initiate a count for an item until the item appears in the stream, and the    rst
time it appears, its score is only 1 (since 1, or (1     c)0, is the weight of the
current item).
if we wish to adapt this method to streams of baskets, there are two modi   -
cations we must make. the    rst is simple. stream elements are baskets rather
than individual items, so many items may appear at a given stream element.
treat each of those items as if they were the    current    item and add 1 to their
score after multiplying all current scores by 1     c, as described in section 4.7.3.
if some items in a basket have no current score, initialize the scores of those
items to 1.

the second modi   cation is trickier. we want to    nd all frequent itemsets,
not just singleton itemsets.
if we were to initialize a count for an itemset
whenever we saw it, we would have too many counts. for example, one basket
of 20 items has over a million subsets, and all of these would have to be initiated
for one basket. on the other hand, as we mentioned, if we use a requirement
above 1 for initiating the scoring of an itemset, then we would never get any
itemsets started, and the method would not work.

a way of dealing with this problem is to start scoring certain itemsets as
soon as we see one instance, but be conservative about which itemsets we start.
we may borrow from the a-priori trick, and only start an itemset i if all its
immediate proper subsets are already being scored. the consequence of this
restriction is that if i is truly frequent, eventually we shall begin to count it,
but we never start an itemset unless it would at least be a candidate in the
sense used in the a-priori algorithm.

example 6.12 : suppose i is a large itemset, but it appears in the stream
periodically, once every 1/2c baskets. then its score, and that of its subsets,
never falls below e   1/2, which is greater than 1/2. thus, once a score is created
for some subset of i, that subset will continue to be scored forever. the    rst
time i appears, only its singleton subsets will have scores created for them.
however, the next time i appears, each of its doubleton subsets will commence
scoring, since each of the immediate subsets of those doubletons is already being
scored. likewise, the kth time i appears, its subsets of size k     1 are all being
scored, so we initiate scores for each of its subsets of size k. eventually, we
reach the size |i|, at which time we start scoring i itself.    

6.5. counting frequent items in a stream

235

6.5.3 hybrid methods

the approach of section 6.5.2 o   ers certain advantages. it requires a limited
amount of work each time a stream element arrives, and it always provides
an up-to-date picture of what is frequent in the decaying window.
its big
disadvantage is that it requires us to maintain scores for each itemset with
a score of at least 1/2. we can limit the number of itemsets being scored
by increasing the value of the parameter c. but the larger c is, the smaller the
decaying window is. thus, we could be forced to accept information that tracks
the local    uctuations in frequency too closely, rather than integrating over a
long period.

we can combine the ideas from sections 6.5.1 and 6.5.2. for example, we
could run a standard algorithm for frequent itemsets on a sample of the stream,
with a conventional threshold for support. the itemsets that are found frequent
by this algorithm will be treated as if they all arrived at the current time. that
is, they each get a score equal to a    xed fraction of their count.

more precisely, suppose the initial sample has b baskets, c is the decay
constant for the decaying window, and the minimum score we wish to accept
for a frequent itemset in the decaying window is s. then the support threshold
for the initial run of the frequent-itemset algorithm is bcs. if an itemset i is
found to have support t in the sample, then it is initially given a score of t/(bc).

example 6.13 : suppose c = 10   6 and the minimum score we wish to accept
in the decaying window is 10. suppose also we take a sample of 108 baskets
from the stream. then when analyzing that sample, we use a support threshold
of 108    10   6    10 = 1000.
consider an itemset i that has support 2000 in the sample. then the initial
score we use for i is 2000/(108    10   6) = 20. after this initiation step, each
time a basket arrives in the stream, the current score will be multiplied by
1    c = 0.999999. if i is a subset of the current basket, then add 1 to the score.
if the score for i goes below 10, then it is considered to be no longer frequent,
so it is dropped from the collection of frequent itemsets.    

we do not, sadly, have a reasonable way of initiating the scoring of new
itemsets. if we have no score for itemset i, and 10 is the minimum score we
want to maintain, there is no way that a single basket can jump its score from
0 to anything more than 1. the best strategy for adding new sets is to run a
new frequent-itemsets calculation on a sample from the stream, and add to the
collection of itemsets being scored any that meet the threshold for that sample
but were not previously being scored.

6.5.4 exercises for section 6.5

!! exercise 6.5.1 : suppose we are counting frequent itemsets in a decaying win-
dow with a decay constant c. suppose also that with id203 p, a given

236

chapter 6. frequent itemsets

stream element (basket) contains both items i and j. additionally, with prob-
ability p the basket contains i but not j, and with id203 p it contains j
but not i. as a function of c and p, what is the fraction of time we shall be
scoring the pair {i, j}?

6.6 summary of chapter 6

    market-basket data: this model of data assumes there are two kinds of
entities: items and baskets. there is a many   many relationship between
items and baskets. typically, baskets are related to small sets of items,
while items may be related to many baskets.

    frequent itemsets: the support for a set of items is the number of baskets
containing all those items. itemsets with support that is at least some
threshold are called frequent itemsets.

    association rules: these are implications that if a basket contains a
certain set of items i, then it is likely to contain another particular item
j as well. the id203 that j is also in a basket containing i is called
the con   dence of the rule. the interest of the rule is the amount by which
the con   dence deviates from the fraction of all baskets that contain j.

    the pair-counting bottleneck : to    nd frequent itemsets, we need to
examine all baskets and count the number of occurrences of sets of a
certain size. for typical data, with a goal of producing a small number of
itemsets that are the most frequent of all, the part that often takes the
most main memory is the counting of pairs of items. thus, methods for
   nding frequent itemsets typically concentrate on how to minimize the
main memory needed to count pairs.

    triangular matrices: while one could use a two-dimensional array to
count pairs, doing so wastes half the space, because there is no need to
count pair {i, j} in both the i-j and j-i array elements. by arranging the
pairs (i, j) for which i < j in lexicographic order, we can store only the
needed counts in a one-dimensional array with no wasted space, and yet
be able to access the count for any pair e   ciently.

    storage of pair counts as triples: if fewer than 1/3 of the possible pairs
actually occur in baskets, then it is more space-e   cient to store counts of
pairs as triples (i, j, c), where c is the count of the pair {i, j}, and i < j.
an index structure such as a hash table allows us to    nd the triple for
(i, j) e   ciently.

    monotonicity of frequent itemsets: an important property of itemsets is
that if a set of items is frequent, then so are all its subsets. we exploit
this property to eliminate the need to count certain itemsets by using its
contrapositive: if an itemset is not frequent, then neither are its supersets.

6.6. summary of chapter 6

237

    the a-priori algorithm for pairs: we can    nd all frequent pairs by
making two passes over the baskets. on the    rst pass, we count the items
themselves, and then determine which items are frequent. on the second
pass, we count only the pairs of items both of which are found frequent
on the    rst pass. monotonicity justi   es our ignoring other pairs.

    finding larger frequent itemsets: a-priori and many other algorithms
allow us to    nd frequent itemsets larger than pairs, if we make one pass
over the baskets for each size itemset, up to some limit. to    nd the
frequent itemsets of size k, monotonicity lets us restrict our attention to
only those itemsets such that all their subsets of size k     1 have already
been found frequent.

    the pcy algorithm: this algorithm improves on a-priori by creating
a hash table on the    rst pass, using all main-memory space that is not
needed to count the items. pairs of items are hashed, and the hash-table
buckets are used as integer counts of the number of times a pair has hashed
to that bucket. then, on the second pass, we only have to count pairs of
frequent items that hashed to a frequent bucket (one whose count is at
least the support threshold) on the    rst pass.

    the multistage algorithm: we can insert additional passes between the
   rst and second pass of the pcy algorithm to hash pairs to other, in-
dependent hash tables. at each intermediate pass, we only have to hash
pairs of frequent items that have hashed to frequent buckets on all previ-
ous passes.

    the multihash algorithm: we can modify the    rst pass of the pcy algo-
rithm to divide available main memory into several hash tables. on the
second pass, we only have to count a pair of frequent items if they hashed
to frequent buckets in all hash tables.

    randomized algorithms: instead of making passes through all the data,
we may choose a random sample of the baskets, small enough that it is
possible to store both the sample and the needed counts of itemsets in
main memory. the support threshold must be scaled down in proportion.
we can then    nd the frequent itemsets for the sample, and hope that it
is a good representation of the data as whole. while this method uses at
most one pass through the whole dataset, it is subject to false positives
(itemsets that are frequent in the sample but not the whole) and false
negatives (itemsets that are frequent in the whole but not the sample).

    the son algorithm: an improvement on the simple randomized algo-
rithm is to divide the entire    le of baskets into segments small enough
that all frequent itemsets for the segment can be found in main memory.
candidate itemsets are those found frequent for at least one segment. a

238

chapter 6. frequent itemsets

second pass allows us to count all the candidates and    nd the exact col-
lection of frequent itemsets. this algorithm is especially appropriate in a
mapreduce setting.

    toivonen   s algorithm: this algorithm starts by    nding frequent itemsets
in a sample, but with the threshold lowered so there is little chance of
missing an itemset that is frequent in the whole. next, we examine the
entire    le of baskets, counting not only the itemsets that are frequent in
the sample, but also, the negative border     itemsets that have not been
found frequent, but all their immediate subsets are. if no member of the
negative border is found frequent in the whole, then the answer is exact.
but if a member of the negative border is found frequent, then the whole
process has to repeat with another sample.

    frequent itemsets in streams: if we use a decaying window with constant
c, then we can start counting an item whenever we see it in a basket. we
start counting an itemset if we see it contained within the current basket,
and all its immediate proper subsets already are being counted. as the
window is decaying, we multiply all counts by 1     c and eliminate those
that are less than 1/2.

6.7 references for chapter 6

the market-basket data model, including association rules and the a-priori
algorithm, are from [1] and [2].

the pcy algorithm is from [4]. the multistage and multihash algorithms

are found in [3].

the son algorithm is from [5]. toivonen   s algorithm appears in [6].

1. r. agrawal, t. imielinski, and a. swami,    mining associations between
sets of items in massive databases,    proc. acm sigmod intl. conf. on
management of data, pp. 207   216, 1993.

2. r. agrawal and r. srikant,    fast algorithms for mining association rules,   

intl. conf. on very large databases, pp. 487   499, 1994.

3. m. fang, n. shivakumar, h. garcia-molina, r. motwani, and j.d. ull-
man,    computing iceberg queries e   ciently,    intl. conf. on very large
databases, pp. 299-310, 1998.

4. j.s. park, m.-s. chen, and p.s. yu,    an e   ective hash-based algorithm
for mining association rules,    proc. acm sigmod intl. conf. on man-
agement of data, pp. 175   186, 1995.

5. a. savasere, e. omiecinski, and s.b. navathe,    an e   cient algorithm for
mining association rules in large databases,    intl. conf. on very large
databases, pp. 432   444, 1995.

6.7. references for chapter 6

239

6. h. toivonen,    sampling large databases for association rules,    intl. conf.

on very large databases, pp. 134   145, 1996.

240

chapter 6. frequent itemsets

chapter 7

id91

id91 is the process of examining a collection of    points,    and grouping
the points into    clusters    according to some distance measure. the goal is that
points in the same cluster have a small distance from one another, while points
in di   erent clusters are at a large distance from one another. a suggestion of
what clusters might look like was seen in fig. 1.1. however, there the intent
was that there were three clusters around three di   erent road intersections, but
two of the clusters blended into one another because they were not su   ciently
separated.

our goal in this chapter is to o   er methods for discovering clusters in data.
we are particularly interested in situations where the data is very large, and/or
where the space either is high-dimensional, or the space is not euclidean at
all. we shall therefore discuss several algorithms that assume the data does
not    t in main memory. however, we begin with the basics: the two general
approaches to id91 and the methods for dealing with clusters in a non-
euclidean space.

7.1

introduction to id91 techniques

we begin by reviewing the notions of distance measures and spaces. the two
major approaches to id91     hierarchical and point-assignment     are de-
   ned. we then turn to a discussion of the    curse of dimensionality,    which
makes id91 in high-dimensional spaces di   cult, but also, as we shall see,
enables some simpli   cations if used correctly in a id91 algorithm.

7.1.1 points, spaces, and distances

a dataset suitable for id91 is a collection of points, which are objects
belonging to some space.
in its most general sense, a space is just a universal
set of points, from which the points in the dataset are drawn. however, we
should be mindful of the common case of a euclidean space (see section 3.5.2),

241

242

chapter 7. id91

which has a number of important properties useful for id91. in particular,
a euclidean space   s points are vectors of real numbers. the length of the vector
is the number of dimensions of the space. the components of the vector are
commonly called coordinates of the represented points.

all spaces for which we can perform a id91 have a distance measure,
giving a distance between any two points in the space. we introduced distances
in section 3.5. the common euclidean distance (square root of the sums of the
squares of the di   erences between the coordinates of the points in each dimen-
sion) serves for all euclidean spaces, although we also mentioned some other
options for distance measures in euclidean spaces, including the manhattan
distance (sum of the magnitudes of the di   erences in each dimension) and the
l   -distance (maximum magnitude of the di   erence in any dimension).

beagles

height

chihuahuas

dachshunds

weight

figure 7.1: heights and weights of dogs taken from three varieties

example 7.1 : classical applications of id91 often involve low-dimen-
sional euclidean spaces. for example, fig. 7.1 shows height and weight mea-
surements of dogs of several varieties. without knowing which dog is of which
variety, we can see just by looking at the diagram that the dogs fall into three
clusters, and those clusters happen to correspond to three varieties. with small
amounts of data, any id91 algorithm will establish the correct clusters, and
simply plotting the points and    eyeballing    the plot will su   ce as well.    

however, modern id91 problems are not so simple. they may involve
euclidean spaces of very high dimension or spaces that are not euclidean at
all. for example, it is challenging to cluster documents by their topic, based on
the occurrence of common, unusual words in the documents. it is challenging
to cluster moviegoers by the type or types of movies they like.

we also considered in section 3.5 distance measures for non-euclidean spa-
ces. these include the jaccard distance, cosine distance, hamming distance,

7.1.

introduction to id91 techniques

243

and id153. recall that the requirements for a function on pairs of points
to be a distance measure are that

1. distances are always nonnegative, and only the distance between a point

and itself is 0.

2. distance is symmetric; it doesn   t matter in which order you consider the

points when computing their distance.

3. distance measures obey the triangle inequality; the distance from x to y

to z is never less than the distance going from x to z directly.

7.1.2 id91 strategies

we can divide (cluster!) id91 algorithms into two groups that follow two
fundamentally di   erent strategies.

1. hierarchical or agglomerative algorithms start with each point in its own
cluster. clusters are combined based on their    closeness,    using one of
many possible de   nitions of    close.    combination stops when further
combination leads to clusters that are undesirable for one of several rea-
sons. for example, we may stop when we have a predetermined number of
clusters, or we may use a measure of compactness for clusters, and refuse
to construct a cluster by combining two smaller clusters if the resulting
cluster has points that are spread out over too large a region.

2. the other class of algorithms involve point assignment. points are con-
sidered in some order, and each one is assigned to the cluster into which
it best    ts. this process is normally preceded by a short phase in which
initial clusters are estimated. variations allow occasional combining or
splitting of clusters, or may allow points to be unassigned if they are
outliers (points too far from any of the current clusters).

algorithms for id91 can also be distinguished by:

(a) whether the algorithm assumes a euclidean space, or whether the al-
gorithm works for an arbitrary distance measure. we shall see that a
key distinction is that in a euclidean space it is possible to summarize
a collection of points by their centroid     the average of the points. in a
non-euclidean space, there is no notion of a centroid, and we are forced
to develop another way to summarize clusters.

(b) whether the algorithm assumes that the data is small enough to    t in
main memory, or whether data must reside in secondary memory, pri-
marily. algorithms for large amounts of data often must take shortcuts,
since it is infeasible to look at all pairs of points, for example. it is also
necessary to summarize clusters in main memory, since we cannot hold
all the points of all the clusters in main memory at the same time.

244

chapter 7. id91

7.1.3 the curse of dimensionality

high-dimensional euclidean spaces have a number of unintuitive properties that
are sometimes referred to as the    curse of dimensionality.    non-euclidean
spaces usually share these anomalies as well. one manifestation of the    curse   
is that in high dimensions, almost all pairs of points are equally far away from
one another. another manifestation is that almost any two vectors are almost
orthogonal. we shall explore each of these in turn.

the distribution of distances in a high-dimensional space

let us consider a d-dimensional euclidean space. suppose we choose n random
points in the unit cube, i.e., points [x1, x2, . . . , xd], where each xi is in the range
0 to 1. if d = 1, we are placing random points on a line of length 1. we expect
that some pairs of points will be very close, e.g., consecutive points on the
line. we also expect that some points will be very far away     those at or near
opposite ends of the line. the average distance between a pair of points is 1/3.1
suppose that d is very large. the euclidean distance between two random

points [x1, x2, . . . , xd] and [y1, y2, . . . , yd] is

d

vuut
xi=1

(xi     yi)2

here, each xi and yi is a random variable chosen uniformly in the range 0 to
1. since d is large, we can expect that for some i, |xi     yi| will be close to 1.
that puts a lower bound of 1 on the distance between almost any two random
points. in fact, a more careful argument can put a stronger lower bound on
the distance between all but a vanishingly small fraction of the pairs of points.
however, the maximum distance between two points is    d, and one can argue
that all but a vanishingly small fraction of the pairs do not have a distance
close to this upper limit. in fact, almost all points will have a distance close to
the average distance.

if there are essentially no pairs of points that are close, it is hard to build
clusters at all. there is little justi   cation for grouping one pair of points and
not another. of course, the data may not be random, and there may be useful
clusters, even in very high-dimensional spaces. however, the argument about
random data suggests that it will be hard to    nd these clusters among so many
pairs that are all at approximately the same distance.

angles between vectors

suppose again that we have three random points a, b, and c in a d-dimensional
space, where d is large. here, we do not assume points are in the unit cube;

1you can prove this fact by evaluating a double integral, but we shall not do the math

here, as it is not central to the discussion.

7.2. hierarchical id91

245

they can be anywhere in the space. what is angle abc? we may assume that
a is the point [x1, x2, . . . , xd] and c is the point [y1, y2, . . . , yd], while b is the
origin. recall from section 3.5.4 that the cosine of the angle abc is the dot
product of a and c divided by the product of the lengths of the vectors a and
c. that is, the cosine is

pd
qpd
i=1 x2

i=1 xiyi

iqpd

i=1 y2
i

as d grows, the denominator grows linearly in d, but the numerator is a sum
of random values, which are as likely to be positive as negative. thus, the
expected value of the numerator is 0, and as d grows, its standard deviation
grows only as    d. thus, for large d, the cosine of the angle between any two
vectors is almost certain to be close to 0, which means the angle is close to 90
degrees.

an important consequence of random vectors being orthogonal is that if we
have three random points a, b, and c, and we know the distance from a to b
is d1, while the distance from b to c is d2, we can assume the distance from a
2. that rule does not hold, even approximately,
if the number of dimensions is small. as an extreme case, if d = 1, then the
distance from a to c would necessarily be d1 + d2 if a and c were on opposite
sides of b, or |d1     d2| if they were on the same side.

to c is approximately pd2

1 + d2

7.1.4 exercises for section 7.1

! exercise 7.1.1 : prove that if you choose two points uniformly and indepen-
dently on a line of length 1, then the expected distance between the points is
1/3.

!! exercise 7.1.2 : if you choose two points uniformly in the unit square, what

is their expected euclidean distance?

! exercise 7.1.3 : suppose we have a d-dimensional euclidean space. consider
vectors whose components are only +1 or    1 in each dimension. note that
each vector has length    d, so the product of their lengths (denominator in
the formula for the cosine of the angle between them) is d. if we chose each
component independently, and a component is as likely to be +1 as    1, what
is the distribution of the value of the numerator of the formula (i.e., the sum
of the products of the corresponding components from each vector)? what can
you say about the expected value of the cosine of the angle between the vectors,
as d grows large?

7.2 hierarchical id91

we begin by considering hierarchical id91 in a euclidean space. this
algorithm can only be used for relatively small datasets, but even so, there

246

chapter 7. id91

are some e   ciencies we can make by careful implementation. when the space
is non-euclidean, there are additional problems associated with hierarchical
id91. we therefore consider    clustroids    and the way we can represent a
cluster when there is no centroid or average point in a cluster.

7.2.1 hierarchical id91 in a euclidean space

any hierarchical id91 algorithm works as follows. we begin with every
point in its own cluster. as time goes on, larger clusters will be constructed by
combining two smaller clusters, and we have to decide in advance:

1. how will clusters be represented?

2. how will we choose which two clusters to merge?

3. when will we stop combining clusters?

once we have answers to these questions, the algorithm can be described suc-
cinctly as:

while it is not time to stop do

pick the best two clusters to merge;
combine those two clusters into one cluster;

end;

to begin, we shall assume the space is euclidean. that allows us to represent
a cluster by its centroid or average of the points in the cluster. note that
in a cluster of one point, that point is the centroid, so we can initialize the
clusters straightforwardly. we can then use the merging rule that the distance
between any two clusters is the euclidean distance between their centroids,
and we should pick the two clusters at the shortest distance. other ways to
de   ne intercluster distance are possible, and we can also pick the best pair of
clusters on a basis other than their distance. we shall discuss some options in
section 7.2.3.

example 7.2 : let us see how the basic hierarchical id91 would work on
the data of fig. 7.2. these points live in a 2-dimensional euclidean space, and
each point is named by its (x, y) coordinates. initially, each point is in a cluster
by itself and is the centroid of that cluster. among all the pairs of points, there
are two pairs that are closest: (10,5) and (11,4) or (11,4) and (12,3). each is
at distance    2. let us break ties arbitrarily and decide to combine (11,4) with
(12,3). the result is shown in fig. 7.3, including the centroid of the new cluster,
which is at (11.5, 3.5).

you might think that (10,5) gets combined with the new cluster next, since it
is so close to (11,4). but our distance rule requires us to compare only cluster
centroids, and the distance from (10,5) to the centroid of the new cluster is
1.5   2, which is slightly greater than 2. thus, now the two closest clusters are

7.2. hierarchical id91

247

(4,10)

(7,10)

(4,8)

(6,8)

(10,5)

(12,6)

(11,4)

(9,3)

(12,3)

(3,4)

(2,2)

(5,2)

figure 7.2: twelve points to be clustered hierarchically

those of the points (4,8) and (4,10). we combine them into one cluster with
centroid (4,9).

at this point, the two closest centroids are (10,5) and (11.5, 3.5), so we
combine these two clusters. the result is a cluster of three points (10,5), (11,4),
and (12,3). the centroid of this cluster is (11,4), which happens to be one of
the points of the cluster, but that situation is coincidental. the state of the
clusters is shown in fig. 7.4.
now, there are several pairs of centroids that are at distance    5, and these
are the closest centroids. we show in fig. 7.5 the result of picking three of
these:

1. (6,8) is combined with the cluster of two elements having centroid (4,9).

2. (2,2) is combined with (3,4).

3. (9,3) is combined with the cluster of three elements having centroid (11,4).

we can proceed to combine clusters further. we shall discuss alternative stop-
ping rules next.    

there are several approaches we might use to stopping the id91 pro-

cess.

1. we could be told, or have a belief, about how many clusters there are in
the data. for example, if we are told that the data about dogs is taken
from chihuahuas, dachshunds, and beagles, then we know to stop when
there are three clusters left.

2. we could stop combining when at some point the best combination of
existing clusters produces a cluster that is inadequate. we shall discuss

248

chapter 7. id91

(4,10)

(7,10)

(4,8)

(6,8)

(10,5)

(12,6)

(11,4)

(11.5,3.5)

(9,3)

(12,3)

(3,4)

(2,2)

(5,2)

figure 7.3: combining the    rst two points into a cluster

various tests for the adequacy of a cluster in section 7.2.3, but for an
example, we could insist that any cluster have an average distance between
the centroid and its points no greater than some limit. this approach is
only sensible if we have a reason to believe that no cluster extends over
too much of the space.

3. we could continue id91 until there is only one cluster. however,
it is meaningless to return a single cluster consisting of all the points.
rather, we return the tree representing the way in which all the points
were combined. this form of answer makes good sense in some applica-
tions, such as one in which the points are genomes of di   erent species,
and the distance measure re   ects the di   erence in the genome.2 then,
the tree represents the evolution of these species, that is, the likely order
in which two species branched from a common ancestor.

example 7.3 : if we complete the id91 of the data of fig. 7.2, the tree
describing how clusters were grouped is the tree shown in fig. 7.6.    

7.2.2 e   ciency of hierarchical id91

the basic algorithm for hierarchical id91 is not very e   cient. at each
step, we must compute the distances between each pair of clusters, in order to
   nd the best merger. the initial step takes o(n2) time, but subsequent steps
take time proportional to (n     1)2, (n     2)2, . . .. the sum of squares up to n is
o(n3), so this algorithm is cubic. thus, it cannot be run except for fairly small
numbers of points.

2this space would not be euclidean, of course, but the principles regarding hierarchical

id91 carry over, with some modi   cations, to non-euclidean id91.

7.2. hierarchical id91

249

(7,10)

(6,8)

(4,10)

(4,9)

(4,8)

(3,4)

(10,5)

(12,6)

(11,4)

(9,3)

(12,3)

(2,2)

(5,2)

figure 7.4: id91 after two additional steps

however, there is a somewhat more e   cient implementation of which we

should be aware.

1. we start, as we must, by computing the distances between all pairs of

points, and this step is o(n2).

2. form the pairs and their distances into a priority queue, so we can always

   nd the smallest distance in one step. this operation is also o(n2).

3. when we decide to merge two clusters c and d, we remove all entries
in the priority queue involving one of these two clusters; that requires
work o(n log n) since there are at most 2n deletions to be performed, and
priority-queue deletion can be performed in o(log n) time.

4. we then compute all the distances between the new cluster and the re-
maining clusters. this work is also o(n log n), as there are at most n
entries to be inserted into the priority queue, and insertion into a priority
queue can also be done in o(log n) time.

since the last two steps are executed at most n times, and the    rst two steps are
executed only once, the overall running time of this algorithm is o(n2 log n).
that is better than o(n3), but it still puts a strong limit on how large n can
be before it becomes infeasible to use this id91 approach.

7.2.3 alternative rules for controlling hierarchical

id91

we have seen one rule for picking the best clusters to merge:    nd the pair with
the smallest distance between their centroids. some other options are:

250

chapter 7. id91

(7,10)

(4,10)

(4,9)

(4,8)

(4.7, 8.7)

(6,8)

(12,6)

(10,5)

(10.5, 3.8)

(11,4)

(9,3)

(12,3)

(3,4)

(2.5, 3)

(2,2)

(5,2)

figure 7.5: three more steps of the hierarchical id91

1. take the distance between two clusters to be the minimum of the distances
between any two points, one chosen from each cluster. for example, in
fig. 7.3 we would next chose to cluster the point (10,5) with the cluster of
two points, since (10,5) has distance    2, and no other pair of unclustered
points is that close. note that in example 7.2, we did make this combi-
nation eventually, but not until we had combined another pair of points.
in general, it is possible that this rule will result in an entirely di   erent
id91 from that obtained using the distance-of-centroids rule.

2. take the distance between two clusters to be the average distance of all

pairs of points, one from each cluster.

(2,2)    (3,4)    (5,2)    (4,8)    (4,10)    (6,8)    (7,10)    (11,4)    (12,3)    (10,5)    (9,3)    (12,6)

figure 7.6: tree showing the complete grouping of the points of fig. 7.2

7.2. hierarchical id91

251

3. the radius of a cluster is the maximum distance between all the points
and the centroid. combine the two clusters whose resulting cluster has
the lowest radius. a slight modi   cation is to combine the clusters whose
result has the lowest average distance between a point and the centroid.
another modi   cation is to use the sum of the squares of the distances
between the points and the centroid. in some algorithms, we shall    nd
these variant de   nitions of    radius    referred to as    the radius.   

4. the diameter of a cluster is the maximum distance between any two
points of the cluster. note that the radius and diameter of a cluster
are not related directly, as they are in a circle, but there is a tendency
for them to be proportional. we may choose to merge those clusters
whose resulting cluster has the smallest diameter. variants of this rule,
analogous to the rule for radius, are possible.

example 7.4 : let us consider the cluster consisting of the    ve points at the
right of fig. 7.2. the centroid of these    ve points is (10.8, 4.2). there is a tie
for the two furthest points from the centroid: (9,3) and (12,6), both at distance
   4.68 = 2.16. thus, the radius is 2.16. for the diameter, we    nd the two
points in the cluster having the greatest distance. these are again (9,3) and
(12,6). their distance is    18 = 4.24, so that is the diameter. notice that the
diameter is not exactly twice the radius, although it is close in this case. the
reason is that the centroid is not on the line between (9,3) and (12,6).    

we also have options in determining when to stop the merging process. we
already mentioned    stop when we have k clusters    for some predetermined k.
here are some other options.

1. stop if the diameter of the cluster that results from the best merger ex-
ceeds a threshold. we can also base this rule on the radius, or on any of
the variants of the radius mentioned above.

2. stop if the density of the cluster that results from the best merger is
below some threshold. the density can be de   ned in many di   erent ways.
roughly, it should be the number of cluster points per unit volume of the
cluster. that ratio can be estimated by the number of points divided by
some power of the diameter or radius of the cluster. the correct power
could be the number of dimensions of the space. sometimes, 1 or 2 is
chosen as the power, regardless of the number of dimensions.

3. stop when there is evidence that the next pair of clusters to be combined
yields a bad cluster. for example, we could track the average diameter
of all the current clusters. as long as we are combining points that truly
belong in a cluster, this average will rise gradually. however, if we combine
two clusters that really don   t deserve to be combined, then the average
diameter will take a sudden jump.

252

chapter 7. id91

example 7.5 : let us reconsider fig. 7.2. it has three natural clusters. we
computed the diameter of the largest     the    ve points at the right     in ex-
ample 7.4; it is 4.24. the diameter of the 3-node cluster at the lower left is
3, the distance between (2,2) and (5,2). the diameter of the 4-node cluster at
the upper left is    13 = 3.61. the average diameter, 3.62, was reached starting
from 0 after nine mergers, so the rise is evidently slow: about 0.4 per merger.
if we are forced to merge two of these natural clusters, the best we can do
is merge the two at the left. the diameter of this cluster is    89 = 9.43; that is
the distance between the two points (2,2) and (7,10). now, the average of the
diameters is (9.43 + 4.24)/2 = 6.84. this average has jumped almost as much
in one step as in all nine previous steps. that comparison indicates that the
last merger was inadvisable, and we should roll it back and stop.    

7.2.4 hierarchical id91 in non-euclidean spaces

when the space is non-euclidean, we need to use some distance measure that
is computed from points, such as jaccard, cosine, or id153. that is, we
cannot base distances on    location    of points. the algorithm of section 7.2.1
requires distances between points to be computed, but presumably we have a
way to compute those distances. a problem arises when we need to represent
a cluster, because we cannot replace a collection of points by their centroid.

example 7.6 : the problem arises for any of the non-euclidean distances we
have discussed, but to be concrete, suppose we are using id153, and we
decide to merge the strings abcd and aecdb. these have id153 3 and
might well be merged. however, there is no string that represents their average,
or that could be thought of as lying naturally between them. we could take
one of the strings that we might pass through when transforming one string to
the other by single insertions or deletions, such as aebcd, but there are many
such options. moreover, when clusters are formed from more than two strings,
the notion of    on the path between    stops making sense.    

given that we cannot combine points in a cluster when the space is non-
euclidean, our only choice is to pick one of the points of the cluster itself to
represent the cluster. ideally, this point is close to all the points of the cluster,
so it in some sense lies in the    center.    we call the representative point the
clustroid. we can select the clustroid in various ways, each designed to, in some
sense, minimize the distances between the clustroid and the other points in
the cluster. common choices include selecting as the clustroid the point that
minimizes:

1. the sum of the distances to the other points in the cluster.

2. the maximum distance to another point in the cluster.

3. the sum of the squares of the distances to the other points in the cluster.

7.2. hierarchical id91

253

example 7.7 : suppose we are using id153, and a cluster consists of
the four points abcd, aecdb, abecb, and ecdab. their distances are found in
the following table:

ecdab

abecb

aecdb

abcd
aecdb
abecb

5
2
4

3
2

3

if we apply the three criteria for being the centroid to each of the four points

of the cluster, we    nd:

point
abcd
aecdb
abecb
ecdab

sum max

sum-sq

11
7
9
11

5
3
4
5

43
17
29
45

we can see from these measurements that whichever of the three criteria we
choose, aecdb will be selected as the clustroid.
in general, di   erent criteria
could yield di   erent clustroids.    

the options for measuring the distance between clusters that were outlined
in section 7.2.3 can be applied in a non-euclidean setting, provided we use the
clustroid in place of the centroid. for example, we can merge the two clusters
whose clustroids are closest. we could also use the average or minimum distance
between all pairs of points from the clusters.

other suggested criteria involved measuring the density of a cluster, based
on the radius or diameter. both these notions make sense in the non-euclidean
environment. the diameter is still the maximum distance between any two
points in the cluster. the radius can be de   ned using the clustroid in place of
the centroid. moreover, it makes sense to use the same sort of evaluation for
the radius as we used to select the clustroid in the    rst place. for example,
if we take the clustroid to be the point with the smallest sum of squares of
distances to the other nodes, then de   ne the radius to be that sum of squares
(or its square root).

finally, section 7.2.3 also discussed criteria for stopping the merging of
clusters. none of these criteria made direct use of the centroid, except through
the notion of radius, and we have already observed that    radius    makes good
sense in non-euclidean spaces. thus, there is no substantial change in the
options for stopping criteria when we move from euclidean to non-euclidean
spaces.

7.2.5 exercises for section 7.2

exercise 7.2.1 : perform a hierarchical id91 of the one-dimensional set
of points 1, 4, 9, 16, 25, 36, 49, 64, 81, assuming clusters are represented by

254

chapter 7. id91

their centroid (average), and at each step the clusters with the closest centroids
are merged.

exercise 7.2.2 : how would the id91 of example 7.2 change if we used
for the distance between two clusters:

(a) the minimum of the distances between any two points, one from each

cluster.

(b) the average of the distances between pairs of points, one from each of the

two clusters.

exercise 7.2.3 : repeat the id91 of example 7.2 if we choose to merge
the two clusters whose resulting cluster has:

(a) the smallest radius.

(b) the smallest diameter.

exercise 7.2.4 : compute the density of each of the three clusters in fig. 7.2,
if    density    is de   ned to be the number of points divided by

(a) the square of the radius.

(b) the diameter (not squared).

what are the densities, according to (a) and (b), of the clusters that result from
the merger of any two of these three clusters. does the di   erence in densities
suggest the clusters should or should not be merged?

exercise 7.2.5 : we can select clustroids for clusters, even if the space is
euclidean. consider the three natural clusters in fig. 7.2, and compute the
clustroids of each, assuming the criterion for selecting the clustroid is the point
with the minimum sum of distances to the other point in the cluster.

! exercise 7.2.6 : consider the space of strings with id153 as the distance
measure. give an example of a set of strings such that if we choose the clustroid
by minimizing the sum of the distances to the other points we get one point
as the clustroid, but if we choose the clustroid by minimizing the maximum
distance to the other points, another point becomes the clustroid.

7.3 id116 algorithms

in this section we begin the study of point-assignment algorithms. the best
known family of id91 algorithms of this type is called id116. they
assume a euclidean space, and they also assume the number of clusters, k,
is known in advance. it is, however, possible to deduce k by trial and error.
after an introduction to the family of id116 algorithms, we shall focus on a
particular algorithm, called bfr after its authors, that enables us to execute
id116 on data that is too large to    t in main memory.

7.3. id116 algorithms

255

7.3.1 id116 basics

a id116 algorithm is outlined in fig. 7.7. there are several ways to select
the initial k points that represent the clusters, and we shall discuss them in
section 7.3.2. the heart of the algorithm is the for-loop, in which we consider
each point other than the k selected points and assign it to the closest cluster,
where    closest    means closest to the centroid of the cluster. note that the
centroid of a cluster can migrate as points are assigned to it. however, since
only points near the cluster are likely to be assigned, the centroid tends not to
move too much.

initially choose k points that are likely to be in

different clusters;

make these points the centroids of their clusters;
for each remaining point p do

find the centroid to which p is closest;
add p to the cluster of that centroid;
adjust the centroid of that cluster to account for p;

end;

figure 7.7: outline of id116 algorithms

an optional step at the end is to    x the centroids of the clusters and to
reassign each point, including the k initial points, to the k clusters. usually,
a point p will be assigned to the same cluster in which it was placed on the
   rst pass. however, there are cases where the centroid of p   s original cluster
moved quite far from p after p was placed there, and p is assigned to a di   erent
cluster on the second pass. in fact, even some of the original k points could
wind up being reassigned. as these examples are unusual, we shall not dwell
on the subject.

7.3.2

initializing clusters for id116

we want to pick points that have a good chance of lying in di   erent clusters.
there are two approaches.

1. pick points that are as far away from one another as possible.

2. cluster a sample of the data, perhaps hierarchically, so there are k clus-
ters. pick a point from each cluster, perhaps that point closest to the
centroid of the cluster.

the second approach requires little elaboration. for the    rst approach,

there are variations. one good choice is:

pick the first point at random;

256

chapter 7. id91

while there are fewer than k points do

add the point whose minimum distance from the selected

points is as large as possible;

end;

example 7.8 : let us consider the twelve points of fig. 7.2, which we repro-
duce here as fig. 7.8. in the worst case, our initial choice of a point is near the
center, say (6,8). the furthest point from (6,8) is (12,3), so that point is chosen
next.

(4,10)

(7,10)

(4,8)

(6,8)

(10,5)

(12,6)

(11,4)

(9,3)

(12,3)

(3,4)

(2,2)

(5,2)

figure 7.8: repeat of fig. 7.2

among the remaining ten points, the one whose minimum distance to either
(6,8) or (12,3) is a maximum is (2,2). that point has distance    52 = 7.21 from
(6,8) and distance    101 = 10.05 to (12,3); thus its    score    is 7.21. you can
check easily that any other point is less than distance 7.21 from at least one of
(6,8) and (12,3). our selection of three starting points is thus (6,8), (12,3), and
(2,2). notice that these three belong to di   erent clusters.

had we started with a di   erent point, say (10,5), we would get a di   erent
set of three initial points.
in this case, the starting points would be (10,5),
(2,2), and (4,10). again, these points belong to the three di   erent clusters.    

7.3.3 picking the right value of k

we may not know the correct value of k to use in a id116 id91. how-
ever, if we can measure the quality of the id91 for various values of k, we
can usually guess what the right value of k is. recall the discussion in sec-
tion 7.2.3, especially example 7.5, where we observed that if we take a measure
of appropriateness for clusters, such as average radius or diameter, that value
will grow slowly, as long as the number of clusters we assume remains at or
above the true number of clusters. however, as soon as we try to form fewer

7.3. id116 algorithms

257

clusters than there really are, the measure will rise precipitously. the idea is
expressed by the diagram of fig. 7.9.

average
diameter

correct value of k

number of clusters

figure 7.9: average diameter or another measure of di   useness rises quickly as
soon as the number of clusters falls below the true number present in the data

if we have no idea what the correct value of k is, we can    nd a good value
in a number of id91 operations that grows only logarithmically with the
true number. begin by running the id116 algorithm for k = 1, 2, 4, 8, . . . .
eventually, you will    nd two values v and 2v between which there is very little
decrease in the average diameter, or whatever measure of cluster cohesion you
are using. we may conclude that the value of k that is justi   ed by the data lies
between v/2 and v. if you use a binary search (discussed below) in that range,
you can    nd the best value for k in another log2 v id91 operations, for a
total of 2 log2 v id91s. since the true value of k is at least v/2, we have
used a number of id91s that is logarithmic in k.

since the notion of    not much change    is imprecise, we cannot say exactly
how much change is too much. however, the binary search can be conducted
as follows, assuming the notion of    not much change    is made precise by some
formula. we know that there is too much change between v/2 and v, or else
we would not have gone on to run a id91 for 2v clusters. suppose at some
point we have narrowed the range of k to between x and y. let z = (x + y)/2.
run a id91 with z as the target number of clusters. if there is not too
much change between z and y, then the true value of k lies between x and z.
so recursively narrow that range to    nd the correct value of k. on the other
hand, if there is too much change between z and y, then use binary search in
the range between z and y instead.

7.3.4 the algorithm of bradley, fayyad, and reina

this algorithm, which we shall refer to as bfr after its authors, is a variant of
id116 that is designed to cluster data in a high-dimensional euclidean space.
it makes a very strong assumption about the shape of clusters: they must
be normally distributed about a centroid. the mean and standard deviation
for a cluster may di   er for di   erent dimensions, but the dimensions must be

258

chapter 7. id91

independent. for instance, in two dimensions a cluster may be cigar-shaped,
but the cigar must not be rotated o    of the axes. figure 7.10 makes the point.

ok

ok

not ok

figure 7.10: the clusters in data for which the bfr algorithm may be used
can have standard deviations that di   er along di   erent axes, but the axes of
the cluster must align with the axes of the space

the bfr algorithm begins by selecting k points, using one of the methods
discussed in section 7.3.2. then, the points of the data    le are read in chunks.
these might be chunks from a distributed    le system or a conventional    le
might be partitioned into chunks of the appropriate size. each chunk must
consist of few enough points that they can be processed in main memory. also
stored in main memory are summaries of the k clusters and some other data,
so the entire memory is not available to store a chunk. the main-memory data
other than the chunk from the input consists of three types of objects:

1. the discard set : these are simple summaries of the clusters themselves.
we shall address the form of cluster summarization shortly. note that
the cluster summaries are not    discarded   ; they are in fact essential.
however, the points that the summary represents are discarded and have
no representation in main memory other than through this summary.

2. the compressed set : these are summaries, similar to the cluster sum-
maries, but for sets of points that have been found close to one another,
but not close to any cluster. the points represented by the compressed
set are also discarded, in the sense that they do not appear explicitly in
main memory. we call the represented sets of points miniclusters.

3. the retained set : certain points can neither be assigned to a cluster nor
are they su   ciently close to any other points that we can represent them
by a compressed set. these points are held in main memory exactly as
they appear in the input    le.

the picture in fig. 7.11 suggests how the points processed so far are represented.
the discard and compressed sets are represented by 2d + 1 values, if the

data is d-dimensional. these numbers are:

(a) the number of points represented, n .

7.3. id116 algorithms

259

points in the
retained set

compressed sets

a cluster.  its points
are in the discard set.

its centroid

figure 7.11: points in the discard, compressed, and retained sets

(b) the sum of the components of all the points in each dimension. this data
is a vector sum of length d, and the component in the ith dimension is
sumi.

(c) the sum of the squares of the components of all the points in each di-
mension. this data is a vector sumsq of length d, and its component in
the ith dimension is sumsqi.

our real goal is to represent a set of points by their count, their centroid and
the standard deviation in each dimension. however, these 2d + 1 values give us
those statistics. n is the count. the centroid   s coordinate in the ith dimension
is the sumi/n , that is the sum in that dimension divided by the number of
points. the variance in the ith dimension is sumsqi/n     (sumi/n )2. we can
compute the standard deviation in each dimension, since it is the square root
of the variance.
example 7.9 : suppose a cluster consists of the points (5, 1), (6,   2), and
(7, 0). then n = 3, sum = [18,   1], and sumsq = [110, 5]. the centroid is
sum/n , or [6,   1/3]. the variance in the    rst dimension is 110/3     (18/3)2 =
0.667, so the standard deviation is    0.667 = 0.816. in the second dimension,
the variance is 5/3     (   1/3)2 = 1.56, so the standard deviation is 1.25.    

7.3.5 processing data in the bfr algorithm

we shall now outline what happens when we process a chunk of points.

260

chapter 7. id91

bene   ts of the n , sum, sumsq representation

there is a signi   cant advantage to representing sets of points as it is done
in the bfr algorithm, rather than by storing n , the centroid, and the
standard deviation in each dimension. consider what we need to do when
we add a new point to a cluster. n is increased by 1, of course. but we
can also add the vector representing the location of the point to sum to
get the new sum, and we can add the squares of the components of the
vector to sumsq to get the new sumsq. had we used the centroid in place
of sum, then we could not adjust the centroid to account for the new point
without doing some calculation involving n , and the recomputation of the
standard deviations would be far more complex as well. similarly, if we
want to combine two sets, we just add corresponding values of n , sum,
and sumsq, while if we used the centroid and standard deviations as a
representation, the calculation would be far more complex.

1. first, all points that are su   ciently close to the centroid of a cluster are
added to that cluster. as described in the box on bene   ts, it is simple
to add the information about the point to the n , sum, and sumsq that
represent the cluster. we then discard the point. the question of what
   su   ciently close    means will be addressed shortly.

2. for the points that are not su   ciently close to any centroid, we clus-
ter them, along with the points in the retained set. any main-memory
id91 algorithm can be used, such as the hierarchical methods dis-
cussed in section 7.2. we must use some criterion for deciding when it is
reasonable to combine two points into a cluster or two clusters into one.
section 7.2.3 covered the ways we might make this decision. clusters of
more than one point are summarized and added to the compressed set.
singleton clusters become the retained set of points.

3. we now have miniclusters derived from our attempt to cluster new points
and the old retained set, and we have the miniclusters from the old com-
pressed set. although none of these miniclusters can be merged with one
of the k clusters, they might merge with one another. the criterion for
merger may again be chosen according to the discussion in section 7.2.3.
note that the form of representation for compressed sets (n , sum, and
sumsq) makes it easy to compute statistics such as the variance for the
combination of two miniclusters that we consider merging.

4. points that are assigned to a cluster or a minicluster, i.e., those that
are not in the retained set, are written out, with their assignment, to
secondary memory.

7.3. id116 algorithms

261

finally, if this is the last chunk of input data, we need to do something
with the compressed and retained sets. we can treat them as outliers, and
never cluster them at all. or, we can assign each point in the retained set to
the cluster of the nearest centroid. we can combine each minicluster with the
cluster whose centroid is closest to the centroid of the minicluster.

an important decision that must be examined is how we decide whether a
new point p is close enough to one of the k clusters that it makes sense to add
p to the cluster. two approaches have been suggested.

(a) add p to a cluster if it not only has the centroid closest to p, but it is
very unlikely that, after all the points have been processed, some other
cluster centroid will be found to be nearer to p. this decision is a complex
statistical calculation. it must assume that points are ordered randomly,
and that we know how many points will be processed in the future. its
advantage is that if we    nd one centroid to be signi   cantly closer to p
than any other, we can add p to that cluster and be done with it, even if
p is very far from all centroids.

(b) we can measure the id203 that, if p belongs to a cluster, it would
be found as far as it is from the centroid of that cluster. this calculation
makes use of the fact that we believe each cluster to consist of normally
distributed points with the axes of the distribution aligned with the axes
of the space. it allows us to make the calculation through the mahalanobis
distance of the point, which we shall describe next.

the mahalanobis distance is essentially the distance between a point and
the centroid of a cluster, normalized by the standard deviation of the cluster
in each dimension. since the bfr algorithm assumes the axes of the cluster
align with the axes of the space, the computation of mahalanobis distance is
especially simple. let p = [p1, p2, . . . , pd] be a point and c = [c1, c2, . . . , cd] the
centroid of a cluster. let   i be the standard deviation of points in the cluster
in the ith dimension. then the mahalanobis distance between p and c is

vuut

d

xi=1(cid:0)

pi     ci

  i

(cid:1)2

that is, we normalize the di   erence between p and c in the ith dimension by
dividing by the standard deviation of the cluster in that dimension. the rest of
the formula combines the normalized distances in each dimension in the normal
way for a euclidean space.

to assign point p to a cluster, we compute the mahalanobis distance between
p and each of the cluster centroids. we choose that cluster whose centroid has
the least mahalanobis distance, and we add p to that cluster provided the
mahalanobis distance is less than a threshold. for instance, suppose we pick
four as the threshold. if data is normally distributed, then the id203 of

262

chapter 7. id91

a value as far as four standard deviations from the mean is less than one in a
million. thus, if the points in the cluster are really normally distributed, then
the id203 that we will fail to include a point that truly belongs is less
than 10   6. and such a point is likely to be assigned to that cluster eventually
anyway, as long as it does not wind up closer to some other centroid as centroids
migrate in response to points added to their cluster.

7.3.6 exercises for section 7.3

exercise 7.3.1 : for the points of fig. 7.8, if we select three starting points
using the method of section 7.3.2, and the    rst point we choose is (3,4), which
other points are selected.

!! exercise 7.3.2 : prove that no matter what point we start with in fig. 7.8, if
we select three starting points by the method of section 7.3.2 we obtain points in
each of the three clusters. hint : you could solve this exhaustively by begining
with each of the twelve points in turn. however, a more generally applicable
solution is to consider the diameters of the three clusters and also consider
the minimum intercluster distance, that is, the minimum distance between two
points chosen from two di   erent clusters. can you prove a general theorem
based on these two parameters of a set of points?

! exercise 7.3.3 : give an example of a dataset and a selection of k initial
centroids such that when the points are reassigned to their nearest centroid at
the end, at least one of the initial k points is reassigned to a di   erent cluster.

exercise 7.3.4 : for the three clusters of fig. 7.8:

(a) compute the representation of the cluster as in the bfr algorithm. that

is, compute n , sum, and sumsq.

(b) compute the variance and standard deviation of each cluster in each of

the two dimensions.

exercise 7.3.5 : suppose a cluster of three-dimensional points has standard
deviations of 2, 3, and 5, in the three dimensions, in that order. compute the
mahalanobis distance between the origin (0, 0, 0) and the point (1,   3, 4).

7.4 the cure algorithm

we now turn to another large-scale-id91 algorithm in the point-assignment
class. this algorithm, called cure (id91 using representatives), as-
sumes a euclidean space. however, it does not assume anything about the
shape of clusters; they need not be normally distributed, and can even have
strange bends, s-shapes, or even rings. instead of representing clusters by their
centroid, it uses a collection of representative points, as the name implies.

7.4. the cure algorithm

263

figure 7.12: two clusters, one surrounding the other

example 7.10 : figure 7.12 is an illustration of two clusters. the inner clus-
ter is an ordinary circle, while the second is a ring around the circle. this
arrangement is not completely pathological. a creature from another galaxy
might look at our solar system and observe that the objects cluster into an inner
circle (the planets) and an outer ring (the kuyper belt), with little in between.
   

7.4.1

initialization in cure

we begin the cure algorithm by:

1. take a small sample of the data and cluster it in main memory. in prin-
ciple, any id91 method could be used, but as cure is designed to
handle oddly shaped clusters, it is often advisable to use a hierarchical
method in which clusters are merged when they have a close pair of points.
this issue is discussed in more detail in example 7.11 below.

2. select a small set of points from each cluster to be representative points.
these points should be chosen to be as far from one another as possible,
using the method described in section 7.3.2.

3. move each of the representative points a    xed fraction of the distance
between its location and the centroid of its cluster. perhaps 20% is a
good fraction to choose. note that this step requires a euclidean space,
since otherwise, there might not be any notion of a line between two
points.

example 7.11 : we could use a hierarchical id91 algorithm on a sample
of the data from fig. 7.12.
if we took as the distance between clusters the
shortest distance between any pair of points, one from each cluster, then we
would correctly    nd the two clusters. that is, pieces of the ring would stick

264

chapter 7. id91

together, and pieces of the inner circle would stick together, but pieces of ring
would always be far away from the pieces of the circle. note that if we used the
rule that the distance between clusters was the distance between their centroids,
then we might not get the intuitively correct result. the reason is that the
centroids of both clusters are in the center of the diagram.

figure 7.13: select representative points from each cluster, as far from one
another as possible

for the second step, we pick the representative points. if the sample from
which the clusters are constructed is large enough, we can count on a cluster   s
sample points at greatest distance from one another lying on the boundary of
the cluster. figure 7.13 suggests what our initial selection of sample points
might look like.

finally, we move the representative points a    xed fraction of the distance
from their true location toward the centroid of the cluster. note that in fig. 7.13
both clusters have their centroid in the same place: the center of the inner circle.
thus, the representative points from the circle move inside the cluster, as was
intended. points on the outer edge of the ring also move into their cluster, but
points on the ring   s inner edge move outside the cluster. the    nal locations of
the representative points from fig. 7.13 are suggested by fig. 7.14.    

7.4.2 completion of the cure algorithm

the next phase of cure is to merge two clusters if they have a pair of rep-
resentative points, one from each cluster, that are su   ciently close. the user
may pick the distance that de   nes    close.    this merging step can repeat, until
there are no more su   ciently close clusters.

example 7.12 : the situation of fig. 7.14 serves as a useful illustration. there
is some argument that the ring and circle should really be merged, because their
centroids are the same. for instance, if the gap between the ring and circle were

7.4. the cure algorithm

265

figure 7.14: moving the representative points 20% of the distance to the clus-
ter   s centroid

much smaller, it might well be argued that combining the points of the ring and
circle into a single cluster re   ected the true state of a   airs. for instance, the
rings of saturn have narrow gaps between them, but it is reasonable to visualize
the rings as a single object, rather than several concentric objects. in the case
of fig. 7.14 the choice of

1. the fraction of the distance to the centroid that we move the representa-

tive points and

2. the choice of how far apart representative points of two clusters need to

be to avoid merger

together determine whether we regard fig. 7.12 as one cluster or two.    

the last step of cure is point assignment. each point p is brought from
secondary storage and compared with the representative points. we assign p
to the cluster of the representative point that is closest to p.

example 7.13 : in our running example, points within the ring will surely
be closer to one of the ring   s representative points than to any representative
point of the circle. likewise, points within the circle will surely be closest to a
representative point of the circle. an outlier     a point not within the ring or
the circle     will be assigned to the ring if it is outside the ring. if the outlier is
between the ring and the circle, it will be assigned to one or the other, somewhat
favoring the ring because its representative points have been moved toward the
circle.    

7.4.3 exercises for section 7.4

exercise 7.4.1 : consider two clusters that are a circle and a surrounding ring,
as in the running example of this section. suppose:

266

chapter 7. id91

i. the radius of the circle is c.

ii. the inner and outer circles forming the ring have radii i and o, respec-

tively.

iii. all representative points for the two clusters are on the boundaries of the

clusters.

iv. representative points are moved 20% of the distance from their initial

position toward the centroid of their cluster.

v. clusters are merged if, after repositioning, there are representative points

from the two clusters at distance d or less.

in terms of d, c, i, and o, under what circumstances will the ring and circle be
merged into a single cluster?

7.5 id91 in non-euclidean spaces

we shall next consider an algorithm that handles non-main-memory data, but
does not require a euclidean space. the algorithm, which we shall refer to as
grgpf for its authors (v. ganti, r. ramakrishnan, j. gehrke, a. powell, and
j. french), takes ideas from both hierarchical and point-assignment approaches.
like cure, it represents clusters by sample points in main memory. however,
it also tries to organize the clusters hierarchically, in a tree, so a new point can
be assigned to the appropriate cluster by passing it down the tree. leaves of
the tree hold summaries of some clusters, and interior nodes hold subsets of the
information describing the clusters reachable through that node. an attempt
is made to group clusters by their distance from one another, so the clusters at
a leaf are close, and the clusters reachable from one interior node are relatively
close as well.

7.5.1 representing clusters in the grgpf algorithm

as we assign points to clusters, the clusters can grow large. most of the points
in a cluster are stored on disk, and are not used in guiding the assignment of
points, although they can be retrieved. the representation of a cluster in main
memory consists of several features. before listing these features, if p is any
point in a cluster, let rowsum(p) be the sum of the squares of the distances
from p to each of the other points in the cluster. note that, although we are not
in a euclidean space, there is some distance measure d that applies to points,
or else it is not possible to cluster points at all. the following features form the
representation of a cluster.

1. n , the number of points in the cluster.

7.5. id91 in non-euclidean spaces

267

2. the clustroid of the cluster, which is de   ned speci   cally to be the point
in the cluster that minimizes the sum of the squares of the distances to
the other points; that is, the clustroid is the point in the cluster with the
smallest rowsum.

3. the rowsum of the clustroid of the cluster.

4. for some chosen constant k, the k points of the cluster that are closest to
the clustroid, and their rowsums. these points are part of the represen-
tation in case the addition of points to the cluster causes the clustroid to
change. the assumption is made that the new clustroid would be one of
these k points near the old clustroid.

5. the k points of the cluster that are furthest from the clustroid and their
rowsums. these points are part of the representation so that we can
consider whether two clusters are close enough to merge. the assumption
is made that if two clusters are close, then a pair of points distant from
their respective clustroids would be close.

7.5.2

initializing the cluster tree

the clusters are organized into a tree, and the nodes of the tree may be very
large, perhaps disk blocks or pages, as would be the case for a b-tree or r-tree,
which the cluster-representing tree resembles. each leaf of the tree holds as
many cluster representations as can    t. note that a cluster representation has
a size that does not depend on the number of points in the cluster.

an interior node of the cluster tree holds a sample of the clustroids of the
clusters represented by each of its subtrees, along with pointers to the roots of
those subtrees. the samples are of    xed size, so the number of children that
an interior node may have is independent of its level. notice that as we go up
the tree, the id203 that a given cluster   s clustroid is part of the sample
diminishes.

we initialize the cluster tree by taking a main-memory sample of the dataset
and id91 it hierarchically. the result of this id91 is a tree t , but t
is not exactly the tree used by the grgpf algorithm. rather, we select from
t certain of its nodes that represent clusters of approximately some desired size
n. these are the initial clusters for the grgpf algorithm, and we place their
representations at the leaf of the cluster-representing tree. we then group clus-
ters with a common ancestor in t into interior nodes of the cluster-representing
tree, so in some sense, clusters descended from one interior node are as close
as possible. in some cases, rebalancing of the cluster-representing tree will be
necessary. this process is similar to the reorganization of a b-tree, and we shall
not examine this issue in detail.

268

chapter 7. id91

7.5.3 adding points in the grgpf algorithm

we now read points from secondary storage and insert each one into the nearest
cluster. we start at the root, and look at the samples of clustroids for each of
the children of the root. whichever child has the clustroid closest to the new
point p is the node we examine next. when we reach any node in the tree, we
look at the sample clustroids for its children and go next to the child with the
clustroid closest to p. note that some of the sample clustroids at a node may
have been seen at a higher level, but each level provides more detail about the
clusters lying below, so we see many new sample clustroids each time we go a
level down the tree.

finally, we reach a leaf. this leaf has the cluster features for each cluster
represented by that leaf, and we pick the cluster whose clustroid is closest to p.
we adjust the representation of this cluster to account for the new node p. in
particular, we:

1. add 1 to n .

2. add the square of the distance between p and each of the nodes q men-
tioned in the representation to rowsum(q). these points q include the
clustroid, the k nearest points, and the k furthest points.

we also estimate the rowsum of p, in case p needs to be part of the represen-
tation (e.g., it turns out to be one of the k points closest to the clustroid). note
we cannot compute rowsum(p) exactly, without going to disk and retrieving
all the points of the cluster. the estimate we use is

rowsum(p) = rowsum(c) + n d2(p, c)

where d(p, c) is the distance between p and the clustroid c. note that n and
rowsum(c) in this formula are the values of these features before they were
adjusted to account for the addition of p.

we might well wonder why this estimate works. in section 7.1.3 we discussed
the    curse of dimensionality,    in particular the observation that in a high-
dimensional euclidean space, almost all angles are right angles. of course the
assumption of the grgpf algorithm is that the space might not be euclidean,
but typically a non-euclidean space also su   ers from the curse of dimensionality,
in that it behaves in many ways like a high-dimensional euclidean space. if we
assume that the angle between p, c, and another point q in the cluster is a right
angle, then the pythagorean theorem tell us that

d2(p, q) = d2(p, c) + d2(c, q)

if we sum over all q other than c, and then add d2(p, c) to rowsum(p) to
account for the fact that the clustroid is one of the points in the cluster, we
derive rowsum(p) = rowsum(c) + n d2(p, c).

now, we must see if the new point p is one of the k closest or furthest
points from the clustroid, and if so, p and its rowsum become a cluster feature,

7.5. id91 in non-euclidean spaces

269

replacing one of the other features     whichever is no longer one of the k closest
or furthest. we also need to consider whether the rowsum for one of the k
closest points q is now less than rowsum(c). that situation could happen if p
were closer to one of these points than to the current clustroid. if so, we swap
the roles of c and q. eventually, it is possible that the true clustroid will no
longer be one of the original k closest points. we have no way of knowing, since
we do not see the other points of the cluster in main memory. however, they
are all stored on disk, and can be brought into main memory periodically for a
recomputation of the cluster features.

7.5.4 splitting and merging clusters

the grgpf algorithm assumes that there is a limit on the radius that a cluster

may have. the particular de   nition used for the radius is prowsum(c)/n ,

where c is the clustroid of the cluster and n the number of points in the cluster.
that is, the radius is the square root of the average square of the distance from
the clustroid of the points in the cluster. if a cluster   s radius grows too large,
it is split into two. the points of that cluster are brought into main memory,
and divided into two clusters to minimize the rowsums. the cluster features
for both clusters are computed.

as a result, the leaf of the split cluster now has one more cluster to represent.
we should manage the cluster tree like a b-tree, so usually, there will be room
in a leaf to add one more cluster. however, if not, then the leaf must be split
into two leaves. to implement the split, we must add another pointer and more
sample clustroids at the parent node. again, there may be extra space, but if
not, then this node too must be split, and we do so to minimize the squares of
the distances between the sample clustroids assigned to di   erent nodes. as in
a b-tree, this splitting can ripple all the way up to the root, which can then be
split if needed.

the worst thing that can happen is that the cluster-representing tree is now
too large to    t in main memory. there is only one thing to do: we make it
smaller by raising the limit on how large the radius of a cluster can be, and we
consider merging pairs of clusters. it is normally su   cient to consider clusters
that are    nearby,    in the sense that their representatives are at the same leaf
or at leaves with a common parent. however, in principle, we can consider
merging any two clusters c1 and c2 into one cluster c.

to merge clusters, we assume that the clustroid of c will be one of the
points that are as far as possible from the clustroid of c1 or the clustroid of
c2. suppose we want to compute the rowsum in c for the point p, which is one
of the k points in c1 that are as far as possible from the centroid of c1. we
use the curse-of-dimensionality argument that says all angles are approximately
right angles, to justify the following formula.

rowsumc (p) = rowsumc1(p) + nc2(cid:0)d2(p, c1) + d2(c1, c2)(cid:1) + rowsumc2(c2)

in the above, we subscript n and rowsum by the cluster to which that feature

270

chapter 7. id91

refers. we use c1 and c2 for the clustroids of c1 and c2, respectively.

in detail, we compute the sum of the squares of the distances from p to all
the nodes in the combined cluster c by beginning with rowsumc1(p) to get
the terms for the points in the same cluster as p. for the nc2 points q in c2,
we consider the path from p to the clustroid of c1, then to the clustroid of c2,
and    nally to q. we assume there is a right angle between the legs from p to
c1 and c1 to c2, and another right angle between the shortest path from p to
c2 and the leg from c2 to q. we then use the pythagorean theorem to justify
computing the square of the length of the path to each q as the sum of the
squares of the three legs.

we must then    nish computing the features for the merged cluster. we
need to consider all the points in the merged cluster for which we know the
rowsum. these are, the centroids of the two clusters, the k points closest to the
clustroids for each cluster, and the k points furthest from the clustroids for each
cluster, with the exception of the point that was chosen as the new clustroid.
we can compute the distances from the new clustroid for each of these 4k + 1
points. we select the k with the smallest distances as the    close    points and
the k with the largest distances as the    far    points. we can then compute the
rowsums for the chosen points, using the same formulas above that we used to
compute the rowsums for the candidate clustroids.

7.5.5 exercises for section 7.5

exercise 7.5.1 : using the cluster representation of section 7.5.1, represent
the twelve points of fig. 7.8 as a single cluster. use parameter k = 2 as the
number of close and distant points to be included in the representation. hint :
since the distance is euclidean, we can get the square of the distance between
two points by taking the sum of the squares of the di   erences along the x- and
y-axes.

exercise 7.5.2 : compute the radius, in the sense used by the grgpf algo-
rithm (square root of the average square of the distance from the clustroid) for
the cluster that is the    ve points in the lower right of fig. 7.8. note that (11,4)
is the clustroid.

7.6 id91 for streams and parallelism

in this section, we shall consider brie   y how one might cluster a stream. the
model we have in mind is one where there is a sliding window (recall sec-
tion 4.1.3) of n points, and we can ask for the centroids or clustroids of the
best clusters formed from the last m of these points, for any m     n . we also
study a similar approach to id91 a large,    xed set of points using mapre-
duce on a computing cluster (no pun intended). this section provides only
a rough outline to suggest the possibilities, which depend on our assumptions
about how clusters evolve in a stream.

7.6. id91 for streams and parallelism

271

7.6.1 the stream-computing model

we assume that each stream element is a point in some space. the sliding
window consists of the most recent n points. our goal is to precluster subsets
of the points in the stream, so that we may quickly answer queries of the form
   what are the clusters of the last m points?    for any m     n . there are many
variants of this query, depending on what we assume about what constitutes
a cluster. for instance, we may use a id116 approach, where we are really
asking that the last m points be partitioned into exactly k clusters. or, we may
allow the number of clusters to vary, but use one of the criteria in section 7.2.3
or 7.2.4 to determine when to stop merging clusters into larger clusters.

we make no restriction regarding the space in which the points of the stream
live. it may be a euclidean space, in which case the answer to the query is the
centroids of the selected clusters. the space may be non-euclidean, in which
case the answer is the clustroids of the selected clusters, where any of the
de   nitions for    clustroid    may be used (see section 7.2.4).

the problem is considerably easier if we assume that all stream elements are
chosen with statistics that do not vary along the stream. then, a sample of the
stream is good enough to estimate the clusters, and we can in e   ect ignore the
stream after a while. however, the stream model normally assumes that the
statistics of the stream elements varies with time. for example, the centroids
of the clusters may migrate slowly as time goes on, or clusters may expand,
contract, divide, or merge.

7.6.2 a stream-id91 algorithm

in this section, we shall present a greatly simpli   ed version of an algorithm
referred to as bdmo (for the authors, b. babcock, m. datar, r. motwani,
and l. o   callaghan). the true version of the algorithm involves much more
complex structures, which are designed to provide performance guarantees in
the worst case.

the bdmo algorithm builds on the methodology for counting ones in a
stream that was described in section 4.6. here are the key similarities and
di   erences:

    like that algorithm, the points of the stream are partitioned into, and
summarized by, buckets whose sizes are a power of two. here, the size of
a bucket is the number of points it represents, rather than the number of
stream elements that are 1.

    as before, the sizes of buckets obey the restriction that there are one or
two of each size, up to some limit. however, we do not assume that the
sequence of allowable bucket sizes starts with 1. rather, they are required
only to form a sequence where each size is twice the previous size, e.g.,
3, 6, 12, 24, . . . .

272

chapter 7. id91

    bucket sizes are again restrained to be nondecreasing as we go back in
time. as in section 4.6, we can conclude that there will be o(log n )
buckets.

    the contents of a bucket consists of:

1. the size of the bucket.

2. the timestamp of the bucket, that is, the most recent point that
contributes to the bucket. as in section 4.6, timestamps can be
recorded modulo n .

3. a collection of records that represent the clusters into which the
points of that bucket have been partitioned. these records contain:

(a) the number of points in the cluster.
(b) the centroid or clustroid of the cluster.
(c) any other parameters necessary to enable us to merge clusters
and maintain approximations to the full set of parameters for the
merged cluster. we shall give some examples when we discuss
the merger process in section 7.6.4.

7.6.3

initializing buckets

our smallest bucket size will be p, a power of 2. thus, every p stream elements,
we create a new bucket, with the most recent p points. the timestamp for this
bucket is the timestamp of the most recent point in the bucket. we may leave
each point in a cluster by itself, or we may perform a id91 of these points
according to whatever id91 strategy we have chosen. for instance, if we
choose a id116 algorithm, then (assuming k < p) we cluster the points into
k clusters by some algorithm.

whatever method we use to cluster initially, we assume it is possible to
compute the centroids or clustroids for the clusters and count the points in
each cluster. this information becomes part of the record for each cluster. we
also compute whatever other parameters for the clusters will be needed in the
merging process.

7.6.4 merging buckets

following the strategy from section 4.6, whenever we create a new bucket, we
need to review the sequence of buckets. first, if some bucket has a timestamp
that is more than n time units prior to the current time, then nothing of that
bucket is in the window, and we may drop it from the list. second, we may have
created three buckets of size p, in which case we must merge the oldest two of
the three. the merger may create two buckets of size 2p, in which case we may
have to merge buckets of increasing sizes, recursively, just as in section 4.6.

to merge two consecutive buckets, we need to do several things:

7.6. id91 for streams and parallelism

273

1. the size of the bucket is twice the sizes of the two buckets being merged.

2. the timestamp for the merged bucket is the timestamp of the more recent

of the two consecutive buckets.

3. we must consider whether to merge clusters, and if so, we need to compute
the parameters of the merged clusters. we shall elaborate on this part of
the algorithm by considering several examples of criteria for merging and
ways to estimate the needed parameters.

example 7.14 : perhaps the simplest case is where we are using a id116
approach in a euclidean space. we represent clusters by the count of their
points and their centroids. each bucket has exactly k clusters, so we can pick
p = k, or we can pick p larger than k and cluster the p points into k clusters when
we create a bucket initially as in section 7.6.3. we must    nd the best matching
between the k clusters of the    rst bucket and the k clusters of the second. here,
   best    means the matching that minimizes the sum of the distances between
the centroids of the matched clusters.

note that we do not consider merging two clusters from the same bucket,
because our assumption is that clusters do not evolve too much between con-
secutive buckets. thus, we would expect to    nd in each of two adjacent buckets
a representation of each of the k    true    clusters that exist in the stream.

when we decide to merge two clusters, one from each bucket, the number
of points in the merged cluster is surely the sum of the numbers of points in the
two clusters. the centroid of the merged cluster is the weighted average of the
centroids of the two clusters, where the weighting is by the numbers of points
in the clusters. that is, if the two clusters have n1 and n2 points, respectively,
and have centroids c1 and c2 (the latter are d-dimensional vectors for some d),
then the combined cluster has n = n1 + n2 points and has centroid

c =

n1c1 + n2c2

n1 + n2

   

example 7.15 : the method of example 7.14 su   ces when the clusters are
changing very slowly. suppose we might expect the cluster centroids to mi-
grate su   ciently quickly that when matching the centroids from two consecu-
tive buckets, we might be faced with an ambiguous situation, where it is not
clear which of two clusters best matches a given cluster from the other bucket.
one way to protect against such a situation is to create more than k clusters in
each bucket, even if we know that, when we query (see section 7.6.5), we shall
have to merge into exactly k clusters. for example, we might choose p to be
much larger than k, and, when we merge, only merge clusters when the result
is su   ciently coherent according to one of the criteria outlined in section 7.2.3.
or, we could use a hierarchical strategy, and make the best merges, so as to
maintain p > k clusters in each bucket.

274

chapter 7. id91

suppose, to be speci   c, that we want to put a limit on the sum of the
distances between all the points of a cluster and its centroid. then in addition
to the count of points and the centroid of a cluster, we can include an estimate
of this sum in the record for a cluster. when we initialize a bucket, we can
compute the sum exactly. but as we merge clusters, this parameter becomes an
estimate only. suppose we merge two clusters, and want to compute the sum of
distances for the merged cluster. use the notation for centroids and counts in
example 7.14, and in addition, let s1 and s2 be the sums for the two clusters.
then we may estimate the radius of the merged cluster to be

n1|c1     c| + n2|c2     c| + s1 + s2

that is, we estimate the distance between any point x and the new centroid
c to be the distance of that point to its old centroid (these distances sum to
s1 + s2, the last two terms in the above expression) plus the distance from
the old centroid to the new (these distances sum to the    rst two terms of the
above expression). note that this estimate is an upper bound, by the triangle
inequality.

an alternative is to replace the sum of distances by the sum of the squares of
the distances from the points to the centroid. if these sums for the two clusters
are t1 and t2, respectively, then we can produce an estimate for the same sum
in the new cluster as

n1|c1     c|2 + n2|c2     c|2 + t1 + t2

this estimate is close to correct if the space is high-dimensional, by the    curse
of dimensionality.       

example 7.16 : our third example will assume a non-euclidean space and no
constraint on the number of clusters. we shall borrow several of the techniques
from the grgpf algorithm of section 7.5. speci   cally, we represent clusters
by their clustroid and rowsum (sum of the squares of the distances from each
node of the cluster to its clustroid). we include in the record for a cluster
information about a set of points at maximum distance from the clustroid,
including their distances from the clustroid and their rowsums. recall that
their purpose is to suggest a clustroid when this cluster is merged with another.
when we merge buckets, we may choose one of many ways to decide which
clusters to merge. for example, we may consider pairs of clusters in order of
the distance between their clustroids. we may also choose to merge clusters
when we consider them, provided the sum of their rowsums is below a certain
limit. alternatively, we may perform the merge if the sum of rowsums divided
by the number of points in the clusters is below a limit. any of the other
strategies discussed for deciding when to merge clusters may be used as well,
provided we arrange to maintain the data (e.g., cluster diameter) necessary to
make decisions.

we then must pick a new clustroid, from among the points most distant
from the clustroids of the two merged clusters. we can compute rowsums for

7.6. id91 for streams and parallelism

275

each of these candidate clustroids using the formulas given in section 7.5.4. we
also follow the strategy given in that section to pick a subset of the distant
points from each cluster to be the set of distant points for the merged cluster,
and to compute the new rowsum and distance-to-clustroid for each.    

7.6.5 answering queries

recall that we assume a query is a request for the clusters of the most recent m
points in the stream, where m     n . because of the strategy we have adopted
of combining buckets as we go back in time, we may not be able to    nd a set
of buckets that covers exactly the last m points. however, if we choose the
smallest set of buckets that cover the last m points, we shall include in these
buckets no more than the last 2m points. we shall produce, as answer to the
query, the centroids or clustroids of all the points in the selected buckets. in
order for the result to be a good approximation to the clusters for exactly the
last m points, we must assume that the points between 2m and m + 1 will not
have radically di   erent statistics from the most recent m points. however, if
the statistics vary too rapidly, recall from section 4.6.6 that a more complex
bucketing scheme can guarantee that we can    nd buckets to cover at most the
last m(1 +   ) points, for any    > 0.

having selected the desired buckets, we pool all their clusters. we then use
some methodology for deciding which clusters to merge. examples 7.14 and
7.16 are illustrations of two approaches to this merger. for instance, if we are
required to produce exactly k clusters, then we can merge the clusters with the
closest centroids until we are left with only k clusters, as in example 7.14. or
we can make a decision whether or not to merge clusters in various ways, as we
sampled in example 7.16.

7.6.6 id91 in a parallel environment

now, let us brie   y consider the use of parallelism available in a computing
cluster.3 we assume we are given a very large collection of points, and we wish
to exploit parallelism to compute the centroids of their clusters. the simplest
approach is to use a mapreduce strategy, but in most cases we are constrained
to use a single reduce task.

begin by creating many map tasks. each task is assigned a subset of the
points. the map function   s job is to cluster the points it is given. its output is
a set of key-value pairs with a    xed key 1, and a value that is the description
of one cluster. this description can be any of the possibilities suggested in
section 7.6.2, such as the centroid, count, and diameter of the cluster.

since all key-value pairs have the same key, there can be only one reduce
task. this task gets descriptions of the clusters produced by each of the map

3do not forget that the term    cluster    has two completely di   erent meanings in this

section.

276

chapter 7. id91

tasks, and must merge them appropriately. we may use the discussion in sec-
tion 7.6.4 as representative of the various strategies we might use to produce
the    nal id91, which is the output of the reduce task.

7.6.7 exercises for section 7.6

exercise 7.6.1 : execute the bdmo algorithm with p = 3 on the following
1-dimensional, euclidean data:

1, 45, 80, 24, 56, 71, 17, 40, 66, 32, 48, 96, 9, 41, 75, 11, 58, 93, 28, 39, 77

the id91 algorithms is id116 with k = 3. only the centroid of a cluster,
along with its count, is needed to represent a cluster.

exercise 7.6.2 : using your clusters from exercise 7.6.1, produce the best
centroids in response to a query asking for a id91 of the last 10 points.

7.7 summary of chapter 7

    id91: clusters are often a useful summary of data that is in the form
of points in some space. to cluster points, we need a distance measure
on that space. ideally, points in the same cluster have small distances be-
tween them, while points in di   erent clusters have large distances between
them.

    id91 algorithms: id91 algorithms generally have one of two
forms. hierarchical id91 algorithms begin with all points in a cluster
of their own, and nearby clusters are merged iteratively. point-assignment
id91 algorithms consider points in turn and assign them to the clus-
ter in which they best    t.

    the curse of dimensionality: points in high-dimensional euclidean spa-
ces, as well as points in non-euclidean spaces often behave unintuitively.
two unexpected properties of these spaces are that random points are
almost always at about the same distance, and random vectors are almost
always orthogonal.

    centroids and clustroids: in a euclidean space, the members of a cluster
can be averaged, and this average is called the centroid. in non-euclidean
spaces, there is no guarantee that points have an    average,    so we are
forced to use one of the members of the cluster as a representative or
typical element of the cluster. that representative is called the clustroid.

    choosing the clustroid : there are many ways we can de   ne a typical point
of a cluster in a non-euclidean space. for example, we could choose the
point with the smallest sum of distances to the other points, the smallest
sum of the squares of those distances, or the smallest maximum distance
to any other point in the cluster.

7.7. summary of chapter 7

277

    radius and diameter : whether or not the space is euclidean, we can de-
   ne the radius of a cluster to be the maximum distance from the centroid
or clustroid to any point in that cluster. we can de   ne the diameter of
the cluster to be the maximum distance between any two points in the
cluster. alternative de   nitions, especially of the radius, are also known,
for example, average distance from the centroid to the other points.

    hierarchical id91: this family of algorithms has many variations,
which di   er primarily in two areas. first, we may chose in various ways
which two clusters to merge next. second, we may decide when to stop
the merge process in various ways.

    picking clusters to merge: one strategy for deciding on the best pair of
clusters to merge in a hierarchical id91 is to pick the clusters with
the closest centroids or clustroids. another approach is to pick the pair of
clusters with the closest points, one from each cluster. a third approach
is to use the average distance between points from the two clusters.

    stopping the merger process: a hierarchical id91 can proceed until
there are a    xed number of clusters left. alternatively, we could merge
until it is impossible to    nd a pair of clusters whose merger is su   ciently
compact, e.g., the merged cluster has a radius or diameter below some
threshold. another approach involves merging as long as the resulting
cluster has a su   ciently high    density,    which can be de   ned in various
ways, but is the number of points divided by some measure of the size of
the cluster, e.g., the radius.

    id116 algorithms: this family of algorithms is of the point-assignment
type and assumes a euclidean space. it is assumed that there are exactly
k clusters for some known k. after picking k initial cluster centroids, the
points are considered one at a time and assigned to the closest centroid.
the centroid of a cluster can migrate during point assignment, and an
optional last step is to reassign all the points, while holding the centroids
   xed at their    nal values obtained during the    rst pass.

    initializing id116 algorithms: one way to    nd k initial centroids is
to pick a random point, and then choose k     1 additional points, each as
far away as possible from the previously chosen points. an alternative is
to start with a small sample of points and use a hierarchical id91 to
merge them into k clusters.

    picking k in a id116 algorithm: if the number of clusters is unknown,
we can use a binary-search technique, trying a id116 id91 with
di   erent values of k. we search for the largest value of k for which a
decrease below k clusters results in a radically higher average diameter
of the clusters. this search can be carried out in a number of id91
operations that is logarithmic in the true value of k.

278

chapter 7. id91

    the bfr algorithm: this algorithm is a version of id116 designed to
handle data that is too large to    t in main memory. it assumes clusters
are normally distributed about the axes.

    representing clusters in bfr: points are read from disk one chunk at a
time. clusters are represented in main memory by the count of the num-
ber of points, the vector sum of all the points, and the vector formed by
summing the squares of the components of the points in each dimension.
other collection of points, too far from a cluster centroid to be included
in a cluster, are represented as    miniclusters    in the same way as the k
clusters, while still other points, which are not near any other point will
be represented as themselves and called    retained    points.

    processing points in bfr: most of the points in a main-memory load
will be assigned to a nearby cluster and the parameters for that cluster
will be adjusted to account for the new points. unassigned points can
be formed into new miniclusters, and these miniclusters can be merged
with previously discovered miniclusters or retained points. after the last
memory load, the miniclusters and retained points can be merged to their
nearest cluster or kept as outliers.

    the cure algorithm: this algorithm is of the point-assignment type.
it is designed for a euclidean space, but clusters can have any shape. it
handles data that is too large to    t in main memory.

    representing clusters in cure : the algorithm begins by id91 a
small sample of points.
it then selects representative points for each
cluster, by picking points in the cluster that are as far away from each
other as possible. the goal is to    nd representative points on the fringes of
the cluster. however, the representative points are then moved a fraction
of the way toward the centroid of the cluster, so they lie somewhat in the
interior of the cluster.

    processing points in cure : after creating representative points for each
cluster, the entire set of points can be read from disk and assigned to a
cluster. we assign a given point to the cluster of the representative point
that is closest to the given point.

    the grgpf algorithm: this algorithm is of the point-assignment type.
it handles data that is too big to    t in main memory, and it does not
assume a euclidean space.

    representing clusters in grgpf : a cluster is represented by the count
of points in the cluster, the clustroid, a set of points nearest the clustroid
and a set of points furthest from the clustroid. the nearby points allow
us to change the clustroid if the cluster evolves, and the distant points
allow for merging clusters e   ciently in appropriate circumstances. for
each of these points, we also record the rowsum, that is the square root

7.7. summary of chapter 7

279

of the sum of the squares of the distances from that point to all the other
points of the cluster.

    tree organization of clusters in grgpf : cluster representations are or-
ganized into a tree structure like a b-tree, where nodes of the tree are
typically disk blocks and contain information about many clusters. the
leaves hold the representation of as many clusters as possible, while inte-
rior nodes hold a sample of the clustroids of the clusters at their descen-
dant leaves. we organize the tree so that the clusters whose representa-
tives are in any subtree are as close as possible.

    processing points in grgpf : after initializing clusters from a sample of
points, we insert each point into the cluster with the nearest clustroid.
because of the tree structure, we can start at the root and choose to visit
the child with the sample clustroid nearest to the given point. following
this rule down one path in the tree leads us to a leaf, where we insert the
point into the cluster with the nearest clustroid on that leaf.

    id91 streams: a generalization of the dgim algorithm (for count-
ing 1   s in the sliding window of a stream) can be used to cluster points
that are part of a slowly evolving stream. the bdmo algorithm uses
buckets similar to those of dgim, with allowable bucket sizes forming a
sequence where each size is twice the previous size.

    representation of buckets in bdmo : the size of a bucket is the number
of points it represents. the bucket itself holds only a representation of the
clusters of these points, not the points themselves. a cluster representa-
tion includes a count of the number of points, the centroid or clustroid,
and other information that is needed for merging clusters according to
some selected strategy.

    merging buckets in bdmo : when buckets must be merged, we    nd the
best matching of clusters, one from each of the buckets, and merge them
in pairs. if the stream evolves slowly, then we expect consecutive buckets
to have almost the same cluster centroids, so this matching makes sense.

    answering queries in bdmo : a query is a length of a su   x of the sliding
window. we take all the clusters in all the buckets that are at least
partially within that su   x and merge them using some strategy. the
resulting clusters are the answer to the query.

    id91 using mapreduce: we can divide the data into chunks and
cluster each chunk in parallel, using a map task. the clusters from each
map task can be further clustered in a single reduce task.

280

chapter 7. id91

7.8 references for chapter 7

the ancestral study of id91 for large-scale data is the birch algorithm
of [6]. the bfr algorithm is from [2]. the cure algorithm is found in [5].

the paper on the grgpf algorithm is [3]. the necessary background
regarding b-trees and r-trees can be found in [4]. the study of id91 on
streams is taken from [1].

1. b. babcock, m. datar, r. motwani, and l. o   callaghan,    maintaining
variance and k-medians over data stream windows,    proc. acm symp.
on principles of database systems, pp. 234   243, 2003.

2. p.s. bradley, u.m. fayyad, and c. reina,    scaling id91 algorithms
to large databases,    proc. knowledge discovery and data mining, pp. 9   
15, 1998.

3. v. ganti, r. ramakrishnan, j. gehrke, a.l. powell, and j.c. french:,
   id91 large datasets in arbitrary metric spaces,    proc. intl. conf.
on data engineering, pp. 502   511, 1999.

4. h. garcia-molina, j.d. ullman, and j. widom, database systems: the
complete book second edition, prentice-hall, upper saddle river, nj,
2009.

5. s. guha, r. rastogi, and k. shim,    cure: an e   cient id91 algo-
rithm for large databases,    proc. acm sigmod intl. conf. on manage-
ment of data, pp. 73   84, 1998.

6. t. zhang, r. ramakrishnan, and m. livny,    birch: an e   cient data
id91 method for very large databases,    proc. acm sigmod intl.
conf. on management of data, pp. 103   114, 1996.

chapter 8

advertising on the web

one of the big surprises of the 21st century has been the ability of all sorts of
interesting web applications to support themselves through advertising, rather
than subscription. while radio and television have managed to use advertising
as their primary revenue source, most media     newspapers and magazines,
for example     have had to use a hybrid approach, combining revenue from
advertising and subscriptions.

by far the most lucrative venue for on-line advertising has been search, and
much of the e   ectiveness of search advertising comes from the    adwords    model
of matching search queries to advertisements. we shall therefore devote much
of this chapter to algorithms for optimizing the way this assignment is done.
the algorithms used are of an unusual type; they are greedy and they are    on-
line    in a particular technical sense to be discussed. we shall therefore digress
to discuss these two algorithmic issues     greediness and on-line algorithms     in
general, before tackling the adwords problem.

a second interesting on-line advertising problem involves selecting items to
advertise at an on-line store. this problem involves    collaborative    ltering,   
where we try to    nd customers with similar behavior in order to suggest they
buy things that similar customers have bought. this subject will be treated in
section 9.3.

8.1

issues in on-line advertising

in this section, we summarize the technical problems that are presented by the
opportunities for on-line advertising. we begin by surveying the types of ads
found on the web.

8.1.1 advertising opportunities

the web o   ers many ways for an advertiser to show their ads to potential
customers. here are the principal venues.

281

282

chapter 8. advertising on the web

1. some sites, such as ebay, craig   s list or auto trading sites allow adver-
tisers to post their ads directly, either for free, for a fee, or a commission.

2. display ads are placed on many web sites. advertisers pay for the display
at a    xed rate per impression (one display of the ad with the download of
the page by some user). normally, a second download of the page, even
by the same user, will result in the display of a di   erent ad and is a second
impression.

3. on-line stores such as amazon show ads in many contexts. the ads
are not paid for by the manufacturers of the product advertised, but are
selected by the store to maximize the id203 that the customer will
be interested in the product. we consider this kind of advertising in
chapter 9.

4. search ads are placed among the results of a search query. advertisers
bid for the right to have their ad shown in response to certain queries, but
they pay only if the ad is clicked on. the particular ads to be shown are
selected by a complex process, to be discussed in this chapter, involving
the search terms that the advertiser has bid for, the amount of their bid,
the observed id203 that the ad will be clicked on, and the total
budget that the advertiser has o   ered for the service.

8.1.2 direct placement of ads

when advertisers can place ads directly, such as a free ad on craig   s list or
the    buy it now    feature at ebay, there are several problems that the site must
deal with. ads are displayed in response to query terms, e.g.,    apartment palo
alto.    the web site can use an inverted index of words, just as a search engine
does (see section 5.1.1) and return those ads that contain all the words in the
query. alternatively, one can ask the advertiser to specify parameters of the ad,
which are stored in a database. for instance, an ad for a used car could specify
the manufacturer, model, color, and year from pull-down menus, so only clearly
understood terms can be used. queryers can use the same menus of terms in
their queries.

ranking ads is a bit more problematic, since there is nothing like the links
on the web to tell us which ads are more    important.    one strategy used is
   most-recent    rst.    that strategy, while equitable, is subject to abuse, where
advertisers post small variations of their ads at frequent intervals. the tech-
nology for discovering ads that are too similar has already been covered, in
section 3.4.

an alternative approach is to try to measure the attractiveness of an ad.
each time it is displayed, record whether or not the queryer clicked on it.
presumably, attractive ads will be clicked on more frequently than those that
are not. however, there are several factors that must be considered in evaluating
ads:

8.1.

issues in on-line advertising

283

1. the position of the ad in a list has great in   uence on whether or not it
is clicked. the    rst on the list has by far the highest id203, and the
id203 drops o    exponentially as the position increases.

2. the ad may have attractiveness that depends on the query terms. for
example, an ad for a used convertible would be more attractive if the
search query includes the term    convertible,    even though it might be a
valid response to queries that look for that make of car, without specifying
whether or not a convertible is wanted.

3. all ads deserve the opportunity to be shown until their click id203
can be approximated closely. if we start all ads out with a click id203
of 0, we shall never show them and thus never learn whether or not they
are attractive ads.

8.1.3

issues for display ads

this form of advertising on the web most resembles advertising in traditional
media. an ad for a chevrolet run in the pages of the new york times is a
display ad, and its e   ectiveness is limited. it may be seen by many people, but
most of them are not interested in buying a car, just bought a car, don   t drive,
or have another good reason to ignore the ad. yet the cost of printing the ad
was still borne by the newspaper and hence by the advertiser. an impression
of a similar ad on the yahoo! home page is going to be relatively ine   ective
for essentially the same reason. the fee for placing such an ad is typically a
fraction of a cent per impression.

the response of traditional media to this lack of focus was to create newspa-
pers or magazines for special interests. if you are a manufacturer of golf clubs,
running your ad in golf digest would give you an order-of-magnitude increase
in the id203 that the person seeing your ad would be interested in it. this
phenomenon explains the existence of many specialized, low-circulation maga-
zines. they are able to charge much more per impression for an ad than is a
general-purpose outlet such as a daily newspaper. the same phenomenon ap-
pears on the web. an ad for golf clubs on sports.yahoo.com/golf has much
more value per impression than does the same ad on the yahoo! home page or
an ad for chevrolets on the yahoo! golf page.

however, the web o   ers an opportunity to tailor display ads in a way that
hardcopy media cannot:
it is possible to use information about the user to
determine which ad they should be shown, regardless of what page they are
looking at.
if it is known that sally likes golf, then it makes sense to show
her an ad for golf clubs, regardless of what page she is looking at. we could
determine sally   s love for golf in various ways:

1. she may belong to a golf-related group on facebook.

2. she may mention    golf    frequently in emails posted on her gmail account.

284

chapter 8. advertising on the web

3. she may spend a lot of time on the yahoo! golf page.

4. she may issue search queries with golf-related terms frequently.

5. she may bookmark the web sites of one or more golf courses.

each of these methods, and many others like these, raise enormous privacy
issues. it is not the purpose of this book to try to resolve those issues, which
in practice probably have no solution that will satisfy all concerns. on the
one hand, people like the free services that have recently become advertising-
supported, and these services depend on advertising being much more e   ective
than conventional ads. there is a general agreement that, if there must be ads,
it is better to see things you might actually use than to have what pages you
view cluttered with irrelevancies. on the other hand, there is great potential
for misuse if the information leaves the realm of the machines that execute
advertising algorithms and get into the hands of real people.

8.2 on-line algorithms

before addressing the question of matching advertisements to search queries, we
shall digress slightly by examining the general class to which such algorithms
belong. this class is referred to as    on-line,    and they generally involve an ap-
proach called    greedy.    we also give, in the next section, a preliminary example
of an on-line greedy algorithm for a simpler problem: maximal matching.

8.2.1 on-line and o   -line algorithms

typical algorithms work as follows. all the data needed by the algorithm is
presented initially. the algorithm can access the data in any order. at the end,
the algorithm produces its answer. such an algorithm is called o   -line.

however, there are times when we cannot see all the data before our al-
gorithm must make some decisions. chapter 4 covered stream mining, where
we could store only a limited amount of the stream, and had to answer queries
about the entire stream when called upon to do so. there is an extreme form of
stream processing, where we must respond with an output after each stream ele-
ment arrives. we thus must decide about each stream element knowing nothing
at all of the future. algorithms of this class are called on-line algorithms.1

as the case in point, selecting ads to show with search queries would be
relatively simple if we could do it o   -line. we would see a month   s worth of
search queries, and look at the bids advertisers made on search terms, as well
as their advertising budgets for the month, and we could then assign ads to

1unfortunately, we are faced with another case of dual meanings, like the coincidence
involving the term    cluster    that we noted in section 7.6.6, where we needed to interpret
properly phrases such as    algorithms for computing clusters on computer clusters.    here, the
term    on-line    refers to the nature of the algorithm, and should not be confused with    on-line   
meaning    on the internet    in phrases such as    on-line algorithms for on-line advertising.   

8.2. on-line algorithms

285

the queries in a way that maximized both the revenue to the search engine and
the number of impressions that each advertiser got. the problem with o   -line
algorithms is that most queryers don   t want to wait a month to get their search
results.

thus, we must use an on-line algorithm to assign ads to search queries.
that is, when a search query arrives, we must select the ads to show with that
query immediately. we can use information about the past, e.g., we do not
have to show an ad if the advertiser   s budget has already been spent, and we
can examine the click-through rate (fraction of the time the ad is clicked on
when it is displayed) that an ad has obtained so far. however, we cannot use
anything about future search queries. for instance, we cannot know whether
there will be lots of queries arriving later and using search terms on which this
advertiser has made higher bids.

example 8.1 : let us take a very simple example of why knowing the future
could help. a manufacturer a of replica antique furniture has bid 10 cents
on the search term    chester   eld   .2 a more conventional manufacturer b has
bid 20 cents on both the terms    chester   eld    and    sofa.    both have monthly
budgets of $100, and there are no other bidders on either of these terms. it is
the beginning of the month, and a search query    chester   eld    has just arrived.
we are allowed to display only one ad with the query.

the obvious thing to do is to display b   s ad, because they bid more. how-
ever, suppose there will be lots of search queries this month for    sofa,    but very
few for    chester   eld.    then a will never spend its $100 budget, while b will
spend its full budget even if we give the query to a. speci   cally, if there will
be at least 500 more queries for either    sofa    or    chester   eld,    then there is
no harm, and potentially a bene   t, in giving the query to a. it will still be
possible for b to spend its entire budget, while we are increasing the amount of
a   s budget that will be spent. note that this argument makes sense both from
the point of view of the search engine, which wants to maximize total revenue,
and from the point of view of both a and b, who presumably want to get all
the impressions that their budgets allow.

if we could know the future, then we would know how many more    sofa   
queries and how many more    chester   eld    queries were going to arrive this
month. if that number is below 500, then we want to give the query to b to
maximize revenue, but if it is 500 or more, then we want to give it to a. since
we don   t know the future, an on-line algorithm cannot always do as well as an
o   -line algorithm.    

8.2.2 id192

many on-line algorithms are of the greedy algorithm type. these algorithms
make their decision in response to each input element by maximizing some
function of the input element and the past.

2a chester   eld is a type of sofa. see, for example, www.chesterfields.info.

286

chapter 8. advertising on the web

example 8.2 : the obvious greedy algorithm for the situation described in
example 8.1 is to assign a query to the highest bidder who still has budget left.
for the data of that example, what will happen is that the    rst 500    sofa    or
   chester   eld    queries will be assigned to b. at that time, b runs out of budget
and is assigned no more queries. after that, the next 1000    chester   eld    queries
are assigned to a, and    sofa    queries get no ad and therefore earn the search
engine no money.

the worst thing that can happen is that 500    chester   eld    queries arrive,
followed by 500    sofa    queries. an o   -line algorithm could optimally assign the
   rst 500 to a, earning $50, and the next 500 to b, earning $100, or a total
of $150. however, the greedy algorithm will assign the    rst 500 to b, earning
$100, and then has no ad for the next 500, earning nothing.    

8.2.3 the competitive ratio

as we see from example 8.2, an on-line algorithm need not give as good a result
as the best o   -line algorithm for the same problem. the most we can expect is
that there will be some constant c less than 1, such that on any input, the result
of a particular on-line algorithm is at least c times the result of the optimum
o   -line algorithm. the constant c, if it exists, is called the competitive ratio for
the on-line algorithm.

example 8.3 : the greedy algorithm, on the particular data of example 8.2,
gives a result that is 2/3 as good as that of the optimum algorithm: $100 versus
$150. that proves that the competitive ratio is no greater than 2/3. but it
could be less. the competitive ratio for an algorithm may depend on what kind
of data is allowed to be input to the algorithm. even if we restrict inputs to
the situation described in example 8.2, but with the bids allowed to vary, then
we can show the greedy algorithm has a competitive ratio no greater than 1/2.
just raise the bid by a to    less than 20 cents. as    approaches 0, the greedy
algorithm still produces only $100, but the return from the optimum algorithm
approaches $200. we can show that it is impossible to do worse than half the
optimum in this simple case, so the competitive ratio is indeed 1/2. however,
we   ll leave this sort of proof for later sections.    

8.2.4 exercises for section 8.2

! exercise 8.2.1 : a popular example of the design of an on-line algorithm to
minimize the competitive ratio is the ski-buying problem.3 suppose you can buy
skis for $100, or you can rent skis for $10 per day. you decide to take up skiing,
but you don   t know if you will like it. you may try skiing for any number of
days and then give it up. the merit of an algorithm is the cost per day of skis,
and we must try to minimize this cost.

3thanks to anna karlin for this example.

8.3. the matching problem

287

one on-line algorithm for making the rent/buy decision is    buy skis im-
mediately.    if you try skiing once, fall down and give it up, then this on-line
algorithm costs you $100 per day, while the optimum o   -line algorithm would
have you rent skis for $10 for the one day you used them. thus, the competitive
ratio of the algorithm    buy skis immediately    is at most 1/10th, and that is
in fact the exact competitive ratio, since using the skis one day is the worst
possible outcome for this algorithm. on the other hand, the on-line algorithm
   always rent skis    has an arbitrarily small competitive ratio. if you turn out to
really like skiing and go regularly, then after n days, you will have paid $10n or
$10/day, while the optimum o   -line algorithm would have bought skis at once,
and paid only $100, or $100/n per day.

your question: design an on-line algorithm for the ski-buying problem that
has the best possible competitive ratio. what is that competitive ratio? hint :
since you could, at any time, have a fall and decide to give up skiing, the only
thing the on-line algorithm can use in making its decision is how many times
previously you have gone skiing.

8.3 the matching problem

we shall now take up a problem that is a simpli   ed version of the problem of
matching ads to search queries. this problem, called    maximal matching,    is
an abstract problem involving bipartite graphs (graphs with two sets of nodes    
left and right     with all edges connecting a node in the left set to a node in the
right set. figure 8.1 is an example of a bipartite graph. nodes 1, 2, 3, and 4
form the left set, while nodes a, b, c, and d form the right set.

8.3.1 matches and perfect matches

suppose we are given a bipartite graph. a matching is a subset of the edges
such that no node is an end of two or more edges. a matching is said to be
perfect if every node appears in the matching. note that a matching can only
be perfect if the left and right sets are of the same size. a matching that is as
large as any other matching for the graph in question is said to be maximal.

example 8.4 : the set of edges {(1, a), (2, b), (3, d)} is a matching for the
bipartite graph of fig. 8.1. each member of the set is an edge of the bipartite
graph, and no node appears more than once. the set of edges

{(1, c), (2, b), (3, d), (4, a)}

is a perfect matching, represented by heavy lines in fig. 8.2. every node appears
exactly once. it is, in fact, the sole perfect matching for this graph, although
some bipartite graphs have more than one perfect matching. the matching of
fig. 8.2 is also maximal, since every perfect matching is maximal.    

288

chapter 8. advertising on the web

1

2

3

4

a

b

c

d

figure 8.1: a bipartite graph

8.3.2 the greedy algorithm for maximal matching

o   -line algorithms for    nding a maximal matching have been studied for dec-
ades, and one can get very close to o(n2) for an n-node graph. on-line algo-
rithms for the problem have also been studied, and it is this class of algorithms
we shall consider here. in particular, the greedy algorithm for maximal match-
ing works as follows. we consider the edges in whatever order they are given.
when we consider (x, y), add this edge to the matching if neither x nor y are
ends of any edge selected for the matching so far. otherwise, skip (x, y).

example 8.5 : let us consider a greedy match for the graph of fig. 8.1. sup-
pose we order the nodes lexicographically, that is, by order of their left node,
breaking ties by the right node. then we consider the edges in the order (1, a),
(1, c), (2, b), (3, b), (3, d), (4, a). the    rst edge, (1, a), surely becomes part of the
matching. the second edge, (1, c), cannot be chosen, because node 1 already
appears in the matching. the third edge, (2, b), is selected, because neither
node 2 nor node b appears in the matching so far. edge (3, b) is rejected for
the match because b is already matched, but then (3, d) is added to the match
because neither 3 nor d has been matched so far. finally, (4, a) is rejected
because a appears in the match. thus, the matching produced by the greedy
algorithm for this ordering of the edges is {(1, a), (2, b), (3, d)}. as we saw,
this matching is not maximal.    

example 8.6 : a greedy match can be even worse than that of example 8.5.
on the graph of fig. 8.1, any ordering that begins with the two edges (1, a)
and (3, b), in either order, will match those two pairs but then will be unable
to match nodes 2 or 4. thus, the size of the resulting match is only 2.    

8.3. the matching problem

289

1

2

3

4

a

b

c

d

figure 8.2: the only perfect matching for the graph of fig. 8.1

8.3.3 competitive ratio for greedy matching

we can show a competitive ratio of 1/2 for the greedy matching algorithm of
section 8.3.2. first, the ratio cannot be more than 1/2. we already saw that
for the graph of fig. 8.1, there is a perfect matching of size 4. however, if
the edges are presented in any of the orders discussed in example 8.6, the size
of the match is only 2, or half the optimum. since the competitive ratio for
an algorithm is the minimum over all possible inputs of the ratio of what that
algorithm achieves to the optimum result, we see that 1/2 is an upper bound
on the competitive ratio.

suppose mo is a maximal matching, and mg is the matching that the greedy
algorithm produces. let l be the set of left nodes that are matched in mo but
not in mg. let r be the set of right nodes that are connected by edges to any
node in l. we claim that every node in r is matched in mg. suppose not;
in particular, suppose node r in r is not matched in mg. then the greedy
algorithm will eventually consider some edge (   , r), where     is in l. at that
time, neither end of this edge is matched, because we have supposed that neither
    nor r is ever matched by the greedy algorithm. that observation contradicts
the de   nition of how the greedy algorithm works; that is, the greedy algorithm
would indeed match (   , r). we conclude that every node in r is matched in
mg.

now, we know several things about the sizes of sets and matchings.

1. |mo|     |mg| +|l|, since among the nodes on the left, only nodes in l can

be matched in mo but not mg.

2. |l|     |r|, because in mo, all the nodes in l were matched.

290

chapter 8. advertising on the web

3. |r|     |mg|, because every node in r is matched in mg.
now, (2) and (3) give us |l|     |mg|. that, together with (1), gives us
|mo|     2|mg|, or |mg|     1
2|mo|. the latter inequality says that the competitive
ratio is at least 1/2. since we already observed that the competitive ratio is no
more than 1/2, we now conclude the ratio is exactly 1/2.

8.3.4 exercises for section 8.3

exercise 8.3.1 : de   ne the graph gn to have the 2n nodes

a0, a1, . . . , an   1, b0, b1, . . . , bn   1

and the following edges. each node ai, for i = 0, 1, . . . , n     1, is connected to
the nodes bj and bk, where

j = 2i mod n and k = (2i + 1) mod n

for instance, the graph g4 has the following edges: (a0, b0), (a0, b1), (a1, b2),
(a1, b3), (a2, b0), (a2, b1), (a3, b2), and (a3, b3).

(a) find a perfect matching for g4.

(b) find a perfect matching for g5.

!! (c) prove that for every n, gn has a perfect matching.

! exercise 8.3.2 : how many perfect matchings do the graphs g4 and g5 of

exercise 8.3.1 have?

! exercise 8.3.3 : whether or not the greedy algorithm gives us a perfect match-
ing for the graph of fig. 8.1 depends on the order in which we consider the edges.
of the 6! possible orders of the six edges, how many give us a perfect match-
ing? give a simple test for distinguishing those orders that do give the perfect
matching from those that do not.

8.4 the adwords problem

we now consider the fundamental problem of search advertising, which we term
the    adwords problem,    because it was    rst encountered in the google adwords
system. we then discuss a greedy algorithm called    balance    that o   ers a good
competitive ratio. we analyze this algorithm for a simpli   ed case of the adwords
problem.

8.4. the adwords problem

291

8.4.1 history of search advertising

around the year 2000, a company called overture (later bought by yahoo!)
introduced a new kind of search. advertisers bid on keywords (words in a
search query), and when a user searched for that keyword, the links to all the
advertisers who bid on that keyword are displayed in the order highest-bid-   rst.
if the advertiser   s link was clicked on, they paid what they had bid.

that sort of search was very useful for the case where the search queryer
really was looking for advertisements, but it was rather useless for the queryer
who was just looking for information. recall our discussion in section 5.1.1
about the point that unless a search engine can provide reliable responses to
queries that are for general information, no one will want to use the search
engine when they are looking to buy something.

several years later, google adapted the idea in a system called adwords. by
that time, the reliability of google was well established, so people were willing
to trust the ads they were shown. google kept the list of responses based on
id95 and other objective criteria separate from the list of ads, so the same
system was useful for the queryer who just wanted information as well as the
queryer looking to buy something.

the adwords system went beyond the earlier system in several ways that

made the selection of ads more complex.

1. google would show only a limited number of ads with each query. thus,
while overture simply ordered all ads for a given keyword, google had to
decide which ads to show, as well as the order in which to show them.

2. users of the adwords system speci   ed a budget: the amount they were
willing to pay for all clicks on their ads in a month. these constraints
make the problem of assigning ads to search queries more complex, as we
hinted at in example 8.1.

3. google did not simply order ads by the amount of the bid, but by the
amount they expected to receive for display of each ad. that is, the click-
through rate was observed for each ad, based on the history of displays of
that ad. the value of an ad was taken to be the product of the bid and
the click-through rate.

8.4.2 de   nition of the adwords problem

of course, the decision regarding which ads to show must be made on-line.
thus, we are only going to consider on-line algorithms for solving the adwords
problem, which is as follows.

    given:

1. a set of bids by advertisers for search queries.

2. a click-through rate for each advertiser-query pair.

292

chapter 8. advertising on the web

3. a budget for each advertiser. we shall assume budgets are for a

month, although any unit of time could be used.

4. a limit on the number of ads to be displayed with each search query.

    respond to each search query with a set of advertisers such that:

1. the size of the set is no larger than the limit on the number of ads

per query.

2. each advertiser has bid on the search query.

3. each advertiser has enough budget left to pay for the ad if it is

clicked upon.

the revenue of a selection of ads is the total value of the ads selected, where
the value of an ad is the product of the bid and the click-through rate for the
ad and query. the merit of an on-line algorithm is the total revenue obtained
over a month (the time unit over which budgets are assumed to apply). we
shall try to measure the competitive ratio for algorithms, that is, the minimum
total revenue for that algorithm, on any sequence of search queries, divided by
the revenue of the optimum o   -line algorithm for the same sequence of search
queries.

8.4.3 the greedy approach to the adwords problem

since only an on-line algorithm is suitable for the adwords problem, we should
   rst examine the performance of the obvious greedy algorithm. we shall make
a number of simpli   cations to the environment; our purpose is to show even-
tually that there is a better algorithm than the obvious greedy algorithm. the
simpli   cations:

(a) there is one ad shown for each query.

(b) all advertisers have the same budget.

(c) all click-through rates are the same.

(d) all bids are either 0 or 1. alternatively, we may assume that the value of

each ad (product of bid and click-through rate) is the same.

the greedy algorithm picks, for each search query, any advertiser who has
bid 1 for that query. the competitive ratio for this algorithm is 1/2, as the
following example shows.

example 8.7 : suppose there are two advertisers a and b, and only two
possible queries, x and y. advertiser a bids only on x, while b bids on both
x and y. the budget for each advertiser is 2. notice the similarity to the
situation in example 8.1; the only di   erences are the fact that the bids by each
advertiser are the same and the budgets are smaller here.

8.4. the adwords problem

293

adwords aspects not in our model

there are several ways in which the real adwords system di   ers from the
simpli   ed model of this section.

matching bids and search queries: in our simpli   ed model, advertis-
ers bid on sets of words, and an advertiser   s bid is eligible to be shown for
search queries that have exactly the same set of words as the advertiser   s
bid. in reality, google, yahoo!, and microsoft all o   er advertisers a feature
known as broad matching, where an ad is eligible to be shown for search
queries that are inexact matches of the bid keywords. examples include
queries that include a subset or superset of keywords, and also queries
that use words with very similar meanings to the words the advertiser bid
on. for such broad matches, search engines charge the advertiser based on
complicated formulas taking into account how closely related the search
query is to the advertiser   s bid. these formulas vary across search engines
and are not made public.

charging advertisers for clicks: in our simpli   ed model, when a user
clicks on an advertiser   s ad, the advertiser is charged the amount they
bid. this policy is known as a    rst-price auction.
in reality, search
engines use a more complicated system known as a second-price auction,
where each advertiser pays approximately the bid of the advertiser who
placed immediately behind them in the auction. for example, the    rst-
place advertiser for a search might pay the bid of the advertiser in second
place, plus one cent. it has been shown that second-price auctions are less
susceptible to being gamed by advertisers than    rst-price auctions and
lead to higher revenues for the search engine.

let the sequence of queries be xxyy. the greedy algorithm is able to allocate
the    rst two x   s to b, whereupon there is no one with an unexpended budget to
pay for the two y   s. the revenue for the greedy algorithm in this case is thus 2.
however, the optimum o   -line algorithm will allocate the x   s to a and the y   s
to b, achieving a revenue of 4. the competitive ratio for the greedy algorithm
is thus no more than 1/2. we can argue that on any sequence of queries the
ratio of the revenues for the greedy and optimal algorithms is at least 1/2, using
essentially the same idea as in section 8.3.3.    

8.4.4 the balance algorithm

there is a simple improvement to the greedy algorithm that gives a competitive
ratio of 3/4 for the simple case of section 8.4.3. this algorithm, called the
balance algorithm, assigns a query to the advertiser who bids on the query and
has the largest remaining budget. ties may be broken arbitrarily.

294

chapter 8. advertising on the web

example 8.8 : consider the same situation as in example 8.7. the balance
algorithm can assign the    rst query x to either a or b, because they both bid
on x and their remaining budgets are the same. however, the second x must be
assigned to the other of a and b, because they then have the larger remaining
budget. the    rst y is assigned to b, since it has budget remaining and is the
only bidder on y. the last y cannot be assigned, since b is out of budget, and
a did not bid. thus, the total revenue for the balance algorithm on this data
is 3.
in comparison, the total revenue for the optimum o   -line algorithm is
4, since it can assign the x   s to a and the y   s to b. our conclusion is that,
for the simpli   ed adwords problem of section 8.4.3, the competitive ratio of
the balance algorithm is no more than 3/4. we shall see next that with only
two advertisers, 3/4 is exactly the competitive ratio, although as the number
of advertisers grows, the competitive ratio lowers to 0.63 (actually 1    1/e) but
no lower.    

8.4.5 a lower bound on competitive ratio for balance

in this section we shall prove that in the simple case of the balance algorithm
that we are considering, the competitive ratio is 3/4. given example 8.8, we
have only to prove that the total revenue obtained by the balance algorithm is
at least 3/4 of the revenue for the optimum o   -line algorithm. thus, consider a
situation in which there are two advertisers, a1 and a2, each with a budget of
b. we shall assume that each query is assigned to an advertiser by the optimum
algorithm. if not, we can delete those queries without a   ecting the revenue of
the optimum algorithm and possibly reducing the revenue of balance. thus, the
lowest possible competitive ratio is achieved when the query sequence consists
only of ads assigned by the optimum algorithm.

we shall also assume that both advertisers    budgets are consumed by the
optimum algorithm. if not, we can reduce the budgets, and again argue that
the revenue of the optimum algorithm is not reduced while that of balance can
only shrink. that change may force us to use di   erent budgets for the two
advertisers, but we shall continue to assume the budgets are both b. we leave
as an exercise the extension of the proof to the case where the budgets of the
two advertisers are di   erent.

figure 8.3 suggests how the 2b queries are assigned to advertisers by the
two algorithms. in (a) we see that b queries are assigned to each of a1 and a2
by the optimum algorithm. now, consider how these same queries are assigned
by balance. first, observe that balance must exhaust the budget of at least one
of the advertisers, say a2. if not, then there would be some query assigned to
neither advertiser, even though both had budget. we know at least one of the
advertisers bids on each query, because that query is assigned in the optimum
algorithm. that situation contradicts how balance is de   ned to operate; it
always assigns a query if it can.

thus, we see in fig. 8.3(b) that a2 is assigned b queries. these queries
could have been assigned to either a1 or a2 by the optimum algorithm. we

8.4. the adwords problem

295

b

b

x

a 1

a 2

(a) optimum

x

y

a 1

a 2

not
used

(b) balance

figure 8.3: illustration of the assignments of queries to advertisers in the opti-
mum and balance algorithms

also see in fig. 8.3(b) that we use y as the number of queries assigned to a1 and
x as b     y. it is our goal to show y     x. that inequality will show the revenue
of balance is at least 3b/2, or 3/4th the revenue of the optimum algorithm.
we note that x is also the number of unassigned queries for the balance
algorithm, and that all the unassigned queries must have been assigned to a2
by the optimum algorithm. the reason is that a1 never runs out of budget,
so any query assigned by the optimum algorithm to a1 is surely bid on by a1.
since a1 always has budget during the running of the balance algorithm, that
algorithm will surely assign this query, either to a1 or to a2.

there are two cases, depending on whether more of the queries that are
assigned to a1 by the optimum algorithm are assigned to a1 or a2 by balance.

1. suppose at least half of these queries are assigned by balance to a1. then

y     b/2, so surely y     x.

2. suppose more than half of these queries are assigned by balance to a2.
consider the last of these queries q that is assigned to a2 by the balance
algorithm. at that time, a2 must have had at least as great a budget

296

chapter 8. advertising on the web

available as a1, or else balance would have assigned query q to a1, just
as the optimum algorithm did. since more than half of the b queries that
the optimum algorithm assigns to a1 are assigned to a2 by balance, we
know that just before q was assigned, the remaining budget of a2 was at
most b/2. therefore, at that time, the remaining budget of a1 was also
at most b/2. since budgets only decrease, we know that x     b/2. it
follows that y     x, since x + y = b.

we conclude that y     x in either case, so the competitive ratio of the balance
algorithm is 3/4.

8.4.6 the balance algorithm with many bidders

when there are many advertisers, the competitive ratio for the balance algo-
rithm can be under 3/4, but not too far below that fraction. the worst case
for balance is as follows.

1. there are n advertisers, a1, a2, . . . , an .

2. each advertiser has a budget b = n !.

3. there are n queries q1, q2, . . . , qn .

4. advertiser ai bids on queries q1, q2, . . . , qi and no other queries.

5. the query sequence consists of n rounds. the ith round consists of b

occurrences of query qi and nothing else.

the optimum o   -line algorithm assigns the b queries qi in the ith round to
ai for all i. thus, all queries are assigned to a bidder, and the total revenue of
the optimum algorithm is n b.

however, the balance algorithm assigns each of the queries in round 1 to
the n advertisers equally, because all bid on q1, and the balance algorithm
prefers the bidder with the greatest remaining budget. thus, each advertiser
gets b/n of the queries q1. now consider the queries q2 in round 2. all but a1
bid on these queries, so they are divided equally among a2 through an , with
each of these n     1 bidders getting b/(n     1) queries. the pattern, suggested
by fig. 8.4, repeats for each round i = 3, 4, . . ., with ai through an getting
b/(n     i + 1) queries.
exhausted. that will happen at the lowest round j such that

however, eventually, the budgets of the higher-numbered advertisers will be

that is,

b(cid:16) 1

n

+

1

n     1

+        +

1

n     j + 1(cid:17)     b

1
n

+

1

n     1

+        +

1

n     j + 1     1

8.4. the adwords problem

297

. . .

b / ( n    2)

b / ( n    1)

b / n

a 1

a 2

a 3

a n   1

a n

figure 8.4: apportioning queries to n advertisers in the worst case

i=1 1/i approaches loge k. using this

euler showed that as k gets large, pk
observation, we can approximate the above sum as loge n     loge(n     j).
we are thus looking for the j such that loge n     loge(n     j) = 1, approxi-
mately. if we replace loge n     loge(n     j) by the equivalent loge(cid:0)n/(n     j)(cid:1)
and exponentiate both sides of the equation loge(cid:0)n/(n     j)(cid:1) = 1, we get
n/(n     j) = e. solving this equation for j, we get

j = n(cid:16)1    

1

e(cid:17)

as the approximate value of j for which all advertisers are either out of budget
or do not bid on any of the remaining queries. thus, the approximate revenue
obtained by the balance algorithm is bn (1     1
e ), that is, the queries of the
   rst j rounds. therefore, the competitive ratio is 1     1
e , or approximately 0.63.

8.4.7 the generalized balance algorithm

the balance algorithm works well when all bids are 0 or 1. however, in practice,
bids can be arbitrary, and with arbitrary bids and budgets balance fails to
weight the sizes of the bids properly. the following example illustrates the
point.

example 8.9 : suppose there are two advertisers a1 and a2, and one query
q. the bids on q and budgets are:

bidder bid budget

a1
a2

1
10

110
100

if there are 10 occurrences of q, the optimum o   -line algorithm will assign
them all to a2 and gain revenue 100. however, because a1   s budget is larger,
balance will assign all ten queries to a1 for a revenue of 10. in fact, one can
extend this idea easily to show that for situations like this one, there is no
competitive ratio higher than 0 that holds for the balance algorithm.    

298

chapter 8. advertising on the web

in order to make balance work in more general situations, we need to make
two modi   cations. first, we need to bias the choice of ad in favor of higher bids.
second, we need to be less absolute about the remaining budget. rather, we
consider the fraction of the budgets remaining, so we are biased toward using
some of each advertiser   s budget. the latter change will make the balance
algorithm more    risk averse   ; it will not leave too much of any advertiser   s
budget unused. it can be shown (see the chapter references) that the following
generalization of the balance algorithm has a competitive ratio of 1     1/e =
0.63.

    suppose that a query q arrives, advertiser ai has bid xi for this query
(note that xi could be 0). also, suppose that fraction fi of the budget
of ai is currently unspent. let   i = xi(1     e   fi). then assign q to the
advertiser ai such that   i is a maximum. break ties arbitrarily.

example 8.10 : consider how the generalized balance algorithm would work
on the data of example 8.9. for the    rst occurrence of query q,

since a1 has bid 1, and fraction 1 of a1   s budget remains. that is,

  1 = 1    (1     e   1)

  1 = 1     1/e = 0.63

on the other hand,   2 = 10    (1     e   1) = 6.3. thus, the    rst q is awarded to
a2.
the same thing happens for each of the q   s. that is,   1 stays at 0.63, while
  2 decreases. however, it never goes below 0.63. even for the 10th q, when
90% of a2   s budget has already been used,   2 = 10    (1     e   1/10). recall
(section 1.3.5) the taylor expansion for ex = 1 + x + x2/2! + x3/3! +       . thus,

e   1/10 = 1    

1
10

+

1
200    

1

6000

+       

or approximately, e   1/10 = 0.905. thus,   2 = 10    0.095 = 0.95.    

we leave unproved the assertion that the competitive ratio for this algorithm
is 1     1/e. we also leave unproved an additional surprising fact: no on-line
algorithm for the adwords problem as described in this section can have a
competitive ratio above 1     1/e.

8.4.8 final observations about the adwords problem

the balance algorithm, as described, does not take into account the possibility
that the click-through rate di   ers for di   erent ads.
it is simple to multiply
the bid by the click-through rate when computing the   i   s, and doing so will
maximize the expected revenue. we can even incorporate information about

8.5. adwords implementation

299

the click-through rate for each ad on each query for which a nonzero amount
has been bid. when faced with the problem of allocating a particular query q,
we incorporate a factor that is the click-through rate for that ad on query q,
when computing each of the      s.

another issue we must consider in practice is the historical frequency of
queries. if, for example, we know that advertiser ai has a budget su   ciently
small that there are sure to be enough queries coming later in the month to
satisfy ai   s demand, then there is no point in boosting   i if some of ai   s budget
has already been expended. that is, maintain   i = xi(1     e   1) as long as we
can expect that there will be enough queries remaining in the month to give ai
its full budget of ads. this change can cause balance to perform worse if the
sequence of queries is governed by an adversary who can control the sequence
of queries. such an adversary can cause the queries ai bid on suddenly to
disappear. however, search engines get so many queries, and their generation
is so random, that it is not necessary in practice to imagine signi   cant deviation
from the norm.

8.4.9 exercises for section 8.4

exercise 8.4.1 : using the simplifying assumptions of example 8.7, suppose
that there are three advertisers, a, b, and c. there are three queries, x, y, and
z. each advertiser has a budget of 2. advertiser a bids only on x; b bids on x
and y, while c bids on x, y, and z. note that on the query sequence xxyyzz,
the optimum o   -line algorithm would yield a revenue of 6, since all queries can
be assigned.

! (a) show that the greedy algorithm will assign at least 4 of these 6 queries.

!! (b) find another sequence of queries such that the greedy algorithm can as-
sign as few as half the queries that the optimum o   -line algorithm assigns
on that sequence.

!! exercise 8.4.2 : extend the proof of section 8.4.5 to the case where the two

advertisers have unequal budgets.

! exercise 8.4.3 : show how to modify example 8.9 by changing the bids and/or

budgets to make the competitive ratio come out as close to 0 as you like.

8.5 adwords implementation

while we should now have an idea of how ads are selected to go with the answer
to a search query, we have not addressed the problem of    nding the bids that
have been made on a given query. as long as bids are for the exact set of
words in a query, the solution is relatively easy. however, there are a number of
extensions to the query/bid matching process that are not as simple. we shall
explain the details in this section.

300

chapter 8. advertising on the web

8.5.1 matching bids and search queries

as we have described the adwords problem, and as it normally appears in
practice, advertisers bid on sets of words.
if a search query occurs having
exactly that set of words in some order, then the bid is said to match the
query, and it becomes a candidate for selection. we can avoid having to deal
with word order by storing all sets of words representing a bid in lexicographic
(alphabetic) order. the list of words in sorted order forms the hash-key for the
bid, and these bids may be stored in a hash table used as an index, as discussed
in section 1.3.2.

search queries also have their words sorted prior to lookup. when we hash
the sorted list, we    nd in the hash table all the bids for exactly that set of words.
they can be retrieved quickly, since we have only to look at the contents of that
bucket.

moreover, there is a good chance that we can keep the entire hash table in
main memory. if there are a million advertisers, each bidding on 100 queries,
and the record of the bid requires 100 bytes, then we require ten gigabytes of
main memory, which is well within the limits of what is feasible for a single
machine. if more space is required, we can split the buckets of the hash table
among as many machines as we need. search queries can be hashed and sent
to the appropriate machine.

in practice, search queries may be arriving far too rapidly for a single ma-
chine, or group of machines that collaborate on a single query at a time, to
handle them all. in that case, the stream of queries is split into as many pieces
as necessary, and each piece is handled by one group of machines. in fact, an-
swering the search query, independent of ads, will require a group of machines
working in parallel anyway, in order that the entire processing of a query can
be done in main memory.

8.5.2 more complex matching problems

however, the potential for matching bids to objects is not limited to the case
where the objects are search queries and the match criterion is same-set-of-
words. for example, google also matches adwords bids to emails. there, the
match criterion is not based on the equality of sets. rather, a bid on a set of
words s matches an email if all the words in s appear anywhere in the email.
this matching problem is much harder. we can still maintain a hash-table
index for the bids, but the number of subsets of words in a hundred-word email
is much too large to look up all the sets, or even all the small sets of (say)
three or fewer words. there are a number of other potential applications of
this sort of matching that, at the time of this writing, are not implemented
but could be. they all involve standing queries     queries that users post to a
site, expecting the site to notify them whenever something matching the query
becomes available at the site. for example:

1. twitter allows one to follow all the    tweets    of a given person. however,

8.5. adwords implementation

301

it is feasible to allow users to specify a set of words, such as

ipod free music

and see all the tweets where all these words appear, not necessarily in
order, and not necessarily adjacent.

2. on-line news sites often allow users to select from among certain key-
words or phrases, e.g.,    healthcare    or    barack obama,    and receive
alerts whenever a new news article contains that word or consecutive
sequence of words. this problem is simpler than the email/adwords prob-
lem for several reasons. matching single words or consecutive sequences
of words, even in a long article, is not as time-consuming as matching
small sets of words. further, the sets of terms that one can search for
is limited, so there aren   t too many    bids.    even if many people want
alerts about the same term, only one index entry, with the list of all those
people associated, is required. however, a more advanced system could
allow users to specify alerts for sets of words in a news article, just as the
adwords system allows anyone to bid on a set of words in an email.

8.5.3 a matching algorithm for documents and bids

we shall o   er an algorithm that will match many    bids    against many    docu-
ments.    as before, a bid is a (typically small) set of words. a document is a
larger set of words, such as an email, tweet, or news article. we assume there
may be hundreds of documents per second arriving, although if there are that
many, the document stream may be split among many machines or groups of
machines. we assume there are many bids, perhaps on the order of a hundred
million or a billion. as always, we want to do as much in main memory as we
can.

we shall, as before, represent a bid by its words listed in some order. there
are two new elements in the representation. first, we shall include a status with
each list of words. the status is an integer indicating how many of the    rst
words on the list have been matched by the current document. when a bid is
stored in the index, its status is always 0.

second, while the order of words could be lexicographic, we can lower the
amount of work by ordering words rarest-   rst. however, since the number
of di   erent words that can appear in emails is essentially unlimited, it is not
feasible to order all words in this manner. as a compromise, we might identify
the n most common words on the web or in a sample of the stream of documents
we are processing. here, n might be a hundred thousand or a million. these
n words are sorted by frequency, and they occupy the end of the list, with the
most frequent words at the very end. all words not among the n most frequent
can be assumed equally infrequent and ordered lexicographically. then, the
words of any document can be ordered. if a word does not appear on the list
of n frequent words, place it at the front of the order, lexicographically. those

302

chapter 8. advertising on the web

words in the document that do appear on the list of most frequent words appear
after the infrequent words, in the reverse order of frequency (i.e., with the most
frequent words of the documents ordered last).

example 8.11 : suppose our document is

   twas brillig, and the slithy toves

   the    is the most frequent word in english, and    and    is only slightly less
frequent. let us suppose that    twas    makes the list of frequent words, although
its frequency is surely lower than that of    the    or    and.    the other words do
not make the list of frequent words.

then the end of the list consists of    twas,       and,    and    the,    in that order,
since that is the inverse order of frequency. the other three words are placed
at the front of the list in lexicographic order. thus,

brillig slithy toves twas and the

is the sequence of words in the document, properly ordered.    

the bids are stored in a hash-table, whose hash key is the    rst word of
the bid, in the order explained above. the record for the bid will also include
information about what to do when the bid is matched. the status is 0 and
need not be stored explicitly. there is another hash table, whose job is to
contain copies of those bids that have been partially matched. these bids have
a status that is at least 1, but less than the number of words in the set.
if
the status is i, then the hash-key for this hash table is the (i + 1)st word. the
arrangement of hash tables is suggested by fig. 8.5. to process a document,
do the following.

1. sort the words of the document in the order discussed above. eliminate

duplicate words.

2. for each word w, in the sorted order:

(i) using w as the hash-key for the table of partially matched bids,    nd

those bids having w as key.

(ii) for each such bid b, if w is the last word of b, move b to the table of

matched bids.

(iii) if w is not the last word of b, add 1 to b   s status, and rehash b using
the word whose position is one more than the new status, as the
hash-key.

(iv) using w as the hash key for the table of all bids,    nd those bids for

which w is their    rst word in the sorted order.

(v) for each such bid b, if there is only one word on its list, copy it to

the table of matched bids.

8.6. summary of chapter 8

303

(d) look up
bids with
hash   key w

word w

hash   table
index of

bids

(f) copy with
status 1 if more
than one word

(e) copy to
output if w

is the only word

hash on second word

(a) look up
bids with
hash   key 

w

partially
matched

bids

list of
matched

bids

(b) move to
output if
w
is the last word

(c) rehash
w
with one higher

status if 
is not last

w

figure 8.5: managing large numbers of bids and large numbers of documents

(vi) if b consists of more than one word, add it, with status 1, to the
table of partially matched bids, using the second word of b as the
hash-key.

3. produce the list of matched bids as the output.

the bene   t of the rarest-   rst order should now be visible. a bid is only
copied to the second hash table if its rarest word appears in the document. in
comparison, if lexicographic order was used, more bids would be copied to the
second hash table. by minimizing the size of that table, we not only reduce the
amount of work in steps 2(i)   2(iii), but we make it more likely that this entire
table can be kept in main memory.

8.6 summary of chapter 8

    targeted advertising: the big advantage that web-based advertising has
over advertising in conventional media such as newspapers is that web
advertising can be selected according to the interests of each individual
user. this advantage has enabled many web services to be supported
entirely by advertising revenue.

    on- and o   -line algorithms: conventional algorithms that are allowed
to see all their data before producing an answer are called o   -line. an on-

304

chapter 8. advertising on the web

line algorithm is required to make a response to each element in a stream
immediately, with knowledge of only the past, not the future elements in
the stream.

    id192: many on-line algorithms are greedy, in the sense that
they select their action at every step by minimizing some objective func-
tion.

    competitive ratio: we can measure the quality of an on-line algorithm
by minimizing, over all possible inputs, the value of the result of the on-
line algorithm compared with the value of the result of the best possible
o   -line algorithm.

    bipartite matching: this problem involves two sets of nodes and a set of
edges between members of the two sets. the goal is to    nd a maximal
matching     as large a set of edges as possible that includes no node more
than once.

    on-line solution to the matching problem: one greedy algorithm for
   nding a match in a bipartite graph (or any graph, for that matter) is
to order the edges in some way, and for each edge in turn, add it to the
match if neither of its ends are yet part of an edge previously selected for
the match. this algorithm can be proved to have a competitive ratio of
1/2; that is, it never fails to match at least half as many nodes as the best
o   -line algorithm matches.

    search ad management : a search engine receives bids from advertisers
on certain search queries. some ads are displayed with each search query,
and the search engine is paid the amount of the bid only if the queryer
clicks on the ad. each advertiser can give a budget, the total amount
they are willing to pay for clicks in a month.

    the adwords problem: the data for the adwords problem is a set of bids
by advertisers on certain search queries, together with a total budget for
each advertiser and information about the historical click-through rate for
each ad for each query. another part of the data is the stream of search
queries received by the search engine. the objective is to select on-line
a    xed-size set of ads in response to each query that will maximize the
revenue to the search engine.

    simpli   ed adwords problem: to see some of the nuances of ad selection,
we considered a simpli   ed version in which all bids are either 0 or 1,
only one ad is shown with each query, and all advertisers have the same
budget. under this model the obvious greedy algorithm of giving the ad
placement to anyone who has bid on the query and has budget remaining
can be shown to have a competitive ratio of 1/2.

8.7. references for chapter 8

305

    the balance algorithm: this algorithm improves on the simple greedy
algorithm. a query   s ad is given to the advertiser who has bid on the query
and has the largest remaining budget. ties can be broken arbitrarily.

    competitive ratio of the balance algorithm: for the simpli   ed adwords
model, the competitive ratio of the balance algorithm is 3/4 for the case
of two advertisers and 1   1/e, or about 63% for any number of advertisers.
    the balance algorithm for the generalized adwords problem: when bid-
ders can make di   ering bids, have di   erent budgets, and have di   erent
click-through rates for di   erent queries, the balance algorithm awards an
ad to the advertiser with the highest value of the function    = x(1   e   f ).
here, x is the product of the bid and the click-through rate for that ad-
vertiser and query, and f is the fraction of the advertiser   s budget that
remains unspent.

    implementing an adwords algorithm: the simplest version of the imple-
mentation serves in situations where the bids are on exactly the set of
words in the search query. we can represent a query by the list of its
words, in sorted order. bids are stored in a hash table or similar struc-
ture, with a hash key equal to the sorted list of words. a search query can
then be matched against bids by a straightforward lookup in the table.

    matching word sets against documents: a harder version of the ad-
words-implementation problem allows bids, which are still small sets of
words as in a search query, to be matched against larger documents, such
as emails or tweets. a bid set matches the document if all the words
appear in the document, in any order and not necessarily adjacent.

    hash storage of word sets: a useful data structure stores the words of
each bid set in the order rarest-   rst. documents have their words sorted
in the same order. word sets are stored in a hash table with the    rst
word, in the rarest-   rst order, as the key.

    processing documents for bid matches: we process the words of the
document rarest-   rst. word sets whose    rst word is the current word
are copied to a temporary hash table, with the second word as the key.
sets already in the temporary hash table are examined to see if the word
that is their key matches the current word, and, if so, they are rehashed
using their next word. sets whose last word is matched are copied to the
output.

8.7 references for chapter 8

[1] is an investigation of the way ad position in   uences the click-through rate.
the balance algorithm was developed in [2] and its application to the ad-

words problem is from [3].

306

chapter 8. advertising on the web

1. n. craswell, o. zoeter, m. taylor, and w. ramsey,    an experimental
comparison of click-position bias models,    proc. intl. conf. on web search
and web data mining pp. 87   94, 2008.

2. b. kalyanasundaram and k.r. pruhs,    an optimal deterministic algo-
rithm for b-matching,    theoretical computer science 233:1   2, pp. 319   
325, 2000.

3. a mehta, a. saberi, u. vazirani, and v. vazirani,    adwords and general-
ized on-line matching,    ieee symp. on foundations of computer science,
pp. 264   273, 2005.

chapter 9

id126s

there is an extensive class of web applications that involve predicting user
responses to options. such a facility is called a id126. we
shall begin this chapter with a survey of the most important examples of these
systems. however, to bring the problem into focus, two good examples of
id126s are:

1. o   ering news articles to on-line newspaper readers, based on a prediction

of reader interests.

2. o   ering customers of an on-line retailer suggestions about what they
might like to buy, based on their past history of purchases and/or product
searches.

id126s use a number of di   erent technologies. we can

classify these systems into two broad groups.

    content-based systems examine properties of the items recommended. for
instance, if a net   ix user has watched many cowboy movies, then recom-
mend a movie classi   ed in the database as having the    cowboy    genre.

    collaborative    ltering systems recommend items based on similarity mea-
sures between users and/or items. the items recommended to a user are
those preferred by similar users. this sort of id126 can
use the groundwork laid in chapter 3 on similarity search and chapter 7
on id91. however, these technologies by themselves are not su   -
cient, and there are some new algorithms that have proven e   ective for
id126s.

9.1 a model for id126s

in this section we introduce a model for id126s, based on
a utility matrix of preferences. we introduce the concept of a    long-tail,   

307

308

chapter 9. id126s

which explains the advantage of on-line vendors over conventional, brick-and-
mortar vendors. we then brie   y survey the sorts of applications in which
id126s have proved useful.

9.1.1 the utility matrix

in a recommendation-system application there are two classes of entities, which
we shall refer to as users and items. users have preferences for certain items,
and these preferences must be teased out of the data. the data itself is repre-
sented as a utility matrix, giving for each user-item pair, a value that represents
what is known about the degree of preference of that user for that item. values
come from an ordered set, e.g., integers 1   5 representing the number of stars
that the user gave as a rating for that item. we assume that the matrix is
sparse, meaning that most entries are    unknown.    an unknown rating implies
that we have no explicit information about the user   s preference for the item.

example 9.1 : in fig. 9.1 we see an example utility matrix, representing users   
ratings of movies on a 1   5 scale, with 5 the highest rating. blanks represent
the situation where the user has not rated the movie. the movie names are
hp1, hp2, and hp3 for harry potter i, ii, and iii, tw for twilight, and sw1,
sw2, and sw3 for star wars episodes 1, 2, and 3. the users are represented
by capital letters a through d.

hp1 hp2 hp3 tw sw1

sw2

sw3

4
5

a
b
c
d

5

3

4

5

2

1

4

5

3

figure 9.1: a utility matrix representing ratings of movies on a 1   5 scale

notice that most user-movie pairs have blanks, meaning the user has not
rated the movie. in practice, the matrix would be even sparser, with the typical
user rating only a tiny fraction of all available movies.    

the goal of a id126 is to predict the blanks in the utility
matrix. for example, would user a like sw2? there is little evidence from
the tiny matrix in fig. 9.1. we might design our id126 to
take into account properties of movies, such as their producer, director, stars,
or even the similarity of their names. if so, we might then note the similarity
between sw1 and sw2, and then conclude that since a did not like sw1, they
were unlikely to enjoy sw2 either. alternatively, with much more data, we
might observe that the people who rated both sw1 and sw2 tended to give
them similar ratings. thus, we could conclude that a would also give sw2 a
low rating, similar to a   s rating of sw1.

9.1. a model for id126s

309

we should also be aware of a slightly di   erent goal that makes sense in many
applications. it is not necessary to predict every blank entry in a utility matrix.
rather, it is only necessary to discover some entries in each row that are likely
to be high. in most applications, the id126 does not o   er
users a ranking of all items, but rather suggests a few that the user should value
highly. it may not even be necessary to    nd all items with the highest expected
ratings, but only to    nd a large subset of those with the highest ratings.

9.1.2 the long tail

before discussing the principal applications of id126s, let us
ponder the long tail phenomenon that makes id126s neces-
sary. physical delivery systems are characterized by a scarcity of resources.
brick-and-mortar stores have limited shelf space, and can show the customer
only a small fraction of all the choices that exist. on the other hand, on-line
stores can make anything that exists available to the customer. thus, a physical
bookstore may have several thousand books on its shelves, but amazon o   ers
millions of books. a physical newspaper can print several dozen articles per
day, while on-line news services o   er thousands per day.

recommendation in the physical world is fairly simple. first, it is not
possible to tailor the store to each individual customer. thus, the choice of
what is made available is governed only by the aggregate numbers. typically, a
bookstore will display only the books that are most popular, and a newspaper
will print only the articles it believes the most people will be interested in.
in the    rst case, sales    gures govern the choices, in the second case, editorial
judgement serves.

the distinction between the physical and on-line worlds has been called
the long tail phenomenon, and it is suggested in fig. 9.2. the vertical axis
represents popularity (the number of times an item is chosen). the items are
ordered on the horizontal axis according to their popularity. physical institu-
tions provide only the most popular items to the left of the vertical line, while
the corresponding on-line institutions provide the entire range of items: the tail
as well as the popular items.

the long-tail phenomenon forces on-line institutions to recommend items
to individual users. it is not possible to present all available items to the user,
the way physical institutions can. neither can we expect users to have heard
of each of the items they might like.

9.1.3 applications of id126s

we have mentioned several important applications of id126s,
but here we shall consolidate the list in a single place.

1. product recommendations: perhaps the most important use of recom-
mendation systems is at on-line retailers. we have noted how amazon
or similar on-line vendors strive to present each returning user with some

310

chapter 9. id126s

the long tail

figure 9.2: the long tail: physical institutions can only provide what is popular,
while on-line institutions can make everything available

suggestions of products that they might like to buy. these suggestions are
not random, but are based on the purchasing decisions made by similar
customers or on other techniques we shall discuss in this chapter.

2. movie recommendations: net   ix o   ers its customers recommendations
of movies they might like. these recommendations are based on ratings
provided by users, much like the ratings suggested in the example utility
matrix of fig. 9.1. the importance of predicting ratings accurately is
so high, that net   ix o   ered a prize of one million dollars for the    rst
algorithm that could beat its own id126 by 10%.1 the
prize was    nally won in 2009, by a team of researchers called    bellkor   s
pragmatic chaos,    after over three years of competition.

3. news articles: news services have attempted to identify articles of in-
terest to readers, based on the articles that they have read in the past.
the similarity might be based on the similarity of important words in the
documents, or on the articles that are read by people with similar reading
tastes. the same principles apply to recommending blogs from among
the millions of blogs available, videos on youtube, or other sites where
content is provided regularly.

1to be exact, the algorithm had to have a root-mean-square error (rmse) that was 10%
less than the rmse of the net   ix algorithm on a test set taken from actual ratings of net   ix
users. to develop an algorithm, contestants were given a training set of data, also taken from
actual net   ix data.

9.1. a model for id126s

311

into thin air and touching the void

an extreme example of how the long tail, together with a well designed
id126 can in   uence events is the story told by chris an-
derson about a book called touching the void. this mountain-climbing
book was not a big seller in its day, but many years after it was pub-
lished, another book on the same topic, called into thin air was pub-
lished. amazon   s id126 noticed a few people who
bought both books, and started recommending touching the void to peo-
ple who bought, or were considering, into thin air. had there been no
on-line bookseller, touching the void might never have been seen by poten-
tial buyers, but in the on-line world, touching the void eventually became
very popular in its own right, in fact, more so than into thin air.

9.1.4 populating the utility matrix

without a utility matrix, it is almost impossible to recommend items. however,
acquiring data from which to build a utility matrix is often di   cult. there are
two general approaches to discovering the value users place on items.

1. we can ask users to rate items. movie ratings are generally obtained this
way, and some on-line stores try to obtain ratings from their purchasers.
sites providing content, such as some news sites or youtube also ask users
to rate items. this approach is limited in its e   ectiveness, since generally
users are unwilling to provide responses, and the information from those
who do may be biased by the very fact that it comes from people willing
to provide ratings.

2. we can make id136s from users    behavior. most obviously, if a user
buys a product at amazon, watches a movie on youtube, or reads a news
article, then the user can be said to    like    this item. note that this sort
of rating system really has only one value: 1 means that the user likes
the item. often, we    nd a utility matrix with this kind of data shown
with 0   s rather than blanks where the user has not purchased or viewed
the item. however, in this case 0 is not a lower rating than 1; it is no
rating at all. more generally, one can infer interest from behavior other
than purchasing. for example, if an amazon customer views information
about an item, we can infer that they are interested in the item, even if
they don   t buy it.

312

chapter 9. id126s

9.2 content-based recommendations

as we mentioned at the beginning of the chapter, there are two basic architec-
tures for a id126:

1. content-based systems focus on properties of items. similarity of items

is determined by measuring the similarity in their properties.

2. collaborative-filtering systems focus on the relationship between users
and items. similarity of items is determined by the similarity of the
ratings of those items by the users who have rated both items.

in this section, we focus on content-based id126s. the next
section will cover collaborative    ltering.

9.2.1

item pro   les

in a content-based system, we must construct for each item a pro   le, which is
a record or collection of records representing important characteristics of that
item. in simple cases, the pro   le consists of some characteristics of the item
that are easily discovered. for example, consider the features of a movie that
might be relevant to a id126.

1. the set of actors of the movie. some viewers prefer movies with their

favorite actors.

2. the director. some viewers have a preference for the work of certain

directors.

3. the year in which the movie was made. some viewers prefer old movies;

others watch only the latest releases.

4. the genre or general type of movie. some viewers like only comedies,

others dramas or romances.

there are many other features of movies that could be used as well. except
for the last, genre, the information is readily available from descriptions of
movies. genre is a vaguer concept. however, movie reviews generally assign
a genre from a set of commonly used terms. for example the internet movie
database (imdb) assigns a genre or genres to every movie. we shall discuss
mechanical construction of genres in section 9.3.3.

many other classes of items also allow us to obtain features from available
data, even if that data must at some point be entered by hand. for instance,
products often have descriptions written by the manufacturer, giving features
relevant to that class of product (e.g., the screen size and cabinet color for a
tv). books have descriptions similar to those for movies, so we can obtain
features such as author, year of publication, and genre. music products such
as cd   s and mp3 downloads have available features such as artist, composer,
and genre.

9.2. content-based recommendations

313

9.2.2 discovering features of documents

there are other classes of items where it is not immediately apparent what the
values of features should be. we shall consider two of them: document collec-
tions and images. documents present special problems, and we shall discuss
the technology for extracting features from documents in this section. images
will be discussed in section 9.2.3 as an important example where user-supplied
features have some hope of success.

there are many kinds of documents for which a id126 can
be useful. for example, there are many news articles published each day, and
we cannot read all of them. a id126 can suggest articles on
topics a user is interested in, but how can we distinguish among topics? web
pages are also a collection of documents. can we suggest pages a user might
want to see? likewise, blogs could be recommended to interested users, if we
could classify blogs by topics.

unfortunately, these classes of documents do not tend to have readily avail-
able information giving features. a substitute that has been useful in practice is
the identi   cation of words that characterize the topic of a document. how we do
the identi   cation was outlined in section 1.3.1. first, eliminate stop words    
the several hundred most common words, which tend to say little about the
topic of a document. for the remaining words, compute the tf.idf score for
each word in the document. the ones with the highest scores are the words
that characterize the document.

we may then take as the features of a document the n words with the highest
tf.idf scores. it is possible to pick n to be the same for all documents, or to
let n be a    xed percentage of the words in the document. we could also choose
to make all words whose tf.idf scores are above a given threshold to be a
part of the feature set.

now, documents are represented by sets of words. intuitively, we expect
these words to express the subjects or main ideas of the document. for example,
in a news article, we would expect the words with the highest tf.idf score to
include the names of people discussed in the article, unusual properties of the
event described, and the location of the event. to measure the similarity of two
documents, there are several natural distance measures we can use:

1. we could use the jaccard distance between the sets of words (recall sec-

tion 3.5.3).

2. we could use the cosine distance (recall section 3.5.4) between the sets,

treated as vectors.

to compute the cosine distance in option (2), think of the sets of high-
tf.idf words as a vector, with one component for each possible word. the
vector has 1 if the word is in the set and 0 if not. since between two docu-
ments there are only a    nite number of words among their two sets, the in   nite
dimensionality of the vectors is unimportant. almost all components are 0 in

314

chapter 9. id126s

two kinds of document similarity

recall that in section 3.4 we gave a method for    nding documents that
were    similar,    using shingling, minhashing, and lsh. there, the notion
of similarity was lexical     documents are similar if they contain large,
identical sequences of characters. for id126s, the notion
of similarity is di   erent. we are interested only in the occurrences of many
important words in both documents, even if there is little lexical similarity
between the documents. however, the methodology for    nding similar
documents remains almost the same. once we have a distance measure,
either jaccard or cosine, we can use minhashing (for jaccard) or random
hyperplanes (for cosine distance; see section 3.7.2) feeding data to an lsh
algorithm to    nd the pairs of documents that are similar in the sense of
sharing many common keywords.

both, and 0   s do not impact the value of the dot product. to be precise, the dot
product is the size of the intersection of the two sets of words, and the lengths
of the vectors are the square roots of the numbers of words in each set. that
calculation lets us compute the cosine of the angle between the vectors as the
dot product divided by the product of the vector lengths.

9.2.3 obtaining item features from tags

let us consider a database of images as an example of a way that features have
been obtained for items. the problem with images is that their data, typically
an array of pixels, does not tell us anything useful about their features. we can
calculate simple properties of pixels, such as the average amount of red in the
picture, but few users are looking for red pictures or especially like red pictures.
there have been a number of attempts to obtain information about features
of items by inviting users to tag the items by entering words or phrases that
describe the item. thus, one picture with a lot of red might be tagged    tianan-
men square,    while another is tagged    sunset at malibu.    the distinction is
not something that could be discovered by existing image-analysis programs.

almost any kind of data can have its features described by tags. one of
the earliest attempts to tag massive amounts of data was the site del.icio.us,
later bought by yahoo!, which invited users to tag web pages. the goal of this
tagging was to make a new method of search available, where users entered a
set of tags as their search query, and the system retrieved the web pages that
had been tagged that way. however, it is also possible to use the tags as a
id126. if it is observed that a user retrieves or bookmarks
many pages with a certain set of tags, then we can recommend other pages with
the same tags.

the problem with tagging as an approach to feature discovery is that the

9.2. content-based recommendations

315

tags from computer games

an interesting direction for encouraging tagging is the    games    approach
pioneered by luis von ahn. he enabled two players to collaborate on the
tag for an image. in rounds, they would suggest a tag, and the tags would
be exchanged. if they agreed, then they    won,    and if not, they would
play another round with the same image, trying to agree simultaneously
on a tag. while an innovative direction to try, it is questionable whether
su   cient public interest can be generated to produce enough free work to
satisfy the needs for tagged data.

process only works if users are willing to take the trouble to create the tags, and
there are enough tags that occasional erroneous ones will not bias the system
too much.

9.2.4 representing item pro   les

our ultimate goal for content-based recommendation is to create both an item
pro   le consisting of feature-value pairs and a user pro   le summarizing the pref-
erences of the user, based of their row of the utility matrix. in section 9.2.2
we suggested how an item pro   le could be constructed. we imagined a vector
of 0   s and 1   s, where a 1 represented the occurrence of a high-tf.idf word
in the document. since features for documents were all words, it was easy to
represent pro   les this way.

we shall try to generalize this vector approach to all sorts of features. it is
easy to do so for features that are sets of discrete values. for example, if one
feature of movies is the set of actors, then imagine that there is a component
for each actor, with 1 if the actor is in the movie, and 0 if not. likewise, we
can have a component for each possible director, and each possible genre. all
these features can be represented using only 0   s and 1   s.

there is another class of features that is not readily represented by boolean
vectors: those features that are numerical. for instance, we might take the
average rating for movies to be a feature,2 and this average is a real number.
it does not make sense to have one component for each of the possible average
ratings, and doing so would cause us to lose the structure implicit in numbers.
that is, two ratings that are close but not identical should be considered more
similar than widely di   ering ratings. likewise, numerical features of products,
such as screen size or disk capacity for pc   s, should be considered similar if
their values do not di   er greatly.

numerical features should be represented by single components of vectors
representing items. these components hold the exact value of that feature.

2the rating is not a very reliable feature, but it will serve as an example.

316

chapter 9. id126s

there is no harm if some components of the vectors are boolean and others are
real-valued or integer-valued. we can still compute the cosine distance between
vectors, although if we do so, we should give some thought to the appropri-
ate scaling of the nonboolean components, so that they neither dominate the
calculation nor are they irrelevant.

example 9.2 : suppose the only features of movies are the set of actors and
the average rating. consider two movies with    ve actors each. two of the
actors are in both movies. also, one movie has an average rating of 3 and the
other an average of 4. the vectors look something like

0
1

1
1

1
0

0
1

1
0

1
1

0
1

1
0

3  
4  

however, there are in principle an in   nite number of additional components,
each with 0   s for both vectors, representing all the possible actors that neither
movie has. since cosine distance of vectors is not a   ected by components in
which both vectors have 0, we need not worry about the e   ect of actors that
are in neither movie.

the last component shown represents the average rating. we have shown
it as having an unknown scaling factor   . in terms of   , we can compute the
cosine of the angle between the vectors. the dot product is 2 + 12  2, and the
lengths of the vectors are    5 + 9  2 and    5 + 16  2. thus, the cosine of the
angle between the vectors is

2 + 12  2

   25 + 125  2 + 144  4

if we choose    = 1, that is, we take the average ratings as they are, then
the value of the above expression is 0.816. if we use    = 2, that is, we double
the ratings, then the cosine is 0.940. that is, the vectors appear much closer
in direction than if we use    = 1. likewise, if we use    = 1/2, then the cosine
is 0.619, making the vectors look quite di   erent. we cannot tell which value of
   is    right,    but we see that the choice of scaling factor for numerical features
a   ects our decision about how similar items are.    

9.2.5 user pro   les

we not only need to create vectors describing items; we need to create vectors
with the same components that describe the user   s preferences. we have the
utility matrix representing the connection between users and items. recall
the nonblank matrix entries could be just 1   s representing user purchases or a
similar connection, or they could be arbitrary numbers representing a rating or
degree of a   ection that the the user has for the item.

with this information, the best estimate we can make regarding which items
the user likes is some aggregation of the pro   les of those items. if the utility
matrix has only 1   s, then the natural aggregate is the average of the components

9.2. content-based recommendations

317

of the vectors representing the item pro   les for the items in which the utility
matrix has 1 for that user.

example 9.3 : suppose items are movies, represented by boolean pro   les with
components corresponding to actors. also, the utility matrix has a 1 if the user
has seen the movie and is blank otherwise. if 20% of the movies that user u
likes have julia roberts as one of the actors, then the user pro   le for u will
have 0.2 in the component for julia roberts.    

if the utility matrix is not boolean, e.g., ratings 1   5, then we can weight
the vectors representing the pro   les of items by the utility value.
it makes
sense to normalize the utilities by subtracting the average value for a user.
that way, we get negative weights for items with a below-average rating, and
positive weights for items with above-average ratings. that e   ect will prove
useful when we discuss in section 9.2.6 how to    nd items that a user should
like.

example 9.4 : consider the same movie information as in example 9.3, but
now suppose the utility matrix has nonblank entries that are ratings in the 1   5
range. suppose user u gives an average rating of 3. there are three movies
with julia roberts as an actor, and those movies got ratings of 3, 4, and 5.
then in the user pro   le of u , the component for julia roberts will have value
that is the average of 3     3, 4     3, and 5     3, that is, a value of 1.
on the other hand, user v gives an average rating of 4, and has also rated
three movies with julia roberts (it doesn   t matter whether or not they are the
same three movies u rated). user v gives these three movies ratings of 2, 3,
and 5. the user pro   le for v has, in the component for julia roberts, the
average of 2     4, 3     4, and 5     4, that is, the value    2/3.    

9.2.6 recommending items to users based on content

with pro   le vectors for both users and items, we can estimate the degree to
which a user would prefer an item by computing the cosine distance between
the user   s and item   s vectors. as in example 9.2, we may wish to scale var-
ious components whose values are not boolean. the random-hyperplane and
locality-sensitive-hashing techniques can be used to place (just) item pro   les
in buckets. in that way, given a user to whom we want to recommend some
items, we can apply the same two techniques     random hyperplanes and lsh    
to determine in which buckets we must look for items that might have a small
cosine distance from the user.

example 9.5 : consider    rst the data of example 9.3. the user   s pro   le will
have components for actors proportional to the likelihood that the actor will
appear in a movie the user likes. thus, the highest recommendations (lowest
cosine distance) belong to the movies with lots of actors that appear in many

318

chapter 9. id126s

of the movies the user likes. as long as actors are the only information we have
about features of movies, that is probably the best we can do.3

now, consider example 9.4. there, we observed that the vector for a user
will have positive numbers for actors that tend to appear in movies the user
likes and negative numbers for actors that tend to appear in movies the user
doesn   t like. consider a movie with many actors the user likes, and only a few
or none that the user doesn   t like. the cosine of the angle between the user   s
and movie   s vectors will be a large positive fraction. that implies an angle close
to 0, and therefore a small cosine distance between the vectors.

next, consider a movie with about as many actors that the user likes as those
the user doesn   t like. in this situation, the cosine of the angle between the user
and movie is around 0, and therefore the angle between the two vectors is around
90 degrees. finally, consider a movie with mostly actors the user doesn   t like.
in that case, the cosine will be a large negative fraction, and the angle between
the two vectors will be close to 180 degrees     the maximum possible cosine
distance.    

9.2.7 classi   cation algorithms

a completely di   erent approach to a id126 using item pro   les
and utility matrices is to treat the problem as one of machine learning. regard
the given data as a training set, and for each user, build a classi   er that predicts
the rating of all items. there are a great number of di   erent classi   ers, and it
is not our purpose to teach this subject here. however, you should be aware
of the option of developing a classi   er for recommendation, so we shall discuss
one common classi   er     id90     brie   y.

a decision tree is a collection of nodes, arranged as a binary tree. the
leaves render decisions; in our case, the decision would be    likes    or    doesn   t
like.    each interior node is a condition on the objects being classi   ed; in our
case the condition would be a predicate involving one or more features of an
item.

to classify an item, we start at the root, and apply the predicate at the root
to the item. if the predicate is true, go to the left child, and if it is false, go to
the right child. then repeat the same process at the node visited, until a leaf
is reached. that leaf classi   es the item as liked or not.

construction of a decision tree requires selection of a predicate for each
interior node. there are many ways of picking the best predicate, but they all
try to arrange that one of the children gets all or most of the positive examples
in the training set (i.e, the items that the given user likes, in our case) and the
other child gets all or most of the negative examples (the items this user does
not like).

3note that the fact all user-vector components will be small fractions does not a   ect the
recommendation, since the cosine calculation involves dividing by the length of each vector.
that is, user vectors will tend to be much shorter than movie vectors, but only the direction
of vectors matters.

9.2. content-based recommendations

319

once we have selected a predicate for a node n , we divide the items into
the two groups: those that satisfy the predicate and those that do not. for
each group, we again    nd the predicate that best separates the positive and
negative examples in that group. these predicates are assigned to the children
of n . this process of dividing the examples and building children can proceed
to any number of levels. we can stop, and create a leaf, if the group of items
for a node is homogeneous; i.e., they are all positive or all negative examples.
however, we may wish to stop and create a leaf with the majority decision
for a group, even if the group contains both positive and negative examples.
the reason is that the statistical signi   cance of a small group may not be high
enough to rely on. for that reason a variant strategy is to create an ensemble of
id90, each using di   erent predicates, but allow the trees to be deeper
than what the available data justi   es. such trees are called over   tted. to
classify an item, apply all the trees in the ensemble, and let them vote on the
outcome. we shall not consider this option here, but give a simple hypothetical
example of a decision tree.

example 9.6 : suppose our items are news articles, and features are the high-
tf.idf words (keywords) in those documents. further suppose there is a user
u who likes articles about baseball, except articles about the new york yankees.
the row of the utility matrix for u has 1 if u has read the article and is blank if
not. we shall take the 1   s as    like    and the blanks as    doesn   t like.    predicates
will be boolean expressions of keywords.

since u generally likes baseball, we might    nd that the best predicate for
the root is    homerun    or (   batter    and    pitcher   ). items that satisfy the
predicate will tend to be positive examples (articles with 1 in the row for u in
the utility matrix), and items that fail to satisfy the predicate will tend to be
negative examples (blanks in the utility-matrix row for u ). figure 9.3 shows
the root as well as the rest of the decision tree.

suppose that the group of articles that do not satisfy the predicate includes
su   ciently few positive examples that we can conclude all of these items are in
the    don   t-like    class. we may then put a leaf with decision    don   t like    as the
right child of the root. however, the articles that satisfy the predicate includes
a number of articles that user u doesn   t like; these are the articles that mention
the yankees. thus, at the left child of the root, we build another predicate.
we might    nd that the predicate    yankees    or    jeter    or    teixeira    is the
best possible indicator of an article about baseball and about the yankees.
thus, we see in fig. 9.3 the left child of the root, which applies this predicate.
both children of this node are leaves, since we may suppose that the items
satisfying this predicate are predominantly negative and those not satisfying it
are predominantly positive.    

unfortunately, classi   ers of all types tend to take a long time to construct.
for instance, if we wish to use id90, we need one tree per user. con-
structing a tree not only requires that we look at all the item pro   les, but we

320

chapter 9. id126s

"homerun" or

("batter" and "pitcher")

"yankees" or

"jeter" or "teixeira"

doesn   t

like

doesn   t

like

likes

figure 9.3: a decision tree

have to consider many di   erent predicates, which could involve complex com-
binations of features. thus, this approach tends to be used only for relatively
small problem sizes.

9.2.8 exercises for section 9.2

exercise 9.2.1 : three computers, a, b, and c, have the numerical features
listed below:

feature
processor speed
disk size
main-memory size

a
3.06
500
6

b
2.68
320
4

c
2.92
640
6

we may imagine these values as de   ning a vector for each computer; for in-
stance, a   s vector is [3.06, 500, 6]. we can compute the cosine distance between
any two of the vectors, but if we do not scale the components, then the disk
size will dominate and make di   erences in the other components essentially in-
visible. let us use 1 as the scale factor for processor speed,    for the disk size,
and    for the main memory size.

(a) in terms of    and   , compute the cosines of the angles between the vectors

for each pair of the three computers.

(b) what are the angles between the vectors if    =    = 1?

(c) what are the angles between the vectors if    = 0.01 and    = 0.5?

9.3. id185

321

! (d) one fair way of selecting scale factors is to make each inversely propor-
tional to the average value in its component. what would be the values
of    and   , and what would be the angles between the vectors?

exercise 9.2.2 : an alternative way of scaling components of a vector is to
begin by normalizing the vectors. that is, compute the average for each com-
ponent and subtract it from that component   s value in each of the vectors.

(a) normalize the vectors for the three computers described in exercise 9.2.1.

!! (b) this question does not require di   cult calculation, but it requires some
serious thought about what angles between vectors mean. when all com-
ponents are nonnegative, as they are in the data of exercise 9.2.1, no
vectors can have an angle greater than 90 degrees. however, when we
normalize vectors, we can (and must) get some negative components, so
the angles can now be anything, that is, 0 to 180 degrees. moreover,
averages are now 0 in every component, so the suggestion in part (d) of
exercise 9.2.1 that we should scale in inverse proportion to the average
makes no sense. suggest a way of    nding an appropriate scale for each
component of normalized vectors. how would you interpret a large or
small angle between normalized vectors? what would the angles be for
the normalized vectors derived from the data in exercise 9.2.1?

exercise 9.2.3 : a certain user has rated the three computers of exercise 9.2.1
as follows: a: 4 stars, b: 2 stars, c: 5 stars.

(a) normalize the ratings for this user.

(b) compute a user pro   le for the user, with components for processor speed,

disk size, and main memory size, based on the data of exercise 9.2.1.

9.3 id185

we shall now take up a signi   cantly di   erent approach to recommendation.
instead of using features of items to determine their similarity, we focus on the
similarity of the user ratings for two items. that is, in place of the item-pro   le
vector for an item, we use its column in the utility matrix. further, instead
of contriving a pro   le vector for users, we represent them by their rows in the
utility matrix. users are similar if their vectors are close according to some
distance measure such as jaccard or cosine distance. recommendation for a
user u is then made by looking at the users that are most similar to u in this
sense, and recommending items that these users like. the process of identifying
similar users and recommending what similar users like is called collaborative
   ltering.

322

chapter 9. id126s

9.3.1 measuring similarity

the    rst question we must deal with is how to measure similarity of users or
items from their rows or columns in the utility matrix. we have reproduced
fig. 9.1 here as fig. 9.4. this data is too small to draw any reliable conclusions,
but its small size will make clear some of the pitfalls in picking a distance
measure. observe speci   cally the users a and c. they rated two movies in
common, but they appear to have almost diametrically opposite opinions of
these movies. we would expect that a good distance measure would make
them rather far apart. here are some alternative measures to consider.

hp1 hp2 hp3 tw sw1

sw2

sw3

4
5

a
b
c
d

5

3

4

5

2

1

4

5

3

figure 9.4: the utility matrix introduced in fig. 9.1

jaccard distance

we could ignore values in the matrix and focus only on the sets of items rated.
if the utility matrix only re   ected purchases, this measure would be a good
one to choose. however, when utilities are more detailed ratings, the jaccard
distance loses important information.

example 9.7 : a and b have an intersection of size 1 and a union of size 5.
thus, their jaccard similarity is 1/5, and their jaccard distance is 4/5; i.e.,
they are very far apart. in comparison, a and c have a jaccard similarity of
2/4, so their jaccard distance is the same, 1/2. thus, a appears closer to c
than to b. yet that conclusion seems intuitively wrong. a and c disagree on
the two movies they both watched, while a and b seem both to have liked the
one movie they watched in common.    

cosine distance

we can treat blanks as a 0 value. this choice is questionable, since it has the
e   ect of treating the lack of a rating as more similar to disliking the movie than
liking it.

example 9.8 : the cosine of the angle between a and b is

   42 + 52 + 12   52 + 52 + 42

4    5

= 0.380

9.3. id185

323

the cosine of the angle between a and c is

5    2 + 1    4

   42 + 52 + 12   22 + 42 + 52

= 0.322

since a larger (positive) cosine implies a smaller angle and therefore a smaller
distance, this measure tells us that a is slightly closer to b than to c.    

rounding the data

we could try to eliminate the apparent similarity between movies a user rates
highly and those with low scores by rounding the ratings. for instance, we could
consider ratings of 3, 4, and 5 as a    1    and consider ratings 1 and 2 as unrated.
the utility matrix would then look as in fig. 9.5. now, the jaccard distance
between a and b is 3/4, while between a and c it is 1; i.e., c appears further
from a than b does, which is intuitively correct. applying cosine distance to
fig. 9.5 allows us to draw the same conclusion.

hp1 hp2 hp3 tw sw1

sw2

sw3

1
1

a
b
c
d

1

1

1

1

1

1

1

figure 9.5: utilities of 3, 4, and 5 have been replaced by 1, while ratings of 1
and 2 are omitted

normalizing ratings

if we normalize ratings, by subtracting from each rating the average rating
of that user, we turn low ratings into negative numbers and high ratings into
positive numbers. if we then take the cosine distance, we    nd that users with
opposite views of the movies they viewed in common will have vectors in almost
opposite directions, and can be considered as far apart as possible. however,
users with similar opinions about the movies rated in common will have a
relatively small angle between them.

example 9.9 : figure 9.6 shows the matrix of fig. 9.4 with all ratings nor-
malized. an interesting e   ect is that d   s ratings have e   ectively disappeared,
because a 0 is the same as a blank when cosine distance is computed. note that
d gave only 3   s and did not di   erentiate among movies, so it is quite possible
that d   s opinions are not worth taking seriously.

let us compute the cosine of the angle between a and b:

(2/3)    (1/3)

p(2/3)2 + (5/3)2 + (   7/3)2p(1/3)2 + (1/3)2 + (   2/3)2

= 0.092

324

chapter 9. id126s

hp3

hp1 hp2
2/3
1/3

1/3    2/3

0

tw sw1
5/3    7/3
   5/3
1/3

sw2

sw3

4/3

0

a
b
c
d

figure 9.6: the utility matrix introduced in fig. 9.1

the cosine of the angle between between a and c is

(5/3)    (   5/3) + (   7/3)    (1/3)

p(2/3)2 + (5/3)2 + (   7/3)2p(   5/3)2 + (1/3)2 + (4/3)2

=    0.559

notice that under this measure, a and c are much further apart than a and
b, and neither pair is very close. both these observations make intuitive sense,
given that a and c disagree on the two movies they rated in common, while a
and b give similar scores to the one movie they rated in common.    

9.3.2 the duality of similarity

the utility matrix can be viewed as telling us about users or about items, or
both.
it is important to realize that any of the techniques we suggested in
section 9.3.1 for    nding similar users can be used on columns of the utility
matrix to    nd similar items. there are two ways in which the symmetry is
broken in practice.

1. we can use information about users to recommend items. that is, given
a user, we can    nd some number of the most similar users, perhaps using
the techniques of chapter 3. we can base our recommendation on the
decisions made by these similar users, e.g., recommend the items that the
greatest number of them have purchased or rated highly. however, there
is no symmetry. even if we    nd pairs of similar items, we need to take
an additional step in order to recommend items to users. this point is
explored further at the end of this subsection.

2. there is a di   erence in the typical behavior of users and items, as it
pertains to similarity. intuitively, items tend to be classi   able in simple
terms. for example, music tends to belong to a single genre. it is impossi-
ble, e.g., for a piece of music to be both 60   s rock and 1700   s baroque. on
the other hand, there are individuals who like both 60   s rock and 1700   s
baroque, and who buy examples of both types of music. the consequence
is that it is easier to discover items that are similar because they belong
to the same genre, than it is to detect that two users are similar because
they prefer one genre in common, while each also likes some genres that
the other doesn   t care for.

9.3. id185

325

as we suggested in (1) above, one way of predicting the value of the utility-
matrix entry for user u and item i is to    nd the n users (for some predetermined
n) most similar to u and average their ratings for item i, counting only those
among the n similar users who have rated i. it is generally better to normalize
the matrix    rst. that is, for each of the n users subtract their average rating
for items from their rating for i. average the di   erence for those users who
have rated i, and then add this average to the average rating that u gives for
all items. this id172 adjusts the estimate in the case that u tends to
give very high or very low ratings, or a large fraction of the similar users who
rated i (of which there may be only a few) are users who tend to rate very high
or very low.

dually, we can use item similarity to estimate the entry for user u and item
i. find the m items most similar to i, for some m, and take the average rating,
among the m items, of the ratings that u has given. as for user-user similarity,
we consider only those items among the m that u has rated, and it is probably
wise to normalize item ratings    rst.

note that whichever approach to estimating entries in the utility matrix we
use, it is not su   cient to    nd only one entry. in order to recommend items to
a user u , we need to estimate every entry in the row of the utility matrix for
u , or at least    nd all or most of the entries in that row that are blank but have
a high estimated value. there is a tradeo    regarding whether we should work
from similar users or similar items.

    if we    nd similar users, then we only have to do the process once for user
u . from the set of similar users we can estimate all the blanks in the
utility matrix for u . if we work from similar items, we have to compute
similar items for almost all items, before we can estimate the row for u .

    on the other hand, item-item similarity often provides more reliable in-
formation, because of the phenomenon observed above, namely that it is
easier to    nd items of the same genre than it is to    nd users that like only
items of a single genre.

whichever method we choose, we should precompute preferred items for each
user, rather than waiting until we need to make a decision. since the utility
matrix evolves slowly, it is generally su   cient to compute it infrequently and
assume that it remains    xed between recomputations.

9.3.3 id91 users and items

it is hard to detect similarity among either items or users, because we have
little information about user-item pairs in the sparse utility matrix.
in the
perspective of section 9.3.2, even if two items belong to the same genre, there
are likely to be very few users who bought or rated both. likewise, even if
two users both like a genre or genres, they may not have bought any items in
common.

326

chapter 9. id126s

one way of dealing with this pitfall is to cluster items and/or users. select
any of the distance measures suggested in section 9.3.1, or any other distance
measure, and use it to perform a id91 of, say, items. any of the methods
suggested in chapter 7 can be used. however, we shall see that there may
be little reason to try to cluster into a small number of clusters immediately.
rather, a hierarchical approach, where we leave many clusters unmerged may
su   ce as a    rst step. for example, we might leave half as many clusters as
there are items.

hp

a 4
b 4.67
c
d 3

tw sw
5

1

2

4.5
3

figure 9.7: utility matrix for users and clusters of items

example 9.10 : figure 9.7 shows what happens to the utility matrix of fig. 9.4
if we manage to cluster the three harry-potter movies into one cluster, denoted
hp, and also cluster the three star-wars movies into one cluster sw.    

having clustered items to an extent, we can revise the utility matrix so the
columns represent clusters of items, and the entry for user u and cluster c is
the average rating that u gave to those members of cluster c that u did rate.
note that u may have rated none of the cluster members, in which case the
entry for u and c is still blank.

we can use this revised utility matrix to cluster users, again using the dis-
tance measure we consider most appropriate. use a id91 algorithm that
again leaves many clusters, e.g., half as many clusters as there are users. re-
vise the utility matrix, so the rows correspond to clusters of users, just as the
columns correspond to clusters of items. as for item-clusters, compute the
entry for a user cluster by averaging the ratings of the users in the cluster.

now, this process can be repeated several times if we like. that is, we can
cluster the item clusters and again merge the columns of the utility matrix that
belong to one cluster. we can then turn to the users again, and cluster the
user clusters. the process can repeat until we have an intuitively reasonable
number of clusters of each kind.

once we have clustered the users and/or items to the desired extent and
computed the cluster-cluster utility matrix, we can estimate entries in the orig-
inal utility matrix as follows. suppose we want to predict the entry for user u
and item i:

(a) find the clusters to which u and i belong, say clusters c and d, respec-

tively.

9.3. id185

327

(b) if the entry in the cluster-cluster utility matrix for c and d is something
other than blank, use this value as the estimated value for the u    i entry
in the original utility matrix.

(c) if the entry for c   d is blank, then use the method outlined in section 9.3.2
to estimate that entry by considering clusters similar to c or d. use the
resulting estimate as the estimate for the u -i entry.

9.3.4 exercises for section 9.3

a
a 4
b
c 2

b
5
3

c

4
1

d
5
3
3

e
1
1

f

2
4

g
3
1
5

h
2

3

figure 9.8: a utility matrix for exercises

exercise 9.3.1 : figure 9.8 is a utility matrix, representing the ratings, on a
1   5 star scale, of eight items, a through h, by three users a, b, and c. compute
the following from the data of this matrix.

(a) treating the utility matrix as boolean, compute the jaccard distance be-

tween each pair of users.

(b) repeat part (a), but use the cosine distance.

(c) treat ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0. compute the

jaccard distance between each pair of users.

(d) repeat part (c), but use the cosine distance.

(e) normalize the matrix by subtracting from each nonblank entry the average

value for its user.

(f) using the normalized matrix from part (e), compute the cosine distance

between each pair of users.

exercise 9.3.2 : in this exercise, we cluster items in the matrix of fig. 9.8.
do the following steps.

(a) cluster the eight items hierarchically into four clusters. the following
method should be used to cluster. replace all 3   s, 4   s, and 5   s by 1 and
replace 1   s, 2   s, and blanks by 0. use the jaccard distance to measure
the distance between the resulting column vectors. for clusters of more
than one element, take the distance between clusters to be the minimum
distance between pairs of elements, one from each cluster.

328

chapter 9. id126s

(b) then, construct from the original matrix of fig. 9.8 a new matrix whose
rows correspond to users, as before, and whose columns correspond to
clusters. compute the entry for a user and cluster of items by averaging
the nonblank entries for that user and all the items in the cluster.

(c) compute the cosine distance between each pair of users, according to your

matrix from part (b).

9.4 id84

an entirely di   erent approach to estimating the blank entries in the utility
matrix is to conjecture that the utility matrix is actually the product of two
long, thin matrices. this view makes sense if there are a relatively small set
of features of items and users that determine the reaction of most users to
most items. in this section, we sketch one approach to discovering two such
matrices; the approach is called    uv-decomposition,    and it is an instance of
a more general theory called svd (id166).

9.4.1 uv-decomposition

consider movies as a case in point. most users respond to a small number
of features; they like certain genres, they may have certain famous actors or
actresses that they like, and perhaps there are a few directors with a signi   cant
following. if we start with the utility matrix m , with n rows and m columns
(i.e., there are n users and m items), then we might be able to    nd a matrix
u with n rows and d columns and a matrix v with d rows and m columns,
such that u v closely approximates m in those entries where m is nonblank.
if so, then we have established that there are d dimensions that allow us to
characterize both users and items closely. we can then use the entry in the
product u v to estimate the corresponding blank entry in utility matrix m .
this process is called uv-decomposition of m .

5
3
2
2
4

   

               

2 4
1 2
3
5 4
4 5

4 3
4 1
1 4
3 5
4

   

               

=

u11 u12
u21 u22
u31 u32
u41 u42
u51 u52

   

               

   

               

  (cid:20) v11

v21

v12
v22

v13
v23

v14
v24

v15

v25 (cid:21)

figure 9.9: uv-decomposition of matrix m

example 9.11 : we shall use as a running example a 5-by-5 matrix m with
all but two of its entries known. we wish to decompose m into a 5-by-2 and
2-by-5 matrix, u and v , respectively. the matrices m , u , and v are shown
in fig. 9.9 with the known entries of m indicated and the matrices u and

9.4. id84

329

v shown with their entries as variables to be determined. this example is
essentially the smallest nontrivial case where there are more known entries than
there are entries in u and v combined, and we therefore can expect that the
best decomposition will not yield a product that agrees exactly in the nonblank
entries of m .    

9.4.2 root-mean-square error

while we can pick among several measures of how close the product u v is to
m , the typical choice is the root-mean-square error (rmse), where we

1. sum, over all nonblank entries in m the square of the di   erence between

that entry and the corresponding entry in the product u v .

2. take the mean (average) of these squares by dividing by the number of

terms in the sum (i.e., the number of nonblank entries in m ).

3. take the square root of the mean.

minimizing the sum of the squares is the same as minimizing the square root
of the average square, so we generally omit the last two steps in our running
example.

1
1
1
1
1

   

               

1
1
1
1
1

   

               

  (cid:20) 1 1

1 1

1 1
1 1

1

1 (cid:21) =

2
2
2
2
2

   

               

2 2
2 2
2 2
2 2
2 2

2 2
2 2
2 2
2 2
2 2

   

               

figure 9.10: matrices u and v with all entries 1

example 9.12 : suppose we guess that u and v should each have entries that
are all 1   s, as shown in fig. 9.10. this is a poor guess, since the product,
consisting of all 2   s, has entries that are much below the average of the entries
in m . nonetheless, we can compute the rmse for this u and v ;
in fact
the regularity in the entries makes the calculation especially easy to follow.
consider the    rst rows of m and u v . we subtract 2 (each entry in u v ) from
the entries in the    rst row of m , to get 3, 0, 2, 2, 1. we square and sum these
to get 18. in the second row, we do the same to get 1,   1, 0, 2,   1, square and
sum to get 7. in the third row, the second column is blank, so that entry is
ignored when computing the rmse. the di   erences are 0, 1,   1, 2 and the sum
of squares is 6. for the fourth row, the di   erences are 0, 3, 2, 1, 3 and the sum
of squares is 23. the    fth row has a blank entry in the last column, so the
di   erences are 2, 2, 3, 2 and the sum of squares is 21. when we sum the sums
from each of the    ve rows, we get 18 + 7 + 6 + 23 + 21 = 75. generally, we shall

330

chapter 9. id126s

stop at this point, but if we want to compute the true rmse, we divide by 23
(the number of nonblank entries in m ) and take the square root. in this case

p75/23 = 1.806 is the rmse.    

9.4.3

incremental computation of a uv-decomposition

finding the uv-decomposition with the least rmse involves starting with
some arbitrarily chosen u and v , and repeatedly adjusting u and v to make
the rmse smaller. we shall consider only adjustments to a single element
of u or v , although in principle, one could make more complex adjustments.
whatever adjustments we allow, in a typical example there will be many lo-
cal minima     matrices u and v such that no allowable adjustment reduces
the rmse. unfortunately, only one of these local minima will be the global
minimum     the matrices u and v that produce the least possible rmse. to
increase our chances of    nding the global minimum, we need to pick many dif-
ferent starting points, that is, di   erent choices of the initial matrices u and v .
however, there is never a guarantee that our best local minimum will be the
global minimum.

we shall start with the u and v of fig. 9.10, where all entries are 1, and do
a few adjustments to some of the entries,    nding the values of those entries that
give the largest possible improvement to the rmse. from these examples, the
general calculation should become obvious, but we shall follow the examples
by the formula for minimizing the rmse by changing a single entry. in what
follows, we shall refer to entries of u and v by their variable names u11, and
so on, as given in fig. 9.9.

example 9.13 : suppose we start with u and v as in fig. 9.10, and we decide
to alter u11 to reduce the rmse as much as possible. let the value of u11 be
x. then the new u and v can be expressed as in fig. 9.11.

   

               

x 1
1
1
1
1
1
1
1
1

   

               

  (cid:20) 1

1

1
1

1 1
1 1

1

1 (cid:21) =

   

               

x + 1 x + 1 x + 1 x + 1 x + 1
2
2
2
2

2
2
2
2

2
2
2
2

2
2
2
2

2
2
2
2

   

               

figure 9.11: making u11 a variable

notice that the only entries of the product that have changed are those in
the    rst row. thus, when we compare u v with m , the only change to the
rmse comes from the    rst row. the contribution to the sum of squares from
the    rst row is

(cid:0)5     (x + 1)(cid:1)2

+(cid:0)2     (x + 1)(cid:1)2

+(cid:0)4     (x + 1)(cid:1)2

+(cid:0)4     (x + 1)(cid:1)2

+(cid:0)3     (x + 1)(cid:1)2

9.4. id84

331

this sum simpli   es to

(4     x)2 + (1     x)2 + (3     x)2 + (3     x)2 + (2     x)2

we want the value of x that minimizes the sum, so we take the derivative and
set that equal to 0, as:

   2   (cid:0)(4     x) + (1     x) + (3     x) + (3     x) + (2     x)(cid:1) = 0

or    2    (13     5x) = 0, from which it follows that x = 2.6.

   

               

2.6 1
1
1
1
1
1
1
1
1

   

               

  (cid:20) 1 1

1 1

1 1
1 1

1

1 (cid:21) =

   

               

3.6 3.6 3.6 3.6
2
2
2
2

2
2
2
2

2
2
2
2

2
2
2
2

3.6
2
2
2
2

   

               

figure 9.12: the best value for u11 is found to be 2.6

figure 9.12 shows u and v after u11 has been set to 2.6. note that the sum
of the squares of the errors in the    rst row has been reduced from 18 to 5.2, so
the total rmse (ignoring average and square root) has been reduced from 75
to 62.2.

   

               

2.6 1
1
1
1
1
1
1
1
1

   

               

  (cid:20) y

1

1 1
1 1

1
1

1

1 (cid:21) =

2.6y + 1
y + 1
y + 1
y + 1
y + 1

   

               

3.6 3.6 3.6 3.6
2
2
2
2

2
2
2
2

2
2
2
2

2
2
2
2

   

               

figure 9.13: v11 becomes a variable y

suppose our next entry to vary is v11. let the value of this entry be y, as
suggested in fig. 9.13. only the    rst column of the product is a   ected by y, so
we need only to compute the sum of the squares of the di   erences between the
entries in the    rst columns of m and u v . this sum is

(cid:0)5    (2.6y + 1)(cid:1)2

+(cid:0)3    (y + 1)(cid:1)2

this expression simpli   es to

+(cid:0)2    (y + 1)(cid:1)2

+(cid:0)2    (y + 1)(cid:1)2

+(cid:0)4    (y + 1)(cid:1)2

(4     2.6y)2 + (2     y)2 + (1     y)2 + (1     y)2 + (3     y)2

as before, we    nd the minimum value of this expression by di   erentiating and
equating to 0, as:

   2   (cid:0)2.6(4     2.6y) + (2     y) + (1     y) + (1     y) + (3     y)(cid:1) = 0

332

chapter 9. id126s

   

               

2.6 1
1
1
1
1
1
1
1
1

   

               

  (cid:20) 1.617 1

1

1

1
1

1 1

1 1 (cid:21) =

   

               

5.204 3.6 3.6 3.6 3.6
2.617 2
2.617 2
2.617 2
2.617 2

2
2
2
2

2
2
2
2

2
2
2
2

   

               

figure 9.14: replace y by 1.617

the solution for y is y = 17.4/10.76 = 1.617. the improved estimates of u and
v are shown in fig. 9.14.

we shall do one more change, to illustrate what happens when entries of m
are blank. we shall vary u31, calling it z temporarily. the new u and v are
shown in fig. 9.15. the value of z a   ects only the entries in the third row.

2.6

1

z

1

1

   

               

1

1

1

1

1

   
  (cid:20) 1.617

1

               

1

1

1

1

1

1

1

1 (cid:21) =

5.204

2.617

3.6

2

3.6

2

3.6

2

3.6

2

1.617z + 1

z + 1

z + 1

z + 1

z + 1

2.617

2.617

2

2

2

2

2

2

2

2

   

               

   

               

figure 9.15: u31 becomes a variable z

we can express the sum of the squares of the errors as

(cid:0)2     (1.617z + 1)(cid:1)2

+(cid:0)3     (z + 1)(cid:1)2

+(cid:0)1     (z + 1)(cid:1)2

+(cid:0)4     (z + 1)(cid:1)2

note that there is no contribution from the element in the second column of
the third row, since this element is blank in m . the expression simpli   es to

(1     1.617z)2 + (2     z)2 + (   z)2 + (3     z)2

the usual process of setting the derivative to 0 gives us

   2   (cid:0)1.617(1     1.617z) + (2     z) + (   z) + (3     z)(cid:1) = 0

whose solution is z = 6.617/5.615 = 1.178. the next estimate of the decompo-
sition u v is shown in fig. 9.16.    

9.4.4 optimizing an arbitrary element

having seen some examples of picking the optimum value for a single element in
the matrix u or v , let us now develop the general formula. as before, assume

9.4. id84

333

2.6

1

1.178

1

1

   

               

1

1

1

1

1

   
  (cid:20) 1.617

1

               

1

1

1

1

1

1

1

1 (cid:21) =

5.204

3.6

2.617

2

3.6

2

3.6

2

3.6

2

2.905

2.178

2.178

2.178

2.178

2.617

2.617

2

2

2

2

2

2

2

2

   

               

   

               

figure 9.16: replace z by 1.178

that m is an n-by-m utility matrix with some entries blank, while u and v
are matrices of dimensions n-by-d and d-by-m, for some d. we shall use mij,
uij, and vij for the entries in row i and column j of m , u , and v , respectively.
also, let p = u v , and use pij for the element in row i and column j of the
product matrix p .

suppose we want to vary urs and    nd the value of this element that mini-
mizes the rmse between m and u v . note that urs a   ects only the elements
in row r of the product p = u v . thus, we need only concern ourselves with
the elements

d

prj =

xk=1

urkvkj = xk6=s

urkvkj + xvsj

for all values of j such that mrj is nonblank. in the expression above, we have
replaced urs, the element we wish to vary, by a variable x, and we use the
convention

    pk6=s is shorthand for the sum for k = 1, 2, . . . , d, except for k = s.

if mrj is a nonblank entry of the matrix m , then the contribution of this

element to the sum of the squares of the errors is

(mrj     prj)2 = (cid:0)mrj    xk6=s

urkvkj     xvsj(cid:1)2

we shall use another convention:

    pj is shorthand for the sum over all j such that mrj is nonblank.

then we can write the sum of the squares of the errors that are a   ected by

the value of x = urs as

xj (cid:0)mrj    xk6=s

urkvkj     xvsj(cid:1)2

take the derivative of the above with respect to x, and set it equal to 0, in
order to    nd the value of x that minimizes the rmse. that is,

xj

   2vsj(cid:0)mrj    xk6=s

urkvkj     xvsj(cid:1) = 0

334

chapter 9. id126s

as in the previous examples, the common factor    2 can be dropped. we solve
the above equation for x, and get

there is an analogous formula for the optimum value of an element of v . if

we want to vary vrs = y, then the value of y that minimizes the rmse is

x = pj vsj(cid:0)mrj    pk6=s urkvkj(cid:1)

y = pi uir(cid:0)mis    pk6=r uikvks(cid:1)

pj v2

sj

pi u2

ir

here, pi is shorthand for the sum over all i such that mis is nonblank, and
pk6=r is the sum over all values of k between 1 and d, except for k = r.

9.4.5 building a complete uv-decomposition algorithm

now, we have the tools to search for the global optimum decomposition of a
utility matrix m . there are four areas where we shall discuss the options.

1. preprocessing of the matrix m .

2. initializing u and v .

3. ordering the optimization of the elements of u and v .

4. ending the attempt at optimization.

preprocessing

because the di   erences in the quality of items and the rating scales of users are
such important factors in determining the missing elements of the matrix m , it
is often useful to remove these in   uences before doing anything else. the idea
was introduced in section 9.3.1. we can subtract from each nonblank element
mij the average rating of user i. then, the resulting matrix can be modi   ed
by subtracting the average rating (in the modi   ed matrix) of item j. it is also
possible to    rst subtract the average rating of item j and then subtract the
average rating of user i in the modi   ed matrix. the results one obtains from
doing things in these two di   erent orders need not be the same, but will tend
to be close. a third option is to normalize by subtracting from mij the average
of the average rating of user i and item j, that is, subtracting one half the sum
of the user average and the item average.

if we choose to normalize m , then when we make predictions, we need to
undo the id172. that is, if whatever prediction method we use results
in estimate e for an element mij of the normalized matrix, then the value
we predict for mij in the true utility matrix is e plus whatever amount was
subtracted from row i and from column j during the id172 process.

9.4. id84

335

initialization

as we mentioned, it is essential that there be some randomness in the way we
seek an optimum solution, because the existence of many local minima justi   es
our running many di   erent optimizations in the hope of reaching the global
minimum on at least one run. we can vary the initial values of u and v , or we
can vary the way we seek the optimum (to be discussed next), or both.

a simple starting point for u and v is to give each element the same value,
and a good choice for this value is that which gives the elements of the product
u v the average of the nonblank elements of m . note that if we have normalized
m , then this value will necessarily be 0. if we have chosen d as the lengths of
the short sides of u and v , and a is the average nonblank element of m , then

if we want many starting points for u and v , then we can perturb the

the elements of u and v should be pa/d.
value pa/d randomly and independently for each of the elements. there are

many options for how we do the perturbation. we have a choice regarding
the distribution of the di   erence. for example we could add to each element a
normally distributed value with mean 0 and some chosen standard deviation.
or we could add a value uniformly chosen from the range    c to +c for some c.

performing the optimization

in order to reach a local minimum from a given starting value of u and v , we
need to pick an order in which we visit the elements of u and v . the simplest
thing to do is pick an order, e.g., row-by-row, for the elements of u and v ,
and visit them in round-robin fashion. note that just because we optimized an
element once does not mean we cannot    nd a better value for that element after
other elements have been adjusted. thus, we need to visit elements repeatedly,
until we have reason to believe that no further improvements are possible.

alternatively, we can follow many di   erent optimization paths from a single
starting value by randomly picking the element to optimize. to make sure that
every element is considered in each round, we could instead choose a permuta-
tion of the elements and follow that order for every round.

converging to a minimum

ideally, at some point the rmse becomes 0, and we know we cannot do better.
in practice, since there are normally many more nonblank elements in m than
there are elements in u and v together, we have no right to expect that we
can reduce the rmse to 0. thus, we have to detect when there is little bene   t
to be had in revisiting elements of u and/or v . we can track the amount of
improvement in the rmse obtained in one round of the optimization, and stop
when that improvement falls below a threshold. a small variation is to observe
the improvements resulting from the optimization of individual elements, and
stop when the maximum improvement during a round is below a threshold.

336

chapter 9. id126s

id119

the technique for    nding a uv-decomposition discussed in section 9.4
is an example of id119. we are given some data points     the
nonblank elements of the matrix m     and for each data point we    nd
the direction of change that most decreases the error function: the rmse
between the current u v product and m . we shall have much more to
say about id119 in section 12.3.4. it is also worth noting that
while we have described the method as visiting each nonblank point of
m several times until we approach a minimum-error decomposition, that
may well be too much work on a large matrix m . thus, an alternative
approach has us look at only a randomly chosen fraction of the data when
seeking to minimize the error. this approach, called stochastic gradient
descent is discussed in section 12.3.5.

avoiding over   tting

one problem that often arises when performing a uv-decomposition is that we
arrive at one of the many local minima that conform well to the given data, but
picks up values in the data that don   t re   ect well the underlying process that
gives rise to the data. that is, although the rmse may be small on the given
data, it doesn   t do well predicting future data. there are several things that can
be done to cope with this problem, which is called over   tting by statisticians.

1. avoid favoring the    rst components to be optimized by only moving the
value of a component a fraction of the way, say half way, from its current
value toward its optimized value.

2. stop revisiting elements of u and v well before the process has converged.

3. take several di   erent uv decompositions, and when predicting a new
entry in the matrix m , take the average of the results of using each
decomposition.

9.4.6 exercises for section 9.4

exercise 9.4.1 : starting with the decomposition of fig. 9.10, we may choose
any of the 20 entries in u or v to optimize    rst. perform this    rst optimization
step assuming we choose: (a) u32 (b) v41.

exercise 9.4.2 : if we wish to start out, as in fig. 9.10, with all u and v
entries set to the same value, what value minimizes the rmse for the matrix
m of our running example?

9.5. the netflix challenge

337

exercise 9.4.3 : starting with the u and v matrices in fig. 9.16, do the
following in order:

(a) reconsider the value of u11. find its new best value, given the changes

that have been made so far.

(b) then choose the best value for u52.

(c) then choose the best value for v22.

exercise 9.4.4 : derive the formula for y (the optimum value of element vrs
given at the end of section 9.4.4.

exercise 9.4.5 : normalize the matrix m of our running example by:

(a) first subtracting from each element the average of its row, and then

subtracting from each element the average of its (modi   ed) column.

(b) first subtracting from each element the average of its column, and then

subtracting from each element the average of its (modi   ed) row.

are there any di   erences in the results of (a) and (b)?

9.5 the net   ix challenge

a signi   cant boost to research into id126s was given when
net   ix o   ered a prize of $1,000,000 to the    rst person or team to beat their
own recommendation algorithm, called cinematch, by 10%. after over three
years of work, the prize was awarded in september, 2009.

the net   ix challenge consisted of a published dataset, giving the ratings by
approximately half a million users on (typically small subsets of) approximately
17,000 movies. this data was selected from a larger dataset, and proposed al-
gorithms were tested on their ability to predict the ratings in a secret remainder
of the larger dataset. the information for each (user, movie) pair in the pub-
lished dataset included a rating (1   5 stars) and the date on which the rating
was made.

the rmse was used to measure the performance of algorithms. cinematch
has an rmse of approximately 0.95; i.e., the typical rating would be o    by
almost one full star. to win the prize, it was necessary that your algorithm
have an rmse that was at most 90% of the rmse of cinematch.

the bibliographic notes for this chapter include references to descriptions
of the winning algorithms. here, we mention some interesting and perhaps
unintuitive facts about the challenge.

    cinematch was not a very good algorithm. in fact, it was discovered early
that the obvious algorithm of predicting, for the rating by user u on movie
m, the average of:

338

chapter 9. id126s

1. the average rating given by u on all rated movies and

2. the average of the ratings for movie m by all users who rated that

movie.

was only 3% worse than cinematch.

    the uv-decomposition algorithm described in section 9.4 was found by
three students (michael harris, je   rey wang, and david kamm) to give
a 7% improvement over cinematch, when coupled with id172 and
a few other tricks.

    the winning entry was actually a combination of several di   erent algo-
rithms that had been developed independently. a second team, which
submitted an entry that would have won, had it been submitted a few
minutes earlier, also was a blend of independent algorithms. this strat-
egy     combining di   erent algorithms     has been used before in a number
of hard problems and is something worth remembering.

    several attempts have been made to use the data contained in imdb, the
internet movie database, to match the names of movies from the net   ix
challenge with their names in imdb, and thus extract useful information
not contained in the net   ix data itself.
imdb has information about
actors and directors, and classi   es movies into one or more of 28 genres.
it was found that genre and other information was not useful. one pos-
sible reason is the machine-learning algorithms were able to discover the
relevant information anyway, and a second is that the entity resolution
problem of matching movie names as given in net   ix and imdb data is
not that easy to solve exactly.

    time of rating turned out to be useful. it appears there are movies that
are more likely to be appreciated by people who rate it immediately after
viewing than by those who wait a while and then rate it.    patch adams   
was given as an example of such a movie. conversely, there are other
movies that were not liked by those who rated it immediately, but were
better appreciated after a while;    memento    was cited as an example.
while one cannot tease out of the data information about how long was
the delay between viewing and rating, it is generally safe to assume that
most people see a movie shortly after it comes out. thus, one can examine
the ratings of any movie to see if its ratings have an upward or downward
slope with time.

9.6 summary of chapter 9

    utility matrices: id126s deal with users and items.
a utility matrix o   ers known information about the degree to which a
user likes an item. normally, most entries are unknown, and the essential

9.6. summary of chapter 9

339

problem of recommending items to users is predicting the values of the
unknown entries based on the values of the known entries.

    two classes of id126s: these systems attempt to pre-
dict a user   s response to an item by discovering similar items and the
response of the user to those. one class of id126 is
content-based; it measures similarity by looking for common features of
the items. a second class of id126 uses collaborative    l-
tering; these measure similarity of users by their item preferences and/or
measure similarity of items by the users who like them.

    item pro   les: these consist of features of items. di   erent kinds of items
have di   erent features on which content-based similarity can be based.
features of documents are typically important or unusual words. prod-
ucts have attributes such as screen size for a television. media such as
movies have a genre and details such as actor or performer. tags can also
be used as features if they can be acquired from interested users.

    user pro   les: a content-based collaborative    ltering system can con-
struct pro   les for users by measuring the frequency with which features
appear in the items the user likes. we can then estimate the degree to
which a user will like an item by the closeness of the item   s pro   le to the
user   s pro   le.

    classi   cation of items: an alternative to constructing a user pro   le is to
build a classi   er for each user, e.g., a decision tree. the row of the utility
matrix for that user becomes the training data, and the classi   er must
predict the response of the user to all items, whether or not the row had
an entry for that item.

    similarity of rows and columns of the utility matrix : collaborative    l-
tering algorithms must measure the similarity of rows and/or columns
of the utility matrix. jaccard distance is appropriate when the matrix
consists only of 1   s and blanks (for    not rated   ). cosine distance works
for more general values in the utility matrix. it is often useful to normal-
ize the utility matrix by subtracting the average value (either by row, by
column, or both) before measuring the cosine distance.

    id91 users and items: since the utility matrix tends to be mostly
blanks, distance measures such as jaccard or cosine often have too little
data with which to compare two rows or two columns. a preliminary
step or steps, in which similarity is used to cluster users and/or items
into small groups with strong similarity, can help provide more common
components with which to compare rows or columns.

    uv-decomposition: one way of predicting the blank values in a utility
matrix is to    nd two long, thin matrices u and v , whose product is an
approximation to the given utility matrix. since the matrix product u v

340

chapter 9. id126s

gives values for all user-item pairs, that value can be used to predict the
value of a blank in the utility matrix. the intuitive reason this method
makes sense is that often there are a relatively small number of issues (that
number is the    thin    dimension of u and v ) that determine whether or
not a user likes an item.

    root-mean-square error : a good measure of how close the product u v
is to the given utility matrix is the rmse (root-mean-square error). the
rmse is computed by averaging the square of the di   erences between
u v and the utility matrix, in those elements where the utility matrix is
nonblank. the square root of this average is the rmse.

    computing u and v : one way of    nding a good choice for u and v in a
uv-decomposition is to start with arbitrary matrices u and v . repeat-
edly adjust one of the elements of u or v to minimize the rmse between
the product u v and the given utility matrix. the process converges to
a local optimum, although to have a good chance of obtaining a global
optimum we must either repeat the process from many starting matrices,
or search from the starting point in many di   erent ways.

    the net   ix challenge: an important driver of research into recommenda-
tion systems was the net   ix challenge. a prize of $1,000,000 was o   ered
for a contestant who could produce an algorithm that was 10% better
than net   ix   s own algorithm at predicting movie ratings by users. the
prize was awarded in sept., 2009.

9.7 references for chapter 9

[1] is a survey of id126s as of 2005. the argument regard-
ing the importance of the long tail in on-line systems is from [2], which was
expanded into a book [3].

[8] discusses the use of computer games to extract tags for items.
see [5] for a discussion of item-item similarity and how amazon designed

its collaborative-   ltering algorithm for product recommendations.

there are three papers describing the three algorithms that, in combination,

won the netflix challenge. they are [4], [6], and [7].

1. g. adomavicius and a. tuzhilin,    towards the next generation of rec-
ommender systems: a survey of the state-of-the-art and possible exten-
sions,    ieee trans. on data and knowledge engineering 17:6, pp. 734   
749, 2005.

2. c. anderson,

http://www.wired.com/wired/archive/12.10/tail.html

9.7. references for chapter 9

341

2004.

3. c. anderson, the long tail: why the future of business is selling less

of more, hyperion books, new york, 2006.

4. y. koren,    the bellkor solution to the net   ix grand prize,   

www.netflixprize.com/assets/grandprize2009 bpc bellkor.pdf

2009.

5. g. linden, b. smith, and j. york,    amazon.com recommendations: item-
to-item collaborative    ltering,    internet computing 7:1, pp. 76   80, 2003.

6. m. piotte and m. chabbert,    the pragmatic theory solution to the net-

   ix grand prize,   

www.netflixprize.com/assets/

grandprize2009 bpc pragmatictheory.pdf

2009.

7. a. toscher, m. jahrer, and r. bell,    the bigchaos solution to the net   ix

grand prize,   

www.netflixprize.com/assets/grandprize2009 bpc bigchaos.pdf

2009.

8. l. von ahn,    games with a purpose,    ieee computer magazine, pp. 96   

98, june 2006.

342

chapter 9. id126s

chapter 10

mining social-network
graphs

there is much information to be gained by analyzing the large-scale data that
is derived from social networks. the best-known example of a social network
is the    friends    relation found on sites like facebook. however, as we shall see
there are many other sources of data that connect people or other entities.

in this chapter, we shall study techniques for analyzing such networks. an
important question about a social network is how to identify    communities,   
that is, subsets of the nodes (people or other entities that form the network)
with unusually strong connections. some of the techniques used to identify
communities are similar to the id91 algorithms we discussed in chapter 7.
however, communities almost never partition the set of nodes in a network.
rather, communities usually overlap. for example, you may belong to several
communities of friends or classmates. the people from one community tend to
know each other, but people from two di   erent communities rarely know each
other. you would not want to be assigned to only one of the communities, nor
would it make sense to cluster all the people from all your communities into
one cluster.

also in this chapter we explore e   cient algorithms for discovering other
properties of graphs. we look at    simrank,    a way to discover similarities
among nodes of a graph. we explore triangle counting as a way to measure the
connectedness of a community. we give e   cient algorithms for exact and ap-
proximate measurement of the neighborhood sizes of nodes in a graph. finally,
we look at e   cient algorithms for computing the transitive closure.

10.1 social networks as graphs

we begin our discussion of social networks by introducing a graph model. not
every graph is a suitable representation of what we intuitively regard as a social

343

344

chapter 10. mining social-network graphs

network. we therefore discuss the idea of    locality,    the property of social
networks that says nodes and edges of the graph tend to cluster in communities.
this section also looks at some of the kinds of social networks that occur in
practice.

10.1.1 what is a social network?

when we think of a social network, we think of facebook, twitter, google+,
or another website that is called a    social network,    and indeed this kind of
network is representative of the broader class of networks called    social.    the
essential characteristics of a social network are:

1. there is a collection of entities that participate in the network. typically,
these entities are people, but they could be something else entirely. we
shall discuss some other examples in section 10.1.3.

2. there is at least one relationship between entities of the network. on
facebook or its ilk, this relationship is called friends. sometimes the
relationship is all-or-nothing; two people are either friends or they are
not. however, in other examples of social networks, the relationship has a
degree. this degree could be discrete; e.g., friends, family, acquaintances,
or none as in google+.
it could be a real number; an example would
be the fraction of the average day that two people spend talking to each
other.

3. there is an assumption of nonrandomness or locality. this condition is
the hardest to formalize, but the intuition is that relationships tend to
cluster. that is, if entity a is related to both b and c, then there is a
higher id203 than average that b and c are related.

10.1.2 social networks as graphs

social networks are naturally modeled as graphs, which we sometimes refer to
as a social graph. the entities are the nodes, and an edge connects two nodes
if the nodes are related by the relationship that characterizes the network. if
there is a degree associated with the relationship, this degree is represented by
labeling the edges. often, social graphs are undirected, as for the facebook
friends graph. but they can be directed graphs, as for example the graphs of
followers on twitter or google+.

example 10.1 : figure 10.1 is an example of a tiny social network. the
entities are the nodes a through g. the relationship, which we might think of
as    friends,    is represented by the edges. for instance, b is friends with a, c,
and d.

is this graph really typical of a social network, in the sense that it exhibits
locality of relationships? first, note that the graph has nine edges out of the

10.1. social networks as graphs

345

a

b

c

d

g

e

f

figure 10.1: example of a small social network

(cid:0)7
2(cid:1) = 21 pairs of nodes that could have had an edge between them. suppose

x, y , and z are nodes of fig. 10.1, with edges between x and y and also
between x and z. what would we expect the id203 of an edge between
y and z to be? if the graph were large, that id203 would be very close
to the fraction of the pairs of nodes that have edges between them, i.e., 9/21
= .429 in this case. however, because the graph is small, there is a noticeable
di   erence between the true id203 and the ratio of the number of edges to
the number of pairs of nodes. since we already know there are edges (x, y )
and (x, z), there are only seven edges remaining. those seven edges could run
between any of the 19 remaining pairs of nodes. thus, the id203 of an
edge (y, z) is 7/19 = .368.

now, we must compute the id203 that the edge (y, z) exists in fig.
10.1, given that edges (x, y ) and (x, z) exist. what we shall actually count
is pairs of nodes that could be y and z, without worrying about which node
is y and which is z.
if x is a, then y and z must be b and c, in some
order. since the edge (b, c) exists, a contributes one positive example (where
the edge does exist) and no negative examples (where the edge is absent). the
cases where x is c, e, or g are essentially the same. in each case, x has only
two neighbors, and the edge between the neighbors exists. thus, we have seen
four positive examples and zero negative examples so far.

now, consider x = f . f has three neighbors, d, e, and g. there are edges
between two of the three pairs of neighbors, but no edge between g and e.
thus, we see two more positive examples and we see our    rst negative example.
if x = b, there are again three neighbors, but only one pair of neighbors,
a and c, has an edge. thus, we have two more negative examples, and one
positive example, for a total of seven positive and three negative. finally, when
x = d, there are four neighbors. of the six pairs of neighbors, only two have
edges between them.

thus, the total number of positive examples is nine and the total number
of negative examples is seven. we see that in fig. 10.1, the fraction of times
the third edge exists is thus 9/16 = .563. this fraction is considerably greater
than the .368 expected value for that fraction. we conclude that fig. 10.1 does
indeed exhibit the locality expected in a social network.    

346

chapter 10. mining social-network graphs

10.1.3 varieties of social networks

there are many examples of social networks other than    friends    networks.
here, let us enumerate some of the other examples of networks that also exhibit
locality of relationships.

telephone networks

here the nodes represent phone numbers, which are really individuals. there
is an edge between two nodes if a call has been placed between those phones
in some    xed period of time, such as last month, or    ever.    the edges could
be weighted by the number of calls made between these phones during the
period. communities in a telephone network will form from groups of people
that communicate frequently: groups of friends, members of a club, or people
working at the same company, for example.

email networks

the nodes represent email addresses, which are again individuals. an edge
represents the fact that there was at least one email in at least one direction
between the two addresses. alternatively, we may only place an edge if there
were emails in both directions.
in that way, we avoid viewing spammers as
   friends    with all their victims. another approach is to label edges as weak or
strong. strong edges represent communication in both directions, while weak
edges indicate that the communication was in one direction only. the com-
munities seen in email networks come from the same sorts of groupings we
mentioned in connection with telephone networks. a similar sort of network
involves people who text other people through their cell phones.

collaboration networks

nodes represent individuals who have published research papers. there is an
edge between two individuals who published one or more papers jointly. option-
ally, we can label edges by the number of joint publications. the communities
in this network are authors working on a particular topic.

an alternative view of the same data is as a graph in which the nodes are
papers. two papers are connected by an edge if they have at least one author
in common. now, we form communities that are collections of papers on the
same topic.

there are several other kinds of data that form two networks in a similar
way. for example, we can look at the people who edit wikipedia articles and
the articles that they edit. two editors are connected if they have edited an
article in common. the communities are groups of editors that are interested
in the same subject. dually, we can build a network of articles, and connect
articles if they have been edited by the same person. here, we get communities
of articles on similar or related subjects.

10.1. social networks as graphs

347

in fact, the data involved in collaborative    ltering, as was discussed in
chapter 9, often can be viewed as forming a pair of networks, one for the
customers and one for the products. customers who buy the same sorts of
products, e.g., science-   ction books, will form communities, and dually, prod-
ucts that are bought by the same customers will form communities, e.g., all
science-   ction books.

other examples of social graphs

many other phenomena give rise to graphs that look something like social
graphs, especially exhibiting locality. examples include: information networks
(documents, web graphs, patents), infrastructure networks (roads, planes, water
pipes, powergrids), biological networks (genes, proteins, food-webs of animals
eating each other), as well as other types, like product co-purchasing networks
(e.g., groupon).

10.1.4 graphs with several node types

there are other social phenomena that involve entities of di   erent types. we
just discussed under the heading of    collaboration networks,    several kinds of
graphs that are really formed from two types of nodes. authorship networks
can be seen to have author nodes and paper nodes. in the discussion above, we
built two social networks by eliminating the nodes of one of the two types, but
we do not have to do that. we can rather think of the structure as a whole.

for a more complex example, users at a site like del.icio.us place tags on
web pages. there are thus three di   erent kinds of entities: users, tags, and
pages. we might think that users were somehow connected if they tended to
use the same tags frequently, or if they tended to tag the same pages. similarly,
tags could be considered related if they appeared on the same pages or were
used by the same users, and pages could be considered similar if they had many
of the same tags or were tagged by many of the same users.

the natural way to represent such information is as a k-partite graph for
some k > 1. we met bipartite graphs, the case k = 2, in section 8.3.
in
general, a k-partite graph consists of k disjoint sets of nodes, with no edges
between nodes of the same set.

example 10.2 : figure 10.2 is an example of a tripartite graph (the case k = 3
of a k-partite graph). there are three sets of nodes, which we may think of
as users {u1, u2}, tags {t1, t2, t3, t4}, and web pages {w1, w2, w3}. notice
that all edges connect nodes from two di   erent sets. we may assume this graph
represents information about the three kinds of entities. for example, the edge
(u1, t2) means that user u1 has placed the tag t2 on at least one page. note
that the graph does not tell us a detail that could be important: who placed
which tag on which page? to represent such ternary information would require
a more complex representation, such as a database relation with three columns
corresponding to users, tags, and pages.    

348

chapter 10. mining social-network graphs

u

1

u

2

w

1

w

2

w

3

t

1

t

2

t

3

t

4

figure 10.2: a tripartite graph representing users, tags, and web pages

10.1.5 exercises for section 10.1

exercise 10.1.1 : it is possible to think of the edges of one graph g as the
nodes of another graph g   . we construct g    from g by the dual construction:

1. if (x, y ) is an edge of g, then xy , representing the unordered set of x
and y is a node of g   . note that xy and y x represent the same node
of g   , not two di   erent nodes.

2. if (x, y ) and (x, z) are edges of g, then in g    there is an edge between
xy and xz. that is, nodes of g    have an edge between them if the
edges of g that these nodes represent have a node (of g) in common.

(a) if we apply the dual construction to a network of friends, what is the

interpretation of the edges of the resulting graph?

(b) apply the dual construction to the graph of fig. 10.1.

! (c) how is the degree of a node xy in g    related to the degrees of x and y

in g?

!! (d) the number of edges of g    is related to the degrees of the nodes of g by

a certain formula. discover that formula.

! (e) what we called the dual is not a true dual, because applying the con-
struction to g    does not necessarily yield a graph isomorphic to g. give
an example graph g where the dual of g    is isomorphic to g and another
example where the dual of g    is not isomorphic to g.

10.2. id91 of social-network graphs

349

10.2 id91 of social-network graphs

an important aspect of social networks is that they contain communities of
entities that are connected by many edges. these typically correspond to groups
of friends at school or groups of researchers interested in the same topic, for
example. in this section, we shall consider id91 of the graph as a way to
identify communities. it turns out that the techniques we learned in chapter 7
are generally unsuitable for the problem of id91 social-network graphs.

10.2.1 distance measures for social-network graphs

if we were to apply standard id91 techniques to a social-network graph,
our    rst step would be to de   ne a distance measure. when the edges of the
graph have labels, these labels might be usable as a distance measure, depending
on what they represented. but when the edges are unlabeled, as in a    friends   
graph, there is not much we can do to de   ne a suitable distance.

our    rst instinct is to assume that nodes are close if they have an edge
between them and distant if not. thus, we could say that the distance d(x, y)
is 0 if there is an edge (x, y) and 1 if there is no such edge. we could use any
other two values, such as 1 and    , as long as the distance is closer when there
is an edge.
neither of these two-valued    distance measures        0 and 1 or 1 and         is
a true distance measure. the reason is that they violate the triangle inequality
when there are three nodes, with two edges between them. that is, if there are
edges (a, b) and (b, c), but no edge (a, c), then the distance from a to c
exceeds the sum of the distances from a to b to c. we could    x this problem
by using, say, distance 1 for an edge and distance 1.5 for a missing edge. but
the problem with two-valued distance functions is not limited to the triangle
inequality, as we shall see in the next section.

10.2.2 applying standard id91 methods

recall from section 7.1.2 that there are two general approaches to id91:
hierarchical (agglomerative) and point-assignment. let us consider how each
of these would work on a social-network graph. first, consider the hierarchical
methods covered in section 7.2. in particular, suppose we use as the intercluster
distance the minimum distance between nodes of the two clusters.

hierarchical id91 of a social-network graph starts by combining some
two nodes that are connected by an edge. successively, edges that are not
between two nodes of the same cluster would be chosen randomly to combine
the clusters to which their two nodes belong. the choices would be random,
because all distances represented by an edge are the same.

example 10.3 : consider again the graph of fig. 10.1, repeated here as fig.
10.3. first, let us agree on what the communities are. at the highest level,

350

chapter 10. mining social-network graphs

it appears that there are two communities {a, b, c} and {d, e, f, g}. how-
ever, we could also view {d, e, f} and {d, f, g} as two subcommunities of
{d, e, f, g}; these two subcommunities overlap in two of their members, and
thus could never be identi   ed by a pure id91 algorithm. finally, we could
consider each pair of individuals that are connected by an edge as a community
of size 2, although such communities are uninteresting.

a

b

c

d

g

e

f

figure 10.3: repeat of fig. 10.1

the problem with hierarchical id91 of a graph like that of fig. 10.3 is
that at some point we are likely to chose to combine b and d, even though
they surely belong in di   erent clusters. the reason we are likely to combine b
and d is that d, and any cluster containing it, is as close to b and any cluster
containing it, as a and c are to b. there is even a 1/9 id203 that the
   rst thing we do is to combine b and d into one cluster.

there are things we can do to reduce the id203 of error. we can
run hierarchical id91 several times and pick the run that gives the most
coherent clusters. we can use a more sophisticated method for measuring the
distance between clusters of more than one node, as discussed in section 7.2.3.
but no matter what we do, in a large graph with many communities there is a
signi   cant chance that in the initial phases we shall use some edges that connect
two nodes that do not belong together in any large community.    

now, consider a point-assignment approach to id91 social networks.
again, the fact that all edges are at the same distance will introduce a number
of random factors that will lead to some nodes being assigned to the wrong
cluster. an example should illustrate the point.

example 10.4 : suppose we try a id116 approach to id91 fig. 10.3.
as we want two clusters, we pick k = 2. if we pick two starting nodes at random,
they might both be in the same cluster. if, as suggested in section 7.3.2, we
start with one randomly chosen node and then pick another as far away as
possible, we don   t do much better; we could thereby pick any pair of nodes not
connected by an edge, e.g., e and g in fig. 10.3.

however, suppose we do get two suitable starting nodes, such as b and f .
we shall then assign a and c to the cluster of b and assign e and g to the
cluster of f . but d is as close to b as it is to f , so it could go either way, even
though it is    obvious    that d belongs with f .

10.2. id91 of social-network graphs

351

if the decision about where to place d is deferred until we have assigned
some other nodes to the clusters, then we shall probably make the right decision.
for instance, if we assign a node to the cluster with the shortest average distance
to all the nodes of the cluster, then d should be assigned to the cluster of f , as
long as we do not try to place d before any other nodes are assigned. however,
in large graphs, we shall surely make mistakes on some of the    rst nodes we
place.    

10.2.3 betweenness

since there are problems with standard id91 methods, several specialized
id91 techniques have been developed to    nd communities in social net-
works. in this section we shall consider one of the simplest, based on    nding
the edges that are least likely to be inside a community.

de   ne the betweenness of an edge (a, b) to be the number of pairs of nodes
x and y such that the edge (a, b) lies on the shortest path between x and y.
to be more precise, since there can be several shortest paths between x and y,
edge (a, b) is credited with the fraction of those shortest paths that include the
edge (a, b). as in golf, a high score is bad. it suggests that the edge (a, b) runs
between two di   erent communities; that is, a and b do not belong to the same
community.

example 10.5 : in fig. 10.3 the edge (b, d) has the highest betweenness, as
should surprise no one. in fact, this edge is on every shortest path between
any of a, b, and c to any of d, e, f , and g. its betweenness is therefore
3    4 = 12. in contrast, the edge (d, f ) is on only four shortest paths: those
from a, b, c, and d to f .    

10.2.4 the girvan-newman algorithm

in order to exploit the betweenness of edges, we need to calculate the number of
shortest paths going through each edge. we shall describe a method called the
girvan-newman (gn) algorithm, which visits each node x once and computes
the number of shortest paths from x to each of the other nodes that go through
each of the edges. the algorithm begins by performing a breadth-   rst search
(bfs) of the graph, starting at the node x. note that the level of each node in
the bfs presentation is the length of the shortest path from x to that node.
thus, the edges that go between nodes at the same level can never be part of
a shortest path from x.

edges between levels are called dag edges (   dag    stands for directed,
each dag edge will be part of at least one shortest path
acyclic graph).
from root x. if there is a dag edge (y, z), where y is at the level above z
(i.e., closer to the root), then we shall call y a parent of z and z a child of y ,
although parents are not necessarily unique in a dag as they would be in a
tree.

352

chapter 10. mining social-network graphs

1

e

1

d

1

f

1

b

g

2

1

a

1

c

level 1

level 2

level 3

figure 10.4: step 1 of the girvan-newman algorithm

example 10.6 : figure 10.4 is a breadth-   rst presentation of the graph of fig.
10.3, starting at node e. solid edges are dag edges and dashed edges connect
nodes at the same level.    

the second step of the gn algorithm is to label each node by the number of
shortest paths that reach it from the root. start by labeling the root 1. then,
from the top down, label each node y by the sum of the labels of its parents.

example 10.7 : in fig. 10.4 are the labels for each of the nodes. first, label
the root e with 1. at level 1 are the nodes d and f . each has only e as a
parent, so they too are labeled 1. nodes b and g are at level 2. b has only
d as a parent, so b   s label is the same as the label of d, which is 1. however,
g has parents d and f , so its label is the sum of their labels, or 2. finally, at
level 3, a and c each have only parent b, so their labels are the label of b,
which is 1.    

the third and    nal step is to calculate for each edge e the sum over all nodes
y of the fraction of shortest paths from the root x to y that go through e.
this calculation involves computing this sum for both nodes and edges, from
the bottom. each node other than the root is given a credit of 1, representing
the shortest path to that node. this credit may be divided among nodes and
edges above, since there could be several di   erent shortest paths to the node.
the rules for the calculation are as follows:

1. each leaf in the dag (a leaf is a node with no dag edges to nodes at

levels below) gets a credit of 1.

2. each node that is not a leaf gets a credit equal to 1 plus the sum of the

credits of the dag edges from that node to the level below.

10.2. id91 of social-network graphs

353

3. a dag edge e entering node z from the level above is given a share of the
credit of z proportional to the fraction of shortest paths from the root to
z that go through e. formally, let the parents of z be y1, y2, . . . , yk. let
pi be the number of shortest paths from the root to yi; this number was
computed in step 2 and is illustrated by the labels in fig. 10.4. then the
j=1 pj.

credit for the edge (yi, z) is the credit of z times pi divided by pk

after performing the credit calculation with each node as the root, we sum
the credits for each edge. then, since each shortest path will have been discov-
ered twice     once when each of its endpoints is the root     we must divide the
credit for each edge by 2.

example 10.8 : let us perform the credit calculation for the bfs presentation
of fig. 10.4. we shall start from level 3 and proceed upwards. first, a and c,
being leaves, get credit 1. each of these nodes have only one parent, so their
credit is given to the edges (b, a) and (b, c), respectively.

e

d

f

3

b

g

1

1

1

1

a

c

1

figure 10.5: final step of the girvan-newman algorithm     levels 3 and 2

at level 2, g is a leaf, so it gets credit 1. b is not a leaf, so it gets credit
equal to 1 plus the credits on the dag edges entering it from below. since
both these edges have credit 1, the credit of b is 3. intuitively 3 represents the
fact that all shortest paths from e to a, b, and c go through b. figure 10.5
shows the credits assigned so far.

now, let us proceed to level 1. b has only one parent, d, so the edge
(d, b) gets the entire credit of b, which is 3. however, g has two parents, d
and f . we therefore need to divide the credit of 1 that g has between the edges
(d, g) and (f, g). in what proportion do we divide? if you examine the labels
of fig. 10.4, you see that both d and f have label 1, representing the fact that
there is one shortest path from e to each of these nodes. thus, we give half
the credit of g to each of these edges; i.e., their credit is each 1/(1 + 1) = 0.5.
had the labels of d and f in fig. 10.4 been 5 and 3, meaning there were    ve

354

chapter 10. mining social-network graphs

shortest paths to d and only three to f , then the credit of edge (d, g) would
have been 5/8 and the credit of edge (f, g) would have been 3/8.

e

4.5

1.5

4.5

d

f

1.5

3

b

3

1

1

0.5

0.5

g

1

1

a

c

1

figure 10.6: final step of the girvan-newman algorithm     completing the
credit calculation

now, we can assign credits to the nodes at level 1. d gets 1 plus the credits
of the edges entering it from below, which are 3 and 0.5. that is, the credit of d
is 4.5. the credit of f is 1 plus the credit of the edge (f, g), or 1.5. finally, the
edges (e, d) and (e, f ) receive the credit of d and f , respectively, since each
of these nodes has only one parent. these credits are all shown in fig. 10.6.

the credit on each of the edges in fig. 10.6 is the contribution to the be-
tweenness of that edge due to shortest paths from e. for example, this contri-
bution for the edge (e, d) is 4.5.    

to complete the betweenness calculation, we have to repeat this calculation
for every node as the root and sum the contributions. finally, we must divide
by 2 to get the true betweenness, since every shortest path will be discovered
twice, once for each of its endpoints.

10.2.5 using betweenness to find communities

the betweenness scores for the edges of a graph behave something like a distance
measure on the nodes of the graph. it is not exactly a distance measure, because
it is not de   ned for pairs of nodes that are unconnected by an edge, and might
not satisfy the triangle inequality even when de   ned. however, we can cluster
by taking the edges in order of increasing betweenness and add them to the
graph one at a time. at each step, the connected components of the graph
form some clusters. the higher the betweenness we allow, the more edges we
get, and the larger the clusters become.

more commonly, this idea is expressed as a process of edge removal. start
with the graph and all its edges; then remove edges with the highest between-

10.2. id91 of social-network graphs

355

ness, until the graph has broken into a suitable number of connected compo-
nents.

example 10.9 : let us start with our running example, the graph of fig. 10.1.
we see it with the betweenness for each edge in fig. 10.7. the calculation of
the betweenness will be left to the reader. the only tricky part of the count
is to observe that between e and g there are two shortest paths, one going
through d and the other through f . thus, each of the edges (d, e), (e, f ),
(d, g), and (g, f ) are credited with half a shortest path.

a

1

5

c

b

5

12

4.5

d

g

4.5

4

1.5

e

f

1.5

figure 10.7: betweenness scores for the graph of fig. 10.1

clearly, edge (b, d) has the highest betweenness, so it is removed    rst.
that leaves us with exactly the communities we observed make the most sense,
namely: {a, b, c} and {d, e, f, g}. however, we can continue to remove
edges. next to leave are (a, b) and (b, c) with a score of 5, followed by (d, e)
and (d, g) with a score of 4.5. then, (d, f ), whose score is 4, would leave the
graph. we see in fig. 10.8 the graph that remains.

a

b

c

d

g

e

f

figure 10.8: all the edges with betweenness 4 or more have been removed

the    communities    of fig. 10.8 look strange. one implication is that a and
c are more closely knit to each other than to b. that is, in some sense b is a
   traitor    to the community {a, b, c} because he has a friend d outside that
community. likewise, d can be seen as a    traitor    to the group {d, e, f, g},
which is why in fig. 10.8, only e, f , and g remain connected.    

356

chapter 10. mining social-network graphs

speeding up the betweenness calculation

if we apply the method of section 10.2.4 to a graph of n nodes and e edges,
it takes o(ne) running time to compute the betweenness of each edge.
that is, bfs from a single node takes o(e) time, as do the two labeling
steps. we must start from each node, so there are n of the computations
described in section 10.2.4.

if the graph is large     and even a million nodes is large when the
algorithm takes o(ne) time     we cannot a   ord to execute it as suggested.
however, if we pick a subset of the nodes at random and use these as
the roots of breadth-   rst searches, we can get an approximation to the
betweenness of each edge that will serve in most applications.

10.2.6 exercises for section 10.2

exercise 10.2.1 : figure 10.9 is an example of a social-network graph. use
the girvan-newman approach to    nd the number of shortest paths from each
of the following nodes that pass through each of the edges. (a) a (b) b.

a

b

c

h

d

i

g

e

f

figure 10.9: graph for exercises

exercise 10.2.2 : using symmetry, the calculations of exercise 10.2.1 are all
you need to compute the betweenness of each edge. do the calculation.

exercise 10.2.3 : using the betweenness values from exercise 10.2.2, deter-
mine reasonable candidates for the communities in fig. 10.9 by removing all
edges with a betweenness above some threshold.

10.3. direct discovery of communities

357

10.3 direct discovery of communities

in the previous section we searched for communities by partitioning all the in-
dividuals in a social network. while this approach is relatively e   cient, it does
have several limitations. it is not possible to place an individual in two di   erent
communities, and everyone is assigned to a community. in this section, we shall
see a technique for discovering communities directly by looking for subsets of
the nodes that have a relatively large number of edges among them. interest-
ingly, the technique for doing this search on a large graph involves    nding large
frequent itemsets, as was discussed in chapter 6.

10.3.1 finding cliques

our    rst thought about how we could    nd sets of nodes with many edges
between them is to start by    nding a large clique (a set of nodes with edges
between any two of them). however, that task is not easy. not only is    nding
maximal cliques np-complete, but it is among the hardest of the np-complete
problems in the sense that even approximating the maximal clique is hard.
further, it is possible to have a set of nodes with almost all edges between
them, and yet have only relatively small cliques.

example 10.10 : suppose our graph has nodes numbered 1, 2, . . . , n and there
is an edge between two nodes i and j unless i and j have the same remain-
der when divided by k. then the fraction of possible edges that are actually
present is approximately (k     1)/k. there are many cliques of size k, of which
{1, 2, . . . , k} is but one example.
yet there are no cliques larger than k. to see why, observe that any set of
k + 1 nodes has two that leave the same remainder when divided by k. this
point is an application of the    pigeonhole principle.    since there are only k
di   erent remainders possible, we cannot have distinct remainders for each of
k + 1 nodes. thus, no set of k + 1 nodes can be a clique in this graph.    

10.3.2 complete bipartite graphs

recall our discussion of bipartite graphs from section 8.3. a complete bipartite
graph consists of s nodes on one side and t nodes on the other side, with all st
possible edges between the nodes of one side and the other present. we denote
this graph by ks,t. you should draw an analogy between complete bipartite
graphs as subgraphs of general bipartite graphs and cliques as subgraphs of
general graphs. in fact, a clique of s nodes is often referred to as a complete
graph and denoted ks, while a complete bipartite subgraph is sometimes called
a bi-clique.

while as we saw in example 10.10, it is not possible to guarantee that a
graph with many edges necessarily has a large clique, it is possible to guar-
antee that a bipartite graph with many edges has a large complete bipartite

358

chapter 10. mining social-network graphs

subgraph.1 we can regard a complete bipartite subgraph (or a clique if we
discovered a large one) as the nucleus of a community and add to it nodes
with many edges to existing members of the community. if the graph itself is
k-partite as discussed in section 10.1.4, then we can take nodes of two types
and the edges between them to form a bipartite graph. in this bipartite graph,
we can search for complete bipartite subgraphs as the nuclei of communities.
for instance, in example 10.2, we could focus on the tag and page nodes of a
graph like fig. 10.2 and try to    nd communities of tags and web pages. such a
community would consist of related tags and related pages that deserved many
or all of those tags.

however, we can also use complete bipartite subgraphs for community    nd-
ing in ordinary graphs where nodes all have the same type. divide the nodes
into two equal groups at random. if a community exists, then we would expect
about half its nodes to fall into each group, and we would expect that about
half its edges would go between groups. thus, we still have a reasonable chance
of identifying a large complete bipartite subgraph in the community. to this
nucleus we can add nodes from either of the two groups, if they have edges to
many of the nodes already identi   ed as belonging to the community.

10.3.3 finding complete bipartite subgraphs

suppose we are given a large bipartite graph g , and we want to    nd instances
of ks,t within it. it is possible to view the problem of    nding instances of ks,t
within g as one of    nding frequent itemsets. for this purpose, let the    items   
be the nodes on one side of g, which we shall call the left side. we assume that
the instance of ks,t we are looking for has t nodes on the left side, and we shall
also assume for e   ciency that t     s. the    baskets    correspond to the nodes
on the other side of g (the right side). the members of the basket for node v
are the nodes of the left side to which v is connected. finally, let the support
threshold be s, the number of nodes that the instance of ks,t has on the right
side.

we can now state the problem of    nding instances of ks,t as that of    nding
frequent itemsets f of size t. that is, if a set of t nodes on the left side is
frequent, then they all occur together in at least s baskets. but the baskets
are the nodes on the right side. each basket corresponds to a node that is
connected to all t of the nodes in f . thus, the frequent itemset of size t and s
of the baskets in which all those items appear form an instance of ks,t.

example 10.11 : recall the bipartite graph of fig. 8.1, which we repeat here as
fig. 10.10. the left side is the nodes {1, 2, 3, 4} and the right side is {a, b, c, d}.
the latter are the baskets, so basket a consists of    items    1 and 4; that is,
a = {1, 4}. similarly, b = {2, 3}, c = {1} and d = {3}.

1it is important to understand that we do not mean a generated subgraph     one formed
by selecting some nodes and including all edges. in this context, we only require that there
be edges between any pair of nodes on di   erent sides. it is also possible that some nodes on
the same side are connected by edges as well.

10.3. direct discovery of communities

359

1

2

3

4

a

b

c

d

figure 10.10: the bipartite graph from fig. 8.1

if s = 2 and t = 1, we must    nd itemsets of size 1 that appear in at least
two baskets. {1} is one such itemset, and {3} is another. however, in this tiny
example there are no itemsets for larger, more interesting values of s and t,
such as s = t = 2.    

10.3.4 why complete bipartite graphs must exist

we must now turn to the matter of demonstrating that any bipartite graph
with a su   ciently high fraction of the edges present will have an instance of
ks,t. in what follows, assume that the graph g has n nodes on the left and
another n nodes on the right. assume the two sides have the same number of
nodes simpli   es the calculation, but the argument generalizes to sides of any
size. finally, let d be the average degree of all nodes.

the argument involves counting the number of frequent itemsets of size t
that a basket with d items contributes to. when we sum this number over all
nodes on the right side, we get the total frequency of all the subsets of size t on

the left. when we divide by (cid:0)n

of size t. at least one must have a frequency that is at least average, so if this
average is at least s, we know an instance of ks,t exists.

t(cid:1), we get the average frequency of all itemsets

now, we provide the detailed calculation. suppose the degree of the ith
node on the right is di; that is, di is the size of the ith basket. then this

basket contributes to (cid:0)di
nodes on the right is pi(cid:0)di
simple example will suggest the reasoning: since (cid:0)di

t(cid:1) itemsets of size t. the total contribution of the n
t(cid:1). the value of this sum depends on the di   s, of

course. however, we know that the average value of di is d. it is known that
this sum is minimized when each di is d. we shall not prove this point, but a

t(cid:1) grows roughly as the tth

360

chapter 10. mining social-network graphs

power of di, moving 1 from a large di to some smaller dj will reduce the sum

t(cid:1) +(cid:0)dj
t (cid:1).

example 10.12 : suppose there are only two nodes, t = 2, and the average

of (cid:0)di
degree of the nodes is 4. then d1 + d2 = 8, and the sum of interest is (cid:0)d1
if d1 = d2 = 4, then the sum is (cid:0)4
d2 = 3, the sum is (cid:0)5
is (cid:0)6

2(cid:1)+(cid:0)d2
2(cid:1).
2(cid:1) = 6 + 6 = 12. however, if d1 = 5 and
2(cid:1) = 10 + 3 = 13. if d1 = 6 and d2 = 2, then the sum

2(cid:1) = 15 + 1 = 16.    

2(cid:1) +(cid:0)4

2(cid:1) +(cid:0)2

2(cid:1) +(cid:0)3

thus, in what follows, we shall assume that all nodes have the average degree
d. so doing minimizes the total contribution to the counts for the itemsets, and
thus makes it least likely that there will be a frequent itemset (itemset with
with support s or more) of size t. observe the following:

    the total contribution of the n nodes on the right to the counts of the
itemsets of size t is n(cid:0)d
t(cid:1).
    the number of itemsets of size t is (cid:0)n
t(cid:1).
    thus, the average count of an itemset of size t is n(cid:0)d

must be at least s if we are to argue that an instance of ks,t exists.

t(cid:1); this expression

t(cid:1)/(cid:0)n

if we expand the binomial coe   cients in terms of factorials, we    nd

n(cid:18)d

t(cid:19)/(cid:18)n

t(cid:19) = nd!(n     t)!t!/(cid:0)(d     t)!t!n!(cid:1) =

n(d)(d     1)       (d     t + 1)/(cid:0)n(n     1)       (n     t + 1)(cid:1)

to simplify the formula above, let us assume that n is much larger than d, and
d is much larger than t. then d(d     1)       (d     t + 1) is approximately dt, and
n(n     1)       (n     t + 1) is approximately nt. we thus require that

n(d/n)t     s

that is, if there is a community with n nodes on each side, the average degree
of the nodes is d, and n(d/n)t     s, then this community is guaranteed to have
a complete bipartite subgraph ks,t. moreover, we can    nd the instance of ks,t
e   ciently, using the methods of chapter 6, even if this small community is
embedded in a much larger graph. that is, we can treat all nodes in the entire
graph as baskets and as items, and run a-priori or one of its improvements on
the entire graph, looking for sets of t items with support s.

example 10.13 : suppose there is a community with 100 nodes on each side,
and the average degree of nodes is 50; i.e., half the possible edges exist. this
community will have an instance of ks,t, provided 100(1/2)t     s. for example,
if t = 2, then s can be as large as 25. if t = 3, s can be 11, and if t = 4, s can
be 6.

10.4. partitioning of graphs

361

unfortunately, the approximation we made gives us a bound on s that is a

little too high. if we revert to the original formula n(cid:0)d
for the case t = 4 we need 100(cid:0)50
4 (cid:1)     s. that is,
100    50    49    48    47
100    99    98    97     s

4(cid:1)/(cid:0)100

t(cid:1)/(cid:0)n

t(cid:1)     s, we see that

the expression on the left is not 6, but only 5.87. however, if the average
support for an itemset of size 4 is 5.87, then it is impossible that all those
itemsets have support 5 or less. thus, we can be sure that at least one itemset
of size 4 has support 6 or more, and an instance of k6.4 exists in this community.
   

10.3.5 exercises for section 10.3

exercise 10.3.1 : for the running example of a social network from fig. 10.1,
how many instances of ks,t are there for:

(a) s = 1 and t = 3.

(b) s = 2 and t = 2.

(c) s = 2 and t = 3.

exercise 10.3.2 : suppose there is a community of 2n nodes. divide the
community into two groups of n members, at random, and form the bipartite
graph between the two groups. suppose that the average degree of the nodes of
the bipartite graph is d. find the set of maximal pairs (t, s), with t     s, such
that an instance of ks,t is guaranteed to exist, for the following combinations
of n and d:

(a) n = 20 and d = 5.

(b) n = 200 and d = 150.

(c) n = 1000 and d = 400.

by    maximal,    we mean there is no di   erent pair (s   , t   ) such that both s        s
and t        t hold.

10.4 partitioning of graphs

in this section, we examine another approach to organizing social-network
graphs. we use some important tools from matrix theory (   spectral meth-
ods   ) to formulate the problem of partitioning a graph to minimize the number
of edges that connect di   erent components. the goal of minimizing the    cut   
size needs to be understood carefully before proceeding. for instance, if you

362

chapter 10. mining social-network graphs

just joined facebook, you are not yet connected to any friends. we do not
want to partition the friends graph with you in one group and the rest of the
world in the other group, even though that would partition the graph without
there being any edges that connect members of the two groups. this cut is not
desirable because the two components are too unequal in size.

10.4.1 what makes a good partition?

given a graph, we would like to divide the nodes into two sets so that the cut, or
set of edges that connect nodes in di   erent sets is minimized. however, we also
want to constrain the selection of the cut so that the two sets are approximately
equal in size. the next example illustrates the point.

example 10.14 : recall our running example of the graph in fig. 10.1. there,
it is evident that the best partition puts {a, b, c} in one set and {d, e, f, g}
in the other. the cut consists only of the edge (b, d) and is of size 1. no
nontrivial cut can be smaller.

a

b

c

h

d

g

e

f

smallest

cut

best cut

figure 10.11: the smallest cut might not be the best cut

in fig. 10.11 is a variant of our example, where we have added the node
h and two extra edges, (h, c) and (c, g). if all we wanted was to minimize
the size of the cut, then the best choice would be to put h in one set and all
the other nodes in the other set. but it should be apparent that if we reject
partitions where one set is too small, then the best we can do is to use the
cut consisting of edges (b, d) and (c, g), which partitions the graph into two
equal-sized sets {a, b, c, h} and {d, e, f, g}.    

10.4.2 normalized cuts

a proper de   nition of a    good    cut must balance the size of the cut itself
against the di   erence in the sizes of the sets that the cut creates. one choice

10.4. partitioning of graphs

363

that serves well is the    normalized cut.    first, de   ne the volume of a set s of
nodes, denoted vol (s), to be the number of edges with at least one end in s.
suppose we partition the nodes of a graph into two disjoint sets s and t .
let cut (s, t ) be the number of edges that connect a node in s to a node in t .
then the normalized cut value for s and t is

cut (s, t )

vol (s)

+

cut(s, t )

vol(t )

example 10.15 : again consider the graph of fig. 10.11. if we choose s = {h}
and t = {a, b, c, d, e, f, g}, then cut (s, t ) = 1. vol(s) = 1, because there
is only one edge connected to h. on the other hand, vol(t ) = 11, because all
the edges have at least one end at a node of t . thus, the normalized cut for
this partition is 1/1 + 1/11 = 1.09.

now, consider the preferred cut for this graph consisting of the edges (b, d)
and (c, g). then s = {a, b, c, h} and t = {d, e, f, g}. cut (s, t ) = 2,
vol (s) = 6, and vol(t ) = 7. the normalized cut for this partition is thus only
2/6 + 2/7 = 0.62.    

10.4.3 some matrices that describe graphs

to develop the theory of how matrix algebra can help us    nd good graph
partitions, we    rst need to learn about three di   erent matrices that describe
aspects of a graph. the    rst should be familiar: the adjacency matrix that has
a 1 in row i and column j if there is an edge between nodes i and j, and 0
otherwise.

a

b

c

d

g

e

f

figure 10.12: repeat of the graph of fig. 10.1

example 10.16 : we repeat our running example graph in fig. 10.12.
its
adjacency matrix appears in fig. 10.13. note that the rows and columns cor-
respond to the nodes a, b, . . . , g in that order. for example, the edge (b, d)
is re   ected by the fact that the entry in row 2 and column 4 is 1 and so is the
entry in row 4 and column 2.    

the second matrix we need is the degree matrix for a graph. this graph has
nonzero entries only on the diagonal. the entry for row and column i is the
degree of the ith node.

364

chapter 10. mining social-network graphs

0 1
1 0
1 1
0 1
0 0
0 0
0 0

   

                           

1 0
1 1
0 0
0 0
0 1
0 1
0 1

0 0
0 0
0 0
1 1
0 1
1 0
0 1

0
0
0
1
0
1
0

   

                           

figure 10.13: the adjacency matrix for fig. 10.12

example 10.17 : the degree matrix for the graph of fig. 10.12 is shown in
fig. 10.14. we use the same order of the nodes as in example 10.16. for
instance, the entry in row 4 and column 4 is 4 because node d has edges to
four other nodes. the entry in row 4 and column 5 is 0, because that entry is
not on the diagonal.    

2 0
0 3
0 0
0 0
0 0
0 0
0 0

   

                           

0 0
0 0
2 0
0 4
0 0
0 0
0 0

0 0
0 0
0 0
0 0
2 0
0 3
0 0

0
0
0
0
0
0
2

   

                           

figure 10.14: the degree matrix for fig. 10.12

suppose our graph has adjacency matrix a and degree matrix d. our third
matrix, called the laplacian matrix, is l = d     a, the di   erence between the
degree matrix and the adjacency matrix. that is, the laplacian matrix l has
the same entries as d on the diagonal. o    the diagonal, at row i and column j,
l has    1 if there is an edge between nodes i and j and 0 if not.
example 10.18 : the laplacian matrix for the graph of fig. 10.12 is shown
in fig. 10.15. notice that each row and each column sums to zero, as must be
the case for any laplacian matrix.    

10.4.4 eigenvalues of the laplacian matrix

we can get a good idea of the best way to partition a graph from the eigenvalues
and eigenvectors of its laplacian matrix.
in section 5.1.2 we observed how the
principal eigenvector (eigenvector associated with the largest eigenvalue) of the
transition matrix of the web told us something useful about the importance of
web pages. in fact, in simple cases (no taxation) the principal eigenvector is the

10.4. partitioning of graphs

365

2
-1
-1 -1
0
-1
0
0
0
0
0
0

-1 -1
0
3 -1 -1
0
2
4
0
0 -1
0 -1
0 -1

   

                           

0
0
0

0
0
0
0
0
0
-1 -1 -1
0
2
-1
-1
0
2

-1
3
-1

   

                           

figure 10.15: the laplacian matrix for fig. 10.12

id95 vector. when dealing with the laplacian matrix, however, it turns
out that the smallest eigenvalues and their eigenvectors reveal the information
we desire.

the smallest eigenvalue for every laplacian matrix is 0, and its correspond-
ing eigenvector is [1, 1, . . . , 1]. to see why, let l be the laplacian matrix for a
graph of n nodes, and let 1 be the column vector of all 1   s with length n. we
claim l1 is a column vector of all 0   s. to see why, consider row i of l. its
diagonal element has the degree d of node i. row i also will have d occurrences
of    1, and all other elements of row i are 0. multiplying row i by column vector
1 has the e   ect of summing the row, and this sum is clearly d + (   1)d = 0.
thus, we can conclude l1 = 01, which demonstrates that 0 is an eigenvalue
and 1 its corresponding eigenvector.

there is a simple way to    nd the second-smallest eigenvalue for any matrix,
such as the laplacian matrix, that is symmetric (the entry in row i and column
j equals the entry in row j and column i). while we shall not prove this
fact, the second-smallest eigenvalue of l is the minimum of xtlx, where x =
[x1, x2, . . . , xn] is a column vector with n components, and the minimum is
taken under the constraints:

1. the length of x is 1; that is pn

i=1 x2

i = 1.

2. x is orthogonal to the eigenvector associated with the smallest eigenvalue.

moreover, the value of x that achieves this minimum is the second eigenvector.
when l is a laplacian matrix for an n-node graph, we know something
more. the eigenvector associated with the smallest eigenvalue is 1. thus, if x
is orthogonal to 1, we must have

xt1 =

n

xi=1

xi = 0

in addition for the laplacian matrix, the expression xtlx has a useful equiv-
alent expression. recall that l = d     a, where d and a are the degree and
adjacency matrices of the same graph. thus, xtlx = xtdx     xtax. let us

366

chapter 10. mining social-network graphs

evaluate the term with d and then the term for a. here, dx is the column vec-
tor [d1x1, d2x2, . . . , dnxn], where di is the degree of the ith node of the graph.

i=1 dix2
i .

thus, xtdx is pn
now, turn to xtax. the ith component of the column vector ax is the sum
of xj over all j such that there is an edge (i, j) in the graph. thus,    xtax is the
sum of    2xixj over all pairs of nodes {i, j} such that there is an edge between
them. note that the factor 2 appears because each set {i, j} corresponds to two
terms,    xixj and    xjxi.
we can group the terms of xtlx in a way that distributes the terms to each
pair {i, j}. from    xtax, we already have the term    2xixj. from xtdx, we
distribute the term dix2
i to the di pairs that include node i. as a result, we
can associate with each pair {i, j} that has an edge between nodes i and j the
j . this expression is equivalent to (xi    xj)2. therefore, we
terms x2
have proved that xtlx equals the sum over all graph edges (i, j) of (xi     xj)2.
recall that the second-smallest eigenvalue is the minimum of this expression
under the constraint that pn
i = 1. intuitively, we minimize it by making
xi and xj close whenever there is an edge between nodes i and j in the graph.
we might imagine that we could choose xi = 1/   n for all i and thus make this
sum 0. however, recall that we are constrained to choose x to be orthogonal to
1, which means the sum of the xi   s is 0. we are also forced to make pn
i be
1, so all components cannot be 0. as a consequence, x must have some positive
and some negative components.

i     2xixj + x2

i=1 x2

i=1 x2

we can obtain a partition of the graph by taking one set to be the nodes
i whose corresponding vector component xi is positive and the other set to
be those whose components are negative. this choice does not guarantee a
partition into sets of equal size, but the sizes are likely to be close. we believe
that the cut between the two sets will have a small number of edges because
(xi   xj)2 is likely to be smaller if both xi and xj have the same sign than if they
have di   erent signs. thus, minimizing xtlx under the required constraints will
tend to give xi and xj the same sign if there is an edge (i, j).

2

1

3

4

6

5

figure 10.16: graph for illustrating partitioning by spectral analysis

example 10.19 : let us apply the above technique to the graph of fig. 10.16.
the laplacian matrix for this graph is shown in fig. 10.17. by standard meth-
ods or math packages we can    nd all the eigenvalues and eigenvectors of this
matrix. we shall simply tabulate them in fig. 10.18, from lowest eigenvalue to

10.4. partitioning of graphs

367

highest. note that we have not scaled the eigenvectors to have length 1, but
could do so easily if we wished.

   

                     

3 -1 -1
2 -1
-1
3
-1 -1
0
0
-1
0
0
0
0 -1
0

0
0
-1
0
0
0
0
-1
0
-1 -1
3
2
-1
-1
3
-1 -1

   

                     

figure 10.17: the laplacian matrix for fig. 10.16

the second eigenvector has three positive and three negative components.
it makes the unsurprising suggestion that one group should be {1, 2, 3}, the
nodes with positive components, and the other group should be {4, 5, 6}.    

eigenvalue
eigenvector

4

3

3

0
1
4    2
1
1
1
1    1    5    1
1    2
1    1
3

1
5
1    5    1    1    1
0
2
1
1
1
0
1    1

1
3    1
1
4    2    1
1

figure 10.18: eigenvalues and eigenvectors for the matrix of fig. 10.17

10.4.5 alternative partitioning methods

the method of section 10.4.4 gives us a good partition of the graph into two
pieces that have a small cut between them. there are several ways we can use
the same eigenvectors to suggest other good choices of partition. first, we are
not constrained to put all the nodes with positive components in the eigenvector
into one group and those with negative components in the other. we could set
the threshold at some point other than zero.

for instance, suppose we modi   ed example 10.19 so that the threshold was
not zero, but    1.5. then the two nodes 4 and 6, with components    1 in the
second eigenvector of fig. 10.18, would join 1, 2, and 3, leaving    ve nodes in one
component and only node 5 in the other. that partition would have a cut of size
two, as did the choice based on the threshold of zero, but the two components
have radically di   erent sizes, so we would tend to prefer our original choice.
however, there are other cases where the threshold zero gives unequally sized
components, as would be the case if we used the third eigenvector in fig. 10.18.

368

chapter 10. mining social-network graphs

we may also want a partition into more than two components. one approach
is to use the method described above to split the graph into two, and then use
it repeatedly on the components to split them as far as desired. a second
approach is to use several of the eigenvectors, not just the second, to partition
the graph. if we use m eigenvectors, and set a threshold for each, we can get a
partition into 2m groups, each group consisting of the nodes that are above or
below threshold for each of the eigenvectors, in a particular pattern.

it is worth noting that each eigenvector except the    rst is the vector x that
minimizes xtlx, subject to the constraint that it is orthogonal to all previous
eigenvectors. this constraint generalizes the constraints we described for the
second eigenvector in a natural way. as a result, while each eigenvector tries
to produce a minimum-sized cut, the fact that successive eigenvectors have to
satisfy more and more constraints generally causes the cuts they describe to be
progressively worse.

example 10.20 : let us reconsider the graph of fig. 10.16, for which the
eigenvectors of its laplacian matrix were tabulated in fig. 10.18. the third
eigenvector, with a threshold of 0, puts nodes 1 and 4 in one group and the
other four nodes in the other. that is not a bad partition, but its cut size is
four, compared with the cut of size two that we get from the second eigenvector.
if we use both the second and third eigenvectors, we put nodes 2 and 3 in
one group, because their components are positive in both eigenvectors. nodes
5 and 6 are in another group, because their components are negative in the
second eigenvector and positive in the third. node 1 is in a group by itself
because it is positive in the second eigenvector and negative in the third, while
node 4 is also in a group by itself because its component is negative in both
eigenvectors. this partition of a six-node graph into four groups is too    ne a
partition to be meaningful. but at least the groups of size two each have an
edge between the nodes, so it is as good as we could ever get for a partition
into groups of these sizes.    

10.4.6 exercises for section 10.4

exercise 10.4.1 : for the graph of fig. 10.9, construct:

(a) the adjacency matrix.

(b) the degree matrix.

(c) the laplacian matrix.

! exercise 10.4.2 : for the laplacian matrix constructed in exercise 10.4.1(c),
   nd the second-smallest eigenvalue and its eigenvector. what partition of the
nodes does it suggest?

!! exercise 10.4.3 : for the laplacian matrix constructed in exercise 10.4.1(c),
construct the third and subsequent smallest eigenvalues and their eigenvectors.

10.5. finding overlapping communities

369

10.5 finding overlapping communities

so far, we have concentrated on id91 a social graph to    nd communities.
but communities are in practice rarely disjoint. in this section, we explain a
method for taking a social graph and    tting a model to it that best explains
how it could have been generated by a mechanism that assumes the id203
that two individuals are connected by an edge (are    friends   ) increases as they
become members of more communities in common. an important tool in this
analysis is    maximum-likelihood estimation,    which we shall explain before
getting to the matter of    nding overlapping communities.

10.5.1 the nature of communities

to begin, let us consider what we would expect two overlapping communities
to look like. our data is a social graph, where nodes are people and there is an
edge between two nodes if the people are    friends.    let us imagine that this
graph represents students at a school, and there are two clubs in this school:
the chess club and the spanish club. it is reasonable to suppose that each
of these clubs forms a community, as does any other club at the school. it is
also reasonable to suppose that two people in the chess club are more likely to
be friends in the graph because they know each other from the club. likewise,
if two people are in the spanish club, then there is a good chance they know
each other, and are likely to be friends.

what if two people are in both clubs? they now have two reasons why they
might know each other, and so we would expect an even greater id203
that they will be friends in the social graph. our conclusion is that we expect
edges to be dense within any community, but we expect edges to be even denser
in the intersection of two communities, denser than that in the intersection of
three communities, and so on. the idea is suggested by fig. 10.19.

10.5.2 maximum-likelihood estimation

before we see the algorithm for    nding communities that have overlap of the
kind suggested in section 10.5.1, let us digress and learn a useful modeling
tool called maximum-likelihood estimation, or id113. the idea behind id113
is that we make an assumption about the generative process (the model ) that
creates instances of some artifact, for example,    friends graphs.    the model has
parameters that determine the id203 of generating any particular instance
of the artifact; this id203 is called the likelihood of those parameter values.
we assume that the value of the parameters that gives the largest value of the
likelihood is the correct model for the observed artifact.

an example should make the id113 principle clear. for instance, we might
wish to generate random graphs. we suppose that each edge is present with
id203 p and not present with id203 1   p, with the presence or absence
of each edge chosen independently. the only parameter we can adjust is p. for

370

chapter 10. mining social-network graphs

chess club

spanish club

intersection

figure 10.19: the overlap of two communities is denser than the nonoverlapping
parts of these communities

each value of p there is a small but nonzero id203 that the graph generated
will be exactly the one we see. following the id113 principle, we shall declare
that the true value of p is the one for which the id203 of generating the
observed graph is the highest.

example 10.21 : consider the graph of fig. 10.19. there are 15 nodes and

23 edges. as there are (cid:0)15
2(cid:1) = 105 pairs of 15 nodes, we see that if each edge
is chosen with id203 p, then the id203 (likelihood) of generating
exactly the graph of fig. 10.19 is given by the function p23(1    p)82. no matter
what value p has between 0 and 1, that is an incredibly tiny number. but the
function does have a maximum, which we can determine by taking its derivative
and setting that to 0. that is:

23p22(1     p)82     82p23(1     p)81 = 0

we can group terms to rewrite the above as

the only way the right side can be 0 is if p is 0 or 1, or the last factor,

p22(1     p)81(cid:0)23(1     p)     82p(cid:1) = 0

is 0. when p is 0 or 1, the value of the likelihood function p23(1     p)82 is
minimized, not maximized, so it must be the last factor that is 0. that is, the

(cid:0)23(1     p)     82p(cid:1)

10.5. finding overlapping communities

371

prior probabilities

when we do an id113 analysis, we generally assume that the parameters
can take any value in their range, and there is no bias in favor of particular
values. however, if that is not the case, then we can multiply the formula
we get for the id203 of the observed artifact being generated, as
a function of the parameter values, by the function that represents the
relative likelihood of those values of the parameter being the true values.
the exercises o   er examples of id113 with assumptions about the prior
distribution of the parameters.

likelihood of generating the graph of fig. 10.19 is maximized when

23     23p     82p = 0

or p = 23/105.

that outcome is hardly a surprise. it says the most likely value for p is the
observed fraction of possible edges that are present in the graph. however, when
we use a more complicated mechanism to generate graphs or other artifacts,
the value of the parameters that produce the observed artifact with maximum
likelihood is far from obvious.    

10.5.3 the a   liation-graph model

we shall now introduce a reasonable mechanism, called the a   liation-graph
model, to generate social graphs from communities. once we see how the pa-
rameters of the model in   uence the likelihood of seeing a given graph, we can
address how one would solve for the values of the parameters that give the
maximum likelihood. the mechanism, called community-a   liation graphs.

1. there is a given number of communities, and there is a given number of

individuals (nodes of the graph).

2. each community can have any set of individuals as members. that is,

the memberships in the communities are parameters of the model.

3. each community c has a id203 pc associated with it, the id203
that two members of community c are connected by an edge because they
are both members of c. these probabilities are also parameters of the
model.

4. if a pair of nodes is in two or more communities, then there is an edge be-
tween them if any of the communities of which both are members justi   es
that edge according to rule (3).

372

chapter 10. mining social-network graphs

we must compute the likelihood that a given graph with the proper number
of nodes is generated by this mechanism. the key observation is how the edge
probabilities are computed, given an assignment of individuals to communities
and values of the pc    s. consider an edge (u, v) between nodes u and v. suppose
u and v are members of communities c and d, but not any other communities.
then the id203 that there is no edge between u and v is the product of
the probabilities that there is no edge due to community c and no edge due
to community d. that is, with id203 (1     pc)(1     pd) there is no edge
(u, v) in the graph, and of course the id203 that there is such an edge is
1 minus that.

more generally, if u and v are members of a nonempty set of communities
m and not any others, then puv, the id203 of an edge between u and v is
given by:

puv = 1     yc in m

(1     pc)

as an important special case, if u and v are not in any communities together,
then we take puv to be   , some very tiny number. we have to choose this
id203 to be nonzero, or else we can never assign a nonzero likelihood
to any set of communities that do not have every pair of individuals sharing
a community. but by taking the id203 to be very small, we bias our
computation to favor solutions such that every observed edge is explained by
joint membership in some community.

if we know which nodes are in which communities, then we can compute
the likelihood of the given graph for these edge probabilities using a simple
generalization of example 10.21. let muv be the set of communities to which
both u and v are assigned. then the likelihood of e being exactly the set of
edges in the observed graph is

y(u,v) in e

puv y(u,v) not in e

(1     puv)

w

x

y

z

figure 10.20: a social graph

example 10.22 : consider the tiny social graph in fig. 10.20. suppose there
are two communities c and d, with associated probabilities pc and pd. also,
suppose that we have determined (or are using as a temporary hypothesis) that
c = {w, x, y} and d = {w, y, z}. to begin, consider the pair of nodes w and

10.5. finding overlapping communities

373

x. mwx = {c}; that is, this pair is in community c but not in community d.
therefore, pwx = 1     (1     pc) = pc .
similarly, x and y are only together in c, y and z are only together in d,
and likewise w and z are only together in d. thus, we    nd pxy = pc and
pyz = pwz = pd. now the pair w and y are together in both communities, so
pwy = 1    (1    pc)(1    pd) = pc + pd     pcpd. finally, x and z are not together
in either community, so pxz =   .
now, we can compute the likelihood of the graph in fig. 10.20, given our
assumptions about membership in the two communities. this likelihood is the
product of the probabilities associated with each of the four pairs of nodes
whose edges appear in the graph, times one minus the id203 for each of
the two pairs whose edges are not there. that is, we want

pwxpwypxypyz(1     pwz)(1     pxz)

substituting the expressions we developed above for each of these probabilities,
we get

(pc)2pd(pc + pd     pcpd)(1     pd)(1       )

note that    is very small, so the last factor is essentially 1 and can be dropped.
we must    nd the values of pc and pd that maximize the above expression.
first, notice that all factors are either independent of pc or grow with pc. the
only hard step in this argument is to remember that pd     1, so

pc + pd     pc pd

must grow positively with pc. it follows that the likelihood is maximized when
pc is as large as possible; that is, pc = 1.

next, we must    nd the value of pd that maximizes the expression, given
that pc = 1. the expression becomes pd(1     pd), and it is easy to see that
this expression has its maximum at pd = 0.5. that is, given c = {w, x, y} and
d = {w, y, z}, the maximum likelihood for the graph in fig. 10.20 occurs when
members of c are certain to have an edge between them and there is a 50%
chance that joint membership in d will cause an edge between the members.
   

however, example 10.22 re   ects only part of the solution. we also need
to    nd an assignment of members to communities such that the maximum
likelihood solution for that assignment is the largest solution for any assignment.
once we    x on an assignment, we can    nd the probabilities, pc , associated with
each community, even for very large graphs with large numbers of communities.
the general method for doing so is called    id119,    a technique that
we introduced in section 9.4.5 and that will be discussed further starting in
section 12.3.4.

unfortunately, it is not obvious how one incorporates the set of members of
each community into the gradient-descent solution, since changes to the com-
position of communities is by discrete steps, not according to some continuous

374

chapter 10. mining social-network graphs

log likelihood

usually, we compute the logarithm of the likelihood function (the log like-
lihood ), rather than the function itself. doing so o   ers several advantages.
products become sums, which often simpli   es the expression. also, sum-
ming many numbers is less prone to numerical rounding errors than is
taking the product of many tiny numbers.

function, as is required for id119. the only feasible way to search
the space of possible assignments of members to communities is by starting
with an assignment and making small changes, say insertion or deletion of one
member for one community. for each such assignment, we can solve for the best
community probabilities (the pc   s) by id119. however,    guring out
what changes to membership lead us in the right direction is tricky, and there
is no guarantee you can even get to the best assignment by making incremental
changes from a starting assignment.

10.5.4 avoiding the use of discrete membership changes

there is a solution to the problem caused by the mechanism of section 10.5.3,
where membership of individuals in communities is discrete; either you are a
member of the community or not. we can think of    strength of membership   
of individuals in communities. intuitively, the stronger the membership of two
individuals in the same community, the more likely it is that this community
will cause them to have an edge between them. in this model, we can adjust
the strength of membership for an individual in a community continuously, just
as we can adjust the id203 associated with a community in the a   liation-
graph model. that improvement allows us to use standard methods, such as
id119, to maximize the expression for likelihood. in the improved
model, we have

1. fixed sets of communities and individuals, as before.

2. for each community c and individual x, there is a strength of membership
parameter fxc . these parameters can take any nonnegative value, and a
value of 0 means the individual is de   nitely not in the community.

3. the id203 that community c causes there to be an edge between

nodes u and v is

pc (u, v) = 1     e   fuc fvc

as before, the id203 of there being an edge between u and v is 1 minus
the id203 that none of the communities causes there to be an edge between
them. that is, each community independently causes edges, and an edge exists

10.5. finding overlapping communities

375

between two nodes if any community causes it to exist. more formally, puv, the
id203 of an edge between nodes u and v can be calculated as

puv = 1    yc (cid:0)1     pc(u, v)(cid:1)

if we substitute the formula for pc (u, v) that is assumed in the model, we get

puv = 1     e    pc fuc fvc

finally, let e be the set of edges in the observed graph. as before, we can
write the formula for the likelihood of the observed graph as the product of the
expression for puv for each edge (u, v) that is in e, times the product of 1    puv
for each edge (u, v) that is not in e. thus, in the new model, the formula for
the likelihood of the graph with edges e is

y(u,v) in e

(1     e    pc fuc fvc ) y(u,v) not in e

e    pc fuc fvc

we can simplify this expression somewhat by taking its logarithm. remem-
ber that maximizing a function also maximizes the logarithm of that function,
and vice versa. so we can take the natural logarithm of the above expression
to replace the products by sums. we also get simpli   cation from the fact that
log(ex) = x.

x(u,v) in e

log(1     e    pc fuc fvc )     x(u,v) not in exc

fuc fvc

(10.1)

we can now    nd the values for the fxc   s that maximizes the expression
(10.1). one way is to use id119 in a manner similar to what was
done in section 9.4.5. that is, we pick one node x, and adjust all the values of
the fxc    s in the direction that most improves the value of (10.1). notice that
the only factors whose values change in response to changes to the fxc    s are
those where one of u and v is x and the other of u and v is a node adjacent to
x. since the degree of a node is typically much less than the number of edges
in the graph, we can avoid looking at most of the terms in (10.1) at each step.

10.5.5 exercises for section 10.5

exercise 10.5.1 : suppose graphs are generated by picking a id203 p and
choosing each edge independently with id203 p, as in example 10.21. for
the graph of fig. 10.20, what value of p gives the maximum likelihood of seeing
that graph? what is the id203 this graph is generated?

exercise 10.5.2 : compute the id113 for the graph in example 10.22 for the
following guesses of the memberships of the two communities.

376

chapter 10. mining social-network graphs

(a) c = {w, x}; c = {y, z}.
(b) c = {w, x, y, z}; c = {x, y, z}.
exercise 10.5.3 : suppose we have a coin, which may not be a fair coin, and
we    ip it some number of times, seeing h heads and t tails.

(a) if the id203 p of getting a head on any    ip is p, what is the id113

for p, in terms of h and t?

(! (b) suppose we are told that there is a 90% id203 that the coin is fair
(i.e., p = 0.5), and a 10% chance that p = 0.1. for what values of h and
t is it more likely that the coin is fair?

!! (c) suppose the a-priori likelihood that p has a particular value is proportional
to |p     0.5|. that is, p is more likely to be near 0 or 1, than around 1/2.
if we see h heads and t tails, what is the maximum likelihood estimate of
p?

10.6 simrank

in this section, we shall take up another approach to analyzing social-network
graphs. this technique, called    simrank,    applies best to graphs with several
types of nodes, although it can in principle be applied to any graph. the
purpose of simrank is to measure the similarity between nodes of the same
type, and it does so by seeing where random walkers on the graph wind up
when starting at a particular node. because calculation must be carried out
once for each starting node, it is limited in the sizes of graphs that can be
analyzed completely in this manner.

10.6.1 random walkers on a social graph

recall our view of id95 in section 5.1 as re   ecting what a    random surfer   
would do if they walked on the web graph. we can similarly think of a per-
son    walking    on a social network. the graph of a social network is generally
undirected, while the web graph is directed. however, the di   erence is unim-
portant. a walker at a node n of an undirected graph will move with equal
id203 to any of the neighbors of n (those nodes with which n shares an
edge).

suppose, for example, that such a walker starts out at node t1 of fig. 10.2,
which we reproduce here as fig. 10.21. at the    rst step, it would go either to
u1 or w1. if to w1, then it would next either come back to t1 or go to t2. if
the walker    rst moved to u1, it would wind up at either t1, t2, or t3 next.

we conclude that, starting at t1, there is a good chance the walker would
visit t2, at least initially, and that chance is better than the chance it would
visit t3 or t4. it would be interesting if we could infer that tags t1 and t2 are

10.6. simrank

377

u

1

u

2

w

1

w

2

w

3

t

1

t

2

t

3

t

4

figure 10.21: repeat of fig. 10.2

therefore related or similar in some way. the evidence is that they have both
been placed on a common web page, w1, and they have also been used by a
common tagger, u1.

however, if we allow the walker to continue traversing the graph at random,
then the id203 that the walker will be at any particular node does not
depend on where it starts out. this conclusion comes from the theory of markov
processes that we mentioned in section 5.1.2, although the independence from
the starting point requires additional conditions besides connectedness that the
graph of fig. 10.21 does satisfy.

10.6.2 id93 with restart

we see from the observations above that it is not possible to measure similar-
ity to a particular node by looking at the limiting distribution of the walker.
however, we have already seen, in section 5.1.5, the introduction of a small
id203 that the walker will stop walking at random. later, we saw in sec-
tion 5.3.2 that there were reasons to select only a subset of web pages as the
teleport set, the pages that the walker would go to when they stopped sur   ng
the web at random.

here, we take this idea to the extreme. as we are focused on one particular
node n of a social network, and want to see where the random walker winds up
on short walks from that node, we modify the matrix of transition probabilities
to have a small additional id203 of transitioning to n from any node.
formally, let m be the transition matrix of the graph g. that is, the entry in
row i and column j of m is 1/k if node j of g has degree k, and one of the
adjacent nodes is i. otherwise, this entry is 0. we shall discuss teleporting in
a moment, but    rst, let us look at a simple example of a transition matrix.

378

chapter 10. mining social-network graphs

example 10.23 : figure 10.22 is an example of a very simple network involving
three pictures, and two tags,    sky    and    tree    that have been placed on some
of them. pictures 1 and 3 have both tags, while picture 2 has only the tag
   sky.    intuitively, we expect that picture 3 is more similar to picture 1 than
picture 2 is, and an analysis using a random walker with restart at picture 1
will support that intuition.

picture 1

picture 2

picture 3

sky

tree

figure 10.22: a simple bipartite social graph

let us order the nodes as picture 1, picture 2, picture 3, sky, tree. then

the transition matrix for the graph of fig. 10.22 is

0
0
0

0
0
0

0
0
0

1/2 1 1/2
1/2 0 1/2

   

               

0

1/3 1/2
1/3
1/3 1/2
0
0

0
0

   

               

for example, the fourth column corresponds to the node    sky,    and this node
connects to each of the tree picture nodes. it therefore has degree three, so the
nonzero entries in its column must be 1/3. the picture nodes correspond to the
   rst three rows and columns, so the entry 1/3 appears in the    rst three rows
of column 4. since the    sky    node does not have an edge to either itself or the
   tree    node, the entries in the last two rows of column 4 are 0.    

as before, let us use    as the id203 that the walker continues at ran-
dom, so 1        is the id203 the walker will teleport to the initial node n .
let en be the column vector that has 1 in the row for node n and 0   s elsewhere.
then if v is the column vector that re   ects the id203 the walker is at each
of the nodes at a particular round, and v    is the id203 the walker is at
each of the nodes at the next round, then v    is related to v by:

v    =   m v + (1       )en

example 10.24 : assume m is the matrix of example 10.23 and    = 0.8.
also, assume that node n is for picture 1; that is, we want to compute the
similarity of other pictures to picture 1. then the equation for the new value

10.6. simrank

379

v    of the distribution that we must iterate is

v    =

   

               

0
0
0

0
0
0

0
0
0

4/15 2/5
4/15
4/15 2/5

0

2/5 4/5 2/5
2/5
2/5

0

0
0

0
0

v +

   

               

   

               

1/5

0
0
0
0

   

               

since the graph of fig. 10.22 is connected, the original matrix m is stochas-
tic, and we can deduce that if the initial vector v has components that sum to
1, then v    will also have components that sum to 1. as a result, we can simplify
the above equation by adding 1/5 to each of the entries in the    rst row of the
matrix. that is, we can iterate the matrix-vector multiplication

v    =

   

               

0
0

1/5 1/5 1/5 7/15 3/5
0
0
2/5 4/5 2/5
2/5
2/5

4/15
4/15 2/5

0
0

0
0

0
0

0

0

v

   

               

if we start with v = en , then the sequence of estimates of the distribution of
the walker that we get is

1
0
0
0
0

   

               

   

               

,

   

               

1/5

0
0

2/5
2/5

   

               

,

   

               

35/75
8/75
20/75
6/75
6/75

   

               

,

   

               

95/375
8/375
20/375
142/375
110/375

   

               

,

   

               

2353/5625
568/5625
1228/5625
786/5625
690/5625

   

               

, . . . ,

   

               

.345
.066
.145
.249
.196

   

               

we observe from the above that in the limit, the walker is more than twice as
likely to be at picture 3 than at picture 2. this analysis con   rms the intuition
that picture 3 is more like picture 1 than picture 2 is.    

there are several additional observations that we may take away from ex-
ample 10.24. first, remember that this analysis applies only to picture 1. if we
wanted to know what pictures were most similar to another picture, we would
have to start the analysis over for that picture. likewise, if we wanted to know
about which tags were most closely associated with the tag    sky    (an uninter-
esting question in this small example, since there is only one other tag), then
we would have to arrange to have the walker teleport only to the    sky    node.
second, notice that convergence takes time, since there is an initial oscil-
lation. that is, initially, all the weight is at the pictures, and at the second
step most of the weight is at the tags. at the third step, most weight is back
at the pictures, but at the fourth step much of the weight moves to the tags
again. however, in the limit there is convergence, with 5/9 of the weight at the
pictures and 4/9 of the weight at the tags. in general, the process converges
for any connected k-partite graph.

380

chapter 10. mining social-network graphs

10.6.3 exercises for section 10.6

exercise 10.6.1 : if, in fig. 10.22 you start the walk from picture 2, what will
be the similarity to picture 2 of the other two pictures? which do you expect
to be more similar to picture 2?

exercise 10.6.2 : if, in fig. 10.22 you start the walk from picture 3, what do
you expect the similarity to the other two pictures to be?

! exercise 10.6.3 : repeat the analysis of example 10.24, and compute the
similarities of picture 1 to the other pictures, if the following modi   cations are
made to fig. 10.22:

(a) the tag    tree    is added to picture 2.

(b) a third tag    water    is added to picture 3.

(c) a third tag    water    is added to both picture 1 and picture 2.

note: the changes are independently done for each part; they are not cumula-
tive.

10.7 counting triangles

one of the most useful properties of social-network graphs is the count of tri-
angles and other simple subgraphs. in this section we shall give methods for
estimating or getting an exact count of triangles in a very large graph. we be-
gin with a motivation for such counts and then give some methods for counting
e   ciently.

10.7.1 why count triangles?

if we start with n nodes and add m edges to a graph at random, there will be
an expected number of triangles in the graph. we can calculate this number

n3/6 sets of three nodes that might be a triangle. the id203 of an edge

without too much di   culty. there are (cid:0)n
between any two given nodes being added is m/(cid:0)n

3(cid:1) sets of three nodes, or approximately
2(cid:1), or approximately 2m/n2.

the id203 that any set of three nodes has edges between each pair, if
those edges are independently chosen to be present or absent is approximately
(2m/n2)3 = 8m3/n6. thus, the expected number of triangles in a graph of
n nodes and m randomly selected edges is approximately (8m3/n6)(n3/6) =
4
3 (m/n)3.

if a graph is a social network with n participants and m pairs of    friends,   
we would expect the number of triangles to be much greater than the value for
a random graph. the reason is that if a and b are friends, and a is also a
friend of c, there should be a much greater chance than average that b and

10.7. counting triangles

381

c are also friends. thus, counting the number of triangles helps us to measure
the extent to which a graph looks like a social network.

we can also look at communities within a social network.

it has been
demonstrated that the age of a community is related to the density of triangles.
that is, when a group has just formed, people pull in their like-minded friends,
but the number of triangles is relatively small. if a brings in friends b and
c, it may well be that b and c do not know each other. as the community
matures, b and c may interact because of their membership in the community.
thus, there is a good chance that at sometime the triangle {a, b, c} will be
completed.

10.7.2 an algorithm for finding triangles

we shall begin our study with an algorithm that has the fastest possible running
time on a single processor. suppose we have a graph of n nodes and m     n
edges. for convenience, assume the nodes are integers 1, 2, . . . , n.
call a node a heavy hitter if its degree is at least    m. a heavy-hitter
triangle is a triangle all three of whose nodes are heavy hitters. we use separate
algorithms to count the heavy-hitter triangles and all other triangles. note that
the number of heavy-hitter nodes is no more than 2   m, since otherwise the sum
of the degrees of the heavy hitter nodes would be more than 2m. since each
edge contributes to the degree of only two nodes, there would then have to be
more than m edges.

assuming the graph is represented by its edges, we preprocess the graph as

follows:

1. compute the degree of each node. this part requires only that we examine
each edge and add 1 to the count of each of its two nodes. the total time
required is o(m).

2. create an index on edges, with the pair of nodes at its ends as the key.
that is, the index allows us to determine, given two nodes, whether the
edge between them exists. a hash table su   ces. it can be constructed in
o(m) time, and the expected time to answer a query about the existence
of an edge is a constant, at least in the expected sense.2

3. create another index of edges, this one with key equal to a single node.
given a node v, we can retrieve the nodes adjacent to v in time propor-
tional to the number of those nodes. again, a hash table, this time with
single nodes as the key, su   ces in the expected sense.

2thus, technically, our algorithm is only optimal in the sense of expected running time,
not worst-case running time. however, hashing of large numbers of items has an extremely
high id203 of behaving according to expectation, and if we happened to choose a hash
function that made some buckets too big, we could rehash until we found a good hash function.

382

chapter 10. mining social-network graphs

we shall order the nodes as follows. first, order nodes by degree. then, if
v and u have the same degree, recall that both v and u are integers, so order
them numerically. that is, we say v     u if and only if either

(i) the degree of v is less than the degree of u, or

(ii) the degrees of u and v are the same, and v < u.

heavy-hitter triangles: there are only o(   m) heavy-hitter nodes, so we
can consider all sets of three of these nodes. there are o(m3/2) possible heavy-
hitter triangles, and using the index on edges we can check if all three edges exist
in o(1) time. therefore, o(m3/2) time is needed to    nd all the heavy-hitter
triangles.

other triangles: we    nd the other triangles a di   erent way. consider each
edge (v1, v2). if both v1 and v2 are heavy hitters, ignore this edge. suppose,
however, that v1 is not a heavy hitter and moreover v1     v2. let u1, u2, . . . , uk
be the nodes adjacent to v1. note that k <    m. we can    nd these nodes,
using the index on nodes, in o(k) time, which is surely o(   m) time. for each
ui we can use the    rst index to check whether edge (ui, v2) exists in o(1) time.
we can also determine the degree of ui in o(1) time, because we have counted
all the nodes    degrees. we count the triangle {v1, v2, ui} if and only if the edge
(ui, v2) exists, and v1     ui. in that way, a triangle is counted only once     when
v1 is the node of the triangle that precedes both other nodes of the triangle
according to the     ordering. thus, the time to process all the nodes adjacent
to v1 is o(   m). since there are m edges, the total time spent counting other
triangles is o(m3/2).

we now see that preprocessing takes o(m) time. the time to    nd heavy-
hitter triangles is o(m3/2), and so is the time to    nd the other triangles. thus,
the total time of the algorithm is o(m3/2).

10.7.3 optimality of the triangle-finding algorithm

it turns out that the algorithm described in section 10.7.2 is, to within an
order of magnitude the best possible. to see why, consider a complete graph

on n nodes. this graph has m = (cid:0)n
2(cid:1) edges and the number of triangles is
(cid:0)n
3(cid:1). since we cannot enumerate triangles in less time than the number of those

triangles, we know any algorithm will take    (n3) time on this graph. however,
m = o(n2), so any algorithm takes    (m3/2) time on this graph.

one might wonder if there were a better algorithm that worked on sparser
graphs than the complete graph. however, we can add to the complete graph
a chain of nodes with any length up to n2. this chain adds no more triangles.
it no more than doubles the number of edges, but makes the number of nodes
as large as we like, in e   ect lowering the ratio of edges to nodes to be as close
to 1 as we like. since there are still    (m3/2) triangles, we see that this lower
bound holds for the full range of possible ratios of m/n.

10.7. counting triangles

383

10.7.4 finding triangles using mapreduce

for a very large graph, we want to use parallelism to speed the computation.
we can express triangle-   nding as a multiway join and use the technique of
section 2.5.3 to optimize the use of a single mapreduce job to count triangles.
it turns out that this use is one where the multiway join technique of that section
is generally much more e   cient than taking two two-way joins. moreover, the
total execution time of the parallel algorithm is essentially the same as the
execution time on a single processor using the algorithm of section 10.7.2.

to begin, assume that the nodes of a graph are numbered 1, 2, . . . , n. we
use a relation e to represent edges. to avoid representing each edge twice,
we assume that if e(a, b) is a tuple of this relation, then not only is there
an edge between nodes a and b, but also, as integers, we have a < b.3
this requirement also eliminates loops (edges from a node to itself), which we
generally assume do not exist in social-network graphs anyway, but which could
lead to    triangles    that really do not involve three di   erent nodes.

using this relation, we can express the set of triangles of the graph whose

edges are e by the natural join

e(x, y )        e(x, z)        e(y, z)

(10.2)

to understand this join, we have to recognize that the attributes of the relation
e are given di   erent names in each of the three uses of e. that is, we imagine
there are three copies of e, each with the same tuples, but with a di   erent
schemas. in sql, this join would be written using a single relation e(a, b) as
follows:

select e1.a, e1.b, e2.b
from e e1, e e2, e e3
where e1.a = e2.a and e1.b = e3.a and e2.b = e3.b

in this query, the equated attributes e1.a and e2.a are represented in our join
by the attribute x. also, e1.b and e3.a are each represented by y ; e2.b and
e3.b are represented by z.

notice that each triangle appears once in this join. the triangle consisting
of nodes v1, v2, and v3 is generated when x, y , and z are these three nodes in
numerical order, i.e., x < y < z. for instance, if the numerical order of the
nodes is v1 < v2 < v3, then x can only be v1, y is v2, and z is v3.

the technique of section 2.5.3 can be used to optimize the join of equa-
tion 10.2. recall the ideas in example 2.9, where we considered the number
of ways in which the values of each attribute should be hashed. in the present
example, the matter is quite simple. the three occurrences of relation e surely
have the same size, so by symmetry, attributes x, y , and z will each be hashed

3do not confuse this simple numerical ordering of the nodes with the order     that we
discussed in section 10.7.2 and which involved the degrees of the nodes. here, node degrees
are not computed and are not relevant.

384

chapter 10. mining social-network graphs

to the same number of buckets. in particular, if we hash nodes to b buckets,
then there will be b3 reducers. each reduce task is associated with a sequence
of three bucket numbers (x, y, z), where each of x, y, and z is in the range 1 to
b.

the map tasks divide the relation e into as many parts as there are map
tasks. suppose one map task is given the tuple e(u, v) to send to certain
reduce tasks. first, think of (u, v) as a tuple of the join term e(x, y ). we
can hash u and v to get the bucket numbers for x and y , but we don   t know
the bucket to which z hashes. thus, we must send e(u, v) to all reducer tasks

that correspond to a sequence of three bucket numbers (cid:0)h(u), h(v), z(cid:1) for any

of the b possible buckets z.

but the same tuple e(u, v) must also be treated as a tuple for the term
e(x, z). we therefore also send the tuple e(u, v) to all reduce tasks that

tuple of the term e(y, z) and send that tuple to all reduce tasks corresponding

correspond to a triple (cid:0)h(u), y, h(v)(cid:1) for any y. finally, we treat e(u, v) as a
to a triple (cid:0)x, h(u), h(v)(cid:1) for any x. the total communication required is thus

3b key-value pairs for each of the m tuples of the edge relation e. that is, the
minimum communication cost is o(mb) if we use b3 reduce tasks.

next, let us compute the total execution cost at all the reduce tasks. as-
sume that the hash function distributes edges su   ciently randomly that the
reduce tasks each get approximately the same number of edges. since the total
number of edges distributed to the b3 reduce tasks is o(mb), it follows that each
task receives o(m/b2) edges. if we use the algorithm of section 10.7.2 at each

reduce task, the total computation at a task is o(cid:0)(m/b2)3/2(cid:1), or o(m3/2/b3).

since there are b3 reduce tasks, the total computation cost is o(m3/2), exactly
as for the one-processor algorithm of section 10.7.2.

10.7.5 using fewer reduce tasks

by a judicious ordering of the nodes, we can lower the number of reduce tasks
by approximately a factor of 6. think of the    name    of the node i as the

pair (cid:0)h(i), i(cid:1), where h is the hash function that we used in section 10.7.4 to

hash nodes to b buckets. order nodes by their name, considering only the
   rst component (i.e., the bucket to which the node hashes), and only using the
second component to break ties among nodes that are in the same bucket.

if we use this ordering of nodes, then the reduce task corresponding to
list of buckets (i, j, k) will be needed only if i     j     k.
if b is large, then
approximately 1/6 of all b3 sequences of integers, each in the range 1 to b, will
satisfy these inequalities. for any b, the number of such sequences is (cid:0)b+2

exercise 10.7.4). thus, the exact ratio is (b + 2)(b + 1)/(6b2).

3 (cid:1) (see

as there are fewer reducers, we get a substantial decrease in the number
of key-value pairs that must be communicated. instead of having to send each
of the m edges to 3b reduce tasks, we need to send each edge to only b tasks.
speci   cally, consider an edge e whose two nodes hash to i and j; these buckets
could be the same or di   erent. for each of the b values of k between 1 and b,

10.7. counting triangles

385

consider the list formed from i, j, and k in sorted order. then the reduce task
that corresponds to this list requires the edge e. but no other reduce tasks
require e.

to compare the communication cost of the method of this section with that
of section 10.7.4, let us    x the number of reduce tasks, say k. then the method
of section 10.7.4 hashes nodes to 3   k buckets, and therefore communicates
3m 3   k key-value pairs. on the other hand, the method of this section hashes
nodes to approximately 3   6k buckets, thus requiring m 3   6 3   k communication.
thus, the ratio of the communication needed by the method of section 10.7.4
to what is needed here is 3/ 3   6 = 1.65.

example 10.25 : consider the straightforward algorithm of section 10.7.4
with b = 6. that is, there are b3 = 216 reduce tasks and the communication
cost is 3mb = 18m. we cannot use exactly 216 reduce tasks with the method of
this section, but we can come very close if we choose b = 10. then, the number

of reduce tasks is (cid:0)12

is, the communication cost is 5/9th of the cost of the straightforward method.
   

3(cid:1) = 220, and the communication cost is mb = 10m. that

10.7.6 exercises for section 10.7

exercise 10.7.1 : how many triangles are there in the graphs:

(a) figure 10.1.

(b) figure 10.9.

! (c) figure 10.2.

exercise 10.7.2 : for each of the graphs of exercise 10.7.1 determine:

(i) what is the minimum degree for a node to be considered a    heavy hitter   ?

(ii) which nodes are heavy hitters?

(iii) which triangles are heavy-hitter triangles?

! exercise 10.7.3 : in this exercise we consider the problem of    nding squares
in a graph. that is, we want to    nd quadruples of nodes a, b, c, d such that the
four edges (a, b), (b, c), (c, d), and (a, d) exist in the graph. assume the graph
is represented by a relation e as in section 10.7.4. it is not possible to write a
single join of four copies of e that expresses all possible squares in the graph,
but we can write three such joins. moreover, in some cases, we need to follow
the join by a selection that eliminates    squares    where one pair of opposite
corners are really the same node. we can assume that node a is numerically
lower than its neighbors b and d, but there are three cases,depending on whether
c is

386

chapter 10. mining social-network graphs

(i) also lower than b and d,

(ii) between b and d, or

(iii) higher than both b and d.

(a) write the natural joins that produce squares satisfying each of the three
conditions above. you can use four di   erent attributes w , x, y , and z,
and assume that there are four copies of relation e with di   erent schemas,
so the joins can each be expressed as natural joins.

(b) for which of these joins do we need a selection to assure that opposite

corners are really di   erent nodes?

!! (c) assume we plan to use k reduce tasks. for each of your joins from (a),
into how many buckets should you hash each of w , x, y , and z in order
to minimize the communication cost?

(d) unlike the case of triangles, it is not guaranteed that each square is pro-
duced only once, although we can be sure that each square is produced by
only one of the three joins. for example, a square in which the two nodes
at opposite corners are each lower numerically than each of the other two
nodes will only be produced by the join (i). for each of the three joins,
how many times does it produce any square that it produces at all?

! exercise 10.7.4 : show that the number of sequences of integers 1     i     j    
k     b is (cid:0)b+2
3 (cid:1). hint : show that these sequences can be placed in a 1-to-1
correspondence with the binary strings of length b + 2 having exactly three 1   s.

10.8 neighborhood properties of graphs

there are several important properties of graphs that relate to the number of
nodes one can reach from a given node along a short path.
in this section
we look at algorithms for solving problems about paths and neighborhoods for
very large graphs. in some cases, exact solutions are not feasible for graphs
with millions of nodes. we therefore look at approximation algorithms as well
as exact algorithms.

10.8.1 directed graphs and neighborhoods

in this section we shall use a directed graph as a model of a network. a directed
graph has a set of nodes and a set of arcs; the latter is a pair of nodes written
u     v. we call u the source and v the target of the arc. the arc is said to be
from u to v.
many kinds of graphs can be modeled by directed graphs. the web is a
major example, where the arc u     v is a link from page u to page v. or, the
arc u     v could mean that telephone subscriber u has called subscriber v in

10.8. neighborhood properties of graphs

387

the past month. for another example, the arc could mean that individual u is
following individual v on twitter. in yet another graph, the arc could mean
that research paper u references paper v.

moreover, all undirected graphs can be represented by directed graphs. in-
stead of the undirected edge (u, v), use two arcs u     v and v     u. thus, the
material of this section also applies to graphs that are inherently undirected,
such as a friends graph in a social network.

a path in a directed graph is a sequence of nodes v0, v1, . . . , vk such that
there are arcs vi     vi+1 for all i = 0, 1, . . . , k     1. the length of this path is k,
the number of arcs along the path. note that there are k + 1 nodes in a path
of length k, and a node by itself is considered a path of length 0.

the neighborhood of radius d for a node v is the set of nodes u for which
there is a path of length at most d from v to u. we denote this neighborhood
by n (v, d). for example, n (v, 0) is always {v}, and n (v, 1) is v plus the set of
nodes to which there is an arc from v. more generally, if v is a set of nodes,
then n (v, d) is the set of nodes u for which there is a path of length d or less
from at least one node in the set v .

the neighborhood pro   le of a node v is the sequence of sizes of its neighbor-
hoods |n (v, 1)|,|n (v, 2)|, . . . . we do not include the neighborhood of distance
0, since its size is always 1.

a

b

c

d

g

e

f

figure 10.23: our small social network; think of it as a directed graph

example 10.26 : consider the undirected graph of fig. 10.1, which we re-
produce here as fig. 10.23. to turn it into a directed graph, think of each
edge as a pair of arcs, one in each direction. for instance, the edge (a, b)
becomes the arcs a     b and b     a. first, consider the neighborhoods of
node a. we know n (a, 0) = {a}. moreover, n (a, 1) = {a, b, c}, since there
are arcs from a only to b and c. furthermore, n (a, 2) = {a, b, c, d} and
n (a, 3) = {a, b, c, d, e, f, g}. neighborhoods for larger radius are all the
same as n (a, 3).
on the other hand, consider node b. we    nd n (b, 0) = {b}, n (b, 1) =
{a, b, c, d}, and n (b, 2) = {a, b, c, d, e, f, g}. we know that b is more
central to the network than a, and this fact is re   ected by the neighborhood
pro   les of the two nodes. node a has pro   le 3, 4, 7, 7, . . ., while b has pro   le
4, 7, 7, . . . . evidently, b is more central than a, because at every distance, its

388

chapter 10. mining social-network graphs

neighborhood is at least as large as that of a. in fact, d is even more central
than b, because its neighborhood pro   le 5, 7, 7, . . . dominates the pro   le of each
of the nodes.    

10.8.2 the diameter of a graph

the diameter of a directed graph is the smallest integer d such that for every
two nodes u and v there is a path of length d or less from u to v. in a directed
graph, this de   nition only makes sense if the graph is strongly connected ; that
is, there is a path from any node to any other node. recall our discussion
of the web in section 5.1.3, where we observed that there is a large strongly
connected subset of the web in the    center,    but that the web as a whole is
not strongly connected. rather, there are some pages that reach nowhere by
links, and some pages that cannot be reached by links.

if the graph is undirected, the de   nition of diameter is the same as for
directed graphs, but the path may traverse the undirected edges in either di-
rection. that is, we treat an undirected edge as a pair of arcs, one in each
direction. the notion of diameter makes sense in an undirected graph as long
as that graph is connected.

example 10.27 : for the graph of fig. 10.23, the diameter is 3. there are
some pairs of nodes, such as a and e, that have no path of length less than 3.
but every pair of nodes has a path from one to the other with length at most 3.
   

we can compute the diameter of a graph by computing the sizes of its
neighborhoods of increasing radius, until at some radius we fail to add any
more nodes. that is, for each node v,    nd the smallest d such that |n (v, d)| =
|n (v, d + 1)|. this d is the tight upper bound on the length of the shortest
path from v to any node it can reach. call it d(v). for instance, we saw from
example 10.26 that d(a) = 3 and d(b) = 2. if there is any node v such that

|n(cid:0)v, d(v)(cid:1)| is not the number of nodes in the entire graph, then the graph is

not strongly connected, and we cannot o   er any    nite integer as its diameter.
however, if the graph is strongly connected, then the diameter of the graph is

maxv(cid:0)d(v)(cid:1).

the reason this computation works is that one way to express n (v, d + 1) is
the union of n (v, d) and the set of all nodes w such that for some u in n (v, d)
there is an arc u     w. that is, we start with n (v, d) and add to it the targets
of all arcs that have a source in n (v, d). if all the arcs with source in n (v, d)
are already in n (v, d), then not only is n (v, d + 1) equal to n (v, d), but all of
n (v, d + 2), n (v, d + 3), . . . will equal n (v, d). finally, we observe that since
n (v, d)     n (v, d+1) the only way |n (v, d)| can equal |n (v, d+1)| is for n (v, d)
and n (v, d + 1) to be the same set. thus, if d is the smallest integer such that
|n (v, d)| = |n (v, d + 1)|, it follows that every node v can reach is reached by a
path of length at most d.

10.8. neighborhood properties of graphs

389

six degrees of separation

there is a famous game called    six degrees of kevin bacon,    the object
of which is to    nd paths of length at most six in the graph whose nodes
are movie stars and whose edges connect stars that played in the same
movie. the conjecture is that in this graph, no movie star is of distance
more than six from kevin bacon. more generally, any two movie stars
can be connected by a path of length at most six; i.e., the diameter of
the graph is six. a small diameter makes computation of neighborhoods
more e   cient, so it would be nice if all social-network graphs exhibited
a similar small diameter. in fact, the phrase    six degrees of separation,   
refers to the conjecture that in the network of all people in the world,
where an edge means that the two people know each other, the diameter is
six. unfortunately, as we shall discuss in section 10.8.3, not all important
graphs exhibit such tight connections.

10.8.3 transitive closure and reachability

the transitive closure of a graph is the set of pairs of nodes (u, v) such that
there is a path from u to v of length zero or more. we shall sometimes write
this assertion as p ath(u, v).4 a related concept is that of reachability. we say
node u reaches node v if p ath(u, v). the problem of computing the transitive
closure is to    nd all pairs of nodes u and v in a graph for which p ath(u, v) is
true. the reachability problem is, given a node u in the graph,    nd all v such
that p ath(u, v) is true.

these two concepts relate to the notions of neighborhoods that we have
seen earlier. in fact, p ath(u, v) is true if and only if v is in n (u,   ), which
we de   ne to be si   0 n (u, i). thus, the reachability problem is to compute
the union of all the neighborhoods of any radius for a given node u. the
discussion in section 10.8.2 reminds us that we can compute the reachable set
for u by computing its neighborhoods up to that smallest radius d for which
n (u, d) = n (u, d + 1).

the two problems     transitive closure and reachability     are related, but
there are many examples of graphs where reachability is feasible and transitive
closure is not. for instance, suppose we have a web graph of a billion nodes.
if we want to    nd the pages (nodes) reachable from a given page, we can do so,
even on a single machine with a large main memory. however, just to produce
the transitive closure of the graph could involve 1018 pairs of nodes, which is
not practical, even using a large cluster of computers.5

4technically, this de   nition gives us the re   exive and transitive closure of the graph, since

p ath(v, v) is always considered true, even if there is no cycle that contains v.

5while we cannot compute the transitive closure completely, we can still learn a great
deal about the structure of a graph, provided there are large strongly connected components.

390

chapter 10. mining social-network graphs

10.8.4 transitive closure via mapreduce

when it comes to parallel implementation, transitive closure is actually more
readily parallelizable than is reachability. if we want to compute n (v,   ), the
set of nodes reachable from node v, without computing the entire transitive
closure, we have no option but to compute the sequence of neighborhoods,
which is essentially a breadth-   rst search of the graph from v.
in relational
terms, suppose we have a relation arc(x, y ) containing those pairs (x, y) such
that there is an arc x     y. we want to compute iteratively a relation reach(x)
that is the set of nodes reachable from node v. after i rounds, reach(x) will
contain all those nodes in n (v, i).

initially, reach(x) contains only the node v. suppose it contains all the
nodes in n (v, i) after some round of mapreduce. to construct n (v, i + 1)
we need to join reach with the arc relation, then project onto the second
component and perform a union of the result with the old value of reach. in
sql terms, we perform

select distinct arc.y
from reach, arc
where arc.x = reach.x;

this query asks us to compute the natural join of reach(x) and arc(x, y ),
which we can do by mapreduce as explained in section 2.3.7. then, we have
to group the result by y and eliminate duplicates, a process that can be done
by another mapreduce job as in section 2.3.8.

how many rounds this process requires depends on how far from v is the
furthest node that v can reach. in many social-network graphs, the diameter is
small, as discussed in the box on    six degrees of separation.    if so, computing
reachability in parallel, using mapreduce or another approach is feasible. few
rounds of computation will be needed and the space requirements are not greater
than the space it takes to represent the graph.

however, there are some graphs where the number of rounds is a serious
impediment. for instance, in a typical portion of the web, it has been found
that most pages reachable from a given page are reachable by paths of length
10   15. yet there are some pairs of pages such that the    rst reaches the second,
but only through paths whose length is measured in the hundreds. for instance,
blogs are sometimes structured so each response is reachable only through the
comment to which it responds. running arguments lead to long paths with
no way to    shortcut    around that path. or a tutorial on the web, with 50
chapters, may be structured so you can only get to chapter i through the page
for chapter i     1.
interestingly, the transitive closure can be computed much faster in parallel
than can strict reachability. by a recursive-doubling technique, we can double

for example, the web graph experiments discussed in section 5.1.3 were done on a graph of
about 200 million nodes. although they never listed all the pairs of nodes in the transitive
closure, they were able to describe the structure of the web.

10.8. neighborhood properties of graphs

391

the length of paths we know about in a single round. thus, on a graph of
diameter d, we need only log2 d rounds, rather than d rounds. if d = 6, the
di   erence is not important, but if d = 1000, log2 d is about 10, so there is a
hundredfold reduction in the number of rounds. the problem, as discussed
above, is that while we can compute the transitive closure quickly, we must
compute many more facts than are needed for a reachability computation on
the same graph, and therefore the space requirements for transitive closure can
greatly exceed the space requirements for reachability. that is, if all we want is
the set reach(v), we can compute the transitive closure of the entire graph, and
then throw away all pairs that do not have v as their    rst component. but we
cannot throw away all those pairs until we are done. during the computation
of the transitive closure, we could wind up computing many facts p ath(x, y),
where neither x nor y is reachable from v, and even if they are reachable from
v, we may not need to know x can reach y.

assuming the graph is small enough that we can compute the transitive
closure in its entirety, we still must be careful how we do so using mapreduce
or another parallelism approach. the simplest recursive-doubling approach is to
start the the relation p ath(x, y ) equal to the arc relation arc(x, y ). suppose
that after i rounds, p ath(x, y ) contains all pairs (x, y) such that there is a
path from x to y of length at most 2i. then if we join p ath with itself at the
next round, we shall discover all those pairs (x, y) such that there is a path
from x to y of length at most 2    2i = 2i+1. the recursive-doubling query in
sql is

select distinct p1.x, p2.y
from path p1, path p2
where p1.y = p2.x;

after computing this query, we get all pairs connected by a path of length
between 2 and 2i+1, assuming p ath contains pairs connected by paths of length
between 1 and 2i. if we take the union of the result of this query with the arc
relation itself, then we get all paths of length between 1 and 2i+1 and can use the
union as the p ath relation in the next round of recursive doubling. the query
itself can be implemented by two mapreduce jobs, one to do the join and the
other to do the union and eliminate duplicates. as we observed for the parallel
reachability computation, the methods of sections 2.3.7 and 2.3.8 su   ce. the
union, discussed in section 2.3.6 doesn   t really require a mapreduce job of its
own; it can be combined with the duplicate-elimination.

if a graph has diameter d, then after log2 d rounds of the above algorithm
p ath contains all pairs (x, y) connected by a path of length at most d; that is,
it contains all pairs in the transitive closure. unless we already know d, one
more round will be necessary to verify that no more pairs can be found, but
for large d, this process takes many fewer rounds than the breadth-   rst search
that we used for reachability.

however, the above recursive-doubling method does a lot of redundant work.

an example should make the point clear.

392

chapter 10. mining social-network graphs

example 10.28 : suppose the shortest path from x0 to x17 is of length 17; in
particular, let there be a path x0     x1                x17. we shall discover the fact
p ath(x0, x17) on the    fth round, when p ath contains all pairs connected by
paths of length up to 16. the same path from x0 to x17 will be discovered 16
times when we join p ath with itself. that is, we can take the fact p ath(x0, x16)
and combine it with p ath(x16, x17) to obtain p ath(x0, x17). or we can combine
p ath(x0, x15) with p ath(x15, x17) to discover the same fact, and so on.    

10.8.5 smart transitive closure

a variant of recursive doubling that avoids discovering the same path more than
once is called smart transitive closure. every path of length greater than 1 can
be broken into a head whose length is a power of 2, followed by a tail whose
length is no greater than the length of the head.

example 10.29 : a path of length 13 has a head consisting of the    rst 8 arcs,
followed by a tail consisting of the last 5 arcs. a path of length 2 is a head of
length 1 followed by a tail of length 1. note that 1 is a power of 2 (the 0th
power), and the tail will be as long as the head whenever the path itself has a
length that is a power of 2.    

to implement smart transitive closure in sql, we introduce a relation
q(x, y ) whose function after the ith round is to hold all pairs of nodes (x, y)
such that the shortest path from x to y is of length exactly 2i. also, after the
ith round, p ath(x, y) will be true if the shortest path from x to y is of length
at most 2i+1     1. note that this interpretation of p ath is slightly di   erent
from the interpretation of p ath in the simple recursive-doubling method given
in section 10.8.4.

initially, set both q and p ath to be copies of the relation arc. after the
ith round, assume that q and p ath have the contents described in the previous
paragraph. note that for the round i = 1, the initial values of q and p ath
initially satisfy the conditions as described for i = 0. on the (i + 1)st round,
we do the following:

1. compute a new value for q by joining it with itself, using the sql query

select distinct q1.x, q2.y
from q q1, q q2
where q1.y = q2.x;

2. subtract p ath from the relation q computed in step 1. note that step (1)
discovers all paths of length 2i+1. but some pairs connected by these paths
may also have shorter paths. the result of step (2) is to leave in q all
and only those pairs (u, v) such that the shortest path from u to v has
length exactly 2i+1.

10.8. neighborhood properties of graphs

393

3. join p ath with the new value of q computed in 2, using the sql query

select distinct q.x, path.y
from q, path
where q.y = path.x

at the beginning of the round p ath contains all (y, z) such that the short-
est path from y to z has a length up to 2i+1     1 from y to z, and the new
value of q contains all pairs (x, y) for which the shortest path from x to
y is of length 2i+1. thus, the result of this query is the set of pairs (x, y)
such that the shortest path from x to y has a length between 2i+1 + 1 and
2i+2     1, inclusive.

4. set the new value of p ath to be the union of the relation computed in
step 3, the new value of q computed in step (1), and the old value of
p ath. these three terms give us all pairs (x, y) whose shortest path is of
length 2i+1 + 1 through 2i+2     1, exactly 2i+1, and 1 through 2i+1     1,
respectively. thus, the union gives us all shortest paths up to length
2i+2     1, as required by the inductive hypothesis about what is true after
each round.

each round of the smart transitive-closure algorithm uses steps that are joins,
aggregations (duplicate eliminations), or unions. a round can thus be imple-
mented as a short sequence of mapreduce jobs. further, a good deal of work
can be saved if these operations can be combined, say by using the more general
patterns of communication permitted by a work   ow system (see section 2.4.1).

10.8.6 transitive closure by graph reduction

a typical directed graph such as the web contains many strongly connected
components (scc   s). we can collapse an scc to a single node as far as com-
puting the transitive closure is concerned, since all the nodes in an scc reach
exactly the same nodes. there is an elegant algorithm for    nding the scc   s of
a graph in time linear in the size of the graph, due to j.e. hopcroft and r.e.
tarjan. however, this algorithm is inherently sequential, based on depth-   rst
search, and so not well suited to parallel impelementation on large graphs.

we can    nd most of the scc   s in a graph by some random node selections
followed by two breadth-   rst searches. moreover, the larger an scc is, the more
likely it is to be collapsed early, thus reducing the size of the graph quickly. the
algorithm for reducing scc   s to single nodes is as follows. let g be the graph
to be reduced, and let g    be g with all the arcs reversed.

1. pick a node v from g at random.

2. find ng(v,   ), the set of nodes reachable from v in g.

394

chapter 10. mining social-network graphs

path facts versus paths

we should be careful to distinguish between a path, which is a sequence of
arcs, and a path fact, which is a statement that there exists a path from
some node x to some node y. the path fact has been shown typically as
p ath(x, y). smart transitive closure discovers each path only once, but it
may discover a path fact more than once. the reason is that often a graph
will have many paths from x to y, and may even have many di   erent paths
from x to y that are of the same length.

not all paths are discovered independently by smart transitive closure.
for instance, if there are arcs w     x     y     z and also arcs x     u     z,
then the path fact p ath(w, z) will be discovered twice, once by combining
p ath(w, y) with p ath(y, z) and again when combining p ath(w, u) with
p ath(u, z). on the other hand, if the arcs were w     x     y     z and w    
v     y, then p ath(w, z) would be discovered only once, when combining
p ath(w, y) with p ath(y, z).

3. find ng   (v,   ), the set of nodes that v reaches in the graph g    that has
the arcs of g reversed. equivalently, this set is those nodes that reach v
in g.

4. construct the scc s containing v, which is ng(v,   )     ng   (v,   ). that
is, v and u are in the same scc of g if and only if v can reach u and u
can reach v.

5. replace scc s by a single node s in g. to do so, delete all nodes in s
from g and add s to the node set of g. delete from g all arcs one or
both ends of which are in s. then, add to the arc set of g an arc s     x
whenever there was an arc in g from any member of s to x. finally, add
an arc x     s if there was an arc from x to any member of s.

we can iterate the above steps a    xed number of times. we can alternatively
iterate until the graph becomes su   ciently small, or we could examine all nodes
v in turn and not stop until each node is in an scc by itself; i.e.,

ng(v,   )     ng   (v,   ) = {v}

for all remaining nodes v. if we make the latter choice, the resulting graph is
called the transitive reduction of the original graph g. the transitive reduction
is always acyclic, since if it had a cycle there would remain an scc of more than
one node. however, it is not necessary to reduce to an acyclic graph, as long
as the resulting graph has su   ciently few nodes that it is feasible to compute
the full transitive closure of this graph; that is, the number of nodes is small
enough that we can deal with a result whose size is proportional to the square
of that number of nodes.

10.8. neighborhood properties of graphs

395

while the transitive closure of the reduced graph is not exactly the same
as the transitive closure of the original graph, the former, plus the information
about what scc each original node belongs to is enough to tell us anything that
the transitive closure of the original graph tells us. if we want to know whether
p ath(u, v) is true in the original graph,    nd the scc   s containing u and v. if
one or both of these nodes have never been combined into an scc, then treat
that node as an scc by itself. if u and v belong to the same scc, then surely
u can reach v. if they belong to di   erent scc   s s and t, respectively, determine
whether s reaches t in the reduced graph. if so, then u reaches v in the original
graph, and if not, then not.

example 10.30 : let us reconsider the    bowtie    picture of the web from
section 5.1.3. the number of nodes in the part of the graph examined was
over 200 million; surely too large to deal with data of size proportional to the
square of that number. there was one large set of nodes called    the scc    that
was regarded as the center of the graph. since about one node in four was
in this scc, it would be collapsed to a single node as soon as any one of its
members was chosen at random. but there are many other scc   s in the web,
even though they were not shown explicitly in the    bowtie.    for instance, the
in-component would have within it many large scc   s. the nodes in one of
these scc   s can reach each other, and can reach some of the other nodes in the
in-component, and of course they can reach all the nodes in the central scc.
the scc   s in the in- and out-components, the tubes, and other structures can
all be collapsed, leading to a far smaller graph.    

10.8.7 approximating the sizes of neighborhoods

in this section we shall take up the problem of computing the neighborhood
pro   le for each node of a large graph. a variant of the problem, which yields
to the same technique, is to    nd the size of the reachable set for each node v,
i.e., the set we have called n (v,   ). recall that for a graph of a billion nodes,
it is totally infeasible to compute the neighborhoods for each node, even using
a very large cluster of machines. however, even if we want only a count of the
nodes in each neighborhood, we need to remember the nodes found so far as
we explore the graph, or else we shall not know whether a found node is new
or one we have seen already.

on the other hand, it is not so hard to    nd an approximation to the size
of each neighborhood, using the flajolet-martin technique discussed in sec-
tion 4.4.2. recall that this technique uses some large number of hash functions;
in this case, the hash functions are applied to the nodes of the graph. the
important property of the bit string we get when we apply hash function h to
node v is the    tail length        the number of 0   s at the end of the string. for
any set of nodes, an estimate of the size of the set is 2r, where r is the length
of the longest tail for any member of the set. thus, instead of storing all the
members of the set, we can instead record only the value of r for that set. of

396

chapter 10. mining social-network graphs

course, there are many hash functions, so we need to record values of r for each
hash function.

example 10.31 : if we use a hash function that produces a 64-bit string, then
six bits are all that are needed to store each value of r. for instance, if there
are a billion nodes, and we want to estimate the size of the neighborhood for
each, we can store the value of r for 20 hash functions for each node in 15
gigabytes.    

if we store tail lengths for each neighborhood, we can use this information
to compute estimates for the larger neighborhoods from our estimates for the
smaller neighborhoods. that is, suppose we have computed our estimates for
|n (v, d)| for all nodes v, and we want to compute estimates for the neighbor-
hoods of radius d + 1. for each hash function h, the value of r for n (v, d + 1)
is the largest of:

1. the tail of v itself and

2. the values of r associated with h and n (u, d), where v     u is an arc of

the graph.

notice that it doesn   t matter whether a node is reachable through only one
successor of v in the graph, or through many di   erent successors. we get
the same estimate in either case. this useful property was the same one we
exploited in section 4.4.2 to avoid having to know whether a stream element
appeared once or many times in the stream.

we shall now describe the complete algorithm, called anf (approximate
neighborhood function). we choose k hash functions h1, h2, . . . , hk. for each
node v and radius d, let ri(v, d) denote the maximum tail length of any node
in n (v, d) using hash function hi. to initialize, let ri(v, 0) be the length of the
tail of hi(v), for all i and v.

for the inductive step, suppose we have computed ri(v, d) for all i and v.
initialize ri(v, d + 1) to be ri(v, d), for all i and v. then, consider all arcs
x     y in the graph, in any order. for each x     y, set ri(x, d + 1) to the
larger of its current value and ri(y, d). observe that the fact we can consider
the arcs in any order may provide a big speedup in the case that we can store
the ri   s in main memory, while the set of arcs is so large it must be stored
on disk. we can stream all the disk blocks containing arcs one at a time, thus
using only one disk access per iteration per disk block used for arc storage.
this advantage is similar to the one we observed in section 6.2.1, where we
pointed out how frequent-itemset algorithms like a-priori could take advantage
of reading market-basket data in a stream, where each disk block was read only
once for each round.

to estimate the size of n (v, d), combine the values of the ri(v, d) for i =
1, 2, . . . , k, as discussed in section 4.4.3. that is, group the r   s into small
groups, take the average, and take the median of the averages.

10.8. neighborhood properties of graphs

397

another improvement to the anf algorithm can be had if we are only
interested in estimating the sizes of the reachable sets, n (v,   ). it is not then
necessary to save ri(v, d) for di   erent radii d. we can maintain one value ri(v)
for each hash function hi and each node v. when on any round, we consider
arc x     y, we simply assign

ri(x) := max(cid:0)ri(x), ri(y)(cid:1)

we can stop the iteration when at some round no value of any ri(v) changes.
or if we know d is the diameter of the graph, we can just iterate d times.

a

b

c

h

d

i

g

e

f

figure 10.24: a graph for exercises on neighborhoods and transitive closure

10.8.8 exercises for section 10.8

exercise 10.8.1 : for the graph of fig. 10.9, which we repeat here as fig. 10.24:

(a) if the graph is represented as a directed graph, how many arcs are there?

(b) what are the neighborhood pro   les for nodes a and b?

(c) what is the diameter of the graph?

(d) how many pairs are in the transitive closure? hint : do not forget that
there are paths of length greater than zero from a node to itself in this
graph.

(e) if we compute the transitive closure by recursive doubling, how many

rounds are needed?

exercise 10.8.2 : the smart transitive closure algorithm breaks paths of any
length into head and tail of speci   c lengths. what are the head and tail lengths
for paths of length 7, 8, and 9?

398

chapter 10. mining social-network graphs

exercise 10.8.3 : consider the running example of a social network,
last
shown in fig. 10.23. suppose we use one hash function h which maps each
node (capital letter) to its ascii code. note that the ascii code for a is
01000001, and the codes for b, c, . . . are sequential, 01000010, 01000011, . . . .

(a) using this hash function, compute the values of r for each node and
radius 1. what are the estimates of the sizes of each neighborhood? how
do the estimates compare with reality?

(b) next, compute the values of r for each node and radius 2. again compute

the estimates of neighborhood sizes and compare with reality.

(c) the diameter of the graph is 3. compute the value of r and the size
estimate for the set of reachable nodes for each of the nodes of the graph.

(d) another hash function g is one plus the ascii code for a letter. repeat
(a) through (c) for hash function g. take the estimate of the size of a
neighborhood to be the average of the estimates given by h and g. how
close are these estimates?

10.9 summary of chapter 10

    social-network graphs: graphs that represent the connections in a social
network are not only large, but they exhibit a form of locality, where small
subsets of nodes (communities) have a much higher density of edges than
the average density.

    communities and clusters: while communities resemble clusters in some
ways, there are also signi   cant di   erences. individuals (nodes) normally
belong to several communities, and the usual distance measures fail to
represent closeness among nodes of a community. as a result, standard
algorithms for    nding clusters in data do not work well for community
   nding.

    betweenness: one way to separate nodes into communities is to measure
the betweenness of edges, which is the sum over all pairs of nodes of the
fraction of shortest paths between those nodes that go through the given
edge. communities are formed by deleting the edges whose betweenness
is above a given threshold.

    the girvan-newman algorithm: the girvan-newman algorithm is an
e   cient technique for computing the betweenness of edges. a breadth-
   rst search from each node is performed, and a sequence of labeling steps
computes the share of paths from the root to each other node that go
through each of the edges. the shares for an edge that are computed for
each root are summed to get the betweenness.

10.9. summary of chapter 10

399

    communities and complete bipartite graphs: a complete bipartite graph
has two groups of nodes, all possible edges between pairs of nodes chosen
one from each group, and no edges between nodes of the same group.
any su   ciently dense community (a set of nodes with many edges among
them) will have a large complete bipartite graph.

    finding complete bipartite graphs: we can    nd complete bipartite graphs
by the same techniques we used for    nding frequent itemsets. nodes of
the graph can be thought of both as the items and as the baskets. the
basket corresponding to a node is the set of adjacent nodes, thought of as
items. a complete bipartite graph with node groups of size t and s can
be thought of as    nding frequent itemsets of size t with support s.

    graph partitioning: one way to    nd communities is to partition a graph
repeatedly into pieces of roughly similar sizes. a cut is a partition of the
nodes of the graph into two sets, and its size is the number of edges that
have one end in each set. the volume of a set of nodes is the number of
edges with at least one end in that set.

    normalized cuts: we can normalize the size of a cut by taking the ratio
of the size of the cut and the volume of each of the two sets formed
by the cut. then add these two ratios to get the normalized cut value.
normalized cuts with a low sum are good, in the sense that they tend to
divide the nodes into two roughly equal parts, and have a relatively small
size.

    adjacency matrices: these matrices describe a graph. the entry in row
i and column j is 1 if there is an edge between nodes i and j, and 0
otherwise.

    degree matrices: the degree matrix for a graph has d in the ith diagonal
entry if d is the degree of the ith node. o    the diagonal, all entries are 0.

    laplacian matrices: the laplacian matrix for a graph is its degree matrix
minus its adjacency matrix. that is, the entry in row i and column i of
the laplacian matrix is the degree of the ith node of the graph, and the
entry in row i and column j, for i 6= j, is    1 if there is an edge between
nodes i and j, and 0 otherwise.

    spectral method for partitioning graphs: the lowest eigenvalue for any
laplacian matrix is 0, and its corresponding eigenvector consists of all
1   s. the eigenvectors corresponding to small eigenvalues can be used to
guide a partition of the graph into two parts of similar size with a small
cut value. for one example, putting the nodes with a positive component
in the eigenvector with the second-smallest eigenvalue into one set and
those with a negative component into the other is usually good.

400

chapter 10. mining social-network graphs

    overlapping communities: typically, individuals are members of several
communities. in graphs describing social networks, it is normal for the
id203 that two individuals are friends to rise as the number of com-
munities of which both are members grows.

    the a   liation-graph model : an appropriate model for membership in
communities is to assume that for each community there is a id203
that because of this community two members become friends (have an
edge in the social network graph). thus, the id203 that two nodes
have an edge is 1 minus the product of the probabilities that none of
the communities of which both are members cause there to be an edge
between them. we then    nd the assignment of nodes to communities and
the values of those probabilities that best describes the observed social
graph.

    maximum-likelihood estimation: an important modeling technique, use-
ful for modeling communities as well as many other things, is to compute,
as a function of all choices of parameter values that the model allows, the
id203 that the observed data would be generated. the values that
yield the highest id203 are assumed to be correct, and called the
maximum-likelihood estimate (id113).

    use of id119 : if we know membership in communities, we can
   nd the id113 by id119 or other methods. however, we cannot
   nd the best membership in communities by id119, because
membership is discrete, not continuous.

    improved community modeling by strength of membership: we can for-
mulate the problem of    nding the id113 of communities in a social graph
by assuming individuals have a strength of membership in each commu-
nity, possibly 0 if they are not a member. if we de   ne the id203 of an
edge between two nodes to be a function of their membership strengths in
their common communities, we can turn the problem of    nding the id113
into a continuous problem and solve it using id119.

    simrank : one way to measure the similarity of nodes in a graph with
several types of nodes is to start a random walker at one node and allow
it to wander, with a    xed id203 of restarting at the same node. the
distribution of where the walker can be expected to be is a good measure
of the similarity of nodes to the starting node. this process must be
repeated with each node as the starting node if we are to get all-pairs
similarity.

    triangles in social networks: the number of triangles per node is an
important measure of the closeness of a community and often re   ects its
maturity. we can enumerate or count the triangles in a graph with m
edges in o(m3/2) time, but no more e   cient algorithm exists in general.

10.9. summary of chapter 10

401

    triangle finding by mapreduce: we can    nd triangles in a single round
of mapreduce by treating it as a three-way join. each edge must be sent
to a number of reducers proportional to the cube root of the total number
of reducers, and the total computation time spent at all the reducers is
proportional to the time of the serial algorithm for triangle    nding.

    neighborhoods: the neighborhood of radius d for a node v in a directed
or undirected graph is the set of nodes reachable from v along paths of
length at most d. the neighborhood pro   le of a node is the sequence of
neighborhood sizes for all distances from 1 upwards. the diameter of a
connected graph is the smallest d for which the neighborhood of radius d
for any starting node includes the entire graph.

    transitive closure: a node v can reach node u if u is in the neighborhood
of v for some radius. the transitive closure of a graph is the set of pairs
of nodes (v, u) such that v can reach u.

    computing transitive closure: since the transitive closure can have a
number of facts equal to the square of the number of nodes of a graph, it
is infeasible to compute transitive closure directly for large graphs. one
approach is to    nd strongly connected components of the graph and col-
lapse them each to a single node before computing the transitive closure.

    transitive closure and mapreduce: we can view transitive closure com-
putation as the iterative join of a path relation (pairs of nodes v and u
such that u is known to be reachable from v) and the arc relation of the
graph. such an approach requires a number of mapreduce rounds equal
to the diameter of the graph.

    transitive closure by recursive doubling: an approach that uses fewer
mapreduce rounds is to join the path relation with itself at each round.
at each round, we double the length of paths that are able to contribute
to the transitive closure. thus, the number of needed rounds is only the
base-2 logarithm of the diameter of the graph.

    smart transitive closure: while recursive doubling can cause the same
path to be considered many times, and thus increases the total compu-
tation time (compared with iteratively joining paths with single arcs), a
variant called smart transitive closure avoids discovering the same path
more than once. the trick is to require that when joining two paths, the
   rst has a length that is a power of 2.

    approximating neighborhood sizes: by using the flajolet-martin tech-
nique for approximating the number of distinct elements in a stream,
we can    nd the neighborhood sizes at di   erent radii approximately. we
maintain a set of tail lengths for each node. to increase the radius by
1, we examine each edge (u, v) and for each tail length for u we set it

402

chapter 10. mining social-network graphs

equal to the corresponding tail length for v if the latter is larger than the
former.

10.10 references for chapter 10

simrank comes from [9]. an alternative approach in [12] views similarity of two
nodes as the propability that id93 from the two nodes will be at the
same node. [4] combines id93 with node classi   cation to predict links
in a social-network graph. [17] looks at the e   ciency of computing simrank as
a personalized id95. the approximate simrank algorithm is from [3].

the girvan-newman algorithm is from [7]. finding communities by search-

ing for complete bipartite graphs appears in [10].

normalized cuts for spectral analysis were introduced in [14]. [20] is a sur-
vey of id106 for    nding clusters, and [6] is a more general survey
of community    nding in graphs.
[11] is an analysis of communities in many
networks encountered in practice.

detection of overlapping communities is explored in [21], [22], and [23].
counting triangles using mapreduce was discussed in [16]. the method
descirbed here is from [1], which also gives a technique that works for any
subgraph. [18] discusses randomized algorithms for triangle    nding.

the anf algorithm was    rst investigated in [13].

[5] gives an additional

speedup to anf.

the smart transitive-closure algorithm was discovered by [8] and [19] in-
dependently. implementation of transitive closure using mapreduce or similar
systems is discussed in [2].

an open-source c++ implementation of many of the algorithms described

in this chapter can be found in the snap library [15].

1. f. n. afrati, d. fotakis, and j. d. ullman,    enumerating subgraph in-

stances by map-reduce,   

http://ilpubs.stanford.edu:8090/1020

2. f.n. afrati and j.d. ullman,    transitive closure and recursive datalog

implemented on clusters,    in proc. edbt (2012).

3. a. andersen, f. chung, and k. lang,    local graph partitioning using
id95 vectors,    proc. 47th annual symposium og foundations of
computer science (2006), pp. 475   486.

4. l. backstrom and j. leskovec,    supervised id93: predicting and
recommending links in social networks,    proc. fourth acm intl. conf.
on web search and data mining (2011), pp. 635   644.

5. p. boldi, m. rosa, and s. vigna,    hyperanf: approximating the neigh-
bourhood function of very large graphs on a budget,    proc. www con-
ference (2011), pp. 625   634.

10.10. references for chapter 10

403

6. s. fortunato,    community detection in graphs,    physics reports 486:3   5

(2010), pp. 75   174.

7. m. girvan and m.e.j. newman,    community structure in social and bi-

ological networks,    proc. natl. acad. sci. 99 (2002), pp. 7821   7826.

8. y.e. ioannidis,    on the computation of the transitive closure of relational
operators,    proc. 12th intl. conf. on very large data bases, pp. 403   411.

9. g. jeh and j. widom,    simrank: a measure of structural-context similar-
ity,    proceedings of the eighth acm sigkdd international conference
on knowledge discovery and data mining (2002), pp. 538   543.

10. r. kumar, p. raghavan, s. rajagopalan, and a. tomkins,    trawling
the web for emerging cyber-communities, computer networks 31:11   16
(may, 1999), pp. 1481   1493.

11. j. leskovec, k.j. lang, a. dasgupta, and m.w. mahoney,    community
structure in large networks: natural cluster sizes and the absence of large
well-de   ned clusters,    http://arxiv.org/abs/0810.1355.

12. s. melnik, h. garcia-molina, and e. rahm,    similarity    ooding: a ver-
satile graph matching algorithm and its application to schema matching,
proc. intl. conf. on data engineering (2002), pp. 117   128.

13. c.r. palmer, p.b. gibbons, and c. faloutsos,    anf: a fast and scalable
tool for data mining in massive graphs,    proc. eighth acm sigkdd
intl. conf. on knowledge discovery and data mining (2002), pp. 81   90.

14. j. shi and j. malik,    normalized cuts and image segmentation,    ieee

trans. on pattern analysis and machine intelligence,    22:8 (2000), pp. 888   
905.

15. stanford network analysis platform, http://snap.stanford.edu.

16. s. suri and s. vassilivitskii,    counting triangles and the curse of the last

reducer,    proc. www conference (2011).

17. h. tong, c. faloutsos, and j.-y. pan,    fast random walk with restart

and its applications,    icdm 2006, pp. 613   622.

18. c.e. tsourakakis, u. kang, g.l. miller, and c. faloutsos,    doulion:
counting triangles in massive graphs with a coin,    proc. fifteenth acm
sigkdd intl. conf. on knowledge discovery and data mining (2009).

19. p. valduriez and h. boral,    evaluation of recursive queries using join

indices,    expert database conf. (1986), pp. 271   293.

20. u. von luxburg,    a tutorial on spectral id91,    statistics and com-

puting bf17:4 (2007), 2007, pp. 395   416.

404

chapter 10. mining social-network graphs

21. j. yang and j. leskovec,    overlapping community detection at scale: a
nonnegative id105 approach,    acm international confer-
ence on web search and data mining, 2013.

22. j. yang, j. mcauley, j. leskovec,    detecting cohesive and 2-mode com-
munities in directed and undirected networks,    acm international con-
ference on web search and data mining, 2014.

23. j. yang, j. mcauley, j. leskovec,    community detection in networks with
node attributes,    ieee international conference on data mining, 2013.

chapter 11

id84

there are many sources of data that can be viewed as a large matrix. we
saw in chapter 5 how the web can be represented as a transition matrix. in
chapter 9, the utility matrix was a point of focus. and in chapter 10 we
examined matrices that represent social networks.
in many of these matrix
applications, the matrix can be summarized by    nding    narrower    matrices
that in some sense are close to the original. these narrow matrices have only a
small number of rows or a small number of columns, and therefore can be used
much more e   ciently than can the original large matrix. the process of    nding
these narrow matrices is called id84.

we saw a preliminary example of id84 in section 9.4.
there, we discussed uv-decomposition of a matrix and gave a simple algorithm
for    nding this decomposition. recall that a large matrix m was decomposed
into two matrices u and v whose product u v was approximately m . the
matrix u had a small number of columns whereas v had a small number of rows,
so each was signi   cantly smaller than m , and yet together they represented
most of the information in m that was useful in predicting ratings of items by
individuals.

in this chapter we shall explore the idea of id84 in
more detail. we begin with a discussion of eigenvalues and their use in    prin-
cipal component analysis    (pca). we cover id166, a
more powerful version of uv-decomposition. finally, because we are always
interested in the largest data sizes we can handle, we look at another form
of decomposition, called cur-decomposition, which is a variant of singular-
value decomposition that keeps the matrices of the decomposition sparse if the
original matrix is sparse.

405

406

chapter 11. id84

11.1 eigenvalues and eigenvectors of symmet-

ric matrices

we shall assume that you are familiar with the basics of matrix algebra: multi-
plication, transpose, determinants, and solving linear equations for example. in
this section, we shall de   ne eigenvalues and eigenvectors of a symmetric matrix
and show how to    nd them. recall a matrix is symmetric if the element in row
i and column j equals the element in row j and column i.

11.1.1 de   nitions

let m be a square matrix. let    be a constant and e a nonzero column vector
with the same number of rows as m . then    is an eigenvalue of m and e is
the corresponding eigenvector of m if m e =   e.

if e is an eigenvector of m and c is any constant, then it is also true that
c e is an eigenvector of m with the same eigenvalue. multiplying a vector by a
constant changes the length of a vector, but not its direction. thus, to avoid
ambiguity regarding the length, we shall require that every eigenvector be a
unit vector, meaning that the sum of the squares of the components of the
vector is 1. even that is not quite enough to make the eigenvector unique,
since we may still multiply by    1 without changing the sum of squares of the
components. thus, we shall normally require that the    rst nonzero component
of an eigenvector be positive.

example 11.1 : let m be the matrix

one of the eigenvectors of m is

(cid:20) 3

2

2

6 (cid:21)

(cid:20) 1/   5
2/   5 (cid:21)

and its corresponding eigenvalue is 7. the equation

2 6 (cid:21)(cid:20) 1/   5
(cid:20) 3 2

2/   5 (cid:21) = 7(cid:20) 1/   5
2/   5 (cid:21)

demonstrates the truth of this claim. note that both sides are equal to

(cid:20) 7/   5
14/   5 (cid:21)

also observe that the eigenvector is a unit vector, because (1/   5)2 + (2/   5)2 =
1/5 + 4/5 = 1.    

11.1. eigenvalues and eigenvectors of symmetric matrices407

11.1.2 computing eigenvalues and eigenvectors

we have already seen one approach to    nding an eigenpair (an eigenvalue and
its corresponding eigenvector) for a suitable matrix m in section 5.1: start with
any unit vector v of the appropriate length and compute m iv iteratively until it
converges.1 when m is a stochastic matrix, the limiting vector is the principal
eigenvector (the eigenvector with the largest eigenvalue), and its corresponding
eigenvalue is 1.2 this method for    nding the principal eigenvector, called power
iteration, works quite generally, although if the principal eigenvalue (eigenvalue
associated with the principal eigenvector) is not 1, then as i grows, the ratio
of m i+1v to m iv approaches the principal eigenvalue while m iv approaches
a vector (probably not a unit vector) with the same direction as the principal
eigenvector.

we shall take up the generalization of the power-iteration method to    nd all
eigenpairs in section 11.1.3. however, there is an o(n3)-running-time method
for computing all the eigenpairs of a symmetric n    n matrix exactly, and this
method will be presented    rst. there will always be n eigenpairs, although in
some cases, some of the eigenvalues will be identical. the method starts by
restating the equation that de   nes eigenpairs, m e =   e as (m       i)e = 0,
where

1. i is the n    n identity matrix with 1   s along the main diagonal and 0   s

elsewhere.

2. 0 is a vector of all 0   s.
a fact of id202 is that in order for (m       i)e = 0 to hold for a
vector e 6= 0, the determinant of m       i must be 0. notice that (m       i)
looks almost like the matrix m , but if m has c in one of its diagonal elements,
then (m       i) has c        there. while the determinant of an n    n matrix has
n! terms, it can be computed in various ways in o(n3) time; an example is the
method of    pivotal condensation.   

the determinant of (m       i) is an nth-degree polynomial in   , from which
we can get the n values of    that are the eigenvalues of m . for any such value,
say c, we can then solve the equation m e = c e. there are n equations in n
unknowns (the n components of e), but since there is no constant term in any
equation, we can only solve for e to within a constant factor. however, using
any solution, we can normalize it so the sum of the squares of the components
is 1, thus obtaining the eigenvector that corresponds to eigenvalue c.
example 11.2 : let us    nd the eigenpairs for the 2    2 matrix m from ex-
ample 11.1. recall m =

(cid:20) 3

2

2

6 (cid:21)

1recall m i denotes multiplying by the matrix m i times, as discussed in section 5.1.2.
2note that a stochastic matrix is not generally symmetric. symmetric matrices and
stochastic matrices are two classes of matrices for which eigenpairs exist and can be exploited.
in this chapter, we focus on techniques for symmetric matrices.

408

chapter 11. id84

then m       i is

(cid:20) 3       

2

2

6        (cid:21)

the determinant of this matrix is (3       )(6       )     4, which we must set to 0.
the equation in    to solve is thus   2     9   + 14 = 0. the roots of this equation
are    = 7 and    = 2; the    rst is the principal eigenvalue, since it is the larger.
let e be the vector of unknowns

(cid:20) x
y (cid:21)

we must solve

(cid:20) 3 2
2 6 (cid:21)(cid:20) x

y (cid:21) = 7(cid:20) x
y (cid:21)

when we multiply the matrix and vector we get two equations

3x+2y = 7x
2x+6y = 7y

notice that both of these equations really say the same thing: y = 2x. thus, a
possible eigenvector is

(cid:20) 1
2 (cid:21)

but that vector is not a unit vector, since the sum of the squares of its compo-
nents is 5, not 1. thus to get the unit vector in the same direction, we divide
each component by    5. that is, the principal eigenvector is

(cid:20) 1/   5
2/   5 (cid:21)

and its eigenvalue is 7. note that this was the eigenpair we explored in exam-
ple 11.1.

for the second eigenpair, we repeat the above with eigenvalue 2 in place of
7. the equation involving the components of e is x =    2y, and the second
eigenvector is

its corresponding eigenvalue is 2, of course.    

(cid:20)

2/   5
   1/   5 (cid:21)

11.1.3 finding eigenpairs by power iteration

we now examine the generalization of the process we used in section 5.1 to
   nd the principal eigenvector, which in that section was the id95 vector    
all we needed from among the various eigenvectors of the stochastic matrix of
the web. we start by computing the principal eigenvector by a slight general-
ization of the approach used in section 5.1. we then modify the matrix to, in

11.1. eigenvalues and eigenvectors of symmetric matrices409

e   ect, remove the principal eigenvector. the result is a new matrix whose prin-
cipal eigenvector is the second eigenvector (eigenvector with the second-largest
eigenvalue) of the original matrix. the process proceeds in that manner, re-
moving each eigenvector as we    nd it, and then using power iteration to    nd
the principal eigenvector of the matrix that remains.

let m be the matrix whose eigenpairs we would like to    nd. start with any

nonzero vector x0 and then iterate:

xk+1 :=

m xk
km xkk

where knk for a matrix or vector n denotes the frobenius norm; that is, the
square root of the sum of the squares of the elements of n . we multiply the
current vector xk by the matrix m until convergence (i.e., kxk     xk+1k is less
than some small, chosen constant). let x be xk for that value of k at which
convergence is obtained. then x is (approximately) the principal eigenvector
of m . to obtain the corresponding eigenvalue we simply compute   1 = xtm x,
which is the equation m x =   x solved for   , since x is a unit vector.

example 11.3 : take the matrix from example 11.2:

m = (cid:20) 3

2

2

6 (cid:21)

and let us start with x0 a vector with 1 for both components. to compute x1,
we multiply m x0 to get

(cid:20) 3 2
2 6 (cid:21)(cid:20) 1

1 (cid:21) = (cid:20) 5
8 (cid:21)

the frobenius norm of the result is    52 + 82 =    89 = 9.434. we obtain x1 by
dividing 5 and 8 by 9.434; that is:

x1 = (cid:20) 0.530
0.848 (cid:21)

for the next iteration, we compute

(cid:20) 3 2
2 6 (cid:21)(cid:20) 0.530

0.848 (cid:21) = (cid:20) 3.286
6.148 (cid:21)

the frobenius norm of the result is 6.971, so we divide to obtain

x2 = (cid:20) 0.471
0.882 (cid:21)

we are converging toward a normal vector whose second component is twice the
   rst. that is, the limiting value of the vector that we obtain by power iteration
is the principal eigenvector:

x = (cid:20) 0.447
0.894 (cid:21)

410

chapter 11. id84

finally, we compute the principal eigenvalue by

   = xtm x = (cid:2) 0.447 0.894 (cid:3)(cid:20) 3 2

2 6 (cid:21)(cid:20) 0.447

0.894 (cid:21) = 6.993

recall from example 11.2 that the true principal eigenvalue is 7. power iteration
will introduce small errors due either to limited precision, as was the case here,
or due to the fact that we stop the iteration before reaching the exact value of
the eigenvector. when we computed id95, the small inaccuracies did not
matter, but when we try to compute all eigenpairs, inaccuracies accumulate if
we are not careful.    

to    nd the second eigenpair we create a new matrix m     = m       1xxt.
then, use power iteration on m     to compute its largest eigenvalue. the ob-
tained x    and       correspond to the second largest eigenvalue and the corre-
sponding eigenvector of matrix m .
intuitively, what we have done is eliminate the in   uence of a given eigen-
vector by setting its associated eigenvalue to zero. the formal justi   cation is
the following two observations. if m     = m       xxt, where x and    are the
eigenpair with the largest eigenvalue, then:

1. x is also an eigenvector of m    , and its corresponding eigenvalue is 0. in

proof, observe that

m    x = (m       xxt)x = m x       xxtx = m x       x = 0

at the next-to-last step we use the fact that xtx = 1 because x is a unit
vector.

2. conversely, if v and   v are an eigenpair of a symmetric matrix m other
than the    rst eigenpair (x,   ), then they are also an eigenpair of m    .
proof :
m    v = (m    )tv = (m       xxt)tv = m tv       x(xtv) = m tv =   vv
this sequence of equalities needs the following justi   cations:

(a) if m is symmetric, then m = m t.
(b) the eigenvectors of a symmetric matrix are orthogonal. that is, the
dot product of any two distinct eigenvectors of a matrix is 0. we do
not prove this statement here.

example 11.4 : continuing example 11.3, we compute

m     = (cid:20) 3 2
(cid:20) 3

2 6 (cid:21)     6.993(cid:20) 0.447
2.795 5.589 (cid:21) = (cid:20)

6 (cid:21)    (cid:20) 1.397 2.795

0.894 (cid:21)(cid:2) 0.447 0.894 (cid:3) =
0.411 (cid:21)
1.603    0.795
   0.795

2

2

we may    nd the second eigenpair by processing the matrix above as we did the
original matrix m .    

11.1. eigenvalues and eigenvectors of symmetric matrices411

11.1.4 the matrix of eigenvectors

suppose we have an n    n symmetric matrix m whose eigenvectors, viewed
as column vectors, are e1, e2, . . . , en. let e be the matrix whose ith column
is ei. then eet = ete = i. the explanation is that the eigenvectors of a
symmetric matrix are orthonormal. that is, they are orthogonal unit vectors.

example 11.5 : for the matrix m of example 11.2, the matrix e is

et is therefore

(cid:20)

1/   5
2/   5 (cid:21)

2/   5
   1/   5
(cid:20) 2/   5    1/   5
2/   5 (cid:21)
1/   5

when we compute eet we get

(cid:20)

4/5 + 1/5    2/5 + 2/5
   2/5 + 2/5

1/5 + 4/5 (cid:21) = (cid:20) 1

0

0

1 (cid:21)

the calculation is similar when we compute ete. notice that the 1   s along
the main diagonal are the sums of the squares of the components of each of the
eigenvectors, which makes sense because they are unit vectors. the 0   s o    the
diagonal re   ect the fact that the entry in the ith row and jth column is the
dot product of the ith and jth eigenvectors. since eigenvectors are orthogonal,
these dot products are 0.    

11.1.5 exercises for section 11.1

exercise 11.1.1 : find the unit vector in the same direction as the vector
[1, 2, 3].

exercise 11.1.2 : complete example 11.4 by computing the principal eigen-
vector of the matrix that was constructed in this example. how close to the
correct solution (from example 11.2) are you?

exercise 11.1.3 : for any symmetric 3    3 matrix
   
   

a       

d       

f       

   
   

c
e

b
c

e

b

there is a cubic equation in    that says the determinant of this matrix is 0. in
terms of a through f ,    nd this equation.

412

chapter 11. id84

exercise 11.1.4 : find the eigenpairs for the following matrix:

1 1
1 2
1 3

   
   

1
3
5

   
   

using the method of section 11.1.2.

! exercise 11.1.5 : find the eigenpairs for the following matrix:

1 1
1 2
1 3

   
   

1
3
6

   
   

using the method of section 11.1.2.

exercise 11.1.6 : for the matrix of exercise 11.1.4:

(a) starting with a vector of three 1   s, use power iteration to    nd an approx-

imate value of the principal eigenvector.

(b) compute an estimate the principal eigenvalue for the matrix.

(c) construct a new matrix by subtracting out the e   ect of the principal

eigenpair, as in section 11.1.3.

(d) from your matrix of (c),    nd the second eigenpair for the original matrix

of exercise 11.1.4.

(e) repeat (c) and (d) to    nd the third eigenpair for the original matrix.

exercise 11.1.7 : repeat exercise 11.1.6 for the matrix of exercise 11.1.5.

11.2 principal-component analysis

principal-component analysis, or pca, is a technique for taking a dataset con-
sisting of a set of tuples representing points in a high-dimensional space and
   nding the directions along which the tuples line up best. the idea is to treat
the set of tuples as a matrix m and    nd the eigenvectors for m m t or m tm .
the matrix of these eigenvectors can be thought of as a rigid rotation in a high-
dimensional space. when you apply this transformation to the original data,
the axis corresponding to the principal eigenvector is the one along which the
points are most    spread out,    more precisely, this axis is the one along which
the variance of the data is maximized. put another way, the points can best be
viewed as lying along this axis, with small deviations from this axis. likewise,
the axis corresponding to the second eigenvector (the eigenvector correspond-
ing to the second-largest eigenvalue) is the axis along which the variance of
distances from the    rst axis is greatest, and so on.

11.2. principal-component analysis

413

we can view pca as a data-mining technique. the high-dimensional data
can be replaced by its projection onto the most important axes. these axes
are the ones corresponding to the largest eigenvalues. thus, the original data
is approximated by data that has many fewer dimensions and that summarizes
well the original data.

11.2.1 an illustrative example

we shall start the exposition with a contrived and simple example.
in this
example, the data is two-dimensional, a number of dimensions that is too small
to make pca really useful. moreover, the data, shown in fig. 11.1 has only
four points, and they are arranged in a simple pattern along the 45-degree line
to make our calculations easy to follow. that is, to anticipate the result, the
points can best be viewed as lying along the axis that is at a 45-degree angle,
with small deviations in the perpendicular direction.

(3,4)

(1,2)

(4,3)

(2,1)

figure 11.1: four points in a two-dimensional space

to begin, let us represent the points by a matrix m with four rows     one
for each point     and two columns, corresponding to the x-axis and y-axis. this
matrix is

m =    
         

1
2
3
4

2
1
4
3

   
         

compute m tm , which is

m tm = (cid:20) 1

2

2 3
1 4

4

3 (cid:21)

1
2
3
4

   
         

2
1
4
3

   
         

= (cid:20) 30 28
28 30 (cid:21)

we may    nd the eigenvalues of the matrix above by solving the equation

(30       )(30       )     28    28 = 0

414

chapter 11. id84

as we did in example 11.2. the solution is    = 58 and    = 2.

following the same procedure as in example 11.2, we must solve

(cid:20) 30 28
28 30 (cid:21)(cid:20) x

y (cid:21) = 58(cid:20) x
y (cid:21)

when we multiply out the matrix and vector we get two equations

30x+28y = 58x
28x+30y = 58y

both equations tell us the same thing: x = y. thus, the unit eigenvector
corresponding to the principal eigenvalue 58 is

(cid:20) 1/   2
1/   2 (cid:21)

for the second eigenvalue, 2, we perform the same process. multiply out

to get the two equations

(cid:20) 30 28
28 30 (cid:21)(cid:20) x

y (cid:21) = 2(cid:20) x
y (cid:21)

30x+28y = 2x
28x+30y = 2y

both equations tell us the same thing: x =    y. thus, the unit eigenvector
corresponding to the principal eigenvalue 2 is

(cid:20)    1/   2
1/   2 (cid:21)

while we promised to write eigenvectors with their    rst component positive,
we choose the opposite here because it makes the transformation of coordinates
easier to follow in this case.

now, let us construct e, the matrix of eigenvectors for the matrix m tm .

placing the principal eigenvector    rst, the matrix of eigenvectors is

e = (cid:20) 1/   2    1/   2
1/   2 (cid:21)

1/   2

any matrix of orthonormal vectors (unit vectors that are orthogonal to one
another) represents a rotation and/or re   ection of the axes of a euclidean space.
the matrix above can be viewed as a rotation 45 degrees counterclockwise. for
example, let us multiply the matrix m that represents each of the points of
fig. 11.1 by e. the product is

m e =    
         

1 2
2 1
3 4
4 3

   
         

(cid:20) 1/   2    1/   2
1/   2

1/   2 (cid:21) =    
         

3/   2
1/   2
3/   2    1/   2
7/   2
1/   2
7/   2    1/   2

   
         

11.2. principal-component analysis

415

(1,2)

(3,4)

(1.5,1.5)

(2,1)

(3.5,3.5)

(4,3)

figure 11.2: figure 11.1 with the axes rotated 45 degrees counterclockwise

we see the    rst point, [1, 2], has been transformed into the point

[3/   2, 1/   2]

if we examine fig. 11.2, with the dashed line representing the new x-axis, we
see that the projection of the    rst point onto that axis places it at distance
3/   2 from the origin. to check this fact, notice that the point of projection for
both the    rst and second points is [1.5, 1.5] in the original coordinate system,
and the distance from the origin to this point is

p(1.5)2 + (1.5)2 = p9/2 = 3/   2

moreover, the new y-axis is, of course, perpendicular to the dashed line. the
   rst point is at distance 1/   2 above the new x-axis in the direction of the
y-axis. that is, the distance between the points [1, 2] and [1.5, 1.5] is

p(1     1.5)2 + (2     1.5)2 = p(   1/2)2 + (1/2)2 = p1/2 = 1/   2

figure 11.3 shows the four points in the rotated coordinate system.

(3/

2

, 1/

)

2

(7/

2

, 1/

)

2

(3/

2

,    1/

)

2

(7/

2

,    1/

2

)

figure 11.3: the points of fig. 11.1 in the new coordinate system

the second point, [2, 1] happens by coincidence to project onto the same
point of the new x-axis. it is 1/   2 below that axis along the new y-axis, as is

416

chapter 11. id84

con   rmed by the fact that the second row in the matrix of transformed points

is [3/   2,   1/   2]. the third point, [3, 4] is transformed into [7/   2, 1/   2] and
the fourth point, [4, 3], is transformed to [7/   2,   1/   2]. that is, they both
project onto the same point of the new x-axis, and that point is at distance
7/   2 from the origin, while they are 1/   2 above and below the new x-axis in
the direction of the new y-axis.

11.2.2 using eigenvectors for id84

from the example we have just worked out, we can see a general principle. if
m is a matrix whose rows each represent a point in a euclidean space with
any number of dimensions, we can compute m tm and compute its eigenpairs.
let e be the matrix whose columns are the eigenvectors, ordered as largest
eigenvalue    rst. de   ne the matrix l to have the eigenvalues of m tm along
the diagonal, largest    rst, and 0   s in all other entries. then, since m tm e =
  e = e   for each eigenvector e and its corresponding eigenvalue   , it follows
that m tm e = el.

we observed that m e is the points of m transformed into a new coordi-
nate space. in this space, the    rst axis (the one corresponding to the largest
eigenvalue) is the most signi   cant; formally, the variance of points along that
axis is the greatest. the second axis, corresponding to the second eigenpair,
is next most signi   cant in the same sense, and the pattern continues for each
of the eigenpairs.
if we want to transform m to a space with fewer dimen-
sions, then the choice that preserves the most signi   cance is the one that uses
the eigenvectors associated with the largest eigenvalues and ignores the other
eigenvalues.

that is, let ek be the    rst k columns of e. then m ek is a k-dimensional

representation of m .

example 11.6 : let m be the matrix from section 11.2.1. this data has only
two dimensions, so the only id84 we can do is to use k = 1;
i.e., project the data onto a one dimensional space. that is, we compute m e1
by

   
         

1 2
2 1
3 4
4 3

   
         

1/   2 (cid:21) =    
(cid:20) 1/   2
         

3/   2
3/   2
7/   2
7/   2

   
         

the e   ect of this transformation is to replace the points of m by their pro-
jections onto the x-axis of fig. 11.3. while the    rst two points project to the
same point, as do the third and fourth points, this representation makes the
best possible one-dimensional distinctions among the points.    

11.2. principal-component analysis

417

11.2.3 the matrix of distances

let us return to the example of section 11.2.1, but instead of starting with
m tm , let us examine the eigenvalues of m m t. since our example m has
more rows than columns, the latter is a bigger matrix than the former, but if
m had more columns than rows, we would actually get a smaller matrix. in
the running example, we have

m m t =    
         

1
2
3
4

2
1
4
3

   
         

(cid:20) 1

2

2 3
1 4

4

3 (cid:21) =    
         

4
5

11 10
5
4
10 11
11 10 25 24
10 11 24 25

   
         

like m tm , we see that m m t is symmetric. the entry in the ith row and
jth column has a simple interpretation; it is the dot product of the vectors
represented by the ith and jth points (rows of m ).

there is a strong relationship between the eigenvalues of m tm and m m t.

suppose e is an eigenvector of m tm ; that is,

m tm e =   e

multiply both sides of this equation by m on the left. then

m m t(m e) = m   e =   (m e)

thus, as long as m e is not the zero vector 0, it will be an eigenvector of m m t
and    will be an eigenvalue of m m t as well as of m tm .

the converse holds as well. that is, if e is an eigenvector of m m t with
corresponding eigenvalue   , then start with m m te =   e and multiply on the
left by m t to conclude that m tm (m te) =   (m te). thus, if m te is not 0,
then    is also an eigenvalue of m tm .

we might wonder what happens when m te = 0.

in that case, m m te
is also 0, but e is not 0 because 0 cannot be an eigenvector. however, since
0 =   e, we conclude that    = 0.

we conclude that the eigenvalues of m m t are the eigenvalues of m tm
plus additional 0   s. if the dimension of m m t were less than the dimension
of m tm , then the opposite would be true; the eigenvalues of m tm would be
those of m m t plus additional 0   s.

1/2

3/   116
3/   116    1/2
7/   116
7/   116    1/2    3/   116

7/   116
7/   116    1/2
1/2    3/   116    1/2

1/2

1/2

   
         

   
         

figure 11.4: eigenvector matrix for m m t

418

chapter 11. id84

example 11.7 : the eigenvalues of m m t for our running example must in-
clude 58 and 2, because those are the eigenvalues of m tm as we observed in
section 11.2.1. since m m t is a 4    4 matrix, it has two other eigenvalues,
which must both be 0. the matrix of eigenvectors corresponding to 58, 2, 0,
and 0 is shown in fig. 11.4.    

11.2.4 exercises for section 11.2

exercise 11.2.1 : let m be the matrix of data points

1
2
3
4

   
         

1
4
9
16

   
         

(a) what are m tm and m m t?

(b) compute the eigenpairs for m tm .

! (c) what do you expect to be the eigenvalues of m m t?

! (d) find the eigenvectors of m m t, using your eigenvalues from part (c).

! exercise 11.2.2 : prove that if m is any matrix, then m tm and m m t are

symmetric.

11.3 id166

we now take up a second form of matrix analysis that leads to a low-dimensional
representation of a high-dimensional matrix. this approach, called singular-
value decomposition (svd), allows an exact representation of any matrix, and
also makes it easy to eliminate the less important parts of that representation to
produce an approximate representation with any desired number of dimensions.
of course the fewer the dimensions we choose, the less accurate will be the
approximation.

we begin with the necessary de   nitions. then, we explore the idea that the
svd de   nes a small number of    concepts    that connect the rows and columns
of the matrix. we show how eliminating the least important concepts gives us a
smaller representation that closely approximates the original matrix. next, we
see how these concepts can be used to query the original matrix more e   ciently,
and    nally we o   er an algorithm for performing the svd itself.

11.3.1 de   nition of svd
let m be an m   n matrix, and let the rank of m be r. recall that the rank of
a matrix is the largest number of rows (or equivalently columns) we can choose

11.3. id166

419

for which no nonzero linear combination of the rows is the all-zero vector 0 (we
say a set of such rows or columns is independent). then we can    nd matrices
u ,   , and v as shown in fig. 11.5 with the following properties:

1. u is an m    r column-orthonormal matrix ; that is, each of its columns is

a unit vector and the dot product of any two columns is 0.

2. v is an n    r column-orthonormal matrix. note that we always use v in

its transposed form, so it is the rows of v t that are orthonormal.

3.    is a diagonal matrix; that is, all elements not on the main diagonal are

0. the elements of    are called the singular values of m .

n

r

r

m

m

=

u

  

n

v t

r

figure 11.5: the form of a id166

example 11.8 : figure 11.6 gives a rank-2 matrix representing ratings of
movies by users. in this contrived example there are two    concepts    underlying
the movies: science-   ction and romance. all the boys rate only science-   ction,
and all the girls rate only romance. it is this existence of two strictly adhered to
concepts that gives the matrix a rank of 2. that is, we may pick one of the    rst
four rows and one of the last three rows and observe that there is no nonzero
linear sum of these rows that is 0. but we cannot pick three independent rows.
for example, if we pick rows 1, 2, and 7, then three times the    rst minus the
second, plus zero times the seventh is 0.

we can make a similar observation about the columns. we may pick one
of the    rst three columns and one of the last two coluns, and they will be
independent, but no set of three columns is independent.

the decomposition of the matrix m from fig. 11.6 into u ,   , and v , with
all elements correct to two signi   cant digits, is shown in fig. 11.7. since the
rank of m is 2, we can use r = 2 in the decomposition. we shall see how to
compute this decomposition in section 11.3.6.    

420

chapter 11. id84

s
t
a
r
 

w

a
r
s

c
a
s
a
b
l
a
n
c
a

t

i
t
a
n
i
c

m

a
t
r
i
x

a

l
i
e
n

joe
jim
john
jack
jill
jenny
jane

1   1   1   0   0
3   3   3   0   0
4   4   4   0   0
5   5   5   0   0
0   0   0   4   4
0   0   0   5   5
0   0   0   2   2

figure 11.6: ratings of movies by users

1 1
3 3
4 4
5 5
0 0
0 0
0 0

   

                           

1 0
3 0
4 0
5 0
0 4
0 5
0 2

m

0
0
0
0
4
5
2

   

                           

=

.14
.42
.56
.70
0
0
0

   

                           

0
0
0
0
.60
.75
.30

   

                           

(cid:20) 12.4

0

0

9.5 (cid:21)(cid:20) .58 .58

0

0

.58
0

0

0

.71 .71 (cid:21)

u

  

v t

figure 11.7: svd for the matrix m of fig. 11.6

11.3.2

interpretation of svd

the key to understanding what svd o   ers is in viewing the r columns of u ,
  , and v as representing concepts that are hidden in the original matrix m . in
example 11.8, these concepts are clear; one is    science    ction    and the other
is    romance.    let us think of the rows of m as people and the columns of
m as movies. then matrix u connects people to concepts. for example, the
person joe, who corresponds to row 1 of m in fig. 11.6, likes only the concept
science    ction. the value 0.14 in the    rst row and    rst column of u is smaller
than some of the other entries in that column, because while joe watches only
science    ction, he doesn   t rate those movies highly. the second column of the
   rst row of u is 0, because joe doesn   t rate romance movies at all.

the matrix v relates movies to concepts. the 0.58 in each of the    rst three
columns of the    rst row of v t indicates that the    rst three movies     the matrix,
alien, and star wars     each are of the science-   ction genre, while the 0   s in
the last two columns of the    rst row say that these movies do not partake of
the concept romance at all. likewise, the second row of v t tells us that the

11.3. id166

421

movies casablanca and titanic are exclusively romances.

finally, the matrix    gives the strength of each of the concepts.

in our
example, the strength of the science-   ction concept is 12.4, while the strength
of the romance concept is 9.5. intuitively, the science-   ction concept is stronger
because the data provides more information about the movies of that genre and
the people who like them.

in general, the concepts will not be so clearly delineated. there will be fewer
0   s in u and v , although    is always a diagonal matrix and will always have
0   s o    the diagonal. the entities represented by the rows and columns of m
(analogous to people and movies in our example) will partake of several di   erent
concepts to varying degrees. in fact, the decomposition of example 11.8 was
especially simple, since the rank of the matrix m was equal to the desired
number of columns of u ,   , and v . we were therefore able to get an exact
decomposition of m with only two columns for each of the three matrices u ,   ,
and v ; the product u   v t, if carried out to in   nite precision, would be exactly
m . in practice, life is not so simple. when the rank of m is greater than the
number of columns we want for the matrices u ,   , and v , the decomposition is
not exact. we need to eliminate from the exact decomposition those columns of
u and v that correspond to the smallest singular values, in order to get the best
approximation. the following example is a slight modi   cation of example 11.8
that will illustrate the point.

s

t
a
r
 

w

a
r
s

c
a
s
a
b
l
a
n
c
a

t

i
t
a
n
i
c

m

a
t
r
i
x

a

l
i
e
n

joe
jim
john
jack
jill
jenny
jane

1   1   1   0   0
3   3   3   0   0
4   4   4   0   0
5   5   5   0   0
0   2   0   4   4
0   0   0   5   5
0   1   0   2   2

figure 11.8: the new matrix m    , with ratings for alien by two additional raters

example 11.9 : figure 11.8 is almost the same as fig. 11.6, but jill and jane
rated alien, although neither liked it very much. the rank of the matrix in
fig. 11.8 is 3; for example the    rst, sixth, and seventh rows are independent,
but you can check that no four rows are independent. figure 11.9 shows the
decomposition of the matrix from fig. 11.8.

we have used three columns for u ,   , and v because they decompose a
matrix of rank three. the columns of u and v still correspond to concepts.
the    rst is still    science    ction    and the second is    romance.    it is harder to

422

chapter 11. id84

1
3
4
5
0
0
0

   

                           

0 0
0 0
0 0
0 0
4 4
5 5
2 2

   

                           

=

1 1
3 3
4 4
5 5
2 0
0 0
1 0

m    

   

                           

.02    .01
.13
.07    .03
.41
.09    .04
.55
.11    .05
.68
.15    .59
.65
.07    .73    .67
.07    .29
.32
u

   

                           

   
   

12.4

0
0

0
9.5
0

0
0
1.3

   
   

   
   

.09

.59 .56

.56
.09
.12    .02 .12    .69    .69
.40    .80 .40
.09

.09

   
   

  

v t

figure 11.9: svd for the matrix m     of fig. 11.8

explain the third column   s concept, but it doesn   t matter all that much, because
its weight, as given by the third nonzero entry in   , is very low compared with
the weights of the    rst two concepts.    

in the next section, we consider eliminating some of the least important
concepts. for instance, we might want to eliminate the third concept in ex-
ample 11.9, since it really doesn   t tell us much, and the fact that its associated
singular value is so small con   rms its unimportance.

11.3.3 id84 using svd

suppose we want to represent a very large matrix m by its svd components u ,
  , and v , but these matrices are also too large to store conveniently. the best
way to reduce the dimensionality of the three matrices is to set the smallest of
the singular values to zero. if we set the s smallest singular values to 0, then
we can also eliminate the corresponding s columns of u and v .

example 11.10 : the decomposition of example 11.9 has three singular val-
ues. suppose we want to reduce the number of dimensions to two. then we
set the smallest of the singular values, which is 1.3, to zero. the e   ect on the
expression in fig. 11.9 is that the third column of u and the third row of v t are

11.3. id166

423

multiplied only by 0   s when we perform the multiplication, so this row and this
column may as well not be there. that is, the approximation to m     obtained
by using only the two largest singular values is that shown in fig. 11.10.

   

                           

.13
.02
.41
.07
.55
.09
.68
.11
.15    .59
.07    .73
.07    .29

   

                           

(cid:20) 12.4

0

0

9.5 (cid:21)(cid:20) .56

.59 .56

.12    .02 .12    .69    .69 (cid:21)

.09

.09

=

   

                           

0.93 0.95 0.93 .014 .014
2.93 2.99 2.93 .000 .000
3.92 4.01 3.92 .026 .026
4.84 4.96 4.84 .040 .040
0.37 1.21 0.37 4.04 4.04
0.35 0.65 0.35 4.87 4.87
0.16 0.57 0.16 1.98 1.98

   

                           

figure 11.10: dropping the lowest singular value from the decomposition of
fig. 11.7

the resulting matrix is quite close to the matrix m     of fig. 11.8. ideally, the
entire di   erence is the result of making the last singular value be 0. however,
in this simple example, much of the di   erence is due to rounding error caused
by the fact that the decomposition of m     was only correct to two signi   cant
digits.    

11.3.4 why zeroing low singular values works

the choice of the lowest singular values to drop when we reduce the number of
dimensions can be shown to minimize the root-mean-square error between the
original matrix m and its approximation. since the number of entries is    xed,
and the square root is a monotone operation, we can simplify and compare
the frobenius norms of the matrices involved. recall that the frobenius norm
of a matrix m , denoted kmk, is the square root of the sum of the squares of
the elements of m . note that if m is the di   erence between one matrix and
its approximation, then kmk is proportional to the rmse (root-mean-square
error) between the matrices.
to explain why choosing the smallest singular values to set to 0 minimizes
the rmse or frobenius norm of the di   erence between m and its approxima-
tion, let us begin with a little matrix algebra. suppose m is the product of
three matrices m = p qr. let mij, pij , qij , and rij be the elements in row i
and column j of m , p , q, and r, respectively. then the de   nition of matrix

424

chapter 11. id84

how many singular values should we retain?

a useful rule of thumb is to retain enough singular values to make up
90% of the energy in   . that is, the sum of the squares of the retained
singular values should be at least 90% of the sum of the squares of all the
singular values. in example 11.10, the total energy is (12.4)2 + (9.5)2 +
(1.3)2 = 245.70, while the retained energy is (12.4)2 + (9.5)2 = 244.01.
thus, we have retained over 99% of the energy. however, were we to
eliminate the second singular value, 9.5, the retained energy would be
only (12.4)2/245.70 or about 63%.

multiplication tells us

mij = xk x   

pikqk   r   j

then

kmk2 = xi xj

(mij )2 = xi xj (cid:16)xk x   

pikqk   r   j(cid:17)2

(11.1)

when we square a sum of terms, as we do on the right side of equation 11.1, we
e   ectively create two copies of the sum (with di   erent indices of summation)
and multiply each term of the    rst sum by each term of the second sum. that
is,

(cid:16)xk x   

pikqk   r   j(cid:17)2

= xk x    xm xn

pikqk   r   jpinqnmrmj

we can thus rewrite equation 11.1 as

kmk2 = xi xj xk x    xn xm

pikqk   r   jpinqnmrmj

(11.2)

now, let us examine the case where p , q, and r are really the svd of m .
that is, p is a column-orthonormal matrix, q is a diagonal matrix, and r is
the transpose of a column-orthonormal matrix. that is, r is row-orthonormal;
its rows are unit vectors and the dot product of any two di   erent rows is 0. to
begin, since q is a diagonal matrix, qk    and qnm will be zero unless k =     and
n = m. we can thus drop the summations for     and m in equation 11.2 and
set k =     and n = m. that is, equation 11.2 becomes

kmk2 = xi xj xk xn

pikqkkrkj pinqnnrnj

(11.3)

next, reorder the summation, so i is the innermost sum. equation 11.3 has
only two factors pik and pin that involve i; all other factors are constants as far
as summation over i is concerned. since p is column-orthonormal, we know

11.3. id166

425

that pi pikpin is 1 if k = n and 0 otherwise. that is, in equation 11.3 we can

set k = n, drop the factors pik and pin, and eliminate the sums over i and n,
yielding

kmk2 = xj xk

qkkrkj qkkrkj

(11.4)

since r is row-orthonormal, pj rkj rkj is 1. thus, we can eliminate the

terms rkj and the sum over j, leaving a very simple formula for the frobenius
norm:

kmk2 = xk

(qkk)2

(11.5)

next, let us apply this formula to a matrix m whose svd is m = u   v t.
let the ith diagonal element of    be   i, and suppose we preserve the    rst n
of the r diagonal elements of   , setting the rest to 0. let       be the resulting
diagonal matrix. let m     = u      v t be the resulting approximation to m . then
m     m     = u (            )v t is the matrix giving the errors that result from our
approximation.
if we apply equation 11.5 to the matrix m     m    , we see that km     m    k2
equals the sum of the squares of the diagonal elements of             . but             
has 0 for the    rst n diagonal elements and   i for the ith diagonal element,
where n < i     r. that is, km     m    k2 is the sum of the squares of the elements
of    that were set to 0. to minimize km     m    k2, pick those elements to be
the smallest in   . doing so gives the least possible value of km     m    k2 under
the constraint that we preserve n of the diagonal elements, and it therefore
minimizes the rmse under the same constraint.

11.3.5 querying using concepts

in this section we shall look at how svd can help us answer certain queries
e   ciently, with good accuracy. let us assume for example that we have de-
composed our original movie-rating data (the rank-2 data of fig. 11.6) into the
svd form of fig. 11.7. quincy is not one of the people represented by the
original matrix, but he wants to use the system to know what movies he would
like. he has only seen one movie, the matrix, and rated it 4. thus, we can
represent quincy by the vector q = [4, 0, 0, 0, 0], as if this were one of the rows
of the original matrix.

if we used a collaborative-   ltering approach, we would try to compare
quincy with the other users represented in the original matrix m .
instead,
we can map quincy into    concept space    by multiplying him by the matrix v
of the decomposition. we    nd qv = [2.32, 0].3 that is to say, quincy is high
in science-   ction interest, and not at all interested in romance.

we now have a representation of quincy in concept space, derived from, but
di   erent from his representation in the original    movie space.    one useful thing
we can do is to map his representation back into movie space by multiplying

3note that fig. 11.7 shows v t, while this multiplication requires v .

426

chapter 11. id84

[2.32, 0] by v t. this product is [1.35, 1.35, 1.35, 0, 0]. it suggests that quincy
would like alien and star wars, but not casablanca or titanic.

another sort of query we can perform in concept space is to    nd users similar
to quincy. we can use v to map all users into concept space. for example,
joe maps to [1.74, 0], and jill maps to [0, 5.68]. notice that in this simple
example, all users are either 100% science-   ction fans or 100% romance fans, so
each vector has a zero in one component. in reality, people are more complex,
and they will have di   erent, but nonzero, levels of interest in various concepts.
in general, we can measure the similarity of users by their cosine distance in
concept space.

example 11.11 : for the case introduced above, note that the concept vectors
for quincy and joe, which are [2.32, 0] and [1.74, 0], respectively, are not the
same, but they have exactly the same direction. that is, their cosine distance
is 0. on the other hand, the vectors for quincy and jill, which are [2.32, 0] and
[0, 5.68], respectively, have a dot product of 0, and therefore their angle is 90
degrees. that is, their cosine distance is 1, the maximum possible.    

11.3.6 computing the svd of a matrix

the svd of a matrix m is strongly connected to the eigenvalues of the symmet-
ric matrices m tm and m m t. this relationship allows us to obtain the svd
of m from the eigenpairs of the latter two matrices. to begin the explanation,
start with m = u   v t, the expression for the svd of m . then

m t = (u   v t)t = (v t)t  tu t = v   tu t

since    is a diagonal matrix, transposing it has no e   ect. thus, m t = v   u t.
now, m tm = v   u tu   v t. remember that u is an orthonormal matrix,

so u tu is the identity matrix of the appropriate size. that is,

m tm = v   2v t

multiply both sides of this equation on the right by v to get

m tm v = v   2v tv

since v is also an orthonormal matrix, we know that v tv is the identity. thus

m tm v = v   2

(11.6)

since    is a diagonal matrix,   2 is also a diagonal matrix whose entry in the
ith row and column is the square of the entry in the same position of   . now,
equation (11.6) should be familiar. it says that v is the matrix of eigenvectors
of m tm and   2 is the diagonal matrix whose entries are the corresponding
eigenvalues.

11.3. id166

427

thus, the same algorithm that computes the eigenpairs for m tm gives us
the matrix v for the svd of m itself. it also gives us the singular values for
this svd; just take the square roots of the eigenvalues for m tm .

only u remains to be computed, but it can be found in the same way we

found v . start with

m m t = u   v t(u   v t)t = u   v tv   u t = u   2u t

then by a series of manipulations analogous to the above, we learn that

m m tu = u   2

that is, u is the matrix of eigenvectors of m m t.

a small detail needs to be explained concerning u and v . each of these
matrices have r columns, while m tm is an n  n matrix and m m t is an m  m
matrix. both n and m are at least as large as r. thus, m tm and m m t should
have an additional n     r and m     r eigenpairs, respectively, and these pairs do
not show up in u , v , and   . since the rank of m is r, all other eigenvalues
will be 0, and these are not useful.

11.3.7 exercises for section 11.3

exercise 11.3.1 : in fig. 11.11 is a matrix m . it has rank 2, as you can see by
observing that the    rst column plus the third column minus twice the second
column equals 0.

1 2
3 4
5 4
0 2
1 3

   

               

3
5
3
4
5

   

               

figure 11.11: matrix m for exercise 11.3.1

(a) compute the matrices m tm and m m t.

! (b) find the eigenvalues for your matrices of part (a).

(c) find the eigenvectors for the matrices of part (a).

(d) find the svd for the original matrix m from parts (b) and (c). note
that there are only two nonzero eigenvalues, so your matrix    should have
only two singular values, while u and v have only two columns.

(e) set your smaller singular value to 0 and compute the one-dimensional

approximation to the matrix m from fig. 11.11.

428

chapter 11. id84

(f) how much of the energy of the original singular values is retained by the

one-dimensional approximation?

exercise 11.3.2 : use the svd from fig. 11.7. suppose leslie assigns rating 3
to alien and rating 4 to titanic, giving us a representation of leslie in    movie
space    of [0, 3, 0, 0, 4]. find the representation of leslie in concept space. what
does that representation predict about how well leslie would like the other
movies appearing in our example data?

! exercise 11.3.3 : demonstrate that the rank of the matrix in fig. 11.8 is 3.

! exercise 11.3.4 : section 11.3.5 showed how to guess the movies a person
would most like. how would you use a similar technique to guess the people
that would most like a given movie, if all you had were the ratings of that movie
by a few people?

11.4 cur decomposition

there is a problem with svd that does not show up in the running example
of section 11.3. in large-data applications, it is normal for the matrix m being
decomposed to be very sparse; that is, most entries are 0. for example, a
matrix representing many documents (as rows) and the words they contain (as
columns) will be sparse, because most words are not present in most documents.
similarly, a matrix of customers and products will be sparse because most
people do not buy most products.

we cannot deal with dense matrices that have millions or billions of rows
and/or columns. however, with svd, even if m is sparse, u and v will be
dense.4 since    is diagonal, it will be sparse, but    is usually much smaller
than u and v , so its sparseness does not help.

in this section, we shall consider another approach to decomposition, called
cur-decomposition. the merit of this approach lies in the fact that if m is
sparse, then the two large matrices (called c and r for    columns    and    rows   )
analogous to u and v in svd are also sparse. only the matrix in the middle
(analogous to    in svd) is dense, but this matrix is small so the density does
not hurt too much.

unlike svd, which gives an exact decomposition as long as the parameter r
is taken to be at least as great as the rank of the matrix m , cur-decomposition
is an approximation no matter how large we make r. there is a theory that
guarantees convergence to m as r gets larger, but typically you have to make r
so large to get, say within 1% that the method becomes impractical. neverthe-
less, a decomposition with a relatively small value of r has a good id203
of being a useful and accurate decomposition.

4in fig. 11.7, it happens that u and v have a signi   cant number of 0   s. however, that is
an artifact of the very regular nature of our example matrix m and is not the case in general.

11.4. cur decomposition

429

why the pseudoinverse works

in general suppose a matrix m is equal to a product of matrices xzy .
if all the inverses exist, then the rule for inverse of a product tell us
m    1 = y    1z    1x    1. since in the case we are interested in, xzy is
an svd, we know x is column-orthonormal and y is row-orthonormal.
in either of these cases, the inverse and the transpose are the same. that
is, xx t is an identity matrix of the appropriate size, and so is y y t.
thus, m    1 = y tz    1x t.

we also know z is a diagonal matrix. if there are no 0   s along the
diagonal, then z    1 is formed from z by taking the numerical inverse of
each diagonal element. it is only when there are 0   s along the diagonal
of z that we are unable to    nd an element for the same position in the
inverse such that we can get an identity matrix when we multiply z by
its inverse. that is why we resort to a    pseudoinverse,    accepting the
fact that the product zz + will not be an identity matrix, but rather a
diagonal matrix where the ith diagonal entry is 1 if the ith element of z
is nonzero and 0 if the ith element of z is 0.

11.4.1 de   nition of cur

let m be a matrix of m rows and n columns. pick a target number of    concepts   
r to be used in the decomposition. a cur-decomposition of m is a randomly
chosen set of r columns of m , which form the m   r matrix c, and a randomly
chosen set of r rows of m , which form the r    n matrix r. there is also an
r    r matrix u that is constructed from c and r as follows:

1. let w be the r    r matrix that is the intersection of the chosen columns
of c and the chosen rows of r. that is, the element in row i and column
j of w is the element of m whose column is the jth column of c and
whose row is the ith row of r.

2. compute the svd of w ; say w = x  y t.

3. compute   +, the moore-penrose pseudoinverse of the diagonal matrix
  . that is, if the ith diagonal element of    is    6= 0, then replace it by
1/  . but if the ith element is 0, leave it as 0.

4. let u = y (  +)2x t.

we shall defer to section 11.4.3 an example where we illustrate the entire cur
process, including the important matter of how the matrices c and r should
be chosen to make the approximation to m have a small expected value.

430

chapter 11. id84

11.4.2 choosing rows and columns properly

recall that the choice of rows and columns is random. however, this choice
must be biased so that the more important rows and columns have a better
chance of being picked. the measure of importance we must use is the square
of the frobenius norm, that is, the sum of the squares of the elements of the
ij, the square of the frobenius norm of m .
then each time we select a row, the id203 pi with which we select row i
ij/f . each time we select a column, the id203 qj with which we

row or column. let f = pi,j m2
is pj m2
select column j is pi m2

ij /f .

s
t
a
r
 

w

a
r
s

c
a
s
a
b
l
a
n
c
a

t

i
t
a
n
i
c

m

a
t
r
i
x

a

l
i
e
n

joe
jim
john
jack
jill
jenny
jane

1   1   1   0   0
3   3   3   0   0
4   4   4   0   0
5   5   5   0   0
0   0   0   4   4
0   0   0   5   5
0   0   0   2   2

figure 11.12: matrix m , repeated from fig. 11.6

example 11.12 : let us reconsider the matrix m from fig. 11.6, which we
repeat here as fig. 11.12. the sum of the squares of the elements of m is 243.
the three columns for the science-   ction movies the matrix, alien, and star
wars each have a squared frobenius norm of 12 + 32 + 42 + 52 = 51, so their
probabilities are each 51/243 = .210. the remaining two columns each have a
squared frobenius norm of 42 + 52 + 22 = 45, and therefore their probabilities
are each 45/243 = .185.

the seven rows of m have squared frobenius norms of 3, 27, 48, 75, 32,
50, and 8, respectively. thus, their respective probabilities are .012, .111, .198,
.309, .132, .206, and .033.    

now, let us select r columns for the matrix c. for each column, we choose
randomly from the columns of m . however, the selection is not with uniform
id203; rather, the jth column is selected with id203 qj. recall that
id203 is the sum of the squares of the elements in that column divided by
the sum of the squares of all the elements of the matrix. each column of c is
chosen independently from the columns of m , so there is some chance that a
column will be selected more than once. we shall discuss how to deal with this
situation after explaining the basics of cur-decomposition.

11.4. cur decomposition

431

having selected each of the columns of m , we scale each column by dividing
its elements by the square root of the expected number of times this column
would be picked. that is, we divide the elements of the jth column of m , if it
is selected, by    rqj. the scaled column of m becomes a column of c.

rows of m are selected for r in the analogous way. for each row of r we
select from the rows of m , choosing row i with id203 pi. recall pi is the
sum of the squares of the elements of the ith row divided by the sum of the
squares of all the elements of m . we then scale each chosen row by dividing
by    rpi if it is the ith row of m that was chosen.

example 11.13 : let r = 2 for our cur-decomposition. suppose that our
random selection of columns from matrix m of fig. 11.12 is    rst alien (the
second column) and then casablanca (the fourth column). the column for alien
is [1, 3, 4, 5, 0, 0, 0]t, and we must scale this column by dividing by    rq2. recall
from example 11.12 that the id203 associated with the alien column is
.210, so the division is by    2    0.210 = 0.648. to two decimal places, the

scaled column for alien is [1.54, 4.63, 6.17, 7.72, 0, 0, 0]t. this column becomes
the    rst column of c.

the second column of c is constructed by taking the column of m for

casablanca, which is [0, 0, 0, 0, 4, 5, 2]t, and dividing it by    rp4 =    2    0.185 =

0.608. thus, the second column of c is [0, 0, 0, 0, 6.58, 8.22, 3.29]t to two deci-
mal places.

now, let us choose the rows for r. the most likely rows to be chosen are
those for jenny and jack, so let   s suppose these rows are indeed chosen, jenny
   rst. the unscaled rows for r are thus

(cid:20) 0

5

0 0
5 5

5 5

0 0 (cid:21)

to scale the row for jenny, we note that its associated id203 is 0.206, so

we divide by    2    0.206 = 0.642. to scale the row for jack, whose associated
id203 is 0.309, we divide by    2    0.309 = 0.786. thus, the matrix r is

(cid:20)

0

0

0

7.79 7.79

6.36 6.36 6.36

0

0

(cid:21)

   

11.4.3 constructing the middle matrix

finally, we must construct the matrix u that connects c and r in the decom-
position. recall that u is an r    r matrix. we start the construction of u
with another matrix of the same size, which we call w . the entry in row i and
column j of w is the entry of m whose row is the one from which we selected
the ith row of r and whose column is the one from which we selected the jth
column of c.

432

chapter 11. id84

example 11.14 : let us follow the selections of rows and columns made in
example 11.13. we claim

w = (cid:20) 0

5

5

0 (cid:21)

the    rst row of w corresponds to the    rst row of r, which is the row for jenny
in the matrix m of fig. 11.12. the 0 in the    rst column is there because that
is the entry in the row of m for jenny and the column for alien; recall that the
   rst column of c was constructed from the column of m for alien. the 5 in the
second column re   ects the 5 in m    s row for jenny and column for casablanca;
the latter is the column of m from which the second column of c was derived.
similarly, the second row of w is the entries in the row for jack and columns
for alien and casablanca, respectively.    

the matrix u is constructed from w by the moore-penrose pseudoin-
verse described in section 11.4.1.
it consists of taking the svd of w , say
w = x  y t, and replacing all nonzero elements in the matrix    of singu-
lar values by their numerical inverses, to obtain the pseudoinverse   +. then
u = y (  +)2x t.

example 11.15 : let us construct u from the matrix w that we constructed
in example 11.14. first, here is the svd for w :

w = (cid:20) 0

5

5

0 (cid:21) = (cid:20) 0

1

1

0 (cid:21)(cid:20) 5

0

0

5 (cid:21)(cid:20) 1 0
0 1 (cid:21)

that is, the three matrices on the right are x,   , and y t, respectively. the
matrix    has no zeros along the diagonal, so each element is replaced by its
numerical inverse to get its moore-penrose pseudoinverse:

  + = (cid:20) 1/5

0

0

1/5 (cid:21)

now x and y are symmetric, so they are their own transposes. thus,

u = y (  +)2x t = (cid:20) 1

0

   

0

1 (cid:21)(cid:20) 1/5

0

0

1/5 (cid:21)2(cid:20) 0 1

1 0 (cid:21) = (cid:20)

0

1/25

1/25

0

(cid:21)

11.4.4 the complete cur decomposition

we now have a method to select randomly the three component matrices c,
u , and r. their product will approximate the original matrix m . as we
mentioned at the beginning of the discussion, the approximation is only for-
mally guaranteed to be close when very large numbers of rows and columns
are selected. however, the intuition is that by selecting rows and columns that
tend to have high    importance    (i.e., high frobenius norm), we are extracting

11.4. cur decomposition

433

cu r =

1.54
4.63
6.17
7.72
0
0
0

   

                           

0
0
0
0
9.30
11.63
4.65

=

   

                           
                           

   

(cid:20)

0

1/25

1/25

0

(cid:21)(cid:20)

0.55 0.55 0.55
1.67 1.67 1.67
2.22 2.22 2.22
2.78 2.78 2.78
0
0
0

0
0
0

0
0
0

0

0

0 11.01 11.01

0 (cid:21)

8.99 8.99 8.99

0

0
0
0
0

0
0
0
0
4.10 4.10
5.12 5.12
2.05 2.05

   

                           

figure 11.13: cur-decomposition of the matrix of fig. 11.12

the most signi   cant parts of the original matrix, even with a small number of
rows and columns. as an example, let us see how well we do with the running
example of this section.

example 11.16 : for our running example, the decomposition is shown in
fig. 11.13. while there is considerable di   erence between this result and the
original matrix m , especially in the science-   ction numbers, the values are in
proportion to their originals. this example is much too small, and the selection
of the small numbers of rows and columns was arbitrary rather than random, for
us to expect close convergence of the cur decomposition to the exact values.
   

11.4.5 eliminating duplicate rows and columns

it is quite possible that a single row or column is selected more than once.
there is no great harm in using the same row twice, although the rank of the
matrices of the decomposition will be less than the number of row and column
choices made. however, it is also possible to combine k rows of r that are each
the same row of the matrix m into a single row of r, thus leaving r with fewer
rows. likewise, k columns of c that each come from the same column of m
can be combined into one column of c. however, for either rows or columns,
the remaining vector should have each of its elements multiplied by    k.

when we merge some rows and/or columns, it is possible that r has fewer
rows than c has columns, or vice versa. as a consequence, w will not be a
square matrix. however, we can still take its pseudoinverse by decomposing it
into w = x  y t, where    is now a diagonal matrix with some all-0 rows or

434

chapter 11. id84

columns, whichever it has more of. to take the pseudoinverse of such a diagonal
matrix, we treat each element on the diagonal as usual (invert nonzero elements
and leave 0 as it is), but then we must transpose the result.

example 11.17 : suppose

   =    
   
  + =    
         

then

   

2 0
0 0
0 0

0 0
0 0
3 0

1/2 0
0
0
0
0
0
0

0
0
1/3
0

   
   
   
         

11.4.6 exercises for section 11.4

exercise 11.4.1 : the svd for the matrix

14

m = (cid:20) 48

14    48 (cid:21)
4/5    3/5 (cid:21)(cid:20) 50

4/5

0

is

14

(cid:20) 48
14    48 (cid:21) = (cid:20) 3/5

find the moore-penrose pseudoinverse of m .

0

25 (cid:21)(cid:20) 4/5    3/5
4/5 (cid:21)

3/5

! exercise 11.4.2 : find the cur-decomposition of the matrix of fig. 11.12

when we pick two    random    rows and columns as follows:

(a) the columns for the matrix and alien and the rows for jim and john.

(b) the columns for alien and star wars and the rows for jack and jill.

(c) the columns for the matrix and titanic and the rows for joe and jane.

! exercise 11.4.3 : find the cur-decomposition of the matrix of fig. 11.12 if
the two    random    rows are both jack and the two columns are star wars and
casablanca.

11.5 summary of chapter 11

    id84: the goal of id84 is to re-
place a large matrix by two or more other matrices whose sizes are much
smaller than the original, but from which the original can be approxi-
mately reconstructed, usually by taking their product.

11.5. summary of chapter 11

435

    eigenvalues and eigenvectors: a matrix may have several eigenvectors
such that when the matrix multiplies the eigenvector, the result is a con-
stant multiple of the eigenvector. that constant is the eigenvalue asso-
ciated with this eigenvector. together the eigenvector and its eigenvalue
are called an eigenpair.

    finding eigenpairs by power iteration: we can    nd the principal eigen-
vector (eigenvector with the largest eigenvalue) by starting with any vec-
tor and repeatedly multiplying the current vector by the matrix to get a
new vector. when the changes to the vector become small, we can treat
the result as a close approximation to the principal eigenvector. by mod-
ifying the matrix, we can then use the same iteration to get the second
eigenpair (that with the second-largest eigenvalue), and similarly get each
of the eigenpairs in turn, in order of decreasing value of the eigenvalue.

    principal-component analysis: this technique for dimensionality reduc-
tion views data consisting of a collection of points in a multidimensional
space as a matrix, with rows corresponding to the points and columns to
the dimensions. the product of this matrix and its transpose has eigen-
pairs, and the principal eigenvector can be viewed as the direction in the
space along which the points best line up. the second eigenvector repre-
sents the direction in which deviations from the principal eigenvector are
the greatest, and so on.

    id84 by pca: by representing the matrix of points
by a small number of its eigenvectors, we can approximate the data in a
way that minimizes the root-mean-square error for the given number of
columns in the representing matrix.

    id166: the id166 of a ma-
trix consists of three matrices, u ,   , and v . the matrices u and v are
column-orthonormal, meaning that as vectors, the columns are orthogo-
nal, and their lengths are 1. the matrix    is a diagonal matrix, and the
values along its diagonal are called singular values. the product of u ,   ,
and the transpose of v equals the original matrix.

    concepts: svd is useful when there are a small number of concepts that
connect the rows and columns of the original matrix. for example, if the
original matrix represents the ratings given by movie viewers (rows) to
movies (columns), the concepts might be the genres of the movies. the
matrix u connects rows to concepts,    represents the strengths of the
concepts, and v connects the concepts to columns.

    queries using the id166: we can use the decom-
position to relate new or hypothetical rows of the original matrix to the
concepts represented by the decomposition. multiply a row by the matrix
v of the decomposition to get a vector indicating the extent to which that
row matches each of the concepts.

436

chapter 11. id84

    using svd for id84: in a complete svd for a ma-
trix, u and v are typically as large as the original. to use fewer columns
for u and v , delete the columns corresponding to the smallest singular
values from u , v , and   . this choice minimizes the error in reconstruct-
ing the original matrix from the modi   ed u ,   , and v .

    decomposing sparse matrices: even in the common case where the given
matrix is sparse, the matrices constructed by svd are dense. the cur
decomposition seeks to decompose a sparse matrix into sparse, smaller
matrices whose product approximates the original matrix.

    cur decomposition: this method chooses from a given sparse matrix
a set of columns c and a set of rows r, which play the role of u and
v t in svd; the user can pick any number of rows and columns. the
choice of rows and columns is made randomly with a distribution that
depends on the frobenius norm, or the square root of the sum of the
squares of the elements. between c and r is a square matrix called u
that is constructed by a pseudo-inverse of the intersection of the chosen
rows and columns.

11.6 references for chapter 11

a well regarded text on matrix algebra is [4].

principal component analysis was    rst discussed over a century ago, in [6].
svd is from [3]. there have been many applications of this idea. two
worth mentioning are [1] dealing with document analysis and [8] dealing with
applications in biology.

the cur decomposition is from [2] and [5]. our description follows a later

work [7].

1. s. deerwester, s.t. dumais, g.w. furnas, t.k. landauer, and r. harsh-
man,    indexing by latent semantic analysis,    j. american society for
information science 41:6 (1990).

2. p. drineas, r. kannan, and m.w. mahoney,    fast monte-carlo algo-
rithms for matrices iii: computing a compressed approximate matrix
decomposition,    siam j. computing 36:1 (2006), pp. 184   206.

3. g.h. golub and w. kahan,    calculating the singular values and pseudo-

inverse of a matrix,    j. siam series b 2:2 (1965), pp. 205   224.

4. g.h.golub and c.f. van loan, matrix computations, jhu press, 1996.

5. m.w. mahoney, m. maggioni, and p. drineas, tensor-cur decomposi-

tions for tensor-based data, sigkdd, pp. 327   336, 2006.

6. k. pearson,    on lines and planes of closest    t to systems of points in

space,    philosophical magazine 2:11 (1901), pp. 559   572.

11.6. references for chapter 11

437

7. j. sun, y. xie, h. zhang, and c. faloutsos,    less is more: compact
matrix decomposition for large sparse graphs,    proc. siam intl. conf.
on data mining, 2007.

8. m.e. wall, a. reichtsteiner and l.m. rocha,    singular value decom-
position and principal component analysis,    in a practical approach to
microarray data analysis (d.p. berrar, w. dubitzky, and m. granzow
eds.), pp. 91   109, kluwer, norwell, ma, 2003.

438

chapter 11. id84

chapter 12

large-scale machine
learning

many algorithms are today classi   ed as    machine learning.    these algorithms
share, with the other algorithms studied in this book, the goal of extracting
information from data. all algorithms for analysis of data are designed to
produce a useful summary of the data, from which decisions are made. among
many examples, the frequent-itemset analysis that we did in chapter 6 produces
information like association rules, which can then be used for planning a sales
strategy or for many other purposes.

however, algorithms called    machine learning    not only summarize our
data; they are perceived as learning a model or classi   er from the data, and thus
discover something about data that will be seen in the future. for instance, the
id91 algorithms discussed in chapter 7 produce clusters that not only tell
us something about the data being analyzed (the training set), but they allow
us to classify future data into one of the clusters that result from the id91
algorithm. thus, machine-learning enthusiasts often speak of id91 with
the neologism    unsupervised learning   ; the term unsupervised refers to the fact
that the input data does not tell the id91 algorithm what the clusters
should be. in supervised machine learning,, which is the subject of this chapter,
the available data includes information about the correct way to classify at least
some of the data. the data classi   ed already is called the training set.

in this chapter, we do not attempt to cover all the di   erent approaches to
machine learning. we concentrate on methods that are suitable for very large
data and that have the potential for parallel implementation. we consider the
classical    id88    approach to learning a data classi   er, where a hyperplane
that separates two classes is sought. then, we look at more modern techniques
involving support-vector machines. similar to id88s, these methods look
for hyperplanes that best divide the classes, so that few, if any, members of the
training set lie close to the hyperplane. we end with a discussion of nearest-
neighbor techniques, where data is classi   ed according to the class(es) of their

439

440

chapter 12. large-scale machine learning

nearest neighbors in some space.

12.1 the machine-learning model

in this brief section we introduce the framework for machine-learning algorithms
and give the basic de   nitions.

12.1.1 training sets

the data to which a machine-learning (often abbreviated ml) algorithm is
applied is called a training set. a training set consists of a set of pairs (x, y),
called training examples, where

    x is a vector of values, often called a feature vector. each value, or feature,
can be categorical (values are taken from a set of discrete values, such as
{red, blue, green}) or numerical (values are integers or real numbers).

    y is the label, the classi   cation value for x.
the objective of the ml process is to discover a function y = f (x) that
best predicts the value of y associated with each value of x. the type of y is in
principle arbitrary, but there are several common and important cases.

1. y is a real number. in this case, the ml problem is called regression.

2. y is a boolean value true-or-false, more commonly written as +1 and    1,

respectively. in this class the problem is binary classi   cation.

3. y is a member of some    nite set. the members of this set can be thought
of as    classes,    and each member represents one class. the problem is
multiclass classi   cation.

4. y is a member of some potentially in   nite set, for example, a parse tree

for x, which is interpreted as a sentence.

12.1.2 some illustrative examples

example 12.1 : recall fig. 7.1, repeated as fig. 12.1, where we plotted the
height and weight of dogs in three classes: beagles, chihuahuas, and dachs-
hunds. we can think of this data as a training set, provided the data includes
the variety of the dog along with each height-weight pair. each pair (x, y) in
the training set consists of a feature vector x of the form [height, weight]. the
associated label y is the variety of the dog. an example of a training-set pair
would be ([5 inches, 2 pounds], chihuahua).

an appropriate way to implement the decision function f would be to imag-
ine two lines, shown dashed in fig. 12.1. the horizontal line represents a height

12.1. the machine-learning model

441

weight = 3

beagles

height

chihuahuas

height = 7

dachshunds

weight

figure 12.1: repeat of fig. 7.1, indicating the heights and weights of certain
dogs

of 7 inches and separates beagles from chihuahuas and dachshunds. the verti-
cal line represents a weight of 3 pounds and separates chihuahuas from beagles
and dachshunds. the algorithm that implements f is:

if (height > 7) print beagle
else if (weight < 3) print chihuahua
else print dachshund;

recall that the original intent of fig. 7.1 was to cluster points without
knowing which variety of dog they represented. that is, the label associated
with a given height-weight vector was not available. here, we are performing
supervised learning with the same data augmented by classi   cations for the
training data.    

example 12.2 : as an example of supervised learning, the four points (1, 2),
(2, 1), (3, 4), and (4, 3) from fig.11.1 (repeated here as fig. 12.2), can be thought
of as a training set, where the vectors are one-dimensional. that is, the point
(1, 2) can be thought of as a pair ([1], 2), where [1] is the one-dimensional feature
vector x, and 2 is the associated label y; the other points can be interpreted
similarly.

suppose we want to    learn    the linear function f (x) = ax + b that best
represents the points of the training set. a natural interpretation of    best   
is that the rmse of the value of f (x) compared with the given value of y is

442

chapter 12. large-scale machine learning

(3,4)

(1,2)

(4,3)

(2,1)

figure 12.2: repeat of fig. 11.1, to be used as a training set

minimized. that is, we want to minimize

4

xx=1

(ax + b     yx)2

where yx is the y-value associated with x. this sum is

(a + b     2)2 + (2a + b     1)2 + (3a + b     4)2 + (4a + b     3)2

simplifying, the sum is 30a2 + 4b2 + 20ab     56a     20b + 30. if we then take the
derivatives with respect to a and b and set them to 0, we get

60a + 20b     56 = 0
20a + 8b     20
= 0

the solution to these equations is a = 3/5 and b = 1. for these values the
rmse is 3.2.

note that the learned straight line is not the principal axis that was dis-
covered for these points in section 11.2.1. that axis was the line with slope
1, going through the origin, i.e., the line y = x. for this line, the rmse is
4. the di   erence is that pca discussed in section 11.2.1 minimizes the sum
of the squares of the lengths of the projections onto the chosen axis, which is
constrained to go through the origin. here, we are minimizing the sum of the
squares of the vertical distances between the points and the line. in fact, even
had we tried to learn the line through the origin with the least rmse, we would
not choose y = x. you can check that y = 14
15 x has a lower rmse than 4.    

example 12.3 : a common application of machine learning involves a training
set where the feature vectors x are boolean-valued and of very high dimension.
we shall focus on data consisting of documents, e.g., emails, web pages, or
newspaper articles. each component represents a word in some large dictio-
nary. we would probably eliminate stop words (very common words) from this
dictionary, because these words tend not to tell us much about the subject mat-
ter of the document. similarly, we might also restrict the dictionary to words

12.1. the machine-learning model

443

with high tf.idf scores (see section 1.3.1) so the words considered would tend
to re   ect the topic or substance of the document.

the training set consists of pairs, where the vector x represents the presence
or absence of each dictionary word in document. the label y could be +1 or
   1, with +1 representing that the document (an email, e.g.) is spam. our goal
would be to train a classi   er to examine future emails and decide whether or not
they are spam. we shall illustrate this use of machine learning in example 12.4.
alternatively, y could be chosen from some    nite set of topics, e.g.,    sports,   
   politics,    and so on. again, x could represent a document, perhaps a web
page. the goal would be to create a classi   er for web pages that assigned a
topic to each.    

12.1.3 approaches to machine learning

there are many forms of ml algorithms, and we shall not cover them all here.
here are the major classes of such algorithms, each of which is distinguished by
the form by which the function f is represented.

1. id90 were discussed brie   y in section 9.2.7. the form of f is
a tree, and each node of the tree has a function of x that determines
to which child or children the search must proceed. while we saw only
binary trees in section 9.2.7, in general a decision tree can have any
number of children for each node. id90 are suitable for binary
and multiclass classi   cation, especially when the dimension of the feature
vector is not too large (large numbers of features can lead to over   tting).

2. id88s are threshold functions applied to the components of the vec-
tor x = [x1, x2, . . . , xn]. a weight wi is associated with the ith component,
for each i = 1, 2, . . . , n, and there is a threshold   . the output is +1 if

n

xi=1

wixi       

and the output is    1 otherwise. a id88 is suitable for binary classi-
   cation, even when the number of features is very large, e.g., the presence
or absence of words in a document. id88s are the topic of sec-
tion 12.2.

3. neural nets are acyclic networks of id88s, with the outputs of some
id88s used as inputs to others. these are suitable for binary or
multiclass classi   cation, since there can be several id88s used as
output, with one or more indicating each class.

4. instance-based learning uses the entire training set to represent the func-
tion f . the calculation of the label y associated with a new feature vector
x can involve examination of the entire training set, although usually some

444

chapter 12. large-scale machine learning

preprocessing of the training set enables the computation of f (x) to pro-
ceed e   ciently. we shall consider an important kind of instance-based
learning, k-nearest-neighbor, in section 12.4. for example, 1-nearest-
neighbor classi   es data by giving it the same class as that of its nearest
training example. there are k-nearest-neighbor algorithms that are ap-
propriate for any kind of classi   cation, although we shall concentrate on
the case where y and the components of x are real numbers.

5. support-vector machines are an advance over the algorithms traditionally
used to select the weights and threshold. the result is a classi   er that
tends to be more accurate on unseen data. we discuss support-vector
machines in section 12.3.

12.1.4 machine-learning architecture

machine-learning algorithms can be classi   ed not only by their general algorith-
mic approach as we discussed in section 12.1.3. but also by their underlying
architecture     the way data is handled and the way it is used to build the model.

training and testing

one general issue regarding the handling of data is that there is a good reason to
withhold some of the available data from the training set. the remaining data
is called the test set. the problem addressed is that many machine-learning
algorithms tend to over   t the data; they pick up on artifacts that occur in the
training set but that are atypical of the larger population of possible data. by
using the test data, and seeing how well the classi   er works on that, we can tell
if the classi   er is over   tting the data. if so, we can restrict the machine-learning
algorithm in some way. for instance, if we are constructing a decision tree, we
can limit the number of levels of the tree.

training set

test
set

model

generation

model

error rate

figure 12.3: the training set helps build the model, and the test set validates
it

figure 12.3 illustrates the train-and-test architecture. we assume all the
data is suitable for training (i.e., the class information is attached to the data),

12.1. the machine-learning model

445

generalization

we should remember that the purpose of creating a model or classi   er is
not to classify the training set, but to classify the data whose class we do
not know. we want that data to be classi   ed correctly, but often we have
no way of knowing whether or not the model does so.
if the nature of
the data changes over time, for instance, if we are trying to detect spam
emails, then we need to measure the performance over time, as best we
can. for example, in the case of spam emails, we can note the rate of
reports of spam emails that were not classi   ed as spam.

but we separate out a small fraction of the available data as the test set. we
use the remaining data to build a suitable model or classi   er. then we feed
the test data to this model. since we know the class of each element of the
test data, we can tell how well the model does on the test data. if the error
rate on the test data is not much worse than the error rate of the model on the
training data itself, then we expect there is little, if any, over   tting, and the
model can be used. on the other hand, if the classi   er performs much worse
on the test data than on the training data, we expect there is over   tting and
need to rethink the way we construct the classi   er.

there is nothing special about the selection of the test data. in fact, we
can repeat the train-then-test process several times using the same data, if we
divide the data into k equal-sized chunks. in turn, we let each chunk be the test
data, and use the remaining k     1 chunks as the training data. this training
architecture is called cross-validation.

batch versus on-line learning

often, as in examples 12.1 and 12.2 we use a batch learning architecture. that
is, the entire training set is available at the beginning of the process, and it is
all used in whatever way the algorithm requires to produce a model once and
for all. the alternative is on-line learning, where the training set arrives in a
stream and, like any stream, cannot be revisited after it is processed. in on-line
learning, we maintain a model at all times. as new training examples arrive,
we may choose to modify the model to account for the new examples. on-line
learning has the advantages that it can

1. deal with very large training sets, because it does not access more than

one training example at a time.

2. adapt to changes in the population of training examples as time goes on.
for instance, google trains its spam-email classi   er this way, adapting
the classi   er for spam as new kinds of spam email are sent by spammers
and indicated to be spam by the recipients.

446

chapter 12. large-scale machine learning

an enhancement of on-line learning, suitable in some cases, is active learn-
ing. here, the classi   er may receive some training examples, but it primarily
receives unclassi   ed data, which it must classify. if the classi   er is unsure of
the classi   cation (e.g., the newly arrived example is very close to the bound-
ary), then the classi   er can ask for ground truth at some signi   cant cost. for
instance, it could send the example to mechanical turk and gather opinions of
real people. in this way, examples near the boundary become training examples
and can be used to modify the classi   er.

feature selection

sometimes, the hardest part of designing a good model or classi   er is    guring
out what features to use as input to the learning algorithm. let us reconsider
example 12.3, where we suggested that we could classify emails as spam or not
spam by looking at the words contained in the email. in fact, we explore in
detail such a classi   er in example 12.4. as discussed in example 12.3, it may
make sense to focus on certain words and not others; e.g., we should eliminate
stop words.

but we should also ask whether there is other information available that
would help us make a better decision about spam. for example, spam is often
generated by particular hosts, either those belonging to the spammers, or hosts
that have been coopted into a    botnet    for the purpose of generating spam.
thus, including the originating host or originating email address into the feature
vector describing an email might enable us to design a better classi   er and lower
the error rate.

creating a training set

it is reasonable to ask where the label information that turns data into a train-
ing set comes from. the obvious method is to create the labels by hand, having
an expert look at each feature vector and classify it properly. recently, crowd-
sourcing techniques have been used to label data. for example, in many appli-
cations it is possible to use mechanical turk to label data. since the    turkers   
are not necessarily reliable, it is wise to use a system that allows the question
to be asked of several di   erent people, until a clear majority is in favor of one
label.

one often can    nd data on the web that is implicitly labeled. for example,
the open directory (dmoz) has millions of pages labeled by topic. that data,
used as a training set, can enable one to classify other pages or documents
according to their topic, based on the frequency of word occurrence. another
approach to classifying by topic is to look at the wikipedia page for a topic and
see what pages it links to. those pages can safely be assumed to be relevant to
the given topic.

in some applications we can use the stars that people use to rate products or
services on sites like amazon or yelp. for example, we might want to estimate
the number of stars that would be assigned to reviews or tweets about a product,

12.2. id88s

447

even if those reviews do not have star ratings. if we use star-labeled reviews
as a training set, we can deduce the words that are most commonly associated
with positive and negative reviews (called id31). the presence of
these words in other reviews can tell us the sentiment of those reviews.

12.1.5 exercises for section 12.1

exercise 12.1.1 : redo example 12.2 for the following di   erent forms of f (x).

(a) require f (x) = ax; i.e., a straight line through the origin.

is the line

y = 14

15 x that we discussed in the example optimal?

(b) require f (x) to be a quadratic, i.e., f (x) = ax2 + bx + c.

12.2 id88s

a id88 is a linear binary classi   er. its input is a vector x = [x1, x2, . . . , xd]
with real-valued components. associated with the id88 is a vector of
weights w = [w1, w2, . . . , wd], also with real-valued components. each percep-
tron has a threshold   . the output of the id88 is +1 if w.x >   , and
the output is    1 if w.x <   . the special case where w.x =    will always be
regarded as    wrong,    in the sense that we shall describe in detail when we get
to section 12.2.1.

the weight vector w de   nes a hyperplane of dimension d    1     the set of all
points x such that w.x =   , as suggested in fig. 12.4. points on the positive
side of the hyperplane are classi   ed +1 and those on the negative side are clas-
si   ed    1. a id88 classi   er works only for data that is linearly separable,
in the sense that there is some hyperplane that separates all the positive points
from all the negative points. if there are many such hyperplanes, the percep-
tron will converge to one of them, and thus will correctly classify all the training
points. if no such hyperplane exists, then the id88 cannot converge to
any particular one. in the next section, we discuss support-vector machines,
which do not have this limitation; they will converge to some separator that,
although not a perfect classi   er, will do as well as possible under the metric to
be described in section 12.3.

12.2.1 training a id88 with zero threshold

to train a id88, we examine the training set and try to    nd a weight
vector w and threshold    such that all the feature vectors with y = +1 (the
positive examples) are on the positive side of the hyperplane and all those with
y =    1 (the negative examples) are on the negative side. it may or may not be
possible to do so, since there is no guarantee that any hyperplane separates all
the positive and negative examples in the training set.

448

chapter 12. large-scale machine learning

w

w.x  =   

figure 12.4: a id88 divides a space by a hyperplane into two half-spaces

we begin by assuming the threshold is 0; the simple augmentation needed
to handle an unknown threshold is discussed in section 12.2.4. the follow-
ing method will converge to some hyperplane that separates the positive and
negative examples, provided one exists.

1. initialize the weight vector w to all 0   s.

2. pick a learning-rate parameter   , which is a small, positive real number.
the choice of    a   ects the convergence of the id88. if    is too small,
then convergence is slow; if it is too big, then the decision boundary will
   dance around    and again will converge slowly, if at all.

3. consider each training example t = (x, y) in turn.

(a) let y    = w.x.
(b) if y    and y have the same sign, then do nothing; t is properly classi-

   ed.

(c) however, if y    and y have di   erent signs, or y    = 0, replace w by

w +   yx. that is, adjust w slightly in the direction of x.

the two-dimensional case of this transformation on w is suggested in fig.
12.5. notice how moving w in the direction of x moves the hyperplane that is
perpendicular to w in such a direction that it makes it more likely that x will
be on the correct side of the hyperplane, although it does not guarantee that
to be the case.

example 12.4 : let us consider training a id88 to recognize spam email.
the training set consists of pairs (x, y) where x is a vector of 0   s and 1   s, with
each component xi corresponding to the presence (xi = 1) or absence (xi = 0)

12.2. id88s

449

  

x1

w

w   

x1

w   .x = 0

w.x = 0

figure 12.5: a misclassi   ed point x1 moves the vector w

of a particular word in the email. the value of y is +1 if the email is known
to be spam and    1 if it is known not to be spam. while the number of words
found in the training set of emails is very large, we shall use a simpli   ed example
where there are only    ve words:    and,       viagra,       the,       of,    and    nigeria.   
figure 12.6 gives the training set of six vectors and their corresponding classes.

and

viagra

a
b
c
d
e
f

1
0
0
1
1
1

1
0
1
0
0
0

the
0
1
1
0
1
1

of
1
1
0
1
0
1

nigeria

1
0
0
0
1
0

y
+1
   1
+1
   1
+1
   1

figure 12.6: training data for spam emails

in this example, we shall use learning rate    = 1/2, and we shall visit
each training example once, in the order shown in fig. 12.6. we begin with
w = [0, 0, 0, 0, 0] and compute w.a = 0. since 0 is not positive, we move w in
the direction of a by performing w := w + (1/2)(+1)a. the new value of w
is thus

w = [0, 0, 0, 0, 0] + [

, 0,

,

1
2
2 , 0, 1

1
2
2 , 1

2 , 1

1
2

,

1
2

] = [

, 0,

1
2

,

1
2

1
2

,

1
2

]

next, consider b. w.b = [ 1

2 ].[0, 0, 1, 1, 0] = 1

2 . since the associated

y for b is    1, b is misclassi   ed. we thus assign

w := w + (1/2)(   1)b = [

1
2

,

1
2

, 0,

1
2

,

1
2

]     [0, 0,

1
2

,

1
2

, 0] = [

1
2

,

1
2

1
2

,   

, 0,

1
2

]

450

chapter 12. large-scale machine learning

pragmatics of training on emails

when we represent emails or other large documents as training examples,
we would not really want to construct the vector of 0   s and 1   s with a
component for every word that appears even once in the collection of
emails. doing so would typically give us sparse vectors with millions of
components. rather, create a table in which all the words appearing in the
emails are assigned integers 1, 2, . . ., indicating their component. when we
process an email in the training set, make a list of the components in which
the vector has 1; i.e., use the standard sparse representation for the vector.
if we eliminate stop words from the representation, or even eliminate words
with a low tf.idf score, then we make the vectors representing emails
signi   cantly sparser and thus compress the data even more. only the
vector w needs to have all its components listed, since it will not be sparse
after a small number of training examples have been processed.

training example c is next. we compute

w.c = [

1
2

,

1
2

,   

1
2

, 0,

1
2

].[0, 1, 1, 0, 0] = 0

since the associated y for c is +1, c is also misclassi   ed. we thus assign

w := w + (1/2)(+1)c = [

1
2

,

1
2

1
2

,   

, 0,

1
2

] + [0,

1
2

,

1
2

, 0, 0] = [

1
2

, 1, 0, 0,

1
2

]

training example d is next to be considered:

w.d = [

1
2

, 1, 0, 0,

1
2

].[1, 0, 0, 1, 0] = 1

since the associated y for d is    1, d is misclassi   ed as well. we thus assign
]

w := w + (1/2)(   1)d = [
for training example e we compute w.e = [0, 1, 0,    1

2 ].[1, 0, 1, 0, 1] = 1
2 .
since the associated y for e is +1, e is classi   ed correctly, and no change to w
is made. similarly, for f we compute

, 0] = [0, 1, 0,   

, 1, 0, 0,

]     [

, 0, 0,

2 , 1

1
2

1
2

1
2

1
2

1
2

1
2

,

w.f = [0, 1, 0,   

1
2

,

1
2

].[1, 0, 1, 1, 0] =    

1
2

so f is correctly classi   ed. if we check a through d, we    nd that this w cor-
rectly classi   es them as well. thus, we have converged to a id88 that
classi   es all the training set examples correctly. it also makes a certain amount
of sense: it says that    viagra    and    nigeria    are indicative of spam, while    of   
is indicative of nonspam. it considers    and    and    the    neutral,    although we
would probably prefer to give    and,       of,    and    the    the same weight.    

12.2. id88s

451

12.2.2 convergence of id88s

as we mentioned at the beginning of this section, if the data points are linearly
separable, then the id88 algorithm will converge to a separator. however,
if the data is not linearly separable, then the algorithm will eventually repeat a
weight vector and loop in   nitely. unfortunately, it is often hard to tell, during
the running of the algorithm, which of these two cases applies. when the data
is large, it is not feasible to remember all previous weight vectors to see whether
we are repeating a vector, and even if we could, the period of repetition would
most likely be so large that we would want to terminate the algorithm long
before we repeated.

a second issue regarding termination is that even if the training data is
linearly separable, the entire dataset might not be linearly separable. the
consequence is that there might not be any value in running the algorithm
for a very large number of rounds, in the hope of converging to a separator.
we therefore need a strategy for deciding when to terminate the id88
algorithm, assuming convergence has not occurred. here are some common
tests for termination.

1. terminate after a    xed number of rounds.

2. terminate when the number of misclassi   ed training points stops chang-

ing.

3. withhold a test set from the training data, and after each round, run the
id88 on the test data. terminate the algorithm when the number
of errors on the test set stops changing.

another technique that will aid convergence is to lower the training rate as
the number of rounds increases. for example, we could allow the training rate
   to start at some initial   0 and lower it to   0/(1 + ct) after the tth round,
where c is some small constant.

12.2.3 the winnow algorithm

there are many other rules one could use to adjust weights for a id88. not
all possible algorithms are guaranteed to converge, even if there is a hyperplane
separating positive and negative examples. one that does converge is called
winnow, and that rule will be described here. winnow assumes that the feature
vectors consist of 0   s and 1   s, and the labels are +1 or    1. unlike the basic
id88 algorithm, which can produce positive or negative components in
the weight vector w, winnow produces only positive weights.

the general winnow algorithm allows for a variety of parameters to be
selected, and we shall only consider one simple variant. however, all variants
have in common the idea that there is a positive threshold   .
if w is the
current weight vector, and x is the feature vector in the training set that we
are currently considering, we compute w.x and compare it with   . if the dot

452

chapter 12. large-scale machine learning

product is too low, and the class for x is +1, then we have to raise the weights of
w in those components where x has 1. we multiply these weights by a number
greater than 1. the larger this number, the greater the training rate, so we
want to pick a number that is not too close to 1 (or convergence will be too
slow) but also not too large (or the weight vector may oscillate). similarly, if
w.x       , but the class of x is    1, then we want to lower the weights of w in
those components where x is 1. we multiply those weights by a number greater
than 0 but less than 1. again, we want to pick a number that is not too close
to 1 but also not too small, to avoid slow convergence or oscillation.

we shall give the details of the algorithm using the factors 2 and 1/2, for
the cases where we want to raise weights and lower weights, respectively. start
the winnow algorithm with a weight vector w = [w1, w2, . . . , wd] all of whose
components are 1, and let the threshold    equal d, the number of dimensions of
the vectors in the training examples. let (x, y) be the next training example
to be considered, where x = [x1, x2, . . . , xd].

1. if w.x >    and y = +1, or w.x <    and y =    1, then the example is

correctly classi   ed, so no change to w is made.

2. if w.x       , but y = +1, then the weights for the components where x has
1 are too low as a group. double each of the corresponding components
of w. that is, if xi = 1 then set wi

:= 2wi.

3. if w.x       , but y =    1, then the weights for the components where x has
1 are too high as a group. halve each of the corresponding components
of w. that is, if xi = 1 then set wi

:= wi/2.

example 12.5 : let us reconsider the training data from fig. 12.6. initialize
w = [1, 1, 1, 1, 1] and let    = 5. first, consider feature vector a = [1, 1, 0, 1, 1].
w.a = 4, which is less than   . since the associated label for a is +1, this
example is misclassi   ed. when a +1-labeled example is misclassi   ed, we must
double all the components where the example has 1; in this case, all but the
third component of a is 1. thus, the new value of w is [2, 2, 1, 2, 2].

next, we consider training example b = [0, 0, 1, 1, 0]. w.b = 3, which is less
than   . however, the associated label for b is    1, so no change to w is needed.
for c = [0, 1, 1, 0, 0] we    nd w.c = 3 <   , while the associated label is +1.
thus, we double the components of w where the corresponding components of
c are 1. these components are the second and third, so the new value of w is
[2, 4, 2, 2, 2].

the next two training examples, d and e require no change, since they are
correctly classi   ed. however, there is a problem with f = [1, 0, 1, 1, 0], since
w.f = 6 >   , while the associated label for f is    1. thus, we must divide the
   rst, third, and fourth components of w by 2, since these are the components
where f has 1. the new value of w is [1, 4, 1, 1, 2].

we still have not converged.

it turns out we must consider each of the
training examples a through f again. at the end of this process, the algorithm

12.2. id88s

453

x

y w.x ok?

and

viagra

a +1
b    1
c +1
d    1
e +1
f    1
a +1
b    1
c +1
d    1
e +1
f    1

4
3
3
4
6
6
8
2
5
2
5
7

no
yes
no
yes
yes
no
yes
yes
no
yes
no
no

1
2

2

1

1

2
1

1
2

4

4

8

8
8

the
1
1

of
1
2

2

1

2

4
2

2

1

1

1
1
2

nigeria

1
2

2

2

2

4
4

figure 12.7: sequence of updates to w performed by the winnow algorithm
on the training set of fig. 12.6

has converged to a weight vector w = [1, 8, 2, 1
2 , 4], which with threshold    = 5
correctly classi   es all of the training examples in fig. 12.6. the details of
the twelve steps to convergence are shown in fig. 12.7. this    gure gives the
associated label y and the computed dot product of w and the given feature
vector. the last    ve columns are the    ve components of w after processing
each training example.    

12.2.4 allowing the threshold to vary

suppose now that the choice of threshold 0, as in section 12.2.1, or threshold d,
as in section 12.2.3 is not desirable, or that we don   t know the best threshold
to use. at the cost of adding another dimension to the feature vectors, we can
treat    as one of the components of the weight vector w. that is:

1. replace the vector of weights w = [w1, w2, . . . , wd] by

w    = [w1, w2, . . . , wd,   ]

2. replace every feature vector x = [x1, x2, . . . , xd] by

x    = [x1, x2, . . . , xd,   1]

then, for the new training set and weight vector, we can treat the threshold
as 0 and use the algorithm of section 12.2.1. the justi   cation is that w   .x        0
is equivalent to pd
i=1 wixi +       1 = w.x          0, which in turn is equivalent to
w.x       . the latter is the condition for a positive response from a id88
with threshold   .

454

chapter 12. large-scale machine learning

we can also apply the winnow algorithm to the modi   ed data. winnow
requires all feature vectors to have 0   s and 1   s, as components. however, we can
allow a    1 in the feature vector component for    if we treat it in the manner
opposite to the way we treat components that are 1. that is, if the training
example is positive, and we need to increase the other weights, we instead divide
the component for the threshold by 2. and if the training example is negative,
and we need to decrease the other weights we multiply the threshold component
by 2.

example 12.6 : let us modify the training set of fig. 12.6 to incorporate a
sixth    word    that represents the negative       of the threshold. the new data
is shown in fig. 12.8.

and

viagra

a
b
c
d
e
f

1
0
0
1
1
1

1
0
1
0
0
0

the
0
1
1
0
1
1

of
1
1
0
1
0
1

nigeria

1
0
0
0
1
0

  
y
   1 +1
   1    1
   1 +1
   1    1
   1 +1
   1    1

figure 12.8: training data for spam emails, with a sixth component representing
the negative of the threshold

we begin with a weight vector w with six 1   s, as shown in the    rst line
of fig. 12.9. when we compute w.a = 3, using the    rst feature vector a, we
are happy because the training example is positive, and so is the dot product.
however, for the second training example, we compute w.b = 1. since the
example is negative and the dot product is positive, we must adjust the weights.
since b has 1   s in the third and fourth components, the 1   s in the corresponding
components of w are replaced by 1/2. the last component, corresponding to   ,
must be doubled. these adjustments give the new weight vector [1, 1, 1
2 , 1, 2]
shown in the third line of fig. 12.9.

2 , 1

x

y w.x ok?

and

viagra

a +1
3
b    1
1
c +1     1
d    1

2
1
2

yes
no
no
no

1

1
1
1
2

1

1
2
2

the
1

1
2
1
1

of
1

1
2
1
2
1
4

nigeria

1

1
1
1

  
1

2
1
2

figure 12.9: sequence of updates to w performed by the winnow algorithm
on the training set of fig. 12.8

the feature vector c is a positive example, but w.c =     1

2 . thus, we must

12.2. id88s

455

double the second and third components of w, because c has 1 in the cor-
responding components, and we must halve the last component of w, which
corresponds to   . the resulting w = [1, 2, 1, 1
2 , 1, 1] is shown in the fourth line
of fig. 12.9. next, d is a negative example. since w.d = 1
2 , we must again
adjust weights. we halve the weights in the    rst and fourth components and
double the last component, yielding w = [ 1
4 , 1, 2]. now, all positive ex-
amples have a positive dot product with the weight vector, and all negative
examples have a negative dot product, so there are no further changes to the
weights.

2 , 2, 1, 1

the designed id88 has a threshold of 2. it has weights 2 and 1 for
   viagra    and    nigeria    and smaller weights for    and    and    of.    it also has
weight 1 for    the,    which suggests that    the    is as indicative of spam as    nige-
ria,    something we doubt is true. nevertheless, this id88 does classify all
examples correctly.    

12.2.5 multiclass id88s

there are several ways in which the basic idea of the id88 can be ex-
tended. we shall discuss transformations that enable hyperplanes to serve for
more complex boundaries in the next section. here, we look at how id88s
can be used to classify data into many classes.

suppose we are given a training set with labels in k di   erent classes. start
by training a id88 for each class; these id88s should each have the
same threshold   . that is, for class i treat a training example (x, i) as a positive
example, and all examples (x, j), where j 6= i, as a negative example. suppose
that the weight vector of the id88 for class i is determined to be wi after
training.

given a new vector x to classify, we compute wi.x for all i = 1, 2, . . . , k. we
take the class of x to be the value of i for which wi.x is the maximum, provided
that value is at least   . otherwise, x is assumed not to belong to any of the k
classes.

for example, suppose we want to classify web pages into a number of topics,
such as sports, politics, medicine, and so on. we can represent web pages by a
vector with 1 for each word present in the page and 0 for words not present (of
course we would only visualize the pages that way; we wouldn   t construct the
vectors in reality). each topic has certain words that tend to indicate that topic.
for instance, sports pages would be full of words like    win,       goal,       played,   
and so on. the weight vector for that topic would give higher weights to the
words that characterize that topic.

a new page could be classi   ed as belonging to the topic that gives the
highest score when the dot product of the page   s vector and the weight vectors
for the topics are computed. an alternative interpretation of the situation is
to classify a page as belonging to all those topics for which the dot product
is above some threshold (presumably a threshold higher than the    used for
training).

456

chapter 12. large-scale machine learning

12.2.6 transforming the training set

while a id88 must use a linear function to separate two classes, it is
always possible to transform the vectors of a training set before applying a
id88-based algorithm to separate the classes. an example should give
the basic idea.

figure 12.10: transforming from rectangular to polar coordinates turns this
training set into one with a separating hyperplane

example 12.7 : in fig. 12.10 we see a plot of places to visit from my home.
the horizontal and vertical coordinates represent latitude and longitude of
places. some of the places have been classi   ed into    day trips        places close
enough to visit in one day     and    excursions,    which require more than a day
to visit. these are the circles and squares, respectively. evidently, there is no
straight line that separates day trips from excursions. however, if we replace
the cartesian coordinates by polar coordinates, then in the transformed space of
polar coordinates, the dashed circle shown in fig. 12.10 becomes a hyperplane.
2, arctan(x2/x1)].
in fact, we can also do id84 of the data. the angle of
2 matters. thus, we can
turn the point vectors into one-component vectors giving the distance of the
point from the origin. associated with the small distances will be the class
label    day trip,    while the larger distances will all be associated with the label
   excursion.    training the id88 is extremely easy.    

formally, we transform the vector x = [x1, x2] into [px2
the point is irrelevant, and only the radius px2
1 + x2

1 + x2

12.2. id88s

457

12.2.7 problems with id88s

despite the extensions discussed above, there are some limitations to the ability
of id88s to classify some data. the biggest problem is that sometimes
the data is inherently not separable by a hyperplane. an example is shown in
fig. 12.11. in this example, points of the two classes mix near the boundary so
that any line through the points will have points of both classes on at least one
of the sides.

figure 12.11: a training set may not allow the existence of any separating
hyperplane

one might argue that, based on the observations of section 12.2.6 it should
be possible to    nd some function on the points that would transform them to
another space where they were linearly separable. that might be the case,
but if so, it would probably be an example of over   tting, the situation where
the classi   er works very well on the training set, because it has been carefully
designed to handle each training example correctly. however, because the clas-
si   er is exploiting details of the training set that do not apply to other examples
that must be classi   ed in the future, the classi   er will not perform well on new
data.

another problem is illustrated in fig. 12.12. usually, if classes can be sep-
arated by one hyperplane, then there are many di   erent hyperplanes that will
separate the points. however, not all hyperplanes are equally good. for in-
stance, if we choose the hyperplane that is furthest clockwise, then the point
indicated by    ?    will be classi   ed as a circle, even though we intuitively see it as
closer to the squares. when we meet support-vector machines in section 12.3,
we shall see that there is a way to insist that the hyperplane chosen be the one
that in a sense divides the space most fairly.

yet another problem is illustrated by fig. 12.13. most rules for training

458

chapter 12. large-scale machine learning

?

figure 12.12: generally, more that one hyperplane can separate the classes if
they can be separated at all

a id88 stop as soon as there are no misclassi   ed points. as a result,
the chosen hyperplane will be one that just manages to classify some of the
points correctly. for instance, the upper line in fig. 12.13 has just managed
to accommodate two of the squares, and the lower line has just managed to
accommodate one of the circles.
if either of these lines represent the    nal
weight vector, then the weights are biased toward one of the classes. that
is, they correctly classify the points in the training set, but the upper line
would classify new squares that are just below it as circles, while the lower line
would classify circles just above it as squares. again, a more equitable choice
of separating hyperplane will be shown in section 12.3.

12.2.8 parallel implementation of id88s

the training of a id88 is an inherently sequential process. if the num-
ber of dimensions of the vectors involved is huge, then we might obtain some
parallelism by computing dot products in parallel. however, as we discussed in
connection with example 12.4, high-dimensional vectors are likely to be sparse
and can be represented more succinctly than would be expected from their
length.

in order to get signi   cant parallelism, we have to modify the id88
algorithm slightly, so that many training examples are used with the same esti-
mated weight vector w. as an example, let us formulate the parallel algorithm
as a mapreduce job.
the map function: each map task is given a chunk of training examples,
and each map task knows the current weight vector w. the map task computes
w.x for each feature vector x = [x1, x2, . . . , xk] in its chunk and compares that

12.2. id88s

459

figure 12.13: id88s converge as soon as the separating hyperplane reaches
the region between classes

dot product with the label y, which is +1 or    1, associated with x. if the signs
agree, no key-value pairs are produced for this training example. however, if
the signs disagree, then for each nonzero component xi of x the key-value pair
(i,   yxi) is produced; here,    is the learning-rate constant used to train this
id88. notice that   yxi is the increment we would like to add to the
current ith component of w, and if xi = 0, then there is no need to produce a
key-value pair. however, in the interests of parallelism, we defer that change
until we can accumulate many changes in the reduce phase.
the reduce function: for each key i, the reduce task that handles key i
adds all the associated increments and then adds that sum to the ith component
of w.

probably, these changes will not be enough to train the id88. if any
changes to w occur, then we need to start a new mapreduce job that does the
same thing, perhaps with di   erent chunks from the training set. however, even
if the entire training set was used on the    rst round, it can be used again, since
its e   ect on w will be di   erent if w has changed.

12.2.9 exercises for section 12.2

exercise 12.2.1 : modify the training set of fig. 12.6 so that example b also
includes the word    nigeria    (yet remains a negative example     perhaps someone
telling about their trip to nigeria). find a weight vector that separates the
positive and negative examples, using:

(a) the basic training method of section 12.2.1.

(b) the winnow method of section 12.2.3.

460

chapter 12. large-scale machine learning

id88s on streaming data

while we have viewed the training set as stored data, available for repeated
use on any number of passes, id88s can also be used in a stream
setting. that is, we may suppose there is an in   nite sequence of training
examples, but that each may be used only once. detecting email spam is
a good example of a training stream. users report spam emails and also
report emails that were classi   ed as spam but are not. each email, as it
arrives, is treated as a training example, and modi   es the current weight
vector, presumably by a very small amount.

if the training set is a stream, we never really converge, and in fact
the data points may well not be linearly separable. however, at all times,
we have an approximation to the best possible separator. moreover, if the
examples in the stream evolve over time, as would be the case for email
spam, then we have an approximation that values recent examples more
than examples from the distant past, much like the exponentially decaying
windows technique from section 4.7.

(c) the basic method with a variable threshold, as suggested in section 12.2.4.

(d) the winnow method with a variable threshold, as suggested in section

12.2.4.

! exercise 12.2.2 : for the following training set:

([1, 2], +1)
([2, 3],   1)

([2, 1], +1)
([3, 2],   1)

describe all the vectors w and thresholds    such that the hyperplane (really a
line) de   ned by w.x        = 0 separates the points correctly.
! exercise 12.2.3 : suppose the following four examples constitute a training

set:

([1, 2],   1)
([2, 1], +1)

([2, 3], +1)
([3, 2],   1)

(a) what happens when you attempt to train a id88 to classify these

points using 0 as the threshold?

!! (b) is it possible to change the threshold and obtain a id88 that cor-

rectly classi   es these points?

(c) suggest a transformation using quadratic polynomials that will transform

these points so they become linearly separable.

12.3. support-vector machines

461

12.3 support-vector machines

we can view a support-vector machine, or id166, as an improvement on the
id88 that is designed to address the problems mentioned in section 12.2.7.
an id166 selects one particular hyperplane that not only separates the points in
the two classes, but does so in a way that maximizes the margin     the distance
between the hyperplane and the closest points of the training set.

12.3.1 the mechanics of an id166

the goal of an id166 is to select a hyperplane w.x + b = 01 that maximizes
the distance    between the hyperplane and any point of the training set. the
idea is suggested by fig. 12.14. there, we see the points of two classes and a
hyperplane dividing them.

support
vectors

  

  

w.x + b = 0

figure 12.14: an id166 selects the hyperplane with the greatest possible margin
   between the hyperplane and the training points

intuitively, we are more certain of the class of points that are far from the
separating hyperplane than we are of points near to that hyperplane. thus, it
is desirable that all the training points be as far from the hyperplane as possible
(but on the correct side of that hyperplane, of course). an added advantage
of choosing the separating hyperplane to have as large a margin as possible is
that there may be points closer to the hyperplane in the full data set but not
in the training set.
if so, we have a better chance that these points will be
classi   ed properly than if we chose a hyperplane that separated the training
points but allowed some points to be very close to the hyperplane itself. in that
case, there is a fair chance that a new point that was near a training point that

1constant b in this formulation of a hyperplane is the same as the negative of the threshold

   in our treatment of id88s in section 12.2.

462

chapter 12. large-scale machine learning

was also near the hyperplane would be misclassi   ed. this issue was discussed
in section 12.2.7 in connection with fig. 12.13.

we also see in fig. 12.14 two parallel hyperplanes at distance    from the
central hyperplane w.x + b = 0, and these each touch one or more of the
support vectors. the latter are the points that actually constrain the dividing
hyperplane, in the sense that they are all at distance    from the hyperplane.
in most cases, a d-dimensional set of points has d + 1 support vectors, as is
the case in fig. 12.14. however, there can be more support vectors if too many
points happen to lie on the parallel hyperplanes. we shall see an example based
on the points of fig. 11.1, where it turns out that all four points are support
vectors, even though two-dimensional data normally has three.

a tentative statement of our goal is:

    given a training set (x1, y1), (x2, y2), . . . , (xn, yn), maximize    (by varying

w and b) subject to the constraint that for all i = 1, 2, . . . , n,

yi(w.xi + b)       

notice that yi, which must be +1 or    1, determines which side of the hyperplane
the point xi must be on, so the     relationship to    is always correct. however, it
may be easier to express this condition as two cases: if y = +1, then w.x+b       ,
and if y =    1, then w.x + b          .
unfortunately, this formulation doesn   t really work properly. the problem
is that by increasing w and b, we can always allow a larger value of   . for
example, suppose that w and b satisfy the constraint above. if we replace w

by 2w and b by 2b, we observe that for all i, yi(cid:0)(2w).xi + 2b(cid:1)     2  . thus, 2w

and 2b is always a better choice that w and b, so there is no best choice and no
maximum   .

12.3.2 normalizing the hyperplane

the solution to the problem that we described intuitively above is to normalize
the weight vector w. that is, the unit of measure perpendicular to the sepa-
rating hyperplane is the unit vector w/kwk. recall that kwk is the frobenius
norm, or the square root of the sum of the squares of the components of w. we
shall require that w be such that the parallel hyperplanes that just touch the
support vectors are described by the equations w.x + b = +1 and w.x + b =    1,
as suggested by fig. 12.15.
our goal becomes to maximize   , which is now the multiple of the unit
vector w/kwk between the separating hyperplane and the parallel hyperplanes
through the support vectors. consider one of the support vectors, say x2 shown
in fig. 12.15. let x1 be the projection of x2 onto the far hyperplane, also as
suggested by fig. 12.15. note that x1 need not be a support vector or even a
point of the training set. the distance from x2 to x1 in units of w/kwk is 2  .
that is,

12.3. support-vector machines

463

x

1

2x

w

/ || w

||

  

  

w.x + b = +1

w.x + b = 0
+ b = 0
+ b =    1

w.x

figure 12.15: normalizing the weight vector for an id166

x1 = x2 + 2  

w
kwk

(12.1)

since x1 is on the hyperplane de   ned by w.x + b = +1, we know that

w.x1 + b = 1. if we substitute for x1 using equation 12.1, we get

w.(cid:16)x2 + 2  

w

kwk(cid:17) + b = 1

regrouping terms, we see

w.x2 + b + 2  

w.w
kwk

= 1

(12.2)

but the    rst two terms of equation 12.2, w.x2 + b, sum to    1, since we know
that x2 is on the hyperplane w.x + b =    1. if we move this    1 from left to
right in equation 12.2 and then divide through by 2, we conclude that

  

w.w
kwk

= 1

(12.3)

notice also that w.w is the sum of the squares of the components of w.

that is, w.w = kwk2. we conclude from equation 12.3 that    = 1/kwk.
this equivalence gives us a way to reformulate the optimization problem
originally stated in section 12.3.1. instead of maximizing   , we want to mini-
mize kwk, which is the inverse of    if we insist on normalizing the scale of w.
that is:

    given a training set (x1, y1), (x2, y2), . . . , (xn, yn), minimize kwk (by vary-

ing w and b) subject to the constraint that for all i = 1, 2, . . . , n,

yi(w.xi + b)     1

464

chapter 12. large-scale machine learning

example 12.8 : let us consider the four points of fig. 11.1, supposing that
they alternate as positive and negative examples. that is, the training set
consists of

([1, 2], +1)
([3, 4], +1)

([2, 1],   1)
([4, 3],   1)

let w = [u, v]. our goal is to minimize    u2 + v2 subject to the constraints
we derive from the four training examples. for the    rst, where x1 = [1, 2] and
y1 = +1, the constraint is (+1)(u + 2v + b) = u + 2v + b     1. for the second,
where x2 = [2, 1] and y2 =    1, the constraint is (   1)(2u + v + b)     1, or
2u + v + b        1. the last two points are analogously handled, and the four
constraints we derive are:

u + 2v + b     1
3u + 4v + b     1

2u + v + b        1
4u + 3v + b        1

we shall cover in detail the subject of how one optimizes with constraints;
the subject is broad and many packages are available for you to use. sec-
tion 12.3.4 discusses one method     id119     in connection with a
more general application of id166, where there is no separating hyperplane. an
illustration of how this method works will appear in example 12.9.

in this simple example, the solution is easy to see: b = 0 and w = [u, v] =
[   1, +1]. it happens that all four constraints are satis   ed exactly; i.e., each
of the four points is a support vector. that case is unusual, since when the
data is two-dimensional, we expect only three support vectors. however, the
fact that the positive and negative examples lie on parallel lines allows all four
constraints to be satis   ed exactly.    

12.3.3 finding optimal approximate separators

we shall now consider    nding an optimal hyperplane in the more general case,
where no matter which hyperplane we chose, there will be some points on the
wrong side, and perhaps some points that are on the correct side, but too close
to the separating hyperplane itself, so the margin requirement is not met. a
typical situation is shown in fig. 12.16. we see two points that are misclassi   ed;
they are on the wrong side of the separating hyperplane w.x + b = 0. we also
see two points that, while they are classi   ed correctly, are too close to the
separating hyperplane. we shall call all these points bad points.

each bad point incurs a penalty when we evaluate a possible hyperplane.
the amount of the penalty, in units to be determined as part of the optimization
process, is shown by the arrow leading to the bad point from the hyperplane
on the wrong side of which the bad point lies. that is, the arrows measure the
distance from the hyperplane w.x + b = 1 or w.x + b =    1. the former is
the baseline for training examples that are supposed to be above the separating
hyperplane (because the label y is +1), and the latter is the baseline for points
that are supposed to be below (because y =    1).

12.3. support-vector machines

465

misclassified

too close
to boundary

w.x + b = +1

w.x + b = 0

w.x + b =    1

figure 12.16: points that are misclassi   ed or are too close to the separating
hyperplane incur a penalty; the amount of the penalty is proportional to the
length of the arrow leading to that point

we have many options regarding the exact formula that we wish to mini-
mize. intuitively, we want kwk to be as small as possible, as we discussed in
section 12.3.2. but we also want the penalties associated with the bad points
to be as small as possible. the most common form of a tradeo    is expressed
by a formula that involves the term kwk2/2 and another term that involves a
constant times the sum of the penalties.
to see why minimizing the term kwk2/2 makes sense, note that minimizing
kwk is the same as minimizing any monotone function of kwk, so it is at least an
option to choose a formula in which we try to minimize kwk2/2. it turns out to
be desirable because its derivative with respect to any component of w is that
component. that is, if w = [w1, w2, . . . , wd], then kwk2/2 is 1
i , so its
partial derivative    /   wi is wi. this situation makes sense because, as we shall
see, the derivative of the penalty term with respect to wi is a constant times each
xi, the corresponding component of each feature vector whose training example
incurs a penalty. that in turn means that the vector w and the vectors of the
training set are commensurate in the units of their components.

2 pn

i=1 w2

thus, we shall consider how to minimize the particular function

f (w, b) =

1
2

d

xj=1

w2

j + c

n

xi=1

d

maxn0, 1     yi(cid:0)

xj=1

wjxij + b(cid:1)o

(12.4)

the    rst term encourages small w, while the second term, involving the con-
stant c that must be chosen properly, represents the penalty for bad points

466

chapter 12. large-scale machine learning

in a manner to be explained below. we assume there are n training exam-
ples (xi, yi) for i = 1, 2, . . . , n, and xi = [xi1, xi2, . . . , xid]. also, as before,
j=1 express the dot

w = [w1, w2, . . . , wd]. note that the two summations pd

product of vectors.

the constant c, called the id173 parameter, re   ects how important
misclassi   cation is. pick a large c if you really do not want to misclassify
points, but you would accept a narrow margin. pick a small c if you are ok
with some misclassi   ed points, but want most of the points to be far away from
the boundary (i.e., the margin is large).

we must explain the penalty function (second term) in equation 12.4. the

summation over i has one term

l(xi, yi) = maxn0, 1     yi(cid:0)

xj=1

d

wj xij + b(cid:1)o

fig. 12.17, and we call its value the hinge loss. let zi = yi(pd

for each training example xi. the quantity l is a hinge function, suggested in
j=1 wj xij + b).
when zi is 1 or more, the value of l is 0. but for smaller values of zi, l rises
linearly as zi decreases.

2

1

0

max{0, 1    z }

   2

   1

0

1

2

3

z = y

(

w .

x

i

i

+ b )

figure 12.17: the hinge function decreases linearly for z     1 and then remains
0

since we shall have need to take the derivative with respect to each wj of
l(xi, yi), note that the derivative of the hinge function is discontinuous. it is
   yixij for zi < 1 and 0 for zi > 1. that is, if yi = +1 (i.e., the ith training
example is positive), then

   l
   wj

= if

d

xj=1

wjxij + b     1 then 0 else     xij

12.3. support-vector machines

467

moreover, if yi =    1 (i.e., the ith training example is negative), then

   l
   wj

= if

d

xj=1

wj xij + b        1 then 0 else xij

the two cases can be summarized as one, if we include the value of yi, as:

   l
   wj

d

= if yi(

xj=1

wjxij + b)     1 then 0 else     yixij

(12.5)

12.3.4 id166 solutions by id119

a common approach to solving equation 12.4 is to use quadratic programming.
for large-scale data, another approach, id119 has an advantage. we
can allow the data to reside on disk, rather than keeping it all in memory, which
is normally required for quadratic solvers. to implement id119, we
compute the derivative of the equation with respect to b and each component
wj of the vector w. since we want to minimize f (w, b), we move b and the
components wj in the direction opposite to the direction of the gradient. the
amount we move each component is proportional to the derivative with respect
to that component.

our    rst step is to use the trick of section 12.2.4 to make b part of the
weight vector w. notice that b is really the negative of a threshold on the dot
product w.x, so we can append a (d + 1)st component b to w and append an
extra component with value +1 to every feature vector in the training set (not
   1 as we did in section 12.2.4).
w in each round. that is, we assign

we must choose a constant    to be the fraction of the gradient that we move

wj := wj       

   f
   wj

for all j = 1, 2, . . . , d + 1.

the derivative    f
   wj

of the    rst term in equation 12.4, 1

i , is easy;
it is wj. however, the second term involves the hinge function, so it is harder
to express. we shall use an if-then expression to describe these derivatives, as
in equation 12.5. that is:

j=1 w2

2 pd

   f
   wj

= wj + c

n

d

xi=1(cid:16)if yi(

xj=1

wj xij + b)     1 then 0 else     yixij(cid:17)

(12.6)

note that this formula gives us a partial derivative with respect to each compo-
nent of w, including wd+1, which is b, as well as to the weights w1, w2, . . . , wd.
we continue to use b instead of the equivalent wd+1 in the if-then condition to
remind us of the form in which the desired hyperplane is described.

to execute the gradient-descent algorithm on a training set, we pick:

468

chapter 12. large-scale machine learning

1. values for the parameters c and   .

2. initial values for w, including the (d + 1)st component b.

then, we repeatedly:

(a) compute the partial derivatives of f (w, b) with respect to the wj   s.

(b) adjust the values of w by subtracting       f
   wj

from each wj.

example 12.9 : figure 12.18 shows six points, three positive and three nega-
tive. we expect that the best separating line will be horizontal, and the only
question is whether or not the separating hyperplane and the scale of w allows
the point (2, 2) to be misclassi   ed or to lie too close to the boundary. initially,
we shall choose w = [0, 1], a vertical vector with a scale of 1, and we shall
choose b =    2. as a result, we see in fig. 12.18 that the point (2, 2) lies on the
initial hyperplane and the three negative points are right at the margin. the
parameter values we shall choose for id119 are c = 0.1, and    = 0.2.

(1,4)

(3,4)

w

(2,2)

margin

initial

hyperplane

margin

(1,1)

(2,1)

(3,1)

figure 12.18: six points for a gradient-descent example

we begin by incorporating b as the third component of w, and for notational
convenience, we shall use u and v as the    rst two components, rather than the
customary w1 and w2. that is, we take w = [u, v, b]. we also expand the two-
dimensional points of the training set with a third component that is always 1.
that is, the training set becomes

([1, 4, 1], +1)
([1, 1, 1],   1)

([2, 2, 1], +1)
([2, 1, 1],   1)

([3, 4, 1], +1)
([3, 1, 1],   1)

in fig. 12.19 we tabulate the if-then conditions and the resulting contri-
butions to the summations over i in equation 12.6. the summation must be

12.3. support-vector machines

469

if
if
if
if
if
if

u + 4v + b     +1
2u + 2v + b     +1
3u + 4v + b     +1
u + v + b        1
2u + v + b        1
3u + v + b        1

then 0
then 0
then 0
then 0
then 0
then 0

else
else
else
else
else
else

for u
   1
   2
   3
+1
+2
+3

for v
   4
   2
   4
+1
+1
+1

for b
   1
   1
   1
+1
+1
+1

figure 12.19: sum each of these terms and multiply by c to get the contribution
of bad points to the derivatives of f with respect to u, v, and b

multiplied by c and added to u, v, or b, as appropriate, to implement equa-
tion 12.6.

the truth or falsehood of each of the six conditions in fig. 12.19 determines
the contribution of the terms in the summations over i in equation 12.6. we
shall represent the status of each condition by a sequence of x   s and o   s, with x
representing a condition that does not hold and o representing one that does.
the    rst few iterations of id119 are shown in fig. 12.20.

(1)
(2)
(3)
(4)
(5)
(6)

b

w = [u, v]
[0.000, 1.000]    2.000
[0.040, 0.840]    1.580
[   0.048, 0.652]    1.304
[   0.118, 0.502]    1.083
[   0.094, 0.542]    0.866
[   0.155, 0.414]    0.733

   /   v
   /   b
bad
   /   u
0.800    2.100
oxoooo    0.200
0.940    1.380
0.440
oxoxxx
0.752    1.104
oxoxxx
0.352
xxxxxx    0.118    0.198    1.083
0.642    0.666
oxoxxx
xxxxxx

0.306

figure 12.20: beginning of the process of id119

consider line (1). it shows the initial value of w = [0, 1]. recall that we use
u and v for the components of w, so u = 0 and v = 1. we also see the initial
value of b =    2. we must use these values of u and v to evaluate the conditions
in fig. 12.19. the    rst of the conditions in fig. 12.19 is u + 4v + b     +1. the
left side is 0 + 4 + (   2) = 2, so the condition is satis   ed. however, the second
condition, 2u + 2v + b     +1 fails. the left side is 0 + 2 + (   2) = 0. the fact
that the sum is 0 means the second point (2, 2) is exactly on the separating
hyperplane, and not outside the margin. the third condition is satis   ed, since
0 + 4 + (   2) = 2     +1. the last three conditions are also satis   ed, and in fact
are satis   ed exactly. for instance, the fourth condition is u + v + b        1. the
left side is 0 + 1 + (   2) =    1. thus, the pattern oxoooo represents the outcome
of these six conditions, as we see in the    rst line of fig. 12.20.

we use these conditions to compute the partial derivatives. for    f /   u, we

470

chapter 12. large-scale machine learning

use u in place of wj in equation 12.6. this expression thus becomes

u + c(cid:0)0 + (   2) + 0 + 0 + 0 + 0(cid:1) = 0 +

1
10

(   2) =    0.2

10(cid:0)0 + (   2) + 0 + 0 + 0 + 0(cid:1) = 0.8. finally, for b we get

the sum multiplying c can be explained this way. for each of the six conditions
of fig. 12.19, take 0 if the condition is satis   ed, and take the value in the
column labeled    for u    if it is not satis   ed. similarly, for v in place of wj we
get    f /   v = 1 + 1
   f /   b =    2 + 1
10(cid:0)0 + (   1) + 0 + 0 + 0 + 0(cid:1) =    2.1.
we can now compute the new w and b that appear on line (2) of fig. 12.20.
since we chose    = 1/5, the new value of u is 0     1
5 (   0.2) =    0.04, the new
5 (0.8) = 0.84, and the new value of b is    2    1
value of v is 1    1
5 (   2.1) =    1.58.
to compute the derivatives shown in line (2) of fig. 12.20 we must    rst check
the conditions of fig. 12.19. while the outcomes of the    rst three conditions
have not changed, the last three are no longer satis   ed. for example, the
fourth condition is u + v + b        1, but 0.04 + 0.84 + (   1.58) =    0.7, which is
not less than    1. thus, the pattern of bad points becomes oxoxxx. we now
have more nonzero terms in the expressions for the derivatives. for example
   f /   u = 0.04 + 1

10(cid:0)0 + (   2) + 0 + 1 + 2 + 3(cid:1) = 0.44.

the values of w and b in line (3) are computed from the derivatives of
line (2) in the same way as they were computed in line (2). the new values
do not change the pattern of bad points; it is still oxoxxx. however, when we
repeat the process for line (4), we    nd that all six conditions are unsatis   ed.
for instance, the    rst condition, u + 4v + b     +1 is not satis   ed, because
(   0.118 + 4    0.502 + (   1.083) = 0.807, which is less than 1. in e   ect, the
   rst point has become too close to the separating hyperplane, even though it is
properly classi   ed.

we can see that in line (5) of fig. 12.20, the problems with the    rst and
third points are corrected, and we go back to pattern oxoxxx of bad points.
however, at line (6), the points have again become too close to the separating
hyperplane, so we revert to the xxxxxx pattern of bad points. you are invited
to continue the sequence of updates to w and b for several more iterations.

one might wonder why the gradient-descent process seems to be converging
on a solution where at least some of the points are inside the margin, when
there is an obvious hyperplane (horizontal, at height 1.5) with a margin of 1/2,
that separates the positive and negative points. the reason is that when we
picked c = 0.1 we were saying that we really don   t care too much whether
there are points inside the margins, or even if points are misclassi   ed. we were
saying also that what was important was a large margin (which corresponds to
a small kwk), even if some points violated that same margin.    

12.3.5 stochastic id119

the gradient-descent algorithm described in section 12.3.4 is often called batch
id119, because at each round, all the training examples are consid-

12.3. support-vector machines

471

ered as a    batch.    while it is e   ective on small datasets, it can be too time-
consuming to execute on a large dataset, where we must visit every training
example, often many times before convergence.

an alternative, called stochastic id119, considers one training ex-
ample, or a few training examples at a time and adjusts the current estimate of
the error function (w in the id166 example) in the direction indicated by only
the small set of training examples considered. additional rounds are possible,
using other sets of training examples; these can be selected randomly or accord-
ing to some    xed strategy. note that it is normal that some members of the
training set are never used in a stochastic id119 algorithm.

example 12.10 : recall the uv-decomposition algorithm discussed in sec-
tion 9.4.3. this algorithm was described as an example of batch gradient de-
scent. we can regard each of the nonblank entries in the matrix m we are
trying to approximate by the product u v as a training example, and the error
function is the root-mean-square error between the product of the current ma-
trices u and v and the matrix m , considering only those elements where m is
nonblank.

however, if m has a very large number of nonblank entries, as would be the
case if m represented, say, purchases of items by amazon customers or movies
that net   ix customers had rated, then it is not practical to make repeated
passes over the entire set of nonblank entries of m when adjusting the entries
in u and v . a stochastic id119 would look at a single nonblank
entry of m and compute the change to each element of u and v that would
make the product u v agree with that element of m . we would not make that
change to the elements of u and v completely, but rather choose some learning
rate    less than 1 and change each element of u and v by the fraction    of the
amount that would be necessary to make u v equal m in the chosen entry.    

12.3.6 parallel implementation of id166

one approach to parallelism for id166 is analogous to what we suggested for
id88s in section 12.2.8. you can start with the current w and b, and
in parallel do several iterations based on each training example. then average
the changes for each of the examples to create a new w and b. if we distribute
w and b to each mapper, then the map tasks can do as many iterations as we
wish to do in one round, and we need use the reduce tasks only to average the
results. one iteration of mapreduce is needed for each round.

a second approach is to follow the prescription given here, but implement
the computation of the second term in equation 12.4 in parallel. the contribu-
tion from each training example can then be summed. this approach requires
one round of mapreduce for each iteration of id119.

472

chapter 12. large-scale machine learning

12.3.7 exercises for section 12.3

exercise 12.3.1 : continue the iterations of fig. 12.20 for three more itera-
tions.

exercise 12.3.2 : the following training set obeys the rule that the positive
examples all have vectors whose components sum to 10 or more, while the sum
is less than 10 for the negative examples.

([3, 4, 5], +1)
([1, 2, 3],   1)

([2, 7, 2], +1)
([3, 3, 2],   1)

([5, 5, 5], +1)
([2, 4, 1],   1)

(a) which of these six vectors are the support vectors?

! (b) suggest a vector w and constant b such that the hyperplane de   ned by
w.x + b = 0 is a good separator for the positive and negative examples.
make sure that the scale of w is such that all points are outside the margin;
that is, for each training example (x, y), you have y(w.x + b)     +1.

! (c) starting with your answer to part (b), use id119 to    nd the
optimum w and b. note that if you start with a separating hyperplane,
and you scale w properly, then the second term of equation 12.4 will
always be 0, which simpli   es your work considerably.

! exercise 12.3.3 : the following training set obeys the rule that the positive
examples all have vectors whose components have an odd sum, while the sum
is even for the negative examples.

([1, 2], +1)
([2, 4],   1)

([3, 4], +1)
([3, 1],   1)

([5, 2], +1)
([7, 3],   1)

(a) suggest a starting vector w and constant b that classi   es at least three of

the points correctly.

!! (b) starting with your answer to (a), use id119 to    nd the optimum

w and b.

12.4 learning from nearest neighbors

in this section we consider several examples of    learning,    where the entire
training set is stored, perhaps, preprocessed in some useful way, and then used
to classify future examples or to compute the value of the label that is most
likely associated with the example. the feature vector of each training example
is treated as a data point in some space. when a new point arrives and must
be classi   ed, we    nd the training example or examples that are closest to the
new point, according to the distance measure for that space. the estimated
label is then computed by combining the closest examples in some way.

12.4. learning from nearest neighbors

473

12.4.1 the framework for nearest-neighbor calculations

the training set is    rst preprocessed and stored. the decisions take place when
a new example, called the query example arrives and must be classi   ed.

there are several decisions we must make in order to design a nearest-
neighbor-based algorithm that will classify query examples. we enumerate
them here.

1. what distance measure do we use?

2. how many of the nearest neighbors do we look at?

3. how do we weight the nearest neighbors? normally, we provide a function
(the id81) of the distance between the query example and its
nearest neighbors in the training set, and use this function to weight the
neighbors.

4. how do we de   ne the label to associate with the query? this label is some
function of the labels of the nearest neighbors, perhaps weighted by the
id81, or perhaps not. if there is no weighting, then the kernel
function need not be speci   ed.

12.4.2 learning with one nearest neighbor

the simplest cases of nearest-neighbor learning are when we choose only the
one neighbor that is nearest the query example. in that case, there is no use
for weighting the neighbors, so the id81 is omitted. there is also
typically only one possible choice for the labeling function: take the label of the
query to be the same as the label of the nearest neighbor.

example 12.11 : figure 12.21 shows some of the examples of dogs that last
appeared in fig. 12.1. we have dropped most of the examples for simplicity,
leaving only three chihuahuas, two dachshunds, and two beagles. since the
height-weight vectors describing the dogs are two-dimensional, there is a simple
and e   cient way to construct a voronoi diagram for the points, in which the
perpendicular bisectors of the lines between each pair of points is constructed.
each point gets a region around it, containing all the points to which it is
the nearest. these regions are always convex, although they may be open to
in   nity in one direction.2 it is also a surprising fact that, even though there are
o(n2) perpendicular bisectors for n points, the voronoi diagram can be found
in o(n log n) time.

in fig. 12.21 we see the voronoi diagram for the seven points. the bound-
aries that separate dogs of di   erent breeds are shown solid, while the boundaries

2while the region belonging to any one point is convex, the union of the regions for two or
more points might not be convex. thus, in fig. 12.21 we see that the region for all dachshunds
and the region for all beagles are not convex. that is, there are points p1 and p2 that are
both classi   ed dachshunds, but the midpoint of the line between p1 and p2 is classi   ed as a
beagle, and vice versa.

474

chapter 12. large-scale machine learning

beagles

chihuahuas

dachshunds

figure 12.21: voronoi diagram for the three breeds of dogs

between dogs of the same breed are shown dashed. suppose a query example
q is provided. note that q is a point in the space of fig. 12.21. we    nd the
region into which q falls, and give q the label of the training example to which
that region belongs. note that it is not too hard to    nd the region of q. we
have to determine to which side of certain lines q falls. this process is the same
as we used in sections 12.2 and 12.3 to compare a vector x with a hyperplane
perpendicular to a vector w. in fact, if the lines that actually form parts of the
voronoi diagram are preprocessed properly, we can make the determination in
o(log n) comparisons; it is not necessary to compare q with all of the o(n log n)
lines that form part of the diagram.    

12.4.3 learning one-dimensional functions

another simple and useful case of nearest-neighbor learning has one-dimensional
data. in this situation, the training examples are of the form ([x], y), and we
shall write them as (x, y), identifying a one-dimensional vector with its lone
component. in e   ect, the training set is a collection of samples of the value
of a function y = f (x) for certain values of x, and we must interpolate the
function f at all points. there are many rules that could be used, and we shall
only outline some of the popular approaches. as discussed in section 12.4.1,
the approaches vary in the number of neighbors they use, whether or not the

12.4. learning from nearest neighbors

475

neighbors are weighted, and if so, how the weight varies with distance.

suppose we use a method with k nearest neighbors, and x is the query point.
let x1, x2, . . . , xk be the k nearest neighbors of x, and let the weight associated
with training point (xi, yi) be wi. then the estimate of the label y for x is
i=1 wi. note that this expression gives the weighted average of

i=1 wiyi/pk
pk

the labels of the k nearest neighbors.

example 12.12 : we shall illustrate four simple rules, using the training set
(1, 1), (2, 2), (3, 4), (4, 8), (5, 4), (6, 2), and (7, 1). these points represent a
function that has a peak at x = 4 and decays exponentially on both sides.
note that this training set has values of x that are evenly spaced. there is no
requirement that the points have any regular pattern. some possible ways to
interpolate values are:

1

2

3

4

5

6

7

1

2

3

4

5

6

7

(a) one nearest neighbor

(b) average of two nearest neighbors

figure 12.22: results of applying the    rst two rules in example 12.12

1. nearest neighbor. use only the one nearest neighbor. there is no need
for a weighting. just take the value of any f (x) to be the label y associ-
ated with the training-set point nearest to query point x. the result of
using this rule on the example training set described above is shown in
fig. 12.22(a).

2. average of the two nearest neighbors. choose 2 as the number of nearest
neighbors to use. the weights of these two are each 1/2, regardless of how
far they are from the query point x. the result of this rule on the example
training set is in fig. 12.22(b).

3. weighted average of the two nearest neighbors. we again choose two
nearest neighbors, but we weight them in inverse proportion to their dis-
tance from the query point. suppose the two neighbors nearest to query

476

chapter 12. large-scale machine learning

point x are x1 and x2. suppose    rst that x1 < x < x2. then the weight
of x1, the inverse of its distance from x, is 1/(x     x1), and the weight of
x2 is 1/(x2     x). the weighted average of the labels is
x2     x(cid:1)

x2     x(cid:1)/(cid:0)

x     x1

x     x1

(cid:0)

y1

y2

+

+

1

1

which, when we multiply numerator and denominator by (x    x1)(x2     x),
simpli   es to

y1(x2     x) + y2(x     x1)

x2     x1

this expression is the linear interpolation of the two nearest neighbors, as
shown in fig. 12.23(a). when both nearest neighbors are on the same side
of the query x, the same weights make sense, and the resulting estimate is
an extrapolation. we see extrapolation in fig. 12.23(a) in the range x = 0
to x = 1. in general, when points are unevenly spaced, we can    nd query
points in the interior where both neighbors are on one side.

4. average of three nearest neighbors. we can average any number of the
nearest neighbors to estimate the label of a query point. figure 12.23(b)
shows what happens on our example training set when the three nearest
neighbors are used.

   

1

2

3

4

5

6

7

1

2

3

4

5

6

7

(a) weighted average of two neighbors

(b) average of three neighbors

figure 12.23: results of applying the last two rules in example 12.12

12.4. learning from nearest neighbors

477

12.4.4 kernel regression

a way to construct a continuous function that represents the data of a training
set well is to consider all points in the training set, but weight the points using a
id81 that decays with distance. a popular choice is to use a normal
distribution (or    bell curve   ), so the weight of a training point x when the
query is q is e   (x   q)2/  2
. here    is the standard deviation of the distribution
and the query q is the mean. roughly, points within distance    of q are heavily
weighted, and those further away have little weight. the advantage of using
a id81 that is itself continuous and that is de   ned for all points in
the training set is to be sure that the resulting function learned from the data
is itself continuous (see exercise 12.4.6 for a discussion of the problem when a
simpler weighting is used).

example 12.13 : let us use the seven training examples of example 12.12.
to make calculation simpler, we shall not use the normal distribution as the
id81, but rather another continuous function of distance, namely
w = 1/(x     q)2. that is, weights decay as the square of the distance. suppose
the query q is 3.5. the weights w1, w2, . . . w7 of the seven training examples
(xi, yi) = (i, 8/2|i   4|) for i = 1, 2, . . . , 7 are shown in fig. 12.24.

xi
(1)
(2)
yi
(3) wi
(4) wiyi

1
1

4/25
4/25

2
2
4/9
8/9

3
4
4
16

4
8
4
32

5
4

6
2

7
1

4/9
16/9

4/25
8/25

4/49
4/49

figure 12.24: weights of points when the query is q = 3.5

lines (1) and (2) of fig. 12.24 give the seven training points. the weight
of each when the query is q = 3.5 is given in line (3). for instance, for x1 = 1,
the weight w1 = 1/(1    3.5)2 = 1/(   2.5)2 = 4/25. then, line (4) shows each yi
weighted by the weight from line (3). for instance, the column for x2 has value
8/9 because w2y2 = 2    (4/9).
to compute the label for the query q = 3.5 we sum the weighted values
of the labels in the training set, as given by line (4) of fig. 12.24; this sum is
51.23. we then divide by the sum of the weights in line (3). this sum is 9.29,
so the ratio is 51.23/9.29 = 5.51. that estimate of the value of the label for
q = 3.5 seems intuitively reasonable, since q lies midway between two points
with labels 4 and 8.    

12.4.5 dealing with high-dimensional euclidean data

we saw in section 12.4.2 that the two-dimensional case of euclidean data is
fairly easy. there are several large-scale data structures that have been de-
veloped for    nding near neighbors when the number of dimensions grows, and

478

chapter 12. large-scale machine learning

problems in the limit for example 12.13

suppose q is exactly equal to one of the training examples x. if we use
the normal distribution as the id81, there is no problem with
the weight of x; it is 1. however, with the id81 discussed in
example 12.13, the weight of x is 1/(x   q)2 =    . fortunately, this weight
appears in both the numerator and denominator of the expression that
estimates the label of q. it can be shown that in the limit as q approaches
x, the label of x dominates all the other terms in both numerator and
denominator, so the estimated label of q is the same as the label of x.
that makes excellent sense, since q = x in the limit.

the training set is large. we shall not cover these structures here, because the
subject could    ll a book by itself, and there are many places available to learn
about these techniques, collectively called multidimensional index structures.
the references for this chapter give some of these sources for information about
such structures as kd-trees, r-trees, and quad trees.

unfortunately, for high-dimensional data, there is little that can be done to
avoid searching a large portion of the data. this fact is another manifestation
of the    curse of dimensionality    from section 7.1.3. two ways to deal with the
   curse    are the following:

1. va files. since we must look at a large fraction of the data anyway in
order to    nd the nearest neighbors of a query point, we could avoid a
complex data structure altogether. accept that we must scan the entire
   le, but do so in a two-stage manner. first, a summary of the    le is
created, using only a small number of bits that approximate the values of
each component of each training vector. for example, if we use only the
high-order (1/4)th of the bits in numerical components, then we can create
a    le that is (1/4)th the size of the full dataset. however, by scanning
this    le we can construct a list of candidates that might be among the k
nearest neighbors of the query q, and this list may be a small fraction of
the entire dataset. we then look up only these candidates in the complete
   le, in order to determine which k are nearest to q.

2. id84. we may treat the vectors of the training set
as a matrix, where the rows are the vectors of the training example, and
the columns correspond to the components of these vectors. apply one of
the dimensionality-reduction techniques of chapter 11, to compress the
vectors to a small number of dimensions, small enough that the techniques
for multidimensional indexing can be used. of course, when processing
a query vector q, the same transformation must be applied to q before
searching for q   s nearest neighbors.

12.4. learning from nearest neighbors

479

12.4.6 dealing with non-euclidean distances

to this point, we have assumed that the distance measure is euclidean. how-
ever, most of the techniques can be adapted naturally to an arbitrary distance
function d. for instance, in section 12.4.4 we talked about using a normal dis-
tribution as a id81. since we were thinking about a one-dimensional
training set in a euclidean space, we wrote the exponent as    (x   q)2. however,
for any distance function d, we can use as the weight of a point x at distance
d(x, q) from the query point q the value of

e   (cid:0)d(x   q)(cid:1)2

/  2

note that this expression makes sense if the data is in some high-dimensional
euclidean space and d is the usual euclidean distance or manhattan distance or
any other distance discussed in section 3.5.2. it also makes sense if d is jaccard
distance or any other distance measure.

however, for jaccard distance and the other distance measures we consid-
ered in section 3.5 we also have the option to use locality-sensitive hashing,
the subject of chapter 3. recall these methods are only approximate, and they
could yield false negatives     training examples that were near neighbors to a
query but that do not show up in a search.

if we are willing to accept such errors occasionally, we can build the buckets
for the training set and keep them as the representation of the training set.
these buckets are designed so we can retrieve all (or almost all, since there can
be false negatives) training-set points that are have a minimum similarity to a
given query q. equivalently, one of the buckets to which the query hashes will
contain all those points within some maximum distance of q. we hope that as
many nearest neighbors of q as our method requires will be found among those
buckets.

yet if di   erent queries have radically di   erent distances to their nearest
neighbors, all is not lost. we can pick several distances d1 < d2 < d3 <        .
build the buckets for locality-sensitive hashing using each of these distances.
for a query q, start with the buckets for distance d1. if we    nd enough near
neighbors, we are done. otherwise, repeat the search using the buckets for d2,
and so on, until enough nearest neighbors are found.

12.4.7 exercises for section 12.4

exercise 12.4.1 : suppose we modi   ed example 12.11 to look at the two
nearest neighbors of a query point q. classify q with the common label if those
two neighbors have the same label, and leave q unclassi   ed if the labels of the
neighbors are di   erent.

(a) sketch the boundaries of the regions for the three dog breeds on fig. 12.21.

! (b) would the boundaries always consist of straight line segments for any

training data?

480

chapter 12. large-scale machine learning

exercise 12.4.2 : suppose we have the following training set

([1, 2], +1)
([3, 4],   1)

([2, 1],   1)
([4, 3], +1)

which is the training set used in example 12.9.
if we use nearest-neighbor
learning with the single nearest neighbor as the estimate of the label of a query
point, which query points are labeled +1?

exercise 12.4.3 : consider the one-dimensional training set

(1, 1), (2, 2), (4, 3), (8, 4), (16, 5), (32, 6)

describe the function f (q), the label that is returned in response to the query
q, when the interpolation used is:

(a) the label of the nearest neighbor.

(b) the average of the labels of the two nearest neighbors.

! (c) the average, weighted by distance, of the two nearest neighbors.

(d) the (unweighted) average of the three nearest neighbors.

! exercise 12.4.4 : apply the id81 of example 12.13 to the data of

exercise 12.4.3. for queries q in the range 2 < q < 4, what is the label of q?

exercise 12.4.5 : what is the function that estimates the label of query points
using the data of example 12.12 and the average of the four nearest neighbors?

!! exercise 12.4.6 : simple weighting functions such as those in example 12.12
need not de   ne a continuous function. we can see that the constructed functions
in fig. 12.22 and fig. 12.23(b) are not continuous, but fig. 12.23(a) is. does the
weighted average of two nearest neighbors always give a continuous function?

12.5 comparison of learning methods

each of the methods discussed in this chapter and elsewhere has its advantages.
in this closing section, we shall consider:

    does the method deal with categorical features or only with numerical

features?

    does the method deal e   ectively with high-dimensional feature vectors?
    is the model that the method constructs intuitively understandable?

12.6. summary of chapter 12

481

id88s and support-vector machines: these methods can handle
millions of features, but they only make sense if the features are numerical.
they only are e   ective if there is a linear separator, or at least a hyperplane
that approximately separates the classes. however, we can separate points by
a nonlinear boundary if we    rst transform the points to make the separator
be linear. the model is expressed by a vector, the normal to the separating
hyperplane. since this vector is often of very high dimension, it can be very
hard to interpret the model.

nearest-neighbor classi   cation and regression: here, the model is the
training set itself, so we expect it to be intuitively understandable. the ap-
proach can deal with multidimensional data, although the larger the number of
dimensions, the sparser the training set will be, and therefore the less likely it
is that we shall    nd a training point very close to the point we need to classify.
that is, the    curse of dimensionality    makes nearest-neighbor methods ques-
tionable in high dimensions. these methods are really only useful for numerical
features, although one could allow categorical features with a small number of
values. for instance, a binary categorical feature like {male, female} could
have the values replaced by 0 and 1, so there was no distance in this dimension
between individuals of the same gender and distance 1 between other pairs of
individuals. however, three or more values cannot be assigned numbers that are
equidistant. finally, nearest-neighbor methods have many parameters to set,
including the distance measure we use (e.g., cosine or euclidean), the number of
neighbors to choose, and the id81 to use. di   erent choices result in
di   erent classi   cation, and in many cases it is not obvious which choices yield
the best results.

id90: unlike the other methods discussed in this chapter, decision
trees are useful for both categorical and numerical features. the models pro-
duced are generally quite understandable, since each decision is represented by
one node of the tree. however, this approach is most useful for low-dimension
feature vectors. building id90 with many levels often leads to over   t-
ting. but if a decision tree has few levels, then it cannot even mention more
than a small number of features. as a result, the best use of id90
is often to create a decision forest of many, low-depth trees and combine their
decision in some way.

12.6 summary of chapter 12

    training sets: a training set consists of a feature vector, each component
of which is a feature, and a label indicating the class to which the object
represented by the feature vector belongs. features can be categorical    
belonging to an enumerated list of values     or numerical.

    test sets and over   tting: when training some classi   er on a training set,
it is useful to remove some of the training set and use the removed data

482

chapter 12. large-scale machine learning

as a test set. after producing a model or classi   er without using the test
set, we can run the classi   er on the test set to see how well it does. if the
classi   er does not perform as well on the test set as on the training set
used, then we have over   t the training set by conforming to peculiarities
of the training-set data which is not present in the data as a whole.

    batch versus on-line learning: in batch learning, the training set is
available at any time and can be used in repeated passes. on-line learning
uses a stream of training examples, each of which can be used only once.

    id88s: this machine-learning method assumes the training set has
only two class labels, positive and negative. id88s work when there
is a hyperplane that separates the feature vectors of the positive examples
from those of the negative examples. we converge to that hyperplane by
adjusting our estimate of the hyperplane by a fraction     the learning rate    
of the direction that is the average of the currently misclassi   ed points.

    the winnow algorithm: this algorithm is a variant of the id88
algorithm that requires components of the feature vectors to be 0 or 1.
training examples are examined in a round-robin fashion, and if the cur-
rent classi   cation of a training example is incorrect, the components of
the estimated separator where the feature vector has 1 are adjusted up or
down, in the direction that will make it more likely this training example
is correctly classi   ed in the next round.

    nonlinear separators: when the training points do not have a linear func-
tion that separates two classes, it may still be possible to use a id88
to classify them. we must    nd a function we can use to transform the
points so that in the transformed space, the separator is a hyperplane.

    support-vector machines: the id166 improves upon id88s by    nd-
ing a separating hyperplane that not only separates the positive and nega-
tive points, but does so in a way that maximizes the margin     the distance
perpendicular to the hyperplane to the nearest points. the points that
lie exactly at this minimum distance are the support vectors. alterna-
tively, the id166 can be designed to allow points that are too close to the
hyperplane, or even on the wrong side of the hyperplane, but minimize
the error due to such misplaced points.

    solving the id166 equations: we can set up a function of the vector that
is normal to the hyperplane, the length of the vector (which determines
the margin), and the penalty for points on the wrong side of the margins.
the id173 parameter determines the relative importance of a wide
margin and a small penalty. the equations can be solved by several
methods, including id119 and quadratic programming.

    nearest-neighbor learning:

in this approach to machine learning, the
entire training set is used as the model. for each (   query   ) point to be

12.7. references for chapter 12

483

classi   ed, we search for its k nearest neighbors in the training set. the
classi   cation of the query point is some function of the labels of these k
neighbors. the simplest case is when k = 1, in which case we can take
the label of the query point to be the label of the nearest neighbor.

    regression: a common case of nearest-neighbor learning, called regres-
sion, occurs when the there is only one feature vector, and it, as well as
the label, are real numbers; i.e., the data de   nes a real-valued function of
one variable. to estimate the label, i.e., the value of the function, for an
unlabeled data point, we can perform some computation involving the k
nearest neighbors. examples include averaging the neighbors or taking a
weighted average, where the weight of a neighbor is some decreasing func-
tion of its distance from the point whose label we are trying to determine.

12.7 references for chapter 12

the id88 was introduced in [11]. [7] introduces the idea of maximizing the
margin around the separating hyperplane. a well-known book on the subject
is [10].

the winnow algorithm is from [9]. also see the analysis in [1].
support-vector machines appeared in [6]. [5] and [4] are useful surveys. [8]
talks about a more e   cient algorithm for the case of sparse features (most com-
ponents of the feature vectors are zero). the use of gradient-descent methods
is found in [2, 3].

1. a. blum,    empirical support for winnow and weighted-majority algo-
rithms: results on a calendar scheduling domain,    machine learning 26
(1997), pp. 5   23.

2. l. bottou,    large-scale machine learning with stochastic gradient de-
scent,    proc. 19th intl. conf. on computational statistics (2010), pp. 177   
187, springer.

3. l. bottou,    stochastic gradient tricks, neural networks,    in tricks of the
trade, reloaded, pp. 430   445, edited by g. montavon, g.b. orr and k.-
r. mueller, lecture notes in computer science (lncs 7700), springer,
2012.

4. c.j.c. burges,    a tutorial on support vector machines for pattern recog-

nition,    data mining and knowledge discovery 2 (1998), pp. 121   167.

5. n. cristianini and j. shawe-taylor, an introduction to support vector
machines and other kernel-based learning methods, cambridge univer-
sity press, 2000.

6. c. cortes and v.n. vapnik,    support-vector networks,    machine learn-

ing 20 (1995), pp. 273   297.

484

chapter 12. large-scale machine learning

7. y. freund and r.e. schapire,    large margin classi   cation using the per-

ceptron algorithm,    machine learning 37 (1999), pp. 277   296.

8. t. joachims,    training linear id166s in linear time.    proc. 12th acm

sigkdd (2006), pp. 217   226.

9. n. littlestone,    learning quickly when irrelevant attributes abound: a
new linear-threshold algorithm,    machine learning 2 (1988), pp. 285   
318.

10. m. minsky and s. papert, id88s: an introduction to computational

geometry (2nd edition), mit press, cambridge ma, 1972.

11. f. rosenblatt,    the id88: a probabilistic model for information
storage and organization in the brain,    psychological review 65:6 (1958),
pp. 386   408.

index

a-priori algorithm, 212, 213, 219
accessible page, 187
active learning, 446
ad-hoc query, 134
adjacency matrix, 363
adomavicius, g., 340
advertising, 16, 116, 204, 281
adwords, 290
a   liation-graph model, 371
afrati, f.n., 70, 402
agglomerative id91, see hierar-

chical id91

aggregation, 34, 37
agrawal, r., 238
alon, n., 162
alon-matias-szegedy algorithm, 146
ampli   cation, 101
analytic query, 53
and-construction, 101
andersen, a., 402
anderson, c., 340, 341
andoni, a., 129
anf, see approximate neighborhood

function

anf algorithm, 396
apache, 24, 71
approximate neighborhood function, 396
arc, 386
archive, 132
ask, 192
association rule, 205, 207
associativity, 27
attribute, 33
auction, 293
austern, m.h., 71
authority, 192

average, 144

b-tree, 280
babcock, b., 162, 280
babu, s., 162
backstrom, l., 402
bag, 40, 76
balance algorithm, 293
balazinska, m., 70
band, 88
bandwidth, 22
basket, see market basket, 202, 204,

205, 234

batch id119, 470
batch learning, 445
bayes net, 4
bdmo algorithm, 271
beer and diapers, 206
bell, r., 341
bellkor   s pragmatic chaos, 310
berkhin, p., 200
berrar, d.p., 437
betweenness, 351
bfr algorithm, 254, 257
bfs, see breadth-   rst search
bi-clique, 357
bid, 291, 293, 300, 301
bigtable, 70
bik, a.j.c., 71
binary classi   cation, 440
biomarker, 205
bipartite graph, 287, 347, 357, 358
birch algorithm, 280
birrell, a., 71
bitmap, 220, 221
block, 12, 21, 180

485

486

index

blog, 188
bloom    lter, 140, 218
bloom, b.h., 162
blum, a., 483
bohannon, p., 70
boldi, p., 402
bonferroni correction, 5
bonferroni   s principle, 4, 5
bookmark, 186
boral, h., 403
borkar, v., 70
bottou, l., 483
bradley, p.s., 280
breadth-   rst search, 351
brick-and-mortar retailer, 204, 308, 309
brin, s., 200
broad matching, 293
broder, a.z., 18, 129, 200
bu, y., 70
bucket, 9, 137, 152, 156, 218, 271
budget, 292, 299
budiu, m., 71
burges, c.j.c., 483
burrows, m., 70

candidate itemset, 215, 228
candidate pair, 88, 219, 222
carey, m., 70
categorical feature, 440, 480
centroid, 243, 246, 252, 255, 259
chabbert, m., 341
chandra, t., 70
chang, f., 70
characteristic matrix, 81
charikar, m.s., 129
chaudhuri, s., 129
checkpoint, 46
chen, m.-s., 238
child, 351
cholera, 3
chronicle data model, 161
chung, f., 402
chunk, 24, 228, 258
cinematch, 337
classi   er, 318, 439

click stream, 133
click-through rate, 285, 291
clique, 357
cloud computing, 16
cloudstore, 24
cluster computing, 21, 22
cluster tree, 266, 267
clustera, 41, 69
id91, 3, 16, 241, 325, 343, 349,

439

clustroid, 246, 252
collaboration network, 346
collaborative    ltering, 4, 17, 75, 281,

307, 321, 347

column-orthonormal matrix, 419
combiner, 27, 177, 179
communication cost, 22, 47, 384
community, 17, 343, 354, 357, 381
community-a   liation graph, 371
commutativity, 27
competitive ratio, 16, 286, 289, 294
complete graph, 357, 358
compressed set, 258
compute node, 21, 22
computer game, 315
computing cloud, see cloud comput-

ing
concept, 420
concept space, 425
con   dence, 206, 207
content-based recommendation, 307,

312

convergence, 451
cooper, b.f., 70
coordinates, 242
cortes, c., 483
cosine distance, 95, 105, 313, 318, 426
counting ones, 150, 271
covering an output, 59
craig   s list, 282
craswell, n., 305
credit, 352
cristianini, n., 483
cross-validation, 445
id104, 446

index

487

cur-decomposition, 405, 428
cure algorithm, 262, 266
currey, j., 71
curse of dimensionality, 244, 268, 478,

481

cut, 362
cyclic permutation, 87
cylinder, 12
czajkowski, g., 71

dag, see directed acyclic graph
darts, 140
das sarma, a., 70
dasgupta, a., 403
data mining, 1
data stream, 16, 232, 270, 284, 460
data-stream-management system, 132
database, 16
datar, m., 129, 162, 280
datar-gionis-indyk-motwani algorithm,

151

dead end, 167, 170, 171, 193
dean, j., 70, 71
decaying window, 157, 234
decision forest, 481
decision tree, 17, 318, 443, 444, 481
deerwester, s., 436
degree, 359, 381
degree matrix, 363
dehnert, j.c., 71
del.icio.us, 314, 347
deletion, 95
dense matrix, 31, 428
density, 251, 253
depth-   rst search, 393
determinant, 407
dewitt, d.j., 71
dfs, see distributed    le system
diagonal matrix, 419
diameter, 251, 253, 388
diapers and beer, 204
di   erence, 33, 36, 40
dimension table, 53
id84, 17, 328, 405,

478

directed acyclic graph, 351
directed graph, 386
discard set, 258
disk, 11, 209, 243, 266
disk block, see block
display ad, 282, 283
distance measure, 92, 241, 349
distinct elements, 142, 145
distributed    le system, 21, 23, 202,

209

dmoz, see open directory
document, 74, 77, 205, 242, 301, 313,

314, 442

document frequency, see inverse doc-

ument frequency

domain, 190
dot product, 95
drineas, p., 436
dryad, 69
dryadlinq, 70
dual construction, 348
dubitzky, w., 437
dumais, s.t., 436
dup-elim task, 43

e, 12
id153, 95, 98
eigenpair, 407
eigenvalue, 167, 364, 406, 416, 417
eigenvector, 167, 364, 406, 411, 416
email, 346
energy, 424
ensemble, 319
entity resolution, 110
equijoin, 34
erlingsson, i., 71
ernst, m., 70
ethernet, 21, 22
euclidean distance, 93, 107, 477
euclidean space, 93, 97, 242, 243, 246,

262

exponentially decaying window, see de-

caying window

extrapolation, 476

facebook, 17, 186, 344

488

index

fact table, 53
failure, 23, 29, 42   44
faloutsos, c., 403, 437
false negative, 88, 99, 227
false positive, 88, 99, 140, 227
family of functions, 100
fang, m., 238
fayyad, u.m., 280
feature, 266, 312   314
feature selection, 446
feature vector, 440, 480
fetterly, d., 71
fikes, a., 70
file, 23, 24, 209, 227
filtering, 139
fingerprint, 113
first-price auction, 293
fixedpoint, 102, 192
flajolet, p., 162
flajolet-martin algorithm, 143, 395
flow graph, 41
fortunato, s., 403
fotakis, d., 402
french, j.c., 280
frequent bucket, 219, 221
frequent itemset, 4, 202, 212, 214, 358,

439

frequent pairs, 213
frequent-items table, 214
freund, y., 484
friends, 344
friends relation, 52
frieze, a.m., 129
frobenius norm, 409, 423
furnas, g.w., 436

gaber, m.m., 18
ganti, v., 129, 280
garcia-molina, h., 18, 200, 238, 280,

403

garofalakis, m., 162
gaussian elimination, 168
gehrke, j., 162, 280
generalization, 445
generated subgraph, 357

genre, 312, 324, 338
gfs, see google    le system
ghemawat, s., 70, 71
gibbons, p.b., 162, 403
gionis, a., 129, 162
girvan, m., 403
girvan-newman algorithm, 351
global minimum, 330
gn algorithm, see girvan-newman al-

gorithm

gobio   , h., 71
golub, g.h., 436
google, 164, 175, 290
google    le system, 24
google+, 344
id119, 17, 336, 373, 467
granzow, m., 437
graph, 45, 57, 343, 344, 380, 387
greedy algorithm, 284, 285, 288, 292
grgpf algorithm, 266
grouping, 26, 34, 37
grouping attribute, 34
groupon, 347
gruber, r.e., 70
guha, s., 280
gunda, p.k., 71
gyongi, z., 200

hadoop, 24, 71
hadoop distributed    le system, 24
hamming distance, 67, 96, 104
harris, m., 338
harshman, r., 436
hash function, 9, 79, 83, 88, 137, 140,

143

hash key, 9, 300
hash table, 9, 11, 12, 211, 218, 221,

222, 300, 302, 381

haveliwala, t.h., 200
hdfs, see hadoop distributed    le sys-

tem

head, 392
heavy hitter, 381
henzinger, m., 129

index

489

hierarchical id91, 243, 245, 263,

326, 349

item pro   le, 312, 315
itemset, 202, 210, 212

hinge loss, 466
hits, 192
hive, 70, 71
hopcroft, j.e., 393
horn, h., 71
howe, b., 70
hsieh, w.c., 70
hub, 192
hyperlink-induced topic search, see hits
hyperplane, 461
hyracks, 41

jaccard distance, 92, 94, 100, 313, 479
jaccard similarity, 74, 82, 92, 187
jacobsen, h.-a., 70
jagadish, h.v., 162
jahrer, m., 341
jeh, g., 403
joachims, t., 484
join, see natural join, see multiway

join, see star join, 383

join task, 43

identical documents, 118
identity matrix, 407
idf, see inverse document frequency
image, 133, 313, 314
imdb, see internet movie database
imielinski, t., 238
immediate subset, 230
immorlica, n., 129
important page, 164
impression, 282
in-component, 169
inaccessible page, 187
independent rows or columns, 419
index, 10, 381
indyk, p., 129, 162
initialize clusters, 255
input, 57
insertion, 95
instance-based learning, 443
interest, 206
internet movie database, 312, 338
interpolation, 476
intersection, 33, 36, 40, 77
into thin air, 311
inverse document frequency, 8, see tf.idf
inverted index, 164, 282
ioannidis, y.e., 403
ip packet, 133
isard, m., 71
isolated component, 170
item, 202, 204, 205, 308, 324, 325

id116, 254
k-partite graph, 347
kahan, w., 436
kalyanasundaram, b., 306
kamm, d., 338
kang, u., 403
kannan, r., 436
karlin, a., 286
kaushik, r., 129
kautz, w.h., 162
id81, 473, 477
key component, 137
key-value pair, 25   27
keyword, 291, 319
kleinberg, j.m., 200
knuth, d.e., 19
koren, y., 341
kosmix, 24
krioukov, a., 71
kumar, r., 18, 71, 200, 403
kumar, v., 19

label, 344, 440
lagrangean multipliers, 51
landauer, t.k., 436
lang, k., 402
lang, k.j., 403
laplacian matrix, 364
lcs, see longest common subsequence
leaf, 352
learning-rate parameter, 448
leiser, n, 71

490

index

length, 146, 387
length indexing, 119
leskovec, j., 402   404
leung, s.-t., 71
likelihood, 369
lin, s., 129
linden, g., 341
linear equations, 168
linear separability, 447, 451
link, 33, 164, 178
link matrix of the web, 193
link spam, 183, 187
littlestone, n., 484
livny, m., 280
local minimum, 330
locality, 344
locality-sensitive family, 104
locality-sensitive function, 99
locality-sensitive hashing, 88, 99, 314,

479

log likelihood, 374
logarithm, 12
long tail, 204, 308, 309
longest common subsequence, 96
lower bound, 61
lsh, see locality-sensitive hashing

machine learning, 2, 17, 318, 439
maggioni, m., 436
maghoul, f., 18, 200
mahalanobis distance, 261
mahoney, m.w., 403, 436
main memory, 209, 210, 218, 243
malewicz, g, 71
malik, j., 403
manber, u., 129
manhattan distance, 93
manning, c.p., 19
many-many matching, 113
many-many relationship, 57, 202
many-one matching, 113
map task, 25, 27
map worker, 28, 29
mapping schema, 58

mapreduce, 16, 21, 24, 30, 177, 179,

229, 275, 383, 390, 458

margin, 461
market basket, 4, 16, 201, 202, 209
markov process, 167, 170, 377
martin, g.n., 162
master controller, 25, 26, 28
master node, 24
matching, 287
matias, y., 162
matrix, 31, see transition matrix of
the web, see stochastic ma-
trix, see substochastic matrix,
177, 192, see utility matrix,
328, see adjacency matrix, see
degree matrix, see laplacian
matrix, see symmetric matrix

id127, 38, 39, 62
matrix of distances, 417
matthew e   ect, 14
maximal itemset, 212
maximal matching, 287
maximum-likelihood estimation, 369
mcauley, j., 404
mean, see average
mechanical turk, 446
median, 144
mehta, a., 306
melnik, s., 403
merging clusters, 246, 249, 260, 264,

269, 273

merton, p., 19
miller, g.l., 403
minhashing, 81, 91, 94, 101, 314
minicluster, 258
minsky, m., 484
minutiae, 113
mirrokni, v.s., 129
mirror page, 75
mitzenmacher, m., 129
ml, see machine learning
id113, see maximum-likelihood estima-

tion

model, 369
moments, 145

index

491

monotonicity, 212
montavon, g., 483
moore-penrose pseudoinverse, 429
most-common elements, 157
motwani, r., 129, 162, 238, 280
mueller, k.-r., 483
multiclass classi   cation, 440, 455
multidimensional index, 478
multihash algorithm, 222
multiplication, 31, see matrix multi-

plication, 177, 192

multiset, see bag
multistage algorithm, 220
multiway join, 49, 383
mumick, i.s., 162
mutation, 98

name node, see master node
natural join, 34, 37, 38, 48
naughton, j.f., 71
navathe, s.b., 238
near-neighbor search, see locality-sens-

itive hashing

nearest neighbor, 17, 444, 472, 481
negative border, 230
negative example, 447
neighbor, 376
neighborhood, 387, 395
neighborhood pro   le, 387
net   ix challenge, 2, 310, 337
network, see social network
neural net, 443
newman, m.e.j., 403
newspaper articles, 115, 301, 310
non-euclidean distance, 252, see co-

sine distance, see id153,
see hamming distance, see jac-
card distance

non-euclidean space, 266, 268
norm, 93
normal distribution, 257
id172, 321, 323, 334
normalized cut, 363
np-complete problem, 357
numerical feature, 440, 480

o   callaghan, l., 280
o   -line algorithm, 284
olston, c., 71
omiecinski, e., 238
on-line advertising, see advertising
on-line algorithm, 16, 284
on-line learning, 445
on-line retailer, 204, 282, 308, 309
open directory, 184, 446
or-construction, 101
orr, g.b., 483
orthogonal vectors, 244, 410
orthonormal matrix, 419, 424
orthonormal vectors, 411, 414
out-component, 169
outlier, 243
output, 57
over   tting, 319, 336, 443, 444, 457,

481

overlapping communities, 369
overture, 291
own pages, 188

paepcke, a., 129
page, l., 163, 200
id95, 3, 16, 31, 32, 42, 163, 165,

177

pairs, see frequent pairs
palmer, c.r., 403
pan, j.-y., 403
papert, s., 484
parent, 351
park, j.s., 238
partition, 361
pass, 210, 213, 221, 226
path, 387
paulson, e., 71
pca, see principal-component analy-

sis

pcy algorithm, 218, 221, 222
pedersen, j., 200
id88, 17, 439, 443, 447, 481
perfect matching, 287
permutation, 82, 87
pig, 70

492

index

pigeonhole principle, 357
piotte, m., 341
pivotal condensation, 407
plagiarism, 75, 205
pnuts, 70
point, 241, 271
point assignment, 243, 254, 350
polyzotis, a., 70
position indexing, 121, 122
positive example, 447
positive integer, 156
powell, a.l., 280
power iteration, 407, 408
power law, 13
predicate, 318
pre   x indexing, 119, 121, 122
pregel, 45
principal eigenvector, 167, 407
principal-component analysis, 405, 412
priority queue, 249
priors, 371
privacy, 284
probe string, 121
pro   le, see item pro   le, see user pro-

   le

projection, 33, 36
pruhs, k.r., 306
pseudoinverse, see moore-penrose pseu-

doinverse

puz, n., 70

quadratic programming, 467
query, 134, 153, 275
query example, 473

r-tree, 280
rack, 22
radius, 251, 253, 387
raghavan, p., 18, 19, 200, 403
rahm, e., 403
rajagopalan, s., 18, 200, 403
ramakrishnan, r., 70, 280
ramsey, w., 305
random hyperplanes, 105, 314
random surfer, 164, 165, 170, 184, 376

randomization, 226
rank, 418
rarest-   rst order, 301
rastogi, r., 162, 280
rating, 308, 311
reachability, 389
id126, 17, 307
recursion, 42
recursive doubling, 391
reduce task, 25, 27
reduce worker, 28, 30
reducer, 27
reducer size, 54, 60
reed, b., 71
re   exive and transitive closure, 389
regression, 440, 477, 481
id173 parameter, 466
reichsteiner, a., 437
reina, c., 280
relation, 33
relational algebra, 32, 33
replication, 24
replication rate, 54, 61
representation, 266
representative point, 263
representative sample, 137
reservoir sampling, 162
restart, 377
retained set, 258
revenue, 292
ripple-carry adder, 156
rmse, see root-mean-square error
robinson, e., 71
rocha, l.m., 437
root-mean-square error, 310, 329, 423
rosa, m., 402
rosenblatt, f., 484
rounding data, 323
row, see tuple
row-orthonormal matrix, 424
rowsum, 266
royalty, j., 71

s-curve, 89, 99
saberi, a., 306

index

493

salihoglu, s., 70
sample, 226, 230, 233, 235, 255, 263,

267

sampling, 136, 150
savasere, a., 238
scc, see strongly connected compo-

nent

schapire, r.e., 484
schema, 33
schutze, h., 19
score, 111
search ad, 282
search engine, 175, 191
search query, 133, 164, 186, 282, 300
second-price auction, 293
secondary storage, see disk
selection, 33, 35
sensor, 133
id31, 447
set, 81, 118, see itemset
set di   erence, see di   erence
shankar, s., 71
shawe-taylor, j., 483
shi, j., 403
shim, k., 280
shingle, 77, 91, 116
shivakumar, n., 238
shopping cart, 204
shortest paths, 45
siddharth, j., 129
signature, 80, 83, 91
signature matrix, 83, 88
silberschatz, a., 162
silberstein, a., 70
similarity, 4, 16, 74, 201, 314, 322
similarity join, 55, 61
simrank, 376
singleton, r.c., 162
singular value, 419, 423, 424
id166, 328, 405,

418, 428

six degrees of separation, 389
sketch, 106
skew, 28
sliding window, 134, 150, 157, 271

smart transitive closure, 392
smith, b., 341
snap, 402
social graph, 344
social network, 17, 343, 344, 405
son algorithm, 228
source, 386
space, 92, 93, 241
spam, see term spam, see link spam,

346, 445

spam farm, 187, 190
spam mass, 190, 191
sparse matrix, 31, 81, 83, 177, 178, 308
spectral partitioning, 361
spider trap, 170, 173, 193
splitting clusters, 269
sql, 22, 33, 70
squares, 385
srikant, r., 238
srivastava, u., 70, 71
standard deviation, 259, 261
standing query, 134
stanford network analysis platform,

see snap

star join, 53
stata, r., 18, 200
statistical model, 1
status, 301
steinbach, m., 19
stochastic id119, 336, 471
stochastic matrix, 167, 407
stop id91, 247, 251, 253
stop words, 8, 79, 116, 205, 313
stream, see data stream
strength of membership, 374
string, 118
striping, 32, 177, 179
strong edge, 346
strongly connected component, 169, 393
strongly connected graph, 167, 388
substochastic matrix, 170
su   x length, 123
summarization, 3
summation, 156
sun, j., 437

494

index

supercomputer, 21
superimposed code, see bloom    lter,

161

supermarket, 204, 226
superstep, 46
supervised learning, 439, 441
support, 202, 227, 228, 230, 232
support vector, 462
support-vector machine, 17, 439, 444,

461, 481

supporting page, 188
suri, s., 403
surprise number, 146
svd, see id166
id166, see support-vector machine
swami, a., 238
symmetric matrix, 365, 406
szegedy, m., 162

tag, 314, 347
tail, 392
tail length, 143, 395
tan, p.-n., 19
target, 386
target page, 188
tarjan, r.e., 393
task, 23
taxation, 170, 173, 188, 193
taylor expansion, 13
taylor, m., 305
telephone call, 346
teleport set, 184, 185, 190, 377
teleportation, 174
tendril, 169
term, 164
term frequency, 8, see tf.idf
term spam, 164, 187
test set, 444, 451
tf, see term frequency
tf.idf, 8, 313, 443
theobald, m., 129
thrashing, 179, 218
threshold, 89, 159, 202, 228, 232, 447,

453

timestamp, 151, 272
toivonen   s algorithm, 230
toivonen, h., 238
tomkins, a., 18, 71, 200, 403
tong, h., 403
topic-sensitive id95, 183, 190
toscher, a., 341
total information awareness, 5
touching the void, 311
training example, 440
training rate, 451
training set, 439, 440, 446, 456
transaction, see basket
transition matrix, 377
transition matrix of the web, 166, 177,

178, 180, 405

transitive closure, 43, 389
transitive reduction, 393
transpose, 193
transposition, 98
tree, 248, 266, 267, see decision tree
triangle, 380
triangle inequality, 93
triangular matrix, 211, 220
tripartite graph, 347
triples method, 211, 220
trustrank, 190
trustworthy page, 190
tsourakakis, c.e., 403
tube, 170
tuple, 33
tuzhilin, a., 340
twitter, 17, 301, 344

ullman, j.d., 18, 70, 71, 238, 280, 402
undirected graph, see graph
union, 33, 36, 40, 77
unit vector, 406, 411
universal set, 118
unsupervised learning, 439
user, 308, 324, 325
user pro   le, 316
utility matrix, 308, 311, 328, 405
uv-decomposition, 328, 338, 405, 471

tia, see total information awareness

va    le, 478

index

495

yu, j.x., 129
yu, p.s., 238
yu, y., 71

zhang, h., 437
zhang, t., 280
zipf   s law, 15, see power law
zoeter, o., 305

valduriez, p., 403
van loan, c.f., 436
vapnik, v.n., 483
variable, 146
vassilvitskii, s., 403
vazirani, u., 306
vazirani, v., 306
vector, 31, 93, 97, 167, 177, 192, 193,

242

vigna, s., 402
vitter, j., 162
volume (of a set of nodes), 363
von ahn, l., 315, 341
von luxburg, u., 403
voronoi diagram, 473

wall, m.e., 437
wall-clock time, 49
wallach, d.a., 70
wang, j., 338
wang, w., 129
weak edge, 346
weaver, d., 70
web structure, 169
weight, 447
weiner, j., 18, 200
whizbang labs, 2
widom, j., 18, 71, 162, 280, 403
wikipedia, 346, 446
window, see sliding window, see de-

caying window

windows, 12
winnow algorithm, 451
word, 205, 242, 313
word count, 26
worker process, 28
work   ow, 41, 43, 47
working store, 132

xiao, c., 129
xie, y., 437

yahoo, 291, 314
yang, j., 404
yerneni, r., 70
york, j., 341

