review of id203 theory

arian maleki and tom do

stanford university

id203 theory is the study of uncertainty. through this class, we will be relying on concepts
from id203 theory for deriving machine learning algorithms. these notes attempt to cover the
basics of id203 theory at a level appropriate for cs 229. the mathematical theory of id203
is very sophisticated, and delves into a branch of analysis known as measure theory. in these notes,
we provide a basic treatment of id203 that does not address these    ner details.

1 elements of id203

in order to de   ne a id203 on a set we need a few basic elements,

    sample space    : the set of all the outcomes of a random experiment. here, each outcome
           can be thought of as a complete description of the state of the real world at the end
of the experiment.
    set of events (or event space) f: a set whose elements a     f (called events) are subsets
of     (i.e., a         is a collection of possible outcomes of an experiment).1.
    id203 measure: a function p : f     r that satis   es the following properties,

- p (a)     0, for all a     f
- p (   ) = 1
- if a1, a2, . . . are disjoint events (i.e., ai     aj =     whenever i (cid:54)= j), then

(cid:88)

p (   iai) =

p (ai)

i
these three properties are called the axioms of id203.
example: consider the event of tossing a six-sided die. the sample space is     = {1, 2, 3, 4, 5, 6}.
we can de   ne different event spaces on this sample space. for example, the simplest event space
is the trivial event space f = {   ,    }. another event space is the set of all subsets of    . for the
   rst event space, the unique id203 measure satisfying the requirements above is given by
p (   ) = 0, p (   ) = 1. for the second event space, one valid id203 measure is to assign the
id203 of each set in the event space to be i
6 where i is the number of elements of that set; for
6 and p ({1, 2, 3}) = 3
example, p ({1, 2, 3, 4}) = 4
6.

properties:
- if a     b =    p (a)     p (b).
- p (a     b)     min(p (a), p (b)).
- (union bound) p (a     b)     p (a) + p (b).
- p (    \ a) = 1     p (a).
- (law of total id203) if a1, . . . , ak are a set of disjoint events such that    k

(cid:80)k

i=1 p (ak) = 1.

i=1ai =    , then

1f should satisfy three properties: (1)         f; (2) a     f =        \ a     f; and (3) a1, a2, . . .     f =   

   iai     f.

1

1.1 id155 and independence

let b be an event with non-zero id203. the id155 of any event a given b is
de   ned as,

p (a|b) (cid:44) p (a     b)

p (b)

in other words, p (a|b) is the id203 measure of the event a after observing the occurrence of
event b. two events are called independent if and only if p (a    b) = p (a)p (b) (or equivalently,
p (a|b) = p (a)). therefore, independence is equivalent to saying that observing b does not have
any effect on the id203 of a.

2 random variables

consider an experiment in which we    ip 10 coins, and we want to know the number of coins that
come up heads. here, the elements of the sample space     are 10-length sequences of heads and
tails. for example, we might have w0 = (cid:104)h, h, t, h, t, h, h, t, t, t(cid:105)        . however, in practice,
we usually do not care about the id203 of obtaining any particular sequence of heads and tails.
instead we usually care about real-valued functions of outcomes, such as the number of heads that
appear among our 10 tosses, or the length of the longest run of tails. these functions, under some
technical conditions, are known as random variables.
more formally, a random variable x is a function x :            r.2 typically, we will denote random
variables using upper case letters x(  ) or more simply x (where the dependence on the random
outcome    is implied). we will denote the value that a random variable may take on using lower
case letters x.
example: in our experiment above, suppose that x(  ) is the number of heads which occur in the
sequence of tosses   . given that only 10 coins are tossed, x(  ) can take only a    nite number of
values, so it is known as a discrete random variable. here, the id203 of the set associated
with a random variable x taking on some speci   c value k is

p (x = k) := p ({   : x(  ) = k}).

example: suppose that x(  ) is a random variable indicating the amount of time it takes for a
radioactive particle to decay. in this case, x(  ) takes on a in   nite number of possible values, so it is
called a continuous random variable. we denote the id203 that x takes on a value between
two real constants a and b (where a < b) as

p (a     x     b) := p ({   : a     x(  )     b}).

2.1 cumulative distribution functions

in order to specify the id203 measures used when dealing with random variables, it is often
convenient to specify alternative functions (cdfs, pdfs, and pmfs) from which the id203
measure governing an experiment immediately follows. in this section and the next two sections,
we describe each of these types of functions in turn.
a cumulative distribution function (cdf) is a function fx : r     [0, 1] which speci   es a proba-
bility measure as,

(1)
by using this function one can calculate the id203 of any event in f.3 figure ?? shows a
sample cdf function.
properties:

fx (x) (cid:44) p (x     x).

2technically speaking, not every function is not acceptable as a random variable. from a measure-theoretic
perspective, random variables must be borel-measurable functions.
intuitively, this restriction ensures that
given a random variable and its underlying outcome space, one can implicitly de   ne the each of the events
of the event space as being sets of outcomes            for which x(  ) satis   es some property (e.g., the event
{   : x(  )     3}).

3this is a remarkable fact and is actually a theorem that is proved in more advanced courses.

2

figure 1: a cumulative distribution function (cdf).

- 0     fx (x)     1.
- limx          fx (x) = 0.
- limx       fx (x) = 1.
- x     y =    fx (x)     fx (y).

2.2 id203 mass functions

when a random variable x takes on a    nite set of possible values (i.e., x is a discrete random
variable), a simpler way to represent the id203 measure associated with a random variable is
to directly specify the id203 of each value that the random variable can assume. in particular,
a id203 mass function (pmf) is a function px :         r such that

px (x) (cid:44) p (x = x).

in the case of discrete random variable, we use the notation v al(x) for the set of possible values
that the random variable x may assume. for example, if x(  ) is a random variable indicating the
number of heads out of ten tosses of coin, then v al(x) = {0, 1, 2, . . . , 10}.
properties:
- 0     px (x)     1.

- (cid:80)
- (cid:80)

x   v al(x) px (x) = 1.
x   a px (x) = p (x     a).

2.3 id203 density functions

for some continuous random variables, the cumulative distribution function fx (x) is differentiable
everywhere. in these cases, we de   ne the id203 density function or pdf as the derivative
of the cdf, i.e.,

fx (x) (cid:44) dfx (x)

dx

.

(2)

note here, that the pdf for a continuous random variable may not always exist (i.e., if fx (x) is not
differentiable everywhere).
according to the properties of differentiation, for very small    x,

p (x     x     x +    x)     fx (x)   x.

(3)

both cdfs and pdfs (when they exist!) can be used for calculating the probabilities of different
events. but it should be emphasized that the value of pdf at any given point x is not the id203

3

of that event, i.e., fx (x) (cid:54)= p (x = x). for example, fx (x) can take on values larger than one (but
the integral of fx (x) over any subset of r will be at most one).
properties:
- fx (x)     0 .

- (cid:82)    
- (cid:82)

       fx (x) = 1.
x   a fx (x)dx = p (x     a).

2.4 expectation
suppose that x is a discrete random variable with pmf px (x) and g : r        r is an arbitrary
function. in this case, g(x) can be considered a random variable, and we de   ne the expectation or
expected value of g(x) as

e[g(x)] (cid:44) (cid:88)
(cid:90)    

e[g(x)] (cid:44)

      

x   v al(x)

g(x)px (x).

g(x)fx (x)dx.

if x is a continuous random variable with pdf fx (x), then the expected value of g(x) is de   ned
as,

intuitively, the expectation of g(x) can be thought of as a    weighted average    of the values that
g(x) can taken on for different values of x, where the weights are given by px (x) or fx (x). as
a special case of the above, note that the expectation, e[x] of a random variable itself is found by
letting g(x) = x; this is also known as the mean of the random variable x.
properties:
- e[a] = a for any constant a     r.
- e[af (x)] = ae[f (x)] for any constant a     r.
- (linearity of expectation) e[f (x) + g(x)] = e[f (x)] + e[g(x)].
- for a discrete random variable x, e[1{x = k}] = p (x = k).

2.5 variance

the variance of a random variable x is a measure of how concentrated the distribution of a random
variable x is around its mean. formally, the variance of a random variable x is de   ned as

v ar[x] (cid:44) e[(x     e(x))2]

using the properties in the previous section, we can derive an alternate expression for the variance:

e[(x     e[x])2] = e[x 2     2e[x]x + e[x]2]

= e[x 2]     2e[x]e[x] + e[x]2
= e[x 2]     e[x]2,

where the second equality follows from linearity of expectations and the fact that e[x] is actually a
constant with respect to the outer expectation.
properties:
- v ar[a] = 0 for any constant a     r.
- v ar[af (x)] = a2v ar[f (x)] for any constant a     r.
example calculate the mean and the variance of the uniform random variable x with pdf fx (x) =
1,    x     [0, 1], 0 elsewhere.

e[x] =

xfx (x)dx =

(cid:90)    

      

(cid:90) 1

xdx =

1
2

.

0

4

(cid:90)    

      

e[x 2] =

(cid:90) 1

x2fx (x)dx =

v ar[x] = e[x 2]     e[x]2 =

x2dx =

1
3

.

    1
4

=

1
12

.

0

1
3

(cid:88)

x   a

(cid:90)

x   a

example: suppose that g(x) = 1{x     a} for some subset a        . what is e[g(x)]?
discrete case:

1{x     a}px (x)dx =

px (x)dx = p (x     a).

e[g(x)] =

x   v al(x)

continuous case:

e[g(x)] =

1{x     a}fx (x)dx =

fx (x)dx = p (x     a).

(cid:88)
(cid:90)    

      

2.6 some common random variables

discrete random variables

    x     bernoulli(p) (where 0     p     1): one if a coin with heads id203 p comes up

heads, zero otherwise.

(cid:26)p

(cid:18)n

(cid:19)

x

p(x) =

if p = 1
1     p if p = 0

p(x) =

px(1     p)n   x

    x     binomial(n, p) (where 0     p     1): the number of heads in n independent    ips of a

coin with heads id203 p.

    x     geometric(p) (where p > 0): the number of    ips of a coin with heads id203 p

until the    rst heads.

p(x) = p(1     p)x   1

    x     p oisson(  ) (where    > 0): a id203 distribution over the nonnegative integers

used for modeling the frequency of rare events.

continuous random variables

p(x) = e        x
x!

    x     u nif orm(a, b) (where a < b): equal id203 density to every value between a

and b on the real line.

    x     exponential(  ) (where    > 0): decaying id203 density over the nonnegative

reals.

(cid:40) 1

b   a
0

(cid:26)  e     x

0

f (x) =

f (x) =

if a     x     b
otherwise

if x     0
otherwise

    x     n ormal(  ,   2): also known as the gaussian distribution

f (x) =

1   
2    

e

2  2 (x     )2
    1

5

figure 2: pdf and cdf of a couple of random variables.

the shape of the pdfs and cdfs of some of these random variables are shown in figure ??.
the following table is the summary of some of the properties of these distributions.

distribution

bernoulli(p)

binomial(n, p)
geometric(p)
p oisson(  )
u nif orm(a, b)

gaussian(  ,   2)
exponential(  )

pdf or pmf

(cid:26) p,
(cid:1) pk(1     p)n   k for 0     k     n np
(cid:0)n

if x = 1
if x = 0.

1     p,

for k = 1, 2, . . .

for k = 1, 2, . . .

1

k

p(1     p)k   1
e       x/x!
b   a    x     (a, b)
    (x     )2
   
1
2  2
  e     x x     0,    > 0
2  

e

  

p

mean variance
p(1     p)
npq
1   p
p2
  
(b   a)2

1
p
  
a+b

2

  
1
  

12

  2
1
  2

3 two random variables

in many situations, however,
thus far, we have considered single random variables.
there may be more than one quantity that we are interested in knowing during a ran-
dom experiment.
in an experiment where we    ip a coin ten times, we
may care about both x(  ) = the number of heads that come up as well as y (  ) =
the length of the longest run of consecutive heads. in this section, we consider the setting of two
random variables.

for instance,

3.1

joint and marginal distributions

suppose that we have two random variables x and y . one way to work with these two random
variables is to consider each of them separately. if we do that we will only need fx (x) and fy (y).
but if we want to know about the values that x and y assume simultaneously during outcomes
of a random experiment, we require a more complicated structure known as the joint cumulative
distribution function of x and y , de   ned by

fxy (x, y) = p (x     x, y     y)

it can be shown that by knowing the joint cumulative distribution function, the id203 of any
event involving x and y can be calculated.

6

the joint cdf fxy (x, y) and the joint distribution functions fx (x) and fy (y) of each variable
separately are related by

fx (x) = lim

fy (y) = lim

y       fxy (x, y)dy
x       fxy (x, y)dx.

here, we call fx (x) and fy (y) the marginal cumulative distribution functions of fxy (x, y).
properties:
- 0     fxy (x, y)     1.
- limx,y       fxy (x, y) = 1.
- limx,y          fxy (x, y) = 0.
- fx (x) = limy       fxy (x, y).

joint and marginal id203 mass functions

3.2
if x and y are discrete random variables, then the joint id203 mass function pxy : r  r    
[0, 1] is de   ned by

pxy (x, y) = p (x = x, y = y).

here, 0     pxy (x, y)     1 for all x, y, and(cid:80)
(cid:88)

(cid:80)

px (x) =

pxy (x, y).

how does the joint pmf over two variables relate to the id203 mass function for each variable
separately? it turns out that

x   v al(x)

y   v al(y ) pxy (x, y) = 1.

and similarly for py (y). in this case, we refer to px (x) as the marginal id203 mass function
of x. in statistics, the process of forming the marginal distribution with respect to one variable by
summing out the other variable is often known as    marginalization.   

y

3.3

joint and marginal id203 density functions

let x and y be two continuous random variables with joint distribution function fxy . in the case
that fxy (x, y) is everywhere differentiable in both x and y, then we can de   ne the joint id203
density function,

fxy (x, y) =

   2fxy (x, y)

   x   y

.

(cid:90)(cid:90)

like in the single-dimensional case, fxy (x, y) (cid:54)= p (x = x, y = y), but rather

fxy (x, y)dxdy = p ((x, y )     a).

may be greater than 1. nonetheless, it must be the case that(cid:82)    

(cid:82)    

note that the values of the id203 density function fxy (x, y) are always nonnegative, but they

      

       fxy (x, y) = 1.

x   a

analagous to the discrete case, we de   ne

fx (x) =

(cid:90)    

      

fxy (x, y)dy,

as the marginal id203 density function (or marginal density) of x, and similarly for fy (y).

7

3.4 conditional distributions

conditional distributions seek to answer the question, what is the id203 distribution over y ,
when we know that x must take on a certain value x? in the discrete case, the id155
mass function of x given y is simply

py |x (y|x) =

pxy (x, y)

px (x)

,

assuming that px (x) (cid:54)= 0.
in the continuous case, the situation is technically a little more complicated because the id203
that a continuous random variable x takes on a speci   c value x is equal to zero4. ignoring this
technical point, we simply de   ne, by analogy to the discrete case, the id155 density
of y given x = x to be

fy |x (y|x) =

fxy (x, y)

fx (x)

,

provided fx (x) (cid:54)= 0.

3.5 bayes   s rule

a useful formula that often arises when trying to derive expression for the id155 of
one variable given another, is bayes   s rule.
in the case of discrete random variables x and y ,

py |x (y|x) =

pxy (x, y)

px (x)

=

if the random variables x and y are continuous,

fy |x (y|x) =

fxy (x, y)

fx (x)

3.6

independence

px|y (x|y)py (y)

(cid:80)
y(cid:48)   v al(y ) px|y (x|y(cid:48))py (y(cid:48))
(cid:82)    
       fx|y (x|y(cid:48))fy (y(cid:48))dy(cid:48) .

fx|y (x|y)fy (y)

=

.

two random variables x and y are independent if fxy (x, y) = fx (x)fy (y) for all values of x
and y. equivalently,

v al(y ).

    for discrete random variables, pxy (x, y) = px (x)py (y) for all x     v al(x), y    
    for discrete random variables, py |x (y|x) = py (y) whenever px (x) (cid:54)= 0 for all y    
    for continuous random variables, fxy (x, y) = fx (x)fy (y) for all x, y     r.
    for continuous random variables, fy |x (y|x) = fy (y) whenever fx (x) (cid:54)= 0 for all y     r.

v al(y ).

4to get around this, a more reasonable way to calculate the conditional cdf is,
p (y     y|x     x     x +    x).

fy |x (y, x) = lim
   x   0

it can be easily seen that if f (x, y) is differentiable in both x, y then,

(cid:90) y

fy |x (y, x) =

fx,y (x,   )

d  

      

fx (x)

and therefore we de   ne the conditional pdf of y given x = x in the following way,

fy |x (y|x) =

fxy (x, y)

fx (x)

8

informally, two random variables x and y are independent if    knowing    the value of one variable
will never have any effect on the id155 distribution of the other variable, that is,
you know all the information about the pair (x, y ) by just knowing f (x) and f (y). the following
lemma formalizes this observation:
lemma 3.1. if x and y are independent then for any subsets a, b     r, we have,

p (x     a, y     b) = p (x     a)p (y     b)

by using the above lemma one can prove that if x is independent of y then any function of x is
independent of any function of y .

3.7 expectation and covariance
suppose that we have two discrete random variables x, y and g : r2        r is a function of these
two random variables. then the expected value of g is de   ned in the following way,

for continuous random variables x, y , the analogous expression is

(cid:88)

e[g(x, y )] (cid:44) (cid:88)
(cid:90)    

e[g(x, y )] =

(cid:90)    

      

      

x   v al(x)

y   v al(y )

g(x, y)pxy (x, y).

g(x, y)fxy (x, y)dxdy.

we can use the concept of expectation to study the relationship of two random variables with each
other. in particular, the covariance of two random variables x and y is de   ned as

cov[x, y ] (cid:44) e[(x     e[x])(y     e[y ])]

using an argument similar to that for variance, we can rewrite this as,

cov[x, y ] = e[(x     e[x])(y     e[y ])]

= e[xy     xe[y ]     y e[x] + e[x]e[y ]]
= e[xy ]     e[x]e[y ]     e[y ]e[x] + e[x]e[y ]]
= e[xy ]     e[x]e[y ].

here, the key step in showing the equality of the two forms of covariance is in the third equality,
where we use the fact that e[x] and e[y ] are actually constants which can be pulled out of the
expectation. when cov[x, y ] = 0, we say that x and y are uncorrelated5.
properties:

- (linearity of expectation) e[f (x, y ) + g(x, y )] = e[f (x, y )] + e[g(x, y )].
- v ar[x + y ] = v ar[x] + v ar[y ] + 2cov[x, y ].
- if x and y are independent, then cov[x, y ] = 0.
- if x and y are independent, then e[f (x)g(y )] = e[f (x)]e[g(y )].

4 multiple random variables

the notions and ideas introduced in the previous section can be generalized to more than
in particular, suppose that we have n continuous random variables,
two random variables.
x1(  ), x2(  ), . . . xn(  ).
in this section, for simplicity of presentation, we focus only on the
continuous case, but the generalization to discrete random variables works similarly.

5however, this is not the same thing as stating that x and y are independent! for example, if x    
u nif orm(   1, 1) and y = x 2, then one can show that x and y are uncorrelated, even though they are not
independent.

9

4.1 basic properties

we can de   ne the joint distribution function of x1, x2, . . . , xn, the joint id203 density
function of x1, x2, . . . , xn, the marginal id203 density function of x1, and the condi-
tional id203 density function of x1 given x2, . . . , xn, as

fx1,x2,...,xn (x1, x2, . . . xn) = p (x1     x1, x2     x2, . . . , xn     xn)

   nfx1,x2,...,xn(x1, x2, . . . xn)

(cid:90)    

(cid:90)    

      

   x1 . . .    xn

      

      
fx1,x2,...,xn(x1, x2, . . . xn)

fx2,...,xn (x1, x2, . . . xn)

fx1,x2,...,xn(x1, x2, . . . xn)dx2 . . . dxn

fx1,x2,...,xn (x1, x2, . . . xn) =

fx1(x1) =
fx1|x2,...,xn (x1|x2, . . . xn) =
(cid:90)

p ((x1, x2, . . . xn)     a) =

to calculate the id203 of an event a     rn we have,

(x1,x2,...xn)   a

fx1,x2,...,xn (x1, x2, . . . xn)dx1dx2 . . . dxn

(4)

chain rule: from the de   nition of conditional probabilities for multiple random variables, one can
show that

f (x1, x2, . . . , xn) = f (xn|x1, x2 . . . , xn   1)f (x1, x2 . . . , xn   1)

= f (xn|x1, x2 . . . , xn   1)f (xn   1|x1, x2 . . . , xn   2)f (x1, x2 . . . , xn   2)

n(cid:89)

= . . . = f (x1)

f (xi|x1, . . . , xi   1).

i=2

independence: for multiple events, a1, . . . , ak, we say that a1, . . . , ak are mutually indepen-
dent if for any subset s     {1, 2, . . . , k}, we have
p (   i   sai) =

(cid:89)

p (ai).

i   s

likewise, we say that random variables x1, . . . , xn are independent if

f (x1, . . . , xn) = f (x1)f (x2)       f (xn).

here, the de   nition of mutual independence is simply the natural generalization of independence of
two random variables to multiple random variables.
independent random variables arise often in machine learning algorithms where we assume that the
training examples belonging to the training set represent independent samples from some unknown
id203 distribution. to make the signi   cance of independence clear, consider a    bad    training
set in which we    rst sample a single training example (x(1), y(1)) from the some unknown distribu-
tion, and then add m     1 copies of the exact same training example to the training set. in this case,
we have (with some abuse of notation)

p ((x(1), y(1)), . . . .(x(m), y(m))) (cid:54)=

p (x(i), y(i)).

despite the fact that the training set has size m, the examples are not independent! while clearly the
procedure described here is not a sensible method for building a training set for a machine learning
algorithm, it turns out that in practice, non-independence of samples does come up often, and it has
the effect of reducing the    effective size    of the training set.

i=1

10

m(cid:89)

4.2 random vectors

suppose that we have n random variables. when working with all these random variables together,
we will often    nd it convenient to put them in a vector x = [x1 x2 . . . xn]t . we call the resulting
vector a random vector (more formally, a random vector is a mapping from     to rn). it should be
clear that random vectors are simply an alternative notation for dealing with n random variables, so
the notions of joint pdf and cdf will apply to random vectors as well.
expectation: consider an arbitrary function from g : rn     r. the expected value of this function
is de   ned as

where(cid:82)

(5)
e[g(x)] =
rn is n consecutive integrations from        to    . if g is a function from rn to rm, then the

g(x1, x2, . . . , xn)fx1,x2,...,xn (x1, x2, . . . xn)dx1dx2 . . . dxn,

rn

expected value of g is the element-wise expected values of the output vector, i.e., if g is

(cid:90)

             .

...

g2(x)

gm(x)

e[g2(x)]

             ,
             g1(x)
             e[g1(x)]
         

e[gm(x)]

...

then,

g(x) =

e[g(x)] =

covariance matrix: for a given random vector x :         rn, its covariance matrix    is the n    n
square matrix whose entries are given by   ij = cov[xi, xj].
from the de   nition of covariance, we have

       cov[x1, xn]
...
       cov[xn, xn]

...

...

cov[xn, x1]

         cov[x1, x1]
          e[x 2
          e[x 2

1 ]

...

1 ]     e[x1]e[x1]

e[xnx1]     e[xn]e[x1]
       e[x1xn]
...
      

...
e[x 2
n]

e[xnx1]

...

   =

=

=

         

       e[x1xn]     e[x1]e[xn]
...
      

n]     e[xn]e[xn]

...

             

e[x 2

         e[x1]e[x1]

...

e[xn]e[x1]

       e[x1]e[xn]
...
       e[xn]e[xn]

...

         

= e[xx t ]     e[x]e[x]t = . . . = e[(x     e[x])(x     e[x])t ].

where the matrix expectation is de   ned in the obvious way.
the covariance matrix has a number of useful properties:
-    (cid:23) 0; that is,    is positive semide   nite.
-    =   t ; that is,    is symmetric.

4.3 the multivariate gaussian distribution

one particularly important example of a id203 distribution over random vectors x is called
the multivariate gaussian or multivariate normal distribution. a random vector x     rn is said
to have a multivariate normal (or gaussian) distribution with mean        rn and covariance matrix
       sn

++ refers to the space of symmetric positive de   nite n    n matrices)

++ (where sn

(cid:18)

    1
2

(cid:19)

(x       )t      1(x       )

.

fx1,x2,...,xn (x1, x2, . . . , xn;   ,   ) =

1

(2  )n/2|  |1/2

exp

11

we write this as x     n (  ,   ). notice that in the case n = 1, this reduces the regular de   nition of
a normal distribution with mean parameter   1 and variance   11.
generally speaking, gaussian random variables are extremely useful in machine learning and statis-
tics for two main reasons. first, they are extremely common when modeling    noise    in statistical
algorithms. quite often, noise can be considered to be the accumulation of a large number of small
independent random perturbations affecting the measurement process; by the central limit theo-
rem, summations of independent random variables will tend to    look gaussian.    second, gaussian
random variables are convenient for many analytical manipulations, because many of the integrals
involving gaussian distributions that arise in practice have simple closed form solutions. we will
encounter this later in the course.

5 other resources

a good textbook on probablity at the level needed for cs229 is the book, a first course on proba-
bility by sheldon ross.

12

