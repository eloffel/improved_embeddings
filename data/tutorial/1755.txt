introduction to id20

guest lecturer: ming-wei chang

cs 546

spring, 2009

guest lecturer: ming-wei chang cs 546

() introduction to id20 1 / 33

spring, 2009

1 / 33

before we start

acknowledgment

we use slides from (jiang and zhai 2007),(daum  e iii 2007) and
(blitzer, mcdonald, and pereira 2006) extensively.

therefore, the pages number are sometimes o   .

guest lecturer: ming-wei chang cs 546

() introduction to id20 2 / 33

spring, 2009

2 / 33

before we start

acknowledgment

we use slides from (jiang and zhai 2007),(daum  e iii 2007) and
(blitzer, mcdonald, and pereira 2006) extensively.

therefore, the pages number are sometimes o   .

please ask questions....

things get interesting only after we
understand something.
we do not need to go over all of the
slides.

(cid:73) but i will try to cover the most

important points.

guest lecturer: ming-wei chang cs 546

() introduction to id20 2 / 33

spring, 2009

2 / 33

the    rst step . . .

the de   nition of id20

guest lecturer: ming-wei chang cs 546

() introduction to id20 3 / 33

spring, 2009

3 / 33

natural language processing

the ultimate goal:
build systems that can understand natural language.

guest lecturer: ming-wei chang cs 546

() introduction to id20 4 / 33

spring, 2009

4 / 33

natural language processing

the ultimate goal:
build systems that can understand natural language.

unfortunately, not easy.

guest lecturer: ming-wei chang cs 546

() introduction to id20 4 / 33

spring, 2009

4 / 33

natural language processing

the ultimate goal:
build systems that can understand natural language.

unfortunately, not easy.

intermediate goals:
build systems can do parsing, tagging, . . . well

guest lecturer: ming-wei chang cs 546

() introduction to id20 4 / 33

spring, 2009

4 / 33

natural language processing

the ultimate goal:
build systems that can understand natural language.

unfortunately, not easy.

intermediate goals:
build systems can do parsing, tagging, . . . well

the current tools: machine learning algorithms

guest lecturer: ming-wei chang cs 546

() introduction to id20 4 / 33

spring, 2009

4 / 33

natural language processing

the ultimate goal:
build systems that can understand natural language.

unfortunately, not easy.

intermediate goals:
build systems can do parsing, tagging, . . . well

the current tools: machine learning algorithms

training data

machine
learning
model

testing data

guest lecturer: ming-wei chang cs 546

() introduction to id20 4 / 33

spring, 2009

4 / 33

example: id39 (ner)

input: jim bought 300 shares of acme corp. in 2006.

guest lecturer: ming-wei chang cs 546

() introduction to id20 5 / 33

spring, 2009

5 / 33

example: id39 (ner)

input: jim bought 300 shares of acme corp. in 2006.

output: [per jim ] bought 300 shares of [org acme corp.] in 2006.

guest lecturer: ming-wei chang cs 546

() introduction to id20 5 / 33

spring, 2009

5 / 33

example: id39 (ner)

input: jim bought 300 shares of acme corp. in 2006.

output: [per jim ] bought 300 shares of [org acme corp.] in 2006.

it is often ambiguous... for example, bush can be a person...

guest lecturer: ming-wei chang cs 546

() introduction to id20 5 / 33

spring, 2009

5 / 33

example: id39 (ner)

input: jim bought 300 shares of acme corp. in 2006.

output: [per jim ] bought 300 shares of [org acme corp.] in 2006.

it is often ambiguous... for example, bush can be a person...

. . . or not.

guest lecturer: ming-wei chang cs 546

() introduction to id20 5 / 33

spring, 2009

5 / 33

how to solve ner

example

b-per o
jim

bought

o o

300

shares

o b-org i-org o
of acme
in

corp.

o
2006

guest lecturer: ming-wei chang cs 546

() introduction to id20 6 / 33

spring, 2009

6 / 33

how to solve ner

example

b-per o
jim

bought

o o

300

shares

o b-org i-org o
of acme
in

corp.

o
2006

feature vectors

for the word    acme   , the feature vector

1 current word and its part of speech tag: acme, nnp
2 two words before the current word
3 two words after the current word

guest lecturer: ming-wei chang cs 546

() introduction to id20 6 / 33

spring, 2009

6 / 33

how to solve ner

example

b-per o
jim

bought

o o

300

shares

o b-org i-org o
of acme
in

corp.

o
2006

feature vectors

for the word    acme   , the feature vector

1 current word and its part of speech tag: acme, nnp
2 two words before the current word
3 two words after the current word

training and testing

a multi-class classi   cation problem: id28, id166

guest lecturer: ming-wei chang cs 546

() introduction to id20 6 / 33

spring, 2009

6 / 33

the current status of ner

quote from wikipedia
   state-of-the-art ner systems produce near-human performance. for
example, the best system entering muc-7 scored 93.39% of f-measure
while human annotators scored 97.60% and 96.95%   

guest lecturer: ming-wei chang cs 546

() introduction to id20 7 / 33

spring, 2009

7 / 33

the current status of ner

quote from wikipedia
   state-of-the-art ner systems produce near-human performance. for
example, the best system entering muc-7 scored 93.39% of f-measure
while human annotators scored 97.60% and 96.95%   

wow, that is so cool! at the end, we    nally solved something!

guest lecturer: ming-wei chang cs 546

() introduction to id20 7 / 33

spring, 2009

7 / 33

the current status of ner

quote from wikipedia
   state-of-the-art ner systems produce near-human performance. for
example, the best system entering muc-7 scored 93.39% of f-measure
while human annotators scored 97.60% and 96.95%   

wow, that is so cool! at the end, we    nally solved something!

truth:the ner problem is still not solved. why?

guest lecturer: ming-wei chang cs 546

() introduction to id20 7 / 33

spring, 2009

7 / 33

the problem: domain over-   tting

the issues of supervised machine learning algorithms:
need labeled data

what people have done: labeled large amount of data on news corpus

however, it is still not enough.....
the web contains all kind of data....

(cid:73) blogs, novels, biomedical documents, . . .
(cid:73) many domains!

we might do a good job on news domain, but not on other domains...

guest lecturer: ming-wei chang cs 546

() introduction to id20 8 / 33

spring, 2009

8 / 33

2id20   many nlp tasks are cast into classification problems   lack of training data in new domains   id20:   pos: wsj     biomedical text   ner: news     blog, speech   spam filtering: public email corpus     personal inboxes   domain overfitting0.281fly     mouse0.541mouse     mouseto find gene/protein from biomedical literature0.641reuters     nyt0.855nyt     nytto find per, loc, org from news textf1train     testner taskpossible solutions?

don   t care (the current solution)

(cid:73) bad performance

guest lecturer: ming-wei chang cs 546

() introduction to id20 9 / 33

spring, 2009

9 / 33

possible solutions?

don   t care (the current solution)

(cid:73) bad performance
annotate more data

(cid:73) annotate data for the new domain?
(cid:73) need create data for each new domain

guest lecturer: ming-wei chang cs 546

() introduction to id20 9 / 33

spring, 2009

9 / 33

possible solutions?

don   t care (the current solution)

(cid:73) bad performance
annotate more data

(cid:73) annotate data for the new domain?
(cid:73) need create data for each new domain

build a generic corpus?

(cid:73) wikipedia
(cid:73) good, still not cover all possible solutions. for example, ner for a

company.

(cid:73) not sure about the performance

guest lecturer: ming-wei chang cs 546

() introduction to id20 9 / 33

spring, 2009

9 / 33

possible solutions?

don   t care (the current solution)

(cid:73) bad performance
annotate more data

(cid:73) annotate data for the new domain?
(cid:73) need create data for each new domain

build a generic corpus?

(cid:73) wikipedia
(cid:73) good, still not cover all possible solutions. for example, ner for a

company.

(cid:73) not sure about the performance

special design algorithms (for each domain)

(cid:73) good, but need to redesign for every domain

guest lecturer: ming-wei chang cs 546

() introduction to id20 9 / 33

spring, 2009

9 / 33

possible solutions?

don   t care (the current solution)

(cid:73) bad performance
annotate more data

(cid:73) annotate data for the new domain?
(cid:73) need create data for each new domain

build a generic corpus?

(cid:73) wikipedia
(cid:73) good, still not cover all possible solutions. for example, ner for a

company.

(cid:73) not sure about the performance

special design algorithms (for each domain)

(cid:73) good, but need to redesign for every domain

our focus: general purpose adaptation algorithms

guest lecturer: ming-wei chang cs 546

() introduction to id20 9 / 33

spring, 2009

9 / 33

why does the performance drop?

di   erent distributions

p(x): the distribution of training and testing data are di   erent
p(y|x): with the same example, the label are di   erent in di   erent
domains

guest lecturer: ming-wei chang cs 546

() introduction to id20 10 / 33

spring, 2009

10 / 33

why does the performance drop?

di   erent distributions

p(x): the distribution of training and testing data are di   erent
p(y|x): with the same example, the label are di   erent in di   erent
domains

unknown words

there are many unseen words in the new domain

guest lecturer: ming-wei chang cs 546

() introduction to id20 10 / 33

spring, 2009

10 / 33

why does the performance drop?

di   erent distributions

p(x): the distribution of training and testing data are di   erent
p(y|x): with the same example, the label are di   erent in di   erent
domains

unknown words

there are many unseen words in the new domain

new types

there are some new types in the new domain. for example, now
predicting locations.

we will not talk about this today.

guest lecturer: ming-wei chang cs 546

() introduction to id20 10 / 33

spring, 2009

10 / 33

what we are going to talk about today

we will talk about several previous works

cover several issues that mention in the previous slides

reminder: it is still an open problem. there might exist better
solutions.
terminology:

(cid:73) source domain, the domain we know a lot
(cid:73) target domain, the domain we do not know (or know very little)
(cid:73) we want to evaluate on target domain

guest lecturer: ming-wei chang cs 546

() introduction to id20 11 / 33

spring, 2009

11 / 33

recap: training a standard id28

training a standard id28 model: a review

training data: l
training procedure:    nd the best w by maximizing p(w|l)

p(w|l) =

p(l|w )p(w )

p(l)

    p(l|w ) p(w)

the    st term: training error , the second term: id173 term

w     arg max log p(l|w ) + log p(w )

guest lecturer: ming-wei chang cs 546

() introduction to id20 12 / 33

spring, 2009

12 / 33

recap: training a standard id28

training a standard id28 model: a review

training data: l
training procedure:    nd the best w by maximizing p(w|l)

p(w|l) =

p(l|w )p(w )

p(l)

    p(l|w ) p(w)

the    st term: training error , the second term: id173 term

w     arg max log p(l|w ) + log p(w )

id173

assume the prior is the gaussian distribution
log p(w ) = 1
similar to id166, want to    nd the maximum margin line.

2  2(cid:107)w(cid:107)2 + c

guest lecturer: ming-wei chang cs 546

() introduction to id20 12 / 33

spring, 2009

12 / 33

outline

di   erent distributions: p(x )

solution: instance weighting

(bickel, br  ueckner, and sche   er 2007), (jiang and zhai 2007)

guest lecturer: ming-wei chang cs 546

() introduction to id20 13 / 33

spring, 2009

13 / 33

outline

di   erent distributions: p(x )

solution: instance weighting

(bickel, br  ueckner, and sche   er 2007), (jiang and zhai 2007)

di   erent distributions: p(y | x )

assume ps (y | x ) is close to pt(y | x )
solution: id173

(evgeniou and pontil 2004), (daum  e iii 2007)

guest lecturer: ming-wei chang cs 546

() introduction to id20 13 / 33

spring, 2009

13 / 33

outline

di   erent distributions: p(x )

solution: instance weighting

(bickel, br  ueckner, and sche   er 2007), (jiang and zhai 2007)

di   erent distributions: p(y | x )

assume ps (y | x ) is close to pt(y | x )
solution: id173

(evgeniou and pontil 2004), (daum  e iii 2007)

unknown words

solution: find the relations between old words and unseen words
through auxiliary tasks

(ando and zhang 2005), (blitzer, mcdonald, and pereira 2006)

guest lecturer: ming-wei chang cs 546

() introduction to id20 13 / 33

spring, 2009

13 / 33

outline

di   erent distributions: p(x )

solution: instance weighting

(bickel, br  ueckner, and sche   er 2007), (jiang and zhai 2007)

di   erent distributions: p(y | x )

assume ps (y | x ) is close to pt(y | x )
solution: id173

(evgeniou and pontil 2004), (daum  e iii 2007)

unknown words

solution: find the relations between old words and unseen words
through auxiliary tasks

(ando and zhang 2005), (blitzer, mcdonald, and pereira 2006)

guest lecturer: ming-wei chang cs 546

() introduction to id20 14 / 33

spring, 2009

14 / 33

why doing instance weighting?

what we really want

training and testing distributions can be di   erent:   ,   

minimize the expected loss on testing data

find a function f that minimize

e(x,y )     [loss(f (x), y )]

guest lecturer: ming-wei chang cs 546

() introduction to id20 15 / 33

spring, 2009

15 / 33

why doing instance weighting?

what we really want

training and testing distributions can be di   erent:   ,   

minimize the expected loss on testing data

find a function f that minimize

e(x,y )     [loss(f (x), y )]

intuitions: a good weighting algorithm should . . .

put more weights on the training examples that are similar to testing
examples

put less weights on the training examples that are not similar to
testing examples

guest lecturer: ming-wei chang cs 546

() introduction to id20 15 / 33

spring, 2009

15 / 33

5the need for id20source domaintarget domain6the need for id20source domaintarget domain11an instance weighting solution(instance adaptation: pt(x) < ps(x))source domaintarget domainpt(x) < ps(x)remove/demote instances12an instance weighting solution(instance adaptation: pt(x) < ps(x))source domaintarget domainpt(x) < ps(x)remove/demote instances13an instance weighting solution(instance adaptation: pt(x) < ps(x))source domaintarget domainpt(x) < ps(x)remove/demote instances14an instance weighting solution(instance adaptation: pt(x) > ps(x))source domaintarget domainpt(x) > ps(x)promote instances15an instance weighting solution(instance adaptation: pt(x) > ps(x))source domaintarget domainpt(x) > ps(x)promote instances16an instance weighting solution(instance adaptation: pt(x) > ps(x))source domaintarget domainpt(x) > ps(x)promote instances17an instance weighting solution(instance adaptation: pt(x) > ps(x))   labeled target domain instances are useful   unlabeled target domain instances may also be usefulsource domaintarget domainpt(x) > ps(x)the problem now: how to    gure out the weights

assumptions: (bickel, br  ueckner, and sche   er 2007)

labeled source data

unlabeled target data

no labeled target data is available

some simple arguments

the weight should be p(x|  )
one can show p(x, y|  ) = p(x, y|  ) p(x|  )
p(x|  )

p(x|  ) (shimodaira 2000)

guest lecturer: ming-wei chang cs 546

() introduction to id20 16 / 33

spring, 2009

16 / 33

the problem now: how to    gure out the weights

assumptions: (bickel, br  ueckner, and sche   er 2007)

labeled source data

unlabeled target data

no labeled target data is available

some simple arguments

the weight should be p(x|  )
one can show p(x, y|  ) = p(x, y|  ) p(x|  )
p(x|  )
the idea: learn another model to estimate p(x|  )
p(x|  )

p(x|  ) (shimodaira 2000)

guest lecturer: ming-wei chang cs 546

() introduction to id20 16 / 33

spring, 2009

16 / 33

figuring out the weights

an additional model

learn a model to predict if the example are coming from training data
or testing data
p(  |x,   ,   ). training:    = 1, testing:    = 0.

guest lecturer: ming-wei chang cs 546

() introduction to id20 17 / 33

spring, 2009

17 / 33

figuring out the weights

an additional model

learn a model to predict if the example are coming from training data
or testing data
p(  |x,   ,   ). training:    = 1, testing:    = 0.
using bayes rules, it is fairly easy to show

p(x|  ) = p(x|   = 1,   ,   ) =

p(   = 1|x,   ,   )p(x|  ,   )

p(   = 1|  ,   )

guest lecturer: ming-wei chang cs 546

() introduction to id20 17 / 33

spring, 2009

17 / 33

figuring out the weights

an additional model

learn a model to predict if the example are coming from training data
or testing data
p(  |x,   ,   ). training:    = 1, testing:    = 0.
using bayes rules, it is fairly easy to show

p(x|  ) = p(x|   = 1,   ,   ) =

p(   = 1|x,   ,   )p(x|  ,   )

p(   = 1|  ,   )

and,

p(x|  )
p(x|  )

p(   = 1|  ,   )p(   = 0|x,   ,   )
p(   = 0|  ,   )p(   = 1|x,   ,   )

=

guest lecturer: ming-wei chang cs 546

() introduction to id20 17 / 33

spring, 2009

17 / 33

what does it mean?

the equation revisited
p(x|  ) = p(  =1|  ,  )
p(x|  )
p(  =0|  ,  )

p(  =0|x,  ,  )
p(  =1|x,  ,  )

guest lecturer: ming-wei chang cs 546

() introduction to id20 18 / 33

spring, 2009

18 / 33

what does it mean?

the equation revisited
p(x|  ) = p(  =1|  ,  )
p(x|  )
p(  =0|  ,  )

p(  =0|x,  ,  )
p(  =1|x,  ,  )

the first term : just calculate the frequency!

the second term : the con   dence from the additional classi   er

guest lecturer: ming-wei chang cs 546

() introduction to id20 18 / 33

spring, 2009

18 / 33

what does it mean?

the equation revisited
p(x|  ) = p(  =1|  ,  )
p(x|  )
p(  =0|  ,  )

p(  =0|x,  ,  )
p(  =1|x,  ,  )

the first term : just calculate the frequency!

the second term : the con   dence from the additional classi   er

algorithm 1: two stages approaches

train a classi   er p(  |x,   ,   )
apply the classi   er on the training instances and get their weight si
minimize the id168 on the training data

(cid:88)

si loss(f (xi ), yi )

i

guest lecturer: ming-wei chang cs 546

() introduction to id20 18 / 33

spring, 2009

18 / 33

an alternative approach: joint learning

training a standard id28 model: a review

training data: l
training procedure:    nd the best w by maximizing p(w|l)

p(w|l) =

p(l|w )p(w )

p(l)

    p(l|w ) p(w)

the    st term: training error , the second term: id173 term

guest lecturer: ming-wei chang cs 546

() introduction to id20 19 / 33

spring, 2009

19 / 33

an alternative approach: joint learning

training a standard id28 model: a review

training data: l
training procedure:    nd the best w by maximizing p(w|l)

p(w|l) =

p(l|w )p(w )

p(l)

    p(l|w ) p(w)

the    st term: training error , the second term: id173 term

the idea: learn two models together

we can learn the classi   cation model (for real task) and the addition
model (for    guring out the weight) together!

guest lecturer: ming-wei chang cs 546

() introduction to id20 19 / 33

spring, 2009

19 / 33

an alternative approach: joint learning (cond.)

training a standard id28 model: a review

max p(w|l)     p(l|w ) p(w)

guest lecturer: ming-wei chang cs 546

() introduction to id20 20 / 33

spring, 2009

20 / 33

an alternative approach: joint learning (cond.)

training a standard id28 model: a review

max p(w|l)     p(l|w ) p(w)

algorithm 2: joint approach

training data l, unlabeled testing data t .

the weight vector for original task w . the weight vector for the
   instance weight    v .

guest lecturer: ming-wei chang cs 546

() introduction to id20 20 / 33

spring, 2009

20 / 33

an alternative approach: joint learning (cond.)

training a standard id28 model: a review

max p(w|l)     p(l|w ) p(w)

algorithm 2: joint approach

training data l, unlabeled testing data t .

the weight vector for original task w . the weight vector for the
   instance weight    v .

p(w , v|l, t ) = p(w|v , l, t )p(v|l, t ) = p(w|v , l)p(v|l, t )

    p(l|v , w ) p(l, t|v ) p(w) p(v)

guest lecturer: ming-wei chang cs 546

() introduction to id20 20 / 33

spring, 2009

20 / 33

an alternative approach: joint learning (cond.)

training a standard id28 model: a review

max p(w|l)     p(l|w ) p(w)

algorithm 2: joint approach

training data l, unlabeled testing data t .

the weight vector for original task w . the weight vector for the
   instance weight    v .

p(w , v|l, t ) = p(w|v , l, t )p(v|l, t ) = p(w|v , l)p(v|l, t )

    p(l|v , w ) p(l, t|v ) p(w) p(v)

weighted training error for w , training error for v

training: use id77 to optimize this function with w and v

guest lecturer: ming-wei chang cs 546

() introduction to id20 20 / 33

spring, 2009

20 / 33

an alternative approach: joint learning (cond.)

training a standard id28 model: a review

max p(w|l)     p(l|w ) p(w)

algorithm 2: joint approach

training data l, unlabeled testing data t .

the weight vector for original task w . the weight vector for the
   instance weight    v .

p(w , v|l, t ) = p(w|v , l, t )p(v|l, t ) = p(w|v , l)p(v|l, t )

    p(l|v , w ) p(l, t|v ) p(w) p(v)

weighted training error for w , training error for v

training: use id77 to optimize this function with w and v

much better than algorithm 1

guest lecturer: ming-wei chang cs 546

() introduction to id20 20 / 33

spring, 2009

20 / 33

using heuristics to    nd weights

the work (jiang and zhai 2007) assumes

(cid:73) source labeled instances
(cid:73) small amount labeled target instances,
(cid:73) large amount labeled target instances

guest lecturer: ming-wei chang cs 546

() introduction to id20 21 / 33

spring, 2009

21 / 33

using heuristics to    nd weights

the work (jiang and zhai 2007) assumes

(cid:73) source labeled instances
(cid:73) small amount labeled target instances,
(cid:73) large amount labeled target instances

they do not train an additional model for    guring out weights of
instances

guest lecturer: ming-wei chang cs 546

() introduction to id20 21 / 33

spring, 2009

21 / 33

using heuristics to    nd weights

the work (jiang and zhai 2007) assumes

(cid:73) source labeled instances
(cid:73) small amount labeled target instances, pt(y|x)
(cid:73) large amount labeled target instances

they do not train an additional model for    guring out weights of
instances

instead, they use the target weight vector to select the examples

(cid:40)

si =

1, if pt(yi|xi ) > t
0, otherwise

guest lecturer: ming-wei chang cs 546

() introduction to id20 21 / 33

spring, 2009

21 / 33

using heuristics to    nd weights

the work (jiang and zhai 2007) assumes

(cid:73) source labeled instances
(cid:73) small amount labeled target instances, pt(y|x)
(cid:73) large amount labeled target instances

they do not train an additional model for    guring out weights of
instances

instead, they use the target weight vector to select the examples

(cid:40)

si =

1, if pt(yi|xi ) > t
0, otherwise

they also found that when training everything together, we should
put more weights on the target labeled data.

guest lecturer: ming-wei chang cs 546

() introduction to id20 21 / 33

spring, 2009

21 / 33

summary: instance weighting

what we have discussed

putting weights on instances is useful in many adaptation tasks

we learn some algorithms and some heuristics about how to use
unlabeled target examples to change the instance weight
two possible solutions:

1 train an additional model to tell if x comes from training or testing
2 using some heuristics to guess the training weights

guest lecturer: ming-wei chang cs 546

() introduction to id20 22 / 33

spring, 2009

22 / 33

outline

di   erent distributions: p(x )

solution: instance weighting

(bickel, br  ueckner, and sche   er 2007), (jiang and zhai 2007)

di   erent distributions: p(y | x )

assume ps (y | x ) is close to pt(y | x )
solution: id173

(evgeniou and pontil 2004), (daum  e iii 2007)

unknown words

solution: find the relations between known words and unseen words
through auxiliary tasks

(ando and zhang 2005), (blitzer, mcdonald, and pereira 2006)

guest lecturer: ming-wei chang cs 546

() introduction to id20 23 / 33

spring, 2009

23 / 33

what is the best training strategy?

assumption

we have both labeled source instances and labeled target instances

the source label distribution is similar to the target distribution

ps (y|x)     pt(y|x)

possible solutions

source only?

target only?

can you think of anything else?

guest lecturer: ming-wei chang cs 546

() introduction to id20 24 / 33

spring, 2009

24 / 33

domain  adaptationslide  11hal  daum    iii  (me@hal3.name)obvious approach 1: srconlytraining timetest timesourcedatatargetdatatargetdatasourcedatadomain  adaptationslide  12hal  daum    iii  (me@hal3.name)obvious approach 2: tgtonlytraining timetest timesourcedatatargetdatatargetdatatargetdatadomain  adaptationslide  13hal  daum    iii  (me@hal3.name)obvious approach 3: alltraining timetest timesourcedatatargetdatatargetdataunioned datasourcedatatargetdatadomain  adaptationslide  14hal  daum    iii  (me@hal3.name)obvious approach 4: weightedtraining timetest timesourcedatatargetdatatargetdataunioned datasourcedatatargetdatadomain  adaptationslide  15hal  daum    iii  (me@hal3.name)obvious approach 5: predtraining timetest timesourcedatatargetdatatargetdatasrconlytarget data(w/ srconly predictions)target data(w/ srconly predictions)domain  adaptationslide  16hal  daum    iii  (me@hal3.name)obvious approach 6: lininttraining timetest timesourcedatatargetdatatargetdatasrconlytgtonly  srconlytgtonly  any other strategies?

next, feature augmentation

(daum  e iii 2007)

guest lecturer: ming-wei chang cs 546

() introduction to id20 25 / 33

spring, 2009

25 / 33

domain  adaptationslide  6hal  daum    iii  (me@hal3.name)prior work     daum   iii and marcutraining timetest timesourcedatatargetdatatargetdatasourcemaxentgeneralmaxenttargetmaxent   mixture model   id136 byconditionalexpectationmaximizationdomain  adaptationslide  7hal  daum    iii  (me@hal3.name)   monitor    versus    the   news domain:     monitor    is a verb     the    is a determinertechnical domain:     monitor    is a noun     the    is a determinerkey idea:share some features (   the   )don't share others (   monitor   )(and let the learner decide which are which)domain  adaptationslide  8hal  daum    iii  (me@hal3.name)feature augmentationwemonitorthetrafficthemonitorisheavynvdndnvrw:monitorp:wen:thec:a+w:thep:monitorn:trafficc:a+w:monitorp:then:isc:a+w:thep:<s>n:monitorc:aa+sw:monitorsp:wesn:thesc:a+sw:thesp:monitorsn:trafficsc:a+tw:monitortp:thetn:istc:a+tw:thetp:<s>tn:monitortc:aa+in feature-vector lingo:  (x)          (x),   (x), 0     (for source domain)  (x)          (x), 0,   (x)       (for target domain)why should this work?originalfeaturesaugmentedfeaturesdomain  adaptationslide  17hal  daum    iii  (me@hal3.name)results     error ratestask           dom         srconly tgtonly    baseline             prior     augmentbn4.982.372.11 (pred)2.061.98bc4.544.073.53 (weight)3.473.47ace-nw4.783.713.56 (pred)3.683.39nerwl2.452.452.12 (all)2.412.12un3.672.462.10 (linint)2.031.91cts2.080.460.40 (all)0.340.32conlltgt2.492.951.75 (wgt/li)1.891.76pubmedtgt12.024.153.95 (linint)3.993.61id98tgt10.293.823.44 (linint)3.353.37wsj6.634.354.30 (weight)4.274.11swbd315.904.154.09 (linint)3.603.51br-cf5.166.274.72 (linint)5.225.15treebr-cg4.325.364.15 (all)4.254.90bank-br-ck5.056.325.01 (prd/li)5.275.41chunkbr-cl5.666.605.39 (wgt/prd)5.995.73br-cm3.576.593.11 (all)4.084.89br-cn4.605.564.19 (prd/li)4.484.42br-cp4.825.624.55 (wgt/prd/li)4.874.78br-cr5.789.135.15 (linint)6.716.30treebank-brown6.355.754.72 (linint)4.724.65some analysis on feature extension: (chang and roth
2008)

this has already been done before!

the feature augmentation equals to a form of id173

guest lecturer: ming-wei chang cs 546

() introduction to id20 26 / 33

spring, 2009

26 / 33

some analysis on feature extension: (chang and roth
2008)

this has already been done before!

the feature augmentation equals to a form of id173

id72: (evgeniou and pontil 2004)

the function for task i: wi + v
the id173 becomes: 1

2(cid:107)v(cid:107)2 + 1

2

(cid:80)k
i=1 (cid:107)wi(cid:107)2

guest lecturer: ming-wei chang cs 546

() introduction to id20 26 / 33

spring, 2009

26 / 33

some analysis on feature extension: (chang and roth
2008)

this has already been done before!

the feature augmentation equals to a form of id173

id72: (evgeniou and pontil 2004)

the function for task i: wi + v
the id173 becomes: 1

2(cid:107)v(cid:107)2 + 1

2

(cid:80)k
i=1 (cid:107)wi(cid:107)2

the question: when does this work?

guest lecturer: ming-wei chang cs 546

() introduction to id20 26 / 33

spring, 2009

26 / 33

why does feature augmentation work? (chang and roth
2008)

intuition

assumption: only two tasks.
the id173 becomes: (cid:107)v(cid:107)2 + (cid:107)ws(cid:107)2 + (cid:107)wt(cid:107)2
function for source: v + ws , function for source: v + wt

guest lecturer: ming-wei chang cs 546

() introduction to id20 27 / 33

spring, 2009

27 / 33

why does feature augmentation work? (chang and roth
2008)

intuition

assumption: only two tasks.
the id173 becomes: (cid:107)v(cid:107)2 + (cid:107)ws(cid:107)2 + (cid:107)wt(cid:107)2
function for source: v + ws , function for source: v + wt
intuition 1: v is shared across the tasks, so we can use some examples
better

guest lecturer: ming-wei chang cs 546

() introduction to id20 27 / 33

spring, 2009

27 / 33

why does feature augmentation work? (chang and roth
2008)

intuition

assumption: only two tasks.
the id173 becomes: (cid:107)v(cid:107)2 + (cid:107)ws(cid:107)2 + (cid:107)wt(cid:107)2
function for source: v + ws , function for source: v + wt
intuition 1: v is shared across the tasks, so we can use some examples
better

intuition 2: v is shared across the tasks, so if two tasks are more
   similar   , aug works better!

guest lecturer: ming-wei chang cs 546

() introduction to id20 27 / 33

spring, 2009

27 / 33

why does feature augmentation work? (chang and roth
2008)

intuition

assumption: only two tasks.
the id173 becomes: (cid:107)v(cid:107)2 + (cid:107)ws(cid:107)2 + (cid:107)wt(cid:107)2
function for source: v + ws , function for source: v + wt
intuition 1: v is shared across the tasks, so we can use some examples
better

intuition 2: v is shared across the tasks, so if two tasks are more
   similar   , aug works better!

is it?

guest lecturer: ming-wei chang cs 546

() introduction to id20 27 / 33

spring, 2009

27 / 33

why does feature augmentation work? (chang and roth
2008)

simple analysis

assume us and ut are the real separating lines
cos(us , ut) is small, aug does not work (nothing to be shared)
cos(us , ut) close to 1, aug does not work (single model is better)
aug only works in    good    range

guest lecturer: ming-wei chang cs 546

() introduction to id20 28 / 33

spring, 2009

28 / 33

arti   cial experiments

guest lecturer: ming-wei chang cs 546

() introduction to id20 29 / 33

spring, 2009

29 / 33

summary: dealing with p(y|x)

when you have labeled target instances, there are many training
algorithms

(cid:73) di   erent ways to combine source labeled data and target labeled data

we can apply multitask learning algorithms in this case

certain algorithms only work for limited situations

guest lecturer: ming-wei chang cs 546

() introduction to id20 30 / 33

spring, 2009

30 / 33

outline

di   erent distributions: p(x )

solution: instance weighting

(bickel, br  ueckner, and sche   er 2007), (jiang and zhai 2007)

di   erent distributions: p(y | x )

assume ps (y | x ) is close to pt(y | x )
solution: id173

(evgeniou and pontil 2004), (daum  e iii 2007)

unknown words

solution: find the relations between old words and unseen words
through auxiliary tasks

(ando and zhang 2005), (blitzer, mcdonald, and pereira 2006)

guest lecturer: ming-wei chang cs 546

() introduction to id20 31 / 33

spring, 2009

31 / 33

  sentiment classification for product reviewsproduct reviewclassifierpositivenegativeid166, na  ve bayes, etc.multiple domainsbookskitchen appliances. . .??????  books & kitchen appliancesrunning with scissors: a memoirtitle: horrible book, horrible.this book was horrible.  i read half of it, suffering from a headache the entire time, and eventually i lit it on fire.  one less copy in the world...don't waste your money.  i wish i had the time spent reading this book back so i could use it for better purposes.  this book wasted my lifeavante deep fryer, chrome & blacktitle: lid does not work well...i love the way the tefal deep fryer cooks, however, i am returning my second one due to a defective lid closure.  the lid may close initially, but after a few uses it no longer stays closed. i will not be purchasing this one again.error increase: 13% => 26%   features & linear models0.30horribleread_halfwaste0...0.10...00.2-11.10.1...-20...-0.3-1.2problem:  if we   ve only trained on book reviews, then  w(defective) = 00  structural correspondence learning (scl)    cut adaptation error by more than 40%    use unlabeled data from the target domain    induce correspondences among different features    read-half, headache   defective, returned    labeled data for source domain will help us build a good classifier for target domainmaximum likelihood id75 (mllr) for speaker adaptation (leggetter & woodland, 1995)   scl: 2-step learning processunlabeled.learn labeled.  learn         should make the domains look as similar as possible    but       should also allow us to classify wellstep 1:  unlabeled     learn correspondence mappingstep 2:  labeled     learn weight vector0.100...0.30.30.7-1.0...-2.100-1...-0.7  scl: making domains look similardefective lidincorrect classification of kitchen review    do not buy the shark portable steamer    . trigger mechanism is defective.     the very nice lady assured me that i must have a defective set    . what a disappointment!    maybe mine was defective    . the directions were unclearunlabeled kitchen contexts    the book is so repetitive that i found myself yelling    . i will definitely not buy another.    a disappointment    . ender was talked about for <#> pages altogether.    it   s unclear    . it   s repetitive and boringunlabeled books contexts  scl: pivot featurespivot features    occur frequently in both domains    characterize the task we want to do    number in the hundreds or thousands    choose using labeled source, unlabeled source & target datascl: words & bigrams that occur frequently in both domainsscl-mi: scl but also based on mutual information with labelsbook   one   <num>   so   all   very   about   they   like   good   whena_must   a_wonderful   loved_it weak   don   t_waste   awful   highly_recommended   and_easy  scl unlabeled step:  pivot predictorsuse pivot features to align other features    mask and predict pivot features using other features    train n linear predictors, one for each binary problem    each pivot predictor implicitly aligns non-pivot features from source & target domainsbinary problem:  does    not buy    appear here?(2) do not buy the shark portable steamer    . trigger mechanism is defective. (1) the book is so repetitive that i found myself yelling    . i will definitely not buy another.  scl: id84        gives n new features    value of ith feature is the propensity to see    not buy    in the same document    we still want fewer new features (1000 is too many)     many pivot predictors give similar information        horrible   ,    terrible   ,    awful       compute svd & use top left singular vectors id45 (lsi), (deerwester et al. 1990)id44 (lda), (blei et al. 2003)  back to linear classifiers0.300...0.10.30.7-1.0...-2.1classifier    source training: learn       &      together    target testing: first apply     , then apply        and   inspirations for scl1.alternating structural optimization (aso)   ando & zhang (jmlr 2005)   inducing structures for semi-supervised learning 1.correspondence id84   ham, lee, & saul (aistats 2003)   learn a low-dimensional representation from high-dimensional correspondences  sentiment classification data   product reviews from amazon.com   books, dvds, kitchen appliances, electronics   2000 labeled reviews from each domain   3000     6000 unlabeled reviews   binary classification problem    positive if 4 stars or more, negative if 2 or fewer   features: unigrams & bigrams   pivots: scl & scl-mi   at train time: minimize huberized hinge loss (zhang, 2004)  negative                    vs.                 positiveplot<#>_pagespredictablefascinatingengagingmust_readgrishamthe_plasticpoorly_designedleakingawkward_toespressoare_perfectyears_nowa_breezebookskitchenvisualizing   (books & kitchen)657075808590d->be->bk->bb->de->dk->dbaselinesclscl-mibooks72.876.879.770.775.475.470.966.168.680.482.477.274.075.870.674.376.272.775.476.9dvdempirical results: books & dvds   sometimes scl can cause increases in error   with only unlabeled data, we misalign featuressummary: unknown words

unknown word: an important problem for id20

instance weighting and generalization can not solve this problem

one solution: try to learn the relations between known words and
unknown words

guest lecturer: ming-wei chang cs 546

() introduction to id20 32 / 33

spring, 2009

32 / 33

summary: id20

id20

an important problem. we only have limited amount of labeled data
and there are so many domains.
existing solutions:

(cid:73) instance weighting
(cid:73) id173
(cid:73) find the relationship between known words and unknown words

guest lecturer: ming-wei chang cs 546

() introduction to id20 33 / 33

spring, 2009

33 / 33

summary: id20

id20

an important problem. we only have limited amount of labeled data
and there are so many domains.
existing solutions:

(cid:73) instance weighting
(cid:73) id173
(cid:73) find the relationship between known words and unknown words

many open problems
better techniques

how to combine those techniques

multiple domains adaptation

. . .

guest lecturer: ming-wei chang cs 546

() introduction to id20 33 / 33

spring, 2009

33 / 33

summary: id20

id20

an important problem. we only have limited amount of labeled data
and there are so many domains.
existing solutions:

(cid:73) instance weighting
(cid:73) id173
(cid:73) find the relationship between known words and unknown words

many open problems
better techniques

how to combine those techniques

multiple domains adaptation

. . .

thank you!!

guest lecturer: ming-wei chang cs 546

() introduction to id20 33 / 33

spring, 2009

33 / 33

ando, r. k. and t. zhang (2005).
a framework for learning predictive structures from multiple tasks and
unlabeled data.
j. mach. learn. res. 6, 1817   1853.

bickel, s., m. br  ueckner, and t. sche   er (2007).
discriminative learning for di   ering training and test distributions.
in z. ghahramani (ed.), proc. of the international conference on
machine learning (icml), pp. 81   88.

blitzer, j., r. mcdonald, and f. pereira (2006).
id20 with structural correspondence learning.
in proc. of the conference on empirical methods for natural
language processing (emnlp).

chang, m.-w. and d. roth (2008, april).
robust feature extension algorithms.
in learning workshop, snowbird.

daum  e iii, h. (2007, june).
frustratingly easy id20.

guest lecturer: ming-wei chang cs 546

() introduction to id20 33 / 33

spring, 2009

33 / 33

in proc. of the annual meeting of the acl, pp. 256   263.

evgeniou, t. and m. pontil (2004).
regularized multi   task learning.
in proc. of kdd, pp. 109   117.

jiang, j. and c. zhai (2007, june).
instance weighting for id20 in nlp.
in proc. of the annual meeting of the acl, pp. 264   271.

shimodaira, h. (2000).
improving predictive id136 un- der covariate shift by weighting the
log-likelihood func- tion.
journal of statistical planning and id136 90.

guest lecturer: ming-wei chang cs 546

() introduction to id20 33 / 33

spring, 2009

33 / 33

