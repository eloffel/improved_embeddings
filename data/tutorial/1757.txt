support vector machine

(and statistical learning theory)

tutorial

jason weston

nec labs america

4 independence way, princeton, usa.

jasonw@nec-labs.com

1 support vector machines: history
(cid:15) id166s introduced in colt-92 by boser, guyon & vapnik. became

rather popular since.

(cid:15) theoretically well motivated algorithm: developed from statistical

learning theory (vapnik & chervonenkis) since the 60s.

(cid:15) empirically good performance: successful applications in many

   elds (bioinformatics, text, image recognition, . . . )

2 support vector machines: history ii
(cid:15) centralized website: www.kernel-machines.org.
(cid:15) several textbooks, e.g.    an introduction to support vector

machines    by cristianini and shawe-taylor is one.

(cid:15) a large and diverse community work on them: from machine
learning, optimization, statistics, neural networks, functional
analysis, etc.

3 support vector machines: basics
[boser, guyon, vapnik    92],[cortes & vapnik    95]

-

+

-

margin

-

-

-

+

+

margin

+

+

nice properties: convex, theoretically motivated, nonlinear with kernels..

4 preliminaries:
(cid:15) machine learning is about learning structure from data.
(cid:15) although the class of algorithms called    id166   s can do more, in this

talk we focus on pattern recognition.

(cid:15) so we want to learn the mapping: x 7! y, where x 2 x is some

object and y 2 y is a class label.

(cid:15) let   s take the simplest case: 2-class classi   cation. so: x 2 rn,

y 2 f(cid:6)1g.

5 example:

suppose we have 50 photographs of elephants and 50 photos of tigers.

vs.

we digitize them into 100 x 100 pixel images, so we have x 2 rn where
n = 10; 000.

now, given a new (different) photograph we want to answer the question:
is it an elephant or a tiger? [we assume it is one or the other.]

6 training sets and prediction models
(cid:15) input/output sets x , y
(cid:15) training set (x1; y1); : : : ; (xm; ym)
(cid:15)    generalization   : given a previously seen x 2 x ,    nd a suitable

y 2 y.

(cid:15) i.e., want to learn a classi   er: y = f(x; (cid:11)), where (cid:11) are the

parameters of the function.

(cid:15) for example, if we are choosing our model from the set of

hyperplanes in rn, then we have:

f(x;fw; bg) = sign(w (cid:1) x + b):

7 empirical risk and the true risk
(cid:15) we can try to learn f(x; (cid:11)) by choosing a function that performs well

on training data:

remp((cid:11)) =

1
m

mx

i=1

   (f(xi; (cid:11)); yi) = training error

where     is the zero-one id168,    (y; ^y) = 1, if y 6= ^y, and 0
otherwise. remp is called the empirical risk.

(cid:15) by doing this we are trying to minimize the overall risk:

z

r((cid:11)) =

   (f(x; (cid:11)); y)dp (x; y) = test error

where p(x,y) is the (unknown) joint distribution function of x and y.

8 choosing the set of functions
what about f(x; (cid:11)) allowing all functions from x to f(cid:6)1g?
training set (x1; y1); : : : ; (xm; ym) 2 x (cid:2) f(cid:6)1g
test set (cid:22)x1; : : : ; (cid:22)x (cid:22)m 2 x ,
such that the two sets do not intersect.

for any f there exists f

(cid:3)

:

(cid:3)(xi) = f(xi) for all i
(cid:3)(xj) 6= f(xj) for all j

1. f

2. f

based on the training data alone, there is no means of choosing which
function is better. on the test set however they give different results. so
generalization is not guaranteed.
=) a restriction must be placed on the functions that we allow.

9 empirical risk and the true risk

s

vapnik & chervonenkis showed that an upper bound on the true risk can
be given by the empirical risk + an additional term:

r((cid:11)) (cid:20) remp((cid:11)) +

h(log( 2m

h + 1)     log( (cid:17)
4 )

m

where h is the vc dimension of the set of functions parameterized by (cid:11).
(cid:15) the vc dimension of a set of functions is a measure of their capacity

or complexity.

(cid:15) if you can describe a lot of different phenomena with a set of

functions then the value of h is large.

[vc dim = the maximum number of points that can be separated in all
possible ways by that set of functions.]

10 vc dimension:

the vc dimension of a set of functions is the maximum number of points
that can be separated in all possible ways by that set of functions. for
hyperplanes in rn, the vc dimension can be shown to be n + 1.

x

x

xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxx

x

x

xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxx

x

x

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

x

x

x

x

x

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

x

x

x

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

x

x

x

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

x

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

x

x

x

x

x

xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx
xxxxxxxxxxxx

x

11 vc dimension and capacity of functions

simpli   cation of bound:

test error (cid:20) training error + complexity of set of models
(cid:15) actually, a lot of bounds of this form have been proved (different
measures of capacity). the complexity function is often called a
regularizer.

(cid:15) if you take a high capacity set of functions (explain a lot) you get low

training error. but you might    over   t   .

(cid:15) if you take a very simple set of models, you have low complexity, but

won   t get low training error.

12 capacity of a set of functions (classi   cation)

[images taken from a talk by b. schoelkopf.]

13 capacity of a set of functions (regression)

y

sine curve fit

hyperplane fit

true function

x

14 controlling the risk: model complexity

bound on the risk

confidence interval

empirical risk
(training error)

h

1

h*

h

n

s
1 s*

 

s
n

15 capacity of hyperplanes

(cid:3)

vapnik & chervonenkis also showed the following:
consider hyperplanes (w (cid:1) x) = 0 where w is normalized w.r.t a set of
points x
the set of decision functions fw(x) = sign(w (cid:1) x) de   ned on x
that jjwjj (cid:20) a has a vc dimension satisfying

such that: mini jw (cid:1) xij = 1:

such

(cid:3)

h (cid:20) r2a2:

.

where r is the radius of the smallest sphere around the origin containing
(cid:3)
x
=) minimize jjwjj2 and have low capacity
=) minimizing jjwjj2 equivalent to obtaining a large margin classi   er

<w  x> + b > 0

,

u

u

u

u

l

l

<w  x> + b < 0

,

w

l

l

{x | <w  x> + b = 0}

,

l

{x | <w  x> + b =    1}

,

{x | <w  x> + b = +1}

,

note:

m

yi =    1

m
x2

w

m

m

,

m

u

u

x1

yi = +1

u

u

{x | <w  x> + b = 0}

,

,
,

<w  x1> + b = +1
<w  x2> + b =    1
,
=>       <w  (x1   x2)> =   2
2
=>
||w||

>
(x1   x2)   =

w
||w||<

,

16 linear support vector machines (at last!)

so, we would like to    nd the function which minimizes an objective like:

training error + complexity term

we write that as:

1
m

mx

i=1

   (f(xi; (cid:11)); yi) + complexity term

for now we will choose the set of hyperplanes (we will extend this later),
so f(x) = (w (cid:1) x) + b:
1
m

   (w (cid:1) xi + b; yi) + jjwjj2

mx

i=1

subject to mini jw (cid:1) xij = 1:

17 linear support vector machines ii

that function before was a little dif   cult to minimize because of the step
function in    (y; ^y) (either 1 or 0).
let   s assume we can separate the data perfectly. then we can optimize
the following:
minimize jjwjj2, subject to:

(w (cid:1) xi + b) (cid:21) 1;
(w (cid:1) xi + b) (cid:20)    1;

if yi = 1
if yi =    1

the last two constraints can be compacted to:
yi(w (cid:1) xi + b) (cid:21) 1

this is a quadratic program.

18 id166s : non-separable case

to deal with the non-separable case, one can rewrite the problem as:

minimize:

subject to:

jjwjj2 + c

mx

(cid:24)i

i=1

this is just the same as the original objective:

(cid:24)i (cid:21) 0

yi(w (cid:1) xi + b) (cid:21) 1     (cid:24)i;
mx

1
m

i=1

   (w (cid:1) xi + b; yi) + jjwjj2

except     is no longer the zero-one loss, but is called the    hinge-loss   :
   (y; ^y) = max(0; 1     y^y). this is still a quadratic program!

-

+

+

-

-

-

-

margin

+

+

-

+

i

-

+

x
19 support vector machines - primal
(cid:15) decision function:

f(x) = w (cid:1) x + b

(cid:15) primal formulation:
min p (w; b) =

1
2

| {z }
kwk2

maximize margin

x

i

h1[ yi f(xi) ]

{z

}

+ c

|

minimize training error

ideally h1 would count the number of errors, approximate with:

1h (z)

hinge loss h1(z) = max(0; 1     z)

0

z

20 id166s : non-linear case

linear classi   ers aren   t complex enough sometimes. id166 solution:

map data into a richer feature space including nonlinear features, then
construct a hyperplane in that space so all other equations are the same!

formally, preprocess the data with:

x 7! (cid:8)(x)

and then learn the map from (cid:8)(x) to y:

f(x) = w (cid:1) (cid:8)(x) + b:

21 id166s : polynomial mapping
p

(cid:8) : r2 ! r3
(x1; x2) 7! (z1; z2; z3) := (x2
1;

(2)x1x2; x2
2)

5

5

5

5

5

m

5

5

m

m

5

x2

5

m

m

m

5

5

m

m

5

5

x1

5

5

5

5

5

5

5

z2

z3

5

5

5

5

5

5

5

5

m
m

m

m
m
m

m

5

m

5

5

5

5

z1

5

22 id166s : non-linear case ii

for example mnist hand-writing recognition.
60,000 training examples, 10000 test examples, 28x28.

linear id166 has around 8.5% test error.
polynomial id166 has around 1% test error.

5

3

4

3

1

3

4

1

9

6

0

5

0

8

8

0

4

7

0

7

4

3

9

6

7

7

6

1

2

4

1

6

1

9

9

4

0

6

6

6

9

1

1

0

3

9

4

3

7

8

2

7

2

5

9

8

5

0

8

0

1

2

4

6

8

0

6

2

3

7

3

8

3

0

5

9

1

1

9

8

1

6

2

7

9

4

0

1

0

3

4

9

7

6

3

1

0

7

4

1

23 id166s : full mnist results

classi   er

linear

3-nearest-neighbor

rbf-id166

tangent distance

lenet

boosted lenet

test error

8.4%

2.4%

1.4 %

1.1 %

1.1 %

0.7 %

translation invariant id166 0.56 %

choosing a good mapping (cid:8)((cid:1)) (encoding prior knowledge + getting right
complexity of function class) for your problem improves results.

24 id166s : the kernel trick

problem: the dimensionality of (cid:8)(x) can be very large, making w hard to
represent explicitly in memory, and hard for the qp to solve.

the representer theorem (kimeldorf & wahba, 1971) shows that (for
id166s as a special case):

mx

i=1

w =

(cid:11)i(cid:8)(xi)

for some variables (cid:11). instead of optimizing w directly we can thus
optimize (cid:11).

mx

the decision rule is now:

(cid:11)i(cid:8)(xi) (cid:1) (cid:8)(x) + b
we call k(xi; x) = (cid:8)(xi) (cid:1) (cid:8)(x) the id81.

f(x) =

i=1

25 support vector machines - kernel trick ii

pm
we can rewrite all the id166 equations we saw before, but with the
w =
(cid:15) decision function:

i=1 (cid:11)i(cid:8)(xi) equation:

f(x) =

(cid:11)i(cid:8)(xi) (cid:1) (cid:8)(x) + b

(cid:11)ik(xi; x) + b

i

x
x
k mx

=

i

i=1

(cid:15) dual formulation:

min p (w; b) =

1
2

|

x

i

+ c

|

h1[ yi f(xi) ]

{z

}

(cid:11)i(cid:8)(xi)k2
}
{z

maximize margin

minimize training error

26 support vector machines - dual

but people normally write it like this:
(cid:15) dual formulation:
x

min
(cid:11)

d((cid:11)) =

1
2

i;j

(cid:11)i (cid:11)j (cid:8)(xi)(cid:1)(cid:8)(xj)   

x

i

yi (cid:11)i s.t.

8<
:  i (cid:11)i=0

0(cid:20)yi (cid:11)i(cid:20)c

(cid:15) dual decision function:

f(x) =

x

i

(cid:11)ik(xi; x) + b

(cid:15) id81 k((cid:1);(cid:1)) is used to make (implicit) nonlinear feature

map, e.g.
    polynomial kernel: k(x; x0) = (x (cid:1) x0 + 1)d.
    rbf kernel:

k(x; x0) = exp(     jjx     x0jj2).

27 polynomial-id166s
the kernel k(x; x0) = (x (cid:1) x0)d gives the same result as the explicit
mapping + dot product that we described before:

(cid:8) : r2 ! r3
(x1; x2) 7! (z1; z2; z3) := (x2
1;
2) (cid:1) (x
(cid:8)((x1; x2) (cid:1) (cid:8)((x
02
0
0
2) = (x2
1;
1; x
1;
02
2 + x2
= x2
2x
1x
2

02
0
1 + 2x1x

(2)x1x2; x2

0
1x2x

p

p
p

(2)x1x2; x2
2)
0
0
(2)x
2; x
1x

02
2)

is the same as:

k(x; x0) = (x (cid:1) x0)2 = ((x1; x2) (cid:1) (x
0

0
= (x1x

0
1 + x2x

2)2 = x2
1x

02
1 + x2
2x

0
1; x

2))2
02
0
2 + 2x1x

0
1x2x

2

interestingly, if d is large the kernel is still only requires n multiplications
to compute, whereas the explicit representation may not    t in memory!

28 rbf-id166s
the rbf kernel k(x; x0) = exp(     jjx     x0jj2) is one of the most
popular id81s. it adds a    bump    around each data point:

mx

f(x) =

(cid:11)i exp(     jjxi     xjj2) + b

i=1

.

x

.

x'

(x)

(x')

using this one can get state-of-the-art results.

f
f
f
29 id166s : more results

there is much more in the    eld of id166s/ kernel machines than we could
cover here, including:
(cid:15) regression, id91, semi-supervised learning and other domains.
(cid:15) lots of other kernels, e.g. string kernels to handle text.
(cid:15) lots of research in modi   cations, e.g. to improve generalization

ability, or tailoring to a particular task.
(cid:15) lots of research in speeding up training.
please see text books such as the ones by cristianini & shawe-taylor or
by schoelkopf and smola.

30 id166s : software

lots of id166 software:
(cid:15) libid166 (c++)
(cid:15) id166light (c)
as well as complete machine learning toolboxes that include id166s:
(cid:15) torch (c++)
(cid:15) spider (matlab)
(cid:15) weka (java)
all available through www.kernel-machines.org.

