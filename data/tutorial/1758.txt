6.863j natural language processing

lecture 11: from feature-based 

grammars to semantics

robert c. berwick
berwick@ai.mit.edu

the menu bar

    administrivia:

    schedule alert: lab 4 out weds. lab time 

today, tomorrow

    please read notes4.pdf!! 

    agenda: 
    feature-based grammars/parsing: 

unification; the question of 
representation

    semantic interpretation via lambda 
calculus: syntax directed translation

6.863j/9.611j lecture 11 sp03

features are everywhere

morphology of a single word: 
verb[head=thrill, tense=present, num=sing, person=3,   ]    

thrills

projection of features up to a bigger phrase 
vp[head=a

, tense=b , num=g    ]     v[head=a
is in the set transitive-verbs

provided a

, tense=b , num=g    ] np

agreement between sister phrases:
s[head=a

, tense=b ]     np[num=g ,   ] vp[head=a

provided a

is in the set transitive-verbs

6.863j/9.611j lecture 11 sp03

, tense=b , num=g    ] 

better approach to factoring 
linguistic knowledge

    use the superposition idea: we superimpose 

one set of constraints on top of another:

1. basic skeleton tree
2. plus the added feature constraints
    s
vp
[num x]

np 
[num x]

[num x]

the guy
[num singular]

eats
[num singular]

6.863j/9.611j lecture 11 sp03

   
or in tree form:

s [number x]

np [number x]

vp [number x]

dt [number x]

n [number x]

v [number x] np

the
[number singular]

guy
[number singular]

eats
[number singular]

6.863j/9.611j lecture 11 sp03

values trickle up

s [number x]

np [number x]

vp [number x]

dt [number sing]

n [number sing]

v [number sing]np

the
[number singular]

guy
[number singular]

eats
[number singular]

6.863j/9.611j lecture 11 sp03

checking features

s [number x]

np [number sing]

vp [number sing]

dt [number sing]

n [number sing]

v [number sing]np

the
[number singular]

guy
[number singular]

eats
[number singular]

6.863j/9.611j lecture 11 sp03

what sort of power do we need 
here?
    we have [feature value] combinations so far
    this seems fairly widespread in language
    we call these atomic  feature-value 

combinations

    other examples: 
1. in english: 
person feature (1st, 2nd, 3rd); 
case feature (degenerate in english: nominative, 

object/accusative, possessive/genitive): i know 
her vs. i know she; 

number feature: plural/sing; definite/indefinite
degree: comparative/superlative

6.863j/9.611j lecture 11 sp03

other languages; formalizing features

    two kinds:
1. syntactic features, purely grammatical function 

example: case in german (nominative, 
accusative, dative case)     relative pronoun 
must agree w/ case of verb with which it is 
construed
wer micht strak is, muss klug sein
who not  strong is, must clever be
nom
who isn   t strong must be clever

nom

6.863j/9.611j lecture 11 sp03

continuing this example

ich nehme, wen         du mir empfiehlst
i      take  whomever you me recommend
acc          acc                  acc
i     take   whomever you recommend to me

*ich nehme, wen      du vertraust
i      take  whomever you trust
acc        acc               dat

6.863j/9.611j lecture 11 sp03

other class of features

2. syntactic features w/ meaning     example, 

number, def/indef., adjective degree

hungarian
akart           egy k  nyvet
he-wanted  

a    book

-def            -def

egy k  nyv amit     akart
a   book which     he-wanted

-def                   -def

6.863j/9.611j lecture 11 sp03

feature structures

    sets of feature-value pairs where:

    features are atomic symbols
    values are atomic symbols or feature structures
    illustrated by attribute-value matrix

feature
feature
...
feature

1

2

n

value
value
....
value

1

2

n

6.863j/9.611j lecture 11 sp03

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
how to formalize?

    let f be a finite set of feature names, let 

a be a set of feature values

    let p be a function from feature names 

to permissible feature values, that is, 
p: f    2a

    now we can define a word category as a 

triple <f, a, p> 

    this is a partial function from feature 

names to feature values

6.863j/9.611j lecture 11 sp03

example 

    f= {cat, plu, per}
    p: 

p(cat)={v, n, adj}
p(per)={1, 2, 3}
p(plu)={+, -}

sleep =    {[cat v], [plu -], [per 1]}
sleep =    {[cat v], [plu +], [per 1]}
sleeps= 
{[cat v], [plu -], [per 3]}
checking whether features are compatible is 

relatively simple here

6.863j/9.611j lecture 11 sp03

    feature values can be feature 

structures themselves     should they be?
    useful when certain features commonly co-

occur, e.g. number and person

cat
agr

np
num
pers

sg
3

    feature path: path through structures to 

value (e.g. 
agr (cid:224) num (cid:224) sg

6.863j/9.611j lecture 11 sp03

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
important question

    do features have to be more complicated 

than this?

    more: hierarchically structured (feature 

structures) (directed acyclic graphs,  
dags, or even beyond)

    then checking for feature compatibility 

amounts to unification

    example

6.863j/9.611j lecture 11 sp03

reentrant structures

    feature structures may also contain features 
that share some feature structure as a value

scat

head

agr

1

num
pers

sg
3

subj

agr

1

    numerical indices indicate the shared values
    big question: do we need nested structures??

6.863j/9.611j lecture 11 sp03

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    number feature
  sg

   num

    number-person features

num
pers

sg
3

    number-person-category features 

(3sgnp)

cat
num
pers

np
sg
3

6.863j/9.611j lecture 11 sp03

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
graphical notation for feature 
structures

6.863j/9.611j lecture 11 sp03

features and grammars

category: n
 agreement:
 agreement:

person: third
number: singular

category

n

agreement

number

person

singular

third

6.863j/9.611j lecture 11 sp03

feature checking by unification

agreement

agreement

number

person

number

person

singular

john

third

plural

third

sleep

agreement

number

person

clash

third

*john sleep

6.863j/9.611j lecture 11 sp03

operations on feature structures

    what will we need to do to these structures?

    check the compatibility of two structures
    merge the information in two structures

    we can do both using unification
    we say that two feature structures can be 

unified if the component features that make 
them up are compatible
    [num sg] u [num sg] = [num sg]
    [num sg] u [num pl] fails!
    [num sg] u [num []] = [num sg]

6.863j/9.611j lecture 11 sp03

    [num sg] u [pers 3] =

num
pers

sg
3

    structure are compatible if they contain 

no features that are incompatible 

    unification of two feature structures:

    are the structures compatible?
    if so, return the union of all feature/value 

pairs

    a failed unification attempt
numpl
pers
3
numpl
pers
3

num
pers
agr

subjagr

sg
3

subj

agr

agr

u

1

1

6.863j/9.611j lecture 11 sp03

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
features, unification and 
grammars
    how do we incorporate feature structures into 

our grammars?
    assume that constituents are objects which have 

feature-structures associated with them

    associate sets of unification constraints with 

grammar rules 

    constraints must be satisfied for rule to be satisfied

    for a grammar rule b 0 (cid:224) b 1    b n
i feature path> = atomic value
i feature path> = <b

    <b
    <b

j feature path>

    nb: if simple feat-val pairs, no nesting, then no 

need for paths

6.863j/9.611j lecture 11 sp03

feature unification examples

(1) [ agreement: [ number:  singular 

person:

first ]  ]

(2) [ agreement: [ number:  singular] 

case:

nominative  ]  

    (1) and (2) can unify, producing (3):
(3) [ agreement: [ number:  singular 

case:

person:
nominative  ]

first ]  

(try overlapping the graph structures 

corresponding to these two)

6.863j/9.611j lecture 11 sp03

feature unification examples

(2) [ agreement: [ number:  singular] 

case:

nominative  ]  

(4) [ agreement: [ number:  singular 
third]  ]

person:

    (2) & (4) can unify, yielding (5):
(5) [ agreement: [ number:  singular 

case:

person:
nominative  ]

third]  

    but (1) and (4) cannot unify because their 

values conflict on <agreement person>

6.863j/9.611j lecture 11 sp03

    to enforce subject/verb number 

agreement

s (cid:224) np vp

<np num> = <vp num>

6.863j/9.611j lecture 11 sp03

head features

    features of most grammatical categories are 
copied from head child to parent (e.g. from v 
to vp, nom to np, n to nom,    )

    these normally written as    head    features, 

e.g.
vp (cid:224) v np
<vp head> = <v head>
np (cid:224) det nom
<np(cid:224) head> = <nom head>
<det head agr> = <nom head agr>
nom (cid:224) n
<nom head> = <n head>

6.863j/9.611j lecture 11 sp03

s

np

vp

det
the

n

v
has

vp

n
plan

vp

v

been

vp

to

vp

v

thrilling

np
otto

v

np

swallow

wanda

s    np[n=1]  vp[n=1]

s

vp[n=1]     v[n=1] vp
v[n=1]     has

[num=1]

np

[num=1]

vp

det
the

[num=1]

n

[num=1]

v
has

vp

[num=1]

n
plan

vp

v

been

vp

np[n=1]     det n[n=1]
n[n=1]     n[n=1] vp
n[n=1]     plan

to

vp

v

thrilling

np
otto

v

np

swallow

wanda

s    np[n=a ]  vp[n=a ]

s

vp[n=a ]     v[n=a ] vp
v[n=1]     has

[num=1]

np

[num=1]

vp

det
the

[num=1]

n

[num=1]

v
has

vp

[num=1]

n
plan

vp

v

been

vp

to

vp

np[n=a ]     det n[n=a ]
n[n=a ]     n[n=a ] vp
n[n=1]     plan

v

np

swallow

wanda

v

thrilling

np
otto

s

[head=thrill]

np

[head=plan]

vp

[head=thrill]

det
the

n

[head=plan]

v
has

vp

[head=thrill]

[head=plan]

n
plan

vp

[head=swallow]

v

been

vp

[head=thrill]

to

vp

[head=swallow]

v

[head=thrill]
thrilling

[head=otto]

np
otto

np[h=a ]     det n[h=a ]
n[h=a ]     n[h=a ] vp
n[h=plan]     plan

v

[head=swallow] [head=wanda]

swallow

wanda

np

s

np

[head=plan]

vp

det
the

n

[head=plan]

v
has

vp

[head=plan]

n
plan

vp

v

been

vp

to

vp

np[h=a ]     det n[h=a ]
n[h=a ]     n[h=a ] vp
n[h=plan]     plan

v

np

swallow

wanda

v

thrilling

np
otto

   why use heads?

s

[head=thrill]

np

[head=plan]

det
the

n

[head=plan]

   morphology (e.g.,word endings)
   n[h=plan,n=1]     plan

vp

[head=thrill]

n[h=plan,n=2+]     plans

   v[h=thrill,tense=prog]    
vp
v[h=thrill,tense=past]    
v[h=go,tense=past]     went

[head=thrill]

v
has

thrilling
thrilled

[head=plan]

n
plan

vp

[head=swallow]

v

been

vp

[head=thrill]

to

vp

[head=swallow]

v

[head=thrill]
thrilling

[head=otto]

np
otto

np[h=a ]     det n[h=a ]
n[h=a ]     n[h=a ] vp
n[h=plan]     plan

v

[head=swallow] [head=wanda]

swallow

wanda

np

   why use heads?

s

[head=thrill]

np

[head=plan]

det
the

n

[head=plan]

[head=plan]

n
plan

vp

[head=swallow]

   subcategorization (i.e., 

transitive vs. intransitive)

vp

   when is vp     v np ok?

[head=thrill]

vp[h=a ]     v[h=a ] np

restrict to a

   transitive_verbs
   when is n     n vp ok?
[head=thrill]

vp
n[h=a ]     n[h=a ] vp

{plan, plot, hope,   }

vp

[head=thrill]

v
has
restrict to a
v

been

to

vp

[head=swallow]

v

[head=thrill]
thrilling

[head=otto]

np
otto

np[h=a ]     det n[h=a ]
n[h=a ]     n[h=a ] vp
n[h=plan]     plan

v

[head=swallow] [head=wanda]

swallow

wanda

np

  
   why use heads?

s

[head=thrill]

equivalently: keep the template
but make prob depend on a

,b

np

[head=plan]

det
the

n

[head=plan]

   selectional restrictions
   vp[h=a ]     v[h=a ] np
   i.e., vp[h=a ]     v[h=a ] np[h=b ]
   don   t fill template in all ways:

[head=thrill]

vp

vp

v
vp[h=thrill]     v[h=thrill] np[h=otto]
has
*vp[h=thrill]     v[h=thrill] np[h=plan]
leave out, or low prob

[head=thrill]

[head=plan]

n
plan

vp

[head=swallow]

v

been

vp

[head=thrill]

to

vp

[head=swallow]

v

[head=thrill]
thrilling

[head=otto]

np
otto

np[h=a ]     det n[h=a ]
n[h=a ]     n[h=a ] vp
n[h=plan]     plan

v

[head=swallow] [head=wanda]

swallow

wanda

np

how can we parse with feature 
structures?
    unification operator: takes 2 features structures 
and returns either a merged feature structure or 
fail

    input structures represented as dags

    features are labels on edges
    values are atomic symbols or dags

    unification algorithm goes through features in 

one input dag1 trying to find corresponding 
features in dag2     if all match, success, else fail

6.863j/9.611j lecture 11 sp03

unification and id117

    goal:

    use feature structures to provide richer 

representation

    block entry into chart of ill-formed constituents

    changes needed to earley

    add feature structures to grammar rules, e.g.

s (cid:224) np vp

<np head agr> = <vp head agr>
<s head> = <vp head>

    add field to states containing dag representing 

feature structure corresponding to state of parse, 
e.g.

s (cid:224)     np vp, [0,0], [], dag

6.863j/9.611j lecture 11 sp03

    add new test to completer operation

    recall: completer adds new states to chart by 
finding states whose     can be advanced (i.e., 
category of next constituent matches that of 
completed constituent)

    now: completer will only advance those states if 

their feature structures unify

    new test for whether to enter a state in the 

chart
    now dags may differ, so check must be more 

complex

    don   t add states that have dags that are more 

specific than states in chart: is new state 
subsumed by existing states?

6.863j/9.611j lecture 11 sp03

general feature grammars    violate 
the properties of natural languages?

    take example from so-called    lexical-

functional grammar    but this applies as 
well to any general unification grammar
    lexical functional grammar (lfg): add 
checking rules to cf rules (also variant 
hpsg)

6.863j/9.611j lecture 11 sp03

example lexical functional 
grammar

    basic cf rule:

s    np vp

    add corresponding    feature checking   

s   

np                          vp
(   

    =    
    what is the interpretation of this?

subj num)=    

6.863j/9.611j lecture 11 sp03

applying feature checking in lfg

[subj [num singular]]

copy up above

s

np
(    subj num)=    

vp

    =    

n

guys

[num plural]

v     =    

[num singular]

sleeps

whatever features from
below

6.863j/9.611j lecture 11 sp03

evidence that you don   t need this 
much power - hierarchy
    linguistic evidence: looks like you just check 
whether features are nondistinct, rather than 
equal or not     variable matching,  not variable 
substitution

    full unification lets you generate unnatural 

languages:
ai,  s.t. i a power of 2     e.g., a, aa, aaaa, 
aaaaaaaa,    
why is this    unnatural        another (seeming) 
property of natural languages:

natural languages seem to obey a constant 

growth property

6.863j/9.611j lecture 11 sp03

constant growth property

    take a language & order its sentences int terms 

of increasing length in terms of # of words 
(what   s shortest sentence in english?)

    claim: $ bound on the    distance gap    between 

any two consecutive sentences in this list, which 
can be specified in advance (fixed)

       intervals    between valid sentences cannot get 

too big     cannot grow w/o bounds
    we can do this a bit more formally

6.863j/9.611j lecture 11 sp03

constant growth

    dfn. a language l is semilinear if the number of 

occurrences of each symbol in any string of l is a 
linear combination of the occurrences of these 
symbols in some fixed, finite set of strings of l.  
    dfn. a language l is constant growth if there is a 
constant c0 and a finite set of constants c s.t. for 
all w   l, where |w|>  c0 $ w       l s.t. |w|=|w   |+c, 
some c    c

    fact. (parikh, 1971). context-free languages are 

semilinear, and constant-growth

    fact. (berwick, 1983). the power of 2 language is 

non constant-growth

6.863j/9.611j lecture 11 sp03

alas, this allows non-constant 
growth, unnatural languages

    can use lfg to generate power of 2 language
    very simple to do
    a   

(   

f) =    

a                  a
(   
a
(   

f) =    

a    

 
 
lets us `count    the number of embeddings on the 

f) =1

right & the left     make sure a power of 2

6.863j/9.611j lecture 11 sp03

example

 [f[f[f =1]]]

a

 [f[f[f =1]]]

 [f[f =1]]

a

(   

f) =    
 [f =1]

a

a

 [f =1]

a

a

a

 [f[f =1]]

(   

f) =    

a
f) =1  (   

a
f) =1  (   

a
a
f) =1  (   

 (   

f) =1

checks ok

6.863j/9.611j lecture 11 sp03

if mismatch anywhere, get a 
feature clash   

 [f[f[f =1]]]

clash!
a

 [f[f =1]]

 [f[f =1]]

a

a

 [f =1]

(   

f) =    
 [f =1]

a

a

 [f =1]

a
f) =1  (   

a
f) =1

 (   

a
f) =1

 (   

fails!

6.863j/9.611j lecture 11 sp03

conclusion then

    if we use too powerful a formalism, it lets 

us write    unnatural    grammars

    this puts burden on the person writing 

the grammar     which may be ok.

    however, child doesn   t presumably do this 

(they don   t get    late days   )

    we want to strive for automatic 

programming     ambitious goal

6.863j/9.611j lecture 11 sp03

summing up

    feature structures encoded rich information 

about components of grammar rules

    unification provides a mechanism for merging 

structures and for comparing them

    feature structures can be quite complex:

    subcategorization constraints
    long-distance dependencies

    unification parsing:

    merge or fail
    modifying earley to do unification parsing

6.863j/9.611j lecture 11 sp03

from syntax to meaning

    what does    understanding    mean
    how can we compute it if we can   t 

represent it

    the    classical    approach: compositional 

semantics

    implementation like a programming 

language

6.863j/9.611j lecture 11 sp03

initial simplifying assumptions

    focus on literal meaning

    conventional meanings of words
    ignore context

6.863j/9.611j lecture 11 sp03

(a)

planner

big picture

(b)

planner

object actions

semantic restrictions yes-no

id136/model

of intentions
(type hierarchy)

message phrase lists

semantic restrictions yes-no

syntax-directed
message con-

struction

id136/model

of intentions
(type hierarchy)

thematic

role interpreter

s parser

thematic role

frames

syntax-directed
message con-

struction

thematic

role interpreter

selectional restrictions yes-no

syntactic structures

pp attachment decisions yes-

no

parser

6.863j/9.611j lecture 11 sp03

example of what we might do

athena>(top-level)
shall i clear the database? (y or n) y
sem-interpret>john saw mary in the park
ok.
sem-interpret>where did john see mary
in the park.
sem-interpret>john gave fido to mary
ok.
sem-interpret>who gave john fido
i don't know
sem-interpret>who gave mary fido
john
sem-interpret >john saw fido
ok.
sem-interpret>who did john see
fido and mary

6.863j/9.611j lecture 11 sp03

what

    the nature (representation) of meaning 

representations vs/ how these are 
assembled

6.863j/9.611j lecture 11 sp03

analogy w/ prog. language

    what is meaning of 3+5*6?
    first parse it into 3+(5*6)

e

n

3

6.863j/9.611j lecture 11 sp03

+

3

*

5

6

e

f

+

e

e

n

5

f

e

n*
6

interpreting in an environment

    how about 3+5*x?
    same thing: the meaning

of x is found from the
environment (it   s 6)

    analogies in language?

+

33

3

3

*

30

5

5

6

x

e

33

f

30

e

+
add

e

f

e

n

5

n*

mult

6

5

6

e

3

n

3

6.863j/9.611j lecture 11 sp03

compiling

    how about 3+5*x?
    don   t know x at compile time
       meaning    at a node

is a piece of code, not a 
number

5*(x+1)-2 is a different expression 
that produces equivalent code 
(can be converted to the 
previous code by optimization)
analogies in language?

6.863j/9.611j lecture 11 sp03

add(3,mult(5,x))
e

f

+
add

e

3

n

3

5

e

n

5

mult(5,x)
e

f

e

n*

mult
x

x

what

    what representation do we want for 

something like
john ate ice-cream    
ate(john, ice-cream)

    id198
    we   ll have to posit something that will do 

the work

    predicate of 2 arguments:

l x l y ate(y, x) 

6.863j/9.611j lecture 11 sp03

how: recover meaning from 
structure

s or ip

vp(np )= ate (john , icecream)

john

np

= l y.ate (y, ice-cream)

vp

john

v
l xl y.ate(y, x)

np

ice-cream

ate

ice-cream

6.863j/9.611j lecture 11 sp03

what counts as understanding?

some notions

    we understand if we can respond appropriately

    ok for commands, questions (these demand response)
       computer, warp speed 5   
       throw axe at dwarf   
       put all of my blocks in the red box   
    imperative programming languages
    database queries and other questions

    we understand statement if we can determine its 

truth
    ok, but if you knew whether it was true, why did 

anyone bother telling it to you?

    comparable notion for understanding np is to compute 

what the np refers to, which might be useful

6.863j/9.611j lecture 11 sp03

what counts as understanding?

some notions

    we understand statement if we know how  to 

determine its truth
    what are exact conditions under which it would be true?

    necessary + sufficient

    equivalently, derive all its consequences 

    what else must be true if we accept the statement?

    philosophers tend to use this definition

    we understand statement if we can use it to 

answer questions  [very similar to above     requires reasoning]
    easy: john ate pizza.  what was eaten by john?
    hard: white   s first move is p-q4.  can black checkmate?
    constructing a procedure  to get the answer is enough

6.863j/9.611j lecture 11 sp03

representing meaning

    what requirements do we have for 

meaning representations?

6.863j/9.611j lecture 11 sp03

what  requirements must 
meaning representations fulfill?

    verifiability: the system should allow us to 

compare representations to facts in a 
knowledge base (kb)
    cat(huey)

    ambiguity: the system should allow us to 

represent meanings unambiguously
    german teachers has 2 representations

    vagueness: the system should allow us to 

represent vagueness
    he lives somewhere in the south of france.

6.863j/9.611j lecture 11 sp03

requirements: id136

    draw valid conclusions based on the 

meaning representation of inputs and its 
store of background knowledge.
does huey eat kibble?
thing(kibble)
eat(huey,x) ^ thing(x)

6.863j/9.611j lecture 11 sp03

requirements: canonical form

    inputs that mean the same thing have the same 

representation.
    huey eats kibble.
    kibble, huey will eat.
    what huey eats is kibble.
    it   s kibble that huey eats.

    alternatives

    four different semantic representations
    store all possible meaning representations in 

knowledge base

6.863j/9.611j lecture 11 sp03

requirements: compositionality

    can get meaning of    brown cow    from 

separate, independent meanings of 
   brown    and    cow   
    brown(x)(cid:217) cow(x)
    i   ve never seen a purple cow, i never 

hope to see one   

6.863j/9.611j lecture 11 sp03

barriers to compositionality

    ce corps qui s   appelait e qui s   appelle 

encore le saint empire romain n   etait en 
aucune maniere ni saint, ni romain, ni 
empire.

    this body, which called itself and still calls 
itself the holy roman empire, was neither 
holy, nor roman, nor an empire -voltaire

6.863j/9.611j lecture 11 sp03

need some kind of logical 
calculus

    not ideal as a meaning representation and 
doesn't do everything we want - but close
    supports the determination of truth
    supports compositionality of meaning
    supports question-answering (via variables)
    supports id136

    what are its elements?
    what else do we need?

6.863j/9.611j lecture 11 sp03

    logical connectives permit compositionality of 

meaning
kibble(x)    
likes(huey,x)
cat(vera) ^ weird(vera)
sleeping(huey) v eating(huey)

    expressions  can be assigned truth values, t 
or f, based on whether the propositions they 
represent are t or f in the world
    atomic formulae are t or f based on their 
presence or absence in a db (closed world 
assumption?)

    composed meanings are inferred from db and 

meaning of logical connectives

6.863j/9.611j lecture 11 sp03

    cat(huey)
    sibling(huey,vera)
    sibling(x,y) ^ cat(x)    
    cat(vera)??
    limitations:

cat(y)

    do    and    and    or    in natural language really 

mean    ^    and    v   ?  
mary got married and had a baby.
your money or your life!
he was happy but ignorant.

    does       

    mean    if   ?  

i   ll go if you promise to wear a tutu.

6.863j/9.611j lecture 11 sp03

    frame
having

haver:  s
hadthing: car

    all represent    linguistic meaning    of i 

have a car

and state of affairs in some world
    all consist of structures, composed of 

symbols representing objects and 
relations among them

6.863j/9.611j lecture 11 sp03

what

    what representation do we want for 

something like
john ate ice-cream    
ate(john, ice-cream)

    id198
    we   ll have to posit something that will do 

the work

    predicate of 2 arguments:

l x l y ate(y, x) 

6.863j/9.611j lecture 11 sp03

lambda application works

    suppose john, ice-cream = constants, 

i.e., l x.x, the identity function

    then lambda substitution does give the 

right results:
l x l y ate(y, x) (ice-cream)(john)   
l y ate(y, ice-cream)(john)   
ate(john, ice-cream)

but    where do we get the l

- forms from?

6.863j/9.611j lecture 11 sp03

example of what we now can do

athena>(top-level)
shall i clear the database? (y or n) y
sem-interpret>john saw mary in the park
ok.
sem-interpret>where did john see mary
in the park.
sem-interpret>john gave fido to mary
ok.
sem-interpret>who gave john fido
i don't know
sem-interpret>who gave mary fido
john
sem-interpret >john saw fido
ok.
sem-interpret>who did john see
fido and mary

6.863j/9.611j lecture 11 sp03

how: to recover meaning from 
structure

s

john=

*
np

vp

*= v*(np*)=

l xl y ate(y,x).ic= 

l y ate(y, ic)

john
l x.x, x=john

*
v

ate

l xl y ate(y,x)

np

*

=ice-cream

ice-cream

l x.x, x=ice-cream

6.863j/9.611j lecture 11 sp03

how

ate(john, ic)

*= vp*(np*)=l y ate(y, ic).john=

s

ate(john, ic)

john=

*
np

*=l y ate(y, ic)

vp

john
l x.x, x=john

*
v

ate

l xl y ate(y,x)

np

*

=ice-cream

ice-cream

l x.x, x=ice-cream

6.863j/9.611j lecture 11 sp03

in this picture

    the meaning of a sentence is the 

composition of a function vp* on an 
argument np*

    the lexical entries are l  forms
    simple nouns are just constants
    verbs are l  forms indicating their argument 

structure

    verb phrases return l

functions as their 

results (in fact     higher order)

6.863j/9.611j lecture 11 sp03

how

    application of the lambda form associated with 

the vp to the lambda form given by the 
argument np

    words just return    themselves    as values (from 

lexicon)

    given parse tree, then by working bottom up as 

shown next, we get to the logical form 
ate(john, ice-cream)

    this predicate can then be evaluated against a 

database     this is model interpretation- to 
return a value, or t/f, etc.

6.863j/9.611j lecture 11 sp03

code     sample rules

syntactic rule
(root ==> s)

semantic rule

(lambda (s)(process-sentence s))

(s ==> np vp)

(lambda (np vp)(funcall vp np)))

(vp ==> v+args)

(lambda (v+args)(lambda (subj)

(funcall v+args subj))))

(v+args ==> v2 np)(lambda (v2 np)

(lambda (subj)

(funcall v2 subj np))))

(np-pro ==> name) #'identity)

6.863j/9.611j lecture 11 sp03

on to semantic interpretation

four basic principles

   
1. rule-to-rule semantic interpretation [aka    syntax-

directed translation   ]: pair syntax, semantic rules.  (gpsg: 
pair each cf rule w/ semantic    action   ; as in compiler theory 
    due to knuth, 1968)

2. compositionality: meaning of a phrase is a function of 
the meaning of its parts and nothing more e.g., meaning of 
s    np vp is f(m(np)    m(vp)) (analog of    context-freeness   
for semantics     local)

3. truth conditional meaning: meaning of s equated with 

conditions that make it true

4. model theoretic semantics: correlation betw. language 

& world via set theory & mappings 

6.863j/9.611j lecture 11 sp03

syntax  & paired semantics

item or rule
verb ate
propn
v
s (or cp)
np
vp

semantic translation
l xl y.ate(y, x)
l x.x
v*= l  for lex entry
s*= vp*(np*)
n*
v*(np*)

6.863j/9.611j lecture 11 sp03

logic: lambda terms

    lambda terms: 

    a way of writing    anonymous functions    

    no function header or function name
    but defines the key thing: behavior of the function
    just as we can talk about 3 without naming it    x   

    let square = l p p*p
    equivalent to int square(p) { return p*p; }
    but we can talk about l p p*p without naming it
    format of a lambda term: l variable expression

6.863j/9.611j lecture 11 sp03

logic: lambda terms

    lambda terms:

    let square = l p p*p
    then square(3) =  (l p p*p)(3) = 3*3
    note: square(x) isn   t a function!  it   s just the value x*x.
    but lx square(x) = l x x*x = l p p*p = square

(proving that these functions are equal     and indeed they are,
as they act the same on all arguments: what is (l x square(x))(y)? )

    let even = l p (p mod 2 == 0)
    even(x) is true if x is even
    how about even(square(x))?  
    l x even(square(x)) is true of numbers with even squares

a predicate: returns true/false

    just apply rules to get l x (even(x*x)) = l x (x*x mod 2 == 0)
    this happens to denote the same predicate as even does

6.863j/9.611j lecture 11 sp03

logic: multiple arguments

    all lambda terms have one argument
    but we can fake multiple arguments ...

    suppose we want to write times(5,6)
    remember: square can be written as l x square(x)
    similarly, times is equivalent to l x l y times(x,y)

    claim that times(5)(6) means same as times(5,6)

    times(5) = (l x l y times(x,y)) (5) = l y times(5,y)

    if this function weren   t anonymous, what would we call it?

    times(5)(6) = (l y times(5,y))(6) = times(5,6)

6.863j/9.611j lecture 11 sp03

logic: multiple arguments

    all lambda terms have one argument
    but we can fake multiple arguments ...

    claim that times(5)(6) means same as times(5,6)

    times(5) = (l x l y times(x,y)) (5) = l y times(5,y)

    if this function weren   t anonymous, what would we call it?

    times(5)(6) = (l y times(5,y))(6) = times(5,6)

   so we can always get away with 1-arg functions ...

   ... which might return a function to take the next 

argument.  whoa.

   we   ll still allow times(x,y) as syntactic sugar, though

6.863j/9.611j lecture 11 sp03

grounding out

    so what does times actually mean???
    how do we get from times(5,6) to 30 ?

    whether times(5,6) = 30 depends on whether symbol times 

actually denotes the multiplication function!

    well, maybe times was defined as another lambda term, 

so substitute to get times(5,6) = (blah blah blah)(5)(6) 

    but we can   t keep doing substitutions forever!
    eventually we have to ground out in a primitive term
    primitive terms are bound to object code

    maybe times(5,6) just executes a multiplication function
    what is executed by loves(john, mary) ?

6.863j/9.611j lecture 11 sp03

logic: interesting constants

    thus, have    constants    that name some of 

the entities and functions (e.g., times):
    eminem - an entity
    red     a predicate on entities

    holds of just the red entities: red(x) is true if x is red!

    loves     a predicate on 2 entities

    loves(eminem,detroit)
    question: what does loves(detroit) denote?

    constants used to define meanings of words
    meanings of phrases will be built from the 

constants & syntactic structure

6.863j/9.611j lecture 11 sp03

how: to recover meaning from 
structure

s

john=

*
np

vp

*= v*(np*)=

l xl y ate(y,x).ic= 

l y ate(y, ic)

john
l x.x, x=john

*
v

ate

l xl y ate(y,x)

np

*

=ice-cream

ice-cream

l x.x, x=ice-cream

6.863j/9.611j lecture 11 sp03

how

ate(john, ic)

*= vp*(np*)=l y ate(y, ic).john=

s

ate(john, ic)

john=

*
np

*=l y ate(y, ic)

vp

john
l x.x, x=john

*
v

ate

l xl y ate(y,x)

np

*

=ice-cream

ice-cream

l x.x, x=ice-cream

6.863j/9.611j lecture 11 sp03

construction step by step     on 
np side

(root ==> s)(lambda (s)(process-sentence s)))

root

s ==> np vp

(lambda (np vp)(funcall vp np))

s (ip)

vp(np )= ate (john , ice-cream)

john

vp
v+args

john

np-pro

name

john

john

v2

ate

np

-

name

np-pro ==> name
#'identity

word-semantics

john

6.863j/9.611j lecture 11 sp03

in this picture

    the meaning of a sentence is the 

composition of a function vp* on an 
argument np*

    the lexical entries are l  forms
    simple nouns are just constants
    verbs are l  forms indicating their argument 

structure

    verb phrases return a function as its 

result

6.863j/9.611j lecture 11 sp03

processing order

    interpret subtree as soon as it is built    eg, as soon as 

rhs of rule is finished (complete subtree)

    picture:    ship off    subtree to semantic interpretation as 

soon as it is    done    syntactically

    allows for off-loading of syntactic short term memory; 

si returns with    ptr    to the interpretation

    natural order to doing things (if process left to right)
    has some psychological validity     tendency to interpret

asap & lower syntactic load

    example:  i told john a ghost story vs. i told john a 

ghost story was the last thing i wanted to hear

6.863j/9.611j lecture 11 sp03

picture 

s

np
name

john

s

np
john

s-i

6.863j/9.611j lecture 11 sp03

paired syntax-semantics

(root ==> s)(lambda (s)(process-sentence s)))

(s ==> np vp)

(lambda (np vp)(funcall vp np))

6.863j/9.611j lecture 11 sp03

