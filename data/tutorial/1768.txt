large-scale deep learning for
intelligent computer systems

jeff dean

google brain team in collaboration with many other teams 

growing use of deep learning at google

# of directories containing model description files

across many 
products/areas:

android
apps
gmail
image understanding
maps
nlp
photos
robotics
speech
translation
many research uses..
youtube
    many others ...

outline

two generations of deep learning software systems:

    1st generation: distbelief [dean et al., nips 2012]
    2nd generation: tensorflow (unpublished)
an overview of how we use these in research and products
plus, ...a new approach for training (people, not models)

google brain project started in 2011, with a focus on 
pushing state-of-the-art in neural networks.  initial 
emphasis:

    use large datasets, and 
    large amounts of computation

to push boundaries of what is possible in perception and 
language understanding

plenty of raw data

    text: trillions of words of english + other languages
    visual data: billions of images and videos
    audio: tens of thousands of hours of speech per day
    user activity: queries, marking messages spam, etc.
    id13: billions of labelled relation triples
    ...

how can we build systems that truly understand this data?

text understanding

   this movie should have never been made. from the poorly 
done animation, to the beyond bad acting. i am not sure at what 
point the people behind this movie said "ok, looks good! lets 
do it!" i was in awe of how truly horrid this movie was.   

turnaround time and effect on research

    minutes, hours:

    interactive research!  instant gratification!

    1-4 days

    tolerable
    interactivity replaced by running many experiments in parallel

    1-4 weeks:

    high value experiments only
    progress stalls

    >1 month

    don   t even try

important property of neural networks

results get better with

more data +

bigger models +
more computation

(better algorithms, new insights and improved 

techniques always help, too!)

how can we train large, powerful models quickly?
    exploit many kinds of parallelism

    model parallelism
    data parallelism

model parallelism

model parallelism

model parallelism

data parallelism

parameter servers

 

model
replicas

data

...
...

data parallelism

parameter servers

 

p

model
replicas

data

...
...

data parallelism

parameter servers

 

   p

p

model
replicas

data

...
...

data parallelism

parameter servers

p    = p +    p

 

   p

p

model
replicas

data

...
...

data parallelism

parameter servers

p    = p +    p

 

p   

...
...

model
replicas

data

data parallelism

parameter servers

 

   p   

p   

model
replicas

data

...
...

data parallelism

parameter servers

p       = p    +    p

 

   p   

p   

model
replicas

data

...
...

data parallelism

parameter servers

p       = p    +    p

 

   p   

p   

model
replicas

data

...
...

data parallelism choices

can do this synchronously:

    n replicas eqivalent to an n times larger batch size
    pro: no noise
    con: less fault tolerant (requires recovery if any single machine fails)

can do this asynchronously:

    con: noise in gradients
    pro: relatively fault tolerant (failure in model replica doesn   t block other 

replicas)

(or hybrid: m asynchronous groups of n synchronous replicas)

data parallelism considerations

want model computation time to be large relative to time to send/receive 
parameters over network

models with fewer parameters, that reuse each parameter multiple times in the 
computation

    mini-batches of size b reuse parameters b times

certain model structures reuse parameter many times within each example:

    convolutional models tend to reuse hundreds or thousands of times per 

example (for different spatial positions)

    recurrent models (lstms, id56s) tend to reuse tens to hundreds of times 

(for unrolling through t time steps during training)

what are some ways that
deep learning is having

a significant impact at google?

sequence to sequence models

oriol vinyals, ilya sutskever & quoc le started looking at how to map one 
sequence to another sequence:

0.1

0.5

1.0

0.0

2.4

the lazy brown fox jumped over the ...

sequence to sequence model

deep lstm

the lazy brown fox jumped over the ...

high dimensional 
representation of a 

sequence

0.1

0.5

1.0

0.0

2.4

connect two, you get a machine translation 
system

le renard brun rapide saute par dessus ...

the quick brown fox jumped over the ...

it works well

wmt   14

state-of-the-art

neural translation model

id7

37.0
37.3

sequence to sequence learning with neural networks
ilya sutskever, oriol vinyals, quoc v. le (nips 2014)

addressing rare word problems in neural translation models (arxiv.org/abs/1410.8206)
thang luong, ilya sutskever, oriol vinyals, quoc v. le, wojciech zaremba

    or a chatbot.

hi shiva, how are you?

i'm fine, thank you!

it produces sensible conversations.

user> hello , i have a problem with my machine
brainstop>  hi
brainstop>  this is shiva .
brainstop>  how are you doing today ?
user> hi shiva , how are you ?
brainstop>  i ' m fine , thank you !
brainstop>  how may i assist you today ?
user> i want to access using vpn
brainstop>  currently the computer is connected to corp network ?
user> no
brainstop>  check this solution : 
brainstop>  https : / / internal_hostname_redacted / articles / connecting - to - 
disconnecting - from - ssl - vpn # heading - 4
user> thanks , bye !
brainstop>  thank you .

    or a parser.

 

n:(s.17 n:(s.17 n:(np.11 p:nnp.53 n:) ...

allen is locked in, regardless of his situ...

it works well

completely learned parser with no parsing-specific code

state of the art results on wsj 23 parsing task

grammar as a foreign language, oriol vinyals, lukasz kaiser, terry koo, slav 
petrov, ilya sutskever, and geoffrey hinton (to appear in nips 2015)
http://arxiv.org/abs/1412.7449

    or something that can learn graph algorithms

output:
convex hull

(or delauney 
triangulation)

(or travelling 
salesman tour)

input:
collection 
of points

id193, oriol vinyals, meire fortunato, & 
navdeep jaitly (to appear in nips 2015)

object recognition improvement over time

predicted human performance

   cat   

id163 challenge winners

image models

   cat   

=

module with 6 

separate 

convolutional 

layers

24 layers deep

going deeper with convolutions

szegedy et al. cvpr 2015

good fine-grained classification

good generalization

both recognized as    meal   

sensible errors

works in practice     for real users

works in practice     for real users

connect sequence and image models, you get a 
captioning system

   a close up of a child holding a stuffed animal   

it works well (id7 scores)

dataset

ms coco

flickr

pascal (xfer learning)

sbu (weak label)

previous sota

show & tell

human

n/a

49

25

11

67

63

59

27

69

68

68

n/a

show and tell: a neural image caption generator,
oriol vinyals, alexander toshev, samy bengio, and dumitru erhan (cvpr 
2015)

tensorflow:

second generation deep learning system

motivations

distbelief (1st system) was great for scalability

not as flexible as we wanted for research purposes

better understanding of problem space allowed us to 
make some dramatic simplifications

tensorflow: expressing high-level ml computations

    core in c++

    very low overhead

    different front ends for specifying/driving the computation

    python and c++ today, easy to add more

c++ front end

python front end

...

core tensorflow execution system

cpu

gpu

android

ios

...

tensorflow example (batch id28)

graph = tf.graph()  
with graph.asdefault():
  examples = tf.constant(train_dataset)   
  labels = tf.constant(train_labels)

# create new computation graph

# training data/labels

  w = tf.variable(tf.truncated_normal([image_size * image_size, num_labels]))
  b = tf.variable(tf.zeros([num_labels]))

# variables

  logits = tf.mat_mul(examples, w) + b
  loss = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(logits, labels))

# training computation

  optimizer = tf.train.gradientdescentoptimizer(0.5).minimize(loss)
  prediction = tf.nn.softmax(logits)

# optimizer to use
# predictions for training data

tensorflow example (batch id28)

graph = tf.graph()  
with graph.asdefault():
  examples = tf.constant(train_dataset)   
  labels = tf.constant(train_labels)

# create new computation graph

# training data/labels

  w = tf.variable(tf.truncated_normal([image_size * image_size, num_labels]))
  b = tf.variable(tf.zeros([num_labels]))

# variables

  logits = tf.mat_mul(examples, w) + b
  loss = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(logits, labels))

# training computation

  optimizer = tf.train.gradientdescentoptimizer(0.5).minimize(loss)
  prediction = tf.nn.softmax(logits)

# optimizer to use
# predictions for training data

with tf.session(graph=graph) as session:
  tf.initializeallvariables().run()
  for step in xrange(num_steps):
    _, l, predictions = session.run([optimizer, loss, prediction])
    if (step % 100 == 0):
      print 'loss at step', step, ':', l
      print 'training accuracy: %.1f%%' % accuracy(predictions, labels)

# run & return 3 values

computation is a dataflow graph

graph of nodes, also called operations or ops.

add

relu

matmul

xent

biases

weights

examples

labels

computation is a dataflow graph

t e n s o r

s

t h  

w i

edges are n-dimensional arrays: tensors

add

relu

matmul

xent

biases

weights

examples

labels

computation is a dataflow graph

t h   s t a t e

w i

'biases' is a variable

some ops compute gradients

   = updates biases

biases

...

add

...

mul

   =

learning rate

computation is a dataflow graph

i b u t e d

d i s t

r

biases

device a

device b

...

add

...

mul

   =

learning rate

devices: processes, machines, gpus, etc

tensorflow: expressing high-level ml computations
automatically runs models on range of platforms:

from phones ...

to single machines (cpu and/or gpus)    

to distributed systems of many 100s of gpu cards

what is in a name?
    tensor: n-dimensional array

    1-dimension: vector
    2-dimension: matrix
    represent many dimensional data flowing through the graph

    e.g. image represented as 3-d tensor rows, cols, color
    flow: computation based on data flow graphs

    lots of operations (nodes in the graph) applied to data flowing through

    tensors flow through the graph        tensorflow   

    edges represent the tensors (data)
    nodes represent the processing

flexible

    general computational infrastructure

    deep learning support is a set of libraries on top of the core
    also useful for other machine learning algorithms
    possibly even for high performance computing (hpc) work
    abstracts away the underlying devices/computational hardware

extensible
    core system defines a number of standard operations 

and kernels (device-specific implementations of 
operations)

    easy to define new operators and/or kernels

deep learning in tensorflow

    typical neural net    layer    maps to one or more tensor operations

   

e.g. hidden layer: activations = relu(weights * inputs + biases)

    library of operations specialized for deep learning

    dozens of high-level operations: 2d and 3d convolutions, pooling, softmax, ...
    standard losses e.g. crossid178, l1, l2
    various optimizers e.g. id119, adagrad, l-bfgs, ...

    auto differentiation
    easy to experiment with (or combine!) a wide variety of different models:

lstms, convolutional models, id12, id23, 
embedding models, id63-like models, ...

no distinct parameter server subsystem
    parameters are now just stateful nodes in the graph
    data parallel training just a more complex graph

update

update

update

model 

computation

model 

computation

model 

computation

parameters

synchronous variant

update

add

gradient

gradient

gradient

model 

computation

model 

computation

model 

computation

parameters

nurturing great researchers

    we   re always looking for people with the potential to become excellent 

machine learning researchers

    the resurgence of deep learning in the last few years has caused a surge of 
interest of people who want to learn more and conduct research in this area

google brain residency program

new one year immersion program in deep learning research

learn to conduct deep learning research w/experts in our team
    fixed one-year employment with salary, benefits, ...
    goal after one year is to have conducted several research projects
    interesting problems, tensorflow, and access to computational resources

google brain residency program

who should apply? 
    people with bsc or msc, ideally in computer science, mathematics or statistics
    completed coursework in calculus, id202, and id203, or equiv.
    programming experience
    motivated, hard working, and have a strong interest in deep learning

google brain residency program

 program application & timeline

google brain residency program

for more information:

g.co/brainresidency

contact us:

brain-residency@google.com

questions?

