   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]theano-tutorial
    2. [10]theano tutorial.ipynb

theano tutorial[11]  

   theano is a software package which allows you to write symbolic code
   and compile it onto different architectures (in particular, cpu and
   gpu). it was developed by machine learning researchers at the
   university of montreal. its use is not limited to machine learning
   applications, but it was designed with machine learning in mind. it's
   especially good for machine learning techniques which are cpu-intensive
   and benefit from parallelization (e.g. large neural networks).

   this tutorial will cover the basic principles of theano, including some
   common mental blocks which come up. it will also cover a simple
   multi-layer id88 example. a more thorough theano tutorial can be
   found here: [12]http://deeplearning.net/software/theano/tutorial/

   any comments or suggestions should be directed to [13]me or feel free
   to [14]submit a pull request.
   in [1]:
%matplotlib inline

   in [2]:
# ensure python 3 forward compatibility
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt
import theano
# by convention, the tensor submodule is loaded as t
import theano.tensor as t

basics[15]  

symbolic variables[16]  

   in theano, all algorithms are defined symbolically. it's more like
   writing out math than writing code. the following theano variables are
   symbolic; they don't have an explicit value.
   in [3]:
# the theano.tensor submodule has various primitive symbolic variable types.
# here, we're defining a scalar (0-d) variable.
# the argument gives the variable its name.
foo = t.scalar('foo')
# now, we can define another variable bar which is just foo squared.
bar = foo**2
# it will also be a theano variable.
print(type(bar))
print(bar.type)
# using theano's pp (pretty print) function, we see that
# bar is defined symbolically as the square of foo
print(theano.pp(bar))

<class 'theano.tensor.var.tensorvariable'>
tensortype(float64, scalar)
(foo ** tensorconstant{2})

functions[17]  

   to actually compute things with theano, you define symbolic functions,
   which can then be called with actual values to retrieve an actual
   value.
   in [4]:
# we can't compute anything with foo and bar yet.
# we need to define a theano function first.
# the first argument of theano.function defines the inputs to the function.
# note that bar relies on foo, so foo is an input to this function.
# theano.function will compile code for computing values of bar given values of
foo
f = theano.function([foo], bar)
print(f(3))

9.0

   in [5]:
# alternatively, in some cases you can use a symbolic variable's eval method.
# this can be more convenient than defining a function.
# the eval method takes a dictionary where the keys are theano variables and the
 values are values for those variables.
print(bar.eval({foo: 3}))

9.0

   in [6]:
# we can also use python functions to construct theano variables.
# it seems pedantic here, but can make syntax cleaner for more complicated examp
les.
def square(x):
    return x**2
bar = square(foo)
print(bar.eval({foo: 3}))

9.0

theano.tensor[18]  

   theano also has variable types for vectors, matrices, and tensors. the
   theano.tensor submodule has various functions for performing operations
   on these variables.
   in [7]:
a = t.matrix('a')
x = t.vector('x')
b = t.vector('b')
y = t.dot(a, x) + b
# note that squaring a matrix is element-wise
z = t.sum(a**2)
# theano.function can compute multiple things at a time
# you can also set default parameter values
# we'll cover theano.config.floatx later
b_default = np.array([0, 0], dtype=theano.config.floatx)
linear_mix = theano.function([a, x, theano.in(b, value=b_default)], [y, z])
# supplying values for a, x, and b
print(linear_mix(np.array([[1, 2, 3],
                           [4, 5, 6]], dtype=theano.config.floatx), #a
                 np.array([1, 2, 3], dtype=theano.config.floatx), #x
                 np.array([4, 5], dtype=theano.config.floatx))) #b
# using the default value for b
print(linear_mix(np.array([[1, 2, 3],
                           [4, 5, 6]], dtype=theano.config.floatx), #a
                 np.array([1, 2, 3], dtype=theano.config.floatx))) #x

[array([ 18.,  37.]), array(91.0)]
[array([ 14.,  32.]), array(91.0)]

shared variables[19]  

   shared variables are a little different - they actually do have an
   explicit value, which can be get/set and is shared across functions
   which use the variable. they're also useful because they have state
   across function calls.
   in [8]:
shared_var = theano.shared(np.array([[1, 2], [3, 4]], dtype=theano.config.floatx
))
# the type of the shared variable is deduced from its initialization
print(shared_var.type())

<tensortype(float64, matrix)>

   in [9]:
# we can set the value of a shared variable using set_value
shared_var.set_value(np.array([[3, 4], [2, 1]], dtype=theano.config.floatx))
# ..and get it using get_value
print(shared_var.get_value())

[[ 3.  4.]
 [ 2.  1.]]

   in [10]:
shared_squared = shared_var**2
# the first argument of theano.function (inputs) tells theano what the arguments
 to the compiled function should be.
# note that because shared_var is shared, it already has a value, so it doesn't
need to be an input to the function.
# therefore, theano implicitly considers shared_var an input to a function using
 shared_squared and so we don't need
# to include it in the inputs argument of theano.function.
function_1 = theano.function([], shared_squared)
print(function_1())

[[  9.  16.]
 [  4.   1.]]

updates[20]  

   the value of a shared variable can be updated in a function by using
   the updates argument of theano.function.
   in [11]:
# we can also update the state of a shared var in a function
subtract = t.matrix('subtract')
# updates takes a dict where keys are shared variables and values are the new va
lue the shared variable should take
# here, updates will set shared_var = shared_var - subtract
function_2 = theano.function([subtract], shared_var, updates={shared_var: shared
_var - subtract})
print("shared_var before subtracting [[1, 1], [1, 1]] using function_2:")
print(shared_var.get_value())
# subtract [[1, 1], [1, 1]] from shared_var
function_2(np.array([[1, 1], [1, 1]], dtype=theano.config.floatx))
print("shared_var after calling function_2:")
print(shared_var.get_value())
# note that this also changes the output of function_1, because shared_var is sh
ared!
print("new output of function_1() (shared_var**2):")
print(function_1())

shared_var before subtracting [[1, 1], [1, 1]] using function_2:
[[ 3.  4.]
 [ 2.  1.]]
shared_var after calling function_2:
[[ 2.  3.]
 [ 1.  0.]]
new output of function_1() (shared_var**2):
[[ 4.  9.]
 [ 1.  0.]]

gradients[21]  

   a pretty huge benefit of using theano is its ability to compute
   gradients. this allows you to symbolically define a function and
   quickly compute its (numerical) derivative without actually deriving
   the derivative.
   in [12]:
# recall that bar = foo**2
# we can compute the gradient of bar with respect to foo like so:
bar_grad = t.grad(bar, foo)
# we expect that bar_grad = 2*foo
bar_grad.eval({foo: 10})

   out[12]:
array(20.0)

   in [13]:
# recall that y = ax + b
# we can also compute a jacobian like so:
y_j = theano.gradient.jacobian(y, x)
linear_mix_j = theano.function([a, x, b], y_j)
# because it's a linear mix, we expect the output to always be a
print(linear_mix_j(np.array([[9, 8, 7], [4, 5, 6]], dtype=theano.config.floatx),
 #a
                   np.array([1, 2, 3], dtype=theano.config.floatx), #x
                   np.array([4, 5], dtype=theano.config.floatx))) #b
# we can also compute the hessian with theano.gradient.hessian (skipping that he
re)

[[ 9.  8.  7.]
 [ 4.  5.  6.]]

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/gof/cmodule
.py:327: runtimewarning: numpy.ndarray size changed, may indicate binary incompa
tibility
  rval = __import__(module_name, {}, {}, [module_name])

debugging[22]  

   debugging in theano can be a little tough because the code which is
   actually being run is pretty far removed from the code you wrote. one
   simple way to sanity check your theano expressions before actually
   compiling any functions is to use test values.
   in [14]:
# let's create another matrix, "b"
b = t.matrix('b')
# and, a symbolic variable which is just a (from above) dotted against b
# at this point, theano doesn't know the shape of a or b, so there's no way for
it to know whether a dot b is valid.
c = t.dot(a, b)
# now, let's try to use it
c.eval({a: np.zeros((3, 4), dtype=theano.config.floatx),
        b: np.zeros((5, 6), dtype=theano.config.floatx)})

---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-14-75863a5c9f35> in <module>()
      6 # now, let's try to use it
      7 c.eval({a: np.zeros((3, 4), dtype=theano.config.floatx),
----> 8         b: np.zeros((5, 6), dtype=theano.config.floatx)})

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/gof/graph.p
yc in eval(self, inputs_to_values)
    465         args = [inputs_to_values[param] for param in inputs]
    466
--> 467         rval = self._fn_cache[inputs](*args)
    468
    469         return rval

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/fun
ction_module.pyc in __call__(self, *args, **kwargs)
    865                     node=self.fn.nodes[self.fn.position_of_error],
    866                     thunk=thunk,
--> 867                     storage_map=getattr(self.fn, 'storage_map', none))
    868             else:
    869                 # old-style linkers raise their own exceptions

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/gof/link.py
c in raise_with_op(node, thunk, exc_info, storage_map)
    312         # extra long error message in that case.
    313         pass
--> 314     reraise(exc_type, exc_value, exc_trace)
    315
    316

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/fun
ction_module.pyc in __call__(self, *args, **kwargs)
    853         t0_fn = time.time()
    854         try:
--> 855             outputs = self.fn()
    856         except exception:
    857             if hasattr(self.fn, 'position_of_error'):

valueerror: shape mismatch: x has 4 cols (and 3 rows) but y has 5 rows (and 6 co
ls)
apply node that caused the error: dot22(a, b)
toposort index: 0
inputs types: [tensortype(float64, matrix), tensortype(float64, matrix)]
inputs shapes: [(3, 4), (5, 6)]
inputs strides: [(32, 8), (48, 8)]
inputs values: ['not shown', 'not shown']
outputs clients: [['output']]

hint: re-running with most theano optimization disabled could give you a back-tr
ace of when this node was created. this can be done with by setting the theano f
lag 'optimizer=fast_compile'. if that does not work, theano optimizations can be
 disabled with 'optimizer=none'.
hint: use the theano flag 'exception_verbosity=high' for a debugprint and storag
e map footprint of this apply node.

   the above error message is a little opaque (and it would be even worse
   had we not given the theano variables a and b names). errors like this
   can be particularly confusing when the theano expression being computed
   is very complex. they also won't ever tell you the line number in your
   python code where a dot b was computed, because the actual code being
   run is not your python code-it's the compiled theano code! fortunately,
   "test values" let us get around this issue. n.b. - not all theano
   methods (for example, and significantly, scan) allow for test values
   in [15]:
# this tells theano we're going to use test values, and to warn when there's an
error with them.
# the setting 'warn' means "warn me when i haven't supplied a test value"
theano.config.compute_test_value = 'warn'
# setting the tag.test_value attribute gives the variable its test value
a.tag.test_value = np.random.random((3, 4)).astype(theano.config.floatx)
b.tag.test_value = np.random.random((5, 6)).astype(theano.config.floatx)
# now, we get an error when we compute c which points us to the correct line!
c = t.dot(a, b)

---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-15-038674a75ca1> in <module>()
      6 b.tag.test_value = np.random.random((5, 6)).astype(theano.config.floatx)
      7 # now, we get an error when we compute c which points us to the correct
line!
----> 8 c = t.dot(a, b)

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/tensor/basi
c.pyc in dot(a, b)
   5417         return tensordot(a, b, [[a.ndim - 1], [numpy.maximum(0, b.ndim -
 2)]])
   5418     else:
-> 5419         return _dot(a, b)
   5420
   5421

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/gof/op.pyc
in __call__(self, *inputs, **kwargs)
    649                 thunk.outputs = [storage_map[v] for v in node.outputs]
    650
--> 651                 required = thunk()
    652                 assert not required  # we provided all inputs
    653

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/gof/op.pyc
in rval(p, i, o, n)
    863             # default arguments are stored in the closure of `rval`
    864             def rval(p=p, i=node_input_storage, o=node_output_storage, n
=node):
--> 865                 r = p(n, [x[0] for x in i], o)
    866                 for o in node.outputs:
    867                     compute_map[o][0] = true

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/tensor/basi
c.pyc in perform(self, node, inp, out)
   5235         # gives a numpy float object but we need to return a 0d
   5236         # ndarray
-> 5237         z[0] = numpy.asarray(numpy.dot(x, y))
   5238
   5239     def grad(self, inp, grads):

valueerror: shapes (3,4) and (5,6) not aligned: 4 (dim 1) != 5 (dim 0)

   in [16]:
# we won't be using test values for the rest of the tutorial.
theano.config.compute_test_value = 'off'

   another place where debugging is useful is when an invalid calculation
   is done, e.g. one which results in nan. by default, theano will
   silently allow these nan values to be computed and used, but this
   silence can be catastrophic to the rest of your theano computation. at
   the cost of speed, we can instead have theano compile functions in
   debugmode, where an invalid computation causes an error
   in [17]:
# a simple division function
num = t.scalar('num')
den = t.scalar('den')
divide = theano.function([num, den], num/den)
print(divide(10, 2))
# this will cause a nan
print(divide(0, 0))

5.0
nan

   in [18]:
# to compile a function in debug mode, just set mode='debugmode'
divide = theano.function([num, den], num/den, mode='debugmode')
# nans now cause errors
print(divide(0, 0))

---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-18-fd8e17a1c37b> in <module>()
      1 # to compile a function in debug mode, just set mode='debugmode'
----> 2 divide = theano.function([num, den], num/den, mode='debugmode')
      3 # nans now cause errors
      4 print(divide(0, 0))

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/fun
ction.pyc in function(inputs, outputs, mode, updates, givens, no_default_updates
, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused
_input)
    306                    on_unused_input=on_unused_input,
    307                    profile=profile,
--> 308                    output_keys=output_keys)
    309     # we need to add the flag check_aliased inputs if we have any mutabl
e or
    310     # borrowed used defined inputs

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/pfu
nc.pyc in pfunc(params, outputs, mode, updates, givens, no_default_updates, acce
pt_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input
, output_keys)
    524                          accept_inplace=accept_inplace, name=name,
    525                          profile=profile, on_unused_input=on_unused_inpu
t,
--> 526                          output_keys=output_keys)
    527
    528

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/fun
ction_module.pyc in orig_function(inputs, outputs, mode, accept_inplace, name, p
rofile, on_unused_input, output_keys)
   1768                    on_unused_input=on_unused_input,
   1769                    output_keys=output_keys).create(
-> 1770             defaults)
   1771
   1772     t2 = time.time()

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/deb
ugmode.pyc in create(self, defaults, trustme, storage_map)
   2638         # get a function instance
   2639         _fn, _i, _o = self.linker.make_thunk(input_storage=input_storage
,
-> 2640                                              storage_map=storage_map)
   2641         fn = self.function_builder(_fn, _i, _o, self.indices,
   2642                                    self.outputs, defaults, self.unpack_s
ingle,

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/gof/link.py
c in make_thunk(self, input_storage, output_storage, storage_map)
    688         return self.make_all(input_storage=input_storage,
    689                              output_storage=output_storage,
--> 690                              storage_map=storage_map)[:3]
    691
    692     def make_all(self, input_storage, output_storage):

/usr/local/lib/python2.7/site-packages/theano-0.7.0-py2.7.egg/theano/compile/deb
ugmode.pyc in make_all(self, profiler, input_storage, output_storage, storage_ma
p)
   1945         # precompute some things for storage pre-allocation
   1946         try:
-> 1947             def_val = int(config.unittests.rseed)
   1948         except valueerror:
   1949             def_val = 666

attributeerror: 'theanoconfigparser' object has no attribute 'unittests'

using the cpu vs gpu[23]  

   theano can transparently compile onto different hardware. what device
   it uses by default depends on your .theanorc file and any environment
   variables defined, as described in detail here:
   [24]http://deeplearning.net/software/theano/library/config.html
   currently, you should use float32 when using most gpus, but most people
   prefer to use float64 on a cpu. for convenience, theano provides the
   floatx configuration variable which designates what float accuracy to
   use. for example, you can run a python script with certain environment
   variables set to use the cpu:

   theano_flags=device=cpu,floatx=float64 python your_script.py

   or gpu:

   theano_flags=device=gpu,floatx=float32 python your_script.py
   in [19]:
# you can get the values being used to configure theano like so:
print(theano.config.device)
print(theano.config.floatx)

cpu
float64

   in [20]:
# you can also get/set them at runtime:
old_floatx = theano.config.floatx
theano.config.floatx = 'float32'

   in [21]:
# be careful that you're actually using floatx!
# for example, the following will cause var to be a float64 regardless of floatx
 due to numpy defaults:
var = theano.shared(np.array([1.3, 2.4]))
print(var.type()) #!!!
# so, whenever you use a numpy array, make sure to set its dtype to theano.confi
g.floatx
var = theano.shared(np.array([1.3, 2.4], dtype=theano.config.floatx))
print(var.type())
# revert to old value
theano.config.floatx = old_floatx

<tensortype(float64, vector)>
<tensortype(float32, vector)>

example: mlp[25]  

   defining a multilayer id88 is out of the scope of this tutorial;
   please see here for background information:
   [26]http://en.wikipedia.org/wiki/multilayer_id88. we will be
   using the convention that datapoints are column vectors.

layer class[27]  

   we'll be defining our multilayer id88 as a series of "layers",
   each applied successively to the input to produce the network output.
   each layer is defined as a class, which stores a weight matrix and a
   bias vector and includes a function for computing the layer's output.

   note that if we weren't using theano, we might expect the output method
   to take in a vector and return the layer's activation in response to
   this input. however, with theano, the output function is instead meant
   to be used to create (using theano.function) a function which can take
   in a vector and return the layer's activation. so, if you were to pass,
   say, a np.ndarray to the layer class's output function, you'd get an
   error. instead, we'll construct a function for actually computing the
   layer's activation outside of the class itself.
   in [22]:
class layer(object):
    def __init__(self, w_init, b_init, activation):
        '''
        a layer of a neural network, computes s(wx + b) where s is a nonlinearit
y and x is the input vector.

        :parameters:
            - w_init : np.ndarray, shape=(n_output, n_input)
                values to initialize the weight matrix to.
            - b_init : np.ndarray, shape=(n_output,)
                values to initialize the bias vector
            - activation : theano.tensor.elemwise.elemwise
                activation function for layer output
        '''
        # retrieve the input and output dimensionality based on w's initializati
on
        n_output, n_input = w_init.shape
        # make sure b is n_output in size
        assert b_init.shape == (n_output,)
        # all parameters should be shared variables.
        # they're used in this class to compute the layer output,
        # but are updated elsewhere when optimizing the network parameters.
        # note that we are explicitly requiring that w_init has the theano.confi
g.floatx dtype
        self.w = theano.shared(value=w_init.astype(theano.config.floatx),
                               # the name parameter is solely for printing purpo
rses
                               name='w',
                               # setting borrow=true allows theano to use user m
emory for this object.
                               # it can make code slightly faster by avoiding a
deep copy on construction.
                               # for more details, see
                               # http://deeplearning.net/software/theano/tutoria
l/aliasing.html
                               borrow=true)
        # we can force our bias vector b to be a column vector using numpy's res
hape method.
        # when b is a column vector, we can pass a matrix-shaped input to the la
yer
        # and get a matrix-shaped output, thanks to broadcasting (described belo
w)
        self.b = theano.shared(value=b_init.reshape(n_output, 1).astype(theano.c
onfig.floatx),
                               name='b',
                               borrow=true,
                               # theano allows for broadcasting, similar to nump
y.
                               # however, you need to explicitly denote which ax
es can be broadcasted.
                               # by setting broadcastable=(false, true), we are
denoting that b
                               # can be broadcast (copied) along its second dime
nsion in order to be
                               # added to another variable.  for more informatio
n, see
                               # http://deeplearning.net/software/theano/library
/tensor/basic.html
                               broadcastable=(false, true))
        self.activation = activation
        # we'll compute the gradient of the cost of the network with respect to
the parameters in this list.
        self.params = [self.w, self.b]

    def output(self, x):
        '''
        compute this layer's output given an input

        :parameters:
            - x : theano.tensor.var.tensorvariable
                theano symbolic variable for layer input

        :returns:
            - output : theano.tensor.var.tensorvariable
                mixed, biased, and activated x
        '''
        # compute linear mix
        lin_output = t.dot(self.w, x) + self.b
        # output is just linear mix if no activation function
        # otherwise, apply the activation function
        return (lin_output if self.activation is none else self.activation(lin_o
utput))

mlp class[28]  

   most of the functionality of our mlp is contained in the layer class;
   the mlp class is essentially just a container for a list of layers and
   their parameters. the output function simply recursively computes the
   output for each layer. finally, the squared_error returns the squared
   euclidean distance between the output of the network given an input and
   the desired (ground truth) output. this function is meant to be used as
   a cost in the setting of minimizing cost over some training data. as
   above, the output and squared error functions are not to be used for
   actually computing values; instead, they're to be used to create
   functions which are used to compute values.
   in [23]:
class mlp(object):
    def __init__(self, w_init, b_init, activations):
        '''
        multi-layer id88 class, computes the composition of a sequence of
layers

        :parameters:
            - w_init : list of np.ndarray, len=n
                values to initialize the weight matrix in each layer to.
                the layer sizes will be inferred from the shape of each matrix i
n w_init
            - b_init : list of np.ndarray, len=n
                values to initialize the bias vector in each layer to
            - activations : list of theano.tensor.elemwise.elemwise, len=n
                activation function for layer output for each layer
        '''
        # make sure the input lists are all of the same length
        assert len(w_init) == len(b_init) == len(activations)

        # initialize lists of layers
        self.layers = []
        # construct the layers
        for w, b, activation in zip(w_init, b_init, activations):
            self.layers.append(layer(w, b, activation))

        # combine parameters from all layers
        self.params = []
        for layer in self.layers:
            self.params += layer.params

    def output(self, x):
        '''
        compute the mlp's output given an input

        :parameters:
            - x : theano.tensor.var.tensorvariable
                theano symbolic variable for network input

        :returns:
            - output : theano.tensor.var.tensorvariable
                x passed through the mlp
        '''
        # recursively compute output
        for layer in self.layers:
            x = layer.output(x)
        return x

    def squared_error(self, x, y):
        '''
        compute the squared euclidean error of the network output against the "t
rue" output y

        :parameters:
            - x : theano.tensor.var.tensorvariable
                theano symbolic variable for network input
            - y : theano.tensor.var.tensorvariable
                theano symbolic variable for desired network output

        :returns:
            - error : theano.tensor.var.tensorvariable
                the squared euclidian distance between the network output and y
        '''
        return t.sum((self.output(x) - y)**2)

id119[29]  

   to train the network, we will minimize the cost (squared euclidean
   distance of network output vs. ground-truth) over a training set using
   id119. when doing id119 on neural nets, it's very
   common to use momentum, which mixes in the previous update to the
   current update. this tends to make the network converge more quickly on
   a good solution and can help avoid local minima in the cost function.
   with traditional id119, we are guaranteed to decrease the
   cost at each iteration. when we use momentum, we lose this guarantee,
   but this is generally seen as a small price to pay for the improvement
   momentum usually gives.

   in theano, we store the previous parameter update as a shared variable
   so that its value is preserved across iterations. then, during the
   gradient update, we not only update the parameters, but we also update
   the previous parameter update shared variable.
   in [24]:
def gradient_updates_momentum(cost, params, learning_rate, momentum):
    '''
    compute updates for id119 with momentum

    :parameters:
        - cost : theano.tensor.var.tensorvariable
            theano cost function to minimize
        - params : list of theano.tensor.var.tensorvariable
            parameters to compute gradient against
        - learning_rate : float
            id119 learning rate
        - momentum : float
            momentum parameter, should be at least 0 (standard id119)
 and less than 1

    :returns:
        updates : list
            list of updates, one for each parameter
    '''
    # make sure momentum is a sane value
    assert momentum < 1 and momentum >= 0
    # list of update steps for each parameter
    updates = []
    # just id119 on cost
    for param in params:
        # for each parameter, we'll create a previous_step shared variable.
        # this variable will keep track of the parameter's update step across it
erations.
        # we initialize it to 0
        previous_step = theano.shared(param.get_value()*0., broadcastable=param.
broadcastable)
        # each parameter is updated by taking a step in the direction of the gra
dient.
        # however, we also "mix in" the previous step according to the given mom
entum value.
        # note that we don't need to derive id26 to compute updates -
 just use t.grad!
        step = momentum*previous_step - learning_rate*t.grad(cost, param)
        # add an update to store the previous step value
        updates.append((previous_step, step))
        # add an update to apply the id119 step to the parameter itse
lf
        updates.append((param, param + step))
    return updates

toy example[30]  

   we'll train our neural network to classify two gaussian-distributed
   clusters in 2d space.
   in [25]:
# training data - two randomly-generated gaussian-distributed clouds of points i
n 2d space
np.random.seed(0)
# number of points
n = 1000
# labels for each cluster
y = np.random.random_integers(0, 1, n)
# mean of each cluster
means = np.array([[-1, 1], [-1, 1]])
# covariance (in x and y direction) of each cluster
covariances = np.random.random_sample((2, 2)) + 1
# dimensions of each point
x = np.vstack([np.random.randn(n)*covariances[0, y] + means[0, y],
               np.random.randn(n)*covariances[1, y] + means[1, y]]).astype(thean
o.config.floatx)
# convert to targets, as floatx
y = y.astype(theano.config.floatx)
# plot the data
plt.figure(figsize=(8, 8))
plt.scatter(x[0, :], x[1, :], c=y, lw=.3, s=3, cmap=plt.cm.cool)
plt.axis([-6, 6, -6, 6])
plt.show()

   [x96d6tq +d2txwaaaabjru5erkjggg== ]
   in [26]:
# first, set the size of each layer (and the number of layers)
# input layer size is training data dimensionality (2)
# output size is just 1-d: class label - 0 or 1
# finally, let the hidden layers be twice the size of the input.
# if we wanted more layers, we could just add another layer size to this list.
layer_sizes = [x.shape[0], x.shape[0]*2, 1]
# set initial parameter values
w_init = []
b_init = []
activations = []
for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):
    # getting the correct initialization matters a lot for non-toy problems.
    # however, here we can just use the following initialization with success:
    # normally distribute initial weights
    w_init.append(np.random.randn(n_output, n_input))
    # set initial biases to 1
    b_init.append(np.ones(n_output))
    # we'll use sigmoid activation for all layers
    # note that this doesn't make a ton of sense when using squared distance
    # because the sigmoid function is bounded on [0, 1].
    activations.append(t.nnet.sigmoid)
# create an instance of the mlp class
mlp = mlp(w_init, b_init, activations)

# create theano variables for the mlp input
mlp_input = t.matrix('mlp_input')
# ... and the desired output
mlp_target = t.vector('mlp_target')
# learning rate and momentum hyperparameter values
# again, for non-toy problems these values can make a big difference
# as to whether the network (quickly) converges on a good local minimum.
learning_rate = 0.01
momentum = 0.9
# create a function for computing the cost of the network given an input
cost = mlp.squared_error(mlp_input, mlp_target)
# create a theano function for training the network
train = theano.function([mlp_input, mlp_target], cost,
                        updates=gradient_updates_momentum(cost, mlp.params, lear
ning_rate, momentum))
# create a theano function for computing the mlp's output given some input
mlp_output = theano.function([mlp_input], mlp.output(mlp_input))

   in [27]:
# keep track of the number of training iterations performed
iteration = 0
# we'll only train the network with 20 iterations.
# a more common technique is to use a hold-out validation set.
# when the validation error starts to increase, the network is overfitting,
# so we stop training the net.  this is called "early stopping", which we won't
do here.
max_iteration = 20
while iteration < max_iteration:
    # train the network using the entire training set.
    # with large datasets, it's much more common to use stochastic or mini-batch
 id119
    # where only a subset (or a single point) of the training set is used at eac
h iteration.
    # this can also help the network to avoid local minima.
    current_cost = train(x, y)
    # get the current network output for all points in the training set
    current_output = mlp_output(x)
    # we can compute the accuracy by thresholding the output
    # and computing the proportion of points whose class match the ground truth
class.
    accuracy = np.mean((current_output > .5) == y)
    # plot network output after this iteration
    plt.figure(figsize=(8, 8))
    plt.scatter(x[0, :], x[1, :], c=current_output,
                lw=.3, s=3, cmap=plt.cm.cool, vmin=0, vmax=1)
    plt.axis([-6, 6, -6, 6])
    plt.title('cost: {:.3f}, accuracy: {:.3f}'.format(float(current_cost), accur
acy))
    plt.show()
    iteration += 1

   [a69hhewnqmzbaaaaaelftksuqmcc ]
   [d9kwiqqqggh
   kaetqgghhkaetaghhhcaejahhbdcaurahbbccacoarncccec+d+kyw5fygpx6waaaabjru5
   erkjg gg== ]
   [og3aaae7otaksrbid35ecgdqcneqqggh5p8oarncccecoarmid35eciasmcgeemibssce
   eeiibygbe0iiirygbewiiyrw4h9kldvbti3xpqaaaabjru5erkjggg== ]
   [q36fupqsau8qffnmb4phudqrchiqqqggh+qwqbygeeel+jxiwiyqqwgfk
   wiqqqgghkaetqgghhkaetaghhhcaejahhbdcaurahbbccaf+b8wqgwkxooibaaaaaelftks
   uqmcc ]
   [4hh4rkherpyiiaaaaasuvork5cyii= ]
   [sqawo7edqrahiyqqwoewk8rbid35ekp+hbewi
   iyrwgbiwiyqqwgfkwiqqqgghkaetqgghhkaetaghhhcaejahhbdcgf8hvzfak3ujnqmaaaa
   asuvo rk5cyii= ]
   [0wasa+yyffeab4q74dqrahiyqqwoemk8rbid35ekp+hbewiiyrwgbiwiyqqwgfk
   wiqqqgghkaetqgghhkaetaghhhcaejahhbdcgf8h5adqehhhsd0aaaaasuvork5cyii= ]
   [aafwnskj67hiaaaaaelftksuqmcc ]
   [4ri7vjmaqviaaaaasuvork5cyii= ]
   [ygbe0iiirygbewiiyrwgbiwiyqqwgfkwiqqqgghkaetqgghhkaetaghhhdgf4mj
   quivgw0laaaaaelftksuqmcc ]
   [zz0waa aabjru5erkjggg== ]
   [7ok4jyudml8aaaaasuvork5cyii= ]
   [adaygv4wxkzdaaaaaelftksuqmcc ]
   [ac8hafxt5gmuaaaaaelftksuqmcc ]
   [b7hwd6z7yjf+jgjdhgbjbrhtgyy4wx9v84atpgggmc4atmggomcyatmgommsyatscm
   mcayadgbm8yyywlgbmwyy4wj4p8avfsnk3bxyiyaaaaasuvork5cyii= ]
   [g8bwbqfwcujha+w+6ak8gldtiouxgtnhsavacwn77h4dgrdza3jndarczehaqaewbaaqcl
   isjrkzgzxbua6awalxdxfpxry4hot4wxpqx89ylxgwfgzfyh3zsa0coxe3khdsyyy0wgqby
   qb2om
   mcb+jxmwy4wxjgnowiwxxpgmoaezxhhjmuaezbhjjmmaezbjjdema07ajdhgmaz+b1bgnam
   6ykgw aaaaaelftksuqmcc ]
   [avogmxuevrnl aaaaaelftksuqmcc ]
   [i6saaaaabjru5e rkjggg== ]
   [4dqbqjbakshcheeiioyjew4hdid35eep+rbcyeeekoqbkweeiioqjjweiiiyqk
   jaeliyqqkpaeliqqqqhaeraqqgihgv8dcgyjwci89byaaaaasuvork5cyii= ]
   [wpejk5ems2hzqaaaabjru5erkjggg== ]

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [31]fastly, rendered by [32]rackspace

   nbviewer github [33]repository.

   nbviewer version: [34]33c4683

   nbconvert version: [35]5.4.0

   rendered (fri, 05 apr 2019 17:56:34 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb
   5. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb
   6. https://github.com/craffel/theano-tutorial/blob/master/theano tutorial.ipynb
   7. https://mybinder.org/v2/gh/craffel/theano-tutorial/master?filepath=theano tutorial.ipynb
   8. https://raw.githubusercontent.com/craffel/theano-tutorial/master/theano tutorial.ipynb
   9. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/tree/master
  10. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/tree/master/theano tutorial.ipynb
  11. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#theano-tutorial
  12. http://deeplearning.net/software/theano/tutorial/
  13. http://colinraffel.com/
  14. https://github.com/craffel/theano-tutorial/pulls
  15. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#basics
  16. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#symbolic-variables
  17. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#functions
  18. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#theano.tensor
  19. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#shared-variables
  20. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#updates
  21. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#gradients
  22. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#debugging
  23. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#using-the-cpu-vs-gpu
  24. http://deeplearning.net/software/theano/library/config.html
  25. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#example:-mlp
  26. http://en.wikipedia.org/wiki/multilayer_id88
  27. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#layer-class
  28. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#mlp-class
  29. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#gradient-descent
  30. https://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb#toy-example
  31. http://www.fastly.com/
  32. https://developer.rackspace.com/?nbviewer=awesome
  33. https://github.com/jupyter/nbviewer
  34. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  35. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
