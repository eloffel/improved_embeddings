   #[1]rss: 40 latest updates [2]rss: 40 newest packages [3]pypi

   [4]skip to main content (button) switch to mobile version

   warning: some features may not work without javascript. please try
   enabling it if you encounter problems.

   [5]pypi
   search pypi ____________________ (button) search

   [6]help [7]donate [8]log in [9]register

   search pypi ____________________ (button) search

theanets 0.7.3

   pip install theanets (button) copy pip instructions
   [10]latest version

   last released: jan 28, 2016

   feedforward and recurrent neural nets using theano

navigation

   [11]project description [12]release history [13]download files

project links

   [14]homepage

statistics

   github statistics: stars: forks: open issues/prs:

   view statistics for this project via [15]libraries.io, or by using
   [16]google bigquery

meta

   license: mit license (mit)

   author: [17]leif johnson

   tags: machine-learning, neural-network, deep-neural-network,
   recurrent-neural-network, autoencoder, sparse-autoencoder, classifier,
   theano

maintainers

   [18]avatar for lmjohns3 from gravatar.com [19]lmjohns3

classifiers

   development status
          [20]3 - alpha

   intended audience
          [21]science/research

   license
          [22]osi approved :: mit license

   operating system
          [23]os independent

   topic
          [24]scientific/engineering
          [25]scientific/engineering :: artificial intelligence

   [26]project description [27]project details [28]release history
   [29]download files

project description

   https://travis-ci.org/lmjohns3/theanets.svg?branch=master
   [30]https://coveralls.io/repos/lmjohns3/theanets/badge.svg?branch=maste
   r

theanets

   the theanets package is a deep learning and neural network toolkit. it
   is written in python to interoperate with excellent tools like numpy
   and scikit-learn, and it uses [31]theano to accelerate computations
   when possible using your gpu. the package aims to provide:
     * a simple api for building and training common types of neural
       network models;
     * thorough documentation;
     * easy-to-read code;
     * and, under the hood, a fully expressive graph computation
       framework.

   the package strives to    make the easy things easy and the difficult
   things possible.    please try it out, and let us know what you think!

installation

   install the latest published code using pip:
pip install theanets

   or download the current source and run it from there:
git clone http://github.com/lmjohns3/theanets
cd theanets
python setup.py develop

   note that the contents of this readme are linked to the version of
   theanets that you are using. if you   re looking at the [32]readme on
   github, you   ll need the current github code; if you   re looking at the
   [33]readme for a pypi release, you   ll need the code associated with
   that release.

quick start: classification

   suppose you want to create a classifier and train it on some
   100-dimensional data points that you   ve classified into 10 categories.
   no problem! with just a few lines you can (a) provide some data, (b)
   build and (c) train a model, and (d) evaluate the model:
import climate
import theanets
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix

climate.enable_default_logging()

# create a classification dataset.
x, y = make_classification(
    n_samples=3000, n_features=100, n_classes=10, n_informative=10)
x = x.astype('f')
y = y.astype('i')
cut = int(len(x) * 0.8)  # training / validation split
train = x[:cut], y[:cut]
valid = x[cut:], y[cut:]

# build a classifier model with 100 inputs and 10 outputs.
net = theanets.classifier(layers=[100, 10])

# train the model using sgd with momentum.
net.train(train, valid, algo='sgd', learning_rate=1e-4, momentum=0.9)

# show confusion matrices on the training/validation splits.
for label, (x, y) in (('training:', train), ('validation:', valid)):
    print(label)
    print(confusion_matrix(y, net.predict(x)))

layers

   the model above is quite simplistic! make it a bit more sophisticated
   by adding a hidden layer:
net = theanets.classifier([100, 1000, 10])

   in fact, you can just as easily create 3 (or any number of) hidden
   layers:
net = theanets.classifier([100, 1000, 1000, 1000, 10])

   by default, hidden layers use the relu transfer function. by passing a
   tuple instead of just an integer, you can change some of these layers
   to use different [34]activations:
maxout = (1000, 'maxout:4')  # maxout with 4 pieces.
net = theanets.classifier([
    100, 1000, maxout, (1000, 'tanh'), 10])

   by passing a dictionary instead, you can specify even more attributes
   of each [35]layer, like how its parameters are initialized:
# sparsely-initialized layer with large nonzero weights.
foo = dict(name='foo', size=1000, std=1, sparsity=0.9)
net = theanets.classifier([
    100, foo, (1000, 'maxout:4'), (1000, 'tanh'), 10])

   specifying layers is the heart of building models in theanets. read
   more about this in [36]specifying layers.

id173

   adding regularizers is easy, too! just pass them to the training
   method. for instance, you can train up a sparse classification model
   with weight decay:
# penalize hidden-unit activity (l1 norm) and weights (l2 norm).
net.train(train, valid, hidden_l1=0.001, weight_l2=0.001)

   in theanets dropout is treated as a regularizer and can be set on many
   layers at once:
net.train(train, valid, hidden_dropout=0.5)

   or just on a specific layer:
net.train(train, valid, dropout={'foo:out': 0.5})

   similarly, you can add gaussian noise to any of the layers (here, just
   to the input layer):
net.train(train, valid, input_noise=0.3)

optimization algorithms

   you can optimize your model using any of the algorithms provided by
   [37]downhill (sgd, nag, rmsprop, adadelta, etc.), or additionally using
   a couple of [38]pretraining methods specific to neural networks.

   you can also make as many successive calls to train() as you like. each
   call can include different training algorithms:
net.train(train, valid, algo='rmsprop')
net.train(train, valid, algo='nag')

   different learning hyperparameters:
net.train(train, valid, algo='rmsprop', learning_rate=0.1)
net.train(train, valid, algo='rmsprop', learning_rate=0.01)

   and different id173 hyperparameters:
net.train(train, valid, input_noise=0.7)
net.train(train, valid, input_noise=0.3)

   training models is a bit more art than science, but theanets tries to
   make it easy to evaluate different training approaches. read more about
   this in [39]training a model.

quick start: recurrent models

   recurrent neural networks are becoming quite important for many
   sequence-based tasks in machine learning; one popular toy example for
   recurrent models is to generate text that   s similar to some body of
   training text.

   in these models, a recurrent classifier is set up to predict the
   identity of the next character in a sequence of text, given all of the
   preceding characters. the inputs to the model are the one-hot encodings
   of a sequence of characters from the text, and the corresponding
   outputs are the class labels of the subsequent character. the theanets
   code has a [40]text helper class that provides easy encoding and
   decoding of text to and from integer classes; using the helper makes
   the top-level code look like:
import numpy as np, re, theanets

chars = re.sub(r'\s+', ' ', open('corpus.txt').read().lower())
txt = theanets.recurrent.text(chars, min_count=10)
a = 1 + len(txt.alpha)  # of letter classes

# create a model to train: input -> gru -> relu -> softmax.
net = theanets.recurrent.classifier([a, (100, 'gru'), (1000, 'relu'), a])

# train the model iteratively; draw a sample after every epoch.
seed = txt.encode(txt.text[300017:300050])
for tm, _ in net.itertrain(txt.classifier_batches(100, 32), momentum=0.9):
    print('{}|{} ({:.1f}%)'.format(
        txt.decode(seed),
        txt.decode(net.predict_sequence(seed, 40)),
        100 * tm['acc']))

   this example uses several features of theanets that make modeling
   neural networks fun and interesting. the model uses a layer of
   [41]id149 to capture the temporal dependencies in the
   data. it also [42]uses a callable to provide data to the model, and
   takes advantage of [43]iterative training to sample an output from the
   model after each training epoch.

   to run this example, download a text you   d like to model (e.g., herman
   melville   s moby dick) and save it in corpus.txt:
curl http://www.gutenberg.org/cache/epub/2701/pg2701.txt > corpus.txt

   then when you run the script, the output might look something like this
   (abbreviated to show patterns):
used for light, but only as an oi|pr vgti ki nliiariiets-a, o t.;to niy  , (16.6
%)
used for light, but only as an oi|s bafsvim-te i"eg nadg tiaraiatlrekls tv (20.2
%)
used for light, but only as an oi|vetr uob bsyeatit is-ad. agtat girirole, (28.5
%)
used for light, but only as an oi|siy thinle wonl'th, in the begme sr"hey  (29.9
%)
used for light, but only as an oi|nr. bonthe the tuout honils ohe thib th  (30.5
%)
used for light, but only as an oi|kg that mand sons an, of,rtopit bale thu (31.0
%)
used for light, but only as an oi|nsm blasc yan, ang theate thor wille han (32.1
%)
used for light, but only as an oi|b thea mevind, int amat ars sif istuad p (33.3
%)
used for light, but only as an oi|msenge bie therale hing, aik asmeatked s (34.1
%)
used for light, but only as an oi|ge," rrermondy ghe e comasnig that urle  (35.5
%)
used for light, but only as an oi|s or thartich comase surt thant seaiceng (36.1
%)
used for light, but only as an oi|s lot fircennor, unding dald bots trre i (37.1
%)
used for light, but only as an oi|st onderass noptand. "peles, suiondes is (38.2
%)
used for light, but only as an oi|gnith. s. lited, anca! stobbease so las, (39.3
%)
used for light, but only as an oi|chics fleet dong berieribus armor has or (40.1
%)
used for light, but only as an oi|cs and quirbout detom tis glome dold pco (41.1
%)
used for light, but only as an oi|nht shome wand, the your at movernife lo (42.0
%)
used for light, but only as an oi|r a reald hind the, with of the from sti (43.0
%)
used for light, but only as an oi|t beftect. how shapellatgen the fortower (44.0
%)
used for light, but only as an oi|rtucated fanns dountetter from fom to wi (45.2
%)
used for light, but only as an oi|r the sea priised tay queequings hearhou (46.8
%)
used for light, but only as an oi|ld, wode, i long ben! but the gentived.  (48.0
%)
used for light, but only as an oi|r wide-no nate was him. "a king to had o (49.1
%)
used for light, but only as an oi|l erol min't defositanable paring our. 4 (50.0
%)
used for light, but only as an oi|l the motion ahab, too, and relay in aha (51.0
%)
used for light, but only as an oi|n dago, and contantly used the coil; but (52.3
%)
used for light, but only as an oi|l starbuckably happoss of the fullies ti (52.4
%)
used for light, but only as an oi|led-bubble most disinuan into the mate-- (53.3
%)
used for light, but only as an oi|len. ye?' 'tis though moby starbuck, and (53.6
%)
used for light, but only as an oi|l, and the pequodeers. but was all this: (53.9
%)
used for light, but only as an oi|ling his first repore to the pequod, sym (54.4
%)
used for light, but only as an oi|led escried; we they like potants--old s (54.3
%)
used for light, but only as an oi|l-ginqueg! i save started her supplain h (54.3
%)
used for light, but only as an oi|l is, the captain all this mildly bounde (54.9
%)

   here, the seed text is shown left of the pipe character, and the
   randomly sampled sequence follows. in parantheses are the per-character
   accuracy values on the training set while training the model. the
   pattern of learning proceeds from almost-random character generation,
   to producing groups of letters separated by spaces, to generating words
   that seem like they might belong in moby dick, things like    captain,   
      ahab, too,    and    constantly used the coil.   

   much amusement can be derived from a temporal model extending itself
   forward in this way. after all, how else would we ever think of
      pequodeers,       starbuckably,    or    ginqueg   ?!

more information

   source: [44]https://github.com/lmjohns3/theanets

   documentation: [45]http://theanets.readthedocs.org

   mailing list: [46]https://groups.google.com/forum/#!forum/theanets

project details

project links

   [47]homepage

statistics

   github statistics: stars: forks: open issues/prs:

   view statistics for this project via [48]libraries.io, or by using
   [49]google bigquery

meta

   license: mit license (mit)

   author: [50]leif johnson

   tags: machine-learning, neural-network, deep-neural-network,
   recurrent-neural-network, autoencoder, sparse-autoencoder, classifier,
   theano

maintainers

   [51]avatar for lmjohns3 from gravatar.com [52]lmjohns3

classifiers

   development status
          [53]3 - alpha

   intended audience
          [54]science/research

   license
          [55]osi approved :: mit license

   operating system
          [56]os independent

   topic
          [57]scientific/engineering
          [58]scientific/engineering :: artificial intelligence

release history [59]release notifications

   this version
   history node
   [60]

   0.7.3

   jan 28, 2016
   history node
   [61]

   0.7.2

   dec 3, 2015
   history node
   [62]

   0.7.1

   dec 3, 2015
   history node
   [63]

   0.7.0

   nov 3, 2015
   history node
   [64]

   0.6.3

   dec 3, 2015
   history node
   [65]

   0.6.2

   jul 14, 2015
   history node
   [66]

   0.6.1

   jul 9, 2015
   history node
   [67]

   0.6.0

   jul 7, 2015
   history node
   [68]

   0.5.3

   apr 22, 2015
   history node
   [69]

   0.5.2

   feb 12, 2015
   history node
   [70]

   0.5.1

   feb 12, 2015
   history node
   [71]

   0.5.0

   feb 10, 2015
   history node
   [72]

   0.4.0

   dec 12, 2014
   history node
   [73]

   0.3.0

   nov 7, 2014
   history node
   [74]

   0.2.0

   jun 3, 2014
   history node
   [75]

   0.1.2

   jan 23, 2014
   history node
   [76]

   0.1.1

   jan 14, 2014
   history node
   [77]

   0.1.0

   jul 23, 2013

download files

   download the file for your platform. if you're not sure which to
   choose, learn more about [78]installing packages.
   filename, size & hash [79]sha256 hash help file type python version
   upload date
   [80]theanets-0.7.3-py2.py3-none-any.whl (73.2 kb) copy sha256 hash
   sha256 wheel 3.5 jan 28, 2016
   [81]theanets-0.7.3.tar.gz (61.6 kb) copy sha256 hash sha256 source none
   jan 28, 2016

   logo

     * help
     * [82]installing packages
     * [83]uploading packages
     * [84]user guide
     * [85]faqs

     * about pypi
     * [86]pypi on twitter
     * [87]infrastructure dashboard
     * [88]package index name retention
     * [89]our sponsors

     * contributing to pypi
     * [90]bugs and feedback
     * [91]contribute on github
     * [92]development credits

     * using pypi
     * [93]code of conduct
     * [94]report security issue
     * [95]privacy policy
     * [96]terms of use
     __________________________________________________________________

   status: [97]all systems operational

   developed and maintained by the python community, for the python
   community.
   [98]donate today!

      2019 [99]python software foundation

   (button) desktop version

   supported by
   [100]elastic elastic search [101]pingdom pingdom monitoring [102]google
   google bigquery [103]sentry sentry error logging [104]aws aws cloud
   computing [105]datadog datadog monitoring [106]fastly fastly cdn
   [107]signalfx signalfx supporter [108]digicert digicert ev certificate
   [109]statuspage statuspage status page

references

   1. https://pypi.org/rss/updates.xml
   2. https://pypi.org/rss/packages.xml
   3. https://pypi.org/opensearch.xml
   4. https://pypi.org/project/theanets/#content
   5. https://pypi.org/
   6. https://pypi.org/help/
   7. https://donate.pypi.org/
   8. https://pypi.org/account/login/
   9. https://pypi.org/account/register/
  10. https://pypi.org/project/theanets/
  11. https://pypi.org/project/theanets/#description
  12. https://pypi.org/project/theanets/#history
  13. https://pypi.org/project/theanets/#files
  14. http://github.com/lmjohns3/theanets
  15. https://libraries.io/pypi/theanets
  16. https://packaging.python.org/guides/analyzing-pypi-package-downloads/
  17. mailto:leif@lmjohns3.com
  18. https://pypi.org/user/lmjohns3/
  19. https://pypi.org/user/lmjohns3/
  20. https://pypi.org/search/?c=development+status+::+3+-+alpha
  21. https://pypi.org/search/?c=intended+audience+::+science/research
  22. https://pypi.org/search/?c=license+::+osi+approved+::+mit+license
  23. https://pypi.org/search/?c=operating+system+::+os+independent
  24. https://pypi.org/search/?c=topic+::+scientific/engineering
  25. https://pypi.org/search/?c=topic+::+scientific/engineering+::+artificial+intelligence
  26. https://pypi.org/project/theanets/#description
  27. https://pypi.org/project/theanets/#data
  28. https://pypi.org/project/theanets/#history
  29. https://pypi.org/project/theanets/#files
  30. https://coveralls.io/r/lmjohns3/theanets?branch=master
  31. http://deeplearning.net/software/theano/
  32. https://github.com/lmjohns3/theanets
  33. https://pypi.python.org/pypi/theanets
  34. http://theanets.readthedocs.org/en/latest/api/activations.html
  35. http://theanets.readthedocs.org/en/latest/api/layers.html
  36. http://localhost:8080/guide.html#guide-creating-specifying-layers
  37. http://downhill.readthedocs.org/
  38. http://theanets.readthedocs.org/en/latest/api/trainers.html
  39. http://theanets.readthedocs.org/en/latest/guide.html#guide-training
  40. http://theanets.readthedocs.org/en/latest/api/generated/theanets.recurrent.text.html
  41. http://theanets.readthedocs.org/en/latest/api/generated/theanets.layers.recurrent.gru.html
  42. http://downhill.readthedocs.org/en/stable/guide.html#data-using-callables
  43. http://downhill.readthedocs.org/en/stable/guide.html#iterative-optimization
  44. https://github.com/lmjohns3/theanets
  45. http://theanets.readthedocs.org/
  46. https://groups.google.com/forum/#!forum/theanets
  47. http://github.com/lmjohns3/theanets
  48. https://libraries.io/pypi/theanets
  49. https://packaging.python.org/guides/analyzing-pypi-package-downloads/
  50. mailto:leif@lmjohns3.com
  51. https://pypi.org/user/lmjohns3/
  52. https://pypi.org/user/lmjohns3/
  53. https://pypi.org/search/?c=development+status+::+3+-+alpha
  54. https://pypi.org/search/?c=intended+audience+::+science/research
  55. https://pypi.org/search/?c=license+::+osi+approved+::+mit+license
  56. https://pypi.org/search/?c=operating+system+::+os+independent
  57. https://pypi.org/search/?c=topic+::+scientific/engineering
  58. https://pypi.org/search/?c=topic+::+scientific/engineering+::+artificial+intelligence
  59. https://pypi.org/help/#project-release-notifications
  60. https://pypi.org/project/theanets/0.7.3/
  61. https://pypi.org/project/theanets/0.7.2/
  62. https://pypi.org/project/theanets/0.7.1/
  63. https://pypi.org/project/theanets/0.7.0/
  64. https://pypi.org/project/theanets/0.6.3/
  65. https://pypi.org/project/theanets/0.6.2/
  66. https://pypi.org/project/theanets/0.6.1/
  67. https://pypi.org/project/theanets/0.6.0/
  68. https://pypi.org/project/theanets/0.5.3/
  69. https://pypi.org/project/theanets/0.5.2/
  70. https://pypi.org/project/theanets/0.5.1/
  71. https://pypi.org/project/theanets/0.5.0/
  72. https://pypi.org/project/theanets/0.4.0/
  73. https://pypi.org/project/theanets/0.3.0/
  74. https://pypi.org/project/theanets/0.2.0/
  75. https://pypi.org/project/theanets/0.1.2/
  76. https://pypi.org/project/theanets/0.1.1/
  77. https://pypi.org/project/theanets/0.1.0/
  78. https://packaging.python.org/installing/
  79. https://pip.pypa.io/en/stable/reference/pip_install/#hash-checking-mode
  80. https://files.pythonhosted.org/packages/58/58/67329f5da252c282adb915a705e8c34be0a964cfbe7b43a2320103dd1a46/theanets-0.7.3-py2.py3-none-any.whl
  81. https://files.pythonhosted.org/packages/45/2f/7cabb204f24f661ea9c4bf838b18acad973dc984fad56831ffb017785c52/theanets-0.7.3.tar.gz
  82. https://packaging.python.org/installing/
  83. https://packaging.python.org/tutorials/packaging-projects/
  84. https://packaging.python.org/
  85. https://pypi.org/help/
  86. https://twitter.com/pypi
  87. https://dtdg.co/pypi
  88. https://www.python.org/dev/peps/pep-0541/
  89. https://pypi.org/sponsors/
  90. https://pypi.org/help/#feedback
  91. https://github.com/pypa/warehouse
  92. https://github.com/pypa/warehouse/graphs/contributors
  93. https://www.pypa.io/en/latest/code-of-conduct/
  94. https://pypi.org/security/
  95. https://www.python.org/privacy/
  96. https://pypi.org/policy/terms-of-use/
  97. https://status.python.org/
  98. https://donate.pypi.org/
  99. https://www.python.org/psf/
 100. https://www.elastic.co/
 101. https://www.pingdom.com/
 102. https://cloud.google.com/
 103. https://getsentry.com/for/python
 104. https://aws.amazon.com/
 105. https://www.datadoghq.com/
 106. https://www.fastly.com/
 107. https://www.signalfx.com/
 108. https://www.digicert.com/
 109. https://statuspage.io/
