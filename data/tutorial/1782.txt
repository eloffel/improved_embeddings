   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    practical guide to implementing neural networks
   in python (using theano) comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]machine learning [94]practical guide to implementing
   neural networks in python (using theano)

   [95]machine learning[96]python

practical guide to implementing neural networks in python (using theano)

   [97]aarshay jain, april 18, 2016

introduction

   in my last article, i discussed the [98]fundamentals of deep learning,
   where i explained the basic working of a id158. if
   you   ve been following this series, today we   ll become familiar with
   practical process of implementing neural network in python (using
   theano package).

   i found various other packages also such as caffe, torch, tensorflow
   etc to do this job. but, theano is no less than and satisfactorily
   execute all the tasks. also, it has multiple benefits which further
   enhances the coding experience in python.

   in this article, i   ll provide a comprehensive practical guide to
   implement neural networks using theano. if you are here for just python
   codes, feel free to skip the sections and learn at your pace. and, if
   you are new to theano, i suggest you to follow the article sequentially
   to gain complete knowledge.

   note:
    1. this article is best suited for users with knowledge of neural
       network & deep learning.
    2. if you don   t know python, [99]start here.
    3. if you don   t know deep learning, [100]start here.

   practical guide to implement neural network in python using theano


table of contents

    1. theano overview
    2. implementing simple expressions
    3. theano variable types
    4. theano functions
    5. modeling a single neuron
    6. modeling a two-layer networks


1. theano overview

   in short, we can define theano as:
     * a programming language which runs on top of python but has its own
       data structure which are tightly integrated with numpy
     * a id202 compiler with defined c-codes at the backend
     * a python package allowing faster implementation of mathematical
       expressions

   as popularly known, theano was developed at the university of montreal
   in 2008. it is used for defining and evaluating mathematical
   expressions in general.

   theano has several features which optimize the processing time of
   expressions. for instance it modifies the symbolic expressions we
   define before converting them to c codes. examples:
     * it makes the expressions faster, for instance it will change {
       (x+y) + (x+y) } to { 2*(x+y) }
     * it makes expressions more stable, for instance it will change {
       exp(a) / exp(a).sum(axis=1) } to { softmax(a) }

   below are some powerful advantages of using theano:
    1. it defines c-codes for different mathematical expressions.
    2. the implementations are much faster as compared to some of the
       python   s default implementations.
    3. due to fast implementations, it works well in case of high
       dimensionality problems.
    4. it allows gpu implementation which works blazingly fast specially
       for problems like deep learning.

   let   s now focus on theano (with example) and try to understand it as a
   programming language.


2. implementing simple expressions

   lets start by implementing a simple mathematical expression, say a
   multiplication in theano and see how the system works. in later
   sections, we will take a deep dive into individual components. the
   general structure of a theano code works in 3 steps:
    1. define variables/objects
    2. define a mathematical expression in the form of a function
    3. evaluate expressions by passing values

   lets look at the following code for simply multiplying 2 numbers:

step 0: import libraries

import numpy as np
import theano.tensor as t
from theano import function

   here, we have simply imported 2 key functions of theano     tensor and
   function.

step 1: define variables

a = t.dscalar('a')
b = t.dscalar('b')

   here 2 variables are defined. note that we have used theano tensor
   object type here. also, the arguments passed to dscalar function are
   just name of tensors which are useful while debugging. they code will
   work even without them.

step 2: define expression

c = a*b
f = function([a,b],c)

   here we have defined a function f which has 2 arguments:
    1. inputs [a,b]: these are inputs to system
    2. output c: this has been previously defined

step 3: evaluate expression

f(1.5,3)

   [101]1. output 1

   now we are simply calling the function with the 2 inputs and we get the
   output as a multiple of the two. in short, we saw how we can define
   mathematical expressions in theano and evaluate them. before we go into
   complex functions, lets understand some inherent properties of theano
   which will be useful in building neural networks.


3. theano variable types

   variables are key building blocks of any programming language. in
   theano, the objects are defined as tensors. a tensor can be understood
   as a generalized form of a vector with dimension t. different
   dimensions are analogous to different types:
     * t = 0: scalar
     * t = 1: vector
     * t = 2: matrix
     * and so on..

   [102]watch this interesting video to get a deeper level of intuition
   into vectors and tensors.

   these variables can be defined similar to our definition of    dscalar   
   in the above code. the various keywords for defining variables are:
     * byte: bscalar, bvector, bmatrix, brow, bcol, btensor3, btensor4
     * 16-bit integers: wscalar, wvector, wmatrix, wrow, wcol, wtensor3,
       wtensor4
     * 32-bit integers: iscalar, ivector, imatrix, irow, icol, itensor3,
       itensor4
     * 64-bit integers: lscalar, lvector, lmatrix, lrow, lcol, ltensor3,
       ltensor4
     * float: fscalar, fvector, fmatrix, frow, fcol, ftensor3, ftensor4
     * double: dscalar, dvector, dmatrix, drow, dcol, dtensor3, dtensor4
     * complex: cscalar, cvector, cmatrix, crow, ccol, ctensor3, ctensor4

   now you understand that we can define variables with different memory
   allocations and dimensions. but this is not an exhaustive list. we can
   define dimensions higher than 4 using a generic tensortype class.
   you   ll find more details [103]here.

   please note that variables of these types are just symbols. they don   t
   have a fixed value and are passed into functions as symbols. they only
   take values when a function is called. but, we often need variables
   which are constants and which we need not pass in all the functions.
   for this theano provides shared variables. these have a fixed value and
   are not of the types discussed above. they can be defined as numpy data
   types or simple constants.

   lets take an example. suppose, we initialize a shared variable as 0 and
   use a function which:
     * takes an input
     * adds the input to the shared variable
     * returns the square of shared variable

   this can be done as:
from theano import shared
x = t.iscalar('x')
sh = shared(0)
f = function([x], sh**2, updates=[(sh,sh+x)])

   note that here function has an additional argument called updates. it
   has to be a list of lists or tuples, each containing 2 elements of form
   (shared_variable, updated_value). the output for 3 subsequent runs is:

   [104]2. shared

   you can see that for each run, it returns the square of the present
   value, i.e. the value before updating. after the run, the value of
   shared variable gets updated. also, note that shared variables have 2
   functions    get_value()    and    set_value()    which are used to read and
   modify the value of shared variables.


4. theano functions

   till now we saw the basic structure of a function and how it handles
   shared variables. lets move forward and discuss couple more things we
   can do with functions:

return multiple values

   we can return multiple values from a function. this can be easily done
   as shown in following example:
a = t.dscalar('a')
f = function([a],[a**2, a**3])
f(3)

   [105]3. multiple output

   we can see that the output is an array with the square and cube of the
   number passed into the function.

computing gradients

   gradient computation is one of the most important part of training a
   deep learning model. this can be done easily in theano. let   s define a
   function as the cube of a variable and determine its gradient.
x = t.dscalar('x')
y = x**3
qy = t.grad(y,x)
f = function([x],qy)
f(4)

   this returns 48 which is 3x^2 for x=4. lets see how theano has
   implemented this derivative using the pretty-print feature as
   following:
from theano import pp  #pretty-print
print(pp(qy))

   [106]4. pp

   in short, it can be explained as: fill(x^3,1)*3*x^3-1 you can see that
   this is exactly the derivative of x^3. note that fill(x^3,1) simply
   means to make a matrix of same shape as x^3 and fill it with 1. this is
   used to handle high dimensionality input and can be ignored in this
   case.

   we can use theano to compute jacobian and hessian matrices as well
   which you can find [107]here.

   there are various other aspects of theano like conditional and looping
   constructs. you can go into further detail using following resources:
    1. [108]theano conditional constructs
    2. [109]theano looping statements
    3. [110]handling shape information


5. modeling a single neuron

   lets start by modeling a single neuron.

   note that i will take examples from my previous article on neuron
   networks here. if you wish to go in the detail of how these work,
   please read [111]this article. for modeling a neuron, lets adopt a 2
   stage process:
    1. implement feed forward pass
          + take inputs and determine output
          + use the fixed weights for this case
    2. implement backward propagation
          + calculate error and gradients
          + update weights using gradients

   lets implement an and gate for this purpose.


feed forward pass

   an and gate can be implemented as:

   2

   now we will define a feed forward network which takes inputs and uses
   the shown weights to determine the output. first we will define a
   neuron which computes the output a.
import theano
import theano.tensor as t
from theano.ifelse import ifelse
import numpy as np

#define variables:
x = t.vector('x')
w = t.vector('w')
b = t.scalar('b')

#define mathematical expression:
z = t.dot(x,w)+b
a = ifelse(t.lt(z,0),0,1)

neuron = theano.function([x,w,b],a)

   i have simply used the steps we saw above. if you are not sure how this
   expression works, please refer to the neural networks article i have
   referred above. now let   s test out all values in the truth table and
   see if the and function has been implemented as desired.
#define inputs and weights
inputs = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
]
weights = [ 1, 1]
bias = -1.5

#iterate through all inputs and find outputs:
for i in range(len(inputs)):
    t = inputs[i]
    out = neuron(t,weights,bias)
    print 'the output for x1=%d | x2=%d is %d' % (t[0],t[1],out)

   [112]5. single neuron

   note that, in this case we had to provide weights while calling the
   function. however, we will be required to update them while training.
   so, its better that we define them as a shared variable. the following
   code implements w as a shared variable. try this out and you   ll get the
   same output.
import theano
import theano.tensor as t
from theano.ifelse import ifelse
import numpy as np

#define variables:
x = t.vector('x')
w = theano.shared(np.array([1,1]))
b = theano.shared(-1.5)

#define mathematical expression:
z = t.dot(x,w)+b
a = ifelse(t.lt(z,0),0,1)

neuron = theano.function([x],a)

#define inputs and weights
inputs = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
]

#iterate through all inputs and find outputs:
for i in range(len(inputs)):
    t = inputs[i]
    out = neuron(t)
    print 'the output for x1=%d | x2=%d is %d' % (t[0],t[1],out)

   now the feedforward step is complete.


backward propagation

   now we have to modify the above code and perform following additional
   steps:
    1. determine the cost or error based on true output
    2. determine gradient of node
    3. update the weights using this gradient

   lets initialize the network as follow:
#gradient
import theano
import theano.tensor as t
from theano.ifelse import ifelse
import numpy as np
from random import random

#define variables:
x = t.matrix('x')
w = theano.shared(np.array([random(),random()]))
b = theano.shared(1.)
learning_rate = 0.01

#define mathematical expression:
z = t.dot(x,w)+b
a = 1/(1+t.exp(-z))

   note that, you will notice a change here as compared to above program.
   i have defined x as a matrix here and not a vector. this is more of a
   vectorized approach where we will determine all the outputs together
   and find the total cost which is required for determining the
   gradients.

   you should also keep in mind that i am using the full-batch gradient
   descent here, i.e. we will use all training observations to update the
   weights.

   let   s determine the cost as follows:
a_hat = t.vector('a_hat') #actual output
cost = -(a_hat*t.log(a) + (1-a_hat)*t.log(1-a)).sum()

   in this code, we have defined a_hat as the actual observations. then we
   determine the cost using a simple logistic cost function since this is
   a classification problem. now lets compute the gradients and define a
   means to update the weights.
dw,db = t.grad(cost,[w,b])

train = function(
    inputs = [x,a_hat],
    outputs = [a,cost],
    updates = [
        [w, w-learning_rate*dw],
        [b, b-learning_rate*db]
    ]
)

   in here, we are first computing gradient of the cost w.r.t. the weights
   for inputs and bias unit. then, the train function here does the weight
   update job. this is an elegant but tricky approach where the weights
   have been defined as shared variables and the updates argument of the
   function is used to update them every time a set of values are passed
   through the model.
#define inputs and weights
inputs = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
]
outputs = [0,0,0,1]

#iterate through all inputs and find outputs:
cost = []
for iteration in range(30000):
    pred, cost_iter = train(inputs, outputs)
    cost.append(cost_iter)

#print the outputs:
print 'the outputs of the nn are:'
for i in range(len(inputs)):
    print 'the output for x1=%d | x2=%d is %.2f' % (inputs[i][0],inputs[i][1],pr
ed[i])

#plot the flow of cost:
print '\nthe flow of cost during model run is as following:'
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(cost)

   [113]6. output single update

   here we have simply defined the inputs, outputs and trained the model.
   while training, we have also recorded the cost and its plot shows that
   our cost reduced towards zero and then finally saturated at a low
   value. the output of the network also matched the desired output
   closely. hence, we have successfully implemented and trained a single
   neuron.


6. modeling a two-layer neural network

   i hope you have understood the last section. if not, please do read it
   multiple times and proceed to this section. along with learning theano,
   this will enhance your understanding of neural networks on the whole.

   lets consolidate our understanding by taking a 2-layer example. to keep
   things simple, i   ll take the xnor example like in my previous article.
   if you wish to explore the nitty-gritty of how it works, i recommend
   reading the [114]previous article.

   the xnor function can be implemented as:

   [115]6

   as a reminder, the truth table of xnor function is:

   [116]8. tt xnor case 1

   now we will directly implement both feed forward and backward at one
   go.

step 1: define variables

import theano
import theano.tensor as t
from theano.ifelse import ifelse
import numpy as np
from random import random

#define variables:
x = t.matrix('x')
w1 = theano.shared(np.array([random(),random()]))
w2 = theano.shared(np.array([random(),random()]))
w3 = theano.shared(np.array([random(),random()]))
b1 = theano.shared(1.)
b2 = theano.shared(1.)
learning_rate = 0.01

   in this step we have defined all the required variables as in the
   previous case. note that now we have 3 weight vectors corresponding to
   each neuron and 2 bias units corresponding to 2 layers.


step 2: define mathematical expression

a1 = 1/(1+t.exp(-t.dot(x,w1)-b1))
a2 = 1/(1+t.exp(-t.dot(x,w2)-b1))
x2 = t.stack([a1,a2],axis=1)
a3 = 1/(1+t.exp(-t.dot(x2,w3)-b2))

   here we have simply defined mathematical expressions for each neuron in
   sequence. note that here an additional step was required where x2 is
   determined. this is required because we want the outputs of a1 and a2
   to be combined into a matrix whose dot product can be taken with the
   weights vector.

   lets explore this a bit further. both a1 and a2 would return a vector
   with 4 units. so if we simply take an array [a1, a2] then we   ll obtain
   something like [ [a11,a12,a13,a14], [a21,a22,a23,a24] ]. however, we
   want this to be [ [a11,a21], [a12,a22], [a13,a23], [a14,a24] ]. the
   stacking function of theano does this job for us.


step 3: define gradient and update rule

a_hat = t.vector('a_hat') #actual output
cost = -(a_hat*t.log(a3) + (1-a_hat)*t.log(1-a3)).sum()
dw1,dw2,dw3,db1,db2 = t.grad(cost,[w1,w2,w3,b1,b2])

train = function(
    inputs = [x,a_hat],
    outputs = [a3,cost],
    updates = [
        [w1, w1-learning_rate*dw1],
        [w2, w2-learning_rate*dw2],
        [w3, w3-learning_rate*dw3],
        [b1, b1-learning_rate*db1],
        [b2, b2-learning_rate*db2]
    ]
)

   this is very similar to the previous case. the key difference being
   that now we have to determine the gradients of 3 weight vectors and 2
   bias units and update them accordingly.


step 4: train the model

inputs = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
]
outputs = [1,0,0,1]

#iterate through all inputs and find outputs:
cost = []
for iteration in range(30000):
    pred, cost_iter = train(inputs, outputs)
    cost.append(cost_iter)

#print the outputs:
print 'the outputs of the nn are:'
for i in range(len(inputs)):
    print 'the output for x1=%d | x2=%d is %.2f' % (inputs[i][0],inputs[i][1],pr
ed[i])

#plot the flow of cost:
print '\nthe flow of cost during model run is as following:'
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(cost)

   [117]7. 2 layer op

   we can see that our network has successfully learned the xnor function.
   also, the cost of the model has reduced to reasonable limit. with this,
   we have successfully implemented a 2-layer network.


end notes

   in this article, we understood the basics of theano package in python
   and how it acts as a programming language. we also implemented some
   basic neural networks using theano. i am sure that implementing neural
   networks on theano will enhance your understanding of nn on the whole.

   if hope you have been able to follow till this point, you really
   deserve a pat on your back. i can understand that theano is not a
   traditional plug and play system like most of sklearn   s ml models. but
   the beauty of neural networks lies in their flexibility and an approach
   like this will allow you a high degree of customization in models. some
   high-level wrappers of theano do exist like keras and lasagne which you
   can check out. but i believe knowing the core of theano will help you
   in using them.

   did you find this article useful ? please feel free to share your
   feedback and questions below. eagerly waiting to interact with you!

check out [118]live competitions and compete with best data scientists
around the world.

   you can also read this article on analytics vidhya's android app
   [119]get it on google play

share this:

     * [120]click to share on linkedin (opens in new window)
     * [121]click to share on facebook (opens in new window)
     * [122]click to share on twitter (opens in new window)
     * [123]click to share on pocket (opens in new window)
     * [124]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [125]backward propagation, [126]deep learning, [127]forward
   propogation, [128]id119, [129]machine learning, [130]matrix
   factorization, [131]neural network, [132]theano
   next article

it   s our 3rd birthday     come & celebrate

   previous article

case study for freshers (level : medium)     call center optimization

[133]aarshay jain

   aarshay is a ml enthusiast, pursuing ms in data science at columbia
   university, graduating in dec 2017. he is currently exploring the
   various ml techniques and writes articles for av to share his knowledge
   with the community.
     *
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [134]discussion portal to get your queries resolved

14 comments

     * gianni says:
       [135]april 18, 2016 at 9:57 am
       thanks aarshay really usefull.
       there   s a big big step from sklearn fit models and build a theano
       functioning model that can make confusion..
       in beetwen there   s keras, that permit to easily build neural
       network models, so maybe it could be a first step from sklearn to
       theano.
       do you have plans for keras full tutorial ? i this case can i
       suggest to develop well the modelling part, from the neural network
       graph to the keras code with a lot of examples ?
       however, good job aarshay.
       [136]reply
          + aarshay jain says:
            [137]april 18, 2016 at 10:03 am
            hi gianni,
            yes you are right. keras and lasagne are somewhere in between.
            i don   t have immediate plans of doing keras but if you are
            interested, you can do a guest blog on keras. lets discuss
            this further offline. please drop me a note on
            [138][email protected].
            regards,
            aarshay
            [139]reply
     * allen kennedy says:
       [140]april 20, 2016 at 7:37 pm
       hi
       thank you very much for this great tutorial.
       i just have one question. you are using the object a_hat as the
       actual outputs, but i cant seem to understand where the outputs are
       initialized in this vector? i   m probably missing something here.
       can you please explain where they are been initialized?
       [141]reply
          + aarshay jain says:
            [142]april 21, 2016 at 2:58 am
            hi,
            theano works a bit differently. theano variables are just
            objects which don   t hold a permanent memory. you can
            understand them as functions. so they get a value only when
            the function is called.
            thus variables are not initialized here. whenever we call the
            theano function, we pass arguments which go into the variables
            being used in the function.
            hope this makes sense.
            [143]reply
               o allen kennedy says:
                 [144]april 22, 2016 at 10:27 pm
                 thanks you for the quick reply. but im still not
                 understanding where a_hat is been called from. i
                 understand that it is just a function and holds n value
                 until it is called,
                 i can see it created as a vector above     a_hat. then the
                 cost calculation calls it, and at this stage it is still
                 a blank vector. then the function train calls it as it as
                 part of the input for inputs. but im struggling to see
                 where it passes any values to it? its probably because i
                 do not know python too well, i mainly use r. but is the
                 input parameter in the train function, taking in matrix x
                 and returning a_hat?
                 [145]reply
                    # aarshay jain says:
                      [146]april 23, 2016 at 5:33 am
                      lets take the last section as an example. step 3 has
                      train defined as:
                      train = function(
                      inputs = [x,a_hat],
                      outputs = [a3,cost],
                      updates = [
                      [w1, w1-learning_rate*dw1],
                      [w2, w2-learning_rate*dw2],
                      [w3, w3-learning_rate*dw3],
                      [b1, b1-learning_rate*db1],
                      [b2, b2-learning_rate*db2]
                      ]
                      )
                      this takes 2 inputs     x and a_hat.
                      in the step 4 of last section, when we call train
                      as:
                      pred, cost_iter = train(inputs, outputs)
                      it has 2 arguments     inputs, outputs. inputs go into
                      x and outputs go into a_hat.
                      hope this makes sense. please feel free to discuss
                      further if needed. i understand its not super
                      intuitive like sklearn.     
                      [147]reply
     * [148]keerthisuresh says:
       [149]may 5, 2016 at 6:59 am
       keep sharing such ideas in the future as well.this was actually
       what i was looking for,and i am glad to came here you keep up the
       fantastic work!my weblog..
       [150]reply
          + aarshay jain says:
            [151]may 5, 2016 at 7:01 am
            thank you keerthi.. i   ll try my best     
            [152]reply
     * [153]babalola, rotimi says:
       [154]may 8, 2016 at 3:38 pm
       thanks for a wonderful post. my question is     for the neural
       network with one layer, you added the bias term in the mathematical
       expression i.e. z = t.dot(x,w) + b.
       but for the multiple layer case you subtracted the bias term i.e.
       a1 = 1/(1+t.exp(-t.dot(x,w1)-b1))
       a2 = 1/(1+t.exp(-t.dot(x,w2)-b1))
       a3 = 1/(1+t.exp(-t.dot(x2,w3)-b2))
       i don   t understand why it is different in both cases. please can
       you explain? thanks
       [155]reply
          + aarshay jain says:
            [156]may 9, 2016 at 5:17 am
            you are welcome.
            regarding the mathematical expression, if you observe
            carefully, both the t.dot(x,w1) and b1 are negative. this is
            because the sigmoid function is 1/(1+e(-x). there x is
               t.dot(x,w1) + b1   . hope this makes sense.
            [157]reply
               o [158]babalola, rotimi says:
                 [159]may 9, 2016 at 3:47 pm
                 yes, it makes sense. the minus in the sigmoid function
                 turns the minus to plus. i hope that is right. thank you
                 once again. your website has helped me a lot. keep it up
                 [160]reply
                    # aarshay jain says:
                      [161]may 9, 2016 at 4:25 pm
                      yup you got it! thanks!
                      [162]reply
     * poornachandra sandur says:
       [163]june 5, 2016 at 6:05 pm
       superb article   
       [164]reply
          + aarshay jain says:
            [165]june 6, 2016 at 9:00 am
            glad you liked it     
            [166]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [167]srk       3924
   2    [2.jpg?date=2019-04-05] [168]mark12    3510
   3    [3.jpg?date=2019-04-05] [169]nilabha   3261
   4    [4.jpg?date=2019-04-05] [170]nitish007 3237
   5    [5.jpg?date=2019-04-05] [171]tezdhar   3082
   [172]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [173]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [174]understanding support vector machine algorithm from examples
       (along with code)
     * [175]essentials of machine learning algorithms (with python and r
       codes)
     * [176]a complete tutorial to learn data science with python from
       scratch
     * [177]7 types of regression techniques you should know!
     * [178]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [179]a simple introduction to anova (with applications in excel)
     * [180]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [181]top 5 machine learning github repositories and reddit discussions
   from march 2019

[182]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [183]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[184]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [185]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[186]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [187]16 opencv functions to start your id161 journey (with
   python code)

[188]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [189][ds-finhack.jpg]

   [190][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [191]about us
     * [192]our team
     * [193]career
     * [194]contact us
     * [195]write for us

   [196]about us
   [197]   
   [198]our team
   [199]   
   [200]careers
   [201]   
   [202]contact us

data scientists

     * [203]blog
     * [204]hackathon
     * [205]discussions
     * [206]apply jobs
     * [207]leaderboard

companies

     * [208]post jobs
     * [209]trainings
     * [210]hiring hackathons
     * [211]advertising
     * [212]reach us

   don't have an account? [213]sign up here.

join our community :

   [214]46336 [215]followers
   [216]20224 [217]followers
   [218]followers
   [219]7513 [220]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [221]privacy policy
     * [222]terms of use
     * [223]refund policy

   don't have an account? [224]sign up here

   iframe: [225]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [226](button) join now

   subscribe!

   iframe: [227]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [228](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/machine-learning/
  94. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
  95. https://www.analyticsvidhya.com/blog/category/machine-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/author/aarshay/
  98. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  99. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 100. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
 101. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/1.-output-1.png
 102. https://www.youtube.com/watch?v=f5liquk0ztw
 103. http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-tensor-creation
 104. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/2.-shared.png
 105. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/3.-multiple-output.png
 106. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/4.-pp.png
 107. http://deeplearning.net/software/theano/tutorial/gradients.html
 108. http://deeplearning.net/software/theano/tutorial/conditions.html
 109. http://deeplearning.net/software/theano/tutorial/loop.html
 110. http://deeplearning.net/software/theano/tutorial/shape_info.html
 111. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
 112. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/5.-single-neuron.png
 113. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/6.-output-single-update.png
 114. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
 115. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/6.jpg
 116. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/8.-tt-xnor-case-1.png
 117. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/7.-2-layer-op.png
 118. http://datahack.analyticsvidhya.com/contest/all
 119. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 120. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/?share=linkedin
 121. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/?share=facebook
 122. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/?share=twitter
 123. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/?share=pocket
 124. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/?share=reddit
 125. https://www.analyticsvidhya.com/blog/tag/backward-propagation/
 126. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 127. https://www.analyticsvidhya.com/blog/tag/forward-propogation/
 128. https://www.analyticsvidhya.com/blog/tag/gradient-descent/
 129. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 130. https://www.analyticsvidhya.com/blog/tag/matrix-factorization/
 131. https://www.analyticsvidhya.com/blog/tag/neural-network/
 132. https://www.analyticsvidhya.com/blog/tag/theano/
 133. https://www.analyticsvidhya.com/blog/author/aarshay/
 134. https://discuss.analyticsvidhya.com/
 135. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109642
 136. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109642
 137. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109643
 138. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#43222231302b223a29222a2d03242e222a2f6d202c2e
 139. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109643
 140. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109723
 141. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109723
 142. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109733
 143. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109733
 144. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109870
 145. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109870
 146. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109885
 147. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-109885
 148. http://www.datawaretools.in/
 149. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110463
 150. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110463
 151. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110464
 152. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110464
 153. http://plaindata.blogspot.com/
 154. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110626
 155. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110626
 156. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110663
 157. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110663
 158. http://plaindata.blogspot.com/
 159. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110699
 160. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110699
 161. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110702
 162. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-110702
 163. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-111897
 164. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-111897
 165. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-111917
 166. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/#comment-111917
 167. https://datahack.analyticsvidhya.com/user/profile/srk
 168. https://datahack.analyticsvidhya.com/user/profile/mark12
 169. https://datahack.analyticsvidhya.com/user/profile/nilabha
 170. https://datahack.analyticsvidhya.com/user/profile/nitish007
 171. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 172. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 173. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 174. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 175. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 176. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 177. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 178. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 179. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 180. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 181. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 182. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 183. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 184. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 185. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 186. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 187. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 188. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 189. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 190. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 191. http://www.analyticsvidhya.com/about-me/
 192. https://www.analyticsvidhya.com/about-me/team/
 193. https://www.analyticsvidhya.com/career-analytics-vidhya/
 194. https://www.analyticsvidhya.com/contact/
 195. https://www.analyticsvidhya.com/about-me/write/
 196. http://www.analyticsvidhya.com/about-me/
 197. https://www.analyticsvidhya.com/about-me/team/
 198. https://www.analyticsvidhya.com/about-me/team/
 199. https://www.analyticsvidhya.com/about-me/team/
 200. https://www.analyticsvidhya.com/career-analytics-vidhya/
 201. https://www.analyticsvidhya.com/about-me/team/
 202. https://www.analyticsvidhya.com/contact/
 203. https://www.analyticsvidhya.com/blog
 204. https://datahack.analyticsvidhya.com/
 205. https://discuss.analyticsvidhya.com/
 206. https://www.analyticsvidhya.com/jobs/
 207. https://datahack.analyticsvidhya.com/users/
 208. https://www.analyticsvidhya.com/corporate/
 209. https://trainings.analyticsvidhya.com/
 210. https://datahack.analyticsvidhya.com/
 211. https://www.analyticsvidhya.com/contact/
 212. https://www.analyticsvidhya.com/contact/
 213. https://datahack.analyticsvidhya.com/signup/
 214. https://www.facebook.com/analyticsvidhya/
 215. https://www.facebook.com/analyticsvidhya/
 216. https://twitter.com/analyticsvidhya
 217. https://twitter.com/analyticsvidhya
 218. https://plus.google.com/+analyticsvidhya
 219. https://in.linkedin.com/company/analytics-vidhya
 220. https://in.linkedin.com/company/analytics-vidhya
 221. https://www.analyticsvidhya.com/privacy-policy/
 222. https://www.analyticsvidhya.com/terms/
 223. https://www.analyticsvidhya.com/refund-policy/
 224. https://id.analyticsvidhya.com/accounts/signup/
 225. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 226. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 227. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 228. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 230. https://www.facebook.com/analyticsvidhya
 231. https://twitter.com/analyticsvidhya
 232. https://plus.google.com/+analyticsvidhya/posts
 233. https://in.linkedin.com/company/analytics-vidhya
 234. https://www.analyticsvidhya.com/blog/2016/04/celebrating-3-years-analytics-vidhyas-success/
 235. https://www.analyticsvidhya.com/blog/2016/04/case-study-level-medium-call-center-optimization/
 236. https://www.analyticsvidhya.com/blog/author/aarshay/
 237. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#d1b0b0a3a2b9b0a8bbb0b8bf91b6bcb0b8bdffb2bebc
 238. https://in.linkedin.com/in/aarshayjain
 239. https://github.com/aarshayj
 240. https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/aarshay
 241. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 242. https://www.facebook.com/analyticsvidhya/
 243. https://twitter.com/analyticsvidhya
 244. https://plus.google.com/+analyticsvidhya
 245. https://plus.google.com/+analyticsvidhya
 246. https://in.linkedin.com/company/analytics-vidhya
 247. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 248. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 249. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 250. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 251. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 252. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 253. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 254. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 255. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 256. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 257. javascript:void(0);
 258. javascript:void(0);
 259. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 260. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 261. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 262. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 263. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 264. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 265. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 266. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 267. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 268. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fneural-networks-python-theano%2f&linkname=practical%20guide%20to%20implementing%20neural%20networks%20in%20python%20%28using%20theano%29
 269. javascript:void(0);
 270. javascript:void(0);
