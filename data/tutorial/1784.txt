   #[1]marek rei    feed [2]marek rei    comments feed [3]marek rei    theano
   tutorial comments feed [4]alternate [5]alternate

   [6]marek rei
   thoughts on machine learning and natural language processing

     * [7]about me

   january 25, 2016

theano tutorial

   this is an introductory tutorial on using theano, the python library.
   i   m going to start from scratch and assume no previous knowledge of
   theano. however, understanding how neural networks work will be useful
   when getting to the code examples towards the end.

   the plan for the tutorial is as follows:
    1. give a basic introduction to theano and explain the important
       concepts.
    2. go over the main operations that we have available in theano.
    3. look at working code examples.

   i recently gave this tutorial as a talk in university of cambridge and
   it turned out to be way more popular than expected. in order to give
   more people access to the material, i   m now writing it up as a blog
   post.

   i do not claim to know everything about theano, and i constantly learn
   new things myself. if you find any errors or have suggestions on how to
   improve this tutorial, do let me know.

   the code examples can be found in the github
   repository: [8]https://github.com/marekrei/theano-tutorial

1. what is theano?

   cyh2gmnwkaeldtl

   theano is a python library for efficiently handling mathematical
   expressions involving multi-dimensional arrays (also known as tensors).
   it is a common choice for implementing neural network models. theano
   has been developed in university of montreal, in a group led by yoshua
   bengio, since 2008.

   some of the features include:
     * automatic differentiation     you only have to implement the forward
       (prediction) part of the model, and theano will automatically
       figure out how to calculate the gradients at various points,
       allowing you to perform id119 for model training.
     * transparent use of a gpu     you can write the same code and run it
       either on cpu or gpu. more specifically, theano will figure out
       which parts of the computation should be moved to the gpu.
     * speed and stability optimisations     theano will internally
       reorganise and optimise your computations, in order to make them
       run faster and be more numerically stable. it will also try to
       compile some operations into c code, in order to speed up the
       computation.

   technically, theano isn   t actually a machine learning library, as it
   doesn   t provide you with pre-built models that you can train on your
   dataset. instead, it is a mathematical library that provides you with
   tools to build your own machine learning models. but if you are looking
   for machine learning toolkits, there are several good ones implemented
   on top of theano:
     * blocks   [9]http://blocks.readthedocs.org/en/latest/
     * keras   [10]http://keras.io/
     * lasagne   [11]http://lasagne.readthedocs.org/en/latest/
     * pylearn2   [12]http://deeplearning.net/software/pylearn2/

2. python refresher

   theano is a python library, so let   s go over some important points in
   python.
     * python is an interpreted language, which makes it more platform
       independent but generally slower than c, for example.
     * python uses dynamic typing. while each variable does have a
       specific type during execution, these are not explicitly stated in
       the code.
     * python uses indentation for block delimiting. so where c or java
       would use curly brackets to separate a block, python uses
       whitespace. here we define a function f to take parameter x and
       return 2*x:
def f(x):
    return 2*x
     * we define a list in python with square brackets:
a = [1,2,3,4,5]
a[1] == 2
     * we define a dictionary (key-value mapping) with curly brackets:
b = {'key1': 1, 'key2':2}
b['key2'] == 2
     * list comprehension is a neat shorthand in python for constructing
       lists. here we loop for 5 steps (values 0-4), and each time add i+1
       to the list:
c = [i+1 for i in range(5)]
c[1] == 2

3. using theano

   in order to use theano, you will need to install the dependencies and
   install theano itself. if you   re using ubuntu (tested for 14.04), you
   might get away with just running these two commands:
sudo apt-get install python-numpy python-scipy python-dev python-pip python-nose
 g++ libopenblas-dev git
sudo pip install theano

   if that doesn   t work for you, take a look at the original theano
   homepage, which contains instructions for various platforms:
   [13]http://deeplearning.net/software/theano/install.html

   to use theano in your python script, include it using:
import theano

4. minimal working example

   here is the smallest example i could come up with, which uses theano
   and actually does something:
import theano
import numpy

x = theano.tensor.fvector('x')
w = theano.shared(numpy.asarray([0.2, 0.7]), 'w')
y = (x * w).sum()

f = theano.function([x], y)

output = f([1.0, 1.0])
print output

   so what   s happening here?

   we first define a theano variable x to be a vector of 32-bit floats,
   and give it name    x   :
x = theano.tensor.fvector('x')

   next, we create a theano variable w, assign its value to be vector
   [0.2, 0.7], and name it    w   :
w = theano.shared(numpy.asarray([0.2, 0.7]), 'w')

   we define y to be the sum of all elements in the element-wise
   multiplication of x and w:
y = (x * w).sum()

   we define a theano function f, which takes as input x and outputs y:
f = theano.function([x], y)

   then call this function, giving as the argument vector [1.0, 1.0],
   essentially setting the value of variable x:
output = f([1.0, 1.0])

   the script prints out the summed product of [0.2, 0.7] and [1.0, 1.0],
   which is:
0.2*1.0 + 0.7*1.0 = 0.9

   don   t worry if the code doesn   t fully make sense. we   ll go over the
   important parts in more detail.

5. symbolic graphs in theano (!)

   i   d say this section contains the most crucial part to understanding
   theano.

   when we are creating a model with theano, we first define a symbolic
   graph of all variables and operations that need to be performed. and
   then we can apply this graph on specific inputs to get outputs.

   for example, what do you think happens when this line of theano code is
   executed in our script?
y = (x * w).sum()

   the system takes x and w, multiplies them together and sums the values.
   right?

   nope

   instead, we create a theano object y that knows its values can be
   calculated as the dot-product of x and w. but the required mathematical
   operations are not performed here. in fact, when this line was executed
   in our example code above, x didn   t even have a value yet.

   by chaining up various operations, we are creating a graph of all the
   variables and functions that need to be used to reach the output
   values. this symbolic graph is also the reason why we can only use
   theano-specific operations when defining our models. if we tried to
   integrate functions from some random python library into our
   network, they would attempt to perform the calculations immediately,
   instead of returning a theano variable as needed. exceptions do exist    
   theano overrides some basic python operators to act as expected, and
   [14]numpy is quite well integrated with theano.

6. variables

   we can define variables which don   t have any values yet. normally,
   these would be used for inputs to our network.

   the variables have to be of a specific type though. for example, here
   we define variable x to be a vector of 32-bit floats, and give it name
      x   :
x = theano.tensor.fvector('x')

   the names are generally useful for debugging and informative error
   messages. theano won   t have access to your python variable names, so
   you have to assign explicit theano names for each variable if you want
   them to be referred to as something more useful than just    a tensor   .

   there are a number of different variable types available, just have a
   look at the list [15]here. some of the more popular ones include:
   constructor  dtype   ndim
   fvector     float32 1
   ivector     int32   1
   fscalar     float32 0
   fmatrix     float32 2
   ftensor3    float32 3
   dtensor3    float64 3

   you can also define a generic vector (or tensor) and set the type with
   an argument:
x = theano.tensor.vector('x', dtype=float32)

   if you don   t set the dtype, you will create vectors of type
   config.floatx. this will become relevant in the section about gpus.

7. shared variables

   we can also define shared variables, which are shared between different
   functions and different function calls. normally, these would be used
   for weights in our neural network. theano will automatically try to
   move shared variables to the gpu, provided one is available, in order
   to speed up computation.

   here we define a shared variable and set its value to [0.2, 0.7].
w = theano.shared(numpy.asarray([0.2, 0.7]), 'w')

   the values in shared variables can be accessed and modified outside of
   our theano functions using these commands:
w.get_value()
w.set_value([0.1, 0.9])

8. functions

   theano functions are basically hooks for interacting with the symbolic
   graph. commonly, we use them for passing input into our network and
   collecting the resulting output.

   here we define a theano function f that takes x as input and returns y
   as output:
f = theano.function([x], y)

   the first parameter is the list of input variables, and the second
   parameter is the list of output variables. although if there   s only one
   output variable (like now) we don   t need to make it into a list.

   when we construct a function, theano takes over and performs some of
   its own magic. it builds the computational graph and optimises it as
   much as possible. it restructures mathematical operations to make them
   faster and more stable, compiles some parts to c, moves some tensors to
   the gpu, etc.

   theano compilation can be controlled by setting the value of mode in
   the environement variable theano_flags:
     * fast_compile     fast to compile, slow to run. python implementations
       only, minimal graph optimisation.
     * fast_run     slow to compile, fast to run. c implementations where
       available, full range of optimisations

9. minimal training example

   here   s a minimal script for actually training something in theano. we
   will be training the weights in w using id119, so that the
   result from the model would be 20 instead of the original 0.9.
import theano
import numpy

x = theano.tensor.fvector('x')
target = theano.tensor.fscalar('target')

w = theano.shared(numpy.asarray([0.2, 0.7]), 'w')
y = (x * w).sum()

cost = theano.tensor.sqr(target - y)
gradients = theano.tensor.grad(cost, [w])
w_updated = w - (0.1 * gradients[0])
updates = [(w, w_updated)]

f = theano.function([x, target], y, updates=updates)

for i in xrange(10):
    output = f([1.0, 1.0], 20.0)
    print output

   we create a second input variable called target, which will act as the
   target value we use for training:
target = theano.tensor.fscalar('target')

   in order to train the model, we need a cost function. here we use a
   simple squared distance from the target:
cost = theano.tensor.sqr(target - y)

   next, we want to calculate the partial gradients for the parameters
   that will be updated, with respect to the cost function. luckily,
   theano will do that for us. we simply call the grad function, pass in
   the real-valued cost and a list of all the variables we want gradients
   for, and it will return a list of those gradients:
gradients = theano.tensor.grad(cost, [w])

   now let   s define a symbolic variable for what the updated version of
   the parameters will look like. using id119, the update rule
   is to subtract the gradient, multiplied by the learning rate:
w_updated = w - (0.1 * gradients[0])

   and next we create a list of updates. more specifically, a list of
   tuples where the first element is the variable we want to update, and
   the second element is a variable containing the values that we want the
   first variable to contain after the update. this is just a syntax that
   theano requires.
updates = [(w, w_updated)]

   have to define a theano function again, with a couple of changes:
f = theano.function([x, target], y, updates=updates)

   it now takes two input arguments     one for the input vector, and
   another for the target value used for training. and the list of updates
   also gets attached to the function as well. every time this function is
   called, we pass in values for x and target, get back the value for y as
   output, and theano performs all the updates in the update list.

   in order to train the parameters, we repeatedly call this function (10
   times in this case). normally, we   d pass in different examples from our
   training data, but for this example we use the same x=[1.0, 1.0] and
   target=20 each time:
for i in xrange(10):
    output = f([1.0, 1.0], 20.0)
    print output

   when the script is executed, the output looks like this:
0.9
8.54
13.124
15.8744
17.52464
18.514784
19.1088704
19.46532224
19.679193344
19.8075160064

   the first time the function is called, the output value is still 0.9
   (like in the previous example), because the updates have not been
   applied yet. but with each consecutive step, the output value becomes
   closer and closer to the desired target 20.

10. useful operations

   this covers the basic logic behind building models with theano. the
   example was very simple, but we are free to define increasingly
   complicated networks, as long as we use theano-specific functions. now
   let   s look at some of these building blocks that we have available.

   evaluate the value of a theano variable

   the eval() function forces the theano variable to calculate and return
   its actual (numerical) value. if we try to just print the variable a,
   we only print its name. but if we use eval(), we get the actual square
   matrix that it is initialised to.
> a = theano.shared(numpy.asarray([[1.0,2.0],[3.0,4.0]]), 'a')
> a
a
> a.eval()
array([[1., 2.],
       [3., 4.]])

   this eval() function isn   t really used for building models, but it can
   be useful for debugging and learning how theano works. in the examples
   below, i will be using the matrix a and the eval() function to print
   the value of each variable and demonstrate how different operations
   work.

   basic element-wise operations: +     * /
c = ((a + a) / 4.0)

array([[ 0.5, 1. ],
       [ 1.5, 2. ]])

   dot product
c = theano.tensor.dot(a, a)

array([[ 7., 10.],
       [15., 22.]])

   id180
c = theano.tensor.nnet.sigmoid(a)
c = theano.tensor.tanh(a)

array([[ 0.76159416,  0.96402758],
       [ 0.99505475,  0.9993293 ]])

   softmax (row-wise)
c = theano.tensor.nnet.softmax(a)

array([[ 0.26894142,  0.73105858],
       [ 0.26894142,  0.73105858]])

   sum
c = a.sum()
c = a.sum(axis=1)

array([ 3.,  7.])

   max
c = a.max()
c = a.max(axis=1)

array([ 2.,  4.])

   argmax
c = theano.tensor.argmax(a)
c = theano.tensor.argmax(a, axis=1)

array([1, 1])

   reshape

   we sometimes need to change the dimensions of a tensor and reshape()
   allows us to do that. it takes as input a tuple containing the new
   shape and returns a new tensor with that shape. in the first example
   below, we shape a square matrix into a 1  4 matrix. in the second
   example, we use -1 which means    as big as the dimension needs to be   .
a = theano.shared(numpy.asarray([[1,2],[3,4]]), 'a')
c = a.reshape((1,4))
array([[1, 2, 3, 4]])

c = a.reshape((-1,))
array([1, 2, 3, 4])

   zeros-like, ones-like

   these functions create new tensors with the same shape but all values
   set to zero or one.
c = theano.tensor.zeros_like(a)
array([[0, 0],
       [0, 0]])

   reorder the tensor dimensions

   sometimes we need to reorder the dimensions in a tensor. in the
   examples below, the dimensions in a two-dimensional matrix are first
   swapped. then,    x    is used to create a brand new dimension.
a.eval()
array([[1, 2],
       [3, 4]])

c = a.dimshuffle((1,0))
array([[1, 3],
       [2, 4]])

c = a.dimshuffle(('x',0,1))
array([[[1, 2],
        [3, 4]]])

   indexing

   using python indexing tricks can make life so much easier. in the
   example below, we make a separate list b containing line numbers, and
   use it to construct a new matrix which contains exactly the lines we
   want from the original matrix. this can be useful when dealing with
   id27s     we can put word ids into a list and use this to
   retrieve exactly the correct sequence of embeddings from the whole
   embedding matrix.
a = theano.shared(numpy.asarray([[1.0,2.0],[3.0,4.0]]), 'a')
array([[1., 2.],
       [3., 4.]])

b = [1,1,0]
c = a[b]
array([[ 3.,  4.],
       [ 3.,  4.],
       [ 1.,  2.]])

   for assignment, we can   t do this:
a[0] = [0.0, 0.0]

   but instead, we can use set_subtensor(), which takes as arguments the
   selection of the original matrix that we want to reassign, and the
   value we want to assign it to. it returns a new tensor that has the
   corresponding values modified.
c = theano.tensor.set_subtensor(a[0],[0.0, 0.0])
array([[ 0.,  0.],
       [ 3.,  4.]])

11. classifier code example

   at this point, it   s time to move on to some more realistic examples.

   take a look at the [16]code for a very basic classifier, which tries to
   train a small network on a tiny (but real) dataset. i won   t walk you
   through it line-by-line any more; you   ve learned all the necessary
   parts by now and there are comments in the code as well.

   the task is to predict whether the gdp per capita for a country is more
   than the average gdp, based on the following features:
     * population density (per suqare km)
     * population growth rate (%)
     * urban population (%)
     * life expectancy at birth (years)
     * fertility rate (births per woman)
     * infant mortality (deaths per 1000 births)
     * enrolment in tertiary education (%)
     * unemployment (%)
     * estimated control of corruption (score)
     * estimated government effectiveness (score)
     * internet users (per 100 people)

   the data/ directory contains the files for training (121 countries) and
   testing (40 countries). each row represents one country, the first
   column is the label, followed by the features. the feature values have
   been normalised, by subtracting the mean and dividing by the standard
   deviation. the label is 1 if the gdp is more than average, and 0
   otherwise.

   once you clone the github repository (or just download the data files),
   you can run the script with:
python classifier.py data/countries-classify-gdp-normalised.train.txt data/count
ries-classify-gdp-normalised.test.txt

   the script will print information about 10 training epochs and the
   result on the test set:
epoch: 0, training_cost: 28.4304042768, training_accuracy: 0.578512396694
epoch: 1, training_cost: 24.5186290354, training_accuracy: 0.619834710744
epoch: 2, training_cost: 22.1283727037, training_accuracy: 0.619834710744
epoch: 3, training_cost: 20.7941253329, training_accuracy: 0.619834710744
epoch: 4, training_cost: 19.9641569475, training_accuracy: 0.619834710744
epoch: 5, training_cost: 19.3749411377, training_accuracy: 0.619834710744
epoch: 6, training_cost: 18.8899216914, training_accuracy: 0.619834710744
epoch: 7, training_cost: 18.4006371608, training_accuracy: 0.677685950413
epoch: 8, training_cost: 17.7210185975, training_accuracy: 0.793388429752
epoch: 9, training_cost: 16.315597037, training_accuracy: 0.876033057851
test_cost: 5.01800578051, test_accuracy: 0.925

12. recurrent functions with scan

   one more important operation to cover is scan, which can be used to
   create various recurrent functions: id56, gru, lstm, etc.

   here is sample code for using scan to define a simple id56 over word
   vectors in the input_vectors matrix:
def id56_step(x, previous_hidden_vector, w_input, w_recurrent):
    hidden_vector = theano.tensor.dot(x, w_input) +
                    theano.tensor.dot(previous_hidden_vector, w_recurrent)
    hidden_vector = theano.tensor.nnet.sigmoid(hidden_vector)

w_input = self.create_parameter_matrix('w_input', (word_embedding_size, recurren
t_size))
w_recurrent = self.create_parameter_matrix('w_recurrent', (recurrent_size, recur
rent_size))
initial_hidden_vector = theano.tensor.alloc(numpy.array(0, dtype=floatx), recurr
ent_size)

hidden_vector, _ = theano.scan(
    id56_step,
    sequences = input_vectors,
    outputs_info = initial_hidden_vector,
    non_sequences = [w_input, w_recurrent]
)

hidden_vector = hidden_vector[-1]

   the scan function is called on line 10 and it takes 4 important
   arguments:
     * fn: the function that is called at every step of the iteration.
     * sequences: the variables that we want to iterate over. if this is a
       matrix, we will be iterating over each row of that matrix.
     * outputs_info: the values that we use as the previous recurrent
       values for the very first step. usually these are just set to 0.
     * non_sequences: any additional variables that we want to pass into
       the function (fn) but don   t want to iterate over.

   we   ve defined the helper function id56_step on line 1, which gets called
   on each row of our input matrix. the scan function will be calling this
   id56_step function internally, so we need to accept any arguments in the
   same order as theano passes them. this is just something you need to
   know when dealing with scan. the order is as follows:
    1. first, the current items from the variables that we are iterating
       over. if we are iterating over a matrix, the current row is passed
       to the function.
    2. next, anything that was output from the function at the previous
       time step. this is what we use to build recursive and recurrent
       representations. at the very first time step, the values will be
       those from outputs_info instead.
    3. finally, anything we specified in non_sequences.

   what comes out from the scan function contains the hidden states (eg
   the id56_step outputs) at each step. not just the last step, but all of
   them. so if you only want the last step, you need to explicitly
   retrieve it by indexing from -1 (the last element). theano is actually
   smart enough to figure out that you   re only using the last result, and
   will optimise to discard all the intermediate ones.

   in order to construct the weight matrices, i   m using a helper function
   (self.create_parameter_matrix, definition not shown here) which takes
   as input the variable name and the shape. this means i don   t need to
   define the weight initialisation part again each time.

13. id56 classifier code example

   time to look at some more code, this time using recurrent functions and
   scan. the [17]script is available at the github repository. in this
   example, i   m using id149 (gru) from    learning phrase
   representations using id56 encoder-decoder for statistical machine
   translation    ([18]cho et al, 2014), which are essentially a simpler
   versions of lstms.

   the task is to classify sentences into 5 classes, based on their
   fine-grained sentiment (very negative, slightly negative, neutral,
   slightly positive, very positive). we use the dataset published in
      recursive deep models for semantic compositionality over a sentiment
   treebank    ([19]socher et al., 2013).

   start by downloading the dataset from
   [20]http://nlp.stanford.edu/sentiment/ (the main zip file) and unpack
   it somewhere. then, create training and test splits in the format that
   is more suitable for us, using the provided script in [21]the
   repository:
python stanford_sentiment_extractor.py 1 full /path/to/sentiment/dataset/ > data
/sentiment.train.txt
python stanford_sentiment_extractor.py 2 full /path/to/sentiment/dataset/ > data
/sentiment.test.txt

   now we can run the classifier with:
python id56classifier.py data/sentiment.train.txt data/sentiment.test.txt

   the script will train for 3 passes over the training data, and will
   then print performance on the test data.
epoch: 0 cost: 25937.7372292 accuracy: 0.285814606742
epoch: 1 cost: 21656.820174 accuracy: 0.350655430712
epoch: 2 cost: 18020.619533 accuracy: 0.429073033708
test_cost: 4784.25137484 test_accuracy: 0.388235294118

   the accuracy on the test set is about 38%, which isn   t a great result.
   but it is quite a difficult task     the current state-of-the-art system
   ([22]tai ei al., 2015) achieves 50.9% accuracy, using a large amount of
   additional phrase-level annotations, and a much bigger network based on
   lstms and parse trees. as there are 5 classes to choose from, a random
   system would get 20% accuracy.

14. running on a gpu

   theano is smart enough to move some parts of the processing to the gpu,
   as long as cuda is installed and a graphics card is made available. to
   install cuda, follow instructions on one of these links:

   [23]https://developer.nvidia.com/cuda-downloads
   [24]http://www.r-tutor.com/gpu-computing/cuda-installation/cuda7.5-ubun
   tu

   then, when running your python script, you need to point theano to the
   cuda installation. i do this by setting the environment variables in
   the command line:
ld_library_path=/usr/lib:/usr/local/cuda-7.5/lib64 theano_flags='cuda.root=/usr/
local/cuda-7.5,device=gpu,floatx=float32' python mycode.py

   this command is for cuda-7.5 in my system. you   ll need to make sure
   that the paths match the cuda installation paths in your machine. if it
   works and theano is using a gpu, the first line that gets printed will
   explicitly say so. something like this:
using gpu device 0: geforce gtx 780

   if you don   t get something similar, it probably means theano is
   not properly hooked up to use the gpu.

   at the time of writing, theano only supports 32-bit variables on the
   gpu, and this is where the floatx=float32 setting comes it. it just
   allows you to set the data type during the execution of the script,
   without writing it into your code. for example, you can define your
   vectors like this:
x = theano.tensor.vector('x', dtype=config.floatx)

   and now you can set floatx to be float32 when running the script on a
   gpu and float64 when running on your cpu.

   finally, if your machine has multiple gpus, you can control which one
   is used for the script by setting device=gpu0, device=gpu1, etc. based
   on personal experience, running multiple theano jobs on the same gpu
   does not give any advantage, so it   s best to send them to different
   ones when possible.

15. drawing the computation graph

   theano provides a command for printing a variable or a function, along
   with all the required computation, as an image:
f = theano.function([x], y)
theano.printing.pydotprint(f, outfile="f.png", var_with_name_simple=true)

   when dealing with very simple models, this can give a nice graphical
   representation. for example, here is a model from our minimal working
   example:

   printed theano function. figure for the theano tutorial.

   however, when the models get more and more complicated, the images also
   tend to get less informative:

   printed graph of a much larger function. figure for the theano
   tutorial.


16. profiling

   finally, theano also provides a useful tool for analysing bottlenecks
   in your code. just set profile=true in theano_flags, and it will print
   information about how much time is spent on different operations in
   your code.
theano_flags='profile=true' python minimal_working_example.py

   example of profiling output from theano. figure for the theano
   tutorial.

17. references

   this concludes the theano tutorial. if you haven   t yet had enough, take
   a look at the following links that i used for inspiration:
   [25]official theano homepage and documentation
   [26]official theano tutorial
   [27]a simple tutorial on theano by jiang guo
   [28]code samples for learning theano by alec radford


   written by [29]marek posted in [30]uncategorized

44 comments

    1.
   [31]january 25, 2016 - 5:26 am       
       wonderful tutorial, thank you very much, o(^   ^)o
       [32]reply
    2.
   [33]february 11, 2016 - 3:22 pm masataka
       thank you for your great tutorial.
       i suspect that    h_prev    and    _h    are swapped at the line 41 of
       id56classifier.py , compared with eq. (7) on the paper of    cho et
       al, 2014   .
       h = (1.0     z) * h_prev + z * _h
       is either one ok to use?
       [34]reply
          +
        [35]february 11, 2016 - 9:34 pm [36]marek
            hi masataka,
            good point. both of them will give a similar result, as the
            sigmoid function can equally well model z and (1-z). but for
            clarity i changed the code to be more similar to the original
            paper. thanks.
            [37]reply
    3.
   [38]march 17, 2016 - 6:08 am shraddha
       nice tutorial.
       what is predicted_value?what is its importance?
       in
       # creating the network
       n_features = len(data_train[0][1])
       classifier = classifier(n_features)
       what n_features indicates?
       thank you
       [39]reply
          +
        [40]april 3, 2016 - 4:48 pm [41]marek
            n_features is the size of the feature vector that is given as
            input to the network.
            the network predicts a class for the data instance. if there
            are only 2 possible classes, we can do this by predicting a
            value between 0 and 1. if the predicted value is close to 0,
            we say it belongs to one class, and if it is close to 1, it
            belongs to the other class.
            [42]reply
               o
             [43]april 21, 2016 - 6:26 am shraddha
                 thank you for reply.
                 what will be the condition in training and testing loop
                 if the number of classes are more than two?? how to
                 predict the value for more than two classes??
                 thank you!!
                 [44]reply
    4.
   [45]march 17, 2016 - 6:09 am shraddha
       hi
       how feature extraction is done in theano and neural network?
       [46]reply
          +
        [47]april 3, 2016 - 4:43 pm [48]marek
            what do you mean by feature extraction?
            [49]reply
          +
        [50]april 18, 2016 - 2:26 pm feras
            feature extraction is separated technique to extract features
            you think they represent the data without redundancy.
            [51]reply
          +
        [52]october 3, 2016 - 8:54 am [53]bert
            himmel, jetzt oute ich schon exlesben nach allem, was ich
            bisher von dir gelesen habe, incl. der tweets, hatte ich da
                berhaupt keine zweifel. der gaydar scheint bei lesben doch
            nicht so richtig zu funktionieren. ich werde dich dann in den
            verschieben, da gibt es auch nette frauen &n;;ps&nbspbnele
            tabler
            [54]reply
    5.
   [55]april 2, 2016 - 5:23 am adepu ravi
       small typo in
       8. functions
       theano functions are basically hooks for interacting with the
          symbolic    graph.
       [56]reply
          +
        [57]april 3, 2016 - 4:41 pm [58]marek
            thanks!
            [59]reply
               o
             [60]july 18, 2016 - 9:38 pm [61]hank
                 you bought a used car and things do wear out. why didn   t
                 you check the car out when you bought it? its your job to
                 make sure you aren   t getting screwed.i don   t know about
                 over there but in the us used cars are sold with a
                    limited wa8r1nty&#r22a; or    as is   . at best they will
                 cover a few major things for a few months and at worst
                 they cover nothing.
                 [62]reply
          +
        [63]july 18, 2016 - 10:44 pm [64]idalia
            yes sir. that too. discipline is the bridge between thought
            and acmncplishmeot. he is young, and needs to mature this
            year. he got a taste of the nectar in the holy grail,
            hopefully he and his teamates burn with an intensity unknown
            to mankind to possess it in 2013.
            [65]reply
    6.
   [66]april 18, 2016 - 2:25 pm feras
       i have two questions
       1- why did you add the id173 term to by summing and adding
       all weights to the cost function, i test the code without it and it
       gave better result ?
       2- is it true how i see it that we are updating the weights on each
       vector iteration and not on each epoch. which i means we are
       updating on single vector and not on batch?
       [67]reply
          +
        [68]may 3, 2016 - 11:24 pm [69]marek
            1. it   s a standard l2 regularisation. i just added it to show
            the implementation, but didn   t try to choose an optimal value
            for l2_regularisation.
            2. yes. i haven   t implemented batch training here. this
            network is trained one instance at a time.
            [70]reply
               o
             [71]july 18, 2016 - 9:50 pm [72]retta
                 though we ha87;#d21n&t seen them on the mainland, we
                 figured they would be everywhere on skye. yet, after
                 arriving on the island, we spent the day driving around
                 broadford and portree and the
                 [73]reply
    7.
   [74]april 22, 2016 - 12:00 pm mor
       hi,
       thank you for sharing this.
       is the initiation of initial_hidden_vector applied every time we
       call the    train    function from the main function? i.e. the h_prev
       will always be initiated with 0.0 when we call the function.
       thanks..
       [75]reply
          +
        [76]may 3, 2016 - 11:17 pm [77]marek
            yes, the initial_hidden_vector has a static value of zeros.
            [78]reply
    8.
   [79]april 25, 2016 - 3:54 am shraddha
       thank you for reply.
       what will be the condition in training and testing loop if the
       number of classes are more than two?? how to predict the value for
       more than two classes??
       thank you!!
       [80]reply
          +
        [81]may 3, 2016 - 11:28 pm [82]marek
            there   s a variable n_classes that controls how many different
            classes the output layer has.
            [83]reply
    9.
   [84]may 5, 2016 - 6:27 am shraddha
       hi
       i have tried to implement the classifier.py code on cpu and gpu. i
       have created .theanorc file to configure theano and gpu. i tried
       for different number of epochs but cpu speed is always greater than
       gpu. my .theanorc file is:
       [global]
       floatx = float32
       device = gpu0
       mode=fast_run
       [gcc]
       cxxflaags = -id:\mingw\include
       [nvcc]
       fastmath = true
       [lib]
       cnmem = 1
       i am using tesla c2075 gpu. i want to increase the execution speed
       on gpu. how to achieve this?
       [85]reply
          +
        [86]october 1, 2016 - 3:37 pm [87]marek
            if you run your code and it prints the name of your gpu into
            the error output, then that generally means theano is using
            the gpu. if it   s running on the gpu and you still need to make
            it faster, then you probably need to think of ways of
            optimising the model itself.
            [88]reply
   10. may 7, 2016 - 12:30 pm pingback: [89]deep learning sw libraries |
       bits and pieces
   11.
   [90]september 16, 2016 - 12:07 pm yiping
       thanks for the post,
       i tried to modify the gru code to lstm. however it gives much
       poorer result (it stabilised to 0.25 even with many iterations).
       below is the code    lstm_step   :
       def lstm_step(x, h_prev, c_prev, w_xm, w_hm, w_xh, w_hh):
       m = theano.tensor.nnet.sigmoid(theano.tensor.dot(x, w_xm) +
       theano.tensor.dot(h_prev, w_hm))
       i = _slice(m, 0, 3)
       f = _slice(m, 1, 3)
       o = _slice(m, 2, 3)
       g = theano.tensor.tanh(theano.tensor.dot(x, w_xh) +
       theano.tensor.dot(h_prev, w_hh))
       c = c_prev * f + g * i
       h = theano.tensor.tanh(c) * o
       return [h, c]
       in the scan block i passed
       outputs_info = [initial_hidden_vector, initial_internal_vector],
       did i make a mistake or just the lstm overfits the data? thanks in
       advance!
       [91]reply
          +
        [92]october 1, 2016 - 3:48 pm [93]marek
            hi yiping,
            there seems to be quite a few variations of lstm going around
            are yours is one of them. if you want to use the    real    lstm,
            i   d recomment following the formulas here:
            [94]http://www.cs.toronto.edu/~graves/asru_2013.pdf
            notice there   s a bias term, some of the weights are diagonal,
            and the output gate is calculated differently from the input
            and forget gates.
            but to be fair, your version would probably work quite
            similarly.
            the low performance could be because of overfitting indeed.
            you could monitor the performance on training and development
            sets     if the training accuracy goes unreasonably high and
            development accuracy starts to drop, you   ve got overfitting
            and need to regularise somehow.
            [95]reply
   12.
   [96]november 4, 2016 - 9:49 am denis
       very nice. i   m looking forward to your tutorial on tensorflow
       ([97]https://www.tensorflow.org/ has a good picture of a dataflow
       graph, the main idea.)
       should anyone want to print gradients in the loop:
       theano-example-how-to-monitor-gradients.py under
       [98]https://gist.github.com/denis-bz .
       unfortunately firefox mangles your tutorial, prints one page then
       comments (on mac osx 10.8);
       if you could put up a pdf too, even more people would read it.
       [99]reply
          +
        [100]november 4, 2016 - 3:36 pm [101]marek
            thanks!
            that   s odd, i just checked the page with firefox 49.0.2 on
            ubuntu and it looked fine. what version are you using?
            [102]reply
               o
             [103]november 4, 2016 - 4:57 pm denis
                 it   s the mac html-to-pdf, so don   t bother, thanks.
                 in firefox 48.0.2 (the max for macosx 10.8, grr) all the
                 text from the    deep learning    picture up to comments is
                 squeezed into a narrow box to the right of the picture.
                 just tried safari, which squeezes a bit, then recovers.
                 [104]reply
          +
        [105]january 5, 2017 - 9:41 pm [106]johnette
            i   ve been loonkig for a post like this for an age
            [107]reply
          +
        [108]may 8, 2017 - 3:33 pm [109]pokemon go teleport hack for
            android
            we   ve arrived at the end of the line and i have what i need!
            [110]reply
   13.
   [111]november 21, 2016 - 8:58 am mark
       thanks a lot!
       i am a new guy in theano,when i run this code on spyder,there is an
       error   
          indexerror: list index out of range    and its hard for me to find
       what   s wrong with it      
       looking forward to your reply
       [112]reply
          +
        [113]november 21, 2016 - 1:47 pm [114]marek
            hi mark,
            1. which code specifically?
            2. what is the full error?
            [115]reply
               o
             [116]november 22, 2016 - 3:16 am mark
                 hi,thanks for your reply, i copied all my running
                 information as follows:
                 runfile(   f:/theano
                 learning/theano-tutorial-master/classifier.py   ,
                 wdir=   f:/theano learning/theano-tutorial-master   )
                 using gpu device 0: geforce gtx 950 (cnmem is disabled,
                 cudnn not available)
                 traceback (most recent call last):
                 file       , line 1, in
                 runfile(   f:/theano
                 learning/theano-tutorial-master/classifier.py   ,
                 wdir=   f:/theano learning/theano-tutorial-master   )
                 file
                    e:\users\win10\anaconda2\lib\site-packages\spyder\utils\
                 site\sitecustomize.py   , line 866, in runfile
                 execfile(filename, namespace)
                 file
                    e:\users\win10\anaconda2\lib\site-packages\spyder\utils\
                 site\sitecustomize.py   , line 87, in execfile
                 exec(compile(scripttext, filename,    exec   ), glob, loc)
                 file    f:/theano
                 learning/theano-tutorial-master/classifier.py   , line 65,
                 in
                 path_train = sys.argv[1]
                 indexerror: list index out of range
                 may these be useful to you
                 [117]reply
                    #
                  [118]november 22, 2016 - 3:19 am [119]marek
                      you need to pass the data files as argument. take a
                      look at the readme file in the repository for
                      instructions on executing the script.
                      [120]reply
   14.
   [121]january 6, 2017 - 11:47 am jackson huang
       hi marek, thank you so much for your extremely helpful tutorial on
       theano.
       would you mind explaining what the [0] means in the id56classifier?
       output = theano.tensor.nnet.softmax([theano.tensor.dot(w_output,
       hidden_vector)])[0]
       thanks!
       [122]reply
   15. february 8, 2017 - 4:53 pm pingback: [123]getting started with the
       conditional image generation project     site title
   16.
   [124]february 27, 2017 - 6:13 am [125]ven hudson
       thanks for sharing the tutorial.
       [126]reply
   17.
   [127]march 6, 2017 - 9:55 am kenton k. yee
       thanks, this was one of the clearest primers on using theano for
       id56s that i found. the only thing i couldn   t get to work is the
       computation graph. even after i installed graphiz, my pydot kept
       saying it couldn   t find graphiz.
       ken
       [128]reply
   18.
   [129]june 9, 2017 - 7:00 am tushar
       just going through the tutorial and found an interesting issue
       while compiling point 9 minimal training example.
       the error posed is: typeerror: (   bad input argument to theano
       function with name    d:/theano practice/grads.py:15    at index 0
       (0-based). \nbacktrace when that variable is created:\n\n
       x = theano.tensor.fvector(\   x\   )\n   ,    tensortype(float32, vector)
       cannot store accurately value [0.1, 1], it would be represented as
       [ 0.1 1. ]. if you do not mind this precision loss, you can: 1)
       explicitly convert your data to a numpy array of dtype float32, or
       2) set    allow_input_downcast=true    when calling    function   .   , [0.1,
       1])
       strangely the code works some time but not always.
       the solution i could found was to use use    in    class attributes to
       allow my function to downcast. my solution is :
       f = theano.function([in(x,strict=false,allow_downcast=true),
       target], y, updates=updates)
       [130]reply
   19.
   [131]october 2, 2017 - 9:34 am [132]cnc turned parts
       carry on nice article
       [133]reply
   20.
   [134]november 27, 2017 - 9:41 pm [135]breastmri.net
         h  re is certainly a great deal to learn abo  t this subject.
       i love all of th   points    ou made.
       [136]reply
   21.
   [137]march 23, 2018 - 7:21 am marko
       hi marek,
       this tutorial is great! i started studying neural networks a couple
       of weeks ago and i have some questions to you about your code.
       1) in section 9 and 11 you have this row
       theano.tensor.sqr(predicted_value     target_value).sum() .
       difference between section 9 and 11 is that you have there this
       term    sum()   . can   t understand why do u have it there? i know that
       we have to compute sum of residuals^{2} but if so, why don   t you
       have this term in the section 9 as well?
       2) next question is about training. as far as i know, we have to
       compute argmin of the cost function with respect to weights. thus,
       we have to take all the training data and the target values and
       minimize the sum of residuals^{2}. however, i can   t understand how
       training in your codes is provided.
       i think that my questions can explained with the fact that i
       understand syntax of the code in a wrong way but i would be really
       pleased if you answer me.
       thank you.
       p.s. i saw that you are doing nlp. this is my research project. is
       it possible to write you an email and talk about it a little bit?
       [138]reply
          +
        [139]april 3, 2018 - 11:28 pm [140]marek
            hi marko,
            1) in section 9, we have a toy example where we   re updating
            the parameters for each datapoint, so the sum is not
            necessary. the code in section 11 is written so that we could
            be working with batches, therefore summing over all the
            datapoints in the batch becomes necessary.
            2) we don   t actually compute argmin, but use id119.
            we define a differentiable cost function, then calculate
            gradients for each parameter and update all the values so that
            they slightly move towards predicting the correct answer each
            time. if you google, i   m sure you   ll find more in-depth
            explanations of id119.
            sure, feel free to drop me an email.
            marek
            [141]reply

post a comment [142]cancel reply

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   you may use the following html:
          <a href="" title=""> <abbr title=""> <acronym title=""> <b>
          <blockquote cite=""> <cite> <code> <del datetime=""> <em> <i> <q
          cite=""> <s> <strike> <strong>

   name (required) ______________________________ email (required - will
   be kept a secret) ______________________________ website
   ______________________________

   post comment

   [143]online representation learning in recurrent neural language models
   [144]analysing nlp publication patterns

   search for: ____________________ search

recent posts

     * [145]ml and nlp publications in 2018
     * [146]57 summaries of machine learning and nlp research
     * [147]ml/nlp publications in 2017
     * [148]attending to characters in neural sequence labeling models
     * [149]nlp and ml publications     looking back at 2016
     * [150]analysing nlp publication patterns
     * [151]theano tutorial
     * [152]online representation learning in recurrent neural language
       models
     * [153]26 things i learned in the deep learning summer school
     * [154]transforming images to feature vectors

   [155]tweets by @marekrei

archives

     * [156]january 2019
     * [157]january 2018
     * [158]january 2017
     * [159]june 2016
     * [160]january 2016
     * [161]august 2015
     * [162]june 2015
     * [163]october 2014
     * [164]september 2014
     * [165]february 2014
     * [166]january 2014

meta

     * [167]log in
     * [168]entries rss
     * [169]comments rss
     * [170]wordpress.org

   [171]about me

references

   1. http://www.marekrei.com/blog/feed/
   2. http://www.marekrei.com/blog/comments/feed/
   3. http://www.marekrei.com/blog/theano-tutorial/feed/
   4. http://www.marekrei.com/blog/wp-json/oembed/1.0/embed?url=http://www.marekrei.com/blog/theano-tutorial/
   5. http://www.marekrei.com/blog/wp-json/oembed/1.0/embed?url=http://www.marekrei.com/blog/theano-tutorial/&format=xml
   6. http://www.marekrei.com/blog/
   7. http://www.marekrei.com/blog/about-me/
   8. https://github.com/marekrei/theano-tutorial
   9. http://blocks.readthedocs.org/en/latest/
  10. http://keras.io/
  11. http://lasagne.readthedocs.org/en/latest/
  12. http://deeplearning.net/software/pylearn2/
  13. http://deeplearning.net/software/theano/install.html
  14. http://www.numpy.org/
  15. http://deeplearning.net/software/theano/library/tensor/basic.html
  16. https://github.com/marekrei/theano-tutorial/blob/master/classifier.py
  17. https://github.com/marekrei/theano-tutorial/blob/master/id56classifier.py
  18. http://arxiv.org/abs/1406.1078
  19. http://www.aclweb.org/anthology/d13-1170
  20. http://nlp.stanford.edu/sentiment/
  21. https://github.com/marekrei/theano-tutorial
  22. https://aclweb.org/anthology/p/p15/p15-1150.pdf
  23. https://developer.nvidia.com/cuda-downloads
  24. http://www.r-tutor.com/gpu-computing/cuda-installation/cuda7.5-ubuntu
  25. http://deeplearning.net/software/theano/
  26. http://deeplearning.net/software/theano/tutorial/
  27. http://ir.hit.edu.cn/~jguo/docs/notes/a_simple_tutorial_on_theano.pdf
  28. https://github.com/newmu/theano-tutorials/
  29. http://www.marekrei.com/blog/author/marek/
  30. http://www.marekrei.com/blog/category/uncategorized/
  31. http://www.marekrei.com/blog/theano-tutorial/#comment-24285
  32. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24285#respond
  33. http://www.marekrei.com/blog/theano-tutorial/#comment-24444
  34. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24444#respond
  35. http://www.marekrei.com/blog/theano-tutorial/#comment-24446
  36. http://www.marekrei.com/blog/author/marek/
  37. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24446#respond
  38. http://www.marekrei.com/blog/theano-tutorial/#comment-24712
  39. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24712#respond
  40. http://www.marekrei.com/blog/theano-tutorial/#comment-24888
  41. http://www.marekrei.com/blog/author/marek/
  42. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24888#respond
  43. http://www.marekrei.com/blog/theano-tutorial/#comment-25037
  44. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25037#respond
  45. http://www.marekrei.com/blog/theano-tutorial/#comment-24713
  46. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24713#respond
  47. http://www.marekrei.com/blog/theano-tutorial/#comment-24887
  48. http://www.marekrei.com/blog/author/marek/
  49. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24887#respond
  50. http://www.marekrei.com/blog/theano-tutorial/#comment-25027
  51. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25027#respond
  52. http://www.marekrei.com/blog/theano-tutorial/#comment-33121
  53. http://ybemwxdxm.com/
  54. http://www.marekrei.com/blog/theano-tutorial/?replytocom=33121#respond
  55. http://www.marekrei.com/blog/theano-tutorial/#comment-24867
  56. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24867#respond
  57. http://www.marekrei.com/blog/theano-tutorial/#comment-24886
  58. http://www.marekrei.com/blog/author/marek/
  59. http://www.marekrei.com/blog/theano-tutorial/?replytocom=24886#respond
  60. http://www.marekrei.com/blog/theano-tutorial/#comment-25629
  61. http://tohfksnwr.com/
  62. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25629#respond
  63. http://www.marekrei.com/blog/theano-tutorial/#comment-25645
  64. http://ffckpzppjlf.com/
  65. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25645#respond
  66. http://www.marekrei.com/blog/theano-tutorial/#comment-25026
  67. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25026#respond
  68. http://www.marekrei.com/blog/theano-tutorial/#comment-25138
  69. http://www.marekrei.com/blog/author/marek/
  70. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25138#respond
  71. http://www.marekrei.com/blog/theano-tutorial/#comment-25632
  72. http://vdnllq.com/
  73. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25632#respond
  74. http://www.marekrei.com/blog/theano-tutorial/#comment-25050
  75. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25050#respond
  76. http://www.marekrei.com/blog/theano-tutorial/#comment-25137
  77. http://www.marekrei.com/blog/author/marek/
  78. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25137#respond
  79. http://www.marekrei.com/blog/theano-tutorial/#comment-25067
  80. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25067#respond
  81. http://www.marekrei.com/blog/theano-tutorial/#comment-25139
  82. http://www.marekrei.com/blog/author/marek/
  83. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25139#respond
  84. http://www.marekrei.com/blog/theano-tutorial/#comment-25151
  85. http://www.marekrei.com/blog/theano-tutorial/?replytocom=25151#respond
  86. http://www.marekrei.com/blog/theano-tutorial/#comment-32934
  87. http://www.marekrei.com/blog/author/marek/
  88. http://www.marekrei.com/blog/theano-tutorial/?replytocom=32934#respond
  89. https://srippa.wordpress.com/2015/11/06/deep-learning-sw-libraries/
  90. http://www.marekrei.com/blog/theano-tutorial/#comment-31292
  91. http://www.marekrei.com/blog/theano-tutorial/?replytocom=31292#respond
  92. http://www.marekrei.com/blog/theano-tutorial/#comment-32935
  93. http://www.marekrei.com/blog/author/marek/
  94. http://www.cs.toronto.edu/~graves/asru_2013.pdf
  95. http://www.marekrei.com/blog/theano-tutorial/?replytocom=32935#respond
  96. http://www.marekrei.com/blog/theano-tutorial/#comment-36571
  97. https://www.tensorflow.org/
  98. https://gist.github.com/denis-bz
  99. http://www.marekrei.com/blog/theano-tutorial/?replytocom=36571#respond
 100. http://www.marekrei.com/blog/theano-tutorial/#comment-36597
 101. http://www.marekrei.com/blog/author/marek/
 102. http://www.marekrei.com/blog/theano-tutorial/?replytocom=36597#respond
 103. http://www.marekrei.com/blog/theano-tutorial/#comment-36603
 104. http://www.marekrei.com/blog/theano-tutorial/?replytocom=36603#respond
 105. http://www.marekrei.com/blog/theano-tutorial/#comment-41793
 106. http://nwuybdny.com/
 107. http://www.marekrei.com/blog/theano-tutorial/?replytocom=41793#respond
 108. http://www.marekrei.com/blog/theano-tutorial/#comment-68764
 109. http://www.newcoinsgenerator.win/pokemon-go-teleport-hack-for-android.html
 110. http://www.marekrei.com/blog/theano-tutorial/?replytocom=68764#respond
 111. http://www.marekrei.com/blog/theano-tutorial/#comment-38048
 112. http://www.marekrei.com/blog/theano-tutorial/?replytocom=38048#respond
 113. http://www.marekrei.com/blog/theano-tutorial/#comment-38064
 114. http://www.marekrei.com/blog/author/marek/
 115. http://www.marekrei.com/blog/theano-tutorial/?replytocom=38064#respond
 116. http://www.marekrei.com/blog/theano-tutorial/#comment-38114
 117. http://www.marekrei.com/blog/theano-tutorial/?replytocom=38114#respond
 118. http://www.marekrei.com/blog/theano-tutorial/#comment-38115
 119. http://www.marekrei.com/blog/author/marek/
 120. http://www.marekrei.com/blog/theano-tutorial/?replytocom=38115#respond
 121. http://www.marekrei.com/blog/theano-tutorial/#comment-41869
 122. http://www.marekrei.com/blog/theano-tutorial/?replytocom=41869#respond
 123. https://dimitrigallos.wordpress.com/2017/02/08/first-blog-post/
 124. http://www.marekrei.com/blog/theano-tutorial/#comment-52867
 125. https://www.mockingfish.com/
 126. http://www.marekrei.com/blog/theano-tutorial/?replytocom=52867#respond
 127. http://www.marekrei.com/blog/theano-tutorial/#comment-54396
 128. http://www.marekrei.com/blog/theano-tutorial/?replytocom=54396#respond
 129. http://www.marekrei.com/blog/theano-tutorial/#comment-73277
 130. http://www.marekrei.com/blog/theano-tutorial/?replytocom=73277#respond
 131. http://www.marekrei.com/blog/theano-tutorial/#comment-83733
 132. http://cnc-turnedparts.com/
 133. http://www.marekrei.com/blog/theano-tutorial/?replytocom=83733#respond
 134. http://www.marekrei.com/blog/theano-tutorial/#comment-85019
 135. http://www.breastmri.net/
 136. http://www.marekrei.com/blog/theano-tutorial/?replytocom=85019#respond
 137. http://www.marekrei.com/blog/theano-tutorial/#comment-92691
 138. http://www.marekrei.com/blog/theano-tutorial/?replytocom=92691#respond
 139. http://www.marekrei.com/blog/theano-tutorial/#comment-93981
 140. http://www.marekrei.com/blog/author/marek/
 141. http://www.marekrei.com/blog/theano-tutorial/?replytocom=93981#respond
 142. http://www.marekrei.com/blog/theano-tutorial/#respond
 143. http://www.marekrei.com/blog/online-representation-learning-in-recurrent-neural-language-models/
 144. http://www.marekrei.com/blog/analysing-nlp-publication-patterns/
 145. http://www.marekrei.com/blog/ml-and-nlp-publications-in-2018/
 146. http://www.marekrei.com/blog/paper-summaries/
 147. http://www.marekrei.com/blog/ml-nlp-publications-in-2017/
 148. http://www.marekrei.com/blog/attending-to-characters-in-neural-sequence-labeling-models/
 149. http://www.marekrei.com/blog/nlp-and-ml-publications-looking-back-at-2016/
 150. http://www.marekrei.com/blog/analysing-nlp-publication-patterns/
 151. http://www.marekrei.com/blog/theano-tutorial/
 152. http://www.marekrei.com/blog/online-representation-learning-in-recurrent-neural-language-models/
 153. http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/
 154. http://www.marekrei.com/blog/transforming-images-to-feature-vectors/
 155. https://twitter.com/marekrei
 156. http://www.marekrei.com/blog/2019/01/
 157. http://www.marekrei.com/blog/2018/01/
 158. http://www.marekrei.com/blog/2017/01/
 159. http://www.marekrei.com/blog/2016/06/
 160. http://www.marekrei.com/blog/2016/01/
 161. http://www.marekrei.com/blog/2015/08/
 162. http://www.marekrei.com/blog/2015/06/
 163. http://www.marekrei.com/blog/2014/10/
 164. http://www.marekrei.com/blog/2014/09/
 165. http://www.marekrei.com/blog/2014/02/
 166. http://www.marekrei.com/blog/2014/01/
 167. http://www.marekrei.com/blog/wp-login.php
 168. http://www.marekrei.com/blog/feed/
 169. http://www.marekrei.com/blog/comments/feed/
 170. https://wordpress.org/
 171. http://www.marekrei.com/
