foundations and trends r    in machine learning
vol. 9, no. 1 (2016) 1   118
c    2016 m. udell, c. horn, r. zadeh and s. boyd
doi: 10.1561/2200000055

generalized low rank models

operations research and information engineering

madeleine udell

cornell university
udell@cornell.edu

corinne horn

electrical engineering
stanford university
cehorn@stanford.edu

reza zadeh

stanford university
rezab@stanford.edu

stephen boyd

electrical engineering
stanford university
boyd@stanford.edu

computational and mathematical engineering

contents

1 introduction

1.1 previous work . . . . . . . . . . . . . . . . . . . . . . . .
1.2 organization . . . . . . . . . . . . . . . . . . . . . . . . .

2 pca and quadratically regularized pca

2.1 pca . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 quadratically regularized pca . . . . . . . . . . . . . . .
2.3 solution methods
. . . . . . . . . . . . . . . . . . . . . .
2.4 missing data and matrix completion . . . . . . . . . . . .
interpretations and applications . . . . . . . . . . . . . . .
2.5
2.6 o sets and scaling
. . . . . . . . . . . . . . . . . . . . .

3 generalized id173

3.1 solution methods
. . . . . . . . . . . . . . . . . . . . . .
3.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 o sets and scaling
. . . . . . . . . . . . . . . . . . . . .

4 generalized id168s

4.1 solution methods
. . . . . . . . . . . . . . . . . . . . . .
4.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 o sets and scaling
. . . . . . . . . . . . . . . . . . . . .

2
4
8

9
9
11
11
15
17
20

21
22
23
29

30
30
31
35

ii

5 id168s for abstract data types

5.1 solution methods
. . . . . . . . . . . . . . . . . . . . . .
5.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 missing data and data imputation . . . . . . . . . . . . .
interpretations and applications . . . . . . . . . . . . . . .
5.4
5.5 o sets and scaling
. . . . . . . . . . . . . . . . . . . . .
5.6 numerical examples . . . . . . . . . . . . . . . . . . . . .

6 multi-dimensional id168s

6.1 examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 o sets and scaling
. . . . . . . . . . . . . . . . . . . . .
6.3 numerical examples . . . . . . . . . . . . . . . . . . . . .

7 fitting low rank models

7.1 alternating minimization . . . . . . . . . . . . . . . . . .
7.2 early stopping . . . . . . . . . . . . . . . . . . . . . . . .
7.3 quadratic objectives . . . . . . . . . . . . . . . . . . . . .
7.4 convergence . . . . . . . . . . . . . . . . . . . . . . . . .
7.5
initialization . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
7.6 global optimality

8 choosing low rank models

8.1 id173 paths . . . . . . . . . . . . . . . . . . . . .
8.2 choosing model parameters . . . . . . . . . . . . . . . . .
8.3 on-line optimization . . . . . . . . . . . . . . . . . . . . .

9 implementations

9.1 python implementation . . . . . . . . . . . . . . . . . . .
9.2
julia implementation . . . . . . . . . . . . . . . . . . . .
9.3 spark implementation . . . . . . . . . . . . . . . . . . . .

acknowledgments

appendices

a examples, id168s, and regularizers

iii

37
38
38
41
42
45
45

54
55
59
59

61
62
62
67
68
70
73

78
78
80
85

87
88
90
94

98

99

100

iv

a.1 quadratically regularized pca . . . . . . . . . . . . . . . 100

references

106

abstract
principal components analysis (pca) is a well-known technique for ap-
proximating a tabular data set by a low rank matrix. here, we extend
the idea of pca to handle arbitrary data sets consisting of numerical,
boolean, categorical, ordinal, and other data types. this framework
encompasses many well known techniques in data analysis, such as
nonnegative id105, matrix completion, sparse and ro-
bust pca, id116, k-svd, and maximum margin matrix factoriza-
tion. the method handles heterogeneous data sets, and leads to coher-
ent schemes for compressing, denoising, and imputing missing entries
across all data types simultaneously. it also admits a number of inter-
esting interpretations of the low rank factors, which allow id91
of examples or of features. we propose several parallel algorithms for
   tting generalized low rank models, and describe implementations and
numerical results.

m. udell, c. horn, r. zadeh and s. boyd. generalized low rank models.
foundations and trends r    in machine learning, vol. 9, no. 1, pp. 1   118, 2016.
doi: 10.1561/2200000055.

1

introduction

in applications of machine learning and data mining, one frequently
encounters large collections of high dimensional data organized into
a table. each row in the table represents an example, and each col-
umn a feature or attribute. these tables may have columns of di erent
(sometimes, non-numeric) types, and often have many missing entries.
for example, in medicine, the table might record patient attributes
or lab tests: each row of the table lists test or survey results for a
particular patient, and each column corresponds to a distinct test or
survey question. the values in the table might be numerical (3.14),
boolean (yes, no), ordinal (never, sometimes, always), or categorical (a,
b, o). tests not administered or questions left blank result in missing
entries in the data set. other examples abound: in    nance, the table
might record known characteristics of companies or asset classes; in
social science settings, it might record survey responses; in marketing,
it might record known customer characteristics and purchase history.
exploratory data analysis can be di cult in this setting. to better
understand a complex data set, one would like to be able to visualize
archetypical examples, to cluster examples, to    nd correlated features,
to    ll in (impute) missing entries, and to remove (or simply identify)

2

3

spurious, anomalous, or noisy data points. this paper introduces a
templated method to enable these analyses even on large data sets with
heterogeneous values and with many missing entries. our approach
will be to embed both the rows (examples) and columns (features)
of the table into the same low dimensional vector space. these low
dimensional vectors can then be plotted, clustered, and used to impute
missing entries or identify anomalous ones.

if the data set consists only of numerical (real-valued) data, then
a simple and well-known technique to    nd this embedding is princi-
pal components analysis (pca). pca    nds a low rank matrix that
minimizes the approximation error, in the least-squares sense, to the
original data set. a factorization of this low rank matrix embeds the
original high dimensional features into a low dimensional space. ex-
tensions of pca can handle missing data values, and can be used to
impute missing entries.

here, we extend pca to approximate an arbitrary data set by re-
placing the least-squares error used in pca with a id168 that
is appropriate for the given data type. another extension beyond pca
is to add id173 on the low dimensional factors to impose or
encourage some structure, such as sparsity or nonnegativity, in the low
dimensional factors. in this paper we use the term generalized low rank
model (glrm) to refer to the problem of approximating a data set as
a product of two low dimensional factors by minimizing an objective
function. the objective will consist of a id168 on the approxima-
tion error together with id173 of the low dimensional factors.
with these extensions of pca, the resulting low rank representation
of the data set still produces a low dimensional embedding of the data
set, as in pca.

many of the low rank modeling problems we must solve will be
familiar. we recover an optimization formulation of nonnegative ma-
trix factorization, matrix completion, sparse and robust pca, id116,
k-svd, and maximum margin id105, to name just a
few.the scope of the problems we consider, however, is more broad,
encompassing many di erent combinations of id168 and regu-
larizer. a few of the choices we consider are shown in tables a.1 and

4

introduction

a.2 of appendix a for reference; all of these are discussed in detail
later in the paper.

these low rank approximation problems are not convex, and in
general cannot be solved globally and e ciently. there are a few ex-
ceptional problems that are known to have convex relaxations which
are tight under certain conditions, and hence are e ciently (globally)
solvable under these conditions. however, all of these approximation
problems can be heuristically (locally) solved by methods that alter-
nate between updating the two factors in the low rank approximation.
each step involves either a convex problem, or a nonconvex problem
that is simple enough that we can solve it exactly. while these alternat-
ing methods need not    nd the globally best low rank approximation,
they are often very useful and e ective for the original data analysis
problem.

1.1 previous work

uni   ed views of id105. we are certainly not the    rst
to note that id105 algorithms may be viewed in a uni   ed
framework, parametrized by a small number of modeling decisions. the
   rst instance we    nd in the literature of this uni   ed view appeared in a
paper by collins, dasgupta, and schapire, [29], extending pca to use
id168s derived from any probabilistic model in the exponential
family. gordon   s generalized2 linear2 models [53] extended the frame-
work to id168s derived from the generalized bregman divergence
of any convex function, which includes models such as independent
components analysis (ica). srebro   s 2004 phd thesis [133] extended
the framework to other id168s, including hinge loss and kl-
divergence loss, and to other regularizers, including the nuclear norm
and max-norm. similarly, chapter 8 in tropp   s 2004 phd thesis [144]
explored a number of new regularizers, presenting a range of cluster-
ing problems as id105 problems with constraints, and
anticipated the k-svd algorithm [4]. singh and gordon [129] o ered
a complete view of the state of the literature on id105
in table 1 of their 2008 paper, and noted that by changing the loss

1.1. previous work

5

function and regularizer, one may recover algorithms including pca,
weighted pca, id116, k-medians,   1 svd, probabilistic latent se-
mantic indexing (plsi), nonnegative id105 with   2 or
kl-divergence loss, exponential family pca, and mmmf. witten et
al. introduced the statistics community to sparsity-inducing matrix fac-
torization in a 2009 paper on penalized matrix decomposition, with
applications to sparse pca and canonical correlation analysis [155].
recently, markovsky   s monograph on low rank approximation [97] re-
viewed some of this literature, with a focus on applications in system,
control, and signal processing. the glrms discussed in this paper
include all of these models, and many more.

heterogeneous data. many authors have proposed the use of low
rank models as a tool for integrating heterogeneous data. the earliest
example of this approach is canonical correlation analysis, developed
by hotelling [63] in 1936 to understand the relations between two sets
of variates in terms of the eigenvectors of their covariance matrix. this
approach was extended by witten et al. [155] to encourage structured
(e.g., sparse) factors. in the 1970s, de leeuw et al. proposed the use of
low rank models to    t data measured in nominal, ordinal and cardinal
levels [37]. more recently, goldberg et al. [52] used a low rank model
to perform transduction (i.e., multi-label learning) in the presence of
missing data by    tting a low rank model to the features and the labels
simultaneously. low rank models have also been used to embed image,
text and video data into a common low dimensional space [54], and have
recently come into vogue in the natural language processing community
as a means to embed words and documents into a low dimensional
vector space [99, 100, 112, 136].

algorithms.
in general, it can be computationally hard to    nd the
global optimum of a generalized low rank model. for example, it is
np-hard to compute an exact solution to id116 [43], nonnegative
id105 [149], and weighted pca and matrix completion
[50], all of which are special cases of low rank models.

6

introduction

however, there are many (e cient) ways to go about    tting a low
rank model, by which we mean    nding a good model with a small
objective value. the resulting model may or may not be the global
solution of the low rank optimization problem. we distinguish a model
   t in this way from the solution to an optimization problem, which
always refers to the global solution.

the id105 literature presents a wide variety of meth-
ods to    t low rank models in a variety of special cases. for exam-
ple, there are variants on alternating minimization (with alternating
least squares as a special case) [37, 158, 141, 35, 36], alternating new-
ton methods [53, 129], (stochastic or incremental) id119
[75, 88, 104, 119, 10, 159, 118], conjugate gradients [120, 134], expecta-
tion minimization (em) (or    soft-impute   ) methods [142, 134, 98, 60],
multiplicative updates [85], and convex relaxations to semide   nite pro-
grams [135, 46, 117, 48].

generally, expectation minimization, which proceeds by iteratively
imputing missing entries in the matrix and solving the fully observed
problem, has been found to underperform relative to other methods
[129]. however, when used in conjunction with computational tricks
exploiting a particular problem structure, such as gram matrix caching,
these methods can still work extremely well [60].

semide   nite programming becomes computationally intractable for
very large (or even just large) scale problems [120]. however, a theoret-
ical analysis of optimality conditions for rank-constrained semide   nite
programs [20] has led to a few algorithms for semide   nite program-
ming based on id105 [19, 1, 70] which guarantee global
optimality and converge quickly if the global solution to the problem is
exactly low rank. fast approximation algorithms for rank-constrained
semide   nite programs have also been developed [127].

recently, there has been a resurgence of interest in methods based
on alternating minimization, as numerous authors have shown that
alternating minimization (suitably initialized, and under a few technical
assumptions) provably converges to the global minimum for a range of
problems including matrix completion [72, 66, 58], robust pca [103],
and dictionary learning [2].

1.1. previous work

7

id119 methods are often preferred for extremely large
scale problems since these methods parallelize naturally in both shared
memory and distributed memory architectures. see [118, 159] and ref-
erences therein for some recent innovative approaches to speeding up
stochastic id119 for id105 by eliminating lock-
ing and reducing interprocess communication. these stochastic non-
locking methods often run faster than their deterministic counterparts;
and for the matrix completion problem in particular, these methods
can be shown to provably converge to the global minimum under the
same conditions required for alternating minimization [38].

contributions. the present paper di ers from previous work in a
number of ways. we are consistently concerned with the meaning of
applying these di erent id168s and regularizers to approximate
a data set. the generality of our view allows us to introduce a number
of id168s and regularizers that have not previously been con-
sidered. moreover, our perspective enables us to extend these ideas to
arbitrary data sets, rather than just matrices of real numbers.

a number of new considerations emerge when considering the prob-
lem so broadly. first, we must face the problem of comparing approx-
imation errors across data of di erent types. for example, we must
choose a scaling to trade o  the loss due to a misclassi   cation of a
categorical value with an error of 0.1 (say) in predicting a real value.
second, we require algorithms that can handle the full gamut of
losses and regularizers, which may be smooth or nonsmooth,    nite or
in   nite valued, with arbitrary domain. this work is the    rst to consider
these problems in such generality, and therefore also the    rst to wrestle
with the algorithmic consequences. below, we give a number of algo-
rithms appropriate for this setting, including many that have not been
previously proposed in the literature. our algorithms are all based on
alternating minimization and variations on alternating minimization
that are more suitable for large scale data and can take advantage of
parallel computing resources.

these algorithms for    tting any glrm are particularly useful for
interactive data analysis: a practitioner can mix and match di erent

8

introduction

id168s and regularizers, and test which combinations provide the
best    t to the data, without having to identify a di erent method to
   t each particular model. we present a few software packages designed
for this purpose, with interfaces in julia, r, java, python, and scala,
in   9.

finally, we present some new results on some old problems. for
example, in appendix a.1, we derive a formula for the solution to
quadratically regularized pca, and show that quadratically regularized
pca has no local nonglobal minima; and in   7.6 we show how to certify
(in some special cases) that a model is a global solution of a glrm.

1.2 organization
the organization of this paper is as follows. in   2 we    rst recall some
properties of pca and its common variations to familiarize the reader
with our notation. we then generalize the id173 on the low
dimensional factors in   3, and the id168 on the approximation
error in   4. returning to the setting of heterogeneous data, we extend
these id84 techniques to abstract data types in   5
and to multi-dimensional id168s in   6. finally, we address algo-
rithms for    tting glrms in   7, discuss a few practical considerations
in choosing a glrm for a particular problem in   8, and describe some
implementations of the algorithms that we have developed in   9.

2

pca and quadratically regularized pca

data matrix.
in this section, we let a    rm   n be a data matrix
consisting of m examples each with n numerical features. thus aij    r
is the value of the jth feature in the ith example, the ith row of a is
the vector of n feature values for the ith example, and the jth column
of a is the vector of the jth feature across our set of m examples.

it is common to represent other data types in a numerical matrix
using certain canonical encoding tricks. for example, boolean data is
often encoded as 1 (for true) and -1 (for false), ordinal data is often
encoded using consecutive integers to represent the consecutive levels of
the variable, and categorical data is often encoded by creating a column
for each possible value of the categorical variable, and representing the
data using a 1 in the column corresponding to the observed value, and
-1 or 0 in all other columns. we will see more systematic and principled
ways to deal with these data types, and others, in   4   6. for now, we
assume the entries in the data matrix consist of real numbers.

2.1 pca
principal components analysis (pca) is one of the oldest and most
widely used tools in data analysis [111, 62, 67]. we review some of its

9

10

pca and quadratically regularized pca

well-known properties here in order to set notation and as a warm-up
to the variants presented later.

pca seeks the best rank-k approximation to the matrix a in the

least-squares sense, by solving

minimize
subject to rank(z)    k,

  a     z  2

f

(2.1)

with variable z    rm   n. here,        f is the frobenius norm of a matrix,
i.e., the square root of the sum of the squares of the entries.
the rank constraint can be encoded implicitly by expressing z in
factored form as z = xy , with x    rm   k, y    rk   n. then the pca
problem can be expressed as

minimize   a     xy   2

f

(2.2)

with variables x    rm   k and y    rk   n. (the factorization of z is of
course not unique.)
de   ne xi    r1   n to be the ith row of x, and yj    rm to be the jth
column of y . thus xiyj = (xy )ij    r denotes a dot or inner product.
(we will use this notation throughout the paper.) using this de   nition,
we can rewrite the objective in problem (2.2) as

m  i=1

n  j=1

(aij     xiyj)2.

we will give several interpretations of the low rank factorization
(x, y ) solving (2.2) in   2.5. but for now, we note that (2.2) can inter-
preted as a method for compressing the n features in the original data
set to k < n new features. the row vector xi is associated with exam-
ple i; we can think of it as a feature vector for the example using the
compressed set of k < n features. the column vector yj is associated
with the original feature j; it can be interpreted as mapping the k new
features onto the original feature j.

2.2. quadratically regularized pca

11

2.2 quadratically regularized pca

we can add quadratic id173 on x and y to the objective. the
quadratically regularized pca problem is

i=1qn

j=1(aij     xiyj)2 +    qm

minimize qm
j=1   yj  2
2,
(2.3)
with variables x    rm   k and y    rk   n, and id173 parameter
       0. problem (2.3) can be written more concisely in matrix form as
(2.4)

2 +    qn

i=1   xi  2

minimize   a     xy   2

f +      x  2

f +      y   2
f .

when     = 0, the problem reduces to the pca problem (2.2).

2.3 solution methods
singular value decomposition.
it is well known that a solution to
(2.2) can be obtained by truncating the singular value decomposition
(svd) of a [44]. the (compact) svd of a is given by a = u v t ,
where u    rm   r and v    rn   r have orthonormal columns, and  =
diag(   1, . . . ,    r)    rr   r, with    1               r > 0 and r = rank(a).
the columns of u = [u1        ur] and v = [v1        vr] are called the left
and right singular vectors of a, respectively, and    1, . . . ,    r are called
the singular values of a.

using the orthogonal invariance of the frobenius norm, we can

rewrite the objective in problem (2.1) as

  a     xy   2

f =         u t xy v   2
f .

that is, we would like to    nd a matrix u t xy v of rank no more
than k approximating the diagonal matrix  . it is easy to see
that there is no better rank k approximation for   than  k =
diag(   1, . . . ,    k, 0, . . . , 0)    rr   r. here we have truncated the svd
to keep only the top k singular values. we can achieve this approxima-
tion by choosing u t xy v =  k, or (using the orthogonality of u and
v ) xy = u kv t . for example, de   ne

uk = [u1        uk],

vk = [v1        vk],

(2.5)

12

pca and quadratically regularized pca

and let

y =  1/2

x = uk 1/2
k ,

k v t
k .

(2.6)
the solution to (2.3) is clearly not unique: if x, y is a solution, then
so is xg, g   1y for any invertible matrix g    rk   k. when    k >    k+1,
all solutions to the pca problem have this form. in particular, letting
g = ti and taking t      , we see that the solution set of the pca
problem is unbounded.
it is less well known that a solution to the quadratically regularized
pca problem can be obtained in the same way. (proofs for the state-
ments below can be found in appendix a.1.) de   ne uk and vk as above,
and let    k = diag((   1        )+, . . . , (   k        )+), where (a)+ = max(a, 0).
here we have both truncated the svd to keep only the top k singu-
lar values, and performed soft-thresholding on the singular values to
reduce their values by    . a solution to the quadratically regularized
pca problem (2.3) is then given by

y =    1/2

x = uk    1/2
k ,

k v t
k .

(2.7)
for     = 0, the solution reduces to the familiar solution to pca (2.2)
obtained by truncating the svd to the top k singular values.

the set of solutions to problem (2.3) is signi   cantly smaller than
that of problem (2.2), although solutions are still not unique: if x, y is
a solution, then so is xt, t    1y for any orthogonal matrix t    rk   k.
when    k >    k+1, all solutions to (2.3) have this form. in particu-
lar, adding quadratic id173 results in a solution set that is
bounded.

the quadratically regularized pca problem (2.3) (including the
pca problem as a special case) is the only problem we will encounter
for which an analytical solution exists. the analytical tractability of
pca explains its popularity as a technique for data analysis in the era
before computers were machines. for example, in his 1933 paper on
pca [62], hotelling computes the solution to his problem using power
iteration to    nd the eigenvalue decomposition of the matrix at a =
v  2v t , and records in the appendix to his paper the intermediate
results at each of the (three) iterations required for the method to
converge.

2.3. solution methods

13

alternating minimization. here we mention a second method for
solving (2.3), which extends more readily to the extensions of pca
that we discuss below. the alternating minimization algorithm simply
alternates between minimizing the objective over the variable x, hold-
ing y    xed, and then minimizing over y , holding x    xed. with an
initial guess for the factors y 0, we repeat the iteration

x l = argmin

y l = argmin

x qa
m  i=1
y qa
m  i=1

n  j=1
n  j=1

(aij     xiyl   1

j

)2 +    

(aij     xl

iyj)ij)2 +    

2rb
m  i=1   xi  2
2rb
n  j=1  yj  2

for l = 1, . . . until a stopping condition is satis   ed. (if x and y are full
rank, or    > 0, the minimizers above are unique; when they are not,
we can take any minimizer.) the objective function is nonincreasing at
each iteration, and therefore bounded. this implies, for    > 0, that the
iterates x l and y l are bounded.

this algorithm does not always work. in particular, it has stationary
points that are not solutions of problem (2.3). in particular, if the rows
of y l lie in a subspace spanned by a subset of the (right) singular
vectors of a, then the columns of x l+1 will lie in a subspace spanned
by the corresponding left singular vectors of a, and vice versa. thus,
if the algorithm is initialized with y 0 orthogonal to any of the top
k (right) singular vectors, then the algorithm (implemented in exact
arithmetic) will not converge to the global solution to the problem.

but all stable stationary points of the iteration are solutions (see
appendix a.1). so as a practical matter, the alternating minimization
method always works, i.e., the objective converges to the optimal value.

parallelizing alternating minimization. alternating minimization
parallelizes easily over examples and features. the problem of mini-
mizing over x splits into m independent minimization problems. we
can solve the simple quadratic problems

minimize qn

j=1(aij     xiyj)2 +      xi  2

2

(2.8)

14

pca and quadratically regularized pca

with variable xi, in parallel, for i = 1, . . . , m. similarly, the problem of
minimizing over y splits into n independent quadratic problems,

minimize qm

i=1(aij     xiyj)2 +      yj  2

2

with variable yj, which can be solved in parallel for j = 1, . . . , n.

(2.9)

caching factorizations. we can speed up the solution of the
quadratic problems using a simple factorization caching technique.

for ease of exposition, we assume here that x and y have full rank

k. the updates (2.8) and (2.9) can be expressed as

x = ay t (y y t +    i)   1,

y = (x t x +    i)   1x t a.

we show below how to e ciently compute x = ay t (y y t +   i)   1; the
y update admits a similar speedup using the same ideas. we assume
here that k is modest, say, not more than a few hundred or a few
thousand. (typical values used in applications are often far smaller,
on the order of tens.) the dimensions m and n, however, can be very
large.

first compute the gram matrix g = y y t using an outer product

expansion

g =

yjyt
j .

n  j=1

this sum can be computed on-line by streaming over the index j, or in
parallel, split over the index j. this property allows us to scale up to
extremely large problems even if we cannot store the entire matrix y in
memory. the computation of the gram matrix requires 2k2n    oating
point operations (   ops), but is trivially parallelizable: with r workers,
we can expect a speedup on the order of r. we next add the diagonal
matrix    i to g in k    ops, and form the cholesky factorization of g+   i
in k3/3    ops and cache the factorization.

in parallel over the rows of a, we compute d = ay t (2kn    ops
per row), and use the factorization of g +    i to compute d(g +    i)   1
with two triangular solves (2k2    ops per row). these computations are
also trivially parallelizable: with r workers, we can expect a speedup
on the order of r.

2.4. missing data and matrix completion

15

hence the total time required for each update with r workers scales
). for k small compared to m and n, the time is

as o( k2(m+n)+kmn
dominated by the computation of ay t .

r

2.4 missing data and matrix completion
suppose we observe only entries aij for (i, j)        { 1, . . . , m}   
{1, . . . , n} from the matrix a, so the other entries are unknown. then
to    nd a low rank matrix that    ts the data well, we solve the problem
(2.10)
with variables x and y , with    > 0. a solution of this problem gives
an estimate   aij = xiyj for the value of those entries (i, j)         that were
not observed. in some applications, this data imputation (i.e., guessing
entries of a matrix that are not known) is the main point.

minimize q(i,j)   (aij     xiyj)2 +      x  2

f +      y   2
f ,

there are two very di erent regimes in which solving the prob-

lem (2.10) may be useful.

imputing missing entries to borrow strength. consider a matrix a
in which very few entries are missing. the typical approach in data
analysis is to simply remove any rows with missing entries from the
matrix and exclude them from subsequent analysis. if instead we solve
the problem above without removing these a ected rows, we    borrow
strength    from the entries that are not missing to improve our global
understanding of the data matrix a. in this regime we are imputing
the (few) missing entries of a, using the examples that ordinarily we
would discard.

low rank matrix completion. now consider a matrix a in which most
entries are missing, i.e., we only observe relatively few of the mn ele-
ments of a, so that by discarding every example with a missing feature
or every feature with a missing example, we would discard the entire
matrix. then the solution to (2.10) becomes even more interesting: we
are guessing all the entries of a (presumed low rank) matrix, given just
a few of them. it is a surprising fact that this is possible: typical re-
sults from the matrix completion literature show that one can recover

16

pca and quadratically regularized pca

an unknown m     n matrix a of low rank r from just about nr log2 n
noisy samples   with an error that is proportional to the noise level
[23, 24, 117, 22], so long as the matrix a satis   es a certain incoherence
condition and the samples   are chosen uniformly at random. these
works use an estimator that minimizes a nuclear norm penalty along
with a data    tting term to encourage low rank structure in the solution.
the argument in   7.6 shows that problem (2.10) is equivalent to

the rank-constrained nuclear-norm regularized convex problem

minimize q(i,j)   (aij     zij)2 + 2     z    
subject to rank(z)    k,

where the nuclear norm   z     (also known as the trace norm) is de-
   ned to be the sum of the singular values of z. thus, the solutions to
problem (2.10) correspond exactly to the solutions of these proposed
estimators so long as the rank k of the model is chosen to be larger
than the true rank r of the matrix a. nuclear norm id173 is
often used to encourage solutions of rank less than k, and has appli-
cations ranging from graph embedding to linear system identi   cation
[46, 92, 102, 130, 107].

low rank matrix completion problems arise in applications like pre-
dicting customer ratings or customer (potential) purchases. here the
matrix consists of the ratings or numbers of purchases that m cus-
tomers give (or make) for each of n products. the vast majority of the
entries in this matrix are missing, since a customer will rate (or pur-
chase) only a small fraction of the total number of products available.
in this application, imputing a missing entry of the matrix as xiyj, for
(i, j)        , is guessing what rating a customer would give a product, if
she were to rate it. this can used as the basis for a recommendation
system, or a marketing plan.

alternating minimization. when      = {1, . . . , m}   { 1, . . . , n}, the
problem (2.10) has no known analytical solution, but it is still easy
to    t a model using alternating minimization. algorithms based on
alternating minimization have been shown to converge quickly (even
geometrically [66]) to a global solution satisfying a recovery guarantee

2.5.

interpretations and applications

17

when the initial values of x and y are chosen carefully [74, 76, 73, 66,
58, 55, 59, 140].

however, none of these results applies to the algorithm     alternat-
ing minimization on problem (2.10)     most often used in practice. for
example, none uses the quadratic regularizer above that corresponds to
the nuclear norm penalized estimator; and all of these analytical results,
until the recent paper [140], rely on using a fresh batch of samples  
for each iteration of alternating minimization. interestingly, hardt [58]
notes that none of these alternating methods achieves the same sample
complexity guarantees found in the convex matrix completion litera-
ture which, unlike the alternating minimization guarantees, match the
information theoretic lower bound [24] up to logarithmic factors. we
expect that these shortcomings     weaker error bounds for more com-
plex algorithms     are an artifact of current proof techniques, rather
than a fundamental limitation of alternating approaches. but for now,
alternating minimization applied to problem (2.10) should still be con-
sidered a (very good) heuristic optimization method.

2.5

interpretations and applications

the recovered matrices x and y in the quadratically regularized pca
problems (2.3) and (2.10) admit a number of interesting interpretations.
we introduce some of these interpretations now; the terminology we use
here will recur throughout the paper. of course these interpretations
are related to each other, and not distinct.

feature compression. quadratically regularized pca (2.3) can be
interpreted as a method for compressing the n features in the original
data set to k < n new features. the row vector xi is associated with
example i; we can think of it as a feature vector for the example using
the compressed set of k < n features. the column vector yj is associated
with the original feature j; it can be interpreted as the mapping from
the original feature j into the k new features.

18

pca and quadratically regularized pca

low-dimensional geometric embedding. we can think of each yj as
associating feature j with a point in a low (k-) dimensional space. sim-
ilarly, each xi associates example i with a point in the low dimensional
space. we can use these low dimensional vectors to judge which fea-
tures (or examples) are similar. for example, we can run a id91
algorithm on the low dimensional vectors yj (or xi) to    nd groups of
similar features (or examples).

archetypes. we can think of each row of y as an archetype which
captures the behavior of one of k idealized and maximally informative
examples. these archetypes might also be called pro   les, factors, or
atoms. every example i = 1, . . . , m is then represented (approximately)
as a linear combination of these archetypes, with the row vector xi giv-
ing the coe cients. the coe cient xil gives the resemblance or loading
of example i to the lth archetype.

archetypical representations. we call xi the representation of exam-
ple i in terms of the archetypes. the rows of x give an embedding
of the examples into rk, where each coordinate axis corresponds to a
di erent archetype. if the archetypes are simple to understand or inter-
pret, then the representation of an example can provide better intuition
about that example.

the examples can be clustered according to their representations
in order to determine a group of similar examples. indeed, one might
choose to apply any machine learning algorithm to the representations
xi rather than to the initial data matrix: in contrast to the initial data,
which may consist of high dimensional vectors with noisy or missing
entries, the representations xi will be low dimensional, less noisy, and
complete.

feature representations. the columns of y embed the features into
rk. here, we think of the columns of x as archetypical features, and
represent each feature j as a linear combination of the archetypical
features. just as with the examples, we might choose to apply any

2.5.

interpretations and applications

19

machine learning algorithm to the feature representations. for exam-
ple, we might    nd clusters of similar features that represent redundant
measurements.

latent variables. each row of x represents an example by a vector in
rk. the matrix y maps these representations back into rm. we might
think of x as discovering the latent variables that best explain the

observed data. if the approximation errorq(i,j)   (aij   xiyj)2 is small,

then we view these latent variables as providing a good explanation or
summary of the full data set.

probabilistic interpretation. we can give a probabilistic interpreta-
tion of x and y , building on the probabilistic model of pca developed
by tipping and bishop [142]. we suppose that the matrices   x and   y
have entries which are generated by taking independent samples from
a normal distribution with mean 0 and variance       1 for    > 0. the
entries in the matrix   x   y are observed with noise   ij    r,

aij = (   x   y )ij +   ij,

where the noise    in the (i, j)th entry is sampled independently from a
standard normal distribution. we observe each entry (i, j)     . then
to    nd the maximum a posteriori (map) estimator (x, y ) of (   x,   y ),
we solve

maximize

   

   

2     x  2

f4 exp3   

exp3   
f4
2     y   2
   r(i,j)    exp!   (aij     xiyj)2" ,

which is equivalent, by taking logs, to (2.3).

this interpretation explains the recommendation we gave above
for imputing missing observations (i, j)        . we simply use the map
estimator xiyj to estimate the missing entry (   x   y )ij. similarly, we can
interpret (xy )ij for (i, j)      as a denoised version of the observation
aij.

20

pca and quadratically regularized pca

auto-encoder. the matrix x encodes the data; the matrix y decodes
it back into the full space. we can view pca as providing the best lin-
ear auto-encoder for the data; among all (bi-linear) low rank encodings
(x) and decodings (y ) of the data, pca minimizes the squared recon-
struction error.

compression. we impose an information bottleneck [143] on the data
by using a low rank auto-encoder to    t the data. pca    nds x and y
to maximize the information transmitted through this k-dimensional
information bottleneck. we can interpret the solution as a compressed
representation of the data, and use it to e ciently store or transmit
the information present in the original data.

2.6 o sets and scaling
for good practical performance of a generalized low rank model, it is
critical to ensure that model assumptions match the data. we saw
above in   2.5 that quadratically regularized pca corresponds to a
model in which features are observed with n(0, 1) errors. if instead
each column j of xy is observed with n(  j,    2
j ) errors, our model is
no longer unbiased, and may    t very poorly, particularly if some of the
column means   j are large.

for this reason it is standard practice to standardize the data before
applying pca or quadratically regularized pca: the column means are
subtracted from each column, and the columns are normalized by their
variances. (this can be done approximately; there is no need to get the
scaling and o set exactly right.) formally, de   ne nj = |{i : (i, j)     }|,
and let

  j = 1

nj   i: (i,j)   

aij,   

2

j = 1

nj     1   i: (i,j)   

(aij       j)2

estimate the mean and variance of each column of the data matrix.
pca or quadratically regularized pca is then applied to the matrix
whose (i, j) entry is (aij       j)/   j.

3

generalized id173

it is easy to see how to extend pca to allow arbitrary id173 on
the rows of x and columns of y . we form the regularized pca problem

j=1   rj(yj),

i=1 ri(xi) +qn

minimize q(i,j)   (aij     xiyj)2 +qm
(3.1)
with variables xi and yj, with given regularizers ri : rk    r   {  } and
  rj : rk    r   {  } for i = 1, . . . , n and j = 1, . . . , m. regularized pca
(3.1) reduces to quadratically regularized pca (2.3) when ri =           2
2,
  rj =           2
the objective in problem (3.1) can be expressed compactly in ma-
trix notation as

2. we do not restrict the regularizers to be convex.

  a     xy   2

f + r(x) +   r(y ),

where r(x) =qn

j=1   rj(yj). the id173
functions r and   r are separable across the rows of x, and the columns
of y , respectively.

i=1 ri(xi) and   r(y ) =qn

in   nite values of ri and   rj are used to enforce constraints on the

values of x and y . for example, the regularizer

ri(x) =i 0

x    0

   otherwise,
21

22

generalized id173

the indicator function of the nonnegative orthant, imposes the con-
straint that xi be nonnegative.

solutions to (3.1) need not be unique, depending on the choice of
regularizers. if x and y are a solution, then so are xt and t    1y ,
where t is any nonsingular matrix that satis   es r(u t) = r(u) for all
u and   r(t    1v ) = r(v ) for all v .

by varying our choice of regularizers r and   r, we are able to rep-
resent a wide range of known models, as well as many new ones. we
will discuss a number of choices for regularizers below, but turn now
to methods for solving the regularized pca problem (3.1).

3.1 solution methods
in general, there is no analytical solution for (3.1). the problem is not
convex, even when r and   r are convex. however, when r and   r are
convex, the problem is bi-convex: it is convex in x when y is    xed,
and convex in y when x is    xed.

alternating minimization. there is no reason to believe that alter-
nating minimization will always converge to the global minimum of the
regularized pca problem (3.1). indeed, we will see many cases below
in which the problem is known to have many local minima. however,
alternating minimization can still be applied in this setting, and it still
parallelizes over the rows of x and columns of y . to minimize over x,
we solve, in parallel,

minimize qj:(i,j)   (aij     xiyj)2 + ri(xi)

(3.2)
with variable xi, for i = 1, . . . , m. similarly, to minimize over y , we
solve, in parallel,

minimize qi:(i,j)   (aij     xiyj)2 +   rj(yj)

with variable yj, for j = 1, . . . , n.

when the regularizers are convex, these problems are convex. when
the regularizers are not convex, there are still many cases in which we
can    nd analytical solutions to the nonconvex subproblems (3.2) and

(3.3)

3.2. examples

23

(3.3), as we will see below. a number of concrete algorithms, in which
these subproblems are solved explicitly, are given in   7.

caching factorizations. often, the x and y updates (3.2) and (3.3)
reduce to convex quadratic programs. for example, this is the case for
nonnegative id105, sparse pca, and quadratic mixtures
(which we de   ne and discuss below in   3.2). the same factorization
caching of the gram matrix that was described above in the case of
pca can be used here to speed up the solution of these updates. vari-
ations on this idea are described in detail in   7.3.

3.2 examples

here and throughout the paper, we present a set of examples chosen for
pedagogical clarity, not for completeness. in all of the examples below,
   > 0 is a parameter that controls the strength of the id173,
and we drop the subscripts from r (or   r) to lighten the notation. of
course, it is possible to mix and match these regularizers, i.e., to choose
di erent ri for di erent i, and choose di erent   rj for di erent j.

nonnegative id105 (nnmf). consider the regular-
ized pca problem (3.1) with r = i+ and   r = i+, where i+ is the
indicator function of the nonnegative reals. (here, and throughout the
paper, we de   ne the indicator function of a set c, to be 0 when its
argument is in c and    otherwise.) then problem (3.1) is nnmf:
a solution gives the matrix best approximating a that has a nonneg-
ative factorization (i.e., a factorization into elementwise nonnegative
matrices) [85]. it is np-hard to solve nnmf problems exactly [149].
however, these problems have a rich analytical structure which can
sometimes be exploited [49, 10, 31], and a wide range of uses in practice
[85, 126, 7, 151, 77, 47]. hence a number of specialized algorithms and
codes for    tting nnmf models are available [86, 91, 78, 80, 13, 79, 81].
we can also replace the nonnegativity constraint with any interval
constraint. for example, r and   r can be 0 if all entries of x and y ,
respectively, are between 0 and 1, and in   nite otherwise.

24

generalized id173

sparse pca.
if very few of the coe cients of x and y are nonzero, it
can be easier to interpret the archetypes and representations. we can
understand each archetype using only a small number of features, and
can understand each example as a combination of only a small number
of archetypes. to get a sparse version of pca, we use a sparsifying
penalty as the id173. many variants on this basic idea have
been proposed, together with a wide variety of algorithms [33, 161,
128, 94, 155, 122, 152].

for example, we could enforce that no entry aij depend on more
than s columns of x or of y by setting r to be the indicator function
of a s-sparse vector, i.e.,

r(x) =i 0

card(x)    s
   otherwise,

and de   ning   r(y) similarly, where card(x) denotes the cardinality
(number of nonzero entries) in the vector x. the updates (3.2) and
(3.3) are not convex using this regularizer, but one can    nd approxi-
mate solutions using a pursuit algorithm (see, e.g., [28, 145]), or exact
solutions (for small s) using the branch and bound method [84, 15].

as a simple example, consider s = 1. here we insist that each xi
have at most one nonzero entry, which means that each example is a
multiple of one of the rows of y . the x-update is easy to carry out,
by evaluating the best quadratic    t of xi with each of the k rows of y .
this reduces to choosing the row of y that has the smallest angle to
the ith row of a.

the s-sparse id173 can be relaxed to a convex, but still
sparsifying, id173 using r(x) =   x  1,   r(y) =   y  1 [161]. in this
case, the x-update reduces to solving a (small)   1-regularized least-
squares problem.

orthogonal nonnegative id105. one well known prop-
erty of pca is that the principal components obtained (i.e., the
columns of x and rows of y ) can be chosen to be orthogonal, so x t x
and y y t are both diagonal. we can impose the same condition on a
nonnegative id105. due to nonnegativity of the matrix,
two columns of x cannot be orthogonal if they both have a nonzero in

3.2. examples

25

the same row. conversely, if x has only one nonzero per row, then its
columns are mutually orthogonal. so an orthogonal nonnegative ma-
trix factorization is identical to a nonnegativity condition in addition to
the 1-sparse condition described above. orthogonal nonnegative matrix
factorization can be achieved by using the regularizer
x    0

card(x) = 1,

r(x) =i 0

   otherwise,

and letting   r(y) be the indicator of the nonnegative orthant, as in
nnmf.

geometrically, we can interpret this problem as modeling the data
a as a union of rays. each row of y , interpreted as a point in rn,
de   nes a ray from the origin passing through that point. orthogonal
nonnegative id105 models each row of x as a point along
one of these rays.

some authors [41] have also considered how to obtain a bi-
orthogonal nonnegative id105, in which both x and y t
have orthogonal columns. by the same argument as above, we see this
is equivalent to requiring both x and y t to have only one positive
entry per row, with the other entries equal to 0.

max-norm id105. we take r =   r =     with

   (x) =i 0

  x  2

2      
   otherwise.

this penalty enforces that
  x  2

2,        ,

  y t  2

2,        ,

is de   ned as
where the (2,  ) norm of a matrix x with rows xi
maxi   xi  2. this is equivalent to requiring the max-norm (sometimes
called the    2-norm) of z = xy , which is de   ned as

  z  max = inf{  x  2,    y t  2,   : xy = z},

to be bounded by   . this penalty has been proposed by [88] as a heuris-
tic for low rank matrix completion, which can perform better than
frobenius norm id173 when the low rank factors are known to
have bounded entries.

26

generalized id173

quadratic id91. consider (3.1) with   r = 0. let r be the indica-
tor function of a selection, i.e.,

r(x) =i 0

x = el for some l   { 1, . . . , k}

   otherwise,

where el is the lth standard basis vector. thus xi encodes the cluster
(one of k) to which the data vector (ai1, . . . , aim) is assigned.

alternating minimization on this problem reproduces the well-
known id116 algorithm (also known as lloyd   s algorithm) [93]. the
y update (3.3) is a least squares problem with the simple solution

ylj = qi:(i,j)    aijxil
qi:(i,j)    xil

,

i.e., each row of y is updated to be the mean of the rows of a assigned
to that archetype. the x update (3.2) is not a convex problem, but
is easily solved. the solution is given by assigning xi to the closest
archetype (often called a cluster centroid in the context of id116):

xi = el   for l   = argminl1qn

j=1(aij     ylj)22.

quadratic mixtures. we can also implement partial assignment of
data vectors to clusters. take   r = 0, and let r be the indicator function
of the set of id203 vectors, i.e.,

r(x) =i 0 qk

l=1 xl = 1,
   otherwise.

xl    0

subspace id91. pca approximates a data set by a single low di-
mensional subspace. we may also be interested in approximating a data
set as a union of low dimensional subspaces. this problem is known as
subspace id91 (see [150] and references therein). subspace clus-
tering may also be thought of as generalizing quadratic id91 to
assign each data vector to a low dimensional subspace rather than to
a single cluster centroid.

to frame subspace id91 as a regularized pca problem (3.1),
partition the columns of x into k blocks. then let r be the indicator

3.2. examples

27

function of block sparsity (i.e., r(x) = 0 if only one block of x has
nonzero entries, and otherwise r(x) =   ).
it is easy to perform alternating minimization on this objective
function. this method is sometimes called the k-planes algorithm [150,
146, 3], which alternates over assigning examples to subspaces, and
   tting the subspaces to the examples. once again, the x update (3.2)
is not a convex problem, but can be easily solved. each block of the
columns of x de   nes a subspace spanned by the corresponding rows of
y . we compute the distance from example i (the ith row of a) to each
subspace (by solving a least squares problem), and assign example i to
the subspace that minimizes the least squares error by setting xi to be
the solution to the corresponding least squares problem.

many other algorithms for this problem have also been proposed,
such as the k-svd [144, 4] and sparse subspace id91 [45], some
with provable guarantees on the quality of the recovered solution [131].

supervised learning. sometimes we want to understand the variation
that a certain set of features can explain, and the variance that remains
unexplainable. to this end, one natural strategy would be to regress
the labels in the dataset on the features; to subtract the predicted
values from the data; and to use pca to understand the remaining
variance. this procedure gives the same answer as the solution to a
single regularized pca problem. here we present the case in which the
features we wish to use in the regression are present in the data as the
   rst column of a. to construct the regularizers, we make sure the    rst
column of a appears as a feature in the supervised learning problem
by setting

ri(x) =i r0(x2, . . . , xk+1) x1 = ai1

otherwise,

  

where r0 = 0 can be chosen as in any regularized pca model. the
id173 on the    rst row of y is the id173 used in the
supervised regression, and the id173 on the other rows will be
that used in regularized pca.

thus we see that regularized pca can naturally combine supervised

and unsupervised learning into a single problem.

28

generalized id173

feature selection. we can use regularized pca to perform feature
selection. consider (3.1) with r(x) =   x  2
2 and   r(y) =   y  2. (notice
that we are not using   y  2
2.) the regularizer   r encourages the matrix
  y to be column-sparse, so many columns are all zero. if   yj = 0, it
means that feature j was uninformative, in the sense that its values
do not help much in predicting any feature in the matrix a (including
feature j itself). in this case we say that feature j was not selected.
for this approach to make sense, it is important that the columns of
the matrix a should have mean zero. alternatively, one can use the de-
biasing regularizers r   and   r   introduced in   3.3 along with the feature
selection regularizer introduced here.

dictionary learning. dictionary learning (also sometimes called
sparse coding) has become a popular method to design concise rep-
resentations for very high dimensional data [106, 87, 95, 96]. these
representations have been shown to perform well when used as features
in subsequent (supervised) machine learning tasks [116]. in dictionary
learning, each row of a is modeled as a linear combination of dictionary
atoms, represented by rows of y . the total size of the dictionary used
is often very large (k     max(m, n)), but each example is represented
using a very small number of atoms. to    t the model, one solves the
regularized pca problem (3.1) with r(x) =   x  1, to induce sparsity in
the number of atoms used to represent any given example, and with
  r(y) =   y  2
2 or   r(y) = i+(c       y  2) for some c > 0    r, in order to
ensure the problem is well posed. (note that our notation transposes
the usual notation in the literature on dictionary learning.)

mix and match.
it is possible to combine these regularizers to obtain
a factorization with any combination of the above properties. as an
example, one may require that both x and y be simultaneously sparse
and nonnegative by choosing

r(x) =   x  1 + i+(x) = 1t x + i+(x),

and similarly for   r(y). similarly, [77] show how to obtain a nonnegative
id105 in which one factor is sparse by using r(x) =   x  2
1+

3.3. o sets and scaling

29

i+(x) and   r(y) =   y  2
a id91 technique.

2 + i+(y); they go on to use this factorization as

3.3 o sets and scaling
in our discussion of the quadratically regularized pca problem (2.3),
we saw that it can often be quite important to standardize the data
before applying pca. conversely, in regularized pca problems such
as nonnegative id105, it makes no sense to standardize
the data, since subtracting column means introduces negative entries
into the matrix.

a    exible approach is to allow an o set in the model: we solve
j=1   rj(yj),

minimize q(i,j)   (aij     xiyj       j)2 +qm

(3.4)
with variables xi, yj, and   j. here,   j takes the role of the column
mean, and in fact will be equal to the column mean in the trivial case
k = 0.

i=1 ri(xi) +qn

an o set may be included in the standard form regularized pca
problem (3.1) by augmenting the problem slightly. suppose we are given
an instance of the problem (3.1), i.e., we are given k, r, and   r. we
can    t an o set term   j by letting k   = k + 1 and modifying the
regularizers. extend the id173 r : rk    r and   r : rk    r to
new regularizers r   : rk+1    r and   r   : rk+1    r which enforce that
the    rst column of x is constant and the    rst row of y is not penalized.
using this scheme, the    rst row of the optimal y will be equal to the
optimal    in (3.4).
explicitly, let

r  (x) =i r(x2, . . . , xk+1) x1 = 1

otherwise,

  

and   r  (y) =   r(y2, . . . , yk+1). (here, we identify r(x) = r(x1, . . . , xk) to
explicitly show the dependence on each coordinate of the vector x, and
similarly for   r.)

it is also possible to introduce row o sets in the same way.

4

generalized id168s

we may also generalize the id168 in pca to form a generalized
low rank model,

minimize q(i,j)    lij(xiyj, aij) +qm

(4.1)
where lij : r     r    r+ are given id168s for i = 1, . . . , m and
j = 1, . . . , n. problem (4.1) reduces to pca with generalized regular-
ization when lij(u, a) = (a     u)2. however, the id168 lij can
now depend on the data aij in a more complex way.

i=1 ri(xi) +qn

j=1   rj(yj),

4.1 solution methods

as before, problem (4.1) is not convex, even when lij, ri and   rj are
convex; but if all these functions are convex, then the problem is bi-
convex.

alternating minimization. alternating minimization can still be used
to    nd a local minimum, and it is still often possible to use factoriza-
tion caching to speed up the solution of the subproblems that arise in

30

4.2. examples

31

alternating minimization. we defer a discussion of how to solve these
subproblems explicitly to   7.

stochastic proximal gradient method. for use with extremely large
scale problems, we discuss fast variants of the basic alternating mini-
mization algorithm in   7. for example, we present an alternating di-
rections stochastic proximal gradient method. this algorithm accesses
the functions lij, ri, and   rj only through a subgradient or proximal
interface, allowing it to generalize trivially to nearly any id168
and regularizer. we defer a more detailed discussion of this method to
  7.

4.2 examples
weighted pca. a simple modi   cation of the pca objective is to
weight the importance of    tting each element in the matrix a. in the
generalized low rank model, we let lij(u   a) = wij(a   u)2, where wij is
a weight, and take r =   r = 0. unlike pca, the weighted pca problem
has no known analytical solution [134]. in fact, it is np-hard to    nd an
exact solution to weighted pca [50], although it is not known whether
it is always possible to    nd approximate solutions of moderate accuracy
e ciently.

robust pca. despite its widespread use, pca is very sensitive to
outliers. many authors have proposed a robust version of pca obtained
by replacing least-squares loss with   1 loss, which is less sensitive to
large outliers [21, 156, 157]. they propose to solve the problem

minimize
  s  1 +   z    
subject to s + z = a.

(4.2)

the authors interpret z as a robust version of the principal compo-
nents of the data matrix a, and s as the sparse, possibly large noise
corrupting the observations.

lij(u, a) = |a   u|, and r(x) =    

we can frame robust pca as a glrm in the following way. if
2, then (4.1) becomes
2  y   2
f .

minimize   a     xy   1 +    

2,   r(y) =    
2  x  2

2  y  2
f +    

2  x  2

32

generalized id168s

using the arguments in   7.6, we can rewrite the problem by introducing
a new variable z = xy as

minimize
subject to rank(z)    k.

  a     z  1 +      z    

this results in a rank-constrained version of the estimator proposed in
the literature on robust pca [156, 21, 157]:

minimize
subject to s + z = a

  s  1 +      z    
rank(z)    k,

where we have introduced the new variable s = a     z.
huber pca. the huber function is de   ned as

using huber loss,

huber(x) =i (1/2)x2
|x|    (1/2)

|x|   1
|x| > 1.

l(u, a) = huber(u     a),

in place of   1 loss also yields an estimator robust to occasionally large
outliers [65]. the huber function is less sensitive to small errors |u    a|
than the   1 norm, but becomes linear in the error for large errors.
this choice of id168 results in a generalized low rank model
formulation that is robust both to large outliers and to small gaussian
perturbations in the data.

previously, the problem of gaussian noise in robust pca has been
treated by decomposing the matrix a = l + s + n into a low rank
matrix l, a sparse matrix s, and a matrix with small gaussian entries
n by minimizing the loss

  l     +   s  1 + (1/2)  n  2

f

over all decompositions a = l + s + n of a [157].

in fact, this formulation is equivalent to huber pca with quadratic
id173 on the factors x and y . the argument showing this is
very similar to the one we made above for robust pca. the only added

4.2. examples

33

ingredient is the observation that

huber(x) = inf{|s| + (1/2)n2 : x = n + s}.

in other words, the huber function is the in   mal convolution of the
negative log likelihood of a gaussian random variable and a laplacian
random variable: it represents the most likely assignment of (additive)
blame for the error x to a gaussian error n and a laplacian error s.

robust regularized pca. we can design robust versions of all the reg-
ularized pca problems above by the same transformation we used to
design robust pca. simply replace the quadratic id168 with an
  1 or huber id168. for example, k-mediods [71, 110] is obtained
by using   1 loss in place of quadratic loss in the quadratic id91
problem. similarly, robust subspace id91 [132] can be obtained by
using an   1 or huber penalty in the subspace id91 problem.

quantile pca. for some applications, it can be much worse to over-
estimate the entries of a than to underestimate them, or vice versa.
one can capture this asymmetry by using the id168

l(u, a) =    (a     u)+ + (1        )(u     a)+

and choosing        (0, 1) appropriately. this id168 is sometimes
called a scalene loss, and can be interpreted as performing quantile
regression, e.g.,    tting the 20th percentile [83, 82].

fractional pca. for other applications, we may be interested in    nd-
ing an approximation of the matrix a whose entries are close to the
original matrix on a relative, rather than an absolute, scale. here, we
assume the entries aij are all positive. the id168

l(u, a) = max3 a     u

u

,

a 4
u     a

can capture this objective. a model (x, y ) with objective value less
than 0.10mn gives a low rank matrix xy that is on average within
10% of the original matrix.

34

generalized id168s

logarithmic pca. logarithmic id168s may also be useful for
   nding an approximation of a that is close on a relative, rather than
absolute, scale. once again, we assume all entries of a are positive.
de   ne the logarithmic loss

l(u, a) = log2(u/a).

this loss is not convex, but has the nice property that it    ts the geo-
metric mean of the data:
argmin

ai)1/n.

u   i

l(u, ai) = (  i

to see this, note that we are solving a least squares problem in log
space. at the solution, log(u) will be the mean of log(ai), i.e.,

log(u) = 1/n  i

log(ai) = loga(  i

ai)1/nb .

exponential family pca.
it is easy to formulate a version of pca
corresponding to any loss in the exponential family. here we give some
interesting id168s generated by exponential families when all the
entries aij are positive. (see [29] for a general treatment of exponential
family pca.) one popular id168 in the exponential family is the
kl-divergence loss,

l(u, a) = a log3 a

u4     a + u,

which corresponds to a poisson generative model [29].

another interesting id168 is the itakura-saito (is) loss,

l(u, a) = log3 a

u4     1 + a

u

,

which has the property that it is scale invariant, so scaling a and u by
the same factor produces the same loss [139]. the is loss corresponds to
tweedie distributions (i.e., distributions for which the variance is some
power of the mean) [147]. this makes it interesting in applications, such
as audio processing, where fractional errors in recovery are perceived.

35

4.3. o sets and scaling

the    -divergence,

l(u, a) =

a   

   (        1) + u   
       

au      1
        1 ,

generalizes both of these losses. with     = 2, we recover quadratic loss;
in the limit as        1, we recover the kl-divergence loss; and in the
limit as        0, we recover the is loss [139].
4.3 o sets and scaling

in   2.6, we saw how to use standardization to rescale the data in order
to compensate for unequal scaling in di erent features. in general, stan-
dardization destroys sparsity in the data by subtracting the (column)
means (which are in general non-zero) from each element of the data
matrix a. it is possible to instead rescale the id168s in order to
compensate for unequal scaling. scaling the id168s instead has
the advantage that no arithmetic is performed directly on the data a,
so sparsity in a is preserved.

a savvy user may be able to select id168s lij that are scaled
to re   ect the importance of    tting di erent columns. however, it is
useful to have a default automatic scaling for times when no savvy
user can be found. the scaling proposed here generalizes the idea of
standardization to a setting with heterogeneous id168s.

given initial id168s lij, which we assume are nonnegative,

for each feature j let

  j = argmin

     i:(i,j)   

lij(  , aij),   

2

j = 1

nj     1   i:(i,j)   

lij(  j, aij).

it is easy to see that   j generalizes the mean of column j, while    2
generalizes the column variance. for example, when lij(u, a) = (u   a)2
for every i = 1, . . . , m, j = 1, . . . , n,   j is the mean and    2
j is the sample
variance of the jth column of a. when lij(u, a) = |u     a| for every
i = 1, . . . , m, j = 1, . . . , n,   j is the median of the jth column of a,
and    2
j is the sum of the absolute values of the deviations of the entries
of the jth column from the median value.

j

36

generalized id168s

to    t a standardized glrm, we rescale the id168s by    2
j

and solve

minimize q(i,j)    lij(aij, xiyj +  j)/   2

j=1   rj(yj).
(4.3)
note that this problem can be recast in the standard form for a gener-
alized low rank model (4.1). for the o set, we may use the same trick
described in   3.3 to encode the o set in the id173; and for the
scaling, we simply replace the original id168 lij by lij/   2
j .

i=1 ri(xi)+qn

j +qm

5

id168s for abstract data types

we began our study of generalized low rank modeling by considering
the best way to approximate a matrix by another matrix of lower rank.
in this section, we apply the same procedure to approximate a data
table that may not consist of real numbers, by choosing a id168
that respects the data type.

we now consider a to be a table consisting of m examples (i.e.,
rows, samples) and n features (i.e., columns, attributes), with each
entry aij drawn from a feature set fj. the feature set fj may be dis-
crete or continuous. so far, we have only considered numerical data
(fj = r for j = 1, . . . , n), but now fj can represent more ab-
stract data types. for example, entries of a can take on boolean val-
ues (fj = {t, f}), integral values (fj = 1, 2, 3, . . .), ordinal values
(fj = {very much, a little, not at all}), or consist of a tuple of these
types (fj = {(a, b) : a    r}).
we are given a id168 lij : r    f j    r. the loss lij(u, a)
describes the approximation error incurred when we represent a feature
value a   f j by the number u    r. we give a number of examples of
these id168s below.

37

id168s for abstract data types

we now formulate a generalized low rank model on the database a

38

as

minimize q(i,j)    lij(xiyj, aij) +qm
j=1   rj(yj), (5.1)
with variables x    rn   k and y    rk   m, and with loss lij as above
and regularizers ri(xi) : r1   k    r and   rj(yj) : rk   1    r (as be-
fore). when the domain of each id168 is r     r, we recover the
generalized low rank model on a matrix (4.1).

i=1 ri(xi) +qn

5.1 solution methods

as before, this problem is not convex, but it is bi-convex if ri, and
  rj are convex, and lij is convex in its    rst argument. the problem is
also separable across samples i = 1, . . . , m and features j = 1, . . . , m.
these properties makes it easy to perform alternating minimization on
this objective. once again, we defer a discussion of how to solve these
subproblems explicitly to   7.

5.2 examples
boolean pca. suppose aij    {   1, 1}m   n, and we wish to approx-
imate this boolean matrix. for example, we might suppose that the
entries of a are generated as noisy, 1-bit observations from an under-
lying low rank matrix xy . surprisingly, it is possible to accurately
estimate the underlying matrix with only a few observations | | from
the matrix by solving problem (5.1) (under a few mild technical condi-
tions) with an appropriate id168 [34].

we may take the loss to be

l(u, a) = (1     au)+,

which is the hinge loss (see figure 5.1), and solve the problem (5.1) with
or without id173. when the id173 is sum of squares
(r(x) =      x  2
2),    xing x and minimizing over yj is
equivalent to training a support vector machine (id166) on a data set

2,   r(y) =      y  2

5.2. examples

39

4

a = 1

a =    1

+
)
u
2
a
   
1
(

0

   2

0
u

2

)
)
u
a
(
p
x
e
+
1
(
g
o
l

3

2

1

0

a = 1

a =    1

   2

0
u

2

figure 5.1: hinge loss.

figure 5.2: logistic loss.

consisting of m examples with features xi and labels aij. hence alter-
nating minimization for the problem (4.1) reduces to repeatedly train-
ing an id166. this model has been previously considered under the
name maximum margin id105 (mmmf) [135, 120].

logistic pca. again supposing aij    {   1, 1}m   n, we can also use a
logistic loss to measure the approximation quality. let

l(u, a) = log(1 + exp(   au))

(see figure 5.2). with this loss,    xing x and minimizing over yj is
equivalent to using id28 to predict the labels aij. this
model has been previously considered under the name logistic pca
[124].

poisson pca. now suppose the data aij are nonnegative integers. we
can use any id168 that might be used in a regression framework
to predict integral data to construct a generalized low rank model for
poisson pca. for example, we can take

l(u, a) = exp(u)     au + a log a     a.

this is the exponential family loss corresponding to poisson data. (it
di ers from the kl-divergence loss from   4.2 only in that u has been
replaced by exp(u), which allows u to take negative values.)

40

id168s for abstract data types

figure 5.3: ordinal hinge loss.

ordinal pca. suppose the data aij records the levels of some ordinal
variable, encoded as {1, 2, . . . , d}. we wish to penalize the entries of the
low rank matrix xy which deviate by many levels from the encoded
ordinal value. a convex version of this penalty is given by the ordinal
hinge loss,

l(u, a) =

a   1  a  =1

(1     u + a  )+ +

d  a  =a+1

(1 + u     a  )+,

(5.2)

which generalizes the hinge loss to ordinal data (see figure 5.3).

this id168 may be useful for encoding likert-scale data in-
dicating degrees of agreement with a question [90]. for example, we
might have

fj = {strongly disagree, disagree, neither agree nor disagree,

agree, strongly agree}.

we can encode these levels as the integers 1, . . . , 5 and use the above
loss to    t a model to ordinal data.

this approach assumes that every increment of error is equally bad:
for example, that approximating    agree    by    strongly disagree    is just
as bad as approximating    neither agree nor disagree    by    agree   . in
  6.1 we introduce a more    exible ordinal id168 that can learn a
more    exible relationship between ordinal labels. for example, it could
determine that the di erence between    agree    and    strongly disagree   
is smaller than the di erence between    neither agree nor disagree    and
   agree   .

5.3. missing data and data imputation

41

interval pca. suppose that the data aij    r2 are tuples denoting
the endpoints of an interval, and we wish to    nd a low rank matrix
whose entries lie inside these intervals. we can capture this objective
using, for example, the deadzone-linear loss

l(u, a) = max((a1     u)+, (u     a2)+).

5.3 missing data and data imputation
we can use the solution (x, y ) to a low rank model to impute values
corresponding to missing data (i, j)        . this process is sometimes
also called id136. above, we saw that for quadratically regularized
pca, the map estimator for the missing entry aij is equal to xiyj.
this is still true for many of the id168s above, such as the huber
function or   1 loss, for which it makes sense for the data to take on any
real value.

however, to approximate abstract data types we must consider a
more nuanced view. while we can still think of the solution (x, y ) to
the generalized low rank model (4.1) in boolean pca as approximating
the boolean matrix a, the solution is not a boolean matrix. instead we
say that we have encoded the original boolean matrix as a real-valued
low rank matrix xy , or that we have embedded the original boolean
matrix into the space of real-valued matrices.

to    ll in missing entries in the original matrix a, we compute the

value   aij that minimizes the loss for xiyj:

  aij = argmin

lij(xiyj, a).

a

this implicitly constrains   aij to lie in the domain fj of lij. when
lij : r     r    r, as is the case for the losses in   4 above (including
  2,   1, and huber loss), then   aij = xiyj. but when the data is of an
abstract type, the minimum argmina lij(u, a) will not in general be
equal to u.
for example, when the data is boolean, lij : {0, 1}    r    r,
we compute the boolean matrix   a implied by our low rank model by
solving

  aij = argmin
a  {0,1}

(a(xy )ij     1)+

42

for mmmf, or

id168s for abstract data types

log(1 + exp(   a(xy )ij))

  aij = argmin
a  {0,1}

for logistic pca. these problems both have the simple solution

  aij = sign(xiyj).

when fj is    nite, id136 partitions the real numbers into regions

ra = {x    r : lij(u, x) = min

a

lij(u, a)}

corresponding to di erent values a   f j. when lij is convex, these
regions are intervals.
we can use the estimate   aij even when (i, j)      was observed.
if the original observations have been corrupted by noise, we can view
  aij as a denoised version of the original data. this is an unusual kind
of denoising: both the noisy (aij) and denoised (   aij) versions of the
data lie in the abstract space fj.
5.4
we have already discussed some interpretations of x and y in the
pca setting. now we reconsider those interpretations in the context
of approximating these abstract data types.

interpretations and applications

archetypes. as before, we can think of each row of y as an archetype
which captures the behavior of an idealized example. however, the rows
of y are real numbers. to represent each archetype l = 1, . . . , k in the
abstract space as yl with (yl)j   f j, we solve
lj(ylj, a).

(yl)j = argmin
a  fj

(here we assume that the loss lij = lj is independent of the exam-
ple i.)

archetypical representations. as before, we call xi the representation
of example i in terms of the archetypes. the rows of x give an embed-
ding of the examples into rk, where each coordinate axis corresponds

5.4.

interpretations and applications

43

to a di erent archetype. if the archetypes are simple to understand
or interpret, then the representation of an example can provide better
intuition about that example.

in contrast to the initial data, which may consist of arbitrarily
complex data types, the representations xi will be low dimensional
vectors, and can easily be plotted, clustered, or used in nearly any kind
of machine learning algorithm. using the generalized low rank model,
we have converted an abstract feature space into a vector space.

feature representations. the columns of y embed the features into
rk. here we think of the columns of x as archetypical features, and
represent each feature j as a linear combination of the archetypical
features. just as with the examples, we might choose to apply any
machine learning algorithm to the feature representations.

this procedure allows us to compare non-numeric features using
their representation in rl. for example, if the features f are likert
variables giving the extent to which respondents on a questionnaire
agree with statements 1, . . . , n, we might be able to say that questions
i and j are similar if   yi     yj   is small; or that question i is a more
polarizing form of question j if yi =    yj, with    > 1.
even more interesting, it allows us to compare features of di erent
types. we could say that the real-valued feature i is similar to likert-
valued question j if   yi     yj   is small.

latent variables. each row of x represents an example by a vector
in rk. the matrix y maps these representations back into the origi-
nal feature space (now nonlinearly) as described in the discussion on
data imputation in   5.3. we might think of x as discovering the latent
variables that best explain the observed data, with the added bene   t
that these latent variables lie in the vector space rk. if the approxi-

mation errorq(i,j)    lij(xiyj, aij) is small, then we view these latent

variables as providing a good explanation or summary of the full data
set.

44

id168s for abstract data types

probabilistic interpretation. we can give a probabilistic interpreta-
tion of x and y , generalizing the hierarchical bayesian model presented
by fithian and mazumder in [48]. we suppose that the matrices   x
and   y are generated according to a id203 distribution with prob-
ability proportional to exp(   r(   x)) and exp(     r(   y )), respectively. our
observations a of the entries in the matrix   z =   x   y are given by

aij =   ij((   x   y )ij),

where the random variable   ij(u) takes value a with id203 pro-
portional to

exp (   lij(u, a)) .

we observe each entry (i, j)     . then to    nd the maximum a poste-
riori (map) estimator (x, y ) of (   x,   y ), we solve
exp1   q(i,j)    lij(xiyj, aij)2 exp(   r(x)) exp(     r(y )),
maximize
which is equivalent, by taking logs, to problem (5.1).
this interpretation gives us a simple way to interpret our procedure
for imputing missing observations (i, j)        . we are simply computing
the map estimator   aij.

auto-encoder. the matrix x encodes the data; the matrix y decodes
it back into the full space. we can view (5.1) as providing the best
linear auto-encoder for the data. among all linear encodings (x) and
decodings (y ) of the data, the abstract generalized low rank model
(5.1) minimizes the reconstruction error measured according to the
id168s lij.

compression. we impose an information bottleneck by using a low
rank auto-encoder to    t the data. the bottleneck is imposed by both
the id84 and the id173, giving both soft
and hard constraints on the information content allowed. the solu-
tion (x, y ) to problem (5.1) maximizes the information transmitted
through this k-dimensional bottleneck, measured according to the loss
functions lij. this x and y give a compressed and real-valued rep-
resentation that may be used to more e ciently store or transmit the
information present in the data.

5.5. o sets and scaling

5.5 o sets and scaling

45

just as in the previous section, better practical performance can often
be achieved by allowing an o set in the model as described in   3.3, and
automatic scaling of id168s as described in   4.3. as we noted in
  4.3, scaling the id168s (instead of standardizing the data) has
the advantage that no arithmetic is performed directly on the data a.
when the data a consists of abstract types, it is quite important that
no arithmetic is performed on the data, so that we need not take the
average of, say,    very much    and    a little   , or subtract it from    not at
all   .

5.6 numerical examples

in this section we give results of some small experiments illustrating
the use of di erent id168s adapted to abstract data types, and
comparing their performance to quadratically regularized pca. to    t
these glrms, we use alternating minimization and solve the subprob-
lems with subid119. this approach is explained more fully in
  7. running the alternating subgradient method multiple times on the
same glrm from di erent initial conditions yields di erent models,
all with very similar (but not identical) objective values.

boolean pca. for this experiment, we generate boolean data a   
{   1, +1}n   m as

a = sign1xtruey true2,

where xtrue    rn   ktrue and y true    rktrue   m have independent, stan-
dard normal entries. we consider a problem instance with m = 50,
n = 50, and ktrue = k = 10.

we    t two glrms to this data to compare their perfor-
mance. boolean pca uses hinge loss l(u, a) = max (1     au, 0) and
quadratic id173 r(u) =   r(u) = .1  u  2
2, and produces the
model (xbool, y bool). quadratically regularized pca uses squared loss
l(u, a) = (u    a)2 and the same quadratic id173, and produces
the model (xreal, y real).

46

id168s for abstract data types

figure 5.4 shows the results of    tting boolean pca to this data.
the    rst column shows the original ground-truth data a; the second
shows the imputed data given the model,   abool, generated by rounding
the entries of xbooly bool to the closest number in 0, 1 (as explained in
  5.3); the third shows the error a      abool. figure 5.4 shows the results
of running quadratically regularized pca on the same data, and shows
a,   areal, and a       areal.
as expected, boolean pca performs substantially better than
quadratically regularized pca on this data set. de   ne the misclas-
si   cation error (percentage of misclassi   ed entries)

   (x, y ; a) = #{(i, j) | aij    = sign (xy )ij}

mn

.

(5.3)

on average over 100 draws from the ground truth data distri-
bution, the misclassi   cation error is much lower using hinge loss
(   (xbool, y bool; a) = 0.0016) than squared loss (   (xreal, y real; a) =
0.0051). the average rms errors

rms(x, y ; a) =qa

1
mn

m  i=1

n  j=1

1/2

(aij     (xy )ij)2rb

using hinge loss (rms(xbool, y bool; a) = 0.0816) and squared loss
(rms(xreal, y real; a) = 0.159) also indicate an advantage for boolean
pca.

figure 5.4: boolean pca on boolean data.

5.6. numerical examples

47

figure 5.5: quadratically regularized pca on boolean data.

censored pca.
in this example, we consider the performance of
boolean pca when only a subset of positive entries in the boolean
matrix a    {   1, 1}m   n have been observed, i.e., the data has been cen-
sored. for example, a retailer might know only a subset of the products
each customer purchased; or a medical clinic might know only a subset
of the diseases a patient has contracted, or of the drugs the patient has
taken. imputation can be used in this setting to (attempt to) distin-
guish true negatives aij =    1 from unobserved positives aij = +1,
(i, j)        .
we generate a low rank matrix b = xy    [0, 1]m   n with x   
rm   k, y    rk   n, where the entries of x and y are drawn from a
uniform distribution on [0, 1], m = n = 300 and k = 3. our data
matrix a is chosen by letting aij = 1 with id203 proportional
to bij, and    1 otherwise; the constant of proportionality is chosen so
that half of the entries in a are positive. we    t a rank 5 glrm to
an observation set   consisting of 10% of the positive entries in the
matrix, drawn uniformly at random, using hinge loss and quadratic
id173. (note that the rank of the model is higher than the
(unobserved) true rank of the data; we will see below in   8.2 how to
choose the right rank for a model.) that is, we    t the low rank model

minimize q(i,j)    max(1   xiyjaij, 0)+   qm

and vary the id173 parameter    .

i=1   xi  2

2+   qn

j=1   yj  2

2

48

id168s for abstract data types

we consider three error metrics to measure the performance of the

   tted model (x, y ): normalized training error,

normalized test error,
1

1

| |   (i,j)   
| c|   (i,j)   c

max(1     aijxiyj, 0),

max(1     aijxiyj, 0),

and precision at 10 (p@10), which is computed as the fraction of the
top ten predicted values not in the observation set, {xiyj : (i, j)     c},
for which aij = 1. (here,  c = {1, . . . , m}   { 1, . . . , n} \  .) precision
at 10 measures the usefulness of the model: if we predicted that the
top 10 unseen elements (i, j) had values +1, how many would we get
right?

figure 5.6 shows the id173 path as     ranges from 0 to 40,
averaged over 50 samples from the distribution generating the data.
here, we see that while the training error decreases as     decreases,
the test error reaches a minimum around     = 5. interestingly, the
precision at 10 improves as the id173 increases; since precision
at 10 is computed using only relative rather than absolute values of the
model, it is insensitive to the shrinkage of the parameters introduced by
the id173. the grey line shows the id203 of identifying a
positive entry by guessing randomly; precision at 10, which exceeds 80%
when     & 30, is signi   cantly higher. this performance is particularly
impressive given that the observations   are generated by sampling
from rather than rounding the auxiliary matrix b.

mixed data types.
in this experiment, we    t a glrm to a data
table with numerical, boolean, and ordinal columns generated as fol-
lows. let n1, n2, and n3 partition the column indices 1, . . . , n. choose
xtrue    rm   ktrue, y true    rktrue   n to have independent, standard nor-
mal entries. assign entries of a as follows:

aij =y_]_[

xiyj
sign (xiyj)
round(3xiyj + 1)

j   n 1
j   n 2
j   n 3,

5.6. numerical examples

train error
test error

p@10

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

8

6

4

2

0

0
0

5
5

15
15

10
30
10
30
id173 parameter
id173 parameter

25
25

20
20

35
35

40
40

49

1
+

f
o

y
t
i
l
i
b
a
b
o
r
p

1

0.8

0.6

0.4

0.2

0

figure 5.6: error metrics for boolean glrm on censored data. the grey line shows
the id203 that a random guess identi   es a positive entry.

where the function round maps a to the nearest integer in the set
{1, . . . , 7}. thus, n1 corresponds to real-valued data; n2 corresponds
to boolean data; and n3 corresponds to ordinal data. we consider a
problem instance in which m = 100, n1 = 40, n2 = 30, n3 = 30, and
ktrue = k = 10.

we    t a heterogeneous loss glrm to this data with id168

lij(u, a) =y_]_[

lreal(u, a)
lbool(u, a)
lord(u, a)

j   n 1
j   n 2
j   n 3,

where lreal(u, a) = (u   a)2, lbool(u, a) = (1   au)+, and lord(u, a) is de-
   ned in (5.2), and with quadratic id173 r(u) =   r(u) = .1  u  2
2.
we    t the glrm to produce the model (xmix, y mix). for compar-
ison, we also    t quadratically regularized pca to the same data,

50

id168s for abstract data types

2, to produce the model (xreal, y real).

using lij(u, a) = (u     a)2 for all j and quadratic id173
r(u) =   r(u) = .1  u  2
figure 5.7 compares the performance of the heterogeneous loss
glrm to quadratically regularized pca    t to the same data.
panel 5.7a shows the results of    tting the heterogeneous loss glrm
above. panel 5.7b shows the results of    tting quadratically regular-
ized pca. the    rst column shows the original ground-truth data a;
the second shows the imputed data given the model,   amix, generated
by rounding the entries of xmixy mix to the closest number in 0, 1 (as
explained in   5.3); the third shows the error a       amix.
to evaluate error for boolean and ordinal data, we use the misclas-
si   cation error     (5.3) de   ned above. for notational convenience, we let
ynl (anl) denote y (a) restricted to the columns nl in order to pick
out real-valued columns (l = 1), boolean columns (l = 2), and ordinal
columns (l = 3).

table 5.1 compare the average error (di erence between imputed
entries and ground truth) over 100 draws from the ground truth distri-
bution for models using heterogeneous loss (xmix, y mix) and quadrat-
ically regularized loss (xreal, y real). columns are labeled by error met-
ric. we use misclassi   cation error     (de   ned in (5.3)) for boolean and
ordinal data and mse for numerical data.

xmix, y mix
xreal , y real

mse(x, yn1; an1)

0.0224
0.0076

   (x, yn2; an2)

0.0074
0.0213

   (x, yn3; an3)

0.0531
0.0618

table 5.1: average error for numerical, boolean, and ordinal features using glrm
with heterogeneous loss and quadratically regularized loss.

missing data. here, we explore the e ect of missing entries on the ac-
curacy of the recovered model. we generate data a as detailed above,
but then censor one large block of entries in the table (constituting
3.75% of numerical, 50% of boolean, and 50% of ordinal data), remov-
ing them from the observed set  .

5.6. numerical examples

51

(a) heterogeneous loss glrm on mixed data.

(b) quadratically regularized pca on mixed data.

figure 5.7: models for mixed data.

figure 5.8 shows the results of    tting three di erent models with
rank 10 to the censored data. panel 5.8a shows the original ground-
truth data a and the block of data that has been removed from the
observation set  . panel 5.8b shows the results of    tting the heteroge-
neous loss glrm described above to the block-censored data: the    rst
column shows the imputed data given the model,   amix, generated by
rounding the entries of xmixy mix to the closest number in {0, 1} (as
explained in   5.3), while the second column shows the error a       amix.
two other models are provided for comparison. panel 5.8c shows the
imputed values   areal and error a      areal obtained by running quadrat-
ically regularized pca on the same data and with the same held-out
block. panel 5.8d shows the imputed values   areal and error a       areal
obtained by running (unregularized) pca on the same data after re-
placing each missing entry with the column mean. while quadradically
regularized pca and the heterogeneous loss glrm performed sim-
ilarly when no data was missing, the heterogeneous loss glrm per-
forms better than quadradically regularized pca when a large block of

52

id168s for abstract data types

remove entries

mixed data types

12
9
6
3
0

 12 9 6 3

glrm rank 10 recovery

error

(a) original data.

12
9
6
3
0

 12 9 6 3

(b) heterogeneous loss glrm on missing data.

qpca rank 10 recovery

error

12
9
6
3
0

 12 9 6 3

(c) quadratically regularized pca on missing data.

error

pca rank 10 recovery

12
9
6
3
0

 12 9 6 3

12
9
6
3
0

 12 9 6 3

3.0
2.4
1.8
1.2
0.6
0.0

 3.0 2.4 1.8 1.2 0.6

3.0
2.4
1.8
1.2
0.6
0.0

 3.0 2.4 1.8 1.2 0.6

3.0
2.4
1.8
1.2
0.6
0.0

 3.0 2.4 1.8 1.2 0.6

(d) pca on missing data    t by replacing missing entries with column mean.

figure 5.8: three methods for imputing a block of heterogeneous missing data.

5.6. numerical examples

53

data is censored; interestingly, using maladapted (quadratic) loss func-
tions for the boolean and ordinal data results in a model that    ts even
the real valued data more poorly. the third (and all too common in
practice) approach, which    lls in missing data with the column mean
and runs pca, performs disastrously.

we compare the average error (di erence between imputed entries
and ground truth) over 100 draws from the ground truth distribution
in table 5.2. as above, we use misclassi   cation error     (de   ned in (5.3))
for boolean and ordinal data and mse for numerical data.

xmix, y mix
xreal , y real

mse(x, yn1; an1)

0.392
0.561

   (x, yn2; an2)

0.2968
0.4029

   (x, yn3; an3)

0.3396
0.9418

table 5.2: average error over imputed data comparing two glrms: one using
heterogeneous loss and one using regularized quadratic loss.

6

multi-dimensional id168s

in this section, we generalize the procedure to allow the id168s
to depend on blocks of the matrix xy , which allows us to represent
abstract data types more naturally. for example, we can now represent
categorical values , permutations, distributions, and rankings.
we are given a id168 lij : r1   dj    f j    r, where dj is
the embedding dimension of feature j, and d =qj dj is the embedding
dimension of the model. the loss lij(u, a) describes the approximation
error incurred when we represent a feature value a   f j by the vector
u    rdj.
let xi    r1   k be the ith row of x (as before), and let yj    rk   dj
be the jth block matrix of y so the columns of yj correspond to the
columns of embedded feature j. we now formulate a multi-dimensional
generalized low rank model on the database a,

minimize q(i,j)    lij(xiyj, aij) +qm

(6.1)
with variables x    rn   k and y    rk   d, and with loss lij as above and
regularizers ri(xi) : r1   k    r (as before) and   rj(yj) : rk   dj    r.
note that the    rst argument of lij is a row vector with dj entries, and
the    rst argument of rj is a matrix with dj columns. when every entry

i=1 ri(xi) +qn

j=1   rj(yj),

54

6.1. examples

55

aij is real-valued (i.e., dj = 1), then we recover the generalized low
rank model (4.1) seen in the previous section.

6.1 examples
categorical pca. suppose that a   f is a categorical variable, tak-
ing on one of d values or labels. identify the labels with the integers
{1, . . . , d}. in (6.1), set

l(u, a) = (1     ua)+ +   a    f, a     =a
and use the quadratic regularizer ri =           2
fixing x and optimizing over y is equivalent to training one id166
per label to separate that label from all the others: the jth column
of y gives the weight vector corresponding to the jth id166. (this is
sometimes called one-vs-all multiclass classi   cation [123].) optimizing
over x identi   es the low-dimensional feature vectors for each example
that allow these id166s to most accurately predict the labels.

2,   r =           2
2.

(1 + ua  )+,

the di erence between categorical pca and boolean pca is in
how missing labels are imputed. to impute a label for entry (i, j) with
feature vector xi according to the procedure described above in 5.3,
we project the representation yj onto the line spanned by xi to form
u = xiyj. given u, the imputed label is simply argmaxl ul. this model
has the interesting property that if column l   of yj lies in the interior of
the convex hull of the columns of yj, then ul   will lie in the interior of
the interval [minl ul, maxl ul] [17]. hence the model will never impute
label l   for any example.

we need not restrict ourselves to the id168 given above.
in fact, any id168 that can be used to train a classi   er for
categorical variables (also called a multi-class classi   er) can be used
to    t a categorical pca model, so long as the id168 depends
only on the inner products between the parameters of the model and
the features corresponding to each example. the id168 becomes
the id168 l used in (6.1); the optimal parameters of the model
give the optimal matrix y , while the implied features will populate
the optimal matrix x. for example, it is possible to use id168s

56

multi-dimensional id168s

derived from error-correcting output codes [40]; the directed acyclic
graph id166 [114]; the crammer-singer multi-class loss [30]; or the
multi-category id166 [89].

of these id168s, only the one-vs-all loss is separable across
the classes a   f . (by separable, we mean that the objective value can
be written as a sum over the classes.) hence    tting a categorical fea-
tures with any other id168s is not the same as    tting d boolean
features. for example, in the crammer-singer loss

l(u, a) = (1     ua + max

a    f, a     =a

u  a)+,

the classes are combined according to their maximum, rather than their
sum. while one-vs-all classi   cation performs about as well as more so-
phisticated id168s on small data sets [123], these more sophis-
ticated nonseparable loss tend to perform much better as the number
of classes (and examples) increases [56].

some interesting nonconvex id168s have also been suggested
for this problem. for example, consider a generalization of hamming
distance to this setting,

l(u, a) =    (ua    = 1) +   a     =a

   (ua      =    1),

where     is a function that returns 1 if its argument is true and 0 oth-
erwise. in this case, alternating minimization with id173 that
enforces a clustered structure in the low rank model (see the discus-
sion of quadratic id91 in   3.2) reproduces the k-modes algorithm
[64].

ordinal pca. we saw in   5 one way to    t a glrm to ordinal data.
the multi-dimensional embedding will be particularly useful when the
best mapping of the ordinal variable onto a linear scale is not uniform;
e.g., if level 1 of the ordinal variable is much more similar to level 2
than level 2 is to level 3. using a larger embedding dimension allows
us to infer the relations between the levels from the data itself. here
we again identify the labels a   f with the integers {1, . . . , d}.

6.1. examples

57

figure 6.1: multi-dimensional ordinal loss. fitting a glrm with this id168
simultaneously    nds the best locations xi for each ordinal observation (here shown
as the numbers 1   4), and the best hyperplanes (here shown as grey lines) to separate
each level from the next. the perpendicular segment on each line shows (as a vector)
the column of y corresponding to that hyperplane.

one approach we can use for (multi-dimensional) ordinal pca is

to solve (6.1) with the id168

l(u, a) =

d   1  a  =1

(1     ia>a  ua  )+,

(6.2)

and with quadratic id173. here, the embedding dimension is
d   1, so u    rd   1. this approach    ts a set of hyperplanes (given by the
columns of y ) separating each level l from the next. the hyperplanes
need not be parallel to each other. fixing x and optimizing over y is
equivalent to training an id166 to separate labels a    l from a > l for
each l   f . fixing y and optimizing over x    nds the low dimensional
features vector for each example that places the example between the
appropriate hyperplanes. (see figure 6.1 for an illustration of an op-
timal    t of this id168, with k = 2, to a simple synthetic data
set.)

(cid:1)(cid:2)(cid:1)(cid:1)(cid:3)(cid:2)(cid:2)(cid:3)(cid:1)(cid:4)(cid:2)(cid:1)(cid:4)(cid:1)(cid:3)(cid:3)(cid:4)(cid:3)(cid:1)(cid:4)(cid:4)(cid:4)(cid:1)(cid:2)(cid:2)(cid:1)(cid:1)(cid:3)(cid:1)(cid:3)58

multi-dimensional id168s

permutation pca. suppose that a is a permutation of the numbers
1, . . . , d. de   ne the permutation loss

l(u, a) =

(1     uai + uai+1)+.

d   1  i=1

this loss is zero if uai > uai+1 + 1 for i = 1, . . . , d     1, and increases
linearly when these inequalities are violated. de   ne sort(u) to return
a permutation   a of the indices 1, . . . , d so that u  ai    u  ai+1 for i =
1, . . . , d     1. it is easy to check that argmina l(u, a) = sort(u). hence
using the permutation id168 in generalized pca (6.1)    nds a
low rank approximation of a given table of permutations.

ranking pca. many variants on the permutation pca problem are
possible. for example, in ranking pca, we interpret the permutation
as a ranking of the choices 1, . . . , d, and penalize deviations of many
levels more strongly than deviations of only one level by choosing the
loss

l(u, a) =

d   1  i=1

d  j=i+1

(1     uai + uaj)+.

from here, it is easy to generalize to a setting in which the rankings
are only partially observed. suppose that we observe pairwise compar-
isons a    { 1, . . . , d}   { 1, . . . , d}, where (i, j)    a means that choice i
was ranked above choice j. then a id168 penalizing deviations
from these observed rankings is

l(u, a) =   (i,j)  a

(1     uai + uaj)+.

many other modi   cations to ranking id168s have been pro-
posed in the literature that interpolate between the the two    rst loss
functions proposed above, or which prioritize correctly predicting the
top ranked choices. these losses include the area under the curve loss
[138], ordered weighted average of pairwise classi   cation losses [148],
the weighted approximate-rank pairwise loss [153], the k-order statis-
tic loss [154], and the accuracy at the top loss [14].

6.2. o sets and scaling

59

6.2 o sets and scaling
just as in the previous section, better practical performance can often
be achieved by allowing an o set in the model as described in   3.3, and
scaling id168s as described in   4.3.

6.3 numerical examples
we    t a low rank model to the 2013 american community survey
(acs) to illustrate how to    t a low rank model to heterogeneous
data.

the acs is a survey administered to 1% of the population of
the united states each year to gather their responses to a variety
of demographic and economic questions. our data sample consists of
m = 3132796 responses gathered from residents of the us, exclud-
ing puerto rico, in the year 2013, on the 23 questions listed in table
6.1.

we    t a rank 10 model to this data using huber loss for real val-
ued data, hinge loss for boolean data, ordinal hinge loss for ordinal
data, one-vs-all categorical loss for categorical data, and id173
parameter     = .1. we allow an o set in the model and scale the loss
functions and id173 as described in   4.3.
in table 6.2, we select a few features j

from the model,
along with their associated vectors yj, and    nd the two features
most similar to them by    nding the two features j   which mini-
mize cos(yj, yj  ). the model automatically groups states which in-
tuitively share demographic features:
for example, three wealthy
states adjoining (but excluding) a major metropolitan area     vir-
ginia, maryland, and connecticut     are grouped together. the
low rank structure also identi   es the results (high water prices)
of the prolonged drought a icting california, and corroborates the
intuition that work leads only to more work: hours worked per
week, weeks worked per year, and education level are highly corre-
lated.

60

multi-dimensional id168s

description
household type
state

commercial use
house on    10 acres
household income
monthly electricity bill

variable
hhtype
stateicp
ownershp own home
commuse
acrehous
hhincome
costelec
costwatr monthly water bill
costgas
foodstmp
hcovany
school
educ
gradeatt
empstat
labforce
classwkr
wkswork2 weeks worked per year
uhrswork usual hours worked per week
looking
migrate1

monthly gas bill
on food stamps
have health insurance
currently in school
highest level of education
highest grade level attained
employment status
in labor force
class of worker

looking for work
migration status

type
categorical
categorical
boolean
boolean
boolean
real
real
real
real
boolean
boolean
boolean
ordinal
ordinal
categorical
boolean
boolean
ordinal
real
boolean
categorical

table 6.1: acs variables.

most similar features
montana, north dakota
illinois, cost of water
oregon, idaho
indiana, michigan

feature
alaska
california
colorado
ohio
pennsylvania massachusetts, new jersey
virginia
maryland, connecticut
hours worked weeks worked, education

table 6.2: most similar features in demography space.

7

fitting low rank models

in this section, we discuss a number of algorithms that may be used
to    t generalized low rank models. as noted earlier, it can be compu-
tationally hard to    nd the global optimum of a generalized low rank
model. for example, it is np-hard to compute an exact solution to k-
means [43], nonnegative id105 [149], and weighted pca
and matrix completion [50] all of which are special cases of low rank
models.

in   7.1, we will examine a number of local optimization methods
based on alternating minimization. algorithms implementing lazy vari-
ants of alternating minimization, such as the alternating gradient, prox-
imal gradient, or stochastic gradient algorithms, are faster per iteration
than alternating minimization, although they may require more iter-
ations for convergence. in numerical experiments, we notice that lazy
variants often converge to points with a lower objective value: it seems
that these lazy variants are less likely to be trapped at a saddle point
than is alternating minimization.   7.4 explores the convergence of these
algorithms in practice.

we then consider a few special cases in which we can show that al-
ternating minimization converges to the global optimum in some sense:

61

62

fitting low rank models

for example, we will see convergence with high id203, approxi-
mately, and in retrospect.   7.5 discusses a few strategies for initializing
these local optimization methods, with provable guarantees in special
cases.   7.6 shows that for problems with convex id168s and
quadratic id173, it is sometimes possible to certify global op-
timality of the resulting model.

7.1 alternating minimization
we showed earlier how to use alternating minimization to    nd an (ap-
proximate) solution to a generalized low rank model. algorithm (1)
shows how to explicitly extend alternating minimization to a general-
ized low rank model (4.1) with observations  .

algorithm 1

given x0, y 0
for k = 1, 2, . . . do

for i = 1, . . . , m do

xk

end for
for j = 1, . . . , n do

i = argminx1qj:(i,j)    lij(xyk   1
j = argminy1qi:(i,j)    lij(xk

, aij) + ri(x)2
i y, aij) +   rj(y)2

end for

yk

j

end for

parallelization. alternating minimization parallelizes naturally over
examples and features. in algorithm 1, the loops over i = 1, . . . , n and
over j = 1, . . . , m may both be executed in parallel.

7.2 early stopping
it is not very useful to spend a lot of e ort optimizing over x before we
have a good estimate for y . if an iterative algorithm is used to compute
the minimum over x, it may make sense to stop the optimization over
x early before going on to update y . in general, we may consider

7.2. early stopping

63

replacing the minimization over x and y above by any update rule that
moves towards the minimum. this templated algorithm is presented
as algorithm 2. empirically, we    nd that this approach often    nds a
better local minimum than performing a full optimization over each
factor in every iteration, in addition to saving computational e ort on
each iteration.
algorithm 2

given x0, y 0
for t = 1, 2, . . . do

for i = 1, . . . , m do

end for
for j = 1, . . . , n do

end for

end for

i = updatelij,ri(xt   1
xt

i

, y t   1, a)

j = updatelij,  rj(y
yt

(t   1)t

j

, x(t)t , at )

we describe below a number of di erent update rules updatel,r by
writing the x update. the y update can be implemented similarly. (in
fact, it can be implemented by substituting   r for r, switching the roles of
x and y , and transposing all matrix arguments.) all of the approaches
outlined below can still be executed in parallel over examples (for the
x update) and features (for the y update).

gradient method. for example, we might take just one gradient step
on the objective. this method can be used as long as l, r, and   r do not
take in   nite values. (if any of these functions f is not di erentiable,
replace   f below by any subgradient of f [12, 18].)
g =   j:(i,j)     lij(xiyj, aij)yj +   ri(xi).

we implement updatel,r as follows. let

then set

i = xt   1
xt

i        tg,

64

fitting low rank models

for some step size    t. for example, the step size rule    t = 1/t, which
guarantees convergence to the globally optimal x if y is    xed [12, 18].
a faster approach in practice might be to use a backtracking line search
[105].

proximal gradient method.
if a function takes on the value   , it
need not have a subgradient at that point, which limits the gradient
update to cases where the regularizer and loss are (   nite) real-valued.
when the regularizer (but not the loss) takes on in   nite values (say, to
represent a hard constraint), we can use a proximal gradient method
instead.

the proximal operator of a function f [109] is

proxf(z) = argmin

x

(f(x) + 1

2  x     z  2
2).

if f is the indicator function of a set c, the proximal operator of f is
just (euclidean) projection onto c.
a proximal gradient update updatel,r is implemented as follows.
let

g =   j:(i,j)     lij(xt   1

i

yt   1

j

, aij)yt   1

j

.

then set

i = prox   tri(xt   1
xt

i        tg),

for some step size    t. the step size rule    t = 1/t guarantees con-
vergence to the globally optimal x if y is    xed, while using a    xed,
but su ciently small, step size     guarantees convergence to a small
o(   ) neighborhood around the optimum [8]. the technical condition
required on the step size is that    t < 1/l, where l is the lipshitz con-
stant of the gradient of the objective function. bolte et al. have shown
that the iterates xt
j produced by the proximal gradient update
rule (which they call proximal alternating linearized minimization, or
palm) globally converge to a critical point of the objective function
under very mild conditions on the id168s and regularizers [11].

i and yt

7.2. early stopping

65

prox-prox method. letting ft(x) = q(i,j)    lij(xiyt

the proximal-proximal (prox-prox) update

j, aij), de   ne

x t+1 = prox   tri(prox   tft(x t)).

the prox-prox update is simply a proximal gradient step on the

objective when ft is replaced by its moreau envelope,

mft(x) = inf

x   1ft(x  ) +   x     x    2
f2 .

(see [109] for details.) the moreau envelope has the same minimizers
as the original objective. thus, just as the proximal gradient method
repeatedly applied to x converges to global minimum of the objective
if y is    xed, the prox-prox method repeatedly applied to x also con-
verges to global minimum of the objective if y is    xed under the same
conditions on the step size    t, for any constant stepsize          g  2
2.
(here,   g  2 = sup  x  2  1   gx  2 is the operator norm of g.)
this update can also be seen as a single iteration of admm when
the dual variable in admm is initialized to 0; see [16]. in the case of
quadratic objectives, we will see below that the prox-prox update can
be applied very e ciently, making iterated prox-prox, or admm, an
e ective means of computing the solution to the subproblems arising
in alternating minimization.

choosing a step size.
in numerical experiments, we    nd that using
a slightly more nuanced rule allowing di erent step sizes for di erent
rows and columns can allow fast progress towards convergence while
ensuring that the value of the objective never increases. the safeguards
on step sizes we propose are quite important in practice: without these
checks, we observe divergence when the initial step sizes are chosen too
large.

motivated by the convergence proof in [8], for each row i, we seek
a step size    i on the order of 1/  gi  2, where gi is the gradient of the
objective function with respect to xi. we start by choosing an initial
step size scale     of the same order as the average gradient of the loss
functions. in the numerical experiments reported here, we choose     =
1.since gi grows with the number of observations ni = |{j : (i, j)     }|

66

fitting low rank models

in row i, we achieve the desired scaling by setting    i =    /ni. we take
a gradient step on each row xi using the step size    i. our procedure
for choosing    j is the same.

we then check whether the objective value for the row,

  j:(i,j)   

lj(xiyj, aij) + ri(xi),

has increased or decreased. if it has increased, then we trust our    rst
order approximation to the objective function less far, and reduce the
step size; if it has decreased, we gain con   dence, and increase the step
size. in the numerical experiments reported below, we decrease the step
size by 30% when the objective increases, and increase the step size by
5% when the objective decreases. this check stabilizes the algorithm
and prevents divergence even when the initial scale has been chosen
poorly.

we then do the same with respect to each column yj: we take a
gradient step, check if the objective value for the column has increased
or decreased, and adjust the step size.

the time per iteration is thus o(k(m + n + | |)): computing the
gradient of the ith id168 with respect to xi takes time o(kni);
computing the proximal operator of the square loss takes time o(k);
summing these over all the rows i = 1, . . . , m gives time o(k(m +
| |)); and adding the same costs for the column updates gives time
o(k(m + n + | |)). the checks on the objective value take time o(k)
per observed entry (to compute the inner product xiyj and value of
the id168 for each observation) and time o(1) per row and
column to compute the value of the regularizer. hence the total time
per iteration is o(k(m + n + | |)).
by partitioning the job of updating di erent rows and di erent
columns onto di erent processors, we can achieve an iteration time of
o(k(m + n + | |)/p) using p processors.

stochastic gradients.
instead of computing the full gradient of l
with respect to xi above, we can replace the gradient g in either the
gradient or proximal gradient method by any stochastic gradient g,

7.3. quadratic objectives

67

which is a vector that satis   es

e g =   j:(i,j)     lij(xiyj, aij)yj.

a stochastic gradient can be computed by sampling j uniformly at
random from among observed features of i, and setting g = |{j : (i, j)   
 }|  lij(xiyj, aij)yj. more samples from {j : (i, j)     } can be used
to compute a less noisy stochastic gradient.
7.3 quadratic objectives
here we describe how to e ciently implement the prox-prox update
rule for quadratic objectives and arbitrary regularizers, extending the
factorization caching technique introduced in   2.3. we assume here
that the objective is given by

  a     xy   2

f + r(x) +   r(y ).

we will concentrate here on the x update; as always, the y update is
exactly analogous.

as in the case of quadratic id173, we    rst form the gram
matrix g = y y t . then the proximal gradient update for x is fast to
evaluate:

prox   kr(x        k(xg     2ay t )).

but we can also take advantage of the ease of inverting the gram
matrix g to design a faster algorithm using the prox-prox update than
is possible with general id168s. for a quadratic objective with
gram matrix g = y t y , the prox-prox update takes the simple form

prox   kr((g + 1

i)   1(ay t + 1
   k

   k

x)).

as in   2.3, we can compute (g + 1
x) in parallel by
   k
   rst caching the factorization of (g+ 1
i)   1. hence it is advantageous
   k
to repeat this update many times before updating y , since most of the
computational e ort is in forming g and ay t .

i)   1(ay t + 1
   k

for example, in the case of nonnegative least squares, this update

is just

 +((g + 1
   k

i)   1(ay t + 1
   k

x)),

where  + projects its argument onto the nonnegative orthant.

68

7.4 convergence

fitting low rank models

alternating minimization need not converge to the same model (or
the same objective value) when initialized at di erent starting points.
through examples, we explore this idea here. these examples are    t
using the julia implementation (presented in   9) of the alternating
proximal gradient updates method. the timing results were obtained
using a single core of a standard laptop computer.

global convergence for quadratically regularized pca. figure 7.1
shows the convergence of the alternating proximal gradient update
method on a quadratically regularized pca problem with randomly
generated, fully observed data a = xtruey true, where entries of xtrue
and y true are drawn from a standard normal distribution. we pick
   ve di erent random initializations of x and y with standard normal
entries to generate    ve di erent convergence trajectories. quadrati-
cally regularized pca is a simple problem with an analytical solution

y
t
i
l
a
m

i
t
p
o
b
u
s

e
v
i
t
c
e
j
b
o

105

104

103

102

101

100

0

1

figure 7.1: convergence of alternating proximal gradient updates on quadratically
regularized pca for n = m = 200, k = 2.

2

3

time (s)

7.4. convergence

69

(see   2), and with no local minimum that is not global (see ap-
pendix a.1). hence it should come as no surprise that the trajectories
all converge to the same, globally optimal value.

local convergence for nonnegative id105. figure 7.2
shows convergence of the same algorithm on a nonnegative matrix fac-
torization model, with data generated in the same way as in figure 7.1.
(note that a has negative entries as well as positive entries, so the
minimal objective value is strictly greater than zero.) here, we plot the
convergence of the objective value, rather than the suboptimality, since
we cannot (e ciently, provably) compute the global minimum of the
objective function even in the rank 1 case [51].

we see that the algorithm converges to a di erent optimal value
(and point) depending on the initialization of x and y . three trajec-
tories converge to the same optimal value (though one does so much

  104

e
u
l
a
v

e
v
i
t
c
e
j
b
o

1.8

1.7

1.6

1.5

1.4

1.3

0

1

figure 7.2: convergence of alternating proximal gradient updates on nnmf for
n = m = 200, k = 2.

2

3

time (s)

70

fitting low rank models

faster than the others), one to a value that is somewhat better, and
one to a value that is substantially worse.

7.5

initialization

above, we saw that alternating minimization can converge to mod-
els with optimal values that di er signi   cantly. here, we discuss two
approaches to initialization that result in provably good solutions, for
special cases of the generalized low rank problem. we then discuss how
to apply these initialization schemes to more general models.

svd. a literature that is by now extensive shows that the svd pro-
vides a provably good initialization for the quadratic matrix completion
problem (2.10) [74, 76, 73, 66, 58, 55]. algorithms based on alternating
minimization have been shown to converge quickly (even geometrically
[66]) to a global solution satisfying a recovery guarantee when the initial
values of x and y are chosen carefully; see   2.4 for more details.

here, we extend the svd initialization previously proposed for ma-
trix completion to one that works well for all pca-like problems: prob-
lems with convex id168s that have been scaled as in   4.3; with
data a that consists of real values, booleans, categoricals, and ordinals;
and with quadratic (or no) id173.

but we will need a matrix on which to perform the svd. what
matrix corresponds to our data table? here, we give a simple proposal
for how to construct such a matrix, motivated by [76, 66, 26]. our key
insight is that the svd is the solution to our problem when the entries
in the table have mean zero and variance one (and all the id168s
are quadratic). our initialization will construct a matrix with mean
zero and variance one from the data table, take its svd, and invert
the construction to produce the correct initialization.

our    rst step is to expand the categorical columns taking on d val-
ues into d boolean columns, and to re-interpret ordinal and boolean
columns as numbers. the scaling we propose below is insensitive to the
values of the numbers in the expansion of the booleans: for example,
using (false, true)= (0, 1) or (false, true)= (   1, 1) produces the same

7.5.

initialization

71

initialization. the scaling is sensitive to the di erences between ordi-
nal values: while encoding (never, sometimes, always) as (1, 2, 3) or as
(   5, 0, 5) will make no di erence, encoding these ordinals as (0, 1, 10)
will result in a di erent initialization.
now we assume that the rows of the data table are independent
and identically distributed; our mission is to standardize the columns.
the observed entries in column j have mean   j and variance    2
j ,

  j = argmin
j = 1
   2

     i:(i,j)   
nj     1   i:(i,j)   

lj(  , aij)

lj(  j, aij),

so the matrix whose (i, j)th entry is (aij       j)/   j for (i, j)      has
columns whose observed entries have mean 0 and variance 1.
each missing entry can be safely replaced with 0 in the scaled ver-
sion of the data without changing the column mean. but the column
variance will decrease to mj/m. if instead we de   ne
(i, j)     
otherwise,

  aij =i m

(aij       j)

   jmj
0

then the column will have mean 0 and variance 1.
take the svd u v t of   a, and let   u    rm   k,        rk   k, and
  v    rn   k denote these matrices truncated to the top k singular vectors
and values. we initialize x =   u    1/2, and y =    1/2   v t diag(   ). the
o set row in the model is initialized with the means, i.e., the kth column
of x is    lled with 1   s, and the kth row of y is    lled with the means,
so ykj =   j.

finally, note that we need not compute the full svd of   a, but
instead can simply compute the top k singular triples. for example,
the randomized top k svd algorithm proposed in [57] computes the
top k singular triples of   a in time linear in | |, m, and n (and quadratic
in k).
figure 7.3 compares the convergence of this svd-based initializa-
tion with random initialization on a low rank model for census data
described in detail in   6.3. we initialize the algorithm at six di erent

72

fitting low rank models

random
random
random
random
random
svd

  105

e
u
l
a
v

e
v
i
t
c
e
j
b
o

8

6

4

2

0

10

30

20
iteration

40

50

figure 7.3: convergence from    ve di erent random initializations, and from the
svd initialization.

points: from    ve di erent random normal initializations (entries of x0
and y 0 drawn iid from n(0, 1)), and from the svd of   a. the svd
initialization produces a better initial value for the objective function,
and also allows the algorithm to converge to a substantially lower    nal
objective value than can be found from any of the    ve random starting
points. this behavior indicates that the    good    local minimum discov-
ered by the svd initialization is located in a basin of attraction that
has low id203 with respect to the measure induced by random
normal initialization.

id116++. the id116++ algorithm is an initialization scheme
designed for quadratic id91 problems [5]. it consists of choosing an
initial cluster centroid at random from the points, and then choosing
the remaining k     1 centroids from the points x that have not yet
been chosen with id203 proportional to d(x)2, where d(x) is the
minimum distance of x to any previously chosen centroid.

7.6. global optimality

73

quadratic id91 is known to be np-hard, even with only two
clusters (k = 2) [43]. however, id116++ followed by alternat-
ing minimization gives a solution with expected approximation ratio
within o(log k) of the optimal value [5]. (here, the expectation is
over the randomization in the initialization algorithm.) in contrast,
an arbitrary initialization of the cluster centers for id116 can re-
sult in a solution whose value is arbitrarily worse than the true opti-
mum.

a similar idea can be used for other low rank models. if the model
rewards a solution that is spread out, as is the case in quadratic cluster-
ing or subspace id91, it may be better to initialize the algorithm
by choosing elements with id203 proportional to a distance mea-
sure, as in id116++. in the id116++ procedure, one can use the
id168 l(u) as the distance metric d.

7.6 global optimality

all generalized low rank models are non-convex, but some are more
non-convex than others. in particular, for some problems, the only im-
portant source of non-convexity is the low rank constraint. for these
problems, it is sometimes possible to certify global optimality of a
model by considering an equivalent rank-constrained convex problem.
the arguments in this section are similar to ones found in [117],
in which recht et al. propose using a factored (nonconvex) formu-
lation of the (convex) nuclear norm regularized estimator in order
to e ciently solve the large-scale sdp arising in a matrix comple-
tion problem. however, the algorithm in [117] relies on a subrou-
tine for    nding a local minimum of an augmented lagrangian which
has the same biconvex form as problem (2.10). finding a local min-
imum of this problem (rather than a saddle point) may be hard.
in this section, we avoid the issue of    nding a local minimum of
the nonconvex problem; we consider instead whether it is possible
to verify global optimality when presented with some putative solu-
tion.

74

fitting low rank models

the factored problem is equivalent to the rank constrained problem.
consider the factored problem

minimize l(xy ) +    

(7.1)
with variables x    rm   k, y    rk   n, where l : rm   n    r is any
convex id168. compare this to the rank-constrained problem

2  y   2
f ,

2  x  2

f +    

minimize l(z) +      z    
subject to rank(z)    k,

(7.2)

with variable z    rm   n. here, we use           to denote the nuclear norm,
the sum of the singular values of a matrix.
theorem 7.1. (x   , y   ) is a solution to the factored problem (7.1) if
and only if z   = x   y    is a solution to the rank-constrained prob-
lem (7.2), and   x     2

f =   y     2

2  z      .

f = 1

we will need the following lemmas to understand the relation be-

tween the rank-constrained problem and the factored problem.
lemma 7.2. let xy = u v t be the svd of xy , where  =
diag(   ). then

       1   

1
2(||x||2

f + ||y ||2
f ).

(7.3)

proof. we may derive this fact as follows:
       1 = tr(u t xy v )

     u t x  f  y v   f
     x  f  y   f
  

1
2(||x||2

f + ||y ||2
f ),

where the    rst inequality above uses the cauchy-schwartz inequality,
the second relies on the orthogonal invariance of the frobenius norm,
and the third follows from the basic inequality ab    1
2(a2 + b2) for any
real numbers a and b.

the following result is well known.

7.6. global optimality

75

1

2(||x||2

f + ||y ||2
f ).

lemma 7.3. for any matrix z,   z     = inf xy =z
proof. writing z = u dv t and recalling the de   nition of the nuclear
norm   z     =        1, we see that lemma 7.2 implies
f + ||y ||2
f ).
but taking x = u 1/2 and y =  1/2v t , we have
f +    1/2  2

  z        inf
xy =z

f ) =        1,

1
2(||x||2

f + ||y ||2

2(   1/2  2

1
2(||x||2

f ) = 1

(using once again the orthogonal invariance of the frobenius norm), so
the bound is satis   ed with equality.

note that the in   mum is achieved by x = u 1/2t and y =

t t  1/2v t for any orthonormal matrix t.

theorem 7.1 follows as a corollary, since l(z) = l(xy ) so long as

z = xy .

the rank constrained problem is sometimes equivalent to an uncon-
strained problem. note that problem (7.2) is still a hard problem to
solve: it is a rank-constrained semide   nite program. on the other hand,
the same problem without the rank constraint is convex and tractable
(though not easy to solve at scale). in particular, it is possible to write
down an optimality condition for the problem
minimize l(z) +      z    

(7.4)
that certi   es that a matrix z is globally optimal. this problem is a
relaxation of problem (7.2), and so has an optimal value that is at
least as small. furthermore, if any solution to problem (7.4) has rank
no more than k, then it is feasible for problem (7.2), so the optimal
values of problem (7.4) and problem (7.2) must be the same. hence
any solution of problem (7.4) with rank no more than k also solves
problem (7.2).

recall that the matrix z is a solution to problem (7.4) if and only

if

0      (l(z) +      z    ),

76

fitting low rank models

where   f (z) is the subgradient of the function f at z. the subgradient
is a set-valued function.

the subgradient of the nuclear norm for z = u v t is easily seen

to be

    z     = {u v t + w : u t w = 0, w v = 0,  w  2    1}.

de   ne the objective obj(z) = l(z) +      z     . then, if g      l(z) and
u v t + w        z    , we can use the convexity of the objective to see
that

obj(z)    obj(z  )    obj(z) +   g +    u v t +    w, z        z  

   obj(z)       g +    u v t +    w   f  z       z  f ,

using the cauchy   schwarz inequality to obtain the last relation. hence
we might say that   g +    u v t +    w   f bounds the (relative) subopti-
mality of the estimate z. furthermore, z is optimal for problem (7.4)
if and only if 0      obj(z), which means that g +    u v t +    w = 0 for
some g      l(z) and u v t + w        z    .
to    nd the tightest bound on the suboptimality of z, we can min-
imize the bound over valid subgradients g and u v t + w:

f

  g +    u v t +    w   2
minimize
subject to   w  2    1
u t w = 0
w v = 0
g      l(z).

(7.5)

if the optimal value of this program is 0, then z is optimal for prob-
lem (7.4).

this result allows us to (sometimes) certify global optimality of
a particular model. given a model (x, y ), we compute the svd of
the product xy = u v t . solve (7.5). if the optimal value is 0, then
(x, y ) is globally optimal.

if g is    xed, we can rewrite problem 7.5 by decomposing g into a
sum of four parts: g   = u u t gv v t , g    = (i     u u t )g(i     v v t ),
and two parts that do not require names, (i     u u t )gv v t and
u u t g(i     v v t ). noticing that the objective decomposes additively

7.6. global optimality

over the components of g, the optimal w is given by

w    =

g   

max(   ,   g     2) .

if   g +    u v t +    w     2

f = 0, then (x, y ) is globally optimal.

77

(7.6)

8

choosing low rank models

8.1 id173 paths
suppose that we wish to understand the entire id173 path for
a glrm; that is, we would like to know the solution (x(   ), y (   )) to
the problem

j=1   rj(yj)

minimize q(i,j)    lij(xiyj, aij) +    qm

i=1 ri(xi) +    qn

as a function of    . frequently, the id173 path may be com-
puted almost as quickly as the solution for a single value of    . we can
achieve this by initially    tting the model with a very high value for    ,
which is often a very easy problem. (for example, when r and   r are
norms, the solution is (x, y ) = (0, 0) for su ciently large    .) then
we may    t models corresponding to smaller and smaller values of     by
initializing the alternating minimization algorithm from our previous
solution. this procedure is sometimes called a homotopy method.

for example, figure 8.1 shows the id173 path for quadrat-
ically regularized huber pca on a synthetic data set. we generate a
dataset a = xy + s with x    rm   k, y    rk   n, and s    rm   n,
with m = n = 300 and k = 3. the entries of x and y are drawn from
a standard normal distribution, while the entries of the sparse noise

78

8.1. id173 paths

79

matrix s are drawn from a uniform distribution on [0, 1] with proba-
bility 0.05, and are 0 otherwise. we choose a rank for the model that is
higher than the (unobserved) true rank of the data; we will see below
in   8.2 how to choose the right rank for a model.

test error
train error

0.35

0.3

0.25

0.2

0.15

0.1

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

0

0.5

1

1.5
   

2

2.5

3

figure 8.1: id173 path.

we    t a rank 5 glrm to an observation set   consisting of 10% of
the entries in the matrix, drawn uniformly at random from {1, . . . , m}   
{1, . . . , n}, using huber loss and quadratic id173, and vary the
id173 parameter. that is, we    t the model
minimize q(i,j)    huber(xiyj, aij) +    qm
j=1   yj  2
i=1   xi  2
2
and vary the id173 parameter    . the    gure plots both the
normalized training error,

2 +    qn

and the normalized test error,

1

| |   (i,j)   
nm    |  |   (i,j)      

1

huber(xiyj, aij),

huber(xiyj, aij),

80

choosing low rank models

of the    tted model (x, y ), for     ranging from 0 to 3. here, we see
that while the training error decreases and     decreases, the test error
reaches a minimum around     = .5. interestingly, it takes only three
times longer (about 3 seconds) to generate the entire id173
path than it does to    t the model for a single value of the id173
parameter (about 1 second).

8.2 choosing model parameters

to form a generalized low rank model, one needs to specify the loss
functions lj, regularizers r and   r, and a rank k. the id168
should usually be chosen by a domain expert to re   ect the intuitive
notion of what it means to       t the data well   . on the other hand, the
regularizers and rank are often chosen based on statistical considera-
tions, so that the model generalizes well to unseen (missing) data.

there are three major considerations to balance in choosing the
id173 and rank of the model. in the following discussion, we
suppose that the regularizers r =    r0 and   r =      r0 have been chosen
up to a scaling    .

compression. a low rank model (x, y ) with rank k and no spar-
sity represents the data table a with only (m + n)k nonzeros, achiev-
ing a compression ratio of (m + n)k/(mn). if the factors x or y are
sparse, then we have used fewer than (m + n)k numbers to represent
the data a, achieving a higher compression ratio. we may want to pick
parameters of the model (k and    ) in order to achieve a good error

q(i,j)    lj(aij     xiyj) for a given compression ratio. for each possible

combination of model parameters, we can    t a low rank model with
those parameters, observing both the error and the compression ratio.
we can then choose the best model parameters (highest compression
rate) achieving the error we require, or the best model parameters (low-
est error rate) achieving the compression we require.

more formally, one can construct an information criterion for low
rank models by analogy with the aikake information criterion (aic)
or the bayesian information criterion (bic). for use in the aic, the

8.2. choosing model parameters

81

number of degrees of freedom in a low rank model can be computed as
the di erence between the number of nonzeros in the model and the
dimensionality of the symmetry group of the problem. for example,
if the model (x, y ) is dense, and the regularizer is invariant under
orthogonal transformations (e.g., r(x) =   x  2
2), then the number of
degrees of freedom is (m + n)k     k2 [142]. minka [101] proposes a
method based on the bic to automatically choose the dimensionality
in pca, and observes that it performs better than cross validation in
identifying the true rank of the model when the number of observations
is small (m, n . 100).

denoising. suppose we observe every entry in a true data matrix
contaminated by noise, e.g., aij = atrue
ij +    ij, with    ij some random
variable. we may wish to choose model parameters to identify the
truth and remove the noise: we would like to    nd k and     to minimize

q(i,j)    lj(atrue

ij     xiyj).

a number of commonly used rules-of-thumb have been proposed in
the case of pca to distinguish the signal (the true rank k of the data)
from the noise, some of which can be generalized to other low rank
models. these include using scree plots, often known as the    elbow
method    [25]; the eigenvalue method; horn   s parallel analysis [61, 42];
and other related methods [162, 115]. a recent, more sophisticated
method adapts the idea of dropout training [137] to regularize low-
rank matrix estimation [68].

some of these methods can easily be adapted to the glrm con-
text. the    elbow method    increases k until the objective value de-
creases less than linearly; the eigenvalue method increases k until the
objective value decreases by less than some threshold; horn   s parallel
analysis increases k until the objective value compares unfavorably to
one generated by    tting a model to data drawn from a synthetic noise
distribution.

cross validation is also simple to apply, and is discussed further
below as a means of predicting missing entries. however, applying cross
validation to the denoising problem is somewhat tricky, since leaving
out too few entries results in over   tting to the noise, while leaving out

82

choosing low rank models

too many results in under   tting to the signal. the optimal number of
entries to leave out may depend on the aspect ratio of the data, as
well as on the type of noise present in the data [113], and is not well
understood except in the case of gaussian noise [108]. we explore the
problem of choosing a holdout size numerically below.

predicting missing entries. suppose we observe some entries in the
matrix and wish to predict the others. a glrm with a higher rank
will always be able to    t the (noisy) data better than one of lower rank.
however, a model with many parameters may also over   t to the noise.
similarly, a glrm with no id173 (    = 0) will always produce

a model with a lower empirical loss q(i,j)    lj(xiyj, aij). hence, we

cannot pick a rank k or id173     simply by considering the
objective value obtained by    tting the low rank model.

but by resampling from the data, we can simulate the performance
of the model on out of sample (missing) data to identify glrms that
neither over nor under   t. here, we discuss a few methods for choosing
model parameters by cross-validation; that is, by resampling from the
data to evaluate the model   s performance. cross validation is commonly
used in regression models to choose parameters such as the regulariza-
tion parameter    , as in figure 8.1. in glrms, cross validation can
also be used to choose the rank k. indeed, using a lower rank k can be
considered another form of model id173.

we can distinguish between three sources of noise or variability in

the data, which give rise to three di erent resampling procedures.

    the rows or columns of the data are chosen at random, i.e., drawn
iid from some population. in this case it makes sense to resample
the rows or columns.
    the rows or columns may be    xed, but the indices of the observed
entries in the matrix are chosen at random. in this case, it makes
sense to resample from the observed entries in the matrix.
    the indices of the observed entries are    xed, but the values are
observed with some measurement error. in this case, it makes
sense to resample the errors in the model.

8.2. choosing model parameters

83

each of these leads to a di erent reasonable kind of resampling
scheme. the    rst two give rise to resampling schemes based on cross
validation (i.e., resampling the rows, columns, or individual entries of
the matrix) which we discuss further below. the third gives rise to
resampling schemes based on the bootstrap or jackknife procedures,
which resample from the errors or residuals after    tting the model. a
number of methods using the third kind of resampling have been pro-
posed in order to perform id136 (i.e., generate con   dence intervals)
for pca; see josse et al. [69] and references therein.
as an example, let us explore the e ect of varying | |/mn,    , and k.
we generate random data as follows. let x    rm   ktrue, y    rktrue   n,
and s    rm   n, with m = n = 300 and ktrue = 3. draw the entries of
x and y from a standard normal distribution, and draw the entries of
the sparse outlier matrix s are drawn from a uniform distribution on
[0, 3] with id203 0.05, and are 0 otherwise. form a = xy + s.
select an observation set   by picking entries in the matrix uniformly
at random from {1, . . . , n}   { 1, . . . , m}. we    t a rank k glrm with
huber loss and quadratic id173           2
2, varying | |/mn,    , and
k, and compute the test error. we average our results over 5 draws
from the distribution generating the data.

in figure 8.2, we see that the true rank k = 3 performs best on
cross-validated error for any number of observations | |. (here, we
show performance for     = 0. the plot for other values of the regular-
ization parameter is qualitatively the same.) interestingly, it is easiest
to identify the true rank with a small number of observations: higher
numbers of observations make it more di cult to over   t to the data
even when allowing higher ranks.

in figure 8.3, we consider the interdependence of our choice of    
and k. id173 is most important when few matrix elements
have been observed: the curve for each k is nearly    at when more than
about 10% of the entries have been observed, so we show here a plot
for | | = .1mn. here, we see that the true rank k = 3 performs best
on cross-validated error for any value of the id173 parameter.
ranks that are too high (k > 3) bene   t from increased id173
   , whereas higher id173 hurts the performance of models with

84

choosing low rank models

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

| |/mn=0.1
| |/mn=0.3
| |/mn=0.5
| |/mn=0.7
| |/mn=0.9

1

2

3
k

4

5

figure 8.2: test error as a function of k, for     = 0.

k=1
k=2
k=3
k=4
k=5

1

0.8

0.6

0.4

0.2

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

0

1

2

   

3

4

5

figure 8.3: test error as a function of     when 10% of entries are observed.

8.3. on-line optimization

85

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

k=1
k=2
k=3
k=4
k=5

0.1

0.3

0.5
| |/mn

0.7

0.9

figure 8.4: test error as a function of observations | |/mn, for     = 0.

k lower than the true rank. that is, regularizing the rank (small k) can
substitute for explicit id173 of the factors (large    ).

finally, in figure 8.4 we consider how the    t of the model depends
on the number of observations. if we correctly guess the rank k = 3, we
   nd that the    t is insensitive to the number of observations. if our rank
is either too high or too low, the    t improves with more observations.

8.3 on-line optimization

suppose that new examples or features are being added to our data
set continuously, and we wish to perform on-line optimization, which
means that we should have a good estimate at any time for the repre-
sentations of those examples xi or features yj which we have seen. this
model is equivalent to adding new rows or columns to the data table
a as the algorithm continues. in this setting, alternating minimization
performs quite well, and has a very natural interpretation. given an

86

choosing low rank models

estimate for y , when a new example is observed in row i, we may solve

minimize qj:(i,j)    lij(aij, xyj) + ri(x)

with variable x to compute a representation for row i. this computation
is exactly the same as one step of alternating minimization. here, we
are    nding the best feature representation for the new example in terms
of the (already well understood) archetypes y . if the number of other
examples previously seen is large, the addition of a single new example
should not change the optimal y by very much; hence if (x, y ) was
previously the global minimum of (4.1), this estimate of the feature
representation for the new example will be very close to its optimal
representation (i.e., the one that minimizes problem (4.1)). a similar
interpretation holds when new columns are added to a.

9

implementations

the authors and collaborators have developed and released four open
source codes for modeling and    tting generalized low rank models:

    a serial implementation written in python;
    a fully featured serial and shared-memory parallel implementa-
tion written in julia;
    a basic distributed implementation written in scala using the
spark framework; and
    a distributed implementation written in java using the h2o
framework, with java and r interfaces.

the julia, spark and h2o implementations use the alternating proxi-
mal gradient method described in   7 to    t glrms, while the python
implementation uses alternating minimization and a cvxpy [39] back-
end for each subproblem. the python implementation is suitable for
problems with no more than a few hundred rows and columns. the
julia implementation is suitable for problems that    t in memory on a
single computer, including those with thousands of columns and mil-
lions of rows. the h2o and spark implementations must be used for

87

88

implementations

larger problem sizes. for most uses, we recommend the julia imple-
mentation or the h2o implementation. as of march 2016, the julia
implementation is the most fully featured, with an ample library of
losses and regularizers, as well as routines to cross validate, impute,
and test goodness-of-   t. for a full description and up-to-date informa-
tion about available functionality of each of these implementations, we
encourage the reader to consult the on-line documentation for each of
these packages.

there are also many implementations available for    tting special
cases of glrms. for example, an implementation capable of    tting
any glrm for which the subproblems in an alternating minimization
method are quadratic programs was recently developed in spark by
debasish das and santanu das [32].

in this section we brie   y discuss the python, julia, and spark
implementations, and report some timing results. the h2o imple-
mentation will not be discussed below; documentation and tutori-
als are available at http://learn.h2o.ai/content/tutorials/glrm/
glrm-tutorial.html.

9.1 python implementation
glrm.py is a python implementation for    tting glrms that can
be found, together with documentation, at https://github.com/
cehorn/glrm.

usage. the user initializes a glrm by specifying

    the data table a (a), stored as a python list of 2-d arrays, where
each 2-d array in a contains all data associated with a particular
id168,
    the list of id168s l (lj, j = 1, . . . , n), that correspond to
the data as speci   ed by a,
    regularizers regx (r) and regy (  r),
    the rank k (k),

9.1. python implementation

89

    an optional list missing_list with the same length as a so that
each entry of missing_list is a list of missing entries correspond-
ing to the data from a, and
    an optional convergence object converge that characterizes the
stopping criterion for the alternating minimization procedure.

the following example illustrates how to use glrm.py to    t a glrm
with boolean (a_bool) and numerical (a_real) data, with quadratic
id173 and a few missing entries.
from glrm import glrm
from glrm.loss import quadraticloss, hingeloss
from glrm.reg import quadraticreg

# import model
# and losses
# and regularizer

a = [a_bool, a_real]
l = [hinge_loss, quadraticloss]
regx = quadraticreg(0.1)
regy = quadraticreg(0.1)
missing_list = [[], [(0,0), (0,1)]]

# data (as list)
# losses (as list)
# penalty scale 0.1

# missing entries

model = glrm(a, l, regx, regy, k, missing_list)
model.fit()

# initialize glrm
# fit glrm

the fit() method automatically adds an o set to the glrm and
scales the id168s as described in   4.3.

glrm.py    ts glrms by alternating minimization. the code in-
stantiates cvxpy problems [39] corresponding to the x- and y -update
steps, then iterates by alternately solving each problem until conver-
gence criteria are met.

the following id168s and regularizers are supported by

glrm.py:

    quadratic loss quadraticloss,
    huber loss huberloss,
    hinge loss hingeloss,
    ordinal loss ordinalloss,
    no id173 zeroreg,

90

implementations

      1 id173 linearreg,
    quadratic id173 quadraticreg, and
    nonnegative constraint nonnegativereg.

users may implement their own id168s (regularizers) using the
abstract class loss (reg).

julia implementation

9.2
lowrankmodels is a code written in julia [9] for modeling and
   tting glrms. the implementation is available on-line at https:
//github.com/madeleineudell/lowrankmodels.jl. we discuss some
aspects of the usage and features of the code here.

order:

j=1 dj is the embedding dimension of the model (see   6).

to form a glrm using lowrankmodels, the user speci   es, in

usage. the lowrankmodels package transposes some of the no-
tation from this paper for computational speed. it approximates the
m    n data table a by a model x t y , where x    rk   m and y    rk   n.
for most glrms, d = n, but for multidimensional id168s,
d =qn
    a: the data (a), which can be any array or array-like data struc-
ture (e.g., a sparse matrix, or a julia dataframe);
    losses: either one id168 to be applied to every entry of a;
or a list of id168s (lj, j = 1, . . . , n), one for each column
of a;
    rx: a regularizer (r) on the rows of x
    ry: a regularizer (  r) on the columns of y ; or a list of regularizers
(  rj, j = 1, . . . , n), one for each column of a, and
    k: the rank (k),

and optional named arguments:

9.2. julia implementation

91

    the observed entries obs ( ), a list of tuples of the indices of the
observed entries in the matrix, which may be omitted if all the
entries in the matrix have been observed;
    initial values x (x) and y (y )
    if offset is true, an o set will be added to the model for each
column; it is false by default
    if scale is true, the losses for each column are scaled as in   4.3;
it is false by default.
    if sparse_na is true, the data matrix a is given as a sparse
matrix, and the keyword argument obs is omitted, implicit zeros
of a will be interpreted as missing entries; sparse_na is true by
default.

for example, the following code forms and    ts a id116 model
with k = 5 on the entries of the matrix a    rm   n in the observation
set obs.
losses = fill(quadratic(),n)
rx = unitonesparse()
ry = zeroreg()
glrm = glrm(a,losses,rx,ry,k,obs=obs)
x,y,ch = fit!(glrm)

# quadratic loss
# x is 1-sparse unit vector
# y is not regularized
# form glrm
# fit glrm

lowrankmodels uses the proximal gradient method described
in   7.2 to    t glrms. the optimal model is returned in the factors x
and y, while ch gives the convergence history. the exclamation mark
su x is a naming convention in julia, and denotes that the function
mutates at least one of its arguments. in this case, it caches the best
   t x and y as glrm.x and glrm.y [27].

losses and regularizers must be of type loss and regularizer,
respectively, and may be chosen from a list of supported losses and
regularizers, shown in table 9.1 and table 9.2 respectively. in the ta-
bles, w, c, and d are parameters: w is the weight of the id168,
and takes default value 1; c is the relative importance of false positive
examples compared to false negative examples, and has default value
1; and d is the number of levels of an ordinal or categorical variable.
users may also implement their own losses and regularizers.

92

loss
quadratic
  1
huber
poisson
logistic
hinge
weighted hinge

ordinal hinge

multinomial

ordinal

multinomial

implementations

code
quadloss(w)
l1loss(w)
huberloss(w)
poissonloss(w)
logisticloss(w)
hingeloss(w)
weightedhingeloss(w,c)

l(u, a)
w(u     a)2
w|u     a|
w huber(u     a)
w(exp(u)     au)
w log(1 + exp(   au))
w max(1     au, 0)
(   (a =    1) + c   (a = 1))
   w(1     au)+
wqa   1
a  =1(1     u + a  )+
+wqd
a  =a+1(1 + u     a  )+
multinomialordinalloss(w,d) w1qa   1
i=1 ui    qd
a  =1 exp(qa     1
+ log(qd
i=a   ui))2
   qd   1
    log3
a  =1 exp(ua   )4
qd

ordinalhingeloss(w,d)

multinomialloss(w,d)

i=a ui

exp(ua)

i=1 ui

table 9.1: id168s available in the lowrankmodels julia package. here
    is a function that returns 1 if its argument is true and 0 otherwise.

regularizer
nothing
quadratic
  1
nonnegative
1-sparse
clustered
mixture

code
zeroreg()
quadreg(w)
onereg(w)
nonnegconstraint()
onesparseconstraint()
unitonesparseconstraint()
simplexconstraint()

r(x)
0
w  x  2
2
w  x  1
i+(x)
i1(x)

i1(x) + i(qk
i+(x) + i(qk

l=1 xl = 1)
l=1 xl = 1)

table 9.2: regularizers available in the lowrankmodels julia package. here
i+ is the indicator of the nonnegative orthant, i1 is the indicator of the 1-sparse
vectors, and i is a function that returns 0 if its argument is true and    otherwise.

shared memory parallelism. lowrankmodels takes advantage of
julia   s sharedarray data structure to implement a shared-memory

9.2. julia implementation

93

parallel    tting procedure. while julia does not yet support threading,
sharedarrays in julia allow separate processes on the same computer
to access the same block of memory at the same time. to    t a model
using multiple processes, lowrankmodels loads the data a and the
initial model x and y into shared memory, broadcasts other problem
data (e.g., the losses and regularizers) to each process, and assigns to
each process a partition of the rows of x and columns of y . at ev-
ery iteration, each process updates its rows of x, its columns of y , and
computes its portion of the objective function, synchronizing after each
of these steps to ensure that, e.g., the x update is completed before
the y update begins; then the master process checks a convergence
criterion and adjusts the step length.

automatic modeling. lowrankmodels is capable of adding o -
sets to a glrm, and of automatically scaling the id168s,
as described in   4.3. it can also pick appropriate id168s for
columns whose types are speci   ed in an array datatypes whose ele-
ments take the values :real, :bool, :ord, or :cat. using these fea-
tures, lowrankmodels implements a method

glrm(dataframe, k, datatypes)

that forms a rank k model on a data frame with datatypes speci   ed in
the array datatypes. this function automatically selects id168s
and id173 that suit the data well, and ignores any missing (na)
element in the data frame. this glrm can then be    t with the function
fit!. by default, the four data types are    t with quadratic loss, logistic
loss, multinomial ordinal loss, and ordinal loss, respectively, but other
mappings can be speci   ed by setting the keyword argument loss_map
to a dictionary mapping datatypes to id168s.

example. as an example, we    t a glrm to the motivational states
questionnaire (msq) data set [121]. this data set measures 3896 sub-
jects on 92 aspects of mood and personality type, as well as recording
the time of day the data were collected. the data include real-valued,
boolean, and ordinal measurements, and approximately 6% of the mea-
surements are missing (na).

94

implementations

the following code loads the msq data set and encodes it in two

dimensions:

using rdatasets
using lowrankmodels
# pick a data set
df = rdatasets.dataset("psych","msq")
# encode it!
x,y,labels,ch = fit(glrm(df,2))

figure 9.1 uses the rows of y as a coordinate system to plot some of
the features of the data set. here we see the automatic embedding sepa-
rates positive from negative emotions along the y axis. this embedding
is notable for being interpretable despite having been generated com-
pletely automatically. of course, better embeddings may be obtained
by a more careful choice of id168s, regularizers, scaling, and
rank k.

9.3 spark implementation

sparkglrm is a code written in scala, built on the spark cluster pro-
gramming framework [160], for modelling and    tting glrms. the im-
plementation is available on-line at http://git.io/glrmspark.

design.
in sparkglrm, the data matrix a is split entry-wise across
many machines, just as in [60]. the model (x, y ) is replicated and
stored in memory on every machine. thus the total computation time
required to    t the model is proportional to the number of nonzeros
divided by the number of cores, with the restriction that the model
should    t in memory. (the authors leave to future work an extension
to models that do not    t in memory, e.g., by using a parameter server
[125].) where possible, hardware acceleration (via breeze and blas)
is used for local id202ic operations.

at every iteration, the current model is broadcast to all machines, so
there is only one copy of the model on each machine. this particularly
important in machines with many cores, because it avoids duplicating

9.3. spark implementation

95

2

1

y

0

-1

atease

con(cid:1)dent

quiet

content

warmhearted

energetic

delighted

aroused

intense

excited

ashamed

fearful

surprised

astonished

scornful
angry

guilty

scared

-2

-0.2

afraid

-0.1

0.0
x

0.1

0.2

figure 9.1: an automatic embedding of the msq [121] data set into two dimen-
sions.

the model on those machines. each core on a machine will process a
partition of the input matrix, using the local copy of the model.

usage. the user provides id168s lij(u, a) indexed by i =
0, . . . , m     1 and j = 0, . . . , n     1, so a di erent id168 can be
de   ned for each column, or even for each entry. each id168 is
de   ned by its gradient (or a subgradient). the method signature is

loss_grad(i: int, j: int, u: double, a: double)

96

implementations

whose implementation can be customized by particular i and j. as an
example, the following line implements squared error loss (l(u, a) =
1/2(u     a)2) for all entries:

u - a

similarly, the user provides functions implementing the proximal
operator of the regularizers r and   r, which take a dense vector and
perform the appropriate proximal operation.

experiments. we ran experiments on several large matrices. for size
comparison, a very popular matrix in the recommender systems com-
munity is the net   ix prize matrix, which has 17770 rows, 480189
columns, and 100480507 nonzeros. below we report results on several
larger matrices, up to 10 times larger. the matrices are generated by
   xing the dimensions and number of nonzeros per row, then uniformly
sampling the locations for the nonzeros, and    nally    lling in those lo-
cations with a uniform random number in [0, 1].

we report iteration times using an amazon ec2 cluster with 10
slaves and one master, of instance type    c3.4xlarge". each machine has
16 cpu cores and 30 gb of ram. we ran sparkglrm to    t two glrms
on matrices of varying sizes. table 9.3 gives results for quadratically
regularized pca (i.e., quadratic loss and quadratic id173) with
k = 5. to illustrate the capability to write and    t custom id168s,
we also    t a glrm using a id168 that depends on the parity of
i + j:

lij(u, a) =i |u     a|
(u     a)2

i + j is even
i + j is odd,

matrix size # nonzeros time per iteration (s)
106     106
106     106
107     107
table 9.3: sparkglrm for quadratically regularized pca, k = 5.

7
11
227

106
109
109

9.3. spark implementation

97

matrix size # nonzeros time per iteration (s)
106     106
106     106
107     107

9
13
294

106
109
109

table 9.4: sparkglrm for custom glrm, k = 10.

with r(x) =   x  1 and   r(y) =   y  2
2, setting k = 10. (this id168
was chosen merely to illustrate the generality of the implementation.
usually losses will be the same for each entry in the same column.) the
results for this custom glrm are given in table 9.4.

the table gives the time per iteration. the number of iterations
required for convergence depends on the size of the ambient dimension.
on the matrices with the dimensions shown in tables 9.3 and 9.4,
convergence typically requires about 100 iterations, but we note that
useful glrms often emerge after only a few tens of iterations.

acknowledgments

the authors are grateful to chris de sa, yash deshpande, nicolas
gillis, maya gupta, trevor hastie, irene kaplow, tamara kolda, lester
mackey, andrea montanari, art owen, haesun park, david price,
chris r  , ben recht, yoram singer, nati srebro, ashok srivastava,
peter stoica, sze-chuan suen, stephen taylor, joel tropp, ben van
roy, and stefan wager for a number of illuminating discussions and
comments on early drafts of this paper; to debasish das and matei za-
haria for their insights into creating a successful spark implementation;
and to anqi fu and h2o.ai for their work on the h2o implementa-
tion. this work was developed with support from the national science
foundation graduate research fellowship program (under grant no.
dge-1147470), the gabilan stanford graduate fellowship, the gerald
j. lieberman fellowship, and the darpa x-data program.

98

appendices

a

examples, id168s, and regularizers

a.1 quadratically regularized pca
in this appendix we describe some properties of the quadratically reg-
ularized pca problem (2.3),

f +      x  2

f +      y   2
f .

minimize   a     xy   2

(a.1)
in the sequel, we let u v t = a be the svd of a and let r be the rank
of a. we assume for convenience that all the nonzero singular values
   1 >    2 >        >    r > 0 of a are distinct.
a.1.1 solution
a solution is given by

x =   u    1/2,

y =    1/2   v t ,

(a.2)
diag((   1    
to prove this, let us consider the optimality conditions of (2.3). the

where   u and   v are de   ned as in (2.5), and    =
   )+, . . . , (   k        )+).
optimality conditions are

   (a     xy )y t +    x = 0,

   (a     xy )t x +    y t = 0.

100

a.1. quadratically regularized pca

101

multiplying the    rst optimality condition on the left by x t and the
second on the left by y and rearranging, we    nd

x t (a     xy )y t =    x t x,

y (a     xy )t x =    y y t ,

which shows, by taking a transpose, that x t x = y y t at any station-
ary point.

we may rewrite the optimality conditions together as

c      i
at       idc x

y td = c

a

0

(xy )t

xy

0 dc x
y td

= c x(y y t )
y t (x t x)d
y td (x t x),
= c x

where we have used the fact that x t x = y y t .

a

a

a

now we see that (x, y t ) lies in an invariant subspace of the matrix

and (x, y t ) must span the corresponding eigenspace. more concretely,

c      i
at       id. recall that v is an invariant subspace of a matrix a if
av = v m for some matrix m. if rank(m)    rank(a), we know that
the eigenvalues of m are eigenvalues of a, and that the corresponding
eigenvectors lie in the span of v .
at       id,
thus the eigenvalues of x t x must be eigenvalues ofc      i
at       id is (symmetric, and therefore) diagonalizable,
notice thatc      i
with eigenvalues              i. the larger eigenvalues        +    i correspond
to the eigenvectors (ui, vi), and the smaller ones               i to (ui,   vi).
now, x t x is positive semide   nite, so the eigenvalues shared by
x t x andc      i
at       id must be positive. hence there is some set | |  
k with    i        for i      such that x has singular values          +    i for
i     . (recall that x t x = y y t , so y has the same singular values as
x.) then (x, y t ) spans the subspace generated by the vectors (ui, vi
for i     . we say the stationary point (x, y ) has active subspace  .
it is easy to verify that xy =qi    ui(   i        )vt

i .

a

102

examples, id168s, and regularizers

each active subspace gives rise to an orbit of stationary points. if
(x, y ) is a stationary point, then (xt, t    1y ) is also a stationary point
so long as
   (a    xy )y t t    t +    xt = 0,
   (a    xy )t xt +    y t t    t = 0,
which is always true if t    t = t, i.e., t is orthogonal. this shows that
the set of stationary points is invariant under orthogonal transforma-
tions.

to simplify what follows, we choose a representative element for
each orbit. represent any stationary point with active subspace   by

x = u (          i)1/2,

y = (           i)1/2v t  ,

where by u  we denote the submatrix of u with columns indexed by  ,
and similarly for   and v . at any value of    , let k  (   ) = max{i :    i   
i " (representative) stationary points, one
   }. then we have qk
for each choice of   the number of (representative) stationary points
is decreasing in    ; when    >    1, the only stationary point is x = 0,
y = 0.

i=0!k  (   )

these stationary points can have quite di erent values. if (x, y )

f + ||y ||2

f +    (||x||2

has active subspace  , then
||a     xy ||2
from this form, it is clear that we should choose   to include the top
singular values i = 1, . . . , k  (   ). choosing any other subset   will result
in a higher (worse) objective value: that is, the other stationary points
are not global minima.

i +  i   1   2 + 2   |   i        |2 .

f ) =  i /   

   2

a.1.2 fixed points of alternating minimization
theorem a.1. the quadratically regularized pca problem (2.3) has
only one local minimum, which is the global minimum.

our proof is similar to that of [6], who proved a related theorem

for the case of pca (2.2).
proof. we showed above that every stationary point of (2.3) has the
i , with      { 1, . . . , k  }, | |   k, and di =    i      .

form xy =qi    uidivt

a.1. quadratically regularized pca

103

i

we use the representative element from each stationary orbit described
above, so each column of x is ui  di and each row of y is   divt
for
some i     . the columns of x are orthogonal, as are the rows of y .
if a stationary point is not the global minimum, then    j >    i for
some i     , j        . below, we show we can always    nd a descent direc-
tion if this condition holds, thus showing that the only local minimum
is the global minimum.

assume we are at a stationary point with    j >    i for some i     ,
j        . we will    nd a descent direction by perturbing xy in direction
j . form   x by replacing the column of x containing ui  di by (ui +
ujvt
   uj)  di, and   y by replacing the row of y containing   divt
i by   di(vi+
   vj)t . now the id173 term increases slightly:

   (     x  2
f +      y   2
=   i     ,i     =i

f +   y   2
f )

f )        (  x  2
(2   ti  ) + 2   di(1 +    2)       i     

= 2   di   2.

2   ti  

f

j     (ui +    uj)di(vi +    vj)t  2
j        uidivt

i + uj(   j        2di)vt

f       a     xy   2
i + uj   jvt

meanwhile, the approximation error decreases:
  a       x   y   2
=   ui   ivt
=   ui(   i     di)vt
    (   i     di)2        2
=.....
c   i     di
      di
      di
= (   i     di)2 + (   j        2di)2 + 2   2d2
=    2   j   2di +    4d2
= 2   2di(di        j) +    4d2
i ,

   j        2did.....

    (   i     di)2        2

i + 2   2d2

2

f

j

j

i

i     (   i     di)2        2

j

f     (   i     di)2        2

j

i   2
j        ujdivt

f

where we have used the rotational invariance of the frobenius norm to
arrive at the third equality above. hence the net change in the objective
value in going from (x, y ) to (   x,   y ) is

2   di   2 + 2   2di(di        j) +    4d2

i = 2   2di(    + di        j) +    4d2

= 2   2di(   i        j) +    4d2
i ,

i

104

examples, id168s, and regularizers

which is negative for small    . hence we have found a descent direction,
showing that any stationary point with    j >    i for some i     , j        
is not a local minimum.

data type loss
real
real
real
real
boolean
boolean
boolean
integer
ordinal

ordinal

l(u, a)
(u     a)2
|u     a|
huber(u     a)
   (a     u)+ + (1        )(u     a)+
log(1 + exp(   au))
(1     ua)+

quadratic
absolute value
huber
quantile
logistic
hinge
weighted hinge (   (a =    1) + c   (a = 1))(1     au)+
poisson
exp(u)     au + a log a     a
ordinal hinge qa   1
a  =1(1     u + a  )++
qd
a  =a+1(1 + u     a  )+
i=1 ui    qd   1
multinomial qa   1
i=a ui+
log1qd   1
a  =1 exp1qa     1
i=1 uiqd   1
(1     ua)+ +qa     =a(1 + ua  )+
   (ua    = 1) +qa     =a    (ua      =    1)
a  =1 exp(ua   )4
    log3
qd

ordinal

exp(ua)

categorical one-vs-all
categorical hamming
categorical multinomial

i=a   ui22

params dim

tilt    

1
1
1
1
1
1
weight c 1
1
1

levels d

levels d

d     1

levels d
levels d
levels d

d

d

d

table a.1: a few id168s. here     is a function that returns 1 if its argument
is true and 0 otherwise. params shows the parameters of the id168, and dim
gives its embedding dimension.

a.1. quadratically regularized pca

105

regularizer
nothing
quadratic
  1
nonnegative
nonnegative   1 regularized
orthogonal nonnegative
s-sparse
clustered
mixture

r(x)
0
  x  2
2
  x  1
i+(x)

  x  1 + i+(x)
i1(x) + i+(x)
card(x)    s

i1(x) + i(qk
i+(x) + i(qk

l=1 xl = 1)
l=1 xl = 1)

table a.2: a few regularizers. here i+ is the indicator of the nonnegative orthant,
i1 is the indicator of the 1-sparse vectors, and i is a function that returns 0 if its
argument is true and    otherwise.

references

[1] j. abernethy, f. bach, t. evgeniou, and j.-p. vert. a new approach to
collaborative    ltering: operator estimation with spectral id173.
the journal of machine learning research, 10:803   826, 2009.

[2] a. agarwal, a. anandkumar, p. jain, and p. netrapalli. learning
sparsely used overcomplete dictionaries via alternating minimization.
arxiv preprint arxiv:1310.7991, 2013.

[3] p. k. agarwal and n. h. mustafa. id116 projective id91. in
proceedings of the 23rd acm sigmod-sigact-sigart symposium
on principles of database systems, pages 155   165. acm, 2004.

[4] m. aharon, m. elad, and a. bruckstein. k-svd: an algorithm for
ieee

designing overcomplete dictionaries for sparse representation.
transactions on signal processing, 54(11):4311   4322, 2006.

[5] d. arthur and s. vassilvitskii. id116++: the advantages of careful
seeding. in proceedings of the eighteenth annual acm-siam sympo-
sium on discrete algorithms, pages 1027   1035. society for industrial
and applied mathematics, 2007.

[6] p. baldi and k. hornik. neural networks and principal component anal-
ysis: learning from examples without local minima. neural networks,
2(1):53   58, 1989.

[7] m. berry, m. browne, a. langville, v. pauca, and r. plemmons. algo-
rithms and applications for approximate nonnegative matrix factoriza-
tion. computational statistics & data analysis, 52(1):155   173, 2007.

106

references

107

[8] d. p. bertsekas. incremental gradient, subgradient, and proximal meth-
ods for id76: a survey. optimization for machine learn-
ing, 2010:1   38, 2011.

[9] j. bezanson, s. karpinski, v. b. shah, and a. edelman.

a fast dynamic language for technical computing.
arxiv:1209.5145, 2012.

julia:
arxiv preprint

[10] v. bittorf, b. recht, c. r  , and j. a. tropp. factoring nonnegative ma-
trices with linear programs. advances in neural information processing
systems, 25:1223   1231, 2012.

[11] j. bolte, s. sabach, and m. teboulle. proximal alternating linearized
minimization for nonconvex and nonsmooth problems. mathematical
programming, pages 1   36, 2013.

[12] j. borwein and a. lewis. convex analysis and nonlinear optimization:
theory and examples, volume 3. springer science & business media,
2010.

[13] r. boyd, b. drake, d. kuang, and h. park. smallk is a c++/python
high-performance software library for nonnegative id105
(nmf) and hierarchical and    at id91 using the nmf; current ver-
sion 1.2.0. http://smallk.github.io/, june 2014.

[14] s. boyd, c. cortes, m. mohri, and a. radovanovic. accuracy at the top.
in advances in neural information processing systems, pages 962   970,
2012.

[15] s. boyd and j. mattingley. branch and bound methods. lecture notes

for ee364b, stanford university, 2003.

[16] s. boyd, n. parikh, e. chu, b. peleato, and j. eckstein. distributed op-
timization and statistical learning via the alternating direction method
of multipliers. foundations and trends in machine learning, 3(1):1   
122, 2011.

[17] s. boyd and l. vandenberghe. id76. cambridge uni-

versity press, 2004.

[18] s. boyd, l. xiao, and a. mutapcic. subgradient methods. lecture notes

for ee364b, stanford university, 2003.

[19] s. burer and r. monteiro. a nonid135 algorithm for
solving semide   nite programs via low-rank factorization. mathematical
programming, 95(2):329   357, 2003.

[20] s. burer and r. d. c. monteiro. local minima and convergence in
low-rank semide   nite programming. mathematical programming, 103,
2005.

108

references

[21] e. cand  s, x. li, y. ma, and j. wright. robust principal component

analysis? journal of the acm (jacm), 58(3):11, 2011.

[22] e. cand  s and y. plan. matrix completion with noise. corr,

abs/0903.3131, 2009.

[23] e. cand  s and b. recht. exact matrix completion via convex optimiza-

tion. corr, abs/0805.4471, 2008.

[24] e. cand  s and t. tao. the power of convex relaxation: near-
optimal matrix completion. ieee transactions on id205,
56(5):2053   2080, 2010.

[25] raymond b cattell. the scree test for the number of factors. multi-

variate behavioral research, 1(2):245   276, 1966.

[26] s. chatterjee. matrix estimation by universal singular value threshold-

ing. the annals of statistics, 43(1):177   214, 2014.

[27] j. chen and a. edelman. parallel pre   x polymorphism permits paral-
lelization, presentation & proof. arxiv preprint arxiv:1410.6449, 2014.
[28] s. chen, d. donoho, and m. saunders. atomic decomposition by basis

pursuit. siam journal on scienti   c computing, 20(1):33   61, 1998.

[29] m. collins, s. dasgupta, and r. schapire. a generalization of principal
component analysis to the exponential family. in advances in neural
information processing systems, volume 13, page 23, 2001.

[30] k. crammer and y. singer. on the algorithmic implementation of mul-
ticlass kernel-based vector machines. the journal of machine learning
research, 2:265   292, 2002.

[31] a. daid113 and y. sun. random projections for non-negative matrix

factorization. arxiv preprint arxiv:1405.4275, 2014.

[32] d. das and s. das. quadratic programing solver for non-negative matrix

factorization with spark. in spark summit 2014, 2014.

[33] a. d   aspremont, l. el ghaoui, m. i. jordan, and g. r. lanckriet. a
direct formulation for sparse pca using semide   nite programming. in
advances in neural information processing systems, volume 16, pages
41   48, 2004.

[34] m. davenport, y. plan, e. berg, and m. wootters. 1-bit matrix com-

pletion. arxiv preprint arxiv:1209.3672, 2012.

[35] j. de leeuw. the gi    system of nonlinear multivariate analysis. data

analysis and informatics, 3:415   424, 1984.

references

109

[36] j. de leeuw and p. mair. gi    methods for optimal scaling in r: the

package homals. journal of statistical software, pages 1   30, 2009.

[37] j. de leeuw, f. young, and y. takane. additive structure in qualitative
data: an alternating least squares method with optimal scaling features.
psychometrika, 41(4):471   503, 1976.

[38] c. de sa, k. olukotun, and c. r  . global convergence of stochas-
tic id119 for some nonconvex matrix problems. corr,
abs/1411.1134, 2014.

[39] s. diamond, e. chu, and s. boyd. cvxpy: a python-embedded model-
ing language for id76, version 0.2. http://cvxpy.org/,
may 2014.

[40] t. dietterich and g. bakiri. solving multiclass learning problems via

error-correcting output codes. corr, cs.ai/9501101, 1995.

[41] c. ding, t. li, w. peng, and h. park. orthogonal nonnegative matrix t-
factorizations for id91. in proceedings of the 12th acm sigkdd
international conference on knowledge discovery and data mining,
pages 126   135. acm, 2006.

[42] a. dinno. implementing horn   s parallel analysis for principal compo-

nent analysis and factor analysis. stata journal, 9(2):291, 2009.

[43] p. drineas, a. frieze, r. kannan, s. vempala, and v. vinay. id91
large graphs via the singular value decomposition. machine learning,
56(1-3):9   33, 2004.

[44] c. eckart and g. young. the approximation of one matrix by another

of lower rank. psychometrika, 1(3):211   218, 1936.

[45] e. elhamifar and r. vidal. sparse subspace id91. in ieee confer-
ence on id161 and pattern recognition, 2009, pages 2790   
2797. ieee, 2009.

[46] m. fazel, h. hindi, and s. boyd. rank minimization and applications in
system theory. in proceedings of the 2004 american control conference
(acc), volume 4, pages 3273   3278. ieee, 2004.

[47] c. f  votte, n. bertin, and j. durrieu. nonnegative id105
with the itakura-saito divergence: with application to music analysis.
neural computation, 21(3):793   830, 2009.

[48] w. fithian and r. mazumder. scalable convex methods for    exible

low-rank matrix modeling. arxiv preprint arxiv:1308.4211, 2013.

[49] n. gillis. nonnegative id105: complexity, algorithms and

applications. phd thesis, ucl, 2011.

110

references

[50] n. gillis and f. glineur. low-rank matrix approximation with weights
or missing data is np-hard. siam journal on matrix analysis and
applications, 32(4):1149   1165, 2011.

[51] nicolas gillis and fran  ois glineur. a continuous characterization of
the maximum-edge biclique problem. journal of global optimization,
58(3):439   464, 2014.

[52] a. goldberg, b. recht, j. xu, r. nowak, and x. zhu. transduction
with matrix completion: three birds with one stone. in advances in
neural information processing systems, pages 757   765, 2010.

[53] g. j. gordon. generalized2 linear2 models.

in advances in neural

information processing systems, pages 577   584, 2002.

[54] a. gress and i. davidson. a    exible framework for projecting heteroge-
neous data. in proceedings of the 23rd acm international conference
on conference on information and knowledge management, cikm    14,
pages 1169   1178, new york, ny, usa, 2014. acm.

[55] s. gunasekar, a. acharya, n. gaur, and j. ghosh. noisy matrix comple-
tion using alternating minimization. in machine learning and knowl-
edge discovery in databases, pages 194   209. springer, 2013.

[56] m. gupta, s. bengio, and j. weston. training highly multiclass clas-
si   ers. the journal of machine learning research, 15(1):1461   1492,
2014.

[57] n. halko, p.-g. martinsson, and j. tropp. finding structure with ran-
domness: probabilistic algorithms for constructing approximate matrix
decompositions. siam review, 53(2):217   288, 2011.

[58] m. hardt. on the provable convergence of alternating minimization for

matrix completion. arxiv preprint arxiv:1312.0925, 2013.

[59] m. hardt and m. wootters. fast matrix completion without the condi-

tion number. arxiv preprint arxiv:1407.4070, 2014.

[60] t. hastie, r. mazumder, j. lee, and r. zadeh. matrix completion and

low-rank svd via fast alternating least squares. arxiv, 2014.

[61] j. horn. a rationale and test for the number of factors in factor analysis.

psychometrika, 30(2):179   185, 1965.

[62] h. hotelling. analysis of a complex of statistical variables into principal

components. journal of educational psychology, 24(6):417, 1933.

[63] h. hotelling. relations between two sets of variates. biometrika, 28(3   

4):321   377, 1936.

references

111

[64] z. huang and m. ng. a fuzzy k-modes algorithm for id91 cate-
gorical data. ieee transactions on fuzzy systems, 7(4):446   452, 1999.

[65] p. huber. robust statistics. wiley, new york, 1981.
[66] p. jain, p. netrapalli, and s. sanghavi. low-rank matrix completion
using alternating minimization. in proceedings of the 45th annual acm
symposium on the theory of computing, pages 665   674. acm, 2013.

[67] i. jolli e. principal component analysis. springer, 1986.
[68] j. josse and s. wager.

work for regularized low-rank matrix estimation.
arxiv:1410.8275, 2014.

stable autoencoding: a    exible frame-
arxiv preprint

[69] j. josse, s. wager, and f. husson. con   dence areas for    xed-e ects

pca. arxiv preprint arxiv:1407.7614, 2014.

[70] m. journ  e, f. bach, p. absil, and r. sepulchre. low-rank optimiza-
tion on the cone of positive semide   nite matrices. siam journal on
optimization, 20(5):2327   2351, 2010.

[71] l. kaufman and p. j. rousseeuw. finding groups in data: an introduc-

tion to cluster analysis, volume 344. john wiley & sons, 2009.

[72] r. keshavan. e cient algorithms for collaborative    ltering. phd thesis,

stanford university, 2012.

[73] r. keshavan and a. montanari. id173 for matrix completion.
in 2010 ieee international symposium on id205 pro-
ceedings (isit), pages 1503   1507. ieee, 2010.

[74] r. keshavan, a. montanari, and s. oh. matrix completion from noisy
entries. in advances in neural information processing systems, pages
952   960, 2009.

[75] r. keshavan and s. oh. a id119 algorithm on the grassman
manifold for matrix completion. arxiv preprint arxiv:0910.5260, 2009.
[76] r. h. keshavan, a. montanari, and s. oh. matrix completion from
a few entries. ieee transactions on id205, 56(6):2980   
2998, 2010.

[77] h. kim and h. park. sparse non-negative id105s via
alternating non-negativity-constrained least squares for microarray data
analysis. bioinformatics, 23(12):1495   1502, 2007.

[78] h. kim and h. park. nonnegative id105 based on alter-
nating nonnegativity constrained least squares and active set method.
siam journal on matrix analysis and applications, 30(2):713   730,
2008.

112

references

[79] j. kim, y. he, and h. park. algorithms for nonnegative matrix and
tensor factorizations: a uni   ed view based on block coordinate descent
framework. journal of global optimization, 58(2):285   319, 2014.

[80] j. kim and h. park. toward faster nonnegative id105:
a new algorithm and comparisons. in eighth ieee international con-
ference on data mining, pages 353   362. ieee, 2008.

[81] j. kim and h. park. fast nonnegative id105: an active-
set-like method and comparisons. siam journal on scienti   c comput-
ing, 33(6):3261   3281, 2011.

[82] r. koenker. quantile regression. cambridge university press, 2005.
[83] r. koenker and j. g. bassett. regression quantiles. econometrica:

journal of the econometric society, pages 33   50, 1978.

[84] e. lawler and d. wood. branch-and-bound methods: a survey. oper-

ations research, 14(4):699   719, 1966.

[85] d. lee and h. seung. learning the parts of objects by non-negative

id105. nature, 401(6755):788   791, 1999.

[86] d. lee and h. seung. algorithms for non-negative id105.
in advances in neural information processing systems, pages 556   562,
2001.

[87] h. lee, a. battle, r. raina, and a. ng. e cient sparse coding algo-
rithms. in advances in neural information processing systems, pages
801   808, 2006.

[88] j. lee, b. recht, r. salakhutdinov, n. srebro, and j. tropp. practical
large-scale optimization for max-norm id173. in advances in
neural information processing systems, pages 1297   1305, 2010.

[89] y. lee, y. lin, and g. wahba. multicategory support vector machines:
theory and application to the classi   cation of microarray data and
satellite radiance data. journal of the american statistical association,
99(465):67   81, 2004.

[90] r. likert. a technique for the measurement of attitudes. archives of

psychology, 1932.

[91] c. lin. projected gradient methods for nonnegative matrix factoriza-

tion. neural computation, 19(10):2756   2779, 2007.

[92] z. liu and l. vandenberghe. interior-point method for nuclear norm
approximation with application to system identi   cation. siam journal
on matrix analysis and applications, 31(3):1235   1256, 2009.

references

113

[93] s. lloyd. least squares quantization in pcm. ieee transactions on

id205, 28(2):129   137, 1982.

[94] l. mackey. de   ation methods for sparse pca. in d. koller, d. schu-
urmans, y. bengio, and l. bottou, editors, advances in neural infor-
mation processing systems, 2009.

[95] j. mairal, f. bach, j. ponce, and g. sapiro. online dictionary learn-
ing for sparse coding. in proceedings of the 26th annual international
conference on machine learning, pages 689   696. acm, 2009.

[96] j. mairal, j. ponce, g. sapiro, a. zisserman, and f. bach. supervised
dictionary learning. in advances in neural information processing sys-
tems, pages 1033   1040, 2009.

[97] i. markovsky. low rank approximation: algorithms, implementation,
applications. communications and control engineering. springer, 2012.
[98] r. mazumder, t. hastie, and r. tibshirani. spectral id173
algorithms for learning large incomplete matrices. the journal of ma-
chine learning research, 11:2287   2322, 2010.

[99] t. mikolov, k. chen, g. corrado, and j. dean. e cient estimation of
word representations in vector space. arxiv preprint arxiv:1301.3781,
2013.

[100] t. mikolov, i. sutskever, k. chen, g. corrado, and j. dean. distributed
representations of words and phrases and their compositionality.
in
advances in neural information processing systems, pages 3111   3119,
2013.

[101] t. minka. automatic choice of dimensionality for pca. in t.k. leen,
t.g. dietterich, and v. tresp, editors, advances in neural information
processing systems, pages 598   604. mit press, 2001.

[102] k. mohan and m. fazel. reweighted nuclear norm minimization with
application to system identi   cation. in proceedings of the 2010 ameri-
can control conference (acc), pages 2953   2959. ieee, 2010.

[103] p. netrapalli, u. niranjan, s. sanghavi, a. anandkumar, and p. jain.
provable non-convex robust pca. in advances in neural information
processing systems, pages 1107   1115, 2014.

[104] f. niu, b. recht, c. r  , and s. wright. hogwild!: a lock-free approach
to parallelizing stochastic id119. in advances in neural in-
formation processing systems, 2011.

[105] j. nocedal and s. wright. numerical optimization. springer science &

business media, 2006.

114

references

[106] b. olshausen and d. field. sparse coding with an overcomplete basis
set: a strategy employed by v1? vision research, 37(23):3311   3325,
1997.

[107] s. osnaga. low rank representations of matrices using nuclear norm

heuristics. phd thesis, colorado state university, 2014.

[108] a. owen and p. perry. bi-cross-validation of the svd and the non-
negative id105. the annals of applied statistics, pages
564   594, 2009.

[109] n. parikh and s. boyd. proximal algorithms. foundations and trends

in optimization, 1(3):123   231, 2013.

[110] h.-s. park and c.-h. jun. a simple and fast algorithm for k-medoids
id91. id109 with applications, 36(2, part 2):3336   3341,
2009.

[111] k. pearson. on lines and planes of closest    t to systems of points in
space. the london, edinburgh, and dublin philosophical magazine and
journal of science, 2(11):559   572, 1901.

[112] j. pennington, r. socher, and c. manning. glove: global vectors for
word representation. proceedings of the empiricial methods in natural
language processing (emnlp 2014), 12, 2014.

[113] p. perry. cross-validation for unsupervised learning. arxiv preprint

arxiv:0909.3052, 2009.

[114] j. platt, n. cristianini, and j. shawe-taylor. large margin dags for
multiclass classi   cation. in advances in neural information processing
systems, pages 547   553, 1999.

[115] k. preacher and r. maccallum. repairing tom swift   s electric factor
analysis machine. understanding statistics: statistical issues in psy-
chology, education, and the social sciences, 2(1):13   43, 2003.

[116] r. raina, a. battle, h. lee, b. packer, and a. ng. self-taught learn-
ing: id21 from unlabeled data. in proceedings of the 24th
international conference on machine learning, pages 759   766. acm,
2007.

[117] b. recht, m. fazel, and p. parrilo. guaranteed minimum-rank solu-
tions of linear matrix equations via nuclear norm minimization. siam
review, 52(3):471   501, august 2010.

[118] b. recht and c. r  . parallel stochastic gradient algorithms for large-
scale matrix completion. mathematical programming computation,
5(2):201   226, 2013.

references

115

[119] b. recht, c. r  , s. wright, and f. niu. hogwild: a lock-free approach
to parallelizing stochastic id119. in advances in neural in-
formation processing systems, pages 693   701, 2011.

[120] j. rennie and n. srebro. fast maximum margin id105
for collaborative prediction. in proceedings of the 22nd international
conference on machine learning, pages 713   719. acm, 2005.

[121] w. revelle and k. anderson. personality, motivation and cognitive
performance: final report to the army research institute on contract
mda 903-93-k-0008. technical report, 1998.

[122] p. richt  rik, m. tak   , and s. ahipa ao lu. alternating maximization:
unifying framework for 8 sparse pca formulations and e cient parallel
codes. arxiv preprint arxiv:1212.4137, 2012.

[123] r. rifkin and a. klautau. in defense of one-vs-all classi   cation. the

journal of machine learning research, 5:101   141, 2004.

[124] a. schein, l. saul, and l. ungar. a generalized linear model for princi-
pal component analysis of binary data.
in proceedings of the ninth
international workshop on arti   cial intelligence and statistics, vol-
ume 38, page 46, 2003.

[125] s. schelter, v. satuluri, and r. zadeh. factorbird     a parameter server
approach to distributed id105. nips 2014 workshop on
distributed machine learning and matrix computations, 2014.

[126] f. shahnaz, m. w. berry, v. p. pauca, and r. j. plemmons. doc-
ument id91 using nonnegative id105. information
processing & management, 42(2):373   386, 2006.

[127] s. shalev-shwartz, a. gonen, and o. shamir. large-scale convex min-
imization with a low-rank constraint. arxiv preprint arxiv:1106.1622,
2011.

[128] h. shen and j. huang. sparse principal component analysis via regular-
ized low rank matrix approximation. journal of multivariate analysis,
99(6):1015   1034, 2008.

[129] a. singh and g. gordon. a uni   ed view of id105 models.
in machine learning and knowledge discovery in databases, pages 358   
373. springer, 2008.

[130] r. smith. nuclear norm minimization methods for frequency domain
subspace identi   cation. in proceedings of the 2010 american control
conference (acc), pages 2689   2694. ieee, 2012.

[131] m. soltanolkotabi and e. candes. a geometric analysis of subspace
id91 with outliers. the annals of statistics, 40(4):2195   2238, 2012.

116

references

[132] m. soltanolkotabi, e. elhamifar, and e. candes. robust subspace clus-

tering. arxiv preprint arxiv:1301.2603, 2013.

[133] n. srebro. learning with id105s. phd thesis, mas-

sachusetts institute of technology, 2004.

[134] n. srebro and t. jaakkola. weighted low-rank approximations.

icml, volume 3, pages 720   727, 2003.

in

[135] n. srebro, j. rennie, and t. jaakkola. maximum-margin matrix fac-
torization. in advances in neural information processing systems, vol-
ume 17, pages 1329   1336, 2004.

[136] v. srikumar and c. manning. learning distributed representations
for structured output prediction. in advances in neural information
processing systems, pages 3266   3274, 2014.

[137] n. srivastava, g. hinton, a. krizhevsky, i. sutskever, and r. salakhut-
dinov. dropout: a simple way to prevent neural networks from over-
   tting. the journal of machine learning research, 15(1):1929   1958,
2014.

[138] h. steck. hinge rank loss and the area under the roc curve.

in
j. n. kok, j. koronacki, r. l. mantaras, s. matwin, d. mladeni , and
a. skowron, editors, machine learning: ecml 2007, volume 4701 of
lecture notes in computer science, pages 347   358. springer berlin hei-
delberg, 2007.

[139] d. l. sun and c. f  votte. alternating direction method of multipliers
for non-negative id105 with the beta-divergence. in ieee
international conference on acoustics, speech, and signal processing
(icassp), 2014.

[140] r. sun and z.-q. luo. guaranteed matrix completion via nonconvex
factorization. in 2015 ieee 56th annual symposium on foundations
of computer science (focs), pages 270   289. ieee, 2015.

[141] y. takane, f. young, and j. de leeuw. nonmetric individual di erences
multidimensional scaling: an alternating least squares method with op-
timal scaling features. psychometrika, 42(1):7   67, 1977.

[142] m. tipping and c. bishop. probabilistic principal component analysis.
journal of the royal statistical society: series b (statistical methodol-
ogy), 61(3):611   622, 1999.

[143] n. tishby, f. pereira, and w. bialek. the information bottleneck

method. arxiv preprint physics/0004057, 2000.

[144] j. tropp. topics in sparse approximation. phd thesis, the university

of texas at austin, 2004.

references

117

[145] j. tropp and a. gilbert. signal recovery from random measurements
via orthogonal matching pursuit. ieee transactions on information
theory, 53(12):4655   4666, 2007.

[146] p. tseng. nearest q-   at to m points. journal of optimization theory

and applications, 105(1):249   252, 2000.

[147] m. tweedie. an index which distinguishes between some important ex-
ponential families. in statistics: applications and new directions. pro-
ceedings of the indian statistical institute golden jubilee international
conference, pages 579   604, 1984.

[148] n. usunier, d. bu oni, and p. gallinari. ranking with ordered weighted
pairwise classi   cation. in proceedings of the 26th annual international
conference on machine learning, pages 1057   1064. acm, 2009.

[149] s. vavasis. on the complexity of nonnegative id105.

siam journal on optimization, 20(3):1364   1377, 2009.

[150] r. vidal. a tutorial on subspace id91. ieee signal processing

magazine, 28(2):52   68, 2010.

[151] t. virtanen. monaural sound source separation by nonnegative matrix
factorization with temporal continuity and sparseness criteria. ieee
transactions on audio, speech, and language processing, 15(3):1066   
1074, 2007.

[152] v. vu, j. cho, j. lei, and k. rohe. fantope projection and selection: a
near-optimal convex relaxation of sparse pca. in c. burges, l. bottou,
m. welling, z. ghahramani, and k. weinberger, editors, advances in
neural information processing systems 26, pages 2670   2678. curran
associates, inc., 2013.

[153] j. weston, s. bengio, and n. usunier. large scale image annotation:
learning to rank with joint word-image embeddings. machine learning,
81(1):21   35, 2010.

[154] j. weston, h. yee, and r. j. weiss. learning to rank recommendations
with the k-order statistic loss. in proceedings of the 7th acm conference
on recommender systems, recsys    13, pages 245   248, new york, ny,
usa, 2013. acm.

[155] d. witten, r. tibshirani, and t. hastie. a penalized matrix decompo-
sition, with applications to sparse principal components and canonical
correlation analysis. biostatistics, page kxp008, 2009.

118

references

[156] j. wright, a. ganesh, s. rao, y. peng, and y. ma. robust principal
component analysis: exact recovery of corrupted low-rank matrices by
id76.
in advances in neural information processing
systems, volume 3, 2009.

[157] h. xu, c. caramanis, and s. sanghavi. robust pca via outlier pursuit.

ieee transactions on id205, 58(5):3047   3064, 2012.

[158] f. young, j. de leeuw, and y. takane. regression with qualitative
and quantitative variables: an alternating least squares method with
optimal scaling features. psychometrika, 41(4):505   529, 1976.

[159] h. yun, h.-f. yu, c.-j. hsieh, s. v. n. vishwanathan, and i. dhillon.
nomad: non-locking, stochastic multi-machine algorithm for asyn-
chronous and decentralized matrix completion.
arxiv preprint
arxiv:1312.0193, 2013.

[160] m. zaharia, m. chowdhury, m. franklin, s. shenker, and i. stoica.
spark: cluster computing with working sets. in proceedings of the 2nd
usenix conference on hot topics in cloud computing, page 10, 2010.
[161] h. zou, t. hastie, and r. tibshirani. sparse principal component anal-
ysis. journal of computational and graphical statistics, 15(2):265   286,
2006.

[162] w. zwick and w. velicer. comparison of    ve rules for determining the
number of components to retain. psychological bulletin, 99(3):432, 1986.

