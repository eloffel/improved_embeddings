natural language understanding
kyunghyun cho

2

language understanding?

modelling?

language understanding

3

topics: natural language understanding
    what does it mean that a machine understands natural languages?
    should we start reading linguistics?

mt history: statistical mt at ibm

fred jelinek, 1988:

   every time i    re a linguist, 
the performance of the 
recognizer goes up.    

- fred jelinek (ibm), 1988

   every time i    re a linguist, the
performance of the recognizer goes up.   

language understanding

4

topics: natural language understanding
    it   s all about telling how likely a sentence is.. 

    how likely is this sentence as an answer to the question?

    q.    who is the president of the united states?   
    likely answer:    obama is the president of the u.s.   
    unlikely answer:    tsipras is the president of america.   

language understanding

5

topics: natural language understanding
    it   s all about telling how likely a sentence is.. 

    how likely is this sentence given this view?

   likely:    two dolphins are diving   
   unlikely:    two men are    ying   

language understanding

6

topics: natural language understanding

it   s all about telling how likely a sentence is.. 

7

language modelling

   
how likely is this sentence?

8

topics: language modelling

    a sentence 

(x1, x2, . . . , xt )

    ex) (   the   ,    cat   ,    is   ,    eating   ,    a   ,    sandwich   ,    on   ,    a   ,    couch   )

    how likely is this sentence? 
    in other words, what is the id203 of                         ?
(x1, x2, . . . , xt )

    i.e., 

p(x1, x2, . . . , xt ) =?

how likely is this sentence?

9

topics: id203 101 - id155
    joint id203  
p(x, y)
    id155 
p(x|y)
    marginal id203         and
p(x)

p(y)

    they are related by 

p(x, y) = p(x|y)p(y) = p(y|x)p(x)
x

y

how likely is this sentence?

10

topics: language modelling as a product of conditionals
    rewrite                            into

p(x1, x2, . . . , xt )

p(x1, x2, . . . , xt ) =

    graphically, 

tyt=1

p(xt | x1, . . . , xt 1)

x1

x2

x3

   

xt

11

statistical lm

topics: statistical language modelling
    maximize the (log-)probabilities of    
sentences in corpora

max ed [log p(x1, x2, . . . , xt )]
    obvious to us, but not to everyone:

       the validity of statistical (information 
theoretic) approach to mt has indeed been 
recognized     as early as 1949. and was 
universally recognized as mistaken [sic] by 
1950.     the crude force of computers is not 
science.      
                 (review of brown et al. (1990))

id165 language modelling

12

(blunsom, 2015)

   
how likely is this sentence?

13

topics: non-parametric approach     id165 modelling
    n-th order markov assumption: why?

p(x1, x2, . . . , xt ) =

   

p(xt | x1, . . . , xt 1)

p(xt | xt n, . . . , xt 1)

    collect id165 statistics from a large corpus:

tyt=1
tyt=1

p(xt|xt n, . . . , xt 1) =

count(xt n, . . . , xt 1, xt)
count(xt n, . . . , xt 1)

how likely is this sentence?

14

topics: non-parametric approach     id165 modelling
    ex) 
    unigram modelling

p(i, would, like, to, . . . , .,h/si)

comparison 1   4-gram

would

word

i

p(i)p(would)p(like)       p(h/si)

    bigram modelling

p(i)p(would|i)p(like|would)       p(h/si| .)

    trigram modelling

p(i)p(would|i)p(like|i, would)      

   

like
to

commend

the

rapporteur

on
his
work

.

</s>
average
perplexity

unigram bigram trigram 4-gram
3.197
2.791
1.290
0.113
8.633
0.880
2.350
1.862
1.978
2.394
1.510
0.000
2.251
4.758

6.684
8.342
9.129
5.081
15.487
3.885
10.840
6.765
10.678
9.993
4.896
4.828
8.051
265.136

3.197
2.884
2.026
0.402
12.335
1.402
7.319
4.140
7.316
4.816
3.020
0.005
4.072
16.817

3.197
2.791
1.031
0.144
8.794
1.084
2.763
4.150
2.367
3.498
1.785
0.000
2.634
6.206

how likely is this sentence?

15

topics: id165 modelling     two closely-related issues

    data sparsity

    # of all possible id165s:        , where      : size of vocabulary 

|v |n

|v |

p(a, tenured, professor, like, drinking, whiskey, .) =
p(a)p(tenured|a) p(professor|a, tenured)

|

=0

{z

}

p(likes|tenured, professor)       p(.|drinking, whiskey)
= 0

how likely is this sentence?

16

topics: id165 modelling     two closely-related issues
    conventional solutions to data sparsity:

    smoothing: 
p(xt|xt n, . . . , xt 1) =

    backoff:

p(xt|xt n, . . . , xt 1) =8>><>>:

count(xt n, . . . , xt 1, xt) +    
count(xt n, . . . , xt 1) +    |v |

(add-    smoothing)

   
   n(xt|xt n, . . . , xt 1),
dn(xt n, . . . , xt 1)p(xt|xt n+1 . . . , xt 1),

if countn(xt n, . . . , xt) > 0
otherwise

(   n: adjusted prediction model, dn: discount factor)

what can go wrong with id165 lm?

how likely is this sentence?

17

topics: id165 modelling     two closely-related issues

    lack of generalization

    (chases, a, dog), (chases, a, cat), (chases, a, rabbit)
    (chases, a, llama)=?

t=1 p(xt | xt n, . . . , xt 1)

18

neural language modelling

   
language modelling

19

topics: neural language modelling

    non-parametric estimator         parametric estimator
count(xt n, . . . , xt 1, xt)
count(xt n, . . . , xt 1)

p(xt|xt n, . . . , xt 1) =

= fxt(xt n, . . . , xt 1)

language modelling

20

topics: neural language modelling
p(xt|xt n, . . . , xt 1) = fxt(xt n, . . . , xt 1)
    building a neural language model (bengio et al., 2000)
(1)1-of-k encoding of each word
(2)continuous space word representation

xt0

st0 = w >xt0, where w 2 r|v |   d

(3)nonlinear hidden layer

h = tanh(u> [st 1; st 2;       ; st n] + b)

, where u 2 rnd   d0 and b 2 rd0

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
gc
n
i
d
o
c
 
k

 

-
f
o
-
1

softmax

nonlinear projection

language modelling

21

topics: neural language modelling
p(xt|xt n, . . . , xt 1) = fxt(xt n, . . . , xt 1)
    building a neural language model (bengio et al., 2000)

(1)unnormalized probabilities
y =v h + c, where v 2 r|v |   d0 and c 2 r|v |
(2)softmax id172
p(xt = i|xt n, . . . , xt 1) =
p|v |j=1 exp(yj)

exp(yi)

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
gc
n
i
d
o
c
 
k

 

-
f
o
-
1

softmax

nonlinear projection

language modelling

1. there are three teams left for the quali   cation.
2. four teams have passed the    rst round.
3. four groups are playing in the    eld.

sample sentences:

22

topics: neural lm generalizes to unseen id165   s
    example sentences

neural net lm: compress the corpus into a continuous function

    there are three teams left for the quali   cation.
    four teams have passed the    rst round.
    four groups are playing in the    eld.

    how likely is groups followed by three?
    why?

threefourteamsgroupsi z: a phrase representation?

embeddings

i z: a phrase representation?

i w: a lookup table of word

language modelling

23

topics: continuous-space representation     embeddings

softmax

nonlinear projection

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
gc
n
i
d
o
c
 
k

 

-
f
o
-
1

24

q&a

25

non-markovian language modelling

   
language modelling

26

topics: markov assumption
    markov assumption in id165 modeling

p(x1, x2, . . . , xt ) =

   

p(xt | x1, . . . , xt 1)

p(xt | xt n, . . . , xt 1)

tyt=1
tyt=1

    issue: dependency beyond the context window is ignored

    ex) the same stump which had impaled the car of many a guest in the past thirty 
years and which he refused to have removed

language modelling

27

topics: non-markovian language modelling
    directly model the original conditional probabilities

tyt=1

p(x1, x2, . . . , xt ) =

p(xt | x1, . . . , xt 1)

    feature extraction + readout

    feature extraction: 
    readout: 

ht = f (x1, x2, . . . , xt 1)

p(xt|x1, . . . , xt 1) = g(ht)

    how can we let     take variable-length input?

f

language modelling

28

topics: language modelling via recursion
    directly model the original conditional probabilities

p(x1, x2, . . . , xt ) =

p(xt | x1, . . . , xt 1)

tyt=1

    recursive construction of 
h0 = 0

    initial condition: 
    recursion: 

f

ht = f (xt 1, ht 1)

ht

f

ht

    we call     an internal hidden state or memory 

xt 1
        summarizes/memorizes the history from       up to   
ht

x1

xt 1

language modelling

29

topics: language modelling via recursion
    example:

p(eating|the, cat, is)

(1) initialization:  
(2) recursion

h0 = 0

(1)  
(2)  
(3)  

h1 = f (h0, the)
h2 = f (h1, cat)
h3 = f (h2, is)

(3) readout: 

p(eating|the, cat, is) = g(h3)

    it works for any number of context words

30

id56 language modelling

   
language modelling

31

topics: recurrent neural network language model
    example:

p(the, cat, is, eating)

(1) initialization:  
(2) recursion with readout

h0 = 0

p(the) = g(h0)

(1)  
(2)  
(3)  

h1 = f (h0, the)
h2 = f (h1, cat)
h3 = f (h2, is)

p(cat|the) = g(h1)
p(is|the, cat) = g(h2)
p(eating|the, cat, is) = g(h3)

(3) combination: 

p(the, cat, is, eating) = g(h0)g(h1)g(h2)g(h3)

    read, update and predict

language modelling

32

topics: recurrent neural network language model
    example:

p(the, cat, is, eating)

p(the)

p(cat| . . .)

p(is| . . .) p(eating| . . .)

h0

h1

the

h2

cat

h3

is

    read, update and predict

language modelling

33

topics: building an id56 language model
    what do we need?
    transition function
    output/readout function 

ht = f (ht 1, xt 1)

p(the)

p(cat| . . .)

p(xt = w|x1, . . . , xt 1) = gw(ht)
p(is| . . .) p(eating| . . .)

h0

h1

the

h2

cat

h3

is

language modelling

34

topics: building an id56 language model - transition function

    inputs 

    input                          : one-hot vector, i.e.,  
    hidden state 

xt 1 2{ 0, 1}|v |
ht 1 2 rd

    parameters 

xt 1 = w 2{ 1, . . . ,|v |}

    input weight matrix                    (often called id27s)
    transition weight matrix
    bias vector   

w 2 rd   |v |
u 2 rd   d

b 2 rd

language modelling

35

topics: building an id56 language model - transition function
    inputs:                         , 
    parameters:                    ,                ,
    naive transition function

w 2 rd   |v | u 2 rd   d b 2 rd

xt 1 2{ 0, 1}|v | ht 1 2 rd

ht 1

u

ht = tanh(w xt 1 + u ht 1 + b)
(1) continuous-space representation of word:
w xt 1
(2) linear transformation of the previous hidden state: 
(3) additive combination of           and           together with
(4) point-wise nonlinear transformation

ht 1

xt 1

u ht 1
b

b

tanh
+
w
xt 1

language modelling

36

topics: building an id56 language model - readout function

    inputs 

    (current) hidden state 

    parameters 

ht 2 rd

    output matrix                    (often called target id27s)
    bias vector   

r 2 r|v |   d

c 2 r|v |

language modelling

37

topics: building an id56 language model - readout function
    inputs 
    parameters                   , 
r 2 r|v |   d
    softmax readout function

ht 2 rd

p(xt = w|x<t) = gw(ht) =

exp(r>wht 1 + cw)
i=1 exp(r>i ht 1 + ci)
(1) linear projection of the hidden state for each possible target word 

vi = r>i ht 1 for all i = 1, . . . ,|v |

(3) transform each projected vector       to be positive 
(4) normalize        s to make them into probabilities of the i-th target words

  pi = exp(vi)

vi

  pi

c 2 r|v |
p|v |

+

/

   

exp

r
ht

language modelling

38

topics: building an id56 language model

    recursion and readout:

    recursion

p(xt = w|x<t)

/

   

+

ht = tanh(w xt 1 + u ht 1 + b)

    readout/output
p(xt = w|x<t) =

exp(r>wht 1)
i=1 exp(r>i ht 1)

p|v |

u

exp
r
tanh
+
w
xt 1

b

39

training id56-lm

   
language modelling

40

topics: cost function
    log-id203 of a sentence

j(   )

(x1, x2, . . . , xt )

log p(x1, x2, . . . , xt ) =

log p(xt | x1, . . . , xt 1)

    train an id56 lm to maximize the log-prob   s of training sentences
    given a training set of      sentences: 

t1), . . . , (xn

1 , . . . , xn

1, . . . , x1

n

txt=1

maximize   

1
n

nxn=1

log p(xn

1 , . . . , xn

() minimize   j(   ) =  

1
n

log p(xn

1 . . . , xn

t |xn

t 1)

tn)

 (x1
tnxt=1
nxn=1

tn ) 

41

language modelling
d =nx1, . . . , xn0o
rj(   , xn)

topics: minibatch stochastic id119 - recap
(1)randomly select a minibatch of      sentences:
(2)compute the gradient of per-sample cost w.r.t.     :
   
(3)compute the minibatch gradient: 

n0

rj(   , d) =

(4)update the parameters

   

1
n0

n0xn=1

rj(   , xn)

(5)repeat until convergence 

         +    rj(   , d)

language modelling

42

topics: id26 through time
    decomposition of a per-sample cost function
    unrolled computational graph

j(   , x) =  

jt(   , xt)

txt=1

jt(   ,   x) = log p(xt =   xt|x<t)

ht 1

   

   

g
r
ht

u

tanh
+
w
xt 1

b

language modelling

43

jt(   ,   x) = log p(xt =   xt|x<t)

rr,ru ,rw ,rb

topics: id26 through time
(1)initialize                           and
t = t
(1)the per-step cost derivative:
@jt
@g
(2)gradient w.r.t.     : 
r
(3)gradient w.r.t.     : 
ht
(4)gradient w.r.t.     :
u @j t
@ht
(5)gradient w.r.t      and   :              ,
@ht
w
@w

@ht
@u
b @j t
@ht

+ @j>t
@ht+1

@g
@r
@g
@ht

@jt
@g
@jt
@g

@ht+1

@ht

@j t
@ht

@ht
@b

u

g
r
ht
+
w
xt 1

u

b

(2)update the parameter gradient and repeat until 

t = 1

rr   rr + @jt
rw   rw + @j t

@r ,ru   ru + @j t
@w ,rb   rb + @j t

@u

@b

n

o t e: i    m   a

a th  a  lot h ere!!

b u sin g  m

44

q&a

code: https://github.com/nyu-dl/dl4mt-tutorial/tree/master/session0

45

id149

   
46

a th  a  lot h ere!!

id149

topics: temporal dependency and vanishing gradient
    how much in   uence does     have on                            ?
log p(xt+n|x<t+n)

ht

@jt+n
@ht

=

@jt+n

@g

@g

@ht+n

@ht+n

@ht+n 1       

@ht+1
@ht

o t e: i    m   a

n

b u sin g  m

    with the naive transition function?
                                       , where 
    let   s rewrite it

= u> @ tanh(a)

@ht+1
@ht

@a

@jt+n
@ht

=

@jt+n

@g

@g

@ht+n

a = w xt + u ht + b

u>diag    @ tanh(at+n)

@at+n

nyn=1
|

   
}

problematic! bengio et al. (1994)

{z

id149

47

topics: temporal dependency and vanishing gradient
    upper bound on the norm of the gradient w.r.t.     ?
ht

u>diag    @ tanh(at+n)

@at+n

    observations 

nyn=1

     

            

@ tanh(at+n)

@at+n

nyn=1    
nyn=1  u>  
yn
n=1  u>   ! 0

@ tanh(at+n)

@at+n ! 0

    

(1) vanishing gradient when                      :
 max(u ) < 1
(2) vanishing gradient when the units are saturated: 
(3) potentially, exploding gradient when                     
 max(u ) > 1

    problem: it   s likely that there   s no learning signal!
              

id149

48

on the di culty of training recurrent neural networks

topics: exploding gradient is less problematic
       when gradients explode so does    
the curvature along v, leading to    
a wall in the error surface   
    solution: gradient clipping
(1)gradient norm clipping 

  r      c

krkr ,if krk   c
r
,otherwise

(2)element-wise gradient clipping

ri   min(c,ri), for all i 2{ 1, . . . , dimr}

pascanu et al. (2013)

figure 6. we plot the error surface of a single hidden unit
recurrent network, highlighting the existence of high cur-
vature walls. the solid lines depicts standard trajectories
that id119 might follow. using dashed arrow
the diagram shows what would happen if the gradients is
rescaled to a    xed size when its norm is above a threshold.

rithm should work even when the rate of growth of the
gradient is not the same as the one of the curvature
(a case for which a second order method would fail
as the ratio between the gradient and curvature could
still explode).

our hypothesis could also help to understand the re-
cent success of the hessian-free approach compared
to other second order methods. there are two key dif-
ferences between hessian-free and most other second-
order algorithms. first, it uses the full hessian matrix
and hence can deal with exploding directions that are
not necessarily axis-aligned. second, it computes a
new estimate of the hessian matrix before each up-
date step and can take into account abrupt changes in
curvature (such as the ones suggested by our hypothe-
sis) while most other approaches use a smoothness as-
sumption, i.e., averaging 2nd order signals over many
steps.

id149

to force the network to increase the norm of @xt
at the
@xk
expense of larger errors (caused by the irrelevant input
entries) and then wait for it to learn to ignore these
irrelevant input entries. this suggest that moving to-
wards increasing the norm of @xt
can not be always
@xk
done while following a descent direction of the error e
(which is, for e.g., what a second order method would
@ht+n
try to do), and therefore we need to enforce it via a
id173 term.

topics: but, vanishing gradient is very problematic
    why does the gradient vanish?

@at+n

    can we simply    maximize                    ?

the regularizer we propose below prefers solutions for
       we need to force the network to increase the norm of           at 
which the error signal preserves norm as it travels back
the expense of larger errors   
in time:

@ht+n

@ht

    

@ht      =     

nyn=1

@ht+n

         ! 0
u>diag    @ tanh(at+n)
    
@ht     
    @e@xk+1
@xk    
  11a
0@
   k =xk
    @e@xk+1   

@xk+1

    pascanu et al. (2013)

    regularize 

    =xk

in order to be computationally e cient, we only use

2

(9)

49

as done in martens and sutskever (2011), we address
the pathological problems proposed by hochreiter and
schmidhuber (1997) that require learning long term
correlations. we refer the reader to this original pa-
per for a detailed description of the tasks and to the
supplementary materials for the complete description
of the experimental setup.

4.1.1. the temporal order problem
we consider the temporal order problem as the pro-
totypical pathological problem, extending our results
to the other proposed tasks afterwards. the input is
a long stream of discrete symbols. at two points in
time (in the beginning and middle of the sequence) a
symbol within {a, b} is emitted. the task consists in
classifying the order (either aa, ab, ba, bb) at the
end of the sequence.

fig. 7 shows the success rate of standard sgd, sgd-c

id149

50

topics: but, vanishing gradient is very problematic
    why does the gradient vanish?

    

@ht+n

@ht      =     

nyn=1

u>diag    @ tanh(at+n)

@at+n

         ! 0

    perhaps, it is a problem with the naive transition function   

ht = tanh(w xt 1 + u ht 1 + b)

    error is backpropagated through every intermediate node

ht

u>
u

u>
u

u>
u

u>
u

ht+n

id149

51

topics: but, vanishing gradient is very problematic
    perhaps, it is a problem with the naive transition function   

ht = tanh(w xt 1 + u ht 1 + b)

    error is backpropagated through every intermediate node

ht

u>
u

u>
u
    temporal shortcut connections

u>
u

ht

u>
u

ht+n

   

ht+n

id149

52

topics: id149 (gru)
    temporal shortcut connections

ht

   

ht+n

    adaptive leaky integration

    update gate
    candidate state  

ht = (1   ut)   ht 1 + ut     ht

ut =  (wuxt 1 + uuht 1 + bu)

  ht = tanh(w xt 1 + u ht 1 + b)

id149

53

topics: id149 (gru)
    pruning connections: avoids the diffusion of signal

ht

    adaptive reset

   

ht+n

  ht = tanh(w xt 1 + u (rt   ht 1) + b)

    reset gate

rt =  (wrxt 1 + urht 1 + br)

id149

54

cho et al. (2014)

topics: id149 (gru)
    update and reset gates

ut =  (wuxt 1 + uuht 1 + bu)
rt =  (wrxt 1 + urht 1 + br)

    candidate hidden state

  ht = tanh(w xt 1 + u (rt   ht 1) + b)

    adaptive leaky integration

ht = (1   ut)   ht 1 + ut     ht

id149

long short-term memory

gated recurrent unit

55

topics: long short-term memory (lstm)
    input, forget and output gates

it =  (wixt 1 + uiht 1 + bi)
ft =  (wf xt 1 + uf ht 1 + bf )
ot =  (woxt 1 + uoht 1 + bo)

    candidate memory cell state

  ct = tanh(w xt 1 + u ht 1 + b)

    adaptive leaky integration

    output

ht = ot   tanh(ct)

ct = ft   ct 1 + it     ct

and, yes, they are very similar.

hochreiter&schmidhuber (1999),
gers et al. (2001)

fcc~++oi56

q&a

57

machine translation

   
id4
(cid:97)(cid:105)(cid:28)(cid:105)(cid:66)(cid:98)(cid:105)(cid:66)(cid:43)(cid:28)(cid:72) (cid:74)(cid:28)(cid:43)(cid:63)(cid:66)(cid:77)(cid:50) (cid:104)(cid:96)(cid:28)(cid:77)(cid:98)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:284) (cid:65)(cid:47)(cid:50)(cid:28)(cid:72) (cid:113)(cid:81)(cid:96)(cid:72)(cid:47)

58

topics: id151
     
log p(f|e) = log p(e|f ) + log p(f )
    translation model: 

log p(e|f )
    fit it with parallel corpora
log p(f )

    language model: 

    fit it with monolingual corpora

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

parallel
corpora

tm

log p(e|f)

+

lm

log p(f)

mono
corpora

e = (economic, growth, has, slowed, down, in, recent, years, .)

log p(f|e)

    the whole task                is conditional language modelling.
! (cid:84)((cid:55) | (cid:50))(cid:44) (cid:84)(cid:96)(cid:81)(cid:35)(cid:28)(cid:35)(cid:66)(cid:72)(cid:66)(cid:105)(cid:118) (cid:81)(cid:55) (cid:28) (cid:105)(cid:28)(cid:96)(cid:59)(cid:50)(cid:105) (cid:98)(cid:50)(cid:77)(cid:105)(cid:50)(cid:77)(cid:43)(cid:50) (cid:55) (cid:59)(cid:66)(cid:112)(cid:50)(cid:77) (cid:28) (cid:98)(cid:81)(cid:109)(cid:96)(cid:43)(cid:50) (cid:98)(cid:50)(cid:77)(cid:105)(cid:50)(cid:77)(cid:43)(cid:50) (cid:50)
! (cid:84)((cid:50) | (cid:55))(cid:44) (cid:98)(cid:81)(cid:64)(cid:43)(cid:28)(cid:72)(cid:72)(cid:50)(cid:47) (cid:105)(cid:96)(cid:28)(cid:77)(cid:98)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:75)(cid:81)(cid:47)(cid:50)(cid:72)
! (cid:84)((cid:55))(cid:44) (cid:85)(cid:105)(cid:28)(cid:96)(cid:59)(cid:50)(cid:105)(cid:86) (cid:72)(cid:28)(cid:77)(cid:59)(cid:109)(cid:28)(cid:59)(cid:50) (cid:75)(cid:81)(cid:47)(cid:50)(cid:72)
! (cid:42)(cid:44) (cid:43)(cid:81)(cid:77)(cid:98)(cid:105)(cid:28)(cid:77)(cid:105)

(cid:72)(cid:81)(cid:59) (cid:84)((cid:55) | (cid:50)) = (cid:72)(cid:81)(cid:59) (cid:84)((cid:50) | (cid:55)) + (cid:72)(cid:81)(cid:59) (cid:84)((cid:55)) + (cid:42)

id4

59

topics: id151 - in reality

(cid:97)(cid:105)(cid:28)(cid:105)(cid:66)(cid:98)(cid:105)(cid:66)(cid:43)(cid:28)(cid:72) (cid:74)(cid:28)(cid:43)(cid:63)(cid:66)(cid:77)(cid:50) (cid:104)(cid:96)(cid:28)(cid:77)(cid:98)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:284) (cid:74)(cid:50)(cid:98)(cid:98)(cid:118) (cid:95)(cid:50)(cid:28)(cid:72)(cid:66)(cid:105)(cid:118)

     

nxn=1

log p(f|e)    
    log-linear model 
    feature function 

    steps:

fn(e, f ) + c

fn(e, f )

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

parallel
corpora

mono
corpora

+
w2
feature

f 2

w1
feature

f 1

feature

w3

f 3

feature

wn

f n

...

(1) experts engineer useful features
(2) use a simple log-linear model 
(3) use a strong, external language model

e = (economic, growth, has, slowed, down, in, recent, years, .)

(cid:72)(cid:81)(cid:59) (cid:84)((cid:55) | (cid:50))    !(cid:66)

  (cid:66)(cid:55)(cid:66)((cid:50), (cid:55)) + (cid:42)

(cid:82)(cid:88) (cid:49)(cid:116)(cid:84)(cid:50)(cid:96)(cid:105)(cid:98) (cid:50)(cid:77)(cid:59)(cid:66)(cid:77)(cid:50)(cid:50)(cid:96) (cid:109)(cid:98)(cid:50)(cid:55)(cid:109)(cid:72) (cid:55)(cid:50)(cid:28)(cid:105)(cid:109)(cid:96)(cid:50)(cid:98)
(cid:107)(cid:88) (cid:108)(cid:98)(cid:50) (cid:28) (cid:98)(cid:66)(cid:75)(cid:84)(cid:72)(cid:50) (cid:75)(cid:81)(cid:47)(cid:50)(cid:72) (cid:85)(cid:72)(cid:81)(cid:59)(cid:64)(cid:72)(cid:66)(cid:77)(cid:50)(cid:28)(cid:96) (cid:75)(cid:81)(cid:47)(cid:50)(cid:72)(cid:86) (cid:55)(cid:96)(cid:81)(cid:75) (cid:75)(cid:28)(cid:43)(cid:63)(cid:66)(cid:77)(cid:50) (cid:72)(cid:50)(cid:28)(cid:96)(cid:77)(cid:66)(cid:77)(cid:59)

id4

60

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

ui

ip

e
l
p
m
a
s
s
d
r
o
w

 

y
t
i
l
i
b
a
b
o
r
p
 
d
r
o
w

d
e
c
o
d
e
r

t
n
e
r
r
u
c
e
r

ezi

t
a
t
s

t
n
e
r
r
u
c
e
r

e hi

t
a
t
s

si

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

 

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
c

wi

g
n
i
d
o
c
 
k

-
f
o
-
1

e = (economic, growth, has, slowed, down, in, recent, years, .)

e
n
c
o
d
e
r

(chrisman, 1991;
forcada&  eco, 1997; 
casta  o&casacuberta, 1997;
kalchbrenner&blunsom, 2013; 
sutskever et al., 2014; 
cho et al., 2014)

id4

61

topics: sequence-to-sequence learning     encoder
     encoder

(cid:76)(cid:50)(cid:109)(cid:96)(cid:28)(cid:72) (cid:74)(cid:28)(cid:43)(cid:63)(cid:66)(cid:77)(cid:50) (cid:104)(cid:96)(cid:28)(cid:77)(cid:98)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:284) (cid:49)(cid:77)(cid:43)(cid:81)(cid:47)(cid:50)(cid:96)

(3)

(4)

(1)1-of-k coding of source words
(2)continuous-space representation
st0 = w >xt0, where w 2 r|v |   d

(3)recursively read words

ht = f (ht 1, st), for t = 1, . . . , t

t
n
e
r
r
u
c
e
r

e hi

t
a
t
s

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

 

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
c

wi

g
n
i
d
o
c
 
k

-
f
o
-
1

(2)

si

(1)

e = (economic, growth, has, slowed, down, in, recent, years, .)

(cid:82)(cid:88) (cid:49)(cid:77)(cid:43)(cid:81)(cid:47)(cid:50) (cid:50)(cid:28)(cid:43)(cid:63) (cid:114)(cid:81)(cid:96)(cid:47) (cid:28)(cid:98) (cid:28) 1(cid:64)(cid:81)(cid:55)(cid:64)(cid:69) (cid:43)(cid:81)(cid:47)(cid:50)(cid:47) (cid:112)(cid:50)(cid:43)(cid:105)(cid:81)(cid:96) (cid:114)(cid:66)
(cid:107)(cid:88) (cid:83)(cid:96)(cid:81)(cid:68)(cid:50)(cid:43)(cid:105) (cid:114)(cid:66) (cid:66)(cid:77)(cid:105)(cid:81) (cid:28) (cid:43)(cid:81)(cid:77)(cid:105)(cid:66)(cid:77)(cid:109)(cid:81)(cid:109)(cid:98) (cid:98)(cid:84)(cid:28)(cid:43)(cid:50)(cid:44) (cid:98)(cid:66)     r(cid:47)
(cid:106)(cid:88) (cid:95)(cid:50)(cid:43)(cid:109)(cid:96)(cid:98)(cid:66)(cid:112)(cid:50)(cid:72)(cid:118) (cid:109)(cid:84)(cid:47)(cid:28)(cid:105)(cid:50) (cid:105)(cid:63)(cid:50) (cid:96)(cid:50)(cid:43)(cid:109)(cid:96)(cid:96)(cid:50)(cid:77)(cid:105) (cid:63)(cid:66)(cid:47)(cid:47)(cid:50)(cid:77) (cid:98)(cid:105)(cid:28)(cid:105)(cid:50)(cid:44)
(cid:57)(cid:88) (cid:43) = (cid:63)(cid:104)(cid:44) (cid:27) (cid:125)(cid:116)(cid:50)(cid:47)(cid:64)(cid:98)(cid:66)(cid:120)(cid:50) (cid:96)(cid:50)(cid:84)(cid:96)(cid:50)(cid:98)(cid:50)(cid:77)(cid:105)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:81)(cid:55) (cid:28) (cid:112)(cid:28)(cid:96)(cid:66)(cid:28)(cid:35)(cid:72)(cid:50)(cid:64)(cid:72)(cid:50)(cid:77)(cid:59)(cid:105)(cid:63) (cid:98)(cid:50)(cid:77)(cid:105)(cid:50)(cid:77)(cid:43)(cid:50)

id4

62

topics: sequence-to-sequence learning     decoder
     decoder

(cid:76)(cid:50)(cid:109)(cid:96)(cid:28)(cid:72) (cid:74)(cid:28)(cid:43)(cid:63)(cid:66)(cid:77)(cid:50) (cid:104)(cid:96)(cid:28)(cid:77)(cid:98)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:284) (cid:46)(cid:50)(cid:43)(cid:81)(cid:47)(cid:50)(cid:96)

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

(3)

(1)recursively update the memory

zt0 = f (zt0 1, ut0 1, ht )

(2)compute the next word prob.

p(ut0|u<t0) / exp(r>ut0

(3)sample a next word

zt0 + but0 )

   id125 is a good idea

e
l
p
m
a
s
s
d
r
o
w

 

ui

(2)

ip

y
t
i
l
i
b
a
b
o
r
p
d
r
o
w

 

t
n
e
r
r
u
c
e
r

e zi
(1)

t
a
t
s

e = (economic, growth, has, slowed, down, in, recent, years, .)

(cid:82)(cid:88) (cid:95)(cid:50)(cid:43)(cid:109)(cid:96)(cid:98)(cid:66)(cid:112)(cid:50)(cid:72)(cid:118) (cid:109)(cid:84)(cid:47)(cid:28)(cid:105)(cid:50) (cid:105)(cid:63)(cid:50) (cid:96)(cid:50)(cid:43)(cid:109)(cid:96)(cid:96)(cid:50)(cid:77)(cid:105) (cid:63)(cid:66)(cid:47)(cid:47)(cid:50)(cid:77) (cid:98)(cid:105)(cid:28)(cid:105)(cid:50)(cid:44) (cid:120)(cid:66) = (cid:55)((cid:120)(cid:66)   1, (cid:109)(cid:66)   1, (cid:43))
(cid:107)(cid:88) (cid:76)(cid:50)(cid:116)(cid:105) (cid:114)(cid:81)(cid:96)(cid:47) (cid:84)(cid:96)(cid:81)(cid:35)(cid:28)(cid:35)(cid:66)(cid:72)(cid:66)(cid:105)(cid:118)(cid:44) (cid:84)((cid:109)(cid:66) = (cid:70) | (cid:109)1, . . . (cid:109)(cid:66)   1)     (cid:50)(cid:116)(cid:84)!(cid:84)   (cid:70) (cid:120)(cid:66)"
(cid:106)(cid:88) (cid:97)(cid:28)(cid:75)(cid:84)(cid:72)(cid:50) (cid:105)(cid:63)(cid:50) (cid:77)(cid:50)(cid:116)(cid:105) (cid:114)(cid:81)(cid:96)(cid:47) (cid:109)(cid:66)

id4

topics: sequence-to-sequence learning     issue
    this is quite an unrealistic model.
    why?

e
l
p
m
a
s
s
d
r
o
w

ui

 

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

   you can   t cram the meaning of a 
whole %&!$# sentence into a 
single $&!#* vector!    ray mooney

ip

y
t
i
l
i
b
a
b
o
r
p
d
r
o
w

 

t
n
e
r
r
u
c
e
r

ezi

t
a
t
s

63

d
e
c
o
d
e
r

t
n
e
r
r
u
c
e
r

e hi

t
a
t
s

si

n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
d
r
o
w

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
c

wi

g
n
i
d
o
c
 
k

-
f
o
-
1

e = (economic, growth, has, slowed, down, in, recent, years, .)

e
n
c
o
d
e
r

id4

(cid:76)(cid:50)(cid:109)(cid:96)(cid:28)(cid:72) (cid:74)(cid:28)(cid:43)(cid:63)(cid:66)(cid:77)(cid:50) (cid:104)(cid:96)(cid:28)(cid:77)(cid:98)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:114)(cid:66)(cid:105)(cid:63) (cid:27)(cid:105)(cid:105)(cid:50)(cid:77)(cid:105)(cid:66)(cid:81)(cid:77) (cid:284) (cid:46)(cid:50)(cid:43)(cid:81)(cid:47)(cid:50)(cid:96)
f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)
e ui

 

64

topics: attention-based model
    encoder: bidirectional id56
    a set of annotation vectors
{h1, h2, . . . , ht}

    attention-based decoder

l
p
m
a
s
s

d
r
o
w

(1)compute attention weights
   t0,t / exp(e(zt0 1, ut0 1, ht))
(2)weighted-sum of the annotation vectors

n
o
i
t
a
t
o
n
n
a

s
r
o
t
c
e
v

hj

ct0 =pt

t=1   t0,tht
(3)use      instead of 

ct0

t
n
e
r
r
u
c
e
r

e zi

t
a
t
s

m
s
i
n
a
h
c
e

m

n
o
i
t
n
e
t
t

a

a

j

attention 
       weight

+

aj   =1

e = (economic, growth, has, slowed, down, in, recent, years, .)

(cid:27)(cid:105) (cid:50)(cid:28)(cid:43)(cid:63) (cid:105)(cid:66)(cid:75)(cid:50)(cid:98)(cid:105)(cid:50)(cid:84) (cid:66)(cid:77) (cid:105)(cid:63)(cid:50) (cid:47)(cid:50)(cid:43)(cid:81)(cid:47)(cid:50)(cid:96)(cid:44)
(cid:82)(cid:88) (cid:42)(cid:81)(cid:75)(cid:84)(cid:109)(cid:105)(cid:50)(cid:98) (cid:28) (cid:96)(cid:50)(cid:72)(cid:50)(cid:112)(cid:28)(cid:77)(cid:43)(cid:50) (cid:98)(cid:43)(cid:81)(cid:96)(cid:50) (cid:81)(cid:55) (cid:50)(cid:28)(cid:43)(cid:63) (cid:28)(cid:77)(cid:77)(cid:81)(cid:105)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77)
(cid:107)(cid:88) (cid:108)(cid:98)(cid:50) (cid:105)(cid:63)(cid:50) (cid:114)(cid:50)(cid:66)(cid:59)(cid:63)(cid:105)(cid:50)(cid:47) (cid:98)(cid:109)(cid:75) (cid:81)(cid:55) (cid:105)(cid:63)(cid:50) (cid:28)(cid:77)(cid:77)(cid:81)(cid:105)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77)(cid:98) (cid:28)(cid:98) (cid:28) (cid:43)(cid:81)(cid:77)(cid:105)(cid:50)(cid:116)(cid:105)

ht

(cid:85)(cid:34)(cid:28)(cid:63)(cid:47)(cid:28)(cid:77)(cid:28)(cid:109) (cid:50)(cid:105) (cid:28)(cid:72)(cid:88)(cid:45) (cid:107)(cid:121)(cid:82)(cid:57)(cid:86)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

id4

65

(cid:49)(cid:77)(cid:59)(cid:72)(cid:66)(cid:98)(cid:63)(cid:64)(cid:54)(cid:96)(cid:50)(cid:77)(cid:43)(cid:63)

topics: attention-based model
    encoder: bidirectional id56
    a set of annotation vectors
{h1, h2, . . . , ht}

    attention-based decoder

(cid:49)(cid:77)(cid:59)(cid:72)(cid:66)(cid:98)(cid:63)(cid:64)(cid:58)(cid:50)(cid:96)(cid:75)(cid:28)(cid:77)

(1)compute attention weights
   t0,t / exp(e(zt0 1, ut0 1, ht))
(2)weighted-sum of the annotation vectors

ct0 =pt

t=1   t0,tht
(3)use      instead of 

ct0

ht

(cid:65) (cid:43)(cid:81)(cid:77)(cid:55)(cid:50)(cid:98)(cid:98) (cid:105)(cid:63)(cid:28)(cid:105) (cid:65) (cid:98)(cid:84)(cid:50)(cid:28)(cid:70) (cid:77)(cid:50)(cid:66)(cid:105)(cid:63)(cid:50)(cid:96) (cid:54)(cid:96)(cid:50)(cid:77)(cid:43)(cid:63) (cid:77)(cid:81)(cid:96) (cid:58)(cid:50)(cid:96)(cid:75)(cid:28)(cid:77)(cid:88) (cid:97)(cid:81)(cid:96)(cid:96)(cid:118)(cid:53)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

66

deep natural language processing

   
67

deep natural language processing

(1) character-level modelling

   
note: translation is horrible, not because of me, but because data is :(

                                                                               d                                              

sequential processing

sequential processing

soft-alignment

also the effect of vitamin in d on cancer

is not clear

(luong et al., 2016; sennrich et al., 2016; chung et al., 2016; jean et al., 2016)

note: translation is horrible, not because of me, but because data is :(

but, there are still too much explicit structures here   

                                                                               d                                              

sequential processing

sequential processing

soft-alignment

also the effect of vitamin in d on cancer

is not clear

(luong et al., 2016; sennrich et al., 2016; chung et al., 2016; jean et al., 2016)

note: translation is horrible, not because of me, but because data is :(

why the hell are we using a sequence of words?

                                                                               d                                              

sequential processing

sequential processing

soft-alignment

also the effect of vitamin in d on cancer

is not clear

there are legitimate reasons    sorta   

1. we strongly believe that a word (lexeme) is a basic unit of meaning.
2. we have an inherent fear of data sparsity.

    the size of state space grows exponentially w.r.t. the length.
    a sentence is longer when counted in letters than in words.

3. we are worried that we cannot train a recurrent neural net.

but, are they really legit reasons?

we strongly believe that a word (lexeme) is a basic unit of meaning.
1.
2. we have an inherent fear of data sparsity.

    the size of state space grows exponentially w.r.t. the length.
    a sentence is longer when counted in letters than in words.
we are worried that we cannot train a recurrent neural net.

3.

   in the proposed model, it will so generalize because    similar    words are expected to have a similar feature 
vector, and because the id203 function is a smooth function of these feature values, a small change in 
the features will induce a small change in the id203    - bengio et al. (2003)

(bengio et al., 2003; xu & rudnicky, 2000)

but, are they really legit reasons?

we strongly believe that a word (lexeme) is a basic unit of meaning.
1.
2. we have an inherent fear of data sparsity.

    the size of state space grows exponentially w.r.t. the length.
    a sentence is longer when counted in letters than in words.
we are worried that we cannot train a recurrent neural net.

3.
   so, given a powerful learning system like an mid56, the convenience of using characters may outweigh 
the extra work of having to learn the words. all our experiments show that an mid56    nds it very easy to 
learn words.    - sutskever et al. (2011)

(sutskever et al., 2011; mikolov, 2012; graves, 2013)

but, are they really legit reasons?

1.
2.

we strongly believe that a word (lexeme) is a basic unit of meaning.
we have an inherent fear of data sparsity.
the size of state space grows exponentially w.r.t. the length.
   
    a sentence is longer when counted in letters than in words.

3. we are worried that we cannot train a recurrent neural net.

   training a recurrent network to learn long range input/output dependencies is a hard problem.    
- bengio et al. (1994)

(bengio et al., 1994; hochreither et al., 2001)

o n   t fe a r!!

d

f

m
t
s
l

+

c

i

c~
+

o

u
r
g

    
    

    

     

in
out

(hochreither & schmidhuber, 1999; gers et al., 2001; cho et al., 2014)

there are legitimate reasons    sorta   

1. we strongly believe that a word (lexeme) is a basic unit of meaning.
2. we have an inherent fear of data sparsity.

    the size of state space grows exponentially w.r.t. the length.
    a sentence is longer when counted in letters than in words.

3. we are worried that we cannot train a recurrent neural net.

note: translation is horrible, not because of me, but because data is :(

problems with treating each and every token separately

1.

inef   cient handling of various morphological variants
    sub-optimal segmentation/id121
       run   ,    runs   ,    ran   ,    running   : one lexeme    run   , but four independent vectors.

2. lack of generalization to novel/rare morphological variants

    for instance,                 in arabic =>    and to his vehicle   

3. one vector for compound words?

       kolmi/vaihe/kilo/watti/tunti/mittari    => one vector?
       kolme    => one vector?

(chung et al., 2016; almahairi et al., 2016; both & blunsom, 2014)

note: translation is horrible, not because of me, but because data is :(

obviously i   m not the    rst one to ask this question   

                                                                               d                                              

sequential processing

sequential processing

soft-alignment

also the effect of vitamin in d on cancer

is not clear

note: translation is horrible, not because of me, but because data is :(

                 

                                                                               d                                              

sequential processing

sequential processing

soft-alignment

also the effect of vitamin in d on cancer

is not clear

v i

t a m i n (luong & manning, 2016; ling et al., 2015ab; kim et al., 2015; 

ballesteros et al., 2015; dos santos & zadrozny, 2014)

addresses

1.

inef   cient handling of various morphological variants
    sub-optimal segmentation/id121
       run   ,    runs   ,    ran   ,    running   : one lexeme    run   , but four independent vectors.

2. lack of generalization to novel/rare morphological variants

    for instance,                 in arabic =>    and to his vehicle   
does not address

3. one vector for compound words?

       kolmi/vaihe/kilo/watti/tunti/mittari    => one vector?
       kolme    => one vector?

4. good segmentation/id121

still relies on

so, we decided to answer this question ourselves   

1. source side: a sequence of bpe-based character id165s
2. target side: an unbroken sequence of characters
     

                    

                 

                           

  

sequential processing

sequential processing

soft-alignment

also the effect of vitam   

in

in d on cancer

is not clear

bpe symbols
(sennrich et al., 2015)

absolutely the same model we have been using so far   

for a better recurrent decoder for character sequences, stay tuned for acl   16. jy will tell us more about it.

so, we decided to answer this question ourselves   

1. large-scale experiments: we want a convincing answer!
2. multiple languages: en       {cz, de, ru, fi}

  

                 

                    

     

                       

sequential processing

soft-alignment

   

sequential processing

also the effect of vitam   

bpe symbols
(sennrich et al., 2015)
for a better recurrent decoder for character sequences, stay tuned for acl   16. jy will tell us more about it.

in d on cancer

is not clear

in

the decoder implicitly learned word-like units automatically.

83

what have we learned?

1. neural mt works with character sequences.

    at least on the target side (though, it works also on the source side ;))

2. a recurrent network implicitly segments a character sequence automatically. 
3. we should   ve asked this question at the very beginning..

85

deep natural language processing

(2) multilingual modelling

   
multilingual translation

multilingual translation: bene   ts

1. positive language transfer across many language pairs/directions 

    solution to low/zero-resource machine translation

2. # of parameters grows linearly w.r.t. the # of languages 

    as opposed to the quadratic explosion when training many single-pair models.

3. multi-source translation without requiring any multi-way parallel text 

    inspired by but contrary to zoph & knight (2016)

4. super fun and cool!

    most important reason..

dong et al. (acl 2015)

decoder (german)

decoder (czech)

decoder (french)

   

alignment (de)

alignment (cz)

alignment (fr)

one-to-many neural mt
1. separate attention mechanism for each target 
2. no support for many source languages 
3. tested on rather small corpora (europarl v7)

encoder (english)

88

   

luong et al. (iclr, nov 2015)

89

many-to-many sequence-to-sequence learning
1. no attention: a single vector space shared across source and target languages/tasks. 
2. limited set of languages tested: english, german + many other tasks

decoder (german)

parser

encoder (german)

encoder (english)

challenges

1. we have a strong belief that (soft-)alignment is speci   c to a language pair. 
2. even if not, there   s a gigantic model space. how can we design a network? 
3. 6 languages (en, cs, de, fi, fr, ru) 

    60+ million bilingual sentence pairs for training 
    the entire model does not    t on one gpu 

is our goal too ambitious? :(

multi-way, multilingual translation
1. 10 language pair   directions from wmt   15 

91

    en        {cs, de, fi, fr, ru}, {cs, de, fi, fr, ru}       en 
2. one alignment model for all the ten pair   directions. 
3. trained with bilingual parallel pairs only 
4. the model was distributed over two gpu   s

for details,    nd this guy!

(firat et al., 2016a)

multi-way, multilingual translation

92

for details,    nd this guy!

settings
    uzbek        english, turkish       english 

1. target language pairs 

2. auxiliary language pairs 

    french        english, spanish       english

93

(firat et al., 2016b; under review)
work done in collaboration with ibm

94

turkish-to-english

1. tr-en: 14.21/17.28 
2. tr-en+es-en: 16.00/17.75
3. tr-en+es-en+fr-en: 16.18/18.13
4. tr-en+es-en+fr-en+en-es: 16.28/18.66
5. ensemble: 20.00/22.56 
    3x tr-en+es-en+fr-en 
    3x tr-en+es-en+fr-en+en-es

alignment

(firat et al., 2016b; under review)
work done in collaboration with ibm

quebec

uzbek-to-english

95

1. uz-en: 6.63/6.45
2. uz-en+tr-en: 8.68/9.34
3. uz-en+tr-en+es-en: 9.55/10.34
4. uz-en+tr-en+es-en+en-tr: 8.93/9.41
5. ensemble: 12.17/12.99 
    3x uz-en+tr-en+es-en 
    3x uz-en+tr-en+es-en+en-tr

alignment

(firat et al., 2016b; under review)
work done in collaboration with ibm

settings

96

1. three languages: english, spanish and french 

    {en, es, fr}         {en, es, fr} 

2. bilingual corpora only during training: en       {es, fr}, {es, fr}       en 
3. multi-language source during test time

alignment

(firat et al., emnlp 2016c)
work done in collaboration with ibm

multi-source translation?

97

(es, fr)        en 

1.
2. two translation strategies
late averaging

early averaging*

alignment

alignment

* (zoph & knight, 2016)
(firat et al., emnlp 2016c)
work done in collaboration with ibm watson r&d

multi-source translation? - yes
single-source translation

multi-source translation

98

but, single-pair models can apparently do multi-source translation..

(firat et al., emnlp 2016c)
work done in collaboration with ibm watson r&d

99

deep natural language processing

(3) larger-context modelling

   
context matters

    what does context tell us? 

    theme/topic of a document 

    what does context tell us in practice? 

    what are the words that are more likely to appear in this document?

context

following sentence

while it's not    awless, some motivations and scenarios 
remain  somewhat  underdeveloped  or  questionable; 
ex_machina is a stunning sci-fi vision that is also a fully 
formed thinking man's thriller. 

with  a  jaw  droopingly  good  turn  from  the 
soon to be megastar vikander, ____?____ is 
another excellent example of what makes the 
____?____

larger-context language modelling
    language modelling as    document modelling    instead of    sentence modelling      

(wang & cho, arxiv 2015; ji et al., arxiv 2015)   
p (d)     p (s1)p (s2)       p (sn )

vs.

    simplest approach (wang & cho, arxiv 2015) 

p (d)     p (s1)p (s2|s1)       p (sn|sn n, . . . , sn 1)

    bag of all the words from the    

n

previous      sentences 
    id56 language model    

conditioned on this bag-of-words

hsi

z0

w1

z1

w2

z2

w3

z3

w4

z4

(sl n, sl n+1, . . . , sl 1)

larger-context language modelling

    late fusion of lstm (wang & cho, arxiv 2015) 

    let the memory cell      model intra-sentence dependencies 
    let the inter-sentence dependencies be fused in later

c

early fusion

late fusion

larger-context language modelling

    it helps obviously (wang & cho, arxiv 2015) 

    especially with the late fusion of context

larger-context language modelling

       what are the words that are more likely to appear in this document?    

    open-class words: nouns, adjectives, verbs and adverbs

imdb

ptb

larger-context machine translation

    toward larger-context machine translation (jean & cho, work in progress*) 

    how to represent the source and target contexts? 
    which is conditioned on the context, encoder, decoder or both?
y2

y1

hsi

fsummary

z0

z1

z2

 y l n, y l n+1, . . . , y l 1 
 x l n, x l n+1, . . . , x l 1 

y3

z3

y4

z4

0

h1

x1

h2

x2

h3

x3

h4

=

c

x4

* unless chris dyer posts something on arxiv tomorrow

dialogue-level machine translation

    hierarchical model for dialogue modelling (serban et al., 2015; sordoni et al., 2015)   
utterance-level id56 + dialogue-level id56

world-context machine translation

    beyond document-level language processing 

    how do we blend intra-document context and world knowledge?

chapter 1

chapter 2

document

p (sl|s1, s2, . . . , sl 1,
d1, d2, . . . , dm )

world  
knowledge

108

thank you!

