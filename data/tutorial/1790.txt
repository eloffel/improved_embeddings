neural networks 
mlss 2015 summer school 

rob fergus  
facebook ai research 

 

 
 

[on leave from courant institute, new york university] 

overview 

(cid:80)    look at some of the recent progress with neural 
net / deep learning models 
      assume familiarity with basic neural nets  

(cid:80)    non-exhaustive coverage  

      huge number of recent papers 

(cid:80)    draw on material from fair colleagues 

      experts in nlp, speech etc. 

thanks! 

facebook ai research colleagues & nyu phd students: 

manohar paluri  antoine bordes 

yaniv taigman 

soumith 
chintala 

emily denton 

jason weston  tomas mikolov  ronan collobert 

marc   aurelio 
ranzato 

sainbayar 
sukhbaatar 

schedule 

(cid:80)    overview 
(cid:80)    vision   

  
  

 
 

  
  

      convnets for image recognition 
      other applications 

 

  

(cid:80)    speech   
  
(cid:80)    nlp  (id56s, lstms)  
(cid:80)    unsupervised learning   
(cid:80)    memory in neural nets 
 

 [~15m] 
 [~1h50m] 

 [~5m] 
 [~20m] 
  [~1h] 
 [~30m] 

motivation 

(cid:80)    a lot of recent successes with large neural 
networks, trained with supervision. 

(cid:80)    id171 crucial to performance in 
many tasks 

(cid:80)    still many open problems however! 

traditional ml approach 

for classi   cation: 

input  
data 

hand-designed 

feature 
extraction 

(cid:80)    features are not learned 

trainable 
classi   er 

class 

prediction 

(cid:80)    trainable classi   er is often generic (e.g. id166) 

case study: object recognition ~2010 

(cid:80)    multitude of hand-designed features currently in use 

       sift, hog, lbp, mser, color-sift            . 

(cid:80)    where next? better classi   ers? or keep building more 

features? 

felzenszwalb,  girshick,  

mcallester and ramanan, pami 2007 

yan & huang  

(winner of pascal 2010 classi   cation competition) 

hand-crafted features + id166 

(cid:80)    lp-    multiple kernel learning 

      gehler and nowozin, on feature combination 
for multiclass object classi   cation, iccv   09 

(cid:80)    39 di   erent kernels 
      phog, sift, v1s+, 
region cov.  etc.   

(cid:80)    mkl only gets  
 few % gain over  
 averaging features 
      features are  
doing the work 

what limits performance? 

(cid:80)    ablation studies on deformable parts model  
       felzenszwalb, girshick, mcallester, ramanan, pami   10 
(cid:80)    replace each part with humans (amazon turk): 

parikh & zitnick, cvpr   10 

   

 

 

 

 

 

 

 

what about learning the features? 

(cid:80)    learn hierarchy 
(cid:80)    all the way from pixels       classi   er 
(cid:80)    one layer extracts features from output of previous layer 

image 
pixels 

layer 1 

layer 2 

layer 3 

trainable 
classi   er 

class 

prediction 

(cid:80)    train all layers jointly 

deep  

learning 

supervised 

recurrent neural net 

convolutional neural net 

boosting 

neural net 

deep (sparse/denoising)  
autoencoder 

deep 

sp 

id88 

id166 
shallow 
autoencoderneural net 

sparse coding 

gmm 

deep belief net 

restricted bm 

bayesnp 

unsupervised 

slide: m. ranzato 

deep learning is  b i g 

 

d
r
a
w
r
o
f
-
d
e
e

       neural nets 
       conv nets 

       main types of deep architectures 
 
 
 
 
 
 
 
 

 
k
c
a
b
-
d
e
e
f

input 

f

input 

 
l
a
n
o
i
t
c
e
r
i
d
-
i

       stacked  
auto-
encoders 
       dbm 

 
t
n
e
r
r
u
c
e
r

       hierar. sparse coding 
       deconv nets 

       recurrent neural nets 
       recursive nets 
       lista 

b

input 

input 

13	
   

ranzato 

deep learning is  b i g 

       main types of learning protocols 
       purely supervised 

 

       backprop + sgd 
       good when there is lots of labeled data. 

       layer-wise unsupervised + superv. linear classifier 

       train each layer in sequence using regularized auto-encoders 

       hold fix the feature extractor, train linear classifier on features 
       good when labeled data is scarce but there is lots of unlabeled 

or rbms 

data. 

       layer-wise unsupervised + supervised backprop 

       train each layer in sequence 
       backprop through the whole system 
       good when learning problem is very difficult. 

14	
   

ranzato 

 

 

 
 
 

deep learning is  b i g 

       main types of learning protocols 
       purely supervised 

 

       backprop + sgd 
       good when there is lots of labeled data. 

       layer-wise unsupervised + superv. linear classifier 

       train each layer in sequence using regularized auto-encoders 

       hold fix the feature extractor, train linear classifier on features 
       good when labeled data is scarce but there is lots of unlabeled 

or rbms 

data. 

       layer-wise unsupervised + supervised backprop 

       train each layer in sequence 
       backprop through the whole system 
       good when learning problem is very difficult. 

15	
   

ranzato 

 

 

 
 
 

deep learning 

for 

id161 

supervised 

recurrent neural net 

convolutional neural net 

boosting 

neural net 

deep (sparse/denoising) autoencoder 

id88 

id166 
shallow 
autoencoderneural net 

sparse coding 

deep belief net 

restricted bm 

gmm 

deep 

sp 

bayesnp 

unsupervised 

slide: m. ranzato 

convolutional neural networks 

(cid:80)    lecun et al. 1989 
(cid:80)    neural network with specialized 
connectivity structure 
      [everyone ok with basic nn?] 

multistage  hubel  wiesel  architecture  

(cid:80)    stack multiple stages of simple cells / complex cells layers 
(cid:80)    higher stages compute more global, more invariant features 
(cid:80)    classi   cation layer on top 
 
history: 
(cid:80)    neocognitron [fukushima 1971-1982] 
(cid:80)    convolutional nets [lecun 1988-2007]  
(cid:80)    hmax [poggio 2002-2006] 
(cid:80)    many others   . 
 

slide: y.lecun 

overview of convnets 

(cid:80)    feed-forward:  
       convolve input 
       non-linearity (recti   ed linear) 
       pooling (local max) 

(cid:80)    supervised 
(cid:80)    train convolutional    lters by  

back-propagating classi   cation error 

feature maps 

pooling 

non-linearity 

convolution (learned) 

input image 

lecun et al. 1998 

convnet successes 

(cid:80)    handwritten text/digits 

       mnist      (0.17% error [ciresan et al. 2011]) 
       arabic & chinese   [ciresan et al. 2012] 

(cid:80)    simpler  recognition benchmarks 

       cifar-10 
       tra   c sign recognition 

 (9.3% error [wan et al. 2013]) 

(cid:80)    0.56% error vs 1.16% for humans [ciresan et al. 2011] 

(cid:80)    but less good at more complex datasets 

       e.g. caltech-101/256 (few training examples)  

application to id163 

 

 

 

 

(cid:80)    ~14 million labeled images, 20k classes 
(cid:80)    images gathered from internet 
(cid:80)    human labels via amazon turk  

[deng et al. cvpr 2009]  

 

 

[nips 2012] 

 

 

 

goal 

(cid:80)    image recognition 
      pixels       class label 
 

[krizhevsky et al. nips 2012] 

krizhevsky et al. [nips2012] 

(cid:80)    same model as lecun   98 but: 
  -   bigger model  (8 layers) 
-    more data    (106 vs 103 images) 
-    gpu implementation (50x speedup over cpu) 
-    better id173 (dropout) 

(cid:80)    7 hidden layers, 650,000 neurons, 60,000,000 parameters 
(cid:80)    trained on 2 gpus for a week 

id163 classification (2010     2015) 

convolutional  
neural nets 

30 

25 

 
)

%

(
 
r
o
r
r

 

e
n
o
i
t
a
c
   
i
s
s
a
l
c
 
5
-
p
o
t

20 

15 

10 

5 

0 

2010 

2011 

2012 

2013 

2014 

human 

2015 

examples 

(cid:80)    from clarifai.com 

examples 

(cid:80)    from clarifai.com 

examples 

(cid:80)    from clarifai.com 

using features on other datasets 

(cid:80)    train model on id163 2012 training set 

(cid:80)    re-train classi   er on new dataset 

      just the top layer (softmax) 

(cid:80)    classify test set of new dataset 

caltech 256 

zeiler & fergus, visualizing and understanding convolutional networks, arxiv 1311.2901, 2013 

(cid:8)
(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)

75
70
65
60
55
50
45
40
35
30
25
 
0

10

 

60

(cid:37)(cid:82)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
(cid:54)(cid:82)(cid:75)(cid:81)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
50

20
(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:83)(cid:72)(cid:85)(cid:239)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)

30

40

caltech 256 

zeiler & fergus, visualizing and understanding convolutional networks, arxiv 1311.2901, 2013 

(cid:8)
(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)

75
70
65
60
55
50
45
40
35
30
25
 
0

10

6 training examples 

(cid:50)(cid:88)(cid:85)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)
(cid:37)(cid:82)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
(cid:54)(cid:82)(cid:75)(cid:81)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
50

20
(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:83)(cid:72)(cid:85)(cid:239)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)

30

40

 

60

the details 

(cid:80)    operations in each layer 

(cid:80)    architecture 

(cid:80)    training 

(cid:80)    results 

components of each layer 

pixels / 
features 

filter with  
learned dictionary 

non-linearity 
 

spatial local 
max pooling 

output features 

filtering 

(cid:80)    convolution 

       filter is learned during training 
       same    lter at each location 

.
.
.

input 

feature map 

filtering 

(cid:80)    local 

       each unit layer above  
look at local window 
 
       but no weight tying 

input 
(cid:80)    e.g. face recognition 

filters 

filtering 

(cid:80)    tiled 

       filters repeat every n 
       more    lters than 

convolution for given 
# features 

input 

filters 

feature maps 

non-linearity 

(cid:80)    recti   ed linear function 

      applied per-pixel 
      output = max(0,input) 

input feature map	
   

output feature map	
   

black	
   =	
   nega   ve;	
   white	
   =	
   posi   ve	
   values	
   

only	
   non-     nega   ve	
   values	
   

non-linearity 

(cid:80)    other choices: 

      tanh 
      sigmoid: 1/(1+exp(-x)) 
      prelu 

 

[delving deep into recti   ers: 
surpassing human-level 
performance on id163 
classi   cation, kaiming he et 
al. arxiv:1502.01852v1.pdf, 
feb 2015 ] 
 

 

 

 

f (yi) =(yi,

 

 

 

aiyi,

f (y)

if yi > 0
if yi     0

.

f (y) = ay

f (y) = y

y

pooling 

(cid:80)    spatial pooling 

      non-overlapping / overlapping regions 
      sum or max 
      boureau et al. icml   10 for theoretical analysis 

max 

sum 

pooling  

(cid:80)    pooling across feature groups 

(cid:80)    additional form of inter-feature competition 
(cid:80)    maxout networks [goodfellow et al. icml 2013] 

pooled
map 1

pooled
map 2

feature
map 1 

feature
map 4

role of pooling  

(cid:80)    spatial pooling 

      invariance to small transformations 
      larger receptive    elds  
(see more of input) 

visualization technique from 
[le et al. nips   10]: 

 
videos from: http://ai.stanford.edu/~quocle/tid98web 

zeiler, fergus [arxiv 2013] 

components of each layer 

pixels / 
features 

filter with  
learned dictionary 

non-linearity 
 

spatial local 
max pooling 

[optional] 
id172 

across data/features 

output  
features 

id172 

(cid:80)    contrast id172 across features 
(cid:80)    see divisive id172 in neuroscience  

input 

filters 

id172 

(cid:80)    contrast id172 (across feature maps) 

      local mean = 0, local std. = 1,    local          7x7 gaussian  
      equalizes the features maps 

feature maps 
 

feature maps 

after contrast id172 

role of feature id172  

(cid:80)    introduces local competition between features 

           explaining away    in id114 
         just like top-down models 
         but more local mechanism 

(cid:80)    also helps to scale activations at each layer better for learning 

       makes energy surface more isotropic 
       so each gradient step makes more progress 

(cid:80)    empirically, seems to help a bit (1-2%) on id163 
(cid:80)    most recent models don   t seem to have use though 

 

 

id172 across data 

(cid:80)    batch id172 
[batch id172: accelerating deep network training by reducing internal 
covariate shift, sergey io   e, christian szegedy, arxiv:1502.03167] 

input: values of x over a mini-batch: b = {x1...m};
output: {yi = bn  ,  (xi)}

parameters to be learned:   ,   

// mini-batch mean

  b    

xi

1
m

1
m

m(cid:31)i=1
m(cid:31)i=1
xi       b(cid:29)  2

// mini-batch variance

(xi       b)2

  2
b    
(cid:30)xi    
yi       (cid:30)xi +        bn  ,  (xi)
algorithm 1: batch normalizing transform, applied to
activation x over a mini-batch.

// normalize
// scale and shift

b +   

0.8

0.7

0.6

0.5

0.4

inception
bn   baseline
bn   x5
bn   x30
bn   x5   sigmoid
steps to match inception
30m

25m

5m

10m

15m

20m

figure 2: single crop validation accuracy of inception
and its batch-normalized variants, vs.
the number of
training steps.

overview of convnets 

(cid:80)    feed-forward:  
       convolve input 
       non-linearity (recti   ed linear) 
       pooling (local max) 

(cid:80)    supervised 
(cid:80)    train convolutional    lters by  

back-propagating classi   cation error 

feature maps 

pooling 

non-linearity 

convolution (learned) 

input image 

lecun et al. 1998 

architecture 

(cid:80)    big issue: how to select 

      manual tuning of features       manual tuning of 
architechtures 

(cid:80)    depth 
(cid:80)    width 
(cid:80)    parameter count 

how to choose architecture 

(cid:80)    many hyper-parameters: 
      # layers, # feature maps 

(cid:80)    cross-validation  
(cid:80)    grid search (need lots of gpus) 
(cid:80)    smarter strategies: 

      random [bergstra & bengio jmlr 2012] 
      gaussian processes [hinton??] 

how important is depth 

(cid:80)       deep    in deep learning 

(cid:80)    ablation study 

(cid:80)    tap o    features 

architecture of krizhevsky et al.  

(cid:80)    8 layers total 
(cid:80)    trained on id163 
dataset [deng et al. cvpr   09] 
(cid:80)    18.2% top-5 error  
(cid:80)    our reimplementation: 

 18.1% top-5 error 

softmax output 

layer 7: full 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

(cid:80)    remove top fully 
connected layer  
      layer 7 

(cid:80)    drop 16 million 
parameters 
(cid:80)    only 1.1% drop in 
performance! 

softmax output 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

(cid:80)    remove both fully connected 
layers  
      layer 6 & 7 

(cid:80)    drop ~50 million parameters 

(cid:80)    5.7% drop in performance 

softmax output 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

(cid:80)    now try removing upper feature 
extractor layers: 
      layers 3 & 4 

(cid:80)    drop ~1 million parameters 

(cid:80)    3.0% drop in performance 

softmax output 

layer 7: full 

layer 6: full 

layer 5: conv + pool 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

(cid:80)    now try removing upper feature 
extractor layers & fully connected: 
      layers 3, 4, 6 ,7 
(cid:80)    now only 4 layers 
(cid:80)    33.5% drop in performance 
 
        depth of network is key 

 

softmax output 

layer 5: conv + pool 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

tapping off features at each layer 

plug features from each layer into linear id166 or soft-max	
   

translation (vertical) 

 
t
u
p
t
u
o

)
s
s
a
c
 

l

e
u
r
t
(

p

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
   60

   40

(cid:20)(cid:19)
9
8
7
(cid:25)
5
(cid:23)
3
(cid:21)
1
(cid:19)
 
(cid:239)(cid:25)(cid:19)

(cid:239)(cid:23)(cid:19)

 

lawn mower
(cid:54)(cid:75)(cid:76)(cid:75)(cid:239)(cid:55)(cid:93)(cid:88)
african crocodile
african grey
entertrainment center

(cid:239)(cid:21)(cid:19)
vertical translation (pixels)

(cid:21)(cid:19)

(cid:19)

(cid:23)(cid:19)

(cid:25)(cid:19)

i

 
7
e
c
n
 
a
t
s
r
d
e
y
a
l

 
l
a
c
n
o
n
a
c

i

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
 
   60

   40

 

40

60

 

40

60

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

   20
vertical translation (pixels)

20

0

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

   20
vertical translation (pixels)

20

0

 
1
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

scale invariance 

 
t
u
p
t
u
o

)
s
s
a
c
 

l

e
u
r
t
(

p

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
1

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

1.2

1.4
scale (ratio)

1.6

1.8

i

 
1
e
c
n
 
a
t
r
s
d
e
y
a
l

 
l
a
c
n
o
n
a
c

i

12

10

8

6

4

2

0
 
1

 

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

1.2

1.4
scale (ratio)

1.6

1.8

 
7
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

0.7

0.6

0.5

0.4

0.3

0.2

0.1

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

0
 
1

1.2

1.4
scale (ratio)

1.6

1.8

 

 

rotation invariance 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0

1.4

1.2

1

0.8

0.6

0.4

0.2

l

 
t
u
p
t
u
o

)
s
s
a
c
 
e
u
r
t
(

p

i

 
7
e
c
n
 
a
t
s
r
d
e
y
a
l

a
c
n
o
n
a
c

 
l

i

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

50

100

150

200

rotation degrees

 

250

300

350

 

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

150

200

rotation degrees

250

300

350

0
 
0

50

100

i

 
1
e
c
n
 
a
t
r
s
d
e
y
a
l

 
l
a
c
n
o
n
a
c

i

15

10

5

0
 
0

50

100

 

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

150

200

rotation degrees

250

300

350

very deep models (1) 

[very deep convolutional networks for large-scale image recognition, 
karen simonyan & andrew zisserman, arxiv:1409.1556, 2014] 
 

y

a
(cid:7)(cid:7) (cid:56)(cid:38)(cid:42)(cid:40)(cid:41)(cid:53)
(cid:45)(cid:34)(cid:58)(cid:38)(cid:51)(cid:52)

a-lrn
(cid:7)(cid:7) (cid:56)(cid:38)(cid:42)(cid:40)(cid:41)(cid:53)
(cid:45)(cid:34)(cid:58)(cid:38)(cid:51)(cid:52)

conv3-64

conv3-64
lrn

conv3-128

conv3-128

conv3-256
conv3-256

conv3-256
conv3-256

conv3-512
conv3-512

conv3-512
conv3-512

conv3-512
conv3-512

conv3-512
conv3-512

d
(cid:7)(cid:12) (cid:56)(cid:38)(cid:42)(cid:40)(cid:41)(cid:53)
(cid:45)(cid:34)(cid:58)(cid:38)(cid:51)(cid:52)

e

(cid:7)(cid:14) (cid:56)(cid:38)(cid:42)(cid:40)(cid:41)(cid:53)
(cid:45)(cid:34)(cid:58)(cid:38)(cid:51)(cid:52)

conv3-64
conv3-64
conv3-128
conv3-128
conv3-256
conv3-256
conv3-256

conv3-512
conv3-512
conv3-512

conv3-512
conv3-512
conv3-512

conv3-64
conv3-64
conv3-128
conv3-128
conv3-256
conv3-256
conv3-256
conv3-256
conv3-512
conv3-512
conv3-512
conv3-512
conv3-512
conv3-512
conv3-512
conv3-512

(cid:20)(cid:48)(cid:47)(cid:55)(cid:27)(cid:38)(cid:53) (cid:20)(cid:48)(cid:47)(cid:62)(cid:40)(cid:54)(cid:51)(cid:34)(cid:53)(cid:42)(cid:48)(cid:47)
b
(cid:7)(cid:12) (cid:56)(cid:38)(cid:42)(cid:40)(cid:41)(cid:53)
(cid:7)(cid:9) (cid:56)(cid:38)(cid:42)(cid:40)(cid:41)(cid:53)
(cid:45)(cid:34)(cid:58)(cid:38)(cid:51)(cid:52)
(cid:45)(cid:34)(cid:58)(cid:38)(cid:51)(cid:52)

c

(cid:42)(cid:47)(cid:49)(cid:54)(cid:53) (cid:1)224    224 (cid:29)(cid:24)(cid:19) (cid:42)(cid:46)(cid:34)(cid:40)(cid:38)(cid:2)

conv3-64
conv3-64
conv3-128
conv3-128
conv3-256
conv3-256

(cid:46)(cid:34)(cid:57)(cid:49)(cid:48)(cid:48)(cid:45)

(cid:46)(cid:34)(cid:57)(cid:49)(cid:48)(cid:48)(cid:45)

conv3-64
conv3-64
conv3-128
conv3-128
conv3-256
conv3-256
conv1-256

(cid:46)(cid:34)(cid:57)(cid:49)(cid:48)(cid:48)(cid:45)

conv3-512
conv3-512

(cid:46)(cid:34)(cid:57)(cid:49)(cid:48)(cid:48)(cid:45)

conv3-512
conv3-512

conv3-512
conv3-512
conv1-512

conv3-512
conv3-512
conv1-512

(cid:46)(cid:34)(cid:57)(cid:49)(cid:48)(cid:48)(cid:45)
fc-4096
fc-4096
fc-1000
(cid:52)(cid:48)(cid:39)(cid:53)(cid:4)(cid:46)(cid:34)(cid:57)

(cid:31)(cid:34)(cid:35)(cid:45)(cid:38) (cid:8)(cid:15) number of parameters (cid:1)(cid:42)(cid:47) (cid:46)(cid:42)(cid:45)(cid:45)(cid:42)(cid:48)(cid:47)(cid:52)(cid:2)(cid:5)
d
138

(cid:27)(cid:38)(cid:53)(cid:56)(cid:48)(cid:51)(cid:44)
(cid:27)(cid:54)(cid:46)(cid:35)(cid:38)(cid:51) (cid:48)(cid:39) (cid:49)(cid:34)(cid:51)(cid:34)(cid:46)(cid:38)(cid:53)(cid:38)(cid:51)(cid:52)

a,a-lrn

c
134

b
133

133

d

e

e
144

non-linearity than single 7x7 layer 

(cid:80)    lots of 3x3 conv layers: more 
(cid:80)    close to soa results on 
id163: 6.8% top-5 val 
(cid:80)    can be hard to train 

(cid:21)(cid:55)(cid:54)(cid:61)(cid:30)(cid:45)(cid:59) (cid:43)(cid:55)(cid:54)(cid:69)(cid:47)(cid:6) (cid:2)(cid:35)(cid:41)(cid:42)(cid:52)(cid:45) (cid:8)(cid:3)
a
a-lrn
b
c

table 3: convnet performance at a single test scale.

(cid:59)(cid:55)(cid:56)(cid:5)(cid:8) (cid:61)(cid:41)(cid:52)(cid:6) (cid:45)(cid:57)(cid:57)(cid:55)(cid:57) (cid:2)(cid:1)(cid:3)

(cid:59)(cid:55)(cid:56)(cid:5)(cid:12) (cid:61)(cid:41)(cid:52)(cid:6) (cid:45)(cid:57)(cid:57)(cid:55)(cid:57) (cid:2)(cid:1)(cid:3)

(cid:58)(cid:53)(cid:41)(cid:52)(cid:52)(cid:45)(cid:58)(cid:59) (cid:49)(cid:53)(cid:41)(cid:47)(cid:45) (cid:58)(cid:49)(cid:44)(cid:45)
(cid:59)(cid:45)(cid:58)(cid:59) (cid:2)q(cid:3)
(cid:59)(cid:57)(cid:41)(cid:49)(cid:54) (cid:2)s(cid:3)
256
256
256
256
(cid:10)(cid:15)(cid:11)
(cid:10)(cid:15)(cid:11)
256
(cid:10)(cid:15)(cid:11)
(cid:10)(cid:15)(cid:11)
256
(cid:10)(cid:15)(cid:11)
(cid:10)(cid:15)(cid:11)

256
256
256
256
(cid:10)(cid:15)(cid:11)
256
(cid:10)(cid:15)(cid:11)
256
(cid:10)(cid:15)(cid:11)

(cid:39)(cid:9)(cid:12)(cid:13)(cid:18)(cid:12)(cid:8)(cid:9)(cid:40)

(cid:39)(cid:9)(cid:12)(cid:13)(cid:18)(cid:12)(cid:8)(cid:9)(cid:40)

(cid:39)(cid:9)(cid:12)(cid:13)(cid:18)(cid:12)(cid:8)(cid:9)(cid:40)

(cid:9)(cid:16)(cid:6)(cid:13)
(cid:9)(cid:16)(cid:6)(cid:14)
(cid:9)(cid:15)(cid:6)(cid:14)
(cid:9)(cid:15)(cid:6)(cid:8)
(cid:9)(cid:15)(cid:6)(cid:8)
(cid:9)(cid:14)(cid:6)(cid:10)
(cid:9)(cid:14)(cid:6)(cid:7)
(cid:9)(cid:13)(cid:6)(cid:15)
(cid:9)(cid:12)(cid:6)(cid:13)
(cid:9)(cid:14)(cid:6)(cid:10)
(cid:9)(cid:13)(cid:6)(cid:16)
25.5

(cid:8)(cid:7)(cid:6)(cid:11)
(cid:8)(cid:7)(cid:6)(cid:12)
(cid:16)(cid:6)(cid:16)
(cid:16)(cid:6)(cid:11)
(cid:16)(cid:6)(cid:10)
(cid:15)(cid:6)(cid:15)
(cid:15)(cid:6)(cid:15)
(cid:15)(cid:6)(cid:14)
(cid:15)(cid:6)(cid:8)
(cid:16)(cid:6)(cid:7)
(cid:15)(cid:6)(cid:14)
8.0

very deep models (2) 

[going deep with convolutions, szegedy et al., arxiv:1409.4842, 2014] 
 

googlenet inception module: 
 
1.    multiple filter scales at each layer 
2.    id84 to keep computational requirements down 

  

 

number of 
filters 

1x1 

3x3 

5x5 

[from http://image-net.org/challenges/
lsvrc/2014/slides/googlenet.pptx] 

(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:22)(cid:91)(cid:22)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:24)(cid:91)(cid:24)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:41)(cid:76)(cid:79)(cid:87)(cid:72)(cid:85)(cid:3)(cid:70)(cid:82)(cid:81)(cid:70)(cid:68)(cid:87)(cid:72)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:51)(cid:85)(cid:72)(cid:89)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:22)(cid:91)(cid:22)(cid:3)(cid:80)(cid:68)(cid:91)(cid:3)(cid:83)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)googlenet vs previous models 

[going deep with convolutions, szegedy et al., arxiv:1409.4842, 2014] 
 

googlenet  

convolution 
pooling 
softmax 
other 

zeiler-fergus architecture (1 tower) 

[from http://image-net.org/challenges/
lsvrc/2014/slides/googlenet.pptx] 

google inception model 
512 

256  480 

480 

832  1024 

832 

512  512 

width of inception modules ranges from 256 filters (in early modules) to 1024 
in top inception modules. 
 
can remove fully connected layers on top completely 
 
number of parameters is reduced to 5 million 
 
6.7% top-5 validation error on imagnet 
 
 

computional cost is 
increased by less than 2x 
compared to krizhevsky   s 
network. (<1.5bn operations/
evaluation) 
 

[from http://image-net.org/challenges/
lsvrc/2014/slides/googlenet.pptx] 

visualizing convnets 

(cid:80)    want to know what they are learning 

(cid:80)    raw coe   cients of learned    lters in higher 
layers di   cult to interpret 

(cid:80)    two classes of method: 

1.    project activations back to pixel space 
2.    optimize input image to maximize a particular 

feature map or class 

visualizing convnets 

(cid:80)    projection from higher layers back to input 
      several similar approaches: 
      visualizing and understanding convolutional 
networks, matt zeiler & rob fergus, eccv 2014 
      deep inside convolutional networks: visualising 
image classi   cation models and saliency maps, 
karen simonyan, andrea vedaldi, andrew zisserman, 
arxiv 1312.6034, 2013 
      object detectors emerge in deep scene id98s, bolei 
zhou, aditya khosla, agata lapedriza, aude oliva, 
antonio torralba, iclr 2015 

projection from higher layers 

[zeiler et al. eccv14]	
   

0	
   

....	
   

0	
   

feature	
   

map	
   

filters	
   

....	
   

filters	
   

	
   

t
e
n
v
n
o
c
e
d

layer 2 reconstruction 

layer 2: feature maps 

layer 1 reconstruction 

layer 1: feature maps 

c
o
n
v
n
e
t

	
   

visualization 

input image 

details of operation 

deconvnet layer 

convnet layer 

unpooling operation 

layer 1 filters 

visualizations of higher layers 

(cid:80)    use id163 2012 validation set 
(cid:80)    push each image through network 

 

feature	
   

map	
   

....	
   

filters	
   

lower	
   layers	
   

input	
   	
   
image	
   

validation images 

(cid:80)    take max activation from 

feature map associated 
with each    lter 

(cid:80)    use deconvnet to project 

back to pixel space 

(cid:80)    use pooling    switches    

peculiar to that activation 

layer 1: top-9 patches 

layer 2: top-1 

layer 2: top-9 

(cid:80)    not samples from model 
(cid:80)   
(cid:80)    non-parametric view on invariances learned by model 

 just parts of input image that give strong activation of this feature map 

layer 2: top-9 patches 

(cid:80)    patches from validation images that give maximal activation of a given feature map  

layer 3: top-1 

layer 3: top-9 

layer 3: top-9 patches 

layer 4: top-1 

layer 4: top-9 

layer 4: top-9 patches 

layer 5: top-1 

layer 5: top-9 

layer 5: top-9 patches 

visualizing convnets 

(cid:80)    optimize input to maximize particular ouput 
      lots of approaches, e.g. erhan et al.  [tech report 
2009], le et al. [nips 2010]. 
      depend on initialization 

(cid:80)    google deepdream  

[http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-
into-neural.html] 
       maximize    banana    

output  

google deepdream 

https://photos.google.com/share/
f1qippx0scl7ozwilt9lnuqliattx4oucj_8ep65_ctvnbms1jnygsgqaiequc1vqwdgq/photo/
af1qipmytxpt0tvz0q5kubkgw8vaq2isxbul02wkzafb?
key=avbxwjhwszg2rjjwlwruvfbbzen1d205budemnhb 

training big convnets 

(cid:80)    stochastic id119 

      compute (noisy estimate of) gradient on small batch 
of data & make step 
      take as many steps as possible (even if they are noisy) 
      large initial learning rate 
      anneal learning rate 

(cid:80)    momentum 

      variants [sutskever icml 2012] 

annealing of learning rate   

(cid:80)    start large, slowly reduce 
(cid:80)    explore di   erent scales of energy surface 

evolution of features during training 

evolution of features during training 

id172 across data 

(cid:80)    batch id172 
[batch id172: accelerating deep network training by reducing internal 
covariate shift, sergey io   e, christian szegedy, arxiv:1502.03167] 

input: values of x over a mini-batch: b = {x1...m};
output: {yi = bn  ,  (xi)}

parameters to be learned:   ,   

// mini-batch mean

  b    

xi

1
m

1
m

m(cid:31)i=1
m(cid:31)i=1
xi       b(cid:29)  2

// mini-batch variance

(xi       b)2

  2
b    
(cid:30)xi    
yi       (cid:30)xi +        bn  ,  (xi)
algorithm 1: batch normalizing transform, applied to
activation x over a mini-batch.

// normalize
// scale and shift

b +   

0.8

0.7

0.6

0.5

0.4

inception
bn   baseline
bn   x5
bn   x30
bn   x5   sigmoid
steps to match inception
30m

25m

5m

10m

15m

20m

figure 2: single crop validation accuracy of inception
and its batch-normalized variants, vs.
the number of
training steps.

automatic tuning of learning rate? 

(cid:80)    adagrad 

j. duchi, e. hazan, and y. singer,    adaptive subgradient methods for 
online leaning and stochastic optimization,    in colt, 2010. 

(cid:80)    adadelta 

adadelta: an adaptive learning rate method, matthew d. 
zeiler, arxiv 1212.5701, 2012. 

xt = 

   

qpt

    =1 g2
   

gt

gt

xt =    

rms[x]t1

rms[g]t

(cid:80)    no more pesky  
learning rates 

t. schaul, s. zhang, and y. lecun,    no more pesky learning rates,    
arxiv:1206.1106, 2012. 

g p

xt =    

1

|diag(ht)|

e[gtw:t]2
e[g2
tw:t]

gt

local minima? 

[the loss surfaces of multilayer networks 
choromanska et al. http://arxiv.org/pdf/1412.0233v3.pdf] 

distribution of test losses 

60

t

40
n
u
o
c

20

0

nhidden

25
50
100
250
500

0.08

0.09

loss

0.10

what about 2nd order methods? 

(cid:80)    newton   s method: 
(cid:80)    full hessian impractical to compute 
(cid:80)    approximations: 

xt = h1

t gt

      diagonal [becker & lecun    88] 
|diag(ht)| +   
      truncated cg [martens, icml   10] 
      per-batch low-rank [sohl-dickstien et al., icml   14] 
      saddle free (|h|) [dauphin et al. nips   14] 

xt = 

1

gt

(cid:80)    generally, extra computation needed seems not worth 
it: take more (dumb) steps instead! 

 

 

 

 

 

 

 

 

 

 

 

 

 

saddle point perspective 

[identifying and attacking the saddle point problem in high-dimensional non-
id76, dauphin et al., nips 2014]  

(cid:80)    during optimization hessian has 
both +ve and    ve eigenvalues 
       and maybe some zeros too (   at 
       at minimum, all are +ve 

directions) 

(cid:80)    cause problems for sgd 

(cid:80)    saddle free  newton (sfn) 
       use |h| (matrix where take 
absolute value of each eigenvalue 
    =    rf|h|   1
of h) 

( )

( )

improving generalization 

(cid:80)    data augmentation (jitter, peturb) 
(cid:80)    weight decay (l1/2 penalty on weights) 
(cid:80)    weight sharing (reduces # parameters) 
(cid:80)    id72 
(cid:80)    inject noise into network 

      dropout [hinton et al. 2012] 
      dropconnect [wan et al. icml 2012] 
      stochastic pooling [zeiler & fergus iclr   13] 

big model + regularize vs small model 

small model 

big model 

big model 
+ regularize 

fooling convnets 

(cid:80)    search for images that are misclassi   ed 

 

by the network  

 

(cid:80)    intriguing properties of neural 

 
 

networks, christian szegedy et al. arxiv 
1312.6199, 2013 

 
 

(cid:80)    deep neural networks are easily 
fooled: high con   dence predictions 
for unrecognizable images, anh 
nguyen, jason yosinski, je    clune, 
arxiv 1412.1897. 

(cid:80)    problem common to any discriminative  

method 

figure 1. evolved images that are unrecognizable to humans,
but that state-of-the-art dnns trained on id163 believe with
 99.6% certainty to be a familiar object. this result highlights
differences between how dnns and humans recognize objects.
i

i di

d d

ith

(b

di

tl

tl

(

)

)

dropout 

(cid:80)    g. e. hinton, n. srivastava, a. krizhevsky, i. sutskever and r. r. 

salakhutdinov, improving neural networks by preventing co-adaptation of 
feature detectors, arxiv:1207.0580 2012 
(cid:80)    fully connected layers only 
(cid:80)    randomly set activations in 
(cid:80)    gives ensemble of models 
(cid:80)    similar to id112 

layer to zero 

[breiman   94], but di   ers in 
that parameters are shared. 

dropconnect 

(cid:80)    wan et al. icml 2013 
(cid:80)    fully-connected layers only 
(cid:80)    random binary mask on weights 

previous layer mask 

 

 

 

 

 

 

   

 

 

   
  
 

 
   

 

  
   
 

  
 

 

  
 

 

 

 

 

 

 

b) dropconnect 
 

mask m 

 

 

 

 

 

 

k
s
a
m

 
t
u
p
t
u
o

 
r
e
y
a
l
 
t
n
e
r
r
u
c

 

 

 

 
c) effective dropout 

 

 

 
mask m    
 

 

 

10   2

y
p
o
r
t

n
e
 
s
s
o
r
c

10   3

 

100

mnist 

 

no   drop train
no   drop test
dropout train
dropout test
dropconnect train
dropconnect test
400

300

200

500
epoch

600

700

800

900

stochastic pooling 

[zeiler and fergus, iclr 2013] 

(cid:80)    for conv layers 
(cid:80)    compute activations     :          
(cid:80)    normalize to sum to 1      ->  
(cid:80)    sample location,  , from multinomial 
(cid:80)    use activation from the location:  

   !

b)!filter!

1.6!

0!

0!

0!

0!

0!

0!

0!

2.4!

0.4!

0!

0!

0!

0!

0!

0!

0!

0.6!

a)!image!

c)!rec0   ed!linear!

d)!ac0va0ons,!ai 

e)!probabili0es,!pi 

sample!a!loca0on!
from!p():!e.g.!!l = 1 

1.6!

f)!sampled!!
!!!!ac0va0on,!s!

other things good to know 

 

       check gradients numerically by finite differences 
       visualize features (feature maps need to be uncorrelated) 
and have high variance. 
 

 

l

s
e
p
m
a
s

good training: hidden units are sparse across samples  
                          and across features.  

hidden unit 

101 
ranzato 

other things good to know 

 

       check gradients numerically by finite differences 
       visualize features (feature maps need to be uncorrelated) 
and have high variance. 
 

 

l

s
e
p
m
a
s

bad training: many hidden units ignore the input and/or 
                       exhibit strong correlations. 

hidden unit 

102 
ranzato 

other things good to know 

 

       check gradients numerically by finite differences 
       visualize features (feature maps need to be uncorrelated) 
and have high variance. 
       visualize parameters 

 

good 

bad 

bad 

bad 

too noisy 

too 

lack 

structure 
good training: learned filters exhibit structure and are uncorrelated.  
103 
ranzato 

correlated 

other things good to know 

 

 

       check gradients numerically by finite differences 
       visualize features (feature maps need to be uncorrelated) 
and have high variance. 
       visualize parameters 
       measure error on both training and validation set. 
       test on a small subset of the data and check the error     0. 
 

 

 

104 
ranzato 

what if it does not work? 

       training diverges: 

       learning rate may be too large     decrease learning rate 
       bprop is buggy     numerical gradient checking 

       parameters collapse / loss is minimized but accuracy is low 

        check id168: 

       is it appropriate for the task you want to solve? 
       does it have degenerate solutions? check    pull-up    term. 

       network is underperforming 

       compute flops and nr. params.      if too small, make net larger 
       visualize hidden units/params     fix optmization   

 

       network is too slow 

net smaller  

       compute flops and nr. params.     gpu,distrib. framework, make 

 

 

 

105 
ranzato 

industry deployment 
(cid:80)    used in facebook, google, microsoft 
(cid:80)    face recognition, image search, photo 
organization   . 
(cid:80)    very fast at test time (~100 images/sec/gpu) 

[taigman et al. deepface: closing the gap to human-level performance in face 
veri   cation, cvpr   14] 

labeled faces in wild dataset 
(cid:80)    task: given pair of images, same person or not? 

[tagman et al. cvpr   14]	
   

detection with convnets 

(cid:80)    so far, all about 
classi   cation 

(cid:80)    what about 
localizing objects 
within the scene? 

two general approaches 

1.    examine very position / scale 

       e.g. overfeat: integrated recognition, localization 

and detection using convolutional networks, 
sermanet et al., iclr 2014 

2.    use some kind of proposal mechanism to 

attend to a set of possible regions 
       e.g. region-id98 [rich feature hierarchies for 
accurate id164 and semantic 
segmentation, girshick et al., cvpr 2014] 

 

sliding window with convnet 

conv 

conv 

conv 

conv 

conv 

full 

full 

sliding window with convnet 

conv 

conv 

conv 

conv 

conv 

full 

full 

224 

input window 

224 

feature extractor 

6 

6 

classifier 

256 

c 
classes 

sliding window with convnet 

conv 

conv 

conv 

conv 

conv 

full 

full 

240 

16 

224 

feature extractor 

input window 

7 

6 

1 

256 

c 
classes 

no need to compute two separate windows ---  just one big input window 

multi-scale sliding window convnet 

feature 
maps 

class  
maps 

256 

c=1000 

feature  
extractor 

256 

classifier 

c=1000 

256 

256 

c=1000 

c=1000 

multi-scale sliding window convnet 

feature 
maps 

bounding box  

maps 

256 

4 

feature  
extractor 

256 

regression 
network 

4 

256 

256 

4 

4 

overfeat     output before nms 

overfeat detection results 

[sermanet et al. iclr 2014] 

r-id98 approach 

[rich feature hierarchies for accurate id164 and semantic segmentation, girshick et al., cvpr 2014] 

(cid:80)    bottom-up proposal 
mechanism 
(cid:80)    scored by classi   er 
(cid:80)    current best 
detection approach  
on pascal voc 

r-id98: regions with id98 features

aeroplane? no.

1. input 
image

2. extract region 
proposals (~2k)

3. compute 
id98 features

person? yes.

...

tvmonitor? no.
4. classify 

regions

figure 1: id164 system overview. our system (1)
takes an input image, (2) extracts around 2000 bottom-up region
proposals, (3) computes features for each proposal using a large
convolutional neural network (id98), and then (4) classi   es each
region using class-speci   c linear id166s. r-id98 achieves a mean
average precision (map) of 53.7% on pascal voc 2010. for
comparison, [34] reports 35.1% map using the same region pro-
posals, but with a spatial pyramid and bag-of-visual-words ap-
proach. the popular deformable part models perform at 33.4%.

(cid:80)    further work combines proposal mechanism with classi   cation network: 
       fast r-id98, ross girshick, arxiv 1504.08083, 2015. 
       faster r-id98: towards real-time id164 with region proposal 
networks, shaoqing ren et al., arxiv 1506.01497, 2015 

warped region...id98video classification 

(cid:80)    want to capture temporal structure  
(cid:80)    3d convolutions & 3d max-pooling 
(cid:80)    e.g. c3d model 

8 convolution, 5 pool, 2 fully-connected layers 
3x3x3 convolution kernels 
2x2x2 pooling kernels 

[learning spatiotemporal features with 3d convolutional networks, tran et al., arxiv:
1412.0767, 2014] 

[slide: manohar paluri] 

action recognition     ucf101 dataset 

 

[slide: manohar paluri] 

action recognition results 

baselines 

use raw pixel 
inputs 

use optical 
flows 

[slide: manohar paluri] 

2d vs 3d convnets 

(cid:80)    ucf101 training 
 

id167 visualization 
 

[slide: manohar paluri] 

sport classification results 

[slide: manohar paluri] 

dense scene labeling 

(cid:80)    classi   cation: pixels -> label 
(cid:80)    detection: pixels -> boxes 

(cid:80)    use convnets to do pixels -> pixels 
      segmentation of image 
      image processing tasks (denoising etc.) 
      don   t want pooling 

dense scene labeling

input image   

semantic map   

(cid:80)    convnet output is per-pixel label map

dense scene labeling

depth   

input image   

semantic map   

(cid:80)    convnet output is per-pixel depth map

dense scene labeling

depth   

input image   

semantic map   

normals   

(cid:80)    convnet output is per-pixel normal map

eigen et al. architecture

input:	
   320x240	
   

96	
   

256	
   

384	
   

384	
   

256	
   

4096	
   

64	
   

64	
   

128	
   

64	
   

64	
   

upsample

63	
   

64	
   

64	
   

upsample

64	
   

conv+pool

concat

convolutions

[predicting depth, surface normals and semantic labels with a common 
multi-scale convolutional architecture, eigen et al., arxiv 1411.4734, 2014]

output:	
   147x109	
   

input:	
   320x240	
   

architecture

upsample

upsample

conv+pool

concat

convolutions

[predicting depth, surface normals and semantic labels with a common 
multi-scale convolutional architecture, eigen et al., arxiv 1411.4734, 2014]

input:	
   320x240	
   

multi-scale convnets

96	
   

256	
   

384	
   

384	
   

256	
   

4096	
   

64	
   

64	
   

128	
   

64	
   

64	
   

upsample

64	
   

64+c	
   

64	
   

upsample

64	
   

conv+pool

concat

convolutions

[predicting depth, surface normals and semantic labels with a common 
multi-scale convolutional architecture, eigen et al., arxiv 1411.4734, 2014]

use appropriate id168s
depth:
ldepth(d, d) =

d	
   =	
   log	
   predicted	
   depth,	
   	
   d*	
   =	
   log	
   true	
   depth	
   

+

1

1

1

[(   xdi)2 + (   ydi)2]

d = d   d
n   i
d2
i    

2n2      i

di   2

n   i

normals
lnormals(n, n) =    
labels
lsemantic(c, c) =    

1

n   i

1

n   i

ni    ni =    

1
n

n    n

angle	
   between	
   	
   
true	
   /	
   predicted	
   	
   
normals	
   

ci log(ci)

per-     pixel	
   so   -     max	
   

[predicting depth, surface normals and semantic labels with a common 
multi-scale convolutional architecture, eigen et al., arxiv 1411.4734, 2014]

depths comparison

eigen	
   nips   14	
   (2	
   scales)	
   

ours	
   

ground	
   truth	
   

[predicting depth, surface normals and semantic labels with a common 
multi-scale convolutional architecture, eigen et al., arxiv 1411.4734, 2014]

surface normals

rgb input

3dp [6]

ladicky&al [16]

wang&al [33]

ours (vgg)

ground truth

[predicting depth, surface normals and semantic labels with a common 
multi-scale convolutional architecture, eigen et al., arxiv 1411.4734, 2014]

scene parsing 

(cid:80)    farabet et al.    learning hierarchical features for scene labeling    pami 2013 

segmentation 

(cid:80)    ciresan et al.    dnn segment neuronal membranes...    nips 2012 
(cid:80)    turaga et al.    maximin learning of image segmentation    nips 2009 

denoising with convnets 

(cid:80)    burger et al.    can plain nns compete with bm3d?    cvpr 2012 

original 

noised 

denoised 

deblurring with convnets 

(cid:80)    blind deconvolution 

      learning to deblur, schuler et al., arxiv 
1406.7444, 2014 

blurry image with
ground truth kernel

result of [zho+13]

psnr 23.17

deblurring result w.
noise agnostic training

psnr 23.29

deblurring result w.
noise speci   c training

psnr 23.41

inpainting with convnets 

(cid:80)    image denoising and inpainting with deep neural 
(cid:80)    mask-speci   c inpainting with deep neural networks, 

networks, xie et al. nips 2012. 

k  hler et al., pattern recognition 2014 

corrupted image

original   

 schmid cvpr   10  

[20] psnr 32.13

ours, psnr 34.22

  k  hler et al.    14 

removing local corruption 

removing local corruption 

(cid:80)    restoring an image taken through a window covered with 

dirt or rain, eigen et al., iccv 2013. 

 

 

convnet + structured learning 

 

 

 

(cid:80)    gradient-based learning 
applied to document 
recognition, yann lecun, 
leon bottou, yoshua bengio 
and patrick ha   ner, proc. 
ieee, nov 1998. 

c1

c3 c5

2345

compose + viterbi
2
33 4 5

answer

sdnn
output
f6

input

viterbi penalty

  

4

gvit

3

t

vit

viterbi
transformer

3
2

3
4

34

1
4

1

4

2

2
3
4

viterbi
path

interpretation
    graph

nn

nn

nn

nn

nn

nn

recognition
transformer

segmentation
      graph

g
int

rect

gseg

convnet + structured learning 

(cid:80)    learning deep structured models, liang-
chieh chen, alexander g. schwing, alan l. 
yuille, raquel urtasun, arxiv 1407.2538, 2014 
(cid:80)    joint training of a convolutional network and 
a graphical model for human pose 
estimation,  j. tompson, a. jain, y. lecun, c. 
bregler, nips 2014 
(cid:80)    lots more recently       

body tracking

(cid:80)    joint training of a convolutional network and a 

graphical model for human pose estimation
 j. tompson, a. jain, y. lecun, c. bregler, nips 2014

14
	
   

body tracking: part detector
(cid:31)(cid:46)(cid:49)(cid:52)(cid:48)(cid:46)(cid:62)(cid:42)(cid:41)(cid:1)(cid:49)(cid:56)(cid:48)(cid:55)(cid:46)(cid:7)(cid:53)(cid:42)(cid:54)(cid:51)(cid:48)(cid:56)(cid:55)(cid:46)(cid:51)(cid:50)(cid:1)(cid:42)   cient model:

14
	
   

body tracking: spatial model
start with mrf formulation

   convolutional priors   
sum-product belief propagation

14
	
   

body tracking: spatial model
implement it as a network (no longer mrf)!



14
	
   

speech 

brief	
   aside	
   -     	
   speech	
   

(cid:80)    also	
   huge	
   impact	
   by	
   neural	
   nets	
   
(cid:80)    tradi   onal	
   approach	
   (pre-     2009):	
   

raw
speech
signal

 

 

 

 

 

 

 

 

 

 

 

 

  

feature
extraction

mfcc / plp

 

 

 

 

 

 

 

 

 

 

wide band spectrogram

8

7

6

5

4

3

2

1

8

7

6

5

4

kh
kh

3

2

1

modeling

gmm / ann

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

decoding

id48

decoded
sequence

acous   c	
   modeling	
   

phoneme	
   classi   ca   on	
   

language	
   	
   
model	
   

0

waveform

spectrogram	
   

0

(cid:80)    very	
   incremental	
   gains	
   in	
   performance	
   
	
   	
   

deep	
   learning	
   technical	
   revolu   on	
   

(cid:80)    first resurgence 

o    a. mohamed, g. dahl and g. hinton "id50 
for phone recognition,    in nips workshop on deep learning 
for id103 and related applications. 2009 

(cid:80)    dnns for large scale tasks 

o    f. seide, g. li, and d. yu,    conversational speech 

transcription using context-dependent deep neural 
networks,    in proc. interspeech 2011. 

(cid:80)    id98s for large scale tasks 

o    t. n. sainath, a. mohamed, b. kingsbury and b. 

ramabhadran, "deep convolutional neural networks for 
lvcsr," in proc. icassp, 2013. 

(cid:80)    lstms for large scale tasks 

o    h. sak, a. senior and f. beaufays,    long short-term 
memory recurrent neural network architectures for large 
scale acoustic modeling," in proc. interspeech, 2014. 

2009 

2011 

2013 

2014 

 

[slide: tara sainath, google, advancements in deep learning, slt keynote, dec 2014.]	
   

dnn acoustic modeling results 
(cid:80)    dnns provide between a 8-25% relative improvement in word error 

rate over gmm/id48 systems across a variety of tasks and 
languages 

(cid:80)    results confirmed by many, many research labs 
 

300	
   hour	
   swb	
   
conversa   onal	
   	
   
telephony	
   

400	
   hour	
   
broadcast	
   news	
   

2000	
   hour	
   
voice	
   search	
   

gmm/id48 

dnn 

%	
   rela   ve	
   	
   
improvement 

14.3 
12.2 
14.7 

16.5 
15.2 
7.9 

16.0	
   
12.2	
   
23.8	
   

[slide: tara sainath, google, advancements in deep learning, slt keynote, dec 2014.]	
   

id98 vs dnn results 
         id98s trained with vtln-warped log-mel fb features 
offer between a 4-12% relative improvement over 
dnns trained with speaker-adapted features (vtln
+fmllr) 

model	
   

bn-     50	
   

bn-     400	
   

swb-     300	
   

baseline	
   gmm/id48	
   

dnn	
   

id98	
   

18.1	
   

15.8	
   

15.0	
   

13.8	
   

13.3	
   

12.0	
   

14.5	
   

12.2	
   

11.5	
   

[sainath	
   et	
   al,	
   icassp	
   2013]	
   

[slide: tara sainath, google, advancements in deep learning, slt keynote, dec 2014.]	
   

end-     to-     end	
   recogni   on	
   

(cid:80)    go	
   directly	
   from	
   raw	
   waveform	
   

raw speech
sequence

feature
learning

modeling

decoding

phoneme
sequence

 

 

convolutional neural network

dw

kw

din

kw

d

convolution

m       

max-pooling

max(  )

conditional random field

 

 

 

 

 

 

phone1

phone2

phone3

dout

d

 

 

 

t = 0 t = 1 t = 2 t = 3 t = 4

 

 

(cid:80)    convolu   onal	
   neural	
   networks-     based	
   con   nuous	
   speech	
   recogni   on	
   
using	
   raw	
   speech	
   signal,	
   palaz,	
   magimai-     doss,	
   collobert,	
   icassp	
   2015.	
   	
   
(cid:80)    superior	
   results	
   on	
   timit	
   (phoneme	
   recog),	
   comparable	
   results	
   on	
   wsj.	
   
	
   	
   

 

 

 

 

 

natural language 

processing 

id38 
       natural language is a sequence of 
       some sentences are more likely than others: 

sequences 

o       how are you ?    has a high id203 
o       how banana you ?     has a low id203 

[slide: wojciech zaremba] 

neural network language models

bengio, y., schwenk, h., sencal, j. s., morin, f., & gauvain, j. l. (2006).
neural probabilistic language models. in innovations in machine learning (pp.
137-186). springer berlin heidelberg.

[slide: antoine border & jason weston, emnlp tutorial 2014 ] 

recurrent neural network language models

key idea: input to predict next word is current word plus context fed-back
from previous word (i.e. remembers the past with recurrent connection).

recurrent neural network based language model. mikolov et al., interspeech,    10.

[slide: antoine border & jason weston, emnlp tutorial 2014 ] 

recurrent neural networks - schema  

my 

name 

is 

name 

is 

wojciech 

[slide: wojciech zaremba] 

id26 through time

    the intuition is that we unfold the id56 in time

    we obtain deep neural network with shared
weights u and w

 

 

 

[slide: thomas mikolov, coling 2014 ] 

id26 through time

    we train the unfolded id56 using normal
id26 + sgd

    in practice, we limit the number of
unfolding steps to 5     10

    it is computationally more efficient to
propagate gradients after few training
examples (batch mode)

tomas mikolov, coling 2014

100

[slide: thomas mikolov, coling 2014 ] 

nnlms vs. id56s: id32 results (mikolov)

recent uses of nnlms and id56s to improve machine translation:
fast and robust nn joint models for machine translation, devlin et al, acl    14.
also kalchbrenner    13, sutskever et al.,    14., cho et al.,    14. .

[slide: antoine border & jason weston, emnlp tutorial 2014 ] 

language modelling     id56 samples 
the meaning of life is that only if an end would 
be of the whole supplier. widespread rules are 
regarded as the companies of refuses to 
deliver. in balance of the nation   s information 
and loan growth associated with the carrier 
thrifts are in the process of slowing the seed 
and commercial paper. 

[slide: wojciech zaremba] 

more depth gives more power 

[slide: wojciech zaremba] 

lstm - long short term memory 

[hochreiter and schmidhuber, neural computation 1997] 
       ad-hoc way of modelling 
       many alternative ways of 

long dependencies 

modelling it 

       next hidden state is 

modification of previous 
hidden state (so 
information doesn   t decay 
too fast). 

 
 

for simple explanation, see [recurrent neural network id173, 
wojciech zaremba, ilya sutskever, oriol vinyals, arxiv 1409.2329, 2014] 

[slide: wojciech zaremba] 

id56-lstms for machine translation 

[sutskever et. al. (2014)] 

sequence to sequence learning with neural networks, 
ilya sutskever, oriol vinyals, quoc le, nips 2014  

learning phrase representations using id56 encoder-decoder for statistical 
machine translation, kyunghyun cho, bart van merrienboer, caglar gulcehre, 
dzmitry bahdanau, fethi bougares, holger schwenk, yoshua bengio, emnlp 
2014 

[slide: wojciech zaremba] 

visualizing internal representation 

id167 projection of network state at end of input sentence 

sequence to sequence learning with neural networks, 
ilya sutskever, oriol vinyals, quoc le, nips 2014  

translation - examples 

    fr: les avionneurs se querellent au sujet de la largeur des si  ges alors que 
de grosses commandes sont en jeu 
 
    google translate: aircraft manufacturers are quarreling about the seat width 
as large orders are at stake 
 
    lstm: aircraft manufacturers are concerned about the width of seats while 
large orders are at stake 
 
    ground truth: jet makers feud over seat width with big orders at stake 
 

[sequence to sequence learning with neural networks, 
ilya sutskever, oriol vinyals, quoc le, nips 2014]  

[slide: wojciech zaremba] 

image captioning: vision + nlp 

 

(cid:80)    generate short text descriptions of  

 

image, given just picture. 

 
 

(cid:80)    use convnet to extract image features 

(cid:80)    id56 or lstm model takes image 

 
 

features as input, generates text 

 
many recent works on this: 
(cid:80)    baidu/ucla: explain images with multimodal recurrent neural networks 
(cid:80)    toronto: unifying visual-semantic embeddings with multimodal neural language models 
(cid:80)    berkeley: long-term recurrent convolutional networks for visual recognition and description 
(cid:80)    google: show and tell: a neural image caption generator 
(cid:80)    stanford: deep visual-semantic alignments for generating image description 
(cid:80)    uml/ut:  translating videos to natural language using deep recurrent neural networks 
(cid:80)    microsoft/cmu:  learning a recurrent visual representation for image id134 
(cid:80)    microsoft:  from captions to visual concepts and back 

image captioning examples 

from captions to visual concepts and back, hao fang    saurabh gupta    forrest iandola    rupesh k. srivastava   , li deng piotr 
dollar, jianfeng gao xiaodong he, margaret mitchell john c. platt, c. lawrence zitnick, geoffrey zweig, cvpr 2015. 

facebook ai research  

(cid:80)    ~50 people working in ml/vision/nlp/speech/ai 
      1/3 are research engineers (some of fb   s best coders) 
      yann lecun is lab director  

(cid:80)    freedom to publish & open-source code 
(cid:80)    easy to productize (1.1b users) 
(cid:80)    labs in: 

      menlo park, california (facebook hq) 
      new york city 
      paris 

(cid:80)    we are hiring! 

references 

[slide 5] 

(cid:80)   
(cid:80)    p. felzenszwalb, r. girshick, d. mcallester, d. ramanan, id164 with 

discriminatively trained part based models,ieee transactions on pattern analysis and 
machine intelligence, vol. 32, no. 9, september 2010 

(cid:80)    zheng song*, qiang chen*, zhongyang huang, yang hua, and shuicheng yan. con  tex  tu  
al  iz  ing ob  ject de  tec  tion and clas  si       ca  tion. in cvpr'11. (* in  di  cates equal con  
tri  bu  tion) [no. 1 per  for  mance in voc'10 clas  si       ca  tion task] 
[slide 6] 
finding the weakest link in person detectors, d. parikh, and c. l. zitnick, cvpr, 2011. 
[slide 7] 

(cid:80)   
(cid:80)   
(cid:80)   
(cid:80)    gehler and nowozin, on feature combination for multiclass object classi   cation, iccv   09 
(cid:80)   
(cid:80)   
(cid:80)   
(cid:80)    yoshua bengio and yann lecun: scaling learning algorithms towards ai, in bottou, l. and 

[slide 8] 
http://www.amazon.com/vision-david-marr/dp/0716712849 
[slide 10]  

chapelle, o. and decoste, d. and weston, j. (eds), large-scale kernel machines, mit 
press, 2007 

references 

(cid:80)   
(cid:80)   

(cid:80)   

[slide 11] 
s. lazebnik, c. schmid, and j. ponce, beyond bags of features: spatial pyramid matching for 
recognizing natural scene categories, cvpr 2006 
[slide 12] 

(cid:80)   
(cid:80)    christoph h. lampert, hannes nickisch, stefan harmeling: "learning to detect unseen 
object classes by between-class attribute transfer", ieee id161 and pattern 
recognition (cvpr), miami, fl, 2009  
[slide 14] riesenhuber, m. & poggio, t. (1999). id187 of object recognition 
in cortex. nature neuroscience 2: 1019-1025. 
http://www.scholarpedia.org/article/neocognitron 

(cid:80)   
(cid:80)    k. fukushima: "neocognitron: a self-organizing neural network model for a mechanism of 

pattern recognition una   ected by shift in position", biological cybernetics, 36[4], pp. 193-202 
(april 1980). 

(cid:80)    y. lecun, l. bottou, y. bengio and p. ha   ner: gradient-based learning applied to 
document recognition, proceedings of the ieee, 86(11):2278-2324, november 1998 

references 

[slide 30] 

(cid:80)   
(cid:80)    y-lan boureau, jean ponce, and yann lecun, a theoretical analysis of feature 

networks. nips, 2010 

pooling in vision algorithms, proc. international conference on machine learning 
(icml'10), 2010  
[slide 31] 

(cid:80)   
(cid:80)    q.v. le, j. ngiam, z. chen, d. chia, p. koh, a.y. ng , tiled convolutional neural 
(cid:80)    http://ai.stanford.edu/~quocle/tid98web 
(cid:80)    matthew d. zeiler, graham w. taylor, and rob fergus, adaptive deconvolutional 
networks for mid and high level id171, international conference on 
id161(november 6-13, 2011) 
[slide 32] 

(cid:80)   
(cid:80)    yuanhao chen, long zhu, chenxi lin, alan yuille, hongjiang zhang. rapid 
id136 on a novel and/or graph for id164, segmentation and 
parsing. nips 2007.  

references 

[slide 36] 

[slide 35] 

rumelhart, j. l. mcclelland, eds. (mit press, cambridge, 1986), pp. 194   281. 

(cid:80)   
(cid:80)    p. smolensky, parallel distributed processing: volume 1: foundations, d. e. 
(cid:80)    g. e. hinton, neural comput. 14, 1711 (2002). 
(cid:80)   
(cid:80)    m. ranzato, y. boureau, y. lecun. "sparse id171 for deep belief 
(cid:80)   
(cid:80)    hinton, g. e. and salakhutdinov, r. r., reducing the dimensionality of data with 
(cid:80)   
(cid:80)    a. torralba, k. p. murphy and w. t. freeman, contextual models for object 

networks". advances in neural information processing systems 20 (nips 2007). 
[slide 39] 

neural networks. science, vol. 313. no. 5786, pp. 504 - 507, 28 july 2006. 
[slide 41] 

detection using boosted random fields, adv. in neural information processing 
systems 17 (nips), pp. 1401-1408, 2005. 

references 

[slide 42] 

(cid:80)   
(cid:80)    ruslan salakhutdinov and geo   rey hinton, deep id82s, 12th 
(cid:80)   
(cid:80)    p. felzenszwalb, r. girshick, d. mcallester, d. ramanan, id164 with 

international conference on arti   cial intelligence and statistics (2009). 
[slide 44] 

discriminatively trained part based models,ieee transactions on pattern 
analysis and machine intelligence, vol. 32, no. 9, september 2010 

structural learning for id164. cvpr 2010.  
[slide 45] 

(cid:80)    long zhu, yuanhao chen, alan yuille, william freeman. latent hierarchical 
(cid:80)   
(cid:80)    matthew d. zeiler, graham w. taylor, and rob fergus, adaptive deconvolutional 
networks for mid and high level id171, international conference on 
id161(november 6-13, 2011) 

references 

[slide 48] 

models, nips 2011 
[slide 50] 

trends in computer graphics and vision, vol.2, no.4, pp 259-362, 2006.   
[slide 49] 

(cid:80)   
(cid:80)    s.c. zhu and d. mumford, a stochastic grammar of images, foundations and 
(cid:80)   
(cid:80)    r. girshick, p. felzenszwalb, d. mcallester, id164 with grammar 
(cid:80)   
(cid:80)    p. felzenszwalb, d. huttenlocher, pictorial structures for object recognition, 
(cid:80)    m. fischler and r. elschlager. the representation and matching of pictoral 
(cid:80)   
(cid:80)    s. fidler, m. boben, a. leonardis. a coarse-to-   ne taxonomy of constellations for 
(cid:80)    s. fidler and a. leonardis. towards scalable representations of object categories: 

international journal of id161, vol. 61, no. 1, january 2005 

fast multi-class id164. eccv 2010.  

structures. (1973) 
[slide 51] 

learning a hierarchy of parts. cvpr 2007. 

references 

[slide 52] 

(cid:80)   
(cid:80)    long zhu, chenxi lin, haoda huang, yuanhao chen, alan yuille. unsupervised structure 

learning: hierarchical recursive composition, suspicious coincidence and competitive 
exclusion. eccv 2008. 
[slide 53] 

(cid:80)   
(cid:80)    hinton, g. e., krizhevsky, a. and wang, s, transforming auto-encoders. icann-11: 
(cid:80)    matthew d. zeiler, graham w. taylor, and rob fergus, adaptive deconvolutional networks 

international conference on arti   cial neural networks, 2011 

for mid and high level id171, international conference on computer 
vision(november 6-13, 2011) 
[slide 54] 

(cid:80)   
(cid:80)    q.v. le, m.a. ranzato, r. monga, m. devin, k. chen, g.s. corrado, j. dean, a.y. ng., 
(cid:80)   
(cid:80)    ruslan salakhutdinov and geo   rey hinton, deep id82s, 12th international 

building high-level features using large scale unsupervised learning. icml, 2012. 
[slide 55] 

conference on arti   cial intelligence and statistics (2009). 

references 

(cid:80)    [slide 56] 
(cid:80)    http://www.image-net.org/challenges/lsvrc/2010/ 
(cid:80)    q.v. le, m.a. ranzato, r. monga, m. devin, k. 
chen, g.s. corrado, j. dean, a.y. ng., building 
high-level features using large scale unsupervised 
learning. icml, 2012. 

(cid:80)    q.v. le, w.y. zou, s.y. yeung, a.y. ng., learning 

hierarchical spatio-temporal features for action 
recognition with independent subspace analysis, 
cvpr 2011 

