series issn: 1938-1743
series issn: 1938-1743
series issn: 1938-1743

synthesis lectures on
synthesis lectures on
synthesis lectures on
mathematics and statistics
mathematics and statistics
mathematics and statistics
series editor: steven g. krantz, washington university, st. louis
series editor: steven g. krantz, washington university, st. louis
series editor: steven g. krantz, washington university, st. louis
matrices in engineering problems
matrices in engineering problems
matrices in engineering problems

marvin j. tobias
marvin j. tobias
marvin j. tobias

t
o
b
i
a
s

t
t
o
o
b
b
i
i
a
a
s
s

this book is intended as an undergraduate text introducing matrix methods as they relate to engi-neering
this book is intended as an undergraduate text introducing matrix methods as they relate to engi-neering
this book is intended as an undergraduate text introducing matrix methods as they relate to engi-neering
problems. it begins with the fundamentals of mathematics of matrices and determinants. matrix inversion
problems. it begins with the fundamentals of mathematics of matrices and determinants. matrix inversion
problems. it begins with the fundamentals of mathematics of matrices and determinants. matrix inversion
is discussed, with an introduction of the well known reduction methods. equation sets are viewed as
is discussed, with an introduction of the well known reduction methods. equation sets are viewed as
is discussed, with an introduction of the well known reduction methods. equation sets are viewed as
vector transformations, and the conditions of their solvability are explored.
vector transformations, and the conditions of their solvability are explored.
vector transformations, and the conditions of their solvability are explored.
orthogonal matrices are introduced with examples showing application to many problems requiring
orthogonal matrices are introduced with examples showing application to many problems requiring
orthogonal matrices are introduced with examples showing application to many problems requiring
three dimensional thinking. the angular velocity matrix is shown to emerge from the differentiation
three dimensional thinking. the angular velocity matrix is shown to emerge from the differentiation
three dimensional thinking. the angular velocity matrix is shown to emerge from the differentiation
of the 3-d orthogonal matrix, leading to the discussion of particle and rigid body dynamics.
of the 3-d orthogonal matrix, leading to the discussion of particle and rigid body dynamics.
of the 3-d orthogonal matrix, leading to the discussion of particle and rigid body dynamics.
the book continues with the eigenvalue problem and its application to multi-variable vibrations.
the book continues with the eigenvalue problem and its application to multi-variable vibrations.
the book continues with the eigenvalue problem and its application to multi-variable vibrations.
because the eigenvalue problem requires some operations with polynomials, a separate discussion of
because the eigenvalue problem requires some operations with polynomials, a separate discussion of
because the eigenvalue problem requires some operations with polynomials, a separate discussion of
these is given in an appendix. the example of the vibrating string is given with a comparison of the
these is given in an appendix. the example of the vibrating string is given with a comparison of the
these is given in an appendix. the example of the vibrating string is given with a comparison of the
matrix analysis to the continuous solution.
matrix analysis to the continuous solution.
matrix analysis to the continuous solution.

about synthesis
about synthesis
about synthesis
this volume is a printed version of a work that appears in the synthesis
this volume is a printed version of a work that appears in the synthesis
this volume is a printed version of a work that appears in the synthesis
digital library of engineering and computer science.  synthesis lectures
digital library of engineering and computer science.  synthesis lectures
digital library of engineering and computer science.  synthesis lectures
provide concise, original presentations of important research and development
provide concise, original presentations of important research and development
provide concise, original presentations of important research and development
topics, published quickly, in digital and print formats. for more information
topics, published quickly, in digital and print formats. for more information
topics, published quickly, in digital and print formats. for more information
visit www.morganclaypool.com
visit www.morganclaypool.com
visit www.morganclaypool.com
&
&
&
w w w . m o r g a n c l a y p o o l . c o m
w w w . m o r g a n c l a y p o o l . c o m
w w w . m o r g a n c l a y p o o l . c o m

morgan      claypool  publishers
morgan      claypool  publishers

morgan      claypool  publishers

isbn: 978-1-60845-658-1
isbn: 978-1-60845-658-1
isbn: 978-1-60845-658-1
90000
90000
90000

c
l
a
y
p
o
o
l

c
l
a
y
p
o
o
l

c
l
a
y
p
o
o
l

9 781608 456581

9 781608 456581
9 781608 456581

&
&
&

cm& morgan      claypool  publishers
cm& morgan      claypool  publishers
cm& morgan      claypool  publishers
matrices in
matrices in
matrices in
engineering problems
engineering problems
engineering problems

marvin j. tobias
marvin j. tobias
marvin j. tobias

synthesis lectures on
synthesis lectures on
synthesis lectures on
mathematics and statistics
mathematics and statistics
mathematics and statistics
steven g. krantz, series editor
steven g. krantz, series editor
steven g. krantz, series editor

m
a
t
r
i
c
e
s

m
m
a
a
t
t
r
r
i
i
c
c
e
e
s
s

 

 

 

 

 

 

i
n
e
n
g

i
i
n
n
e
e
n
n
g
g

 

 

 

i
n
e
e
r
i
n
g
p
r
o
b
l
e
m
s

i
i
n
n
e
e
e
e
r
r
i
i
n
n
g
g
p
p
r
r
o
o
b
b
l
l
e
e
m
m
s
s

m
o
r
g
a
n

m
m
o
o
r
r
g
g
a
a
n
n

 

 

 

&

&
&

 

 

 

matrices in engineering problems

synthesis lectures on

mathematics and statistics

editor
steven g. krantz, washington university, st. louis

matrices in engineering problems
marvin j. tobias
2011

the integral: a crux for analysis
steven g. krantz
2011

statistics is easy! second edition
dennis shasha and manda wilson
2010

lectures on financial mathematics: discrete asset pricing
greg anderson and alec n. kercheval
2010

jordan canonical form: theory and practice
steven h. weintraub
2009

the geometry of walker manifolds
miguel brozos-v  zquez, eduardo garc  a-r  o, peter gilkey, stana nikcevic, and r  mon
v  zquez-lorenzo
2009

an introduction to multivariable mathematics
leon simon
2008

jordan canonical form: application to differential equations
steven h. weintraub
2008

statistics is easy!
dennis shasha and manda wilson
2008

a gyrovector space approach to hyperbolic geometry
abraham albert ungar
2008

iii

copyright    2011 by morgan & claypool

all rights reserved. no part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means   electronic, mechanical, photocopy, recording, or any other except for brief quotations in
printed reviews, without the prior permission of the publisher.

matrices in engineering problems

marvin j. tobias

www.morganclaypool.com

isbn: 9781608456581
isbn: 9781608456598

paperback
ebook

doi 10.2200/s00352ed1v01y201105mas010

a publication in the morgan & claypool publishers series
synthesis lectures on mathematics and statistics

lecture #10
series editor: steven g. krantz, washington university, st. louis
series issn
synthesis lectures on mathematics and statistics
print 1938-1743 electronic 1938-1751

matrices in engineering problems

marvin j. tobias

synthesis lectures on mathematics and statistics #10

cm&

morgan

&

claypool

publishers

abstract
this book is intended as an undergraduate text introducing matrix methods as they relate to engi-
neering problems. it begins with the fundamentals of mathematics of matrices and determinants.
matrix inversion is discussed, with an introduction of the well known reduction methods. equation
sets are viewed as vector transformations, and the conditions of their solvability are explored.

orthogonal matrices are introduced with examples showing application to many problems
requiring three dimensional thinking. the angular velocity matrix is shown to emerge from the
differentiation of the 3-d orthogonal matrix, leading to the discussion of particle and rigid body
dynamics.

the book continues with the eigenvalue problem and its application to multi-variable vi-
brations. because the eigenvalue problem requires some operations with polynomials, a separate
discussion of these is given in an appendix. the example of the vibrating string is given with a
comparison of the matrix analysis to the continuous solution.

keywords
matrices , vector sets, determinants, determinant expansion, matrix inversion, gauss
reduction, lu decomposition, simultaneous equations, solvability, id75,
orthogonal vectors & matrices, orthogonal transforms, coordinate rotation, eulerian
angles, angular velocity and momentum, dynamics, eigenvalues, eigenvalue analysis,
characteristic polynomial, vibrating systems, non-conservative systems, runge-kutta
integration

1

contents

vii

preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii

1.3

1.2

matrix fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1
1.1 de   nition of a matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
elemetary matrix algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2.1 addition (including subtraction) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.2 multiplication by a scalar
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.3 vector multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.4 id127 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.2.5 transposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
basic types of matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3.1 the unit matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3.2 the diagonal matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3.3 orthogonal matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3.4 triangular matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.5 symmetric and skew-symmetric matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.6 complex matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.3.7 the inverse matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.4
transformation matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.5 matrix partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
interesting vector products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.6
1.6.1 an interpretation of ax = c . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.6.2 the (nx1x1xn) vector product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.6.3 vector cross product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.7.1 an example id127 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.7.2 an example matrix triple product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.7.3 multiplication of complex matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

1.7

1.8

viii
2

3

determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2 general de   nition of a determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
permutations and inversions of indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.3
2.3.1 inversions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.3.2 an example determinant expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
properties of determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4
2.5
the rank of a determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.6 minors and cofactors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.6.1 expansions by minors   laplace expansions . . . . . . . . . . . . . . . . . . . . . . . . 33
2.6.2 expansion by lower order minors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.6.3 the determinant of a matrix product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.7 geometry: lines, areas, and volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
the adjoint and inverse matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
2.8
2.8.1 rank of the adjoint matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.9 determinant evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.9.1 pivotal condensation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.9.2 gaussian reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.9.3 rank of the determinant less than n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.10 examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.10.1 cramer   s rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.10.2 an example complex determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.10.3 the    characteristic determinant    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.11 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

matrix inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
elementary operations in matrix form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.2
3.2.1 diagonalization using elementary matrices . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.3 gauss-jordan reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.3.1 singular matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
the gauss reduction method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.4.1 gauss reduction in detail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.4.2 example gauss reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
lu decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.5.1 lu decomposition in detail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

3.4

3.5

ix

3.5.2 example lu decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.6 matrix inversion by partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.7
additional topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
3.7.1 column id172 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.7.2 improving the inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.7.3 inverse of a triangular matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.7.4 inversion by orthogonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.7.5 inversion of a complex matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
3.8.1 inversion using partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

3.8

3.9

4.3

linear simultaneous equation sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
vectors and vector sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.2
4.2.1 linear independence of a vector set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.2.2 rank of a vector set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
simultaneous equation sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.3.1 square equation sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.3.2 underdetermined equation sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
4.3.3 overdetermined equation sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
id75 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
4.4.1 example regression problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.4.2 quadratic curve fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
lagrange interpolation polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.5.1 interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.5.2 the lagrange polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

4.5

4.6

4.4

orthogonal transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.2 orthogonal matrices and transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.2.1 righthanded coordinates, and positive angle . . . . . . . . . . . . . . . . . . . . . . 107
example coordinate transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.3.1 earth-centered coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.3.2 rotation about a vector (not a coordinate axis) . . . . . . . . . . . . . . . . . . . 112
5.3.3 rotation about all three coordinate axes . . . . . . . . . . . . . . . . . . . . . . . . . 115

5.3

4

5

x

6

5.5.1 velocity of a point on a wheel

5.3.4 solar angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.3.5 image rotation in computer graphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
congruent and similarity matrix transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
5.4
5.5 differentiation of matrices, angular velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
5.6 dynamics of a particle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.7
rigid body dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.7.1 rotation of a rigid body . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.7.2 moment of momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.7.3 the inertia matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5.7.4 the torque equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

5.8
5.9

6.4

matrix eigenvalue analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
6.1
6.2
the eigenvalue problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
6.2.1 the characteristic equation and eigenvalues . . . . . . . . . . . . . . . . . . . . . . . 146
6.2.2 synthesis of a by its eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . 147
6.2.3 example analysis of a nonsymmetric 3x3 . . . . . . . . . . . . . . . . . . . . . . . . . 148
6.2.4 eigenvalue analysis of symmetric matrices . . . . . . . . . . . . . . . . . . . . . . . . 151
6.3 geometry of the eigenvalue problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
6.3.1 non-symmetric matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.3.2 matrix with a double root . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
the eigenvectors and orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
6.4.1 inverse of the characteristic matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.4.2 vibrating string problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
the cayley-hamilton theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.5.1 functions of a square matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.5.2 sylvester   s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.6 mechanics of the eigenvalue problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
6.6.1 calculating the characteristic equation coef   cients . . . . . . . . . . . . . . . . . 166
6.6.2 factoring the characteristic equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
6.6.3 calculation of the eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
example eigenvalue analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.7.1 example eigenvalue analysis; complex case . . . . . . . . . . . . . . . . . . . . . . . 168

6.5

6.7

7

6.7.2 eigenvalues by matrix iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
the eigenvalue analysis of similar matrices; danilevsky   s method . . . . . . . . . . 171
6.8.1 danilevsky   s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
6.8.2 example of danilevsky   s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6.8.3 danilevsky   s method   zero pivot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180

6.8

6.9

xi

7.3

matrix analysis of vibrating systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
7.1
setting up equations, lagrange   s equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
7.2
7.2.1 generalized form of lagrange   s equations . . . . . . . . . . . . . . . . . . . . . . . . . 185
7.2.2 mechanical / electrical analogies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
7.2.3 examples using the lagrange equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
vibration of conservative systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
7.3.1 conservative systems     the initial value problem . . . . . . . . . . . . . . . . . . 191
7.3.2 interpretation of equation (7.23) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
7.3.3 conservative systems - sinusoidal response . . . . . . . . . . . . . . . . . . . . . . . 197
7.3.4 vibrations in a continuous medium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
nonconservative systems. viscous damping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
7.4.1 the initial value problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
7.4.2 sinusoidal response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
7.4.3 determining the vector coef   cients for the driven system . . . . . . . . . . 209
7.4.4 sinusoidal response     nonzero initial conditions . . . . . . . . . . . . . . . . . . 211
steady state sinusoidal response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7.5.1 analysis of ladder networks; the cumulant . . . . . . . . . . . . . . . . . . . . . . . 214
runge-kutta integration of differential equations . . . . . . . . . . . . . . . . . . . . . . . . 216
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

7.6
7.7

7.4

7.5

a partial differentiation of bilinear and quadratic forms . . . . . . . . . . . . . . . . . . . . 223
b
polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
b.1
polynomial basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
polynomial arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
b.2
b.2.1 evaluating a polynomial at a aiven value . . . . . . . . . . . . . . . . . . . . . . . . . . 232
evaluating polynomial roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
b.3.1 the laguerre method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
b.3.2 the id77 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
b.3.3 an example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234

b.3

xii

c the vibrating string . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
c.1 the digitized     matrix solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
c.2 the continuous function solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
c.3 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241

d solar energy geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
d.1 yearly energy output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
d.2 an example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
d.3 tracking the sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247

e answers to selected exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
e.1 chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
e.2 chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
e.3 chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
e.4 chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
e.5 chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
e.6 chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
e.7 chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

author   s biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263

index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265

preface

the primary objective of this book is to present matrices as they relate to engineering problems.
it began as a set of notes used in lectures to    b    course (applied mathematics) classes of the general
electric advanced engineering program. matrix analysis is a valuable tool used in nearly all the
engineering sciences.

the approach is practical rather than strictly mathematical. introductory mathematics is fol-
lowed by example applications. often, pseudo-programming (   pascal-like   ) code is used in descrip-
tion of a method. in some parts of the book the emphasis is on the program. matrix manipulations
are fun to program and provide good learning/practice experience.

a working knowledge of matrix methods provides insight into coordinate transforms , rota-
tions, dynamics, and vibrating systems, and many others problems. the fact that the subject matter
is closely tied to programming makes it more interesting and more valuable to the engineer.

the    rst three chapters of the book introduce notation and basic matrix (and determinant)
operations. it is well to study the notation, of course, but parts of chapter 2 may already be known
to the student. however, these chapters can be recommended for the programming exercise that
they provide.

chapter 3 is devoted to matrix inversion and its problems. the computer methods discussed

are the gauss reduction and lu decomposition.

chapter 4 explores the solution to simultaneous equation sets. the equations of linear regres-

sion are developed as an example of a very    over-determined    set of linear equations.

chapter 5 provides the reader with a matrix    framework    for visualizing in three dimensions,
and extrapolating to n-dimensions.the equations of particle and rigid body dynamics are developed
in matrix form.

chapters 6 and 7 are largely concerned with the eigenvalue problem   especially as it relates
to multi-dimensional vibration problems. the approach given for solving both conservative and
non-conservative systems emphasizes the use of the computer.

marvin j. tobias
june 2011

c h a p t e r 1

matrix fundamentals

1

1.1 definition of a matrix
a matrix is de   ned to be a rectangular array of functional or numeric elements, arranged in row
or column order. most important in this de   nition is that (at most) two subscripts, or indices, are
required to identify a given element: a row subscript, and a column subscript. that is, a matrix is
a 2-dimensional array. included within the de   nition are arrays in which the maximum value of
one, or both subscripts is unity. for example, a single    list    of elements, arranged in a single row or
column, is referred to as a    row    or    column    matrix. even a single element may be referred to as a
one-by-one (i.e., 1x1) matrix.

by way of illustration, the following matrix,    a,    is diagrammed:

   
            

a =

a11
a21
      
am1

a12
a22
      
am2

a13
      
      
am3

   
            

      
a1n
      
a2n
      
. . .
       amn

the above rectangular matrix has m rows, and n columns.the purpose of this book will be to discuss
and de   ne the arithmetic (and mathematics) of such arrays. practical applications will be discussed,
in which the array will often be viewed and manipulated as a single entity. once the notation of
matrices is learned, there follows a very large advantage in being able to work with the array as an
entity, without being encumbered with the arithmetic manipulation of the numeric values inside.
that is, one of the big advantages is that of    bookkeeping.   

carrying this illustration further, we write an m-by-n set of id202ic equations as:

            
         

= c1
a11x1 + a12x2 + a13x3 +        + a1nxn
= c2
a21x1 + a22x2 + a23x3 +        + a2nxn
      
       =       
am1x1 + am2x2 + am3x3 +        + amnxn = cm .

      

      

      

(1.1)

the above de   nes a set of m-equations in n-unknowns, the solutions to which will be explored in a
later chapter. right now, the point is to compare the equation set (1.1) with the de   nition of the m
row by n column matrix above. this chapter will concentrate on the basic rules of matrices, which

2

1. matrix fundamentals
will, among other things, allow us to write the set (1.1) as:

ax = c

(1.2)

wherein the a matrix has the form diagrammed above. in (1.2), each of the literal symbols represents
a matrix. the a matrix is a rectangular one, with m rows, and n columns. the x matrix has n rows
and just one column. it is usually referred to as a    vector,    as is the matrix, c, which has m rows and,
again, just one column. as mentioned earlier, x and c can also be called column matrices (or column
vectors).

it will be noted immediately that, although (1.2) is beautifully compact, it does not convey all
the information of (1.1). that is, (1.2) does not make the    dimensionality    clear: it is not evident
that a is m rows by n columns. this information must come from the context of the discussion   a
fairly small price to pay.
if the set (1.2) is    square    (i.e., m = n), then associated with the matrix a will be a
   determinant,    written |a|, or |aij|, whose elements are those of a, and in the same row, column
relationship. note the    absolute value    bars. this notation is not only convenient, but meaningful,
since a determinant, though written as an array, does evaluate to a single functional, or numeric,
value (but this |a| must not be assumed to be necessarily positive).

(cid:12)(cid:12)a

(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a11
a21
      
an1

a12
a22

an2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).

       a1n
       a2n
       akn
       ann

determinants are of great interest in this study of matrices. they    determine    the character-
istics of the related matrix, and play a particularly important role in the solution to simultaneous
equation sets. some of the methods used to evaluate determinants will be discussed in the next
chapter. at this point it is enough to simply establish that determinants are de   ned for square arrays
only, and that they are scalar quantities.

1.1.1 notation
matrices in which both indices are > 1, like the matrix a in (1.2), will be written using an upper
case letter, boldfaced. equivalently, we may denote such a matrix as [aij ]. since dimensionality must
be set in the context of discussion, it will often be done as: a(mxn). the expression within these
parentheses is read as:    m-by-n.    the row index will always be stated    rst. the vectors x, and c may
be written as {x} and {c}, and when necessary, {x}(nx1), although it will be quite rare to have to write
this in this way. in particular, once it is clear that a is (mxn), we will see that the dimensions of {x}
and {c}, in (1.2), are determined.

the matrix or vector, itself (as an entity), is written in boldface type. however, the elements of
the matrix are not bold, and may be written as [aij ], and {x}, for example (not bold.). however, it
is sometimes necessary to refer to a row or column within a rectangular (or square) matrix. in such
case it will be written in boldface; i.e., {a1} would refer to a column within a.

      
    written as v or
{v1, v2, v3}
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 3.1

   
    =

a13
a23
a33

a12
a22
a32

v2
v3

   
    a11

a21
a31

1.2. elemetary matrix algebra 3

the {x} and {c} vectors in (1.2) are    column    vectors. there can also, be cases in which the row
dimension is unity: a (1xn) vector. such a vector is called a    row    vector. it will be written within
text as [v]. please be careful to note the difference between [v] and {v}. for example, if we were to
select vectors from the matrix a, the row vectors would have n elements, but the column vectors
would have m elements   a very signi   cant difference. notice also that [v] (a row vector) will not be
confused with [aij ] (a rectangular matrix).

within a text discussion, it would be very unwieldy to write the elements of a column vector
vertically down the page. therefore, if the elements of either a row or column vector must be
delineated, it will be done across the page (   horizontally   ). a three element column vector, {v},

would be written as: {v1, v2, v3}.       
    v1

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) .

(cid:20)

a three element row vector would be written [u1, u2, u3], with square brackets.

some notation examples, (numerical values chosen at random):

a = a(3x3) =

0 1.6
2.2 5.2 1.1
1.0 3.2 4.4

(1.3)

note that a12 (for example) refers to the element in the    rst row and second column (in the example
its value is 0).the row subscript is always given    rst. ordinarily, the square brace, [. .], is the notation
for a matrix (while the single vertical bar denotes its determinant, |a|), but, notice that the double
vertical bar is sometimes used to denote a matrix.

as will be seen in coming chapters, a matrix is often viewed as an assemblage of vectors. for
example, a in (1.3), may be viewed as three row vectors, [ak]. note that the entity within the square
braces must be shown bold, because it refers to a vector, (i.e., ak), not an element. a could also be
viewed as three column vectors, {ak}. note that the type of braces used distinguishes between a row
or a column vector. for example, with reference to (1.3):

[ a2] =(cid:17)

(cid:18);

{a2} =(cid:19)

2.2 5.2 1.1

0 5.2 3.2

and, also note that {a2} is a column vector, but, is written across the page (for convenience). within
text it would be written as { a2 } = { 0, 5.2, 3.2 }, with commas.

it is extremely dif   cult to strictly adhere to an unambiguous set of notation rules. then, new
rules, possibly contradictory, may be found throughout the book. the most important    rule    is to
describe each topic clearly. notation rules may sometimes be    bent    to    t the discussion.

1.2 elemetary matrix algebra
in order to develop an elementary matrix algebra, the de   nitions of matrix equality, and the basic
operations of addition, and multiplication, must be agreed upon. it will be found that there are some

4

1. matrix fundamentals
fundamental differences between matrix algebra and that of    ordinary    algebra, which deals with
   scalar    entities   those ordinary numbers and functions whose dimension is 1x1. but, the rules of
matrix algebra are logical, and will seem obvious rather than obtuse or complicated.
to begin, two matrices are equal iff (iff       if and only if    ) the dimensions of each are the
same, and their corresponding elements are equal. for example, a = b iff they both have the same
dimensions, mxn, and aij = bij , for all i and j.

1.2.1 addition (including subtraction)
the sum of two (or more) matrices is formed by summing corresponding elements:

c = b    a implies
[cij] = [bij]    [aij] .

(1.4)

note that if the two matrices are of different dimensionality then corresponding elements cannot
be found, in which case addition is not de   ned. matrix addition is de   ned only when b and a have
the same numbers of rows and columns, respectively. when this is the case, the matrices a and b are
said to be    conformable in addition.    if all the elements of a are respectively the negatives of those
of b, then the sum, c, will have all zero elements. in such case, c is known as a    null    matrix (the
   zero    of matrix algebra). also, if a happened to be null, then c would be equal to b, cij = bij for
all i and j.

since addition is commutative for the elements of the matrix, then matrix addition itself is

commutative. that is, a + b = b + a.

1.2.2 multiplication by a scalar
the matrix (k)a is formed by multiplying every element of a by the scalar (k). note that the
notation (k), with parentheses, is used here. however, the notation, ka, will also be used. neither
(k)a, nor ka, will be confused with id127, because row, or column, vectors (also
expressed in lower case) must be written as {k}, or [k]. in passing, we note that if a is square (nxn),
and is multiplied by the scalar, k, then the determinant of a will be multiplied by kn. conversely,
then (k)|a| will mean the multiplication of a single row, or column, by k. more on this, later.

1.2.3 vector multiplication
since rectangular matrices are composed of vectors, we will    rst discuss vector products, before
de   ning the product of these    larger    matrices. the most important product of two vectors is their
   dot product,    or    scalar product.    this product results in a scalar   just as does the vector dot
product in vector analysis. furthermore, the numerical result is the same also, since it is the sum of

the products of the corresponding elements.

vector dot product     u     v

    [u1        un]

1.2. elemetary matrix algebra 5

         
       = (u1v1 + u2v2 +        + unvn) = n(cid:21)

j=1

uj vj .

         
       v1
...
vn

it may help to visualize the premultiplying row vector    swinging into the vertical,    and then mul-
tiplying element-by-element, as in the following diagram. nevertheless, the premultiplying vector
must be a row vector.

...

=

note that both vectors must have the same number of terms (elements). that is, the two
vectors must have the    same dimensions.    if such were not the case, the two vectors would not be
   conformable, in multiplication.    most important is that the dot product is always seen as the product
of a row vector times a column vector; and its result is a (1x1) matrix (i.e., a scalar). in this regard, the
most meaningful notation for the vector dot product is [u]{v}, or [v]{u}.

in analytic geometry, two vectors are written: u = u1i + u2j + u3k, and v = v1i + v2j + v3k, where
i, j, and k, are    unit vectors    in the directions of an    xyz    coordinate set (for example, i may be the
unit vector in the    x   -direction). the dot product of the two is:

u     v = |u||v| cos    = (u1i + u2j + u3k)(v1i + v2j + v3k) = n(cid:21)

uj vj

j=1

where |u||v| refers to the (scalar) product of their respective magnitudes, and    is the angle between
the two. in carrying out the multiplication, the following relationships are used:

i     j = i     k = j     k = 0, orthogonal axes;
i     i = j     j = k     k = 1, unit length.

in more than three dimensions, the idea is the same, but, we soon run out of (i, j, k,    ) unit vectors.
when many dimensions are possible, the unit vectors might be denoted as 1, 2, 3, 4    , and since
there may be several coordinate sets in consideration, we might distinguish these by subscript. for
example, 1x might be the unit vector along axis 1 of the x-set, while 1y would have the same meaning

6

1. matrix fundamentals
in the y-set. more often, the vector is simply written {v1, v2, v3,    }. although, we may have trouble
visualizing vectors in more than 3 dimensions, we simply draw the analogy to the 3 dimensional
case.

note that, just as in 3 dimensions, the n-dimensional dot product can produce a zero result
even when neither of the vectors is zero. that is, cos    could be zero, in which case the vectors are
perpendicular, or    orthogonal.   
the product v   v is always conformable, and is the sum of the squared elements of v. again, by
analogy with 3 dimensions, v   v is the    square of the length    of v, and sqrt(v   v) is |v|, the    length   
of the n dimensional vector. also u   v is the product |u||v| multiplied by the cosine of the angle
between u and v (as in vector analysis in 3 dimensions).

the product {v}[u] (a column vector times a row vector) is also conformable, when u and v
have the same dimensions. given that both vectors are (nx1), the product is an (nxn) square matrix.
this result will be reviewed again in the next paragraphs. see (1.21), section 1.6.

1.2.4 id127
in (1.2), the product ax is set equal to the vector c. apparently, then, the product of a rectangular
matrix and a vector is another vector. from (1.1), it will be seen that (in ax=c) each (scalar) element
of c is the sum of the element-by-element products of a row vector of a by the column x: the    rst row
vector of a is: [a1] = [a11, a12,    , a1n]. the product [a1]{x} is c1, the    rst element of the vector c.
that is (from (1.1)):

a11x1 + a12x2 +        + a1nxn = [a1]{x} = n(cid:21)

a1j xj = c1 .

j=1

the above equation is nothing more than a rewrite of the    rst equation in (1.1). but, the
important point to get here is that the left side of the above is the dot product [a1]{x}. the concept
of id127 is simply the extension of this to the case where there are more columns in
the    post-   multiplier.
in the general case, c=ab (i.e., c=a times b), each element of c is the result of a dot product
of a row from a and a column from b. in particular, the general element cij = ai     bj . the concept
is shown diagrammatically in figure 1.1.

   
               

a11
a21
      
am1

      
      

a12
      
      

   
               

   
          b11

b21
      
bk1

a1k
a2k
      
amk

   
          =

   
         

b12
      
      

      

b1n
b2n
      
bkn

[a1]{b1}
[a2]{b1}
      
[am]{b1}

[a1]{b2}
[a2]{b2}
      

      
      
      

[a1]{bn}
[a2]{bn}
      
[am]{bn}

   
          .

figure 1.1: the row-times-column dot product concept in id127n.

1.2. elemetary matrix algebra 7

the    gure is intended to emphasize the    row times column    dot product concept; so the a
matrix is shown    partitioned    into rows (by the horizontal lines), and the b matrix is partitioned
into columns. in the    gure, the a matrix is shown with m rows, and k columns, i.e., a(mxk). the b
matrix has k rows and n columns, b(kxn). the c matrix elements are all the results of a vector dot
product. the following statements de   ne id127, and will clarify the dimensionality
of c.

(cid:129) each element of the product matrix, cij , is the result of the dot product [ai]{bj }.

cij = [ai]{bj} = k(cid:21)

s=1

ais bsj .

(1.5)

(cid:129) if the dot product [ai]{bj } is to be conformable, the number of terms in ai must be the same
as the number of terms in bj . then the number of columns in a must equal the number of rows
in b. thus, b must have k rows, conforming to the k columns in a.

(cid:129) the conformability of ab does not depend on the number of rows of a, nor the number of

columns of b.

(cid:129) as each succeeding row in a is selected (to form the next dot product), a new row is created
in the result, c. then, c must have the same number of rows as a. the same reasoning shows
that c must have the same number of columns as b. therefore, c is (mxn).

two (mxn) matrices are conformable in addition,but not in multiplication.for conformability
in the multiplication, ab, we must have a(mxk)b(kxn). that is, the underlined dimensions must
be the same. at    rst, this may be confusing. but, there is a simple way to write down, and immediately
determine conformability: just write the two sets of dimensions within the same parentheses, and
   cancel    the internal numbers, if they are the same. then if a is (mxk), and b is (kxn), we write
(mxkxk      xn)   (mxn). in this case, since the columns of a, and the rows of b are equal in number;
then the    k   s cancel.    this simple expression not only tells us that a and b are conformable, but
also indicates that the resultant matrix will be (mxn). if the    k   s don   t cancel,    i.e., the two inside
dimensions in the expression are not the same, then a and b are not conformable in multiplication.
when both matrices are (mxn), we have (mxnxmxn) in which case the internal subscripts do not
match.
a    rst
(2x3x3x2)   (2x2):

example of id127,

following product,

consider

the

as

(cid:22)

ab =

3 4    1
6
2 0

(cid:23)    
    3
5
7    2
6
6

(cid:22)

   
    =

(cid:23)

31
1
42 46

.

(1.6)

these matrices are also conformable in reverse order. the reader should calculate the product ba,
and take special note that the result is (3x2x2      x3)     (3x3). the very same matrices, a and b, but

8

1. matrix fundamentals
very different results   which illustrates thatid127 is not commutative . that is, in
general ab (cid:4)= ba. the product ba may not even be conformable in multiplication, even though ab
is perfectly legal. for emphasis, however, please note that in general ab (cid:4)= ba even if both products
are conformable. try a few simple matrix products to prove that this is the case (we will see, shortly,
that in some cases, multiplication is commutative).

because of the non-commutative nature of the matrix product the order of the product, must
be stated explicitly. for example, ab can be described as    the premultiplication of b, by a,    or
alternatively,    the post multiplication of a, by b.   

id127 is, however, associative. that is:

a(bc) = (ab)c = abc .

(1.7)

it does not matter whether we form the product bc,    rst, then premultiply by a, or form ab, then
postmultiply by c. further, it is distributive:

a(b + c) = ab + ac .

(1.8)

from (1.7), we may draw the id136 that the powers of a (necessarily square) matrix, say a, are
de   ned: a2 = a(a), a3 = a(a)(a), and so on. then, it follows that matrix polynomials are also
valid:

p(a) = c0an + c1an   1 +        + cn   1a + cni .

(1.9)
in (1.9), the coef   cients, ci, are scalar constants; cn multiplies the    unit matrix,    i, de   ned in sec-
tion 1.3, below.

because id127 is so fundamental to our study, the reader should try
several examples, to become sure of the method. in each case, write the expressions like
(2x3x3      x2) to see how these indicate the conformability and the dimensions of the result.

(cid:5)

(cid:5)

(   a prime   ). a
(cid:5)
(nxm) under transposition. also, {v}

1.2.5 transposition
the matrix transpose of a is written a
is obtained by interchanging the rows and
(cid:5) => [v], that is, the
columns of a. then a(mxn) becomes a
transpose of a column is a row, and vice versa. the transpose operation is very important. for clarity,
it may sometimes be necessary to write a transpose as at, rather than a
transpose of a matrix product: suppose c = ab, and we wish to express the transpose, c
, in terms
of a
. remember that the cij element of c is the dot product of the ith row of a into the
jth column of b. upon transposition, the columns of b become rows, and the rows of a become
(cid:5)
columns. it therefore follows that in order to preserve the correct dot products, we must take the b
and a

matrices in reverse order. that is:

(cid:5)
and b

.

(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5) = (ab)
c
as a check, consider the 2,3 element of c

(cid:5) = b
(cid:5)
(cid:5)

(cid:5)     btat .

a

it is clear that the element c32 is obtained by [a3]{b2}. the element c

. it is the same as the 3,2 element of c. from (1.6),
(cid:5)
3}.

(cid:5)
(cid:5)
23 is obtained by [b
2]{a

(1.10)

apparently then, the reasoning of (1.10) is correct. this is known as the    reversal rule    of matrix
transposition. by logical extension of this rule, to continued products:

1.3. basic types of matrices 9

(cid:5) = (abc)
d

(cid:5) = (c)
(cid:5)
(cid:5)

(cid:5) = c
(cid:5)
(cid:5)
, then b

(ab)
(cid:5)

(cid:5)

(cid:5)

(cid:5)
a

b

.

(1.11)

note that for any matrix, a(mxn), that if b = a
under transposition. such a matrix is called    symmetric.    see the next section, below.

= b = a

a

a. that is, b is unchanged

basic types of matrices

1.3
1.3.1 the unit matrix
a square (nxn) matrix whose ij elements are zero for i (cid:4)= j, and whose elements ii are unity, is
de   ned as the    unit matrix,    i. i corresponds to unity in scalar mathematics. for example, if they are
conformable, i{x} = {x}, or ai =a. just as in scalar algebra, the multiplication of a matrix, a(nxn),
by the unit matrix, i(nxn), leaves a unaltered. further, i commutes with any square matrix of the
(cid:5)
same dimensions (i.e., ia = ai = a). note also that i = i(i), and i

= i.

in the unit matrix, i, the unity elements are said to lie in the    principal diagonal,    or the    main

diagonal.    the    off-diagonal    elements are zero.
delta.    by de   nition,   ij = 1, for i = j, and   ij = 0, for i (cid:4)= j.

the unit matrix can also be written as [  ij ]. the symbol,      ij ,    is known as the    kronecker

1.3.2 the diagonal matrix
if the main diagonal elements are not unity, but all elements off this diagonal are zero, then the
matrix is a    diagonal matrix.    the diagonal elements are not, in general, equal in value. in the cases
in which the main diagonal elements are equal, the matrix is called a    scalar matrix.    (in the matrix
polynomial, written above, (1.9), cni is a scalar matrix.)

the product of two diagonal matrices is another diagonal matrix, whose main diagonal el-
ements are the products of the corresponding elements of the two given matrices. clearly, then,
diagonal matrix products commute. however, if a is not diagonal, and b is diagonal, the product is
not commutative. in ba, the corresponding rows of a are multiplied by the diagonal elements of
b, while in ab, the corresponding columns of a are multiplied by the diagonal elements of b. try
both cases, to be assured that this is true.

1.3.3 orthogonal matrices
the rows (and/or) columns of an orthogonal matrix are perpendicular (orthogonal), in the very same
sense meant in vector analysis. that is, the dot product of any row with another is zero. a simple
example is:

(cid:22)

cos        sin   
cos   
sin   

(cid:23)

a (2x2) orthogonal matrix.

10

1. matrix fundamentals

clearly, the rows and columns of the above 2x2 are orthogonal; their dot products are zero. in
the case of this example, the matrix is also said to be    orthonormal,    because the lengths of the
rows/columns are normalized to 1.0 (i.e., the dot product of any row/column into itself is 1.0). the
orthogonal matrix has frequent application in engineering problems.
(cid:5)
given an nxn orthonormal matrix, a, it should be clear that a

a = i, the unit matrix, because
a
a simply forms all the dot products of the columns of a with each other. only when a column
is dotted into itself is there a nonzero result, and that result will lie on the main diagonal, and will
have the value unity. more generally, if a is just orthogonal (not normalized), then a diagonal matrix
results from the a

a product.

(cid:5)

(cid:5)

1.3.4 triangular matrices
if the matrix, a, has all zero elements below the main diagonal, it is known as an    upper triangular   
matrix. the transpose of an upper triangular matrix   one with all zero elements above the main
diagonal   is called    lower triangular.   

such matrices are very important because (1) their determinant is easily calculated as the
product of its main diagonal terms, and (2) its inverse is similarly easy to determine. the following
example (though not a matrix inversion) indicates the ease of solution of a triagular set of equations:

   
    1 2    1

   
   

0 3
0 0

3
5

      
    x1

x2
x3

      
    =

      
    7

9
5

      
    .

since the last equation is    uncoupled,    x3 = 1 by inspection. once x3 is known, x2 can be solved,
and then x1 follows.

it is not surprising that many methods for solving determinants, equation sets, and matrix

inversions incorporate matrix triangularization.

symmetric and skew-symmetric matrices

1.3.5
a matrix which is unchanged under transposition is known as    symmetric.    for example the matrix,
a, below, is symmetric (a
a =

   
    w =

   
    a

0    w3

   
   

   
   

= a),

w2

(cid:5)

w3

e f
e
g
b
f
c
g
symmetric

   w2
skew-symmetric
(note: a, b, c, e, f, g, and wi, are scalar elements)

w1

0    w1
0

and we note that {ai} = [ai], i.e., corresponding rows and columns are equal. for example, row 1:
[a, e, f ] equals column 1: {a, e, f}.

(cid:5)
are usually symmetric. later on, we will have use for the fact that, for any real matrix, b, b

symmetric matrices play a large part in engineering problems. for example, energy functions
b is

1.3. basic types of matrices 11

(cid:5)

(cid:5)

if w

always a square, symmetric matrix. that is, in general, b is (mxn), and the product (nxmxmxn) is
(nxn), i.e., square. it is obvious that (b
=    w, then w is called a    skew-symmetric matrix.    since the principal diagonal
elements are unchanged under transposition, then necessarily, the main diagonal elements of a
skew-symmetric matrix must be zero. the most prominent example of a skew-symmetric matrix is
the angular velocity matrix (chapter 5).

b, i.e., the product matrix is symmetric.

(cid:5)
b)

= b

(cid:5)

1.3.6 complex matrices
a matrix, z, whose elements are complex numbers can be written [zij ], where zij = xij + jyij , or
      1). the latter form shows a    separation    of the real
z = x + jy, (where    j    is the notation for
and imaginary parts into separate matrices. in this notation, both x and y , are composed of real
numbers. a matrix, w = x     jy, is called the    conjugate    of z. the transpose of w is referred to
as the    associate    of z.

the sum,or product,of two complex matrices can be formed in the straightforward,element by
element, way   using complex arithmetic   or using the second notation, (z = x + jy), previously
coded (real arithmetic) routines can be used, since x and y are composed of real numbers. for
example:

z1z2 = (x1x2     y1y2) + j (x1y2 + y1x2) .

the hermitian matrix: if the elements of the complex matrix, z = x + jy, are such that x is symmetric,
and y is skew symmetric, then z is known as an    hermitian    matrix. the hermitian matrix is equal
to its    associate.    that is, if z is hermitian, then z is equal to the conjugate of its transpose. the
hermitian matrix (with its symmetrical real part) is similar in ways to the (entirely real) symmetric
matrix.

1.3.7 the inverse matrix
thus, far, we have not de   ned matrix division. in the general case, no such operation as a/b exists.
however, if a is a square matrix, then there may be a matrix, b, such that ab = i. in this case,
the matrix b is referred to as the    inverse    of a, and is written with    1 in superscript as b = a
   1.
similarly, a = b

   
   

   1. the notation a/b or a = 1/b is never used.
   
   
   
   1
2 2
1
0 1
1    3 0

the matrices, a and b, shown below, are examples:
6    2
2    1
2

   
      3
   1
3    5
   1.
note that inverse matrices commute (i.e., aa

   1, and a = b

   1 = a

a =

b =

and, since ab = ba then b = a

is true by multiplying ab and then ba to show that they are the same.

   
   

   
   1 0 0

0 1 0
0 0 1

ab =

   1a). using the example, prove that this

12

1. matrix fundamentals

finding the solution to a (square) set of id202ic equations (when the solution is unique)

is equivalent to    nding the inverse of the coef   cient matrix:
ax = c, then
)ax = (a
)c;
   1
x = (a
   1
)c

given

   1

(a

assuming that a

   1 exists.

(1.12)

not every matrix has an inverse. for example, an nxm (non-square) matrix does not. some square
(nxn) matrices do not have an inverse. those that do not are called    singular matrices.   

the inverse of a diagonal matrix is another diagonal matrix, whose principal diagonal elements
are the reciprocals of the corresponding elements of the given matrix. clearly, then, a diagonal matrix
with a zero element on the main diagonal, is    singular.   

also, the transpose of an inverse matrix is equal to the inverse of its transpose. that is, given

a    non-singular    matrix, a:

then

) = i
(cid:5) = (i)
(cid:5) = i
(cid:5)
and, by postmultiplying by the inverse of a-transpose, (a
   1 .

   1
a(a
   1
(a(a
   1
(cid:5)
(a

(cid:5) = (a
(cid:5)

))
a

   1

(a

)

)

)

(cid:5) = i

   1:

)

the above equation shows not only the proof of the above statement, it also shows that the inverse
of a symmetric matrix is also symmetric.

by similar reasoning, consider the matrix product, c = ab. postmultiplying by b

   1

   1 = a .

cb

   1:

now, postmultiply by a

) = i .

c(b
   1 must be equal to the product b

   1
   1. that is, the reversal rule applies to the product
then, c
of matrices: the inverse of the product of two matrices is equal to the product of their individual
inverses, taken in the reverse order. this fact is sometimes referred to as the    reversal rule    of matrix
multiplication. it is worth reviewing that this reverse order phenomenon was also found in forming
the transpose of the product of two matrices, (1.10).

   1a
   1a

1.4 transformation matrices
it is frequently necessary to manipulate rows, columns, elements within a matrix. section 3.2 of
chapter 3 discusses three    elementary operations    that are useful in diagonalizing a matrix. these

operations are brie   y introduced here simply because they are good practice, and give excellent
insight in the basic operations.

1.4. transformation matrices 13

if a unit matrix row/column i is interchanged with row/column j, and that altered unit matrix

is used as a premultiplier on a, the rows i and j of a are interchanged.

   
    0 1 0

1 0 0
0 0 1

   
   

   
    a11

a21
a31

   
    =

   
    a21

a11
a31

a12
a22
a32

a13
a23
a33

as a postmultiplier:   

    a11

a21
a31

   
   

   
    0 1 0

1 0 0
0 0 1

   
    =

   
    a12

a22
a32

a12
a22
a32

a13
a23
a33

   
    .

   
    .

a22
a12
a32

a23
a13
a33

a11
a21
a31

a23
a13
a33

if the ith main diagonal element of the unit matrix is multiplied by a factor, k, and then that altered
unit matrix is used as a premultiplier on a, the corresponding row of a is multiplied by k:

   
    1 0 0

0 k
0
0 0 1

   
   

   
    a11

a21
a31

   
    =

   
    a21

ka11
a31

a12
a22
a32

a13
a23
a33

   
    .

a22
ka12
a32

a23
ka13
a33

as a postmultiplier:   

    a11

a21
a31

   
   

   
    1 0 0

0 k
0
0 0 1

   
    =

   
    a11

a21
a31

a12
a22
a32

a13
a23
a33

   
    .

ka12
ka22
ka32

a13
a23
a33

lastly, if the ijth (i (cid:4)= j ) element of i is replaced by a factor k, and the altered unit matrix is used as
a premultiplier, then to the elements of the ith row are added k times the elements of the jth row:

   
    1 0 0

1 0
k
0 0 1

   
   

   
    a11

a21
a31

   
    =

   
   

a12
a22
a32

a13
a23
a33

a11

ka11 + a21

a12

ka12 + a22

a13

ka13 + a23

a31

a32

a33

(1.17)

   
    .

of the three operative matrices, this last one is the most important. it would be worthwhile for the
reader to experiment with these operations   especially the last.

as an example use of such transformations, the following a(3x3) will be changed into triangle

form (the original 3,1 element is already zero).

(1.13)

(1.14)

(1.15)

(1.16)

14

1. matrix fundamentals

element 2, 1     0

element 3, 2     0

   
   

   
    1 0 0
   
    3    1 0

1
1 0
3
0 0 1

0
0

14
2
3
2 1

   
    3    1 0
   1
   
   
0
   
    1
0 0
1 0
0
0    2 1

5 2
2 1

   
    =
   
    =

   
    3    1 0
   
    3    1 0

14
2
3
2 1

0
0

2
2
3
0 1

0
0

   
    3    1 0
   1
0

5 2
2 1

   
   

   
    a =
   
    .

1.5 matrix partitioning
it is sometimes convenient to partition a given matrix into    submatrices,    accomplished by drawing
horizontal and vertical lines (the partitions) between the elements. such partitions are often used in
the multiplication of matrices.they are largely (but not completely) arbitrary. consider the following
matrix product, c = ab:

   
          a11

a21
a31
a41

c =

   
         

   
          b11

b21
b31
b41

a12
a22
a32
a42

a13
a23
a33
a43

a14
a24
a34
a44

   
          .

b12
b22
b32
b42

b13
b23
b33
b43

b14
b24
b34
b44

two lines (horizontal and vertical) partition the a matrix, while a single horizontal line partitions
the b matrix, in (1.18). that is, a is partitioned into 4 submatrices, b into 2. the product, then, can
be written:

(cid:22)

(cid:23)(cid:22)

(cid:23)

(cid:22)

(cid:23)

(1.18)

(1.19)

a1 a2
a3 a4

a1b1 + a2b2
c =
a3b1 + a4b2
a1 = (3x3); a2 = (3x1); a3 = (1x3); a4 = (1x1)
b1 = (3x4); b2 = (1x4) .

b1
b2

=

the check for conformable product matrices:
a1b1 = (3x3x3x4) = (3x4)
a3b1 = (1x3x3x4) = (1x4)

a2b2 = (3x1x1x4) = (3x4)
a4b2 = (1x1x1x4) = (1x4) .

these checks show that the submatrices given as products in (1.19) are conformable in multiplication,
and those shown as sums are conformable in addition. note that the matrix c is 4x4. it is partitioned
horizontally, into a 3x4 and a 1x4 (just like b).

the submatrices of a and b are conformable because the vertical line in a divides the columns
of a the same as the horizontal line in b divides its rows. note that if the vertical line in a changes

position, it forces the line in b to change position. but, the horizontal line, in a is arbitrary. it can be
moved anywhere without destroying conformability.

follow through the example below:

1.5. matrix partitioning 15

using the same de   nitions for the submatrices:

   
          .

0 3
0    1 2
5    1 0
2 1
0

   
    0 0 0 0

6 0 4 2
9 0 6 3

   
   

1
3
3

0
2
3
2

c =

   
         

   
          3

   
          6    1
   
    +
a1b1 + a2b2 =
a3b1 + a4b2 = [11     2 1 4] + [6 0 4 2] .
   
         

1
2
   1
4
2
1    1
0
2    1
0
   
    25
4
   2    5
   
          25

7    3 11
11    6
5
0
2

c =

7    3 11
11    2
7
5
6
5
6

10
7    5
17    2

then, the product ab is:

now, move the horizontal partitioning line in a up one row. note that the check of matrix con-
formability is:

a1b1 = (2x3x3x4) = (2x4)
a2b2 = (2x1x1x4) = (2x4)
a3b1 = (2x3x3x4) = (2x4)
a4b1 = (2x1x1x4) = (2x4) .

the important point is that these remain conformable no matter where the horizontal line is moved
in matrix a. it may be worthwhile to continue, by    nding the ab product, as done above, but with
the new partitioning.

in order to introduce partitioning, a simple 4x4 example was used. such an example fails to
show the value of partitioning (it would be simpler to just multiply ab). partitioning is of value
in cases of very large matrices. for example, partitioning can be used in the inversion process for
large matrices as a method for controlling roundoff error. also, partitioning is sometimes used
conceptually   where the submatrices are actually the given matrices of the problem. both of these
uses will be seen later in this book.

id127, itself, is done (conceptually) by    rst partitioning the premultiplier by
rows, and the postmultiplier by columns. then, each element of the product matrix is the    dot
product    of these partitions. yet, this rather basic conception can be changed. for example, try to

16

1. matrix fundamentals

visualize the premultiplier partitioned into columns and the postmultiplier in rows   in, say, an nxn
product. now, each (of the n) column times row products yields an nxn matrix; the sum of these n
products produces the end result.

finally, please note that partitioning is here referred to product matrices. it should be clear
that partitioning for addition (somewhat trivial) would be quite different. for example, none of the
matrices above are partitioned to be conformable in addition (i.e., for a + b = c).

interesting vector products

1.6
1.6.1 an interpretation of ax = c
in the previous discussion of id127, the equation set ax = c was used to show that
each element ci is the dot product [ai]{x}. but, there is another, very interesting, interpretation of
the equation set ax = c. a review of equation (1.1) shows that each xi multiplies only the terms in
the column {ai}. then, equation (1.1) can be written:

{a1}x1 + {a2}x2 +        + {an}xn = {c} .

(1.20)

the vector c is therefore seen to be formed from the weighted sum of the column vectors of a, the
weighting factors being the variables, xi. it is this interpretation of the equation set that leads to the
terminology of    transform    when referring to the set.

as an example of this interpretation, we return to an earlier example

(cid:22)

4    1
6
0

3
2

(cid:23)    
    3
5
7    2
6
6

(cid:22)

   
    =

(cid:23)

.

31
42

1
46

the columns {31, 42} and {1, 46} are found as weighted sums of the premultiplier columns:

(cid:24)

(cid:25)

(cid:24)

(cid:25)

(cid:24)

(cid:25)

31
42

= 3

3
2

+ 7

4
0

+ 6

(cid:25)

(cid:24)    1

6

, and

(cid:24)

(cid:25)

(cid:24)

(cid:25)

(cid:24)

(cid:25)

1
46

= 5

3
2

    2

4
0

+ 6

(cid:24)    1

6

(cid:25)

.

1.6.2 the (nx1x1xn) vector product
in the paragraph on vector products, it was mentioned that two vectors could be multiplied to form
a rectangular (very much non-vector) matrix. in three dimensions, consider v(3x1) times u(1x3).
note that they are conformable, (3x1x1x3), and this particular result is (3x3):

      
   (cid:17)

      
    v1

v2
v3

(cid:18) =

   
    v1u1

v2u1
v3u1

   
    .

v1u2
v2u2
v3u2

v1u3
v2u3
v3u3

u1 u2 u3

(1.21)

each row of v consists of just one element, and each column of u has one element. then, each dot
product elements of the product matrix has just the one vu term.

this is an unusual product of two vectors and is not at all the same result as v   u, (the dot
product). the result shows again that id127 is non-commutative. this particular
product is very important and useful when u and v are    eigenvectors      (chapters 6 and 7).

1.7. examples 17

1.6.3 vector cross product
there is no direct operation between two vectors (written as nx1 matrices) that results in the vector
product (or cross-product) of the two. however, by expressing the    rst vector, say {u} as a (3x3)
matrix, we can obtain a vector that is the equivalent of the vector analysis product, (u    v) (this is
only de   ned in three dimensions, of course):

   
   

0    u3

u2

0    u1
0

u1

u3

   u2

   
   

      
    v1

v2
v3

      
    =

      
    u2v3     u3v2
u3v1     u1v3
u1v2     u2v1

      
    .

(1.22)

the above may seem to be a very contrived construction of the vector product   and it is. however,
this kind of matrix will be seen to come up in just this way, in problems in kinetics, where the u
matrix contains the elements of an angular velocity vector. the (3x3) u matrix is    skew-symmetric   
(see section 1.3).

1.7 examples
   
1.7.1 an example id127
    ; b =

   
    2 1   1 3

given a =

(cid:22)

(cid:23)

5
0 4
1    1 0

1 2

. the check for conformability is (3x2x2x3). then

the result will be c(3x3) = ab. when the elements are written out to show the operations involved,
the result is:

   
    2(5) + 1(1)
2(4) + 1(0)
   1(5) + 3(1)    1(0) + 3(   1)    1(4) + 3(0)
1(4) + 2(0)
1(5) + 2(1)

2(0) + 1(   1)
1(0) + 2(   1)

   
    .

c = ab =

that is, all the column vectors of the product c are linear combinations of the two column vectors
in a. for example: c1 = 5a1 + 1a2 (note the bold, lower case    a   , subscripted, denotes a vector in
a   usually a column vector). thus, all three of the column vectors of c lie in the same plane   the
plane de   ned by the intersecting column vectors of a.

the same points can be made concerning the row vectors of c. these are all linear com-
binations of the rows of b, and they lie in the plane de   ned by the intersection of the b row
vectors.

in later chapters, matrices like c, above, will be discussed in some length. it will be shown

that they are    singular    matrices, whose determinant is zero.

18

1. matrix fundamentals

1.7.2 an example matrix triple product
in the study of vibrating systems, a particular triple product is important   one in which the middle
term is a diagonal matrix. p = adc, where d = [  ii dii], with nonzero elements on the main diagonal
only. the matrices involved are square, nxn. when a matrix is postmultiplied by a diagonal matrix
(as a is here), the effect is that the diagonal elements multiply onto the respective columns of the
premultiplier (a, in this case). the ad product is shown as

ad = [{a1}d11,{ a2}d22,      {an}dnn] ,

where a is partitioned into columns, and then those columns are multiplied by their respective dij
elements. that is, d11 multiplies {a1}, and so forth. now postmultiply by c, having    rst partitioned
it into rows. note that {ai}[cj ] is conformable: (nx1x1xn) = (nxn). so the product

(cid:21)

adc =

djj{aj}[cj] (the sum of n matrices, each nxn .)

j

although this result appears cumbersome, it can be a delight, because the djj factor and the two
corresponding vectors, all are related, in an    eigenvalue    analysis (chapters 6 and 7).

1.7.3 multiplication of complex matrices
this example shows the product of two complex matrices, a and c. the a matrix can be written:

   
   
    + j
   1.021 1.503
2.001
1.000 0.002    5.247
1.002 0.002    8.055

   
   
   .
   0.010 2.330 10.258

1.123 3.884 14.055
1.222 5.566 20.103

a =

however, the intent here is to emphasize another way in which a matrix may be shown in this
book   as a tabulation of values (usually with double bars at left and right). in the case of complex
matrices, the imaginary parts will be shown immediately under the real parts, as in a and c, below.
for example, a12 = 1.503 + j2.330.

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

matrix a

1.021 1.503
2.001
0.010 2.330
10.258
1.000 0.002    5.247
1.123 3.884
14.055
1.002 0.002    8.055
20.103
1.222 5.566

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

matrix c

   4.120
3.259
6.110    3.589
5.225    2.661
   3.840

2.125
6.005    3.010

3.124
0.011

0.000
6.120
0.000    3.580

1.751
4.777

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

being square, a and c are conformable in either order. this example    nds the product ca,
obtained in the usual way   using complex arithmetic. complex matrices are manipulated just like

1.8. exercises 19

real ones   but, with the considerable increase in operations required, simply because of the complex
numbers involved. it is recommended that the reader calculate a few terms of ca just    for practice.   

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

matrix ca

6.157    6.537    63.107
97.675
10.119

29.723

10.230
7.901

1.776
22.801
   1.271
43.970
6.057    12.668    91.931
101.522
10.219

33.519

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

a sample calculation for the element 2,3 of ca is shown here:
ca23 = [c2]{a3} = c21a13 + c22a23 + c23a33

c21a13

2.001 + j10.258    5.225     j3.840
c22a23     5.247 + j14.055        2.661 + j6.005
c23a33     8.055 + j20.103    2.125     j3.010

49.846 + j45.914
=
=     70.438     j68.909
43.393 + j66.964
=
22.801 +j43.970

=

ca23

ca could also be calculated by separating real and imaginary parts into separate matrices, as discussed
earlier. in this case, the product would be found as

ca = (crar     ci ai ) + j (crai + ci ar)

where, for example, cr = the matrix formed from just the real part of c. all the arithmetic in this
way becomes real. note that the order of the product matrices is important.

1.8 exercises
1.1.

show that a + b = b + a; that is, matrix addition is always    associative.   

1.2. how many vectors can be selected from the (mxn) matrix, a?

1.3. given two matrices, a and b, that are conformable (i.e., the product ab is conformable),

(cid:5)
is the product ba ever conformable? is a

(cid:5)

b

conformable? is b

a

conformable?

(cid:5)

(cid:5)

1.4. given u(4x4) whose    rst row is u1 = [5.11 2.46 0.567 6.91], and v(4x4) whose    rst
column is v1 ={3.03 -0.821 1.44 -2.02},    nd the value of w11 in the product w = uv.
time yourself in this calculation and use it to estimate how long it would take to manually
determine w, given all the terms of u and v.

20

1. matrix fundamentals
1.5. determine the product {v1}[u1], using the de   nitions of u1 and v1 from problem 1.4.

1.6. given the matrix equation a(nxn)x = c, express the vector c as a weighted sum of the

column vectors of a.

1.7.

find the vectors u and v,

and

   
   
   
   
   
       1
   
       4 1    2
v =
1 2    1
(cid:18)   
u =(cid:17)    1 2 3
1
5 1
       4 1    2
1 2    1
1
5 1

2
3

   
    .

1.8. given the matrix de   nitions at right: find the most ef   cient way to calculate

(cid:22)

1.9.

for a = 1   

7

a) abcv
b) v1u1v2u2
c) u1ab

(cid:23)

2
3
1    2

   nd a2, a3, and a10.

a, b, c = (nxn)
v1, v2 = (nx1)

1.10. solve the following equations for a: c = a + b and c = ab.
1.11. given that p (x) = x2     2x     2,    nd p (a), for a =

(cid:22)

(cid:23)

.

1    2
2    1

1.12. if ab = k[  ij ]    nd a

   1.

1.13. given the (2x2) orthogonal matrix,t(   ),    nd t2 and compare the result with t(2   ). using

(cid:22)

this information,    nd t6(   
cos        sin   
t(   ) =
cos   
sin   

.

(cid:23)

36 ).

1.14. show that the matrices t1 and t2 are orthogonal, i.e., that their rows/columns are mutually
1 indicates the transpose of t1. find the product, t1t2 and

(cid:5)

perpendicular. the notation t
show that this product is orthogonal.
t1 =

   
    cos        sin   

   
    ; t2 =

cos   

   
   

0
0
0 1

sin   
0

cos    0
0 1
(cid:5)

    sin   

sin   
0
0 cos   

   
   .

1.15. find the product

   
    1 1 1

1 1 1
1 1 1

   
   

   
    3
6
4
3    5
   2
0    8
1

   
   .

1.16. in the matrix product p = abc show that pij = [ai]b{cj}.

1.8. exercises 21

1.17. given the (4x4) matrix [aij ], determine an elementary transformation matrix that will

cause element a31 to vanish (go to zero).

c h a p t e r 2

determinants

23

introduction

2.1
the de   nition of a determinant is derived from the solution of id202ic equations. since the
single variable case is trivial, we will begin with the (2x2):
a11x1 + a12x2 = c1
a21x1 + a22x2 = c2

(2.1)

.

to eliminate x2, we multiply the    rst equation by a22, and the second by a12, then subtract the
second from the    rst:

(a11a22     a12a21)x1 = (a22c1     a12c2) .

equivalently, we may eliminate x1 (by the same methods):

(a11a22     a12a21)x2 = (a11c2     a21c1) .

(2.2)

(2.3)

the coef   cients on both sides of (2.2) and (2.3) can be viewed as expansions via    cross-multiplication   
of determinant arrays, as follows:

                                                
                                             

a11
a21
-

a11
a21
-

c1
c2
-

a12
a22
+

c1
c2

+

a12
a22
+

= (a11a22     a12a21)

= (a11c2     c1a21)

= (c1a22     a12c2)

(2.4)

the square arrays in (2.4)    expand,    by the cross   multiplication indicated, to the scalars shown on
the right sides. and, we de   ne the determinant in terms of its expansion. expansions are de   ned only
for square arrays. the result of the expansion is a scalar expression, or numeric value. that is, the
determinant is a scalar value. further, from (2.2) and (2.3), the values of the variables are found as
the ratio of these expanded determinants   all of which are known, given in the problem.

24

2. determinants

three variables:

a11x1 + a12x2 + a13x3 = c1
a21x1 + a22x2 + a23x3 = c2
a31x1 + a32x2 + a33x3 = c3 .

(2.5)

by the very same processes of elimination used above, we    nd that xj is expressed as the ratio of
two determinants:

xj = dj
d

; where d =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) a11

a21
a31

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a12
a22
a32

a13
a23
a33

known as cramer   s rule, where the expansion of d is given by:

d = a11a22a33     a11a23a32 + a12a23a31
    a12a21a33 + a13a21a32     a13a22a31

(2.6)

(2.7)

and the expansion of dj follows the same rules, after replacing the ith column of d with the vector,
c = {ci}.

the    cross-multiplication    algorithm for the (3x3) is much more complex than for the (2x2).
it is shown diagrammatically below. comparison of the diagram to equation (2.7) shows them to
be the same.

it will be noted that there are six terms in the expansion, rather than two, and further, in both

expansions, half the terms are negative, half positive.

note also, that each term in the expansion of the (2x2) has two factors, and each term in the

expansion of the (3x3) has three factors.

2.2. general definition of a determinant 25

now examine the row and column subscripts within each term: every row (and column)
subscript is represented once   and only once. this fact is extremely important to the developments
that follow.

continuing with the expansions of the (4x4), then (5x5), and so on, it would be found that
there are 24 terms in the expansion of the (4x4), and 120 terms in the (5x5) expansion. in these
cases, also, exactly half of the terms are positive, half negative. (note: the statement that a term is
   positive    or    negative    does not refer to the signs of the factors within the term. half the terms in
the expansion of a determinant with all positive elements will be negative.) in these cases we would
also    nd that within every term, each row, and each column, subscript is represented exactly once.
then, each term in the expansion of a (4x4) has four factors, and each term in the expansion of the
(5x5) contains    ve factors.

2.2 general definition of a determinant
the general (nxn), determinant, |a|, can be represented as a square, two-dimensional array of ele-
ments, each with two subscripts   the    rst indicating the element row position, the second indicating
column position. notwithstanding the two-dimensional representation, the determinant is a scalar.
that is, it    expands    (as discussed above) to a scalar value   either a numeric, or a literal (function).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a13
a23
a33
      
an3
the expansion of |a| can be written as shown here:

a11
a21
a31
      
an1

a12
a22
a32
      
an2
(cid:21)

(cid:12)(cid:12)a

(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

       a1n
       a2n
       a3n
      
. . .
       ann

(   1)s a1i a2j a3k

      

(2.8)

most of the remaining discussion in sections 2.2 and 2.3 will refer to, and clarify, this equation.
with respect to it, we note the following de   nitive statements:
1. the determinant, |a| expands into a sum of product terms.
2. each term is the product of n elements from the array, where (nxn) is the dimension of the

array.

3. no two elements in any one term can come from the same row, or column. for example,
equation (2.8) implies that the elements were selected    in row order.    the    rst element from
row 1 (any column, the ith), the second from row 2 (and any column except the ith), and so
on. this process continues until all possible terms have been selected. as an example, every
term in the expansion of a (4x4) will look like:

a1i a2j a3ka4l

26

2. determinants

where i can be any one of 4 columns, j can be any one of three columns (but not the ith), k
can be one of two, and l must be the one remaining. in the (4x4) expansion, then, there will
be 4x3x2x1, or 24 terms.

4. in general, there will be n! terms in the expansion (2.8).

5. the sign to be af   xed to each term will depend on the value of s, the superscript of the (   1)
factor, in (2.8). in every expansion, exactly half the terms will have a positive leading sign, half
a negative one.

the 5 statements given above de   ne the general determinant, except for the term, (   1)s, in equa-
tion (2.8), whose value depends on whether the superscript, s, is odd or even. this will be the subject
of the next section.

also, equation (2.8) and its discussion imply that elements are to be selected    in row order   
(the    rst element coming from the    rst row, etc.). but, of course, these factors could obviously be
reordered within the terms. that is, they could have been chosen    in column order,    or indeed, in
any order   just so long as no row or column index appears more than once in every term. but, it is
important to note that every term in a determinant expansion can be arranged such that either row
or column index appears in numeric order. the other index will then be in some permutation of this
order. since there are n! permutations of n things, there are n! terms in the expansion.

2.3

permutations and inversions of indices

the subject of permutations is concerned with the arrangements of given things, or objects, in which
the order of the arrangements is important. as an example, we ask: in how many ways can the digits
1,2,3,4 be arranged into a four digit number? (equivalently: how many permutations are there of 4
things taken four at a time?). all the possible permutations of this example are:

1234 2134 3124 4123
1243 2143 3142 4132
1324 2314 3214 4213
1342 2341 3241 4231
1423 2413 3412 4312
1432 2431 3421 4321

2.3. permutations and inversions of indices 27

there are clearly 24 permutations. this was pretty obvious from the beginning, since in choosing an
arrangement, we can choose any of the digits in the    rst place, (4), then we have one less to choose
from (3) for the second, two for the third place, and just one for the fourth. there are, then, 4! total
choices.

in general, there are n! permutations of n things taken n at a time.
in the expansion of a (4x4) determinant, every term contains four elements, chosen (as
described previously) from the rows and columns of the array, such that each row/column index
appears just once in each term. the four (product) elements in every term can be arranged such that
either row (or column) indices are in numeric order. when this is done, the other index will appear
in all possible (n!) permutations. these permutations, for a (4x4) case, are given in the table, above.
the above table represents only the column indices. adding the row indices (in 1234 order),

the following lists all term indices.

11 22 33 44
11 22 34 43
11 23 32 44
11 23 34 42
11 24 32 43
11 24 33 42

12 21 33 44
13 21 32 44 14 21 32 43
12 21 34 43
13 21 34 42 14 21 33 42
12 23 31 44
13 22 31 44 14 22 31 43
12 23 34 41     13 22 34 41 14 22 33 41
13 24 31 42 14 23 31 42
12 24 31 43
12 24 33 41
13 24 32 41 14 23 32 41

in the table, only the indices are shown. for example, (12 23 34 41), see arrow, represents the
term a12a23a34a41. the column indices are in the same order as those given in the earlier table. in
fact, the earlier table is clearer. there is no information contained in the repetitive numeric order of
row indices.

the main point is: given a method to write down all the permutations of n things taken n at a
time, we can directly write down all the terms in the expansion of an (nxn) determinant. developing
the method is non-trivial, and more important, it is still unclear how to af   x leading signs to these
terms.

inversions

2.3.1
given a permutation of n indices: ijklm        , an    inversion    is de   ned as a transposition of adjacent
indices. for example, ijklm undergoes one inversion by interchanging, say k with j (notice: the
inversion is the interchange of adjacent indices). more speci   cally, we de   ne the    inversions    in the
permutation ijklm          , to be equal to the number of such transpositions of adjacent indices to arrive at
the numeric order 12345        . for example, the permutation 3241 has four inversions:

(1)

(2)

3241     3214     3124     1324     1234

(4)

(3)

with these four inversions,    natural numeric order    (1234) is restored to the permutation (3241).
in equation (2.8), de   ning the expansion of a determinant, the exponent,    s,    on the (   1) factor is

28

2. determinants

de   ned as the number of inversions in the indices of that term. given that row indices are (arbitrarily
taken) in numeric order, then s would be the inversions in the column indices. in the example (3241)
given here, since s=4, the term (13 22 34 41) would be given a positive leading sign. note that the
numeric value of s is not important   just whether it is odd, or even.

there is an easy way to determine the inversions, s. given the permutation, take each digit in
turn, and determine the number of digits to its right which are numerically less. example: (3241):
3 has two inversions (it is larger than both 2 and 1, which are to its right). 2 has one inversion, and
4 has one. the total is 4.

just for practice, consider (45312). there are 8 total inversions. now, interchange the    5    with
the    2,    and determine inversions of (42315). there are 5. note that the inversions changed by an
odd number. it may be a good exercise to write the inversions down in each case   showing that
numeric order is restored.

we now have the capability to expand any (nxn) determinant via (2.8). without a computer, it
would be a lengthy, and arduous process if n > 4. even with a computer, there are very few programs
available which use (2.8) directly to evaluate determinants.

up to this point there is an implication that the value of s depends upon just how the elements
are ordered within the terms in the expansion.but,the sign of terms in the expansion must not depend
on an arbitrary order of the products. we will now show that the sign will not change   however, it
is true that the numeric value of s depends on the ordering.
given a permutation ijklm        , if any two adjacent indices are transposed, the change in s will
be either +1, or    1. for example, if k and j are transposed, giving ikjlm        , the change will be +1 if
k > j, or it will be    1 if k < j. obviously, the contributing inversions from the other indices will be
unchanged. for example, (45312)     (43512), changes by    1, since 3 < 5. by inspection, (43512)
does indeed have 7 inversions. (45312) has 8.
if the elements in a term in (2.8) are reordered, both row and column indices are reordered.
then, if two adjacent elements in a term are transposed, the row inversions will change by +1, or
   1. the column inversions will also change by +1, or    1. therefore, as the term is reordered by a
series of such transpositions, the total inversions, considering both row and column, must change by an
even number. the numeric value of s will, in general, change, but it will remain either even or odd.
thus, the sign of the term does not change by some arbitrary reordering of terms. an example: in a
(4x4) the term:

13

21

34

42 s=3 (inversions of column indices)

will have a leading negative sign (s odd). if we    scramble    the elements to:

34

42

13

21 s=9 (inversions of both row and column) .

note that s has changed signi   cantly, but the leading sign of the term still is determined to be
negative. the numeric value of s is always minimized if the elements are given in row order, or
column order. furthermore, s is the same in either case. the same term with column indices in order

gives:

2.3. permutations and inversions of indices 29

21

42

13

34 s=3 (inversions of row indices) .

using this fact, an important conclusion can be drawn: a square matrix has the same determinant as
its transpose. that is, |a| = |a
(cid:5)|. by de   nition, the matrix [aij ] is transposed by interchanging rows
and columns. one term in |a| (in a (4x4) case) would be (21 42 13 34). its corresponding term in
|a
(cid:5)| will be (12 24 31 43). clearly, these both have the same values for s, and the numeric values
of the elements are identical. this is true for all corresponding terms; and the argument obviously
holds for the (nxn) case. thus, the assertion is proved.
before leaving the subject of inversions, we will show that if any two indices in a permutation
are transposed, the inversions change by an odd number. given ijklm        , let p equal the number of
indices between the two which are to be interchanged. for example, if j is to be interchanged with
m then p=2, since there are two indices between m and j. choosing j    rst, it is moved to the right,
over k, and then over l, and reinserted. this amounts to p (2) transpositions of adjacent indices.
now, m is removed, and moved to the left over p+1 (3) indices, and inserted into the place vacated
by j. in the whole operation, there are 2p+1 transpositions. since 2p+1 is necessarily odd, then s
will have changed by an odd number. this fact proves that if two rows, or columns of a determinant
are interchanged, the sign of the determinant is reversed. this will be discussed later as one property
of a determinant.

2.3.2 an example determinant expansion
the following (4x4) determinant expansion is shown as an example of the method discussed in
this article. that is, each of the 24 (4!) terms is found by determining all possible permutations of
1,2,3,4, and using these as the column subscripts. the row subscripts are taken in numeric order.
the leading sign of each term is determined by the method of inversions. note: by coincidence, the
products in every term turned out to be positive.

(cid:12)(cid:12)a

(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

3

   2
3    4    5
4    7    6
   3

2    5
6
9
4    10

5

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

the two tables, below, show each term of the expansion followed by its value. the s(  ) column
gives the value of the inversions (   s   ) and the leading sign. the    sum    is the running accumulated
value of the signed terms. the accumulation runs from top to bottom of the 1st (left) table, then

30

2. determinants

continues in the second. the    nal value of    sum    is |a|.

term

a11a22a33a44
a11a22a34a43
a11a23a32a44
a11a23a34a42
a11a24a32a43
a11a24a33a42
a12a21a33a44
a12a21a34a43
a12a23a31a44
a12a23a34a41
a12a24a31a43
a12a24a33a41

value
480
288
700
450
336
360
540
324
600
405
288
324

s (  ) sum
0(+) +480
1(   ) +192
1(   )    508
2(+)    58
2(+) +278
3(   )    82
1(   )    622
2(+)    298
2(+) +302
3(   )    103
3(   )    391
3(+)    67

term

a13a21a32a44
a13a21a34a42
a13a22a31a44
a13a22a34a41
a13a24a31a42
a13a24a32a41
a14a21a32a43
a14a21a33a42
a14a22a31a43
a14a22a33a41
a14a23a31a42
a14a23a32a41

value
420
270
320
216
240
252
420
450
320
360
500
525

s (  )
2(+)
3(   )
3(   )
4(+)
4(+)
5(   )
3(   )
4(+)
4(+)
5(   )
5(   )
6(+)

sum
+353
+83
   237
   21
+219
   33
   453
   3
+317
   43
   54
   18

|a| = - 18

2.4

properties of determinants

(cid:5)
1. a square matrix, a, and its transpose, a

, have the same determinant. this property is proven

in section 2.3.1, top of page 29.

2. if any row, or column, of a determinant contains all zero elements, that determinant equals zero.
every term in the expansion of |a| must contain exactly one element from every row (column)
of |a|. then, every term in the expansion contains a zero factor. thus, |a|= 0.

3. the determinant of a diagonal matrix is equal to the product of its diagonal elements. clearly, in
the expansion of any determinant, one term is (11 22 33          ); and this term will have a leading
+ sign (since there are no inversions in either index). every other term in the expansion will
contain a zero factor.

4. if any row, or column, of a determinant is multiplied by a constant value, the result is that the
determinant is multiplied by this amount. each term in the expansion must contain a factor
that is multiplied by the constant.
it is interesting to note this difference between matrices and determinants. if a matrix is
multiplied by a scalar, k, every element is multiplied by that scalar. then, if the matrix is
(nxn), the effect is that its determinant is multiplied by kn.

5. if two rows, or columns, of a determinant are interchanged, the sign of the determinant is
reversed. when any two rows of |a| are interchanged, the order of the column indices in the
general term will not have changed, but two of the row indices will have been exchanged. since
the exchange of two indices changes the inversions by an odd number, the sign af   xed to this

2.4. properties of determinants 31

term must be reversed. because every term in the expansion must contain elements from these
two rows, the signs of all terms in the expansion change; the sign of |a| must be reversed. if
instead of two rows, two columns are interchanged, the columns indices in the general term
are exchanged causing the same sign reversal.
very similar reasoning is used in the proof of the next property.

6. if two rows, or columns, of a determinant are identical, its expansion is zero. to start, consider

the example of a (4x4), whose 2nd and 4th rows are the same.

a12a24a33a41
a12a21a33a44

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a11
a21
a31
a41

a12
a22
a32
a42

a13
a23
a33
a43

a14
a24
a34
a44

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

two terms in the expansion of |a| are also shown   and, note that these terms are equal in
value, because a21 = a41 and a24 = a44. but, these terms will have opposite leading signs
(the column subscripts are 2431 in the    rst term and 2134 in the other). that is, 2134 is
derived from 2431 by interchanging the second and fourth subscripts. interchanging these
two subscripts changes the inversions by an odd number.
this argument holds in the general (nxn) case.every term in the expansion has a corresponding
identical term, the one whose column subscripts are reversed in the elements whose rows are
identical (note that in the example, the column subscripts interchanged are the second and
fourth   in the rows that are the same). thus, the leading signs are always opposite, and all
corresponding terms cancel   giving a zero result.

7. if some amounts are added to the elements of a row, or column, then the effect is the same as
the sum of the original determinant, plus a new determinant with the row (column) in question
replaced by the adders.
for example, a (2x2), with the amounts d1 and d2 added to the    rst column:

(cid:12)(cid:12)(cid:12)(cid:12) a11 + d1

a21 + d2

(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12) a11

(cid:12)(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)(cid:12) d1

(cid:12)(cid:12)(cid:12)(cid:12) .

a12
a22

a12
a22

a12
a22

(2.9)
after addition of the di factors, we will refer to the resulting determinant as |b|. since every
term in the expansion of |a| contains a factor ai1, then every term in the |b| expansion will
have a factor:

a21

d2

(ai1 + di ) .

then, every term breaks into two, the    rst being the same as that in the expansion of |a|, and
the other, from the expansion of a determinant |a| but, whose    rst column (in this case) is
replaced by the additive factors. nothing in this argument depends upon the adders necessarily

32

2. determinants

being added to the    rst column. and the argument holds if the adders are on a row rather
than a column.
now, if the di adders happen to be a constant, k, times the elements of some other column,
then (after factoring the k) the second determinant in (2.9) is one in which two rows are
identical. in that case, the second determinant is zero, by property 6. then:

8. if to any row, or column, there is added a constant factor multiplied by the corresponding elements
of any other row, or column, the value of the determinant is unchanged. this is an extremely
important property. it is almost always utilized in the expansion of determinants.
it is important to generalize property 2. consider an (nxn) determinant, |a|, one of whose
rows (or columns), say the jth, is initially zero   i.e., all the jth elements are zero. then, by
property 2, |a|=0. now, by repetitive use of property 8, add to the jth (row/column) arbitrary
multiples of other rows (columns). the result is (assuming a row):

rowj = c1(row 1) + c2(row 2) +          + cn(row n)

(2.10)

where the row-sums in (2.10) are to be viewed as element-by-element additions. for example,
   row1+row2    would be viewed just like two vectors would be added:
|(a11 + a21)(a12 + a22)       (a1n + a2n)| .

also in (2.10), some, but not all, of the ck values could be zero.
now, the jth row (originally all zero), is no longer zero, and its elements are not equal to
those of any other single row. yet, property 8 insists that the value of the determinant has not
changed by these additions (or subtractions; note that some, or all of the ck could be negative).
then, the value of |a| must still be zero. we, therefore, conclude the property:

9. if any row, or column, of a determinant is a    linear combination    of the other rows, or columns,
then that determinant is zero. by de   nition, the    linear combination    is the summation given
in (2.10).

the reader may remember this property from the algebraic solution to sets of linear equations. in
order to achieve a unique solution of n equations in n unknowns, n independent equations are needed.
if one (or more) of these equations is a    linear combination    of the others, then a unique solution
does not exist. this will be a subject in a later chapter.

2.5 the rank of a determinant
continuing the discussion of the last article, consider an (nxn) determinant whose value is zero by
virtue of property 9. if just one of its rows (or columns) is a linear combination of the others, then its
   rank    is said to be (n     1). if two rows (or columns) are linear combinations of the others, then its
rank is (n     2). and so on. on the other hand, if all n of the rows (or columns) of |a| are    linearly

2.6. minors and cofactors 33

independent    (i.e., none of the rows (columns) is a linear combination of the others), then the rank
of |a| is n (and its determinant is nonzero).
a more accurate way to say the above is: if the rows (columns) of a determinant are linear
combinations of (n     k) independent rows (columns), then the rank of the determinant is (n     k).
in summary, an (nxn) determinant (or square matrix) may have a maximum rank of n, if the
determinant does not vanish (not zero). its minimum rank would be zero, if all its elements are zero
(a trivial case).
if |a| is not zero, its rank is n. if its rank is (n     1) then there exists at least one (n   1xn   1)
determinant made up of the rows (columns) of |a| that is not zero. if its rank is (n     2), then at least
one (n   2xn   2) non-zero determinant can be found. and so on. the subject of    rank of a matrix   
will come up in a future chapter.

2.6 minors and cofactors
if one, or more, rows and columns are deleted from a determinant, the result is a determinant of
lower order, and is called a    minor    of the original. if just one row and one column are deleted, the
resulting       rst minor    is of order (n     1). clearly, within an |nxn|, there exist n2    rst minors. the
   second minor    is of order (n     2), and is the result of deleting 2 rows and 2 columns. in this same
way, minors of various orders can be de   ned. a minor is a determinant, and must, therefore, always
have the same number of rows as columns.

the elements which lie at the intersections of the deleted rows and columns also form a
determinant, which is called the    complement    of the minor. note that the complement of a    rst
minor is a single 1x1 element.
of particular interest are the    rst minors.these are of order n     1, and result from the deletion
of the ith row, and jth column. the complement is the element aij , and the minor will be denoted
mij .

2.6.1 expansions by minors   laplace expansions
the laplace expansion is de   ned as follows. select any number, say r, rows (or columns) from |a|.
then, the value of |a| is equal to the sum of products of all the rth order minors contained in these
r rows (columns) each multiplied by its corresponding algebraic complement (the complement with
the correct leading sign attached). of greatest importance are the    rst minors.

expansion by first minors
the table below has been taken from section 2.3. it lists all the indices in the expansion of a (4x4).
in this table, leading signs have been added, according to the inversions rules already discussed.

inspection of the    rst column in table 2.1 shows that, when a11 is factored, the terms repre-

sented in this column can be written:

a11(a22a33a44     a22a34a43     a23a32a44 + a23a34a42 + a24a32a42     a24a33a42)

34

2. determinants

+ 11 22 33 44
    11 22 34 43
    11 23 32 44
+ 11 23 34 42
+ 11 24 32 43
    11 24 33 42

table 2.1:

    12 21 33 44
+ 12 21 34 43
+ 12 23 31 44
    12 23 34 41
    12 24 31 43
+ 12 24 33 41

+ 13 21 32 44
    13 21 34 42
    13 22 31 44
+ 13 22 34 41
+ 13 24 31 42
    13 24 32 41

    14 21 32 43
+ 14 21 33 42
    14 22 31 43
    14 22 33 41
    14 23 31 42
+ 14 23 32 41

the terms within parentheses are the expansion of the determinant below.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) a22

a32
a42

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a23
a33
a43

a24
a34
a44

but, this is m11, the minor of a11. this column, then, can be expressed as a11m11.

using the same reasoning on the second column of table 2.1, the result is    a12m12. note
that the sign is negative, because the sign of all the terms in the second column are reversed from
those in the    rst (the    algebraic complement    then is    a12). continuing for all four columns:

|a| = a11m11     a12m12 + a13m13     a14m14 .

in the general (nxn) case, with row number 1:

(cid:12)(cid:12) = n(cid:21)

(cid:12)(cid:12)a

j=1

(   1)j   1a1j m1j .

(2.11)

(2.12)

this is proven in the following manner:

there are n a1j m1j terms in (2.12), and each of these terms contain (n     1)! product terms of
the original determinant. all of the product terms are unique, and fall within the de   nition of terms
in |a|. that is, they are all terms in |a|. since there are n(n     1)! total terms, and all from |a|, then
all n! terms in |a| are represented. note, again, that all are unique. none of the terms containing
a1k contain a1m, and vice versa. it must be concluded that (2.12) contains all the terms in |a|.

inversions and the leading signs in (2.12)
any minor, mij , expansion has the same leading term signs as the expansion of any (n   1xn   1)
determinant. that is, the deletion of the ith row and jth column does not introduce any inversions;
an obvious, but important point. then, the main diagonal term in mij will always be positive, in
|mij|. now, considering a1j m1j , we will choose a leading sign by considering the product of a1j
times this main diagonal term in |m1j|.

then, this sign is determined only by the inversions of the j subscript in the a1j factor
(remember, there are no inversions in the diagonal term in m1j ). the number of inversions is j     1.
therefore, the superscript on the (   1) factor is j     1.

2.6. minors and cofactors 35

the general laplace expansion of |a| in first minors
in general, |a| can be expanded in terms of any row or column:

                  
               

(cid:12)(cid:12)a
(cid:12)(cid:12)a

(cid:12)(cid:12) = n(cid:21)
(cid:12)(cid:12) = n(cid:21)

j=1

i=1

(   1)(i+j )aij mij ith row minors
(   1)(i+j )aij mij jth column minors

(2.13)

the generalization to any (ith) row (the above proof concerned the    rst row), follows directly, after
   rst reversing the ith with the (i     1)th, then with the (i     2)nd, and so on, until the ith row appears
in the    rst row position, followed by row1, then row2, etc. note that this is not the same as just
interchanging the    rst and ith rows. with the ith row in the    rst position, the same arguments as
above lead to the result. the row reversing operation, described above occurs i     1 times, and each
one introduces a change in sign (by property 5 of section 2.4). then, when we combine these sign
changes with those in equation (2.12), the exponent on the (   1) term becomes i     1 + j     1, or
i + j     2, or i + j.
the argument which shows that |a| can be expanded in terms of column minors as well as
row minors is simply based on the property that |a| = |a
(cid:5)|, i.e., property 1. after transposition of
|a|, all the above arguments hold.
note that the (   rst) minor of an element is the coef   cient of that element in the general
expansion   i.e., the element aij occurs in exactly (n     1)! terms in |a|, and those terms are given
by mij .

in summary, the laplace expansion provides a concise and clear picture of the expansion
of a determinant   easier to visualize than the term by term expansion de   ned in equation (2.8).
however, expansion by minors is no more, or less, than the term by term expansion.

the ideas of the present section are illustrated in the example, below:

36

2. determinants

(cid:12)(cid:12)a

(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

3
1 4
0    2 1
   5    1 1

determinant
to be expanded

expansion of |a| by minors of the    rst row

term-by-term expansion
(   6)
(   3)
( 0)
(   5)
( 0)
(40)

sign
a11a22a33 = +
a11a23a32 =    
a12a21a33 =    
a12a23a31 = +
a13a21a32 = +
a13a22a31 =    

|a| =    48

|a| = +a11m11     a12m12 + a13m13

= +3(   2 + 1)     1(0 + 5) + 4(0     10) =    48 .

(note that the 6 terms of the above equation correspond to those in the term by term expansion).
expansion of |a| by minors of the second row

|a| = 0     2(3 + 20)     1(   3 + 5) =    48 .

expansion of |a| by minors of column one

|a| = +a11m11     a21m21 + a31m31

= +3(   2 + 1)     0(1 + 4) + (   5)(1 + 8) =    48 .

expansion by minors of any row or column would yield the same result. the reader should prove
this, for practice.

other sums of products of elements times their minors can be formed. for example, consider

the main diagonal elements times their minors:(cid:21)

aii mii .

i

this summation at    rst appears to represent n! terms. but, this summation contains non-unique
terms, and is not the expansion of |a|. for example, both a11m11 and a22m22 contain the main
diagonal product term a11a22a33        the only expansions of    rst minors that result in |a| are given
by (2.13).

cofactors
the leading signs in equations (2.13) produce an alternating pattern of signs,as shown in the diagram
below (and also evident in equation (2.11)). if we associate these signs with their corresponding
   rst minors, the results are de   ned as    cofactors.   

2.6. minors and cofactors 37

+     +     + . . .
    +     +     . . .
+     +     + . . .

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

. . .

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

. . .

(cid:21)
(cid:21)

j

|a| =
|a| =

the cofactor of the ijth element will be denoted as aij :

aij = (   1)i+j mij

(2.14)

and equations (2.13) can be rewritten as:

aij aij

(row cofactors)

(cid:21)

|d| =

(2.15)
if the ith row of |a| is replaced by some new elements, dj , then the new determinant so de   ned is:

(column cofactors) .

aij aij

i

dj aij .

(2.16)
note, especially, that the ith row cofactors of|d| are the same as those of|a|, and this fact is re   ected
in (2.16). now, if the new elements dj are the elements from some other row (say, the kth), then
the expansion of |d| is that of a determinant with two identical rows; and |d| must be zero, by
property 6 of section 2.4.

j

then, the sum of products of any row (or column) elements times the cofactors of any other row

(or column) is identically zero. (cid:21)
(cid:21)

j

if i = k
if i (cid:4)= k .

(2.17)

the above is an important and informative result, as is illustrated by a continuation of the previous
(3x3) example:

the original determinant is:

(cid:12)(cid:12)akj
(cid:12)(cid:12)akj
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

aij

aif

j

(cid:12)(cid:12)a

(cid:12)(cid:12) =

(cid:12)(cid:12)
= |a|,
(cid:12)(cid:12) = 0,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).

3
1 4
0    2 1
   5    1 1

38

2. determinants

now arrange all of the signed mij minors (i.e., cofactors) into a matrix, as follows:

   
    m11    m12 m13
   m21 m22    m23
m31    m32 m33

   
    =

   
       1    5    10
   5
23    2
9    3    6

   
    = matrix of cofactors

where, for example, a22 = m22 = (3 + 20) = 23, and a31 = m31 =(1   4(   2)) = 9.
   
now, postmultiply the a matrix by the transpose of the matrix of cofactors:
   

   
      48

   
   

   
    =

   
    3
1 4
0    2 1
   5    1 1

[a]

   
       1    5
9
23    3
   5
(cid:17)
(cid:18)
   10    2    6

aadj

0
0
0    48
0
0    48
0

(cid:12)(cid:12)a

(cid:12)(cid:12) i

(2.18)

equation (2.18) is a direct illustration of equations (2.17). the transposed matrix of cofactors is
de   ned as the    adjoint    of the original a matrix. it is written as aa, or aadj. the product of the    rst
row of a times the adjoint columns gives a nonzero result only when the column contains the row
1 cofactors   i.e., the 1st column of aa. section 2.8, below continues the discussion of the adjoint
matrix, and its relation to the inverse matrix.

2.6.2 expansion by lower order minors
the laplace expansion is simply a systematic method of deriving all the terms in the term-by-term
expansion, equation (2.8). although expansion by    rst minors is probably the most important, it is
of interest to note that |a| can be expanded by other minors, as well.

starting again from the de   nition of the laplace expansion, we can select any number, say
r, rows (columns) within which to form complements. each of these complements will be rxr de-
terminants. the (n-rxn-r) minors of these complements will then be    lower order minors.    both
the complement and its minor are minors of the original determinant, a source of confusion. in
this discussion, the complements formed within the chosen r rows will be called    complementary
minors.    each of these will have a    minor    (and a signed minor, or cofactor).

within the r rows of an nxn determinant we can form n!

r!(n   r)! complements (i.e., combina-
tions of n things taken r at a time). each complement will have r! terms, while its minor will have
(n     r)! terms. then the sum of products of all complements by their minors will produce

total number of terms =

n!

r!(n     r)!    r!(n     r)! = n! .

since complement and minor are formed from different columns and rows, then each of the terms
so formed are truly from |a|. therefore, the n! totality of them are the expansion of |a|.

in determining the cofactor leading sign we look at the term which arises from the main
diagonal of the complement and multiplies the main diagonal terms of its minor. since both of these

factors are main diagonal, there are no inversions within them. however, when they are multiplied
together, the number of inversions determines the leading sign.

the method will be numerically illustrated by using the (4x4) example given in section 2.3.2,

page 29, shown again here.

2.6. minors and cofactors 39

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

|a| =

3

   2
3    4    5
4    7    6
   3

2    5
6
9
4    10

5

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

there are 4(4   1)/2 (=6) complement 2x2s that can be formed in the    rst two rows of |a|. these are
from columns: (1&2), (1&3), (1&4), (2&3), (2&4), (3&4). each of these has a 2x2 minor. their
products are summed to expand |a|:

col   s compl   t minor

leading sign = inv{a11a22a33a44} = + ; signed result =    24

a21

(cid:12)(cid:12)(cid:12)(cid:12)a11
(cid:12)(cid:12)(cid:12)(cid:12)a11
(cid:12)(cid:12)(cid:12)(cid:12)a11

a21

a21

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

a43

(cid:12)(cid:12)(cid:12)(cid:12)a33
(cid:12)(cid:12)(cid:12)(cid:12)a32
(cid:12)(cid:12)(cid:12)(cid:12)a32

a42

a42

a12
a22

a13
a23

a14
a24

(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12) =

product

3
3    4

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)   2
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)   2
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)   2    5

2
3    5

9
4    10

(cid:12)(cid:12)(cid:12)(cid:12)   6
(cid:12)(cid:12)(cid:12)(cid:12)   7
(cid:12)(cid:12)(cid:12)(cid:12)   7    6

9
5    10

result

(cid:12)(cid:12)(cid:12)(cid:12) =    24
(cid:12)(cid:12)(cid:12)(cid:12) = 100
(cid:12)(cid:12)(cid:12)(cid:12) = 6

6

5

4

3

a34
a44

a34
a44

a33
a43

leading sign = inv{a11a23a32a44} =     ; signed result =     100

leading sign = inv {a11a24a32a43} =     ; signed result = + 6

(1&2)

(1&3)

(1&4)

40

2. determinants

col   s compl   t minor

leading sign = inv{a12a23a31a44} = + ; signed result = + 91

a22

(cid:12)(cid:12)(cid:12)(cid:12)a12
(cid:12)(cid:12)(cid:12)(cid:12)a12
(cid:12)(cid:12)(cid:12)(cid:12)a13

a22

a23

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

a41

(cid:12)(cid:12)(cid:12)(cid:12)a31
(cid:12)(cid:12)(cid:12)(cid:12)a31
(cid:12)(cid:12)(cid:12)(cid:12)a31

a41

a41

a13
a23

a14
a24

a14
a24

(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12) =

product

2
   4    5

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12) 3
(cid:12)(cid:12)(cid:12)(cid:12) 3    5
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)   2    5

   4

6

   5

6

result

9
   3    10

(cid:12)(cid:12)(cid:12)(cid:12) 4
(cid:12)(cid:12)(cid:12)(cid:12) = 91
(cid:12)(cid:12)(cid:12)(cid:12) 4    6
(cid:12)(cid:12)(cid:12)(cid:12) = 4
(cid:12)(cid:12)(cid:12)(cid:12) = 13
(cid:12)(cid:12)(cid:12)(cid:12) 4    7

   3

4

   3

5

a34
a44

a33
a43

a32
a42

leading sign = inv{a12a24a31a43} =     ; signed result =     4

leading sign = inv{a13a24a31a42} = + ; signed result = + 13 .

(2&3)

(2&4)

(3&4)

adding the signed results, above, yields the value of |a| =    18, (the same as in section 2.3.2).
if |a| were (7x7), and its    rst 3 rows are chosen in which to form the complements (which
will be 3x3   s), the number of complements will be equal to 35, the number of combinations of 7
things taken 3 at a time:

number of complements =

n!
r!(n     r)! = 7!

3!4! = 7     6     5
3     2

= 35 .

each of the 3x3 complements will have a 4x4 cofactor, formed within the lower 4 rows, and using
the 4 columns that are not used in the complement. each complement expands to 3! = 6 terms, and
its cofactor has 4! = 24 terms. then the total number of terms will be 35   6   24 = 7!. this is the
correct number of terms needed in the expansion of a 7x7, and note that every term is taken from
elements of separate rows and columns, as required.

in determining the cofactor leading sign we look at the term which arises from the main
diagonal of the 3x3 and multiplies the main diagonal terms of its minor. for example, one of the
complements will be formed using the    rst 3 rows and columns 1, 4, and 6. its main diagonal term
is

a11a24a36 and the cofactor main diagonal is a42a53a65a77 .

of course, there are no inversions within these. however, when these terms are multiplied:

a11a24a36a42a53a65a77     inversions in 1462357 = 5 (odd) .

therefore, the leading sign of the product of this complement times its minor must be negative.

(cid:22)

(cid:23)

c =

0 x   i b

2.6.3 the determinant of a matrix product
the laplace expansion methods are not convenient for use in expanding determinants, but they
give valuable insight into the problem. for example, consider:

2.7. geometry: lines, areas, and volumes 41

where c = (2nx2n) and the partitioned matrices are (nxn) .

to    nd |c|, expansion by complements in the    rst n rows is the obvious choice   there will only be
one such complement since all others will have a zero column. the complement will be |x|, but it
might appear that the negative sign on i may alter the sign of the result depending upon n-odd or
even. but, look at the inversions in the column indices. they will be:

[(n + 1)(n + 2)       (2n)][(1    2    3  , ,  n)] example: if n = 2: column indices are 3412 .

clearly, there will always be n2 inversions. how nice. whenever n is odd, the leading sign is negative,
just    canceling    the negative value of |   i|. thus, |c| = |x| for any n.

this result is prominent in the proof that the determinant of a matrix product is the product

of their determinants. consider the matrix equation

(cid:22)

(cid:23)(cid:22)

c =

i a
i
0

a 0
   i b

0 ab
   i
b

(cid:22)

(cid:23)

=

(cid:23)

.

(2.19)

since the matrices on each side of the equality are identical, they must have the same determinant.
so, we may    take determinants of both sides.    in so doing, note that the    rst matrix on the left is a
   fundamental operations    matrix which causes sums and/or differences of rows to be combined with
the original rows in the second matrix. these operations do not affect the value of the determinant
(property 8). so:

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12) =
0 ab
   i
(2.20)
b
the determinant on the left, expanded by minors of the    rst n rows, is clearly equal to |a||b|. the
determinant on the right has just been shown to be |ab|. then:
(cid:12)(cid:12)
(cid:12)(cid:12)|b| =(cid:12)(cid:12)ab

(cid:12)(cid:12)(cid:12)(cid:12) a 0
   i b
|c| =(cid:12)(cid:12)a

|c| =

(cid:12)(cid:12)(cid:12)(cid:12) .

(2.21)

.

the extension of this to multiple matrices in the product is obvious.

2.7 geometry: lines, areas, and volumes
the    two-point form    of the equation of a line can be written as the following determinant

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

x
x1
x2

y
y1
y2

1
1
1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0     y = y2     y1

x2     x1

x + y1x2     y2x1
x2     x1

.

(2.22)

42

2. determinants

note that the equation is satis   ed at both x1, y1 and x2, y2, from determinant property 6.

the equation of a parabola passing through points (x1, y1), (x2, y2), (x3, y3) is given by

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

y
y1
y2
y3

x
x1
x2
x3

x2
x2
1
x2
2
x2
3

1
1
1
1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0     y = ax2 + bx + c

(2.23)

where the coef   cients a, b, and c are the ratios of the minors of the determinant. the equation is
often used in parabolic interpolation, wherein given data is locally    tted to a parabola. in that case,
(x1, x2, and x3) are taken to be (-1, 0, and +1), and the resulting equation becomes

y = y2 + 1

2 (y3     y1)x + (y1     2y2 + y3)x2; for y in the local interval .

(2.24)

the area of a triangle ( ), one of whose vertices at the origin is given by

2 (x1y2     x2y1) = 1

1

2

for example, area  oab, in the diagram.

(cid:12)(cid:12)(cid:12)(cid:12) x1

x2

(cid:12)(cid:12)(cid:12)(cid:12) .

y1
y2

(2.25)

(2.26)

(cid:12)(cid:12)(cid:12)(cid:12) .

y1
y3

to    nd the area of  abc,whose vertices are not at the origin, use (2.25)

 abc =  oab +  obc      oac
 abc = 1

(cid:12)(cid:12)(cid:12)(cid:12) x1

x2

2

y1
y2

y2
y3

(cid:12)(cid:12)(cid:12)(cid:12) x1

x3

but, this is just the expansion of a 3x3 determinant:

2

x3

(cid:12)(cid:12)(cid:12)(cid:12) x2
(cid:12)(cid:12)(cid:12)(cid:12) + 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x1

2

x2
x3

2

(cid:12)(cid:12)(cid:12)(cid:12)     1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

 abc = 1

y1
y2
y3

1
1
1

the determinant value interpreted as a volume

2.7. geometry: lines, areas, and volumes 43

a point of greater interest and importance is made by considering the equation of a plane
de   ned by three points, p1, p2,and p3,in space.equation 2.27,below,   rst shows the general equation
of a plane, and, second, the expansion of the determinant f (x, y, z) by its    rst row complements:

x
x1
x2
x3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) y +

1
1
1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x1 y1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) +

z 1
1
z1
1
z2
1
z3
ax + by + cz + d = 0
1
1
1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0

x2 y2
x3 y3

z1
z2
z3

y
y1
y2
y3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x1 y1

x2 y2
x3 y3

f (x, y, z) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)y1

y2
y3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x +

1
1
1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)z1 x1

z2 x2
z3 x3

z1
z2
z3

            
         

(2.27)

comparison of the two equations shows that f is the equation of a plane de   ned by the three points,
pj . note the following diagram of a (three dimensional) tetrahedron with the triange p1p2p3, as
its base (shown shaded):

(cid:3)

(cid:3)

(cid:2)

(cid:4)(cid:1)

(cid:5)

(cid:1)

(cid:4)(cid:2)
(cid:2)

(cid:4)(cid:3)

the coef   cients of the variable in (2.27) are triangular areas, as shown in the previous paragraphs (see
equation 2.26). these triangles are the projections of triangle p1p2p3 onto the coordinate planes.
let   represent the are of triangle p1p2p3. then  yz =   cos   ,  xz =   cos   ,  xy =   cos   ,
where the angles   ,   , and    are the direction cosines of the normal from o, perpendicular to the
plane (e.g.,    is the angle between the x-axis the normal)1.
a2 + b2 + c2 converts it to the    normal form    of
the equation of a plane in which the coef   cients of the variables become the direction cosines of the
1this    cosine effect    will be seen again in chapter 5, in the section    solar angles,    on page 116.

division of the    rst equation (2.27) by

   

44

2. determinants

normal, and the constant term the distance from o to the plane   the length, p, of the normal. that
is:

p =

   d

(cid:26)
(2.28)
a2 + b2 + c2
  area of the base(triangle p1p2p3)    the length of the

.

the volume of this tetrahedron is given as 1
3
normal to the plane.
noting that a, b, and c are related to the areas of the projected triangle (e.g., a = 2 yz), then

a2 + b2 + c2 = 2
(cid:26)
cos2    + cos2    + cos2    = 1, the direction cosines are the coordinates of a    unit vector.   

= 2 

+  2

+  2

 2
yz

xy

zx

(cid:27)
cos2    + cos2    + cos2    = 2  .

(cid:27)

(cid:26)

the term
therefore,

volume = 1
3

p      = 1
3

   

d

a2 + b2 + c2

   

  

a2 + b2 + c2

2

= 1
6

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x1

x2
x3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

y1
y2
y3

z1
z2
z3

d is the value of the determinant de   ned by the vectors opi, the constant term in f (x, y, z).

this important result shows that the value of a determinant can be equated to a    volume.    in
more than 3 dimensions, the volume cannot be visualized   but, just envision the 3 dimensional case
and let the mathematics take over for larger dimensionality.

a determinant (its expanded value) can become very small just because its vectors (rows or
columns) are themselves small, or a subset of them is small. this will be easy to see, and can be
changed by re-scaling, making its rows balanced in size numerically.

after rescaling, the value of the determinant becomes a measure of its    skew      the orientation
of vectors within the set. for example, in the present case, if the point p1 were to move toward the
line p2p3, the volume of the tetrahedron would decrease. at the limit, if p1 reaches this line then
op 1 falls into the plane de   ned by the other two vectors   the volume, and hence the determinant
value, will be zero. at the other extreme, these vectors could be mutually orthogonal, minimum
   skew.   

2.8 the adjoint and inverse matrices
the adjoint matrix   de   ned in section 2.6, page 38 as the transpose of the matrix of cofactors   is
denoted as aa or aadj. equation (2.18) leads directly to the statement of (2.29):

(cid:12)(cid:12) i, for any square matrix.

aaa =(cid:12)(cid:12)a
(cid:12)(cid:12) = i, when |a| (cid:4)= 0 .
[a] aa(cid:12)(cid:12)a

(2.29)

(2.30)

2.8. the adjoint and inverse matrices 45

in chapter 1, the    inverse matrix    (of a) was de   ned as a matrix which, when pre- or postmultiplied
by a, produces the unit matrix, i. (2.30) shows just such a case. the adjoint matrix with each of
its elements divided by |a|, as shown in (2.30) is clearly the inverse of a. the adjoint and inverse
matrices are de   ned only for square matrices. if |a| = 0 the inverse of a, written a
   1, is not
de   ned   the matrix is    singular.   

the inverse matrix, de   ned in (2.30), also commutes with a. that is:

a[aa] = [aa]a = |a|i

(aa     aadj) .

(2.31)
the column cofactors of |a| are in the rows of aadj (aadj is the transpose of the matrix of cofactors).
so, the product [aadj]a forms the products of these column cofactors by the columns of a. the
arguments already given show that this result is |a|i.

a

   1 = [aadj]/|a|
a
   1a = aa
   1 = i
   1, the result is b = a

(2.32)
(2.33)

equations (2.32) and (2.33) de   ne a unique inverse. suppose, to the contrary, that a matrix, b, exists
   1. by starting with ab =
such that ba = i. by simply postmultiplying by a
i, it is similarly shown that a

   1 is unique.

2.8.1 rank of the adjoint matrix
in section 2.5 the    rank    of a determinant was discussed. the rank of a square matrix is the same
as that of its determinant. if a matrix is non-singular then its rank is the same as its order (i.e., an
nxn matrix is of order    n,    and its rank is    n   ). in this case, the rank of its adjoint matrix is also n.
conversely, the rank of a singular matrix is necessarily less than n. if that rank is n     1, then, from
section 2.5, at least one determinant of order n     1 can be found that is nonzero.
the adjoint matrix is made up of these n     1 determinant values.therefore, the adjoint matrix
cannot be null, yet its product with the original a matrix has to be null, from equation (2.29), above.
if the rank of a is less than n     1, then every n     1 minor of a is null, and the adjoint therefore is
null (its rank is zero).
the interesting case is when a has rank n     1. in this case, the rank of aadj is unity. all
of its rows (columns) are linear combinations of a single row (column). this important result will
be discussed in some detail in the chapter on solutions to linear simultaneous equations. for now,
consider an example 4x4:

   
          11

a =

12

27 17
1    1    3
0
13
5
7
42 27
26

8
37

   
         ; aa =

   
         

620

   186
84    280    420
   90
204    680    1020

930    124
56
450    60
136

300

a is singular, |a| = 0, with rank 3. its adjoint has the rank of one. all columns of aa are a multiple
of {     31, 14,   15, 34}. also, note that any column of the adjoint and, in fact, any multiple of
{     31, 14,   15, 34} is a solution to ax = 0.

   
          .

46

2. determinants

2.9 determinant evaluation
the foregoing lays out the characteristics and properties of determinants, but implies very labori-
ous work in actually calculating their values. fortunately, this is not the case. modern methods of
expansion are straightforward, and easy to program. they do involve a lot of calculation but far less
than the direct methods already discussed.

practical evaluation of determinants involves some method of condensation (i.e., reduction
to a lower order of determinant). repeated applications of the method eventually lead to the scalar
result,|a|.these methods are equivalent to the    elimination of xj ,    as discussed in the very beginning
of this chapter.the array concept of the determinant lends itself to the de   nition of arrays in popular
programming languages, and the    repeated applications    mentioned above lead to program looping.

2.9.1 pivotal condensation
   pivotal condensation    is a name more dif   cult than the method. the idea is easily described, easily
understood, easily done   and fun to program. the description here will be via example using a 4x4:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =   d

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a11
a21
a31
a41

a12
a22
a32
a42

a13
a23
a33
a43

a14
a24
a34
a44

b11
0
b31
b41

b12
0
b32
b42

b13
1
b33
b43

b14
0
b34
b44

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =    d

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) b11

b31
b41

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

b12
b32
b42

b14
b34
b44

(2.34)

the determinant |a| is manipulated to produce an equivalent d|b|. the determinant |b| in (2.34)
can be expanded by the elements of its second row. the result will clearly be a determinant of
3rd order as shown. a minus sign is chosen, in this case, because of the factor (   1)(2+3) resulting
from having chosen the 2,3 element as the    pivot,    the resulting    condensed    determinant is then
operated upon in the same manner to produce further condensations until the product string of d
factors multiply to the    nal result.

in this example, the second row is arbitrarily chosen for the    rst    pivot row.    rather than
make an arbitrary choice, the largest element (absolute value) is chosen as the pivotal element. the
pivot does not have to be the largest, but that   s a good choice   and avoids the possible choice of a
pivot equal to zero. if the matrix is complex, just choose the element, x + jy, with the largest sum
of absolute values of real and imaginary parts (|x| + |y|).
at the beginning step in each cycle, let the pivot (largest) element be apq. if the pth row is
divided by this value, the new pqth element will have the value 1.0. in the equivalent d|b|, the factor
d is set equal to apq, (determinant property 4, in section 2.4). now, from each column the proper
multiple of column q is subtracted such that all elements of row p become zero. these operations
do not change the value of the determinant (by property 8). in the column subtractions it is not
necessary to actually calculate the values in row p, since it is already known that they will be zeros.
also, column q remains the same   is skipped from calculations. the new, condensed determinant
does not take any of its elements from the pth row, or qth column of |b|

condensing the determinant uses equation (2.13), expansion by    rst minors. the minus sign

is taken if the cofactor has the opposite sign from the minor, as in (2.14), i.e., (   1)(p+q).

2.9. determinant evaluation 47

2.9.2 gaussian reduction
the foregoing paragraphs indicate that determinant evaluation amounts to the repeated application
of a simple algorithm. gaussian reduction is one of these simple algorithms, and it is not very
different than pivotal condensation. as it progresses, the determinant is condensed to a smaller and
smaller array until the determinant value becomes the product of n factors (and these factors are the
   pivots,    just as before).

the objective of gaussian reduction is to reduce the given determinant to an equivalent

triangular one like the following:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a11

a12
0 a22
0
0

a13
a23
0 a33
0

a14
a24
a34
0 a44

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

4x4 upper triangular determinant

its value is easily seen to be equal to the product of its main diagonal elements. all other terms
contain the zero as a factor. the pivots, then, are these aii elements.

as before, at each stage, the pivots chosen are the largest elements in the condensed deter-
minant. in general, of course, these are not found on the main diagonal. they must be moved there
by row and column exchanges. if both a row and a column exchange occur, the determinant value is
not changed. as an example: at the    rst stage the largest element is found to be a34. to bring this
element to the a11 position, row three is exchanged with row one and column four is exchanged
with column one. since two sign changes are made, they cancel. if only one (either column or row)
exchange occurs, the value of the determinant changes sign. thus, it is necessary to keep track of
these exchanges.

after the exchange(s) occur the method is very like pivotal condensation, except it is not
desired to divide the pivotal row by the pivot (to reduce the pivot position to unity). but, the rest of
the pivotal column is reduced to zero, just as the pivotal rows were reduced in the previous method.

(cid:12)(cid:12)a

(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a11
0
0
0

a12
c22
ck2
. . .

      
      
ckk

a1n
      

id98

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

cij = aij     a1j

ij = 2, 3       n

ai1
a11

the above display shows the    rst stage, after the largest element has been moved to the a11 position.
if we make the calculations indicated, the elements in the    rst column become zero:

at j = 1;

cij = aij     ai1

a1j = ai1     ai1

a11 = 0

a11

a11

48

2. determinants

and the condensed determinant is |cij|. in (row) vector terms, the elements cij are formed by
subtracting from each ith row the proper multiple of the pivot row (row 1, in stage 1):

[ci] = [ai]     ai1

a11

[a1] .

(2.35)

in (2.35) the boldface type identi   es vectors and the square brackets indicate [row] vectors, not
{column} vectors.
the method is:

(1) set p = 1; p is de   ned as the pivot row index. as such, it will take values 1..n (n is the order

of the determinant). the pivots, then will have the subscripts pp.

(2) find the element with the largest absolute value (if the determinant is complex, the absolute

value could be used, or just the largest sum of abs values of real and imaginary parts).
exchange rows and columns to move the largest element to the a
used here to indicate that the values of these elements change as the procedure continues.

(cid:5)
pp position. the    prime    is

(cid:5)
a
(cid:5)

a

ip

pp

(3) now, for all i rows below the pth, subtract

times the pth row, as in (2.35). note that it is

unnecessary to operate on elements in, or to the left of, the pth column. they will all be zero.

(4) now, increment p. if this new value is less than n, then repeat steps 2 and 3. if p = n, the
procedure is complete. the determinant can now be evaluated as the product of the diagonal
elements.

the method can become confusing with the exchange of rows and columns. otherwise, it is quite
straightforward. the labor is alleviated by the use of a computer; and the programming is enjoyable,
tricky only in keeping track of row/column exchanges.

in this regard, it is unnecessary to actually exchange data rows/columns. lists can be kept,

indicating where they are. for example, evaluating a 4x4, the row list:

row list = rlist(i) = 1,2,3,4. if rows 2 and 4 are exchanged, rlist(i) = 1,4,3,2 .

the same thing can be said for the column list.
lists. that is, an element a(i, j ), in |a|, now must be referred to as a(rlist(i), clist(j)).

of course, this leads to the complication that the elements must be accessed through these

the gaussian reduction method can be done using    partial pivoting,    in which the pivot
elements are always chosen from the pivot column. this reduces the exchanges down to just row
interchanges. within the subject at hand, full pivoting is just as easy. the advantage (reduced com-
plexity) of partial pivoting is noticeable in solving simultaneous equations, and/or calculating the
inverse of a matrix. there will be more about this in the following chapter.

the reduction of a 5x5 will provide an example. at each stage, the new pivot is shown within
a box. in each case the pivot is the largest element within the condensed matrix. at stage 1, the

2.9. determinant evaluation 49

2,3 element is the largest in the entire 5x5; at stage 2, 3.4 is the largest within the 4x4 (row and
column 1 excluded). one by one, these pivots are brought to the main diagonal, and the elements

below them are zeroed by subtraction of

times the pivot row, as discussed above.

these elements are not actually calculated, just crossed out   indicating zeroes (in fact, it is

pp

easier to follow the method with these crossed out numbers than it would be with zeroes).

(cid:5)
a
(cid:5)

a

ip

stage 1
input
det.

stage 2

stage 3

stage 4

stage 5

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1.00000
   2.00000
   1.00000
0.00000
1.00000

0.00000    3.00000
2.00000
1.00000
5.00000    2.00000    2.00000
1.00000
1.00000    3.00000
1.00000
3.00000
0.00000    1.00000    1.00000
3.00000
1.00000    4.00000
3.00000
5.00000
1.00000    2.00000    2.00000    2.00000
5.00000
   3.00000
0.60000    0.20000    0.20000
0.80000
2.20000    1.80000
3.00000
0.40000
0.20000
   1.00000
0.20000    0.40000    1.40000
2.60000
   4.00000
1.80000    0.60000
3.40000
1.40000
5.00000    2.00000    2.00000    2.00000
3.40000    0.60000
   4.00000
1.40000
3.00000    1.80000    0.11765
2.94118
   1.00000
2.60000
   3.00000
0.80000    0.05882    0.52941
5.00000    2.00000    2.00000    2.00000
1.00000
   4.00000
1.40000    0.60000
3.40000
1.80000
3.00000    1.80000
2.94118    0.11765
1.35294
2.60000    2.47059    0.04000    0.04000
   1.00000
   3.00000
0.80000    0.52941    0.08000
0.42000
1.00000    2.00000
5.00000    2.00000    2.00000
1.80000    0.60000
   4.00000
1.40000
3.40000
3.00000    1.80000
1.35294    0.11765
2.94118
0.80000    2.47059
0.42000    0.08000
   3.00000
2.60000    0.52941    0.04000    0.04762
   1.00000

1.00000
1.80000
1.35294
0.05882    2.47059    1.17647
0.17647

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

at stage 3, the new pivot needs only a column exchange to arrive at the main diagonal. then, the
determinant value must be given a leading negative sign (at all other stages, both a row and a column
exchange are required): |a| =    {5.0    3.4    2.94118    0.42    (   0.04762)} = 1.0.

50

2. determinants

gaussian reduction is easy to program, and it is an ef   cient method. in the next chapter it
will be seen again in developing the inverse matrix, and in the solution to linear equation sets. if the
method is to be used in hand calculations, it is easier and less confusing to use    partial pivoting,   
where the pivots are chosen from successive columns, 1, then 2, and so on. in this way, column
exchanges are not necessary. for small determinants, where roundoff will not be a problem, pivoting
can be avoided altogether (but, zero pivots must be avoided).

pivotal condensation is also ef   cient, and is especially easy to use in hand calculations. it lacks
the    extension    to be used in the solution to equation sets. since it is necessary to keep track of
deleted rows and columns, the program is handy to use in calculating minors   selected rows and
columns are marked as deleted at the outset.

2.9.3 rank of the determinant less than n
when the rank of |a| is n     1 (then |a| = 0), the procedure (algorithm) described above calculates
a 0 in the n, n position. for example, if the last (   fth) row of the preceding matrix is replaced with
the sum of the    rst two rows, stage 5    nds a zero in the 5,5 position. the determinant is zero:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

5.00000    2.00000    2.00000 1.00000    2.00000
   1.00000
2.60000    1.40000 0.20000    0.40000
   3.00000    1.80000
1.23077 0.53846    0.07692
0.23077 0.43750    0.06250
   3.00000
0.80000
0.23077 0.43750
2.00000
0.80000
0.00000

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

rank = 4,
stage 5

there is obviously a non-zero 4th order determinant.
if the rank of the original n    n matrix is n     q then the algorithm will result in a q-by-q array
of zero elements at the lower right. the non-zero determinant at upper left will be n     q    n     q.

2.10 examples
2.10.1 cramer   s rule
at the beginning of this chapter, cramer   s rule was invoked in the discussion of the solution to
three equations in three unknowns. in the light of the later discussion of the    adjoint    matrix in
section 2.8, we can revisit this rule. given the n-dimensional ax = c, premultiply both sides by aadj.

aaax =(cid:12)(cid:12)a

(cid:12)(cid:12) x = aac =

   
          a11 a21

a12
      
      
a1n a2n

       an1
      
      
      
       ann

   
         {c} .

(note that aadj = aa). in the equation, the elements of aadj are the transposed, signed,    rst minors
of |a|. the product of a times its adjoint (from section 2.8) is |a|i. the result on the left, then, is
|a| multiplying each x element.

looking at x1 for example, |a|x1 = a11c1 + a21c2 +        + an1cn. but, the expression on

the right is just the laplace expansion of |a| with its    rst column replaced by the {c} vector.
then, each xi is obtained as the ratio of two determinants. the determinant in the numerator
is |a| with its ith column replaced by the {c} vector, and the denominator is |a| itself. this is
cramer   s rule.

2.10. examples 51

2.10.2 an example complex determinant
in chapter 1 the sum of two matrices is given as the sum of the individual elements. then

c = [aik + j bik] = a + jb

and we can think of the matrix as a single one with complex elements or as two separate matrices.

(note that the notation j =       1    interferes with    the notation of referring to columns with the
subscript    j   ).
the objective in this example is to determine |c| = | aik + j bik |. if the routine available
handles complex numbers, then |c| is evaluated without further complication. but, it is possible to
evaluate |c| using only real arithmetic. this will be illustrated in the simplest case   a 2x2.

we will use    vector notation    |c| = |c1c2|.

(cid:24)

ck =

a1k + j b1k
a2k + j b2k

(cid:25)

(cid:24)

=

(cid:25)

(cid:24)

(cid:25)

a1k
a2k

+ j

b1k
b2k

= ak + jbk .

now, using determinant property 7:

(cid:12)(cid:12)
(cid:12)(cid:12) b1c2
(cid:12)(cid:12) a1b2
(cid:12)(cid:12)
| c1c2 | = | a1c2 | + j
(cid:12)(cid:12) b1c2
(cid:12)(cid:12)    (cid:12)(cid:12) b1b2
(cid:12)(cid:12) b1a2
(cid:12)(cid:12) = j
| a1c2 | = | a1a2 | + j
(cid:19)(cid:12)(cid:12) a1b2
(cid:12)(cid:12) + j
(cid:12)(cid:12) +(cid:12)(cid:12) b1a2
(cid:12)(cid:12)(cid:20)
and therefore | c| = | a1a2 |    (cid:12)(cid:12) b1b2

j

.

(cid:12)(cid:12)

the same method can be used in expanding any complex nxn determinant. the result will

be 2n determinants to expand, but, at least they will be real.

2.10.3 the    characteristic determinant   
associated with a matrix a(nxn) is a special determinant with a single variable, usually denoted   .
the matrix a(  ) = a       i is simply formed by subtracting    from its main diagonal elements. the
determinant of a(  ) is an nth order polynomial in the parameter. again using a 2x2:

(cid:12)(cid:12) a(  )

(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12) a11       

a21

(cid:12)(cid:12)(cid:12)(cid:12) .

a12

a22       

52

2. determinants

in this particular case, the use of property 7 is    the hard way,    but for higher order determinants it
is easier, and can be programmed.

(cid:12)(cid:12)(cid:12)(cid:12) a11       

(cid:12)(cid:12)(cid:12)(cid:12)    a12
the    characteristic polynomial    is, then: p(  ) =   2     (a11 + a22)   +(cid:12)(cid:12)a
(cid:12)(cid:12).

(cid:12)(cid:12)(cid:12)(cid:12) a11

(cid:12)(cid:12)(cid:12)(cid:12) a11

0
a21   

a22       

(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)    

(cid:12)(cid:12)(cid:12)(cid:12)    

0 a22

a12
a22

a12

a21

a21

(cid:12)(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)(cid:12)    0

0   

(cid:12)(cid:12)(cid:12)(cid:12) .

2.11 exercises
2.1.

find the inversions in the digit sequences below:

5741326

35421

123465

654321.

2.2. determine which of the terms, below, are terms in the expansion of a determinant. for

those that are legal, determine the leading sign.

a34a33a14a21

a41a32a21a14a55

b13b24b33b42

b44b12b31b23

c43c22c14c51c35 .

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2.3. expand the following determinants

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) b =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

a =

1
2
0    4
1
2    2
   1

1 2 3 4
0 1 2 3
0 2 0 1
0 3 0 5
2.4. expand |b|, above, using 2x2 complements from rows 3 and 4.
2.5. expand |b| above by completing its transformation to triangular form.
2.6. expand |a| above using pivotal condensation.
   
(cid:23)
(cid:22)
          and b =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)a11 + j b11

   
          1
find |a b| and |b a|.

2.8. expand the determinant c =

2.7. given a and b: a =

2    3 0
   1

a21 + j b21
a31 + j b31
use the method given in section 2.10.2.

a12 + j b12
a22 + j b22
a32 + j b32

a13 + j b13
a23 + j b23
a33 + j b33

3
0    1
2
2
1
4

1
1 2    2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.

2.9. given a(12x12). how many terms are in the term-by term expansion of |a|? how many
factors are in each term? how long would it take your pc to calculate |a|, term-by-term?
2.10. a (5x5) determinant is to be expanded by complements from its    rst 3 rows. one such

2.11. exercises 53

complement is

2.11. determine the rank of the matrix a1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) a11

a21
a31

a1 =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) . what leading sign should be placed on this term?
   
   
          .
         

   
          5    3

11    5
   2
1
   1    3    1    2
1
3

0    4

a2 =

3

5

a13
a23
a33

a14
a24
a34

   
          1 2

4
3
6
8
2 4
3 6
9 12
4 8 12 16

2.12. using gaussian reduction methods, reduce a2 to triangle form, and determine its rank.

use    partial pivoting    (i.e., select pivots such that column interchanges are not required).

2.13. given a 3x3 determinant made up of differentiable functions yij (x),show that the derivative

of the determinant is given by:

d

dx

(cid:5)

(cid:12)(cid:12) y1y2y3
(cid:12)(cid:12) =(cid:12)(cid:12)y
      
      
    y1j
   ; and
yj =

1y2y3

(cid:5)

(cid:12)(cid:12) +(cid:12)(cid:12) y1y
= y

dyj
dx

y2j
y3j

(cid:5)

(cid:12)(cid:12) +(cid:12)(cid:12) y1y2y
(cid:12)(cid:12);
         
         
       y
       .

y

3

(cid:5)
(cid:5)
1j
(cid:5)
2j
3j

y

2y3
=

(cid:5)

j

c h a p t e r 3

matrix inversion

55

introduction

3.1
this chapter will discuss matrix inversion, and the very closely related subject of the solution of
simultaneous equation sets. the inversion matrix arrays will necessarily be square, (nxn), for which
the inversion process is de   ned     and for which the determinant is de   ned.

emphasis is placed on the mechanical methods used in the inversion process. the next chap-
ter will consider simultaneous equation sets as    vector transformations,    and is oriented toward a
geometric interpretation, and considerations of compatibility.
in chapter 2, section 2.8, it was shown that a square matrix, whose determinant,|a|, is other

than zero, possesses an    inverse matrix,    a

   1, such that:
   1a = aa
a

   1 = i

(3.1)

where i is de   ned as the (nxn) unit matrix. the elements of the inverse matrix are the    cofactors   
of a divided by |a|; the cofactors being arranged into the    adjoint matrix.   

   1 = [aadj]/|a|

a

(|a| (cid:4)= 0) .

(3.2)

the adjoint matrix is the transpose of the matrix of cofactors; its columns contain the row cofactors
of a. the cofactor of aij is the signed    rst minor of aij , the leading sign being determined negative
if i + j is odd, positive if it is even. then, the inverse matrix is composed entirely of determinants;
the minor is the (n-1xn-1) determinant formed by deleting the row and column of the aij term.
therefore,the inverse could be determined by this de   nition.but,these calculations are quite lengthy.
instead, pivotal reduction methods will be discussed     including the gauss reduction which was
discussed in the previous chapter. this simple method will be shown to be an amazingly effective
tool for inverting matrices and solving simultaneous linear equations.

as a preliminary step, the    elementary transformation matrices    (chapter 1, section 1.4) will

be revisited, to provide further insight, and some justi   cation for later methods.

3.2 elementary operations in matrix form
three elementary operations were used in the previous chapter, in diagonalizing a determinant.they
are: (1) to any row (column) is added a multiple of another row (column). (2) a row (column) is
divided by some factor. (3) two rows (columns) are interchanged (this occurs when a pivot element
is brought to the main diagonal). these operations can be put into matrix form.

56

3. matrix inversion

operation 1. q ij (k). starting with the (nxn) unit matrix, replace the ijth element (i (cid:4)= j ) with a
factor kij . now if a matrix, a, is premultiplied by this    transform    matrix:

the matrix b is the same as a, except that to its ith row is added k times its jth row. note the 3x3

example q 23(k):   

i (cid:4)= j .

qij (k)a = b;
   
   

   
    =

1
2
2 -1
2

0
1 -1

   
   

   
    3

    1 0 0

0 1 k
0 0 1

3
0 + k
1

2
1
2     k    1 + 2k
   1
2

   
    .

(3.3)

(3.4)

the reader should try other examples     with the factor k in all the nondiagonal locations of q ij .
note that in every case (wherever the k factor is     as long as it is not on the main diagonal),
the determinant, |q ij|, is 1. furthermore, from the previous chapter on determinants, the value of
|a| is unchanged by this fundamental operation, i.e., |b| = |q ij||a| = |a|.
   
   
    .
    3
2 + k
1
2    1 + 2k
0
1    1
2     k

now, in the case b = aq ij (k), (postmultiplication of a by the same type of transformation):

   
    1 0 0

   
    =

   
    3

1
2
2 -1
2

0 1 k
0 0 1

0
1 -1

   
   

(3.5)

in this case, to the jth column is added k times the ith column     where i and j are the row, column
positions of k. note the difference, compared to premultiplication.

again note that |q ij| = 1, and that |a| = |b|.

q2(k)

   
   

0
1 -1

3
0    k
1

0 k 0
0 0 1

1
2
2 -1
2

   
    1 0 0

and, in postmultiplication the jth column is multiplied:

   
   
    3
   
   
   
   
    1 0 0

operation 2. q j (k): beginning with the unit matrix, replace the jth main diagonal element with a
factor, k. it should be obvious that premultiplying a with this q j (k) will multiply elements of the
jth row of a by k:

   
   
   
    =
   
   
    =
    3
2    k
1
2    1    k
0
1    1
2    k
in this case, |q j (k)| = k, and |b| = |q j (k)||a| = k|a|.
operation 3. q i   j . interchange row (or column) i with row (or column) j of the unit matrix. now,
   
premultiply a by this q matrix:
    1    1

   
    3
1
2
2    1
0
1    1
2

2
1
2    k    1    k
   1
2

   
    0 0 1

0 1 0
0 0 k

   
    3

   
    .

   
   

(3.7)

(3.6)

   
    =

   
    .

(3.8)

0 1 0
1 0 0

1
2
2 -1
2

0
1 -1

2
2    1
2
1

0
3

in this case, with q i   j formed by interchanging rows one and three of the unit matrix, the result
of b = q i   j a is that the same rows of a are interchanged. in postmultiplication:

3.2. elementary operations in matrix form 57

   
    3
1
2
2    1
0
1    1
2

   
   

   
    1 0 0

0 0 1
0 1 0

   
    =

   
    3
1
2
0    1
2
2    1
1

   
    .

(3.9)

not surprisingly, in this case, with rows (columns) 2 and 3 of i interchanged, these same columns
of a are interchanged.
the determinant |q i   j| =    1, and |b| =    |a|. this is analogous to the property that inter-

changing two rows (columns) of a matrix changes the sign of the determinant.

3.2.1 diagonalization using elementary matrices
the diagonalization or triangularization of a matrix, a, can be accomplished by a series of these
elementary operations. these, in turn, can be visualized as pre-, and/or postmultiplication of a by
the elementary transform matrices. note: in the equations below, the symbol q is used without
indication of its type. this is done so that the    nal transformation is more clearly shown as the
product of the individual operations.

(3.10)
where q = q mq m   1 .. q 2q 1 (a series of m elementary operations, each of which is of a type
discussed above) and b is, optionally, diagonal, or triangular. then
   1, and therefore
   1q .

   1 = b
   1q
   1 = b
a

(3.11)
(3.12)

a

b = qa

the b matrix, whether diagonal or triangular, is easy to invert. the q matrix is developed during
the procedure     and note that its inverse is not required. then, the method is a good learning tool,
it provides the basis for the very practical inversion tools, and is not an unreasonable one to use for
small matrices, by hand.

as an example of the method, the (3x3) used above, as the a matrix, will be transformed, by

means of a premultiplier q matrix, to diagonal form. the q matrices are:

q 31(   1/3); unit matrix with element (3,1) replaced with    1/3; changes the 3rd row of a to

q 32(2/3); unit matrix with element (3,2) replaced with 2/3; changes the 3rd row of a to

q 12(   1/2); unit matrix with element (1,2) replaced with    1/2; changes the 1st row of a to

(cid:17)

(cid:17)

(cid:18)

(cid:18)

0 4/3    4/3
(cid:18)
(cid:17)

0 0 2/3

3 0 5/2

58

3. matrix inversion

q 13(   15/4); unit matrix with element (1,3) replaced with    3 3/4; changes the 1st row of a to

q 23(3/2); unit matrix with element (2,3) replaced with 1 1/2; changes the 2nd row of a to

(cid:17)

(cid:17)

3 0 0

0 2 0

(cid:18)

(cid:18)

note that these changes    drive the off-diagonal elements of a to zero.    now, to    nd the accumulated
q matrix, the above must be multiplied in the order

q = q23(3/2)q13(   15/4)q12(   1/2)q32(2/3)q31(   1/3) .

note that each of the q matrices are of type 1, (q ij ) whose determinant = 1. none of the unit
matrix elements replaced is on the main diagonal.

q =

   
    .

(cid:28)

(cid:28)
(cid:28)
   
(cid:28)
(cid:28)
    9
4    3    15
(cid:28)
   1
2
3
   1
3 2
   
    3 0

2
3

4
2
1

the reader may want to verify that the determinant |q | = 1.
   
    .
(cid:28)
0
0
3

b=qa =

0 2
0 0 2

(3.13)

(3.14)

now, the inversion of a is simply given by b

   1q, as shown in equation (3.12).

inversion of a diagonal matrix
of course, the inversion of the diagonal b matrix is very simple. for example, if we premultiply b
by a unit matrix with its (1,1) element replaced with 1/3 (i.e., operation 2, q 1(1/3)), then the    rst
row of b is divided by 3. just exactly what is needed.

then, to invert a diagonal matrix b premultiply by a unit matrix whose diagonal elements are
replaced by the reciprocals of the corresponding diagonal elements of b. premultiplying both sides
of (3.14) by such a matrix, b becomes the unit matrix, while a new q matrix (say, q    ) is formed
on the right. from (3.15), it can be seen that this new q     is the inverse of a (i.e., i = q   a).

   
    1

3
0
0

   
    b=

   
    1

3
0
0

0
1
2
0

0
0
3
2

0
1
2
0

0
0
3
2

   
    qa     i = q

(cid:5)

a =

(cid:28)
   
(cid:28)
    3
(cid:28)
   1
   1

(cid:28)
(cid:28)
4    1    5
(cid:28)
3
4
2
3

1
1

4
4
2

   
    a.

(3.15)

during the formation of the elementary operations, we could have decided to reduce the diagonal
elements of a to unity as the operations progressed, rather than waiting to do it at the end. the
results would obviously be the same.

3.3. gauss-jordan reduction 59

3.3 gauss-jordan reduction

matrix inversion can be thought of as an algorithm     a series of elementary operations which result
in the inverse of the input. the foregoing shows that the inverse is a product of those elementary
operations in matrix form. gauss-jordan is the name of the method whose objective is speci   cally to
operate on the input (using the elementary operations, but not in matrix form), until the unit matrix
emerges. if these same operations are concurrently performed on a unit matrix, it will emerge as the
inverse of the input.

to emphasize the concurrency of these operations, they are performed on an    augmented
matrix    as shown in (3.16). in partitioning these two matrices side by side, no matrix operation is
implied. the columns of i are simply added on to those of the input, forming an nx2n matrix.

a |i =

   
   

   
    3
1
2
2    1
0
1    1
2
   1, the result would obviously be i|a

1 0 0
0 1 0
0 0 1

   1. however, the
if this matrix were to be multiplied by a
inverse is not yet known, so we must think in terms of an algorithm, a method by which a can be
   reduced    to the unit matrix. if these operations succeed in this reduction, then     taken together
   1. if that is true, then their operation on the    augmented    columns will cause
    they must be a
this inverse to appear on the right.

the method is basically the same as that used in all the methods of this chapter.    pivots    are
to be (re)located along the main diagonal. in general, row and column interchanges are required.
however, these will be omitted in this discussion for reasons of clarity. (note that if a row interchange
is to be made, the interchange would include the augmented elements.) column interchanges are only
between columns of a. these must be taken into account, later.

the    pivot row    is then divided by this element, and this row is used to eliminate (reduce to

zero) all other elements in the    pivot column.   

(3.16)

(3.17)

   
    1.0 a1

12
a22
a32

a21
a31

   
    .

a1
13
a23
a33

b1
0 0
11
0 1 0
0 0 1

in (3.17), the augmented matrix is shown just after the    rst row is divided by a11. note that all
elements in the row are changed (so they are shown with the superscript    1   ). in particular, the
1,1 element of the unit matrix is no longer 1.0, since it has been divided as well.

just as in the previous chapter, the elements below this    rst pivot will be reduced to zero by
subtracting the proper multiple of row 1 from the other rows. the result is shown in (3.18). at this
point, the    rst    elimination    step is complete. to begin the second step, the 2,2 element is taken as
the pivot. row 2 will be divided by this element and the new row will be used to eliminate all the

60

3. matrix inversion

elements in column two     both above and below the main diagonal.

   
            

1.0 a1
12
0 a1
22
0 a1
32

a1
13
a1
23
a1
33

b1
11
b1
21
b1
31

0 0

1 0

0 1

   
             .

(3.18)

using the augmented matrix from (3.16), the procedure is shown in 3 decimal places (rather than
fractions). the 1st pivot element is a11 (i.e., 3.0). dividing the 1st row (including augmenting
columns) by this element:

subtracting row 1 from row 3

0.333
0.667
2.000    1.000
0.000
1.000    1.000
2.000

0.333 0.000 0.000
0.000 1.000 0.000
0.000 0.000 1.000

0.667
0.333
0.333 0.000 0.000
2.000    1.000
0.000
0.000 1.000 0.000
0.000    1.333
1.333    0.333 0.000 1.000

the new pivot is a22 (2.000). after dividing row 2 by a22, the other two elements in the second
column are eliminated in the following two steps:
0.333    0.167 0.000
0.833
0.000
1.000    0.500
0.000
0.000
0.500 0.000
0.000    1.333
1.333    0.333
0.000 1.000
0.333    0.167 0.000
0.833
0.000
1.000    0.500
0.500 0.000
0.000
0.667    0.333
0.000
0.667 1.000

0.000
0.000

the last pivot is a33 (0.667). the third row is divided by this amount, and then the other elements
in column 3 are eliminated:

0.500
1.000

0.750    1.000    1.250
0.000
0.000 1.000    0.500
0.000
0.000
1.000    0.500
0.000 0.000
1.500
0.750    1.000    1.250
0.000
0.000    0.250
0.750
1.000    0.500
1.500
   
   
   ;
    3/4    1    5/4
   1/4
1/2

1.000
1.000

   1 =

3/4
3/2

1
1

a

the last 3 columns of the above augmented matrix are the inverse of the given matrix, a.

(3.19)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.000
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.000
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.000
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.000
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.000 0.000
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.000 0.000
   
   3
1
2
2    1
0
1    1
2

0.000 1.000
0.000 0.000

a =

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) .
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) .
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) .
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
   
    .

in the event that the given problem requires the solution to ax = c, the inverse is not needed. in
this case, the augmented matrix would contain the single column, c, or perhaps multiple columns,
if several solutions are to be found. the method would be exactly the same     the input a would be
reduced to i while the given column(s) develop into the required solution vectors.

3.4. the gauss reduction method 61

singular matrices

3.3.1
if the a matrix is singular, zero (or near zero) elements will appear on, and to the right of, the main
diagonal. results from gauss-jordan reduction of a (6x6) are shown here, to illustrate the condition:

x1

x2

x3

x4

1.00000 0.00000 0.00000 0.00000
0.00000 1.00000 0.00000 0.00000
0.00000 0.00000 1.00000 0.00000
0.00000 0.00000 0.00000 1.00000
0.00000 0.00000 0.00000 0.00000
0.00000 0.00000 0.00000 0.00000    0.00000

x6
x.x
x.x
x.x
x.x

x5
x.x
x.x
x.x
x.x
0.00000    0.00000
0.00000

in the above case the upper left 4x4 diagonalizes normally     pivot elements within the expected
range of the problem. then, suddenly, the 5,5 pivot value drops to (near) zero (note the underlined
values). care must be taken in the programming for this condition     roundoff errors prevent the
pivot from being exactly zero. note the terms    0.00000. these indicate a negative value which
is zero to    ve decimal places, but apparently not exactly zero. the point is that a sudden drop in
absolute value must be sensed (i.e., well below the range of expected values).
the elements above these pivots, indicated by    x.x,    will not be zero.

in general, if the rank of a(nxn) is n, the procedure completes normally (a is non-singular).
if the rank of a is r < n, then an rxr unit matrix is calculated normally, in the upper left of the
augmented matrix, but a qxq (q = n     r) array of (near) zeros will appear at lower right.

in the case where the inverse of a is required, obviously, the procedure and the problem are
at an end     since no inverse exists. in the case ax = c, no unique solution exists. however, a    general
solution    may be found if the equation set is    compatible.    this possibility will be discussed in more
detail in the following chapter.

the gauss-jordan method as a matrix inverter will not be pursued further because it is
inef   cient compared to other methods. however, it is a marvelous tool for determining many char-
acteristics of vector sets and matrices     the subject of chapter 4.

3.4 the gauss reduction method

the objective of this method is a triangular matrix form (rather than the unit matrix) emerging
from the input. in other respects it is the same as gauss-jordan. in particular, the pivot elements are

62

3. matrix inversion

always on the main diagonal, and in general, row/column interchanges are necessary to put them
there.

   
          1.0

a21
a31
a41

   
             .

   
         

   
             c1

1
c2
c3
c4

a1
11
a22
a32
a42

a1
12
a23
a33
a43

a1
13
a24
a34
a44

the diagram above shows a 4x4 with one single augmenting column.this column is the right-hand
side of ax = c. the    rst row (including c1) has already been divided by the pivot, a11. to indicate
the changes of value, the elements in row 1 are given a superscript. the elements under a11 are now
to be reduced to zero. this can be accomplished in row 2 by subtracting from it a21 times row 1.
and, the leading elements of the other rows are eliminated in this same fashion. note that row 1
includes the c1 element, and when the ai1multiples of row 1 are subtracted from the lower rows, the ci
elements will be changed. also, there may be several, even many, augmenting columns (the n columns
of a unit matrix, perhaps). these additional columns would take part in the operations in the same
way that the c column, above, does. in this discussion, the c columns occupy a separate matrix,
c(nxm), rather than be the    augmenting columns    of a. today, it is unlikely that these operations
are to be performed by hand; so the visualization of the    side-by-side    columns is unnecessary. in
the computer program, moving these columns into the a matrix would be a wasted effort.

of course, it is not necessary to actually calculate any of the elements in column 1. the top

(pivot) value will be 1.0, and the elements below it will be 0.0.
de   ne k to be the index to the pivot. then, k sequences from 1 to n   1, where n is the order of
the matrix (when k = n, there are no elements to    eliminate.    however, the nth row of the augmented
matrix must be divided by this n, n pivot value.). at any stage k < n, the method described above
can be written into the    pascal-like    code shown below.
the steps shown are within an outer loop which steps k from 1 to n     1. note that in every stage,
the elements operated upon are those to the right of, and below, the pivot. the elements above the
pivot are not affected. two (identical) loops are shown in the code     one for a (j = k+1 to n) and
the other for the augmented, c, columns (j = 1 to m). the code shown emphasizes that the same
operations are carried out on the augmenting rows (the variables cij ).
if the data is truly in a single augmented a(nxn+m) matrix, the code could be written with
just one loop indexed from k+1 to n + m. however, the    augmented matrix    concept need not to be
taken literally as far as data storage in the computer is concerned.

the triangular objective is reached when the ann element is chosen as the pivot. no further

reduction is necessary at this point; however, the nth row must be divided by ann.

3.4. the gauss reduction method 63

gauss reduction method    code   

for j = k+1 to n do
begin

akj = akj /akk
for i = k+1 to n do
begin

aij = aij - aik * akj

end;

end;
for j = 1 to m do
begin

{note: m = number of augmenting columns}

ckj = ckj /akk
for i = k+1 to n do
begin

cij = cij - aik * ckj

end;

end;

as a simple example ax = c:

(cid:17)

a|c

(cid:18) =

the method quickly produces the triangular form:

   
    , where c = {4, 0, 2}

4
0
2

   
    3
1
2
2    1
0
1    1
2
   
    1 1/3

2/3
1    1/2
1
0

0
0

   
    .

4/3
0
1

x3 = 1 .
x2     1
x1 + 1

2 x3 = 0; x2 = 1
3 x3 = 4
3 x2 + 2

2

3

; x1 = 1
2 .

now, the solution for x3 is apparent, and from there, each unknown can be obtained in    reverse
order   :

(3.20)

(3.21)

(3.22)

this reverse order solution method is often called    back substitution.   

3.4.1 gauss reduction in detail
the method, including full pivoting, is described here. row and column exchanges will be accom-
plished by exchanging indexes in row and column lists, rather than exchanging data rows/columns.

64

3. matrix inversion

three lists are used: a row list,    rlist,    a column list,    clist,    and a second column list,    blist.    the blist
remembers the column exchanges, and the order in which they occur.
one method change is made here: the pivot rows will not be divided by the pivot element, as is done
in the earlier description. however, the rows below the pivot are operated on by the same values as
before (the division step is included in these row subtractions. see the variable x in step 3 below).
the example problem given below will be followed more easily in doing this, and also this change
in method converts more directly into lu decomposition.
in the steps, below, the term    condensed determinant    refers to the square array |akk, ann|,

from the pivot (k,k) to the (n,n) term in the given a matrix.

1. initialization. if the data rows and columns are not actually going to be exchanged, the lists
through which the data is accessed must be initialized. rlist[j] = j and clist[j] = j are set, and
the blist is set to all zeros for j = 1 to n (the order of the matrix).

2. maximum element. at each stage, k (a total of n     1 stages for an nxn matrix), the largest
element in the condensed determinant is chosen. it is found in the pth row, qth column. in
general, p is not in the pivot (kth) row, and q is not in the kth column.
then rlist[k]   rlist[p] and clist[k]   clist[q] (the symbol     indicates    exchange   ). also, if a
column (clist) exchange did occur, blist[k] is set to q.

3. central operation loop. at each stage, k, the objective is to zero the elements under the pivot
element akk. the following    pascal-like    code is the best way to describe this. in particular, the
pivot rows are not divided by the pivot elements. instead, the variable x is employed to contain
the ratio of beginning element value to pivot, as shown here:

for i = k+1 to n do { n is the order of input matrix }
begin

x = aik
for j = k+1 to n do aij = aij     x    akj

akk

end;

the index k is the row/column of the pivot and the indexing deserves special attention. the
element aij , for example, would ordinarily be accessed by a[i,j]. however, because of row and
column interchanges it becomes a[rlist[i],clist[j]]. then the temporary variable x, above, is

x = a[rlist[i],clist[k]]/a[rlist[k],clist[k]] .

this is the price that is paid for being able to exchange the list indexes rather than the data
rows/columns. notice also that both i and j run from k+1 to n.

the operations 2 and 3 are repeated for the stages k = 1 to k = n   1 (when the pivot is the
(n,n) element, the matrix is already triangular).

4. back substitution. an upper triangular set of equations, ax = c is solved from xn back up to

x1 according to the following (easily veri   ed) relations:

3.4. the gauss reduction method 65

      
   ci     n(cid:21)

k=i+1

      
   ;

aikxk

xi = 1

aii

i = n, n     1,       , 1; note: xn = cn

ann

.

(3.23)

if there are multiple {c} columns (for example, the augmented matrix includes a unit ma-
trix), then (3.23) is executed for each row of each column. see the back substitution code in
section 3.5.1.

in the computer implementation, the x-vector overwrites {c}. then, in (3.23) just replace xi
with ci. note that cn = cn/ann. since the c values are found (overwritten) in reverse order, each
ci depends only upon ck values where k > i, which have just been overwritten.

5. unscramble rlist. because of full pivoting, column interchanges occur. when they do, the
solution variables, though calculated correctly, come out in a scrambled order. to rectify this,
the blist was kept, which remembers the column exchange (if any) and in which stage it
occurred.

the initialized blist contains all zeros. if a column exchange occurs at stage k, then blist[k] is set
to the column, q, in which the new pivot was found. after the gauss reduction, unscrambling
of the rlist must be done in the reverse order:

for i = n   1 downto 1 do if blist[i] (cid:4)= 0 then rlist[i]     rlist[blist[i]]

again, the symbol     indicates interchange.

6. unscramble data. at this point, the rlist order is correct but this is not 1, 2, 3 order. it is then
necessary to physically arrange the {c} data columns into 1, 2, 3 order (the user of the routine
cannot be expected to view the output solution vectors    through    the rlist).

data storage
in the computer implementation of the above, the input a matrix is operated upon directly.the input
is thus destroyed in favor of the triangular form. similarly, the {c} vectors are destroyed, becoming
the output solution vectors. if the input {c} vectors are the unit matrix, then of course this matrix is
replaced by a

   1.

66

3. matrix inversion

3.4.2 example gauss reduction
the gaussian reduction of a 5x5 set of equations is presented as an example. its data is given with
little discussion     intended as check values for the reader   s own programmed solution.

   
               

1 0    3
1
2
   2 1
5    2    2
   1 1
1    3
3
0 0    1    1
3
1 1    4
3
5

   
               

                  
               

                  
                =

                  
               

                  
                .

   2
1

4   3

5

x1
x2
x3
x4
x5

(3.24)

the determinant of the a(5x5) matrix is 1.0, chosen for clarity (so that the solution vector {x}
would have integer values). the following table lists the pivots chosen during the procedure, their
p,q locations, and their values. also in the table are the resultant rlist and blist values (i.e., the rlist
and blist are shown with their    nal values, at the end of the gaussian reduction):

p q
value
2
3
5.00000
5 5
3.40000
3 4
2.94118
5 5
0.42000
5 5    0.04762

clist
3
5
4
2
1

rlist blist
3
5
4
5
0

2
5
3
1
4

un
scrambled
rlist
4
1
2
3
5

the output augmented matrix, at the termination of the triangularization process, is shown in the
following table. note: this is a printout of a[rlist[i], clist[j]].

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

5.0000    2.0000    2.0000
   4.0000
1.4000
3.00000
2.94118
   3.00000
0.5294
   1.00000

matrix a after gauss triangularization c-column
1.00000
5.80000
6.47059
   1.60000
   1.95238

1.00000    2.0000
1.80000    0.6000
1.35294    0.1176
0.42000    0.0800
-0.0476

3.4000
1.8000
0.8000
2.6000    2.4706    0.0400

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

the back substitution starts at the bottom of this augmented matrix. for example:

x[rlist[5]] =    1.95238/   0.04762 = 41
x[rlist[4]] = (0.08*41   1.6)/0.42 = 4

the next table shows the completed results of back substitution, and the unscrambling of the rlist.
at the left of the table is a copy of the c-column printout.

the column is accessed via rlist, so for example, the    rst c value is that which is    pointed to   
by the    rst index in rlist (i.e., rlist[1]). since rlist[1] = 2, that    rst value must occupy location 2 in the
c-column. by looking back    through rlist    in this way, the data can be placed in its actual locations.

3.4. the gauss reduction method 67

c-column
c[rlist[i]]
1.00000
5.80000
6.47059
-1.60000
-1.95238

rlist
2
5
3
1
4

actual 
data 

location
-1.60000
1.00000
6.47059
-1.95238
5.80000

data 
after 
back 

substitution

4.00
19.00
2.00
41.00
6.00

un-

scrambled 

rlist
4
1
2
3
5

final 
data
41.00
4.00
19.00
2.00
6.00

next to this data are the {x} values after back substitution. then, using the corrected (un-

scrambled) rlist, the correct order of the data can be obtained.

the pascal-like code for unscrambling the rlist, then the data, is given below. note that the
rlist becomes scrambled in the reduction process because column interchanges occur (due to full
pivoting). if only partial pivoting is used, the rlist would not need to be    unscrambled.   

unscramble rlist - - - }

{
for i:=n-1 downto 1 do if blist[i] <> 0 then
begin j:=rlist[i]; rlist[i]:=rlist[blist[i]];
rlist[blist[i]]:=j; end;

next, the output vector(s) must be unscrambled, to cease dependence upon the rlist. note that the
output vectors are in the same storage space as the input c-vectors     thus the code still refers to
them as c-vectors.

{ unscramble the rows of the c-vectors - - - }
for p:=1 to n do if rlist[p] <> p then
begin
begin
begin

for i:=p to n do if rlist[i] = p then k:=i;
for j:=1 to m do { notenotenote: m is the number of c-vectors
begin
begin
begin { exchange c[rlist[k]] with c[rlist[p]]

}

}

x:=c[rlist[k],j];
c[rlist[k],j]:=c[rlist[p],j];
c[rlist[p],j]:=x;

endendend;
rlist[k]:=rlist[p]; rlist[p]:=p;

endendend; { c matrix now contains the sol   n vectors in order

}

68

3. matrix inversion

partial pivoting
step 2 of the gauss reduction procedure outlined above describes a    maximum element    routine
which chooses the largest (absolute value) element in the reduced matrix. if it is desired that no
column exchanges occur in the transfer of the pivot element to the pivot position, the maximum
element search could be con   ned to the pivot column only. the largest element in this column is
found and a row exchange then occurs. this method is called    partial pivoting.   

the method outlined here accommodates partial pivoting by simply changing the maximum
element routine. of course, the rlist will not have to be unscrambled, the blist is now super   uous,
its content remaining at all zeroes (see the rlist unscramble routine, above).

lu decomposition

3.5
with a couple of very minor changes, the foregoing gaussian method can become    lu decompo-
sition.    these two methods are fundamentally the same, and achieve exactly the same numerical
results. nevertheless, there is reason for our interest in lu.

this method    nds a very clever use for the lower element positions (below the main diagonal)
as the input matrix is being reduced. remember that in gaussian reduction (and lu decomposition
as well), all elements below the main diagonal are reduced to zero     this is the objective of the
method. in lu (decomposition) these element positions are stored with data that can be used later
to    reduce    the input c-vectors. then, the initial input to lu is just the amatrix, without any
   augmenting columns.    the initial output is the    decomposition    of a into l (lower) and u (upper)
triangular matrices, as shown here (a 4x4 example):

   
          a11

a21
a31
a41

   
             

   
          u11 u12 u13 u14

l21 u22 u23 u24
l31
l32 u33 u34
l41
l43 u44
l42

   
          .

a12
a22
a32
a42

a13
a23
a33
a43

a14
a24
a34
a44

(3.25)

the lower triangular matrix, l, has all unity (1.0) main diagonal elements. and (3.25) can be taken
literally in equation form: a = lu. the uij elements are exactly those that are calculated in gaussian
reduction     given that the pivot rows are not divided by the pivots.

the advantage in all this is that once a is decomposed, any number of c-vector columns can
be input and solved without reducing a again. in effect, the lij elements remember the operations
that are to be made on the augmented vectors. since pivoting must be used, the row and column
interchanges must be remembered as well, of course. in the case of full pivoting with index switching
rather than actual row/column exchanges     as in the previous section     the rlist, clist, and blist
must all be saved. the solution to ax = c proceeds:

(cid:5) = c
lc
ux = c

(cid:5)

(forward substitution)
(back substitution) .

(3.26)
(3.27)

3.5. lu decomposition 69

for every input c vector, (3.26) must be solved to obtain c   .then, (3.27) is solved to    nd the solution
x, of the given equation set, given that c vector. the c    vector is the same as that which would have
emerged from gaussian reduction, prior to back substitution. since both l and u are triangular, these
equations solve easily. the solution of (3.27) (back substitution) has already been discussed, and is a
basic algorithm in gaussian reduction. the solution to the upper triangular set (3.26) is very similar,
called    forward substitution    whose algorithm is almost identical to back substitution.

3.5.1 lu decomposition in detail
the detailed description of lu follows that for gaussian reduction, almost exactly. full pivoting
will be used again in this method (partial pivoting is a viable alternative). then, the    rst 2 steps are
the same as previous method, and the important step 3 is only trivially different:

for i = k+1 to n do { n is the order of input matrix }
begin

lik = aik
for j = k+1 to n do aij = aij     lik    akj

{note this difference from gauss}

akk

end;

remember that the above code is within an outer loop whose index is k, running from 1 to n-1.thus,
the lij elements are nothing more than the ratios of the pivots divided into the leading elements of
each row     immediately below the pivot. rather than form the ratio aik
in a temporary variable,
akk
x, (as in gauss reduction) these ratios are simply stored into the    unused    below-diagonal element
positions again, the indexing is not simple, as implied above. as before, the element aij is indexed:

all of the subscripted variables in the code must be accessed through rlist and clist.

aij = a[rlist[i],clist[j]] .

forward substitution
the solution to (3.26) is known as    forward substitution.    it is the solution to a lower triangular set
of equations; (3.28) gives a 4x4 example:

   
          1

l21
l31
l41

   
         

   
          c

(cid:5)
(cid:5)
1
c
(cid:5)
2
c
(cid:5)
3
c
4

   
          =

   
          c1

c2
c3
c4

   
          .

0
1
l32
l42

0 0
0 0
1 0
1

l43

(3.28)

(cid:5)
1

= c1; and c

in this case, (as contrasted with back substitution), the solution proceeds in 1, 2, 3 order, i.e., from
= c2     l21c1. in general, (since main diagonal elements are 1.0):
(cid:5)
1. obviously, c
c
= ci     i   1(cid:21)

for i = 1, 2, 3, . . .n .

(3.29)

lij c

(cid:5)
2

(cid:5)

(cid:5)

c
i

j

j=1

70

3. matrix inversion

the lij elements over write the below-diagonal aij elements. then, in a computer program, these
would still be accessed as a[rlist[i],clist[j]]. also, the c    values overwrite the c values. in (3.29), lij
could be written aij , and there is no real need for the    primes    on c.

note how similar this is to back substitution. this forward substitution is to be done on every

input c-vector (or each column of the input unit matrix, if the routine is to calculate an inverse).

from this point, the lu method is again the same as gauss reduction. the data must be
unscrambled.the unscramble method depends on the pivoting that was used. assuming full pivoting
with the index lists, as described before, the unscrambling is the same as before.

3.5.2 example lu decomposition
when the lu changes are made to the gauss reduction, the resulting lu matrix is not triangular
as shown in (3.30). this matrix is not lu , it is    lu[rlist[i], clist[j]]    just like the one given in the
gauss example, section 3.4.2 (in fact, note the similarity).

   2.0000    2.0000
   2.0000
5.0000
1.00000
   0.80000
1.80000    0.60000
3.40000
1.40000
0.60000    0.52941
1.35294    0.11765
2.94118
0.23529    0.17999
0.60000
0.42000
0.08000
0.76471    0.84000    0.95238
   0.20000
-0.04762

(3.30)

if the l*u matrix product is taken (remembering that the data must be accessed via rlist and

clist) the result is the original a matrix. its rows/columns will not be scrambled.

// ------------------------------------------ forward substitution

for i:=2 to n do {a is lu matrix }
begin
begin
begin {n is matrix order}

sum:=0; p:=rlist[i]; {c is the right side vector}
for j:=1 to i-1 do
begin
begin
begin

q:=clist[j]; sum:=sum+a[p,q]*c[rlist[j]];

end;end;end;
c[p]:=c[p]-sum;

end;end;end;

// ---------------------------- back substitution

p:=rlist[n]; q:=clist[n];
c[p]:=c[p] / a[rlist[n],clist[n]];
for i:=n-1 downto 1 do
begin
begin
begin

sum:=0; p:=rlist[i]; for j:=i+1 to n do
begin
begin
begin

q:=clist[j]; sum:=sum+a[p,q]*c[rlist[j]];

3.6. matrix inversion by partitioning 71

end;end;end;
c[p]:=(c[p]-sum)/a[p,clist[i]];

end;end;end;

note: at the end of forward substitution, the problem is exactly like the gauss example. the
(cid:5)
c
column is the same as that given in the augmented matrix of the gauss reduction. the back
substitution is the same as was done in that example. the unscambling routines are also the same.

3.6 matrix inversion by partitioning
when the order of the inversion matrix is large, roundoff error is an especially important consider-
ation due to the huge number of operations involved. if a large matrix inversion could be attacked
in a series of smaller inversions, with iterative improvement at each step, the possibility is that the
roundoff error might be held at an acceptable level.

partitioning the large matrix affords the ability of such an attack.

(cid:22)

m(nxn) =

(cid:23)

a (n1xn1) d (n1xn2)
g (n2xn1) b (n2xn2)

n = n1     n2

inversion by partitioning can be regarded as a generalization of reduction (elimination) meth-
ods. in gaussian reduction, for example, each stage    reduces    the given matrix one unknown by
solving it in terms of the remaining ones. now we consider eliminating whole sets of unknowns.
the diagram above shows a matrix m(nxn). it is partitioned into 4 submatrices   not usually of the
same size. in this case, n = n1 + n2. the diagram implies n2 > n1, but that need not be true. it is
required that a be square, however, since the    rst step is to obtain its inverse.

consider the equation set which incorporates these partitions:

(cid:24)

note that x and c1 are (n1x1); y and c2 are (n2x1) .

(3.31)

ax + dy = c1
gx + by = c2
(cid:29)
               
            

solving these matrix equations just like any 2-by-2 set will result in

(cid:30)

   1

m

[c] =

(cid:24)

(cid:23)

=

(cid:22)

x
y

a1c1 + d1c2 = x
g1c1 + b1c2 = y

whose partitions a1, d1, g1, and b1 are those of the inverse matrix, m

   1 + a
a1 (n1xn1) = a
   1dh
d1 (n2xn1) =    a
   1
   1dh
g1 (n1xn2) =    h
   1
   1ga
b1 (n2xn2) = h
   1

   1ga

   1

(3.32)

   1. the results are

.

(3.33)

72

3. matrix inversion

the price that is paid for being able to invert the nxn matrix by inverting the two smaller matrices
is the large amount of id127, and the roundoff errors that are bound to accrue.
nevertheless, the method should be considered for the inversion of large matrices.

a numerical example of the method is given in section 3.8.1.

3.7 additional topics
both gauss reduction or the lu method are excellent tools for determining inverses, or for solving
linear equation sets. since lu offers the advantage that additional solutions can be obtained from
additional c-vectors, its    forward substitution    is preferred.

in both of these methods, when an inverse is required the    c-vector    input consists of the n
columns of a unit matrix. in most programs, the input a matrix is overwritten during the inversion
process and the unit matrix input is overwritten with the inverse.

because the lu method can be entered with just the a matrix without any augmenting
columns, it is ef   cient in the calculation of determinants as well. the one precaution is in the
row/column interchanges. in the general case both row and column are exchanged to place the
largest element at the pivot position. in this case the determinant is unchanged since there are two
sign changes. however, if just a row or a column exchange occurs, the determinant value must be
multiplied by    1. note that in the 5x5 example, above, the product of the diagonal elements is    1,
but the correct determinant value is +1. in the 3rd stage, only a column exchange occurred, thereby
multiplying the product of diagonal elements by    1.

the    essential computer effort    in matrix inversion is the number of lengthy    oating point

operations required.

usually only multiplications and divisions are counted, although additions (subtractions) are
sometimes included. the number of these operations required for an lu decomposition can be
determined with reference to the inset diagram, below.

figure 3.1: floating point operations in a lu decomposition.

at every cycle, k, a new element is moved to the main diagonal. this element is multiplied
by the accumulated value of the determinant at that point. underneath the new pivot there are n   k

elements which must be divided by the pivot (to determine the lij elements). adjacent to each of
these are n   k elements, aij , from whom are subtracted a product. then, for the kt h execution of the
outer loop there are n   k divisions, and (n   k)2 multiplications and subtractions. the multiplication
for the determinant value is neglected since it is not an essential part of the method. then, the
number of divisions:

3.7. additional topics 73

the number of multiplications and subtractions are

div = n   1(cid:21)
mult = n   1(cid:21)

k=1

k=1

(n     k) = 1

2 n (n     1) .

(n     k)2 = 1

6 n (n     1)(2n     1) .

the sum of divisions plus multiplications
2 n (n     1) + 1

1

6 n (n     1)(2n     1) = 1

3 n (n 2     1) .

in a matrix inversion there are n    c-vectors    most of whose elements are zero. however, it is very
unusual for the program to take advantage of this fact. therefore, we will consider the general case
in which n c-vectors are input. in this case, entirely similar reasoning leads to

forward substitution ops = 1
backward substitution ops = 1

2 n 2(n     1)
2 n 2(n + 1) .

the total inversion process, then, requires n 3 operations. with the speed and precision of modern
computers this numbers is a problem only when the matrix is very large. although the inversion
of these very large systems is outside the scope of this work, several of the following paragraphs
speak to the problem by discussing column id172, improving the inverse, and inversion by
orthogonalization.

3.7.1 column id172
if the determinant of the matrix is    ill-conditioned,    the inversion process may accumulate error or
even fail. in section 2.7 of chapter 2 it was shown that a determinant value, |a|, is geometrically
related to the n-dimensional volume enclosed within the column vectors of a. if one or more of
these vectors is disproportionately small, the determinant value will be small. the condition is easily
spotted, and easily    xed. simply write:

ax = c     a1x1 + a2x2 + a3x3 +        = c

now, change the variables, xj =   j yj and set the    value such that its vector, a, is normalized to
unit length.

74

3. matrix inversion

the other source of problem is determinant    skew.    in the worst case one or more of the
column vectors is a linear combination of the others   the matrix is singular, no inverse exists. in
less severe cases the input matrix may be resolved into the product of an orthogonal matrix and a
triangular one; see section 3.7.4.

improving the inverse

3.7.2
matrix inversion is characterized by a large number of simple arithmetic operations (in fact, on
the order of n 3 of them). it is not unusual for the inverse process to lose precision due to the
accumulation of roundoff error. the accumulation is greater the larger the matrix, of course, and is
particularly troublesome when the matrix is nearly singular.

the matrix equation ax = i de   nes x as a

in general, the input matrix is not known exactly, with element values the result of measure-
ment. then, an exact inverse is rarely required. instead, we invoke a clever iterative process which
can usually restore all the precision that is meaningful to the problem.
   1. each column of i is the product of ax, where
x is the corresponding column within x. for simplicity, then, consider ax = b, where the vector b is
any one of the vectors in i. this equation set is to be solved for x, using lu decomposition followed
by forward and back substitution.
of course the set has an exact solution, x, but the accumulation of roundoff error produces a
somewhat different vector, x0 = x +  x. the (hopefully small)  x is the departure from the exact
solution, and it produces a    residual    vector,  b. that is:

a(x +  x) = b +  b,
a x =  b .

and since ax = b

(3.34)
(3.35)

now,  b is simply ax0     b, and b is known     it   s one of the columns of i. then the iterative process
is:

1. save the input a matrix. use lu and forward, back substitution     x0.
2. multiply ax0 and subtract b      b. if the elements of  b are small enough, then stop, else:
3. use forward back substitution with  b as input      x.
4. subtract  x from x0     de   nes a new x0.
5. go back to step 2.

when a stop occurs at step 2, the  b is within the required precision and the x vector is the improved
solution.

especially note that step 3 does not involve lu decomposition. the lu matrix already exists, having
been produced in step 1. remember that this is the primary advantage of lu, compared to gauss
reduction     the ability to input any number of vectors, after the input matrix has been decomposed.

3.7. additional topics 75

in step 1, the input a matrix is saved because the lu decomposition overwrites the a input. it is
the saved version that is used in the multiplication in step 2.

in step 2, it is desirable, and may be necessary, to use greater precision in the calculation for  b.
this could be very dif   cult, since it is likely that the original x0 was done with the longest    oating
point data length. this is only necessary when trying to attain the precision of the computers data
length. in the usual case, iteration improves the solution. it cannot hurt the solution as long as the
 b vectors are decreasing.

it is meaningless to require greater precision in the inverse than that in the input a matrix. if

a matrix b is found such that:

ab = i + r

(3.36)

and r (residual matrix) is beyond the practical precision of a, then b is the inverse of a. in general,
the precision of b will be less than that of a. in most cases, 3 or 4 iterations will be enough. of
course, the entire procedure must be repeated for all the vectors in the inverse, changing the location
of the unit value in the input b column.

inverse of a triangular matrix

3.7.3
the algorithm for the inversion of a triangular matrix is much more direct than that for the general
matrix. consider an upper triangular matrix, p. its elements below the main diagonal are all zero;
those on the main diagonal are all nonzero; and those above it, are not (all) zero. the determinant,
|p|, is given by the product of its main diagonal elements (hence, none of these may be zero). the
inverse of p is, say, q. it will also be an upper triangular matrix. its main diagonal elements are the
reciprocals of those of p.
now, we consider the product qp = i. as in any matrix product, the ijth element of i is given
by qi   pj , the dot product of the ith row of q by the jth column of p. using a 4x4 example, we
have:

   
          q11
[q1
]{p1
[q1
]{p2
[q1
]{p3
[q2
]{p3

   
          = i

   
         

   
          p11 p12 p13 p14

q12
0 q22
0
0

q13
q23
0 q33
0

q14
q24
q34
0 q44

0 p22 p23 p24
0
0 p33 p34
0 p44
0
0
} = q11p11 = 1 ; then, q11 = 1/p11 ;
} = q11p12 + q12p22 = 0; solve for q12 ;
} = q11p13 + q12p23 + q13p33 = 0; solve for q13 ;
} = q22p23 + q23p33 = 0; solve for q23 .

the above may be generalized to:

j   1(cid:21)
k=i

qikpkj

qij =     1

pjj

(3.37)

(3.38)

76

3. matrix inversion

where nxn is the order of the matrix , and in the given order:

i = 1, 2, 3 . . . n;
j = i, i + 1, i + 2, . . . n; (j > i);
k = i, i + 1, . . . j     1 .

(3.39)

a    pascal-like    description is:

for i:=1 to n do
for j:=i to n do
begin

if j = i then qjj:=1/pjj else
begin

qij:=0;
for k:=i to j-1 do qij:=qij + qik * pkj;
qij:= - qij/pjj;

end;

end;

an algorithm for the inversion of a lower triangular matrix, p, is given below. in this case,
the elements of p above the main diagonal are all zero. the inverse matrix, q, will also be lower
triangular. then, qij = 0, if i < j. further, qii = 1/pii, and also, the determinant of both p and q is
given by the product of their diagonal elements.

for the lower triangular elements (i.e., i > j):
qij =    qjj

qikpkj

(cid:21)

where, in the given order (and n is the order of the matrix):

k

i = n, n     1, n     2, . . . 1.
j = i     1, i     2 . . . 1.
k = j + 1, . . . i .

the elements of q are calculated from the lower right corner toward the upper left corner. that
is, the nth row is calculated from the (n,n-1) element to the (n,1) element. then, the n-1st row (not
including the main diagonal, since it is already de   ned as the reciprocal of the p main diagonal),
and so on. as an example of the method, consider the following p matrix:

   
          1

2
3
4

p =

   
          .

0
2
5
7

0
0
3
8

0
0
0
4

(3.40)

(3.41)

(3.42)

its inverse, q, is:

   
         

7

q =

a few sample calculations are:

(cid:28)
(cid:28)
(cid:28)
1
0
   1
(cid:28)
(cid:28)
1
2
3    5
2
6
1
24    2
12 19

(cid:28)
(cid:28)

0
0
3
3 1

(cid:28)

0
0
0
4

   
          .

3.7. additional topics 77

(3.43)

q43 =    q33(q44p43) =    1/3(1/4)(8) =    2/3
q41 =    q11(q42p21 + q43p31 + q44p41)
q41 =    1[1/4 (4) + (   2/3)(3) + (19/24)(2)] = 7/12
q32 =    q22(q33)(p32) =    1/2(1/3)(5) =    5/6 .

inversion by orthogonalization

3.7.4
it is a remarkable fact that a general square, nonsingular matrix can be resolved into the product of
an orthogonal matrix, say v, times a triangular matrix, p. both v and p are easy to invert!

the news isn   t all rosy, however. the method is susceptible to roundoff error, so it is not
recommended as a matrix inverter. but, it does work, given enough precision, and besides, the
method is a very interesting one to develop.
given a(nxn), we set about deriving the orthogonal matrix in the following way. consider
a as an assemblage of column vectors a1, a2,        ak,       , an, where ak is the kth column of a.
select the    rst column and normalize it to unit length. this new unit vector will be v1:

(cid:27)

v1 = a1

l1

;

l1 =

+ a2

2

+        + a2
n .

(3.44)

a2
1

the second vector, v2, is chosen to be in the same plane as v1 and a2, a linear combination of these
2 = c1v1 + c2a2. the prime merely indicates an unnormalized vector. since v1 and
two vectors: v
v2 must be orthogonal we dot v1 with v

2 and solve for c1 (c2 can be set to 1).

(cid:5)

(cid:5)

v1     v

2 = c1v1     v1 + c2a2     v1 = 0
(cid:5)
c2 = 1, c1 =    a2     v1
2 = a2     (a2     v1)v1 ; v2 = v
(cid:5)
v

(cid:5)

2
l2

.

note that v1 and v2 are orthogonal.

in the same manner v

(cid:5)

3 = a3     (v1     a3)v1     (v2     a3)v2 and in general

j = aj     j   1(cid:21)
(cid:5)
v
i=1
pij = vi     aj .

pij vi where

(3.45)

(3.46)

(3.47)

78

3. matrix inversion

the pij factors can be arranged into an upper triangular matrix, with the main diagonal elements
being the id172 lengths of the vectors, lj . note that the jth column of p provides the pij
factors in (3.46).

further, solving (3.46) for aj

(cid:27)

aj = pjj vj + p(j   1)j vj   1 +        + p1j v1
pjj = lj =

+        + v2
nj .

+ v2

v2
1j

2j

that is:

a = vp .

the inversion of a is now a relatively simple matter. the triangular q = p
   1 = v t ). then
earlier, and the inverse of v is obtained by transposition (v

   1 = qv t .

a

(3.48)
   1 has been discussed

(3.49)

inversion of a complex matrix

3.7.5
the gauss reduction method, and any other method that will successfully invert a real matrix, will
work equally well on a matrix whose elements are complex     given that the routines used support
complex arithmetic. some minor adjustments must be made. for example, the routine which chooses
the largest element now must be made to determine the absolute value of a complex number.

complex arithmetic can be dif   cult to do if the compiler itself does not recognize the complex
type. also, the need for inversion of a complex matrix may not arise often enough. so, whatever the
reason, it may be required to invert the complex matrix using only real arithmetic, and real numbers:
find a complex matrix b such that ab = i, where a is complex. then:
j 2 =    1 .

(ar + jai )(br + jbi ) = i;

(3.50)

equating real (subscript, r) and imaginary (subscript, i) parts:

arbr     aibi = i and arbi =    aibr .

r aibr and (ar + aia
then, assuming that ar has an inverse, bi =    a
   1
and the elements of the complex matrix br + jbi are
br = (ar + aia
   1
   1
r ai )
bi =    a
   1
r aibr .

(cid:24)

r ai )br = i.
   1

(3.51)

notice that the increased dif   culty of complex numbers cannot be avoided. although just two
matrices must be inverted, both just nxn, there is a lot of id127 involved.

inversion using partitions

3.8 examples
3.8.1
this example intends to simulate the inversion of a large matrix. for reasons of clarity and lack of
space, this    large    matrix, m, is only 8x8. its inversion will be affected by inverting no larger than a
3x3 array. the process is straightforward, but the    bookkeeping    becomes cumbersome. to begin,
m is partitioned as shown, with a 3x3 in the upper left.

3.8. examples 79

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

3.00
2.00
4.00    5.00
2.00    1.00
0.00
1.00
3.00

1.00    2.00
4.00
1.00
0.00
0.00
3.00    1.00
5.00    2.00    1.00
1.00
1.00    1.00
2.00
2.00
3.00
3.00    2.00
4.00
0.00
3.00
   2.00
2.00    2.00
1.00    1.00
5.00
3.00
3.00
2.00
1.00
4.00
0.00    1.00    2.00
1.00
0.00
0.00    1.00    1.00
4.00    2.00
(cid:22)
(cid:22)

(cid:23)(cid:22)

0.00
0.00
1.00
0.00
1.00
0.00    4.00
0.00
2.00
2.00
1.00

2.00

(cid:23)

(cid:23)

a(3x3) d(3x5)
g(5x3) b(5x5)

=

x
y

c1
c2

= ax + dy = c1
gx + by = c2

.

the four partitions of m are named according to the set of equations in (3.31):

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

if a(3x3) is inverted the vector, x, can be solved for in terms of the remaining unknowns:

x = a

   1(c1     dy)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 1.00    2.00

3.00    1.00
2.00

3.00
2.00
4.00    5.00

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

a =

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

   3.00
19.00    11.00
14.00    8.00

2.00    1.00
7.00
5.00

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

   1 =

a

plugging the x value back into the second equation, then solving for y, yields:

(b     ga

   1d)y = c2     ga
hy = c2     ga

   1c1
   1c1; where h=b     ga

   1d .
   1 known, h is known     and it has the dimensions of b(5x5).

note that, with a

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

h =

   20.00    7.00    25.00    11.00    5.00
   50.00    23.00    77.00    17.00    11.00
   78.00    37.00    118.00    28.00    23.00
   82.00    44.00    125.00    27.00    20.00
51.00
14.00

30.00

74.00

18.00

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

80

3. matrix inversion
   1 has an integer inverse. because of this coincidence, h is also an integer
it was a coincidence that a
   1. since the
matrix. h
largest array that can be inverted is 3x3, h must be partitioned     again with a 3x3 in the upper
left position.

   1 will surely not be    so lucky,    and in order to proceed, we must have h

the partitions of h will be named a2, d2, g2, and b2, occupying the same positions as those

in the original matrix, m. proceeding as before:

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

a2 =

a2(3x3)w + d2(3x2)z = d1; w = a
g2(2x3)w + b2(2x2)z = d2

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

2 (d1   d2z)
   1
   0.2419
0.1900
0.1004    0.3477

0.1774    0.0645
0.7348    0.5197
0.1971

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

   1
a
2

=

   20.00    7.00    25.00
   50.00    23.00    77.00
   78.00    37.00    118.00

as before, h2 = (b2     g2a
h2 will be 2x2, the same dimensions as b2, and will be easy to invert.

   1
   1
2 d2), which can now be written because a
2

(cid:16)(cid:16)(cid:16)(cid:16)

h2

1.6129
17.9355
   2.3907    21.8244

(cid:16)(cid:16)(cid:16)(cid:16)

   1
h
2

(cid:16)(cid:16)(cid:16)(cid:16)    2.8427    2.3361

0.3114

0.2101

is known. this time,

(cid:16)(cid:16)(cid:16)(cid:16)

   1 must be found, which requires a complete solution to the above

the cumbersome part is that h
equation set

2 (d1     d2z)
w = a
   1
2 (d2     g2a
z = h
   1
   1
2 d1)
(cid:31)

.

(cid:18) =
   1. since a

w
z

(cid:23)

(cid:22)

the value for z must now be plugged back into the expression for w. with some algebra, and
rearrangement, the results are like those given in equations (3.33):
   1
   1
2 d2h2g2a
   h
   1
   1
2
2 g2a
2

   1
   1
2 d2h
   1
2
h
2

 (cid:22)

   1
a
2

+ a

(cid:30)(cid:17)

   a

w
z

(cid:22)

(cid:23)

(cid:23)

   1

=

=

(cid:29)

h

d

   1
2

+ a

with h

   1
are known, each of the 4 partitions of h
and h
this equation de   nes h
   1
2
calculated     for example, its upper left 3x3 is a
2

   1
   1
2 g2a
2 d2h
   1 known, using the format of equation (3.33) m
   1 becomes:
   6.0672
4.7815    6.6349    2.4435
0.1513    0.0476    0.1289
4.4118    5.8889    2.0915
   1.0840
   1.4706
   3.0168
0.2605    0.1746    0.1293

2.8721    1.7526
5.2418    0.1438    5.3707    4.3445
   4.241
1.6097
3.6303
0.1438
0.0392    0.0098
0.0266
0.0252
1.3595    3.8562
0.1307
3.2353
0.5345    0.3296
0.0425    0.9911    0.6807
0.9967
1.5817    0.0621    1.6503    1.4118
0.9935    0.5458
2.8105    0.0359    2.8427    2.3361
1.7180    1.1881
0.2101

0.1662    0.0621    0.1928

   1
2 . see m

4.6564
0.2507
3.8954

1.3968
2.2222
4.3016

   1, below.

   1 =

m

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

0.3114

8.2063

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

.

d1
d2
   1 can be

for example, the lower right 5x5 of m
written above.

   1 is b1 = h

   1
   1. in turn, its lower right 2x2 is h
2

as

3.9. exercises 81

if the inverse of m is not required, just the solution to the equation set (3.31), many of the

tedious matrix operations can be avoided.
y = h
x = a

   1(c2     ga
   1(c1     dy)

   1c1)

the solution, above, does require that both a and h must be inverted. however, when this is done,
fewer operations remain, and these equations can be solved using    matrix-times-vector    operations
rather than    matrix-times-matrix   .the savings are considerable.

as this example indicates, when the matrix b (and consequently, h1) is still too large to be
inverted directly, additional    partitioning    is required until that lower right matrix is within the range
to be inverted   possibly a lengthy process. but, the    nal result will retain greater precision than a
direct approach.

3.9 exercises
3.1. given the matrix, a, below, determine q such that the product qa produces zero element

values in the    rst column of a, except, a11.
what is the determinant value, |a|?

   
    1

a =

3    1
1
6    1

3 11
2

   
   .

3.3.

3.4.

3.2. with the a given in problem 1, determine the solution to ax = c = {4, 12, 7}. note that {c}

is a column vector.
find the inverse of the complex matrix a =

   
    1 + j0
1 + j8
1 + j2
0 + j1    1 + j1    6 + j3
1 + j1
0 + j5    8 + j15
find the inverse of the complex matrix a = ar + jai
   
   
    and ai =
    1    2
3    1
2

   
    1    1 0

3
2
4    5

   
    .

ar =

0
2

   
   .

0 0
0 1
   1 = b. if two columns in a are exchanged, how is the

3.5. given the equations ax = c, and a

solution, x, affected? how is b affected?

82

3. matrix inversion
3.6. perform an lu decomposition on the 5x5 matrix in section 3.4.2. do not use pivoting.

show that l*u does not equal the input s matrix.

3.7. using the result from exercise 3.6, solve the example problem using the same right-side

c-vector from equation (3.24).

c h a p t e r 4

83

linear simultaneous equation

sets

introduction

4.1
this chapter turns to an interpretation of the solution to linear equation sets, using a geometric
approach and insight. we will look at an equation set in several different (and perhaps new) ways,
and consider the solvability and compatibility of an equation set. most of the mechanics of solution
have already been discussed. this chapter intends to be largely conceptual.

many applications in mechanics, dynamics, and electric circuits depend on the insights gained,

and presented here.

we begin by de   ning the equation set ax = b as    nonhomogeneous    because the b vector is
assumed to be nonzero. associated with this set is the    homogeneous    set, ax = 0; the same set, but
with the b vector replaced by the zero vector. in the event that matrix a is nonsingular, and has an
inverse, the homogeneous set plays no part. but, when a is singular, we will    nd interest in both ax
(cid:5)
= 0, and in a

x = 0 (the transposed homogeneous set).

vectors and vector sets

4.2
in order to gain greater insight into its solution, the equation set will be interpreted as a    vector
transformation.    the equation ax = y    transforms    the columns of a(nxm) into the vector y.
alternatively, y is    synthesized    as a linear vector sum of the column vectors of a.

ax = y =

               
            

               
             x1 +

               
               
             x2 +        +
            
= a1x1 + a2x2 +        + amxm = y .

a12
a22
...
an2

a11
a21
...
an1

               
            

               
             xm =

               
            

               
             , or:

y1
y2
...
yn

a1m
a2m
...
anm

(4.1)

(4.2)

in this quite general example, a is (nxm); there are m vectors (the columns of a), each with n
coordinates (dimensions     the rows of a). it is often instructive to draw the same vector picture
of the transposed matrix, i.e., a

, whose n column vectors are the rows of a.

(cid:5)

to begin, the discussion of vectors in chapter 1 is reviewed and enlarged upon in the para-

graphs, below.

84

4. linear simultaneous equation sets

in two dimensions, a vector, v, is described as {vx,vy}, where the subscripts    x    and    y    refer
to unit vectors in a rectangular coordinate set. these unit vectors could be written as {1,0} and {0,1},
showing both their orthogonality and their unit length. vx and vy are the components of v along
the coordinate axes. extension to three dimensions is simply: v = {vx, vy, vz}, and in either 2 or 3
dimensions, it is convenient to use subscripted letters (e.g.,    x,       y,    and    z   ) to refer to the unit
vectors (the coordinate axes).

the two and three-dimensional cases are familiar, and easily visualized. but, in generalizing
to greater than 3 dimensions, visualization is lost. for this reason, the plan will be to view the various
concepts in the 2 and 3 dimensional cases; then simply extend the reasoning into n-dimensions. for
example, the de   nition of a vector:

v = {vx , vy , vz} in three dimensions
v = {v1, v2, . . . vn} in    n    dimensions

(4.3)
(4.4)

is extended to n-dimensions with only a relatively minor change in notation: the coordinate axes
are now given numbers, rather than    x, y, z, . .    letters. but these n coordinate axes are still perceived
as rectangular axes, in an    n dimensional space,    and the values, vj are the components of v along
these axes. in fact, the component vj is de   ned as the product of the (n-dimensional) length of v
multiplied by the cosine of the angle between v and the jth coordinate axis (i.e., the concept of the
   direction cosine,    in n dimensions).

if v is composed of real components, its length is de   ned as
+ v2

|v| = sqrt(v2
|v| = sqrt(v   v) = sqrt(v

+        + v2
v) .

n) =

+ v2

v2
1

1

2

2

(cid:5)

(cid:27)

+        + v2

n

(4.5)
(4.6)

(cid:5)

in (4.6), the notation (v   v) and (v
v) or (vtv) denote the dot product of v into itself, in n dimensions
(   n-space   ). in general, the dot product of two vectors, u   v, is simply the sum of products of the
respective components of the vectors. equivalently, this (scalar) dot product can be expressed as the
product of their magnitudes, multiplied by the cosine of the angle between them. also, two (nonzero)
vectors are said to be    orthogonal    in n dimensions if their dot product is zero:
u = vtu = (v1u1 + v2u2 +        + vnun) = 0 .

(4.7)

v

(cid:5)

a vector whose length, |v|, is unity is called a    unit vector.   
if the vectors consist of complex numbers, the de   nitions must be modi   ed. for this purpose,
a new notation is introduced: if c = a + jb is a complex number, its    complex conjugate    (i.e., the
number a     jb) is denoted   c.
v). similarly, the    hermitian   
scalar product between two complex vectors, u and v, is   u     v (which is generally complex, and not
equal to   v     u).

then the length of a complex vector, v, is|v| = sqrt(  v   v) = sqrt(  v

(cid:5)

4.2. vectors and vector sets 85

4.2.1 linear independence of a vector set
a set of vectors, a1, a2, a3,    am, is said to be    linearly independent    if no (scalar) constants, ck, can
be found which relates them in the following way:

a1c1a2c2a3c3 +        + amcm = 0     a(n    m)c(m    1) = 0(n    1) .

(4.8)

note that there are m vectors, each with n coordinates (dimensions).

in 2-space, and with two vectors, equation (4.8) becomes: c1a1 + c2a2 = 0. in this simple
case, if nonzero values for c1 and c2 can be found, it means that the two vectors are scalar multiples of
one another. the (dependent) vectors are collinear. such vectors    use    only 1 of the two dimensions
available (although these vectors may not be parallel to either of the coordinate axes). conversely,
in 2-space, any two vectors that are not collinear, are linearly independent, and are said to       ll    the
space   two constants cannot be found which relate them in the sense of (4.8). furthermore, the
determinant of the square a(2x2) matrix formed of the vector components will be non-zero (a will
not be singular).
note that in 2-space, three vectors are necessarily dependent, whether or not they    ll the space. in
general, in an m-space, more than m vectors form a dependent set.

in 3-space, three vectors which do not lie in a plane are linearly independent, the case in
fig. 4.1, i.e., a2 and a3 lie within plane-p, a1 does not. it is clearly not possible to derive any one of

figure 4.1:

the a vectors as a linear sum of the other two. the equation:

a{c} = c1a1 + c2a2 + c3a3 = 0

(4.9)

has no solution (except {c} = {0}).

now, slide the tip of the a1 down the normal until the vector lies in the plane-p. clearly, any
one of the vectors can now be obtained as a linear sum of the other two by a simple vector addition
and (4.9) has a non-trivial solution. the 3 vectors do not    ll the 3-space (the term    3-space    is used

86

4. linear simultaneous equation sets

to describe a 3 dimensioanl space. then, the term    n-space    will refer to a space of n dimensions).
with all three vectors in plane-p it is possible to    nd a fourth vector orthogonal to all three; for
example, the vector n. in general, this circumstance is determined by the existence of a non-trivial
solution to the transposed set,

(cid:5){z} = 0.

a

(cid:5)

(mxn)z(nx1 = 0(mx1) .

a

(4.10)

(cid:5)
the original m vectors are row vectors in a

. if non-trivial z vectors can be found, they are

orthogonal to the original set.

summarizing: the linear (in)dependence of the m vectors in n-space is determined by inves-
tigating the possible (non-trival) solutions of equations (4.8) and (4.10). gauss-jordan reduction
(section 3.3) is often used in this investigation.

4.2.2 rank of a vector set
the rank of a vector set, a(mxn), is equal to the order of the largest nonvanishing determinant
that can be formed from the matrix a(nxm); and the largest non-vanishing determinant cannot be
greater then the smaller of n and m.
in the event that m < n (more dimensions than vectors), and the rank is r < m, the set is
dependent and there will be m     r solutions to the equation set (4.8). if r = m then the vector set
is independent, and (4.8) has only the trivial solution. this is also true for the    square    case, m = n.
if m > n, the rank of a cannot be greater than n. necessarily, the m vectors are dependent,
and non-trivial solutions will be found for (4.8). again, the rank could be less than n, in which case
the (many) m vectors still do not    ll the n-space.

an obvious example is the a(4x3), shown below, with three 4-dimensional unit vectors.
clearly, the three vectors are independent, although there are only 3 vectors, and the 4-space is not
   lled. because the 4-space is not    lled, there must be a vector orthogonal to all the 3 (unit) vectors
(cid:5)
shown     one independent solution to a
x = 0. clearly, that solution is the fourth unit vector. the
   rank    of a is 3     the size of the largest non-zero determinant that can be formed from the elements
of the vectors.

   
          .

   
          1 0 0

0 1 0
0 0 1
0 0 0

also, note that given a y vector: ax = {y1, y2, y3, 0} (which, obviously, lies in the same

subspace), the solution to ax = y is x = {y1, y2, y3}. but, if y has y4 (cid:4)= 0, the set has no solution.

not quite so obvious is the next example, again 3 vectors, in 4-space.

   
          2    5 5

3    3 6
2 1
1
   1    8 1

   
          .

4.2. vectors and vector sets 87
(cid:5)
as in the previous case, there are only 3 vectors, and a

z = 0 must have at least one nontrivial
solution. if the vectors are independent, then ax = 0 has only the solution x = 0; however, if they are
(cid:5)
dependent, then ax = 0 has a solution, and the transposed set, a
z = 0, has more than one solution.
note that z is a 4 dimensional vector, while x is 3 dimensional.

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

      
    =

      
    x1

2    5 5
3    3 6
2 1
1
   1    8 1

1 0
1.667
0 1    0.333
0
0 0
0
0 0

the gauss-jordan method, introduced in chapter 3, provides an important tool for deter-
(cid:5)
mining the (in)dependence of these vectors, and the solutions to both the ax = 0 set, and the a
z =
0, if any exist. gauss-jordan operates on the input matrix with only elementary operations, thus not
altering the rank of the given set. for this example:

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) gauss-jordan    
the 2x2 unit matrix formed at the upper left of the reduced set indicates that the rank is 2 (the
largest non-zero determinant). also, the reduction gives the solution to (4.9). the value of x3 can
be set arbitrarily (say, x3 = k), and:
      
      
       5/3
    k
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) gauss-jordan    
(cid:25)
(cid:25)
(cid:24)
            
            
            
         
          k1 +
          =

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)1 0    1
(cid:25)
(cid:24)    3
            
            
          k2 .
         

it is instructive to continue this example by solving the transposed set. since the rank is two we
expect a two-fold in   nity of solutions. the gauss-jordan of the transposed set is

3 1    1
   5    3 2    8
1
5

the rank is two, so z3 and z4 can be set arbitrarily (say k1 and k2).

3
1    2.333
0
0

(a single in   nity of solutions) .

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) 2

(cid:24)
            
         

0 1
0 0

k1 +

1/3
1

and so

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

1   1

1   1

then

6 1

x2
x3

z1
z2

=

k2 .

7/3
   3
7/3
0
1

when the set is square, a(nxn), probably the most important case, if the determinant, |a|, is zero
then the vectors are dependent. there will be an independent, non-unique solution for each level of
   degeneracy    (i.e., n     r = 1, 2,    ) where r is the rank.

z1
z2
z3
z4

1
0

88

4. linear simultaneous equation sets

simultaneous equation sets

4.3
this section considers equation sets, ax = c in which the right-hand side, c, is non-zero.the equation
set can be viewed as a vector transformation in which {c} is to be synthesized by a linear weighted
sum of the left-hand column vectors (if possible). the problem is to    nd the weight factors (the
elements of the x column).

square equation sets

4.3.1
writing equation (4.1) as a vector equation, with m = n (   square   ):

               
             x1 +

               
            

               
            

a11
a21
...
an1

a12
a22
...
an2

               
             x2 +        +

               
            

               
             xn = y=

               
             .

               
            

y1
y2
...
yn

(4.11)

a1n
a2n
...
ann

a(nxn)x=y    

the columns of a are the vectors to be added, using weighting factors, xj , resulting in an output
vector y. these equations are de   nitely    coupled    (into a single vector equation). but, if a vector,
say, v1, could be found, that is simultaneously orthogonal to (i.e., perpendicular to) all the a vectors
in (4.11) save the    rst     that is:

v1     aj = 0;

for j=2, 3,        n .

then, we could dot v1 through (4.11):

(v1     a1)x1 + (v1     a2)x2 +        + (v1     an)xn = (v1     y) .

(4.12)
all the products (v1     aj ) are zero, except the    rst (i.e., j = 1). then: (v1     a1)x1 = (v1     y), and:

x1 = (v1     y)
(v1     a1)

.

(4.13)

next, if a vector v2 could be found that is orthogonal to all except a2 then the same procedure could
be used to uncouple x2 from the rest. and, so on. of course, it may not be easy to    nd successive
vectors, vj , such that each is orthogonal to all but the jth a-vector. but, in 2 and three dimensions
it is easy. a    2-space    example is:

choose v1 = {2,    1} and v2 = {0, 1}. then:

given ax = y    
(cid:17)
(cid:17)

and

0 1

(cid:24)

(cid:25)

1   2

(cid:23)(cid:24)

(cid:22)    2 1

0 2

(cid:18) (cid:24)    2
2    1
(cid:25)
(cid:18) (cid:24)

0

x1
x2

0

=

(cid:25)

(cid:24)    2
(cid:25)
x1 =(cid:17)
x2 =(cid:17)

0 1

1
2

(cid:25)

(cid:24)

(cid:25)

1
2

x2 = y =
(cid:25)

x1 +
(cid:18) (cid:24)
(cid:25)
1   2
    x2 =    1 .

1   2

    x1 =    1

2    1
(cid:18) (cid:24)

.

(4.14)

4.3. simultaneous equation sets 89

a 3-space example is much more interesting. equation (4.15) shows a general (3x3) equation set,
using the vector form. figure 4.1 is reproduced below, for reference. if the set is independent, then
      
|a| (cid:4)= 0, and a1 does not lie within (on) the plane, but has a component along   n.
    .

      
    x3 = y =

      
    x2 +

      
    x1 +

      
    a12

      
    a11

      
    a13

      
    y1

(4.15)

a21
a31

a22
a32

a23
a33

y2
y3

as before, a vector, v1, is required, and it must be orthogonal to both a2 and a3. but, the    gure already
shows this; i.e.,   n is clearly normal to a2 and a3. then, dot-through (4.15) by a vector parallel to   n,
and only a coef   cient on x1 will remain on the left side of the equation. and, it is easy to de   ne a
vector along   n. the vector cross product of a2   a3 will do nicely (the cross product a3   a2 would
do just as well). then, just as in the (2x2) case (since v1 = a2   a3):

x1 = (a2    a3     y)
(a2    a3     a1)

.

(4.16)

note that figure 4.2 is perfectly general. that is, any 2 of the 3 a vectors can be chosen to de   ne a
plane, then the remaining vector is viewed in terms of its projection onto the plane, and its component
normal to it. from that point, the solution for each of the x values is the same as above, and will
have the form of (4.16).

figure 4.2: redrawn of figure 4.1.

when the dimensions are > 3, the ability to draw pictures, and visualize results is lost. but,
the approach is valid. in fact, the above examples have really been an interpretation of the solution by
premultiplication of the inverse matrix. given that b is the inverse of a, and ax = y,

ba = i; and, therefore: bax = x = by = a

(4.17)
clearly, the rows, bi, of b are orthogonal to the columns, aj , of a     except when i = j (in
which case the dot product is unity). and, since the product is commutative, the rows of a are

   1y .

90

4. linear simultaneous equation sets

in the same orthogonal relationship with the columns of b. then, in any number of dimensions,
premultiplication by the inverse matrix    uncouples    the given equation set.
in summary: given a non singular matrix a, the equation set ax = y has a unique solution, for any
vector, y. that solution is obtained by premultiplying the equation by the inverse matrix. the solution
vector, x, can be viewed as the set of coef   cients in the synthesis of y by the column vectors within a, as
   base vectors.   

return to the 3 dimensional example discussed above. but, now slide the tip of a1 down
the normal, until a1 lies in the plane-p (all 3 vectors now lie in the plane). see figure 4.3. in this

figure 4.3:

case, when the cross product of any 2 of the a vectors is found, it will be orthogonal to all three of
them. the method of solution clearly fails. the reason is that the a vectors are no longer linearly
independent. the equation

c1a1 + c2a2 + c3a3 = 0

now has a non-trivial, non-unique, solution; the transposed set will have at least one solution.

   
    1
1
0    2    1
2    4    1

1

   
   . these column vectors lie in a plane whose normal

as an example, a =

lies along a line {2, 3,    1}, which is the (only) solution to a
2}.

(cid:5)

z = 0. the solution to ax = 0 is k{   1,    1,

if the given non homogeneous set is ax = y = {0, 1, 3}, a solution may not be possible, unless
the y vector also lies within the subspace occupied by the a column vectors. the test for this is that
y must be orthogonal to all independent solutions to the transposed set. in this example, the test
product z(cid:129)y = 0, and the set is compatible.

4.3. simultaneous equation sets 91

then, a total (complete) solution is

      
       1   1

2

      
    +

      
    2
1   3

      
    .

x = k

which is the sum of all solutions to the homogeneous set, plus any solution to the non homogeneous
set.

the rank of the original equation set may be less than n   1:

x1 + 2x2 + 3x3 = y1
x1 + 2x2 + 3x3 = y2
x1 + 2x2 + 3x3 = y3 .

(4.18)

now the columns of a are collinear; the rank of a being n   2 (n= 3). it can therefore be anticipated
that there will be a double in   nity of solutions to the homogeneous set (i.e., two arbitrary constants).
the two solutions to ax = 0 are k1{-1, -1, 1}, and k2{-5, 4, -1}. these solutions are not
only independent, they are orthogonal (their dot product is zero). while this orthogonality is not
necessary (just linear independence will do), it is not surprising that two orthogonal vectors could
be found: because, two dimensions are not included in the columns of a     that is, a plane. within
this plane, there are an in   nity of sets of orthogonal vectors.

the solution k1{-1, -1, 1} was found by inspection. the second solution can always be found

that is orthogonal to both the    rst row of a, and {-1, -1, 1} by solving:

(cid:22)

1
2 3
   1    1 1

(cid:23)      
    z1

z2
z3

(cid:25)

(cid:24)

      
    =

0
0

whose solution is k2{-5, 4, -1}.

given a y vector, in (4.18), which results in a compatible set, the solution will be:

      
       1   1

1

      
    + k2

      
       5
4   1

(cid:24)

      
    +

x = k1

(cid:25)

any solution to the
non-homogeneous set

.

(4.19)

the y vector in (4.18) must be collinear with the direction of all the {a} vectors, {1, 1, 1}. any y
vector which is orthogonal to the plane whose normal is {1, 1, 1} is necessarily in the direction {1,
1, 1}, and will hence, be compatible. vectors that lie in this plane are solutions to:

   
    1 1 1

2 2 2
3 3 3

   
   {z} = {0} .

(cid:5)

z =

a

(4.20)

92

4. linear simultaneous equation sets

there are two solutions, of course. they are k1{-1, 1, 0}, and k2{1, 1, -2}. again, these solutions are
orthogonal (not necessary, but this ensures linear independence). in (4.18), if a y vector is given that
is orthogonal to both of these solutions to (4.20), then compatibility is assured; else, the given set of
equations is incompatible, and has no solution.

in this simple (3x3) example, it is easy to see the compatibility requirement. in the general
case it will not be possible to visualize geometrically. but, in the general (nxn) case: ax = y, when
the rank of a is r < n, and n is the order of a, there will be n-r solutions to the homogeneous equations
a

z=0. if the given y vector is orthogonal to all of these solutions, then the given set is compatible.

(cid:5)

as was shown in the example, there will also be n-r solutions to the homogeneous set ax = 0.
the complete solution to the original set is the sum of these latter solutions, and any solution of the
nonhomogeneous set.

4.3.2 underdetermined equation sets
given ax = y in which a is nxm, and n < m, the set is    underdetermined        i.e., there are an
insuf   cient number of equations to determine the x vector uniquely. if the set is compatible, non-
unique solutions will be possible.

when the set is viewed as a vector equation, two cases are apparent. first, if the rank of a
is n, then the solution is much like the square, nonsingular set. assuming that the    rst n columns
of a have rank n (or renumbering the columns and x vector components so that this is so), these n
vectors can be partitioned:

bu + dv = y

(4.21)

where, now the b matrix comprises just the (nxn)    rst (nonsingular) columns of a. the vector u is
u = {x1, x2,    xn}, the    rst n components of x, and v is v = {xn+1,    xm}, the remaining components
of x. matrix d holds the remaining columns of the original a matrix. since b is nonsingular, then
a solution for u can be found, in terms of y, and v whose components can be assigned arbitrarily:

u = b

   1y     b

   1dv

(v arbitrary) .

(4.22)

that is, there are m   n arbitrary constants in the solution (there is an m   n fold in   nity of solutions).
if the rank of a is less than n, there may be no solutions at all, unless the y vector lies within

the same subspace as the a vector set. consider the following (4x5) example:

   
          1    1 0    1

0
1 6    5    2
2 3    1
1
0 1    1    1

3
   1
1

   
          x =

            
         

            
          .

   1
3
4
0

(4.23)

the gauss-jordan reduction method terminates at:

4.3. simultaneous equation sets 93

x1
1
0
0
0

x2
0
1
0
0

x4

x5

x3
0
0
1    1
0
0

0    2
1    2
1
0

c   1

0
1
0

(4.24)

where the column set apart at the right is the    augmenting    column, originally, the y vector. since
the    nal row is all zero (including the augmenting column) the set is compatible, and has the rank 3.
then x4 and x5 can be set arbitrarily (say, x4 = k1, and x5 = k2), and the complete solution is

                  
               

                  
                k1 +

                  
               

                  
                k2 +

                  
               

                  
                .

   1
0
1
0
0

2

2   1

0
1

0   1

1
1
0

{x} =

   
               

3    1
1
1
   1
0
2
1
3
0
1
6
   1    5    1    1
0    2
1    1

   
               {z} = {0} .

(4.25)

(4.26)

the gauss jordan reduction shows the compatibility, and if compatible, shows the complete solution.

although the gauss-jordan reduction solves the problem, it is instructive to derive it in the
manner of the previous section, and show that the set is compatible. the homogeneous transposed
set is:

this set has the solution {z} = {1,   1, 1, 3}. the dot product of this solution vector, with the original
y vector, {   1, 3, 4, 0} must be zero for the set to be compatible. this is clearly so.

(cid:5)

incidentally, in this example the z vector can be found, by deleting the last equation of the
transposed set, and calculating the adjoint matrix. since it is known that the rank of both a and
a
is 3, the adjoint matrix will be of rank 1. then, at least one of its columns will be nonzero, and
the solution to (4.26). if the rank of a were less than 3, the adjoint would be null, and this method
could not have been used.

4.3.3 overdetermined equation sets
when the number of equations, n, is greater than the number of columns, m, the set is said to be
   overdetermined.    stated the other way, interpreting the set as a vector equation, the set is    overde-
termined    when the dimensionality of the vectors, n, is larger than their number, m. however, it is
possible for a set to appear to be overdetermined, simply by having more equations than unknowns,

94

4. linear simultaneous equation sets

when, in fact, it is underdetermined because the equations are not independent. that is, if the rank
of a(nxm) is less than m, the set is really underdetermined.

(cid:5)
since a
is (mxn), whose rank cannot be greater than m, there will always be nontrivial
(cid:5)
solutions to a
z=0. therefore, there will always be compatibility conditions to be met. thus, the de-
termination of compatibility may become the larger problem. after the set is found to be compatible,
the extra equations can be discarded (resulting in an mxm), and the set solved.

but, there is another way. from the    geometry    of the set, itself, it may appear worthwhile to

premultiply the given set by a

(cid:5)

:

(cid:5)

ax = a
(cid:5)

a

y .

(4.27)

the matrix a
a is (mxm), the smaller of the two dimensions, and its rank should be the same as
that of a itself. surprisingly enough, this is one time that appearance does suggest an appropriate

(cid:5)

(cid:5)

a| exists, the equation set (4.27) is compatible whether or not the given set is
approach. if |a
compatible. if the given set is compatible, the solution to (4.27) yields the correct x vector. if the
given set is incompatible, the solution to the above is    the best available    in the so-called    least
squares sense.    the following article will derive a solution to ax = b which minimizes the sum of
squared error. it will be the same as the solution to (4.27).

least squares solutions
given ax = b, where a is (nxm), and n > m, any given {x}, will yield an ax vector with some
amount of error, e:

e = ax     b a(nxm); e, b are (nx1), x(mx1), and n > m .

(4.28)
if the original set is compatible, and n     m of the equations are functions of the    rst m, then it is
possible to derive an exact solution (with e= 0). the least squares situation arises when the set is
incompatible and any x vector results in errors. the least square criterion de   nes the    best    x solution
as the one in which the sum of the squared error is minimized. the sum of squared error is given by
e (the scalar dot product of e   e):
(cid:5)
e
(cid:5)
e
(cid:5)
e

e = (ax     b)
(cid:5)
e = x
ax     x
(cid:5)

(ax     b), or
(cid:5)
(cid:5)
a

b     b

a

(cid:5)

(cid:5)

ax + b
(cid:5)
b .
ax express the same dot product (b   ax). then b
(cid:5)
b .

ax     2x

e = x
(cid:5)

b + b

(cid:5)
a

a

e

(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5)
ax = x

(cid:5)

a

b:

(4.29)

both x

(cid:5)

(cid:5)

a

b and b

(cid:5)

4.3. simultaneous equation sets 95

(cid:5)

(cid:5)
a

the (scalar) term x
ax is called a    quadratic    form, because in its expansion, the variables appear
as a second degree product, xi xj , in every term. also required in the de   nition is that the (necessarily
(cid:5)
square) matrix be symmetric. note that a
b could be called a    bilinear
form,    if one considers the b vector as a variable. in that case, xi bj appear as products (hence    bi-
(cid:5)
linear   ). it is not required that the matrix (a
is not (it   s
not even square).
the method is to take the partial derivatives of e
and equate them simultaneously to zero. the resultant x vector minimizes e

(cid:5)
, in this case), be symmetric; and, indeed, a

e with respect to each of the m variables, xi, in turn,

(cid:5)
a is symmetric.the term x

e.

a

(cid:5)

(cid:5)

(cid:5)

                                 
                              

                                 
                              

(cid:5)

e
   e
   x1
(cid:5)
   e
e
   x2
(cid:5)

...
e
   e
   xm

                                 
                              

                                 
                              

   

    x1

   

    x2
...

   

    xm

    =

= {0} .

(4.30)

(cid:5)
e.
the solution to the equation set that results from (4.30) is the x vector which minimizes e
appendix a discusses the partial differentiation of bilinear and quadratic forms. it begins by de   ning
the vector differential operator,    .

.

(4.31)

using this de   nition (4.30) becomes    e
(cid:5)
a

e =    (x
(cid:5)

    e
(cid:5)

e = 0 and from (4.29)
(cid:5)
ax)         (2x
b) +     (b

a

(cid:5)

(cid:5)

(cid:5)

b) = 0 .

the b vector is not a function of x, so the last term is 0. appendix a    nds    (x
(cid:5)
   (2x

b. then:

b) = 2a

a

(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5)
ax) = 2a

a

(4.32)
ax, and

   e
ax     2a
e = 2a
(cid:5)
(cid:5)
ax = a
(cid:5)
(cid:5)
a
b

(cid:5)

b = {0}

(4.33)
(4.34)
this remarkable result indicates that the minimum squared error will be obtained when the x vector
is de   ned by solution to the square (mxm) set of (4.34). the original a is nxm, so a
a is mxm).
by hypothesis a has the rank m, so a

(see equation (4.27)) .

(cid:5)

(cid:5)

a is nonsingular giving
x = (a
(cid:5)

   1a
a)

b .

(cid:5)

(4.35)

what   s more, if the original set is compatible, (4.35) yields the unique solution!

96

4. linear simultaneous equation sets

id75

4.4
the engineering sciences are based upon physical entities and the relationships between them.
however, the relationships are most often expressed in exact equation form, implying a knowledge
of the exact values of the variables they contain. usually, this is not the case. many physical variables
are the result of empirical measurement. for example, in dynamics, a velocity or acceleration is known
as a result of observations. it may be known accurately, but not exactly.

over a limited range the relationship between variables, though not known, may be assumed
to be linear. then,    id75    is used to determine a    best    straight line relationship. most
often, a least squares    t to the data is chosen to de   ne the    best    t.    there are some good statistical
reasons for this choice; and (perhaps the most compelling reason) the least squares analysis is easy
to perform.

it has already been decided that the relationship between a dependent variable, y, known only
by a set of observed data points, yi, and an independent variable, x, is a linear curve, part of which
is shown in figure 4.4.

(4.36)
if exact (x, y) data could be obtained, it would only take two pairs to determine c1 and c2. but,

y = c1x + c2 .

figure 4.4:

the relationship between x and y is a complicated one and the data contains observation errors.
the problem is, then, to determine a    best    t    curve so that other y-data can be predicted from
given x-data.    best    is determined to be a least squares    t to the data. in general, quite a few
(x, y) measurements are taken over the range of interest, in an attempt to    average-out    as much
observation error as possible. thus, if an equation yi = c1xi + c2 is written for every one of the
observations, a very overdetermined equation set results.

the xi data need not be equi-spaced (as implied by fig. 4.4), and some (but not all) of the yi
points may be redundant measurements at the same value of xi. the objective is, of course, to allow
the error to    average out,    yielding a regression line that is accurate to within the requirements of
the physical problem.

then, given the set of n observed (x, y) data points, write:

y = xc = [x]{c}

(4.37)

where y = {yi}, the y observed data, and x containing the x-data:

   
          x1

x2
      
xn

   
         

1
1
      
1

x =

(cid:24)

(cid:25)

c =

c1
c2

4.4. id75 97

               
            

y1
y2
...
yn

               
             .

y =

equation (4.37) is an overdetermined (nx2) set of linear equations in the unknown variables c1 and
c2. the previous article, and equation (4.35), provide the solution:

c = (x
(cid:5)

   1[x
x)

(cid:5)]y .

(4.38)

(cid:5)

x matrix is 2x2, clearly symmetric and nonsingular, unless the data is all at the same

in (4.38), the x
xi. the columns {c} and [x
x =

x

(cid:5)

(cid:5)

(cid:5)
]y are 2x1 (x

(cid:31) (cid:21)
(cid:21)

(cid:21)

is 2xn, times y(nx1)).

 

xi

n

(cid:31) (cid:21)

 

xi yi(cid:21)

yi

(cid:5)

y =

x

x2
i

xi

.

(4.39)

in these equations, the summations are to be taken over the index, i, from 1 to n. to avoid messy
(cid:5)
matrix terms, the inverse of x
x will be expressed in terms of its adjoint and its determinant in the
following:

carrying out the product terms indicated in (4.38), the solutions for c1 and c2 are:

(cid:21)

"(cid:21)

#2

xi

.

   

x2
i

(cid:31)

(cid:17)

(cid:5)

x

x

(cid:18)
adj =

   !

 

(cid:5)

xi

xi
x2
i

n    !
(cid:12)(cid:12)x
!
!
xi yi    !
!
!
c1 = n
!
!
yi    !
    (
!
!
    (

n
x2
i
n

c2 =

x2
i

x2
i

x

(cid:12)(cid:12) = n
!
!

yi

xi
xi )2

xi

xi yi

.

xi )2

(4.40)

(4.41)

(4.42)

some additional algebraic work can be done on these two equations, which will result in an ap-
pearance that is much more appealing. first, de   ne the average values of yi and xi as   x and   y,
where:

!

!

  x =

xi

and
to reduce c2, subtract   y from both sides of (4.42):
yi    !
!
!
!
    (

c2       y =

!

x2
i
n

n

xi

x2
i

  y =
!

xi yi

xi )2

yi

n

   

.

!

yi

n

.

(4.43)

98

4. linear simultaneous equation sets

now, on the right-hand side of (4.43), gather both terms over a common denominator, and note
that a term, n

yi, cancels. the result is:

!

!

x2
i

!

xi yi    !
!
!
!
    (

x2
i

xi (

n (n

!

yi )

xi
xi )2)

c2       y =    

.

(4.44)

compare the right side of (4.44) to (4.41), and write:

c2       y =      xc1;

c2 =   y       xc1.

or

(4.45)

to reduce c1 (equation (4.41)),    rst work on the denominator. note that:

(cid:21)

(xi       x)2 =
=
!

(cid:21)
(cid:21)

(cid:21)
    2  x
    n   x2.

x2
i
x2
i

xi + n   x2

!

then the denominator is simply n
is

(xi       x)(yi       y). this yields the    nal regression line equation:

(xi       x)2. and in similar fashion, it is found that the numerator

!

!
(xi       x)(yi       y)

(xi       x)2

.

(4.46)

y =   y + c1(x       x); where c1 =

which is the    nal result.

4.4.1 example regression problem
as an example of the method, the following analysis determines the dependence of the diameter
of a cylindrical part on the temperature of a heat treating process. over the range of temperatures
involved, this dependence is assumed to be linear:

d = c1t + c2 =   d + c1(t       t )

(4.47)

where d is the diameter and t is temperature. the data obtained in the laboratory is tabulated and
shown graphically in figure 4.5. the temperature, t, is given in thousands of degrees; diameter, d,
measured in inches.   d is average diameter, and   t is average process temperature.
there are 12 sets of (t,d) data points available   12 equations d = c1t + c2   an overdeter-
mined and incompatible 12x2 set in c1 and c2. id75 determines these unknowns using
the least squares best    t of the data to a straight line, called    regression line.   

4.4. id75 99

test data

t
1.10
1.10
1.15
1.20
1.20
1.28
1.30
1.30
1.30
1.40
1.40
1.40

d

1.039
1.045
1.037
1.030
1.049
1.033
1.033
1.030
1.020
1.023
1.021
1.012

figure 4.5: id75 diagram.

from the given data, the following results are calculated:
average temperature,   t = 1.2608
average diameter,   d = 1.031
c1 =    0.07986 inches per 1000 deg .

the equation of the regression line drawn in figure 4.5 is:

d =   d + c1(t       t) .

(4.48)

4.4.2 quadratic curve fit
the regression method is not limited to a linear curve    t.the data may be    t to a quadratic equation.
the starting point would be (compare this with 4.36):

y = a1x2 + a2x + a3 = {x2}a1 + {x}a2 + {1}a3 = xa

(4.49)
there are three columns in x(nx3) and three variables, aj in{a}. just as before, the least squares solu-
tion is obtained by premultiplying by x
x. the subsequent
inversion yields:

, this time resulting in a (3x3) matrix, x

(cid:5)

(cid:5)

a = {a1, a2, a3} =(cid:17)

(cid:5)

x

x

(cid:18)1(cid:17)

(cid:5)(cid:18)

x

y .

(4.50)

100

4. linear simultaneous equation sets

lagrange interpolation polynomials
interpolation

4.5
4.5.1
the curve    tting problem of the previous section involves a very overdetermined equation set. the
resulting best-   t curve is not expected to pass through any of the given points exactly. the very idea
is to achieve    smoothing    of data obtained by measurement.

the objectives of the interpolation problem are quite different. a set of (xk, yk) values are
given, and these represent the true values of a continuous, integrable function y = f (x), and at each
of the given points, yk = f (xk). the function itself may or may not be known.

a relatively simple representation of f (x) is desired, that will pass through the given points
exactly and can be used to interpolate values of f (x) at intermediate points, x, within the given
range.

one approach is to simply    curve    t    the n data points in the same manner as in the previous
section, but using an (nxn) matrix     not overdetermined. the result will of course be a polynomial
of degree n     1:

p(x) = c1 + c2x + c3x2 +        + cnxn   1
   
   
                =

   
               

   

2

                  

                  

1

       xn   1
       xn   1
       xn   1
      
      
       xn   1

k

n

x1
x2

1
1
1       
1       
1

xn

x2
1
x2
      
2
      
x2
n

c1
c2
ck      
cn

(4.51a)

(4.51b)

   
                .

   
               

y1
y2
yk      
yn

xc = y =

whose coef   cients, c, are to be determined by:

this is similar to the least square    t problem, but the set is obviously not overdetermined. the
indicated approach to determine the c coef   cients, is to    simply    invert the x matrix. the resulting
function p(x) will pass through the given (xj , yj ) points.
the matrix, x, has some interesting characteristics. note that if x1 were to take on any of the
values, x2,     xn, the determinant, |x|, vanishes because |x| then would have two identical rows.
for the same reason, the determinant vanishes if x2 assumes any of the values x3,    xn. and so
on. apparently, |x| is some function of the xk values which vanishes if any two of the values are
the same. this is such a powerful characteristic that we might deduce a product of all the possible
differences of the xk values (equation (4.52)). an additional factor, f , is added, since the product
of differences can only be deduced as proportional to |x|.

|x| = f (xn     xn   1)       (xn     x1)(xn   1     xn   2)       (xn   1     x1)             (x2     x1) .

(4.52)

in the general case, there will be n(n   1)
must have the factors:

2

terms in (4.52). as an example, if n = 4, its determinant

f (x4     x3)(x4     x2)(x4     x1)(x3     x2)(x3     x1)(x2     x1) .

(4.53)

4.5. lagrange interpolation polynomials 101

n

3

       xn   1

note that the x with the lower valued index is subtracted from that with the higher index regardless
of the respective numeric values of the two.
to determine the value of f , note that the main diagonal term in the determinant expansion
is (1    x2    x2
). but, in (4.52), the very    rst term will be just that, when the products
are multiplied out. therefore, the factor is f = 1, and the determinant is simply the product of the
difference terms.
unfortunately, the elements of the adjoint matrix are not so easily found     although these,
too, contain factors of the type (xj     xi ). further, the x matrix is usually ill-conditioned. note that
there could be huge differences in the [ xij ] terms and may be dif   cult to accurately invert in the
   normal    way. for such reasons, equation (4.51b) is rarely attacked directly.

4.5.2 the lagrange polynomials
the lagrange interpolation polynomial is de   ned as

p(x) = (x     x2)(x     x3)       (x     xn)
(x1     x2)(x1     x3)       (x1     xn)
+        + (x     x1)(x     x2)       (x     xn   1)
(xn     x2)(xn     x3)       (xn     xn   1)

y1 + (x     x1)(x     x3)       (x     xn)
(x2     x3)(x2     x4)       (x2     xn)

y2+

(4.54)
it   s a bit messy looking, but it does the job. p(x) is a continuous function and p(xk) = yk. each of
the terms in (4.54) is, itself, an n     1 degree polynomial and can be written compactly as:

yn.

qi (x)yi = n$

j=1
j(cid:4)=i

(x     xj )
(xi     xj )

yi

(4.55)

and p(x) is the sum of the (4.55) terms.

when attacked this way, there is no matrix or matrix inversion. the equations (4.54)
and (4.55) can be used directly (there are ways to do the numerical calculations ef   ciently). but
both approaches arrive at the same result, so there must be a very close relationship between them.
in order to show this, write the polynomial qi (x) as

qi (x)= a1i + a2i x +        + ani xn   1 and qi (xk)= a1i + a2i xk +        + ani xn   1

(4.56)
note that in (4.56), the kronecker delta is used because qi (xk) = 0 unless k = i, where qi (xi ) = 1.
the equation for qi (xk) can be written as a vector dot product

=   ik.

k

xk     ai =   ik .

in (4.57) the vector ai is formed from the n coef   cients, aik; the vector xk = {1 xk
the ith row vector of x. the two are orthogonal unless i = k, as shown in (4.57).

(4.57)
} is

       xn   1

k

102

4. linear simultaneous equation sets

for clarity, consider the 4th order problem, and the following matrix product:

   
          1 x1

1 x2
1 x3
1 x4

   
         

   
          a11

a21
a31
a41

x2
1
x2
2
x2
3
x2
4

x3
1
x3
2
x3
3
x3
4

xa =

   
          .

a12
a22
a32
a42

a13
a23
a33
a43

a14
a24
a34
a44

(4.58)

the columns of a are the coef   cients of the qi (x) polynomial. for example:

= a11 + a21x + a31x2 + a41x3

(4.59)

q1(x) = (x     x2)(x     x3)(x     x4)
(x1     x2)(x1     x3)(x1     x4)
   x2x3x4
a11 =
a21 =
a31 =
a41 =

(x1     x2)(x1     x3)(x1     x4)
(x1     x2)(x1     x3)(x1     x4)
(x1     x2)(x1     x3)(x1     x4)
(x1     x2)(x1     x3)(x1     x4)

x4x2 + x4x3 + x3x2
   (x2 + x3 + x4)

                                       
                                    

1

see footnote1.

(4.60)

(cid:19)

(cid:20)

this column vector a1 = {a11 a21 a31 a41} is orthogonal to

unless k = 1, in
which case the dot product is 1. check it out. since the other columns of a are similarly constructed,
it must be true that a is the inverse of x. then, returning to equations (4.51a) and (4.51b), the
   nal interpolation polynomial is

1 xk x2

k x3

k

(rewrite (4.51a))

p(x) = c1 + c2x + c3x2 +        + cnxn   1

where c = ay, with the elements of a determined as in equations (4.58) through (4.60).

4.6 exercises
4.1. given the 3 vectors: a1 = {   1, 2, 5}, a2 = {2,    1, 0}, and a3 = {   5, 2, 3}, expressed by their
coordinates along rectangular axes,    nd the length of each and the direction cosines of each
with respect to the coordinate system base vectors.
are these vectors linearly independent?

4.2.

find the solution to ax = c with a(3x3) and c given below, by purely vector operations.

   
       1

2    5
2
3

2    1
0
5

   
    x =

      
    .

      
    1

0
1

ax = c =

1the numerators of these equations (4.60) can be written directly. see appendix b,    polynomials,    equations (b.3) and (b.4)
describing the relationships between the roots of a polynomial and its coef   cients.

4.3. given the vectors from problem 1, form three vectors:

b1 = a2     a1, b2 = a3     a2, and b3 = a1     a3 .

are the b vectors linearly independent? is there a non trivial solution to bx = c, where b is
formed using the new b-vectors, and c is de   ned in problem 2? explain your answers.

4.6. exercises 103

4.4.

find the rank of the 3x5 matrix, m:

   
    3
1    1
1

0    1 2 5
2 0 1
2    5 2 3

   
    .

m =

4.5. with the m(3x5) matrix above

(a) determine whether or not the columns of m are independent.
(b) determine whether or not the rows of a are independent.
(c) find the solutions (if any) to mx = 0.
(d) what are the conditions necessary for mx = y to have a solution?

4.6. given the matrix, a(5x4), below   
               

   
                .

3    1
1
0
   6    7
2    3
   4    5
1    2
2
3
1
0
   1
1    4    2

(a) determine whether or not the columns of a are independent.
(b) are the rows of a independent?
(c) find a column z(5x1) that is orthogonal to all the columns of a.
(d) given a column vector y={   8, 18, 11,    8,    2}, determine whether or not ax = y is
(e) given a column vector y ={   6, 0, 13, 6,    3}, determine whether

if such a column cannot be found, explain why.

compatible. if so, solve for x.

or not ax = y is compatible. if so, solve for x.

4.7. determine which is the better    t: (a) the linear    t, or (b) the quadratic    t in the diameter

vs temperature problem.

4.8. given ax = c (a non-singular, b = a

   1), discuss the following in a    vector sense:   

104

4. linear simultaneous equation sets

(a) a columns i and j are interchanged, how is b affected? how is the solution, x, affected?
(b) a rows i and j are interchanged, how is b affected? how is the solution, x, affected?
(c) if rows i and j of the vector, c, are interchanged, how are b and x affected?

4.9. determine whether or not there are values of the    parameter for which a solution exists in

the equation set below.    

          1
3
   4
0
8       2  

2

   
   

      
    x1

x2
x3

      
    =

      
    0

0
0

      
    .

(a) how many such values exist?
(b) for each one,    nd the general solution to the set.

4.10. find the least squares best solution for c1 and c2 in the equation set below.

1.00c1 + c2 = 1.83
1.50c1 + c2 = 1.98
1.80c1 + c2 = 2.09
2.00c1 + c2 = 2.17
3.10c1 + c2 = 2.52
3.20c1 + c2 = 2.56
3.30c1 + c2 = 2.59 .

4.11. using equations (4.60) show that q1     xj =   1j where q1 is the vector formed from the

%

&

coef   cients of the q1(x) polynomial and xj =

1 xj x2

j x3

j

, the jth row of x.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = (x1x3 + x1x4 + x3x4)(x4     x1)(x4     x3)(x3     x1).

4.12. show that

|x| =

hint: note the subscript numbering in |x|. start with the x(4x4), and delete row and
column 2.

4.13. in the polynomial

(x     xj ) = x6 + c1x5 +        + cn    nd c2 and c3. describe the for-

mation of each of the coef   cients.

x3
1
x3
3
x3
4

1
1 x2
3
1 x2
4

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1 x2
6   
j=1

c h a p t e r 5

105

orthogonal transforms

introduction

5.1
this chapter will explore other uses and characteristics of the transform equation ax = y. in this new
case, however, the transform matrix will be an orthogonal one (see de   nition in chapter 1); and so,
it will not be denoted by the letter    a,    but, by some (hopefully more descriptive) letter     usually
   t,       p,    or    q.   

this chapter will be largely   conceptual,    with emphasis on three dimensional thinking.we will
be concerned with physical displacements and motions in the real world; three linear displacement
coordinates, plus angular displacement, and motion, about the three coordinates. there will not be
much extension of concept into n-space, although orthogonal transforms are certainly not limited
to 3-space. the next chapter will include some very interesting examples in n-space.

these are relatively simple concepts. but, they are of great value to the engineer, who is often
required to conceptualize in three dimensions. the transform matrix will be seen to provide an
invaluable framework for his thinking, and approach to problem solving.

5.2 orthogonal matrices and transforms
the de   nition of an orthogonal matrix is one whose transpose is equal to its inverse. then, given
the orthogonal matrix, t:

(cid:5) = t
   1
t
(cid:5) = t
(cid:5)
tt

t = i .

(5.1)

the dot product of any two columns (rows) of an orthogonal matrix is zero. the dot product of the
column (row) by itself is 1. then, the orthogonal matrix is also    orthonormal.   

the usual function of such a matrix is to describe rotation in a 2 or 3-dimensional system.
the transform equation x = ty relates the coordinates of a vector as measured in two rectangular
coordinate systems. in two dimensions, consider two coordinate sets, x and y, that are collinear
(superimposed). in this case, any vector, say r, has identical components when represented in either
the x-set, or the y-set. the transform relating coordinates in the two sets is y = ix, where x and y
are 2x1 vectors representing coordinates in the x and y sets, and the transform matrix is the 2x2
unit matrix (note that the unit matrix, i, is orthogonal). however, this case is trivial.

next, the y-set is rotated in the + direction (counterclockwise) by an amount   . now, the
coordinates of r are different in the y-set, and there is a nontrivial, orthogonal,    transformation   
between the two sets. we will de   ne this transform.

106

5. orthogonal transforms

in the x-set, the vector is described as (figure 5.1):
rx1 = rm cos(   +   )
rx2 = rm sin(   +   )

where rm is the absolute magnitude (length) of r. then:

rx1 = rm(cos    cos        sin    sin   )
rx2 = rm(cos    sin    + sin    cos   ).
since rm cos    and rm sin    are the coordinates of r in the y-set:

(cid:22)

rx1 = ry1 cos        ry2 sin   
rx2 = ry1 sin    + ry2 cos   
(cid:23)
x =

cos        sin   
cos   
sin   

or

y

or

x = ty.

(5.2)

in (5.2), since the transform represents any vector, the reference to r is omitted. this equation set

figure 5.1:

de   nes the x-set coordinates of a vector in terms of its y-set coordinates. note that the transform
matrix, t, is orthogonal. its columns (rows) are mutually perpendicular, with the dot product of zero.
furthermore, the columns and rows are normalized to unity:

matrices of the type t are the subject of this entire chapter. such transforms preserve both
linear and angular measurement. for example, in (5.2), the squared length of a vector in the x-set
can be denoted x

(cid:5)

x. since x = ty, then
x = y
(cid:5)

(cid:5)
x

(cid:5)

ty = y
(cid:5)

(cid:5)
y (since t

t = i) .

t

that is, the length is the same in either set. the very same reasoning shows that, given two unit
vectors u and v, known in the x-set as ux and vx, the cos of the angle between them is ux   vx. this
same value results when the dot product of the two is taken in the y-set.

in order to de   ne the inverse transform of (5.2), we need only transpose the t matrix:

5.2. orthogonal matrices and transforms 107

(cid:22)

y =

(cid:23)

cos   
    sin   

sin   
cos   

x

or

y = t

(cid:5)

x .

(5.3)

note that which matrix is called t, and which t

(cid:5)

, is largely a matter of choice.

so, of what use is this transform matrix? to see the answer, just consider the r vector in
motion. conceptually, we attach this vector to the y-set. its rotations are those of its coordinate set.
and, we can simply describe any rectilinear motion in this set. then, to see the total motion, we just
transform the vector back into the    inertial    (   xed) x-set.

the 3-dimensional case is a trivial extension of (5.2) and (5.3). from the figure 5.1, above,
just include the +x3 and +y3 axes coming directly upward     out of the plane of the page. especially
note that the rotation    occurs around these axes; they therefore remain collinear (and the coordinate
of any vector in this direction is measured the same in both x and y-sets). then:

   
       cos   

sin   
0

y =

   
    x .

sin   
cos   

0
0
0 1

(5.4)

equation (5.4) is the transform matrix between the inertial x-set, and a y-set, which has been rotated
by a + angle    about the x3 axis. the inverse transform is simply the transpose of the matrix in (5.4).

5.2.1 righthanded coordinates, and positive angle
one must be careful to describe a 3-dimensional coordinate set by the so called    right hand rule,   
and to de   ne positive angle in the same way. in figure 5.1, the positive x1-axis is directed toward the
right, the positive x2 axis upward (from the bottom of the page toward the top). then, the positive
x3 axis necessarily must come out of the page, toward you (the negative x3 axis is, then, directed
away from you, into the pages of the book). all of the coordinate sets constructed in this chapter will
follow this rule.

another way to see this is: curl the    ngers of your right hand from the +1-axis to the +2-axis.
then, your thumb will point in the direction of the positive 3-axis. now, do the same with the 2-axis,
toward the 3-axis. the thumb will point to the positive 1-axis. finally, assure yourself by curling the
right    ngers from the +3-axis toward the +1-axis. the thumb will now point toward positive 2-axis.
see the next section, where the vector (cross) product is discussed.

positive angle will be measured in the same sense: rotation about any positive axis will itself
be plus in the direction of the curled    ngers of the right hand     counterclockwise, when the positive
axis is in the same direction as the thumb.

these rules are very important. an incorrect sign can easily occur, and be very dif   cult to trace

to a coordinate set improperly constructed.

now, consider any orthogonal transform in which we regard the x-set as    stationary,    with
the y-set having undergone some series of rotations. in 3 dimensions, de   ne unit vectors in both

108

5. orthogonal transforms

sets, as follows: 1x, 2x, 3x are the de   ned unit vectors in the x-set, in the directions along the x1, x2,
x3 axes respectively. in the same way, de   ne the unit vectors 1y, 2y, and 3y in the y-set. then

   
    1y     1x
2y     1x
3y     1x

   
    x .

1y     2x
2y     2x
3y     2x

1y     3x
2y     3x
3y     3x

y = tx =

(5.5)

that is, the elements of t are the dot products of the respective unit vectors, as shown. in the speci   c
case of the transform (5.4), comparison of (5.5) with (5.4) shows that (see figure 5.1):

the components of 1y in
the x-set

the components of 2y in
the x-set

t11 = 1y     1x = cos   
t12 = 1y     2x = cos(90        ) = sin   
t13 = 1y     3x = cos(90) = 0
t21 = 2y     1x = cos(90 +    ) =     sin   
t22 = 2y     2x = cos   
t23 = 2y     3x = cos(90) = 0
t31 = 3y     1x = cos(90) = 0
t32 = 3y     2x = cos(90) = 0
t33 = 3y     3x = cos(0) = 1
in the above, the reference to    90    implies angular measurement in degrees     i.e., 90 degrees.
in (5.5), the    rst row dots the 1y unit vector into the x set unit vectors     each in turn. the second
row dots the 2y vector; the third row, the 3y vector, into the x-set unit vectors, in turn. if the in-
verse transform is required, then just transpose (5.5). if one cares to memorize these dot products,
the transform matrix can be written directly, rather than going through the development that pre-
cedes (5.2).these transform matrices will be found all through this chapter, so it is well to see clearly
the manner of their construction. it is very simple, but it can be    tricky,    and sign errors can result.

the components of 3y in
the x-set

5.3 example coordinate transforms
in a practical case, the    complete    transform is usually the result of a series of simple transforms
    each one being a rotation about one of the coordinate axes, with a transform equation similar
to (5.4). for example, we may start by a rotation of a y-set relative to the    xed x-set:

y = t1x

where t1 is an orthogonal matrix of the type in (5.4). next, we may have a rotation of another
coordinate set, say a z-set, relative to the y-set:

z = t2y .

then, the (   nal) combined transform, between the z- and x-sets is:

z = t2t1x = tx; t = t2t1 .

5.3. example coordinate transforms 109

both t1 and t2 are orthogonal. it is easy to show that the product, t, is also orthogonal, by
multiplying t2t1 by its transpose

(cid:5)

t = [t1

t

(cid:5)

(cid:5)
2t2t1] = i .
t

5.3.1 earth-centered coordinates
a very practical, yet simple, example is the construction of earth-centered coordinates. to de   ne
the motion of a rocket or orbiting body, the observations of position and velocity taken at a station
located at the surface of the earth must be transformed to a coordinate set located at earth center.
the example given here will be to develop the transform of a station located at longitude    and
latitude    back to an earth-centered set.

figure 5.2: earth-centerd coordinates.

it will be assume that the earth is a perfect sphere of radius, r, although this is actually not
the case     the earth radius is some 10 miles less at the poles than at the equator. the x-set will be
at earth center, with x1 pointing at the zeroth longitude. the x1x2 plane lies in the equatorial plane;
the x3 axis points from earth center toward the north pole.

an intermediate z-set is constructed at longitude   , but with zero latitude; i.e., located along
the equator. we will    rst relate the z-set to the x-set, then relate the y-set to the z-set, and,    nally,
combine the two.

looking down upon the x1x2 (equatorial) plane, the z-set has its z1 axis pointing directly
skyward, z2 points east, z3 northward.the radius of the earth is r. since these are the same conditions

110

5. orthogonal transforms

as those of equation (5.4), we can write directly:

x = t1z     x =

   
    cos        sin   

cos   

sin   
0

0
0
0 1

   
    z .

(5.6)

the z-set and x-set are not collocated. nevertheless, equation (5.6) accurately represents the angular
displacement between the two sets. now, superimpose a y-set onto the z coordinates, and then slip
the new y-set directly north, remaining at longitude,   , and keeping the y2y3 plane tangent to the
sphere. when the y-set has been slipped through an angle   , figure 5.3 can be used to develop a
transform between the two coordinate sets. note that y1 points skyward, y2 east, and y3 north. also,

figure 5.3: z-y transform.

   
    cos    0     sin   

the z2 and y2 axes continue to be parallel.
z = t2y =
   
    cos    cos        sin        sin    cos   
cos        sin    sin   
cos   

x = t1t2y = ty =

   
    y .

cos    sin   
sin   

0 1
sin    0

0
cos   

0

then, the overall transform is given by eliminating z between (5.6) and (5.7). that is

   
    y .

(5.7)

(5.8)

5.3. example coordinate transforms 111

note that    is measured eastward from zero degrees longitude to 360 degrees, not the usually given
east longitude and west longitude (wherein    is an angle between 0 and 180 degrees). in this
measure, then, points in the united states will have    values greater than 230 degrees. the latitude
is measured in the usual way, from zero degrees at the equator, to 90 degrees at the north pole.

the radius, r from earth center to the station, is given by {r, 0, 0}, measured in the y-set. we

transform r into the x-set via t in (5.8). the result is:

      
    r cos    cos   

r sin    cos   
r sin   

      
    .

rx =

these are the well known polar coordinates of the vector. note that although the y- and x-sets do not
have the same origin, vectors known in either set can be transformed to the other. more importantly,
the above vector rx must be added to position vector observations taken at the station, (y-set) and
then transformed to the x-set. for example, radar data, taken from several stations is transformed
   rst to a single station. this data de   nes, say, the instantaneous position of an orbiting body in its
local coordinates. its position relative to the inertial coordinates is rx plus the transformed position
into the x-set. that is (with t taken from (5.8)):
px= [t]py

+ rx .

(5.9)

the time derivative of (5.9) de   nes velocity. in cases wherein the rotation of the earth must be taken
into account,    becomes a time dependent variable. thus, the matrix t must be differentiated. we
will consider the differentiation of a matrix in a later section.

as a check of the transform,t, plug all the y-set unit vectors, in turn, into (5.8).the results in
each case, of course, would be the columns of t     and the direction cosines of each of the y-set unit
vectors, expressed in the x-set. for example, note that column 2 of t depends only upon   . that
does check: the unit vector {0, 1, 0}y is parallel to the x1x2 plane, and it projects onto that plane as
{cos(90 +   ), sin(90 +   ), 0}. the point is that if this same reasoning had been used at the beginning,
it would not have been necessary to develop an intermediate z-set. the transform (5.8) could be
written directly. however, the reader should try this, and note that it is not easy. the 3 dimensional
thinking required is confusing, and prone to error. in most cases, it is safer and easier to develop
such transforms in a series of simple steps.

sometimes, a rotation takes place about an axis that is not one of the coordinate axes given
in the problem. in that case, (as will be seen in the example problem, below), an intermediate set is
set up speci   cally to orient the rotation about one of its coordinate axis. to do this, it is necessary
to take the cross product of two existing vectors to generate one of the coordinate axes in the new,
rotated set. for this reason, we should    rst review this product (see also, chapter 1, section 1.2).

the    vector product,    or    cross product    of two vectors produces a vector which is orthogonal
to both of the vectors crossed. in contrast, it will be recalled that the dot product of two vectors
produces a scalar. the magnitude of the new vector is the product of the input vector magnitudes

(5.10)

(5.11)

ux    vx = ux    cross    vx    

112

5. orthogonal transforms

times the sine of the angle between them. for example, consider two vectors, u = {u1, u2, u3}, and
v = {v1, v2, v3} in a coordinate system, x. their cross product is a vector, whose elements can be
found by the    rst row    expansion    of the following determinant. this    expansion    is quite special,
however, involving the unit vectors as the    rst row elements. in this (fabricated) way, the result is a
three-dimensional vector rather than a scalar.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1x

2x

3x
u1 u2 u3
v1
v3

v2

   
   

      
    v1

v2
v3

      
    u2v3     u3v2
u3v1     u1v3
u1v2     u2v1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    
      
      
    u2v3    u3v2
    =
u3v1    u1v3
u1v2    u2v1

      
    .

      
    .

the same result can be obtained by premultiplying v by a skew symmetric matrix made from the
elements of u, as given in (5.11), below:
0    u3

u2

   
   

ux    vx = uv =

u3

   u2

0    u1
0

u1

equation (5.11) can be    read in reverse:    a matrix-vector product in which the premultiplying matrix
is skew symmetric can be interpreted as a vector cross product.

the resultant vector from (5.10) or (5.11) has to be orthogonal to both vx and ux. it is a
worthwhile exercise for the reader to prove that this is true.
note that the product (u    v) is different than (v    u). speci   cally, if rows 2 and 3 of the
determinant in (5.10) are interchanged, the determinant expansion (5.10) will yield (v    u). and,
the elements will be of reversed sign. then (v    u) is the negative of (u    v). again, the righthand
rule is handy: curl your right    ngers from +u to +v (the    ngers being parallel to the plane of u and
v), the outstretched thumb will point in the positive direction of (u    v).
with the unit vectors of a right-handed coordinate system, curl your    ngers from +1x to +2x     note
that the thumb points in the direction of +3x. the order is, of course, important. for example, if one
were to cross 2x into 1x, the result would point the 3x axis in the wrong direction. the following
equations summarize the correct results:

1x    2x = 3x
2x    3x = 1x
3x    1x = 2x .

(5.12)

5.3.2 rotation about a vector (not a coordinate axis)
consider two coordinate sets, x and y. initially, they are superimposed, but the y-set is free to rotate,
the x-set is    xed. now, enter the vector, r = {   3,   4, 5}, and    glue    its base to the origin of the y-set.
at this point, the coordinates of r are the same in both the x-set, and the y-set. now, looking down
r, from its tip toward the origin, rotate r through a positive (counterclockwise) angle   . note that
the y-set must rotate as well; however, the rotation is not in any of the coordinate planes of this set.

figure 5.4 shows the two superimposed x- and y-sets, and the r vector with a positive angular
rotation indicated. the problem that will be discussed is the construction of the transform between
the rotated y-set, and the    xed x-set.

5.3. example coordinate transforms 113

figure 5.4:

first, de   ne a coordinate set (say, w) one of whose coordinate planes lies in the plane of the
rotation (then, one of its axes will be along the vector, r). its origin is    xed to that of the x-set (i.e.,
the w-set will not rotate). somewhat arbitrarily, de   ne the unit vector along r as the 3-axis of the

w-set (i.e., w3). since the length of r is rm =    
(cid:19)    3    4 5

3w = 1

9 + 16 + 25 = 5
rm = 5

(cid:20);

   
2:
   
2 .

rm

now, construct the 1-2 plane of the w-set. the speci   c direction of each of these two axes is quite
arbitrary, but, they certainly must be orthogonal to the 3w axis. if we cross 3x into 3w the result will
be perpendicular to both 3x and 3w and it will point in the general direction of 1x (not necessary,
but easier to visualize). normalized, it   ll be the 2w axis:

(cid:19)

(cid:20)

.

4    3 0

(cid:24)

   3

rm

4
rm

3x    3w =

now, following the relations (5.12), cross 2w into 3w, to de   ne 1w. this cross product will yield the
unit vector 1w directly (already normalized):
1w = 1

(cid:20)

.

the 3 vectors just de   ned as 1w, 2w, and 3w, de   ne, in turn, the transform matrix, between the x-set
and the w-set:

(cid:25)

0

rm

normalized = 2w = 1
5
(cid:19)    3    4    5
                     

   4
   3
5
   4

rm
4
5
   3

   3

(cid:5) =

   

rm

rm

rm

   

                      .

   5

rm

0

5
rm

w = t

(cid:5)

x; where t

(5.13)

114

5. orthogonal transforms

(cid:5)

that is, the 1st row of t
is 1w, the second row is 2w, etc. to provide con   dence that we have the
transform in the right order, put {   3,   4, 5} (the coordinates of r in the x set) into (5.13). these
coordinates will transform through (5.13) to a vector in the w-set with a w3 component equal to rm,
and the w1 and w2 components equal to zero.

the inverse transform, x = tw, is also determined by simply transposing the matrix t
the 1-2 plane of the w-set is the plane of rotation. note, however, that the w-set is not rotated.
instead, we will de   ne a new z-set, originally superimposed upon the w-set, but then rotated through
the required    angle. the transform between the w and z sets can be written directly, because it is
the same as that de   ned in equation (5.4), above:

.

(cid:5)

z = qw     z =

   
    cos        sin   

cos   

sin   
0

0
0
0 1

   
    w .

now that the transforms (5.13) and (5.14) are known, we can proceed with the solution to the
problem. originally, before the rotation, the y-set and x-set are superimposed. therefore, equa-
tion (5.13) holds for the y-set as well, and since before rotation the w- and z-sets are superimposed:

y = tw = tz (before rotation) .

(5.15)
after the rotation, (5.13) still relates the x-set to the w-set because neither of them moves. more
importantly, (5.15) can still be used to relate the z-set to the y-set after rotation, because they move
together:

plugging the de   nition of (5.14) into (5.16):

y = tz (after rotation) .

and since, from (5.13), w = t

(cid:5)

and its inverse is (obviously):

y = tqw

x, the    nal transform is:
y = tqt
(cid:5)
x = tq
(cid:5)
(cid:5)

t

x

(5.19)
we have already seen that the transform of a vector, say x, is done through the premultiplication of
x by some matrix, t (y = tx). now, (5.19) implies that the rotational matrix, q, is transformed by
both pre- and post multiplication (i.e., tqt
). and this is, indeed the general case     matrices are
transformed by pre- and post multiplication by the transforming matrices. this transformation of q
produces the rotation given in q, as observed in the x- and y-sets, respectively.

y .

(cid:5)

in (5.18), if we call the overall transform matrix w, then w = tqt

. the matrix w is the
   transform    of q. the transforming matrix, t, is orthogonal. in this case, as will be discussed in a
later article, w and q are said to be related by a    congruent    transform.

(cid:5)

section 5.4, below, discusses the transformation of matrices.

(5.14)

(5.16)

(5.17)

(5.18)

5.3. example coordinate transforms 115

5.3.3 rotation about all three coordinate axes
in this section, we will develop a transform which includes rotation about all of the coordinate axes
(in three dimensions). the 3 angles of rotation will be denoted    1,    2, and    3. these have been
referred to as the    eulerian    rotations, for it was euler who showed that it is always possible to go
from any initial orientation of coordinates, to any    nal orientation, by rotations about the three axes
of the coordinate set     in a speci   c order. in the development, below, we will choose the order 3, 2,
1, somewhat arbitrarily. the angles will be referred to as    pitch,       roll,    and    yaw,    as if the axes lie
within an airframe, with the positive x2 axis pointing    ahead,    and the positive x1 axis pointing out
the right wing. the angles,    i, are de   ned as the rotations about their respective axes, xi.

figure 5.5: rotation about all 3 axes.

for clarity in the equations to follow, de   ne ci = cos   i, and si = sin   i. shown below are the

transforms around each axis, corresponding to the diagram below.

pitch (rotation about x1)

x = t1y =

   
    1
0
0 c1    s1
0

c1

s1

0

   
    .

(5.20)

note in the diagram that the positive x1 axis is out of the paper.

the    airplane coordinates    are the y-set. the fuselage still lies along the 12 axis, but it is the
y2-axis. for example, a vector {0, 1, 0} (along the axis of the aircraft     in the y-set) will have the
coordinates {0, c1, s1} in the x-set     showing a pitch upward.

116

5. orthogonal transforms

roll (rotation about x2)

x = t2y =

   
    c2
   s2

0
0 1

s2
0
0 c2

   
    y

(5.21)

again, the airplane coordinates are the y-set. the positive x2 axis is up, out of the paper.

yaw (rotation about x3)

x = t3y =

   
    c3    s3

s3
0

0
0
0 1

c3

   
    y

if rotations are taken in 3, 2, 1 order, then x = t3t2t1y = ty, where t is given in (5.23):

   
    c2c3 s1s2c3     c1s3 s1s3 + c1s2c3
c2s3 c1c3 + s1s2s3 c1s2s3     s1c3
   s2

c1c2

s1c2

   
    .

t =

(5.22)

(5.23)

it is to be noted, here, that the order of this product is important in that the    nal result is different for
any different order. for example, if an aircraft rolls 90 degrees, and then pitches    up    by 90 degrees,
the result is quite different than if it had pitched up 90 degrees, and then rolled. in the order given
here, yaw is    rst, then roll, then pitch.
to make equations easier to read, the    shorthand,    cj = cos   j     and sj = sin   j , is used

above. this kind of shorthand will be used throughout this book.

solar angles

5.3.4
a solar panel converts the radiant energy from the sun to an electrical output. the output is pro-
portional to the area of the panel exposed to the sun   s rays (the    effective area   ). the diagram below
shows a single square foot of the panel surface. the lower half (plain view) shows this square area
from above; the upper half shows an edge-view of the same area. if the sun is directly above that
surface, the entire square foot is exposed as in the lower half, but when the sun   s rays are at an angle,
(cid:5)
one of the dimensions of the area reduces (compare the length d (= 1 ft) to the length d
in the
diagram, above). the effective area is proportional to the ratio of these dimensions. in numerical
terms, that ratio, cf , is equal to the trigonometric cosine of the    angle of incidence,    i, between the

5.3. example coordinate transforms 117

sun ray and the panel normal,   n. to constrain i to angles between plus and minus 90
vector    is perceived as the vector from the panel toward the sun (the negative of that shown).

   

, the    sun

in order to calculate cf, a unit    sun vector    and the unit    panel vector    must be calculated.
the dot product of these two unit vectors yields the required cosine of the angle of incidence. both
of these vectors must be de   ned in the same coordinate set. that set might be de   ned at the surface
of the solar panel or elsewhere (possibly at earth center). because the transforms between sets will be
orthogonal, any convenient set will produce the same results (i.e., angle measurement is preserved).
there are two rotations involved. first, the earth orbits about the sun. a coordinate set at the
earth center, the o-set, can be used to describe this motion, and de   ne the sun vector. second, the
earth   s rotation about its axis requires a second set (the e-set), one of whose coordinate axes collinear
with the earth   s axis.

the o-set: arbitrarily, make the o3 axis orthogonal to the orbit plane with +o3 pointing to
celestial north, the o1o2 plane in the orbital plane, and the o1 axis directed toward the sun. the
coordinates of the sun vector in this set are then{1, 0, 0}. see figure 5.6.

figure 5.6: earth orbit in the o1, o2 plane.

118

5. orthogonal transforms

the o-set is inertial (   xed in space), with the orbit rotation simulated by varying angle   .
   
when    is 0, it is the march (spring) equinox, when    = 90
the earth axis is tilting directly toward
the sun along o1     the summer solstice (about june 20). from figure 5.6, the o3 coordinate of the
earth   s axis is cos(   ), written c   . its projection on the o1o2 plane is s   . then sin(  ) is equal to the
o1 component of the unit vector r divided by s   , and cos(   ) is equal to the o2 component divided
by s   . then the unit vector earth axis has the components {s   s  , s   c  , c   }. note that s   s      
sin(   ) sin(  ); as before, the trigonometric functions are given by their    rst character, capitalized. the
angle    is the (constant) 23.5

tilt of the earth axis.

   

the e-set: rotation of the earth about its axis is de   ned in the e-set, {e1, e2, e3}. choose e3
to be collinear with the earth axis; then its e1e2 plane will be the equatorial plane. the e3 unit vector
has the same o-set coordinates de   ned above: {s   s  , s   c  , c   }.
now, cross e3    o3 to de   ne e1. the result, {s   c  ,    s   s  , 0}, is a vector orthogonal to e3
and so must lie in the equatorial plane as required. it must be normalized to unit length, yielding the
e1 coordinates in the o-set: {c  ,    s  , 0}. since e1 is also orthogonal to o3, it is in the earth orbit
plane as well as the equatorial plane.
finally, the e2 axis is de   ned by crossing e3    e1 = {c   s  , c   c  ,    s   }, a unit vector.
this completes the de   nition of the e-set in terms of the o-set coordinates. using the results of
equation (5.5), the transform relating these sets is

   
   

e = t1o =

   s  

c  

0
c   s   c   c      s  

   
    o .

(5.24)

s   s  

s   c  

c  

note that the three vectors just de   ned are used as the rows of the transform matrix t1. also, the
   
e-set is de   ned solely by    and   . the value of    is (0   360
) depending on a    day number,    chosen
   
(0   364). on day 0,    = 0, on day 92    is approximately 90
.
since the sun vector (say,   s) has the coordinates {1, 0, 0} in the o-set, the    rst column of t1

gives the coordinates of the sun vector in the e-set: {c  , c   s  , s   s  }.

sun latitude: the e3 sun vector coordinate, s   s   , is the cosine of the angle between the e3

axis and the sun vector (the o1 axis). this de   nes the    sun latitude,      s:

  s =   
2

    arccos(sin    sin    ) .

   
since    is constant, 23.5
   
increases to 23.5
  s becomes negative, as    increases from 180 to 360.

,   s is a function of   . when    = 0,    = 0; as    increases to 180,   s
, then drops back to zero. during the winter months in the northern hemisphere,

during a day, the earth rotates 360

while moving in its orbit less than a degree. then during
this 24-hour period, consider the earth orbit position as    xed (i.e.,    constant), making   s constant,
and the same for all longitudes. then the longitude of the sun collector (the panel) is arbitrary.
, in line with the e1 axis at    solar noon.    movement
of the vector   s simulates time   the passing of the sun across the sky. values of   s > 0 corresponds
to times before noon,   s < 0 afternoon times.

   
figure 5.7, shows the panel longitude at 0

   

5.3. example coordinate transforms 119

figure 5.7: earth rotation simulated by moving   s through an angle    s.

in this e-set, the projection of   s onto the e1e2 plane has the coordinates {c  , s  , 0}. then,

the coordinates of the sun vector in the set are {c   c  s, s   c  s, s  s}.

the x-set: an additional coordinate set is required in which to de   ne the    panel vector    (the
normal to the solar panel surface). refer to figure 5.2 used in the construction of earth-centered
coordinates. in this case, the x-set is at the solar panel, the e-set is earth centered. equation (5.8) can
be used directly, changing only the names of the coordinate sets, and setting    = 0. as in figure 5.2,
the angle    is the latitude of the panel.

   
    c   0    s  
   
    c   0
s  
0
   s   0 c  

0 1
s   0

0
c  

0 1

   
    x ; or x =
      
   
   
    c  s c  s

s  s c  s
s  s

0 1

   
    e and

   
    c   0
s  
0
   s   0 c  
      
      
    c   c  s c  s + s   s  s
    =
c   s  s     s   c  s c  s

s  s c  s

e = t2x =

sx =

      
    .

(5.25)

(5.26)

where

   is the latitude of the sun panel.
  s is the    sun latitude,     s =   
  s is the sun movement simulating earth rotation (see figure 5.7).

    arccos(s   s  )

2

on any given day, determined by the value of   , the only variable in this equation is   s. the latitude
of the panel is, of course, constant; the sun latitude is assumed constant. the next succeeding day is
set by incrementing    by 360/365.25 degrees.

panel vector
the x-set has its x1 axis pointing straight upward along a radius of the earth, its x2x3 plane is tangent
to the earth surface (see the y-set in figure 5.2). the +x2 axis points east, +x3 north.
figure 5.8 is very similar to figure 5.7.the panel normal,   p, is de   ned in terms of its    azimuth
and elevation        the angles   p and   p, respectively. if the panel were laying on the ground the

120

5. orthogonal transforms

figure 5.8: solar panel normal,   p.

normal would be collinear with x1. now, just move the panel vector to the desired angles   p and   p.
in this diagram, the projection of the normal onto the x2x3 plane has the length cos  p, and the x1
component is sin  p. the panel vector, then, is:

px = {s  p, s  pc  p,    c  pc  p} .

(5.27)

with both the sun vector and the panel vector de   ned, the cosine factor, cf , is px     sx.

in residential applications, the two panel angles are often dictated by the roof of the building,
its pitch angle and its orientation from south. in industrial applications (on a    at roof ) the panel is
movable and able to    track    the sun.

appendix d contains a discussion of the use of these equations (5.26) and (5.27) in deter-

mining    solar energy geometric effects.   

image rotation in computer graphics

5.3.5
computer graphics work has excellent use for the matrix t in (5.23). consider a graphic (picture)
consisting of an    assemblage of points,    pn, in a three-dimensional space. the position of each
point is known in the y-set by its three coordinates. certain of the points are to be connected on
the monitor by (usually straight) lines forming the image seen by the user. the computer must
   remember    not only the 3-coordinate positions of the points, but also which ones are connected.
these points, together with their interconnections, may represent the (transparent) drawing of a
machine part, or an entire machine.

the computer user often must be shown different views of the object being represented. so
the graphics program must provide means by which the points appear to rotate about any of the
three axes through the object. of course, the display can only draw two coordinates onto the plane of
the screen, but, the user must be given the perspective of three dimensions. the screen coordinates
are clearly inertial (   xed). they can be chosen as any two of the 3 x-set coordinates     say x1, and x2.
usually, the + x1 axis is from left to right along the top of the screen, and + x2 is from top toward the

5.4. congruent and similarity matrix transforms 121

bottom of the display. the non-inertial y-set is located at the centroid of the object, and probably
at the center of the screen. in this case, then, the y-set is offset from the upper left corner of the
screen, to its center, by the amounts h0 (horizontal offset), and v0 (vertical offset). these offsets are
1/2 the horizontal and vertical pixels of resolution of the screen.

when the command to rotate the object is given, the program uses equations like (5.23) to
reposition the points and project them into computer screen coordinates. when the image is next
displayed, the same points (in their new positions) are interconnected by lines, and the image will
appear to have rotated by the given angles.

if the image is required to appear to move, dynamically, the rotations must then be taken in
incremental fashion. at each increment, the image must be erased, then rotated again, and redis-
played     rapidly enough to give the impression of rotational motion at the screen. if the drawing is
complicated, there will be many points, pn. since a vector multiplication is required for each point,
plus reconnection of the points by lines, it can be seen that the computer must have a very large main
memory, and be capable of high speed arithmetic (      oating point   ) operations. it has only been in
recent years that such computers have been generally available.

computer graphics software has become very complex. the above discussion omits all of the
drawing part, the interaction with the user     virtually all of the very dif   cult problems. but, the
transform matrix (5.23) is one of the many tools that make sophisticated graphics possible.

5.4 congruent and similarity matrix transforms

earlier paragraphs have shown that a vector     a mathematical, and possibly physical, entity    
can be viewed from different frames of reference, different coordinate sets. there is no particular
signi   cance to any given    frame,    and we can easily erect a different one to afford a better perspective.
this is especially true for orthogonal reference frames which retain the vector length. the vector
transforms as easily as a single matrix-times-vector product.

the same can be said of a matrix, and functions of matrices. a matrix may be viewed from a
given reference set, or it can be transformed, along with the vectors upon which it may be operating,
to a new set affording a more convenient view. it is of interest to see how a matrix is transformed.
consider again the vector equation ax= b. the coordinates in which a, x, and b are described
are quite arbitrary.then, it may become necessary to transform these vectors using a (general) matrix
p. the transform need not be an orthogonal one, so consider that p is a nonsingular matrix whose
inverse is p

   1. using p, we obtain:

x = p  x

and

b = p  b

(5.28)

in which   x refers to the transformed vector x, and   b refers to the transformed b. of course, our main
interest is in the original matrix equation, and how b is obtained from x. upon substitution of the
transform into the original equation ax = b:
ap  x= p  b

   1ap   x =   b .

or p

(5.29)

122

5. orthogonal transforms

in the second equation of (5.29) the matrix a is transformed to the new coordinates by combined
pre- and post-multiplication. the transform of a is:

  a = p

   1ap .

(5.30)

the two matrices, a and   a are said to be    similar    matrices, and the transform is called a    similarity
transform.    since p and its inverse have reciprocal determinants, then (5.30) shows that a and   a
have the same determinant (i.e., |a| = |   a|).

now it will be shown that algebraic functions of a are transformed in the manner of (5.30),

and thus, these functions are invariant under similarity transforms.

matrix product
the product is transformed

  a   b     p

   1app

   1bp = p

   1(ab)p .

matrix addition/subtraction  a      b     p

   1ap    p

   1bp = p

   1(a    b)p .

matrix inversion

given that   a =p

   1ap, then by the inversion of a product rule:   a

   1 = p

   1a

   1p.

then, all these operations transform just as a itself transforms     these operations remain

invariant under similarity transformation.

matrix transposition
this case is somewhat different.

given   a = p

   1ap, by transposition of a product:   a

.this is not the same as the
transformation of a unless p is orthogonal. if the matrix, p, is not orthogonal then the operation
of transposition is not invariant under transformation.

a

(cid:5) = p

   1](cid:5)

(cid:5)[p

(cid:5)

three out of four isn   t bad. functions of matrices which involve addition/subtraction, multi-

plication, and inversion, remain invariant under similarity transformation:
   1,

   1)     f (

f (a, b, c, , , a

  c, , ,

   1, c

   1, b

  a,

  b,

  a

  b

   1,

  c

   1) .

that is, a given function of matrices    implies    the same function of the same matrices, transformed
to some new coordinate system by a similarity transform, as long as the function includes just those
operations which passed the above test. for example, a given polynomial in a:
c0an + c1an   1 + c2an   2 +        + cn   1a + cni = 0

implies the same polynomial, with the same coef   cients, in the transformed matrix   a.

5.5. differentiation of matrices, angular velocity 123

if the transforming matrix is orthogonal, the transform is called    congruent,    and as described
(cid:5)
earlier, the invariant functions will include transposition. further, if a -a
= 0, the matrix is sym-
metric. since the subtraction is invariant under congruent transformation then symmetric matrices
remain symmetric under such transformation.

5.5 differentiation of matrices, angular

velocity

the objectives of this section will be to de   ne the derivative of a matrix whose elements are variable
functions, and then to use this de   nition in the development of the angular velocity matrix. of
course, angular velocity is a vector quantity. it was shown earlier that the vector cross product can be
affected by the product of a 3x3 skew-symmetric matrix elements times a 3x1 vector. in fact, this
is just how the angular velocity vector emerges in this development.

suppose the elements of the matrix a are functions of a scalar variable, t. then:

a(t ) = [aij (t )] .

now, if t is incremented by dt, note that each element of a is incremented     that is

a(t + dt ) = [aij (t + dt )] .
(cid:23)

(cid:22)

then, if the original a matrix is subtracted, the result divided by dt, and the limit taken as dt
approaches zero, we see that the overall result is
[a(t )] =

(5.31)

daij (t )

d

.

dt

dt

that is, the differentiation of a is accomplished by differentiating each element of a. now, con-
sidering the variable to be time, t, we denote the time derivative as

a(t )       a     at .

d

dt

(5.32)

notice the unusual notation at for the derivative of a. we can de   ne the following derivatives:

[a + b] =   a +   b = at + bt and
[ab] =   ab +   ab = atb + abt .

d

dt
d

dt

(5.33)

(5.34)

the results (5.33) and (5.34) are just like their scalar counterparts. however, in (5.34), the original
product order, ab, must be maintained in the derivative of the product. of course, if more than two
matrices are involved in the product then

[abc]t = at[bc] + a[bc]t = atbc + abtc + abct

124

5. orthogonal transforms

and again the order of the product is maintained.

the derivative of a

   1 can be found by noting that

   1 = i
aa
   1 + a   a
  aa
  a
   1 =    a

then
   1 = 0
   1   aa
   1 .

5.5.1 velocity of a point on a wheel
a point, p, rides on the periphery of a wheel (or disk), as shown in figure 5.9. the axis of the wheel
is attached to a shaft (in the plane of the paper) which is also capable of rotation.

as in previous examples, intermediate coordinate sets are used, with each one describing one

angular displacement (and velocity) about one of its axes. in this case, the angles are   2 and   3.

figure 5.9:

an inertial (   xed) x-set is set up at the point    o    in the    gure, with axes as shown (the +x3
axis is up, out of the paper). a y-set is constructed, also at point o, which rotates about its y2 axis
(collinear with x2). this rotation angle is denoted   2. lastly, a z-set is constructed at point o, which
rotates with angle   3 about the y3, z3 axes.

as observed in the z-set, the point p is    xed, with coordinates {rp, 0, 0}; and note that the
point p does remain at a constant distance from point o   equal to the radius of the disk. that is,
all the motion is angular rotation.

to    nd the velocity of the point p, the vector rx must    rst be found. its time derivative is the
velocity of p. to    nd rx, vector rz is transformed from the z-set to the x-set. the two transforms

5.5. differentiation of matrices, angular velocity 125

are t2 and t3.

where

x = t2y
y = t3z
x = t2t3z

then
and

z = t
(cid:5)
(cid:5)
3t
2x

   
    c  2
   s  2

0
0 1

s  2
0
0 c  2

   
    c  3    s  3

c  3

s  3
0

0
0
0 1

t2 =

t3 =

(5.35)
(5.36)
(5.37)

(5.38)

(5.39)

   
   

   
   

in (5.38) and (5.39),    c    is to be read as    cos,    and    s    as    sin;    for example: s  2 = sin   2.

de   ne the vector from the center of rotation to the point p as r. then rz is the vector r as
seen in the z-set, rx is the same vector, seen in the inertial x-set. in order to derive the velocity of p,
we must differentiate rx     in the inertial x-set system that can    see    all the motion. from (5.37):

rx = t2t3rz .

the vector rz is simply {r, 0, 0}. then, de   ning v as the velocity of point p:

vx =   rx =

(cid:30)

  t3

(cid:29)   t2t3 + t2
(cid:29)   t2t

(cid:5)
2

(cid:5)

t3

t2

(cid:5)

rx =

+ t2(

in (5.40), we can eliminate rz through the use of (5.37):

(cid:29)   t2t3 + t2

  t3

(cid:30)

vx =   rx =

rz .

(cid:30)

  t3t

(cid:5)
(cid:5)
3)t
2

(5.40)

rx = wxrx .

(5.41)

126

5. orthogonal transforms

in (5.41) the two important products are:

   
       s  2
0
c  2
0 0
0
   c  2
0    s  2
   
       s  3    c  3
0
c  3    s  3
0
0 0

0

   
   
   
   

0    s  2
0
c  2

   
    c  2
   
    c  3
   s  3 c  3

0 1
0

s  2

s  3

0
0
0 1

0

   
    =
   
    =

   
   
       2
   
    0        3
    3

0 0
0 0
0

0

    2

0
0

0
0 0
0 0

   
    and
   
   

  t2t2

(cid:5) =     2

  t3t3

(cid:5) =     3

where, again, s means sine (e.g., s  2 = sin   2), and c means cosine.

(cid:5)

  tj tj
axis, then   tj tj

it should be clear that the elements of angular velocity are emerging in the products of these
(cid:5)
matrices. that is, if tj is the transform matrix de   ning rotation about the jth (inertial)
provides the jth component of angular velocity. also, in (5.41), note that the
components of rotation about the 3-axis must be transformed back to the inertial x-set, while the
rotation about the 2-axis is already described in the x-set, and need not be transformed. note again
that the transform af a matrix is accomplished by pre- and postmultiplying matrices. speci   cally, in

the wx matrix, the components,   t3t3

(cid:5)

, must be transformed, while those from   t2t2

(cid:5)

angular velocity matrices which    emerge    in this way are always    skew symmetric.    that is

wx =   t2t2
   
   

(cid:5) =

w =    w

(cid:5) + t2(

  t3t3

(cid:5)

(cid:5)

.

)t2

   
    .

0      3

  2

0      1
0

  1

  3

     2

do not.

(5.42)

(5.43)

in the general case, with the transform t = t1t2t3, (rotation about all three coordinate axes) the
angular velocity matrix would be:

wx =   t1t1

(cid:5) + t1(

  t2t2

(cid:5)

(cid:5) + t1t2(

)t1

  t3t3

(cid:5)

(cid:5)

(cid:5)

.

t1

)t2

and, again note the transformation of the 2-axis and 3-axis angular velocity components.

in the example problem of figure 5.9, multiplying the terms out in (5.42)
    2
    3

wx =   t2t2

(cid:5) + t2(

(cid:5) =

)t2

c  2

0    c  2
    3
       2

s  2

    3
0    s  2
    3

0

   
   

   
    .

therefore, the angular velocity (vector quantity) for the problem is:

(cid:5)

  t3t3
      
      1

  2
  3

      x =

      
    =

      
   

      
    .

    3 sin   2    2
    3 cos   2

(5.44)

(5.45)

5.6. dynamics of a particle 127

in hindsight, the angular velocity,       , could be calculated in vector form. the z-set    sees   
no rotation, the y-set    sees    the vector       3 = {0, 0,     3} about its 3-axis, and the x-set    sees     2 =
{0,     2, 0}. then instead of transforming matrices, the simpler vector would do:
      
   
   
    0
0    3

   
    c  2
   s  2

      
    + t2

      
    0
0    3

      
    0    2

      
    0    2

s  2
0
0 c  2

      
    =

      
    +

      x =

0
0 1

      
   

(5.46)

0

0

which clearly has the same result. in the general case, with the transform t = t1t2t3, the angular
velocity vector would be:

(5.47)
the terms       j are vectors with non-zero element values only at the jth element.the angular velocity
matrix, w, can then be written by simply putting the elements from (5.47) into their proper places
in a skew-symmetric matrix.

       =       1 + t1      2 + t1t2      3 .

returning then to (5.41), the velocity of the point p is
vx = wrx = (      x   rx ) .

(5.48)

the velocity of p is equal to the cross product of the total angular velocity times the vector, r, both
expressed in the inertial x-set.this result is certainly obvious.but,the importance of the development
is the introduction of angular velocity as a skew symmetric matrix quantity that emerges in the form
. furthermore, the development leaves no uncertainty as to the correct vector quantities to
be cross multiplied; and for this reason it is more than just an introduction. the next section will
continue with the same matrix and vector quantities.

  tt

(cid:5)

5.6 dynamics of a particle
in the study of classical mechanics, the velocity and acceleration of a particle in motion are developed
as vector entities. the development is troublesome because part of the motion is described in a
moving coordinate system. in the classic vector development some of the terms in these equations
mysteriously appear as    correction terms.    using the insight gained through matrix manipulation,
and speci   cally, the angular velocity matrix, we will develop the equations directly, and watch the
   correction terms    as they appear.

in figure 5.10, the position of the point p is determined by the vector r in a non-inertial
y-set. the position of the y-set is determined by the vector r and angular motion between the
coordinate sets is measured in the transformation matrix, t. we will determine the absolute velocity
and acceleration of the point as vector equations, and identify, in a matrix sense, each of the terms.

from the    gure:

(5.49)
and the subscripts, x, are the reminder that to derive a true (   absolute   ) velocity we must differentiate
in the x-set. the transform between coordinate sets is x = ty. speci   cally, note that r is known in

      x = rx + rx

128

5. orthogonal transforms

figure 5.10: particle dynamics.

the y-set and must be transformed as rx = try in (5.49). then, by differentiation:

    x =   rx + t  ry+   try
    x =   rx + t  ry +   tt
    x =   rx+

(cid:19)  ry

(cid:20)

(cid:5)

rx =   rx +   try + (      x   rx ) .

(cid:5)

x + (          rx )

(5.50)
(5.51)
(cid:19)  ry
(cid:20)
which is the absolute velocity of the point p, expressed in the x-set. the (    r) term emerges the
same way that it did in the last section:   tt
x is
used to emphasize that the derivative of ry is taken, and the results then transformed to the x-set
(not the derivative of rx). the derivative of ry is usually referred to as the    apparent velocity.    it is
the velocity that would be measured without any knowledge that the y-set is not inertial. note that
this matrix development is straightforward and leaves no question as to which coordinate set the
vectors are to be de   ned in.
often, it is desired to express the absolute velocity in the non-inertial set. this can be done
by simply transforming     x to the y-set. however, it is interesting to start back at the    rst of (5.50),
and to consider the transform of the angular velocity:

is w, and note that w is wx. the notation

wy = t
(cid:5)
wxt
but wx =   tt
(cid:5)
(cid:5)   tt
then wy = t

(cid:5)

t = t

(cid:5)   t .

now, returning to (5.50)

{     x}y
{     x}y
{     x}y

(cid:5)   rx +   ry+t

(cid:5)   try
= t
= {   rx}y +   ry + wyry
= {   rx}y +   ry + (      y    ry )

(5.52)

(5.53)

(5.54)

which is the absolute velocity of p, expressed in the y-set. note the similarity to (5.51).

5.6. dynamics of a particle 129

now, for the acceleration, we must differentiate the    rst (5.50) equation:

    x =   rx + t  ry +   t  ry +   t  ry+   try
    x =   rx + t  ry + 2   tt
(cid:5){  ry}x+   tt
(cid:5)
rx
    x =   rx + {  ry}x + 2[wx]{  ry}x +   tt

(cid:5)

rx .

(5.55)

in (5.55), note the new correction term consisting of the angular velocity crossed into the    apparent
(cid:5) = wx . by

velocity    transformed to the x-set. but, to interpret further, consider the de   nition   tt

differentiation:

[   tt

(cid:5)] =   tt

(cid:5) +   t   t

(cid:5) =   wx; then   tt

(cid:5) =   wx       t   t

(cid:5)

.

d

dt

since the angular velocity matrix is skew symmetric:

  tt
then w2
  tt
   nally

x

(cid:5) = wx =    w
(cid:5)
= [   tt
(cid:5)][   t   t
(cid:5) =   wx + w2

x .

x

=    t   t
(cid:5)
(cid:5)] =       t   t

(cid:5)

then, taking this back to (5.55):

    x =   rx + {  ry}x + 2[wx]{  ry}x +   tt
    x =   rx + {  ry}x + 2[wx]{  ry}x + [   wx + w2
        x

=   rx + {  ry}x + 2(      x    {  ry}x ) + (        x    rx ) +       x    (      x    rx )

]rx

rx

(cid:5)

x

(5.56)
(5.57)

both (5.56) and (5.57) show the    nal result for the absolute acceleration. the vector form, (5.57),
is the form most often seen. note that w2r = wwr is simply the cross product of a cross product,
shown as the    nal term of (5.57).the absolute acceleration, then, has three cross product    correction
terms.    this also shows that when parts of the total motion of the particle are described in a non-
inertial coordinate set, the equations of motion can become somewhat complicated.

this absolute acceleration is transformed to the y-set in the same way that velocity is trans-

formed. this time, however, we must transform w2.
wxt][t

xt = [t
w2

t

(cid:5)

(cid:5)

(cid:5)

wxt] = w2

y

(5.58)

which shows that the square of wx (or, in fact, any integer power of wx ) transforms just like wx.
then, basically we must transform the equation:

    x =   rx + t  ry + 2[wx]{  ry}x + [   wx + w2

]rx .

x

the transformation is accomplished by premultiplying the above equation by t

{     x}y = {   rx}y +   ry + 2t
{     x}y = {   rx}y +   ry + 2[wy]{  ry} + [   wy + w2
{        x

}y = {   rx}y +   ry + 2(      y      ry ) + (        y    ry ) +       y    (      y    ry ) .

(cid:5)[   wx + w2
]ry

wxt  ry + t

]try

(cid:5)

x

y

(5.59)
(5.60)

(cid:5)

. then:

130

5. orthogonal transforms

    

  r
  ry
2(        r)

(         r)
      (      r)

the absolute acceleration of the particle, as found in an iner-
tial coordinate system, although the quantity can be expressed
in (transformed to) any set.
the absolute acceleration of the origin of the non-inertial set
relative to the inertial set.
the apparent acceleration of the point p, as measured in the
non-inertial coordinate set.
the compound acceleration of coriolis; a correction term
that must be applied whenever there is angular motion and
apparent velocity, simultaneously.
the correction term that relates the acceleration of the point
to a change in the angular velocity.
the well known centripetal acceleration, in the amount of
w2 times the radius, r.

and, note again the similarity of these equations to (5.56) and (5.57). because of this simi-
larity, they can be discussed in general terms     being speci   c about the coordinate set only when it
is important (for example, in the discussion of apparent acceleration).

5.7 rigid body dynamics

the analysis of rigid body dynamics follows from that of a single particle in that the body is perceived
as an aggregate of particles.the dynamics of one chosen particle is examined, and then a summation
is made to include all such particles.

in the diagram, a rigid body is indicated by the wavy, closed, line. a chosen, ith, particle is
(cid:5)
located in an inertial set by the vector   . the center, o
of a non-inertial z-set is located by the
vector d. within the z-set, the vector ri locates the particle. as in the previous section, both sets are

figure 5.11:

required because the z-set is often the one in which the particle is observed, but the x-set is required
in which to do the differentiation necessary in the use of newton   s second law. since the ith is just
one of the particles, the vectors r and    must be given subscripts:

5.7. rigid body dynamics 131

  i = d + ri .

using newton   s second law for the ith particle:

fi = mi     i .

where

fi is the total force applied to the particle
mi is the mass of the particle, and
    i is its absolute acceleration.

the force on the particle is the result of both internal forces, f ij , from the adjoining particles (two
subscripts), and the external applied force, f i (one subscript). then fi = fij + fi.
making the substitutions for fi and for     i,the equation of equilibrium is obtained by summing

over all the particles in the system (rigid body):
fi =

fij +

(cid:21)

(cid:21)

(cid:21)

i

i

i

  d +   ri ) .

mi (

(5.63)

since the particles do not move relative to one another within the rigid body, each f ij must be
accompanied by an equal but opposite force, f j i. then the sum of forces f ij must be zero. the sum
of f i is simply the external force vector, f on the body. also, in (5.63), the vector d is independent
of which particle is chosen, so the sum is just that of the particle masses:

f = m

  d +

mi  ri where m is the total mass of the rigid body.

(cid:21)

i

an important simpli   cation results if the mass points are located in relation to the center of gravity
of the body. this is accomplished in the    gure by rede   ning ri:

ri = rc + (ri )c .
(cid:21)
(cid:21)

(cid:5)

in (5.65), rc is a    xed vector from the origin o
emanates from the cg and terminates at the ith mass point. then
mi (  ri )c .

mi  rc +

mi  ri =

(cid:21)

i

i

i

by de   nition of the center of gravity, the last term in (5.66) is zero. thus,

f = m

  d + m  rc = m(

  d +   rc) .

to the center of gravity, cg. the new vector (ri )c

the motion of translation can be determined by treating the rigid body as a single particle, with the
total mass located at its center of gravity.

(5.61)

(5.62)

(5.64)

(5.65)

(5.66)

(5.67)

132

5. orthogonal transforms

5.7.1 rotation of a rigid body
the analysis of the rotation of the rigid body is more complex than that of its translational motion.
however, it is also based on newton   s laws. it will be shown that an external    torque    produces a
change in    angular momentum    in the same way that the external force produces a change in linear
momentum.

first, two important assertions are discussed, that will provide    physical picture    of a rigid

body rotating with angular velocity   .

1. all lines within the body rotate at the angular velocity equal to   . this    rst point is intuitive,

since (in the    gure), it is clear that rotations of oa and ob are both equal to   .

2. the complete angular velocity,   , of the rigid body, can be visualized as occurring about any

arbitrarily chosen point. this assertion is not obvious.

the    gure below shows an arbitrarily chosen line, ab within the rigid body. at the instant shown,
the center of rotation is at the point o. these three points form the triangle oab. the velocities of
a and b are

va = a  ; vb = b  .

the component of velocity along the line ab must be the same for both points a and b (because
the body is rigid).

then va sin    = vb sin    , or a  a sin    = b  b sin   .
= b
but using the law of sines:
, note that
sin   
a sin    = b sin    .

sin   

a

then   a =   b. that is, the arbitrarily chosen line, ab rotates at the angular velocity,   : all lines
within the rigid body rotate at the same angular velocity, assertion (1).

5.7. rigid body dynamics 133

the velocity of b relative to a equals c  ab = vb cos        va cos    = b   cos        a   cos   .

now, consider the point a as the    apparent center of rotation,    and the velocity of b about a.
again from the law of sines, c = a

, and b = a

. then

sin   
sin   

sin   
sin   

a  ba

= a  

sin   
sin   

cos        a   cos   ; and note that    =          .
a  ab sin(         ) = a  (sin    cos        cos    sin   ) = a   sin(         ) .

sin   
sin   

then   ab =   . thus, (assertion 2), point b rotates about a with the total angular velocity of the
rigid body.
the total rotational motion of a rigid body can be considered to occur about any convenient point. the
complete motion of the body is then the sum of the translational motion of this point plus the rotation
around it. in the usual case, if the point chosen is the cg, the equations of both translation and
rotation are simpli   ed.

5.7.2 moment of momentum
in figure 5.11, the momentum of the ith particle is given by mivi. newton   s second law states that
the time rate of change of this momentum is equal to the net force acting upon it. equation (5.62)
is rewritten:

(5.68)
(cid:5)
the momentum, mivi, of the ith particle (figure 5.11) produces a moment about the point o
de   ned as its moment of momentum.    its value is determined as the cross product:

(mivi ) .

dt

,

fi = d

moment of momentum     hi = ri    mivi .

(5.69)

the    angular momentum    (or    moment of momentum   ) of the rigid body is the sum of the moments
of all the particles within the body

(cid:21)

i

h =
(cid:21)

miri    vi .
(cid:21)

h =

miri      d +

miri      ri

i

i

note that the moment of momentum/angular momentum, h, is a vector quantity. the velocity is
vi =     i =   d +   ri. then

(5.70)

and this is the general expression for angular momentum, in terms of the inertial coordinate set. the
   rst term in (5.70) will vanish if:

!

miri = 0; or

1. the origin of the inertial set is at the cg of the rigid body,

2. the origin of the non-inertial set is    xed,   d = 0.

i

134

5. orthogonal transforms

in either of these cases, this term vanishes. further, since the motion is rotational,   ri =           ri

by expressing (5.71) in matrix terms (and noting that           r =    r          ):

(cid:21)

i

h =
(cid:21)

miri              ri .
(cid:21)

h =

miriwri =    

mir2

i       

(5.71)

(5.72)

i

i

in which ri is the skew-symmetric matrix of the ri coordinates, and w is the skew-symmetric
matrix of the    coordinates.
in (5.68), note that the cross of ri into fi produces a torque, t = ri    fi = ri    (fij + fi ). for
the same reason that the internal forces cancel when summed over all particles, their contribution to
torque also cancels. the result is that the torque is simply the moment of the external forces applied
to the rigid body. then the torque required to produce a change in the angular momentum of a rigid
body is

(cid:31)(cid:21)
t = r    f = d
t = d

dt

dt

i

 

h, see footnote1
=     d

miriwri

(cid:31)(cid:21)

 

mir2

i       

.

(5.73)

dt

i

although (5.73) correctly expresses the torque in terms of angular momentum, it is not in a form
that is useful.

5.7.3 the inertia matrix
the problem in (5.73) is with ri, the skew symmetric matrix formed from the coordinates of the
vector, ri.

   
   

0    r3

r2

0    r1
0

r1

r3

   r2

   
   

.

(i)

(5.74)

ri = {r1, r2, r3}i     ri =

the subscript, i, has been omitted from the terms within ri, but it must be remembered that there
is a different ri for each particle. the problem, however, is that the ri components vary as the rigid
body turns relative to the inertial axes. this can be remedied by expressing these terms in the non-
inertial set     at the expense of a somewhat more complicated angular velocity, whose direction and
magnitude may change with the motion. it will be worth it.
the transform between the inertial x set and the moving z set is the orthogonal matrix, t.
for the vectors involved we write vx = tvz, and note that the matrices, (r and w) transform as
1although the symbol, t, is used to denote torque, there should be no confusion with    t,    which is used to de   ne a 3x3 transform
matrix. the elements of t will not be shown in bold type.

(cid:5)

t

mt. then, the angular momentum is:

(cid:21)
thz =
mi (riw)xt(ri )z =    
mi (riw)z(ri )z =    !
hz =!

i

i

i

(cid:21)

5.7. rigid body dynamics 135

mi (r2

i )xt  z

(5.75)

i

mi (r2

i )z  z .

and the term    !

i

i

now the components of each particle are constant since the non inertial set is    xed in the body;
is a physical characteristic of the body itself. it is de   ned as the    inertia

mir2

matrix    of the body. since the symbol    i    denotes the unit matrix, the inertia matrix will be assigned
the letter    j.   

(cid:21)

i

j =    

=

mir2

i

   
    r2

2

(cid:21)

mi

i

3

+ r2
   r2r1
   r3r1

   r1r2
+ r2
r2
   r3r2
1

3

   r1r3
   r2r3
+ r2
r2
1

2

   
    .

(5.76)

note that j is necessarily de   ned in the z set, with the z axes    xed in the rigid body. its terms arise
because of a moment arm between the velocity of the particle and a given axis. the main diagonal
terms are called    moments of inertia.    in these terms the moment arm is the same as the radius of
the velocity vector, giving rise to squared    r    factors. the off-diagonal terms are called     products of
inertia,    in which the moment arm is different than the radius of the velocity vector.

physical picture of the inertia matrix
the diagram shows a single mass point, m, rotating about the x1 axis. the mass is located within
the non-inertial set by r = {r1, r2, r3}. its velocity is v = {0,   v cos    , v sin   }.

since the velocity vector is given by    r  1:
r3    r2
0
   r3
0
r2    r1

      1    r =    r      1 =

   
   

r1
0

   
   

      
      1

0
0

      
    =

      
   v1 = 0

v2
v3

      
    =

      
    0   r3  1

r2  1

      
   

136

5. orthogonal transforms

angular momentum is h = r    (          r) =    r2       =

      
     1

(
+ r2
r2
     1r1r2
2
     1r1r3

3

)
      
   

3 )     velocity component =   1

(cid:27)
r2
    velocity component =   1r2,
2
    velocity component =   1r3,

+ r2
  1(r2
     1r1r2
2
     1r1r3

+ r2

3 , moment arm =

(cid:27)
r2
moment arm = r1 ;
2
moment arm = r1 .

+ r2

3

;

note that rj components vary as the point mass rotates. for this reason, a non-inertial set whose
axes perform the rotation(s) is always used.

every particle contributes to the inertia matrix. as the particles are summed, each brings both
moments of inertia. the products of inertia might cancel, while the moments of inertia can only
add, being inherently positive.

m

m

in the diagram above, the two mass points are arranged symmetrically, and the product terms
cancel   the r2 and r3 coordinates are equal and opposite in sign. to achieve this result in the rigid
body, the non-inertial set is set along the axes of symmetry.

inertia matrix of the rigid body
in the limit, the particles become in   nitesimal, but in   nite in number, and the summations become
integrals in (5.77) producing the inertia matrix of (5.78).

in the following, the vector ri = {r1, r2, r3} is expressed in the z-set (z1, z2, z3). the elemental

mass points, dm, are equal to the mass density,    , times an elemental volume, dv :

5.7. rigid body dynamics 137

+ z2
+ z2
+ z2

3)dv

3)dv

2)dv

i

i

(cid:21)
(cid:21)
(cid:21)
(cid:21)
(cid:21)
(cid:21)

i

i

i

i11 =
i22 =
i33 =
i12 =
i13 =
i23 =

*
*
*

   (z2
2

   (z2
1

   (z2
1

v

v

v

v

   z1z2dv

mi (r2
2

mi (r2
1

3 )    
+ r2
3 )    
+ r2
+ r2
2 )    
*
mi (r2
1
mi r1r2    
*
mi r1r3    
*
mi r2r3    
   
    i11    i12    i13
   i21
i22    i23
   i31    i32

   z1z3dv

i33

v

v

   z2z3dv .

   
    .

and the inertia matrix is written:

i

j =

(5.77)

(5.78)

the elements of j are given in upper case   against the rules of this work. but, it is simply too
common for the inertia terms to be named this way. the rules must bend, and there is no confusion
with the unit matrix.

as mentioned above, it is advantageous to set the non-inertial axes along axes of symme-
try to get rid of the off-diagonal terms in (5.78). this is usually done visually, but the matrix in
equation (5.78) can always be reduced to diagonal form by the eigenvalue methods discussed in
chapter 6. thus, every rigid body has axes of symmetry. however, in practice it is rarely worth the
effort to diagonalize j.

5.7.4 the torque equation
the torque required is given as the time rate of change of angular momentum, tx = d
be expressed in terms of tz, by the equation tx = ttz. then

dt

hx. tx can

transforming tx to the z-set

tx = d

dt

thz =   thz +   thz .

tz = t

(cid:5)

tx =   t

(cid:5)

thz +   hz .

(5.79)

(5.80)

138

5. orthogonal transforms

the matrix   tt
result is t

(cid:5)

(cid:5)   t = wz. then,    nally:

has previously been de   ned as wx. if this matrix is transformed to the z set, the

tz = wzhz +   hz ;
+ j     z .
tz = wzj  z

(5.81)

equations (5.81) have been developed directly from (5.71). then, they assume that the center of the
z-set is either at the center of gravity of the body, or that the center is at a stationary point (actually
the point is only required to be non accelerating).

5.8 examples
the following simple example illustrates the concepts of momentum and torque.

two small    mass points    are attached to a weightless rod of length 2a. the rod is tilted at an
angle    from the horizontal, and rotates with an angular velocity,   , about a vertical axis at its center,
marked o in the diagram. is a torque required, and if so, what is its magnitude?

as soon as the centrifugal forces, f , are added to the diagram, it is clear that an external
balancing torque is required. each force is in the amount of m  2 acos   . these forces produce a
(total) moment about the negative x3 axis of 2m  2a2 cos    sin   . to maintain the motion a torque
of this same amount is required, about the positive x3 axis.

this torque can be found by using the equation, t = wj  where
  = {0,   , 0}; r =

r2

   
    ; r2 = 2

   
       r2
r1r2    r2

r1r2

2

   
   

0
0    r1
0

r1

the r matrix for each mass point is the negative of the other; but, the r2 matrices are identical,
and add     which accounts for the    2,    above. note the product terms.

0
0
+ r2
2 )

0

1

1

0    (r2
      
       r1r2

      
    = 2m  

r2
1
0

   
   

      
    0

  
0

   
    .

      
    .

j  =    2mr2  = 2m

r2
   r1r2
2
0

   r1r2
r2
1
0 r2
1

0
0
+ r2

2

0
0
   r2
   
   

then t = 2m  

   
    0 0   
0 0
      0

      
   .
since r1 = a cos    and r2 = a sin   ,the external applied torque is the same as predicted above.

      
    = 2m  2

      
    = 2m  2

      
       r1r2

0
0
a2 cos    sin   

0
0
r1r2

      
   

      
   

   
   

5.8. examples 139

r2
1
0

0
0

this torque would have to be applied by the mechanism that holds the rod at the center, o.

rotating plate
the square plate, dimensions, a by a, has moments of inertia i11 and i22 about the x1 and x2 axes,
shown in the diagram. it is set into rotational motion about the x2 axis at the rate of    r/sec. what
are the torques involved?

there are obviously no product of inertia terms, j is diagonal, with elements i11, i22, and 0

   
    0 0   
0 0
      0

0
0

   
   

   
    i11

0
0 i22
0

0
0
0 i33

   
   

      
    0

  
0

      
    =

   
    0 0   
0 0
      0

0
0

   
   

      
   

0
i11  
0

      
    = {0}.

t = wj  =

this is expected.

now, incline the rotational axis at an angle   . the question: is the torque still zero? the x2
component of the angular motion produces the same analysis, and result as the previous problem.
the x1 component also has the same analysis and result. so, surely this could be regarded as proof
that these problems are the same, even though it intuitively seems that the plate should be    out of
balance.   

   
   

in the equation below, de   ne s     sin   ; and c     sin   
      
   
   
      s

0
t=wj  =
0
0 i33
there is no external torque required because i11 = i22.

0
0      s
     c   s
0

   
   
   
    i11

0
0 i22
0

  c
0

  c

0
0

      
   =

      
   

      
   ={0}.

0
0
  2sc(i22     i11)

the spinning top
a top is diagrammed below, shown within a non-inertial coordinate set. its center of gravity, cg, is
at a distance a from its apex, its weight is mg. the moment of inertia about its vertical z1 axis is i11;

140

5. orthogonal transforms

the moments of inertia i22 and i33, about these respective, axes are equal because of symmetry. for
the same reason, there are no product of inertia terms.

the top is caused to spin with its apex on a horizontal (x2, x3) plane at point o. the x1 axis
is vertical, the x-set is inertial. the apex is not held at o, but, there is just enough friction to hold it
in place without slipping.

5.8. examples 141

the top spins at the rate   1 about its centroidal, z1, axis which makes an angle    with x1. in
addition to its spin, the axis of the top will    precess    (rotate) about the x1 axis, and    nutate    toward,
or away from vertical (i.e., the    nutation    rate is de   ned as the time derivative of angle   ).

the study of the motion of the top is a popular subject in the literature. we will only set up
the problem and determine the equation of motion (the torque equation), in order to illustrate the
matrices involved.

the precession rate is to be measured by a rotation of an intermediate coordinate y-set, initially
collinear with the x-set, but free to rotate about its y1 axis, with the precession of the top. the axis
of the top will be    xed in the y11 y2 plane, an arbitrary choice. this will de   ne an angle    whose
time derivative will be the precession rate.

   
    1
0
0 cos        sin   
cos   
0

sin   

0

   
    y . (5.82)

x = t1 y =

=      and note that the y-set experiences this rotation.

the precession rate will be

d  

dt

now, de   ne the rest of the z-set, whose z1 axis is collinear with the axis of the top. as noted

above, it tips at an angle    with the x1 and y1 axes. using the    gure at the left

   
    cos        sin   

cos   

sin   
0

0
0
0 1

   
    z .

(5.83)

y = t2z =

it is not necessary to de   ne a set that spins with the top. all the terms, angular velocity, momentum,
the force, mg, etc., will be the same in the z-set as they would be in a set which rotates with the top.
in particular, the inertia matrix will be the same, because the top is symmetric about its axis.

then the transform between the z and x sets is simply x = t1t2z.
in the constructing of the torque equation, differentiaion must be done in the inertial x-set:

tx= d

dt

hx = d

dt

(t1t2hz) =   t1t2hz + t1

  t2hz + t1t2

  hz .

142

5. orthogonal transforms

transforming this torque to the z-set,

(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5)

1

2t

2(t

1tx = t

  t1)t2hz + t

  t2hz +   hz .
  t1 is the precession rate about the x1 axis: {     , 0, 0}
   
   

tz= t
   
   ; transforms to t

  t1)t2 =

2(t

2

1

(cid:5)

(cid:5)

0
0
     sin   

the matrix t

(cid:5)

  t1 =

1

t

(cid:5)

1

   
    0

0
0

0
0
0         
    
0

the matrix t

  t2 describes the tip (nutation) about the y3 axis, t

(cid:5)

(cid:5)

2

2

  t2 =

when summed together these matrices form the wz matrix

wz = t

(cid:5)

2t

(cid:5)

  t1t2 + t

(cid:5)

2

  t2 =

1

   
   

0
    
     sin   

                 sin   
0          cos   
0

     cos   

(5.84)

   
    .

(5.85)

0          sin   
0          cos   
   
0
   .

0
0 0
0 0

     cos   
   
    0        
    
   
    .

0

the related vector representation is wz = {      cos   ,          sin   ,      }. note that this does not include the
rotation of the top, which is {  1, 0, 0} in the z-set.

the three dimensional torque equation is given in (5.86). note that the bold       z, and its
derivative, are vectors. the elements of       z are given in (5.87), including the scalar spin,   1. this
total angular velocity must be used in (5.86).

the angular momentum of the top is the product of the inertia matrix and the total angular
velocity vector. its rate of change is the inertia matrix times the derivative of total angular velocity.
the    nal equation is

tz= wzhz +   hz = wzj       + j         
      
         cos    +   
         sin       

i33 = i22;        =

   
   ;

0
0 i22
0

0
0
0 i33

   
    i11

j =

      
    .

(5.86)

(5.87)

where wz is de   ned in (5.85), and:

the expansion of (5.86) into its three coordinate elements in three non-linear differential equations
whose solutions are a numerical analysis problem.however,there are simpli   cations that are solvable.
for example, the initial setup, above, implies that the only external torque is the moment of the weight
of the top about the z3 axis, t = {0, 0, mga sin   }.

in this case, over a short period, the precession and spin rates, and the angle    are assumed
constant. the expansion of (5.86) then yields non-zero values only about z3. the three variables are
the,   ,   1, and     , in an algebraic equation.

5.9. exercises 143

5.9 exercises

5.1. an airplane is to    y, direct, from a point a, 74 degrees west longitude and 41 degrees (north)
latitude (roughly on the east coast of the us), to a point b, 122 degrees west longitude and
41 degrees latitude (roughly on the west coast). assume a spherical earth, with a radius
equal to 4000mi.

(a) construct a coordinate y-set at the point a, as in equation (5.8) of the text.

(b) what is the great circle distance between the two points?

(c) if the airplane simply    ies west, along the 41 degree latitude, what is that distance?

(d) after takeoff, what is the correct heading to    y the great circle path?

   
    cos
    sin

  
5
  
5

sin

cos

  
5
  
5

   
    ; r=

   
   . find t10.
      
      
      
    r1
    ;   =
      1

  2
  3

r2
r3

      
    .

5.2. a two-dimensional transform matrix is t =

5.3. given;

   
    0    r3
   r2

r1

r3

r2

0    r1
0

   
    ; w=

   
   

r=

0      3

  2

0      1
0

  1

  3

     2

(a) is rwr equal to r  (    r) or equal to (r    )  r ?
(b) is rwr = r2   ?
(c) is w2r equal to       (    r) or equal to (        )  r ?

5.4.

it has been shown that no external torques are required to maintain the

144

5. orthogonal transforms

rotation of the square    at plate about an inclined axis. the 4a by 2a plate in the diagram is
   
to rotate at constant angular velocity about z1, inclined at 45
. find the torques required, if
any. the plate has mass    m,    and its moment of inertia about x1 is i11 = ma2
3

.

5.5. the spinning top, discussed in section 5.8, is to be put into the state of    steady precession,   
in which      and    (the precessing and spin rates) remain constant, and the nutation rate is
zero (   remains constant).

determine the rate of change of angular momentum, as a function of these constants, that
balances the single external torque produced by the weight of the top (magsin   ).

c h a p t e r 6

145

matrix eigenvalue analysis

introduction

6.1
matrix analysis is particularly interesting because of the insight that it brings to so many areas of
engineering. with the advent of the modern computer, much of the numerical labor is at least
transferred into the fascinating realm of programming.

perhaps the single most interesting matrix analysis is that which will now be discussed. it has
fundamental bearing on the solution to many differential equations governing vibration problems,
and the analysis of electrical networks. the eigenvalue problem is basically concerned with the
transformation of vectors and matrices in a most advantageous fashion.

6.2 the eigenvalue problem
the beginning is simple enough: concerning the transform

ax = y

(6.1)

where a is a general, real, square matrix, we ask whether or not an (input) x vector can be found
such that the (output) y vector is proportional to x. that is:

the constant    is the (scalar) factor of proportionality. we can bring   x to the left side of (6.2):

ax =   x .

[a       i]x = a(  )x = 0 .

(6.2)

(6.3)

in (6.3), the notation [a       i] is used rather than the more familiar (a       i) in order to emphasize
that the quantity within the    [. .]    is a square matrix. a is nxn, x is nx1, i is the nxn unit matrix;
so the right side zero is an nx1 null column. the matrix [a -   i] is often referred to as a(  ), the
   lambda matrix,    or    characteristic matrix.   

when a is not symmetric, the    companion    equation (6.4) must also be considered:

z[a       i] = 0

(6.4)

where, now, z is 1xn (a row vector), and the 0 is a null 1xn row. as will be seen, these two equations
are    bound together,    and will be solved together.

from chapter 4, the homogeneous sets (6.3) and (6.4) have nontrivial solution iff the matrix
[a       i] is singular. furthermore, in this treatment of the problem, we will require that the rank of
the matrix [a       i] be n-1. this condition is met by most engineering problems of interest.

146

6. matrix eigenvalue analysis

6.2.1 the characteristic equation and eigenvalues
in order for [a -   i] to be singular, the determinant must vanish:

(cid:12)(cid:12) a       i

(cid:12)(cid:12) = (   1)n

a11       
a21
      
an1

a12
a22       
      
an2

      
      
a22       
      

a1n
      
      
ann       

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0.

(6.5)

the expansion of the determinant in (6.5) clearly will result in a polynomial of degree n. the
multiplier (-1)n is used simply to cause the coef   cient of   n to be positive (and, the determinant (6.5)
would be more accurately written as |  i   a|). thus,

f (  ) =   n + c1  n   1 + . . . cn   1   + cn = 0 .

(6.6)

f (  ) is called the    characteristic equation    and the polynomial is called the    characteristic polyno-
mial    related to the matrix a. the coef   cients, ck, are all functions of the [aij ] elements, and the
coef   cient cn is equal to the determinant of a (and the product of the    values). now, represent the
polynomial in (6.6) in its factored form:

f (  ) = (         1)(         2)(         3)       (         n) = 0

(6.7)
and it is clear that for each    =   j , f (  j ) = 0.these roots of the characteristic equation are called the
   eigenvalues,    or    characteristic values    of a. since (6.6) and (6.5) represent the same equation,
these   j values also cause the determinant in (6.5) to vanish. if the   j eigenvalues are all distinct (i.e.,
no two roots the same), then the above constraint that the rank of the determinant be n     1 will be
met. except for a short discussion concerning what happens when multiple roots occur, this chapter
will assume distinct roots. in the general case, in which a is not symmetric, the eigenvalues may
be complex numbers. while this fact is not much of a conceptual dif   culty, it does pose calculation
problems.
now, consider the case    =   1. the determinant |a(  1)| is zero and there will be exactly one
solution to each of the equations (6.3) and (6.4) above. these solutions, being associated with the
eigenvalue,   1, are known as    eigenvectors,    or    characteristic vectors.    equation (6.3) will yield a
column vector, and (6.4) will yield a row vector. these vectors will    emerge    together.

the adjoint of [a       1i] will be of unit rank. all its rows (columns) will be proportional to
each other (some, but not all, of the rows (columns) of the adjoint may be null). denote the row
eigenvector as u1, and the column eigenvector as v1. the adjoint of [a       1i] can be written

aa(  1) = [a       1i]adj = k{v1}[u1] .

(6.8)

the vector product given in (6.8) is nx1x1xn = nxn. it is certainly not the dot product of u1 and
v1. (in fact, every matrix of unit rank can be written as this type of single column times single row.
that is essentially the de   nition of rank = 1.)

6.2. the eigenvalue problem 147

any column of the adjoint of (6.8) solves [a       1i]x = 0. yet, there can be only one solution.
thus, all the adjoint columns must be the same   i.e., proportional, differing only in magnitude.
that is, only the eigenvector   s direction is obtained.

similarly, every (non-zero) row of the adjoint solves z[a       1i] = 0 (note, again that this
is a row equation). then, for any given eigenvalue, (6.8) yields exactly one row, and one column
eigenvector. and, since the vector magnitudes are arbitrary, we can always choose scaling such that
the dot product of u1 and v1 is equal to +1 (unity).

in the same way, all n eigenvectors are obtained from their respective adjoint matrices. and,
each time they are scaled to +1. it will now be shown that vj is orthogonal to ui, for the subscript i
not equal to j. write (v for column vectors, and u for rows)

avj =   j vj ; note that vj is (nx1)
uia =   iui ; note that ui is (1xn) .

premultiply the    rst of (6.9) by [ui] and postmultiply the second by {vj }. the left sides of both
equations will then be identical. if the two are subtracted, the result is

ui     vj (  i       j ) = 0 .

since the eigenvalues are distinct (by hypothesis), then (  i       j ) cannot be zero. thus, the dot
product ui   vj must be zero, proving that the two eigenvectors are orthogonal. but, the original
choice of i and j was arbitrary. so, the assertion of orthogonality must be true for any choice. then,
if all the row eigenvectors are collected into the square matrix, u, and the column eigenvectors
collected into v (and remembering that ui   vi can be normalized to +1):

uv = vu = i,

(the unit matrix) .

also, the entire eigenvalue problem can be displayed in the following 2 equations:

av = v 
ua =  u .

in (6.12), the matrix,  , is a diagonal matrix whose main diagonal elements are the eigenvalues    
arranged, carefully, in the same order in which the eigenvectors are placed in the matrices u and v.
now, choose the    rst of the equations (6.12), and premultiply by u. using (6.11), the result is:

uav =   .

(6.9)

(6.10)

(6.11)

(6.12)

(6.13)

that is, the a matrix is transformed by u and v into its    eigenvalue matrix,     .

synthesis of a by its eigenvalues and eigenvectors

6.2.2
premultiplying the second of equations (6.12) by v reveals an interesting, and useful result.

a = v u .

148

6. matrix eigenvalue analysis

the eigenvalue analysis    resolves    matrix a into its component eigenvalues and vectors.this becomes
more evident if it is remembered that premultiplying u by   has the effect of multiplying every
element of row uj by its corresponding   j . now, simply visualize this and also partition v by columns
and the  u product by rows:

a = n(cid:21)

j

  j{vj}[uj] .

(6.14)

a is shown as a sum of n matrices;   j {vj }[uj ], each nxn, and each composed only of corresponding
eigenvalues and eigenvectors. it is instructive to postmultiply the eigenvector vk on both sides
of (6.14)

avk = n(cid:21)

  j{vj}[uj]{vk} .

now, all the products uj   vk vanish (the eigenvectors are orthogonal), except the uk   vk one (which
is normalized to +1). then

j

avk =   kvk

which is the same as equation (6.2), with the appropriate subscripts.

in (6.14), if any one of the nxn matrices in the summation were to be subtracted away, a new
matrix, say b, would result. b would have all the same eigenvalues and vectors that a possesses    
except the one subtracted away. this fact is useful in    matrix iteration    (not yet discussed here), in
which iterative techniques are used to obtain eigenvalues and vectors, one at a time. when one set is
found, its effects can be subtracted away, to move on to iterate for the next. see the article on matrix
iteration in section 6.7.2 of this chapter.

6.2.3 example analysis of a nonsymmetric 3x3
to illustrate the eigenvalue problem numerically, consider the following 3x3:

   
    25    44
12    21
   3

18
8
6    4

   
    .

a =

this matrix is a particularly simple one numerically. but, its analysis will nevertheless illustrate the
eigenvalue problem. eigenvalues will be denoted using   , and the characteristic equation is the
expansion of the determinant:

f (  ) = (   1)3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 25       

   44
12    21       
   3

18
8
6    4       

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0 .

(6.15)

since (   1)3 =    1, negate the    rst row:
|(       25)

+ 44

    18| ,

and expand by    rst minors of the    rst row,

f (  ) = (       25)[(   21       )(   4       )     48]     44[12(   4       ) + 24]

    18[72 + 3(   21       )] = 0

6.2. the eigenvalue problem 149

which reduces algebraically to:

f (  ) =   3     7       6 = 0 .

(6.16)
notice that in this case there is no   2 term (i.e., its coef   cient is zero), and that the    trace    of a
(the sum of its diagonal elements) is also zero. in fact, the (negative) trace of a is always equal to
its coef   cient in its characteristic polynomial.

by inspection,    1 is a root of (6.16). dividing by (   + 1), and factoring the quadratic:

f (  ) = (   + 1)(  2            6) = (   + 1)(   + 2)(       3) .

(6.17)
the three roots,    1,   2, and 3, are the three eigenvalues of a. for each eigenvalue there will be
two eigenvectors   a row eigenvector, and a column eigenvector.

with   1 =    1, and denoting [a       1i], as a(  1):

   
    26    44
12    20
   3

18
8
6    3

   
   ;

a(  1) =

26x1     44x2 + 18x3 = 0
12x1     20x2 + 8x3 = 0
   3x1 + 6x2     3x3 = 0

.

the solution of the linear equation set at the above/right determines the eigenvector, v1. since a(  1)
is known to be singular, this set must have a non-trivial solution. one way to do this is to set x3
arbitrarily (say, x3 = 1), delete the third equation, and solve the remaining two variables:

26x1     44x2 =    18
12x1     20x2 =    8

from which it is found that x1 = x2 = x3 = 1.
|a(  1)| is equal to zero. however, the adjoint matrix must have at least one non zero row and
column (the rank of a(  1) is n     1). then, by calculating the adjoint, both row and columns are
   
found:
   ;

   
    .

aa(  1) =

a(  1) =

(6.18)

   
    26    44
12    20
   3

18
8
6    3

   
    12    24 8
12    24 8
12    24 8

from this adjoint, any row can be chosen as the row vector, and any column can be chosen as
the column vector, for example [12,   24, 8] for the row vector, and {12, 12, 12} for the column.
however, the eigenvectors emerge in direction only, then any multiples of these vectors are also
eigenvectors. then:

u1 = [3,   6, 2]

and v1 = {1, 1, 1}

150

6. matrix eigenvalue analysis

where u1 denotes the row vector, and v1 denotes the column. since u1     v1 product must be 1,
normalize by multiplying u1 by    1.

u1 = [   3, 6,   2]

and v1 = {1, 1, 1}

now, if these two vectors are truly eigenvectors they must solve

and they do:

(a       1i)v1 = 0; & u1(a       1i) = 0 .
(cid:18)

(cid:18)   
(cid:17)    3 6    2
    26    44
18
12    20
8
   3
6    3
      
   
   
   
    26    44
    1
12    20
   3

1
1

and

0 0 0

      
    .

   
    =(cid:17)
      
      
    =
    0
   
    10
20    10
0
0
0
15    30
15

0
0

aa(  2) =

   
    27    44
12    19
   3

18
8
6    2

18
8
6    3
   
   ;

with   2 =    2

a(  2) =

   
    .

(6.19)

in the same manner as before,

u2 = [   1, 2,   1]

and v2 = {2, 0,   3}

note that the adjoint has a zero row, which must not be chosen as an eigenvector. this is
simple by sight, but if the computer is choosing eigenvectors, it must be taught to avoid such things.
with   3 = 3:
a(  3) =

   
    .

(6.20)

   
    120    200 80
60    100 40
0
0
and v3 = {2, 1, 0}

aa(  3) =

0

now that the eigenvectors have all been chosen, and normalized uk     vk = 1, the 3x3 u and v
matrices are:

   
    22    44
12    24
   3

18
8
6    7

   
   ;
u3 = [3,   5, 2]
   
   ;

6    2
2    1
2

   
       3
   1
3    5

u =

   
    1
2 2
0 1
1
1    3 0

   
    .

and v =

(6.21)

these matrices are inverses, i.e., uv = vu = i, and

av = v  (see equations (6.12))

where   is the (3x3) diagonal eigenvalue matrix. now, postmultiply by u:

avu = a = v u .

(6.22)

6.2. the eigenvalue problem 151

   
   

   
       1

   
    1
2 2
1
0 1
1    3 0

which shows the synthesis of a by its eigenvalues and eigenvectors. in this example:

v u =

18
8
6    4
an important result. alternatively, the matrices u and v    diagonalize    the original matrix:

0 0
0    2 0
0 3
0

   
   

   
       3
   1
3    5

6    2
2    1
2

   
    =

   
    25    44
12    21
   3

   
    = a .

   
       3
   1
3    5

6    2
2    1
2

   
   

uav =

   
    25    44
12    21
   3

18
8
6    4

b =

   
       1

0 0
0    2 0
0 3
0

(6.23)

   
    .

18
8
6    4

uav =  
   
   

   
    25    44
12    21
   3
   
       3 6    2
   3 6    2
   3 6    2

   
       

   
    =

   
    1
2 2
0 1
1
1    3 0
   
    22    38
9    15
   6

   
    =

   
    .

16
6
12    6

(6.24)

now, to illustrate the point about the synthesis of a via its eigenvalues and vectors, from the matrix
a, subtract the 3x3 =   1{v1}[u1]:

an analysis of b shows that it still possesses the eigenvalues    2, and 3, but, in place of   1 =    1, its
  1 is zero (b is singular). interestingly, all its eigenvectors are the same   even u1 and v1. however,
u1 and v1 can play no part in the synthesis of b, because these are multiplied by zero.

6.2.4 eigenvalue analysis of symmetric matrices
in the general (non-symmetric) case, 2 equations are required to de   ne the eigenvalue problem
(equations (6.3) and (6.4)). when the given matrix is symmetric, a simpli   cation occurs. if (6.3) is
transposed (  i is diagonal) the result is x
= a, and the result is
that the row vector is simply the transposed column vector.

       i] = 0. but, in this case a

(cid:5)
[a

(cid:5)

(cid:5)

for any eigenvalue,   i the adjoint matrix [a       ii]adj is also a symmetric matrix, proportional

to the product of viv

(cid:5)
i. any nonzero row or column can be chosen.

the orthogonality of these eigenvectors is shown in the following way. for any two vectors,

write:

avi =   ivi
avj =   j vj .

now, premultiply the    rst of these by vj and the second by vi.

j avi =   iv
(cid:5)
(cid:5)
v
j vi
iavj =   j v
(cid:5)
(cid:5)
v
ivj .

(6.25)

(6.26)

152

6. matrix eigenvalue analysis

if the second of these is transposed, the left sides become identical, because a is symmetric. then,
when the two are subtracted, as before, the eigenvectors must be orthogonal (again assuming distinct
eigenvalues). this orthogonality can be expressed in terms of all the eigenvectors, as

(cid:5)
v

v = i (compare with (6.11)) .

and the entire eigenvalue problem can be displayed in the single equation,

av = v  (compare with (6.12)) .

the diagonalization of a is shown by premultiplying by v

(cid:5)

the synthesis of a is given by postmultiplying: av v

and (6.14) becomes:

(cid:5)
v

av = v
(cid:5)

v  =   (compare with (6.13)) .
a = n(cid:21)

  j{vj}[vj] .

= a = v v

(cid:5)

(cid:5)

(6.27)

(6.28)

(6.29)

(6.30)

again note that the vector product shown here is nx1x1xn, resulting in nxn matrices.

j

6.3 geometry of the eigenvalue problem
the dot product of a vector x, times itself, is equal to the sum of squares of its elements. if this sum
is equated to unity, we have

(cid:5)

x = x2

1

x

+ x2

2

+        + x2

= 1 .

n

in two or three dimensions, the above equation is identi   ed as that of a circle, or sphere, of unit
radius. by analogy, the n dimensional case, written above, is called an n-dimensional sphere.

the dot product of x into the vector  x, where   is a diagonal matrix, is x

unity:

(cid:5)

 x =   1x2

1

x

+   2x2

2

+        +   nx2

n

= 1 .

 x. equated to

(6.31)

(cid:5)

depending upon the sign of the    values,the above equation in three dimensions would be an ellipsoid
or hyperboloid. for our purposes, it is most bene   cial to visualize an ellipsoid. in the accompanying
   gure, note that the coordinate axes are aligned along the principal axes of the ellipsoid. but, in (6.31),
if we affect an arbitrary orthogonal coordinate transform, x = tq. then:

(cid:5)

(cid:5)
t

 tq = q
(cid:5)

aq = 1

q

(6.32)

and note that a is a symmetric (not diagonal) matrix whose eigenvalues are in   and whose eigen-
vectors are in the transform matrix t.

chapter 5, shows that such a transform amounts to a series of rotations about the axes of a
rectangular coordinate system   apparently, in this case,rotating the axes away from the principal axes

6.3. geometry of the eigenvalue problem 153

of the ellipsoidal surface. again from chapter 5, all vectors and angles remain invariant under such
a transform. therefore, the surface itself does not change, just the coordinate perspective through
which it is viewed.

in practical situations the coordinate axes are rarely aligned along the principal axes. instead,
the    quadratic form    is derived as the dot product of a vector x multiplied by its transform ax   as
in (6.33), below. in chapter 4, section 4.3, the    quadratic form    was introduced. the equation of
the ellipsoid described here is just such a form:
f = x
(cid:5)

ax = 1

(6.33)

a scalar, de   ned by the symmetric matrix, a. in general the form is the equation of an n-dimensional
ellipsoid, whose principal axes do not lie along the axes of the coordinate set. when the form f is
expanded, it includes    cross product terms,    involving xi xj in addition to the squared terms found
in (6.31). the problem is to affect a coordinate transform, such that f appears with squared terms
only.

figure 6.1 shows a simple 2-dimensional case. x is simply the vector drawn from coordinate
center to any arbitrarily chosen point. at that same point, the normal to the surface is identi   ed as

figure 6.1:

154

6. matrix eigenvalue analysis

the vector n. analytic geometry tells us that n is proportional to the column vector    f :

(cid:24)

(cid:25)

(cid:24)

(cid:25)

   f =

   f

   xi

=

   f

   x1

,

   f

   x2

,        ,

   f

   xn

.

(6.34)

that is, the ith direction cosine of n is proportional to the partial of f with respect to the ith
coordinate. assembling all these together as in (6.34) derives the    f column vector. but, again
from appendix a, equation (a.12):

   f =    (x

(cid:5)

ax) = 2ax .

(6.35)

in general, the normal, n, is different from x in both direction and magnitude, as in the    gure.
however, note that along the principal axes of the ellipse the vector x, itself, is normal to the surface.
then, at these points, the vectors x and n are collinear, and are proportional:

n = 2ax =   x .

(6.36)

note that equation (6.36) is simply the statement of the eigenvalue problem (with the constant 2
absorbed into the proportionality factor,   ).

the eigenvalue analysis leads to a solution for n characteristic numbers (eigenvalues),    and

their n eigenvectors, v. assembling these quantities into matrix form:

  = [  j   ij]; the diagonal matrix of eigenvalues
v = [vj]; the orthogonal matrix of eigenvectors .

(6.37)

the given matrix a is symmetric, so v is orthogonal (the rows of v are the row eigenvectors). now,
de   ne the new coordinates as the q-set, where x = vq. the quadratic form is

f = x

(cid:5)

ax = q

(cid:5)

v

(cid:5)

avq = q

(cid:5)

 q =   1q2

1

+   2q2

2

+        +   nq2
n .

(cid:5)

av transforms a to the diagonal matrix,  , f is now composed of squared terms only, the
because v
familiar form of the ellipsoid from analytic geometry. such a transform preserves both magnitude
and angle, the square roots of the reciprocals of the eigenvalues are equal to the lengths of the semi
major axes.

6.3.1 non-symmetric matrices
in the general, nonsymmetric eigenvalue problem, there are two sets of eigenvectors, that are iden-
ti   ed as ui (the row set), and vi (the column set). when the full complement of vectors is gathered
together, these occupy the rows and columns, respectively, of u, and v. however, neither u nor v
is orthogonal. neither of these, then, represent rectangular coordinate sets. instead, they represent
base vectors in two    skewed    systems in which the u (as well as the v) axes are at oblique angles
(within each set).

6.3. geometry of the eigenvalue problem 155

but, u and v are inverses. then, ui   vj =   ij . that is, the u axes are orthogonal to the v. in
the oblique (nonsymmetric) case, it takes two sets of coordinates to take part in a coordinate transform
and, the diagonalization of a quadratic form (now called a    bilinear form   ).
in the rectangular set, a given vector, r = {r1, r2,        , rn}, is represented by the rj set of numbers,
each of which is determined by taking the dot product of r with the jth base vector. a transform
of r to a new orthogonal set is given by vr, where v is an orthogonal transform matrix relating the
new unit vector axes to the old ones.

but, in an oblique system, this convenience is absent. any vector, say r, has two sets of

(cid:24)

(cid:25)

coordinate values:

r =

r1v1
r2v2
  1u1   2u2

      
rnvn
         nun

where ui and vi represent unit vectors. that is (for example), the scalar r1 is the coordinate of r in
the direction of the unit vector v1, while   1 is the coordinate of r in the direction of the unit vector
u1. furthermore, in order to determine r1 we must take the dot product of r, not with the unit vector
v1, but, with the unit vector, u1. similarly, given another vector y:
y1,        , yn in the v-set
  1,        ,   n in the u-set

y =

(cid:24)

(cid:25)

(the unit vectors are omitted) the dot product of the two vectors is given by

r     y = r1  1 + r2  2 +        + rn  n =   1y1 +   2y2 +        +   nyn .

dot products must be taken between coordinates of the two sets.the products involving terms like ri yi,
or   i    i, have absolutely no meaning. with this in mind, we write the form:

(cid:5)

ax = 1 (a nonsymmetric) .

  

the principal axes of the form are still those that are normal to the surface, and the form is once again
expressed in the de   ning equation ax =   x. however,    and x are the two different representations
of the same entity. another expression is required in order to derive    the other half     of the normal
to the surface. that is, the transposed set
(cid:5)

(cid:5) = x
(cid:5)

(cid:5)
a

   = 1 .

(  

ax)

(cid:5)

this time the de   ning equation is a
   =      which will bring out the companion set coordinates of
the vector which is proportional to the normal. note here, that    is represented as a column. in our
development of the eigenvalue problem, this same equation is written as a row equation because it
was important to identify this half of the problem,    the row half.    that is:

(cid:5)

   =              a =      

a

are the same equation, but the second form, with   a makes it clear that    is a row vector.

156

6. matrix eigenvalue analysis

the eigenvalue analysis    nds that two sets of n vectors emerge (as in section 6.2), and they
are mutually orthogonal. in the geometric sense, there is only one set of principal axes, and these
are orthogonal. but the analysis of them is required to take place within the two oblique, mutually
orthogonal systems.
set. the extra complication is removed. thus, with the transforms    =z

once this is accomplished, a transform is made to the principal axes, which are an orthogonal

u and x = vz.

(cid:5)

f =   ax = z

(cid:5)

uavz = z

(cid:5)

 z = 1 .

and the transformation of f is complete.

yet, not all the complications can be avoided. in the symmetric case, the eigenvalues are always
real, and a full complement of eigenvectors can always be determined. in the general case this is not
true. both eigenvalues and eigenvectors may be complex numbers; the matrices u, v, and  , then
complex. when eigenvalues are repeated in the symmetric case, it simply means that ellipses become
circles, providing another degree of choice in choosing rectangular axes.

in the oblique case, when repeated eigenvalues occur, it might be that some of the oblique
axes collapse into one, and it cannot be guaranteed that a full set of eigenvectors can be found. thus,
the eigenvalue problem and its geometric representation is far easier when the quadratic form is
originally given in terms of a symmetric matrix a.

matrices that arise in engineering problems are often symmetric, and the associated quadratic
form has physical as well as geometric signi   cance. for this reason the symmetric eigenvalue problem
is particularly important. however, it is also true that eigenvalue analysis is often required of non-
symmetric matrices, with complex roots and vectors. for example, the kinetic and potential energies
in vibrating systems are described by quadratic forms. however, when energy dissipation terms are
involved, the system is dynamically described by a nonsymmetric matrix, with complex eigenvalues
and eigenvectors. such systems will be discussed in the following chapter.

6.3.2 matrix with a double root
when a non symmetric matrix, a, is found to have a repeated root, there is the question of whether
or not the matrix is defective   does a possess a full complement of eigenvectors?

   
    0    2    2

   
   

a =

as an example consider

1
2
whose characteristic polynomial is (       1)(       2)(       2).
for the eigenvalue    = 1 the adjoint of a(  1) is the product [a     2i][a     2i], from which a
row and a column vector emerge. for the double root,    = 2, the matrix [a     i] [a     2i] is null (no

1
0

3
0

6.4. the eigenvectors and orthogonality 157

row or column can be chosen as an eigenvector). so, look at [a     2i]x
   
    x .

[a     2i]x = a(  2)x =

   
    2
2
   1    1    1
0
0

2

0

this matrix clearly has rank = 1. two independent vectors can be found that are orthogonal to the
rows/columns of a(  2).thus, a is not defective   it has all three eigenvectors. if a(  2) had rank = 2,
only one eigenvector could be found, and a would be defective. this a matrix satis   es an equation
f (a) that is of lower rank than the cayley-hamilton equation, namely [a     2i] [a     2i] = 0.this
leaves just enough room for the de   nition of the necessary eigenvectors.
now, consider the matrix below. its characteristic equation is f (  ) = (   + 1)2(   + 2) = 0; a
double root    =    1.

   
    0
0
0
1
   2    5    4

1
0

   
   

a =
   
    whose rank > 1, and [a + i][a + 2i] is not null.

in this case,

[a       1i] =

   
    1
0
0
1
   2    5    3

1
1

then, only one eigenvector can be found, and the matrix is defective.

6.4 the eigenvectors and orthogonality
the importance of orthogonality, and just what it means, cannot be overemphasized. the fact that
eigenvectors come in orthogonal sets makes them very special   they are the stuff solutions are made
of. the matrix, itself, is synthesized by its eigenvectors. equation (6.14) rewritten, here:

a = n(cid:21)

  j{vj}[uj];

ui     vj =   ij .

j

the solution to equation sets involves some sort of    diagonalizing    (reduction) of the matrix so that
a solution for one of the variables can be made without interference from the others (i.e., decoupling
the original equations). note the simpli   cation that occurs if the eigenvalues of the matrix are known
in advance:

given ax = c, just transform the x vector by x = vz. then:

ax = c     avz = c
uavz =  z = uc .

158

6. matrix eigenvalue analysis

now, the equations are decoupled in the variables, z. each equation can be solved individually, the
  matrix is inverted by simply taking the reciprocals of its diagonal elements. then just transform
back to the x-set, z = ux:

z =  
   1uc
ux =  
   1uc
vux = x = v 

   1uc .

granted, in the general case this approach is not practical, because it is at least as dif   cult to obtain
the eigenvalue analysis as it is to invert the original matrix. however, the point here is to illustrate
the    power    of the orthogonal eigenvector set. furthermore, in the next chapter this approach is
used, and is practical in the case of differential equation sets.

inverse of the characteristic matrix

6.4.1
the inverse of the characteristic matrix is found in the same manner as above. it will be shown here
because of its importance to the solution of differential equation sets in chapter 7.

the solution to [a       i]x = d is required   tantamount to the inversion of [a       i]. orthog-
onality of the eigenvectors is required; thus the eigenvalue analysis of a (i.e., the matrices u, v and
 ) must be known. the solution will be shown here for the non-symmetric case.

the vector x is    rst transformed via x = vz (then z = ux)

[a       i]vz = d and then premultiply by u :
u[a       i]vz = ud = [uav     uiv  ]z = ud

= [        i]z = ud .

but, the matrix [        i] is easy to invert   it is a diagonal matrix. so z = [       i]
now, x is determined by the inverse transform z = ux

ux = [        i]   1ud,     x = v[        i

   1ud .

then, the inverse of the characteristic matrix is

[a       i]   1 = v[        i]   1u note:    (cid:4)=   k .
[a       i]   1 = n(cid:21)

this equation can be interpreted in the manner of (6.14):
{vj}[uj]
         j

;    (cid:4)=   j .

j

   1ud.

(6.38)

(6.39)

the fundamentally important concept of orthogonality is not just found in matrix analysis. it carries
over from orthogonality of vector sets into orthogonality of continuous functions within a given
range. our    rst exposure to the concept is in the determination of fourier series coef   cients.

an excellent example of the way that an orthogonal set of eigenvectors is used to build the
solution to a problem is given in the following paragraphs. it then shows the    evolution    of the
matrix/vector solution into the continuous solution of the vibrating string problem.

6.4. the eigenvectors and orthogonality 159

6.4.2 vibrating string problem
a tightly stretched string of lengh l and mass m, vibrates freely following an initial deformation.
the problem is to determine the equations of the vibration at points along the string as functions
of time.

the matrix approach, summarized here, divides the continuous string into n parts of mass m,
and concentrating it into a single point at the center (of the part). points, mk, are located horizontally
by xk; the de   ection of the string at that point is measured by yk. a load, p , is applied at the kth
point (loads can be applied only at these points), and a    free-body diagram    at that point determines
the displacement, yk(xk), as a function of the load, the tension, t , the position of the point, xk, and
the point at which the load is applied.

summarizing for all points, a load vector, p, is formed, and the resulting matrix equation

relating the displacements of the loads is y = wp.

the elements of the vector, y, are the displacements at the sequential points. the p vector
gives the load at these points, and the elements, wij , of the symmetric matrix w, are the de   ections
at xi, due to unit loads at xj . w is referred to as the    in   uence matrix.   

appendix c develops the following set of second order differential equations:
2 ) ; for i     j .

2 )(n     j + 1

y(t ) =     lm

n3 (i     1

t

the solution to this equation is a weighted sum of the eigenvectors of w:

vr (ar cos   r t + br sin   r t )

(6.40)

w   y(t ) ; with wij = 1
y(t ) = n(cid:21)

r=1

and the orthogonality of these vectors is used to determine the coef   cients ar and br.speci   cally,note
(cid:5)
what happens when the solution equation is multiplied by v
s. since the vector set is an orthogonal
one, only one term in the series survives,    decoupling    the as (or bs ) coef   cient.

note that the vectors are    spatial    in the sense that they describe a possible spatial shape of
de   ections along the string.they are not time-variable (although they are multiplied by time variable
functions). these spatial-template shapes are called    normal modes,    and they can be plotted along
an abscissa in the x dimension; such as that, below.

160

6. matrix eigenvalue analysis

the graph shown here plots the    rst four eigenvectors, with the string divided into 12 parts.
the black rectangles represent the mass points along the string. it is evident that these modes are in
the shape of sinusoids, and are an orthogonal set (easiest to see this are numbers 1 and 2).

the continuous function approach: as the number of divisions of the string increases toward in   nity,
the vector function, y(t ), becomes a continuous function y(x, t ). there comes a point in its solution
when

,

"

#+

y(x, t ) =

sin

n   x

l

an cos

n   at

l

+ bn sin

n   at

l

.

(6.41)

   (cid:21)
n=1

the similarity between this and the matrix approach is striking! compare (6.40) to (6.41). in this
case, the solution is an in   nite summation of (continuous) sinusoidal functions, that are an orthogonal
set, over the interval from 0 to l:

l*

(cid:24)

note that

sin

0

n   x

l

sin

k   x

l

=

0, k (cid:4)= n
2 , k = n

l

.

then, to determine the coef   cients, an and bn, (6.39) is multiplied by sin
the interval   very much like taking the dot product of two modes of the vector y(t ).

l

n   x

and integrated over

6.5 the cayley-hamilton theorem
intertwined with the eigenvalue analysis is a most amazing, and famous result, independently found
by cayley and hamilton. it has no parallel in conventional algebra. brie   y, this theorem states that
any square matrix identically satis   es its own characteristic equation. the most direct way to develop
this theorem is given by lanczos [3] as follows:

the equation [a       1i]x = 0 is solved by c1v1, c1 arbitrary. the equation:

[a       1i][a       2i]x = 0

is solved by x = c1v1 + c2v2 (i.e., any linear combination of v1 and v2):

[a       1i][a       2i](c1v1 + c2v2) = [a       1i][a       2i]c1v1 + [a       1i][a       2i]c2v2 .

6.5. the cayley-hamilton theorem 161

the second term is obviously zero. the    rst term:

[a       1i][a       2i]c1v1 = c1[a       1i](av1       2v1)
= c1[a       1i](  1v1       2v1) .

which is also obviously zero. using this same reasoning, adding one more term at each step, we see
that

(a       1i)(a       2i)(a       3i)       (a       ni)x = 0

is satis   ed by any linear combination of all the eigenvectors. but, these vectors are linearly indepen-
dent, and they    ll the n-space.thus, any vector at all can be represented by such a linear combination.
then every n-dimensional vector satis   es this equation. the only way that this could occur is that
the equation is an identity.

(a       1i)(a       2i)(a       3i)       (a       ni)     0 identically .

(6.42)

it must be noted here that this proof assumes that the matrix a has a full set of eigenvectors. for
some    defective    matrices (that are non-symmetric, and have repeated eigenvalues), a full set does
not exist. however, it has been proven, via a limiting process, that even in the defective case, the
cayley-hamilton theorem is still true. it should also be mentioned that not all non-symmetric
matrices, even with repeated roots, are    defective.   

from section 6.2 above, given the matrix, a, the determinant of [a       i] expands to the

characteristic equation:

f (  ) = c0  n + c1  n   1 +        + cn   1   + cn = 0 .

the cayley hamilton theorem states that:

f (a) = c0an + c1an   1 +        + cn   1a + cni     [0] .
an amazing and powerful theorem. for example, by multiplying through by a

c0an   1 + c1an   2 +        + cn   1i + cna

   1 = [0] .

((6.6)rewrite)

(6.43)

   1

then

   1= (c0an   1+c1an   2 +        + cn   1i)/cn .

a

(6.44)
by the same reasoning (6.43) shows that any power of a(nxn) can be represented in terms of powers
of a no greater than n     1. for example, the a matrix shown here is that which was used in the
previous eigenvalue analysis, the characteristic equation was:
f (  ) =   3     7       6 = 0 .

162

6. matrix eigenvalue analysis

then

a3 = 7a + 6i
a4 = 7a2 + 6a, and
a5 = 7a3 + 6a2 = 6a2 + 49a + 42i .

any power of this a is a function of the a matrix raised to powers no greater than 2, and the unit
matrix. using the given a matrix, try it!

   
    25    44
12    21
   3

18
8
6    4

   
    .

a =

of a

the eigenvalue analysis and cayley hamilton theorem also provide the solution to the analysis
   1. de   ne b as a

   1. now premultiply (6.43) by bn
f (b) = c0i + c1b +        + cn   1bn   1 + cn bn = [0] .

(6.45)

then the characteristic polynomial for b has the same coef   cients as that for a, except in reverse
order   and therefore its roots are the reciprocals of those of f (a) (see the appendix b,    polynomi-
als   ). but are the eigenvectors of b the same as those of a? by de   nition, ba = i. now assume that
b has the same eigenvectors, and set b = v b u (a =v au has already been shown to be true).
the matrices  a and  b are the eigenvalue matrices of a and b.

ba = v bua = (v bu) (v au)= (v b) ( au)= vu = i .

(6.46)

since this product does produce the unit matrix, and since the inverse of a must be unique, it follows
that b = v bu is that inverse   i.e., that the eigenvectors of b are the same as those of a, and the
eigenvalues are the reciprocals.

the calculations involved in an eigenvalue analysis are at least as complex as those involved
in the inversion process. therefore, it is unlikely that an eigenvalue analysis would ever be done just
to determine a

   1. perhaps it might be useful when a is very nearly singular.

6.5.1 functions of a square matrix
this discussion makes frequent use of the transforms de   ned by the eigenvalue problem and is
therefore limited to square matrices, a, which have a full complement of eigenvectors.

note that a = v u, and a2 = (v u)(v u) = v 2u. extending this

an = v nu

which shows that the eigenvectors of an are the same as those for a, and the eigenvalues are the
nth power of those of a. this same argument holds for any polynomial in a.

p (a) = c0an + c1an   1 +        + cn   1a + cn

(6.47)

(6.48)

6.5. the cayley-hamilton theorem 163

by transforming a = v u, the polynomial, p (a) is diagonalized to p ( ).then p (a) also has the
eigenvectors of a, and its jth eigenvalue equals p (  j ), where   j is the jth eigenvalue of a. an
example is shown above for p (a) = a5. in addition, if (6.48) is postmultiplied by vk, and since
akvk =   kvk:

p (a)vk = p (  k)vk .

(6.49)

general polynomial fnctions
the algebraic effort involved in actually expressing p (a) in terms of the lower degrees of a can be
daunting. the following development will help a great deal. note, here that the general polynomial
is given as p (i.e., upper case), while the characteristic polynomial will be denoted p (lower case).
then,    rst divide p by p. the result will be a quotient, q, and a remainder, r:

p (x)

p(x)

= q(x)+ r(x)

; then p (x) = p(x)q(x) + r(x) and therefore:

(6.50)

p(x)

p (a) = p(a)q(a) + r(a) .

but, p(a) is identically equal to zero, by the cayley-hamilton theorem. so, p (a) = r(a). a simple
example is when x5 is divided by p(x), the characteristic polynomial for the example matrix at the
beginning of section 6.5. the remainder is 6x2 + 49x + 42, which is consistent with the a5 given
above.

for a formidable-looking example,    nd p (a), where p (x) given below, and a is the same

matrix

p (x) = x6     x5     7x4 + 31x3 + 40x2     19x + 5 .

the bulk of the work can be done before a is inserted. just use synthetic division to divide p (x) by
the characteristic polynomial, p(x) = x3     7x     6, and then retain the remainder, 10x2     5x +17.

p (a) = 10a2     5a + 17i .

this method, is handy, easy to use for polynomial functions. it can be extended beyond matric func-
tions to analytic functions. however, its extension involves the lagrange polynomials (see chapter 4,
section 4.5) and arrives at the same method that is to be discussed next.

sylvester   s theorem

6.5.2
general functions of a. this method is directly related to the lagrange interpolation method, and
could possibly be deduced from it. to derive it,    rst de   ne the characteristic polynomial as p(  ),
and then consider the polynomial, pk(  ):

(         j ); for example p1(  ) = (         2)(         3)       (         n)

pk(  ) = n$

j(cid:4)=k

164

6. matrix eigenvalue analysis

which contains all the factors in p(  ) except (         k). for the a matrix shown, p1(  ) = (   + 2)(      
3). then:

(6.51)
for the a matrix given here, p1(a) = (a + 2i)(a     3i), and in the general (nxn) case, there will
be n     1 (a       i) terms. each pk will be a polynomial of degree n     1.

j(cid:4)=k

pk(a) = n$

(a       j i) .

   
    25    44
12    21
   3

18
8
6    4

   
   

a =

note that pk(a)vj = 0, except when j = k, as was shown in the development of the cayley-

hamilton theorem. and when j = k (in the example 3x3)

p1(a)v1 = (a +   2i)(a       3i)v1 = (  1       2)(  1       3)v1 .

(to derive this, use the fact that av1 =   1v1.)

(  k       j ) .

(6.52)

$
j(cid:4)=k

now, de   ne the problem: given a general polynomial, p (a), determine a set of n coef   cients ck,
such that

now, just postmultiply successively by vj (j = 1, 2, , n). when j = k: and p (a)vk = p (  k)vk

ckpk(a) .

(6.53)

in general: pk(a)vk = vk
p (a) = n(cid:21)
$
p (  k)vk = ck
j(cid:4)=k
   
p (  k)
(  k       j )
j(cid:4)=k

(  k       j )vk

ck =

k=1

.

and solving for ck:

(6.54)

plugging these constants back into (6.53), with the de   nitions of the pk(a) polynomials:

p (  k)

.

(6.55)

the ratios of product factors are often referred to as zk(a), and (6.55) is written

p (  k)zk(a) .

(6.56)

   
   
j(cid:4)=k
j(cid:4)=k

(a       j i)
(  k       j )

p (a) = n(cid:21)
p (a) = n(cid:21)

k

k

6.6. mechanics of the eigenvalue problem 165

the foregoing development assumes distinct eigenvalues . in (6.55) the numerator terms are the
adjoint of the matrix a(  ) = [a       i], and the denominator is the derivative of p(  ) evaluated at
  k. the equation can be generalized, and rewritten as:

f (a) = n(cid:21)

k=1

f (  k)

aa(  k)
p
(  k)

(cid:5)

(6.57)

where it is known as sylvester   s theorem. equations (6.56) and (6.57) are more different than they
appear. the function f can be any analytic function, and aa is the adjoint of [a       i] whether or
not it has repeated roots (and the function p also represents the lowest degree polynomial satis   ed
by a). thus, sylvester   s theorem is more general than (6.56).

when the matrix, a, has distinct eigenvalues equations (6.55) and (6.57) are the same. that

aa(  k) = n   

j(cid:4)=k

(a       j i) and p

(cid:5)

(  k       j )

(  k) = n   

j(cid:4)=k

is

and (6.56) will be extended into analytic functions which possess an in   nite series expansion. the
question of convergence will not be addressed. however, the series themselves converge, and the
cayley-hamilton theorem says that any sub-series of terms can be written in terms of a polynomial
of degree n     1. therefore, convergence will be assumed.

then the matrix series ea =
z1 = (a       2i)(a       3i)
(  1       1)(  1       3)

and

is a valid equation. suppose a is 3x3. then

; z2 = (a       1i)(a       3i)
(  2       1)(  2       3)
ea = e  1 z1 + e  2 z2 + e  3 z3 .

; and z3 = (a       2i)(a       3i)
(  3       1)(  3       2)

(6.58)

   (cid:21)
k=0

ak
k!

it will take a lot of algebraic manipulation to    condense    equation (6.58) into a single matrix; but
note that it   s just algebra. the usual phrase here is    this will be left as an exercise for the student.   

6.6 mechanics of the eigenvalue problem
ef   cient eigenvalue analysis is a problem in numerical analysis   beyond the scope of this work.the
steps described below are those that illustrate the problem and the matrix characteristics. they are

(cid:129) determine the characteristic equation (calculation of the polynomial coef   cients).

(cid:129) factor the characteristic equation, to obtain the eigenvalues,   i.

(cid:129) for each value,   i,    nd the corresponding eigenvectors.

in a later section, a more sophisticated method is presented, which cleverly transforms the
given matrix into one whose eigenvalues and eigenvectors are easily calculated   even when these
are complex numbers. known as danilevsky   s method, it is far superior to these methods for realistic
matrices. and, even so, there may be methods that are superior to danilevsky   s.

166

6. matrix eigenvalue analysis

6.6.1 calculating the characteristic equation coefficients
pipes1 reports that maxime b  cher has shown that the coef   cients are related to the    traces    (sum
of the diagonal elements) of the powers of the input matrix, a. let sj denote the trace of the ith
power of a:

s1 = trace[a] = tr[a],

s2 = trace[a2], . . .

sn = trace[an]

then the coef   cients, ck, of the characteristic equation (6.6) are calculated successively, as follows:

c0 = 1
c1 =    s1
c2 =    (c1s1 + s2)2;
ck =    (ck   1s1 + ck   2s2) + . . . + c1sk   1 + sk)/ k

and, in general:

(6.59)

this relationship is easily programmed, providing an easy method for developing p(  ). also,

the powers of the a matrix can be saved to be used later (in determining the adjoints, aa(  i )).

6.6.2 factoring the characteristic equation
there are handbook methods for factoring polynomials up to degree 4.although there will not be any
examples herein resulting in p(  ) of higher degree, appendix b,   polynomials,    discusses polynomial
arithmetic and outlines computer methods, including root determination, real or complex.

finding the roots of a polynomial requires a computer; and the computer routines for poly-

nomial manipulation are very simple. see appendix b.

6.6.3 calculation of the eigenvectors
using gauss-jordan reduction
the matrix[a       j i] is singular, and in this discussion will be assumed to have rank n     1.then, the
gauss-jordan is an excellent tool to derive the eigenvectors one at a time.the method is described in
section 3.3. a 4x4 will be used in illustration from the point at which the gauss-jordan reduction
of [a       j i] terminates. if a(  j ) is complex, the reduction must be done in a complex arithmetic.

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

1 0 0 z1
0 1 0 z2
0 0 1 z3
0
0 0 0

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

the reduced matrix will appear as in the diagram. if   j is complex, then the    z    values shown
here will be complex. there will be a complete row of zero values along the bottom, showing that a
solution {x1, x2, x3, x4} does exist, with the value for x4 chosen arbitrarily, say k.

1see [4], page 90.

the complete solution is:

x1 =

            
         

   z1   z2   z3

1

            
          k.

6.6. mechanics of the eigenvalue problem 167

the reduction/solution will have to be repeated for each eigenvalue, and again for the transposed
matrix to obtain the row eigenvectors.

calculation of the adjoint of [a       j i]
this method has been shown in an earlier example. it derives both the row and column eigenvectors
together.

from the cayley-hamilton theorem, denoting the characteristic equation as p(  ) = 0:

p(a) = [a       1i][a       2i]          [a       ni] = [0] .

since the numbering of the eigenvalues is arbitrary, we can write the ith term    rst in the above, and
then gather the rest of the product terms into a polynomial, pi:

where

[a -   ii]pi (a) = [0]
pi (a) = n$
[a -   ki];

k(cid:4)=i

(n     1 product terms).

(6.60)

since a(  i )aa(  i ) = |a(  i )|i = [0], and comparing this to (6.60), note that pi(a) is the adjoint
of a(  i ). pi(a) will not be null, as long as   i is distinct   not a repeated root of the characteristic
equation. therefore, pi(a) will be the source of the eigenvectors.
pi can be found by synthetic division of p(  ). if the synthetic division is done in complex
arithmetic, then pi (  ) is found by the synthetic division of p(  ) by (         i ). if the division routine
accepts only reals, and   i is complex, (a + j b), then its conjugate is also a root and the divisor can
be the quadratic,   2 + 2a   + a2 + b2.

the result of this division must then be multiplied by (       a + j b):
   (       a + j b)

pi (  ) =

p (  )

  2 + 2a   + a2 + b2

(6.61)

when pi (  ) has been found, id127s are then needed to derive pi (a) = aa(  i ).

this method of determining the adjoint of [a -   ii], containing the eigenvectors, has the
advantage that operations with complex numbers are minimized. only the    nal multiplication by
[a     (a + j b)i] involves complex arithmetic.the powers of the original matrix are available, having
been calculated for de   ning the coef   cients of the characteristic polynomial.
numbers), they must be normalized   usually such that xj   zi = 1   de   ning vj and ui.

when the initial row, x, and column, z, vectors have been determined (in general complex

168

6. matrix eigenvalue analysis

6.7 example eigenvalue analysis
6.7.1 example eigenvalue analysis; complex case
the methods of eigenvalue analysis discussed in section 6.6 are valid for matrices whose eigenvalues
and eigenvectors are complex. the following matrix analysis follows the outlined method, showing
the complex results. the given matrix is:

   
      4.0

a =

3.0 3.0
5.0    2.0 2.0
6.0 1.0
0.0

   
   

(6.62)

the given a matrix is non-symmetric. its elements are integer (shown in decimal form). since it is
of third order, expect at least one real root (to the characteristic equation); and if there are complex
roots, these will emerge in complex conjugate pairs.

the traces of the powers of a are given, below. b  chers formulae are then used to    nd the

coef   cients of the characteristic polynomial, p(x):

traces

[a]
[a]2
[a]3

-5.0
75.0
-107.0

of course, c0 = 1, and the characteristic equation reads:

coef   cients

c1
c2
c3

5.0
-25.0
-131.0

f (  ) =   3 + 5  2     25       131 = 0 .

(6.63)

the three roots of this polynomial are the eigenvalues (  i ) of a. they are:

  1 = 5.05929 + j0.00000
  2 =    5.02965 + j0.77174
  3 =    5.02965     j0.77174 .

the termination point of the gauss-jordan reduction is shown below for   2, for both a and a

(cid:5)

:

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

[  2i     a]

1.0 0.0    0.189071
0.0 0.0
0.233048
0.0 1.0
1.004941
0.0 0.0    0.128624
0.0
0.0 0.0
0.0 0.0
0.0

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

[  2i     a

(cid:5)]
1.0 0.0
2.261977
0.0 0.0    0.567977
0.0 1.0    0.378141
0.466093
0.0 0.0

0.0 0.0
0.0 0.0

0.0
0.0

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

6.7. example eigenvalue analysis 169

in the above table, the matrix elements are complex, with the imaginary parts shown below the reals.
in both cases, the 3rd element value can be chosen arbitrarily (choose 1 + j0), and the column and
row vectors are

      
    0.189071     j0.233048
   1.0004941 + j0.128624

1.0 + j0.0

      
   ; and z =

      
       2.261977 + j0.567977
0.378141     j0.466095

1.0 + j0.0

      
    .

x =

after id172, x and z will become the eigenvectors v2 and u2. further, v3 and u3 are just the
complex conjugates of v2 and u2.
indicated in (6.61) is simply (         1), and p2(  ) = (         1)(         3). then aadj(  2) is:

the adjoint method is illustrated by calculating aadj(  2). in this case the result of the division

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

2.315223

5.672121    0.0889421    3.088942
   6.99143
2.315223
   30.14824
5.612826
12.94071
3.858705    5.447948
1.543482
30.000    6.177884    12.47612
4.630447    3.132725
0.0000

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

if the    rst column of this table is divided by 30.00 it will show agreement with the x column
obtained by the gauss-jordan reduction. note that this table yields row and column vectors for both
the complex eigenvalues, because they are complex conjugates.

the normalized eigenvectors
the eigenvectors emerge    in direction only.    their magnitudes are arbitrary. as before, the row
eigenvectors are the rows of the matrix u; and the column eigenvectors are the columns of v. then,
we will normalize these vectors such that uv = i, by dividing each element of both ui and vi by the
square root of the unnormalized dot product. the resulting (complex) u and v matrices are given
in the table below. the imaginary parts are again shown directly below the reals

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

v matrix
0.40334    0.70538    0.70538
0.00000    0.28089
0.28089
1.06836
0.49150
1.06836
2.32999    2.32999
0.00000
0.72648    0.75401    0.75401
0.00000    2.41504
2.41504

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

u matrix

0.65814
0.00000

0.40334
0.73079
0.00000
0.00000
   0.70538
0.18861
0.26401
   0.28089    0.05103
0.19047
   0.70538
0.26401
0.18861
0.05103    0.19047
0.28089

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

170

6. matrix eigenvalue analysis

6.7.2 eigenvalues by matrix iteration
if a is a square matrix with distinct eigenvalues, then any arbitrary vector, x0, can be expressed as a
  kvk. if the vector is multiplied by a,

linear combination of the eigenvectors of a. thus, x0 = n!
  kavk = n(cid:21)

x1 = ax0 = n(cid:21)

the result is x1 = ax0. using ax =   x, write:

  k  kvk .

(6.64)

k=1

k=1

k=1

now, if a multiple of x1 (say, wx1) is premultiplied by a, the result is x2 = w
kvk. if this
process is continued, the term in the summation which has the largest eigenvalue will predominate.
that is, after r iterations, the rth power of the largest eigenvalue,   k, will be much greater than the
others, and

  k  2

n!
k=1

xr         k  r
kvk

(6.65)

showing that the iterative process converges toward the eigenvector multiplied by the eigenvalue
times a proportionality factor. we can absorb the factor into the vector, and control the iteration by
its convergence to the eigenvalue.

this suggests a method for determining the largest eigenvalue and eigenvector of a. pick the
   rst eigenvector arbitrarily, say {1, 1, 1,     1}. premultiply a. then pick one of the elements of the
resultant vector (say, the largest), and normalize the vector such that the chosen element becomes
1.0. once you pick an element, stick with it   always normalize such that this one becomes unity.
save the normalizing factor; it converges to the eigenvalue. repeat the process until the change in
the factor is negligible.

the method is so simple that an example probably will show it best. in the table, the 3x3
matrix, a, is at left. the multiplying vectors are next (the    rst one being all ones). next comes the
result of the id127; then the normalizing   with the factor shown    rst

1
3

matrix

x vector
9
7
1.0000000
1.0000000 =
3
3
   3    1    1
1.0000000
1.0000000
9
7
0.5294120 =
3
3
   3    1    1    0.2941170
1.0000000
9
0.4960630 =
3
   3    1    1    0.4330702

1
3

7
3

1
3

product
factor
17.0000000
9.0000000 = 17.0000000
   5.0000000
7.4705882
3.7058823 = 7.4705882
   3.2352941
6.4645669
3.1889764 = 6.4645669
   3.0629921

normalized
1.0000000
0.5294117
   0.2941170
1.0000000
0.4960630
   0.4330702
1.0000000
0.4933009
   0.4738124

6.8. the eigenvalue analysis of similar matrices; danilevsky   s method 171

in the example shown, if the iterations were continued, the eigenvalue would emerge as 6.00 and
the eigenvector converges to {1.0, 0.5,   0.5}.
note: this matrix is not symmetric. it could therefore have had complex eigenvalues and vectors. in
that case, the convergence is quite different. although iteration does work for the complex case, it
will not be discussed here. even if the eigenvalues and vectors are real, it is necessary to transpose a
and iterate for the row eigenvector (the eigenvalue will be the same).
if it is desired to continue the procedure for the next largest eigenvalue   vector, then the new
matrix is formed by subtracting out the results of the    rst iteration: b = a       1v1u1. as in any
iterative procedure, it is necessary to keep many signi   cant    gures. even then, only the    rst    few   
results will be within acceptable accuracy. usually iteration is only done on large symmetric matrices,
and only to derive the    rst one or two eigenvalues   vectors.

6.8 the eigenvalue analysis of similar matrices;

danilevsky   s method

the eigenvalue analysis of a matrix has always been considered formidable, especially before the
digital computer was available to do the messy calculations. it is no surprise, therefore, to    nd that
methods have been developed to shorten and simplify the work. in the present day, the best of these
methods are those that are coded easily on the computer. a method will be discussed which uses a
   similarity transform    to develop a new matrix whose eigenvalue analysis is very simple to perform.
and in this case, the matrix transformation, somewhat analogous to the gauss-jordan reduction
method in determinants, is not a dif   cult one.

in chapter 5, section 5.4, the subject of    similar    matrices is introduced. in particular, two

matrices, say a and p, are de   ned as being    similar    if there exists a relation:

a = sps

   1 .

(6.66)

that is, the pre- and postmultiplying transform matrices are inverses of one another. of special
interest at present is the fact that similar matrices are possessed of the same eigenvalues. to show
this we begin with the cayley-hamilton equation for a:

(a       1i)(a       2i)(a       3i)          (a       ni)v = 0 .

we can substitute a from (6.66) into (6.67)

   1       1i)(sps

   1 -   2i)(sps

   1       3i)        (sps

   1       ni)v = 0

and since i = ss

(sps
   1:
   1       1ss
(sps
s(p       1i)(p       2i)(p       3i)          (p       ni)s
(p       1i)(p       2i)(p       3i)          (p       ni)x = 0;

   1       2ss

   1)(sps

   1)(sps

   1       3ss

   1)          (sps
   1v= 0
x = s

   1v .

   1       nss

(6.67)

   1)v = 0

(6.68)

172

6. matrix eigenvalue analysis

(6.68) clearly shows the same cayley-hamilton equation, with the same eigenvalues, as (6.67).

of course, there are in   nities of similarity transforms. the trick is to    nd one in which the

analysis of the p matrix is easier to perform than the analysis of the original a.

in danilevsky   s method this is de   nitely the case.

6.8.1 danilevsky   s method
the objective of this method is to derive, from the given input a matrix, the similar p matrix:

   
          p11 p12 p13 p14

   
          .

1
0
0

0
1
0

0
0
1

0
0
0

p =

in the above, and some of the displays that follow, a 4x4 will be shown, in preference to writing out
a completely general case. the 4x4 will be clearer to follow (extension to nxn should be obvious).
note that the unity elements are not on the main diagonal     but, are one diagonal down.
all of the data in the original a matrix has been    squeezed    into the elements of the    rst row. in
order to derive the characteristic equation, we subtract    from the main diagonal and solve for the
determinant. this is most easily done by expanding in minors of the    rst row. the result:

f (  ) =   n     p11  n   1         p1,n   1       p1n = 0,
f (  ) =   4     p11  3     p12  2     p13       p14 = 0,

in general
in the 4x4 .

(6.69)

that is, the    rst row elements of p are none other than the (negatives of ) the characteristic equation
coef   cients. we are assured that the eigenvalues of p are the same as those of a, by the argument
above.we must therefore conclude that the characteristic equation is the same, and is given by (6.69).
once the characteristic equation is derived, a separate method is used to determine the eigenvalues, the
roots of the polynomial.

since p appears so very different from a, it would seem that the transform would be a very
complex one. but, that is not the case. the transform is affected sequentially by a series of very
   1
k   1 and mk   1, where k takes on the values n, n-1,    , 2, (n being the order of
simple matrices, m
a). note that the m matrices are required to be inverses.
then, the    rst transform will be an   1 = m
   1
n   1amn   1. next, transform an   1, an   2, etc.:
an   2 = m
n   2an   1mn   2 = m
   1
   1
   1
n   1amn   1)mn   2
n   2(m
n   3an   2mn   3 = m
an   3 = m
   1
   1
   1
         
n   3(m
n   2(m
       p = s
   1as = m
[a]mn   1        m2m1
       m
   1
   1
   1
; and s = 1   
   1 = n   1   
n   1
1 m
2
   1
   1
m
k .

   1
n   1amn   1)mn   2)mn   3

until    nally

m

s

k

k=1

k=n   1

6.8. the eigenvalue analysis of similar matrices; danilevsky   s method 173

   
            

   
a picture of the matrices m (with k = n) is:

   
            ; mn   1 =

                  

m

=

   1
n   1

0
0
an3

0
1
an2

1
0
an1

0
0
ann

1
0
    an1
an,n   1
0
a description of these matrices is: (for k = n, n-1,    2)

0

0

0

1

0
1
    an2
an,n   1
0

0
0
+ 1
an,n   1
0

0
0
    ann
an,n   1
1

   

                   .

(6.70)

   1
k   1

matrix m

this matrix is a unit matrix, with its k     1
row replaced by the elements of the kth row
(in (6.70), k = n).

matrix mk   1

this matrix is a unit matrix, with its k     1
row replaced by the negatives of the kth row
elements divided by the k, k     1 element.
however, the k     1, k     1 element is posi-
tive, and just the reciprocal of the k, k     1
element.

a note: equation (6.70) shows the character    n,    because in that display the second to last row is
shown. but, in the later transforms, it is not the n-1 row that is modi   ed. so, the references to    n   
in these equations changes, but of course the order of the matrix does not. at each step an index    k   
(whose initial value was n) will decrease, causing the corresponding row in m to move up.

for example, in the second step (i.e., n-2), we de   ne mn   2 and m

   1
n   2. they are constructed
from a unit matrix, with the n-2nd row taken from elements of the n-1st row of the newly de   ned
an   1 matrix. they are just like those of (6.70)     but with the modi   ed row    moved up    one.

when k equals 2, then k-1 is equal to 1 (the 1st row), the    nal two m matrices are formed
    from unit matrices, with their    rst rows taken from the elements of the 2nd row of the matrix
de   ned in the previous step. when the transform of this step is completed, the p matrix is complete.
equation (6.70) implies that a great many matrices must be kept around during the transform,
but in fact none of the m matrices need actually be calculated or saved. instead, each transform is
done in two parts:

1)b = [a]mn   1; (this is again shown with k = n)
2)c = m
   1
n   1[b]. (the result, c, is an   1) .

and

these are done using the following algorithms:

               
            

), for i < k, and j (cid:4)= k     1

bij = aij     (ai,k   1)(
akj
ak,k   1
bi,k   1 = (ai,k   1)(
1
)
bkj = 0, for all j (cid:4)= k     1; bk,k   1 = 1 .

ak,k   1

(6.71)

for k = n, n     1, n     2, . . . , 2

174

6. matrix eigenvalue analysis

note especially, the last row of b. in a 4x4, that row will be {0, 0, 1, 0}. this is already the last row
   1
n   1 will not disturb the last row. in fact, the only
of p. note also that the premultiplication of m
row affected by this premultiplication is the k-1st row. that is, in c = m
   1
k   1

[b]:

         
       cij = bij for all i (cid:4)= k     1, and all j
ck   1,j = n(cid:21)

aks bsj , for all j .

s=1

(6.72)

in forming b and c, it is not necessary to actually multiply matrices. the relations shown in (6.71)
and (6.72) are all that are needed. in carrying on to the next step, we simply set a equal to c, and
then proceed with k decreased by one. that is, we    move up one row.    and so it goes until k = 2.
notice that the de   nition of the m matrix elements includes a division. for example, in the
   rst step we divide by an,n   1. if any of these terms happens to be zero, then one must search upward
along the n-1st column (or the    k-1st    column) to    nd a corresponding element that is not zero;
   1 by the unit
and then interchange the two rows. this is the same as multiplying both m and m
matrix, with the same two rows interchanged. then the transform remains a similar transform, and
the development can proceed normally. in the event that no nonzero element can be found, the
method fails.

   1
k matrices (i.e., those de   ned in equation (6.70)) are never actually
   1 matrices will not be determined unless there is a reason to do so. if only
   1 are not needed. but, a complete eigenvalue analysis requires

calculated, the s and s
the eigenvalues are required, s and s
the vectors as well. equation (6.68) already implies that these matrices will, then, be required.

since the mk and m

distinct eigenvalues. the following paragraphs outline the method for determining the eigenvectors.
it will be noted that for each eigenvalue, just one pair of eigenvectors (row and column) is formed. if
the eigenvalues are not distinct, the method fails.

de   ning the eigenvectors
returning to the eigenvalue analysis of p; for each root, we have the following equation to solve for
the column eigenvectors: (the eigenvectors of p, are de   ned as x, column and z, row):

   
          p11       i

   
         

            
         

x1
x2
x3
x4

            
          = 0 .

p12

1      i
0
0

p13
p14
0
0
1      i
0
1      i
0

(6.73)

first, arbitrarily assign the value 1.0 to x4 (xn, in general). then, using the last 3 equations (n-1
equations in general), the elements of the ith x vector are:

for k = n     1, n     2, . . . , 1 .

(6.74)

xn = 1;

xk =   xk+1,
i ,   2

i ,   i, 1 }.

for example, in the 4x4 case, xi = {   3

(6.75)

(6.76)

6.8. the eigenvalue analysis of similar matrices; danilevsky   s method 175

for the row eigenvectors, we have [zi][p-  ii] = 0, a row equation:

(cid:17)

(cid:18)

z1

z2

z3

z4

   
          p11       i

   
          = 0.

p12

1      i
0
0

p14
p13
0
0
1      i
0
1      i
0

in this case, set z1 = 1, and then

zk =   i zk   1     p1,k   1, for k = 2, 3, . . . n .

because of the simplicity of p, its eigenvectors are easily derived. but, although the eigenvalues of
p are the same as those of a, the eigenvectors are different. starting with [a-  ii]vi:
   1vi     (p       ii)xi

(a       ii)vi = (sps
   1vi, where x is the column vector in (6.73).
we see that we must transform vi by s
therefore, to obtain v from x, we must premultiply by s. in the row vector case, the logic is very
similar, and the resulting transforms are:

   1       ii)vi = s(p       ii)s
   1. that is xi = s

vi = sxi
[ui] = [zi]s

   1 .

(6.77)

in equations (6.77), the square brackets are used just to emphasize that ui and zi are row vectors.
   1 are used in the de   nition of the vectors, then for a complete eigenvalue analysis
since both s and s
these matrices must be retained as the similarity transform proceeds.

recall, from (6.71) and (6.72), that the original a matrix is updated via the intermediate
matrices, b, and c (only one of which has to be kept     i.e., c is the    in-place    update of b). in the
same sense, de   ne a matrix,   s, which will be used to update s. at the end of each update cycle, s will
be set equal to   s, and the next cycle will again update   s. the emerging matrix s
   1 will be updated

in-place.

the relationships are very similar to those of (6.71) and (6.72):

                     
                  

   1, and   s initialized to unit matrices)

(s, s
  sk   1,j =     ak,j
ak,k   1
  si,j = si,j     si,k   1     ak,j
ak,k   1
  si,k   1 = si,k   1
= n(cid:21)
ak,k   1
   1
k   1,j

ak,p     s

s

,

p=1

, for all j (cid:4)= k     1;   sk   1,k   1 = 1
ak,k   1

, for (k     1) < i < n, and j (cid:4)= k     1

for (k     1) < i < n, and j = k     1.
   1
k,j , for all j .

(6.78)

(6.79)

176

6. matrix eigenvalue analysis

   1
i,j , means the i, jth element of s

   1.

in (6.79), the display, s
after the eigenvectors are determined, see (6.77), they still must be normalized, such that the
product ui   vi = 1. the task is trivial when the vectors are real; it is somewhat tricky when they are
complex.

in the event that the original a matrix is symmetric, only the x (column) vectors are needed.
they transform via the    rst equation (6.77), and are normalized to unit length easily, since they are
real.

6.8.2 example of danilevsky   s method
the following a matrix will be discussed at some length in the next chapter

   

                  

a =

0
0
0
0
    16
7
9
9
1    2

1
0
    2
9
1
7

   

                  

0
1
1
9
    2
7

(6.80)

danilevsky   s method will be used, here to determine the eigenvalues and eigenvectors of a. the
form of this matrix (whose upper half consists of a null matrix, and a unit matrix) arises in vibrations
problems in which damping is present. thus, physical considerations indicate that the eigenvalues
will be complex (with negative real parts). in turn, the eigenvectors will also be complex. since a is
real, the 4 eigenvalues will be in 2 pairs of complex conjugates. the 4 eigenvectors will also come in
2 pairs of complex conjugates.

(a)

0.000000
0.000000
1.000000
0.000000
0.000000
0.000000
0.000000
1.000000
   1.777778
0.777778    0.222222
0.111111
0.142857    0.285714
1.000000    2.000000

   1
the    rst display, (a), is simply the input. the next two, marked (1) and (2), are the s and s
matrices of the    rst transform step. note that since they are the    rst step, then s will be equal to
m

   1
n   1, and s

   1 will be equal to mn   1.
1.000000
0.000000 0.000000 0.000000
0.000000
1.000000 0.000000 0.000000
   7.000000 14.000000 7.000000 2.000000
0.000000 0.000000 1.000000
0.000000

(1)
(s)

(2)
   1)
(s

0.000000 0.000000
1.000000 0.000000

1.000000
0.000000
0.000000
0.000000
1.000000    2.000000 0.142857    0.285714
1.000000
0.000000

0.000000 0.000000

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

6.8. the eigenvalue analysis of similar matrices; danilevsky   s method 177

notice that in matrix (1), the (3,1) element is
(1)3,1 =     a41

a43

=    7.0

matrix (3) was not actually calculated as a matrix product, but instead, the relations (6.71) and (6.72)
were used. matrix (3) is now the new a matrix (in the text, it was labeled an   1).

   7.000000 14.000000 7.000000
2.000000
0.000000 0.000000
0.000000
1.000000
   7.031748 13.666671 6.492066    0.047618
0.000000
0.000000

0.000000 1.000000

(3)
(a)

matrices (4), (5), and (6) are the results of the second transform. (4) is not the s matrix, yet.
it is the product of mn   1 and mn   2. the matrix (6) has its last 2 rows transformed, on its way to
becoming the p matrix.

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(4)
(s)

(5)
   1)
(s

(6)
(a)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(7)
(s)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

1.000000 0.000000
0.000000 0.000000
0.514518 0.073171    0.475029 0.003484
0.349593 2.048780
0.203252 1.024390
0.000000 0.000000
0.000000 1.000000

1.000000
0.000000 0.000000
0.000000
0.682540 0.927438    1.902494
   0.539682
1.000000    2.000000 0.142857    0.285714
1.000000
0.000000

0.000000 0.000000

1.024390

0.203252
2.048780
   1.429217    0.711188    2.505872    0.739837
0.000000
0.000000
0.000000
0.000000

1.000000
0.000000

0.000000
1.000000

0.349593

   0.699684    0.497607    1.753318    0.517652
   0.360000    0.182857    1.377143    0.262857
   0.142212
1.943567
0.000000
1.000000

0.923251    0.006772
0.000000
0.000000

after the    nal transformation, all three of the matrices are fully formed. (9) now displays the

p matrix, and (7) and (8) are s and s

   1, respectively.

178

6. matrix eigenvalue analysis

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(8)
   1)
(s

(9)
(p)

   3.551272
4.526329    1.017565
1.329158
   0.539682
0.927438    1.902494
0.682540
1.000000    2.000000
0.142857    0.285714
0.000000
0.000000
1.000000
0.000000
   0.507937    3.825397    0.730159    2.777778
1.000000
1.000000
0.000000
0.000000
0.000000
0.000000

0.000000
1.000000
0.000000

0.000000
0.000000
1.000000

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)
(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

from (9), the characteristic equation is:

p4 + 0.507937p3 + 3.825397p2 + 0.730159p + 2.777778 = 0

to six decimal places. the calculations used    extended    type variables for high precision.

  1,   2 =    0.06250    j0.99811
  3,   4 =    0.19147    j1.16555 .

the matrices (7) and (8) are inverses, because each stage in their derivation used inverse matrices.
   1 produces the original a matrix. these
further, since a and p are similar, then the product sps
two checking operations will be left to the reader.
   1 are all real. however, the eigenvalues are obviously complex, and
so will be the eigenvectors. the development of the    rst eigenvector is shown in the accompanying
table.

it is notable that p, s, s

these vectors are determined by    rst calculating the x and z vectors (eigenvectors of p), using
equations (6.74) and (6.76). from there, the v and u vectors are found by using the transforms
in (6.77). the    rst column in the table shows the    rst x vector (the x vectors are eigenvectors of p).
then v1= sx1. the middle column shows the result of this calculation.

first column eigenvector

v1(norm)
0.01101
0.46837

v1

x1vector
0.18655    0.04481
   0.98266    1.00038
   0.99233    0.06249
   0.12476    0.99798
   0.06250
0.99811

1.00000
0.00000

0.01931
0.46742
1.00129    0.46817
0.01780    0.01823
1.00000    0.46775
0.00000    0.00994

6.8. the eigenvalue analysis of similar matrices; danilevsky   s method 179

all the vectors, both row and column, are transformed similarly, de   ning the (complex) matrices u
and v. after that, these two matrices must be normalized such that their product is the unit matrix.
the id172 can be accomplished in many ways (each might produce a different normalized
v1 vector in the table). the choice made, here, was to divide both ui and vi by the square root of
the dot product of ui   vi.

6.8.3 danilevsky   s method   zero pivot
each loop of danilevsky   s method uses the (k, k     1) element as a divisor. if this elelment ap-
proximates zero, the method will fail unless an altering change can be made. such a change is
possible   which will be shown using the example of the 6x6 shown here.

   

                        

(cid:5)
a
(cid:5)
11
a
(cid:5)
21
a
(cid:5)
31
a
41
0
0

(cid:5)
a
(cid:5)
12
a
(cid:5)
22
a
(cid:5)
32
a
42
0
0

(cid:5)
a
(cid:5)
13
a
(cid:5)
23
a
33
0
0
0

(cid:5)
a
(cid:5)
14
a
(cid:5)
24
a
(cid:5)
34
a
44
1
0

(cid:5)
a
(cid:5)
15
a
(cid:5)
25
a
(cid:5)
35
a
45
0
1

(cid:5)
a
(cid:5)
16
a
(cid:5)
26
a
(cid:5)
36
a
46
0
0

   

                        

in the position shown, the value of k is 4 and the (4,3) element happens to be zero (note that the
elements are shown    primed    indicating that these are not the original aij values; eg., a43 was not
necessarily zero at the beginning of the procedure).

at this point, the elements in row k (4, here) to the left of the zero element are tested for non
zero. in this example, if either the 41 or 42 elements are non zero, the procedure can be continued by
exchanging the column containing the nonzero element with the k-1 column.the column exchange
can be viewed as postmultiplying by a unit matrix with the same columns exchanged, ij,k   1.
recall that each stage of the danilevsky reduction involves the calculation of the type:

(cid:5)(cid:5) = m
   1
k   1a

(cid:5)

a

mk   1 .

and that these m matrices are very carefully constructed to be inverses.then, the postmultiplication
by ij,k   1 must be accompanied (   balanced   ) by the premultiplication of the inverse of ij,k   1. but the
inverse of ij,k   1 is simply ij,k   1.that is, the balancing operation to be performed is the interchange
of rows j and k     1. in this example, assume that a
(cid:5)
41 is non zero. in this case, columns 1 and 3
would be interchanged, and rows 1 and 3 interchanged. in this way, the method can be continued,
and the    similarity    of the a and p matrices is maintained.

180

6. matrix eigenvalue analysis

if all the elements in the kth row are zero, the row and column interchanges described above

(cid:5)
do not help. this case is illustrated with a 6x6 a

   

                         .

matrix below.
(cid:5)
(cid:5)
a
a
(cid:5)
(cid:5)
14
16
a
a
(cid:5)
(cid:5)
24
26
a
a
(cid:5)
(cid:5)
34
36
a
a
44
46
1
0
0
0

(cid:5)
a
(cid:5)
15
a
(cid:5)
25
a
(cid:5)
35
a
45
0
1

   

                        

(cid:5)
a
(cid:5)
11
a
(cid:5)
21
a
31
0
0
0

(cid:5)
a
(cid:5)
12
a
(cid:5)
22
a
32
0
0
0

(cid:5)
a
(cid:5)
13
a
(cid:5)
23
a
33
0
0
0

(cid:22)

(cid:23)

.

(cid:5) =

a

(cid:5)

1 a
a
0 a

(cid:5)
(cid:5)

3

2

note that this matrix is    naturally    partitioned

(cid:5)
the matrix a
2 is already in the correct form, and the elements in its top row are the negative
(cid:5)
coef   cients of (in this case) a 3rd degree polynomial. further, the matrix a
1 can now be analyzed
separately   which will result in another 3rd degree polynomial. the roots of these two polynomials
are the eigenvalues of the original matrix.
   1 ceases. to correct this, use the transforms for a
(cid:5)

(cid:5)
when the original method fails (6x6, at the point shown in a

1. that is, the danilevsky method produces

) the development of s and

s

this is a 3x3 transformation, in this case, with q the transform matrix. now, form m and m

   1:

(cid:5)

   1a

p

(cid:5) = q
(cid:22)

   1 =

m

   1
q
0

0
i

(cid:23)

q (see equation (6.70)) .

(cid:22)

(cid:23)

.

, and m =

q 0
0
i

note that these are 6x6 inverse matrices. pre- and post- multiply these onto the original 6x6
transform matrices, s and s

   1. the result will be the overall 6x6 transform matrices.

p = [m

   1s

   1] a [s m] .

thus, even in this case, the complete danilevsky similarity transform is available.

6.9 exercises
6.1. derive the general characteristic equation for a (3x3) by expanding
6.2. using the expansion from exercise 1,    nd the characteristic equation and then the eigen-

  i     a

(cid:12)(cid:12)

(cid:12)(cid:12).

values and vectors for the matrix, a:

a =

   
    23
4    6
   18
6
1
75 12    20

   
   

6.9. exercises 181

6.4.

6.3. using the eigenvalue data from problem 2,    nd the   k{vk}[uk] matrices (for k =1, 2, 3) and
   nd the sum of these three matrices.
(a) using the same data,    nd the matrix b =   1{v1}[u1] +   2{v2}[u2] and    nd its charac-
teristic equation.
(b) show that the eigenvectors of b are the same as those for a.
(c) given b =

   
   , and the a matrix from problem 2,    nd ba and

   
   

3

9    6    4
   12
4
24    18    11
(cid:22)
(cid:23)
5    3
   3
(cid:23)
(cid:22)    0.7
5
2
   0.6 1.5

2

   

a.

,    nd

,    nd sin(a).

ab. explain.
for the matrix a = 1

for the matrix a =

(a) given ax =   x + c, de   ne the conditions under which a solution exists.
(b) solve the equation assuming the necessary conditions.
(c) if the (2x2) a matrix is that from problem 5, and the c vector is c = {1,     1} solve the
set in terms of the parameter   . does a solution exist when    = 1?

in the polynomial

(x     xj ) = x6 + c1x5 +        + cn    nd c2 and c3. describe the for-

6   
j=1
   
    6    3
   3
0    3

   
   

0
6    3
4

mation of each of the coef   cients.

6.9. given a =

(a) use danilevsky   s method to    nd the coef   cients of its characteristic polynomial.
(b) use matrix iteration (section 6.7.2) to    nd the largest eigenvalue.
(c)    divide-out    the root from (b) and solve the quadratic for the remaining eigenvalues

of a.

6.5.

6.6.

6.7.

6.8.

c h a p t e r 7

183

matrix analysis of vibrating

systems

introduction

7.1
the eigenvalue problem, the details of which were discussed in the previous chapter, has application
in many important areas in engineering. certainly one of the most interesting is in the study of
(linearized) vibrating systems.1 these systems are a perfect and direct example of the characteristic
value problem. we will begin there, and add the non-homogeneous set as well:

(cid:24)

ax       x = 0
ax       x = c

given that a is    diagonalizable    (we omit the defective matrix case from discussion), there exist n
values of    (   eigenvalues   ) for which the homogeneous set has a solution. for each of these values the
associated solutions are the    eigenvectors,    u (row) and v (column). these two sets of solutions are
orthogonal to one another: ui   vj = 0 (i (cid:4)= j). in the event that a is symmetric, the u set is simply
the transpose of the v. in either event, the sets are normalized such that ui   vi = 1.

for the non-homogeneous equation we assume a solution of the form x = vy. then:

uav =  

            
uavy       y = uc ; (
) :
avy       vy = c ; premultiply by u :
         
(        i)y = uc
y = (        i)
   1 uc
x = vy = v(        i)
   1uc .
(cid:23)

(        i )

   1 =

  ij    

1

(  j       )

therefore

(7.3)
apparently, the inverse of (a      i) is v(        i)
   1u. of course this inverse does not exist when   
(cid:22)
is equal to one of the eigenvalues,   i. this fact is the more clear when the inverse is written as:

a diagonal matrix (note the kronecker delta,   ij ). then, in general the non-homogeneous set has
no solution when    equals one of the eigenvalues. if, however, the vector, c, is orthogonal to ui,
then the solution (7.3) holds: we maintain the orthogonality while allowing    to approach   i. in the
1it is assumed that the reader is familiar with the differential equations which govern the motion of linear vibrating systems.

(7.1)

(7.2)

184

7. matrix analysis of vibrating systems

particular case in which one of the eigenvalues is zero, a is singular. it may be recalled (chapter 4,
section 4.3) that ax = c was shown to have no solution, when a is singular, unless c is orthogonal
to all solutions of the transposed set, a   z = 0. in (7.3), above, the row vectors, ui are solutions to the
transposed set.

there are other displays and interpretations of (7.3). the most important of these will show

that the equation (7.3) can be written:

x = v(        i)

   1uc = n(cid:21)

i=1

{vi}[ui]
(  i       )

c = n(cid:21)

i=1

ui     c
(  i       )

vi .

(7.4)

the    rst summation shown in (7.4), is a summation of nxn matrices, {vi}[ui], each of which is
postmultiplied by c. the second summation shows the result of the multiplication, changing into a
sum of the vectors vi, multiplied by the scalar dot products divided by the    difference terms. this,
   nal, form will be found to be most interesting, and will provide a direct solution to the differential
equations of the vibration problem.

it will be found that much of this chapter deals with equations like (7.4). in particular, the
non-homogeneous differential equations have a solution whose form is exactly the same. in that
sense, we have already summarized much of this chapter.

setting up equations, lagrange   s equations

7.2
the systems that will be discussed herein are simple; their equations of motion will be almost trivial
to set up. however, those that are found in practice are often anything but simple. it is therefore
worthwhile to mention lagrange   s equations. his intentions were to simplify and formalize the
derivation of equations     the force diagrams, and the (tricky) determination of the correct sign to
attach to the forces.

beginning at the most simple, a mass m = w

g is suspended on a spring of spring constant, k,
in figure 7.1. assume that motion is constrained to be    vertical    and in the plane of the paper. if the

figure 7.1:

7.2. setting up equations, lagrange   s equations 185

mass is disturbed from equilibrium, the ensuing motion will be oscillatory in this one dimension.

the mathematical spring is de   ned to produce a restraining force on the mass proportional
to a change of its length. the constant of proportionality is the parameter, k. in the force diagram
to the left side of the    gure, the upward force is k(xw + x). the force kxw is exactly the amount
necessary to statically balance the weight, w .

if the mass is disturbed from its static equilibrium position, newtons laws are used to equate

the acceleration to the unbalanced force:

m  x =    kx .

(7.5)

as vibration continues, energy is continually being transferred from kinetic to potential, and back
again. no energy is lost from this theoretical system, since it has no energy dissipation terms. the
kinetic (t ) and potential (v ) energies can be written as

= m  x and dv

dx

d   x
note that dt
as follows:

= kx, and therefore these terms could be introduced into equation (7.5)

x2 .

t = m
2
(cid:22)

  x2 and v = k
2
(cid:23)

d

dt

dt

d   x

+ dv

dx

= 0 .

then the original equations of motion can be written in this way, which is the lagrange equation
for this system.

in a more general system, there may be multiple coordinates required to describe the system
motion. these may not all be rectilinear motion; the equations may describe torsion and angular
motion, or charges/currents in electrical networks. then, we must introduce the idea of    general-
ized coordinates,    q, and presuppose that multiple coordinates are present, which turn the spatial
derivatives into partial derivatives:

(cid:22)

(cid:23)

d

dt

    t

      q

+     v

    q

= 0

which is lagrange   s equation for conservative systems with no external forces present.

7.2.1 generalized form of lagrange   s equations
one of the most useful forms of the equations is
+     d
      qi

= fi (component of external force)

        t
    qi

+     v
    qi

      qi

(cid:22)

(cid:23)

    t

dt

d

(1)

(2)

(3)

(4) .

(1) inertial forces, derived from kinetic energy.

(7.6)

(7.7)

186

7. matrix analysis of vibrating systems

(2) gyroscopic and centrifugal forces. derived from kinetic energy, from changes in direction.

(3) potential forces.

(4) viscous damping forces.

derived from    rayleigh   s dissipation function.    d.
rayleigh   s dissipation function is usually denoted by the letter    f.    here a    d    is used to avoid
confusion with the external force, f. the term     d
refers to the derivative of rayleigh   s dissipation
    qi
function. it is introduced in order to account for the effects of dissipative, frictional effects. in this
function, the forces are considered to be proportional to the velocity term   qi. for a single particle
the function is simply d = 1/2c  x2. the parameter, c is the proportionality between the dissipation
force and the velocity which produces it. its electrical analog describes the power loss in the electrical
network, 1/2ri2.

for the systems of interest here, the kinetic, potential, and dissipation functions are simple

quadratic forms. for example, for a system of springs and masses:
and d = 1
2
(cid:25)

m  q; v =1
2q
(cid:24)
(cid:25)

then, we could de   ne the vectors
   q    

and      q =

t = 1
2

kq;

(cid:24)

  q

(cid:5)

(cid:5)

   

    qi

   

      qi

.

(cid:5)

  q

c  q .

in this case, we can write lagrange   s equations as:

d
dt

     q t +      q d +    q v = f
m  x + c  x + kx = f

7.2.2 mechanical / electrical analogies
the following is the equation of motion for the simple spring-mass system, accompanied by the
voltage equation for the r-l-c circuit     the diagrams for both are shown in figure 7.2.

      
    m  x + c  x + kx = f0 sin   t
= e0 sin   t .
l  q + r   q + q

c

it is apparent that the mathematics is the same for both systems, and that the solutions will consist
of damped sinusoids. these systems are, then, analogues of one another. from figure 7.2, and the
equations, the following analogues can be de   ned.the list, below, is adequate to compare and discuss
the systems dealt with herein; but, it is not an exhaustive list.

7.2. setting up equations, lagrange   s equations 187

figure 7.2: mechanical and electrical analogues.

mechanical
displacement, x
velocity,   x
force, f
mass, m
spring constant, k
compliance, 1
k
damping coef   cient, c

electrical
charge, q
current,   q or i
voltage, e
inductance, l
elastance, s = 1/c
capacitance, c
resistance, r

most of the examples to be discussed in later paragraphs will be mechanical systems. it is

important to note that the same solutions can be applied to their electrical analogies.

7.2.3 examples using the lagrange equations
as an example of the method, consider the electrical network in figure 7.3. by inspection

figure 7.3: electrical lrc network.

188

7. matrix analysis of vibrating systems

(cid:22)

(cid:23)(cid:24)

(cid:25)

1

2t = l1i2
2v = s1q2
s = 1

1

2v = [q1 q2]
2d = r1i2

1

i1
i2

2

= [i1 i2]

+ l2i2
*
+ s2(q1     q2)2 + s3q2
(cid:22)
c and q =
s1 + s2

   s2
   s2 s2 + s3
+ r2(i1     i2)2 + r3i2

0
l1
0 l2
; where
(cid:25)
(cid:23)(cid:24)
(cid:22)

= [i1 i2]

q1
q2

idt

2

2

r1 + r2

   r2
   r2 r2 + r3

(cid:23)(cid:24)

(cid:25)

i1
i2

then the equation set is: l  q + r  q + sq = e where l, r, and s are the 2x2 matrices, above. this
is a voltage equation, and could (perhaps more easily) have been determined using kirchhoff    s laws.
even in this case, though, note that there was no trouble or hesitation with the correct signs to
use. for example, in both v and d the difference terms, e.g., (i1     i2)2 could have been written
(i2     i1)2.

further, it is often not that easy. try this next example     a double pendulum. the use of

lagrange   s equations comes in particularly handy.

take, as the origin, the point of support of both pendulums, o. the inertial, rectangular
coordinates x and y are to be measured from this point, and the generalized coordinates   1 and   2
will be referred to x, and y. the upper weight is at (x1, y1), the lower at (x2, y2). the kinetic energy

figure 7.4: double pendulum.

is

there are 4 relations between the generalized and the inertial coordinates. they are

-

t = m1
2

(  x2

1

+   y2

1 ) + m2

2

(  x2

2

+   y2
2 ) .

x1 = l1 sin   1; x2 = l1 sin   1 + l2 sin   2;
y1 = l1 cos   1; y2 = l1 cos   1 + l2 cos   2 .

7.3. vibration of conservative systems 189

these relations must be differentiated and plugged into the expression for t, to eliminate x and y in
favor of the angular measurements. the result is
+ l2

    2 cos(  1       2)] .
the potential energy is solely due to vertical position within the gravitational    eld:

t = m1
2

+ 2l1 l2

+ m2
2

[l2

     2

     2

     2

    1

l2
1

1

1

1

2

2

v = m1gl1(1     cos   1) + m2gl1(1     cos   1) + m2gl2(1     cos   2) + constant.

the form of lagrange   s equation to use is:

(cid:22)

(cid:23)

d

dt

    t

        i

        t
      i

+     v
      i

= 0;

i = 1, 2

after some algebraic manipulation of the derivatives involved, the two nonlinear equations in
  1 and   2 are:

    1 + (m1 + m2)g sin   1 + m2l2{    2 cos(  1       2) +      2

l2

(m1 + m2)l1
    2 + g sin   2 + l1{    1 cos(  1       2)          2
.

(cid:31)

1 sin(  1       2)} .
(cid:31)

these equations can be linearized, for small amplitude vibrations, to:
(m1 + m2)g 0

(m1 + m2)l1 m2l2

1 sin(  1       2)}
 -

.

 -     1    2

+

l1

l2

0

g

  1

  2

.

this problem, and especially its derivation, is a classic one found in many applied mathematics texts.
the derivation is included here to show the power and comparative ease of the lagrange equations.
it is doubtful that any other approach would be successful. fortunately, the other examples used in
this chapter are very much simpler.

vibration of conservative systems

7.3
begin with an analysis of    conservative systems,    which have no dissipative elements     no    dashpots   
in the mechanical case, no resistance elements in the electrical network.the absence of such elements
makes these networks    conservative    in that no energy escapes the system. vibrations once started
continue inde   nitely.

the analysis of conservative systems is simpler, and moreover, will provide the method by

which the more complex non-conservative networks are handled.

both of the diagrams of figure 7.5 depict conservative systems in which two dynamic variables
are required to describe the complete vibration (e.g., 2 currents, i1 and i2, in figure 7.5 (a)). the
analysis will not be limited to two variables,since the development will be in terms of matrix elements.
the two networks of figure 7.5 are analogues. as discussed in the previous section, the same
equation type is used for both. there are two basic ways in which to derive these    equations of

190

7. matrix analysis of vibrating systems

figure 7.5: (a) electrical lc network, (b) analogous mechanical system.

motion.    first, for the electrical network, we could use kirchhoff    s laws, summing voltage drops
around each loop:

*
*

*
*

di1
dt
di2
dt

+ 1
c1
+ 1
c3

l1

l2

i1dt + 1
i2dt + 1

c2

c2

(i1     i2)dt = e(t )
(i2     i1)dt = 0 .

alternatively, by using the t and v from figure 7.3 (the same as figure 7.5 (a), just neglect the
resistance elements), in terms of charge, q, and using elastance in place of capacitance:

(cid:31)

 

(cid:31)

0
l1
0 l2

  q +

s1 + s2
   s2

   s2
s2 + s3

 

-

q =

.

e(t )
0

.

(7.8)

l  q + sq = e(t ) =

the mechanical equivalent of using kirchhoff    s laws would be to sum forces on each of the masses,
m1 and m2, and (using newton   s laws) equating to the acceleration force. however, since the systems
of figure 7.5 (a) and 7.5 (b) are analogues, and knowing that the analog of inductance is mass, m,
the analog of q (charge) is displacement, x, and the analog of elastance is spring stiffness, k, the
equations for the mechanical system can be written directly:

m  x + kx = df (t );

d = {1, 0} .

(7.9)

the matrix elements can be taken directly from their analogues in (7.8):

(cid:22)

(cid:23)

(cid:22)

0
m1
0 m2

{  x} +

k1 + k2
   k2

   k2
k2 + k3

(cid:23)

{x} =(cid:19)

(cid:20) =(cid:19)

(cid:20)

d

f (t ).

(7.10)

f (t )

7.3. vibration of conservative systems 191

note that f (t ) is a scalar multiplier of the vector, d. in the example, d = {1, 0}, signifying that the
driving function is applied to m1 only. of course this need not be the case     f (t ) might well be
applied to all the masses, or a different force function might be applied to each. in this latter case
(different drivers) solutions for each excitation are determined separately, then added together at the
end. this strategy is successful when the subject systems are linear.

in an nxn case (e.g., n masses in figure 7.5 (b)), the equations of motion are still written:

m  x + kx = df (t )
l  q + sq = de(t )

.

(7.11)

in paragraphs that follow, solutions for the    rst of equations (7.11) will be discussed. it should be
clear that the analysis holds equally for the electrical analog.

in (7.11), the matrix m is often diagonal, and always symmetric and positive de   nite. the
matrix k is often tridiagonal (having non zero elements on only the main diagonal, and the adjoining
   codiagonals   ), always symmetric, and positive. it may not be positive de   nite, because it is sometimes
singular. the result is that the eigenvalues and eigenvectors describing these networks will always
be real (not complex). further, the m and k matrices will be diagonalized simultaneously by means
of the eigenvectors, as shown in following paragraphs.

7.3.1 conservative systems     the initial value problem
beginning with (7.11), the driving vector is neglected and the resultant set solved to determine the
   natural vibrations    which would occur if the system is disturbed from its static equilibrium state.
at the instant of the disturbance, each mass may be given an initial displacement, x0, and/or an
initial velocity,   x0. we will see that these two initial conditions will be just enough to determine the
constants of integration in the solution. the homogeneous equations are:

m  x + kx = {0} .

   1m, or the inverse dynamical
   1).we will use the inverse dynamical matrix,and will call it    a.   that is,by premultiplying

often, this set is written in terms of the    dynamical matrix,    d = k
matrix (d
by m
(7.12)
assume a solution set of the form x = vej   t ,   x = j   vej   t ,   x =      2vej   t and (7.12) becomes:

  x + ax = {0}, where a = m

   1 the set becomes

   1k .

         
      

     2v + av = {0}
av =   2v
av =   v, with   =  2 .

(7.13)

192

7. matrix analysis of vibrating systems

the eigenvalue problem is discussed in chapter 6 where it is shown that if a is nxn, there will be
n solutions to (7.13), each associated with a separate eigenvalue (for now, this discussion is limited
to    distinct    eigenvalues). the matrix a is generally not symmetric, although its eigenvalues will all
be real. then for each eigenvalue,   i:

avi =   ivi
uia =   iui .

(7.14)

that is, the non-symmetric matrix a has both row eigenvectors, ui, and column eigenvectors, vi.
the ui vector associated with   j is orthogonal to the vi vector associated with   i (ui   vj = 0, i (cid:4)= j ).
the row vectors are brought together into matrix, u, and the columns, respectively, numbered, into
v, usually normalize such that uv = i. this is all well known, from chapter 6.

in this case, however, some additional orthogonality conditions exist. in (7.14), premultiply

by m, remembering that a = m

   1k, then write, for two eigenvectors:

premultiply the    rst equation by v
both k and m are symmetric:

kvi =   imvi
kvj =   j mvj .
(cid:5)
(cid:5)
j and the second by v
i. now, transpose the second equation. since
j kvi =   iv
(cid:5)
(cid:5)
j mvi
v
j kvi =   j v
(cid:5)
(cid:5)
v
j mvi .

(7.15)

now, when the second equation is subtracted from the    rst, the identical left sides cancel

(  i       j )v

j mvi = 0 .
(cid:5)

since the two eigenvalues are not equal (by hypothesis), then it must be concluded that

j mvi = 0, and thus v
(cid:5)
v

j kvi = 0 ,
(cid:5)

and this is an important and useful conclusion. the column vectors, v, are said to be orthogonal
   with respect to m, or k.    the total equation set can be assembled as follows:

kv = mv 
kv) = (v

(cid:5)

mv)  .

(cid:5)

(v

(7.16)

in (7.16), the v matrix is the ordered assemblage of the column eigenvectors. the    matrix is
diagonal, with its ordered set of eigenvalues on the diagonal.    ordered    means that the position
of the eigenvalue on the diagonal of    must correspond with the position of its eigenvector in v.
the second equation (7.16) is clearly all-diagonal. the eigenvector set diagonalizes both m and k,
simultaneously. if the eigenvectors are normalized to v

mv = i, then v
(cid:5)

kv will be equal to    .

(cid:5)

7.3. vibration of conservative systems 193

in addition to all this    new orthogonality,    recall from chapter 6:

av = v , and ua =  u .
and if u and v are normalized such that uv = i(the usual case)

uav =  ,

(with uv = vu = i) .

m1

a =

the system shown in figure 7.5(b) will be used to illustrate the analysis of conservative systems.
using equation (7.10), with the parameter values from the    gure, a    by-hand    eigenvalue analysis is
given below. in a more complex case this eigenvalue analysis would be done by computer

   
          k1 + k2
   k2
   k2
k2 + k3
      
)   + k1k2 + k1k3 + k2k3
using the values m1 = 9, m2 = 7, k1 = 9, k2 = 7, k3 = 7 from figure 7.5 (b):
-
= 1.0

   
          k1 + k2
   k2
(cid:12)(cid:12)a       i
(cid:22)

   k2
k2 + k3
(cid:12)(cid:12) =(cid:12)(cid:12)a(  )
(cid:23)

   
         

      

m1m2

m2

m1

m2

m2

m1

m1

m2

.

   
         ; (a       i) = a(  ) =
(cid:12)(cid:12) =   2 + (
+ k2 + k3
(cid:12)(cid:12)a(  )
 
 

k1 + k2
(cid:12)(cid:12) =   2     34
9    + 175
-
.

;     v1 =

    7

1
1

m1

m2

;

63

(cid:25)

(cid:24)

;     v2 =

7

9   1

  1 =   2
  2 =   2

1

2

= 7
= 25

7

9

and u1 = {1,

}

7
9

and u2 = {1,    1}
(cid:23)

(cid:22)

a =

    7
9
2

16
9

   1

(cid:31)
(cid:31)

a       1i =

a       2i =
(cid:23)
(cid:22)

7
9

9
   1
1
   1     7
   1     7

9

9

then v =
(cid:22)

1
1    1
(cid:23)
the product v

7
9

, and after normalizing for uv = i, u = 1
mv , (not normalized to equal i, since uv has been normalized to equal i) is

9
7
9    9

16

.

(cid:5)

kv will not be  . however, note that:

16
0

0
112
9

(cid:5)
and therefore v

(cid:31)

 

(cid:5)

kv =

v

16
0

0
    112

9

25
9

, which does equal v

(cid:5)

mv  (see (7.16), above).

it is worthwhile to show that uav =  . that is
    7
1    1

uav = 1

9
7
9    9

(cid:22)

16
9

16

9

(cid:23)(cid:31)

 (cid:31)

 

(cid:22)

=

(cid:23)

.

1
0

0
25
9

7
9

1
1    1

(7.17)

194

7. matrix analysis of vibrating systems

with the eigenvalue analysis complete, and its results in hand, return to the initial value problem:

m  x + kx = {0}       x + m

   1kx = {0} =   x + ax = {0} .

with the knowledge that the eigenvector matrices diagonalize a, the equation set can be    decoupled   
by the vector transform x = vy. then:

  x + ax = {0}; substitute x = vy
v  y + avy = {0}; premultiply by u
uv  y + uavy = {0}; where uv = i
  y +  y = {0} .

(7.18)

this wonderful result produces a y equation set that is completely decoupled     each yi can be
solved for separately, from   yi +   i yi = 0, a very simple differential equation. we    nd the solution

yi = ai cos   i t + bi sin   i t; where   2

=   i .

i

now, assemble the individual solutions together to form the vector solution to (7.18), in the 2x2
case, it is simple to write the expanded matrices:

 (cid:31)

 

cos   1t

0

0

cos   2t

a1

a2

(7.19)

(cid:31)

    y =
 

b1

b2

.

.
y1 = a1 cos   1t + b1 sin   1t
 (cid:31)
y2 = a2 cos   2t + b2 sin   2t

(cid:31)

+

sin   1t

0

0

sin   2t

(cid:17)

(cid:17)

(cid:18)

(cid:18)
y = [c]a + [s]b; where

s

(cid:17)

c

(cid:18)     [  ij cos   i t] and
0, i (cid:4)= j
1, i = j

(cid:24)

in the general (i.e., nxn) case, the form of the solution set is the same. then for the nxn case, de   ne
two diagonal matrices

such that:

and

c

(cid:17)

(cid:18)     [  ij sin   i t] .

s

(7.20)

the symbol,   ij , is the    kronecker delta:      ij =
construct for the    cos matrix    and    sin matrix    as used, above. in (7.20) there are two columns
(nx1) of undetermined coef   cients (2 times n coef   cients in all). but, we have 2 columns of initial
conditions that must factor into the solution. these (2 times n) conditions will serve to determine
the a and b coef   cient vectors. denote the condition vectors as x0 and   x0, whose elements represent
the initial displacement, and initial velocity of the masses in the system. these vectors must be
transformed via y = ux to obtain the initial values for the variables y.

which forces the diagonal matrix

s

(cid:17)
(cid:18)
(cid:17)
= [0], and
(cid:18)
(cid:18)
a +(cid:17)
y =(cid:17)
t=0
  y =    (cid:17)
(cid:18)(cid:17)
(cid:18)
a +(cid:17)
b;   y0 =(cid:17)
b; y0 = a = ux0

(cid:18)
t=0 =i. then, from (7.20):
(cid:18)
(cid:18)(cid:17)

b = u  x0 .

(cid:18)

c

c

c

  

  

  

s

s

first notice that

(7.21)

in the second of (7.21) the matrix

and since x =vy, we premultiply (7.22) by v to return to the x variables.

7.3. vibration of conservative systems 195

is diagonal = [  ij   i]. then a = ux0 and b =(cid:17)
(cid:18)
ux0 +(cid:17)

(cid:18)(cid:17)

c

  

s

(cid:18)   1 u  x0

  

  

(cid:18)
(cid:17)
y =(cid:17)
(cid:17)

(cid:18)

x = v

ux0 + v

c

(cid:18)   1 u  x0
(cid:18)   1 u  x0 .

  

(cid:17)

(cid:18)(cid:17)

s

(7.22)

(7.23)

(cid:17)

(cid:18)

(cid:17)

(cid:18)

are func-
at    rst, (7.23) appears to be very formidable, and not easily programmed.
tions of time. a straightforward expansion would be very messy. fortunately, there is an excellent
interpretation of (7.23) which not only makes it clearer to    see,    but, is also easily programmed.

and

c

s

i=1

interpretation of equation (7.23)

7.3.2
in the previous chapter there is a discussion of the synthesis of a matrix by its eigenvalues and
eigenvectors. equation (6.14) of that chapter reads:
  i{vi}[ui] .

a = n(cid:21)

(chapter 6, (6.14))

this result occurs through an interpretation of a = v u. the central idea is that   is a diagonal
matrix. its jth main diagonal element, i.e.,   j , multiplies the jth column of v (or form the product
(cid:17)
(cid:18)
 u    rst, in which case, the jth eigenvalue will be a multiplier on the jth row vector in u).
c

the same logic is used here concerning the term v

ux0 in (7.23). in this case,

(cid:17)

(cid:18)

diagonal matrix. its jth term is cos   j t and it multiplies the jth column of v. now, view the v
matrix as partitioned by columns:

is the
c

(cid:17)

(cid:18) =

v

c

   
   

      
   

      
   

      
   

      
      

v2 cos   2t

      
      

v1 cos   1t

   
   

      
   

      
      

vn cos   nt

c

(cid:17)
(cid:18)
      
         

      
   

and the u matrix partitioned by rows, and the product is written:

u = v1u1 cos   1t + v2u2 cos   2t +        + vnun cos   nt = n(cid:21)

(cid:17)

(cid:18)

v

c

i=1

viui cos   i t

(cid:17)

(cid:18)

a summation of nxn   s, each multiplied by the corresponding diagonal element of the center matrix.
this is the same (desired) result as before (chapter 6; (6.14))     with the   i values replaced by the
cos   i t terms. now, the term v
u does not look at all formidable, since the time varying terms
appear as multipliers on an entire matrix entity.

c

(cid:17)

(cid:18)

and, it gets better. note that v

u involves summing n nxn   s. but, when the x0 vector is
post multiplied, it actually simpli   es the sum     it is easier to operate with vectors than matrices. since

c

196

7. matrix analysis of vibrating systems

the ui terms are rows, they are    available    to dot into the x0 vector. then;

(cid:17)

v

c

(cid:18)

ux0 = n(cid:21)

i=1

vi (ui     x0) cos   i t .

(7.24)

and the term is now composed of just n eigenvectors, weighted as shown in (7.24). this is easily
visualized, and easily coded. the time dependent (cos) terms are straightforward scalar multipliers,
as are the dot product ui     x0 terms.
(cid:29)

(cid:18)   1 is still just a diagonal, sandwiched between v and u. the inverse

returning to (7.23), its second term can now be written out by inspection. note that the

(cid:18)   1 is just

(cid:18)(cid:17)

(cid:30)

(cid:17)

(cid:17)

  

  

s

product
.

  ij

1
  i

and now, putting (7.14) and (7.25) together:

v

(cid:17)

s

  

(cid:18)(cid:17)

(cid:18)   1 u  x0 = n(cid:21)
vi (ui     x0) cos   i t + n(cid:21)

i=1

x = n(cid:21)

i=1

vi (ui       x0)

1
  i

sin   i t

vi (ui       x0)

1
  i

sin   i t.

i=1

(7.25)

(7.26)

this is a general result, the initial value problem solution applicable to nxn systems (networks).
notice that the eigenvalue analysis is    sum and substance    of the solution. except for the given
initial conditions, all terms are from that analysis (it is required that the eigenvalue analysis produces
both sets of eigenvectors, normalized such that uv = i.)
  x0 = {0, 1}, we    nd

in the particular 2x2 example from figure 7.5 (b), with the initial conditions x0 {1, 0} and

16

16

u1     x0 = 9
u2     x0 = 9
u1       x0 = 9
u2       x0 = 9
-
.

16

16

16

16

16

   0 = 9
   1 + 7
16 )    0 = 9
   1 + (    9
   1 = 7
   0 + 7
16 )    1 =     9
   0 + (    9
16 .
.
-

16

16

plugging these values into (7.26):

(cid:24)

(cid:25)

x= 9
16

1
1

cos 1t + 9
16

7

9   1

cos 5

3 t + 7

16

1
1

sin 1t    9

16

    3

5

.

-

7

9   1

sin 5

3 t.

(7.27)

from the display in (7.27), it is clear how the v vectors sum to form the total solution. these v
eigenvectors are called the    normal modes    of the vibration.the absolute amplitudes of the vibration
are of course strongly affected by the initial conditions. but, at each of the frequencies, the ratios of the

7.3. vibration of conservative systems 197

amplitudes remains always the same     in the proportions given in the eigenvectors. the eigenvectors
form the structure of the solution.

figure 7.6, below, shows this pictorially. notice that the 1 rad/sec vibration is in phase, and in
3 rad/sec vibration is out of phase, in the ratio of    7:9. the total motion

the proportion of 1:1. the 5
for both masses is shown in the right-hand diagram of the    gure.

figure 7.6: the normal modes of the system of figure 7.5 (b), and how they sum together.

the    gure shows several seconds of the solution of the initial value problem from figure 7.5 (b),

with the initial conditions { 1 0 }, as discussed, above.

mathematically speaking, this motion would continue forever, without dying out, because this
is a conservative system in which there are no elements to dissipate energy. of course, such systems
cannot be found in nature. there will always be some    damping    (resistance to motion), the simplest
of which will be discussed below. also, there are usually non-linearities, which we will not discuss.

7.3.3 conservative systems - sinusoidal response
consider, now, the same (conservative) system as before (figure 7.5 (b)), but, now include the driving
vector, as in equations (7.11)

m  x + kx = f (t ) = {d}f (t ) .

(7.28)

198

7. matrix analysis of vibrating systems

we will assume the function f (t ) = cos   t. premultiply (7.28) by m
again make the vector transform x = vy

   1 (m is nonsingular) and

  

(cid:17)

  2

  y =    (cid:17)
(cid:18)

(cid:18)

  2

y cos   t:

  x + ax = m
   1d cos t
  y +  y = um
   1d cos   t
(cid:18)
  y =    (cid:17)
y sin   t;
we assume the particular solution y = y cos   t;
   (cid:17)
(cid:18)
(cid:18) =(cid:17)
(cid:18)(cid:17)
(cid:17)
(cid:18)   1 um
y =(cid:17)
y +  y = um
   1d;
  ij (  i       2)
   1d
(cid:18)
(cid:17)
a +(cid:17)
(cid:18)
(cid:18)
is a diagonal matrix; and note that   i =   2
i .
(cid:30)   1
(cid:29)
y =(cid:17)
a +(cid:17)
(cid:18)
b. then:
   1d cos   t .
x = n(cid:21)

  ij (  2
the homogeneous solution is already known to be
      2)

    n(cid:21)

vi (ui     m

(7.29)
assuming that the system is initially at rest x0 = y0 =   x0 =   y0 = 0, it is a simple matter to solve for
a (b is clearly 0), and the solution, x = vy, becomes

.

(7.30)

      2)

   1d)

   1d)

  ij (  2

b +

where

um

  2

(cid:18)

c

c

  

  

s

s

i

i

cos   i t
      2
  2
i

i=1

note that the    rst term (the summation multiplied by the driving frequency) need not be written
as a summation. since the only function of time is already a separate multiplier, this term could be
   1d cos   t.that is, the single time
   interpreted-back    into the matrix operations: v
function, cos   t, (   without subscript refers to the driving frequency) multiplies all of the eigenvectors
in its summation. in the second summation, each multiplier, cos   i t, multiplies its corresponding
vector, vi. because of this (second) term, the vector summation is required     and it is the same sum as
in the previous term. therefore, the vector form of (7.30) is clearer, and the corresponding program
simpler, written this way. in fact, (7.30) can be written:

        2

cos   t
      2
  2
i
(cid:17)

i=1

vi (ui     m
(cid:18)   1 um

x = n(cid:21)

i=1

vi (ui     m

   1d)

cos   t     cos   i t

      2

  2
i

.

(7.31)

a conservative system should not be driven at a frequency equal, or very close to, one of the natural,
   mode frequencies.    equation (7.30) clearly indicates why, with the difference frequencies in the
denominator. however, note that if the corresponding dot product term ui     m
   1d is zero, then
that eigenvector-term will not appear in the sum. the condition required for this to be true can be
determined as follows:
mv = p is a diagonal matrix.the values of the diagonal

it has already been established that v

(cid:5)

elements of p (they   ll all be positive) depend on the id172. but, the inverse of p is:

   1 = um

   1u

(cid:5)

p

7.3. vibration of conservative systems 199

which is clearly diagonal. then, if the vector d is set equal to, say, u1, then the dot product of
u2     m
   1u1 will be zero     allowing the system to be driven at (or very close to)   2.
now, if the initial conditions x0 and   x0 are not zero, then a bit of arithmetic gives:
vi(ui     x0) cos   i t + n(cid:21)
x = n(cid:21)

sin   i t + n(cid:21)

cos   t     cos   i t

vi(ui       x0)

vi (ui     m

   1d)

.

1
  i

i=1

i=1

i=1

      2

  2
i

(7.32)
note that this is the sum of the initial value problem, plus the driven system solution with zero
initial conditions (the sum of equations (7.26) and (7.31)).

7.3.4 vibrations in a continuous medium
the vibrations in a continuous medium, like a beam, string, or reed, can be simulated in a matrix
approach by    digitizing    the medium.this approach is used in appendix c in the study of a vibrating
string. here, we consider a beam, or reed, using the same method.

let it be required to    nd the lower natural frequencies and normal modes of a vibrating can-
tilever beam. like the analysis of the vibrating string, the beam is to be    divided    into a (large) number
of segments. the matrix that results is symmetric, large. usually only a few natural frequencies are
required, a situation that lends itself to the use of matrix iteration. see section 6.7.2.

the diagram below shows a cantilevered beam. we visualize the mass of the beam to be
concentrated at n points along its length     the remaining structure of the beam retains its bending
properties. the mathematical model is no longer one of a continuous beam described by a partial

differential equation. instead it resembles a spring mass system of order n . distances are to be
measured from the support (left) end. the length, l, is divided into n parts, at the center of each
lies the mass, m, of that part. let us number (index) the mass points from the left, starting from 1,
and note that the dimension to the kth point is

xk = l
2n

(k     1) + l
2n

= l
2n

(2k     1) .

(7.33)

that is, the kth mass point lies at a distance xk from the support end, where k and xk are given
by (7.33). the total mass of the beam is m, and each mass point has the mass m = m/n.

200

7. matrix analysis of vibrating systems

from the equations governing the bending of such a beam, the de   ection, y(x), at a point x,

caused by a load, p, applied at a point s, is

y(x) = px2
y(x) = ps2

ei

ei

"
"

#
#

    x
6
    s
6

s
2
x
2

;
;

x < s

x > s

where e is young   s modulus, the ratio of stress, psi, to strain, in./in.; and i is the second moment
of the beam   s cross sectional area, in.4. note that the two equations are reciprocal in x and s.

the de   ection at x, caused by the load at s, is the same as a de   ection at s caused by the load

at x. this    reciprocity    assures that the matrix to be de   ned below, will be symmetric.

de   ections are of interest only at the mass points. in particular, the de   ection y(xi ) is written

yi and we denote the positions of the loads, pj

yi = 1

ei

(cid:5)

2l2
i
4n 2

(cid:5)

j
l
4n

(cid:5)

    i

l
12n

,

+

(cid:5), as xj
pj; note:

(cid:5). then the de   ections are

(cid:5)

i

, j

(cid:5) = 1, 3, 5       (2n     1)

(cid:5)

i

< j

(cid:5)

,

.

(7.34)

(cid:21)

j

+

(cid:5)

the de   ection at point i
(cid:5). the term on the right side (a function of i
xj
 i,j . the matrix equation can then be written

is the sum of the elemental de   ections caused by all the loads at points
) de   nes the elements of an n by n matrix

and j

(cid:5)

(cid:5)

(cid:31)
(y) = l3
 i,j =

ei

+

, 

(cid:5)

(cid:5)     i
3

;  j,i =  i,j;

(cid:5) = 2i     1,

i

(cid:5) = 2j     1 .

j

(7.35)

j

 i,j (p)

(cid:5)

2

i

16n 3

as explained, the vector y ={yi} is the de   ections at the mass points. the vector p consists of the
loads at these same points. in the vibrating beam, these are the d   alembert inertial loads and are
seen to be in the negative direction     opposite to the direction of positive de   ection:

pj =    m  yj =     m

n

  yj .

now, equation (7.35) can be updated:
y =     ml3
ei
(cid:5)
 i,j = i

2

   y
+

(cid:5)

(cid:5)     i
3

j

16n 4

,

;  j,i =  i,j .

assuming solutions of the form y = vcos  t (  y =    v  2 cos   t ):

v cos   t =     ml3
  v =  v; where    = ei

ei

 (   v  2 cos   t )

  2ml3 .

(7.36)

(7.37)

7.4. nonconservative systems. viscous damping 201

the last of equations (7.37) is the eigenvalue problem, where the elements of   are de   ned in (7.36).
from this point, iteration is used to derive the eigenvalues and eigenvectors as discussed in chapter 6.
note that the iteration procedure converges to the largest eigenvalue, but to the smallest natural
frequency     the one of most interest.

this very interesting approach, is also very simple. however, it is approximate by nature, and

becomes more so as the matrix is de   ated after each eigenvalue, eigenvector is found.

7.4 nonconservative systems. viscous damping
the introduction of viscous damping terms is a very serious complication. there is no (eigenvector)
matrix which simultaneously diagonalizes 3 matrices. then, the equations:

l  q + r  q + sq = {d}e(t )
m  x + c  x + kx = {d}f (t )

(7.38)

cannot easily be attacked directly, at least in the same way that conservative systems were. for
example, if x = ve  t is substituted into (7.38), then, for the homogeneous system:

(cid:29)

m  2 + c   + k

(cid:30)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

wherein the nxn matrix elements can be written:

m11  2 + c11   + k11 m12  2 + c12   + k12
m21  2 + c21   + k21
mn1  2 + cn1   + kn1

. . . . . .
. . . . . .
. . . . . .

. . . . . .

v = 0

. . . . . . m1n  2 + c1n   + k1n

. . . . . .
. . . . . .

. . . . . . .
. . . . . .

. . . . . . mnn  2 + id98   + knn

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

this is the    lambda matrix    for the system. the characteristic equation can be found by hand-
calculating the determinant of the matrix. the order of the polynomial will be 2n, there will be 2n
eigenvalues, and each will be associated with a row and column vector. but, there does not appear to
be a more systematic way to attack the problem     one that uses the power of the computer, and/or
one that provides the familiar ax      x = 0, characteristic equation.

a systematic approach is available. the central point is to reduce the equation set to    rst order.
this can be done in the general case, where the original equations are order m.the result is a lambda
matrix of the recognizable type.

first, de   ne the operator p = d

dt

, and note that the original set is written:

[a0pn + a1pn   1 +        + an   1p + an]x = f

where n > 1, and the order of ai is m. assuming that the matrix a0 is not singular, the set can be
reduced to    rst order, after premultiplying by the inverse of a0:

202

7. matrix analysis of vibrating systems

the reduced set is: py = ay + g where the vectors y and g are given by

y = {x1 . . . xm, px1 . . . pxm, . . . . . . , pn   1x1 . . . pn   1xm}, and
g = {0, 0, . . . , a
0 f} .
   1

the new a matrix in py = ay + g is nmxnm. it is diagrammed, below. it is partitioned into square
matrices of order m: each 0 represents an mxm null, and im represents the order m unit matrix:
note that the im submatrices are not on the main diagonal, but, the    rst upper codiagonal.

   
               

a =

im
0
0
0
      
      
0
0
   an    an   1

       0
0
im        0
       0
      

0

0
0
      
im
   a1

   
                , an nxn with each element an mxm.

the mxm submatrices are ai = a

   1
0 ai.

in particular, when the equations are the lagrangian equations for small vibrations of a linear

system, in the generalized coordinates, q, then
m  q + c  q + kq = f ,

|m| not equal to zero

can be reduced to the 2mx2m set

  z     az = b

   1h, where a =

(cid:22)

i
0
   1k    m
   m
   1c

(cid:23)

;

(cid:22)

and b =

(cid:23)

.

c m
m 0

the very same results are achieved by a method attributed to k. a foss,2 although the development
which follows is slightly different. it is worthwhile to follow from the beginning, because it very
carefully preserves symmetric submatrices.

considering the second equation of (7.38), a trivial equation is added:

(cid:25)

now, de   ne h =

(cid:24)

d
0

(cid:22)

m  x + c  x + kx = {d}f (t )
m  x     m  x = 0
(cid:24)
(cid:25)
x  x
(cid:23)(cid:24)   x  x
(cid:25)

k
0
0    m

(cid:23)(cid:24)

(cid:22)

+

x  x

(cid:25)

, and write the set (7.39) as:

(cid:24)

=

and z =

c m
m 0

(7.39)

(7.40)

(cid:25)

f
0

= h.

2   coordinates which uncouple the equations of motion of damped linear systems,    submitted to the asme applied mechanics
division, may, 1957, by k. a foss, massachusetts institute of technology.

this can be written:

b  z + gz = h; where b =

7.4. nonconservative systems. viscous damping 203

(cid:22)

(cid:23)

(cid:22)

(cid:23)

c m
m 0

and g =

,

k
0
0    m

.

(7.41)

the inverse of b is

in (7.41), 0 is the nxn null matrix, and it should be recalled that m, c, and k are symmetric.
therefore, both b and g are symmetric! a good sign, it looks like the conservative case, except that
the z vector appears in    rst derivative, rather than second.
   1
m
0
   1    m
   1
   1cm
m
(cid:22)

   1 =
b
   1:
and so premultiply (7.41) by b
  z     az = b
   1h, where a =

.

(7.42)

(cid:23)

(cid:22)

(cid:23)

i
0
   m
   1k    m
   1c

the 2nx2n a matrix is the same as given above. now, (7.42) resembles the conservative case and
will have similar orthogonality conditions.

7.4.1 the initial value problem
as with the conservative case, this problem begins with (7.42), but with a zero driving vector:

  z     az = 0 .

(7.43)

in (7.43) the solution vector z = ve  t is assumed, with the result:

  ve  t     ave  t = 0,

or av       v = 0

and the eigenvalue problem is evident.

this time, knowledge of the physical system predicts that the nonsymmetric matrix, a, has

complex eigenvalues and eigenvectors. the situation is summarized as:

(cid:129) the original equation set is nxn. the symmetric matrices m, k, and c are real, not complex.

(cid:129) matrix a is 2nx2n. it is not symmetric, but, it is real.
(cid:129) the eigenvalues are in complex conjugate pairs   k =   k    j   k. there are n pairs of these.
they will be found via the eigenvalue analysis. if the physical system is stable, the real parts
of the eigenvalues will (must) be negative.

(cid:129) as a matter of convenience, it will be assumed that the odd numbered eigenvalues are chosen
as those with positive frequencies (e.g.,   1 =   1 + j   1, then   2 =   1     j   1).

204

7. matrix analysis of vibrating systems
(cid:129) each eigenvalue-pair is associated with a pair of column eigenvectors, and a pair of row eigen-
vectors.these are also complex conjugates. even though they are    paired,    each of these entities
retains its own number. then, for example,   1 and   2 are complex conjugate eigenvalues, and
the associated vectors (v1,v2) and (u1,u2) are complex conjugates and are also available from
the eigenvalue analysis. as before, it is assumed that the normalizing has been done such that
u and v are reciprocal matrices: ui     vj = 0, and ui     vi = 1.
as in the conservative case, the eigenvectors, v, diagonalize both b and g, simultaneously.

the proof of this is the same as before:(cid:24)

gvi = bvi   i
gvj = bvj   j .

(cid:5)

      
   

(cid:5)
if the    rst of these equations is multiplied by v
i, and then the second is
transposed (both g and b are symmetric), the left sides will again be identical, and will cancel,
leaving:

(cid:5)
j and the second by v

(cid:5)

j bvi (  i       j ) = 0

v

(cid:5)

which leads to the conclusion that v
bv were normalized
to equal i, then v
gv would equal  . however, for now it will be assumed that both row and
column eigenvectors are available, and that they are normalized such that uv = i. then, in (7.43),
the transform z = vy is made:

bv and v

(cid:5)

(cid:5)
gv are diagonal, and that if v

v  y     avy = 0
(uv)  y     (uav)y = 0
  y      y = 0

(7.44)
and the last of (7.44) shows that the equations are decoupled. each equation   yi       i yi = 0 can
be solved separately. for each one the assumed solution is yi = y0e  i t (where y0 is the initial value,
yi(0). note that the vector z0 contains the initial conditions for both displacement and velocity,
because z = {x,   x}, and that y0 = vz0. these solutions can be assembled into matrix form:

.

(7.45)
the matrix, [  ike  k t], is diagonal. the return transform, back to the z vector, is made by simply
premultiplying by v:

y = [  ike  k t]y0 .

this solution must be    interpreted    in exactly the same way that the conservative system solution
was interpreted. then:

vy = z = v[  ike  k t]uz0 .
z = vy = v[  ike  k t]uz0 = 2n(cid:21)

vk(uk     z0)e  k t

k=1

7.4. nonconservative systems. viscous damping 205

and since   k =   k    j   k

z = 2n(cid:21)

  j   k t = 2n(cid:21)

vk(uk     z0)e  k t e

vk(uk     z0)e  k t (cos   kt    j sin   kt ) .

k=1

(7.46)
the vectors vk(uk     z0) and vk+1(uk+1     z0) (where k is odd) are complex, and conjugate. also,
the numbering is such that when k is odd,    is positive. now, de   ne the ith element of the vector
vk(uk     z0)     ai+jbi, and write the pair of terms as:

k=1

that is, the imaginary parts cancel. therefore:

(ai + j bi )(cos   kt + j sin   kt ) + (ai     j bi )(cos   kt     j sin   kt )
2ai cos   i t     2bi sin   i t
xi (1     i     n) = 2n   1(cid:21)

e  k t (2aki cos   kt     2bki sin   kt ); where
bki = im {vki (uk     z0)} .

aki = re {vki (uk     z0)};

k=1,3

(7.47)

in (7.47), the notation re{} reads    real part of     whatever is in the brackets, and im{} reads    the
imaginary part of     whatever is in the brackets.

non-conservative system example
to illustrate, take the earlier conservative system problem (figure 7.5 (b)) and add    dashpots    as
shown in the accompanying figure 7.7. give each of these the value of 1 unit of force per unit of

figure 7.7: nonconservative mechanical system.

velocity. in this case, then, the m and k matrices will be the same as before, while the damping
matrix will have the values:

(cid:22)

c =

c1 + c2
   c2

   c2
c2 + c3

(cid:22)

(cid:23)

=

(cid:23)

.

2    1
   1
2

206

7. matrix analysis of vibrating systems

following the methods previously discussed, the de   ning equation is m  x + c  x + kx = 0, for the
initial value problem. the reduced (1st order) equation set is given by equation (7.42) with the a
matrix:

(cid:22)

a =

i
0
   1k    m
   1c
m

   
            

(cid:23)

=

   
            

(7.48)

1
0
    2

9
1
7

0
1
1
9

    2

7

0
0
    16

0
0
7
9

9

1    2

the analysis of a yields the eigenvalues and eigenvectors shown below.

eigenvalues:   1,2 =    0.06250    j0.99811;   3,4 =    0.19147    j1.65552

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

row eigenvectors, u

   0.00587    0.02761    0.60194    0.46775
   0.60314    0.46647    0.02350    0.00994
   0.00587    0.02761    0.60194    0.46775
0.00994
0.60314
0.46647
0.02350
0.05629    0.04696
0.53062    0.53171
0.88447    0.88621
0.02174    0.03752
0.05629    0.04696
0.53062    0.53171
   0.88447
0.03752

0.88621    0.02174

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

column eigenvectors, v

0.01102    0.01837    0.01837
0.01102
0.46837    0.46837    0.24716
0.24716
0.01429
0.01931
0.01931
0.01429
0.46742    0.46742
0.31952    0.31952
   0.46817    0.46817
0.41270
0.41270
   0.01828
0.01691    0.01691
0.01828
   0.46775    0.46775    0.53171    0.53171
   0.00994
0.00994    0.03752
0.03752

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

it appears that the damping is quite    light    inasmuch as the (negative) real parts of the eigenvalues
are small. the frequencies of the undamped case were 1.0 and 5/3. note that damping lowers these
frequencies (to 0.998 and 1.655, respectively). in the eigenvector display, the imaginary parts are
shown below the reals. for example, u11 =    0.00587     j0.60314.

as a point of interest, note that the bottom 2 elements of the columns, vi, are equal to   i
times the top. the row vectors are those which will be dotted into the initial value vector, z0. they
are displayed, here, in rows. they are also in complex conjugate pairs, but unlike the columns, the
last 2 elements are not    times the    rst two elements.

below are shown the vk column eigenvectors, weighted (multiplied) by the scalar quantities
(uk   z0). these determine the x variable coef   cients. note that the vectors are complex conjugate.

7.4. nonconservative systems. viscous damping 207

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

vk(uk     z0) vectors
0.28192
0.28192
0.21807
0.21807
   0.22857
0.10194    0.10195
0.22857
0.27741    0.27742    0.27742
0.27741
   0.23321
0.23321    0.13980
0.13980
0.21053    0.21053    0.21053
0.21052
0.29568    0.29568
0.34150    0.34150
0.28456
0.21544
0.21544
0.28456
0.29147    0.29147    0.43250
0.43250

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

to derive the coef   cients of the sin and cos terms, take the columns in pairs, and follow the interpreta-
tion (7.47). for example, 0.5638 is 0.28192 + 0.28192, while 0.4664 is the sum 0.2332     (   0.2332).
the total result:

(cid:23)

(cid:24)(cid:22)
(cid:24)(cid:22)

0.5638
0.5548

0.4361
   0.5548

cos 0.9981t +
(cid:23)

cos 1.655t +

(cid:22)

(cid:23)
(cid:22)    0.2039

0.4572
0.4664

0.2796

   0.0625t

x = e
+ e

   0.1915t

(cid:25)

sin 0.9981t

(cid:23)

(cid:25)

sin 1.655t

.

(7.49)

if the velocities,   x, were required, the lower halves of the weighted eigenvectors might be handy (z
de   nes both x and   x), although it may be as easy to differentiate x.

sinusoidal response

7.4.2
this is a more tedious problem, algebraically, than the initial value problem. equation (7.42) provides
the starting point, this time including the driving vector.

(cid:22)

(cid:23)

  z     az = b

   1h, where a =

i
0
   1k    m
   m
   1c

.

(7.42) rewrite

in this equation, the vector, h, is {f, 0}, where f is the driving vector. the case, f = d cos   t, will be
considered here. the product b

(cid:22)

   1h is much simpler than it looks.
(cid:24)

(cid:23)(cid:24)

(cid:25)

(cid:25)

   1h =

b

0
   1    m
m

   1
m
   1
   1cm

=

f
0

0
   1d
m

cos   t .

208

7. matrix analysis of vibrating systems

in (7.42) the transform z = vy is made, knowing that the matrix a will then easily be diagonalized

  z     az = b
v  y     avy = b
  y      y = ub
%
  yi       i y =   i cos   t, the typical equation in the set
, the ith component of ub

   1h, transform z = vy :
   1h, now, premultiply by u :
   1h
   1h

  i =

ub

&

   1h .

i

the left side of   yi       i yi =   i cos   t will be a perfect differential if it is multiplied by e

     i t. then:

d

dt

(yi e

yi e

*
     i t ) =   i e
     i t cos   t
     i t =   i
yi =

     i t
     i t cos   t =   i e
+   2
  2
i
(   sin   t       i cos   t )

  i

e

+   2

(   sin   t       i cos   t )

  2
i
     i t, is simply divided back out. the integral of the right side is taken
where the integrating factor, e
from a table of integrals. the constants of integration are omitted, because only the    steady state   
portion of the solution is required here. the transient portion, ce  i t, has already been found (for the
initial value problem). the complete solution is then:

yi = ci e  i t +   i

+   2

  2
i

(   sin   t       i cos   t ) .

(7.50)

in (7.50) the constant c no longer is the initial value, y0. in order to calculate c, the initial conditions
must be applied to this complete solution. the initial conditions assumed are z0 = 0. that is, both
initial position and velocity are assumed zero. the result is
e  i t +   i

(   sin   t       i cos   t ) .

(7.51)

c =   i   i
+   2

  2
i

, and yi =   i   i
+   2

  2
i

+   2

  2
i

the assembly of solutions into matrix form yields:

z=v[   i e  i t
+   2

  2
i

]ub

   1h+v[

1
+   2

  2
i

]ub

   1h   sin   t   v[

  i

+  2

  2
i

]ub

   1h cos   t.

(7.52)

it is important to note that each of the terms within the square braces is a diagonal matrix. just as
before, the nature of the solutions is each term will be from the sum of vectors v, multiplied by scalar
quantities taken from within the diagonal matrix. the transient part is:

   1h = 2n(cid:21)
]ub

zt r = v[   i e  i t
+   2

  2
i

vk(uk   b

   1h)

  ke  k t
+   2
  2
k

.

k=1

in turn, this breaks into two terms, since e  k t = e  k t+j   k t = e  k t (cos   kt + j sin   kt ).

7.4. nonconservative systems. viscous damping 209

since all of the terms will be handled in this same way, the    rst thing to do is to form the
2nx2n matrix, c, which begins as the matrix, v, but then has each of its columns multiplied by the
term (uk     b
1
+   2 . note that the multiplier is dependent on k, the column number, but all
of the elements of the vector vk are multiplied by the same (scalar) amount. from the elements of
this matrix will come the coef   cients of the four terms (four, since zt r breaks into two).

   1h)

  2
k

7.4.3 determining the vector coefficients for the driven

system

below is the diagram of a 4x4 (the elements, cij ). its columns are partitioned into upper and lower
halves (cu and cl).

k (cid:16)    1

cu
cu
cl
cl

c11
c21
c31
c41

(cid:24)

(cid:25)

cuk
clk

ck =
(cid:25)

2

c12
c22
c32
c42

3

c13
c23
c33
c43

4

c14
c24
c34
c44

(uk     b
  2
k

   1h)
+   2
(cid:25)

= vk
(cid:24)

each column is a weighted eigenvector of the damped vibration problem:

.

(7.53)

(cid:24)

x  x

, and so vk =

each 2nx1 vector,vk,(4x1 in the diagram) is an eigenvector of the reduced,   rst order equation

note that the entire multiplier on vk in this expression is a scalar.
b  z + gz = h, where z =
, where ek is an nx1 eigenvector of the
original equation set. then, in (7.53), cl =   kcu. although the diagram shows a 4x4 (and so the
original set is a 2x2, as in the example problem), the results given here are applicable to the nxn
case in which the input is dcos   t. this is not a completely    general input case,    but the method
is the same for any linear input. for each, however, the multiplier in (7.53) will be different. now,
consider each solution term separately.
transient solution, xt r

ek
  kek

in (1) and (2), below, consider k to be odd (i.e., 1, 3, 5,    2n-1).

(1) e  k t cos   kt: the nx1 coef   cient vector is clk + cl,k+1. note that the imaginary parts will

cancel, the two identical real parts will add. note also, that the sum is on cl, (lower).

(2) e  k t sin   kt: the nx1 coef   cient is j(clk     cl,k+1). in this case, the real parts will cancel, and

the imaginaries will double. multiplying then by j will make the vector real.

210

7. matrix analysis of vibrating systems

steady state solution, xss

(1)    sin   t. the nx1 coef   cient will be

2n(cid:21)
k=1

cuk. sum the upper half vector. note that the

imaginary parts will cancel. thus, the sum will actually be only the real parts. also the sum is
for all values of k. especially note that the summation must be multiplied by   .

(2) cos   t. the term is negative, so the coef   cient is     2n(cid:21)

clk, the lower half summed and then

negated. again the imaginary parts will cancel.

k=1

non-conservative system example (continued)
returning to the damped system of figure 7.7, with an input of f = {18cos2t, 0}, with zero initial
conditions. the eigenvalue analysis is the same; all that must be done is the construction of the c
matrix from the row and column eigenvalue matrices, and build the solution.

(cid:25)

(cid:24)

ck =

cuk
clk

= vk

(uk     b
  2
k

   1h)
+   2

=

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

0.01068
0.01068
0.07469
0.07469
   0.18720
0.18720    0.16648
0.16648
0.00734    0.10268    0.10268
0.00734
   0.18709
0.21191    0.21191
0.18709
0.18618
0.18618
0.26131
0.26131
0.02236    0.02236
0.15552    0.15552
0.18628    0.33116    0.33116
0.18628
0.01902    0.01902    0.21056
0.21056

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

from the data in the accompanying    c    matrix, and using the rules given above, the complete
solution to the driven system can be written:

(cid:25)

(cid:25)

   0.0625t

(cid:24)(cid:24)
x = e
(cid:24)(cid:24)
+ e
(cid:24)    0.8950
+

   0.1915t

0.2898

0.3724
0.3726
(cid:25)
0.5226
   0.6623
cos 2t +

cos 0.99811t    
(cid:25)
cos 1.6555t +
(cid:25)
(cid:24)
0.3414
   0.3814

(cid:24)

(cid:25)
(cid:24)    0.3110

0.0447
0.0380

0.4211

(cid:25)

sin 2t

(cid:25)

(7.54)

sin 0.99811t

sin 1.6555t

sample calculations:

   0.8950 =    (0.18618 + 0.18618 + 0.26131 + 0.26131)
0.4211 = j (   j0.21056     j0.21056)

since the initial positions are zero, the coef   cients of the cos terms should sum to zero

0.3724 + 0.5226     0.8950 = 0
0.3726     0.6623 + 0.2898 = 0

a further check would be to differentiate for   x, and again check for a zero value at t = 0. this
exercise will be left to the reader.

7.5. steady state sinusoidal response 211

sinusoidal response     nonzero initial conditions

7.4.4
this case will be included here, because the result is very interesting. equation (7.50) is the start:

yi = ci e  i t +   i

+   2

  2
i

(   sin   t       i cos   t ) .

(7.50) rewrite

the (now non-zero) initial conditions can be applied here, letting y0i represent the initial value for
yi. then:

y0i = ci       i   i
+   2
yi = y0i e  i t +   i   i
+   2

  2
i

;     ci = y0i +   i   i
+   2
, and therefore
(   sin   t       i cos   t ) .

e  i t +   i

  2
i
+   2

  2
i

(7.55)
this is exactly like (7.51), but with the initial value solution ([e  i t]y0 in vector form) simply added
in. therefore, the interesting result is that, assuming the initial conditions to be the same as those
chosen earlier in the solution to the initial value problem, then the solution (7.55) will be the sum
of equations (7.54) and (7.50). this same result occurred in the solution to the conservative system
with sinusoidal driving function. see (7.32).

  2
i

if the initial conditions are different than those resulting in (7.50) then, we would only need

to re-solve the initial conditions problem, and add its solution to (7.54).

steady state sinusoidal response

7.5
often in the solution of vibrating systems, only the steady state response from sinusoidal input is
desired. this is a very signi   cant reduction in effort compared to the complete solution since the
eigenvalue analysis and transient solution are avoided.

a most simple example is the system in the previous figure 7.5 whose equation is;

m  x + kx = d cos   t =

cos 2t .

(cid:24)

(cid:25)

18
0

a solution x = a cos 2t is assumed. then   x =    a  2 cos   t =    4a cos 2t, and
a = [k     4m]   1d, and then x = [k     4m]   1d cos 2t .

(7.56)

(7.57)

note that [k     4m] must not be singular. also, in this idealized system the transient solution
continues forever, which denies that this is the    steady state    solution. rather, it is the vibration of
the system at the driving frequency.

212

7. matrix analysis of vibrating systems

using the parameter values from figure 7.5, it will be found that (7.57) agrees with the

sinusoidal response part of the complete solution found on page 187:
   1d cos 2t .

x = v[        2i]   1um

(7.58)

when the system includes dissipative elements, the situation is more complicated. for example,
let it be required to    nd the steady state response of the mechanical system in figure 7.7. the non
conservative system whose complete solution is the example used in the text.

m  x + c  x + kx = d cos 2t .

(7.59)
now, an assumed solution of a cos 2t will not do, because of the c  x term. but, in this case, the driver
can be changed to dej   t = d(cos   t + j sin   t ). this introduces the required imaginary part to the
driver. with an assumed solution of aej   t, real and imaginary parts satisfy the equations separately
    so that the real part of the result comes from the cos input while the imaginary comes from the
j sin input. when the actualinput is just cos   t, the output is taken to be the real part of the result
of the analysis:

(   m  2 + cj    + k)aej   t = dej   t
a = [k       2m + j   c]   1d
x = re{aej   t} .

(7.60)

where re{ } reads    real part of{ }.    equations (7.59) and (7.60) are general: the matrices are nxn,
de   ned in the same way as previously discussed, although the speci   c example is a 2x2 system. the
one complication is that the square nxn matrix to be inverted is now complex     and note that the
vector, a(nx1), will then, be complex. in this example, using the parameters of figure 7.7:

(cid:24)    0.8950     j0.34145
0.28972 + j0.38137

(cid:25)

a =

.

(7.61)
when this column for a is plugged back into x = re{aej   t} the result checks with the values given in
the steady state portion of the complete solution to the nonconservative system.(see equation (7.55)).
steady state response is often the requirement in electrical networks. for illustration, consider
the electrical network shown here, which is the analog of the nonconservative mechanical system
discussed above. the electrical network with these parameters is not very realistic with r in ohms,
l in henries, and c in farads, but it will serve to illustrate the method. then:
, and p     d

(lp2 + rp + s)q =

cos   t; s = 1

(7.62)

(cid:24)

(cid:25)

.

c

dt

18
0

l1 = 9, l2 = 7, s1 = 9, s2 = 7, s3 = 7, and r1 = r2 = r3 = 1, and with e(t ) = 18 cos   t.

let it be required to    nd the steady state output voltage, e2, over a range of frequencies of the input.
from the analysis of the mechanical system, the two resonant frequencies are 1 rps and 1.667 rps for

7.5. steady state sinusoidal response 213

the conservative system and slightly less for the system with damping.a reasonable range,then would
be from about 1/2 rps to 2 rps. the example uses the same parameters as those in the mass-spring
system.

for    = 2, the steady state solution to the set is identical to that given for the nonconservative
mechanical system except it results in the charges q1 and q2,, which will have to be differentiated
(multiplied by j  ) to obtain the current values, i. in this example, only i2 is of concern since i2r3 is
equal to the voltage output desired. using numbers from (7.61), above:

i = re{j   aej   t}
a = [s       2l + j   r]   1d
e2 = i2r3 = (0.28975 + j0.38137)j   ej   t = (   0.76274 + j0.5795)ej2t .

(7.63)

it is common to write this result as an amplitude and phase angle. the amplitude is the sum of the
squares of these numbers (0.91759), then
   0.76274
e2 = 0.91759(
0.91759
e2 = 0.91759ej (  t+  );

)ej   t = 0.91759ej   ej   t
   1 0.5795
   0.76274

= 2.49rad = 142.7
   

+ j
   = tan

0.5795
0.91759

(7.64)

.

the objective, now, is to repeat this same solution, but using    over the range 1/2 rps to 2 rps.

the results are shown below.the greatest amplitude comes near the lower resonant frequency,

and there is hardly any increase in magnitude at the upper resonant frequency.

the chart was created by stepping the driving frequency from 0.5 rps to 2.5 rps in 0.05
increments (fewer increments could have been used). at each frequency, the 2x2 complex matrix is
inverted and premultiplied into the d vector, {18, 0}; in total there were a large number of operations
    done by pc computer (in milliseconds).

in the following paragraphs, this same problem (i.e.,    nding amplitude and phase of the output
over the same range of frequencies) will be accomplished a different way, and using a    special    type
of determinant.

214

7. matrix analysis of vibrating systems

figure 7.8: (a) example network. (b) ladder network.

7.5.1 analysis of ladder networks; the cumulant3
when the network is a so-called    ladder    network, as shown in these diagrams, its analysis is again
simpli   ed. the    example network    shown is a ladder network; note the similarity of the two
diagrams. in the ladder, the series elements are denoted by    z,    impedance values, while the parallel
elements are identi   ed as    y,    admittance values.

the numbering of these parameters follows the rule: if a series impedance comes    rst (as in
the example), then impedances have odd numbers, beginning with one     along with the currents
through them     while the admittances are even, with their associated voltages. if the leftmost
immittance is an admittance (e.g., set z1 = 0 in the diagram), then the admittances will have odd
numbers (impedances will be even).

the ladder could have any number of rungs. the numbering scheme is continued in the
manner described; and will run from 0 to 2n (or to 2n +1 when there is an impedance at the output
end), where n is the number of rungs. of interest here is the two rung example network shown.

3see    synthesis of filters    by herrero and willoner; prentice-hall ee series, 1966. the methods of this reference are based upon
the cumulant.

7.5. steady state sinusoidal response 215

comparing the two diagrams, z1 will be equal to. r1 + l1j    + s

for steady state analysis.
the other immittances are calculated in the same manner. the voltages across the top of the ladder
are related, and the currents summed in the ladder rungs, by kirchhoff    s laws. this results in the
following equation set:

j   

e0 = i1z1 + e2
i1     i3 = e2y2
e2 = i3z3 + e4
i3 = e4y4 .

in matrix format:

   
          z1

1
   1
y2
0    1
0

0
1
z3

0
0
1
0    1 y4

   
         

            
         

            
          =

            
         

            
          .

e0
0
0
0

i1
e2
i3
e4

(7.65)

these equations are written for the example problem, but the point is the form of the matrix in (7.65).
it has the immittance values on the main diagonal (from 1 to 2n), with    1   s    in the upper codiagonal,
and    -1   s    in the lower. the determinant, d, of this matrix is called a    cumulant.    with such a
structured form, and so many zeros, it is not surprising that the cumulant is    special.    for example,
its numeric value, given the values of the diagonal elements, is easily calculated by the following
algorithm:
let the main diagonal elements be designated a1, a2,        a2n. then, the determinant value

is found by:

a [0] := 1 + j0;
for k
begin

:= 1 to 2*n do

a[k] := {calculate the immittance value here}
if k > 1 then a[k] := a[k]*a[k-1] + a[k-2];

end;
d := a[k]

{note: d is the determinant value}

this algorithm assumes that a separate function (unique for every ladder network) is coded to
calculate the a[k] values     i.e., the (complex) immittances that lie along the main diagonal of the
matrix in (7.65). this code is executed for each frequency value. the separate function calculates an
impedance value for k-odd, or an admittance value for k-even. for the example network, when k =
1, the function would return the complex number: r1 + j (l1       s1

).

following cramer   s rule, the output voltage is found by replacing the 4th column in d with
the column on the right side of the equation (7.65). this determinant, call it d4, expanded by the
4t h column elements, has the simple value, e0. then:

  

e4 = d4

d

= e0

d

.

(7.66)

in the example problem, e0 = 18. it will be found that this method produces the same results that
were obtained, above.

216

7. matrix analysis of vibrating systems

7.6 runge-kutta integration of differential

equations

it would be of interest to verify the methods (results) of the previous section.to do so we will discuss
the runge-kutta numerical method for integration of ordinary differential equations. we will reap
a double bene   t, since the method itself is a matrix application.

numerical methods are approximate in nature, since they essentially extrapolate the equation
set from its initial conditions. but, they are quick to set up and straightforward in implementation.
numerical solutions are of great value in the analysis of nonlinear problems. as in this case, such
methods can also be valuable indicators of the validity of a direct approach.
numerical methods are based on the following: given a differential equation set that can be
put into the form   x = f (x, t ), we divide the independent variable, t, into equal increments,   , such
that tn = n  , and at the nth increment,   xn = f (xn, tn). the determination of x at the next step,
tn+1:

xn+1 = xn +      f (xn, tn) .

that is, the n+1 step is estimated by adding to the previous values (at step n), the step size times
a best estimate of the derivative of the function relating the x vector and the independent variable
(time, t ). the various methods differ in their estimation of the derivative. notice that we must
manipulate the functions into a    rst order derivative form. this step has already been done in the
previous section. for a refresher, a given equation set:

                  
               

now, premultiply by a

[a0pn + a1pn   1 +        + an   1p + an]x = d where p     d

(7.67)
and the ai are mxm matrices. we de   ne x = x1, px = x2, p2x = x3, and so forth, up to pn   1x = xn.
then:

, p2     d2
dt 2 ,

etc.

dt

a0  xn + a1xn +        + an   1x2 + anx1 = d .
(assumed to be nonsingular), and it is easy to write, directly:

   1
0

   
               

                  
                =
i = a

(cid:5)

   
               

                  
               

                  
                +

                  
               

  x1  x2  x3      

       0
0
im        0
       0
      

0

  xn

0
0
      
im
   a
(cid:5)

im
0
      
0
n    a
(cid:5)
n   1

0
0
      
0
   a
(cid:5)
0 ai.with the obvious de   nitions,   z = az + h, and we are back to the equation
   1
the matrices a
set of section 7.4. in general, if the order of the original a matrices is m, the order of (7.68) will
be nm. for the systems discussed in this chapter, the degree n = 2 and the order is, therefore 2m.
note that all the zeros shown in (7.68) are matrices. in the a matrix, they are mxm null matrices,
and in the h vector they are mx1 columns.

0
0      
0
   1
a
0 d

x1
x2
x3      
xn

(7.68)

1

                  
                .

7.6. runge-kutta integration of differential equations 217

the method to be described is the runge-kutta. it has reasonable accuracy, and it is a popular
method, in current use. we will develop the method and run it against the problem, without a
technical discussion of its relative merit, and/or its accuracy compared to other methods.

the initial value problem applies the initial conditions to z, and to h at time t0. advancing
the solution from step (time increment,   ) to step follows the runge-kutta algorithm below. then
to advance from the nth step to the n + 1st involves four intermediate steps (estimates), which are
then put together in a manner like simpsons rule to form the step:

y1 = azn+h(tn); tn = n  
y2 = a(zn
+   
2 y1) + h(tn + 1
2    )
y3 = a(zn
+   
2 y2) + h(tn + 1
2    )
y4 = a(zn
+   y3) + h(tn +    ), then
zn+1 = zn +   
6 (y1 + 2y2 + 2y3 + y4) .

(7.69)

in order to verify the direct approach to the non-conservative system, we return to equation (7.43):
  z     az = 0, or = h, depending on whether or not the system is driven.

the non-conservative system of the previous section has

(cid:22)

a =

i
0
   1k    m
   m
   1c

   
               

(cid:23)

=

   
               ; and h =

            
         

            
          cos t .

0
0
2
0

(7.70)

0
0
    16

0
0
7
9

1
0
    2

9
1
7

0
1
1
9

    2

7

9

1    2

in the    rst example, the (two-mass, three spring) system wasn   t driven, but had the initial condi-
tion z0 = { 1 0 0 1 }, (i.e., x10 = 1 and   x20 = 1). following the method (5,3), and noting the
absence of a driving vector, the y1 vector (at tn = 0) is simply az0.

if this method is continued, using a step size of 0.1 second, the curves shown below are the
result. as an example, at time = 1.0 sec. the values for x1 and x2 are 0.44974 and 0.91952. these
values agree with the true solution (see equation (7.50)) to 4 decimal places. making the step size
smaller does not increase agreement, but the true solution numbers were only taken to 5 places. the
point is that the runge-kutta produces an amazingly accurate replica of the true solution; certainly
close enough to validate the    true solution.   

the runge-kutta requires 4 matrix-vector multiplications, plus the same number of vector
additions, per step. this problem ran to 120 steps. thus, a lot of calculations were required to arrive
at these curves. imagine the task in the days before the computer! but, with a reasonably modern
desk top, the tabular results are obtained almost instantly.

the second example of the previous section concerned the same mechanical system, but with
a cosine input force directed at mass 1, in the amount of 18 cos 2t. the introduction of the driving
vector just requires the addition of the h vector and a change to zero initial conditions (the two
masses are initially at rest).

218

7. matrix analysis of vibrating systems

over a 12 second time period, with a step size of 0.1 sec, the results again agree with the true

solution in the    rst 4 decimals.

the graph below plots the motion of the two masses.

7.7 exercises
7.1. reduce the equation,   a  x + b  x + c  x + dx = f (t ) to a set of 1st order equations.
7.2. the single spring-mass system (a) shown below, has the natural frequency

(cid:27)

   =

k

m (system restricted to vertical motion) .

if another spring and mass, with identical properties, is added as in (b), how many natural
frequencies are there, what are they, and are any of them equal to    ?

7.7. exercises 219

7.3. a machine weighing w1 pounds is suspended on a foundation with spring constant, k =

2k1. the machine is subjected to a vibrating force, f0 cos   t, frequencies very close to

/

;

k

m1

where m1 = w1

g

.

a vibration absorber is to be added, consisting of weight w2 and spring constant k2. de-
termine the relationship between k2 and w2 such that the motion of w1 is minimized.

7.4. given the text example non-conservative system of figure 7.7, the matrix actually analyzed
is the    reduced set    matrix, a, given in equation (7.48). this 4x4 has 4 eigenvalues, eigen-
vectors. if we de   ne eigenvectors as vi = qi,   iqi, for i = 1, 2, 3, 4, show that each   i , qi
pair satisfy the given equation (m  2
i

+ c  i + k)qi.

show that, in general, if   i, qi solve the given equation, then so do their complex conjugates.

220

7. matrix analysis of vibrating systems

7.5. calculate the eigenvalues and vectors for the system shown in the    gure; then    nd the

motion for all 3 masses, given the initial conditions:

x10 (w1) = 2 inches, to the right
x20 (w2) = 1 inch, to the right
x30 (w3) =    1 inch, to the left
initial velocities = 0

the units for k are #/in, and for c, # per in/sec.

7.6. the system shown in problem 5 is traveling to the right at 50 in/sec, when the container
box is suddenly stopped (at time t = 0). find the motion of w1, and the unbalanced force
on w1 over the following second of time. what is the maximum force on w1?

7.7.

if, in the system of problem 5, w2 and w3 are    anchored together    (so that they must move
together) would the results of problem 6 change?

7.8. calculate the immittance values for the low pass    lter in the diagram, at frequencies of 500
(input to output voltage

and 1000 cps. solve the related cumulant to compare the ratio
    see text equation (7.66) at the two frequencies.

e0
e8

capacitances are in microfarads, inductance is in millihenries, and resistance values in ohms.
note: the consistent set of parameters is ohms, henries, farads.

at zero frequency, the network is a simple voltage divider (shown here). in this case, the
ratio of input to output voltage is 2.0 (6 db).

7.7. exercises 221

a p p e n d i x a

223

partial differentiation of

bilinear and quadratic forms
begin with the de   nition of the partial differential operator,    del,       . this operator is de   ned as a
column vector, an (mx1) matrix:

                                 
                              

   

    x1

   

    x2
...
   

    xm

                                 
                              

    =

.

(a.1)

the column length is determined by the number of independent variables, (x1, x2,        xm), there are
m of these, and del should be written     x. indicating the independent variable. herein, the subscript
will be omitted unless there could be a confusion.
note that     is an operator; it is meaningless standing alone (a.1). but, given a function

q(x1, x2, . . . xm):

   q =    q(x1, x2,        xm) =

                                 
                              

    q

    x1
    q

    x2
...
    q

    xm

                                 
                              

(a.2)

certainly is meaningful, it being all the partials of q arranged into a column vector. as with all
operators, one must be careful with the order in which the symbols are written. q    is as meaningless
as     itself.

224 a. partial differentiation of bilinear and quadratic forms

notation

in this appendix the superscript    t    will be used
to indicate transposition. example: at.

this will allow the use of the    prime    to indicate
differentitation

(cid:5)
ii

y

=    yj
   xi

given a set of functions yk(x1, x2        xm); k = 1 . . . n, they are arranged into an (nx1) column
vector and denoted in boldface, y. although there is no matrix operation in which a column vector
operates on another column vector, there is a way to display all of the partials of y with respect to
the variables, xi. it is written as    yt. note that y is transposed to a row vector.

   yt =

(cid:17)

y1

y2

       yn

(cid:18) =

                                 
                              

                                 
                              

   

    x1

   

    x2
...
   

    xm

using the notation y

(cid:5)

ij

   

                              

    y1
    x1
    y1
    x2
...
    y1
    xm

    y2
    x1
    y2
    x2
...
      

      
      

. . .
      

    yn
    x1
    yn
    x2
...
    yn
    xm

   

                               (mxn) .

(a.3)

=    yj
   
   xi

   yt =

, the equation (a.3) reads:

                  

      

. . .

(cid:5)
11
(cid:5)
21

y

y

(cid:5)
12
(cid:5)
22

y

y

(cid:5)
m1

y

(cid:5)
m2

y

(cid:5)
1n
(cid:5)
2n

y

y

(cid:5)

mn

y

   

                   where y

(cid:5)

ij

=    yj
   xi

.

(a.4)

the column-by-row matrix product is unusual, but it is conformable. note that the ith row has all
partials with respect to xi, and the jth column contains the partials of yj . also, notice that    yt
cannot be transposed: the result would again have     operating on nothing.
a    bilinear form    is the dot product of two vectors, say y   w, whose elements are functions of
m independent x-variables, i.e., yk(x1, x2        xm) and wk(x1, x2        xm). arranged into column
vectors, both y and w must have the same number of elements, i.e., k = 1, , n, and, for example
y = {y1, y2,        yn}. then

y   w = ytw = y1w1 + y2w2 +        + ynwn .

(a.5)

225

note that each term contains a yw product with each variable in the    rst power     thus,    bilinear.   
if the functions wi = yi, the form would be a    quadratic form,    containing only y2
the dot product in (a.5) is a scalar, and so (from equation (a.2)) we expect that    (ytw) is
a vector, with each row, i, containing the partials y   w with respect to xi. in performing row by row
differentiation, we choose to take all the partials of y    rst, then those of w. the general (kth) row is:

i terms

(cid:17)

(cid:5)
k1

y

(cid:5)
k2

y

(cid:18)

w +(cid:17)
   
            

w
w

(cid:5)

kn

       y
   
            {w} +

(cid:5)
(cid:5)
1n
2n

y
y

now, when all the m rows are written, (i = 1,, m), the result will be
       w

      

   
            

(cid:5)
(cid:5)
11
21

y
y

(cid:5)
m1

y

(cid:5)
(cid:5)
12
22

y
y

(cid:5)
m2

y

   (ytw)=

. . .

y

(cid:5)

mn

(cid:5)
11 w
(cid:5)
21 w

(cid:5)
(cid:5)
12
22

(cid:5)
m1 w

(cid:5)
m2

w

. . .

(cid:5)

w

mn

(cid:5)
k1 w

(cid:5)
k2

w

(cid:18)

(a.6)

(cid:5)

kn

y .

       y
   
            {y}= (   ytw)+(   wt)y.

(cid:5)
(cid:5)
1n
2n

w

as expected, the result is a (1xn) vector.

(a.7)
in (a.5), if w = az the dot product would be y   w = ytaz. the a matrix (which    transforms   
z to w) is necessarily nxn. since the elements of a are not functions of the xj variables, the inclusion
of a does not add complication:

   (ytw) =    (ytaz) = (   yt)az + (   ztat)y = (   yt)az + (   zt)aty .

in the case z = y and a is a symmetric matrix, the dot product is a quadratic form ytay and

   (ytay) = 2(   yt)ay .

(a.8)

(a.9)

in chapter 4 the quadratic form in the regression problem is ete = xtatax - 2xtatb + btb, in which
the {y} variables are, respectively, yk = xk. in this case, y
=   ij (the kronecker delta, whose
value is zero except when i = j when it is unity). then:

= x

ij

ij

(cid:5)

(cid:5)

   ete =    (xtatax)     2(   xt)atb = 2(   xt)atax     2(   xt)a

(a.10)
since the b vector is not a function of the x variables, its derivative is zero. now, (   xt) is [  ij ], the
unit matrix, so:

b .

(cid:5)

(cid:5)

   ete = 2atax     2a
(cid:5)

(cid:5)

b .
=   ij . then:

the interest in chapter 6 is just the differentiation of the quadratic form. beginning with equa-
tion (a.9), note that again yk = xk, and    yt = y

= x

ij

ij

   (xtax) = 2(   xt)ax = 2ax .

(a.12)

(a.11)

a p p e n d i x b
polynomials

227

associated with every square (nxn) matrix is a characteristic polynomial equation:

f (  ) = c0  n + c1  n   1 +        cn   1   + cn = 0

whose degree is n. the roots of this polynomial are the eigenvalues of the matrix. our interest in
polynomials is fueled by the requirement to    nd these eigenvalues. toward that end, some of the
basic arithmetic algorithms are discussed here, with a display of    pascal-like    code. then, an outline
of a recommended method for determining polynomial roots is given.

b.1 polynomial basics
in this appendix the polynomial will be written as:

p(x) = c0xn + c1xn   1 +        + cn   1x + cn = 0 .

(b.1)

0
the coef   cient, c0, the coef   cient of the highest power term, can always be made to equal unity. only
c0 and cn are required to be non-zero and both c0 and cn can be made unity by the transformation
x = kz with k = n
with its corresponding power always equals n.

. note that in the representation, (b.1), the sum of the subscript on each term

cn
c0

the equation (b.1) is not an identity; there are exactly n (generally complex)    roots,    xj ,
that cause p(x) to vanish. in this discussion, we consider only polynomials with real coef   cients. as
a consequence, if a root is complex, its complex conjugate must also be a root. if the degree, n, is odd,
there must be at least one real root.
it is sometimes desirable to de   ne a related polynomial, de   ned by the transform x = 1/z:
(b.2)

p(z) = cnzn + cn   1zn   1 +        + c1z + c0 = 0

which is the same as (b.1), but with the coef   cients taken in reverse order, and possessing the
inverses of the roots of (b.1). a root x > 1 is transformed to a root z < 1. as an example, if the root
extraction method converges to the smallest (absolute value) root    rst, then (b.2) might be used to
obtain roots in the reverse order.

denoting the roots of (b.1) as xj , j = 1..n, the polynomial can be written as in (b.3):

(x     xj ) = (x     x1)(x     x2)       (x     xn) .

(b.3)

p(x) = n$

j=1

228 b. polynomials

by performing the indicated multiplication of the factors in (b.3), the relationships between the
coef   cients and the roots can be derived:

                                                         
                                                      

j

xj =    sum of all roots
xi xj = sum of root products taken 2 at a time

c1 =     n(cid:21)
c2 = n!
c3 =     n(cid:21)
xi xj xk =    sum of root products taken 3 at a time
                     etc.
n$
cn = (   1)n

xj = product of all roots.

i<j <k

i<j

(b.4)

j

the notation of (b.4) is unusual. the coef   cient c3 is the negative sum of the products of the roots,
taken 3 at a time. if n were 4, then c3 would be x1x2x3 + x1x2x4 + x1x3x4 + x2x3x4.the coef   cient
c4 is the sum of all products of roots, taken four at a time. and so on, until there is only the single
term product of all the roots, equal to cn. note the alternating signs in (b.4).

at    rst, it may seem unlikely that the operations in (b.4) will always produce real coef   cients.
however, in the simple example given (n = 4), if x3 and x4 are complex conjugates and x1 and x2
are conjugates, it is easily seen that the terms forming c   s will be conjugates     their sums, real.

equations (b.4) provide insight into the character of the roots, but if a set of roots is given,
these relationships are not useful in calculating the coef   cients. the recommended algorithm to
generate the coef   cients from the roots is surprisingly simple. the    pascal-like    code is given below.
since the roots are generally complex, the routine must be executed using complex variable data
types and using complex arithmetic. note: c [0] must be 1.0 and real.

c[0]:=(1+j0);
for k:=1 to n do
begin

{calculate the coefficients c from roots, x}

c[k]:=0+j0; { j = sqrt of -1 }
for i:=k downto 1 do if (i = 1) then c[i]:=c[i]-x[k] else

c[i]:=c[i]-x[k]*c[i-1];

end;

pascal does not have a complex type, nor does it support complex arithmetic directly. thus,
the complex type must be de   ned in the program, and complex arithmetic must be done in separate
procedures.

b.2. polynomial arithmetic 229

b.2 polynomial arithmetic
polynomials are added/subtracted by adding/subtracting the coef   cients of    like    terms (those having
the same degree in the variable, x). polynomial multiplication:

an(x) = a0xn +        + an = b m(x)cl(x)

(b.5)

is affected by multiplying every term in b by every term in c, and collecting    like terms.    note,
in (b.5), the superscript on the capital letters indicates the degree of the polynomial. the result, a,
will clearly be a polynomial of degree n = l + m.

it is instructive to indicate the terms to be collected by means of a diagram,table b.1, showing
an example whose product is to be a7 = b3c4. from the    rst row of the table, the coef   cient a0
will be just the product of b0 and c0. the succeeding rows indicate the terms to be multiplied and
summed:

                  
               

a0 = b0c0
a1 = b0c1 + b1c0
a2 = b0c2 + b1c1 + b2c0
a3 = b0c3 + b1c2 + b2c1 + b3c0
       = etc

(b.6)

table b.1:

the table was constructed by writing the a coef   cients down the left, and putting the higher
order (i.e., c) coef   cients along the top. at the intersection of row/column, the b term is chosen
such that its exponent, plus the top row exponent add to the a power at the left. note that the
terms of b(x) appear to    slide    across the table from left toward right. in fact, if the b coef   cients
are written on a    sliding strip    the algorithm is shown clearly in table b.2, below.

in this scheme, the    a    coef   cient that lies just below b0 is determined by the products of the b
and c row terms. for example, when b0 slides under c3 the a3 coef   cient is calculated by multiplying

230 b. polynomials

table b.2: polynomial multiplication, by a    sliding strip    method.

the adjacent b and c coef   cients in the same column, going from right to left:

a3 = b0c3 + b1c2 + b2c1 + b3c0 .

the computer method just    automates    table b.1 and equations (b.5) and (b.6).

the computer routine for multiplying polynomials b and c is written directly from the sliding
strip display. it uses index    k    to slide the b coef   cients along,    j    to choose a b coef   cient, and (k-j) to
choose the c. nb and nc are the respective degrees of the polynomials b and c. this routine is done
in real arithmetic     we consider only polynomials with real coef   cients. the coef   cient variables
are given in the lower case corresponding to the upper case polynomial designation.

{polynomial multiply: a = b times c}

for k:=0 to nb+nc do
begin

a[k]:=0; for j:=0 to nb do
if((k-j)<=nc) and ((k-j)>=0)then a[k]:=a[k]+b[j]*c[k-j];

end;

to develop polynomial division, equations (b.5) can be solved for the coef   cients, cj . note

that the coef   cient b0 must equal one:

                  
               

c0 = a0
c1 = a1     b1c0
c2 = a2     b1c1     b2c0
c3 = a3     b1c2     b2c1     b3c0
       = etc

where b0 = 1.0 .

(b.7)

in polynomial division, (c= a divided by b, equations (b.7)), the graphic scheme is similar, but
different in that the divisor (b) coef   cients must be reversed in sign, except for b0, required to be
1.0. most importantly, the b coef   cients multiply previously determined c coef   cients. in this case,
it is clearer if the rows for results, and sliding strip are interchanged. in table b.3, the quotient, c
polynomial coef   cients, are to be calculated.

the only b coef   cient that multiplies the a row is b0 = 1. all the rest of the b coef   cients
multiply previously determined c coef   cients (   feedback   ). for example, slide the b strip until the
   1    is under c4. (c4 has not yet been determined, but the c terms to its left have been). thus:

c4 = a4     b1c3     b2c2     b3c1 .

b.2. polynomial arithmetic 231

table b.3:

note, again, that only the    +1    (i.e., b0) on the b strip    reaches over    to multiply the a strip, a4. this
sliding strip scheme is one method of performing    synthetic division,   

notice that there can be no further c coef   cients after c4. these locations are zero. however,
the synthetic division process continues until the 1 slides under a7.the terms (three, in this example)
so calculated are the remainder, r, coef   cients, whose degree is one less than the divisor. in this
example

r0 = a5     b1c4     b2c3     b3c2
r1 = a6     b1    0     b2c4     b3c3 = a6     b2c4     b3c3
r2 = a7     b1    0     b2    0     b3c4 = a7     b3c4 .

as an example, divide a7(x) = x7 + 2x6 + 5x5 + 17x4     49x3 + 20x2 + 54x     18 by
b3(x) = x3 + 10x     3. the result will be c4(x) with remainder, r2(x).

table b.4:

the a coef   cients are entered into the    xed strip, and the sliding strip is prepared with the b
coef   cients reversed in sign (except for the    1   ). note that zero coef   cients are included. the results
strip show the calculated c coef   cients for the 4th degree polynomial, c. the remainder coef   cients
are shown in the bottom row. the position of the sliding strip is at the point that the coef   cient r0
is to be calculated (20 + 0    7     10    0     5    3 = 5).

if calculations were to be made by hand, there is really no reason to prefer the sliding strip
way to do synthetic division. however, calculations are to be made by computer     and the sliding
strip method clearly shows the algorithm.

the pascal code to accomplish this division is shown below. the dividend is a, the divisor,
b, the quotient is q, and the remainder is rem. order(a) = n, order(b) = m. the index k    pushes
the strip along,    and the index j is used to gather up the product terms:

begin {begin synthetic division----}

232 b. polynomials

q [0]:=a[0];

for k:=1 to n do if k <= (n-m) then
begin

q[k]:=a[k];
for j:=1 to m do if k-j > -1 then q[k]:=q[k]-b[j]*q[k-j];

end else
begin

{when k > n-m the remainder coeffs are calculate   d)

rem[k-n+m-1]:=a[k];
for j:=1 to m do if (k-j) <= n-m then
rem[k-n+m-1]:=rem[k-n+m-1]-b[j]*q[k-j];

end;

end;

again, no complex arithmetic is involved.

b.2.1 evaluating a polynomial at a aiven value
given p (x),    nd p (x0). this is equivalent to dividing p by (x     x0). back to synthetic division.
and there is a    bonus,    upon repeated applications of synthetic division, p (x0)    rst appears, then
(cid:5)
(x0)! the reason that this
p
is a real bonus is that the id77 for determining roots use these values.

(x0) (the apostrophe is used here to denote differentiation), then 1/2p

(cid:5)(cid:5)

as an example, take the polynomial, c, and determine these values at x =    2.

c(x) = x4 + 2x3     5x2 + 7

at x =    2 .

the following table b.5 repeatedly divides by (x     x0) = (x + 2).the sign of    b1    must be reversed,
so that the sliding tab is (   2 1). also note that the degree of each row is one less than the one above
(e.g., the degree of p

(x) is one less than p (x)).

(cid:5)

table b.5:

the numeric results are shown in the double outlined boxes (example, p

(   2) = 12). the
slide strip that produced all these results is shown in the bottom row. it slides one less column in
evaluating each derivative. note that the       xed strip    moves down one row. example: the    xed strip
for the calculation of p

(   2) is the row just above.

(cid:5)

(cid:5)

b.3. evaluating polynomial roots 233

in the above table, the rows can be    lled in simultaneously. the table can be    lled column
by column, rather than row by row. this makes the computer coding all the easier. the code given
below takes advantage of this. the polynomial is of degree n and its coef   cients are c[k]. the value
of x is given in xx.the value of the polynomial at x is in f, the derivative at x in f1, and 1/2 the second
derivative in f2. in the code, below, the polynomial coef   cients are in array c[k].

begin {evaluate the polynomial and its derivatives at xx}

f:=c [0]; f1:=c[0]; f2:=c[0];
for k:=1 to n do
begin

f:=c[k]+xx*f;
if k < n then f1:=f+xx*f1;
if k < n-1 then f2:=f1+xx*f2;

end;

end;

a very simple routine. however, xx may be complex, and so the routine must be done in complex
arithmetic.

b.3 evaluating polynomial roots

deriving the polynomial roots from its coef   cients is certainly not a simple problem. for polynomials
of degree greater than 4 there is no direct approach     only iterative methods are available. a
recommended approach is summarized as follows:

(cid:129) laguerre   s method (below) is recommended to provide an initial estimate of a root.

(cid:129) when a close estimate is found (laguerre), then newton   s method produces excellent results.

(cid:129) using the result from newton   s method, the original polynomial can be    de   ated    by dividing
the polynomial by the newly found root factor(s), to a product of the root factor times a
polynomial of lesser degree. in this way, the degree of each    stage    becomes reduced until
a quadratic, cubic or quartic polynomial remains. at that point, the remaining roots can be
calculated directly.

the equations for the laguerre and id77s will be given here, without derivation.

b.3.1 the laguerre method
given a polynomial, p (x), of degree n, and an initial estimate of a root at x0, de   ne:

(cid:5)

g     p

(x0)
p (x0)

and h     g2     p

(cid:5)(cid:5)

(x0)
p (x0)

.

234 b. polynomials

(note: the apostrophe is used here to denote differentiation.)

then h =

g   (cid:26)

   n

(n     1)(n h     g2)

, and the next estimate of the root is x0 + h .

b.3.2 the id77
when a root is located closely enough to ensure convergence of the id77:

(cid:5)

=     p

(x0)
p (x0)

(cid:5)(cid:5)
+ 1
2 p
(cid:5)
p

(x0)
(x0)

.

1
h

note that all the factors (p and its derivatives) are obtained directly via the above    pascal-like   
routine. just put them together to    nd h. the next estimate of the root is x0 + h.

b.3.3 an example
an example will provide a general overview of the methods involved:

p (x) = 1.0x5 + 2.0x4 + 3.0x3 + 4.0x2 + 5.0x + 6.0 .

using an initial value of 0 + j0, the laguerre method quickly    nds roots near (   0.806    j1.223).
then newton re   nes the roots to (   0.8057865    j1.22290471).

the de   ation process reduces the polynomial to third degree (note that two roots have been

found so that the de   ation is from degree 5 to degree 3), with coef   cients:

1.000000

0.388427

0.229234

2.797480

this cubic polynomial can be solved directly. however, for the example we continue the iterative
methods.
the laguerre method works with this new polynomial to    nd (0.552    j1.253). now, the
id77 is used     but with this difference: newton will always use the original polynomial,
because inaccuracies in the    rst roots will effect the accuracies of the de   ated polynomial. newton
   nds (0.551685    j1.2533352).
now a polynomial x + 1.491798, whose root is obviously    1.491798. and this completes the

process.

it should be noted that these calculations must be done with high precision. this example
problem was done using the    extended    variable type in turbo pascal (delphi). the    rst root was
actually determined to be:

   8.05786469389031e     0001

1.22290471337441e + 0000

root evaluation is very sensitive to small changes in the coef   cients. it may be that this sensitivity
can be reduced somewhat by the transformation given at the beginning of the appendix, which

results in both c0 and cn being set to 1.0. for the problems used in preparing this appendix, the
accuracy was so great that the transformation appeared to make no difference. but in more realistic
problems, it might.

b.3. evaluating polynomial roots 235

a p p e n d i x c

the vibrating string

237

this appendix analyzes the vibrations in a stretched string,    rst using a digitized matrix approach,
then comparing it with the analysis of the continuous string. refer to chapter 6, where the interest
is in the    normal modes    and how they sum together. in chapter 7, the interest is in applying the
same kind of digitized approach to the vibration analysis of a beam.

c.1 the digitized     matrix solution

the    gure shows the string anchored at each end and pulled tight. the string length, l, is imagined
to be divided into n equal segments. figure c.1(a) shows n = 8.

figure c.1: vibrating string.

the mass, m, of each segment is concentrated at the center of the segment. the string itself

is then considered weightless. the mass, m, is equal to the total mass, m, divided by n.

when a vertical load, p, is applied to the j th mass, the string is deformed as in (b), and
is resisted by the tension in the string. the static weights, mg, are very small compared to tension
forces. neglecting these weights, then, p = t (sin   1 + sin   2). the deformation is small enough to

238 c. the vibrating string

consider sin   = tan  , and cos   = 1 (note the chosen positive x and y directions, in (c)). then

sin   1     y(xj )
p = y(xj )t

xj

and sin   2     y(xj )
(cid:23)
(cid:22)
l     xj
= t l
1
xj

+ 1
l     xj
then the de   ection at xj due to the load, p , at xj , is

, and

y(xj )

xj (l     xj )

.

y(xj ) = p xj (l     xj )

.

t l

the diagram, (c), similar triangles, shows that, for xi < xj ,

y(xi )
y(xj )

= xi
xj

    y(xi ) = p xi

t l

(l     xj ), for xi < xj .

for xi > xj just exchange xj and xi,     y(xi ) = p xj

(l     xi ), for xi > xj .

t l

this is by virtue of the    reciprocity theorem    i.e., the de   ection at xi due to the load at xj is

the same as the de   ection at xj due to the same load at xi. or, just work out the geometry.
[wij ], can be de   ned. first, note that xk = l

using these equations, and setting the load, p , to unity, a dimensionless    in   uence matrix,   

wij = xi
wij = l

t l

(l     xj ) = 1

t l

t n2 (i     1

2 )(n     j + 1

(cid:29)

(cid:30)

n (k     1
(cid:30) (cid:29)

2 ), for k =1 ..n. then, for xi < xj :
l     l

l

n (j     1
2 )

n (i     1
2 )
2 ); dimensionless except for the

l

t

multiplier .

the [w] matrix will be symmetric (prove it), and we should extract the l/t term as a multiplier. now,
if (vertical) loads are applied to some, or all the mass points, given as the elements of vector, p, the
resultant de   ection y is just y = l
t wp. for example, the displacement y1 is equal to its displacement
due to the load at j = 1, plus that at j = 2, plus     etc.

if the string is vibrating freely, the loads are just the inertial forces,    mi   yi =     m

  yi

n

y(t ) =     lm

t

w  y(t ); with wij = 1

n3 (i     1

2 )(n     j + 1
2 ) .

note that an additional    n    factor is absorbed into w, which has the eigenvalues,   i, and the
eigenvectors, vi. given the eigenvalue analysis of w, the solution to this vector differential equation
is constructed as a linear summation of its eigenvectors (or    normal modes   ):

y(t ) = n(cid:21)

r=1

vr (ar cos   r t + br sin   r t ); with   r =

/

t

lm  r

.

c.2. the continuous function solution 239

the coef   cients ar and br can be determined from the initial conditions by using the orthogonality
property of the eigenvectors: vi     vj =   ij (i.e., the eigenvectors are normalized). that is, by setting
t = 0, the ar coef   cients are found from the initial positions of the masses, and the br coef   cients
are determined from the initial velocities of the masses.

vr ar; now dot through with the vector vk :     vk     y(0) =

vk     vr ar

(cid:21)

vr   r br; again dot through by vk : vk       y(0) =

.

r

(cid:21)

r

vk     vr   r br , and

(cid:21)
(cid:21)

r

y(0) =
ar = vr     y(0)
  y(0) =
br = vr       y(0)

r

  r

summary: in this analysis, the eigenvalues determine the frequencies (in terms of length, mass,
and tension) and for each of these there is a corresponding eigenvector, or normal mode. a normal
mode is a spatial description of the string. a sum of these modes builds the solution along the
x-dimension (the y vector). each mode brings along a constant and sinusoidal time function. there
are just enough initial conditions to determine the constants, since each mass point starts (time t =
0) with an initial position and velocity.

the determination of the constants depends on the orthogonality of the    normal modes,   

c.2 the continuous function solution
our objective is to compare the continuous solution to the one above; therefore, its derivation will
be very brief. the highlights presented here follow the derivation in [4, p. 431].
conditions are given, say, as y0(x), and   y0(x).

the string is now viewed as continuous, and a solution y(x, t ) is sought. as above, the initial

the governing equation for the vibration of the string is the one dimensional wave equation:

=    2y
   t 2
where t , l, and m are de   ned as in the matrix solution.

we assume a solution of the form y(x, t ) = ej   t f (x).

   2y
   x2

t l

m

   2y
   x2

= ej   t f

(cid:5)(cid:5)

, and

   2y
   t 2

=      2ej   t f .

substituting these into the wave equation, results in the ordinary differential equation:

f

(cid:5)(cid:5) + m  2
f (x) = c1 sin

t l

0
0
f = 0, whose solution is

  x + c2 cos

m

t l

m

t l

  x .

.

2
   (cid:21)
n=1

240 c. the vibrating string

since f (0) = 0, c2 must be zero. however, f (l) = 0 also, and this cannot mean that c1 = 0, else
there would be only the trivial solution. nevertheless
  l = sin
   = n  , or   n =

n  , since sin = 0 at these values.

this condition can be met if

   = 0 .

0

0

0

0

ml

ml

sin

t l

m

t

t

that is, there are an in   nity of values,   n, where n can be any integer > 0. for each of these

t

ml

values, there are corresponding functions, fn(x) and yn(x, t )

note that both the real (cos) and imaginary (sin) parts of y(x, t ) solve the wave equation, their sum
is also a solution:

n   x

l

fn(x) = c1 sin
1

; and yn(x, t ) = c1ej   nt sin
0

0

yn = sin

n   x

l

an cos

t

ml

n   t + bn sin

t

ml

n   t

n   x

l

.

and the    nal solution is the in   nite sum of all the yn functions, y(x, t ) =

yn.

at time t = 0 the initial position of the string is y0(x):

y(x, 0) = y0(x) =

an sin

n   x

l

(cid:21)

n

and this shows clearly that the string position is a sum of sin functions, in the same way that the
matrix solution string position is a sum of eigenvectors. furthermore, a group of sin functions can
form an orthogonal set     as shown in work with fourier series. from a mathematical handbook

*

  

0

*

sin2 ax dx =   
2

.

  

0

then with the change of variable z = l

   x, the second integral value is l
2 .

sin ax sin bx dx = 0, (b (cid:4)= a); and
*
*

(cid:21)

l

the coef   cients bk are calculated in the same way, resulting in the    nal solution:

k   x

l

1

dx =
0

then

y0(x) sin

0

l

sin

k   x

n   x

sin

l

l

an

0

n

y(x, t ) =

   (cid:21)
l*
n=1
an = 2

l

0

0
n   t + bn sin
l*

sin

n   x

l

an cos

t

ml

y0(x) sin

n   x

l

dx, and bn = 2

n   l

0

ak .

dx = l
2
2

t

ml

n   t

  y0(x) sin

n   x

l

dx .

it is truly remarkable how similar the two solutions are.

c.3. exercises 241

c.3 exercises
c.1. show that the [w] matrix is symmetric.

c.2. show that y(t ) = n!

arvr cos   r t is a solution to y(t ) =     lm

t w  y(t ).

r

a p p e n d i x d

solar energy geometry

243

the distribution of solar energy, weather patterns, and the seasons, all depend on the geometry
between the earth and the sun. apparently, we are just far enough from the sun to bene   t greatly
from its heat without having been cooked into some other kind of life form(s). the tilt of the earth   s
axis provides our seasonal variations, and the geometry problem of solar energy     its variation, both
daily and seasonally.

although the orbit of the earth is not quite circular, it is assumed to be so in this picture.

the view is the earth orbit from above the north pole. the earth is shown in its 4 most
important positions in this orbit. note the direction of earth motion in the orbit, and also the
direction of earth   s rotation about its axis (straight line emerging from the small circle). this axis
   
always points to the right and is tilted at an angle of 23.5
relative to the plane of the orbit. note
the short arc on the left sides of the 4 earths shown; this is the equator, visible because of the tilt.

all points on earth   s surface at a given latitude experience the same variations of radiant energy.
the earth   s rotation produces the most frequent variation     from dawn    til dusk. but, every day is
different because of the earth   s travel in its orbit and that 23.5
axis tilt. for example, summer in the

   

244 d. solar energy geometry

northern hemisphere occurs when the north pole is tilted toward the sun. it is then winter in the
southern hemisphere.

1

0.8

0.6

0.4

0.2

0

r
e
w
o
p
 
r
a
o
s

l

4

6

8

10

14

12
hour

16

18

20

the equations necessary for calculating these variations are developed in chapter 5     solar
   
angles. an example variation is shown here; the point chosen is at latitude 35
(north), its longitude
is arbitrary. the top curve represents june 21, the middle curve represents both march 21 and sept.
21, the bottom one december 21 (approximate dates). similar curves could be drawn for all days in
the year.
the above diagram is the plot of the    cosine factor,    cf = sx    px for the days given, and over
the time span shown. the results were calculated as if there were a solar panel lying on the ground
at the given latitude. the    panel vector    has the coordinates px = {1, 0, 0}. the sun vector, sx, has
the coordinates given in chapter 5, (5.26):

      
    c   c  s c  s + s   s  s
c   s  s     s   c  s c  s

s  s c  s

sx =

      
    (c     cos, s     sin) .

(chapter 5 (5.26))

the greek characters (angles) are as de   ned in chapter 5, and the capitals c and s are cos and sin,
respectively.the angle    is the panel latitude,   s, the    sun latitude,    and   s measures the sweep angle
of the sun vector as the earth rotates (producing the daylight hours).
if enough solar panels were laid on the ground whose total power output is 1 kw (kilowatt)
when subjected to the direct rays of the sun (cf = 1), the vertical axis of the diagram could be read
as kw. the abscissa is in hours (24 hour clock), 12 being noon. the area(s) beneath the curves are
the kw-hr energy(s) received during the respective days.

of course the panel(s) are never just laid horizontal. on a    at roof the panel is oriented
north-south with the north edge elevated as shown here. the optimum elevation angle is equal to
the latitude of the panel. for example, if the installation is at 35
latitude, the best elevation angle is
35

   

   

.

245

the chapter 5 panel vector equation is:

px = {s  p, s  pc  p,    c  pc  p} .

(chapter 5 (5.27))

   

   

   

(90 - 35)

in terms of this equation,   p is set at 55
(i.e., the panel center line is directly
north-south).the results are signi   cantly different, as shown in this    power pro   le,    the solar power
curves for all 365 days of the year fall between the top and bottom curves shown     however, in a
rather complicated way. the top curve represents both day 0 and day 180. days 92 and 274 follow
the lower curve, but days after the fall equinox become shorter. the day 274 daytime is between the
vertical lines     from about 7.2 to .16.8 (9.6 hours).

,   p is 0

   
is located farther north, say 45
45

almost the same results are found for installations at other latitudes. for example, if the panel
(near the latitude of fargo, nd), and the elevation angle is set at
(there is some difference: the fall and

, the results are about the same as those shown here for 35

   

   

246 d. solar energy geometry

winter days are shorter). solar energy is more popular in the southwestern us states because they
receive more sunny days than states farther north.

of course, installations in the southern hemisphere are the    reverse    of these. when the

northern hemisphere tips toward the sun, the southern hemisphere is tipped away.

d.1 yearly energy output
as mentioned, the abscissa of these power curves has the units of time. therefore, the area under
the power curve for any given day determines the total energy output during that day. these daily
energy    gures can be plotted to show seasonal variation, and when all 365 days are summed the
result is the total yearly energy.

because the installation is usually on a pitched roof where the panel elevation is given by roof

orientation and pitch, it is of some interest to compare results over a range of elevation angles.

this diagram shows energy pro   les for a panel at latitude 35

   

, given panel elevations of 0, 10,

20, 30, and 40 degrees, in the order of 1 to 5, respectively. for all 5 curves    p = 0.

the total energy outputs per year are;
   

1. 0

   
2. 10
   
3. 20
   
4. 30
   
5. 40

elevation, 2238
elevation, 2442 (   7.5%)
elevation, 2578 (   2.4%)
elevation, 2639 (    < 1%)
elevation, 2624 (    < 1%)

the numbers in parentheses are the comparisons to 2641     the yearly energy when the panel is at
   
elevation. the 1st result has the panel horizontal (an unrealistic choice); all the rest show less
35
than a 10% loss. a solar panel on a house whose roof has a 4 inch per foot pitch would have an
   
elevation of 18
   

(unless, of course, the panels are elevated further).

   

the total energy pro   le from the installation at 45

latitude, whose panel is elevated at 45

,

is very similar to curve 4, above. its total is slightly lower, at 2607.

d.2. an example 247

d.2 an example
   
the following compares panel(s) placed on a roof sloping at 20
off south toward the east (   p = +45
   
oriented directly south, and whose elevation is 35

   

), whose axis points 45
), with the same panel(s) placed optimally     on a    at roof,

(  p = 70

. both sets of panels are at 35

latitude.

   

   

   

   
the comparative power pro   les (curve 1) for the    35

optimum case    have already been shown.
the energy pro   le comparison is interesting     especially because the    70   45   case (curve 2, above)
does better than    optimum    in the early months of the year. it does lose overall     its total energy
per year is 2476 compared to 2641, a loss of only 6+%.

these energy comparisons suggest that the elevation angle should be increased at the fall
equinox, or even more often. this leads to the possibility of the panel    tracking    the sun position    
changing the panel angles to point more directly toward the sun.

d.3 tracking the sun
continuous tracking systems     used in large, industrial solar energy collectors     include sensors
to determine the exact position of the sun, then point the collector toward it. but, the sun   s position
is repeated every year (with corrections) as shown in this chart. tracking angles: curve 1 is azimuth
(from south), 2 is elevation above the horizon. day 0 is the spring equinox.

248 d. solar energy geometry

tracking both of these angles is sophisticated and expensive. but, with almost no sophistica-

tion, and less expense, excellent tracking results can be obtained.

this chart illustrates the effect of varying the panel elevation angle. the panel angle is varied
elevation, so that the noon power output

   

by subtracting the sun latitude from the initial (day 0) 35
is always the maximum 1.0.

   
curve 1 elevation is the    xed    optimum    of 35

shown in the previous chart. there are two
curves-2.the smooth one shows the results of varying the panel angle every day.the discontinuous
one shows varying it only every 30 days. all results shown are at panel latitude 35
, and the axes of
   
the panels are directly north-south (   p = 0). at the spring and fall equinox 35
is truly optimum,
and all curves are the same on these days.

   

the curves 2 yearly energy output is 2866, and 2852     very nearly the same, and show an

8% increase over that of curve 1     2641 (the smooth curve is +8.5%).

it is assumed that the monthly panel angle changes can be done manually, at the same time

the panels are cleaned. the support structure must be marked with at least 12 positions.

in order to make any further improvement in output, the panel will have to be moved during
each sun day     and this cannot be done manually. further, the best way to do this is to rotate the
panel about its north-south axis, not swing the panel to an azimuth angle.

if the sun panel surface faces the sun directly, every minute of every day in the year, the total

output energy number is 4377     the theoretical best.

d.3. tracking the sun 249

the data in the following chart is obtained by changing the panel elevation angle monthly, as
above, and then rotating the panel about its north-south axis during daylight hours. an initial panel
   
rotation angle of +60
(toward the east) is set before dawn. from 8am until 4pm the panel is rotated
   
at 15
at 4pm, when rotation stops.
the 8 to 4 rotation hours are used during all 365 days.

per hour (the same rate that the earth turns), leaving it at -60

   

the total yearly energy output is 4259; very nearly the same output that could be obtained by

a sophisticated tracking system.

but, it is likely that even this relatively simple tracking system is not economical for residential
systems, due to the low energy output of a single solar panel. the power required to rotate it may be
more than it is worth.

solar panels on rooftops are like wings in the wind. they must be    rmly anchored. when the

panel has to move (rotate) the additional mechanical problems could be signi   cant.

nevertheless, these methods may be economic when adapted to larger systems.
this appendix has demonstrated the use of equations developed in chapter 5. the numbers
calculated are concerned with only the geometry, not with the physics and mechanical problems
associated with real solar panels. for this reason comparative data has been emphasized. for example,
changing the panel elevation angle monthly will certainly not produce energy equal to 2852, but it
probably will increase the actual solar panel output by 7%.

the development algorithms might need some alterations. for example, the earth orbit is not

circular, but elliptical. its orbital speed, assumed to be constant, actually varies.

to implement any algorithm for tracking, the controller (computer) would have to know the

clock time that the sun will be directly overhead (i.e., solar noon).

a p p e n d i x e

251

answers to selected exercises

e.1 chapter 1

2. there are m row vectors and n column vectors.

3. if a and b are square and conformable, ba is conformable. if a is mxn and b is nxm, ba

is conformable

4. u1    v1 = 0.32192
5. the product {v1}[u1] is a 4x4 matrix:

   
         

1.7180

7.4538

15.833
20.9373
   4.1953    2.0197    0.4655    5.6731
9.9504
7.3584
   10.3222    4.9692    1.14534    13.9582

0.8165

3.5424

   
         

6. c = x1a1 + x2a2 +        + xnan
7. v = {0, 0, 0}. u = [ 21, 6, 3 ]

8. it is more ef   cient in all cases to calculate vectors. for example, in determining abcv multiply

cv    rst. in (2) v1u1v2u2, calculate the dot product u1v2    rst

   
    cos    cos        sin   

14. t1t2 =

cos    sin   
sin    sin   
cos   
(cid:5)
trices is always orthogonal. that is: t1t2(t

sin    cos   
    sin   

cos   
0

   
   , and note that the product of orthogonal ma-
1) = i

(cid:5)

2t

17. the transformation matrix is formed by replacing the 3,1 element (a zero) of a unit matrix by

the ratio     a31

.

a11

e.2 chapter 2

1. 5741326, s = 13; 35421, s = 8; 123465, s = 1; 654321, s = 15
2. b44b12b31b23 = b12b23b31b44 s = 2, plus

c43c22c14c51c35, s = 9, minus

252 e. answers to selected exercises

3. |a| = 9; |b| =    14

7. the rank of a1 is 1. it has just one independent vector.

11. the rank of a2 is 2.

13. each factor of the expansion of the 3x3 will have 3 terms. they can all be arranged into
column order. now, just differentiate. the result will be three expansions in exactly the form
to be shown.

1. q =

   
   
e.3 chapter 3
   , whose determinant = 1;
    1 0 0
   3 1 0
      
   
   2 0 1
    1 3    1
    4
0   1

   
    ; qc =

|a| = 2; qa =

0 2
0 0

4
1

      
   

2. from qax = qc, x3 =    1, x2 = 2, and then x1 =    3, solved in that order.

3. in this book, complex matrices are often written with imaginary parts above the reals. the
inverse is written here in this notation. note also, the double bars (instead of bracket), denoting
a matrix.

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

8.00
7.00

   2.00    3.00
6.00    2.00
   1.00    2.00
6.00
6.00    2.00
5.00
   3.00
   1.00
0.00         2.00

1.00
0.00

(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)

   1 =
a

both b and x will be interchanged. compare the following to problems 1 & 2, above:

5. given ax = c, and b = a

given ax = c, with a =

   1, if two columns of a are interchanged, corresponding rows of
   
    3

   
   , then a

   
    2.5
   8.5    1.5
   2
0

0.5    2
7
1

1    1
1
2    1

   
    ,

11 3
6

   1 =

      
    2   3   1

      
   

and x =

e.4 chapter 4
2. a2    a3 =    {3, 6, 1};
4. the rank of m is just 2

e.4. chapter 4

253

(cid:28)
a2    a3     a1 =    14; a2    a3     c =    4. then x1 = 2

7

5(a). neither the columns nor the rows of m are independent.

5(c). since the rank is 2, then x3, x4, and x5 can be chosen arbitrarily, and

.

-

(cid:25)

=

(cid:24)

x1
x2

1
3
7
3

-

.

x3    

x4    

2
3
2
3

-

.

5
3
2
3

x5

5(d). given mx = y, the set will be compatible iff y is orthogonal to {-1, 2, 1}.

6(a). the columns of a are independent.

6(b). the rows of a are dependent.

6(c). z = {   14,    1, 20, 39, 1} is orthogonal to the columns of m.

6(d). x = {   3, 1, 2,    1}

6(e). with the given y vector the set is incompatible.

12. begin with the x matrix shown at right. the desired determinant is obtained by striking out
row 2 and column 2 of x. however, this determinant is just the minor of the 2,2 element in
x. |x| = (x4     x3)(x4     x2)(x4     x2)(x3     x2)(x3     x1)(x2     x1)

   
               

x =

   
               

1 x1
1 x2
1 x3
1 x4

x2
1
x2
2
x2
3
x2
4

x3
1
x3
2
x3
3
x3
4

in the text discussing lagrange polynomials it is shown that xa = i, that is, the two matrices
are inverses. then the minor of the x22 element is a22 multiplied by |x|.

a22 =

x1x3 + x1x4 + x3x4

(x2     x1)(x2     x3)(x2     x4)

|x|a22 is equal to the determinant of the reduced matrix given in the exercise.

254 e. answers to selected exercises

e.5 chapter 5
1(a). x = ty; t =

   
    cos    cos        sin        sin    cos   
cos        sin    sin   
cos   

cos    sin   
sin   

0

   
   ;

      
       = longitude of point a
   = latitude of point a
   ;    = 41
   

  =286

      
   

1(b). the great circle distance is 2496 mi.

1(c). the distance along the 41   latitude line is 2529 mi.

1(d). looking down at point a (down the y1 axis) there is an angle    between the negative y2 axis

and the edge of the great circle path.

a unit vector along the great circle edge has the dimensions {0,    cos   , sin   }.the dot product
of this vector and the normal to the great circle plane must be zero. using this information,
the angle is determined to be 16.28  . the heading is, then, 286.28  .

3. r2  =    rwr =    r    (     r) =    (r      )    r, and note that          = 0
4. the inertia matrix has no cross product terms because of symmetry:

   
    1 0

0
0 4
0
0 0 i33

   
   . the required torque is ma2  2

2

jx = ma2

3

, and most important, its direction

is perpendicular to the surface of the plate as it turns.

e.6 chapter 6

1. the determinant a(  ) =

a12

a22       

a32

a13
a23

a33       

a21
a31

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) a11       
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) a11
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

a21
a31

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) a11       

a21
a31

7t h property of determinants (see chapter 2). as an example

a12

a22       

a32

a13
a23

a33       

a12

a22       

a32

a13
a23

a33       

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) can be expanded by the use of the
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   

0 a22       
0

a33       

a13
a23

a12

a32

e.6. chapter 6

255

with continued use of this property, the following results are obtained:

(cid:12)(cid:12)a(  )
(cid:12)(cid:12) = c0  3 + c1  2 + c2   + c3
c0 =    1
(cid:12)(cid:12)(cid:12)(cid:12) a11
(cid:12)(cid:12)(cid:12)(cid:12) a11
c1 = a11 + a22 + a33
c2 =    
c3 =(cid:12)(cid:12)a
(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)    

a13
a33

a31

a21

(cid:12)(cid:12)(cid:12)(cid:12)    

(cid:12)(cid:12)(cid:12)(cid:12) a22

a32

a12
a22

(cid:12)(cid:12)(cid:12)(cid:12)

a23
a33

this expansion is easier than it appears, and can be generalized to work for characteristic
determinants of higher order.

2. using the above expansion, the characteristic polynomial for the given matrix is:

   
   

= (       1)(   + 2)(       5)

p (  ) = c0  3 + c1  2 + c2   + c3 =      3 + 4  2 + 7       10 =   3     4  2     7   + 10
   
    which

72
24
72    12    24
the adjoint of [a     i] is [a + 2i][a     5i] = a2     3a     10i =
   216
72
yields the two vectors v1 = {1,   1, 3} and u1 = [ 6, 1, -2 ], and note that u1(cid:129)v1 = 1.
in the same manner, the other two vector pairs are determined such that:
u =

   
   ; v =
   
   , and |b(  )| =   3 +   2     2   = 0 =   (       1)(   + 2).

   
   , normalized such that uv = vu = i

   
    1    2 1
   1
2 0
3    7 3

12

36

4. b =

note that the third eigenvalue is zero (and b is singular). however,
adj{b(   = 0)} =    2

   
   , which yields the same eigenvectors that were found for

3
1

   
    6    1    2
0    1
   
0
1
    18    1    6
   18
6
60    3    20
   
    1 1 0

1

0 0 0
3 3 0

5. the (symmetric) a matrix has the eigenvalues, 1 and 4. its eigenvectors are v =

the original a matrix.

(cid:22)

(cid:23)

1   
2

1 1   1 1

256 e. answers to selected exercises

the square root matrix will have the same eigenvectors as a. when this matrix is multiplied
by itself the result is v t  vv t  v = v t  2v. therefore, the square root matrix must be:

1
2

(cid:22)

(cid:23)(cid:22)

2 0
0 1

1 1   1 1

(cid:23)(cid:22)
(cid:22)    0.7
2
   0.6 1.5
=aa(.3) = [a     0.5i] =

(cid:23)

(cid:22)

(cid:23)

.

= 1
2

3    1
   1
3

1    1
1
1
(cid:23)
is   2     0.8   + 0.15 = (       0.3)(       0.5)
(cid:22)    1.2 2
   0.6 1

(cid:23)

;adj{a(  =0.5)} = [a     0.3i] =
(cid:22)    5 10
   3

(cid:23)

=

6

(cid:23)

; z2(  2 = .5) = [a     0.5i]
(0.5     0.3)
(cid:23)

6    10
3    5
(cid:22)    0.624006 1.839053
   0.551716 1.398952

6. the characteristic equation of

(cid:23)

adj{a(  =0.3)}

(cid:22)    1
2
   0.6 1.2
z1(  1 = .3) = [a     0.3i]
(0.3     0.5)

(cid:22)

=

sin(a) = sin(  1)z1 + sin(  2)z2 =

six decimals are given in the event that you would like to show that sin 2(a) + cos 2(a) = i.
] which insists that there is no solution for

7(a). the solution includes the diagonal matrix [  ij

1
  j     

   =   j .

7(c). yet, when the c vector is orthogonal to the j th normal mode (as in this case), the term
         j does not appear in the solution; thus, a solution exists at that critical e-value.
   
though mathematically correct, it would be very dangerous to depend on this, physically.
    which yields the required coef   cients. the

   
    16    66 54

9. a reduces to the p matrix

1
0

0
1

0
0

column vectors are the same as the row vectors, since the given matrix is symmetric.
the roots are 1.08352, 9.86399, and 5.05249
also used in this calculation are the following s and s

s =

   
   ; s

9

9

   
    1
   1 1
1 2
0     1
3
1 1
3
3
0
0
1
   1] = i.

   1 matrices:
   
    9    30 25
0    3
0
0

4
1

   
   

   1 =

note that the matrix product [s][s

e.7. chapter 7

257

e.7 chapter 7
problems 2 & 3. with x1 and x2 measured from    xed locations, the equations of motion of both
are m  x + kx = df cos   t where:
; k =

; a = m

m =

(cid:31)

 

(cid:22)

(cid:23)

(cid:22)

(cid:23)

m1

k1 + k2    k2
   k2

k2

0
m1
0 m2

2. in this case, m1 = m2 and k1 = k2, and there is no driving force. now, set    =

   1k =

k1+k2
   k2

   k2
(cid:27)
; det =  2     3  2   +   4; then    = 3       

m1
k2
m2

m2

k

(cid:23)

m and

5

  2

2

(cid:22)

[i       a] =

       2  2

  2

  2
         2

and the 2 resonant frequencies (rad/sec), are 0.618   and 1.618  .

3. this time, the k and m values are not the same, and there is a driving force as shown above. in
this equation we assume a solution x = acos  t. this reduces the above equation to an algebraic
one by which we can solve for the maximum amplitudes (a1 and a2) of x1 and x2.

     2ma + ka = f; where f = {f0, 0}
(cid:22)
a = [k       2m]   1f
k1 + k2       2m1
[k       2m] =

   k2
set d equal to the determinant of [k -   2m]. then:

(cid:23)

   k2

k2       2m2
(cid:25)
(cid:23)(cid:24)

(cid:22)

a = 1

d

k2       2m2

k2

k2

k1 + k2       2m1

f0
0

note that if the ratio of k2 divided by m2 is made equal to   2, the term k2       2m2 goes to
zero. in both problems 2 and 3, the mass, m1 will be motionless, if m1 is driven at the resonant
frequency of the single mass/spring system   unless the determinant also goes to zero. does it?

5. the matrices are: (it is easier to work with the inverse of m)

   
    38.64

0.0
0.0

   1 =

m

   
   ;

0.0
12.88
0.0

0.0
0.0
12.88

   
   ; k =
   
    0.6    0.3    0.3
   0.3
   0.3

0.8
0

0
0.8

   
    10    5    5
   5
   5
   
   

15
0

0
15

c =

258 e. answers to selected exercises
i
0
   1k    m
   m
   1c

the a matrix is: a =

(cid:22)

(cid:23)

, a 6x6 matrix.

   5.1520
   j12.91

an eigenvalue analysis of this a matrix yields the characteristic numbers:
   2.65415
   j9.89244
the    solution vectors    are given in the table, below. these vectors are the coef   cients of the
cosine and sine terms in the solution. see equation (4.12) in chapter 7. all these decimal places
are not necessary, but it will make it easier for the student to check answers.

   14.08986
   j16061

  1

0.434494
0.324751
0.324751

0.184987
0.126032
0.126032

  2

  3

0.000

0.000
1.287241
0.399084    0.324751    0.298648
1.000
   1.000    0.399084    0.324751    0.298648

1.565506

note how the initial conditions are met by the sums of the cosine term coef   cients. for
example, x10 = 2 = 0.434494 + 1.560556.

6. this problem differs from 5 by only the initial conditions. the motion of the masses, and the

unbalanced force on w1 are plotted here:

the force is in lbs., its maximum is 12.8#, at time 0.11sec. the graph shows the displacement
plot for x1 and x2. the motion of w3 is the same as x2.

7. symmetry suggests that the motion of w1 and w2 will be the same as in the previous problem.
then, a tie between them would make no difference, given the same initial conditions, as in
the problem.

8. at 500 cps the immitances are:

e.7. chapter 7

259

z5
y6
z7
y8

500
j1.571e-3
j5.531e2
j1.571e-3

j5.531e2
j1.571e-3
j4.712e2
2.0e-3

z1
y2
z3
y4
at this frequency the voltage ratio e0
nearly the same as the zero frequency ratio (it   s within the pass of the    lter). but, the ratio at
1kcps is    15.11+j77.77 = 79.22 @ 1.763 rad, or 38 db.

e8 is    1.91 + j0.668 = 2.028 @ 2.81 rad. this is very

bibliography

261

[1] hildebrand, f. b., methods of applied mathematics. prentice-hall, englewood cliffs, new

jersey, 1958.
chapter 1 of this book is an excellent introduction to:    matrices, determinants, and linear
equations.   

[2] frazer, r. a., duncan, w. j., collar, m.a., elementary matrices, and some applications to

dynamics and differential equations. cambridge press.
this is a very complete treatise,very concise.it isn   t all that   elementary.    whatever the question,
the answer is probably in this book.

[3] lanczos, c., applied analysis. prentice-hall, englewood cliffs, new jersey, 1958. 160

a wonderful writer and    explainer.    my favorite.

[4] pipes, l. a., applied mathematics for engineers and physicists. mcgraw-hill book co., 1958.

166, 239

[5] pipes, l. a., matrix methods for engineering. prentice-hall, englewood cliffs, new jersey,

1963.
both of these books are gems. matrix methods is a must for any engineer.

[6] wylie, c. r., advanced engineering mathematics. mcgraw hill, 1960.

a    ne undergraduate book, written by a great teacher at university of utah.

[7] faddeeva, v. n., computational methods of id202. dover publications, inc. new york.
this reference has in   uenced this book through the notes of a dear friend and boss, dr. lee.
i. wilkinson at the general electric co., and later, honeywell. the method of danilevsky
described herein follows lee   s notes     which references this book.

[8] press, w., flannery, b.,teukolsky, s, and vetterling, w. numerical recipes in pascal. cambridge

university press, 1989.
laguerre   s method for determining the initial estimate of a polynomial root, described in
appendix b, was taken from this book, page 296.
this reference describes crout   s method, which is more ef   cient in the solution of simultaneous
equations than that found in this work.

author   s biography

263

marvin j. tobias
a native utahn (salt lake), marvin j. tobias graduated in mechanical engineering (me) from
the university of utah.

he joined the general electric company and began a series of rotating assignments through
various departments in the east, initially in the general engineering laboratory, then in the jet en-
gine department. in the fall, marv successfully applied for a three year intensive training program   
the advanced engineering program. during the latter years of the program his assignments were
in radar signal and data processing.

assignments in both me and ee proved the need for further education and training in

mathematics, so the middle year (b course   applied mathematics) was particularly interesting.

over the years, marv became a lecturer for the b course, developing many notes on matrix
algebra and calculus. as the pc became more powerful, with sophisticated word processing and
graphics software, those notes became the content of this book.

265

index

adjoint matrix, 35

adjoint and inverse, 45
in cramer   s rule, 50
of characteristic equation, 146, 160

algebraic equations, 1
analysis of ladder networks, 214
angular momentum, 133
angular rotation, 123
angular velocity matrix, 126
augmented matrix, 59   62, 68, 69, 71

backward substitution, 63, 65, 67   70

code, 70

characteristic determinant, 51
characteristic equation, 146
characteristic matrix, 145

adjoint of, 149
inverse of, 158

characteristic polynomial, 146
characteristic values, see eigenvalues
combinations, 38, 40
computer graphics, 120
conservative system

example, 191
sinusoidal response, 197
transient response, 191, 196

coordinate transforms, 108

earth centered coordinates, 109

cramer   s rule, 24, 50, 215
cumulant, the, 214

decoupled equations, 158, 194, 204
defective matrix, 157
del operator, the, 95
determinant, 2

cofactors, 37
complex determinant, 51
cramer   s rule, 24, 50
de   nition, 23, 25
evaluation, 46

complex pivot, 46
gaussian reduction, 47
pivotal condensation, 46
rlist, clist, 48

expansion, 23
expansion by lower order minors, 38
expansion by minors, 33

   rst minors, 33

geometric concepts, 41
inversions of indices, 27
laplace expansion, 33, 35
minors and cofactors, 33
of matrix product, 41
permutation of indices, 26
properties of, 30
rank less than n, 50
rank of, 32

differential equations: reduced to 1st order,

201, 202

direction cosine, 84
double pendulum, 188
dynamical matrix, 191

266

index

dynamics of a particle, 127
acceleration, 129, 130
velocity, 128

dynamics of a rigid body, 130
angular momentum, 133
examples, 138
inertia matrix, 134
moment of momentum, 133
rotation, 132
spinning top, 135
torque equation, 137
translation, 131

eigenvalue analysis

danilevsky   s method, 171
example, 148, 168
geometry of, 152
non-symmetric matrix, 148, 168
of similar matrices, 171
symmetric matrices, 151

eigenvalue problem

de   nition of, 145
in vibrating systems, 183

eigenvalues, 146

in complex conjugate pairs, 203
matrix with a double root, 156
of symmetric matrices, 151

eigenvectors, 146
direction, 147
normal modes, 159
normalized, 147
orthogonality, 157
row, column, 147

energy method, see lagrange   s equations
equations of motion, 185

conservative system, 186
non-conservative system, 185, 186

eulerian rotations, 115

forward substitution, 68, 69, 71   73

code, 70

gauss reduction, 47, 61

code, 62
example, 66
method, 47, 62
pivoting, 62, 64, 66, 67
rlist, clist, 64

gauss-jordan reduction, 59, 87, 93

pivoting in, 59, 60
singular matrices, 61

inertia

moment of inertia, 135
product of inertia, 135

initial value problem solution, 191
interpolation, 100
inversion

of a complex matrix, 78
of a triangular matrix, 75
of diagonal matrix, 58

kirchhoff    s laws, 188
kronecker delta, 189

lagrange interpolation polynomials, 100
lagrange   s equations, 184, 185
law of sines, 132
linear equation sets, 55, 83
as vector transform, 83
compatibility, 91, 92
geometry of, 88
least squares solutions, 94
non unique solution, 90
overdetermined, 93
square, n = m, 88
underdetermined, 91
vector solution, 89

linear independence: of vector set, 85   86

id75, 96
equations for, 98
example, 98

lu decomposition, 68

example, 70
pivoting in, 68, 69

matrix

adjoint and inverse, 44
adjoint rank, 45
de   nition of, 1
dimensions, 1, 2

matrix algebra, 3

addition, subtraction, 4
conformability, 4
conformable, 7
multiplication, 6
multiplication by a scalar, 4
non-commutative, 8
of complex matrices, 18
partitioning, 7
vector multiplication, 4
matrix differentiation, 123
matrix inversion, 55

by orthogonalization, 77
by partitioning, 71

example, 79

computer operations in, 72
diagonalization, 57
elementary operations, 55
gauss reduction, 61

algorithm, 63
partial pivoting, 68
pivoting, 62

gauss-jordan reduction, 59
improving the inverse, 74
lu decomposition, 68

example, 70

of a complex matrix, 78

index 267

of a triangular matrix, 75
of diagonal matrix, 58
singular matrices, 61
matrix partitioning, 7, 14

in id127, 14

matrix polynomial, 122
matrix transforms, 121

of matrix product, 122
of matrix sum, 122
of the inverse matrix, 122
of the transpose matrix, 122

matrix types, 9

adjoint matrix, 38, 44
complex matrix, 11
diagonal matrix, 9
inverse matrix, 11
orthogonal matrix, 9
skew-symmetric matrix, 10
symmetric matrix, 10
triangular matrix, 10
unit matrix, 9

mechanical/electrical analogues, 186

n dimensional space, 84
newton   s laws, 185
nonconservative system, 201

example problem, 203
initial problem solution, 203, 207
sinusoidal response, 207, 211
sinusoidal response example, 210

normal modes, 197
notation, 2

matrix, 3
vector, 2, 84

orthogonal

matrix, 105
transform, 105
vectors, 84, 88

268

index

orthonormal, 105

partial pivoting, see gauss reduction
partitioning, see matrix partitioning
pivot, see gauss reduction
polar coordinates, 111
polynomial arithmetic, 229

   nding roots of, 233

de   ation, 233
laguerre method, 234
id77, 234

generation from roots, 227
value at a point, 232

solar energy geometry, see appendix d

solar cosine factor, 117, 244

steady state sinusoidal response, 211

example, 213

submatrix, see matrix partitioning
synthesis of a matrix, 147

trace of a matrix, 149
transform matrices, 13
transposition, 8

of inverse, 12
of product, 8

polynomials, see appendix b

unit vectors, 107

quadratic form, 152, 186

differentiation of, 223

rank

of matrix, 92
of vector set, 86

rayleigh   s dissipation function, 186
reciprocity, 200
reduced matrix, 148
references, 261
right hand rule, 107

positive angle, 107

rlist, clist, 64
runge-kutta integration, 216

simpson   s rule, 216

similar matrices, 121
similarity transforms, see matrix transforms
simultaneous equation sets, see linear

equation sets

solar angles, 116

sun latitude, 118, 119
sun panel vector, 119, 120
sun vector, 117

vector

column, and row, 3
cross product, 17
dimensionality, 84
dot product, 5
row, 3
unit vector, 83

vector sets, 83

   ll a space, 85
linear independence, 85
transposed set, 83

vibrating string, 159
see appendix c

vibrating systems, 183

decoupled equations, 194, 204
setting up equations, 186

vibration of conservative systems, 185, 189

equations of motion, 186, 190

vibration problem

differential equations of, 185

vibrations in a continuous medium, 199

vibrating beam, 199
vibrating string, 159

