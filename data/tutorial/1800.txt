deep	learning	i	

supervised	learning	

russ	salakhutdinov	

machine learning department 
carnegie mellon university 

canadian institute for advanced research	

mining	for	structure	

massive	increase	in	both	computa:onal	power	and	the	amount	of	
data	available	from	web,	video	cameras,	laboratory	measurements.	

images	&	video	

text	&	language		

speech	&	audio	

gene	expression	

product		
recommenda:on	

rela:onal	data/		
social	network	

fmri	

tumor	region	

      	develop	sta:s:cal	models	that	can	discover	underlying	structure,	cause,	or	
sta:s:cal	correla:on	from	data	in	unsupervised	or	semi-supervised	way.		
      	mul:ple	applica:on	domains.	

mining	for	structure	

massive	increase	in	both	computa:onal	power	and	the	amount	of	
data	available	from	web,	video	cameras,	laboratory	measurements.	

images	&	video	

text	&	language		

speech	&	audio	

gene	expression	

product		
recommenda:on	

rela:onal	data/		
social	network	

deep	learning	models	that			
fmri	
support	id136s	and	discover	
structure	at	mul:ple	levels.	

tumor	region	

      	develop	sta:s:cal	models	that	can	discover	underlying	structure,	cause,	or	
sta:s:cal	correla:on	from	data	in	unsupervised	or	semi-supervised	way.		
      	mul:ple	applica:on	domains.	

impact	of	deep	learning	

      	speech	recogni:on	
      	computer	vision	
      	recommender	systems		
      	language	understanding		
      	drug	discovery	and	medical	
image	analysis		

deep	genera:ve	model	
reuters	dataset:	804,414		
newswire	stories:	unsupervised	

model	p(document)	

interbank markets

european community 
monetary/economic  

energy markets

leading          
economic         
indicators       

disasters and 
accidents     

legal/judicial

bag	of	words	

accounts/
earnings 

government 
borrowings 

(hinton & salakhutdinov, science 2006)

example:	understanding	images	

tags:	

strangers,		coworkers,		conven:oneers,		
auendants,		patrons	

nearest	neighbor	sentence:	

people	taking	pictures	of	a	crazy	person	

model	samples	
      	a	group	of	people	in	a	crowded	area	.	
      	a	group	of	people	are	walking	and	talking	.	
      	a	group	of	people,	standing	around	and	talking	.	

cap:on	genera:on	

talk	roadmap	

part	1:	supervised	learning:	deep	networks		

       de   ni:on	of	neural	networks	
       training	neural	networks		
       recent	op:miza:on	/	regulariza:on	techniques	

part	2:	unsupervised	learning:	learning	deep	
genera:ve	models	

part	3:	open	research	ques:ons	

learning	feature	representa:ons	

pixel	1	

pixel	2	

learning 
algorithm 

input	space	

segway	
non-segway	

	

1

	
l

e
x
i
p

pixel	2	

learning	feature	representa:ons	

handle	

	

1

	
l

e
x
i
p

feature 

representation 

learning 
algorithm 

wheel	

input	space	

segway	
non-segway	

feature	space	

	
l

e
e
h
w

pixel	2	

handle	

tradi:onal	approaches	

data 

feature 
extraction 

learning 
algorithm 

object	
detec:on	

audio	
classi   ca:on	

image	

vision	features	

recogni:on	

audio	

audio	features	

speaker	

iden:   ca:on	

computer	vision	features	

sift	

textons	

hog	

gist	

rift	

audio	features	

spectrogram	

mfcc	

flux	

zcr	

rollo   	

audio	features	

spectrogram	

representa:on	learning:	
mfcc	
can	we	automa:cally	learn	
these	representa:ons?	

flux	

zcr	

rollo   	

neural networks online course 

       disclaimer: some of the material and slides for this lecture were 
borrowed from hugo larochelle   s class on neural networks: 
https://sites.google.com/site/deeplearningsummerschool2016/ 

       hugo   s class covers 
many other topics: 
convolutional networks, 
neural language model, 
id82s, 
autoencoders, sparse 
coding, etc. 

       we will use his 
material for some of the 
other lectures.  

feedforward neural networks 

       definition of neural networks  

-    forward propagation 
-    types of units 
-    capacity of neural networks 

       how to train neural nets:  

-    id168 
-    id26 with id119 

       more recent techniques: 

-    dropout 
-    batch id172 
-    unsupervised pre-training 

d  epartement d   informatique

hugo.larochelle@usherbrooke.ca

september 6, 2012

universit  e de sherbrooke

math for my slides    feedforward neural network   .

september 6, 2012

abstract

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

abstract

abstract

artificial neuron 
abstract

hugo.larochelle@usherbrooke.ca
       neuron pre-activation (or input activation): 
september 6, 2012

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    x1 xd b w1 wd
       neuron output activation: 
abstract
    h(x) = g(a(x)) = g(b +pi wixi)
    x1 xd b w1 wd
    x1 xd b w1 wd
    w
    w
    w
    x1 xd b w1 wd
    {
where 
     
    {
    {
    w
    g(  ) b
     
    g(  ) b
    {
     
    g(  ) b
    h(x) = g(a(x))
    g(  ) b
    h(x) = g(a(x))
    h(x) = g(a(x))
    h(x) = g(a(x))

 are the weights (parameters) 
 is the bias term 
 is called the activation function   

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

i,j xj   
i pj w (1)
i,j xj   
i,j xj   
i pj w (1)
i pj w (1)

math for my slides    feedforward neural network   .

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

abstract
abstract
artificial neuron 

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

       output activation of the neuron: 

    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    x1 xd b w1 wd
    w
    w
range is 
    {
    {
determined  
    g(  ) b
by  
    g(  ) b
    h(x) = g(a(x))
    h(x) = g(a(x))

bias only changes 
the position of the 
riff 

(from pascal vincent   s slides) 

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    o(x) = g(out)(b(2) + w(2)>x)
    o(x) = g(out)(b(2) + w(2)>x)

i,j xj   
i,j xj   
i pj w (1)
i pj w (1)

    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    w
activation function  
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b
    h(x) = g(a(x))

1

       sigmoid activation function:  

      squashes the neuron   s 
output between 0 and 1  

      always positive 
      bounded 
      strictly increasing  

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    o(x) = g(out)(b(2) + w(2)>x)

i pj w (1)

       rectified linear (relu) activation function:  

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

1

    g(a) = a
    g(a) = sigm(a) =
activation function  
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    h(x) = g(a(x))

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    o(x) = g(out)(b(2) + w(2)>x)

i pj w (1)

      bounded below by 0 

(always non-negative) 

     

tends to produce units 
with sparse activities 
      not upper bounded 
      strictly increasing  

i

i

i,j

i,j

b(2)

b(1)
i

exp(2a)+1

b(1)
i

exp(2a)+1
exp(2a)+1

exp(a)+exp( a) = exp(2a) 1
exp(a)+exp( a) = exp(2a) 1

    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b
    g(a) = max(0, a)
    g(a) = max(0, a)
single hidden layer neural net 
    g(a) = max(0, a)
    w (1)
xj h(x)i w(2)
b(2)
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    g(a) = reclin(a) = max(0, a)
       hidden layer pre-activation: 
    h(x) = g(a(x))
    w (1)
xj h(x)i w(2)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    p(y = 1|x)
i +pj w (1)
    p(y = 1|x)
    h(x) = g(a(x))
    g(  ) b
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
i +pj w (1)
    g(  ) b
    f (x) = o(b(2) + w(2)>x)
    w (1)
xj h(x)i w(2)
    f (x) = o(b(2) + w(2)>x)
xj h(x)i w(2)
    w (1)
i
       hidden layer activation: 
i
    h(x) = g(a(x))
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
       output layer activation: 
    f(x) = o   b(2) + w(2)>h(1)x   
    f(x) = o   b(2) + w(2)>x   

i,j xj   
i +pj w (1)
i,j xj   
i +pj w (1)

b(1)
b(1)
i
i

b(2)
b(2)

i,j
i,j

1

1

output activation 
function 

1
1

exp(ac )

exp(ac )

exp(ac )

pc exp(ac)i>
multilayer neural net 
pc exp(ac)i>
pc exp(ac)i>

    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
    p(y = c|x)
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
       consider a network with l hidden layers.  
    f (x)
pc exp(ac) . . .
    p(y = c|x)
-    layer pre-activation for k>0 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
-    hidden layer activation  
    f (x)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
   from 1 to l: 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
-    output layer activation (k=l+1): 
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)

    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)

exp(ac )

capacity of neural nets 

       consider a single layer neural network 

r  eseaux de neurones

2

x2

1

z

1

0

-1

-1

0

0

-1

x1

1

zk

y1

1

0

-1

-1

0

1

0

-1

x1

x2

1

biais
bias 

.5

output 
sortie k
wkj

y2

y2

1

-1

0

cach  ee j
hidden 
wji
entr  ee i
input 

-1

y1

.7

-.4

-1.5

1 1

1

1

x1

x2

x2

1

z=-1

z=+1

r1

-1

r2

x1

1

x2

1

0

1

-1

x1

-1

0

(from pascal vincent   s slides) 

capacity of neural nets 

       consider a single layer neural network 

(from pascal vincent   s slides) 

universal approximation 

       universal approximation theorem (hornik, 1991): 

-       a single hidden layer neural network with a linear output 
unit can approximate any continuous function arbitrarily well, 
given enough hidden units       

       this applies for sigmoid, tanh and many other activation 
functions. 

       however, this does not mean that there is learning algorithm that 
can find the necessary parameter values.  

feedforward neural networks 

       how neural networks predict f(x) given an input x: 

-    forward propagation 
-    types of units 
-    capacity of neural networks 

       how to train neural nets:  

-    id168 
-    id26 with id119 

       more recent techniques: 

-    dropout 
-    batch id172 
-    unsupervised pre-training 

    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

       empirical risk minimization: 

training  

arg min

   

1

t xt

l(f(x(t);    ), y(t)) +     (   )

id168  

regularizer 

       learning is cast as optimization.  

      for classification problems, we would like to minimize 
classification error. 

      id168 can sometimes be viewed as a surrogate for 
what we want to optimize (e.g. upper bound)  

feedforward neural network

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

   

abstract
hugo larochelle

l(f (x(t);    ), y(t)) +     (   )

math for my slides    feedforward neural network   .

t pt r   l(f (x(t);    ), y(t))    r      (   )
t xt

    l(f (x(t);    ), y(t))
      =   1
       (   )
              +  
   
t pt r   l(f (x(t);    ), y(t))    r      (   )
stochastic gradient descend 
1
      =   1
    {x 2 rd | rxf (x) = 0}
arg min
    v>r2
xf (x)v > 0 8v
              +  
       perform updates after seeing each example:  
    f (x)
-    initialize:  
    v>r2
    {x 2 rd | rxf (x) = 0}
xf (x)v < 0 8v
    l(f (x(t);    ), y(t))
            {w(1), b(1), . . . , w(l+1), b(l+1)}
-    for t=1:t 
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
xf (x)v > 0 8v
    l(f (x(t);    ), y(t))
       (   )
-    for each training example  
    (x(t), y(t))
    v>r2
xf (x)v < 0 8v
    r   l(f (x(t);    ), y(t))
      =   1
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
       (   )
    f (x)
              +      
    f (x)
    r      (   )
    l(f (x(t);    ), y(t))
    {x 2 rd | rxf (x) = 0}
       to train a neural net, we need: 
    f (x)
5
    f (x)
    f (x)c = p(y = c|x)
    l(f(x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
       id168: 
    l(f (x(t);    ), y(t))
    x(t) y(t)
    r   l(f (x(t);    ), y(t))
       (   )
       a procedure to compute gradients: 
    r   l(f (x(t);    ), y(t))
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    v>r2
xf (x)v < 0 8v
    r   l(f (x(t);    ), y(t))
       regularizer and its gradient:          ,   
    r      (   )
       (   )
       (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    f (x)c = p(y = c|x)
    r      (   )
    r      (   )
    (x(t), y(t))
    r      (   )

t pt r   l(f (x(t);    ), y(t))    r      (   )

5
math for my slides    feedforward neural network   .

iteration of all examples 

training epoch 

abstract

   

= 

september 13, 2012

@

f (x)c   log f (x)y =  1(y=c)

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

computational flow graph  

       forward propagation can be represented 
as an acyclic flow graph 

       forward propagation can be implemented 
in a modular way: 

      each box can be an object with an fprop 
method, that computes the value of the 
box given its children 

      calling the fprop method of each box in 
the right order yields forward propagation 

computational flow graph  

       each object also has a bprop method 
-    it computes the gradient of the loss with 
respect to each child box.  

       by calling bprop in the reverse order, we 
obtain id26 

       training protocol: 

machine learning

model selection 

t b    = 1
    t 1
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f (x;    )

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
    supervised learning example: (x, y) x y
t b    = 1
    t 1
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    supervised learning example: (x, y) x y
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}
      hyper-parameter search: hidden layer size, learning rate, 
number of iterations/epochs, etc. 
    f (x;    )
-    estimate generalization performance using the test set 
    dvalid dtest

-    train your model on the training set  
-    for model selection, use validation set  

t pt(x(t)  b  )(x(t)  b  )>

       generalization is the behavior of the model on unseen 
examples.  

early stopping 

       to select the number of epochs, stop training when validation set 
error increases (with some look ahead). 

2   

       make updates based on a mini-batch of examples (instead of a 
single example): 

@x     f (x+   ) f (x    )
mini-batch, momentum 

    w(3) w(2) w(1) x f (x)
    @f (x)
    f (x) x    
    f (x +    ) f (x      )
    p1t=1    t = 1
the gradient is the average regularized loss for that mini-batch 
    p1t=1    2
can give a more accurate estimate of the gradient 
can leverage matrix/matrix operations, which are more efficient 
       t =    
       t =    
    r(t)

    = r   l(f (x(t)), y(t)) +  r(t 1)

       momentum: can use an exponential average of previous 
gradients: 

t  0.5 <       1  

t < 1    t

     
     
     

1+ t

   

can get pass plateaus more quickly, by       gaining momentum       

     

feedforward neural networks 

       how neural networks predict f(x) given an input x: 

-    forward propagation 
-    types of units 
-    capacity of neural networks 

       how to train neural nets:  

-    id168 
-    id26 with id119 

       more recent techniques: 

-    dropout 
-    batch id172 
-    unsupervised pre-training 

learning distributed representations 
       deep learning is research on learning models with multilayer 
representations 

      multilayer (feed-forward) neural networks  
      multilayer graphical model (deep belief network, deep boltzmann 

machine) 

       each layer learns       distributed representation       
      units in a layer are not mutually exclusive 

each unit is a separate feature of the input 
two units can be       active       at the same time 

      
      
 units do not correspond to a partitioning (id91) of the inputs 
      

in id91, an input can only belong to a single cluster 

     

local vs. distributed representations  
      	id91,	nearest	
neighbors,	rbf	id166,	local	
density	es:mators			
      	parameters	for	each	region.	
      	#	of	regions	is	linear	with							
		#	of	parameters.	

      	rbms,	factor	models,	
pca,	sparse	coding,	
deep	models	

local	regions	

c1=1	
c2=1	

c1=0	
c2=1	

c1=1	
c2=0	

c1=0	
c2=0	

c1	

c2	

c3	

learned	
prototypes	

(bengio, 2009, foundations and 
trends in machine learning)

local vs. distributed representations  
      	id91,	nearest	
neighbors,	rbf	id166,	local	
density	es:mators			
      	parameters	for	each	region.	
      	#	of	regions	is	linear	with							
		#	of	parameters.	

      	rbms,	factor	models,	
pca,	sparse	coding,	
deep	models	

c1=1	
c2=1	
c3=1	

local	regions	

c1=1	
c2=1	
c3=0	

c1=1	
c2=0	
c3=0	

c1	

c2	

c1=0	
c2=0	
c3=0	
c3	

learned	
prototypes	

c1=0	
c2=1	
c3=0	

c1=0	
c2=1	
c3=1	

c1=0	
c2=0	
c3=1	

local vs. distributed representations  
      	id91,	nearest	
neighbors,	rbf	id166,	local	
density	es:mators			
      	parameters	for	each	region.	
      	#	of	regions	is	linear	with							
		#	of	parameters.	

      	rbms,	factor	models,	
pca,	sparse	coding,	
deep	models	
      	each	parameter	a   ects	many	
regions,	not	just	local.	
      	#	of	regions	grows	(roughly)	
exponen:ally	in	#	of	parameters.	
c1=1	
c1=0	
c2=1	
c2=0	
c3=0	
c3=1	

c1=0	
c2=1	
c3=0	

c1=1	
c2=1	
c3=1	

c1=1	
c2=1	
c3=0	

local	regions	

c1=0	
c2=0	
c3=0	
c3	

c1	

c2	

learned	
prototypes	

c1=0	
c2=0	
c3=1	

inspiration from visual cortex 

why training is hard 

       first hypothesis: hard optimization 
problem (underfitting) 

     
     

vanishing gradient problem 
saturated units block gradient 
propagation 

       this is a well known problem in 
recurrent neural networks 

why training is hard 

       second hypothesis: overfitting 
      we are exploring a space of complex functions 
     

deep nets usually have lots of parameters 

       might be in a high variance / low bias situation 

why training is hard 

       first hypothesis (underfitting): better optimize 

      use better optimization tools (e.g. batch-id172, second 

order methods, such as kfac) 

      use gpus, distributed computing.  

       second hypothesis (overfitting): use better id173 

      unsupervised pre-training 
      stochastic drop-out training 

       for many large-scale practical problems, you will need to use both: 
better optimization and better id173!  

unsupervised pre-training 

       initialize hidden layers using unsupervised learning 

     

force network to represent latent structure of input distribution 

      encourage hidden layers to encode that structure 

unsupervised pre-training 

       initialize hidden layers using unsupervised learning 

     

this is a harder task than supervised learning (classification) 

      hence we expect less overfitting 

math for my slides    autoencoders   .

hugo larochelle
autoencoders: preview 
h(x) = g(a(x))

d  epartement d   informatique
universit  e de sherbrooke

       feed-forward neural network trained to reproduce its input at the 
output layer 

hugo.larochelle@usherbrooke.ca

= sigm(b + wx)

october 16, 2012
decoder 

math for my slides    autoencoders   .

    f (x)    bx l(f (x)) =pk(bxk   xk)2

abstract

= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

encoder 

for binary units 

h(x) = g(a(x))

= sigm(b + wx)

bx = o(ba(x))

    f (x)    bx l(f (x)) =pk(bxk   xk)2

   

       id168 for binary inputs 

h(x) = g(a(x))

= sigm(b + wx)

      cross-id178 error function 

autoencoders: preview 
bx = o(c + w   h(x))
= sigm(c + w   h(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
    f(x)    bx l(f(x)) =pk(bxk   xk)2
bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

2pk(bxk   xk)2

     
      we use a linear activation function at the output 

= sigm(c + w   h(x))

sum of squared differences 

       id168 for real-valued inputs 

    f (x)    bx l(f (x)) = 1
    rba(x(t))l(f (x(t))) =bx(t)   x(t)

a(x(t)) (= b + wx(t)
h(x(t)) (= sigm(a(x(t)))

ba(x(t)) (= c + w>h(x(t))

pre-training 
       we will use a greedy, layer-wise procedure 

train one layer at a time with unsupervised criterion 
fix the parameters of previous hidden layers 

     
     
      previous layers can be viewed as feature extraction 

fine-tuning 

       once all layers are pre-trained 
     
     

add output layer 
train the whole network using 
supervised learning 

       we call this last phase fine-tuning 
     
all parameters are       tuned       for the 
supervised task at hand 
representation is adjusted to be more 
discriminative 

     

why training is hard 

       first hypothesis (underfitting): better optimize 

      use better optimization tools (e.g. batch-id172, second 

order methods, such as kfac) 

      use gpus, distributed computing.  

       second hypothesis (overfitting): use better id173 

      unsupervised pre-training 
      stochastic drop-out training 

       for many large-scale practical problems, you will need to use both: 
better optimization and better id173!  

dropout 

       key idea: cripple neural network by removing hidden units 
stochastically 

     

     

     

each hidden unit is set to 0 with 
id203 0.5 

hidden units cannot co-adapt to 
other units 

hidden units must be more 
generally useful 

       could use a different dropout 
id203, but 0.5 usually works well 

exp(ac )

exp(ac )

layer pre-activation for k>0 

    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
dropout 
    p(y = c|x)
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
       use random binary masks m(k)  
    f (x)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
     
    f (x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
     
hidden layer activation (k=1 to l): 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
      output activation (k=l+1) 
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)

exp(ac )

dropout at test time  

       at test time, we replace the masks by their expectation 

     
     

this is simply the constant vector 0.5 if dropout id203 is 0.5 
for single hidden layer: equivalent to taking the geometric average 
of all neural networks, with all possible binary masks 

       can be combined with unsupervised pre-training 
       beats regular id26 on many datasets 

       ensemble: can be viewed as a geometric average of exponential 
number of networks.  

why training is hard 

       first hypothesis (underfitting): better optimize 

      use better optimization tools (e.g. batch-id172, second 

order methods, such as kfac) 

      use gpus, distributed computing.  

       second hypothesis (overfitting): use better id173 

      unsupervised pre-training 
      stochastic drop-out training 

       for many large-scale practical problems, you will need to use both: 
better optimization and better id173!  

batch id172 

       normalizing the inputs will speed up training (lecun et al. 1998) 
could id172 be useful at the level of the hidden layers? 

     

       batch id172 is an attempt to do that (ioffe and szegedy, 2014) 

     

     
     
     

each unit   s pre-activation is normalized (mean subtraction, stddev 
division) 
during training, mean and stddev is computed for each minibatch 
id26 takes into account the id172 
at test time, the global mean / stddev is used 

batch id172 

activation function (     and    are trained) 

 and    are trained) 

learned linear transformation to adapt to non-linear 

malize each dimension
!x(k) =

xi

1
m

  b    

// mini-batch mean

m#i=1
batch id172 
m#i=1
xi       b"  2

x(k)     e[x(k)]
"var[x(k)]
  2
b    
       why normalize the pre-activation? 
!xi    
yi       !xi +        bn  ,  (xi)
algorithm 1: batch normalizing transform, applied to
activation x over a mini-batch.

can help keep the pre-activation in a non-saturating regime 
(though the linear transform                             could cancel this 
effect) 

// normalize
// scale and shift

       use the global mean and stddev at test time. 

// mini-batch variance

(xi       b)2

b +   

1
m

     

where the expectation and variance are computed over the
training data set. as shown in (lecun et al., 1998b), such
id172 speeds up convergence, even when the fea-
tures are not decorrelated.
note that simply normalizing each input of a layer may
change what the layer can represent. for instance, nor-
malizing the inputs of a sigmoid would constrain them to
the linear regime of the nonlinearity. to address this, we
make sure that the transformation inserted in the network
can represent the identity transform. to accomplish this,

     

the bn transform can be added to a network to manip-
ulate any activation. in the notation y = bn  ,  (x), we

removes the stochasticity of the mean and stddev 

     

requires a final phase where, from the first to the last hidden layer 
      
      

propagate all training data to that layer 
compute and store the global mean and stddev of each unit 

3

     

for early stopping, could use a running average 

optimization tricks 

       sgd with momentum, batch-id172, and dropout usually 
works very well 

       pick learning rate by running on a subset of the data 
      start with large learning rate & divide by 2 until loss does not diverge 
      decay learning rate by a factor of ~100 or more by the end of training  

       use relu nonlinearity  

        initialize parameters so that each feature across layers has 
similar variance. avoid units in saturation. 

[from marc'aurelio ranzato, cvpr 2014 tutorial] 

visualization 

       check gradients numerically by finite differences 

       visualize features (features need to be uncorrelated) and have 
high variance 

       good training: hidden units 
are sparse across samples  

[from marc'aurelio ranzato, cvpr 2014 tutorial] 

visualization 

       check gradients numerically by finite differences 

       visualize features (features need to be uncorrelated) and have 
high variance 

        visualize parameters: learned features should exhibit structure 
and should be uncorrelated and are uncorrelated  

visualization 

       check gradients numerically by finite differences 

       visualize features (features need to be uncorrelated) and have 
high variance 

       bad training: many hidden 
units ignore the input and/or 
exhibit strong correlations 

id161  

       design algorithms that can process visual data to accomplish a given task:  

     

for example, object recognition: given an input image, identify 
which object it contains 

deep convolutional nets 

very deep network 

prediction 

   . 

       convolution 
       pooling 
       id172 
       densely connected 

high-level feature 
space 

deep convolutional nets 

    

    

pooling 

convolution 

convnets: examples 

       id42, house number and traffic sign 
classification 

convnets: examples 

       pedestrian detection 

(sermanet et al., pedestrian detection with unsupervised multi-stage, cvpr 2013)

convnets: examples 

       id164  

sermanet et al., overfeat: integrated recognition, localization, 2013
girshick et al., rich feature hierarchies for accurate id164, 2013
szegedy et al., dnn for id164, nips 2013

id163 dataset 

       1.2 million images, 1000 classes 

examples of hammer 

(deng et al., id163: a large scale hierarchical image database, cvpr 2009)

important	breakthrough	

      	deep	convolu:onal	nets	for	vision	(supervised)		

krizhevsky, a., sutskever, i. and hinton, g. e., id163 classi   cation with deep 
convolutional neural networks, nips, 2012. 

1.2	million	training	images	
1000	classes	

architecture  

       how can we select the right architecture: 
      manual tuning of features is now replaced with the manual tuning 

of architechtures 

       depth 
       width 
       parameter count 

how to choose architecture  

       many hyper-parameters: 
      number of layers, number of feature maps 

       cross validation 

       grid search (need lots of gpus) 

       smarter strategies  
      random search  
      bayesian optimization  

alexnet 

       8 layers total 

       trained on id163 
dataset [deng et al. cvpr   09] 

       18.2% top-5 error  

[from rob fergus    cifar 2016 tutorial] 

softmax output 

layer 7: full 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

alexnet 

       remove top fully connected layer 7  

softmax output 

       drop ~16 million parameters 

       only 1.1% drop in performance! 

[from rob fergus    cifar 2016 tutorial] 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

alexnet 

       let us remove upper feature extractor layers 
and fully connected: 

softmax output 

     

layers 3,4, 6 and 7 

       drop ~50 million parameters 

       33.5 drop in performance! 

       depth of the network is the key.  

[from rob fergus    cifar 2016 tutorial] 

layer 6: full 

layer 5: conv + pool 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

googlenet 

       24 layer model that uses so-called inception 
module.  

convolution 
pooling 
softmax 
other 

(szegedy et al., going deep with convolutions, 2014)

googlenet 

       googlenet inception module: 

      multiple filter scales at each layer 
      id84 to keep computational requirements down 

number 
of filters 

1x1 

3x3 

5x5 

(a) inception module, na    ve version

(b) inception module with dimension reductions

figure 2: inception module

(szegedy et al., going deep with convolutions, 2014)
increase in the number of outputs from stage to stage. even while this architecture might cover the

(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:22)(cid:91)(cid:22)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:24)(cid:91)(cid:24)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:41)(cid:76)(cid:79)(cid:87)(cid:72)(cid:85)(cid:3)(cid:70)(cid:82)(cid:81)(cid:70)(cid:68)(cid:87)(cid:72)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:51)(cid:85)(cid:72)(cid:89)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:22)(cid:91)(cid:22)(cid:3)(cid:80)(cid:68)(cid:91)(cid:3)(cid:83)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:20)(cid:91)(cid:20)(cid:3)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)googlenet 

       width of inception modules ranges from 256 filters (in early modules) to 
1024 in top inception modules. 
       can remove fully connected layers on top completely 
       number of parameters is reduced to 5 million 
       6.7% top-5 validation error on imagnet 

(szegedy et al., going deep with convolutions, 2014)

deep residual learning for image recognition

xiangyu zhang

microsoft research

shaoqing ren

residual networks  

jian sun

{kahe, v-xiangz, v-shren, jiansun}@microsoft.com

really, really deep convnets do not train well,  
e.g. cifar10: 

top-1 err.
28.07

output 

size: 224

output 

size: 112

output 

size: 56

vgg-19

34-layer plain

34-layer residual

image

image

image

3x3 conv, 64

3x3 conv, 64

pool, /2

3x3 conv, 128

3x3 conv, 128

7x7 conv, 64, /2

7x7 conv, 64, /2

pool, /2

pool, /2

pool, /2

3x3 conv, 256

64-d

3x3 conv, 64

256-d

3x3 conv, 64

3x3 conv, 256

3x3, 64

relu
3x3 conv, 256

3x3, 64

3x3 conv, 256

3x3 conv, 64

1x1, 64

3x3 conv, 64

3x3 conv, 64

relu

3x3, 64

relu

3x3 conv, 64

3x3 conv, 64

1x1, 256

3x3 conv, 64

relu

relu

3x3 conv, 64

3x3 conv, 64

3x3 conv, 64

3x3 conv, 64

residual network. based on the above plain network, we
insert shortcut connections (fig. 3, right) which turn the
network into its counterpart residual version. the identity
shortcuts (eqn.(1)) can be directly used when the input and
output are of the same dimensions (solid line shortcuts in
fig. 3). when the dimensions increase (dotted line shortcuts
in fig. 3), we consider two options: (a) the shortcut still
performs identity mapping, with extra zero entries padded
for increasing dimensions. this option introduces no extra
parameter; (b) the projection shortcut in eqn.(2) is used to
match dimensions (done by 1   1 convolutions). for both
options, when the shortcuts go across feature maps of two
sizes, they are performed with a stride of 2.
3.4. implementation

20

10

)

%

(
 
r
o
r
r
e
 

g
n
i
n
i
a
r
t

 

0 
0

1

2

3

4
iter. (1e4)

 

56-layer

20-layer

5

6

20

10

)

%

(
 
r
o
r
r
e
 
t
s
e
t

 

0
0

1

model
vgg-16 [41]
googlenet [44]
prelu-net [13]
plain-34
resnet-34 a
resnet-34 b
resnet-34 c
resnet-50
resnet-101
resnet-152

56-layer

20-layer

-

 

24.27
28.54
25.03
24.52
24.19
22.85
21.75
21.43

top-5 err.

9.33
9.15
7.38
10.02
7.76
7.46
7.40
6.71
6.05
5.71

5

6

2

3

4
iter. (1e4)

table 3. error rates (%, 10-crop testing) on id163 validation.
vgg-16 is based on our test. resnet-50/101/152 are of option b
that only uses projections for increasing dimensions.

x

greatly bene   ted from very deep models.

figure 1. training error (left) and test error (right) on cifar-10
key idea: introduce    pass 
with 20-layer and 56-layer    plain    networks. the deeper network
method
through    into each layer 
has higher training error, and thus test error. similar phenomena
vgg [41] (ilsvrc   14)
on id163 is presented in fig. 4.
googlenet [44] (ilsvrc   14)
vgg [41] (v5)
prelu-net [13]
thus only residual now 
bn-inception [16]
resnet-34 b
needs to be learned 
resnet-34 c
resnet-50
resnet-101
resnet-152

driven by the signi   cance of depth, a question arises: is
learning better networks as easy as stacking more layers?
an obstacle to answering this question was the notorious
problem of vanishing/exploding gradients [1, 9], which
hamper convergence from the beginning. this problem,
f(x)
method
however, has been largely addressed by normalized initial-
vgg [41] (ilsvrc   14)
ization [23, 9, 37, 13] and intermediate id172 layers
googlenet [44] (ilsvrc   14)
vgg [41] (v5)
[16], which enable networks with tens of layers to start con-
prelu-net [13]
f(x)(cid:1)+(cid:1)x
verging for stochastic id119 (sgd) with back-
bn-inception [16]
resnet (ilsvrc   15)
propagation [22].
when deeper networks are able to start converging, a
degradation problem has been exposed: with the network
depth increasing, accuracy gets saturated (which might be

figure 2. residual learning: a building block.
(he, zhang, ren, sun, cvpr 2016)

weight layer

weight layer

identity

x

relu

relu

table 4. error rates (%) of single-model results on the id163
validation set (except     reported on the test set).

with ensembling, 3.57% top-5 
test error on id163 

table 5. error rates (%) of ensembles. the top-5 error is on the
test set of id163 and reported by the test server.

size: 28

output 

pool, /2

figure 5. a deeper residual function f for id163. left: a
building block (on 56   56 feature maps) as in fig. 3 for resnet-
34. right: a    bottleneck    building block for resnet-50/101/152.

3x3 conv, 128, /2

3x3 conv, 128, /2

3x3 conv, 128

3x3 conv, 128

3x3 conv, 512

3x3 conv, 512

3x3 conv, 128

3x3 conv, 128

3x3 conv, 512

3x3 conv, 128

3x3 conv, 128

output 

size: 14

pool, /2

3x3 conv, 256

3x3 conv, 256

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 256

3x3 conv, 256

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 512

3x3 conv, 512

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256, /2

3x3 conv, 256, /2

top-1 err.

-
-

top-5 err.
8.43   
7.89
7.1
5.71
5.81
5.71
5.60
5.25
4.60
4.49

parameter-free, identity shortcuts help with training. next
we investigate projection shortcuts (eqn.(2)). in table 3 we
compare three options: (a) zero-padding shortcuts are used
for increasing dimensions, and all shortcuts are parameter-
free (the same as table 2 and fig. 4 right); (b) projec-
tion shortcuts are used for increasing dimensions, and other
shortcuts are identity; and (c) all shortcuts are projections.
table 3 shows that all three options are considerably bet-
ter than the plain counterpart. b is slightly better than a. we
argue that this is because the zero-padded dimensions in a
indeed have no residual learning. c is marginally better than
b, and we attribute this to the extra parameters introduced
by many (thirteen) projection shortcuts. but the small dif-
ferences among a/b/c indicate that projection shortcuts are
not essential for addressing the degradation problem. so we
do not use option c in the rest of this paper, to reduce mem-
ory/time complexity and model sizes. identity shortcuts are
particularly important for not increasing the complexity of
the bottleneck architectures that are introduced below.
deeper bottleneck architectures. next we describe our
deeper nets for id163. because of concerns on the train-
ing time that we can afford, we modify the building block
as a bottleneck design4. for each residual function f, we
use a stack of 3 layers instead of 2 (fig. 5). the three layers
are 1   1, 3   3, and 1   1 convolutions, where the 1   1 layers
are responsible for reducing and then increasing (restoring)
dimensions, leaving the 3   3 layer a bottleneck with smaller
input/output dimensions. fig. 5 shows an example, where
both designs have similar time complexity.
the parameter-free identity shortcuts are particularly im-
figure 3. example network architectures for id163. left: the
portant for the bottleneck architectures. if the identity short-

24.4
21.59
21.99
21.84
21.53
20.74
id163 test set, and won the 1st place in the ilsvrc
19.87
19.38
2015 classi   cation competition. the extremely deep rep-
resentations also have excellent generalization performance
on other recognition tasks, and lead us to further win the
1st places on: id163 detection, id163 localization,
coco detection, and coco segmentation in ilsvrc &
coco 2015 competitions. this strong evidence shows that
the residual learning principle is generic, and we expect that
it is applicable in other vision and non-vision problems.

7.32
6.66
6.8
4.94
4.82
3.57

top-5 err. (test)

3x3 conv, 512, /2

3x3 conv, 512, /2

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

avg pool

avg pool

pool, /2

output 

output 

fc 1000

fc 4096

fc 4096

fc 1000

fc 1000

size: 1

size: 7

our implementation for id163 follows the practice
in [21, 41]. the image is resized with its shorter side ran-
domly sampled in [256, 480] for scale augmentation [41].
a 224   224 crop is randomly sampled from an image or its
horizontal    ip, with the per-pixel mean subtracted [21]. the
standard color augmentation in [21] is used. we adopt batch
id172 (bn) [16] right after each convolution and
before activation, following [16]. we initialize the weights
as in [13] and train all plain/residual nets from scratch. we
use sgd with a mini-batch size of 256. the learning rate
starts from 0.1 and is divided by 10 when the error plateaus,
and the models are trained for up to 60    104 iterations. we
use a weight decay of 0.0001 and a momentum of 0.9. we
do not use dropout [14], following the practice in [16].

in testing, for comparison studies we adopt the standard
10-crop testing [21]. for best results, we adopt the fully-
convolutional form as in [41, 13], and average the scores
at multiple scales (images are resized such that the shorter
side is in {224, 256, 384, 480, 640}).
4. experiments
4.1. id163 classi   cation

we evaluate our method on the id163 2012 classi   -
cation dataset [36] that consists of 1000 classes. the models
are trained on the 1.28 million training images, and evalu-
ated on the 50k validation images. we also obtain a    nal
result on the 100k test images, reported by the test server.
we evaluate both top-1 and top-5 error rates.
plain networks. we    rst evaluate 18-layer and 34-layer
plain nets. the 34-layer plain net is in fig. 3 (middle). the
18-layer plain net is of a similar form. see table 1 for de-

are comparably good or better than the constructed solution

resnet reduces the top-1 error by 3.5% (table 2), resulting
from the successfully reduced training error (fig. 4 right vs.

deeper neural networks are more dif   cult to train. we
present a residual learning framework to ease the training
of networks that are substantially deeper than those used
previously. we explicitly reformulate the layers as learn-
ing residual functions with reference to the layer inputs, in-
stead of learning unreferenced functions. we provide com-
prehensive empirical evidence showing that these residual
networks are easier to optimize, and can gain accuracy from
considerably increased depth. on the id163 dataset we
evaluate residual nets with a depth of up to 152 layers   8   
deeper than vgg nets [41] but still having lower complex-
ity. an ensemble of these residual nets achieves 3.57% error
on the id163 test set. this result won the 1st place on the
ilsvrc 2015 classi   cation task. we also present analysis

the depth of representations is of central importance
for many visual recognition tasks. solely due to our ex-
tremely deep representations, we obtain a 28% relative im-
provement on the coco id164 dataset. deep
residual nets are foundations of our submissions to ilsvrc
& coco 2015 competitions1, where we also won the 1st
places on the tasks of id163 detection, id163 local-
ization, coco detection, and coco segmentation.

choosing the architecture 

       task dependent 

       cross-validation 

        [convolution     pooling]* + fully connected layer  

       the more data: the more layers and the more kernels 
     
     

look at the number of parameters at each layer 
look at the number of flops at each layer 

       computational resources 

[from marc'aurelio ranzato, cvpr 2014 tutorial] 

end	of	part	1	

