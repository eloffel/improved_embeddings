deep	learning	ii	

unsupervised	learning	

russ	salakhutdinov	
machine learning department 
carnegie mellon university

canadian institute of advanced research

talk	roadmap	

part	1:	supervised	learning:	deep	networks		

part	2:	unsupervised	learning:	learning	deep	
generaave	models	

part	3:	open	research	quesaons	

unsupervised	learning	

non-probabilisac	models	

      sparse	coding	
      autoencoders	
      others	(e.g.	id116)	

probabilisac	(generaave)	
models	

tractable	models	
      fully	observed	

belief	nets	

      nade	
      pixelid56	

non-tractable	models	
      boltzmann	machines	
      variaaonal	

autoencoders	

      helmholtz	machines	
      many	others   	

      generaave	adversarial	

      moment	matching	

networks	

networks	

explicit	density	p(x)	

implicit	density	

talk	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	generaave	models	

      restricted	boltzmann	machines	
      deep	belief	networks	and	deep	boltzmann	machines	
      helmholtz	machines	/	variaaonal	autoencoders		

      	generaave	adversarial	networks		
      	model	evaluaaon		

sparse	coding	

      	sparse	coding	(olshausen	&	field,	1996).	originally	developed	
to	explain	early	visual	processing	in	the	brain	(edge	detecaon).		
      	objecave:	given	a	set	of	input	data	vectors																														
learn	a	dicaonary	of	bases																																such	that:					

sparse:	mostly	zeros	

      	each	data	vector	is	represented	as	a	sparse	linear	combinaaon	
of	bases.	

sparse	coding	

				natural	images	

learned	bases:		   edges   	

new	example 

= 0.8 *                   + 0.3 *                     + 0.5 * 

          + 0.5 *       	

     x 

     = 0.8 *       						         +  0.3 *        					

	[0,	0,	   	0.8,	   ,	0.3,	   ,	0.5,	   ]	=	coe   cients	(feature	representaaon)		
slide	credit:	honglak	lee	

sparse	coding:	training	

      	input	image	patches:		
      	learn	dicaonary	of	bases:	

reconstrucaon	error	

sparsity	penalty	

      	alternaang	opamizaaon:	

1.    fix	dicaonary	of	bases																											and	solve	for	
2.    fix	acavaaons	a,	opamize	the	dicaonary	of	bases	(convex	

acavaaons	a	(a	standard	lasso	problem).			

qp	problem).		

sparse	coding:	tesang	time	

      	input:	a	new		image	patch	x*	,	and	k	learned	bases			
      	output:	sparse	representaaon	a	of	an	image	patch	x*.		

= 0.8 *                   + 0.3 *                     + 0.5 * 

          + 0.5 *       	

     x*       = 0.8 *       						         +  0.3 *        					

	[0,	0,	   	0.8,	   ,	0.3,	   ,	0.5,	   ]	=	coe   cients	(feature	representaaon)		

image	classi   caaon	
evaluated	on	caltech101	object	category	dataset.	

classification 

algorithm 

(id166) 

input	image 

learned		
bases 

features	(coe   cients) 

	9k	images,	101	classes	

algorithm	

accuracy	

baseline	(fei-fei	et	al.,	2004)	

pca	

sparse	coding	

16%	
37%	
47%	

slide	credit:	honglak	lee	

(lee, battle, raina, ng, nips 2007)

interpreang	sparse	coding	

a	

sparse	features	

g(a)	

explicit	
linear	
decoding	

x   	

a	

x	

f(x)	

implicit	
nonlinear	
encoding	

      	sparse,	over-complete	representaaon	a.	
      	encoding	a	=	f(x)	is	implicit	and	nonlinear	funcaon	of	x.		
      	reconstrucaon	(or	decoding)	x   	=	g(a)	is	linear	and	explicit.		

autoencoder	

feature representation 

feed-back, 
generative, 
top-down 
path	

decoder 

encoder 

feed-forward,  
bottom-up	

input image 

      	details	of	what	goes	insider	the	encoder	and	decoder	maler!	
      	need	constraints	to	avoid	learning	an	idenaty.		

autoencoder	

 binary features z 

decoder 
filters d 

linear 
function 
path	

dz 

z=  (wx) 

encoder 
filters w. 

sigmoid 
function	

input image x 

autoencoder	

 binary features z 

  (wtz) 

z=  (wx) 

encoder 
filters w. 

sigmoid 
function	

decoder 
filters d 
path	

binary input x 

      	need	addiaonal	constraints	to	avoid	learning	an	idenaty.		
      	relates	to	restricted	boltzmann	machines	(later).		

math for my slides    autoencoders   .

autoencoders	

hugo larochelle
h(x) = g(a(x))

d  epartement d   informatique
universit  e de sherbrooke

      	feed-forward	neural	network	trained	to	reproduce	its	input	at	the	output	
layer	

hugo.larochelle@usherbrooke.ca

= sigm(b + wx)

october 16, 2012
decoder	

math for my slides    autoencoders   .

    f (x)    bx l(f (x)) =pk(bxk   xk)2

abstract

= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

for	binary	units	

encoder	
h(x) = g(a(x))

= sigm(b + wx)

bx = o(ba(x))

    f (x)    bx l(f (x)) =pk(bxk   xk)2

id168 

   

h(x) = g(a(x))

       id168 for binary inputs 

= sigm(b + wx)

= sigm(c + w   h(x))

      cross-id178 error function (reconstruction loss) 

bx = o(c + w   h(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

    f(x)    bx l(f(x)) =pk(bxk   xk)2

2pk(bxk   xk)2

     
sum of squared differences (reconstruction loss) 
      we use a linear activation function at the output 

= sigm(c + w   h(x))

       id168 for real-valued inputs 

    f (x)    bx l(f (x)) = 1
    rba(x(t))l(f (x(t))) =bx(t)   x(t)

a(x(t)) (= b + wx(t)
h(x(t)) (= sigm(a(x(t)))

ba(x(t)) (= c + w>h(x(t))

autoencoder	

 linear features z 

wz 

z=wx 

input image x 

      		if	the	hidden	and	output	layers	
are	linear,	it	will	learn	hidden	units	
that	are	a	linear	funcaon	of	the	data	
and	minimize	the	squared	error.	
      	the	k	hidden	units	will	span	the	
same	space	as	the	   rst	k	principal	
components.	the	weight	vectors	
may	not	be	orthogonal.		

      	with	nonlinear	hidden	units,	we	have	a	nonlinear	
generalizaaon	of	pca.	

= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

   k,   k (u  ,   k)>    x w    w

      similar to dropouts on the input layer 
      gaussian additive noise 

       idea: representation should be robust to introduction of noise: 
     

random assignment of subset of 
inputs to 0, with id203 

denoising autoencoder 

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = sigm(c + w   h(ex))
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    l(f (x(t))) +  ||rx(t)h(x(t))||2
 x
   
    p(ex|x)     ex
 x
    l(f (x(t))) =  pk   x(t)

t pt

(vincent et al., icml 2008)

k log(bx(t)

f

       id168 compares 
reconstruction with the noiseless 
x
input 

       reconstruction     computed 
from the corrupted input 

||rx(t)h(x(t))||2

k ) + (1   x(t)

   k,   k (u  ,   k)>    x w    w

= i   k,       1 u> x
=     1

(7)

   k,   k (u  ,   k)>    x w    w

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and

consequently rewrite equation 6 as

q? l(q?, ex)]}

h = max
denoising autoencoder 
= max

   0 {eeq0(ex)[max
   0,q?{eeq0(ex)[l(q?, ex)]}
t0=1 x(t0)   

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
    p(ex|x)     ex

t pt

qd(  x|x)

g   0(f   (  x))

x

  x

  x

x
x

figure 2. manifold learning perspective.
suppose
training data (   ) concentrate near a low-dimensional man-

ifold. corrupted examples (.) obtained by applying cor-

where the third line is obtained because (   ,    0)

we chose p(y ) uniform,

have no in   uence on eeq0(x,ex,y )[log p(y )] because
eeq0(x,ex)[log p(ex|x)], and the last line is obtained
p(x|y = f   (ex)) is a bg   0 (f   (ex)).

by inspection of the de   nition of lih in eq. 2, when

4.3. other theoretical perspectives

rect
rect
rect-img
rect-img
convex
convex

2.15  0.13
2.15  0.13
24.04  0.37
24.04  0.37
19.13  0.34
19.13  0.34

2.15  0.13
2.15  0.13
24.05  0.37
24.05  0.37
19.82  0.35
19.82  0.35

learned filters 

4.71  0.19
4.71  0.19
23.69  0.37
23.69  0.37
19.92  0.35
19.92  0.35

2.41  0.13
2.41  0.13
24.05  0.37
24.05  0.37
18.41  0.34
18.41  0.34

2.60  0.14
2.60  0.14
22.50  0.37
22.50  0.37
18.63  0.34
18.63  0.34

non-corrupted 

25% corrupted input 

(a) no destroyed inputs

(a) no destroyed inputs

(b) 25% destruction

(b) 25% destruction

(vincent et al., icml 2008)

2.15  0.13
rect
24.05  0.37
rect-img
19.82  0.35
convex

4.71  0.19
23.69  0.37
19.92  0.35

2.15  0.13
24.04  0.37
19.13  0.34

2.15  0.13
24.05  0.37
19.82  0.35

2.60  0.14
2.41  0.13
24.05  0.37
22.50  0.37
18.41  0.34
18.63  0.34
learned filters 

4.71  0.19
23.69  0.37
19.92  0.35

1.99  0.12 (10%)
2.41  0.13
21.59  0.36 (25%)
24.05  0.37
19.06  0.34 (10%)
18.41  0.34

2.60  0.14
22.50  0.37
18.63  0.34

non-corrupted 

50% corrupted input 

(a) no destroyed inputs

(b) 25% destruction
(a) no destroyed inputs

(b) 25% destruction

(c) 50% destruction

(vincent et al., icml 2008)

predicave	sparse	decomposiaon	

l1 sparsity 

decoder 
filters d 
path	

at training 
time 
path	

 binary features z 

dz 

z=  (wx) 

encoder 
filters w. 

sigmoid 
function	

real-valued input x 

decoder	

encoder	

(kavukcuoglu, ranzato, fergus, lecun, 2009)

stacked	autoencoders	

class labels 

decoder 

encoder 

features 

sparsity 

decoder 

encoder 

features 

sparsity 

decoder 

encoder 

input x 

stacked	autoencoders	

class labels 

decoder 

encoder 

features 

sparsity 

decoder 

encoder 

features 

greedy	layer-wise	learning.		

decoder 

encoder 

sparsity 

input x 

stacked	autoencoders	

class labels 

      	remove	decoders	and	
use	feed-forward	part.		
      	standard,	or	
convoluaonal	neural	
network	architecture.		
      	parameters	can	be	
   ne-tuned	using	
backpropagaaon.		

encoder 

features 
encoder 

features 
encoder 

input x 

deep	autoencoders	

decoder

top
rbm

rbm

rbm

30

w

500

4

500

w
1000

3

1000
w
2000

2

2000
w

1

t
1

t
2

t
3

t
4
code layer
4

w
2000
w
1000
w

500

w

30

w

500

w
1000
w
2000
w

3

2

1

2000

1000

w +(cid:1)

t
1

w +(cid:1)

t
2

w +(cid:1)

t
3

8

7

6

500

w

30

t
4

+(cid:1)

5

w +(cid:1)

4

500

w +(cid:1)

3

1000

w +(cid:1)

2

2000

4

3

2

w +(cid:1)

1

1

rbm

encoder

pretraining

unrolling

fine(cid:1)tuning

deep	autoencoders	

      	25x25	   	2000	   	1000	   	500	   	30	autoencoder	to	extract	30-d	real-
valued	codes	for	oliver	face	patches.			

      	top:	random	samples	from	the	test	dataset.			
      	middle:	reconstrucaons	by	the	30-dimensional	deep	autoencoder.	
      	bobom:	reconstrucaons	by	the	30-dimenanoal	pca.		

(hinton and salakhutdinov, science 2006)

informaaon	retrieval	

interbank markets

european community 
monetary/economic  

2-d	lsa	space	

energy markets

leading          
economic         
indicators       

disasters and 
accidents     

legal/judicial

accounts/
earnings 

government 
borrowings 

      	the	reuters	corpus	volume	ii	contains	804,414	newswire	stories	
(randomly	split	into	402,207	training	and	402,207	test).	
      	   bag-of-words   	representaaon:	each	aracle	is	represented	as	a	vector	
containing	the	counts	of	the	most	frequently	used	2000	words	in	the	
training	set.	

(hinton and salakhutdinov, science 2006)

semanac	hashing	

address space

european community 
monetary/economic

disasters and 
accidents

semantically
similar
documents

semantic
hashing
function

document 

energy markets

government 
borrowing

accounts/earnings

      	learn	to	map	documents	into	semanfc	20-d	binary	codes.	
      	retrieve	similar	documents	stored	at	the	nearby	addresses	with	no	
search	at	all.	

(salakhutdinov and hinton, sigir 2007)

searching	large	image	database	

using	binary	codes	

      	map	images	into	binary	codes	for	fast	retrieval.	

      	small codes, torralba, fergus, weiss, cvpr 2008
       spectral hashing, y. weiss, a. torralba, r. fergus, nips 2008
       kulis and darrell, nips 2009, gong and lazebnik, cvpr 2011
       norouzi and fleet, icml 2011,

unsupervised	learning	

non-probabilisac	models	

      sparse	coding	
      autoencoders	
      others	(e.g.	id116)	

probabilisac	(generaave)	
models	

tractable	models	
      fully	observed	

belief	nets	

      nade	
      pixelid56	

non-tractable	models	
      boltzmann	machines	
      variaaonal	

autoencoders	

      helmholtz	machines	
      many	others   	

      generaave	adversarial	

      moment	matching	

networks	

networks	

explicit	density	p(x)	

implicit	density	

talk	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	generaave	models	

      restricted	boltzmann	machines	
      deep	belief	networks	and	deep	boltzmann	machines	
      helmholtz	machines	/	variaaonal	autoencoders		

      	generaave	adversarial	networks		
      	model	evaluaaon		

deep	generaave	model	

sanskrit	

model	p(image)	

25,000	characters	from	50	
alphabets	around	the	world.	

      	3,000	hidden	variables	
      	784		observed	variables	
			(28	by	28	images)	
      	about	2	million	parameters	

bernoulli	markov	random	field	

deep	generaave	model	

condiaonal	
simulaaon	

p(image|paraal	image)	 bernoulli	markov	random	field	

deep	generaave	model	

condiaonal	
simulaaon	

why	so	di   cult?	

28	

28	

possible	images!	

p(image|paraal	image)	

bernoulli	markov	random	field	

maximum likelihood

fully-visible belief net

      	explicitly	model	condiaonal	probabiliaes:	

   

fully	observed	models	
       = arg max
ex   pdata log pmodel(x |    )
nyi=2

pmodel(xi | x1, . . . , xi 1)

pmodel(x) = pmodel(x1)

each	condiaonal	can	be	a	
complicated	neural	network	

      	a	number	of	successful	models,	including		
      nade,	rnade	(larochelle,	et.al.	

20011)	
pixel	id98	(van	den	ord	et.	al.	2016)	
pixel	id56	(van	den	ord	et.	al.	2016)	

     
     

pixel	id98	

restricted	boltzmann	machines	

feature	detectors	

		hidden	variables	

pair-wise	

unary	

image						visible	variables	

rbm	is	a	markov	random	field	with:	
      	stochasac	binary	visible	variables																										
      	stochasac	binary	hidden	variables																							
      	biparate	connecaons.	

markov random    elds, id82s, id148. 

learning	features	

observed		data		

subset	of	25,000	characters	

learned	w:		   edges   	
subset	of	1000	features	

new	image:	

=	

   .	

logisac	funcaon:	suitable	for	
modeling	binary	images	

model	learning	

		hidden	units	

given	a	set	of	i.i.d.	training	examples		

	

	

	

														,	we	want	to	learn		

model	parameters

	

	

						.				

maximize	log-likelihood	objecave:	

image						visible	units	

derivaave	of	the	log-likelihood:	

di   cult	to	compute:	exponenaally	many		
con   guraaons	

model	learning	

		hidden	variables	

derivaave	of	the	log-likelihood:		

image						visible	variables	

easy	to	
compute	exactly	

di   cult	to	compute:	
exponenaally	many	
con   guraaons.		

use	mcmc	

approximate	maximum	likelihood	learning	

approximate	learning	

      	an	approximaaon	to	the	gradient	of	the	log-likelihood	objecave:		

      	replace	the	average	over	all	possible	input	con   guraaons	by	samples.	
      	run	mcmc	chain	(gibbs	sampling)	starang	from	the	observed	
examples.	

      	iniaalize	v0	=	v		
      	sample	h0	from	p(h	|	v0)		
      	for	t=1:t		

-	sample	vt	from	p(v	|	ht-1)	
	-	sample	ht	from	p(h	|	vt)	

approximate	ml	learning	for	rbms	
run	markov	chain	(alternaang	gibbs	sampling):	

   	

data	

t=1	

t=	in   nity	

equilibrium	
distribuaon	

contrasave	divergence	

a	quick	way	to	learn	rbm:	

data	

reconstructed	data	

update	model	parameters:	

      	start	with	a	training	vector	
on	the	visible	units.	
      	update	all	the	hidden	units	
in	parallel.		
      	update	the	all	the	visible	
units	in	parallel	to	get	a	
   reconstrucaon   .	
      	update	the	hidden	units	
again.	

implementaaon:	~10	lines	of	matlab	code.	

(hinton, neural computation 2002)

rbms	for	real-valued	data	

		hidden	variables	

pair-wise	

unary	

image						visible	variables	

gaussian-bernoulli	rbm:	
      	stochasac	real-valued	visible	variables																		
      	stochasac	binary	hidden	variables																						
      	biparate	connecaons.	

rbms	for	real-valued	data	

		hidden	variables	

pair-wise	

unary	

image						visible	variables	

4	million	unlabelled	images	

learned	features	(out	of	10,000)	

rbms	for	real-valued	data	

4	million	unlabelled	images	

learned	features	(out	of	10,000)	

=  0.9 *            +  0.8 *            + 0.6 *                

new	image	

rbms	for	word	counts	

pair-wise	

unary	

hjaj1a

exp0@
dxi=1

w k

ijvk

0	
0	
0	
1	
0	

i hj +

p   (vk

p   (v, h) =

fxj=1

kxk=1

1
z(   )

dxi=1
kxk=1
exp   bk
i +pf
q=1 exp   bq
i +pf
pk
replicated	sowmax	model:	undirected	topic	model:	
      	stochasac	1-of-k	visible	variables.	
      	stochasac	binary	hidden	variables																							
      	biparate	connecaons.	

i = 1|h) =

vk
i bk

i +

fxj=1
ij   

ij   

j=1 hjw k

j=1 hjw q

(salakhutdinov & hinton, nips 2010, srivastava & salakhutdinov, nips 2012)

rbms	for	word	counts	

pair-wise	

unary	

0	
0	
0	
1	
0	

p   (v, h) =

1
z(   )

exp0@
dxi=1

kxk=1

p   (vk

i = 1|h) =

hjaj1a

w k

ijvk

i hj +

fxj=1

dxi=1
kxk=1
exp   bk
i +pf
q=1 exp   bq
i +pf
pk

vk
i bk

i +

fxj=1
ij   

ij   

j=1 hjw k

j=1 hjw q

reuters	dataset:	
804,414	unlabeled	
newswire	stories	
bag-of-words		

learned	features:	``topics      	

russian	
russia	
moscow	
yeltsin	
soviet	

clinton	
house	
president	
bill	
congress	

computer	
system	
product	
sowware	
develop	

trade	
country	
import	
world	
economy	

stock	
wall	
street	
point	
dow	

rbms	for	word	counts	

one-step	reconstrucaon	from	the	replicated	sowmax	
model.		

collaboraave	filtering	

binary	hidden:	user	preferences	

h

w1

v

mulanomial	visible:	user	raangs	

nezlix	dataset:		
480,189	users		
17,770	movies		
over	100	million	raangs	

learned	features:	``genre      	

fahrenheit	9/11	
bowling	for	columbine	
the	people	vs.	larry	flynt	
canadian	bacon	
la	dolce	vita	

independence	day	
the	day	awer	tomorrow	
con	air	
men	in	black	ii	
men	in	black	

friday	the	13th	
the	texas	chainsaw	massacre	
children	of	the	corn	
child's	play	
the	return	of	michael	myers	

scary	movie	
naked	gun		
hot	shots!	
american	pie		
police	academy	

(salakhutdinov, mnih, hinton, icml 2007)

di   erent	data	modaliaes	

      	binary/gaussian/sowmax	rbms:	all	have	binary	hidden	
variables	but	use	them	to	model	di   erent	kinds	of	data.	

binary	

real-valued	

1-of-k	

0	
0	
0	
1	
0	

      	it	is	easy	to	infer	the	states	of	the	hidden	variables:		

product	of	experts	

the	joint	distribuaon	is	given	by:	

marginalizing	over	hidden	variables:	

product	of	experts	

government	
authority	
power	
empire	
federaaon	

clinton	
house	
president	
bill	
congress	

bribery	
corrupaon	
dishonesty	
corrupt	
fraud	

ma   a	
business	
gang	
mob	
insider	

stock	
wall	
street	
point	
dow	

   	

silvio	berlusconi	

topics	   government   ,	   corrupaon   	
and	   ma   a   	can	combine	to	give	very	
high	id203	to	a	word	   silvio	
berlusconi   .	

product	of	experts	

the	joint	distribuaon	is	given	by:	

marginalizing	over	hidden	variables:	

50

product	of	experts	

government	
authority	
power	
empire	
federaaon	

)

%

clinton	
(
house	
 
n
o
president	
s
bill	
c
e
congress	
r
p

i

i

40

30

20

10

replicated 
softmax 50   d

bribery	
corrupaon	
dishonesty	
lda 50   d
corrupt	
fraud	

ma   a	
business	
gang	
mob	
insider	

stock	
wall	
street	
point	
dow	

   	

0.001     0.006     0.051     0.4        1.6       6.4       25.6      100 

topics	   government   ,	   corrupaon   	
and	   ma   a   	can	combine	to	give	very	
high	id203	to	a	word	   silvio	
berlusconi   .	

recall (%) 

silvio	berlusconi	

talk	roadmap	

      	basic	building	blocks	(non-probabilisac	models):	

     
sparse	coding	
      autoencoders	

      	deep	generaave	models	

      restricted	boltzmann	machines	
      deep	boltzmann	machines	
      helmholtz	machines	/	variaaonal	autoencoders		

      	generaave	adversarial	networks		

h3

h2

h1

v

deep	belief	network		

w3

w2

w1

      	probabilisac	generaave	model.		
      	contains	mulaple	layers	of	nonlinear	
representaaon.	
      	fast,	greedy	layer-wise	pretraining	
algorithm.		
      	inferring	the	states	of	the	latent	
variables	in	highest	layers	is	easy.		

      	inferring	the	states	of	the	latent	variables	in	highest	layers	
is	easy.		

(hinton et al. neural computation 2006)

deep	belief	network		

low-level	features:	
edges	

built	from	unlabeled	inputs.		

input:	pixels	

image	

(hinton et al. neural computation 2006)

deep	belief	network	

internal	representaaons	capture	
higher-order	staasacal	structure	

higher-level	features:	
combinaaon	of	edges	

low-level	features:	
edges	

built	from	unlabeled	inputs.		

input:	pixels	

image	

(hinton et al. neural computation 2006)

deep	belief	network	

hidden	
layers	

visible	layer	

rbm	

sigmoid	
belief	
network	

deep	belief	network	

deep	belief	network	

the	joint	id203	

distribuaon	factorizes:	

rbm	

sigmoid		
belief		
network	

sigmoid	belief		
network	

rbm	

generaave	
process	

deep	belief	network	

approximate	
id136	

h3

h1

h2

v

w3

w2

w1

dbn	layer-wise	training	

       learn	an	rbm	with	an	input	
layer	v	and	a	hidden	layer	h.	

h

w1

v

dbn	layer-wise	training	

       learn	an	rbm	with	an	input	
layer	v	and	a	hidden	layer	h.	

       treat	inferred	values																																

	

	

	

	

			as	the	data	

for	training	2nd-layer	rbm.	
       learn	and	freeze	2nd	layer	

rbm.	

h2

v

w1   

h1

w1

dbn	layer-wise	training	

       learn	an	rbm	with	an	input	
layer	v	and	a	hidden	layer	h.	

unsupervised	feature	learning.	

       treat	inferred	values																																

	

	

	

	

			as	the	data	

for	training	2nd-layer	rbm.	
       learn	and	freeze	2nd	layer	

rbm.	

       proceed	to	the	next	layer.	

h3

h1

w3

w2

w1

h2

v

dbn	layer-wise	training	

       learn	an	rbm	with	an	input	
layer	v	and	a	hidden	layer	h.	

unsupervised	feature	learning.	

       treat	inferred	values																																

h3

	

	

	

	

			as	the	data	

for	training	2nd-layer	rbm.	
       learn	and	freeze	2nd	layer	

rbm.	
layerwise	pretraining		improves	
       proceed	to	the	next	layer.	
h1
variaaonal	lower	bound	

h2

v

w3

w2

w1

why	this	pre-training	works?	

       greedy	training	improves	variaaonal	lower	bound!	

       for	any	approximaang	

distribuaon			

h

w1

v

why	this	pre-training	works?	

       greedy	training	improves	variaaonal	lower	bound.	
       rbm	and	2-layer	dbn	are	equivalent	
h2

when																				

       the	lower	bound	is	aght	and	
the	log-likelihood	improves	by	
greedy	training.	

       for	any	approximaang	

distribuaon			

w1   

h1

w1

v

train	2nd-layer	rbm	

learning	part-based	representaaon	

convoluaonal	dbn	

faces	

h3

h1

h2

v

w3

w2

w1

groups	of	parts.	

object	parts	

trained	on	face	images.	

(lee, grosse, ranganath, ng, icml 2009)

learning	part-based	representaaon	
faces	
chairs	

elephants	

cars	

(lee, grosse, ranganath, ng, icml 2009)

learning	part-based	representaaon	

groups	of	parts.	

class-speci   c	object	
parts	

trained	from	mulaple	
classes	(cars,	faces,	
motorbikes,	airplanes).	

(lee, grosse, ranganath, ng, icml 2009)

