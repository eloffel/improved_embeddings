deep	learning	iii	

unsupervised	learning	

russ	salakhutdinov	
machine learning department 
carnegie mellon university

canadian institute of advanced research

unsupervised	learning	

non-probabilis;c	models	

      sparse	coding	
      autoencoders	
      others	(e.g.	id116)	

probabilis;c	(genera;ve)	
models	

tractable	models	
      fully	observed	

belief	nets	

      nade	
      pixelid56	

non-tractable	models	
      boltzmann	machines	
      varia;onal	

autoencoders	

      helmholtz	machines	
      many	others   	

      genera;ve	adversarial	

      moment	matching	

networks	

networks	

explicit	density	p(x)	

implicit	density	

talk	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera;ve	models	

      restricted	boltzmann	machines	
      deep	belief	networks	and	deep	boltzmann	machines	
      helmholtz	machines	/	varia;onal	autoencoders		

      	genera;ve	adversarial	networks		
      	model	evalua;on		

dbns	vs.	dbms	

deep belief network
h3

deep id82
h3

h2

h1

v

w3

w2

w1

h2

h1

v

w3

w2

w1

dbns	are	hybrid	models:		

      	id136	in	dbns	is	problema;c	due	to	explaining	away.	
      	only	greedy	pretrainig,	no	joint	op/miza/on	over	all	layers.		
      	approximate	id136	is	feed-forward:	no	bo6om-up	and	top-down.				

mathema;cal	formula;on	

deep	boltzmann	machine	

model	parameters	

h3

h2

h1

v

       dependencies	between	hidden	variables.	
       all	connec;ons	are	undirected.	
       bovom-up	and	top-down:	

w3

w2

w1

bovom-up	

top-down	

input	

unlike	many	exis;ng	feed-forward	models:	convnet	(lecun),	
hmax	(poggio	et.al.),	deep	belief	nets	(hinton	et.al.)	

mathema;cal	formula;on	

deep	boltzmann	machine	

neural	network		

output	

deep	belief	network	

h3

h2

h1

v

h3

w3

h2

w2

h1

w1

v

h3
w3
h2
w2
h1
w1
v

w3

w2

w1

input	

unlike	many	exis;ng	feed-forward	models:	convnet	(lecun),	
hmax	(poggio),	deep	belief	nets	(hinton)	

mathema;cal	formula;on	

deep	boltzmann	machine	

neural	network		

output	

deep	belief	network	

h3

h2

h1

v

h3

w3

h2

w2

h1

w1

v

h3
w3
h2
w2
h1
w1
v

w3

i

w2

n
f
e
r
e
n
c
e

	

w1

input	

unlike	many	exis;ng	feed-forward	models:	convnet	(lecun),	
hmax	(poggio),	deep	belief	nets	(hinton)	

mathema;cal	formula;on	

deep	boltzmann	machine	

model	parameters	

h3

h2

h1

v

w3

w2

w1

       dependencies	between	hidden	variables.	

maximum	likelihood	learning:	

problem:	both	expecta;ons	are	

intractable!	

learning	rule	for	undirected	graphical	models:		
mrfs,	crfs,	factor	graphs.		

approximate	learning	

h3

h2

h1

v

(approximate)	maximum	likelihood:	

       both	expecta;ons	are	intractable!		

w3

w2

w1

not	factorial	any	more!	

approximate	learning	

h3

h2

h1

v

w3

w2

w1

(approximate)	maximum	likelihood:	

data	

not	factorial	any	more!	

approximate	learning	

h3

h2

h1

v

w3

w2

w1

(approximate)	maximum	likelihood:	

varia;onal	
	id136	

stochas;c	
approxima;on		
(mcmc-based)	

not	factorial	any	more!	

previous	work	

many	approaches	for	learning	boltzmann	machines	have	been	
proposed	over	the	last	20	years:	
      	hinton	and	sejnowski	(1983),	
      	peterson	and	anderson	(1987)	
      	galland	(1991)		
      	kappen	and	rodriguez	(1998)	
      	lawrence,	bishop,	and	jordan	(1998)	
      	tanaka	(1998)		
      	welling	and	hinton	(2002)		
      	zhu	and	liu	(2002)	
      	welling	and	teh	(2003)	
      	yasuda	and	tanaka	(2009)			

real-world	applica;ons	   	thousands		
of	hidden	and	observed	variables	
with	millions	of	parameters.	

many	of	the	previous	approaches	were	not	successful	for	learning	
general	boltzmann	machines	with	hidden	variables.	

algorithms	based	on	contras;ve	divergence,	score	matching,	pseudo-
likelihood,	composite	likelihood,	mcmc-id113,	piecewise	learning,	cannot	
handle	mul;ple	layers	of	hidden	variables.			

new	learning	algorithm	

posterior	id136	

condi;onal	

approximate	
condi;onal	

simulate	from	the	model	
uncondi;onal	

approximate	the	
joint	distribu;on	

(salakhutdinov, 2008; nips 2009)

new	learning	algorithm	

posterior	id136	

condi;onal	

approximate	
condi;onal	

simulate	from	the	model	
uncondi;onal	

approximate	the	
joint	distribu;on	

data-dependent	

data-independent	

density	

match		

(salakhutdinov, 2008; nips 2009)

new	learning	algorithm	

posterior	id136	

condi;onal	

approximate	
condi;onal	

mean-field	

simulate	from	the	model	
uncondi;onal	

approximate	the	
joint	distribu;on	

markov	chain	
monte	carlo	

data-dependent	

data-independent	

match		

key	idea:	
data-dependent:					varia/onal	id136,	mean-   eld	theory	
data-independent:		stochas/c	approxima/on,	mcmc	based	

stochas;c	approxima;on	

time		t=1	

t=2	

t=3	

h2

h1

v

update	

h2

h1

v

update	

h2

h1

v

update						and						sequen;ally,		where	

       generate

	

	

	

	

						by	simula;ng	from	a	markov	chain	

that	leaves									invariant	(e.g.	gibbs	or	m-h	sampler)	

       update 						by	replacing	intractable		

	

								with	a	point	

es;mate		

in	prac;ce	we	simulate	several	markov	chains	in	parallel.	

(robbins and monro, ann. math. stats, 1957;
 l. younes,  id203 theory 1989)

learning	algorithm	

update	rule	decomposes:	

true	gradient	

perturba;on	term	

almost	sure	convergence	guarantees	as	learning	rate			

problem:	high-dimensional	data:	
the	id203	landscape	is		
highly	mul;modal.	

markov	chain	
monte	carlo	

key	insight:	the	transi;on	operator	can	be		
any	valid	transi;on	operator	   	tempered		
transi;ons,	parallel/simulated	tempering.	

connec;ons	to	the	theory	of	stochas;c	approxima;on	and	adap;ve	mcmc.	

varia;onal	id136	

approximate	intractable	distribu;on																	with	simpler,	tractable	
distribu;on 	

					:	

posterior	id136	

mean-field	

varia;onal	lower	bound	

minimize	kl	between	approxima;ng	and	true	
distribu;ons	with	respect	to	varia;onal	parameters					.		

varia;onal	id136	

approximate	intractable	distribu;on																	with	simpler,	tractable	
distribu;on 	

					:	

posterior	id136	

mean-field	

varia;onal	lower	bound	

mean-field:	choose	a	fully	factorized	distribu;on:	

with	

varia/onal	id136:	maximize	the	lower	bound	w.r.t.	
varia;onal	parameters					.		

nonlinear	   xed-	
point	equa;ons:	

varia;onal	id136	

approximate	intractable	distribu;on																	with	simpler,	tractable	
distribu;on 	

					:	

posterior	id136	

mean-field	

varia;onal	lower	bound	

uncondi;onal	simula;on	

1.	varia/onal	id136:	maximize	the	lower		
bound	w.r.t.	varia;onal	parameters						

markov	chain	
monte	carlo	

2.	mcmc:	apply	stochas;c	approxima;on		
to	update	model	parameters					

										

almost	sure	convergence	guarantees	to	an	asympto;cally	
stable	point.	

varia;onal	id136	

approximate	intractable	distribu;on																	with	simpler,	tractable	
distribu;on 	

					:	

posterior	id136	

mean-field	

varia;onal	lower	bound	

uncondi;onal	simula;on	

markov	chain	
monte	carlo	

1.	varia/onal	id136:	maximize	the	lower		
bound	w.r.t.	varia;onal	parameters						

fast	id136	
learning	can	scale	to	
millions	of	examples	

2.	mcmc:	apply	stochas;c	approxima;on		
to	update	model	parameters					

										

almost	sure	convergence	guarantees	to	an	asympto;cally	
stable	point.	

good	genera;ve	model?	

handwriven	characters	

good	genera;ve	model?	

handwriven	characters	

good	genera;ve	model?	

handwriven	characters	

simulated	

real	data	

good	genera;ve	model?	

handwriven	characters	

real	data	

simulated	

good	genera;ve	model?	

handwriven	characters	

good	genera;ve	model?	

mnist	handwriven	digit	dataset	

handwri;ng	recogni;on	

mnist	dataset	

60,000	examples	of	10	digits	
learning	algorithm	
logis;c	regression	
id92		
neural	net	(plav	2005)	
id166	(decoste	et.al.	2002)	
deep	autoencoder	
(bengio	et.	al.	2007)		
deep	belief	net	
(hinton	et.	al.	2006)		
dbm		

error	
12.0%	
3.09%	
1.53%	
1.40%	
1.40%	

0.95%	

1.20%	

op;cal	character	recogni;on	

42,152	examples	of	26	english	levers		

learning	algorithm	
logis;c	regression	
id92		
neural	net	
id166	(larochelle	et.al.	2009)	
deep	autoencoder	
(bengio	et.	al.	2007)		
deep	belief	net	
(larochelle	et.	al.	2009)		
dbm	

error	
22.14%	
18.92%	
14.62%	
9.70%	
10.05%	

9.68%	

8.40%	

permuta;on-invariant	version.	

genera;ve	model	of	3-d	objects	

24,000	examples,	5	object	categories,	5	di   erent	objects	within	each		
category,	6	lightning	condi;ons,	9	eleva;ons,	18	azimuths.			

3-d	object	recogni;on	

pavern	comple;on	

learning	algorithm	
logis;c	regression	
id92	(lecun	2004)	
id166	(bengio	&	lecun		2007)	
deep	belief	net	(nair	&	
hinton		2009)		
dbm	

error	
22.5%	
18.92%	
11.6%	
9.0%	

7.2%	

permuta;on-invariant	version.	

where	else	can	we	use	
genera;ve	models?	

data	   	collec;on	of	modali;es	

car,	
automobile	

sunset,	
paci   cocean,	
bakerbeach,	
seashore,	ocean	

      	mul;media	content	on	the	web	-	
image	+	text	+	audio.	
      	product	recommenda;on	
systems.	

      	robo;cs	applica;ons.	

touch	sensors	

motor	control	

vision	

audio	

challenges	-	i		

image	

text	

sunset,	paci   c	ocean,	
baker	beach,	seashore,	

ocean	

dense	

sparse	

very	di   erent	input	
representa;ons	
      	images	   	real-valued,	dense	
      	text	   	discrete,	sparse		

di   cult	to	learn	
cross-modal	features	
from	low-level	
representa;ons.	

challenges	-	ii		

image	

text	

noisy	and	missing	data	

pentax,	k10d,	
pentaxda50200,	
kangarooisland,	sa,	
australiansealion	

mickikrimmel,	
mickipedia,	
headshot	

<	no	text>	

unseulpixel,	
naturey	

challenges	-	ii		

image	

text	

text	generated	by	the	model	

pentax,	k10d,	
pentaxda50200,	
kangarooisland,	sa,	
australiansealion	

mickikrimmel,	
mickipedia,	
headshot	

<	no	text>	

unseulpixel,	
naturey	

beach,	sea,	surf,	strand,	
shore,	wave,	seascape,	
sand,	ocean,	waves	

portrait,	girl,	woman,	lady,	
blonde,	prevy,	gorgeous,	
expression,	model	

night,	nove,	tra   c,	light,	
lights,	parking,	darkness,	
lowlight,	nacht,	glow	

fall,	autumn,	trees,	leaves,	
foliage,	forest,	woods,	
branches,	path	

a	simple	mul;modal	model	

      	use	a	joint	binary	hidden	layer.	
      	problem:		inputs	have	very	di   erent	sta;s;cal	
proper;es.	
      	di   cult	to	learn	cross-modal	features.	

real-valued	

0	
0	
0	
1	
0	

1-of-k	

mul;modal	dbm	

gaussian	model	

dense,	real-valued	
image	features	

replicated	soumax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)

mul;modal	dbm	

gaussian	model	

dense,	real-valued	
image	features	

replicated	soumax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)

mul;modal	dbm	

gaussian	model	

dense,	real-valued	
image	features	

replicated	soumax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)

mul;modal	dbm	

bovom-up	

+	

top-down	

gaussian	model	

dense,	real-valued	
image	features	

replicated	soumax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)

text	generated	from	images	

given

		

given

generated 	
dog,	cat,	pet,	kiven,	
puppy,	ginger,	tongue,	
kivy,	dogs,	furry	

sea,	france,	boat,	mer,	
beach,	river,	bretagne,	
plage,	brivany	

portrait,	child,	kid,	
ritravo,	kids,	children,	
boy,	cute,	boys,	italy	

generated 	

		
insect,	buver   y,	insects,	
bug,	buver   ies,	
lepidoptera	

gra   ;,	streetart,	stencil,	
s;cker,	urbanart,	gra   ,	
sanfrancisco	

canada,	nature,	
sunrise,	ontario,	fog,	
mist,	bc,	morning	

text	generated	from	images	

given

generated 	

		

portrait,	women,	army,	soldier,	
mother,	postcard,	soldiers	

obama,	barackobama,	elec;on,	
poli;cs,	president,	hope,	change,	
sanfrancisco,	conven;on,	rally	

water,	glass,	beer,	bovle,	
drink,	wine,	bubbles,	splash,	
drops,	drop	

genera;ng	text	from	images	

samples	drawn	auer	
every	50	steps	of	
gibbs	updates	

mir-flickr	dataset	
      	1	million	images	along	with	user-assigned	tags.	

sculpture,	beauty,	
stone	

d80	

nikon,	abigfave,	
goldstaraward,	d80,	
nikond80	

food,	cupcake,	
vegan	

anawesomeshot,	
theperfectphotographer,	
   ash,	damniwishidtakenthat,	
spiritofphotography	

nikon,	green,	light,	
photoshop,	apple,	d70	

white,	yellow,	
abstract,	lines,	bus,	
graphic	

sky,	geotagged,	
re   ec;on,	cielo,	
bilbao,	re   ejo	

(huiskes et al., 2010)

results	

      	logis;c	regression	on	top-level	representa;on.	
      	mul;modal	inputs	

mean	average	precision	

learning	algorithm	
random	
lda	[huiskes	et.	al.]	
id166	[huiskes	et.	al.]	
dbm-labelled	
deep	belief	net	
autoencoder	
dbm	

map	
0.124	
0.492	
0.475	
0.526	
0.638	
0.638	
0.641	

precision@50	

0.124	
0.754	
0.758	
0.791	
0.867	
0.875	
0.873	

labeled	
25k	
examples	

+	1	million	
unlabelled	

talk	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera;ve	models	

      restricted	boltzmann	machines	
      deep	belief	network,	deep	boltzmann	machines	
      helmholtz	machines	/	varia;onal	autoencoders		

      	genera;ve	adversarial	networks		
      	model	evalua;on	

helmholtz	machines	

      	hinton,	g.	e.,	dayan,	p.,	frey,	b.	j.	and	neal,	r.,	science	1995	

      	kingma	&	welling,	2014	
      	rezende,	mohamed,	daan,	2014	
      	mnih	&	gregor,	2014		
      	bornschein	&	bengio,	2015	
      	tang	&	salakhutdinov,	2013			

genera;ve	
process	

w3

w2

w1

approximate	
id136	
h3

h2

h1

v

input	data	

helmholtz	machines,	dbns,	dbms	
helmholtz 
deep boltzmann 
machine
machine

deep belief 
network

h3

h2

h1

v

w3

w2

w1

h3

h2

h1

v

w3

w2

w1

h3

h2

h1

v

w3

w2

w1

varia;onal	autoencoders	(vaes)		
      	the	vae	de   nes	a	genera;ve	process	in	terms	of	ancestral	
sampling	through	a	cascade	of	hidden	stochas;c	layers:		

genera;ve	
process	

each	term	may	denote	a	
complicated	nonlinear	rela;onship		

h3

h2

h1

v

w3

w2

w1

input	data	

      

      

					denotes	parameters	
of	vae.		

				is	the	number	of	
stochas/c	layers.	

       sampling	and	id203	
evalua;on	is	tractable	for	
each																						.		

vae:	example	

      	the	vae	de   nes	a	genera;ve	process	in	terms	of	ancestral	
sampling	through	a	cascade	of	hidden	stochas;c	layers:		

stochas;c	layer	

determinis;c	
layer	

stochas;c	layer	

this	term	denotes	a	one-layer	
neural	net.	

      

      

					denotes	parameters	
of	vae.		

				is	the	number	of	
stochas/c	layers.	

       sampling	and	id203	
evalua;on	is	tractable	for	
each																						.		

varia;onal	bound	

      	the	vae	is	trained	to	maximize	the	varia;onal	lower	bound:	

       trading	o   	the	data	log-likelihood	and	the	kl	divergence	

from	the	true	posterior.		

h3

h2

h1

v

w3

w2

w1

input	data	

       hard	to	op;mize	the	varia;onal	bound	
with	respect	to	the	recogni;on	network	
(high-variance).		
       key	idea	of	kingma	and	welling	is	to	use	

reparameteriza;on	trick.		

reparameteriza;on	trick	
      	assume	that	the	recogni;on	distribu;on	is	gaussian:	

				with	mean	and	covariance	computed	from	the	state	of	the	hidden	

units	at	the	previous	layer.		

       alterna;vely,	we	can	express	this	in	term	of	auxiliary	variable:			

reparameteriza;on	trick	
      	assume	that	the	recogni;on	distribu;on	is	gaussian:	

       or	

       the	recogni;on	distribu;on																										can	be	expressed	in	

terms	of	a	determinis;c	mapping:				

determinis;c	
encoder	

distribu;on	of			
does	not	depend	on	

compu;ng	the	gradients	

       the	gradient	w.r.t	the	parameters:	both	recogni;on	and	

genera;ve:	

autoencoder	

gradients	can	be	
computed	by	backprop	

the	mapping	h	is	a	determinis;c	
neural	net	for	   xed				.		

importance	weighted	autoencoders	
       can	improve	vae	by	using	following	k-sample	importance	

weigh;ng	of	the	log-likelihood:		

h3

h2

h1

v

w3

w2

w1

input	data	

unnormalized	
importance	weights		

				where																								are	sampled	

from	the	recogni;on	network.	

 (burda, grosse, salakhutdinov, iclr 2016)

importance	weighted	autoencoders	
       can	improve	vae	by	using	following	k-sample	importance	

weigh;ng	of	the	log-likelihood:		

       this	is	a	lower	bound	on	the	marginal	log-likelihood:	

       special	case	of	k=1:	same	as	standard	vae	objec;ve.		
       using	more	samples	  	improves	the	;ghtness	of	the	bound.	

tighter	lower	bound	

       using	more	samples	can	only	improve	the	;ghtness	of	the	

bound.	

       for	all	k,	the	lower	bounds	sa;sfy:	

       moreover	if																													is	bounded,	then:	

compu;ng	the	gradients	

       we	can	use	the	unbiased	es;mate	of	the	gradient	using	

reparameteriza;on	trick:	

where	we	de   ne	normalized	importance	weights:	

iwaes	vs.	vaes	

       draw	k-samples	form	the	recogni;on	network			

-    or	k-sets	of	auxiliary	variables				.							

       obtain	the	following	monte	carlo	es;mate	of	the	gradient:	

       compare	this	to	the	vae   s	es;mate	of	the	gradient:		

mo;va;ng	example	

      	can	we	generate	images	from	natural	language	descrip;ons?	

a	stop	sign	is	   ying	in	
blue	skies		

a	pale	yellow	school	bus	
is	   ying	in	blue	skies		

a	herd	of	elephants	is	
   ying	in	blue	skies		

a	large	commercial	airplane	
is	   ying	in	blue	skies		

(mansimov, parisotto, ba, salakhutdinov, 2015) 

genera;ng	images	from	cap;ons	

stochas;c	
layer	

      	genera;ve	model:	stochas;c	recurrent	network,	chained	
sequence	of	varia;onal	autoencoders,	with	a	single	stochas;c	layer.	
      	recogni;on	model:	determinis;c	recurrent	network.	

(gregor et al., 2015) 

flipping	colors	

a	yellow	school	bus	parked	
in	the	parking	lot	

a	red	school	bus	parked	in	
the	parking	lot	

a	green	school	bus	parked	in	
the	parking	lot	

a	blue	school	bus	parked	in	
the	parking	lot	

novel	scene	composi;ons	

a	toilet	seat	sits	open	in	the	
bathroom	

a	toilet	seat	sits	open	in	the	
grass	   eld	

ask	google?	

bloomberg	news	

