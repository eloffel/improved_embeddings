deep	learning	iv	

model	evalua4on	and	open	

ques4ons	

russ	salakhutdinov	
machine learning department 
carnegie mellon university

canadian institute of advanced research

talk	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera4ve	models	

      restricted	boltzmann	machines	
      deep	belief	network,	deep	boltzmann	machines	
      helmholtz	machines	/	varia4onal	autoencoders		

      	genera4ve	adversarial	networks		
      	model	evalua4on	

genera4ve	adversarial	networks	
      	there	is	no	explicit	de   ni4on	of	the	density	for	p(x)	   	only	need	to	
be	able	to	sample	from	it.	
      	no	varia4onal	learning,	no	maximum-likelihood	es4ma4on,	no	
mcmc.	how?	

      	by	playing	a	game!	

genera4ve	adversarial	networks	
      	set	up	a	game	between	two	players:	
     
      generator	g	

discriminator	d		

      	discriminator	d	tries	to	discriminate	between:		
     
     

a	sample	from	the	data	distribu4on.		
and	a	sample	from	the	generator	g.	

      	the	generator	g	awempts	to	   fool   	d	by	genera4ng	samples	that	
are	hard	for	d	to	dis4nguish	from	the	real	data.	

(id3, goodfellow et al., nips 2014)

genera4ve	adversarial	networks	

slide credit: ian goodfellow

genera4ve	adversarial	networks	

slide credit: ian goodfellow

genera4ve	adversarial	networks	

slide credit: ian goodfellow

log z(   ) =

d   i

z(   )

genera4ve	adversarial	networks	
p(x, h) = p(x | h(1))p(h(1) | h(2)) . . . p(h(l 1) | h(l))p(h(l))
      	minimax	value	func4on	

generator:	generate	samples	
that	d	would	classify	as	real	

in other words, d and g play the following two-player minimax game with value function v (g, d):

min
g

max

d

v (d, g) = ex   pdata(x)[log d(x)] + ez   pz(z)[log(1   d(g(z)))].

p(x)

log p(x) =

d
d   i

d
d   i

p(x)

p(x | h)p(h)

generator:	
pushes	down	

discriminator:	
pushes	up	

discriminator:	classify	
data	as	being	real			

p(x) =xh

discriminator:	classify	
generator	samples	as	
being	fake		

in the next section, we present a theoretical analysis of adversarial nets, essentially showing that
the training criterion allows one to recover the data generating distribution as g and d are given
enough capacity, i.e., in the non-parametric limit. see figure 1 for a less formal, more pedagogical
explanation of the approach. in practice, we must implement the game using an iterative, numerical
approach. optimizing d to completion in the inner loop of training is computationally prohibitive,
and on    nite datasets would result in over   tting. instead, we alternate between k steps of optimizing
d and one step of optimizing g. this results in d being maintained near its optimal solution, so
long as g changes slowly enough. this strategy is analogous to the way that sml/pcd [31, 29]
training maintains samples from a markov chain from one learning step to the next in order to avoid
burning in a markov chain as part of the inner loop of learning. the procedure is formally presented
in algorithm 1.
in practice, equation 1 may not provide suf   cient gradient for g to learn well. early in learning,
when g is poor, d can reject samples with high con   dence because they are clearly different from

      	op4mal	strategy	for	discriminator	is:	

pdata(x) + pmodel(x)

pdata(x)

d(x) =

dcgan	architecture	

(radford, metz, chintalaet, 2015)

lsun	bedrooms:	samples	

(radford, metz, chintalaet, 2015)

cifar		

training	

samples	

(salimans	et.	al.,	2016)	

id163	

training	

samples	

(salimans	et.	al.,	2016)	

id163:	cherry-picked	results	

      	open	ques4on:	how	can	we	quan4ta4vely	evaluate	these	models!		

slide	credit:	ian	goodfellow	

talk	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera4ve	models	

      restricted	boltzmann	machines	
      deep	belief	network,	deep	boltzmann	machines	
      helmholtz	machines	/	varia4onal	autoencoders		

      	genera4ve	adversarial	networks		
      	model	evalua4on	

markov	random	fields	
graphical	models:	powerful	framework	for	represen4ng	
dependency	structure	between	random	variables.	

par44on	func4on:	di   cult	to	
compute	

       goal:	obtain	good	es4mates	of												.			

restricted	boltzmann	machines	

		hidden	variables	

bipar4te		
structure	

stochas4c	binary	visible	variables	
are	connected	to	stochas4c	binary	hidden	
variables

								.		

	

the	energy	of	the	joint	con   gura4on:		

image						visible	variables	

id203	of	the	joint	con   gura4on	is	given	by	the	boltzmann	
distribu4on:	

model	parameters.	

markov	random	   elds,	boltzmann	machines,	log-linear	models.	

tractable	

intractable	

genera4ve	model	

      	which	model	is	a	bewer	genera4ve	model?	

model	a	

model	b	

model	selec4on	

      	more	generally,	how	can	we	choose	between	models?	

rbm	samples	

mixture	of	bernoulli   s	

compare													on	the	valida4on	set:		

need	an	es4mate	of	par44on	func4on		

model	selec4on	

      	more	generally,	how	can	we	choose	between	models?	

rbm	samples	
mob,	test	log-id203:
rbm,	test	log-id203:

mixture	of	bernoulli   s	

	
	

	-137.64	nats/digit		
			-86.35	nats/digit		

di   erence	of	about	50	nats!	

simple	importance	sampling	
      	two	distribu4ons	de   ned	on							with	id203	distribu4on	
func4ons																																													and	

proposal,	easy	to	sample	
from	distribu4on		
      	under	mild	condi4ons:	

intractable,	target	
distribu4on	

      	get	unbiased	es4mate	of	using	monte	carlo	approxima4on:		

      	in	high-dimensional	spaces,	the	variance	will	be	high	(or	in   nite).	

annealed	importance	sampling	

      	consider	a	sequence	of	intermediate	distribu4ons:	
																																with																							and																						.	

      	one	general	way	is	to	use	geometric	averages:	

with																																															chosen	by	the	user.		

      	if												is	the	uniform	distribu4on,	then:	

inverse	temperature	
annealing by averaging moments, 
grosse et al., nips, 2013

hence	the	term	annealing.		

(neal, statistics and computing, 2001 )

f (x) = f1(x) ,

which explains the term    annealing   
think of   as an inverse temperature

      	move	gradually	from	hower	distribu4on	to	colder	distribu4on:		

annealed	importance	sampling	

      	need	to	de   ne	transi4on	operator																						that	leaves					
roger grosse (joint work with chris maddison, ruslan salakhutdinov)
annealing between distributions by averaging moments
invariant	(e.g.	gibbs	sampling)	   	easy	to	implement!	

may 22, 2013

rbms	with	geometric	averages	

      	restricted	boltzmann	machines	trained	on	mnist.		

samples	from	target	
distribu4on		

ais	with	geometric	
averages	

problems	with	undirected	models	
      	ais	provides	an	unbiased	es4mator:																												.	in	general,	
we	are	interested	in	es4ma4ng		
      	by	jensen   s	inequality:		

      	by	markov   s	inequality:	very	unlikely	to	overes4mate		

      	compute	log-id203	on	the	test	set:		

stochas4c	lower	
bound!	

overes4mate	

underes4mate	

mo4va4on:	rbm	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

mo4va4on:	rbm	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

random	

mo4va4on:	rbm	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

random	

mo4va4on:	rbm	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

random	

v	

mo4va4on:	rbm	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

random	

v	

mo4va4on:	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

   	

random	

v	

t=	in   nity	

1	gibbs	step:	transi4on	operator	t.			

equilibrium	
distribu4on	

mo4va4on:	sampling	

run	markov	chain	(alterna4ng	gibbs	sampling):	

   	

random	

v	

t=	1000	

1	gibbs	step:	transi4on	operator	t.			

pretend:	
equilibrium	
distribu4on	

unrolled	rbm	as	a	deep	

	genera4ve	model	

random	(uniform)	

unrolled	rbm	as	a	deep	

	genera4ve	model	

random	(uniform)	

   	

unrolled	rbm	as	a	deep	

	genera4ve	model	

random	(uniform)	

   	

unrolled	rbm	as	a	deep	

	genera4ve	model	

random	(uniform)	

   	

unrolled	rbm	as	a	deep	

	genera4ve	model	

random	(uniform)	

   	

observed	data	

unrolled	rbm	as	a	deep	

	genera4ve	model	

random	(uniform)	

   	

      	if	we	use	in   nite	number	of	layers,	
then:	

      	otherwise,	deep	genera4ve	model	is	
just	an	approxima4on	to	an	rbm.		

observed	data	

(burda, grosse, salakhutdinov, aistats 2015)

reverse	ais	es4mator	(raise)	

   

      	let	us	consider																													where	v	is	
observed	and	h	is	unobserved.		
      	de   ne	the	following	genera4ve	
process	(sequence	of	ais	distribu2ons):	

      	genera4ve	model,	that	we	call	the	
annealing	model:	

(burda, grosse, salakhutdinov, aistats 2015)

reverse	ais	es4mator	(raise)	

      	as	k	goes	to	in   nity:	

   

      	we	would	like	to	es4mate																	.		
      	we	use	reverse	chain	as	our	proposal:	

assume	tractable,	which	is	
the	case	for	rbms	

observed	data	

      	can	be	easily	extended	to	non-tractable	
posteriors,	e.g.	dbms,	dbns.	

reverse	ais	es4mator	(raise)	
      	we	now	have	our	genera4ve	model	(theore4cal	construct):	

      	proposal	starts	at	the	data	and	melts	the	distribu4on:	

      	we	then	obtain:	

      	tends	to	underes4mate	rather	than	overes4mate	log-probs!		

mnist		

      	rbm	with	500	hidden	units	trained	on	mnist.		
      	ini4al	distribu4on	is	uniform:	ais	is	o   	by	20	nats!	

omniglot	dataset	
      		rbm	with	500	hidden	units	trained	on	omniglot.		

y
t
i
l
i

b
a
b
o
r
p
 
g
o
l
 
e
g
a
r
e
v
a
 
f
o
 
e
t
a
m

i
t
s
e

t
e
s
a
t
a
d
 
t
s
e
t
 
e
h
t
 
n
o

ais estimates

raise estimates

number of intermediate distributions

mnist	and	omniglot	results	

      	raise	errs	on	the	side	of	underes4ma4ng	the	log-likelihood.		
      	note	that	the	gap	is	very	small.	
      	csl:	conserva4ve	sampling-based	log-likelihood	(csl)	es4mator		
of	bengio	et.	al.			

bengio, yao, cho. bounding the test log- 
likelihood of generative models, 2013

dbms	and	dbns	

deep	boltzmann	machine		

deep	believe	network	

decoder-based	models	

      	decoder-based	models:	transform	samples	from	some	simple	
distribu4on	(e.g.	normal)	to	the	data	manifold:	

genera4ve	
process	

determinis4c	neural	network	

     

varia4onal	autoencoders	(vaes)	(knigma	
and	welling,	2014)		

      genera4ve	adversarial	networks	(gans)	

(goodfellow	et.al.,	2014)	

      genera4ve	moment	matching	networks	

(gmmns)	(li	&	swersky,	2015;	dziugaite	et	
al.,	2015)	

decoder-based	models	

      	decoder-based	models:	transform	samples	from	some	simple	
distribu4on	(e.g.	normal)	to	the	data	manifold:	

genera4ve	
process	

determinis4c	neural	network	

     

varia4onal	autoencoders	(vaes)	(knigma	
and	welling,	2014)		

      genera4ve	adversarial	networks	(gans)	

      genera4ve	moment	matching	networks	

(goodfellow	et.al.,	2014)	
ais can be used to properly 
evaluate decoder-based models
(wu, burda, salakhutdinov, grosse, 2016)

(gmmns)	(li	&	swersky,	2015;	dziugaite	et	
al.,	2015)	

talk	roadmap	

part	1:	supervised	learning:	deep	networks		

       de   ni4on	of	neural	networks	
       training	neural	networks		
       recent	op4miza4on	/	regulariza4on	techniques	

part	2:	unsupervised	learning:	learning	deep	
genera4ve	models	

part	3:	open	research	problems	

(some)	open	problems	

       unsupervised	learning	/	transfer	learning	/	

one-shot	learning	

       reasoning,	awen4on,	and	memory	

       natural	language	understanding	

       deep	reinforcement	learning		

(some)	open	problems	

       unsupervised	learning	/	transfer	learning	/	

one-shot	learning	

       reasoning,	awen4on,	and	memory	

       natural	language	understanding	

       deep	reinforcement	learning		

sequence	to	sequence	learning	
learned	
representa7on	

output	sequence	

encoder	

decoder	

input	sequence	

       id56	encoder-decoders	
for	machine	transla4on	
(sutskever	et	al.	2014;	
cho	et	al.	2014;	
kalchbrenner	et	al.	2013,	
srivastava	et.al.,	2015)	

skip-thought	model		

generate	previous	sentence	

encoder	

sentence	

generate	forward	sentence	

(kiros et al., nips 2015)

learning	objec4ve	

      	objec4ve:	the	sum	of	the	log-probabili4es	for	the	next	and	
previous	sentences	condi4oned	on	the	encoder	representa4on:	

representa4on	of	
encoder	

forward	sentence		

previous	sentence		

      	data:	book-11k	corpus:	

seman4c	relatedness		

semeval	
2014	sub-
missions	

results	
reported	
by	tai	et.al.	

ours	

      	our	models	outperform	all	previous	systems	from	the	semeval	
2014	compe44on.		

(kiros et al., nips 2015)

seman4c	relatedness		

recurrent	neural	network	
      	how	similar	the	two	sentences	are	on	the	scale	1	to	5?	

ground	truth	5.0			

	

		predic4on	4.9		

a	man	is	driving	a	car.	

a	car	is	being	driven	by	a	man.	

ground	truth	2.9			

	

		predic4on	3.5		

a	girl	is	looking	at	a	
woman	in	costume.	

a	girl	in	costume	looks	like	
a	woman.	

ground	truth	2.6			

	

		predic4on	4.4		

a	person	is	performing	
tricks	on	a	motorcycle	

the	performer	is	tricking	a	
person	on	a	motorcycle		

	paraphrase	detec4on	

       microsos	research	paraphrase	corpus:	for	two	sentences	one	

must	predict	whether	or	not	they	are	paraphrases.		

recursive	
auto-
encoders	

best	
published	
results	

ours	

       the	training	set	

contains	4076	sentence	
pairs	(2753	are	posi4ve)		

       the	test	set	contains	
1725	pairs	(1147	are	
posi4ve).	

neural	story	telling	

sample	from	the	genera7ve	model	
(recurrent	neural	network):	
she	was	in	love	with	him	for	the	   rst	
4me	in	months,	so	she	had	no	
inten4on	of	escaping.		

the	sun	had	risen	from	the	ocean,	making	her	feel	more	alive	
than	normal	.	she	is	beau4ful,	but	the	truth	is	that	i	do	not	
know	what	to	do.	the	sun	was	just	star4ng	to	fade	away,	
leaving	people	scawered	around	the	atlan4c	ocean.		

(kiros et al., nips 2015)

recurrent	neural	network		

nonlinearity		

hidden	state		at	
previous	4me	step	

input	at	4me	
step	t	

h1	

h2	

h3	

x1	

x2	

x3	

mul4plica4ve	integra4on	

      	replace	

      	with		

      	or	more	generally	

wu et al., nips 2016

   who	did	what   	dataset		

      	document:	japanese	prime	minister	taro	aso	said	on	friday	he	
would	call	for	stronger	monitoring	of	interna4onal	   nance	at	the	
g20	summit	next	week      	us	treasury	secretary	timothy	
geithner	has	said	president	barack	obama		would	discuss	new	
global	   nancial	regulatory	standards	at	the	london	summit.			

      	query:		us	president	barack	will	push	higher	   nancial	
regulatory	standards	for	across	the	globe	at	the	upcoming	g20	
summit	in	london	xxx	said	on	thursday		
      	answer:		timothy	geithner		

onishi, wang, bansal, gimpel, mcallester. 
who did what: a large-scale person-centered 
cloze dataset. emnlp, 2016.

represen4ng	document/query	

      	forward	id56	reads	sentences			
from	les	to	right:	

      	backward	id56	reads	sentences	
from	right	to	les:	

      	the	hidden	states	are	then	concatenated:	

      		use	grus	to	encode	a	document	and	a	query:	

gated	awen4on	(ga)	mechanism		
      	for	each	word	in	document	d,	we	form	a	token-speci   c	
representa4on	of	the	query	q:	

     

use	the	element-wise	mul4plica4on	
operator	to	model	the	interac4ons	
between								and	

(dhingra, liu, yang, cohen, salakhutdinov, 2016)

mul4-hop	architecture	

      	many	qa	tasks	require	reasoning	over	mul4ple	sentences.		
      	need	to	performs	several	passes	over	the	context.	

a   ect	of	mul4plica4ve	ga4ng		

      	performance	of	di   erent	ga4ng	func4ons	on	   who	did	
what   	(wdw)	dataset.	

(some)	open	problems	

       unsupervised	learning	/	transfer	learning	/	

one-shot	learning	

       reasoning	and	natural	language	

understanding	

       deep	reinforcement	learning		

one-shot	learning	
   zarc    

(lake, salakhutdinov, tenenbaum, science, 2015) 

one-shot	learning	
   segway    
   zarc    

how	can	we	learn	a	novel	concept	   	a	high	dimensional	
sta4s4cal	object	   	from	few	examples.			

(lake, salakhutdinov, tenenbaum, science, 2015) 

one-shot	learning:		
humans	vs.	machines	

reinforcement	learning	

      	can	a	single	network	play	
many	games	at	once?	

      	can	we	learn	new	games	
faster	by	using	knowledge	
about	the	previous	games?	

figure	credit:	nando	de	freitas	

(mnih et al., 2014, rusu et al., 2015, wang et al., 2015)

actor-mimic	net	in	ac4on	

      	the	mul4task	network	can	match	expert	performance	on	8	
games	(we	are	extending	this	to	more	games).	

(parisotto, ba, salakhutdinov, iclr 2016) 

transfer	learning	

      	can	the	network	learn	new	games	faster	by	leveraging	
knowledge	about	the	previous	games	it	learned.		

transfer	

no	transfer	

star	gunner	

       e   cient	learning	algorithms	for	deep	unsupervised	models	

summary	

text	&	image	retrieval	/		
object	recogni7on	

image	tagging	

learning	a	category	
hierarchy	

speech	recogni7on	

id48	decoder	

mosque,	tower,	
building,	cathedral,	
dome,	castle	

mul7modal	data	

object	detec7on	

sunset,	paci   c	ocean,	
beach,	seashore	

       deep	models	improve	the	current	state-of-the	art	in	many	

applica4on	domains:	
      object	recogni4on	and	detec4on,	text	and	image	retrieval,	handwriwen	

character	and	speech	recogni4on,	and	others.	

thank	you	

