succinct data structures for nlp-at-scale

matthias petri trevor cohn

computing and information systems
the university of melbourne, australia

first.last@unimelb.edu.au

november 20, 2016

who are we?

trevor cohn, university of melbourne

probabilistic machine learning for structured problems in
language: np bayes, deep learning, etc.
applications to machine translation, social media, parsing,
summarisation, multilingual transfer.

matthias petri, university of melbourne

data compression, succinct data structures, text
indexing, compressed text indexes, algorithmic
engineering, terabyte scale text processing
machine translation, information retrieval, bioinformatics

who are we?

tutorial based partly on research
[shareghi et al., 2015, shareghi et al., 2016b] with collaborators
at monash university:

ehsan shareghi

gholamreza ha   ari

outline

1 introduction and motivation (15 minutes)

2 basic technologies and notation (20 minutes)

3 index based pattern matching (20 minutes)

break (20 minutes)

4 pattern matching using compressed indexes (40 minutes)

5 applications to nlp (30 minutes)

what

why

who and where

introduction and motivation (15 mins)

1 what

2 why

3 who and where

what

why

who and where

what is it?

id206 for working with large data
sets
desiderata

miminise space requirement
maintaining e   cient searchability

classes of compression do just this! near-optimal
compression, with minor e   ect on runtime
e.g., bitvector and integer compression, wavelet trees,
compressed su   x array, compressed su   x trees

what

why

who and where

why do we need it?

era of    big data   : text corpora are often 100s of gigabytes
or terabytes in size (e.g., commoncrawl, twitter)
even simple algorithms like counting id165s become
di   cult
one solution is to use distributed computing, however can
be very ine   cient
succinct data structures provide a compelling alternative,
providing compression and e   cient access
complex algorithms become possible in memory, rather
than requiring cluster and disk access

what

why

who and where

why do we need it?

era of    big data   : text corpora are often 100s of gigabytes
or terabytes in size (e.g., commoncrawl, twitter)
even simple algorithms like counting id165s become
di   cult
one solution is to use distributed computing, however can
be very ine   cient
succinct data structures provide a compelling alternative,
providing compression and e   cient access
complex algorithms become possible in memory, rather
than requiring cluster and disk access

e.g., in   nite order language model possible, with runtime similar
to current    xed order models, and lower space requirement.

what

why

who and where

who uses it and where is it used?

surprisingly few applications in nlp

bioinformatics, genome assembly
information retrieval, graph search (facebook)
search engine auto-complete
trajectory compression and retrieval
xml storage and retrieval (xpath queries)
geo-spartial databases
...

bitvectors

rank and select

succinct tree representations

variable size integers

basic technologies and notation (20 mins)

1 bitvectors

2 rank and select

3 succinct tree representations

4 variable size integers

bitvectors

rank and select

succinct tree representations

variable size integers

basic building blocks: the bitvector

de   nition
a bitvector (or bit array) b of length n compactly stores n
binary numbers using n bits.

example

0
1

1
1

2
0

3
0

4
1

5
1

6
0

7
1

8
0

9 10 11
1
0

1

b

b[0] = 1, b[1] = 1, b[2] = 0, b[n     1] = b[11] = 0 etc.

bitvectors

rank and select

succinct tree representations

variable size integers

bitvector operations

access and set
b[0] = 1, b[0] = b[1]

logical operations

a or b, a and b, a xor b

advanced operations

popcount(b): number of one bits set
msb set(b): most signi   cant bit set
lsb set(b): least signi   cant bit set

bitvectors

rank and select

succinct tree representations

variable size integers

operation rank

de   nitions
rank1(b, j): how many 1   s are in b[0, j]

rank0(b, j): how many 0   s are in b[0, j]

example

0
1

1
1

2
0

3
0

4
1

5
1

6
0

7
1

8
0

9 10 11
1
0

1

b

rank1(b, 7) = 5
rank0(b, 7) = 8     rank1(b, 7) = 3

bitvectors

rank and select

succinct tree representations

variable size integers

operation select

de   nitions
select1(b, j): where is the j-th (start count at 0) 1 in b

select0(b, j): where is the j-th (start count at 0) 0 in b

inverse of rank: rank1(b, select1(b, j)) = j

example

0
1

1
1

2
0

3
0

4
1

5
1

6
0

7
1

8
0

9 10 11
1
0

1

b

select1(b, 4) = 7
select0(b, 3) = 8

bitvectors

rank and select

succinct tree representations

variable size integers

complexity of operations rank and select

simple and slow

scan the whole bitvector using o(1) extra space and o(n) time
to answer both rank and select

constant time rank
periodically store the absolute count up till that position
explicitly. only scan a small part of the bitvector to get the right
answer. space usage: n + o(n) bits. runtime: o(1). in
practice: 25% extra space.

constant time select
similar to rank but more complex as blocks are based on the
number of 1/0 observed

bitvectors

rank and select

succinct tree representations

variable size integers

compressed bitvectors

idea
if only few 1   s or id91 present in the bitvector, we can use
compression techniques to substantially reduce space usage while
e   ciently supporting operations rank and select

in practice
bitvector of size 1 gib with 10% of all bits randomly set to 1:
encodings:

elias-fano [   73]: x mib
rrr [   02]: y mib

bitvectors

rank and select

succinct tree representations

variable size integers

bitvectors - practical performance

how fast are rank and select in practice? experiment: cost
per operation averaged over 1m executions: (code)
uncompressed:

bv size access rank select space

1mb
10mb
1gb
10gb

3ns
10ns
26ns
78ns

4ns
14ns
36ns
98ns

47ns
85ns
303ns
372ns

127%
126%
126%
126%

compressed:

bv size access

rank select space

1mb
10mb
1gb
10gb

68ns
99ns
292ns
466ns

65ns
88ns
275ns
424ns

49ns
58ns
219ns
336ns

33%
30%
32%
30%

bitvectors

rank and select

succinct tree representations

variable size integers

using rank and select

basic building block of many compressed / succinct data
structures
di   erent implementations provide a variety of time and
space trade-o   s
implemented an ready to use in sdsl and many others:

http://github.com/simongog/sdsl-lite
http://github.com/facebook/folly
http://sux.di.unimi.it
http://github.com/ot/succinct

used in practice! for example: facebook graph search
(unicorn)

bitvectors

rank and select

succinct tree representations

variable size integers

succinct tree representations

idea
instead of storing pointers and objects,    atten the tree structure
into a bitvector and use rank and select to navigate

from

t y p e d e f

s t r u c t {
v o i d     d a t a ;
// 64 b i t s
n o d e t     l e f t ;
// 64 b i t s
n o d e t     r i g h t ;
// 64 b i t s
n o d e t     p a r e n t ; // 64 b i t s

} n o d e t ;

to
bitvector + rank + select + data (    2 bits per node)

bitvectors

rank and select

succinct tree representations

variable size integers

succinct tree representations

de   nition: succinct data structure
a succinct data structure uses space    close    to the information
theoretical lower bound, but still supports operations
time-e   ciently.

succinct tree representations:

there number of unique binary trees containing n nodes is
(roughly) 4n. to di   erentiate between them we need at least
log2(4n) = 2n bits. thus, a succinct tree representations should
require 2n bits (plus a bit more).

bitvectors

rank and select

succinct tree representations

variable size integers

louds level order unary degree sequence

louds
a succinct representation of a rooted, ordered tree containing
nodes with arbitrary degree [jacobson   89]

example:

bitvectors

rank and select

succinct tree representations

variable size integers

louds step 1

add pseudo root:

bitvectors

rank and select

succinct tree representations

variable size integers

louds step 2

for each node unary encode the number of children:

bitvectors

rank and select

succinct tree representations

variable size integers

louds step 3

write out unary encodings in level order:

louds sequence l = 0100010011010101111

bitvectors

rank and select

succinct tree representations

variable size integers

louds nodes

each node (except the pseudo root) is represented twice

once as    0    in the child list of its parent
once as the terminal (   1   ) in its child list

represent node v by the index of its corresponding    0   
i.e. root corresponds to    0   
a total of 2n bits are used to represent the tree shape!

bitvectors

rank and select

succinct tree representations

variable size integers

louds navigation

use rank and select to navigate the tree in constant time

examples:

compute node degree
i n t n o d e d e g r e e ( i n t v ) {
r e t u r n 0

i f
i d = rank0(l, v)
r e t u r n select1(l, id + 2)
   select1(l, id + 1)     1

l e a f ( v )

i s

}

return the i-th child of node v

i n t

i f

c h i l d ( i n t v , i ) {
i > n o d e d e g r e e ( v )
r e t u r n    1

i d = rank0(l, v)
r e t u r n select1(l, id + 1) + i

}

complete construction, load, storage and navigation code of
louds is only 200 lines of c++ code.

bitvectors

rank and select

succinct tree representations

variable size integers

variable size integers

using 32 or 64 bit integers to store mostly small numbers is
wasteful
many e   cient encoding schemes exist to reduce space
usage

bitvectors

rank and select

succinct tree representations

variable size integers

variable byte compression

idea
use variable number of bytes to represent integers. each byte
contains 7 bits    payload    and one continuation bit.

examples

storage cost

number

encoding

824
5

00000110
10000101

10111000

number range
0     127
128     16383
16384     2097151

number of bytes

1
2
3

bitvectors

rank and select

succinct tree representations

variable size integers

variable byte compression - algorithm

encoding

decoding

1: function encode(x)
while x >= 128 do
2:
3:
4:
5:
6:
7: end function

end while
write(x + 128)

write(x mod 128)
x = x    128

x = 0
y =readbyte(bytes)
while y < 128 do

1: function decode(bytes)
2:
3:
4:
5:
6:

x = 128    x + y

y =readbyte(bytes)

end while
x = 128    x + (y     128)
return x

7:
8:
9:
10: end function

bitvectors

rank and select

succinct tree representations

variable size integers

variable sized integer sequences

problem
sequences of vbyte encoded numbers can not be accessed at
arbitrary positions

solution: directly addressable variable-length codes (dac)
separate the indicator bits into a bitvector and use rank and
select to access integers in o(1) time. [brisboa et al.   09]

bitvectors

rank and select

succinct tree representations

variable size integers

dac - concept

sample vbyte encoded sequence of integers:

01010101 11110111 11000111 00110110 01110110 10000100 11101011 10000110 01101011 10000001 10000000 10001000

dac restructuring of the vbyte encoded sequence of integers:

01010101 11000111 00110110 11101011 10000110 01101011 10000000 10001000

11110111 01110110 10000001

10000100

separate the indicator bits:

01011011

1010101 1000111 0110110 1101011 0000110 1101011 0000000 0001000

101

1

1110111 1110110 0000001

0000100

bitvectors

rank and select

succinct tree representations

variable size integers

dac - access

01011011

1010101 1000111 0110110 1101011 0000110 1101011 0000000 0001000

101

1

1110111 1110110 0000001

0000100

accessing element a[5]:

access indicator bit of the    rst level at position 5: i1[5] = 0
0 in the indicator bit implies the number uses at least 2
bytes
perform rank0(i1, 5) = 3 to determine the number of
integers in a[0, 5] with at least two bytes
access i2[3     1] = 1 to determine that number a[5] has
two bytes.
access payloads and recover number in o(1) time.

bitvectors

rank and select

succinct tree representations

variable size integers

practical exercise

su   x trees

su   x arrays

compressed su   x arrays

index based pattern matching (20 mins)

5 su   x trees

6 su   x arrays

7 compressed su   x arrays

su   x trees

su   x arrays

compressed su   x arrays

pattern matching

de   nition
given a text t of size n,    nd all occurrences (or just count) of
pattern p of length m.

online pattern matching

preprocess p , scan t . examples: kmp, boyer-moore, bmh
etc. o(n + m) search time.

o   ine pattern matching

preprocess t , build index. examples: inverted index, su   x
tree, su   x array. o(m) search time.

su   x trees

su   x arrays

compressed su   x arrays

su   x tree (weiner   73)

data structure capable of processing t in o(n) time and
answering search queries in o(n) space and o(m) time.
optimal from a theoretical perspective.
all su   xes of t into a trie (a tree with edge labels)
contains n leaf nodes corresponding to the n su   xes of t
search for a pattern p is performed by    nding the subtree
corresponding to all su   xes pre   xed by p

su   x trees

su   x arrays

compressed su   x arrays

su   x tree - example

t =abracadabracarab$

su   x trees

su   x arrays

compressed su   x arrays

su   x tree - example

t =abracadabracarab$

su   xes:

0
1
2
3
4
5
6
7
8

abracadabracarab$
bracadabracarab$
racadabracarab$
acadabracarab$
cadabracarab$
adabracarab$
dabracarab$
abracarab$
bracarab$

9
10
11
12
13
14
15
16

racarab$
acarab$
carab$
arab$
rab$
ab$
b$
$

su   x trees

su   x arrays

compressed su   x arrays

su   x tree - example

$

a

b

rab$

c
a

d..$

r

a

c

a

$

ca

d..$

ra

6

d..$ r

a

b

$

5

12

15

4

11

d..$ r

.

.

$

d..$ r

a

b

$

c

a

b$

13

d..$ r

a

b

$

3

10

1

8

2

9

16

b

r

a

c

a

$

14

d..$ c

.

.

$

0

7

su   x trees

su   x arrays

compressed su   x arrays

su   x tree - search for    aca   

$

a

b

rab$

c
a

d..$

r

a

c

a

$

ca

d..$

ra

6

d..$ r

a

b

$

5

12

15

4

11

d..$ r

.

.

$

d..$ r

a

b

$

c

a

b$

13

d..$ r

a

b

$

3

10

1

8

2

9

16

b

r

a

c

a

$

14

d..$ c

.

.

$

0

7

su   x trees

su   x arrays

compressed su   x arrays

su   x tree - problems

space usage in practice is large. 20     40 times n for highly
optimized implementations.
only useable for small datasets.

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays (manber   89)

reduce space of su   x tree by only storing the n leaf
pointers into the text
requires n log n bits for the pointers plus t to perform
search
in practice 5     9n bytes for character alphabets
search for p using binary search

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - example

t =abracadabracarab$

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - example

t =abracadabracarab$

su   xes:

0
1
2
3
4
5
6
7
8

abracadabracarab$
bracadabracarab$
racadabracarab$
acadabracarab$
cadabracarab$
adabracarab$
dabracarab$
abracarab$
bracarab$

9
10
11
12
13
14
15
16

racarab$
acarab$
carab$
arab$
rab$
ab$
b$
$

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - example

t =abracadabracarab$

sorted su   xes:

16
14
0
7
3
10
5
12

$
ab$
abracadabracarab$
abracarab$
acadabracarab$
acarab$
adabracarab$
arab$

15
1
8
4
11
6
13
2
9

b$
bracadabracarab$
bracarab$
cadabracarab$
carab$
dabracarab$
rab$
racadabracarab$
racarab$

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - example

t =abracadabracarab$

0

a

1

b

2

r

0

1

2

16 14 0

3

a

3

7

4

c

5

a

6

d

7

a

8

b

9 10 11 12 13 14 15 16

r

a

c

a

r

a

b

$

4

5

6

7

8

9 10 11 12 13 14 15 16

3 10 5 12 15 1

8

4 11 6 13 2

9

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - search

t =abracadabracarab$, p =abr

0

a

1

b

2

r

0

1

2

16 14 0

3

a

3

7

4

c

5

a

6

d

7

a

8

b

9 10 11 12 13 14 15 16

r

a

c

a

r

a

b

b

4

5

6

7

8

9 10 11 12 13 14 15 16

3 10 5 12 15 1

8

4 11 6 13 2

9

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - search

t =abracadabracarab$, p =abr

0

a

1

b

2

r

0

1

2

16 14 0

3

a

3

7

4

c

5

a

6

d

7

a

8

b

9 10 11 12 13 14 15 16

r

a

c

a

r

a

b

$

4

5

6

7

8

9 10 11 12 13 14 15 16

3 10 5 12 15 1

8

4 11 6 13 2

9

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - search

t =abracadabracarab$, p =abr

0

a

1

b

2

r

0

1

2

16 14 0

3

a

3

7

4

c

5

a

6

d

7

a

8

b

9 10 11 12 13 14 15 16

r

a

c

a

r

a

b

b

4

5

6

7

8

9 10 11 12 13 14 15 16

3 10 5 12 15 1

8

4 11 6 13 2

9

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - search

t =abracadabracarab$, p =abr

0

a

1

b

2

r

0

1

2

16 14 0

3

a

3

7

4

c

5

a

6

d

7

a

8

b

9 10 11 12 13 14 15 16

r

a

c

a

r

a

b

b

4

5

6

7

8

9 10 11 12 13 14 15 16

3 10 5 12 15 1

8

4 11 6 13 2

9

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays - search

t =abracadabracarab$,

0

a

1

b

2

r

3

a

4

c

5

a

6

d

7

a

8

b

9 10 11 12 13 14 15 16

r

a

c

a

r

a

b

b

lb rb

0

1

2

16 14 0

3

7

4

5

6

7

8

9 10 11 12 13 14 15 16

3 10 5 12 15 1

8

4 11 6 13 2

9

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays / trees - resource consumption

in practice:

su   x trees requires     20n bytes of space (for e   cient
implementations)
su   x arrays require 5     9n bytes of space
comparable search performance

example: 5gb english text requires 45gb for a character level
su   x array index and up to 200gb for su   x trees

su   x trees

su   x arrays

compressed su   x arrays

su   x arrays / trees - construction

in theory: both can be constructed in optimal o(n) time

in practice:

su   x trees and su   x arrays construction can be
parallelized
most e   cient su   x array construction algorithm in practice
are note o(n)
e   cient semi-external memory construction algorithms exist
parallel su   x array construction algorithms can index
20mib/s (24 threads) in-memory and 4mib/s in external
memory
su   x arrays of terabyte scale text collection can be
constructed. practical!
word-level su   x array construction also possible.

su   x trees

su   x arrays

compressed su   x arrays

dilemma

there is lots of work out there which proposes solutions for
di   erent problems based on su   x trees
su   x trees (and to a certain extend su   x arrays) are not
really applicable for large scale problems
however, large scale su   x arrays can be constructed
e   ciently without requiring large amounts of memory

solutions:

external or semi-external memory representation of su   x
trees / arrays

su   x trees

su   x arrays

compressed su   x arrays

dilemma

there is lots of work out there which proposes solutions for
di   erent problems based on su   x trees
su   x trees (and to a certain extend su   x arrays) are not
really applicable for large scale problems
however, large scale su   x arrays can be constructed
e   ciently without requiring large amounts of memory

solutions:

external or semi-external memory representation of su   x
trees / arrays
compression?

su   x trees

su   x arrays

compressed su   x arrays

external / semi-external su   x indexes

string-b tree

cache-oblivious
complicated
not implemented anywhere (not practical?)

su   x trees

su   x arrays

compressed su   x arrays

compressed su   x arrays and trees

idea
utilize data compression techniques to substantially reduce the
space of su   x arrays/trees while retaining their functionality

compressed su   x arrays (csa):

use space equivalent to the compressed size of the input
text. not 4-8 times more! example: 1gb english text
compressed to roughly 300mb using gzip. csa uses
roughly 300mb (sometimes less)!
provide more functionality than regular su   x arrays
implicitly contain the original text, no need to retain it. not
needed for query processing
similar search e   ciency than regular su   x arrays.
used to index terabytes of data on a reasonably powerful
machine!

su   x trees

su   x arrays

compressed su   x arrays

csa and cst in practice using sdsl

i n t main ( i n t a r g c , char       a r g v ) {

1 #i n c l u d e     s d s l / s u f f i x a r r a y s . hpp    
2 #i n c l u d e <i o s t r e a m >
3
4
5
6
7
8
9
10
11
12

s t d : : s t r i n g i n p u t
s t d : : s t r i n g o u t
s d s l : : c s a w t <> c s a ;
s d s l : : c o n s t r u c t ( csa , i n p u t
s t d : : c o u t <<    csa s i z e =    

s d s l : : s t o r e t o f i l e ( csa , o u t

}

f i l e = a r g v [ 1 ] ;

f i l e = a r g v [ 2 ] ;

f i l e , 1 ) ;

f i l e ) ;

<< s d s l : : s i z e i n m e g a b y t e s ( c s a ) << s t d : : e n d l ;

how does it work? find out after the break!

su   x trees

su   x arrays

compressed su   x arrays

break time

see you back here in 20 minutes!

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

compressed indexes (40 mins)

1 csa internals

2 bwt

3 wavelet trees

4 csa usage

5 compressed su   x trees

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

compressed su   x arrays - overview

two practical approaches developed independently:

csa-sada: proposed by grossi and vitter in 2000.
practical re   nements by sadakane also in 2000.
csa-wt: also referred to as the fm-index. proposed by
ferragina and manzini in 2000.

many practical (and theoretical) improvements to compression,
query speed since then. e   cient implementations available in
sdsl: csa sada<> and csa wt<>.

for now, we focus on csa-wt.

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt or the fm-index

utilizes the burrows-wheeler transform (bwt) used in
compression tools such as bzip2

requires rank and select on non-binary alphabets

heavily utilize compressed bitvector representations

theoretical bound on space usage related to compressibility
(id178) of the input text

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

the burrows-wheeler transform (bwt)

reversible text permutation

initially proposed by burrows and wheeler as a compression
tool. the bwt is more compressible than the original text!

de   ned as bw t [i] = t [sa[i]     1 mod n]

in words: bw t [i] is the symbol preceding su   x sa[i] in t

why does it work? how is it related to searching?

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - example

t =abracadabracarab$

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - example

t =abracadabracarab$

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

abracadabracarab$
bracadabracarab$
racadabracarab$
acadabracarab$
cadabracarab$
adabracarab$
dabracarab$
abracarab$
bracarab$
racarab$
acarab$
carab$
arab$
rab$
ab$
b$
$

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - example

t =abracadabracarab$

su   x array

16
14
0
7
3
10
5
12
15
1
8
4
11
6
13
2
9

$
ab$
abracadabracarab$
abracarab$
acadabracarab$
acarab$
adabracarab$
arab$
b$
bracadabracarab$
bracarab$
cadabracarab$
carab$
dabracarab$
rab$
racadabracarab$
racarab$

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - example

t =abracadabracarab$

su   x array

bwt

16
14
0
7
3
10
5
12
15
1
8
4
11
6
13
2
9

$
ab$
abracadabracarab
abracarab$
acadabracarab$
acarab$
adabracarab$
arab$
b$
bracadabracarab$
bracarab$
cadabracarab$
carab$
dabracarab$
rab$
racadabracarab$
racarab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - example

t =abracadabracarab$

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

bwt

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

1. sort bwt
to retrieve    rst
column f

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

2. find last sym-
bol $ in f at
position 0 and
write to output

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

b$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

2. symbol pre-
ceding $ in t is
bw t [0] = b.
write to output

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

b$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

3. as there
are no b before
bw t [0], we
know that this b
corresponds to
the    rst b in f
at pos f [8].

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

ab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

4. the symbol
preceding f [8] is
bw t [8] = a.
output!

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

ab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

5. map that a
back to f at
position f [1]

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

rab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

6. output
bw t [1] = r
and map r to
f [14]

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

arab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

7. output
bw t [14] = a
and map a to
f [7]

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

arab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

why does
bw t [14] = a
map to f [7]?

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

all a preceding
bw t [14] = a
preceed su   xes
smaller than
sa[14].

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

arab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

thus, among the suf-
   xes starting with a,
the one preceding
sa[14] must be the
last one.

t =
0
$
1
a
2
a
3
a
4
a
5
a
6
a
7
a
8
b
9
b
10
b
11
c
12
c
13
d
14
r
15
r
16
r

arab$

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

bwt - reconstructing t from bwt

t =abracadabracarab$
b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

search backwards,
start by    nding the
r interval in f

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

search backwards,
start by    nding the
r interval in f

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

how many b   s are
the r interval in
bw t [14, 16]? 2

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

how many su   xes
starting with b are
smaller than those 2?
1 at bw t [0]

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

thus, all su   xes start-
ing with br are in
sa[9, 10].

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

how many of the suf-
   xes starting with br
are preceded by a? 2

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

how many of the suf-
   xes smaller than br
are preceded by a? 1

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

t =abracadabracarab$, p =abr

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

there are 2 occur-
rences of abr in t cor-
responding to su   xes
sa[2, 3]

$
a
a
a
a
a
a
a
b
b
b
c
c
d
r
r
r

b
r
$
d
r
r
c
c
a
a
a
a
a
a
a
b
b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

searching using the bwt

we only require f and bw t to search and recover t

we only had to count the number of times a symbol s
occurs within an interval, and before that interval
bw t [i, j]

equivalent to ranks(bw t, i) and ranks(bw t, j)

need to perform rank on non-binary alphabets e   ciently

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - overview

data structure to perform rank and select on non-binary
alphabets of size    in o(log2   ) time

decompose non-binary rank operations into binary rank   s
via tree decomposition

space usage n log    + o(n log   ) bits. same as original
sequence + rank + select overhead

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b r $ d r
b

r c c a a a

b

a

a

a

a

symbol codeword

$
a
b
c
d
r

00
010
011
10
110
111

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

a

b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

0 1 2 3
r d r
r
1 0 1 1

a

b

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - example

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - what is actually stored

0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0

1 0 1 1 1 1 1 1 1 1 1

1 1 1 1 0 0

$

1 0 0 0 0 0 0 0 1 1

c

1 0 1 1

a

b

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b
b r $ d r
r c c a a a
0 1 0 1 1 1 1 1 0 0 0
0

b
0

a
0

a
0

a
0

a
0

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b r $ d r
b
a
0 1 0 1 1 1 1 1 0 0 0 0

r c c a a

b

0

0

0

0

0

a

a

a

a

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b r $ d r
b
a
0 1 0 1 1 1 1 1 0 0 0 0

r c c a a

b

0

0

0

0

0

a

a

a

a

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b r $ d r
b
a
0 1 0 1 1 1 1 1 0 0 0 0

r c c a a

b

0

0

0

0

0

a

a

a

a

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b r $ d r
b
a
0 1 0 1 1 1 1 1 0 0 0 0

r c c a a

b

0

0

0

0

0

a

a

a

a

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - performing ranka(bw t, 11)

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
b r $ d r
b
a
0 1 0 1 1 1 1 1 0 0 0 0

r c c a a

b

0

0

0

0

0

a

a

a

a

0 1 2 3 4 5 6 7 8 9 10
b $ a a a a a a a b b
1 0 1 1 1 1 1 1 1 1 1

0 1 2 3 4 5
r d r
r c c
1 1 1 1 0 0

$

0 1 2 3 4 5 6 7 8 9
b a a a a a a a b b
1 0 0 0 0 0 0 0 1 1

c

a

b

0 1 2 3
r d r
r
1 0 1 1

d

r

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - space usage

currently: n log    + o(n log   ) bits. still larger than the original
text!

how can we do better?

compressed bitvectors

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - space usage

currently: n log    + o(n log   ) bits. still larger than the original
text!

how can we do better?

picking the codewords for each symbol smarter!

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

wavelet trees - space usage

currently

hu   man shape:

symbol freq codeword

symbol freq codeword

$
a
b
c
d
r

1
7
3
2
1
3

00
010
011
10
110
111

$
a
b
c
d
r

1
7
3
2
1
3

1100
0
101
111
1101
100

bits per symbol: 2.82

bits per symbol: 2.29

space usage of hu   man shaped wavelet tree:
h0(t )n + o(h0(t )n) bits.

even better: hu   man shape + compressed bitvectors

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt - space usage in practice

dna.200mb

proteins.200mb

4 k

3 k

2 k

]
s
n
[

r
e
t
c
a
r
a
h
c

r
e
p

1 k

0 k
4 k

dblp.xml.200mb

english.200mb

e
m

i
t

3 k

t
n
u
o
c

2 k

1 k

0 k

25%

50%

75%
index size [% of original text size]

100%

25%

50%

75%

100%

index

csa
csa-sada

csa++
csa-opf

fm-hf-bvil
fm-hf-rrr

fm-fb-bvil
fm-fb-hyb

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt - trade-o   s in sdsl

f i l e = a r g v [ 1 ] ;

1 #i n c l u d e     s d s l / s u f f i x a r r a y s . hpp    
2 #i n c l u d e     s d s l / b i t v e c t o r s . hpp    
3 #i n c l u d e     s d s l / w a v e l e t t r e e s . hpp    
4
i n t main ( i n t a r g c , char       a r g v ) {
5
6
7
8
9
10
11
12
13
14
15
16

s t d : : s t r i n g i n p u t
// u s e a c o m p r e s s e d b i t v e c t o r
u s i n g b v t y p e = s d s l : : h y b v e c t o r <>;
// u s e a huffman s h a p e d w a v e l e t
t r e e
u s i n g w t t y p e = s d s l : : w t h u f f <b v t y p e >;
// u s e a wt b a s e d csa
u s i n g c s a t y p e = s d s l : : c s a w t <w t t y p e >;
c s a t y p e c s a ;
s d s l : : c o n s t r u c t ( csa , i n p u t
s d s l : : s t o r e t o f i l e ( csa , o u t

f i l e , 1 ) ;
f i l e ) ;

}

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt - trade-o   s in sdsl

s o we j u s t u s e

// u s e a r e g u l a r b i t v e c t o r

// 5% o v e r h e a d r a n k s t r u c t u r e

// don     t need s e l e c t
// s c a n n i n g which i s o( n )

1
2 u s i n g b v t y p e = s d s l : : b i t v e c t o r ;
3
4 u s i n g r a n k t y p e = s d s l : : r a n k s u p p o r t v 5 <1>;
5
6
7 u s i n g s e l e c t 1 t y p e = s d s l : : s e l e c t s u p p o r t s c a n <1>;
8 u s i n g s e l e c t 0 t y p e = s d s l : : s e l e c t s u p p o r t s c a n <0>;
9
10 u s i n g w t t y p e = s d s l : : w t h u f f <b v t y p e ,
11
12
13
14 u s i n g c s a t y p e = s d s l : : c s a w t <w t t y p e >;
15
16
17

c s a t y p e c s a ;
s d s l : : c o n s t r u c t ( csa , i n p u t
s d s l : : s t o r e t o f i l e ( csa , o u t

r a n k t y p e ,
s e l e c t 1 t y p e ,
s e l e c t 0 t y p e >;

// u s e a huffman s h a p e d w a v e l e t

f i l e , 1 ) ;
f i l e ) ;

t r e e

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt - searching

i n t main ( i n t a r g c , char       a r g v ) {

s t d : : s t r i n g i n p u t
s d s l : : c s a w t <> c s a ;
s d s l : : c o n s t r u c t ( csa , i n p u t

f i l e = a r g v [ 1 ] ;

f i l e , 1 ) ;

s t d : : s t r i n g p a t t e r n =     a b r     ;
auto nocc = s d s l : : c o u n t ( csa , p a t t e r n ) ;
auto o c c s = s d s l : : l o c a t e ( csa , p a t t e r n ) ;
f o r ( auto& o c c : o c c s ) {

s t d : : c o u t <<     f o u n d a t pos    

<< o c c << s t d : : e n d l ;

}
auto s n i p p e t = s d s l : : e x t r a c t ( csa , 5 , 1 2 ) ;
s t d : : c o u t <<     s n i p p e t =        

<< s n i p p e t <<             << s t d : : e n d l ;

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt - searching - utf-8

sdsl::csa_wt<> csa; //
sdsl::construct(csa, "this-file.cpp", 1);
std::cout << "count("") : "

<< sdsl::count(csa, "") << endl;

auto occs = sdsl::locate(csa, "\n");
sort(occs.begin(), occs.end());
auto max_line_length = occs[0];
for (size_t i=1; i < occs.size(); ++i)

max_line_length = std::max(max_line_length,

occs[i]-occs[i-1]+1);

std::cout << "max line length : "

<< max_line_length << endl;

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa-wt - searching - words

32 bit integer words:

sdsl::csa_wt_int<> csa;
// file containing uint32_t ints
sdsl::construct(csa, "words.u32", 5);
std::vector<uint32_t> pattern = {532432,43433};
std::cout << "count() : "

<< sdsl::count(csa,pattern) << endl;

log2    bit words in sdsl format:
sdsl::csa_wt_int<> csa;
// file containing a serialized sdsl::int_vector ints
sdsl::construct(csa, "words.sdsl", 0);
std::vector<uint32_t> pattern = {532432,43433};
std::cout << "count() : "

<< sdsl::count(csa,pattern) << endl;

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

csa - usage resources

tutorial:
http://simongog.github.io/assets/data/sdsl-slides/tutorial

cheatsheet:
http://simongog.github.io/assets/data/sdsl-cheatsheet.pdf

examples: https://github.com/simongog/sdsl-lite/examples

tests: https://github.com/simongog/sdsl-lite/test

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

compressed su   x trees

compressed representation of a su   x tree

internally uses a csa

store extra information to represent tree shape and node
depth information

three di   erent cst types available in sdsl

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

compressed su   x trees - cst

use a succinct tree representation to store su   x tree shape

compress the lcp array to store node depth information

operations:
root, parent, first child, iterators, sibling, depth,
node depth, edge, children... many more!

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

cst - example

( auto v :
c o u t << c s t . d e p t h ( v ) <<       [    << c s t . l b ( v ) <<     ,    

}
auto v = c s t . s e l e c t
l e a f ( 2 ) ;
f o r
( auto i t = c s t . b e g i n ( v ) ;
auto node =     i t ;
c o u t << c s t . d e p t h ( v ) <<       [    << c s t . l b ( v ) <<     ,    

!= c s t . end ( v ) ; ++i t ) {

i t

c s t ) {

<< c s t . r b ( v ) <<     ]     << e n d l ;

s d s l : : c s t s c t 3 <c s a t y p e > c s t ;
s d s l : : c o n s t r u c t i m ( c s t ,     a n a n a s     , 1 ) ;
f o r

1 u s i n g c s a t y p e = s d s l : : c s a w t <>;
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

}
v = c s t . p a r e n t ( c s t . s e l e c t
f o r

<< c s t . r b ( v ) <<     ]     << e n d l ;

<< c s t . r b ( v ) <<     ]     << e n d l ;

l e a f ( 4 ) ) ;

}

( auto i t = c s t . b e g i n ( v ) ;
c o u t << c s t . d e p t h ( v ) <<       [    << c s t . l b ( v ) <<     ,    

i t

!= c s t . end ( v ) ; ++i t ) {

csa internals

bwt

wavelet trees

csa usage

compressed su   x trees

cst - space usage visualization

http://simongog.github.io/assets/data/space-vis.html

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

applications to nlp (30 mins)

1 applications to nlp

2 lm fundamentals

3 lm complexity

4 lms meet sa/st

5 query and construct

6 experiments

7 other apps

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

application to nlp: language modelling

1 applications to nlp

2 lm fundamentals

3 lm complexity

4 lms meet sa/st

5 query and construct

6 experiments

7 other apps

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

language models & succinct data structures

count-based language models:

p (wi|w1, . . . , wi   1)     p (k)(wi|wi   k, . . . , wi   1)

estimation from k-gram corpus statistics using st/sa

based arounds su   x arrays [zhang and vogel, 2006]
and su   x trees [kennington et al., 2012]
practical using csa/cst [shareghi et al., 2016b]

in all cases, on-the-   y calculation and no cap on k required.1

related, machine translation

lookup of (dis)contiguous    phrases   , as part of dynamic
phrase-table [callison-burch et al., 2005, lopez, 2008].

1caps needed on smoothing parameters [shareghi et al., 2016a].

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

faster & cheaper language model research

commonly, store probabilities for k-grams explicitly.

e   cient storage

tries and hash tables for fast lookup [hea   eld, 2011]
lossy data structures [talbot and osborne, 2007]
storage of approximate probabilities using quantisation and
pruning [pauls and klein, 2011]
parallel    distributed    algorithms [brants et al., 2007]

overall: fast, but limited to    xed m-gram, and intensive
hardware requirements.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

language models

de   nition
a language model de   nes id203 p (wi|w1, . . . , wi   1), often
with a markov assumption, i.e., p     p (k)(wi|wi   k, . . . , wi   1).

example: id113 for k-gram lm

p (k)(wi|wi   1

i   k) =

c(wi
i   k)
c(wi   1
i   k)

using count of context, c(wi   1
count of full k-gram, c(wi
i   k)
   = (wi, wi+1, . . . , wj)

notation: wj
i

i   k); and

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

smoothed count-based language models

interpolate or backo    from higher to lower order models
i   k)p (k   1)(wi|wi   1

i   k) + g(wi   1

p (k)(wi|wi   1

i   k) = f (wi

i   k+1)

terminating at unigram id113, p (1).

selecting f and g functions

interpolation f is a discounted function of the context and

k-gram counts, reserving some mass for g

backo    only one of f or g term is non-zero, based on

whether full pattern is found

involved computation of either the discount or normalisation.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

kneser-ney smoothing (kneser and ney, 1995; chen and goodman, 1998)

intuition
not all k-grams should be treated equally     k-grams occurring
in fewer contexts should carry lower weight.

example

fransisco is a common unigram, but only occurs in one context,
san franscisco
treat unigram fransisco as having count 1.

enacted through formulation based occurrence counts for
scoring component k < m grams and discount smoothing.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

kneser-ney smoothing (kneser and ney, 1995; chen and goodman, 1998)

p (k)(wi|wi   1

i   k) = f (wi

i   k) + g(wi   1

i   k)p (k   1)(wi|wi   1

i   k+1)

highest order k = m

f (wi

[c(wi

i   k) =

g(wi   1

i   k+1)     dk]+
c(wi   1
i   k+1)
dkn1+(wi   1
c(wi   1
i   k+1)
0     dk < 1 are discount
constants.

i   k   1  )

i   k) =

lower orders k < m

i   k+1)     dk]+

[n1+(   wi
n1+(   wi   1
i   k+1  )
i   k+1  )
n1+(   wi   1
i   k+1  )

dkn1+(wi   1

f (wi

i   k) =

g(wi   1

i   k) =

uses unique context counts,
rather than counts directly.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

modi   ed kneser ney

discount component now a function of the k-gram count /
occurrence count

dk : [0, 1, 2, 3+]     r

now must incorporate the number of k-grams with given pre   x

consequence: complication to g term!

i   k+1  );
i   k+1  ); and

with count 1, n1(wi   1
with count 2, n2(wi   1
with count 3 or greater, n1+     n1     n2.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

su   cient statistics

kneser ney id203 compution requires the following:

c(wj
i )
n1+(wj

i  )
n1+(   wj
n1+(   wj
i  )
i  )
i  )

n1(wj
n2(wj

i )

basic counts

                                  occurrence counts

other smoothing methods also require forms of occurrence
counts, e.g., good-turing, witten-bell.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

construction and querying

probabilities computed ahead of time

calculate a static hashtable or trie mapping k-grams to
their id203 and backo    values.
big: number of possible & observed k-grams grows with k

querying

lookup the longest matching span including the current token,
and without the token. id203 computed from the full score
and context backo   .

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

query cost german europarl, kenlm trie

022545067590003006009001,2002345678910memorytimetext corpus  382mbnumbered & bzip compressed 67mbmibsecsapplications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

cost of construction german europarl, kenlm trie

04509001,3501,80007501,5002,2503,0002345678910memorytimemibsecsapplications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

precomputing versus on-the-   y

precomputing approach

does not scale gracefully to high order m;
large training corpora also problematic

can be computed directly from a cst

cst captures unlimited order k-grams (no limit on m);
many (but not all) statistics cheap to retrieve
lm probabilities computed on-the-   y

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

su   cient statistics captured in su   x structures

t =abracadabracarab$

i

1

2

3

sai

16 14 0

tsai

tsai   1

$

b

a

r

a

$

4

7

a

d

5

6

7

8

9 10 11 12 13 14 15 16 17

3 10 5 12 15 1

a

r

a

r

a

c

a

c

b

a

b

a

8

b

a

4 11 6 13 2

c

a

c

a

d

a

r

a

r

b

9

r

b

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

su   cient statistics captured in su   x structures

t =abracadabracarab$

i

1

2

3

sai

16 14 0

tsai

tsai   1

$

b

a

r

a

$

4

7

a

d

5

6

7

8

9 10 11 12 13 14 15 16 17

3 10 5 12 15 1

a

r

a

r

a

c

a

c

b

a

b

a

8

b

a

4 11 6 13 2

c

a

c

a

d

a

r

a

r

b

9

r

b

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

su   cient statistics captured in su   x structures

t =abracadabracarab$

i

1

2

3

sai

16 14 0

tsai

tsai   1

$

b

a

r

a

$

4

7

a

d

5

6

7

8

9 10 11 12 13 14 15 16 17

3 10 5 12 15 1

a

r

a

r

a

c

a

c

b

a

b

a

8

b

a

4 11 6 13 2

c

a

c

a

d

a

r

a

r

b

9

r

b

c(abra) = 2 from csa
range between lb = 3 and rb = 4, inclusive

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

su   cient statistics captured in su   x structures

t =abracadabracarab$

i

1

2

3

sai

16 14 0

tsai

tsai   1

$

b

a

r

a

$

4

7

a

d

5

6

7

8

9 10 11 12 13 14 15 16 17

3 10 5 12 15 1

a

r

a

r

a

c

a

c

b

a

b

a

8

b

a

4 11 6 13 2

c

a

c

a

d

a

r

a

r

b

9

r

b

c(abra) = 2 from csa
range between lb = 3 and rb = 4, inclusive

n1+(   abra) = 2 from bwt (wavelet tree)

size of set of preceeding symbols {$, d}

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

occurrence counts from the su   x tree

$

a

b

rab$

c

a

d..$

r

a

c

a

$

15

5

12

d..$ r

.

.

$

d..$ r

a

b

$

ca

d..$

ra

6

d..$ r

a

b

$

4

11

c

a

b$

13

d..$ r

a

b

$

3

10

1

8

2

9

16

b

r

a

c

a

$

14

d..$ c

.

.

$

0

7

number of proceeding symbols, n1+(    ), is either

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

occurrence counts from the su   x tree

$

a

b

rab$

c

a

d..$

r

a

c

a

$

15

5

12

d..$ r

.

.

$

d..$ r

a

b

$

ca

d..$

ra

6

d..$ r

a

b

$

4

11

c

a

b$

13

d..$ r

a

b

$

3

10

1

8

2

9

16

b

r

a

c

a

$

14

d..$ c

.

.

$

0

7

number of proceeding symbols, n1+(    ), is either

1 if internal to an edge (e.g.,    =abra)

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

occurrence counts from the su   x tree

$

a

b

rab$

c

a

d..$

r

a

c

a

$

15

5

12

d..$ r

.

.

$

d..$ r

a

b

$

ca

d..$

ra

6

d..$ r

a

b

$

4

11

c

a

b$

13

d..$ r

a

b

$

3

10

1

8

2

9

16

b

r

a

c

a

$

14

d..$ c

.

.

$

0

7

number of proceeding symbols, n1+(    ), is either

1 if internal to an edge (e.g.,    =abra)
degree(v) otherwise (e.g.,    =ab with degree 2)

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

more di   cult occurrence counts

how to handle occurrence counts to both sides,

n1+(       ) = |{w  v, s.t. c(w  v)     1}|
ni(    ) = |{  v, s.t. c(  v) = i}|

and speci   c value i occurrence counts,

no simple mapping to csa/cst algorithm

iterative (costly!) solution used instead:

enumerate extensions to one side
accumulate counts (to the other side, or query if c = i)

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

algorithm outline

step 1: search for pattern

backward search for each symbol, in right-to-left order.
results in bounds [lb, rb] of matching patterns.

step 2:    nd statistics

count c(a b r a) = rb     lb     1 (or 0 on failure.)

left occ. n1+(   wj

preceeding symbols.)

i ) can be computed from bwt (over

i  ) based on shape of the su   x tree.

right occ. n1+(wj

twin occ. etc . . . increasingly complex . . .

nb. illustrating ideas with basic sa/sts; in practice csa/csts.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

step 2: compute statistics

given range [lb, rb] for matching pattern,   , can compute:

count, c(  ) = (rb     lb + 1)

occurrence count, n1+(     ) = interval-symbols(lb, rb)
o(n1+(     )    log   ) where    is the size of the vocabulary

o(1); and

with time complexity

what about the other required occurrence counts?

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

querying algorithm: one-shot

p (ham)

green

eggs

and

ham

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

querying algorithm: one-shot

p (ham|and)

p (ham)

green

eggs

and

ham

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

querying algorithm: one-shot

p (ham|eggs and)

p (ham|and)

p (ham)

green

eggs

and

ham

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

querying algorithm: one-shot

p (ham|green eggs and)

p (ham|eggs and)

p (ham|and)

p (ham)

green

eggs

and

ham

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

querying algorithm: one-shot

p (ham|green eggs and)

p (ham|eggs and)

p (ham|and)

p (ham)

green

eggs

and

ham

at each step: 1) extend search for context and full pattern;
2) compute c and/or n 1+ counts.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

querying algorithm: full sentence

reuse matches
full matches in one step become context matches for next step.
e.g., green eggs and ham     green eggs and

recycle the csa matches from previous query, halving
search cost
n.b., can   t recycle counts, as mostly use di   erent types of
occurrence counts on numerator cf denominator

unlimited application

no bound on size of match, can continue until pattern unseen in
training corpus.

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

construction algorithm

1 sort su   xes (on disk)
2 construct csa
3 construct cst
4 compute discounts

e   cient using traversal of k-grams in the cst (up to a
given depth)

5 precompute some expensive values

again use traversal of k-grams in the cst

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

accelerating expensive counts

iterative calls, e.g., n1+(       ) account for majority of runtime.

solution: cache common values

store values for common entries, i.e., highest nodes in cst
values are integers, mostly with low values     very
compressable!

technique

store bit vector, bv, of length n, where bv[i] records
whether value for i is cached
store cached values in an integer vector, v, in linear order
retrieve ith value using v[rank1(bv, i)]

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

e   ect of caching

+15-20% space requirement (    10-gram)

2358  010s20sn'123+(a .)n123+(a .)n1+(. a .)n1+(. a)n1+(a .)backward   searchon   the   flytime (sec)2358  04ms8msprecomputedm   gramtime (msec)applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

timing versus other lms: small de europarl

construction

load+query

10000

)
s
(

e
m

i
t

1000

100

10

1

0.1

1.0

10.0

0.1

memory usage (gib)

1.0

10.0

cst on-the-   y
cst precompute

kenlm (trie)
kenlm (probing)

srilm

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

timing versus other lms: large de commoncrawl

construction

load+query

100

]

i

b
g

[

y
r
o
m
e
m

10

1

10k

]
s
d
n
o
c
e
s
[

e
m
t

i

1k

100

m

2gram
3gram
4gram
5gram
8gram
10gram

method

ken (pop.)
ken (lazy)
cst

1

4

8

16

32

1

4

input size [gib]

8

16

32

construction

load+query

1

4

8

16

32

1

4

input size [gib]

8

16

32

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

perplexity: usefulness of large or in   nite context

s
u
p
r
o
c

e
d

t
s
e
t
s
w
e
n

size (m)

perplexity

training

tokens sents m = 3 m = 5 m = 10

europarl
ncrawl2007
ncrawl2008
ncrawl2013
ncrawl2014

55
37
126
641
845

2.2 1004.8 973.3
2.0 514.8 493.5
6.8 427.7 404.8
35.1 268.9 229.8
46.3 247.6 195.2

971.4
488.9
400.0
225.6
189.3

all combined 2560 139.3 211.8 158.9

151.5

ccrawl32g

5540 426.6 336.6 292.8

287.8

e

n unit time (s) mem (gib) m = 5 m = 10 m = 20 m =    
68.80
2.33

8164
17 935

73.45
3.93

6.29
18.58

68.76
2.37

68.66
2.69

word
byte

d
r
o
w
b
1

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

practical exercise

finding concordances for an arbitrary k-gram pattern:

outline

   nd count of k-gram in large corpus
show tokens to left or right, sorted by count
   nd pairs of tokens occurring to left and right

tools

building a csa and cst
searching for pattern
querying cst path label & children (to right)
querying wt for symbols to left

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

semi-external indexes

semi-external su   x array (rosa)

store the    top    part of a su   x tree in memory (using a
compressed structure)
if pattern short and frequent. answer from in-memory
structure (fast!)
if pattern long or infrequent perform disk access
implemented, complicated, currently not used in practice

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

range minimum/maximum queries

given an array a of n items
for any range a[i, j] answer in constant time, what is the
largest / smallest item in the range
space usage: 2n + o(n) bits. a not required!

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

compressed tries / dictionaries

support lookup(s) which returns unique id if string s is in
dict or    1 otherwise
support retrieve(i) return string with id i
very compact. 10%     20% of original data
very fast lookup times
e   cient construction

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

graph compression

applications to nlp lm fundamentals lm complexity lms meet sa/st query and construct experiments other apps

other applications

store the    top    part of a su   x tree in memory (using a
compressed structure)
if pattern short and frequent. answer from in-memory
structure (fast!)
if pattern long or infrequent perform disk access
implemented, complicated, currently not used in practice

conclusions / take-home message

basic succinct structures rely on bitvectors and operations
rank and select
more complex structures are composed of these basic
building blocks
many trade-o   s exist
practical, highly engineered open source implementations
exist and can be used within minutes in industry and
academia
other    elds such as information retrieval, bioinformatics
have seen many papers using these succinct structures in
recent years

resources

compact data structures,
a practical approach
gonzalo navarro
isbn 978-1-107-15238-0. 570 pages.
cambridge university press, 2016

resources ii

overview of compressed text indexes:
[ferragina et al., 2008, navarro and m  akinen, 2007]
bitvectors: [gog and petri, 2014]
document retrieval: [navarro, 2014a]
compressed su   x trees:
[sadakane, 2007, ohlebusch et al., 2010]
wavelet trees: [navarro, 2014b]
compressed tree representations:
[navarro and sadakane, 2016]

references i

brants, t., popat, a. c., xu, p., och, f. j., and dean, j. (2007).
large language models in machine translation.
in proceedings of the 2007 joint conference on empirical methods in
natural language processing and computational natural language
learning (emnlp-conll), pages 858   867, prague, czech republic.
association for computational linguistics.

callison-burch, c., bannard, c. j., and schroeder, j. (2005).
scaling phrase-based id151 to larger corpora
and longer phrases.
in proceedings of the annual meeting of the association for
computational linguistics.

ferragina, p., gonz  alez, r., navarro, g., and venturini, r. (2008).
compressed text indexes: from theory to practice.
acm j. of exp. algorithmics, 13.

references ii

gog, s. and petri, m. (2014).
optimized succinct data structures for massive data.
softw., pract. exper., 44(11):1287   1314.

hea   eld, k. (2011).
kenlm: faster and smaller language model queries.
in proceedings of the workshop on id151.

kennington, c. r., kay, m., and friedrich, a. (2012).
su   x trees as language models.
in proceedings of the conference on language resources and
evaluation.

lopez, a. (2008).
machine translation by pattern matching.
phd thesis, university of maryland.

references iii

navarro, g. (2014a).
spaces, trees and colors: the algorithmic landscape of document
retrieval on sequences.
acm comp. surv., 46(4.52).

navarro, g. (2014b).
wavelet trees for all.
journal of discrete algorithms, 25:2   20.

navarro, g. and m  akinen, v. (2007).
compressed full-text indexes.
acm comp. surv., 39(1):2.

navarro, g. and sadakane, k. (2016).
compressed tree representations.
in encyclopedia of algorithms, pages 397   401.

references iv

ohlebusch, e., fischer, j., and gog, s. (2010).

cst++.

in proceedings of the international symposium on string processing
and information retrieval.

pauls, a. and klein, d. (2011).

faster and smaller id165 language models.

in proceedings of the annual meeting of the association for
computational linguistics: human language technologies.

sadakane, k. (2007).

compressed su   x trees with full functionality.

theory of computing systems, 41(4):589   607.

references v

shareghi, e., cohn, t., and ha   ari, g. (2016a).

richer interpolative smoothing based on modi   ed kneser-ney language
modeling.

in proceedings of the 2016 conference on empirical methods in
natural language processing, pages 944   949, austin, texas.
association for computational linguistics.

shareghi, e., petri, m., ha   ari, g., and cohn, t. (2015).

compact, e   cient and unlimited capacity: id38 with
compressed su   x trees.

in proceedings of the conference on empirical methods in natural
language processing.

references vi

shareghi, e., petri, m., ha   ari, g., and cohn, t. (2016b).

fast, small and exact: in   nite-order language modelling with
compressed su   x trees.

transactions of the association for computational linguistics,
4:477   490.

talbot, d. and osborne, m. (2007).

randomised language modelling for id151.

in proceedings of the annual meeting of the association for
computational linguistics.

zhang, y. and vogel, s. (2006).

su   x array and its applications in empirical natural language
processing.

technical report, cmu, pittsburgh pa.

