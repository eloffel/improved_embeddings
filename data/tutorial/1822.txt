compositional distributional models of meaning

dimitri kartsaklis mehrnoosh sadrzadeh

school of electronic engineering

and computer science

coling 2016

11th december 2016

osaka, japan

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

1/63

in a nutshell

compositional distributional models of meaning (cdms)
extend id65 to the phrase/sentence level.

they provide a function that produces a vectorial
representation of the meaning of a phrase or a sentence from
the distributional vectors of its words.

useful in every nlp task: sentence similarity, paraphrase
detection, id31, machine translation etc.

in this tutorial:

we review three generic classes of cdms: vector mixtures,
tensor-based models and neural models.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

2/63

outline

1

introduction

2 vector mixture models

3 tensor-based models

4 neural models

5 afterword

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

3/63

computers and meaning

how can we de   ne computational linguistics?

computational linguistics is the scienti   c and engineering
discipline concerned with understanding written and
spoken language from a computational perspective.

   stanford encyclopedia of philosophy1

1http://plato.stanford.edu

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

4/63

id152

the principle of compositionality

the meaning of a complex expression is determined by the
meanings of its parts and the rules used for combining them.

montague grammar: a systematic way of processing
fragments of the english language in order to get semantic
representations capturing their meaning.

there is in my opinion no important theoretical
di   erence between natural languages and the
arti   cial languages of logicians.

   richard montague, universal grammar (1970)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

5/63

syntax-to-semantics correspondence (1/2)

a lexicon:

(1)

a. every (cid:96) dt :   p.  q.   x[p(x)     q(x)]
b. man (cid:96) n :   y .man(y )
c. walks (cid:96) vi :   z.walk(z)

a parse tree, so syntax guides the semantic composition:

s

np

v in

dt

n

walks

every

man

np     dt n : [[n]]([[dt]])
s     np vin : [[vin ]]([[np]])

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

6/63

syntax-to-semantics correspondence (2/2)

logical forms of compounds are computed via   -reduction:

s

   x[man(x)     walk(x)]

np

  q.   x[man(x)     q(x)]

v in

  z.walk(z)

walks

dt

  p.  q.   x[p(x)     q(x)]

n

  y .man(y )

every

man

the semantic value of a sentence can be true or false.
can we do better than that?

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

7/63

the meaning of words

distributional hypothesis

words that occur in similar contexts have similar meanings
[harris, 1958].

the functional interplay of philosophy and
...and among works of dystopian
the rapid advance in
...calculus, which are more popular in
but because
...the value of opinions formed in
...if
...is an art, not an exact
...factors shaping the future of our civilization:
...certainty which every new discovery in
...if the new technology of computer
he got a
...frightened by the powers of destruction
...but there is also specialization in

?
?
?
?
?
?
?
?
?
?
?
?
?
?

should, as a minimum, guarantee...
   ction...
today suggests...
-oriented schools.
is based on mathematics...
as well as in the religions...
can discover the laws of human nature....
.
and religion.
either replaces or reshapes.
is to grow signi   cantly
scholarship to yale.
has given...
and technology...

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

8/63

the meaning of words

distributional hypothesis

words that occur in similar contexts have similar meanings
[harris, 1958].

the functional interplay of philosophy and science should, as a minimum, guarantee...

...and among works of dystopian science    ction...

the rapid advance in science today suggests...
...calculus, which are more popular in science -oriented schools.

but because science is based on mathematics...
...the value of opinions formed in science as well as in the religions...

...if science can discover the laws of human nature....

...is an art, not an exact science .

...factors shaping the future of our civilization: science and religion.

...certainty which every new discovery in science either replaces or reshapes.

...if the new technology of computer science is to grow signi   cantly

...frightened by the powers of destruction science has given...

...but there is also specialization in science and technology...

he got a science scholarship to yale.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

8/63

distributional models of meaning

a word is a vector of co-occurrence statistics with every other
word in a selected subset of the vocabulary:

semantic relatedness is usually based on cosine similarity:

sim(      v ,      u ) = cos         v ,      u =

(cid:104)      v          u (cid:105)
(cid:107)      v (cid:107)(cid:107)      u (cid:107)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

9/63

milkcutedogbankmoney128501catcatdogaccountmoneypeta real vector space

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

10/63

3020100102030404030201001020catdogpetkittenmousehorselionelephanttigerparroteaglepigeonravenseagullmoneystockbankfinancecurrencyprofitcreditmarketbusinessbrokeraccountlaptopdataprocessortechnologyrammegabytekeyboardmonitorintelmotherboardmicrochipcpugamefootballscorechampionshipplayerleagueteamthe necessity for a uni   ed model

distributional models of meaning are quantitative, but they
do not scale up to phrases and sentences; there is not enough
data:

even if we had an in   nitely large corpus,
what the context of a sentence would be?

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

11/63

the role of compositionality

compositional distributional models

we can produce a sentence vector by composing the vectors
of the words in that sentence.

      s = f (      w1,      w2, . . . ,      wn)

three generic classes of cdms:

vector mixture models [mitchell and lapata (2010)]

tensor-based models [coecke, sadrzadeh, clark (2010); baroni and
zamparelli (2010)]

neural models [socher et al. (2012); kalchbrenner et al. (2014)]

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

12/63

a cdms hierarchy

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

13/63

applications (1/2)

why cdms are important?

the problem of producing robust representations for the
meaning of phrases and sentences is at the heart of every
task related to natural language.

paraphrase detection
problem: given two sentences, decide if they say the same
thing in di   erent words
solution: measure the cosine similarity between the sentence
vectors
id31
problem: extract the general sentiment from a sentence or a
document
solution: train a classi   er using sentence vectors as input

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

14/63

applications (2/2)

id123
problem: decide if one sentence logically infers a di   erent one
solution: examine the feature inclusion properties of the
sentence vectors
machine translation
problem: automatically translate one sentence into a di   erent
language
solution: encode the source sentence into a vector, then use
this vector to decode a surface form into the target language

and so on. many other potential applications exist...

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

15/63

outline

1

introduction

2 vector mixture models

3 tensor-based models

4 neural models

5 afterword

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

16/63

element-wise vector composition

the easiest way to compose two vectors is by working
element-wise [mitchell and lapata (2010)]:

(cid:88)
            w1w2 =         w1 +         w2 =
(cid:88)
            w1w2 =       w1 (cid:12)       w2 =

i

)      ni

(  c w1

i

i +   c w2
      ni

c w1
i c w2

i

an element-wise    mixture    of the input elements:

i

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

17/63

==vector mixturetensor-basedproperties of vector mixture models

words, phrases and sentences share the same vector space

a bag-of-word approach. word order does not play a role:

         
dog +

         
bites +          man =          man +

         
bites +

         
dog

feature-wise, vector addition can be seen as feature union,
and vector multiplication as feature intersection

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

18/63

vector mixtures: intuition

the distributional vector of a word shows the extent to which
this word is related to other words of the vocabulary

for a verb, the components of its vector are related to the
action described by the verb

i.e. the vector for the word    run    shows the extent to which a
   dog    can run, a    car    can run, a    table    can run and so on

so, the element-wise composition of
extent to which things that are related to dogs can also run
(and vice versa); in other words:

         
dog with       run shows the

the resulting vector shows how compatible is the verb
with the speci   c subject.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

19/63

vector mixtures: pros and cons

distinguishing feature:

all words contribute equally to the    nal result.

pros:

trivial to implement

surprisingly e   ective in practice

cons:

a bag-of-word approach

does not distinguish between the type-logical identities of the
words

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

20/63

outline

1

introduction

2 vector mixture models

3 tensor-based models

4 neural models

5 afterword

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

21/63

relational words as functions

in a vector mixture model, an adjective is of the same order as
the noun it modi   es, and both contribute equally to the result.

one step further: relational words are multi-linear maps
(tensors of various orders) that can be applied to one or more
arguments (vectors).

formalized in the context of compact closed categories by
coecke, sadrzadeh and clark (2010).

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

22/63

==vector mixturetensor-basedquantizing the grammar

coecke, sadrzadeh and clark (2010):

pregroup grammars are structurally homomorphic with the
category of    nite-dimensional vector spaces and linear maps
(both share compact closure)

in abstract terms, there exists a structure-preserving passage
from grammar to meaning:

f : grammar     meaning

the meaning of a sentence w1w2 . . . wn with grammatical
derivation    is de   ned as:

                        
w1w2 . . . wn := f(  )(      w1           w2     . . .           wn)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

23/63

pregroup grammars

a pregroup grammar p(  ,b) is a relation that assigns gram-
matical types from a pregroup algebra freely generated over
a set of atomic types b to words of a vocabulary   .

a pregroup algebra is a partially ordered monoid, where each
element p has a left and a right adjoint such that:
pl    p     1     p    pl

p    pr     1     pr    p

elements of the pregroup are basic (atomic) grammatical
types, e.g. b = {n, s}.
atomic grammatical types can be combined to form types of
higher order (e.g. n    nl or nr    s    nl )
a sentence w1w2 . . . wn (with word wi to be of type ti ) is
grammatical whenever:

t1    t2    . . .    tn     s

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

24/63

pregroup derivation: example

p    pr     1     pr    p

pl    p     1     p    pl

s

np

vp

trembling

shadows

play

hide-and-seek

n nl

n

nr s nl

n

adj

n

v

n

trembling

shadows

play

hide-and-seek

n    nl    n    nr    s    nl    n     n    1    nr    s    1

= n    nr    s
    1    s
= s

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

25/63

compact closed categories

a monoidal category (c,   , i ) is compact closed when every
object has a left and a right adjoint, for which the following
morphisms exist:
 r       i

al     a  l       i

  r       ar     a

  l       a     al

a     ar

pregroup grammars are cccs, with   and    maps
corresponding to the partial orders
fdvect, the category of    nite-dimensional vector spaces and
linear maps, is a also a (symmetric) ccc:

  maps correspond to inner product
   maps to identity maps and multiples of those

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

26/63

a functor from syntax to semantics

we de   ne a strongly monoidal functor f such that:

f : p(  ,b)     fdvect

f(p) = p    p     b
f(1) = r

f(p    q) = f(p)     f(q)

f(pr ) = f(pl ) = f(p)

f(p     q) = f(p)     f(q)

f( r ) = f( l ) = inner product in fdvect
f(  r ) = f(  l ) = identity maps in fdvect

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

27/63

a multi-linear model

the grammatical type of a word de   nes the vector space
in which the word lives:

nouns are vectors in n;
adjectives are linear maps n     n, i.e elements in
n     n;
intransitive verbs are linear maps n     s, i.e. elements
in n     s;
transitive verbs are bi-linear maps n     n     s, i.e.
elements of n     s     n;

the composition operation is tensor contraction, i.e.
elimination of matching dimensions by application of inner
product.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

28/63

categorical composition: example

s

np

vp

trembling shadows

play hide-and-seek

n nl

n

nr s nl

n

adj

n

v

n

trembling

shadows

play

hide-and-seek

type reduction morphism:

n    1s )     (1n     l
( r

f(cid:104)

n    1s )     (1n     l
( r

n    1nr   s     l
n)
( n     1s )     (1n      n     1n   s      n )

n    1nr  s     l
(cid:105)(cid:16)
(cid:16)

n) : n    nl    n    nr    s    nl    n     s

(cid:17)
shadows     play                                   
(cid:17)
shadows     play                                   
shadows    play                                  
play     n     s     n

trembling                       
trembling                       
trembling                      
trembling     n     n

hide-and-seek

hide-and-seek

hide-and-seek

=

=

                  
shadows,

                              
hide-and-seek     n

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

29/63

creating relational tensors: extensional approach

a relational word is de   ned as the set of its arguments:

[[red]] = {car , door , dress, ink,      }

grefenstette and sadrzadeh (2011):
         
subj i

(cid:88)

(cid:88)

         nouni

verbint =

adj =

i

i

(cid:88)

i

         
subj i          

obj i

verbtr =

kartsaklis and sadrzadeh (2016):

(cid:88)

i

adj =

            nouni                 nouni
         
subj i    

(cid:88)

verbtr =

i

(cid:32)         

verbint =
      
obj i

subj i +
2

(cid:88)
         
subji              
(cid:33)
          

subji

obj i

i

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

30/63

creating relational tensors: statistical approach

baroni and zamparelli (2010):

create holistic distributional vectors for whole compounds (as
if they were words) and use them to train a id75
model.

(cid:34)

(cid:88)

i

1
2m

  adj = arg min

adj

(cid:35)

(adj                nouni                          
adj nouni )2

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

31/63

red   car = red car   door = red door   dress = red dress   ink = red inkfunctional words

certain classes of words, such as determiners, relative
pronouns, prepositions, or coordinators occur in almost every
possible context.

thus, they are considered semantically vacuous from a
distributional perspective and most often they are simply
ignored.

in the tensor-based setting, these special words can be mod-
elled by exploiting additional mathematical structures, such
as frobenius algebras and bialgebras.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

32/63

frobenius algebras in fdvect

given a symmetric ccc (c,   , i ), an object x     c has a
frobenius structure on it if there exist morphisms:
    : x     x     x ,    : x     i

   : x     x     x ,    : i     x

and

conforming to the frobenius condition:

(       1x )     (1x        ) =            = (1x       )     (        1x )

in fdvect, any vector space v with a    xed basis {      vi }i has a

commutative special frobenius algebra over it [coecke and
pavlovic, 2006]:

    :       vi

(cid:55)          vi           vi

   :       vi           vi

(cid:55)          vi

it can be seen as copying and merging of the basis.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

33/63

frobenius algebras: relative pronouns

how to represent relative pronouns in a tensor-based setting?

a relative clause modi   es the head noun of a phrase:

the man

who

likes

mary

the man

likes

mary

n

n r n s l n

n r s n l

n

=

n

n r n l

n

the result is a merging of the vectors of the noun and the
relative clause:

         man (cid:12) (likes                

mary )

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

34/63

[sadrzadeh, clark, coecke (2013)]

frobenius algebras: coordination

copying and merging are the key processes in coordination:

sleeps and snores

john

sleeps

snores

and

john

n

nr

s

s r

nrr nr

s

s l

n

nr

s

n

(cid:55)   

nr

s

nr

s

nr

s

the subject is copied by a    -map and interacts individually
with the two verbs
the results are merged together with a   -map

         
johnt    (sleep (cid:12) snore)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

35/63

[kartsaklis (2016)]

tensor-based models: intuition

tensor-based composition goes beyond a simple compatibility
check between the two argument vectors; it transforms the
input into an output of possibly di   erent type.

a verb, for example, is a function that takes as input a noun
and transforms it into a sentence:

fint : n     s

ftr : n    n     s

size and form of the sentence space become tunable
parameters of the models, and can depend on the task.
taking s = {( 0
1 ) , ( 1
to formal semantics.

0 )}, for example, provides an equivalent

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

36/63

tensor-based models: pros and cons

distinguishing feature:

relational words are multi-linear maps acting on arguments

pros:

aligned with the formal semantics perspective

more powerful than vector mixtures

flexible regarding the representation of functional words, such
as relative pronouns and prepositions.

cons:

every logical and functional word must be assigned to an
appropriate tensor representation   it   s not always clear how

space complexity problems for functions of higher arity (e.g. a
ditransitive verb is a tensor of order 4)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

37/63

outline

1

introduction

2 vector mixture models

3 tensor-based models

4 neural models

5 afterword

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

38/63

an arti   cial neuron

the xi s form the input vector
the wji s is a set of weights associated with the i-th output of
the layer

f is a non-linear function such as tanh or sigmoid
ai is the i-th output of the layer, computed as:

ai = f (w1i x1 + w2i x2 + w3i x3)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

39/63

a simple neural net

a feed-forward neural network with one hidden layer:

h1 = f (w11x1+w21x2+w31x3+w41x4+w51x5+b1)
h2 = f (w12x1+w22x2+w32x3+w42x4+w52x5+b2)
h3 = f (w13x1+w23x2+w33x3+w43x4+w53x5+b3)

      
h = f (w(1)      x +

      
b (1))

or

similarly:

      y = f (w(2)      

h +

      
b (2))

note that w(1)     r3  5 and w(2)     r2  3
f is a non-linear function such as tanh or sigmoid
(take f = id and you have a tensor-based model)

a universal approximator

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

40/63

objective functions

the goal of nn training is to    nd the set of parameters that
optimizes a given objective function

or, to put it di   erently, to minimize an error function.

assume, for example, the goal of the nn is to produce a

vector       y that matches a speci   c target vector

      
t . the

function:

e =

1
2m

(cid:88)

i

||      
ti           yi ||2

gives the total error across all training instances.

we want to set the weights of the nn such that e becomes
zero or as close to zero as possible.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

41/63

id119

take steps proportional to the negative of the gradient of e
at the current point.

  t =   t   1          e (  t   1)

  t: the parameters of
the model at time
step t

  : a learning rate

(graph taken from    the beginner programmer    blog,

http://   rsttimeprogrammer.blogspot.co.uk)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

42/63

id26 of errors

how do we compute the error terms at the inner layers?

these are computed based one the errors of the next layer by
using id26. in general:

  k =   t

k   k+1 (cid:12) f (cid:48)(zk )

  k is the error vector at layer k

  k is the weight matrix of layer k

zk is the weighted sum at the output of layer k
f (cid:48) is the derivative of the non-linear function f

just an application of the chain rule for derivatives.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

43/63

recurrent and recursive nns

standard nns assume that inputs are independent of each
other

that is not the case in language; a word, for example, always
depends on the previous words in the same sentence

in a recurrent nn, connections form a directed cycle so that
each output depends on the previous ones

a recursive nn is applied recursively following a speci   c
structure.

recurrent nn

recursive nn

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

44/63

inputoutputinputoutputid56s for composition

pollack (1990); socher et al. (2011;2012):

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

45/63

unsupervised learning with nns

how can we train a nn in an unsupervised manner?

train the network to reproduce its input via an expansion
layer:

use the output of the hidden layer as a compressed version of
the input [socher et al. (2011)]

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

46/63

id137 (1/2)

id56s are e   ective, but fail to capture long-range
dependencies such as:

the movie i liked and john said mary and ann

really hated

   vanishing gradient    problem: back-propagating the error
requires the multiplication of many very small numbers
together, and training for the bottom layers starts to stall.

id137 (lstms) (hochreiter and
schmidhuber, 1997) provide a solution, by equipping each
neuron with an internal state.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

47/63

id137 (2/2)

id56

lstm

(diagrams taken from christopher olah   s blog, http://colah.github.io/)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

48/63

linguistically aware nns

nn-based methods come mainly from image processing. how
can we make them more linguistically aware?

cheng and kartsaklis (2015):

take into account
syntax, by optimizing
against a scrambled
version of each sentence

dynamically
disambiguate the
meaning of words during
training based on their
context

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

49/63

main (ambiguous)vectorssense vectorsgatecompositionallayerphrase vectorplausibility layercompositional layersentence vectorplausibility layerconvolutional nns

originated in pattern recognition [fukushima, 1980]

small    lters apply on every position of the input vector:

capable of extracting    ne-grained local features independently
of the exact position in input

features become increasingly global as more layers are stacked

each convolutional layer is usually followed by a pooling layer

top layer is fully connected, usually a soft-max classi   er

application to language: collobert and weston (2008)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

50/63

did98s for modelling sentences

kalchbrenner, grefenstette

and blunsom (2014): a deep

architecture using dynamic

k-max pooling

syntactic structure is
induced automatically:

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

51/63

(figures reused with permission)

beyond sentence level

an additional convolutional layer can provide document vectors
[denil et al. (2014)]:

(figure reused with permission)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

52/63

neural models: intuition (1/2)

recall that tensor-based composition involves a linear
transformation of the input into some output.

neural models make this process more e   ective by applying
consecutive non-linear layers of transformation.

a nn does not only project a noun vector onto a sentence
space, but it can also transform the geometry of the space
itself in order to make it re   ect better the relationships be-
tween the points (sentences) in it.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

53/63

neural models: intuition (2/2)

example: although there is no linear map to send an input
x     {0, 1} to the correct xor value, the function can be
approximated by a simple nn with one hidden layer.

points in (b) can be seen as representing two semantically
distinct groups of sentences, which the nn is able to
distinguish (while a linear map cannot)

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

54/63

neural models: pros and cons

distinguishing feature:

drastic transformation of the sentence space.

pros:

non-linearity and layered approach allow the simulation of a
very wide range of functions
word vectors are parameters of the model, optimized during
training
state-of-the-art results in a number of nlp tasks

cons:

requires expensive training based on id26
di   cult to discover the right con   guration
a    black-box    approach: not easy to correlate inner workings
with output

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

55/63

outline

1

introduction

2 vector mixture models

3 tensor-based models

4 neural models

5 afterword

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

56/63

refresher: a cdms hierarchy

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

57/63

open issues-future work

no convincing solution for logical connectives, negation,
quanti   ers and so on.

functional words, such as prepositions and relative pronouns,
are also a problem.

sentence space is usually identi   ed with word space. this is
convenient, but is it the right thing to do?

solutions depend on the speci   c cdm class   e.g. not much
to do in a vector mixture setting
important: how can we make nns more linguistically aware?
[cheng and kartsaklis (2015)]

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

58/63

summary

cdms provide quantitative semantic representations for
sentences (or even documents)

element-wise operations on word vectors constitute an easy
and reasonably e   ective way to get sentence vectors

categorical compositional distributional models allow
reasoning on a theoretical level   a glass box approach

neural models are extremely powerful and e   ective; still a
black-box approach, not easy to explain why a speci   c
con   guration works and some other does not.

convolutional networks seem to constitute the most promising
solution to the problem of capturing the meaning of sentences

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

59/63

thank you for your attention!

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

60/63

references i

baroni, m. and zamparelli, r. (2010).
nouns are vectors, adjectives are matrices.
in proceedings of conference on empirical methods in natural language processing (emnlp).

cheng, j. and kartsaklis, d. (2015).
syntax-aware multi-sense id27s for deep compositional models of meaning.
in proceedings of the 2015 conference on empirical methods in natural language processing, pages
1531   1542, lisbon, portugal. association for computational linguistics.

coecke, b., sadrzadeh, m., and clark, s. (2010).
mathematical foundations for a compositional distributional model of meaning. lambek festschrift.
linguistic analysis, 36:345   384.

collobert, r. and weston, j. (2008).
a uni   ed architecture for natural language processing: deep neural networks with multitask learning.
in proceedings of the 25th international conference on machine learning, pages 160   167. acm.

denil, m., demiraj, a., kalchbrenner, n., blunsom, p., and de freitas, n. (2014).
modelling, visualising and summarising documents with a single convolutional neural network.
technical report arxiv:1406.3830, university of oxford.

grefenstette, e. and sadrzadeh, m. (2011).
experimental support for a categorical compositional distributional model of meaning.
in proceedings of the 2011 conference on empirical methods in natural language processing, pages
1394   1404, edinburgh, scotland, uk. association for computational linguistics.

harris, z. (1968).
mathematical structures of language.
wiley.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

61/63

references ii

kalchbrenner, n., grefenstette, e., and blunsom, p. (2014).
a convolutional neural network for modelling sentences.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1:
long papers), pages 655   665, baltimore, maryland. association for computational linguistics.

kartsaklis, d. (2015).
compositional id65 with compact closed categories and frobenius algebras.
phd thesis, university of oxford.

kartsaklis, d. and sadrzadeh, m. (2016).
a compositional distributional inclusion hypothesis.
in proceedings of the 2017 conference on logical aspects of computational linguistics, nancy, france.
springer.

mitchell, j. and lapata, m. (2010).
composition in distributional models of semantics.
cognitive science, 34(8):1388   1439.

montague, r. (1970a).
english as a formal language.
in linguaggi nella societ`a e nella tecnica, pages 189   224. edizioni di comunit`a, milan.

montague, r. (1970b).
universal grammar.
theoria, 36:373   398.

sadrzadeh, m., clark, s., and coecke, b. (2013).
the frobenius anatomy of word meanings i: subject and object relative pronouns.
journal of logic and computation, advance access.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

62/63

references iii

sadrzadeh, m., clark, s., and coecke, b. (2014).
the frobenius anatomy of word meanings ii: possessive relative pronouns.
journal of logic and computation.

socher, r., huang, e., pennington, j., ng, a., and manning, c. (2011).
dynamic pooling and unfolding recursive autoencoders for paraphrase detection.
advances in neural information processing systems, 24.

socher, r., huval, b., manning, c., and a., n. (2012).
semantic compositionality through recursive matrix-vector spaces.
in conference on empirical methods in natural language processing 2012.

socher, r., manning, c., and ng, a. (2010).
learning continuous phrase representations and syntactic parsing with id56s.
in proceedings of the nips-2010 deep learning and unsupervised id171 workshop.

d. kartsaklis, m. sadrzadeh

compositional distributional models of meaning

63/63

