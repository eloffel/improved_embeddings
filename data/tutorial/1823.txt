id4

rico sennrich

institute for language, cognition and computation

university of edinburgh

may 18 2016

rico sennrich

id4

1 / 65

id4

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

rico sennrich

id4

1 / 65

1 neural network crash course

2

introduction to id4

neural language models
attentional encoder-decoder

3

recent research, opportunities and challenges in neural machine
translation

rico sennrich

id4

2 / 65

id4

1 neural network crash course

2

introduction to id4

neural language models
attentional encoder-decoder

3

recent research, opportunities and challenges
in id4

rico sennrich

id4

3 / 65

building block: arti   cial neurons

analogy to biological neurons

input     dendrites
activation function     neuron       res    if voltage threshold is reached
output     axon

   christoph burgmer cc-by-sa-3.0
https://commons.wikimedia.org/wiki/file:artificialneuronmodel_english.png

rico sennrich

id4

4 / 65

a simple neural network

neural networks can solve non-linear functions.

xor

truth table

a b output
0
0
1
1

0
1
0
1

0
1
1
0

x1

x2

1

0.5

0.5

1

a

b

c

1

-2

1

d

y

(neurons arranged in layers, and    re if input is     1)

rico sennrich

id4

5 / 65

a simple neural network: math

neural networks can be implemented via matrix operations

network

x1

x2

1

0.5

0

0

0.5

1

a

b

c

1

-2

1

d

y

calculation of x (cid:55)    y

1
0
0.5 0.5
0
1

w1 =      
w2 =(cid:2)1    2 1(cid:3)

       h1 =      
y =(cid:2)d(cid:3)

a
b
c

       x =(cid:20)x1
x2(cid:21)

  (z) = z     1
h1 =   (w1x)
y =   (w2h1)

rico sennrich

id4

6 / 65

a simple neural network: python code

import numpy

#activation function
def phi(x):

return numpy.greater_equal(x,1).astype(int)

def nn(x):

w1 = numpy.array([ [1, 0.5, 0], [0, 0.5, 1] ])
w2 = numpy.array([[1], [-2], [1]])
h1 = phi(x.dot(w1))
h2 = phi(h1.dot(w2))
return h2

print nn(numpy.array([1, 0]))

rico sennrich

id4

7 / 65

training a neural network

id119
requirements:

labelled training data (supervised learning)
differentiable objective function

in forward pass, compute network output
compare output to true label to compute error
move all parameters in direction that minimizes error
chain rule allows ef   cient computation of gradient for each parameter
in backward pass     id26 of error
we approximate gradient on small minibatches to perform frequent
updates     stochastic id119

rico sennrich

id4

8 / 65

id180

id180

desirable:

differentiable (for stochastic id119)
monotonic (for convexity)
non-linear id180 essential to learn non-linear functions

identity (linear)
sigmoid
tanh
recti   ed linear unit (relu)

   3

   2

   1

y

3

2

1

   1

1

2

3

x

rico sennrich

id4

9 / 65

further basics

hyperparameters:

number and size of layers
minibatch size
learning rate
...

initialisation of weight matrices
stopping criterion
id173 (dropout)
bias units (always-on input)
more complex architectures (recurrent/convolutional networks)

rico sennrich

id4

10 / 65

resources

theano http://deeplearning.net/software/theano/
torch http://torch.ch/
tensor   ow https://www.tensorflow.org/
toolkits provide useful abstractions for neural networks:

routines for n-dimensional arrays (tensors)
simple use of different id202 backends (cpu/gpu)
automatic differentiation

rico sennrich

id4

11 / 65

id4

1 neural network crash course

2

introduction to id4

neural language models
attentional encoder-decoder

3

recent research, opportunities and challenges
in id4

rico sennrich

id4

12 / 65

language modelling

chain rule and markov assumption

a sentence t of length n is a sequence w1, . . . , wn

p(t ) = p(w1, . . . , wn)

n(cid:89)
n(cid:89)

i=1

i=1

=

   

p(wi|w0, . . . , wi   1)

(chain rule)

p(wi|wi   k, . . . , wi   1)

(markov assumption: id165 model)

rico sennrich

id4

13 / 65

id165 language model with feedforward neural network

[vaswani et al., 2013]

id165 nnlm [bengio et al., 2003]

input: context of n-1 previous words
output: id203 distribution for next word
linear embedding layer with shared weights
one or several hidden layers

rico sennrich

id4

14 / 65

representing words as vectors

one-hot encoding

example vocabulary:    man,    runs   ,    the   ,    .   
input/output for p(runs|the man):

x0 =            

0
0
1
0

            

x1 =            

1
0
0
0

            

ytrue =            

0
1
0
0

            

size of input/output vector: vocabulary size
embedding layer is lower-dimensional and dense

smaller weight matrices
network learns to group similar words to similar point in vector space

rico sennrich

id4

15 / 65

softmax activation function

softmax function

p(y = j|x) =

exj

(cid:80)k exk

softmax function normalizes output vector to id203 distribution
    computational cost linear to vocabulary size (!)
ideally: id203 1 for correct word; 0 for rest
sgd with softmax output minimizes cross-id178 (and hence
perplexity) of neural network

rico sennrich

id4

16 / 65

feedforward neural language model: math

[vaswani et al., 2013]

h1 =   w1(ex1, ex2)
y = softmax(w2h1)

rico sennrich

id4

17 / 65

feedforward neural language model in smt

ffnlm

can be integrated as a feature in the log-linear smt model
[schwenk et al., 2006]
costly due to id127s and softmax
solutions:

n-best reranking
variants of softmax (hierarchical softmax, self-id172 [nce])
shallow networks; premultiplication of hidden layer

scale well to many input words
    models with source context [devlin et al., 2014]

rico sennrich

id4

18 / 65

recurrent neural network language model (id56lm)

id56lm

motivation: condition on arbitrarily long context
    no markov assumption
we read in one word at a time, and update hidden state incrementally
hidden state is initialized as empty vector at time step 0
parameters:

embedding matrix e
feedforward matrices w1, w2
recurrent matrix u

hi =(cid:40)0,

tanh(w1exi + u hi   1)

yi = softmax(w2hi   1)

, if i = 0
, if i > 0

rico sennrich

id4

19 / 65

id56 variants

gated units

alternative to plain id56
sigmoid layers    act as    gates    that control    ow of information
allows passing of information over long time
    avoids vanishing gradient problem
strong empirical results
popular variants:

long short term memory (lstm) (shown)
gated recurrent unit (gru)

rico sennrich

id4

20 / 65
christopher olah http://colah.github.io/posts/2015-08-understanding-lstms/

id56 variants

gated units

alternative to plain id56
sigmoid layers    act as    gates    that control    ow of information
allows passing of information over long time
    avoids vanishing gradient problem
strong empirical results
popular variants:

long short term memory (lstm) (shown)
gated recurrent unit (gru)

rico sennrich

id4

20 / 65
christopher olah http://colah.github.io/posts/2015-08-understanding-lstms/

id4

1 neural network crash course

2

introduction to id4

neural language models
attentional encoder-decoder

3

recent research, opportunities and challenges
in id4

rico sennrich

id4

21 / 65

translation modelling

decomposition of translation problem (for id4)

a source sentence s of length m is a sequence x1, . . . , xm
a target sentence t of length n is a sequence y1, . . . , yn

t     = arg max

p(t|s)

t

p(t|s) = p(y1, . . . , yn|x1, . . . , xm)

=

p(yi|y0, . . . , yi   1, x1, . . . , xm)

n(cid:89)

i=1

rico sennrich

id4

22 / 65

translating with id56s

encoder-decoder [sutskever et al., 2014, cho et al., 2014]

two id56s (lstm or gru):

encoder reads input and produces hidden state representations
decoder produces output, based on last encoder hidden state

encoder and decoder are learned jointly
    supervision signal from parallel text is backpropagated

rico sennrich

id4

23 / 65

kyunghyun cho http://devblogs.nvidia.com/parallelforall/
introduction-neural-machine-translation-gpus-part-2/

id4: information bottleneck

summary vector

last encoder hidden-state    summarizes    source sentence
can    xed-size vector represent meaning of arbitrarily long sentence?
empirically, quality decreases for long sentences
reversing source sentence brings some improvement
[sutskever et al., 2014]

rico sennrich

id4

[sutskever et al., 2014]

24 / 65

attentional encoder-decoder

encoder

goal: avoid bottleneck of summary vector
use bidirectional id56, and concatenate forward and backward states
    annotation vector hi
represent source sentence as vector of n annotations
    variable-length representation

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

rico sennrich

id4

25 / 65

attentional encoder-decoder

attention

problem: how to incorporate variable-length context into hidden state?
attention model computes context vector as weighted average of
annotations
weights are computed by feedforward neural network with softmax
activation

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

rico sennrich

id4

26 / 65

attentional encoder-decoder: math

simpli   cations of model by [bahdanau et al., 2015] (for illustration)

plain id56 instead of gru
simpler output layer
we do not show bias terms

notation

w , u, e, c, v are weight matrices (of different dimensionality)

ex one-hot to embedding (e.g. 50000    512)
wx embedding to hidden (e.g. 512    1024)
ux hidden to hidden (e.g. 1024    1024)
c context (2x hidden) to hidden (e.g. 2048    1024)
...

separate weight matrices for encoder and decoder (e.g. ex and ey)
input x of length tx; output y of length ty

rico sennrich

id4

27 / 65

attentional encoder-decoder: math

encoder

      h j =(cid:40)0,
      h j =(cid:40)0,

tanh(      w xexxj +       u xhj   1)
tanh(      w xexxj +       u xhj+1)

hj = (      h j,      h j)

, if j = 0
, if j > 0
, if j = tx + 1
, if j     tx

rico sennrich

id4

28 / 65

attentional encoder-decoder: math

decoder

si =(cid:40)tanh(ws      h i),

tanh(wyeyyi + uysi   1 + cci)
ti = tanh(uosi   1 + voeyyi   1 + coci)
yi = softmax(woti)

, if i = 0
, if i > 0

attention model

eij = v(cid:62)a tanh(wasi   1 + uahj)
  ij = softmax(eij)

ci =

tx(cid:88)j=1

  ijhj

rico sennrich

id4

29 / 65

attention model

attention model

side effect: we obtain alignment between source and target sentence
applications:

visualisation
replace unknown words with back-off dictionary [jean et al., 2015]
...?

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

rico sennrich

id4

30 / 65

attention model

attention model also works with images:

rico sennrich

id4

[cho et al., 2015]

31 / 65

attention model

[cho et al., 2015]

rico sennrich

id4

32 / 65

id4: decoding

decoding

exact search intractable: n|vocab| for each possible output length n
    approximative search for best translation
given decoder state si, compute id203 of each output word yi

sampling: pick a random word (considering id203)
greedy search: pick the most probable word
id125: pick the k most probable words, and compute yi+1 for
each hypothesis in beam

id125 with small beam (k     10) seems suf   cient for neural
machine translation
ensemble: compute id203 distribution for next word with multiple
models, and use (geometric) average

rico sennrich

id4

33 / 65

further reading

secondary literature

lecture notes by kyunghyun cho: [cho, 2015]
introduction to lstm (and gru):

http://colah.github.io/posts/2015-08-understanding-lstms/

   id151    by philipp koehn (unpublished 2nd edition)

rico sennrich

id4

34 / 65

(a small selection of) resources

feedforward neural lm toolkits

cslm http://www-lium.univ-lemans.fr/cslm/
nplm https://github.com/moses-smt/nplm
oxlm https://github.com/pauldb89/oxlm

id4 tools

dl4mt-tutorial (theano) https://github.com/nyu-dl/dl4mt-tutorial
(our branch: nematus https://github.com/rsennrich/nematus)
id4.matlab https://github.com/lmthang/id4.matlab
id195 (tensor   ow) https://www.tensorflow.org/versions/r0.8/tutorials/id195/index.html

rico sennrich

id4

35 / 65

do it yourself

sample    les and instructions for training id4 model
https://github.com/rsennrich/wmt16-scripts
pre-trained models to test decoding (and for further experiments)
http://statmt.org/rsennrich/wmt16_systems/
please let me know about gaps in documentation
id4 tools (previous slide) may also contain instructions/samples

rico sennrich

id4

36 / 65

id4

1 neural network crash course

2

introduction to id4

neural language models
attentional encoder-decoder

3

recent research, opportunities and challenges
in id4

rico sennrich

id4

37 / 65

state of neural mt

attentional encoder-decoder networks are state of the art in mt
similar models used for other nlp tasks

rico sennrich

id4

38 / 65

attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind

nyu-umontreal

cambridge
uedin-syntax
kit/limsi

kit

uedin-pbmt
jhu-syntax

id7
34.2
32.3
30.8
30.6
30.6
29.1
29.0
28.4
26.6

table: wmt16 results for en   de

system

uedin-id4
uedin-pbmt
jhu-pbmt

uedin-syntax

kit

jhu-syntax

id7
38.6
35.1
34.5
34.4
33.9
31.0

table: wmt16 results for de   en

rico sennrich

id4

39 / 65

attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind

nyu-umontreal

cambridge
uedin-syntax
kit/limsi

kit

uedin-pbmt
jhu-syntax

id7
34.2
32.3
30.8
30.6
30.6
29.1
29.0
28.4
26.6

system

uedin-id4
uedin-pbmt
jhu-pbmt

uedin-syntax

kit

jhu-syntax

id7
38.6
35.1
34.5
34.4
33.9
31.0

table: wmt16 results for de   en

table: wmt16 results for en   de

pure id4

rico sennrich

id4

39 / 65

attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind

nyu-umontreal

cambridge
uedin-syntax
kit/limsi

kit

uedin-pbmt
jhu-syntax

id7
34.2
32.3
30.8
30.6
30.6
29.1
29.0
28.4
26.6

system

uedin-id4
uedin-pbmt
jhu-pbmt

uedin-syntax

kit

jhu-syntax

id7
38.6
35.1
34.5
34.4
33.9
31.0

table: wmt16 results for de   en

table: wmt16 results for en   de

pure id4
id4 component

rico sennrich

id4

39 / 65

attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind

nyu-umontreal

cambridge
uedin-syntax
kit/limsi

kit

uedin-pbmt
jhu-syntax

id7
34.2
32.3
30.8
30.6
30.6
29.1
29.0
28.4
26.6

table: wmt16 results for en   de

system

uedin-id4
uedin-pbmt
jhu-pbmt

uedin-syntax

kit

jhu-syntax

id7
38.6
35.1
34.5
34.4
33.9
31.0

table: wmt16 results for de   en

pure id4
id4 component
other neural components

rico sennrich

id4

39 / 65

attentional encoder-decoders (id4) are sota
uedin-id4
jhu-pbmt
pjatk

nyu-umontreal

uedin-id4

jhu-pbmt
cu-chimera

uedin-cu-syntax

cu-tamchyna
cu-tectomt

cu-mergedtrees

25.8
23.6
23.6
21.0
20.9
20.8
14.7
8.2

table: wmt16 results for en   cs

uedin-pbmt
uedin-id4
uedin-syntax

jhu-pbmt

limsi

35.2
33.9
33.6
32.2
31.0

31.4
30.4
28.3
13.3

cu-mergedtrees

table: wmt16 results for cs   en

qt21-himl-syscomb

uedin-id4

rwth-syscomb

uedin-pbmt

uedin-lmu-hiero

kit

lmu-cuni
limsi

jhu-pbmt

usfd-rescoring

28.9
28.1
27.1
26.8
25.9
25.8
24.3
23.9
23.5
23.1

table: wmt16 results for ro   en

table: wmt16 results for en   ro

rico sennrich

id4

39 / 65

attentional encoder-decoders (id4) are sota
uedin-pbmt
uedin-syntax
promt-smt
uh-factored

uedin-id4
amu-uedin
jhu-pbmt

23.4
20.4
20.3
19.3
19.1

26.0
25.3
24.0
23.6
23.5
23.1
20.9

limsi

afrl-mitll

nyu-umontreal

afrl-mitll-verb-annot

table: wmt16 results for en   ru

amu-uedin

nrc

uedin-id4

afrl-mitll

afrl-mitll-contrast

29.1
29.1
28.0
27.6
27.0

table: wmt16 results for ru   en

jhu-pbmt

table: wmt16 results for fi   en

abumatran-combo

abumatra-id4
nyu-umontreal
abumatran-pbsmt

jhu-pbmt

uh-factored
jhu-hltcoe

uut
aalto

17.4
17.2
15.1
14.6
13.8
12.8
11.9
11.6
11.6

table: wmt16 results for en   fi

rico sennrich

id4

39 / 65

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
unsere digitalen leben haben die notwendigkeit, stark, lebenslustig und erfolgreich zu erscheinen, verdoppelt [...]
our digital lives have doubled the need to appear strong, fun-loving and successful [...]
our digital lives have doubled the need to appear strong, lifelike and successful [...]
our digital lives are lively, strong, and to be successful, doubled [...]

rico sennrich

id4

40 / 65

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

rico sennrich

id4

41 / 65

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0
attacker https://commons.wikimedia.org/wiki/file:wikibully.jpg
golf club https://commons.wikimedia.org/wiki/file:golf_club,_callawax_x-20_8_iron_-_iii.jpg / cc-by-sa-3.0

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

racket

rico sennrich

id4

41 / 65

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0
attacker https://commons.wikimedia.org/wiki/file:wikibully.jpg
golf club https://commons.wikimedia.org/wiki/file:golf_club,_callawax_x-20_8_iron_-_iii.jpg / cc-by-sa-3.0

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

racket

attacker

rico sennrich

id4

41 / 65

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0
attacker https://commons.wikimedia.org/wiki/file:wikibully.jpg
golf club https://commons.wikimedia.org/wiki/file:golf_club,_callawax_x-20_8_iron_-_iii.jpg / cc-by-sa-3.0

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

racket

attacker

club

rico sennrich

id4

41 / 65

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0
attacker https://commons.wikimedia.org/wiki/file:wikibully.jpg
golf club https://commons.wikimedia.org/wiki/file:golf_club,_callawax_x-20_8_iron_-_iii.jpg / cc-by-sa-3.0

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
ein jahr sp  ter machten die fed-repr  sentanten diese k  rzungen r  ckg  ngig.
a year later, fed of   cials reversed those cuts.
a year later, fedex of   cials reversed those cuts.
a year later, the fed representatives made these cuts.

rico sennrich

id4

42 / 65

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
ein jahr sp  ter machten die fed-repr  sentanten diese k  rzungen r  ckg  ngig.
a year later, fed of   cials reversed those cuts.
a year later, fedex of   cials reversed those cuts.
a year later, the fed representatives made these cuts.

rico sennrich

id4

42 / 65

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
titelverteidiger ist drittligaabsteiger spvgg unterhaching.
the defending champions are spvgg unterhaching, who have been relegated to the third league.
defending champion is third-round pick spvgg underhaching.
title defender drittligaabsteiger week 2.

rico sennrich

id4

43 / 65

selected examples from wmt16

system
source
reference
uedin-id4
uedin-pbsmt

sentence
titelverteidiger ist drittligaabsteiger spvgg unterhaching.
the defending champions are spvgg unterhaching, who have been relegated to the third league.
defending champion is third-round pick spvgg underhaching.
title defender drittligaabsteiger week 2.

rico sennrich

id4

43 / 65

comparison between phrase-based and neural mt

pro neural mt

improved grammaticality [neubig et al., 2015]

word order
insertion/deletion of function words
morphological agreement

pro phrase-based/syntax-based smt

minor degradation in lexical choice? [neubig et al., 2015]

others

rare/unseen words are problematic for both:

pbsmt suffers from data sparseness and noisy phrase alignment
(our) id4 system attempts subword-level translation

rico sennrich

id4

44 / 65

why is neural mt output more grammatical?

neural mt

end-to-end trained model
generalization via continuous space representation
output conditioned on full source text and target history

phrase-based smt

log-linear combination of many    weak    features
data sparsenesss triggers back-off to smaller units
strong independence assumptions

rico sennrich

id4

45 / 65

ef   ciency

speed bottlenecks

id127
    use of highly parallel hardware (gpus)
softmax (scales with vocabulary size). solutions:

lms: hierarchical softmax; noise-contrastive estimation;
self-id172
id4: approximate softmax through subset of vocabulary
[jean et al., 2015]

id4 training vs. decoding (on fast gpu)

training: slow (1-3 weeks)
decoding: fast (100 000   500 000 sentences / day)a

awith nvidia titan x and amunn (https://github.com/emjotde/amunn)

rico sennrich

id4

46 / 65

open-vocabulary translation

why is vocabulary size a problem?

size of one-hot input/output vector is linear to vocabulary size
large vocabularies are space inef   cient
large output vocabularies are time inef   cient
typical network vocabulary size: 30 000   100 000

what about out-of-vocabulary words?

training set vocabulary typically larger than network vocabulary
(1 million words or more)
at translation time, we regularly encounter novel words:

names: barack obama
morph. complex words: hand|gep  ck|geb  hr (   carry-on bag fee   )
numbers, urls etc.

rico sennrich

id4

47 / 65

open-vocabulary translation

solutions

copy unknown words, or translate with back-off dictionary
[jean et al., 2015, luong et al., 2015b, g  l  ehre et al., 2016]
    works for names (if alphabet is shared), and 1-to-1 aligned words
use subword units (characters or others) for input/output vocabulary
    model can learn translation of seen words on subword level
    model can translate unseen words if translation is transparent

rico sennrich

id4

48 / 65

transparent translations

transparent translations

some translations are semantically/phonologically transparent
    no memorization needed; can be translated via subword units
morphologically complex words (e.g. compounds):

solar system (english)
sonnen|system (german)
nap|rendszer (hungarian)

named entities:

barack obama (english; german)
                      (russian)
        (ba-ra-ku o-ba-ma) (japanese)

cognates and loanwords:

claustrophobia (english)
klaustrophobie (german)
                           (russian)

many rare/unseen words belong to one of these categories

rico sennrich

id4

49 / 65

subword id4

flat representation [sennrich et al., 2015b, chung et al., 2016]

sentence is a sequence of subword units

hierarchical representation
[ling et al., 2015, luong and manning, 2016]

sentence is a sequence of words
words are a sequence of subword units

open question: should attention be on level of words or subwords?

rico sennrich

id4

50 / 65

underreviewasaconferencepaperaticlr2016variables,thesourceattentionaandthetargetcontextlfp   1,theid203ofagivenwordtypetpbeingthenexttranslatedwordtpisgivenby:p(tp|a,lfp   1)=exp(estpaa+stpllfp   1)pj   [0,t]exp(esjaa+sjllfp   1),wheresaandslaretheparametersthatmaptheconditionedvectorsintoascoreforeachwordtypeinthetargetlanguagevocabularyt.theparametersforaspeci   cwordtypejareobtainedassjaandsjl,respectively.then,scoresarenormalizedintoaid203.2.2character-basedmachinetranslationwenowpresentouradaptationoftheword-basedneuralnetworkmodeltooperateovercharactersequencesratherthanwordsequences.however,unlikepreviousapproachesthatattempttodiscardthenotionofwordscompletely(vilaretal.,2007;neubigetal.,2013),weproposeanhierarhicalarchitecture,whichreplacesthewordlookuptables(steps1and3)andthewordsoftmax(step6)withcharacter-basedalternatives,whichcomposethenotionofwordsfromindividualcharacters.theadvantageofthisapproachisthatwebene   tfrompropertiesofcharacter-basedapproaches(e.g.compactnessandorthographicsensitivity),butcanalsoeasilybeincorporatedintoanyword-basedneuralapproaches.character-basedwordrepresentationtheworkin(lingetal.,2015;ballesterosetal.,2015)proposesacompositionalmodelforlearningwordvectorsfromcharacters.similartowordlookuptables,awordstringsjismappedintoads,w-dimensionalvector,butratherthanallocatingparam-etersforeachindividualwordtype,thewordvectorsjiscomposedbyaseriesoftransformationusingitscharactersequencesj,0,...,sj,x.* c2w compositional modelblstmwhereword vector for "where"figure2:illustrationofthec2wmodel.squareboxesrepresentvectorsofneuronactivations.theillustrationofthemodelisshownin2.essentially,themodelbuildsarepresentationofthewordusingcharacters,byreadingcharactersfroid113fttorightandvice-versa.moreformally,givenanin-putwordsj=sj,0,...,sj,x,themodelprojectseachcharacterintoacontinuousds,c-dimensionalvectorssj,0,...,sj,xusingacharacterlookuptable.then,itbuildsaforwardlstmstatese-quencehf0,...,hfkbyreadingthecharactervectorssj,0,...,sj,x.another,backwardlstmreadsthecharactervectorsinthereverseordergeneratingthebackwardstateshbk,...,hb0.finally,the4subword id4

choice of subword unit

character-level: small vocabulary, long sequences
morphemes (?): hard to control vocabulary size
hybrid choice: shortlist of words, subwords for rare words
variable-length character id165s: byte-pair encoding (bpe)

open research question which subid40 is best choice in
terms of ef   ciency and effectiveness.

rico sennrich

id4

51 / 65

byte pair encoding for id40

id40 with byte-pair encoding [sennrich et al., 2015b]

actually a merge algorithm, starting from characters
iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: original vocabulary + one symbol per merge

   l o w </w>    : 5
   l o w e r </w>    : 2
   n e w e s t </w>    : 6
   w i d e s t </w>    : 3

rico sennrich

id4

52 / 65

byte pair encoding for id40

id40 with byte-pair encoding [sennrich et al., 2015b]

actually a merge algorithm, starting from characters
iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: original vocabulary + one symbol per merge

(   e   ,    s   ) : 9

   l o w </w>    : 5
   l o w e r </w>    : 2
   n e w es t </w>    : 6
   w i d es t </w>    : 3

rico sennrich

id4

52 / 65

byte pair encoding for id40

id40 with byte-pair encoding [sennrich et al., 2015b]

actually a merge algorithm, starting from characters
iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: original vocabulary + one symbol per merge

   l o w </w>    : 5
   l o w e r </w>    : 2
   n e w est </w>    : 6
   w i d est </w>    : 3

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9

rico sennrich

id4

52 / 65

byte pair encoding for id40

id40 with byte-pair encoding [sennrich et al., 2015b]

actually a merge algorithm, starting from characters
iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: original vocabulary + one symbol per merge

   l o w </w>    : 5
   l o w e r </w>    : 2
   n e w est</w>    : 6
   w i d est</w>    : 3

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9

rico sennrich

id4

52 / 65

byte pair encoding for id40

id40 with byte-pair encoding [sennrich et al., 2015b]

actually a merge algorithm, starting from characters
iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: original vocabulary + one symbol per merge

   lo w </w>    : 5
   lo w e r </w>    : 2
   n e w est</w>    : 6
   w i d est</w>    : 3

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7

rico sennrich

id4

52 / 65

byte pair encoding for id40

id40 with byte-pair encoding [sennrich et al., 2015b]

actually a merge algorithm, starting from characters
iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: original vocabulary + one symbol per merge

   low </w>    : 5
   low e r </w>    : 2
   n e w est</w>    : 6
   w i d est</w>    : 3

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7
...

rico sennrich

id4

52 / 65

byte pair encoding for id40

why bpe?

# tokens

good trade-off between vocabulary size and text length
# unk
segmentation
1079
none
characters
0
bpe (60k operations)
0
learned operations can be applied to unknown words
    open-vocabulary

# types
100 m 1 750 000
550 m
3000
112 m
63 000

   l o w e s t </w>   

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7

rico sennrich

id4

53 / 65

byte pair encoding for id40

why bpe?

# tokens

good trade-off between vocabulary size and text length
# unk
segmentation
1079
none
characters
0
bpe (60k operations)
0
learned operations can be applied to unknown words
    open-vocabulary

# types
100 m 1 750 000
550 m
3000
112 m
63 000

   l o w es t </w>   

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7

rico sennrich

id4

53 / 65

byte pair encoding for id40

why bpe?

# tokens

good trade-off between vocabulary size and text length
# unk
segmentation
1079
none
characters
0
bpe (60k operations)
0
learned operations can be applied to unknown words
    open-vocabulary

# types
100 m 1 750 000
550 m
3000
112 m
63 000

   l o w est </w>   

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7

rico sennrich

id4

53 / 65

byte pair encoding for id40

why bpe?

# tokens

good trade-off between vocabulary size and text length
# unk
segmentation
1079
none
characters
0
bpe (60k operations)
0
learned operations can be applied to unknown words
    open-vocabulary

# types
100 m 1 750 000
550 m
3000
112 m
63 000

   l o w est</w>   

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7

rico sennrich

id4

53 / 65

byte pair encoding for id40

why bpe?

# tokens

good trade-off between vocabulary size and text length
# unk
segmentation
1079
none
characters
0
bpe (60k operations)
0
learned operations can be applied to unknown words
    open-vocabulary

# types
100 m 1 750 000
550 m
3000
112 m
63 000

   lo w est</w>   

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7

rico sennrich

id4

53 / 65

byte pair encoding for id40

why bpe?

# tokens

good trade-off between vocabulary size and text length
# unk
segmentation
1079
none
characters
0
bpe (60k operations)
0
learned operations can be applied to unknown words
    open-vocabulary

# types
100 m 1 750 000
550 m
3000
112 m
63 000

   low est</w>   

(   e   ,    s   ) : 9
(   es   ,    t   ) : 9
(   est   ,    </w>   ) : 9
(   l   ,    o   ) : 7
(   lo   ,    w   ) : 7

rico sennrich

id4

53 / 65

byte pair encoding for id40

system
source
reference
copy unknown words
character bigrams
bpe (joint vocabulary)
source
reference
copy unknown words
character bigrams
bpe (joint vocabulary)

sentence
health research institutes
gesundheitsforschungsinstitute
forschungsinstitute
fo|rs|ch|un|gs|in|st|it|ut|io|ne|n
gesundheits|forsch|ungsin|stitute
asinine situation
dumme situation
asinine situation     unk     asinine
as|in|in|e situation     as|in|en|si|tu|at|io|n
as|in|ine situation     as|in|in-|situation

table: english   german translation example.    |    marks subword boundaries.

system
sentence
source
mirzayeva
reference
                 (mirzaeva)
copy unknown words
mirzayeva     unk     mirzayeva
character bigrams
mi|rz|ay|ev|a         |    |    |     (mi|rz|ae|va)
bpe (joint vocabulary) mir|za|yeva           |    |       (mir|za|eva)

table: english   russian translation example.    |    marks subword boundaries.

rico sennrich

id4

54 / 65

architecture variants

convolution network as encoder [kalchbrenner and blunsom, 2013]
treelstm as encoder [eriguchi et al., 2016]
modi   cations to attention mechanism
[luong et al., 2015a, feng et al., 2016, tu et al., 2016, mi et al., 2016]
    goal of better modelling distortion, coverage, etc.
reward symmetry between source-to-target and target-to-source
attention [cohn et al., 2016, cheng et al., 2015]

rico sennrich

id4

55 / 65

sequence-level training

problem: at training time, target-side history is reliable;
at test time, it is not.
solution: instead of using gold context, sample from the model to
obtain target context
[shen et al., 2015, ranzato et al., 2015, bengio et al., 2015]
more ef   cient cross id178 training remains in use to initialize
weights

rico sennrich

id4

56 / 65

training data: monolingual

why train on monolingual data?

cheaper to create/collect
parallel data is scarce for many language pairs
id20 with in-domain monolingual data

rico sennrich

id4

57 / 65

training data: monolingual

solutions/1

shallow fusion: rerank with language model [g  l  ehre et al., 2015]
deep fusion: extra, lm-speci   c hidden layer [g  l  ehre et al., 2015]

rico sennrich

id4

58 / 65

[g  l  ehre et al., 2015]

(a)shallowfusion(sec.4.1)(b)deepfusion(sec.4.2)figure1:graphicalillustrationsoftheproposedfusionmethods.learnedbythelmfrommonolingualcorporaisnotoverwritten.itispossibletousemonolingualcorporaaswellwhile   netuningalltheparame-ters,butinthispaper,wealteronlytheoutputpa-rametersinthestageof   netuning.4.2.1balancingthelmandtminorderforthedecoderto   exiblybalancethein-putfromthelmandtm,weaugmentthedecoderwitha   controller   mechanism.theneedto   ex-iblybalancethesignalsarisesdependingontheworkbeingtranslated.forinstance,inthecaseofzh-en,therearenochinesewordsthatcorre-spondtoarticlesinenglish,inwhichcasethelmmaybemoreinformative.ontheotherhand,ifanounistobetranslated,itmaybebettertoig-noreanysignalfromthelm,asitmaypreventthedecoderfromchoosingthecorrecttranslation.in-tuitively,thismechanismhelpsthemodeldynami-callyweightthedifferentmodelsdependingonthewordbeingtranslated.thecontrollermechanismisimplementedasafunctiontakingthehiddenstateofthelmasinputandcomputinggt=  (cid:16)v>gslmt+bg(cid:17),(7)where  isalogisticsigmoidfunction.vgandbgarelearnedparameters.theoutputofthecontrolleristhenmultipliedwiththehiddenstateofthelm.thisletsthede-coderusethesignalfromthetmfully,whilethecontrollercontrolsthemagnitudeofthelmsig-nal.inourexperiments,weempiricallyfoundthatitwasbettertoinitializethebiasbgtoasmall,neg-ativenumber.thisallowsthedecodertodecidetheimportanceofthelmonlywhenitisdeemednecessary.5datasetsweevaluatetheproposedapproachesonfourdi-versetasks:chinesetoenglish(zh-en),turkishtoenglish(tr-en),germantoenglish(de-en)andczechtoenglish(cs-en).wedescribeeachofthesedatasetsinmoredetailbelow.5.1parallelcorpora5.1.1zh-en:opeid4   15weusetheparallelcorporamadeavailableasapartofthenistopeid4   15challenge.sentence-alignedpairsfromthreedomainsarecombinedtoformatrainingset:(1)sms/chatand(2)conversationaltelephonespeech(cts)fromdarpaboltproject,and(3)newsgroup-s/weblogsfromdarpagaleproject.intotal,thetrainingsetconsistsof430ksentencepairs(seetable1forthedetailedstatistics).wetraininallourexperiments,wesetbg=   1toensurethatgtisinitially0.26onaverage.training data: monolingual

solutions/2

decoder is already a language model. train encoder-decoder with
added monolingual data [sennrich et al., 2015a]

ti = tanh(uosi   1 + voeyyi   1 + coci)
yi = softmax(woti)

how do we get approximation of context vector ci?

dummy source context (moderately effective)
automatically back-translate monolingual data into source language

name
pbsmt [haddow et al., 2015]
id4 [g  l  ehre et al., 2015]
shallow fusion [g  l  ehre et al., 2015]
deep fusion [g  l  ehre et al., 2015]
id4 baseline
+back-translated monolingual data

2014
28.8
23.6
23.7
24.0
25.9
29.5

2015
29.3

-
-
-

26.7
30.4

table: de   en translation performance (id7) on wmt training/test sets.

rico sennrich

id4

59 / 65

training data: multilingual

multi-source translation [zoph and knight, 2016]
we can condition on multiple input sentences

bene   ts:

one source text may contain information that is undespeci   ed in other
    possible quality gains
we need multiple source sentences at training and decoding time

drawbacks:

rico sennrich

id4

60 / 65

a b c <eos> w x y z <eos> z y x w a b c <eos> w x y z <eos> z y x w i j k figure2:multi-sourceencoder-decodermodelformt.wehavetwosourcesentences(cbaandkji)indifferentlanguages.eachlanguagehasitsownencoder;itpassesits   nalhiddenandcellstatetoasetofcombiners(inblack).theoutputofacombinerisahiddenstateandcellstateofthesamedimension.theinputgateofatypicallstmcell.inequa-tion4,therearetwoforgetgatesindexedbythesubscriptithatserveastheforgetgatesforeachoftheincomingcellsforeachoftheencoders.inequation5,orepresentstheoutputgateofanor-mallstm.i,f,o,anduareallsize-1000vectors.2.3multi-sourceattentionoursingle-sourceattentionmodelismodeledoffthelocal-pattentionmodelwithfeedinputfromluongetal.(2015b),wherehiddenstatesfromthetopdecoderlayercanlookbackatthetophiddenstatesfromtheencoder.thetopdecoderhiddenstateiscombinedwithaweightedsumoftheen-coderhiddenstates,tomakeabetterhiddenstatevector(  ht),whichispassedtothesoftmaxoutputlayer.withinput-feeding,thehiddenstatefromtheattentionmodelissentdowntothebottomde-coderlayeratthenexttimestep.thelocal-pattentionmodelfromluongetal.(2015b)worksasfollows.first,apositiontolookatinthesourceencoderispredictedbyequation9:pt=s  sigmoid(vtptanh(wpht))(9)sisthesourcesentencelength,andvpandwparelearnedparameters,withvpbeingavectorofdimension1000,andwpbeingamatrixofdimen-sion1000x1000.afterptiscomputed,awindowofsize2d+1islookedatinthetoplayerofthesourceencodercenteredaroundpt(d=10).foreachhiddenstateinthiswindow,wecomputeanalignmentscoreat(s),between0and1.thisalignmentscoreiscomputedbyequations10,11and12:at(s)=align(ht,hs)exp(cid:16)   (s   pt)22  2(cid:17)(10)align(ht,hs)=exp(score(ht,hs))ps0exp(score(ht,hs0))(11)score(ht,hs)=httwahs(12)inequation10,  issettobed/2andsisthesourceindexforthathiddenstate.waisalearn-ableparameterofdimension1000x1000.onceallofthealignmentsarecalculated,ctiscreatedbytakingaweightedsumofallsourcehid-denstatesmultipliedbytheiralignmentweight.the   nalhiddenstatesenttothesoftmaxlayerisgivenby:  ht=tanh(cid:16)wc[ht;ct](cid:17)(13)wemodifythisattentionmodeltolookatbothsourceencoderssimultaneously.wecreateacon-textvectorfromeachsourceencodernamedc1tandc2tinsteadofthejustctinthesingle-sourceattentionmodel:  ht=tanh(cid:16)wc[ht;c1t;c2t](cid:17)(14)inourmulti-sourceattentionmodelwenowhavetwoptvariables,oneforeachsourceencoder.training data: multilingual

multilingual models [dong et al., 2015, firat et al., 2016]
we can share layers of the model across language pairs

bene   ts:

id21 from one language pair to the other
    possible quality gains, especially for low-resourced language pairs
scalability: do we need n 2     n independent models for n languages?
    sharing of parameters allows linear growth
generalization to language pairs with no training data untested

drawbacks:

rico sennrich

id4

61 / 65

figure2:multi-tasklearningframeworkformultiple-targetlanguagetranslationfigure3:optimizationforendtomulti-endmodel3.4translationwithbeamsearchalthoughparallelcorporaareavailablefortheencoderandthedecodermodelinginthetrainingphrase,thegroundtruthisnotavailableduringtesttime.duringtesttime,translationisproducedby   ndingthemostlikelysequenceviabeamsearch.  y=argmaxyp(ytp|stp)(15)giventhetargetdirectionwewanttotranslateto,beamsearchisperformedwiththesharedencoderandaspeci   ctargetdecoderwheresearchspacebelongstothedecodertp.weadoptbeamsearchalgorithmsimilarasitisusedinsmtsystem(koehn,2004)exceptthatweonlyutilizescoresproducedbyeachdecoderasfeatures.thesizeofbeamis10inourexperimentsforspeedupconsideration.beamsearchisendeduntiltheend-of-sentenceeossymbolisgenerated.4experimentsweconductedtwogroupsofexperimentstoshowtheeffectivenessofourframework.thegoalofthe   rstexperimentistoshowthatmulti-tasklearninghelpstoimprovetranslationperformancegivenenoughtrainingcorporaforalllanguagepairs.inthesecondexperiment,weshowthatforsomeresource-poorlanguagepairswithafewparalleltrainingdata,theirtranslationperformancecouldbeimprovedaswell.4.1datasettheeuroparlcorpusisamulti-lingualcorpusincluding21europeanlanguages.hereweonlychoosefourlanguagepairsforourexperiments.thesourcelanguageisenglishforalllanguagepairs.andthetargetlanguagesarespanish(es),french(fr),portuguese(pt)anddutch(nl).todemonstratethevalidityofourlearningframework,wedosomepreprocessingonthetrainingset.forthesourcelanguage,weuse30kofthemostfrequentwordsforsourcelanguagevocabularywhichissharedacrossdifferentlanguagepairsand30kmostfrequentwordsforeachtargetlanguage.out-of-vocabularywordsaredenotedasunknownwords,andwemaintaindifferentunknownwordlabelsfordifferentlanguages.fortestsets,wealsorestrictallwordsinthetestsettobefromourtrainingvocabularyandmarktheoovwordsasthecorrespondinglabelsasinthetrainingdata.thesizeoftrainingcorpusinexperiment1and2islistedintable1where1727training data: other tasks

multi-task models [luong et al., 2016]

other tasks can be modelled with sequence-to-sequence models
we can share layers between translation and other tasks

rico sennrich

id4

62 / 65

publishedasaconferencepaperaticlr2016figure1:sequencetosequencelearningexamples   (left)machinetranslation(sutskeveretal.,2014)and(right)constituentparsing(vinyalsetal.,2015a).andgermanbyupto+1.5id7pointsoverstrongsingle-taskbaselinesonthewmtbenchmarks.furthermore,wehaveestablishedanewstate-of-the-artresultinconstituentparsingwith93.0f1.wealsoexploretwounsupervisedlearningobjectives,sequenceautoencoders(dai&le,2015)andskip-thoughtvectors(kirosetal.,2015),andrevealtheirinterestingpropertiesinthemtlsetting:autoencoderhelpslessintermsofperplexitiesbutmoreonid7scorescomparedtoskip-thought.2sequencetosequencelearningsequencetosequencelearning(id195)aimstodirectlymodeltheconditionalid203p(y|x)ofmappinganinputsequence,x1,...,xn,intoanoutputsequence,y1,...,ym.itaccomplishessuchgoalthroughtheencoder-decoderframeworkproposedbysutskeveretal.(2014)andchoetal.(2014).asillustratedinfigure1,theencodercomputesarepresentationsforeachinputsequence.basedonthatinputrepresentation,thedecodergeneratesanoutputsequence,oneunitatatime,andhence,decomposestheconditionalid203as:logp(y|x)=xmj=1logp(yj|y<j,x,s)(1)anaturalmodelforsequentialdataistherecurrentneuralnetwork(id56),whichisusedbymostoftherecentid195work.thesework,however,differintermsof:(a)architecture   fromunidirec-tional,tobidirectional,anddeepmulti-layerid56s;and(b)id56type   whicharelong-shorttermmemory(lstm)(hochreiter&schmidhuber,1997)andthegatedrecurrentunit(choetal.,2014).anotherimportantdifferencebetweenid195workliesinwhatconstitutestheinputrepresen-tations.theearlyid195work(sutskeveretal.,2014;choetal.,2014;luongetal.,2015b;vinyalsetal.,2015b)usesonlythelastencoderstatetoinitializethedecoderandsetss=[]ineq.(1).recently,bahdanauetal.(2015)proposesanattentionmechanism,awaytoprovideid195modelswitharandomaccessmemory,tohandlelonginputsequences.thisisaccomplishedbysettingsineq.(1)tobethesetofencoderhiddenstatesalreadycomputed.onthedecoderside,ateachtimestep,theattentionmechanismwilldecidehowmuchinformationtoretrievefromthatmemorybylearningwheretofocus,i.e.,computingthealignmentweightsforallinputpositions.recentworksuchas(xuetal.,2015;jeanetal.,2015a;luongetal.,2015a;vinyalsetal.,2015a)hasfoundthatitiscrucialtoempowerid195modelswiththeattentionmechanism.3multi-tasksequence-to-sequencelearningwegeneralizetheworkofdongetal.(2015)tothemulti-tasksequence-to-sequencelearningset-tingthatincludesthetasksofmachinetranslation(mt),constituencyparsing,andimagecaptiongeneration.dependingwhichtasksinvolved,weproposetocategorizemulti-taskid195learningintothreegeneralsettings.inaddition,wewilldiscusstheunsupervisedlearningtasksconsideredaswellasthelearningprocess.3.1one-to-manysettingthisschemeinvolvesoneencoderandmultipledecodersfortasksinwhichtheencodercanbeshared,asillustratedinfigure2.theinputtoeachtaskisasequenceofenglishwords.aseparatedecoderisusedtogenerateeachsequenceofoutputunitswhichcanbeeither(a)asequenceoftags2id4 as a component in id148

id148

model ensembling is well-established
reranking output of phrase-based/syntax-based with id4
[neubig et al., 2015]
incorporating id4 as a feature function into pbsmt
[junczys-dowmunt et al., 2016]
    results depend on relative performance of pbsmt and id4
log-linear combination of different neural models

left-to-right and right-to-left [liu et al., 2016]
source-to-target and target-to-source [li and jurafsky, 2016]

rico sennrich

id4

63 / 65

some future directions for (neural) mt research

(better) solutions to new(ish) problems

oovs, coverage, ef   ciency...
new solutions to old problems

consider context beyond sentence boundary
reward semantic adequacy of translation
deal with underspeci   ed input

chinese tense
spanish zero pronouns
english politeness

new opportunities

one model for many language pairs?
tight integration with other nlp tasks

rico sennrich

id4

64 / 65

lab session this afternoon

hands-on session with loose guidance
theano (+cuda) installation session
train your own id4 model
https://github.com/rsennrich/wmt16-scripts
try decoding with existing model
http://statmt.org/rsennrich/wmt16_systems/

rico sennrich

id4

65 / 65

thank you!

rico sennrich

id4

66 / 65

bibliography i

bahdanau, d., cho, k., and bengio, y. (2015).
id4 by jointly learning to align and translate.
in proceedings of the international conference on learning representations (iclr).

bengio, s., vinyals, o., jaitly, n., and shazeer, n. (2015).
scheduled sampling for sequence prediction with recurrent neural networks.
corr, abs/1506.03099.

bengio, y., ducharme, r., vincent, p., and janvin, c. (2003).
a neural probabilistic language model.
j. mach. learn. res., 3:1137   1155.

cheng, y., shen, s., he, z., he, w., wu, h., sun, m., and liu, y. (2015).
agreement-based joint training for bidirectional attention-based id4.
corr, abs/1512.04650.

cho, k. (2015).
natural language understanding with distributed representation.
corr, abs/1511.07916.

cho, k., courville, a., and bengio, y. (2015).
describing multimedia content using attention-based encoder-decoder networks.

cho, k., van merrienboer, b., gulcehre, c., bahdanau, d., bougares, f., schwenk, h., and bengio, y. (2014).
learning phrase representations using id56 encoder   decoder for id151.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1724   1734,
doha, qatar. association for computational linguistics.

rico sennrich

id4

67 / 65

bibliography ii

chung, j., cho, k., and bengio, y. (2016).
a character-level decoder without explicit segmentation for id4.
corr, abs/1603.06147.

cohn, t., hoang, c. d. v., vymolova, e., yao, k., dyer, c., and haffari, g. (2016).
incorporating structural alignment biases into an attentional neural translation model.
corr, abs/1601.01085.

devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j. (2014).
fast and robust neural network joint models for id151.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1370   1380, baltimore, maryland. association for computational linguistics.

dong, d., wu, h., he, w., yu, d., and wang, h. (2015).
id72 for multiple language translation.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 1723   1732, beijing, china. association for computational linguistics.

eriguchi, a., hashimoto, k., and tsuruoka, y. (2016).
tree-to-sequence attentional id4.
arxiv e-prints.

feng, s., liu, s., li, m., and zhou, m. (2016).
implicit distortion and fertility models for attention-based encoder-decoder id4 model.
corr, abs/1601.03317.

rico sennrich

id4

68 / 65

bibliography iii

firat, o., cho, k., and bengio, y. (2016).
multi-way, multilingual id4 with a shared attention mechanism.
arxiv e-prints.

g  l  ehre, c., ahn, s., nallapati, r., zhou, b., and bengio, y. (2016).
pointing the unknown words.
corr, abs/1603.08148.

g  l  ehre, c., firat, o., xu, k., cho, k., barrault, l., lin, h., bougares, f., schwenk, h., and bengio, y. (2015).
on using monolingual corpora in id4.
corr, abs/1503.03535.

haddow, b., huck, m., birch, a., bogoychev, n., and koehn, p. (2015).
the edinburgh/jhu phrase-based machine translation systems for wmt 2015.
in proceedings of the tenth workshop on id151, pages 126   133, lisbon, portugal. association for
computational linguistics.

jean, s., cho, k., memisevic, r., and bengio, y. (2015).
on using very large target vocabulary for id4.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 1   10, beijing, china. association for computational linguistics.

junczys-dowmunt, m., dwojak, t., and sennrich, r. (2016).
the amu-uedin submission to the wmt16 news translation task: attention-based id4 models as feature functions in
phrase-based smt.
in first conference on machine translation (wmt16), berlin, germany.

rico sennrich

id4

69 / 65

bibliography iv

kalchbrenner, n. and blunsom, p. (2013).
recurrent continuous translation models.
in proceedings of the 2013 conference on empirical methods in natural language processing, seattle. association for
computational linguistics.

li, j. and jurafsky, d. (2016).
mutual information and diverse decoding improve id4.
corr, abs/1601.00372.

ling, w., trancoso, i., dyer, c., and black, a. w. (2015).
character-based id4.
arxiv e-prints.

liu, l., utiyama, m., finch, a., and sumita, e. (2016).
agreement on target-bidirectional id4 .
in naacl hlt 16, san diego, ca.

luong, m., le, q. v., sutskever, i., vinyals, o., and kaiser, l. (2016).
multi-task sequence to sequence learning.
in iclr 2016.

luong, m.-t. and manning, c. d. (2016).
achieving open vocabulary id4 with hybrid word-character models.
arxiv e-prints.

luong, t., pham, h., and manning, c. d. (2015a).
effective approaches to attention-based id4.
in proceedings of the 2015 conference on empirical methods in natural language processing, pages 1412   1421, lisbon,
portugal. association for computational linguistics.

rico sennrich

id4

70 / 65

bibliography v

luong, t., sutskever, i., le, q., vinyals, o., and zaremba, w. (2015b).
addressing the rare word problem in id4.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 11   19, beijing, china. association for computational linguistics.

mi, h., sankaran, b., wang, z., and ittycheriah, a. (2016).
a coverage embedding model for id4.
arxiv e-prints.

neubig, g., morishita, m., and nakamura, s. (2015).
neural reranking improves subjective quality of machine translation: naist at wat2015.
in proceedings of the 2nd workshop on asian translation (wat2015), pages 35   41, kyoto, japan.

ranzato, m., chopra, s., auli, m., and zaremba, w. (2015).
sequence level training with recurrent neural networks.
corr, abs/1511.06732.

schwenk, h., dechelotte, d., and gauvain, j.-l. (2006).
continuous space language models for id151.
in proceedings of the coling/acl 2006 main conference poster sessions, pages 723   730, sydney, australia.

sennrich, r., haddow, b., and birch, a. (2015a).
improving id4 models with monolingual data.
arxiv e-prints.

sennrich, r., haddow, b., and birch, a. (2015b).
id4 of rare words with subword units.
corr, abs/1508.07909.

rico sennrich

id4

71 / 65

bibliography vi

shen, s., cheng, y., he, z., he, w., wu, h., sun, m., and liu, y. (2015).
minimum risk training for id4.
corr, abs/1512.02433.

sutskever, i., vinyals, o., and le, q. v. (2014).
sequence to sequence learning with neural networks.
in
advances in neural information processing systems 27: annual conference on neural information processing systems 2014,
pages 3104   3112, montreal, quebec, canada.

tu, z., lu, z., liu, y., liu, x., and li, h. (2016).
coverage-based id4.
corr, abs/1601.04811.

vaswani, a., zhao, y., fossum, v., and chiang, d. (2013).
decoding with large-scale neural language models improves translation.
in proceedings of the 2013 conference on empirical methods in natural language processing, emnlp 2013, pages
1387   1392, seattle, washington, usa.

zoph, b. and knight, k. (2016).
multi-source neural translation.
in naacl hlt 2016.

rico sennrich

id4

72 / 65

hands-on session: install theano

theano depends on bleeding-edge numpy
my suggestion: to avoid version con   icts, install in python virtual
environment

pip install    user virtualenv #install virtualenv
virtualenv virtual_environment #create environemnt
source virtual_environment/bin/activate #start environment
pip install numpy numexpr cython tables theano ipdb #install theano

you may need to install blas library and other dependencies
on debian linux:
sudo apt-get install liblapack-dev libblas-dev gfortran
libhdf5-serial-dev
if you have nvidia gpu, you should install cuda
if you don   t, training will be too slow

rico sennrich

id4

73 / 65

hands-on session: train your own model

sample scripts and con   g at
https://github.com/rsennrich/wmt16-scripts
requirements:

mosesdecoder (just for preprocessing, no installation required)
git clone https://github.com/moses-smt/mosesdecoder
subword-id4 (for bpe segmentation)
git clone https://github.com/rsennrich/subword-id4
nematus (dl4mt fork; for training id4)
git clone https://www.github.com/rsennrich/nematus

set paths in shell scripts, then execute preprocess.sh, then train.sh
to train actual model, use more data, and be patient
script prints status after every 1000 minibatches
       30 min if cuda is set up properly
(we train for 300000   600000 minibatches)

rico sennrich

id4

74 / 65

hands-on session: use pre-trained model

download model(s) from
http://statmt.org/rsennrich/wmt16_systems/
requirements:

mosesdecoder (just for preprocessing, no installation required)
git clone https://github.com/moses-smt/mosesdecoder
subword-id4 (for bpe segmentation)
git clone https://github.com/rsennrich/subword-id4
nematus (dl4mt fork; for decoding)
git clone https://www.github.com/rsennrich/nematus

set paths in translate.sh, then execute:
echo "this is a test." | ./translate.sh

rico sennrich

id4

75 / 65

