topics, trends, and resources 

in 

natural language processing (nlp) 

mohit bansal 
tti-chicago 

 

 

(csc2523,    visual recognition with text   , utoronto, winter 2015     01/21/2015)  

(various slides adapted/borrowed from dan klein   s and chris manning   s course slides) 

preface/disclaimer 

!     this is meant to be a short (2-3 hours), overview/summary style 

lecture on some major topics and trends in nlp, with plenty of 
resource pointers (demos, software, references) 

 
!     hence, it only covers 4-5 topics in some detail, e.g., tagging, 

parsing, coreference, and semantics (distributional, 
compositional, id29, q&a) 

 
!     for some remaining topics, citations and pointers are provided; 
also, please refer to the full nlp courses and books cited at the 
end for detailed material 

inline cites can be matched with full references at the end 

!    
!     comments/suggestions welcome: mbansal@ttic.edu 

nlp examples 

!     id53 

nlp examples 

!     machine translation 

nlp examples 

!     automatic id103 

contents 

!     part-of-speech tagging 
!     syntactic parsing: constituent, dependency, id35, others 
!     coreference resolution 
!     id65: pmi, nns, cca 
!     id152 i: vector-form, deep learning 
!     id152 ii: logic-form, id29, q&a 
!     other topics: id31, machine translation, taxonomies, wsi/

wsd, ner, diachronics, summarization, generation, multimodal, ... 

!     some next topics: humor, sarcasm, idioms, human-like dialog, poetry 

part-of-speech tagging 

!     tag sequence of words with syntactic categories (noun, 

verb, preposition,    ) 

!     useful in itself: 

!     text-to-speech: read, lead, record  
!     lemmatization: saw[v]     see, saw[n]     saw	

!     shallow chunking: grep {jj | nn}* {nn | nns}	


!     useful for downstream tasks (e.g., in parsing, and as 

features in various word/text classification tasks) 

!     demos: http://nlp.stanford.edu:8080/corenlp/  

id32 tagset 

cc 
cd 
dt 
ex 
fw 
in 
jj 
jjr 
jjs 
md 
nn 
nnp 
nnps 
nns 
pos 
prp 
prp$ 
rb 
rbr 
rbs 
rp 
to 
uh 
vb 
vbd 
vbg 
vbn 
vbp 
vbz 
wdt 
wp 
wp$ 
wrb 

conjunction, coordinating 

numeral, cardinal 

determiner 

existential there 

foreign word 

preposition or conjunction, subordinating 

adjective or numeral, ordinal 

adjective, comparative 
adjective, superlative 

modal auxiliary 

noun, common, singular or mass 

noun, proper, singular 
noun, proper, plural 
noun, common, plural 

genitive marker 
pronoun, personal 
pronoun, possessive 

adverb 

adverb, comparative 
adverb, superlative 

particle 

"to" as preposition or infinitive marker 

interjection 

verb, base form 
verb, past tense 

verb, present participle or gerund 

verb, past participle 

verb, present tense, not 3rd person singular 

verb, present tense, 3rd person singular 

wh-determiner 
wh-pronoun 

wh-pronoun, possessive 

wh-adverb 

and both but either or

mid-1890 nine-thirty 0.5 one

a all an every no that the

there 

gemeinschaft hund ich jeux
among whether out on by if
third ill-mannered regrettable

braver cheaper taller
bravest cheapest tallest
can may might will would 

cabbage thermostat investment subhumanity

motown cougar yvette liverpool

americans materials states

undergraduates bric-a-brac averages

' 's 

hers himself it we them

her his mine my our ours their thy your 
occasionally maddeningly adventurously
further gloomier heavier less-perfectly

best biggest nearest worst 

aboard away back by on open through

to 

huh howdy uh whammo shucks heck

ask bring fire see take

pleaded swiped registered saw

stirring focusing approaching erasing
dilapidated imitated reunifed unsettled
twist appear comprise mold postpone

bases reconstructs marks uses

that what whatever which whichever 
that what whatever which who whom

whose 

however whenever where why 

part(cid:882)of(cid:882)speech(cid:3)ambiguity
part-of-speech ambiguities 

!     a word can have multiple parts of speech 
(cid:131) words(cid:3)can(cid:3)have(cid:3)multiple(cid:3)parts(cid:3)of(cid:3)speech

vbd                    vb            
vbn    vbz        vbp        vbz
nnp    nns        nn         nns    cd      nn
fed raises interest rates 0.5 percent

(cid:131) two(cid:3)basic(cid:3)sources(cid:3)of(cid:3)constraint:
!     disambiguating features: lexical identity (word), context, 
morphology (suffixes, prefixes), capitalization, 
gazetteers (dictionaries),     
(cid:131) many(cid:3)more(cid:3)possible(cid:3)features:

(cid:131) grammatical(cid:3)environment
(cid:131) identity(cid:3)of(cid:3)the(cid:3)current(cid:3)word

(cid:131) suffixes,(cid:3)capitalization,(cid:3)name(cid:3)databases(cid:3)(gazetteers),(cid:3)etc   

classic(cid:3)solution:(cid:3)id48s
classic solution: id48s 

w1

wn

w2

(cid:131) in(cid:3)a(cid:3)trigram(cid:3)tagger,(cid:3)states(cid:3)=(cid:3)tag(cid:3)pairs

(cid:131) we(cid:3)want(cid:3)a(cid:3)model(cid:3)of(cid:3)sequences(cid:3)s(cid:3)and(cid:3)observations(cid:3)w

 

<(cid:105),(cid:105)>
s0
s0

< (cid:105), t1>
s1
s1

< t1, t2>
s2
s2

< tn-1, tn>
sn

sn

wn

wn

w1
w1

w2
w2

 

(cid:131) assumptions:

 
!     trigram id48: states = tag-pairs 
(cid:131)
!     estimating transitions: standard smoothing w/ backoff 
(cid:131) usually(cid:3)a(cid:3)dedicated(cid:3)start(cid:3)and(cid:3)end(cid:3)state(cid:3)/(cid:3)word
(cid:131) tag/state(cid:3)sequence(cid:3)is(cid:3)generated(cid:3)by(cid:3)a(cid:3)markov model
!     estimating emissions: use unknown word classes (affixes, 
(cid:131) words(cid:3)are(cid:3)chosen(cid:3)independently,(cid:3)conditioned(cid:3)only(cid:3)on(cid:3)the(cid:3)tag/state
(cid:131) these(cid:3)are(cid:3)totally(cid:3)broken(cid:3)assumptions:(cid:3)why?

states(cid:3)are(cid:3)tag(cid:3)n(cid:882)grams

shapes) and estimate p(t|w) and invert 

!     id136: choose most likely (viterbi) sequence under model  

[brants, 2000] 

id52: other models 

!     discriminative sequence models with richer features: 
memms, crfs (soa ~= 97%/90% known/unknown) 

 
!     universal pos tagset for multilingual and cross-lingual 

tagging and parsing [petrov et al., 2012] 
 12 tags: noun, verb, adj, adv, pron, det, adp, num, conj, prt, ., x 
 
!     unsupervised tagging also works reasonably well! 

[yarowsky et al., 2001; xi and hwa, 2005; berg-kirkpatrick et al., 2010; 
christodoulopoulos et al., 2010; das and petrov, 2011] 

[brill, 1995; ratnaparkhi, 1996; toutanova and manning, 2000; toutanova et al., 2003] 

syntactic parsing -- constituent 

!     phrase-structure parsing or bracketing 

vp 

s 

vbd	


met	


np	


nnp 

 john	


np 

prp 

 her	


!     demos: http://tomato.banatao.berkeley.edu:8080/parser/parser.html  

id140 

!     a context-free grammar is a tuple <n, t, s, r> 

 

n : the set of non-terminals 

phrasal categories: s, np, vp, adjp, etc. 
parts-of-speech (pre-terminals): nn, jj, dt, vb 

 
t : the set of terminals (the words) 
 
s : the start symbol 

often written as root or top 
not usually the sentence non-terminal s 

 
r : the set of rules 

of the form x     y1 y2     yk, with x, yi     n 
examples: s     np vp,   vp     vp cc vp 
also called rewrites, productions, or local trees 

id140 

!     a pid18: 

x) 

 
!     adds a top-down production id203 per rule p(y1 y2     yk | 

!     allows us to find the    most probable parse    for a sentence 
!     the id203 of a parse is just the product of the 

probabilities of the individual rules 

treebank pid18 

treebank(cid:3)pid18s

[charniak 96]

       need a pid18 for broad coverage parsing 
(cid:131) use(cid:3)pid18s(cid:3)for(cid:3)broad(cid:3)coverage(cid:3)parsing
       extracting a grammar right off the trees is not effective: 
(cid:131) can(cid:3)take(cid:3)a(cid:3)grammar(cid:3)right(cid:3)off(cid:3)the(cid:3)trees(cid:3)(doesn   t(cid:3)work(cid:3)well):

root (cid:111) s
s (cid:111) np vp .
np (cid:111) prp
vp (cid:111) vbd adjp

root     s 
 1 
s     np vp .
 1 
np     prp  
 1 
vp     vbd adjp  1 
   .. 

 
1
 
1
 
1
1

   ..

 

model
baseline

model 
baseline 

f1
72.0

f1 
72.0 

[charniak, 1996] 

3

conditional(cid:3)independence?

grammar refinement 

 

 

-she	


-noise	


!     conditional independence assumptions often too strong! not every 

(cid:131) not(cid:3)every(cid:3)np(cid:3)expansion(cid:3)can(cid:3)fill(cid:3)every(cid:3)np(cid:3)slot

np expansion can fill every np slot 

(cid:131) a(cid:3)grammar(cid:3)with(cid:3)symbols(cid:3)like(cid:3)   np   (cid:3)won   t(cid:3)be(cid:3)context(cid:882)free
(cid:131) statistically,(cid:3)conditional(cid:3)independence(cid:3)too(cid:3)strong

!     better results by enriching the grammar e.g.,  

 

!     lexicalization [collins, 1999; charniak, 2000] 

conditional(cid:3)independence?

grammar refinement 

 

 

^s	


^vp	


!     conditional independence assumptions often too strong! not every 

(cid:131) not(cid:3)every(cid:3)np(cid:3)expansion(cid:3)can(cid:3)fill(cid:3)every(cid:3)np(cid:3)slot

np expansion can fill every np slot 

(cid:131) a(cid:3)grammar(cid:3)with(cid:3)symbols(cid:3)like(cid:3)   np   (cid:3)won   t(cid:3)be(cid:3)context(cid:882)free
(cid:131) statistically,(cid:3)conditional(cid:3)independence(cid:3)too(cid:3)strong

!     better results by enriching the grammar e.g.,  

 

!     lexicalization [collins, 1999; charniak, 2000] 
 ! markovization, manual tag-splitting [johnson, 1998; klein & manning, 2003] 

conditional(cid:3)independence?

grammar refinement 

 

 

-3	


-7	


!     conditional independence assumptions often too strong! not every 

(cid:131) not(cid:3)every(cid:3)np(cid:3)expansion(cid:3)can(cid:3)fill(cid:3)every(cid:3)np(cid:3)slot

np expansion can fill every np slot 

(cid:131) a(cid:3)grammar(cid:3)with(cid:3)symbols(cid:3)like(cid:3)   np   (cid:3)won   t(cid:3)be(cid:3)context(cid:882)free
(cid:131) statistically,(cid:3)conditional(cid:3)independence(cid:3)too(cid:3)strong

!     better results by enriching the grammar e.g.,  

 

!     lexicalization [collins, 1999; charniak, 2000] 
 ! markovization, manual tag-splitting [johnson, 1998; klein & manning, 2003] 
!     latent tag-splitting [matsuzaki et al., 2005; petrov et al., 2006] 

cky parsing algorithm (bottom-up) 

   bestscore(s) 

x 

 for (i : [0,n-1]) 
   for (x : tags[s[i]]) 
     score[x][i][i+1] = tagscore(x,s[i]) 
 for (diff : [2,n]) 
   for (i : [0,n-diff]) 
     j = i + diff 
     for (x->yz : rule) 
       for (k : [i+1, j-1]) 
         score[x][i][j] = max{score[x][i][j], score(x->yz) 
 
 
 
 

  *score[y][i][k]  
  *score[z][k][j]} 

i                       k                      j 

 
 

 
 

 
 

 
 

 
 

 
 

 
 

 
 

 
 

 
 

 
 

y 

z 

[cocke, 1970; kasami, 1965; younger, 1967] 

some results 

!     collins, 1999 ! 88.6 f1 (generative lexical) 
! charniak and johnson, 2005 ! 89.7 / 91.3 f1 

(generative lexical / reranking) 

! petrov et al., 2006 ! 90.7 f1 (generative unlexical) 
! mcclosky et al., 2006     92.1 f1 (generative + 

reranking + self   training) 

syntactic parsing -- dependency 

!     predicting directed head-modifier relationship pairs 

prep 

num 

dobj 

num 

pobj 

raising         $      30      million     from     debt 

!     demos: http://nlp.stanford.edu:8080/corenlp/  

syntactic parsing -- dependency 

!     pure (projective, 1st order) id33 is only 

cubic [eisner, 1996] 

!     non-projective id33 useful for czech & 

other languages     mst algorithms [mcdonald et al., 2005] 

parsing: other models and methods 

!     id35 [steedman, 1996, 2000; clark and curran, 

2004] 

2003] 

1990; bod, 1993; goodman, 1996; bansal and klein, 2010] 

!     transition-based id33 [yamada and matsumoto, 2003; nivre, 
 !     tree-insertion grammar, dop [schabes and waters, 1995; hwa, 1998; scha, 
 !     tree-adjoining grammar [resnik, 1992; joshi and schabes, 1998; chiang, 2000] 
!     shift-reduce parser [nivre and scholz, 2004; sagae and lavie, 2005] 
!     other: reranking, a*, k-best, self-training, co-training, system 
combination, cross-lingual transfer [sarkar, 2001; steedman et al., 2003; 
charniak and johnson, 2005; hwa et al., 2005; huang and chiang, 2005; mcclosky et al., 
2006; fossum and knight, 2009; pauls and klein, 2009; mcdonald et al., 2011] 

!     other demos: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/demo, 

http://4.easy-id35.appspot.com/  

world knowledge is important 

clean the dishes in 

the sink.	


web features for syntactic parsing 

dependency: 

they considered running the ad during the super bowl. 

constituent: 
vp

vbd

considered

vbg

running

vp

s

vp

vbg

np

running

the ad

pp

in

np

during

the super bowl

s

vp

np

the ad

vbd

considered

pp

in

np

during

the super bowl

[nakov and hearst 2005; pitler et al., 2010; bansal and klein, 2011] 

web features for syntactic parsing 

they considered running the ad during the super bowl. 

web ngrams 

count(running it during)         >          count(considered it during)	


!     7-10% relative error reduction over 90-92% parsers 

s
a
u

 

92.5 

91.5 

90.5 

mcdonald & pereira 2006 

us 

[bansal and klein, 2011] 

unsup. representations for parsing 

!     discrete or continuous, trained on large amounts of context 

!     brown (brown et al., 1992): 

p

obj

0

1

00

01

10

11

000

001

010

011

100

101 110

111

ms. haag plays elianti

.

figure 1: an example of a labeled dependency tree. the
tree contains a special token    *    which is always the root
of the tree. each arc is directed from head to modi   er and
has a label describing the function of the attachment.

apple pear apple ibm bought
in
figure 2: an example of a brown word-cluster hierarchy.
each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
!     skipgram (mikolov et al., 2013): 
indicates a left branch and 1 indicates a right branch.

skip 

run of

mikolov et al., 2013!

 

and id91, section 3 describes the cluster-based
features, section 4 presents our experimental results,
section 5 discusses related work, and section 6 con-

recent work (buchholz and marsi, 2006; nivre
et al., 2007) has focused on id33.
dependency syntax represents syntactic informa-

input 

projection 

output 

lowing maximization:

parse(x; w) = argmax

y y(x) xr y

w(t-2) 
w    f(x, r)
w(t-1) 

context  
window 

w(t) 

 
above, we have assumed that each part is scored
w 
 
by a linear model with parameters w and feature-
 
mapping f(  ). for many different part factoriza-
tions and structure domains y(  ), it is possible to
solve the above maximization ef   ciently, and several
recent efforts have concentrated on designing new
maximization algorithms with increased context-
sensitivity (eisner, 2000; mcdonald et al., 2005b;

few mins. vs. days/weeks/months!! 

w(t+1) 

w(t+2) 

apple  
 
pear
 apple  

 !  
 !  
 !  

 000 
 001 
 010 

 

 apple  !  [0.65  0.15  -0.21  0.15  0.70  -0.90] 
 pear
 !  [0.51  0.05  -0.32  0.20  0.80  -0.95] 
 apple  !  [0.11  0.33  0.51  -0.05  -0.41  0.50] 

[koo et al., 2008; bansal et al., 2014] 

unsup. representations for parsing 

!     condition on dependency context instead of linear, then 

convert each dependency to a tuple: 

dep label	
   
[pmod<l>       regulation<g>     of       safety   pmod<l>] 

grandparent	
   

dep label	
   

parent	
   

child	
   

[mr., mrs., ms., prof., iii, jr., dr.] 
[jeffrey, william, dan, robert, stephen, peter, john, richard, ...] 
[portugal, iran, cuba, ecuador, greece, thailand, indonesia,    ] 

[his, your, her, its, their, my, our] 
[your, our, its, my, his, their, her] 
[truly, wildly, politically, financially, completely, potentially, ...] 

!     10% rel. error reduction 

over 90-92% parsers 

 

s
a
u

92.5 

91.5 

90.5 

mcdonald & pereira 2006 

us 

[bansal et al., 2014] 

coreference resolution 

president barack obama received the serve america 
act  after  congress     vote.  he  signed  the  bill  last 
thursday. the president said it would greatly increase 
service opportunities for the american people. 

!     mentions to entity/event clusters 
!     demos: h#p://nlp.stanford.edu:8080/corenlp/process 

mention-pair models 

president barack obama   received   the   serve america act    after   congress        vote .   he    signed   the bill        

a1

pair-wise classification approach:	
   

a3
(a1,   m)	
   

a2

a(m)
features	
   f	
   

wtf	
   

pair-wise 
classifier 

m
coref(a1,   m)

[soon et al. 2001, ng and cardie 2002; bengtson and roth, 2008; stoyanov et al., 2010] 

mention-pair model 

for each mention m, 

m

[soon et al. 2001, ng and cardie 2002; bengtson and roth, 2008; stoyanov et al., 2010] 

standard features 

npi 

npj 

feature 
soon_str 
number 

gender 

appositive 

description 

do the strings match after removing determiners ? 
do npi and npj agree in number ? 
do npi and npj agree in gender ? 
are the nps in an appositive relationship ? 

id138_class  do npi and npj have the same id138 class ? 

type 
lexical 

grammatical 

semantic 

positional 

sentnum 

alias 

is one np an alias of the other ? 
distance between the nps in terms of # of sentences 

!     weaknesses: all pairs, transitivity/independence errors 

(he     obama     she), insufficient information 

[soon et al. 2001, ng and cardie 2002; bengtson and roth, 2008; stoyanov et al., 2010] 

entity-centric models 

!     each coreference decision is globally informed by 

previously clustered mentions and their shared attributes 

lee et al. deterministic coreference resolution based on entity-centric, precision-ranked rules

!     lee et al., 2013   s 

deterministic (rule-based) 
system: multiple, cautious 
sieves from high to low 
precision 

 
! durrett et al., 2013   s 
entity-level model is 
discriminative, 
probabilistic using factor 
graphs and bp 

figure 1
the architecture of our coreference system.

[haghighi and klein, 2009; lee et al., 2013; durrett et al., 2013] 

mention-ranking models (learned) 

!     log-linear model to select at most 1 antecedent for 

each mention or determine that it begins a new cluster 

men9on\ranking%architecture

p r(ai = a|x) / exp(w>f (i, a, x))

[1stword=a]
[length=2]

...

[voters;they]
[nom\pronoun]

...

a1
new

a2
new

1

a3
new

1
2

a4
new

1
2
3

[voters]1%agree%when%[they]1%are%given%[a%chance]2%to%decide%if%[they]1%...%

denis%and%baldridge%(2008),%durre4%et%al.%(2013)

[denis and baldridge, 2008; durrett and klein, 2013] 

adding knowledge to coref 

!     external corpora: web, wikipedia, yago, framenet, gender/

number/person lists/classifiers, 3d images, videos 

!     methods:  

!     self-training, id64 
!     co-occurrence, distributional, and pattern-based features 
!     entity linking 
!     visual cues from 3d images and videos 

 
! daum   iii and marcu, 2005; markert and nissim, 2005; bergsma 

and lin, 2006; ponzetto and strube, 2006; haghighi and klein, 
2009; kobdani et al., 2011; rahman and ng, 2011; bansal and 
klein, 2012; durrett and klein, 2014; kong et al., 2014; 
ramanathan et al., 2014 

web features for coreference 

count(obama * president)    vs   count(jobs * president)	


when obama met jobs , the president discussed the    	


[bansal and klein, 2012] 

web features for coreference 

count(obama signed bills)   vs   count(jobs signed bills)	


when obama met jobs , the     he signed bills that    	


setup: standard train/dev/test splits on ace 2004, 2005 

results 

results 

setup: standard train/dev/test splits on ace 2004, 2005 

 
1
f

 
 

c
u
m

72 

71 

70 

69 

68 

69.5 

!

e
n

i
l

e
s
a
b

71.3 

70.7 

70.4 

69.8  70.0 

!

c
c
o
o
c
+

!
t
s
r
a
e
h
+

!

y
t
i
t

n
e
+

!
r
e
t
s
u
c
+

l

!

n
u
o
n
o
r
p
+

[bansal and klein, acl 2012] 

72 

68 

 

1
f

 
 

c
u
m

69.1 

67.0 

results 

64 

setup: standard train/dev/test splits on ace 2004, 2005 

haghighi & klein, 2010 

us 

82 

78 

 
1
f

 
 
3
b

74 

[bansal and klein, acl 2012] 

80.0 

77.0 

haghighi & klein, 2010 

us 

[bansal and klein, acl 2012] 

[bansal and klein, 2012] 

what are you talking about? text-to-image coreference

chen kong1

dahua lin3

visual cues for coreference 

raquel urtasun2,3

sanja fidler2,3

mohit bansal3

1tsinghua university,

2university of toronto,

3tti chicago

kc10@mails.tsinghua.edu.cn, {dhlin,mbansal}@ttic.edu,{fidler,urtasun}@cs.toronto.edu

!     joint coreference and 3d image recognition 

abstract

in this paper we exploit natural sentential descriptions
of rgb-d scenes in order to improve 3d id29.
importantly, in doing so, we reason about which particular
object each noun/pronoun is referring to in the image. this
allows us to utilize visual information in order to disam-
biguate the so-called coreference resolution problem that
arises in text. towards this goal, we propose a structure
prediction model that exploits potentials computed from text
and rgb-d imagery to reason about the class of the 3d ob-
jects, the scene type, as well as to align the nouns/pronouns
with the referred visual objects. we demonstrate the effec-
tiveness of our approach on the challenging nyu-rgbd v2
# words min # sent max sent min words max words
dataset, which we enrich with natural lingual descriptions.
we show that our approach signi   cantly improves 3d de-
tection and scene classi   cation accuracy, and is able to re-
liably estimate the text-to-image alignment. furthermore,
by using textual and visual information, we are also able to
successfully deal with coreference in text, improving upon
the state-of-the-art stanford coreference system [15].

table 2. statistics per description.

83%

0.48

144

10

6

0.0

0.0

.02

0.0

0.0

0.0

.04

0.0

.12

0.0

0.0

.15

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

# nouns of interest # pronouns # scene mentioned scene correct

0.0

0.0

1.0

0.0

0.0

0.0

.05

.02

0.0

0.0

0.0

0.0

0.0

0.0

0.0

.01

.01

0.0

.02

0.0

0.0

0.0

.95

0.0

.05

0.0

0.0

0.0

imagine a scenario where you wake up late on a satur-
day morning and all you want is for your personal robot to
bring you a shot of bloody mary. you could say    it is in the

0.0

0.0

0.0

0.0

.38

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

.06

0.0

0.0

.12

0.0

0.0

0.0

0.0

.33

0.0

0.0

0.0

f-measure

recall
94.2%
85.7%
93.0%
96.0%

precision
94.7%
85.7%
64.2%
55.8%

object class

scene
color
size

94.4%
85.7%
75.9%
70.6%
table 3. parser accuracy (based on stanford   s parser [31])
figure 1. our model uses lingual descriptions (a string of depen-
dent sentences) to improve visual scene parsing as well as to de-
termine which visual objects the text is referring to. we also deal
f1
with coreference within text (e.g., pronouns like    it    or    them   ).
76.15 75.59
70.02 78.15
system is key for the deployment of such systems. to date,
however, attempts to utilize more complex natural descrip-
tions are rare. this is due to the inherent dif   culties of both
tributes for the linked pronouns as well. our annotation
natural language processing and visual recognition, as well

table 4. co-reference accuracy of [15] and our model.

method
stanford [15]
ours

f1
62.59 62.07
51.08 63.44

precision recall

precision recall

61.56
83.69

75.05
88.42

muc

b3

[kong, lin, bansal, urtasun, and fidler, 2014] 

id65 

!     words occurring in similar context have similar 
linguistic behavior (meaning) [harris, 1954; firth, 1957] 

!     traditional approach: context-counting vectors 

!     count left and right context in window 
!     reweight with pmi or llr 
!     reduce dimensionality with svd or nnmf 
 [pereira et al., 1993; lund & burgess, 1996; lin, 1998; lin and pantel, 2001; 
 sahlgren, 2006; pado & lapata, 2007; turney and pantel, 2010; baroni and 
 lenci, 2010] 

 

 
!     more word representations: hierarchical id91 

based on bigram lm ll  

root

p

obj

      [brown et al., 1992] 

nmod

sbj

ms. haag plays elianti

.

*

figure 1: an example of a labeled dependency tree. the
tree contains a special token    *    which is always the root
of the tree. each arc is directed from head to modi   er and
has a label describing the function of the attachment.

0

1

00

01

10

11

000

001

010

011

100

101 110

111

apple pear apple ibm bought
in
figure 2: an example of a brown word-cluster hierarchy.
each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
indicates a left branch and 1 indicates a right branch.

run of

id65 -- nns 

!     newer approach: context-predicting vectors (nns) 

!     senna [collobert and weston, 2008; collobert et al., 2011]: multi-layer 

dnn w/ ranking-loss objective; bow and sentence-level feature 
layers, followed by std. nn layers. similar to [bengio et al., 2003]. 

bengio, ducharme, vincent and jauvin

i-th output = p(wt = i | context)

. . .

softmax

. . .

most  computation here

. . .

tanh

. . .

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

index for

figure 1: neural architecture: f (i,wt 1,       ,wt n+1) = g(i,c(wt 1),       ,c(wt n+1)) where g is the

id65 -- nns 

!     huang [huang et al., 2012]: add global, document-level context 

id65 -- nns 

!     cbow, skip, id97 [mikolov et al., 2013]: simple, super-fast nn w/ no 

hidden layer. continuous bow model predicts word given context, skip-
gram model predicts surrounding words given current word 

 
 

!     other: [mnih and hinton, 2007; turian et al., 2010] 

figure 1: new model architectures. the cbow architecture predicts the current word based on the
context, and the skip-gram predicts surrounding words given the current word.

!     comparison of count vs. predict (winner) [baroni et al., 2014] 
!     demos: h#ps://code.google.com/p/id97,	
   

r words from the future of the current word as correct labels. this will require us to do r     2
word classi   cations, with the current word as input, and each of the r + r words as output. in the
following experiments, we use c = 10.

h#p://metaop<mize.com/projects/wordreprs/, h#p://ml.nec-     labs.com/senna/	
   

4 results

to compare the quality of different versions of word vectors, previous papers typically use a table
showing example words and their most similar words, and understand them intuitively. although
it is easy to show that word france is similar to italy and perhaps some other countries, it is much
more challenging when subjecting those vectors in a more complex similarity task, as follows. we

w(t-2)w(t+1)w(t-1)w(t+2)w(t)sum       input         projection         outputw(t)          input         projection      outputw(t-2)w(t-1)w(t+1)w(t+2)                   cbow                                                   skip-gramid65 

!     other approaches: id106, e.g., cca 

!     word-context correlation [dhillon et al., 2011, 2012] 
!     multilingual correlation [faruqui and dyer, 2014] 
 

!     some current/next directions: train task-tailored 
embeddings to capture specific types of similarity/
semantics, e.g., 
!     dependency context [bansal et al., 2014, levy and goldberg, 2014] 
!     predicate-argument structures [hashimoto et al., 2014; 

madhyastha et al., 2014] 

!     lexicon evidence (ppdb, id138, framenet) [xu et 

al., 2014; yu and dredze, 2014; faruqui et al., 2014] 

id152 i: nns 

!     composing, combining word vectors to representations 

for longer units: phrases, sentences, paragraphs,     

!     initial approaches: point-wise sum, multiplication    

[mitchell and lapata, 2010; blacoe and lapata, 2012] 

!     vector-matrix compositionality [baroni and zamparelli, 2010; 
zanzotto et al., 2010; grefenstette and sadrzadeh, 2011; socher et al., 2011; 
yessenalina and cardie, 2011] 

!     linguistic information added via say parses [socher et al., 

2011b, 2012, 2013a, 2013b, 2014; hermann and blunsom, 2013] 

id152 i: nns 

! socher et al., 2011: recursive autoencoders 

(unsupervised) on constituent parse trees 

 
!     the unfolding autoencoder which tries to reconstruct all 

leaf nodes underneath each node.  

id152 i: nns 

nsubj
dobj

prep
pobj

pobj

root

det

partmod

det

prep

poss

det

a

man

wearing

a

helmet

jumps

! socher et al., 2013a, 2014: id56s on constituent and 

figure 2: example of a full dependency tree for a longer sentence. the dt-id56 will compute vector representations
at every word that represents that word and an arbitrary number of child nodes. the    nal representation is computed
at the root node, here at the verb jumps. note that more important activity and object words are higher up in this tree
structure.

dependency parse trees 

his

bike

near

a

beach

on

supervised model of huang et al. (2012) which can
learn single word vector representations from both
local and global contexts. the idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for window-document pairs where
one word is replaced by a random word. when
such a network is optimized via id119 the
derivatives backpropagate into a id27
matrix a which stores word vectors as columns. in
figure 3: example of a syntactically untied id56
order to predict correct scores the vectors in the ma-
in which the function to compute a parent vector
trix capture co-occurrence statistics. we use d = 50
in all our experiments. the embedding matrix x
depends on the syntactic categories of its children
is then used by    nding the column index i of each
which we assume are given for now.
word: [w] = i and retrieving the corresponding col-
umn xw from x. henceforth, we represent an input
sentence s as an ordered list of (word,vector) pairs:
pid18. hence, cvgs combine discrete, syntactic
s = ((w1, xw1), . . . , (wm, xwm)).
rule probabilities and continuous vector composi-
the sequence of words (w1, . . . , wm) is
tions. the idea is that the syntactic categories of

next,

figure 3: example of a dt-id56 tree structure for com-
puting a sentence representation in a bottom up fashion.

dency tree for this sentence can be summarized by
the following set of (child, parent) edges: d =
{(1, 2), (2, 0), (3, 2), (4, 2), (5, 4)}.
the dt-id56 model will compute parent vectors
at each word that include all the dependent (chil-
dren) nodes in a bottom up fashion using a com-
positionality function g    which is parameterized by
all the model parameters    . to this end, the algo-

(a, a=       )        (b, b=       )       (c, c=       )p(1), p(1)=        p(2), p(2)=        = f   w(b,c)bc= f   w(a,p  )ap(1)(1)students                 bikes           nightride at          x1x2x3x4x5h1h2h3h4h5christopher d. manning, andrew y. ng and christopher potts

stanford university, stanford, ca 94305, usa

richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu

id152 i: nns 

{jeaneis,manning,cgpotts}@stanford.edu

! socher et al., 2013b: sentiment compositionality 

semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. to remedy this, we introduce a
sentiment treebank. it includes    ne grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-

   

0

   

0

this

0

   lm

   

   

0

does

0

n   t

+

care

0

.

+

0

0

about

+

+

0

0

+

0

+

0

of

+

+

+

+ +

other

kind

intelligent

humor

+

0

,

+

+

cleverness

0

or

0

any

0

wit

!     demos: h#p://nlp.stanford.edu:8080/sen<ment/rntndemo.html 
figure 1: example of the recursive neural tensor net-
work accurately predicting 5 sentiment classes, very neg-

[yessenalina and cardie, 2011; socher et al., 2013b] 

id152 i: nns 

!     various other approaches: [das and smith, 2009; collobert et al., 
2011; grefenstette et al., 2013; hashimoto et al., 2014; madhyastha et al., 2014; 
chen and manning, 2014] 

 
!     new deep learning based generation: end-to-end mt, 
parsing, id134 for images, videos [sutskever 
et al., 2014; vinyals et al., 2014a, 2014b; karpathy and fei-fei, 2014; kiros et al., 
2014; donahue et al., 2014; fang et al., 2014; venugopalan et al., 2014] 

!     demos: h#p://deeplearning.net/demos/,	
   

h#p://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/,	
   
h#ps://www.metamind.io/ 

id152 ii: logic form 

!     logic-based, id29 
!     useful for q&a, ie, grounding, comprehension tasks 

(summarization, reading tasks) 

!     a lot of focus on id53 

!     demos: h#p://demo.ark.cs.cmu.edu/parse,	
   www.google.com,	
   facebook	
   

graph	
   search 

id53 

!     initial approaches to q&a: pattern matching, pattern 

learning, query rewriting, information extraction 

!     next came a large-scale, open-domain ie system like 

articles

ibm watson	
   

[ferrucci et al., 2010] 

deep q&a: id29 

!     complex, free-form, multi-clause questions 

deep q&a: id29 

!     complex, free-form, multi-clause questions 

id29: logic forms 
parse tree with associated 
semantics 

!     parsing with logic (booleans, individuals, functions) and 

lambda forms 

sentence 

loves(john,mary) 

noun phrase 

john 

verb phrase 

  x.loves(x,mary) 

name 
john 

verb 

  y.  x.loves(x,y) 

   john    
john 

   loves    

  y.  x.loves(x,y) 

noun phrase 

name 
mary 

   mary    
mary 

[wong and mooney, 2007; zettlemoyer and collins, 2007; poon and domingos, 2009;  
artzi and zettlemoyer, 2011, 2013; kwiatkowski et al., 2013; cai and yates, 2013;  
berant et al., 2013; poon 2013; berant and liang, 2014; iyyer et al., 2014] 

id29 ideas 

!     various recent ideas/extensions:  

!     unsupervised sp (id91 lambda forms) 
!     grounded usp (via databases) 
!     dependency-based id152 (dcs) 
!     id35 
!     id64 w/ conversations 
!     on-the-fly ontology matching 
!     id53 on freebase 
!     id141 
!     id56s for q&a 
!     comparison with ie approaches 

[wong and mooney, 2007; zettlemoyer and collins, 2007; poon and domingos, 2009;  
artzi and zettlemoyer, 2011, 2013; kwiatkowski et al., 2013; cai and yates, 2013;  
berant et al., 2013; poon 2013; berant and liang, 2014; iyyer et al., 2014; yao and van durne, 2014] 

in this paper, we train a semantic parser that
scales up to freebase. instead of relying on
annotated logical forms, which is especially
expensive to obtain at large scale, we learn
from question-answer pairs. the main chal-
lenge in this setting is narrowing down the
huge number of possible logical predicates for
a given question. we tackle this problem in
two ways: first, we build a coarse mapping
from phrases to predicates using a knowledge
base and a large text corpus. second, we
use a bridging operation to generate additional
predicates based on neighboring predicates.
on the dataset of cai and yates (2013), despite
not having annotated logical forms, our sys-

id29 on freebase 

 

occidental college, columbia university

execute on database

type.university u education.barackobama

type.university

education

alignment

bridging

 
 
 
 
 
 
 
 
 
mapping questions to answers via latent logical forms. to narrow down the logical 
predicate space, they use a (i) coarse alignment based on freebase and a text corpus and 
(ii) a bridging operation that generates predicates compatible with neighboring predicates.  

figure 1: our task is to map questions to answers via la-
tent logical forms. to narrow down the space of logical
predicates, we use a (i) coarse alignment based on free-

alignment

barackobama

obama

college

which

go to

did

?

[berant et al., 2013] 

id29 via id141 

 

what party did clay establish?

paraphrase model

what political party founded by henry clay?

... what event involved the people henry clay?

type.event u involved.henryclay

type.politicalparty u founder.henryclay ...

 
 
 
 
 
 
 
 
figure 1:
for each candidate logical form (red), they generate canonical utterances (purple). the 
id29 via id141: for each
model is trained to paraphrase the input utterance (green) into the canonical utterances 
candidate logical form (in red), we generate canonical utter-
associated with the correct denotation (blue).  
ances (in purple). the model is trained to paraphrase the in-
put utterance (in green) into the canonical utterances associ-
ated with the correct denotation (in blue).

whig party

[berant and liang, 2014] 

id29 via ontology matching 

 

direct

(traditional)

underspeci   ed

logical
form

ontology
matching

(kwiatkowski et al. 2013)

logical
form

utterance

paraphrase

(berant and liang, 2014) 

canonical
utterance
(this work)

 
 
 
 
 
 
 
the main challenge in id29 is the mismatch between language and the 
knowledge base. (a) traditional: map utterances directly to logical forms, (b) kwiatkowski 
et al. (2013): map utterance to intermediate, underspecified logical form, then perform 
ontology matching to handle the mismatch, (c) berant and liang (2014): generate 
intermediate, canonical text utterances for logical forms, then use paraphrase models. 

figure 2: the main challenge in id29 is cop-
ing with the mismatch between language and the kb. (a)
traditionally, id29 maps utterances directly to
logical forms. (b) kwiatkowski et al. (2013) map the utter-
ance to an underspeci   ed logical form, and perform ontology
matching to handle the mismatch. (c) we approach the prob-
lem in the other direction, generating canonical utterances for

[kwiatkowski et al., 2013; berant and liang, 2014] 

free917 (cai and yates, 2013), which has
917 questions manually authored by annota-
tors. on webquestions, we obtain a relative
improvement of 12% in accuracy over
state-of-the-art, and on free917 we match the
current best performing system.
code of our system parasempre is released
at
http://www-nlp.stanford.edu/
software/sempre/.
2 setup
our task is as follows: given (i) a knowledge
base k, and (ii) a training set of question-answer
pairs {(xi, yi)}n
maps new questions x to answers y via latent log-

other topics 

!     machine translation [brown et al., 1990, 1993; vogel et al., 1996; wu, 1997; papineni et al., 

2002; och and ney, 2002; och, 2003; galley et al., 2004; koehn, 2004; chiang et al., 2005; liang et al., 2006a, 
2006b; marcu et al., 2006; koehn et al., 2007; gimpel and smith, 2008; mi et al., 2008; chiang, 2010; galley and 
manning, 2010; bansal et al., 2011; kalchbrenner and blunsom, 2013; vaswani et al., 2013; auli et al., 2013; 
devlin et al., 2014; sutskever et al., 2014,    many more] 
 (demos: h#p://www.statmt.org/moses/?n=public.demos,	
   h#p://lisa.iro.umontreal.ca/mt-     demo,	
   
	
   h#ps://translate.google.com/)	
   

	
   

	
   

 
!     id31 [hatzivassiloglou and mckeown, 1997; das and chen, 2001; tong, 2001; 

turney, 2002; pang et al., 2002; nenkova and passonneau, 2004; wiebe et al., 2005; thomas et al., 2006; 
snyder and barzilay, 2007; ding et al., 2008; pang and lee, 2008; bansal et al., 2008; nakagawa et al., 2010; 
liu, 2012; socher et al., 2011, 2013; ...] 
 (demos: h#p://nlp.stanford.edu:8080/sen<ment/rntndemo.html,	
   h#p://text-     processing.com/demo/sen<ment/) 

 
!     summarization [teufel and moens, 1997; carbonell and goldstein, 1998; knight and marcu, 

2001; white et al., 2001; lin, 2003, 2004; daum   iii, 2006; zajic, et al., 2006; shen et al., 2007; yih et al., 2007; 
schilder and kondadadi, 2008; martins and smith, 2009; gillick and favre, 2009; woodsend and lapata, 2010; 
wang and cardie, 2012; hong and nenkova, 2014;    ] 
 (demos: https://semantria.com/demo, http://www.summly.com/) 

!     taxonomy/ontology induction [widdows, 2003; snow et al., 2006; yang and 

callan, 2009; kozareva and hovy, 2010; poon and domingos, 2010; navigli et al., 2011; lao et al., 2012; 
fountain and lapata, 2012; bansal et al., 2014;    ] 

[*not exhaustive, various other references] 

many other topics     

!     id38 
 
!     id51/induction, ner 

!     id96 and text classification/categorization 
!     discourse 
 
! diachronics (historical linguistics, language 

reconstruction) 

!     decipherment and ocr 

some next topics 

!     metaphors, idioms 

 

you: 
siri: 

 i am under the weather today.	

 the weather   s looking good today     	


!     sarcasm, insult, irony, humor 
 
!     generating realistic stories, poetry,     

!     human-like id71 (turing test) 

resources: software and demos 

!    

!    

id52: http://nlp.stanford.edu/software/tagger.shtml, https://code.google.com/p/universal-pos-tags/, 
http://www.ark.cs.cmu.edu/tweetnlp/,     
parsing: https://code.google.com/p/berkeleyparser/, http://nlp.stanford.edu/software/lex-parser.shtml, 
https://github.com/bllip/bllip-parser, http://www.cs.columbia.edu/~mcollins/code.html,  
http://www.ark.cs.cmu.edu/turboparser/ 

!     coreference: http://nlp.stanford.edu/software/dcoref.shtml, http://nlp.cs.berkeley.edu/projects/coref.shtml, 

http://www.cs.utah.edu/nlp/reconcile/, http://www.bart-coref.org/, 
http://cogcomp.cs.illinois.edu/page/software_view/coref 

 !     id27s: https://code.google.com/p/id97, http://metaoptimize.com/projects/wordreprs/, 

http://ml.nec-labs.com/senna/, http://nlp.stanford.edu/projects/glove/, 
http://ttic.uchicago.edu/~mbansal/data/syntacticembeddings.zip, 
http://www.socher.org/index.php/main/
improvingwordrepresentationsviaglobalcontextandmultiplewordprototypes, 
http://www.wordvectors.org/web-eacl14-vectors/de-projected-en-512.txt.gz 

 !     compositional embeddings: http://nlp.stanford.edu/sentiment/, http://nal.co/did98, 

http://www.socher.org/index.php/main/parsingwithcompositionalvectorgrammars, 
http://www.socher.org/index.php/main/
dynamicpoolingandunfoldingrecursiveautoencodersforparaphrasedetection 
semantic paring, q&a (id152 ii): http://www-nlp.stanford.edu/software/sempre/, 
https://bitbucket.org/yoavartzi/spf, https://code.google.com/p/jacana/, http://cs.umd.edu/~miyyer/qblearn/, 
http://alchemy.cs.washington.edu/usp/, http://www.ark.cs.cmu.edu/semafor/,  

!    

!     most of the demo links are inline with each topic   s slides 

resources: courses and books 

!     berkeley nlp course: http://www.cs.berkeley.edu/~klein/cs288/fa14/ 
!     cmu nlp course: www.ark.cs.cmu.edu/nlp 
 !     stanford nlp course: http://web.stanford.edu/class/cs224n 
 !     many others: brown, columbia, cornell, jhu, mit, maryland, upenn,     
 !     books:  

!

jurafsky and martin, speech and language processing, 2nd edition, 
2009 

!     manning and shuetze, foundations of statistical natural language 

processing 

!     many others references (in the material above)     

references 

artzi, yoav, and luke zettlemoyer. "id64 semantic parsers from conversations." proceedings of the conference on empirical methods 
in natural language processing. association for computational linguistics, 2011. 
 
artzi, yoav, and luke zettlemoyer. "weakly supervised learning of semantic parsers for mapping instructions to actions." tacl 1 (2013): 
49-62. 
 
auli, michael, michel galley, chris quirk, and geoffrey zweig. "joint language and translation modeling with recurrent neural networks." in 
emnlp, pp. 1044-1054. 2013. 
 
bansal, mohit, claire cardie, and lillian lee. "the power of negative thinking: exploiting label disagreement in the min-cut classification 
framework." incoling (posters), pp. 15-18. 2008. 
 
bansal, mohit, and dan klein. "simple, accurate parsing with an all-fragments grammar." proceedings of the 48th annual meeting of the 
association for computational linguistics. association for computational linguistics, 2010. 
 
bansal, mohit, and dan klein. "web-scale features for full-scale parsing."proceedings of the 49th annual meeting of the association for 
computational linguistics: human language technologies-volume 1. association for computational linguistics, 2011. 
 
bansal, mohit, chris quirk, and robert c. moore. "gappy phrasal alignment by agreement." in proceedings of the 49th annual meeting of the 
association for computational linguistics: human language technologies-volume 1, pp. 1308-1317. association for computational 
linguistics, 2011. 
 
bansal, mohit, and dan klein. "coreference semantics from web features."proceedings of the 50th annual meeting of the association for 
computational linguistics: long papers-volume 1. association for computational linguistics, 2012. 
 
bansal, mohit, david burkett, gerard de melo, and dan klein. "structured learning for taxonomy induction with belief propagation." acl, 2014 
 
bansal, mohit, kevin gimpel, and karen livescu. "tailoring continuous word representations for id33." proceedings of the 
annual meeting of the association for computational linguistics. 2014. 
 
baroni, marco, and alessandro lenci. "distributional memory: a general framework for corpus-based semantics." computational linguistics 
36.4 (2010): 673-721. 
 

references 

baroni, marco, and roberto zamparelli. "nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic 
space."proceedings of the 2010 conference on empirical methods in natural language processing. association for computational linguistics, 
2010. 
 
baroni, marco, georgiana dinu, and germ  n kruszewski. "don   t count, predict! a systematic comparison of context-counting vs. context-
predicting semantic vectors." proceedings of the 52nd annual meeting of the association for computational linguistics. vol. 1. 2014. 
 
bengio, yoshua, r  jean ducharme, pascal vincent, and christian jauvin. "a neural probabilistic language model." journal of machine 
learning research 3 (2003): 1137-1155. 
 
bengtson, eric, and dan roth. "understanding the value of features for coreference resolution." proceedings of the conference on empirical 
methods in natural language processing. association for computational linguistics, 2008. 
 
berant, jonathan, andrew chou, roy frostig, and percy liang. "id29 on freebase from question-answer pairs." in emnlp, pp. 
1533-1544. 2013. 
 
berant, jonathan, and percy liang. "id29 via id141."proceedings of acl. 2014. 
 
berg-kirkpatrick, taylor, alexandre bouchard-c  t  , john denero, and dan klein. "painless unsupervised learning with features."human 
language technologies: the 2010 annual conference of the north american chapter of the association for computational linguistics. 
association for computational linguistics, 2010. 
 
bergsma, shane, and dekang lin. "id64 path-based pronoun resolution." proceedings of the 21st international conference on 
computational linguistics and the 44th annual meeting of the association for computational linguistics. association for computational 
linguistics, 2006. 
 
blacoe, william, and mirella lapata. "a comparison of vector-based representations for semantic composition." proceedings of the 2012 joint 
conference on empirical methods in natural language processing and computational natural language learning. association for 
computational linguistics, 2012. 
 
bod, rens. "using an annotated corpus as a stochastic grammar." proceedings of the sixth conference on european chapter of the association 
for computational linguistics. association for computational linguistics, 1993. 
 

references 

brants, thorsten. "tnt: a statistical part-of-speech tagger." proceedings of the sixth conference on applied natural language processing. 
association for computational linguistics, 2000. 
 
brill, eric. "transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging." 
computational linguistics21.4 (1995): 543-565. 
 
brown, peter f., peter v. desouza, robert l. mercer, vincent j. della pietra, and jenifer c. lai. "class-based id165 models of natural 
language."computational linguistics 18.4 (1992): 467-479. 
 
brown, peter f., vincent j. della pietra, stephen a. della pietra, and robert l. mercer. "the mathematics of id151: 
parameter estimation." computational linguistics 19, no. 2 (1993): 263-311. 
 
cai, qingqing, and alexander yates. "large-scale id29 via schema matching and lexicon extension." in acl (1), pp. 423-433. 
2013. 
 
charniak, eugene. "tree-bank grammars." proceedings of the national conference on artificial intelligence. 1996. 
 
charniak, eugene. "a maximum-id178-inspired parser." proceedings of the 1st north american chapter of the association for computational 
linguistics conference. association for computational linguistics, 2000. 
 
charniak, eugene, and mark johnson. "coarse-to-fine n-best parsing and maxent discriminative reranking." proceedings of the 43rd annual 
meeting on association for computational linguistics. association for computational linguistics, 2005. 
 
chen, danqi, and christopher d. manning. "a fast and accurate dependency parser using neural networks." proceedings of the 2014 
conference on empirical methods in natural language processing (emnlp). 2014. 
 
chiang, david. "statistical parsing with an automatically-extracted id34." proceedings of the 38th annual meeting on 
association for computational linguistics. association for computational linguistics, 2000. 
 
chiang, david. "a hierarchical phrase-based model for id151." in proceedings of the 43rd annual meeting on 
association for computational linguistics, pp. 263-270. association for computational linguistics, 2005. 
 
chiang, david. "learning to translate with source and target syntax." inproceedings of the 48th annual meeting of the association for 
computational linguistics, pp. 1443-1452. association for computational linguistics, 2010. 

references 

christodoulopoulos, christos, sharon goldwater, and mark steedman. "two decades of unsupervised pos induction: how far have we 
come?."proceedings of the 2010 conference on empirical methods in natural language processing. association for computational linguistics, 
2010. 
 
clark, stephen, and james r. curran. "parsing the wsj using id35 and id148." proceedings of the 42nd annual meeting on 
association for computational linguistics. association for computational linguistics, 2004. 
 
cocke, john. "programming languages and their compilers." preliminary notes (technical report) (2nd revised ed.). cims, nyu., 1970. 
 
collins, michael.    head-driven statistical models for natural language parsing   . diss. university of pennsylvania, 1999. 
 
collobert, ronan, and jason weston. "a unified architecture for natural language processing: deep neural networks with multitask 
learning."proceedings of the 25th international conference on machine learning. acm, 2008. 
 
collobert, ronan, jason weston, l  on bottou, michael karlen, koray kavukcuoglu, and pavel kuksa. "natural language processing (almost) 
from scratch."the journal of machine learning research 12 (2011): 2493-2537. 
 
das, dipanjan, and noah a. smith. "paraphrase identification as probabilistic quasi-synchronous recognition." proceedings of the joint 
conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp: 
volume 1-volume 1. association for computational linguistics, 2009. 
 
das, dipanjan, and slav petrov. "unsupervised part-of-speech tagging with bilingual graph-based projections." proceedings of the 49th annual 
meeting of the association for computational linguistics: human language technologies-volume 1. association for computational linguistics, 
2011. 
 
das, sanjiv, and mike chen. "yahoo! for amazon: extracting market sentiment from stock message boards." in proceedings of the asia pacific 
finance association annual conference (apfa), vol. 35, p. 43. 2001. 
 
daum   iii, hal, and daniel marcu. "a large-scale exploration of effective global features for a joint entity detection and tracking model." 
proceedings of the conference on human language technology and empirical methods in natural language processing. association for 
computational linguistics, 2005. 
 
denis, pascal, and jason baldridge. "specialized models and ranking for coreference resolution." proceedings of the conference on empirical 
methods in natural language processing. association for computational linguistics, 2008. 

references 

devlin, jacob, rabih zbib, zhongqiang huang, thomas lamar, richard schwartz, and john makhoul. "fast and robust neural network joint 
models for id151." in 52nd annual meeting of the association for computational linguistics, baltimore, md, usa, june. 
2014. 
 
dhillon, paramveer, dean p. foster, and lyle h. ungar. "multi-view learning of id27s via cca." advances in neural information 
processing systems. 2011. 
 
dhillon, paramveer, jordan rodu, dean foster, and lyle ungar. "two step cca: a new spectral method for estimating vector models of 
words." proceedings of the 29th international conference on machine learning (icml-12). 2012. 
 
ding, xiaowen, bing liu, and philip s. yu. "a holistic lexicon-based approach to opinion mining." in proceedings of the 2008 international 
conference on web search and data mining, pp. 231-240. acm, 2008. 
 
donahue, jeff, lisa anne hendricks, sergio guadarrama, marcus rohrbach, subhashini venugopalan, kate saenko, and trevor darrell. 
"long-term recurrent convolutional networks for visual recognition and description." arxiv preprint arxiv:1411.4389 (2014). 
 
durrett, greg, david leo wright hall, and dan klein. "decentralized entity-level modeling for coreference resolution." acl (1). 2013. 
 
durrett, greg, and dan klein. "easy victories and uphill battles in coreference resolution." emnlp. 2013. 
 
durrett, greg, and dan klein. "a joint model for entity analysis: coreference, typing, and linking." transactions of the association for 
computational linguistics 2 (2014): 477-490. 
 
eisner, jason m. "three new probabilistic models for id33: an exploration." proceedings of the 16th conference on 
computational linguistics-volume 1. association for computational linguistics, 1996. 
 
fang, hao, saurabh gupta, forrest iandola, rupesh srivastava, li deng, piotr doll  r, jianfeng gao, xiaodong he, margaret mitchell, john c. 
platt, lawrence zitnick, and geoffrey zweig. "from captions to visual concepts and back." arxiv preprint arxiv:1411.4952 (2014). 
 
faruqui, manaal, and chris dyer. "improving vector space word representations using multilingual correlation." proc. of eacl. association for 
computational linguistics (2014). 
 
faruqui, manaal, jesse dodge, sujay k. jauhar, chris dyer, eduard hovy, and noah a. smith. "retrofitting word vectors to semantic 
lexicons." arxiv preprint arxiv:1411.4166 (2014). 

references 

ferrucci, david, eric brown, jennifer chu-carroll, james fan, david gondek, aditya a. kalyanpur, adam lally, j. william murdock, eric 
nyberg, john prager, nico schlaefer, and chris welty. "building watson: an overview of the deepqa project." ai magazine 31.3 (2010): 59-79. 
 
firth, john r. "a synopsis of linguistic theory 1930   55 (special volume of the philological society)." (1957). 
 
fossum, victoria, and kevin knight. "combining constituent parsers."proceedings of human language technologies: the 2009 annual 
conference of the north american chapter of the association for computational linguistics, companion volume: short papers. association for 
computational linguistics, 2009. 
 
fountain, trevor, and mirella lapata. "taxonomy induction using hierarchical random graphs." in proceedings of the 2012 conference of the 
north american chapter of the association for computational linguistics: human language technologies, pp. 466-476. association for 
computational linguistics, 2012. 
 
galley, michel, mark hopkins, kevin knight, and daniel marcu. what's in a translation rule. naacl, 2004. 
 
galley, michel, and christopher d. manning. "accurate non-hierarchical phrase-based translation." in human language technologies: the 
2010 annual conference of the north american chapter of the association for computational linguistics, pp. 966-974. association for 
computational linguistics, 2010. 
 
gillick, dan, and benoit favre. "a scalable global model for summarization." inproceedings of the workshop on integer id135 for 
natural langauge processing, pp. 10-18. association for computational linguistics, 2009. 
 
gimpel, kevin, and noah a. smith. "rich source-side context for id151." in proceedings of the third workshop on 
id151, pp. 9-17. association for computational linguistics, 2008. 
 
goodman, joshua. "efficient algorithms for parsing the dop model." arxiv preprint cmp-lg/9604008 (1996). 
 
grefenstette, edward, and mehrnoosh sadrzadeh. "experimental support for a categorical compositional distributional model of meaning." 
proceedings of the conference on empirical methods in natural language processing. association for computational linguistics, 2011. 
 
grefenstette, e., g. dinu, y. zhang, m. sadrzadeh, and m. baroni. "multi-step regression learning for compositional distributional 
semantics." proceedings of the 10th international conference on computational semantics (iwcs 2013). 2013. 
 

references 

haghighi, aria, and dan klein. "simple coreference resolution with rich syntactic and semantic features." proceedings of the 2009 conference 
on empirical methods in natural language processing: volume 3-volume 3. association for computational linguistics, 2009. 
 
hashimoto, kazuma, pontus stenetorp, makoto miwa, and yoshimasa tsuruoka. "jointly learning word representations and composition 
functions using predicate-argument structures." proceedings of the 2014 conference on empirical methods in natural language processing 
(emnlp). 2014. 
 
hatzivassiloglou, vasileios, and kathleen r. mckeown. "predicting the semantic orientation of adjectives." in proceedings of the 35th annual 
meeting of the association for computational linguistics and eighth conference of the european chapter of the association for computational 
linguistics, pp. 174-181. association for computational linguistics, 1997. 
 
harris, zellig s. "distributional structure." word (1954). 
 
hermann, karl moritz, and phil blunsom. "the role of syntax in vector space models of id152." acl (1). 2013. 
 
hong, kai, and ani nenkova. "improving the estimation of word importance for news id57." in proceedings of eacl. 
2014. 
 
huang, eric h., richard socher, christopher d. manning, and andrew y. ng. "improving word representations via global context and multiple 
word prototypes." proceedings of the 50th annual meeting of the association for computational linguistics: long papers-volume 1. association 
for computational linguistics, 2012. 
 
huang, liang, and david chiang. "better k-best parsing." proceedings of the ninth international workshop on parsing technology. association 
for computational linguistics, 2005. 
 
hwa, rebecca. "an empirical evaluation of probabilistic lexicalized tree insertion grammars." proceedings of the 17th international conference 
on computational linguistics-volume 1. association for computational linguistics, 1998. 
 
hwa, rebecca, philip resnik, amy weinberg, clara cabezas, and okan kolak. "id64 parsers via syntactic projection across parallel 
texts." natural language engineering 11.03 (2005): 311-325. 
 
iyyer, mohit, jordan boyd-graber, leonardo claudino, richard socher, and hal daum   iii. "a neural network for factoid id53 
over paragraphs." in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pp. 633-644. 2014. 

references 

johnson, mark. "pid18 models of linguistic tree representations."computational linguistics 24.4 (1998): 613-632. 
 
kalchbrenner, nal, and phil blunsom. "recurrent continuous translation models." in emnlp, pp. 1700-1709. 2013. 
 
karpathy, andrej, and li fei-fei. "deep visual-semantic alignments for generating image descriptions." arxiv preprint arxiv:1412.2306 (2014). 
 
kasami, t.    an efficient recognition and syntax algorithm for context-free languages   . scientific report afcrl-65-758, air force cambridge 
research laboratory, bedford ma, 1965. 
 
klein, dan, and christopher d. manning. "accurate unlexicalized parsing."proceedings of the 41st annual meeting on association for 
computational linguistics-volume 1. association for computational linguistics, 2003. 
 
kiros, ryan, ruslan salakhutdinov, and richard s. zemel. "unifying visual-semantic embeddings with multimodal neural language models." 
arxiv preprint arxiv:1411.2539 (2014). 
 
knight, kevin, and daniel marcu. "summarization beyond sentence extraction: a probabilistic approach to sentence compression." artificial 
intelligence 139, no. 1 (2002): 91-107. 
 
kobdani, hamidreza, hinrich sch  tze, michael schiehlen, and hans kamp. "id64 coreference resolution using word associations." 
proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies-volume 1. 
association for computational linguistics, 2011. 
 
koehn, philipp. "pharaoh: a id125 decoder for phrase-based id151 models." in machine translation: from real 
users to research, pp. 115-124. springer berlin heidelberg, 2004. 
 
koehn, philipp, hieu hoang, alexandra birch, chris callison-burch, marcello federico, nicola bertoldi, brooke cowan, wade shen, christine 
moran, richard zens, chris dyer, ond  ej bojar, alexandra constantin, and evan herbst. "moses: open source toolkit for statistical machine 
translation." in proceedings of the 45th annual meeting of the acl on interactive poster and demonstration sessions, pp. 177-180. association 
for computational linguistics, 2007. 
 
kong, chen, dahua lin, mohit bansal, raquel urtasun, and sanja fidler. "what are you talking about? text-to-image coreference."computer 
vision and pattern recognition (cvpr), 2014 ieee conference on. ieee, 2014. 
 
koo, terry, xavier carreras, and michael collins. "simple semi-supervised id33." (2008). 

references 

kozareva, zornitsa, and eduard hovy. "a semi-supervised method to learn and construct taxonomies using the web." in proceedings of the 
2010 conference on empirical methods in natural language processing, pp. 1110-1118. association for computational linguistics, 2010. 
 
kwiatkowski, tom, eunsol choi, yoav artzi, and luke zettlemoyer. "scaling semantic parsers with on-the-fly ontology matching." in 
proceedings of the conference on empirical methods in natural language processing (emnlp), 2013. 
 
lao, ni, amarnag subramanya, fernando pereira, and william w. cohen. "reading the web with learned syntactic-semantic id136 rules." 
inproceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language 
learning, pp. 1017-1026. association for computational linguistics, 2012. 
 
lee, heeyoung, angel chang, yves peirsman, nathanael chambers, mihai surdeanu, and dan jurafsky. "deterministic coreference resolution 
based on entity-centric, precision-ranked rules." computational linguistics 39.4 (2013): 885-916. 
 
levy, omer, and yoav goldberg. "dependency-based id27s."proceedings of the 52nd annual meeting of the association for 
computational linguistics. vol. 2. 2014. 
 
liang, percy, alexandre bouchard-c  t  , dan klein, and ben taskar. "an end-to-end discriminative approach to machine translation." in 
proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the association for 
computational linguistics, pp. 761-768. association for computational linguistics, 2006. 
 
liang, percy, ben taskar, and dan klein. "alignment by agreement." inproceedings of the main conference on human language technology 
conference of the north american chapter of the association of computational linguistics, pp. 104-111. association for computational 
linguistics, 2006. 
 
lin, chin-yew. "improving summarization performance by sentence compression: a pilot study." in proceedings of the sixth international 
workshop on information retrieval with asian languages-volume 11, pp. 1-8. association for computational linguistics, 2003. 
 
lin, chin-yew. "id8: a package for automatic evaluation of summaries." intext summarization branches out: proceedings of the acl-04 
workshop, pp. 74-81. 2004. 
 
lin, dekang. "an information-theoretic definition of similarity." icml. vol. 98. 1998. 
 
lin, dekang, and patrick pantel. "induction of semantic classes from natural language text." proceedings of the seventh acm sigkdd 
international conference on knowledge discovery and data mining. acm, 2001. 

references 

liu, bing. "id31 and opinion mining." synthesis lectures on human language technologies 5, no. 1 (2012): 1-167. 
 
madhyastha, pranava s., xavier carreras p  rez, and ariadna quattoni. "learning task-specific bilexical embeddings." coling, 2014. 
 
markert, katja, and malvina nissim. "comparing knowledge sources for nominal id2." computational linguistics 31.3 (2005): 
367-402. 
 
marcu, daniel, wei wang, abdessamad echihabi, and kevin knight. "spmt: id151 with syntactified target language 
phrases." inproceedings of the 2006 conference on empirical methods in natural language processing, pp. 44-52. association for 
computational linguistics, 2006. 
 
martins, andr   ft, and noah a. smith. "summarization with a joint model for sentence extraction and compression." proceedings of the 
workshop on integer id135 for natural langauge processing. association for computational linguistics, 2009. 
 
matsuzaki, takuya, yusuke miyao, and jun'ichi tsujii. "probabilistic id18 with latent annotations." proceedings of the 43rd annual meeting on 
association for computational linguistics. association for computational linguistics, 2005. 
 
mcclosky, david, eugene charniak, and mark johnson. "effective self-training for parsing." proceedings of the main conference on human 
language technology conference of the north american chapter of the association of computational linguistics. association for computational 
linguistics, 2006. 
 
mcdonald, ryan, fernando pereira, kiril ribarov, and jan haji  . "non-projective id33 using spanning tree algorithms." 
proceedings of the conference on human language technology and empirical methods in natural language processing. association for 
computational linguistics, 2005. 
 
mcdonald, ryan, slav petrov, and keith hall. "multi-source transfer of delexicalized dependency parsers." proceedings of the conference on 
empirical methods in natural language processing. association for computational linguistics, 2011. 
 
mi, haitao, liang huang, and qun liu. "forest-based translation." in acl, pp. 192-199. 2008. 
 
mikolov, tomas, ilya sutskever, kai chen, greg s. corrado, and jeff dean. "distributed representations of words and phrases and their 
compositionality." advances in neural information processing systems. 2013. 
 
mitchell, jeff, and mirella lapata. "composition in distributional models of semantics." cognitive science 34.8 (2010): 1388-1429. 

references 

mnih, andriy, and geoffrey hinton. "three new id114 for statistical language modelling." proceedings of the 24th international 
conference on machine learning. acm, 2007. 
 
nakagawa, tetsuji, kentaro inui, and sadao kurohashi. "dependency tree-based sentiment classification using crfs with hidden variables." in 
human language technologies: the 2010 annual conference of the north american chapter of the association for computational linguistics, 
pp. 786-794. association for computational linguistics, 2010. 
 
nakov, preslav, and marti hearst. "using the web as an implicit training set: application to structural ambiguity resolution." proceedings of the 
conference on human language technology and empirical methods in natural language processing. association for computational 
linguistics, 2005. 
 
navigli, roberto, paola velardi, and stefano faralli. "a graph-based algorithm for inducing lexical taxonomies from scratch." in ijcai, pp. 
1872-1877. 2011. 
 
ng, vincent, and claire cardie. "improving machine learning approaches to coreference resolution." proceedings of the 40th annual meeting 
on association for computational linguistics. association for computational linguistics, 2002. 
 
nivre, joakim. "an efficient algorithm for projective id33."proceedings of the 8th international workshop on parsing 
technologies (iwpt. 2003. 
 
nivre, joakim, and mario scholz. "deterministic id33 of english text." proceedings of the 20th international conference on 
computational linguistics. association for computational linguistics, 2004. 
 
och, franz josef, and hermann ney. "discriminative training and maximum id178 models for id151." in proceedings 
of the 40th annual meeting on association for computational linguistics, pp. 295-302. association for computational linguistics, 2002. 
 
och, franz josef. "minimum error rate training in id151." in proceedings of the 41st annual meeting on association for 
computational linguistics-volume 1, pp. 160-167. association for computational linguistics, 2003. 
 
pad  , sebastian, and mirella lapata. "dependency-based construction of semantic space models." computational linguistics 33.2 (2007): 
161-199. 

references 

pang, bo, lillian lee, and shivakumar vaithyanathan. "thumbs up?: sentiment classification using machine learning techniques." in 
proceedings of the acl-02 conference on empirical methods in natural language processing-volume 10, pp. 79-86. association for 
computational linguistics, 2002. 
 
pang, bo, and lillian lee. "opinion mining and id31."foundations and trends in information retrieval 2, no. 1-2 (2008): 1-135. 
 
papineni, kishore, salim roukos, todd ward, and wei-jing zhu. "id7: a method for automatic evaluation of machine translation." in 
proceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318. association for computational linguistics, 
2002. 
 
pauls, adam, and dan klein. "k-best a* parsing." proceedings of the joint conference of the 47th annual meeting of the acl and the 4th 
international joint conference on natural language processing of the afnlp: volume 2-volume 2. association for computational linguistics, 
2009. 
 
pereira, fernando, naftali tishby, and lillian lee. "distributional id91 of english words." proceedings of the 31st annual meeting on 
association for computational linguistics. association for computational linguistics, 1993. 
 
petrov, slav, leon barrett, romain thibaux, and dan klein. "learning accurate, compact, and interpretable tree annotation." proceedings of 
the 21st international conference on computational linguistics and the 44th annual meeting of the association for computational linguistics. 
association for computational linguistics, 2006. 
 
petrov, slav, dipanjan das, and ryan mcdonald. "a universal part-of-speech tagset." lrec 2012. 
 
pitler, emily, shane bergsma, dekang lin, and kenneth church. "using web-scale id165s to improve base np parsing performance." 
proceedings of the 23rd international conference on computational linguistics. association for computational linguistics, 2010. 
 
ponzetto, simone paolo, and michael strube. "exploiting id14, id138 and wikipedia for coreference resolution." 
proceedings of the main conference on human language technology conference of the north american chapter of the association of 
computational linguistics. association for computational linguistics, 2006. 
 
poon, hoifung, and pedro domingos. "unsupervised id29.    proceedings of the 2009 conference on empirical methods in natural 
language processing: volume 1-volume 1. association for computational linguistics, 2009. 

references 

poon, hoifung, and pedro domingos. "unsupervised ontology induction from text." in proceedings of the 48th annual meeting of the 
association for computational linguistics, pp. 296-305. association for computational linguistics, 2010. 
 
poon, hoifung. "grounded unsupervised id29." acl (1). 2013. 
 
rahman, altaf, and vincent ng. "coreference resolution with world knowledge."proceedings of the 49th annual meeting of the association for 
computational linguistics: human language technologies-volume 1. association for computational linguistics, 2011. 
 
ramanathan, vignesh, armand joulin, percy liang, and li fei-fei. "linking people in videos with    their    names using coreference resolution." 
id161   eccv 2014. springer international publishing, 2014. 95-110. 
 
ratnaparkhi, adwait. "a maximum id178 model for part-of-speech tagging."proceedings of the conference on empirical methods in natural 
language processing. vol. 1. 1996. 
 
resnik, philip. "probabilistic tree-adjoining grammar as a framework for statistical natural language processing." proceedings of the 14th 
conference on computational linguistics-volume 2. association for computational linguistics, 1992. 
 
sagae, kenji, and alon lavie. "a best-first probabilistic shift-reduce parser."proceedings of the coling/acl on main conference poster 
sessions. association for computational linguistics, 2006. 
 
sahlgren, magnus. "the word-space model: using distributional analysis to represent syntagmatic and paradigmatic relations between words 
in high-dimensional vector spaces." (2006). 
 
sarkar, anoop. "applying co-training methods to statistical parsing."proceedings of the second meeting of the north american chapter of the 
association for computational linguistics on language technologies. association for computational linguistics, 2001. 
 
schabes, yves, and richard c. waters. "tree insertion grammar: cubic-time, parsable formalism that lexicalizes context-free grammar without 
changing the trees produced." computational linguistics 21.4 (1995): 479-513. 
 
schilder, frank, and ravikumar kondadadi. "fastsum: fast and accurate query-based id57." in proceedings of the 
46th annual meeting of the association for computational linguistics on human language technologies: short papers, pp. 205-208. 
association for computational linguistics, 2008. 
 

references 

shen, dou, jian-tao sun, hua li, qiang yang, and zheng chen. "document summarization using id49." in ijcai, vol. 
7, pp. 2862-2867. 2007. 
 
snow, rion, daniel jurafsky, and andrew y. ng. "semantic taxonomy induction from heterogenous evidence." in proceedings of the 21st 
international conference on computational linguistics and the 44th annual meeting of the association for computational linguistics, pp. 
801-808. association for computational linguistics, 2006. 
 
snyder, benjamin, and regina barzilay. "multiple aspect ranking using the good grief algorithm." in hlt-naacl, pp. 300-307. 2007. 
 
socher, richard, eric h. huang, jeffrey pennin, christopher d. manning, and andrew y. ng. "dynamic pooling and unfolding recursive 
autoencoders for paraphrase detection." advances in neural information processing systems. 2011. 
 
socher, richard, jeffrey pennington, eric h. huang, andrew y. ng, and christopher d. manning. "semi-supervised recursive autoencoders for 
predicting sentiment distributions." proceedings of the conference on empirical methods in natural language processing. association for 
computational linguistics, 2011. 
 
socher, richard, brody huval, christopher d. manning, and andrew y. ng. "semantic compositionality through recursive matrix-vector spaces." 
proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language 
learning. association for computational linguistics, 2012. 
 
socher, richard, john bauer, christopher d. manning, and andrew y. ng. "parsing with compositional vector grammars." in proceedings of the 
acl conference. 2013. 
 
socher, richard, alex perelygin, jean y. wu, jason chuang, christopher d. manning, andrew y. ng, and christopher potts. "recursive deep 
models for semantic compositionality over a sentiment treebank." proceedings of the conference on empirical methods in natural language 
processing (emnlp). 2013. 
 
socher, richard, q. le, c. manning, and a. ng. "grounded id152 for finding and describing images with sentences." 
transactions of the association for computational linguistics, 2014. 
 
soon, wee meng, hwee tou ng, and daniel chung yong lim. "a machine learning approach to coreference resolution of noun phrases." 
computational linguistics 27.4 (2001): 521-544. 
 
steedman, mark. "surface structure and interpretation." (1996). 

references 

steedman, mark. the syntactic process. vol. 35. cambridge: mit press, 2000. 
 
steedman, mark, miles osborne, anoop sarkar, stephen clark, rebecca hwa, julia hockenmaier, paul ruhlen, steven baker, and jeremiah 
crim. "id64 statistical parsers from small datasets."proceedings of the tenth conference on european chapter of the association for 
computational linguistics-volume 1. association for computational linguistics, 2003. 
 
stoyanov, veselin, claire cardie, nathan gilbert, ellen riloff, david buttler, and david hysom. "coreference resolution with reconcile." 
proceedings of the acl 2010 conference short papers. association for computational linguistics, 2010. 
 
sutskever, ilya, oriol vinyals, and quoc vv le. "sequence to sequence learning with neural networks." advances in neural information 
processing systems. 2014. 
 
teufel, simone, and marc moens. "summarizing scientific articles: experiments with relevance and rhetorical status." computational linguistics 
28, no. 4 (2002): 409-445. 
 
thomas, matt, bo pang, and lillian lee. "get out the vote: determining support or opposition from congressional floor-debate transcripts." in 
proceedings of the 2006 conference on empirical methods in natural language processing, pp. 327-335. association for computational 
linguistics, 2006. 
 
tong, richard m. "an operational system for detecting and tracking opinions in on-line discussion." in working notes of the acm sigir 2001 
workshop on operational text classification, vol. 1, p. 6. 2001. 
 
toutanova, kristina, dan klein, christopher d. manning, and yoram singer. "feature-rich part-of-speech tagging with a cyclic dependency 
network." proceedings of the 2003 conference of the north american chapter of the association for computational linguistics on human 
language technology-volume 1. association for computational linguistics, 2003. 
 
toutanova, kristina, and christopher d. manning. "enriching the knowledge sources used in a maximum id178 part-of-speech tagger." 
proceedings of the 2000 joint sigdat conference on empirical methods in natural language processing and very large corpora: held in 
conjunction with the 38th annual meeting of the association for computational linguistics-volume 13. association for computational 
linguistics, 2000. 
 
turian, joseph, lev ratinov, and yoshua bengio. "word representations: a simple and general method for semi-supervised learning." 
proceedings of the 48th annual meeting of the association for computational linguistics. association for computational linguistics, 2010. 

references 

turney, peter d. "thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews." in proceedings of the 
40th annual meeting on association for computational linguistics, pp. 417-424. association for computational linguistics, 2002. 
 
turney, peter d., and patrick pantel. "from frequency to meaning: vector space models of semantics." journal of artificial intelligence research 
37.1 (2010): 141-188. 
 
vaswani, ashish, yinggong zhao, victoria fossum, and david chiang. "decoding with large-scale neural language models improves 
translation." inemnlp, pp. 1387-1392. 2013. 
 
venugopalan, subhashini, huijuan xu, jeff donahue, marcus rohrbach, raymond mooney, and kate saenko. "translating videos to natural 
language using deep recurrent neural networks." arxiv preprint arxiv:1412.4729 (2014). 
 
vinyals, oriol, alexander toshev, samy bengio, and dumitru erhan. "show and tell: a neural image caption generator." arxiv preprint arxiv:
1411.4555 (2014). 
 
vinyals, oriol, lukasz kaiser, terry koo, slav petrov, ilya sutskever, and geoffrey hinton. "grammar as a foreign language." arxiv preprint 
arxiv:1412.7449 (2014). 
 
vogel, stephan, hermann ney, and christoph tillmann. "id48-based word alignment in statistical translation." in proceedings of the 16th 
conference on computational linguistics-volume 2, pp. 836-841. association for computational linguistics, 1996. 
 
wang, lu, and claire cardie. "focused meeting summarization via unsupervised id36." in proceedings of the 13th annual 
meeting of the special interest group on discourse and dialogue, pp. 304-313. association for computational linguistics, 2012. 
 
white, michael, tanya korelsky, claire cardie, vincent ng, david pierce, and kiri wagstaff. "multidocument summarization via information 
extraction." inproceedings of the first international conference on human language technology research, pp. 1-7. association for computational 
linguistics, 2001. 
 
widdows, dominic. "unsupervised methods for developing taxonomies by combining syntactic and statistical information." in proceedings of 
the 2003 conference of the north american chapter of the association for computational linguistics on human language technology-volume 
1, pp. 197-204. association for computational linguistics, 2003. 
 
wiebe, janyce, theresa wilson, and claire cardie. "annotating expressions of opinions and emotions in language." language resources and 
evaluation 39, no. 2-3 (2005): 165-210. 

references 

woodsend, kristian, and mirella lapata. "automatic generation of story highlights." in proceedings of the 48th annual meeting of the 
association for computational linguistics, pp. 565-574. association for computational linguistics, 2010. 
 
wong, yuk wah, and raymond j. mooney. "learning synchronous grammars for id29 with id198." annual meeting-
association for computational linguistics. vol. 45. no. 1. 2007. 
 
wu, dekai. "stochastic inversion transduction grammars and bilingual parsing of parallel corpora." computational linguistics 23, no. 3 (1997): 
377-403. 
 
xi, chenhai, and rebecca hwa. "a backoff model for id64 resources for non-english languages." proceedings of the conference on 
human language technology and empirical methods in natural language processing. association for computational linguistics, 2005. 
 
xu, chang, yalong bai, jiang bian, bin gao, gang wang, xiaoguang liu, and tie-yan liu. "rc-net: a general framework for incorporating 
knowledge into word representations." proceedings of the 23rd acm international conference on conference on information and knowledge 
management. acm, 2014. 
 
yamada, hiroyasu, and yuji matsumoto. "statistical dependency analysis with support vector machines." proceedings of iwpt. vol. 3. 2003. 
 
yang, hui, and jamie callan. "a metric-based framework for automatic taxonomy induction." in proceedings of the joint conference of the 47th 
annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp: volume 1-volume 1, pp. 
271-279. association for computational linguistics, 2009. 
 
yao, xuchen, and benjamin van durme. "information extraction over structured data: id53 with freebase." in proceedings of 
acl. 2014. 
 
yarowsky, david, grace ngai, and richard wicentowski. "inducing multilingual text analysis tools via robust projection across aligned corpora." 
proceedings of the first international conference on human language technology research. association for computational linguistics, 2001. 
 
yessenalina, ainur, and claire cardie. "compositional matrix-space models for id31." proceedings of the conference on 
empirical methods in natural language processing. association for computational linguistics, 2011. 
 
yih, wen-tau, joshua goodman, lucy vanderwende, and hisami suzuki. "id57 by maximizing informative content-
words." inijcai, vol. 2007, p. 20th. 2007. 

references 

younger, daniel h. "recognition and parsing of context-free languages in time n^3." information and control 10.2 (1967): 189-208. 
 
yu, mo and dredze, mark.    improving lexical embeddings with semantic knowledge.    proceedings of the 52nd annual meeting of the 
association for computational linguistics, 2014. 
 
zajic, david m., bonnie dorr, jimmy lin, and richard schwartz. "sentence compression as a component of a id57 
system." inproceedings of the 2006 document understanding workshop, new york. 2006. 
 
zanzotto, fabio massimo, ioannis korkontzelos, francesca fallucchi, and suresh manandhar. "estimating linear models for compositional 
id65." proceedings of the 23rd international conference on computational linguistics. association for computational 
linguistics, 2010. 
 
zettlemoyer, luke s., and michael collins. "online learning of relaxed id35 grammars for parsing to logical form." in proceedings of the 2007 
joint conference on empirical methods in natural language processing and computational natural language learning (emnlp-conll-2007. 
2007. 
 

thank you! 

http://ttic.uchicago.edu/~mbansal/ 

