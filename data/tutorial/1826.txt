neural networks for natural 
language understanding

sam bowman
department of linguistics and nlp group
stanford university
 
with chris potts, chris manning

today

promising signs that neural network models can learn to 
handle semantic tasks:
    sentiment and semantic similarity (e.g., tai et al. 2015)
    paraphrase detection (socher et al. 2011)
    machine translation (e.g., sutskever et al. 2014, 

bahdanau et al. 2014)

how do these models work?

how well can they handle anything we   d recognize as 
meaning?

today

how do these models work?
    survey: deep learning models for nlu

how well can they handle anything we   d recognize as 
meaning?
    a measure of success: natural language id136
    three experiments on artificial data
    frontiers: what about real language?

nns for sentence meaning

input: word vectors

not

that

bad

output: sentence vectors

not

that

bad

not that bad

training: supervised classification over sentence vectors

not

that

bad

not that bad

prediction: 2/5

nns for sentence meaning

input: word vectors

not

that

bad

output: sentence vectors

not

that

bad

not that bad

training: supervised classification over sentence vectors

not

that

bad

not that bad

prediction: 2/5
label: 3/5

baseline: sum of words

       (          )

that

not

bad

softmax classifier
prediction: 3/5

    words and constituents are 25-

500d vectors.

    optimize with sgd
    gradients from backprop

recurrent nns for text

composition nn layers
s
not

not that

not
learned word vectors

that

softmax classifier
prediction: 3/5

not that bad

bad

    words and constituents are 25-

500d vectors.

    one  learned composition function:

      y = f(m[x yprev]  + b)

    optimize with sgd
    gradients from backprop (through 

time)

f(x) = tanh(x)
     ...usually

recurrent nns for text

composition lstm layers
not
not
not that
not
not

not
not
learned word vectors

that
that

softmax classifier
prediction: 3/5
prediction: 3/5

not
not
not that bad

bad
bad

    lstm (long short term memory) models are id56s 

with a more complex learned activation function meant 
to do a better job of preserving information across long 
sequences.

 hochreiter and  schmidhuber 1997

my focus: treeid56s*

label: 3/5

not that bad

softmax classifier

composition nn layer

not

that bad

composition nn layer

that

bad

learned word vectors

    sequence of operations follows parse tree
    different sentence? different tree structure.
*id56 = recurrent nn  and  treeid56 = tree-structured recursive nn. aagh.

socher et al. 2011

my focus: treeid56s

label: 3/5

not that bad

softmax classifier

composition nn layer

not

that bad

composition nn layer

that

bad

learned word vectors

    basic treeid56 uses the same kind of learned function 

as an id56:
      y = f(m[xl xr]  + b)

socher et al. 2011

variants: dependency treeid56s

    dependency tree id56s
y = mheadxhead + f(mrel(1)x1) + f(mrel(2)x2)...

label: 3/5

softmax classifier

the movie isn   t bad

cop

nsubj

the movie

movie

is

is

neg
n   t

n   t

learned word vectors

bad

words transformed into constituents

det

the

the

socher et al. 2014

variants: matrix-vector treeid56

    matrix-vector id56 

composition functions:
     y = f(mv[ba ab])

y = mm[a b]

softmax classifier

composition nn layer

label: 3/5

not that bad

not

that bad

that

bad
learned word vectors
and word matrices

socher et al. 2012

variants: treerntn

    recursive neural tensor 

network composition 
function:

y = f(xlm[1...n]xr + m[xl xr] + b)

label: 3/5

not that bad

softmax classifier

composition ntn layer

composition ntn layer

not

that bad

that

bad
learned word vectors

chen et al. 2013, socher et al. 2013

other nns for sentence meaning

and more:
    tree autoencoders (socher et al 2011)
    treelstms (tai et al 2015)
    convolutional nns for text (kalchbrenner et al. 2014)
    character-level convolution (zhang and lecun 2015)

...

the big question

how well are supervised neural network 
models able to learn representations of 
sentence meaning?

the big question

how well are supervised neural network 
models able to learn representations of 
sentence meaning?

the big question

how well are supervised neural network 
models able to learn representations of 
sentence meaning?

don   t ask what meanings are. ask what they do, and find 
something that does that. 

-david lewis, paraphrased

the task: natural language id136

james byron dean refused to move without blue jeans

{entails, contradicts, neither}

james dean didn   t dance without pants

bill   s thesis

the task: natural language id136

claim: simple task to define, but engages the full 
complexity of id152:
    lexical entailment
    quantification
    coreference
    lexical/scope ambiguity
    commonsense knowledge
    propositional attitudes
    modality
    factivity and implicativity

...

lexical relations

experimental approach: train on relational statements 
generated from some formal system, test on other such 
relational statements.

the model needs to:
    learn the relations between individual words. (lexical 

relations)

formulating a learning problem

training data:
dance entails 
move
waltz neutral
tango
tango entails
dance
sleep contradicts dance
waltz entails
dance

memorization (training set):
dance ??? move
waltz ??? tango

generalization (test set):
sleep ??? waltz
tango ??? move

maccartney   s natural logic

an implementable logic for natural language id136 
without logical forms. (maccartney and manning    09)
    sound logical interpretation (icard and moss    13)

natural logic: relations

seven possible relations between phrases/sentences:

venn

symbol

name

slide from bill maccartney

example

x     y

x     y

x     y

x ^ y

x | y

x     y

x # y

equivalence

forward entailment

(strict)

reverse entailment

(strict)

negation

(exhaustive exclusion)

alternation

(non-exhaustive exclusion)

cover

(exhaustive non-exclusion)

couch     sofa

crow     bird

european     french

human ^ nonhuman

cat | dog

animal     nonhuman

independence

hungry # hippo

natural logic: relation joins

maccartney   s join table: a r b     b r    c     a {r     r   } c

{animal     cat, cat     kitten}      animal     kitten
{cat     animal, animal ^ non-animal}      cat | non-animal

natural logic: relation joins

can our nns learn to make these id136s

over pairs of embedding vectors?

lexical relations: data generation

{a,b}

{a}

{a,b,c}

{a,c}

{b,c}

{c}

{b}

{}

lexical relations: data generation

p1, p2
{a,b}

p5, p6
{a}

{a,b,c}

p3
{a,c}

{b}

{}

p4
{b,c}

p7, p8
{c}

lexical relations
lexical relations: data generation

extracted relations:
p1     p2
p1  ^  p7
p1     p5
p4     p8
p2     p5
p5     p6
p5  |   p7
p7     p4
p7 ^   p1
p8     p1

p4
{b,c}

p7, p8
{c}

p1, p2
{a,b}

p5, p6
{a}

{a,b,c}

p3
{a,c}

{b}

{}

lexical relations
lexical relations: data generation

train:
p1     p2
p1     p5
p4     p8

p5  |   p7
p7 ^   p1

test:

p1  ^  p7

p2     p5
p5     p6
p7     p4
p8     p4

p4
{b,c}

p7, p8
{c}

p1, p2
{a,b}

p5, p6
{a}

{a,b,c}

p3
{a,c}

{b}

{}

lexical relations
lexical relations: data generation

train:
p1     p2
p1     p5
p4     p8

p5  |   p7
p7 ^   p1

test:

p1  ^  p7

p2     p5
p5     p6
p7     p4
p8     p4

p4
{b,c}

p7, p8
{c}

p1, p2
{a,b}

p5, p6
{a}

{a,b,c}

p3
{a,c}

{b}

{}

a minimal nn for lexical relations

    words are learned embedding vectors.

    one plain treeid56 or 

treerntn layer

    softmax emits relation labels

softmax classifier

p(entailment) = 0.9

comparison n(t)n layer

jeans vs. pants

    learn everything with sgd.

jeans

pants
learned word vectors

lexical relations: training

    80 random terms (p1 - p80) denoting sets

    sampled with replacement from the powerset of the 

set of 7 entities (a-g) 

    6400 statements, yielding:
    3200 training examples
    about 2900 provable test examples

(~7% not provable)

lexical relations: results

    both models tuned, then trained to convergence on five 

randomly generated datasets

    reported figures: % correct (macroaveraged f1)

    both nns used 15d embeddings, 75d comparison layer

lexical relations: conclusions

    success! ntns can learn lexical entailment networks

    no special optimization techniques required
    good generalization even with small training sets

    open questions:

    geometric theory of lexical relations?
    relationship between the number of terms and the 

number of dimensions in the embedding?

recursion in id118

experimental approach: train on relational statements 
generated from some formal system, test on other such 
relational statements.

the model needs to:
    learn the relations between individual words. (lexical 

relations)

recursion in id118

experimental approach: train on relational statements 
generated from some formal system, test on other such 
relational statements.

the model needs to:
    learn the relations between individual words. (lexical 

    learn how lexical relations impact phrasal relations. 

relations)

(projectivity)

recursion in id118

experimental approach: train on relational statements 
generated from some formal system, test on other such 
relational statements.

the model needs to:
    learn the relations between individual words. (lexical 

relations)

    learn how lexical relations impact phrasal relations. 

(projectivity)
    this needs to be recursively applicable!
a     a,   a ^ (not a),   a     (not (not a)),   ...

recursion in id118

data: randomly generated sentences with and, or, and not
    6 proposition variables (a-f), at most 4 per example
    propositions are variables over unknown truth values 

(264 possible representations)

    train on statements with at most 4 operators, test with 

more.

nli with treeid56s

    model structure:

    two trees, then a separately learned comparison 

layer, then a classifier:

p( | ) = 0.8

softmax classifier

not a vs. a and b

comparison n(t)n layer

not a

a and b

composition rn(t)n layer

not

a

a

and b

learned word vectors

and

b

recursion in id118

today

how do these models work?
    survey: deep learning models for nlu

how well can they handle anything we   d recognize as 
meaning?
    a measure of success: natural language id136
    can nns learn lexical relations?
    can treeid56s learn recursive functions?
    what about quantification and monotonicity?
    frontiers: what about real language?

quantifiers

experimental paradigm: train on relational statements 
generated from some formal system, test on other such 
relational statements.

the model needs to:
    learn the relations between individual words. (lexical 

    learn how lexical relations impact phrasal relations. 

relations)

(projectivity)

quantifiers

experimental paradigm: train on relational statements 
generated from some formal system, test on other such 
relational statements.

the model needs to:
    learn the relations between individual words. (lexical 

    learn how lexical relations impact phrasal relations. 

    quantifiers present some of the harder cases of both of 

relations)

(projectivity)

these.

two experiments

quantifiers

    small vocabulary

    three basic types:

    quantifiers: some, all, no, most, two, three, not-

all, not-most, less-than-two, less-than-three

    predicates: dog, cat, mammal, animal    
    negation: not 

    60k examples generated using a generative 

implementation of the relevant portion of maccartney 
and manning   s logic. 

    all sentences of the form qpp, with optional negation 

on each predicate.

quantifier results

most freq. class (# only)
25d sumnn (sum of words)
25d treeid56
25d treerntn

train
35.4%
96.9%
99.6%
100%

test
35.4%
93.9%
99.2%
99.7%

summary: artificial data

    simple ntns can encode relation composition 

accurately.

    tree structured models can learn recursive functions, 
and can apply them in structures that are (somewhat) 
larger than those seen in training.

    tree structured models can learn to reason with 

quantifiers.

do we see these behaviors in id123 with real 
natural language?

natural language id136 data

    to do nli on real english, we need to teach an nn 

model english almost from scratch.
    what data do we have to work with:

    glove/id97 (useful w/ any data source)
    sick: thousands of examples created by editing 

and pairing hundreds of sentences.

    rte: hundreds of examples created by hand.
    denotationgraph: millions of extremely noisy 
examples (~73% correct?) constructed fully 
automatically.

results on sick (+dg, +tricks) so far 

sick train dg train

test

most freq. class
30 dim treeid56
50 dim treerntn

56.7%
95.4%
97.8%

50.0%
67.0%
74.0%

56.7%
74.9%
76.9%

are we competitive? sort of...

best result (uillinois)

84.5% 

    interannotator agreement!

median submission (out of 18):  77%
our treerntn: 

76.9%

we   re the only purely-learned system in the competition: 
everything but the parser is trained from the supplied data.

is it realistic to learn from sick?

a guy is mowing the lawn.
grass is being mowed by a man.
entailment

a guy is mowing the lawn
there is no guy mowing the lawn.
contradiction

a guy is mowing the lawn
there is no man mowing grass.
contradiction

...

natural language id136 data

    to do nli on real english, we need to teach an nn 

model english almost from scratch.
    what data do we have to work with:

    glove/id97 (useful w/ any data source)
    sick: thousands of examples created by editing 

and pairing hundreds of sentences.

    rte: hundreds of examples created by hand.
    denotationgraph: millions of extremely noisy 
examples (~73% correct?) constructed fully 
automatically.

natural language id136 data

    to do nli on real english, we need to teach an nn 

model english almost from scratch.
    what data do we have to work with:

    glove/id97 (useful w/ any data source)
    sick: thousands of examples created by editing 

and pairing hundreds of sentences.

    rte: hundreds of examples created by hand.
    denotationgraph: millions of extremely noisy 
examples (~73% correct?) constructed fully 
automatically.

    stanford nli corpus: ~600k examples, written by 

turkers.

the stanford nli corpus

some examples

a young boy rides a bike down a snow covered road.
a child is outside.
entailment

2 female babies eating chips.
two female babies are enjoying chips.
neutral

a woman in an apron shopping at a market.
a woman in an apron is preparing dinner.
contradiction

results?

not much yet:
    train on sick + dg, test on sick: so-so
    train on snli: stay tuned!

interested in being one of the first to work on this? the draft 
corpus is available to the class.

deep learning for text: logistics

    lots of knobs to twiddle:

    optimization method (plain sgd, adagrad, ...)
    dimensionality
    initialization, id173
    type of layer function/nonlinearity
    train/test split

...

    good references for general nn methods (though 

   standard    methods change often):
    an incomplete book from the bengio lab: http://www.

iro.umontreal.ca/~bengioy/dlbook/

    coursera lectures from geoff hinton:

https://www.coursera.org/course/neuralnets

deep learning for text: logistics

    typical training times for models like the ones seen 

here: 4-48h

    no standard deep learning library yet can do everything 

you   ll want for language.
    caffe (python), theano (python), torch (lua) all 

very strong for at least some model types.

    try my matlab codebase for an easy start with:

    id56, lstm
    treeid56, treerntn, treelstm

thanks!

more questions?

sbowman@stanford.edu

