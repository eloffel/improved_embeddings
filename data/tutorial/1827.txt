neural   language   
models   and   word   
embeddings	

piotr mirowski, microsoft bing 

 

london big-o meetup 

june 25, 2014 

ackowledgements	

       at&t labs research 

o    sumit chopra (now at facebook)  
o    srinivas bangalore 
o    suhrid balakrishnan 
       new york university 

o    yann lecun (now at facebook) 

       microsoft 

o    geoffrey zweig 
o    bhaskar mitra 
o    abhishek arun 

2 

outline	

       motivations 

o    probabilistic language models (lms) and id165s 
o    id65 
       neural probabilistic lms 

o    vector-space representation of words 
o    neural probabilistic language model 
o    log-bilinear (lbl) lms 
o    recurrent neural network lms 

       applications 

o    word representation 
o    id103 and machine translation 
o    sentence completion and linguistic regularities 
bag-of-word-vector approaches 
o    continuous bag-of-words and skip-gram models 
scalability with large vocabularies 
o   
tree-structured lms 
o    negative sampling 

      

      

3 

outline	

       motivations 

o    probabilistic language models (lms) and id165s 
o    id65 
       neural probabilistic lms 

o    vector-space representation of words 
o    neural probabilistic language model 
o    log-bilinear (lbl) lms 
o    recurrent neural network lms 

       applications 

o    word representation 
o    id103 and machine translation 
o    sentence completion and linguistic regularities 
bag-of-word-vector approaches 
o    continuous bag-of-words and skip-gram models 
scalability with large vocabularies 
o   
tree-structured lms 
o    negative sampling 

      

      

4 

motivations   (1):   

language   modeling	

       applications: 

o    id103 
o    machine translation 

       id38 aims at quantifying  

the likelihood of a text (sentence, query   ) 

       score/rank candidates in n-best list 

o    example: hub-4 tv broadcast transcripts 

100-best list of candidate sentences 
returned by the acoustic model: 
choose sentence  
with highest combined  
lm log-likelihood 
and acoustic model score 

the american popular culture 
americans popular culture 
american popular culture 
the nerds in popular culture 
mayor kind popular culture 
near can popular culture 
the mere kind popular culture 
... 

5 

motivations   (1):   

language   modeling	

       id203 of a sequence of words: 

wwpwp
(
2

=

)

(

,

1

,...,

t ww
,
t

1
   

)

       id155 of an upcoming word: 

wwwp
(
2

,

t

1

,...,

w
   t
1

)

       chain rule of id203: 
,...,

wwwp
(
2

wwp
(
2

ww
t
t

,...,

=

)

,

,

,

|

t

1

1

t

1
   

w
t

1
   

)

       (n-1)th order markov assumption 
w
)
t

wwp
(
2

wwp
(

ww
t
t

w
nt
+   

,...,

,...,

nt
1
+   

   

1
   

1
   

)

,

,

,

|

t

2

1

t

       id165s and word context of n-1 words  

1
=

t

   

t

1
=

   

the!cat!sat!on!the!mat!

twp
(

|

w

t
t

1
5 =   
   

15.0)

6 

motivations   (1):   

limitations   of   n-        grams	

the!cat!sat!on!the!mat!
the!cat!sat!on!the!hat!
the!cat!sat!on!the!sat!

twp
(
twp
(
twp
(

|
|
|

w
w
w

t
t
t
t
t
t

15.0)
1
5 =   
   
05.0)
1
5 =   
   
0)
1
5 =   
   

       limitation: discrete model (each word is a token) 

o   

incomplete coverage of the training dataset 
vocabulary of size v words: vn possible id165s (exponential in n) 
my! cat!sat!on!the!mat!

w

twp
(

1
5 =   
   

?

)

|

t
t

o    no notion of semantic similarity between word tokens 

the!cat!sat!on!the!rug!

twp
(

|

w

t
t

1
5 =   
   

)

?

7 

motivations   (2):   

word   representation	

       bag-of-words: 

words are tokens in vocabulary of size v 
       example: unigram distribution of words 

token   count   (top   50)         corpus:   ap   news   (1995),   16m   words,   v=17965	

	
>
s
/
<

	

,

	
e
h
t

	

.

	
>
n
w
o
n
k
n
u
<

	
>
n
u
o
n
_
r
e
p
o
r
p
<

	
f
o

	
o
t

	
d
n
a

	
a

	
n
#

	
n

i

	

`
`

	

'  
'  

	
s

'  

	
d
i
a
s

	
r
o
f

	
e
h

	
s
a
w

	
s
i

	

-        

	
n
o

	
h
t
i

w

	
y
b

	
s
i

h

	

)

	

(

	
t
u
b

	
e
v
a
h

	
t
a

	
s
a

	
n
a

	
e
r
a

	
o
h
w

	
e
r
e
w

	

i

	

;

	
t
o
n

	
s
a
h

	
e
b

	
d
a
h

	
n
a
#

	

l
l
i

w

	
n
$
#

	

:

	
r
o

	
t
u
o
b
a

l

	
d
u
o
w

	
r
e
t
f
a

	
e
l
p
o
e
p

token   count   (bocom   50)         corpus:   ap   news   (1995)	

	
g
n
u
t

	
t
u
o
_
g
i
d

	
d
e
t
p
m
e
t

	
e
s
o
n
g
a
i
d

	
h
c
u
o
r
c

	
t
u
o
_
k
c
a
b

	
s
y
o
v
n
e

	
s
r
e
p
e
e
k

	
d
e
l
c
i

n
o
r
h
c

	
e
n

i
t
s
i
r
p

	
y
d
o
b
a
e
p

	
k
c
u
l
_
d
o
o
g

	
t
u
o
_
g
n

i
k
o
o
l

	
o
g
r
o
f

	
s
r
a
t
s
-        
o
c

	
n
o
n
n
e
l
_
n
h
o

j

	
s
t
s
e
v
r
a
h

	
r
u
o
h
-        
h
s
u
r

	
l
a
n
o
i
t
i
s
n
a
r
t

	
g
n

i
l
l
e
s
_
t
s
e
b

	
r
a
u
g
a

j

	
s
p
o
t
f
o
o
r

	
e
t
a
l
p
e
m
a
n

	
s
e
i
c
n
a
p
e
r
c
s
i
d

	
y
s
u
o
l

	
s
n
u
g
t
o
h
s

	
r
a
m
m
a
r
g

	
r
e
b
m
e
m
_
   
a
t
s

	
n
o
r
e
d
l
a
c

	
s
t
o
i
r
t
a
p

	
l
a
n
o
i
t
r
o
p
o
r
p

	
n
o
i
t
a
n
m
o
n
e
r

i

	
t
u
n

l
a
w

	
n
a
r
e
h
t
u

l

	
n
o
i
t
a
d
o
m
m
o
c
c
a

	
s
o
g
o
l

	
s
l
l

u
p

i

	
g
n
d
l
e
   

	
t
n
o
m
u
a
e
b

	
e
i
l
l
o
c

	
y
a
d
k
e
e
w

	
s
e
g
r
o
e
g

	
t
a
p
s

	
l
a
i
t
n
e
r
r
o
t

	
e
d
a
m
d
n
a
h

	
n
o
i
t
c
u
d
e
d
_
x
a
t

u
q
e
_
s
n
o
i
t
a
c
i
n
u
m
m
o
c

	

l
l
i
b
_
s
n
o
i
t
a
i
r
p
o
r
p
p
a

	
t
n
a
l
i
b
u

j

	
s
r
e
d
l
o
h
-        
t
e
k
c
i
t

8 

1200000	
1000000	
800000	
600000	
400000	
200000	
0	

0	
5	
10	
15	
20	
25	
30	
35	

motivations   (2):   

distributional   semantics	
       how to represent the meaning of a word? 
       using vectors of elements (called    features   ) 
o    option 1: using other words (bag-of-words representation) 
o    option 2: learn the word representation features 

       exploit collocation of words (word context) 

the!cat!sat!on!the!mat!

twp
(

|

w

t
t

1
5 =   
   

15.0)

         [...] this article is about the cat species that is commonly kept [...] 
[...] cats disambiguation . the domestic cat ( felis catus or felis [...] 
               [...] pet , or simply the cat when there is no need [...] 
           [...] to killing small prey . cat senses fit a crepuscular and [...] 
            [...] a social species , and cat communication includes a variety of [...] 
             [...] grunting ) as well as cat pheromones , and types of [...] 
                [...] , a hobby known as cat fancy . failure to control [...] 

[http://en.wikipedia.org/wiki/cat] 

9 

idea:      

combine   two   approaches	

learning word representations  

and 

learning language models 

outline	

       motivations 

o    probabilistic language models (lms) and id165s 
o    id65 
       neural probabilistic lms 

o    vector-space representation of words 
o    neural probabilistic language model 
o    log-bilinear (lbl) lms 
o    recurrent neural network lms 

       applications 

o    word representation 
o    id103 and machine translation 
o    sentence completion and linguistic regularities 
bag-of-word-vector approaches 
o    continuous bag-of-words and skip-gram models 
scalability with large vocabularies 
o   
tree-structured lms 
o    negative sampling 

      

      

11 

learning   probabilistic   
language   models	

       learn joint likelihood of training sentences 

under (n-1)th order markov assumption 
using id165s 

wwp
(
2

,

1

,...,

ww
t
t

1
   

,

)

=

t

   

t

1
=

wwwp
(
2

,

|

1

t

,...,

w
t

1
   

)

   

t

   

t

1
=

wp
(
t

|

w

t
1
   
nt
1
+   

)

target word 
word history 

tw
w
t
1
   
nt
1
+   

=

w
nt
1
+   

,

w
nt
+   

2

,...,

w
t

1
   

       maximize the log-likelihood: 

o    assuming a parametric model    

t

   

1
=

t

log

twp
(

|

w

t
1
   
nt
1
+   

  
),

12 

vector-        space   

representation   of   words	

   one-hot    of    one-of-v    
representation  
of a word token at position t  
in the text corpus,  
with vocabulary of size v 

tw

vector-space representation  
of any word v  
in the vocabulary 
using a vector of dimension d 

vz

zv	


also called 
distributed representation 

1	

v	

v	

1	

d	

vector-space representation 
of the prediction  
of target word wt 
(we predict a vector of size d) 

tz   

ntz
t
1
   
1
+   

vector-space representation  
of the tth word history/context: 
e.g., concatenation  
of n-1 vectors of size d 

   t	


zt-1	


zt-2	


zt-1	


13 

learning   continuous   
space   language   models	

      

input: 
o    word history (one-hot or distributed representation) 

       output: 

o    target word (one-hot or distributed representation) 

       function that approximates word likelihood: 

o    linear transform  
o    feed-forward neural network 
o    recurrent neural network 
o    continuous bag-of-words 
o    skip-gram 
o        

14 

learning   continuous   
space   language   models	

       how do we learn the word representations z  

for each word in the vocabulary? 

       how do we learn the model that predicts  

the next word or its representation    t 
given a word history? 

       simultaneous learning of model  

and representation 

15 

vector-        space   

representation   of   words	
       compare two words using vector representations: 

o    dot product 
o    cosine similarity 
o    euclidean distance 

       bi-linear scoring function at position t: 

    ,
z
(

t

   
zz
t
t

s

v

)

s

v

=

=

=

+

)

  

b
v

vs
( )
  

v
t
;,1
   
1

(
w
o    parametric model    predicts next word 
o    bias bv for word v related to unigram probabilities of word v 
o    given a predicted vector    t,  
the actual predicted word is the 1-nearest neighbour of    t 

[mnih & hinton, 2007] 

16 

word   probabilities   from   

vector-        space   representation	
       normalized id203 using softmax function 

(
vwp
=

t

|

w

)
t
1
    =
1

)

v

,

   
tz
se

s

e
(
v
    =
v
1'

   
tz

(

v
,

'

)

       bi-linear scoring function at position t: 

    ,
z
(

t

   
zz
t
t

s

v

)

s

v

=

=

=

+

)

  

b
v

vs
( )
  

v
t
;,1
   
1

(
w
o    parametric model    predicts next word 
o    bias bv for word v related to unigram probabilities of word v 
o    given a predicted vector    t,  
the actual predicted word is the 1-nearest neighbour of    t 

[mnih & hinton, 2007] 

17 

loss   function	

       log-likelihood model: 

log

wwp
(
2

,

1

,...,

ww
t
t

1
   

,

)

=

log

      
            
      

t

   

t

1
=

wp
(
t

|

w

t
1
   
1

)

      
=            
      

t
t

   

1
=

log

wp
(
t

|

w

t
1
   
1

)

(
wwp

=

t

|

w

)
t
1
    =
1

       id168 to maximize: 

o    log-likelihood 
=

(
wwp

log

l
t

=

t

|

w

t
1
   
1

)

=

ws
( )
  

   

log

v
v

    =

1

s

  

v
( )

e

ws
(
  

)

e
v
    =
v

1

s

  

v
( )

e

in general, loss defined as: score of the right answer + id172 term 

o   
o    id172 term is expensive to compute 

18 

neural   probabilistic   
language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


word	

embedding	

in dimension   
d=30	


r	


r	


r	


r	


r	


neural network 
100 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

function z_hist = embedding_fprop(model, w)!
% get the embeddings for all words in w!
z_hist = model.r(:, w);!
z_hist = reshape(z_hist, length(w)*model.dim_z, 1);!

r=(zv)	


1	

d	

1	

v	

[bengio et al, 2001, 2003; schwenk et al,    connectionist language modelling for large vocabulary 

continuous id103   , icassp 2002] 

19 

neural   probabilistic   
language   model	

function s = neuralnet_fprop(model, z_hist)!
% one hidden layer neural network!
o = model.a * z_hist + model.bias_a;!
h = tanh(o);!
s = model.b * h + model.bias_b;!
!
h
s

(
az
t
1
   
  
nt
1
+   
bh
b
+

=
=

b

)

+

a

b

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


word	

embedding	

in dimension   
d=30	


r	


r	


r	


r	


r	


neural network 
100 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

[bengio et al, 2001, 2003; schwenk et al,    connectionist language modelling for large vocabulary 

continuous id103   , icassp 2002] 

20 

neural   probabilistic   
language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


word	

embedding	

in dimension   
d=30	


r	


r	


r	


r	


r	


neural network 
100 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

function p = softmax_fprop(s)!
% id203 estimation!
p_num = exp(s);!
p = p_num / sum(p_num); !
ws
(
  
t
(
wp
t
e

e
( )   

w 1
t
   
nt
1
+   

)

=

|

v

v

s

)

  

[bengio et al, 2001, 2003; schwenk et al,    connectionist language modelling for large vocabulary 

continuous id103   , icassp 2002] 

21 

neural   probabilistic   
language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


word	

embedding	

in dimension   
d=30	


r	


r	


r	


r	


r	


neural network 
100 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

complexity: (n-1)  d + (n-1)  d  h + h  v 

h
s

=
=

(
az
t
1
   
  
nt
1
+   
bh
b
+

b

+

b

a

)

(
wp
t

|

w 1
t
   
nt
1
+   

)

=

)

ws
(
  
t
e

e
( )   

v

v

s

  

wt	

mat!

outperforms best id165s 
(class-based kneyser-ney 
back-off 5-grams) by 7% 

took months to train 
(in 2001-2002) on ap news 
corpus (14m words) 

[bengio et al, 2001, 2003; schwenk et al,    connectionist language modelling for large vocabulary 

continuous id103   , icassp 2002] 

22 

log-        bilinear   

language   model	

function z_hat = lbl_fprop(model, z_hist)!
% simple linear transform!
z_hat = model.c * z_hist + model.bias_c;!

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


c	


   t	


e	


zt	


   
z

t

=

cz

t
1
   
nt
1
+   

+

b

c

word	

embedding	

in dimension   
d=100	


discrete word space 
{1, ..., v}   
v=18k words	


r	


r	


r	


r	


r	


r	


simple matrix 
multiplication 

wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

[mnih & hinton, 2007] 

23 

log-        bilinear   

language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


c	


   t	


e	


zt	


word	

embedding	

in dimension   
d=100	


discrete word space 
{1, ..., v}   
v=18k words	


r	


r	


r	


r	


r	


r	


simple matrix 
multiplication 

wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

function s = ...   
  score_fprop(z_hat, model)!
s = model.r    * z_hat + model.bias_v;!
vs
( )
  
(
wp
t

=
v
w 1
t
|
   
nt
1
+   

b
+
v
)

   
zz
t
t

=

v

)

ws
(
  
t
e

e
( )   

v

s

  

[mnih & hinton, 2007] 

24 

log-        bilinear   

language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


c	


   t	


e	


zt	


   
z

t

=

cz

t
1
   
nt
1
+   

+

b

c

r	


r	


r	


r	


r	


r	


simple matrix 
multiplication 

vs
( )
  
(
wp
t

   
zz
t
t

=
v
w 1
t
|
   
nt
1
+   

b
+
v
)

=

word	

embedding	

in dimension   
d=100	


discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

complexity: (n-1)  d + (n-1)  d  d + d  v 

)

ws
(
  
t
e

e
( )   

v

v

s

  

slightly better than 
best id165s 
(class-based kneyser-ney 
back-off 5-grams) 
takes days to train 
(in 2007) on ap news 
corpus (14 million words) 

[mnih & hinton, 2007] 

25 

nonlinear   log-        bilinear   

language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


   t	


e	


zt	


h
   
z

t

(
az
t
1
   
=
  
nt
1
+   
bh
b
=
+

b

+

b

a

)

word	

embedding	

in dimension   
d=100	


r	


r	


r	


r	


r	


neural network 
200 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

complexity: (n-1)  d + (n-1)  d  h + h  d + d  v 

r	


vs
( )
  
(
wp
t

   
zz
t
t

=
v
w 1
t
|
   
nt
1
+   

b
+
v
)

=

)

ws
(
  
t
e

e
( )   

v

v

s

  

wt	

mat!

outperforms best id165s 
(class-based kneyser-ney 
back-off 5-grams) by 24% 

took weeks to train 
(in 2009-2010) on ap news 
corpus (14m words) 

[mnih & hinton, neural computation, 2009] 

26 

recurrent   neural   net   
(id56)   language   model	

time-delay 

id27  
space    d 
in dimension  
d=30 to 250 

1-layer 

neural network 

with d output units 

z

zt-1	


w	


h	


id27 

matrix 

u	


zt	


v	


o	


discrete word 
space {1, ..., m} 
m>100k words 

wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

wt	

mat!

complexity: d  d + d  d + d  v 

+

uw

)t

=

t
x
( )

t

   1

wz
(
  
1
xe
   +

= 1

  

o =
(
wp
t

tvz
w
|

t
1
   
nt
1
+   

)

=

y

t

=

)

wo
(
e

e
( )   

vo

v

handles longer word history 
(~10 words) as well  
as 10-gram feed-forward nnlm 
 
training algorithm: bptt 
back-propagation through time 

[mikolov et al, 2010, 2011] 

27 

learning      

neural   language   models	
       maximize the log-likelihood of observed data, 

w.r.t. parameters    of the neural language model 
l
t
arg

(
wwp
(
log

w
t
1
   
=
t
1
(
t wwp

)
ws
( )
=
  
)  w
)
,

    =

log

log

t
1
   
1

v
v

v
( )

=

=

   

e

|

|

1

s

  

max
  

       parameters    (in a neural language model): 

o    id27 matrix r and bias bv 
o    neural weights: a, ba, b, bb 

       id119 with learning rate   : 

  

  
      

  

tl
   
  
   

maximizing   the   loss   function	

(
wwp

=

t

|

w

)
t
1
    =
1

ws
(
  

)

e
v
    =
v

1

s

  

v
( )

e

       maximum likelihood learning: 

  

t

s

1

|

e

=

=

   

=

=

)

v
( )

v
v

w

t
1
   
1

log

log

log

(
wwp

   
  
   

    =

ws
( )
  

(
wwp

l
t
o    gradient of log-likelihood w.r.t. parameters   : 
l
   
t
  
   
l
   
vs
( )
t
  
  
   
o    use the chain rule of gradients 

ws
( )
  

    =

   
  
   

   
  
   

(
vp

)1

t
1
   
1

w

w

t
   
1

v
v

)

=

=

   

|

|

1

t

29 

maximizing   the   loss   function:   

example   of   lbl	

  

t

)

s

v

1

|

e

=

=

=

+

v
( )

w

ws
(
  

   
  
   

vs
( )
  

ws
( )
  

)
t
1
    =
1

e
v
    =
v

       maximum likelihood learning: 
   
zz
t
t

(
wwp
o    gradient of log-likelihood w.r.t. parameters   : 
l
   
t
  
   
function [dl_dz_hat, dl_dr, dl_dbias_v, w] = ...   
  loss_backprop(z_hat, model, p, w)!
% gradient of loss w.r.t. word bias parameter!
dl_dbias_v = -p;!
dl_dbias_v(w) = 1 - p;!
% gradient of loss w.r.t. prediction of (n)lbl model!
dl_dz_hat = model.r(:, w)     model.r * p;!
% gradient of loss w.r.t. vocabulary matrix r!
dl_dr =    z_hat * p   ;!
dl_dr(:, w) = z_hat * (1     p(w));!
o    neural net: back-propagate gradient 

vs
( )
  

    =

   
  
   

(
vp

t
1
   
1

w

v
v

)

   

|

1

tl
   
z   
   
t

l
   
t
  
   

=

l
   
t
   
z
   

t

   
z
   
t
  
   

b
v

r=(zv)	


1	

v	

1	

d	

30 

learning      

neural   language   models	

randomly choose a mini-batch 
(e.g., 1000 consecutive words) 

1.    forward-propagate 

through id27s 
and through model 

2.    estimate word likelihood (loss) 
3.    back-propagate loss 
4.    gradient step to update model 

stochastic   gradient   
descent   (sgd)	
       choice of the learning hyperparameters 

o    learning rate? 
o    learning rate decay? 
o    id173 (l2-norm) of the parameters? 
o    momentum term on the parameters? 

       use cross-validation on validation set 

o    e.g., on ap news (16m words) 

       training set: 14m words 
       validation set: 1m words 
       test set: 1m words 

outline	

       motivations 

o    probabilistic language models (lms) and id165s 
o    id65 
       neural probabilistic lms 

o    vector-space representation of words 
o    neural probabilistic language model 
o    log-bilinear (lbl) lms 
o    recurrent neural network lms 

       applications 

o    word representation 
o    id103 and machine translation 
o    sentence completion and linguistic regularities 
bag-of-word-vector approaches 
o    continuous bag-of-words and skip-gram models 
scalability with large vocabularies 
o   
tree-structured lms 
o    negative sampling 

      

      

33 

word   embeddings   
obtained   on   reuters	
       example of id27s obtained using our 

language model on the reuters corpus 
(1.5 million words, vocabulary v=12k words), vector 
space of dimension d=100 

       for each word, the 10 nearest neighbours in the 

vector space retrieved using cosine similarity: 

[mirowski, chopra, balakrishnan and bangalore (2010)  

   feature-rich continuous language models for id103   , slt] 

34 

word   embeddings   

obtained   on   ap   news	

example of word 
embeddings 
obtained using our 
lm on ap news 
(14m words, v=17k), 
d=100 
 
the word 
embedding matrix r 
was projected in 2d 
by stochastic id167 
[van der maaten, 
jmlr 2008] 

   time series modelling with hidden variables and gradient-based algorithms   , nyu phd thesis] 

[mirowski (2010)  

35 

word   embeddings   

obtained   on   ap   news	

example of word 
embeddings 
obtained using our 
lm on ap news 
(14m words, v=17k), 
d=100 
 
the word 
embedding matrix r 
was projected in 2d 
by stochastic id167 
[van der maaten, 
jmlr 2008] 

   time series modelling with hidden variables and gradient-based algorithms   , nyu phd thesis] 

[mirowski (2010)  

36 

word   embeddings   

obtained   on   ap   news	

example of word 
embeddings 
obtained using our 
lm on ap news 
(14m words, v=17k), 
d=100 
 
the word 
embedding matrix r 
was projected in 2d 
by stochastic id167 
[van der maaten, 
jmlr 2008] 

   time series modelling with hidden variables and gradient-based algorithms   , nyu phd thesis] 

[mirowski (2010)  

37 

word   embeddings   

obtained   on   ap   news	

example of word 
embeddings 
obtained using our 
lm on ap news 
(14m words, v=17k), 
d=100 
 
the word 
embedding matrix r 
was projected in 2d 
by stochastic id167 
[van der maaten, 
jmlr 2008] 

   time series modelling with hidden variables and gradient-based algorithms   , nyu phd thesis] 

[mirowski (2010)  

38 

performance   of   lbl   on   
speech   recognition	

hub-4 tv broadcast 
transcripts 
vocabulary v=25k 
(with proper nouns & 
numbers) 
train on 1m words 
validate on 50k words 
test on 800 sentences 

re-rank top 100 
candidate sentences, 
provided for each 
spoken sentence 
by a id103 
system (acoustic model 
+ simple trigram) 

#topics	


pos	


-	

-	

-	

-	

0	

0	

0	

5	

5	

5	


-	

-	

-	

-	

-	


f=34	

f=3	


-	


f=34	

f=3	


word 
accuracy	

63.7%	

63.5%	

66.6%	

57.8%	

64.1%	

64.1%	

64.1%	

64.2%	

64.6%	

64.6%	


method	


at&t watson [gof   n et al, 2005]	


kn 5-grams on 100-best list	

oracle: best of 100-best list	

oracle: worst of 100-best list	


log-bilinear models with nonlinearity   

and optional pos tag inputs   

and lda topic model mixtures	


[mirowski et al, 2010] 

39 

syntactic   and   semantic   

tests   with   id56	

observed that id27s obtained by id56-lda 
have linguistic regularities    a    is to    b    as    c    is to _ 
syntactic: king is to kings as queen is to queens 
semantic: clothing is to shirt as dish is to bowl 

vector   o   set   method	

z1	


[image credits: mikolov et al (2013)    efficient 
estimation of word representation in vector 

space   , arxiv] 

-        	
 +	
 =	
   	


z2	


z3	


zv	


cosine   

similarity	

[mikolov, yih and zweig, 2013] 

40 

microsoft   research   

sentence   completion   task	

       1024 sentences with 1 missing word each 
       5 choices for each word 

o    ground truth and 4 impostor words 
that is his generous   fault, but on the whole he   s a good worker. 
that is his mother   s   fault, but on the whole he   s a good worker. 
that is his successful fault, but on the whole he   s a good worker. 
that is his main       fault, but on the whole he   s a good worker. 
that is his favourite  fault, but on the whole he   s a good worker. 

       human performance: 90% accuracy 

[image credits: mikolov et al (2013)    efficient 
estimation of word representation in vector 

space   , arxiv] 

[zweig & burges, 2011; mikolov et al, 2013a;  

http://research.microsoft.com/apps/pubs/default.aspx?id=157031 ] 

41 

outline	

       motivations 

o    probabilistic language models (lms) and id165s 
o    id65 
       neural probabilistic lms 

o    vector-space representation of words 
o    neural probabilistic language model 
o    log-bilinear (lbl) lms 
o    recurrent neural network lms 

       applications 

o    word representation 
o    id103 and machine translation 
o    sentence completion and linguistic regularities 
bag-of-word-vector approaches 
o    continuous bag-of-words and skip-gram models 
scalability with large vocabularies 
o   
tree-structured lms 
o    negative sampling 

      

      

42 

continuous   bag-        of-        words	

id27  
space    d 
in dimension  
d=100 to 300 

simple sum 

h	


h

=

c

   

   =

c

i

id27 
matrices 

u	


u	


u	


u	


w	


ctz

   

o =
(
wp
t

wh
|

ww

t
1 ,
   
ct
   

)
ct
=+
t
1
+

)

wo
(
e

e
( )   

vo

v

discrete word 
space {1, ..., v} 
v>100k words 

wt-2	

 wt-1	

the!cat!

wt+1	

 wt+2	

on! the!

wt	

sat!

extremely efficient estimation of 
id27s in matrix u 
without a language model. 
can be used as input to neural lm. 
enables much larger datasets, e.g., 
google news (6b words, v=1m) 

complexity: 2c  d + d  v 

complexity: 2c  d + d  log(v) (hierarchical softmax using tree factorization) 

[mikolov et al, 2013a; mnih & kavukcuoglu, 2013; 

http://code.google.com/p/id97 ] 

43 

skip-        gram	

id27  
space    d 
in dimension  
d=100 to 1000 

	


zt

t,z

input

id27 
matrices 

w	


w	


w	

 w	


u	


)

cvs
(
,
  
wp
(

ct
+

z

=
w
|
t

t
v
output
,
)

=

z
t
,
e
   

v

)

input
cws
,
(
  
e

s

(

  

cv
,

)

discrete word 
space {1, ..., v} 
v>100k words 

wt-2	

 wt-1	

the!cat!

wt+1	

 wt+2	

on! the!

wt	

sat!

extremely efficient estimation of 
id27s in matrix u 
without a language model. 
can be used as input to neural lm. 
enables much larger datasets, e.g., 
google news (33b words, v=1m) 

complexity: 2c  d + 2c  d  v 

complexity: 2c  d + 2c  d  log(v) (hierarchical softmax using tree factorization) 

complexity: 2c  d + 2c  d  (k+1) (negative sampling with k negative examples) 

[mikolov et al, 2013a, 2013b; mnih & kavukcuoglu, 2013; 

http://code.google.com/p/id97 ] 

44 

vector-        space   word   

representation   without   lm	

word and phrase representation 
learned by skip-gram  
exhibit linear structure that enables 
analogies with vector arithmetics. 
 
this is due to training objective,  
input and output (before softmax)  
are in linear relationship. 
 
the sum of vectors in the id168 
is the sum of log-probabilities  
(or log of product of probabilities),  
i.e., comparable to the and function. 

[image credits: mikolov et al (2013) 

   distributed representations of words and 
phrases and their compositionality   , nips] 

[mikolov et al, 2013a, 2013b; http://code.google.com/p/id97] 

45 

examples   of   id97   

embeddings	

example of word 
embeddings 
obtained using 
id97 on the 
3.2b word 
wikipedia: 
       vocabulary 
       continuous 

v=2m 

vector space 
d=200 
trained using 
cbow 

      

debt	
   
debts	
   

aa	
   
aaarm	
   

decrease	
   
increase	
   

met	
   
mee4ng	
   

slow	
   
slower	
    marseille	
   

france	
   

jesus	
   
christ	
   

xbox	
   
playsta4on	
   

increases	
   

repayments	
    samavat	
   
	
   
repayment	
    obukhovskii	
    decreased	
    meets	
   
	
   
monetary	
   

emerlec	
   

greatly	
   

meet	
   

had	
   

fast	
   

french	
   

resurrec4on	
   

slowing	
    nantes	
   

savior	
   

wii	
   

xbla	
   

slows	
   

vichy	
   

miscl	
   

wiiware	
   

payments	
   
	
   
repay	
   

gunss	
   

decreasing	
    welcomed	
   

slowed	
    paris	
   

cruci   ed	
   

gamecube	
   

dekhen	
   

increased	
   

insisted	
   

faster	
   

bordeaux	
   

god	
   

mortgage	
    minizini	
   

decreases	
   

acquainted	
   

sluggish	
    aubagne	
   

apostles	
   

repaid	
   

bf	
   

reduces	
   

sa4s   ed	
   

quicker	
    vend	
   

apostle	
   

nintendo	
   

kinect	
   

dsiware	
   

re   nancing	
    mortardepth	
    reduce	
   

   rst	
   

pace	
   

vienne	
   

bickertonite	
   

eshop	
   

bailouts	
   

ee	
   

increasing	
   

persuaded	
   

slowly	
   

toulouse	
   

pretribula4onal	
   

dreamcast	
   

[mikolov et al, 2013a, 2013b; http://code.google.com/p/id97] 

46 

semantic-        syntactic   word   

evaluation   task	

[image credits: mikolov et al (2013)    efficient 
estimation of word representation in vector 

space   , arxiv] 

[mikolov et al, 2013a, 2013b; http://code.google.com/p/id97] 

47 

performance   on   the   

semantic-        syntactic   task	

[image credits: mikolov et al (2013)    efficient 
estimation of word representation in vector 

space   , arxiv] 

[image credits: mikolov et al (2013) 

   distributed representations of words and 
phrases and their compositionality   , nips] 

word and phrase representation learned by skip-gram  
exhibit linear structure that enables analogies with vector arithmetics. 
due to training objective, input and output (before softmax) in linear relationship. 
sum of vectors is like sum of log-probabilities, i.e. log of product of probabilities, 
i.e., and function. 
	

[mikolov et al, 2013a, 2013b; http://code.google.com/p/id97] 

48 

outline	

       motivations 

o    probabilistic language models (lms) and id165s 
o    id65 
       neural probabilistic lms 

o    vector-space representation of words 
o    neural probabilistic language model 
o    log-bilinear (lbl) lms 
o    recurrent neural network lms 

       applications 

o    word representation 
o    id103 and machine translation 
o    sentence completion and linguistic regularities 
bag-of-word-vector approaches 
o    continuous bag-of-words and skip-gram models 
scalability with large vocabularies 
o   
tree-structured lms 
o    negative sampling 

      

      

49 

computational   bocleneck   
of   large   vocabularies	
       bulk of computation at 

target word 

word history 

scoring function 

softmax 

)(tw
   tw
1
1
ts
( )
v
sg
)
(

v

(
vwp
=

t

| w

t
   1
1

)

)v

v

s

=

(
= w
t
,1
   
1
e
s
v
e
    =
v
1'
)tsg
( )
(

=

v

s

v

'

word prediction  
and at input word 
embedding layers 
       training can take 

days or even weeks 
       large vocabularies: 
o    ap news (14m words; v=17k) 
o    hub-4 (1m words; v=25k) 
o    google news (6b words, v=1m) 
o    wikipedia (3.2b, v=2m) 
       strategies to compress 

output softmax  

50 

hierarchical   softmax   
by   grouping   words	

target word 

word history 

scoring function 

softmax 

)(tw
   tw
1
1
vs
( )
  
vsg
( )
(
)

=

  

s

(
vwp
=

t

|

)
w =
t
   1
1

)vsg
(
( )

  

)  

v
( )

(
w
=

  

v
t   
;,1
1
e
s
v
    =
v
1'

s

  

v

(

'

)

e

t

(
vwp
=
(
vwp
=

t

|

w

t
1
   
1

|

w

t
1
   
1

)
)

=

=

t
1
   
1

|

w

(
cp
id19
(
( )
)

  

  

t
1
   
1

|

w

)
  
)vid19
(
)

(
vp
(

,

  

,

)c

       group words  

into disjoint classes: 
o    e.g., 20 classes 

with frequency binning 
o    use unigram frequency 
o    top 5% words (   the   ) go to class 1 
o    following 5% words go to class 2 

       factorize word 
id203 into: 
o    class id203 
o    class-conditional  

word id203 

       speed-up factor: 

o    o(|v|) to o(|c|+max|vc|) 

[mikolov et al, 2011, auli et al, 2013] 

51 

hierarchical   softmax   
by   grouping   words	

target word 

word history 

scoring function 

softmax 

)(tw
   tw
1
1
vs
( )
  
vsg
( )
(
)

=

  

s

(
vwp
=

t

|

)
w =
t
   1
1

)vsg
(
( )

  

)  

v
( )

(
w
=

  

v
t   
;,1
1
e
s
v
    =
v
1'

s

  

v

(

'

)

e

t

(
vwp
=
(
vwp
=

t

|

w

t
1
   
1

|

w

t
1
   
1

)
)

=

=

t
1
   
1

|

w

(
cp
id19
(
( )
)

  

  

t
1
   
1

|

w

)
  
)vid19
(
)

(
vp
(

,

  

,

)c

[image credits: mikolov et al (2011)    extensions 

of recurrent neural network language 

model   , icassp] 

[mikolov et al, 2011, auli et al, 2013] 

52 

negative   sampling	
       id203 estimation as binary classification 

problem: 
o    positive examples (data) vs. negative examples (noise) 
o    scaling factor k: noisy samples k times more likely than data samples 

       noise distribution: based on unigram word probabilities 

=

|1

w
,

(
dp

ws
(
)
  
kp
       negative sampling 

w 1
t
   
1

e
+

ws
(
  

)

=

e

noise

)

o    remove id172 term in probabilities 

( )w

(
dp

|1

w
,

=

w

t
   1
1

)

)ws
( )
(
  =
  

l
t

'

=

log

ws
(
( )
)
  
  

+

e

p
noise

[
log

(
   
  

vs
(
  
i

]
)
)

k

   

1
=

i

       compare to maximum likelihood learning: 

l
t

=

ws
( )
  

   

log

v
v

    =

1

s

  

v
( )

e

[mnih & teh, 2012; mnih & kavukcuoglu, 2013; mikolov et al, 2013a, 2013b] 

53 

speed-        up   over   full   softmax	

lbl with full softmax, 
trained on apnews data, 
14m words, v=17k 
7days	

skip-gram (context 5) 
with phrases, trained 
using negative sampling, 
on google data, 
33g words, v=692k + phrases 
1 day	

[image credits: mikolov et al (2013) 

   distributed representations of words and 
phrases and their compositionality   , nips] 

lbl (2-gram, 100d)  
with full softmax, 1 day	
lbl (2-gram, 100d) with 
noise contrastive estimation 
1.5 hours	
id56 (100d) with 
50-class hierarchical softmax 
0.5 hours (own experience)	

id56 (hs) 

50 classes 

0.5 
[image credits: mnih & teh (2012)    a fast and 
simple algorithm for training neura probabilistic 

145.4 

language models   , icml] 

[mnih & teh, 2012; mikolov et al, 2010-2012, 2013b] 

penn 
treebank 
data 
(900k words, 
v=10k)	

54 

thank   you!	
       further references: following this slide 
       basic (n)lbl matlab code: available on demand 
       contact: piotr.mirowski@computer.org 

references	

       basic id165s with smoothing and backtracking 

(no word vector representation): 
o    s. katz, (1987) 

"estimation of probabilities from sparse data for the language model 
component of a speech recognizer", 
ieee transactions on acoustics, speech and signal processing,  
vol. assp-35, no. 3, pp. 400   401 
https://www.mscs.mu.edu/~cstruble/moodle/file.php/3/papers/
01165125.pdf 

o    s. f. chen and j. goodman (1996) 

"an empirical study of smoothing techniques for language modelling", 
acl 
http://acl.ldc.upenn.edu/p/p96/p96-1041.pdf?origin=publication_detail  

o    a. stolcke (2002) 

"srilm - an extensible id38 toolkit    
icslp, pp. 901   904 
http://my.fit.edu/~vkepuska/ece5527/projects/fall2011/sundaresan,
%20venkata%20subramanyan/srilm/doc/paper.pdf 

56 

references	

       neural network language models: 

o    y. bengio, r. ducharme, p. vincent and j.-l. jauvin (2001, 2003) 

"a neural probabilistic language model", 
nips (2000) 13:933-938 
j. machine learning research (2003) 3:1137-115 
http://www.iro.umontreal.ca/~lisa/pointeurs/
bengioducharmevincentjauvin_jmlr.pdf 

o    f. morin and y. bengio (2005) 

   hierarchical probabilistic neural network language model", 
aistats 
http://core.kmi.open.ac.uk/download/pdf/22017.pdf#page=255 
o    y. bengio, h. schwenk, j.-s. sen  cal, f. morin, j.-l. gauvain (2006) 

"neural probabilistic language models", 
innovations in machine learning, vol. 194, pp 137-186 
http://rd.springer.com/chapter/10.1007/3-540-33486-6_6 

57 

references	

       linear and/or nonlinear (neural network-based) 

language models: 
o    a. mnih and g. hinton (2007) 

"three new id114 for statistical language modelling", 
icml, pp. 641   648, http://www.cs.utoronto.ca/~hinton/absps/threenew.pdf 

o    a. mnih, y. zhang, and g. hinton (2009) 

"improving a statistical language model through non-linear prediction", 
neurocomputing, vol. 72, no. 7-9, pp. 1414     1418 
http://www.sciencedirect.com/science/article/pii/s0925231209000083 

o    a. mnih and y.-w. teh (2012) 

"a fast and simple algorithm for training neural probabilistic language models    
icml, http://arxiv.org/pdf/1206.6426 
o    a. mnih and k. kavukcuoglu (2013) 

   learning id27s efficiently with noise-contrastive estimation    
nips 
http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-
noise-contrastive-estimation.pdf 

58 

references	

       recurrent neural networks 

(long-term memory of word context): 
o    tomas mikolov, m karafiat, j cernocky, s khudanpur (2010) 

"recurrent neural network-based language model    
interspeech 

o    t. mikolov, s. kombrink, l. burger, j. cernocky and s. khudanpur (2011) 

   extensions of recurrent neural network language model    
icassp   

o    tomas mikolov and geoff zweig (2012) 

"context-dependent recurrent neural network language model    
ieee speech language technologies   

o    tomas mikolov, wen-tau yih and geoffrey zweig (2013) 

"linguistic regularities in continuous spaceword representations" 
naacl-hlt 
https://www.aclweb.org/anthology/n/n13/n13-1090.pdf 

o    http://research.microsoft.com/en-us/projects/id56/default.aspx  

59 

references	

       applications: 

o    p. mirowski, s. chopra, s. balakrishnan and s. bangalore (2010)  

   feature-rich continuous language models for id103   ,  
slt  

o    g. zweig and c. burges (2011) 

   the microsoft research sentence completion challenge    
msr technical report msr-tr-2011-129 

o    http://research.microsoft.com/apps/pubs/default.aspx?id=157031  
o    m. auli, m. galley, c. quirk and g. zweig (2013) 

   joint language and translation modeling with recurrent neural 
networks    
emnlp 

o    k. yao, g. zweig, m.-y. hwang, y. shi and d. yu (2013) 

   recurrent neural networks for language understanding    
interspeech 

60 

references	

       continuous bags of words, skip-grams, id97: 

o    tomas mikolov et al (2013) 

   efficient estimation of word representation in vector space    
arxiv.1301.3781v3 

o    tomas mikolov et al (2013) 

   distributed representation of words and phrases and their 
compositionality    
arxiv.1310.4546v1, nips 

o    http://code.google.com/p/id97 

61 

probabilistic   

language   models	

       goal:  

score sentences according to their likelihood 
o    machine translation: 

       p(high winds tonight) > p(large winds tonight) 

o    spell correction 

       the office is about fifteen minuets from my house 
       p(about fifteen minutes from) > p(about fifteen minuets from) 

o    id103 

       p(i saw a van) >> p(eyes awe of an) 
       re-ranking n-best lists of sentences produced by an acoustic model,  

taking the best 

       secondary goal:  

sentence completion or generation 

slide courtesy of abhishek arun 

63 

example   of   a   bigram   
language   model	

training data 
there is a big house 

i buy a house 

they buy the new house 

test data 

s1: 
they buy a big house 
p(s1) = 0.333 * 1 * 0.5 * 0.5 * 1 
p(s1) = 0.0833 

s2: 
they buy a new house 
p(s2) = ? 

model 

p(big|a) = 0.5 
p(is|there) = 1 
p(buy|they) = 1 
p(house|a) = 0.5 
p(buy|i) = 1 
p(a|buy) = 0.5 
p(new|the) = 1 
p(house|big) = 1 
p(the|buy) = 0.5 
p(a| is) = 1 
p(house|new) = 1 
p(they| < s >) = .333 

wwp
(
 ,
2

1

w
...,
 
 ,
t

  ) 
=

t

   

t

1
=

wwp
(
t

|

t

)

1
   

slide courtesy of abhishek arun 

64 

intuitive   view   
of   perplexity	

       how well can we predict next word? 

i	
   always	
   order	
   pizza	
   with	
   cheese	
   and	
   ____	
   
the	
   33rd	
   president	
   of	
   the	
   us	
   was	
   ____	
   
i	
   saw	
   a	
   ____	
   

o    a random predictor would give each word id203 1/v 

where v is the size of the vocabulary 

o    a better model of a text should assign a higher id203  

to the word that actually occurs 

       perplexity: 

o       how many words are likely to happen, given the context    
o    perplexity of 1 means that the model recites the text by heart 
o    perplexity of v means that the model produces uniform random guesses 
o    the lower the perplexity, the better the language model 

mushrooms 0.1 
pepperoni 0.1 
anchovies 0.01 
   . 
fried rice 0.0001 
   . 
and 1e-100 

slide courtesy of abhishek arun 

65 

nonlinear   log-        bilinear   

language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


   t	


e	


zt	


fprop 
1.   

word	

embedding	

in dimension   
d=100	


r	


r	


r	


r	


r	


neural network 
200 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

2.   

3.   

r	


wt	

mat!

look-up embeddings 
of the words 
in the id165 using r 
forward propagate 
through  
the neural net 
look-up all vocabulary 
words using r 
and compute 
energy and probabilities 
(computationally  
expensive) 

[mnih & hinton, neural computation, 2009] 

66 

nonlinear   log-        bilinear   

language   model	

id27 
space    d	


zt-5	


zt-4	


zt-3	


zt-2	


zt-1	


a	


h	


b	


   t	


e	


zt	


word	

embedding	

in dimension   
d=100	


r	


r	


r	


r	


r	


neural network 
200 hidden units 
v output units 
followed by 

softmax 

discrete word space 
{1, ..., v}   
v=18k words	


wt-5	

 wt-4	

 wt-3	

 wt-2	

 wt-1	

the!cat!sat! on! the!

r	


wt	

mat!

backprop 
1.    compute gradients 
of loss w.r.t. output 
of the neural net, 
back-propagate 
through neural net 
layers b and a 
(computationally  
expensive) 
2.    back-propagate 

further down to word 
embeddings r 

3.    compute gradients 

of loss w.r.t. words 
of all vocabulary, 
back-propagate to r 

[mnih & hinton, neural computation, 2009] 

67 

stochastic      

gradient   descent	

[lecun et al, "efficient backprop", neural networks: tricks of the trade, 1998; 

bottou, "stochastic learning", slides from a talk in tubingen, 2003] 

stochastic      

gradient   descent	

[lecun et al, "efficient backprop", neural networks: tricks of the trade, 1998; 

bottou, "stochastic learning", slides from a talk in tubingen, 2003] 

perplexity   of   id56   
language   models	

id32 
v=10k vocabulary 
train on 900k words 
validate on 80k words 
test on 80k words 

test ppx 

123.3 

104.4 

98.5 

86.9 

model 

kneyser-ney back-off 5-grams 

nonlinear lbl (100d) 
[mnih & hinton, 2009, using our implementation] 
nlbl (100d) + 5 topics lda 
[mirowski, 2010, using our implementation] 
id56 (200d) + 40 topics lda 
[mikolov & zweig, 2012, using id56 toolbox] 
ap news 
v=17k vocabulary 
train on 14m words 
validate on 1m words 
test on 1m words 

id56 toolbox: http://research.microsoft.com/en-us/projects/id56/default.aspx] 

[mirowski, 2010; mikolov & zweig, 2012; 

70 

semantic-        syntactic   word   

evaluation   task	

[image credits: mikolov et al (2013)    efficient 
estimation of word representation in vector 

space   , arxiv] 

[mikolov et al, 2013a, 2013b; http://code.google.com/p/id97] 

71 

semantic-        syntactic   word   

evaluation   task	

[image credits: mikolov et al (2013)    efficient 
estimation of word representation in vector 

space   , arxiv] 

[mikolov et al, 2013a, 2013b; http://code.google.com/p/id97] 

72 

noise-        contrastive   estimation	
       id155 of word w in the data: 

(
wwp

=

t

|

w

)
t
1
    =
1

ws
(
  

)

e
v
    =
v

s

  

v
( )

e

1

       id155 that word w comes  

from data d and not from the noise distribution: 
ws
(
)
(
  
dp
kp
o    auxiliary binary classification problem: 

p
w
d
w
( )

w
( )
kp

(
dp

w 1
t
   
1

( )w

p
w
d

e
+

t
1
   
1

w
,

w
,

ws
(
  

|1

|1

w

noise

)

)

=

=

=

=

+

e

t
1
   
1

t
1
   
1

noise

)

       positive examples (data) vs. negative examples (noise) 

o    scaling factor k: noisy samples k times more likely than data samples 

       noise distribution: based on unigram word probabilities 

o    empirically, model can cope with un-normalized probabilities: 

( )w

1
   
1

p t
w
d

(
wp

   

w
( )
[mnih & teh, 2012; mnih & kavukcuoglu, 2013; mikolov et al, 2013a, 2013b] 

  w
,

t
1
   
1

)ws
  

   

e

|

(

)

73 

noise-        contrastive   estimation	
       id155 that word w comes  

from data d and not from the noise distribution: 
(
dp
o    auxiliary binary classification problem: 

ws
(
)
  
kp

w 1
t
   
1

( )w

e
+

w
,

ws
(
  

|1

noise

)

=

=

e

)

       positive examples (data) vs. negative examples (noise) 

o    scaling factor k: noisy samples k times more likely than data samples 

o   

       noise distribution: based on unigram word probabilities 
wsws
( )
( )
  
  

introduce log of difference between: 

=

  

       score of word w under data distribution 
       and unigram distribution score of word w 

log   

kp

noise

( )w

(
dp

|1

w
,

w

=

)
)ws
t
(
( )
      1
  
  
1

=

  

x
( )

1
xe
   +

= 1

[mnih & teh, 2012; mnih & kavukcuoglu, 2013; mikolov et al, 2013a, 2013b] 

74 

noise-        contrastive   estimation	

(
dp

|1

w
,

=

w

t
1
   
1

)

=

t
1
   
1

p
w
d

p
w
d
w
( )

t
1
   
1

+

w
( )
kp

noise

( )w

(
dp

|1

w
,

=

w 1
t
   
1

)

=

ws
(
  

)

e

e
+

ws
(
)
  
kp

( )w

noise

       new id168 to maximize: 

e

t
1
   
w
p
1
d
(
1

=
'

l
'
t
l
   
t
  
   

[
log

(
dp

|1

w
,

=

w

t
1
   
1

]
)

+

ke

p
noise

[
log

(
  
     =

ws
( )
)
)
  

   
  
   

ws
( )
  

   

k

   

=1

i

vs
(
(
  
  
  
i

|0

w
,

w

=

t
   
1

]1
)

vs
)i
(
  

(
dp
   
  
   

)
)

       compare to maximum likelihood learning: 

l
   
t
  
   

=

   
  
   

ws
( )
  

   

v
v

    =

1

(
vp

|

w

t
1
   
1

)

   
  
   

vs
( )
  

[mnih & teh, 2012; mnih & kavukcuoglu, 2013; mikolov et al, 2013a, 2013b] 

75 

