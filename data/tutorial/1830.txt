quality estimation for language output

applications

carolina scarton, gustavo paetzold and lucia specia

university of she   eld, uk

coling, osaka, 11 dec 2016

quality estimation

(cid:73) approaches to predict the quality of a language output
application     no access to    true    output for comparison

(cid:73) motivations:

(cid:73) evaluation of language output applications is hard: no single

gold-standard

(cid:73) for nlp systems in use, gold-standards are not available

(cid:73) some work done for other nlp tasks, e.g. parsing

quality estimation - parsing

task [ravi et al., 2008]

(cid:73) given: a statistical parser and its training data and some

chunk of text

(cid:73) estimate of the f-measure of the parse trees produced for that

chunk of text

features

(cid:73) text-based, e.g. length, lm perplexity
(cid:73) parse tree, e.g. number of certain syntactic labels such as

punctuation

(cid:73) pseudo-ref parse tree: similarity to output of another parser

training

(cid:73) training data labelled for f-measure based on gold-standard
(cid:73) learner optimised for correlation of f-measure

quality estimation - parsing

very high correlation and low error (in-domain): rmse = 0.014

quality estimation - parsing

(cid:73) very close to actual f-measure:

baseline (mean of dev set)

prediction

actual f-measure

baseline (mean of dev set)

prediction

actual f-measure

in-domain (wsj)

90.48
90.85
91.13

out-of-domain (brown)

90.48
86.96
86.34

(cid:73) simpler task: one possible good output; f-measure is very

telling

quality estimation - summarisation

task: predict quality of automatically produced summaries
without human summaries as references [louis and nenkova, 2013]

(cid:73) features:

(cid:73) distribution similarity and topic words     high correlation

with pyramid and responsiveness

(cid:73) pseudo-references:

(cid:73) outputs of o   -the-shelf as systems     additional summary

models

(cid:73) high correlation with human scores, even on their own

(cid:73) linear combination of features     regression task

quality estimation - summarisation

[singh and jin, 2016]:

(cid:73) features addressing informativeness (idf, concreteness,
id165 similarities), coherence (lsa) and topics (lda)
(cid:73) pairwise classi   cation and regression tasks predicting

responsiveness and linguistic quality

(cid:73) best results for regression models     responsiveness

(around 60% of accuracy)

quality estimation - simpli   cation

task: predict the quality of automatically simpli   ed versions of
text

(cid:73) quality features:

(cid:73) length measures
(cid:73) token counts/ratios
(cid:73) language model probabilities
(cid:73) translation probabilities

(cid:73) simplicity ones:

(cid:73) linguistic relationships
(cid:73) simplicity measures
(cid:73) readability metrics
(cid:73) psycholinguistic features

(cid:73) embeddings features

quality estimation - simpli   cation

qats 2016 shared task

(cid:73) the    rst qe task for text simpli   cation

(cid:73) 9 teams
(cid:73) 24 systems

(cid:73) training set: 505 instances
(cid:73) test set: 126 instances

quality estimation - simpli   cation

qats 2016 shared task

(cid:73) 2 tracks:

(cid:73) regression: 1/2/3/4/5
(cid:73) classi   cation: good/ok/bad

(cid:73) 4 aspects:

(cid:73) grammaticality
(cid:73) meaning preservation
(cid:73) simplicity
(cid:73) overall

quality estimation - simpli   cation

qats 2016 shared task: baselines
(cid:73) regression and classi   cation:

(cid:73) id7
(cid:73) ter
(cid:73) wer
(cid:73) meteor

(cid:73) classi   cation only:

(cid:73) majority class
(cid:73) id166 with all metrics

quality estimation - simpli   cation

systems:

system

uolgp
osvcml

simplenets
iit

clac

ml

gps
forests

lstms
id112

forests

deep(indi)bow mlps
misc.
smh
ms
misc
id166
uow

features

readability, sentiment,

quest features and embeddings
embeddings,
etc
embeddings
language models, meteor and com-
plexity
language models, embeddings, length,
frequency, etc
bag-of-words
quest features and mt metrics
mt metrics
quest features, semantic similarity
and simplicity metrics

quality estimation - simpli   cation

id74:

(cid:73) regression: pearson
(cid:73) classi   cation: accuracy

winners:

regression
grammaticality osvcml1
meaning
iit-meteor
simplicity
osvcml1
overall
osvcml2

classi   cation
majority-class
smh-logistic

smh-randforest-b
simplenets-id562

quality estimation - machine translation

task: predict the quality of an mt system output without
reference translations

(cid:73) quality:    uency, adequacy, post-editing e   ort, etc.
(cid:73) general method: supervised ml from features + quality

labels

(cid:73) started circa 2001 - con   dence estimation
(cid:73) how con   dent mt system is in a translation
(cid:73) mostly word-level prediction from smt internal features

(cid:73) now: broader area, commercial interest

motivation - post-editing

mt: the king closed hearings monday with deputy canary coalition
ana maria oramas gonz  alez -moro, who said, in line with the above,
that    there is room to have government in the coming months,   
although he did not disclose prints rey about reports francesco manetto.
monarch oramas transmitted to his conviction that     soon there will be
an election    because looks unlikely that rajoy or sanchez can form a
government.

motivation - post-editing

mt: the king closed hearings monday with deputy canary coalition
ana maria oramas gonz  alez -moro, who said, in line with the above,
that    there is room to have government in the coming months,   
although he did not disclose prints rey about reports francesco manetto.
monarch oramas transmitted to his conviction that     soon there will be
an election    because looks unlikely that rajoy or sanchez can form a
government.

src: el rey cerr  o las audiencias del lunes con la diputada de coalici  on
canaria ana mar    a oramas gonz  alez-moro, quien asegur  o, en la l    nea de
los anteriores, que    no hay ambiente de tener gobierno en los pr  oximos
meses   , aunque no desvel  o las impresiones del rey al respecto, informa
francesco manetto. oramas transmiti  o al monarca su convicci  on de que
   pronto habr  a un proceso electoral   , porque ve poco probable que rajoy
o s  anchez puedan formar gobierno.

by google translate

motivation - gisting

by google translate

  target: site security should be included in sex educationcurriculum for studentssource:                                              reference: site security requirements should be included in theeducation curriculum for studentsmotivation - gisting

by google translate

  target: the road boycotted a friend ... indian robin hoodkilled the poor after 32 years of prosecution.source:                                ..                                                           32                             reference: death of the indian robin hood, highway robberand friend of the poor, after 32 years on the run.uses

quality = can we publish it as is?

quality = can a reader get the gist?

quality = is it worth post-editing it?

quality = how much e   ort to    x it?

quality = which words need    xing?

quality = which version of the text is more reliable?

general method

general method

main components to build a qe system:

1. de   nition of quality: what to predict and at what level

(cid:73) word/phrase
(cid:73) sentence
(cid:73) document

2. (human) labelled data (for quality)
3. features
4. machine learning algorithm

features

source texttranslatioid4 systemconfidence indicatorscomplexity indicatorsfluency indicatorsadequacyindicatorssentence-level qe

(cid:73) most popular level

(cid:73) mt systems work at sentence-level
(cid:73) pe is done at sentence-level

(cid:73) easier to get labelled data
(cid:73) practical for post-editing purposes (edits, time, e   ort)

sentence-level qe - features

mt system-independent features:

(cid:73) sf - source complexity features:

(cid:73) source sentence length
(cid:73) source sentence type/token ratio
(cid:73) average source word length
(cid:73) source sentence 3-gram lm score
(cid:73) percentage of source 1 to 3-grams seen in the mt training

corpus

(cid:73) depth of syntactic tree

(cid:73) tf - target    uency features:

(cid:73) target sentence 3-gram lm score
(cid:73) translation sentence length
(cid:73) proportion of mismatching opening/closing brackets and

quotation marks in translation

(cid:73) coherence of the target sentence

sentence-level qe - features

(cid:73) af - adequacy features:

(cid:73) ratio of number of tokens btw source & target and v.v.
(cid:73) absolute di   erence btw no tokens in source & target
(cid:73) absolute di   erence btw no brackets, numbers, punctuation

symbols in source & target

(cid:73) ratio of no, content-/non-content words btw source & target
(cid:73) ratio of nouns/verbs/pronouns/etc btw source & target
(cid:73) proportion of dependency relations with constituents aligned

btw source & target

(cid:73) di   erence btw depth of the syntactic trees of source & target
(cid:73) di   erence btw no pp/np/vp/adjp/advp/conjp phrase labels in

source & target

(cid:73) di   erence btw no    person   /   location   /   organization    (aligned)

entities in source & target

(cid:73) proportion of matching base-phrase types at di   erent levels of

source & target parse trees

sentence-level qe - features

(cid:73) con   dence features:

(cid:73) score of the hypothesis (mt global score)
(cid:73) size of nbest list
(cid:73) using n-best to build lm: sentence id165 log-id203
(cid:73) individual model features (phrase probabilities, etc.)
(cid:73) maximum/minimum/average size of the phrases in translation
(cid:73) proportion of unknown/untranslated words
(cid:73) n-best list density (vocabulary size / average sentence length)
(cid:73) id153 of the current hypothesis to the center hypothesis
(cid:73) search graph info: total hypotheses, % discarded / pruned /

recombined search graph nodes

(cid:73) others:

(cid:73) quality prediction for words/phrases in sentence
(cid:73) embeddings or other vector representations

sentence-level qe - algorithms

(cid:73) mostly regression algorithms (id166, gp)
(cid:73) binary classi   cation: good/bad
(cid:73) kernel methods perform better
(cid:73) tree kernel methods for syntactic trees
(cid:73) nn are di   cult to train (small datasets)

sentence-level qe - predicting hter @wmt16

languages, data and mt systems

(cid:73) 12k/1k/2k train/dev/test english     german (qt21)
(cid:73) one smt system
(cid:73) it domain
(cid:73) post-edited by professional translators
(cid:73) labelling: hter

sentence-level qe - results @wmt16

english-german

system id
    ysda/sntx+id7+id166
postech/sent-id56-qv2
shef-lium/id166-nn-emb-quest
postech/sent-id56-qv3
shef-lium/id166-nn-both-emb
ugent-lt3/scate-id1662
ufal/multivec
rtm/rtm-fs-svr
uu/uu-id166
ugent-lt3/scate-id1661
rtm/rtm-svr
baseline id166
shef/simplenets-src
shef/simplenets-tgt

pearson    

spearman    

0.525
0.460
0.451
0.447
0.430
0.412
0.377
0.376
0.370
0.363
0.358
0.351
0.182
0.182

   

0.483
0.474
0.466
0.452
0.418
0.410
0.400
0.405
0.375
0.384
0.390

   
   

sentence-level qe - best results @wmt16

(cid:73) ysda: features about complexity of source (depth of parse

tree, speci   c constructions), pseudo-reference, back
translation, web-scale lm, and word alignments. trained to
predict id7 scores, followed by a linear svr to predict
hter from id7 scores.

(cid:73) postech: id56 with two components: (i) two bidirectional
id56s on the source and target sentences plus (ii) other id56s
for predicting the    nal quality: (i) is an id56-based modi   ed
id4 model that generates a sequence of vectors about target
words    translation quality. (ii) predicts the quality at sentence
level. each component is trained separately: (i) relies on the
europarl parallel corpus, (ii) relies on the qe task data.

sentence-level qe - challenges

(cid:73) data: how to obtain objective labels, for di   erent languages

and domains, which are comparable across translators?

(cid:73) how to adapt models over time?     online learning

[c. de souza et al., 2015]

(cid:73) how to deal with biases from annotators (or domains)?    

id72 [cohn and specia, 2013]

sentence-level qe - learning from multiple annotators

(cid:73) perception of quality varies
(cid:73) e.g.: english-spanish translations labelled for pe e   ort

between 1 (bad) and 5 (perfect)

(cid:73) 3 annotators: average of 1k scores: 4; 3.7; 3.3

sentence-level qe - learning from multiple annotators

[cohn and specia, 2013]

sentence-level qe - learning from multiple annotators

[cohn and specia, 2013]

sentence-level qe - learning from multiple annotators

[shah and specia, 2016]

sentence-level qe - learning from multiple annotators

predict en-fr using en-fr & en-es [shah and specia, 2016]

word-level qe

some applications require    ne-grained information on quality:

(cid:73) highlight words that need    xing
(cid:73) inform readers of portions of sentence that are not reliable

seemingly a more challenging task

(cid:73) a quality label is to be predicted for each target word
(cid:73) sparsity is a serious issue
(cid:73) skewed distribution towards good
(cid:73) errors are interdependent

word-level qe - labels

(cid:73) predict binary good/bad labels
(cid:73) predict general types of edits:

(cid:73) shift
(cid:73) replacement
(cid:73) insertion
(cid:73) deletion is an issue

(cid:73) predict speci   c errors. e.g. mqm in wmt14

word-level qe - features

(cid:73) target token, its left & right token
(cid:73) source token aligned to target token, its left & right tokens
(cid:73) boolean dictionary    ag: whether target token is a stopword, a

punctuation mark, a proper noun, a number

(cid:73) dangling token    ag (null link)
(cid:73) lm of id165s with target token ti : (ti   2, ti   1, ti ),

(ti   1, ti , ti+1), (ti , ti+1, ti+2)

(cid:73) order of the highest order id165 which starts/ends with the

source/target token

(cid:73) pos tag of target/source token
(cid:73) number of senses of target/source token in id138
(cid:73) pseudo-reference    ag: 1 if token belongs to pseudo-reference,

0 otherwise

word-level qe - algorithms

(cid:73) sequence labelling algorithms, like crf

(cid:73) classi   cation algorithms: each word tagged independently

(cid:73) nn:

(cid:73) mlp with bilingual id27s and standard features
(cid:73) id56s

word-level qe @wmt16

languages, data and mt systems

(cid:73) same as for t1
(cid:73) labelling done with tercom:

(cid:73) ok = unchanged
(cid:73) bad = insertion, substitution

(cid:73) instances: <source word, mt word, ok/bad label>

training
dev
test

sentences
12, 000
1, 000
2, 000

words % of bad words
21.4
19.54
19.31

210, 958
19, 487
34, 531

new evaluation metric:

f1-multiplied = f1-ok    f1-bad

challenge: skewed class distribution

word-level qe - results @wmt16

english-german

system id
    unbabel/ensemble
unbabel/linear
ugent-lt3/scate-rf
ugent-lt3/scate-ens
postech/word-id56-qv3
postech/word-id56-qv2
ualacant/sbi-online-baseline
cdacm/id56
shef/shef-mime-1
shef/shef-mime-0.3
baseline crf
rtm/s5-rtm-glmd
ualacant/sbi-online
rtm/s4-rtm-glmd
all ok baseline
all bad baseline

f1-mult    

f1-bad

f1-ok

0.495
0.463
0.411
0.381
0.380
0.376
0.367
0.353
0.338
0.330
0.324
0.308
0.290
0.273
0.0
0.0

0.560
0.529
0.492
0.464
0.447
0.454
0.456
0.419
0.403
0.391
0.368
0.349
0.406
0.307
0.0
0.323

0.885
0.875
0.836
0.821
0.850
0.828
0.805
0.842
0.839
0.845
0.880
0.882
0.715
0.888
0.893
0.0

word-level qe - results @wmt16

(cid:73) unbabel: linear sequential model with baseline features +

dependency-based features (relations, heads, siblings and
grandparents, etc.), and predictions by an ensemble method
that uses a stacked architecture which combines three neural
systems: one feedforward and two recurrent ones.

(cid:73) ugent: 41 features + baseline feature set to train binary
id79 classi   ers. features capture accuracy errors
using word and phrase alignment probabilities,    uency errors
using language models, and terminology errors using a
bilingual terminology list.

word-level qe - challenges

(cid:73) data:

(cid:73) labelling is expensive
(cid:73) labelling from post-editing not reliable     need better
(cid:73) data sparsity and skewness are hard to overcome    

alignment methods

(cid:73) injecting errors or    ltering positive cases

[logacheva and specia, 2015]

(cid:73) errors are rarely isolated     how to model interdependencies?

    phrase-level qe - wmt16

document-level qe

(cid:73) prediction of a single label for entire documents

(cid:73) assumption: quality of a document is more than the simple

aggregation of its sentence-level quality scores

(cid:73) while certain sentences are perfect in isolation, their

combination in context may lead to an incoherent document
(cid:73) a sentence can be poor in isolation, but good in context as it

may bene   t from information in surrounding sentences

(cid:73) application: use as is (no pe) for gisting purposes

document-level qe - labels

(cid:73) notion of quality is very subjective

[scarton and specia, 2014]

(cid:73) human labels: hard and expensive to obtain, no datasets

available

(cid:73) most work predicts meteor/id7 against an

independently created reference. not ideal:

(cid:73) low variance across documents
(cid:73) do not capture discourse issues

(cid:73) alternative: task-based labels

(cid:73) 2-stage post-editing
(cid:73) reading comprehension tests

document-level qe - features

(cid:73) average or doc-level counts of sentence-level features
(cid:73) word/lemma/noun repetition in source/target doc and ratio
(cid:73) number of pronouns in source/target doc
(cid:73) number of discourse connectives (expansion, temporal,

contingency, comparison & non-discourse)

(cid:73) number of edu (elementary discourse units) breaks in

source/target doc

(cid:73) number of rst nucleus relations in source/target doc
(cid:73) number of rst satellite relations in source/target doc
(cid:73) average quality prediction for sentences in docs

algorithms: same as for sentence-level

document-level qe @wmt16

languages, data and mt systems

(cid:73) english     spanish
(cid:73) documents by all wmt8-13 translation task mt systems
(cid:73) 146/62 documents for training/test
(cid:73) labelling: 2-stage post-editing method

1. pe1: sentences are post-edited in arbitrary order (no context)
2. pe2: post-edited sentences are further edited within

document context

document-level qe @wmt16

label

(cid:73) linear combination of hter values:

w1    pe1    mt + w2    pe2    pe1

(cid:73) w1 and w2 are learnt empirically to:

(cid:73) maximise model performance (mae/pearson) and/or
(cid:73) maximise data variation (stdev/avg)

document-level qe - results @wmt16

system id pearson   s r

spearman   s       

english-spanish
    ushef/base-emb-gp
    rtm/rtm-fs+pls-tree
rtm/rtm-fs-svr
baseline id166
ushef/graph-disc

0.391
0.356
0.293
0.286
0.256

0.393
0.476
0.360
0.354
0.285

(cid:73) ushef: 17 baseline features + id27s from source
documents combined using gp. document embeddings are the
average of the id27s in the document. gp model
was trained with 2 kernels: one for the 17 baseline features
and another for the 500 features from the embeddings.

document-level qe - new label

mae gain (%) compared to    mean    baseline:

document-level qe - new label

document-level qe - challenges

(cid:73) quality label still an open issue

(cid:73) should take into account purpose of translation
(cid:73) should reliably distinguish di   erent documents

(cid:73) feature engineering: few tools for discourse processing

(cid:73) topic and structure of document
(cid:73) relationship between its sentences/paragraphs

(cid:73) relevance information needed: how to factor it in

(cid:73) features: qe for sentence + sentence ranking methods

[turchi et al., 2012]

(cid:73) labels

participants @wmt16

wl/pl

sl dl

centre for development of advanced computing, india
pohang university of science and technology,
republic of korea
referential translation machines, turkey
university of she   eld, uk
university of she   eld, uk &
lab. d   informatique de l   universit  e du maine, france
university of alicante, spain
nile university, egypt &
charles university, czech republic
ghent university, belgium
unbabel, portugal
uppsala university, sweden
yandex, russia

x

x
x
x

x

x
x

x
x

x
x
x

x

x
x

x
x

neural nets for qe

as features:

(cid:73) nlm for sentence-level
(cid:73) id27s for word, sentence and doc-level

as learning algorithm:

(cid:73) mlp proved e   ective until 2015
(cid:73) 2016 submissions use id56s for sentence-level (postech,

simplenets), word-level (unbabel), phrase-level (cdac)

qe in practice

does qe help?

(cid:73) time to post-edit subset of sentences predicted as    low pe

e   ort    vs time to post-edit random subset of sentences
[specia, 2011]

language

no qe

qe

fr-en
en-es

0.75 words/sec
0.32 words/sec

1.09 words/sec
0.57 words/sec

qe in practice

(cid:73) productivity increase [turchi et al., 2015]
(cid:73) comparison btw post-editing with and without qe
(cid:73) predictions shown with binary colour codes (green vs red)

    qe in practice

(cid:73) mt system selection: id7 scores [specia and shah, 2016]

majority class best qe-selected best mt system

en-de
de-en
en-es
es-en

16.14
25.81
30.88
30.13

18.10
28.75
33.45
38.73

17.04
27.96
25.89
37.83

qe in practice

(cid:73) smt self-learning: de-en smt enhanced with mt data

   best    according to qe [specia and shah, 2016]

baseline(itera,on(1(itera,on(2(itera,on(3(itera,on(4(itera,on(5(itera,on(6(smt(18.43(18.78(19.1(19.21(19.46(19.45(19.42(rbmt(18.43(18.62(18.99(19.11(19.29(19.25(19.29(references(18.43(18.91(19.17(19.33(19.42(19.41(19.43(random(18.43(18.59(18.91(19.11(19.1(19.21(19.27(18(18.2(18.4(18.6(18.8(19(19.2(19.4(19.6(id7%qe in practice

(cid:73) smt self-learning: en-de smt enhanced with mt data

   best    according to qe [specia and shah, 2016]

baseline(itera,on(1(itera,on(2(itera,on(3(itera,on(4(itera,on(5(itera,on(6(smt(13.31(13.62(13.99(14.4(14.31(14.42(14.39(rbmt(13.31(13.43(13.74(13.99(14.21(14.31(14.25(references(13.31(13.72(14.09(14.2(14.49(14.44(14.43(random(13.31(13.4(13.65(13.92(14.2(14.23(14.25(13(13.2(13.4(13.6(13.8(14(14.2(14.4(14.6(id7%quality estimation for language output

applications

carolina scarton, gustavo paetzold and lucia specia

university of she   eld, uk

coling, osaka, 11 dec 2016

references i

c. de souza, j. g., negri, m., ricci, e., and turchi, m. (2015).
online multitask learning for machine translation quality estimation.
in 53rd annual meeting of the association for computational linguistics,
pages 219   228, beijing, china.

cohn, t. and specia, l. (2013).
modelling annotator bias with multi-task gaussian processes: an
application to machine translation quality estimation.
in 51st annual meeting of the association for computational linguistics,
acl, pages 32   42, so   a, bulgaria.

logacheva, v. and specia, l. (2015).
the role of arti   cially generated negative data for quality estimation of
machine translation.
in 18th annual conference of the european association for machine
translation, eamt, antalya, turkey.

references ii

louis, a. and nenkova, a. (2013).
automatically assessing machine summary content without a gold
standard.
computational linguistics, 39(2):267   300.

ravi, s., knight, k., and soricut, r. (2008).
automatic prediction of parser accuracy.
in conference on empirical methods in natural language processing,
pages 887   896, honolulu, hawaii.

scarton, c. and specia, l. (2014).
document-level translation quality estimation: exploring discourse and
pseudo-references.
in 17th annual conference of the european association for machine
translation, eamt, pages 101   108, dubrovnik, croatia.

references iii

shah, k. and specia, l. (2016).
large-scale multitask learning for machine translation quality estimation.
in conference of the north american chapter of the association for
computational linguistics: human language technologies, pages
558   567, san diego, california.

singh, a. and jin, w. (2016).
ranking summaries for informativeness and coherence without reference
summaries.
in the twenty-ninth international florida arti   cial intelligence research
society conference, pages 104   109, key largo, florida.

specia, l. (2011).
exploiting objective annotations for measuring translation post-editing
e   ort.
in 15th conference of the european association for machine translation,
pages 73   80, leuven.

references iv

specia, l. and shah, k. (2016).
machine translation quality estimation: applications and future
perspectives, page to appear.
springer.

turchi, m., negri, m., and federico, m. (2015).
mt quality estimation for computer-assisted translation: does it really
help?
in 53rd annual meeting of the association for computational linguistics,
pages 530   535, beijing, china.

turchi, m., specia, l., and steinberger, j. (2012).
relevance ranking for translated texts.
in 16th annual conference of the european association for machine
translation, eamt, pages 153   160, trento, italy.

