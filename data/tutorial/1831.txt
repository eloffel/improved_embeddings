emnlp tutorial 2016

methods and theories for 

large-scale id170

xu sun

yansong feng

peking university

peking university

xusun@pku.edu.cn

fengyansong@pku.edu.cn

content and lecturer

models & id173

    conventional model

    latent model

    neural model

xu sun

structures & applications

    sequence structure

    tree/graph structure

yansong feng

2

content and lecturer

models & id173

    conventional model

    latent model

    neural model

xu sun

structures & applications

    sequence structure

    tree/graph structure

yansong feng

3

models and id173 for 
large-scale id170

xu sun

peking university

xusun@pku.edu.cn

outline

introduction

model

conventional model

latent model

neural model

id173

how-tos

5

outline

introduction

model

conventional model

latent model

neural model

id173

how-tos

6

what is id170/classification?

    types of classification

binary classification

multiclass classification

structured classification

7

why id170?

    structures are important in natural language 

processing

linguists also attempt to understand the rules regarding 
language structures

pragmatics

syntax

semantics

morphology

id102

and many 

more

r wardhaugh. introduction to linguistics. 

8

why id170?

    challenges in nlp involve understanding and

generation

    understanding the structures in natural languages is 

an essential step towards the goal

m bates. models of natural language understanding.
d jurafsky and jh martin. speech and language processing.
cd manning and h sch  tze. foundations of statistical natural language processing.

9

why id170?

    however, most of the time, structure prediction is

not straight-forward

why it is hard? --natural languages encode all of 

the structures in a linear form

n chomsky. language and mind.
n chomsky. reflections on language.
mak halliday. language structure and language function.
n chomsky. knowledge of language: its nature, origin, and use.
d biber, s conrad, r reppen. corpus linguistics: investigating language structure and use.

10

why id170?

    id170 helps to recover the structures 

in natural languages

id52 (collins, acl 2002; gimenez & marquez, lrec 
2004.; shen et al., acl 2007; s  egaard, acl-hlt 2011; sun, nips 2014; 
collobert et al., jmlr 2011; huang et al., 2015)

chunking (kudo & matsumoto, naacl 2001; collins, acl 2002; 
mcdonald et al., hlt-emnlp 2005; sun et al., coling 2008; collobert 
et al, jmlr 2011; huang et al., 2015 )

ner (florian et al., hlt-naacl 2003; chieu, conll 2003; ando & 
zhang, jmlr 2005; collobert et al., jmlr 2011; passos et al., conll 
2014; huang et al., 2015)

parsing (earley, 1970; collins, eacl 1997; klein & manning, acl 
2003; sha & pereira, hlt-naacl 2003; nivre, iwpt 2003; collins, acl 
2004; nivre & scholz, coling 2004; mcdonald, hlt-emnlp 2005; 
zhang & clark, emnlp 2008; zhang & nivre, acl-hlt 2011;socher, et 
at., emnlp 2013; chen & manning, emnlp 2014)

id40, summarization, machine 
translation...

11

outline

introduction

model

conventional model

latent model

neural model

id173

how-tos

12

outline

    conditional random field
    structure id88
    mira
    probabilistic id88

conventional 

model

reduce 
annotation 
engineering

latent    
model

    latent dynamic crf
    latent variable id88

    recurrent neural network
    long short-term memory
    lstm-crf

neural model

reduce 
feature 
engineering

13

id49 (crfs)

    proposed by lafferty et al. (2001)

    maximize a id155

                  ,      =

1

    (    ,    )

exp                       (    ,     )

             ,      =          exp                       (       ,     )

    global model

    predict global structure, not local classifier

    training: globally normalized objective

    decode: viterbi algorithm 

y1

x1

y2

x2

y3

x3

y4

x4

yn

xn

14

id49 (crfs)

    proposed by lafferty et al. (2001)

    maximize a id155

                  ,      =

1

    (    ,    )

exp                       (    ,     )

             ,      =          exp                       (       ,     )

    global model

    predict global structure, not local classifier

    training: globally normalized objective

    decode: viterbi algorithm 

but the training speed is quite 
slow   

15

structured id88

    proposed by collins (2002)

    simple and fast

    no gradient computation

    update only on error

    viterbi decode

m

k

g

h

l

i

j

    theoretical guarantee

a

b

c

d

e

f

    converge if data is separable

16

structured id88

    proposed by collins (2002)

    simple and fast

    no gradient computation

    update only on error

    viterbi decode

    theoretical guarantee

    converge if data is separable

17

structured id88

    inexact search

    greedy search

    id125

    parameter update

    early update (collins & roark, acl 2004)

    margin infused relaxed algorithm (mira) ( crammer et al. 2006)

    max-violation (huang et al., naacl 2012)

figure from huang et al. (2012)

18

for large-scale id170

    current id170 methods are not ideal

    a trade-off between accuracy and speed   

low                            performance                        high

crfs

k-best mira

mira

structured id88

fast                                speed                             slow

19

for large-scale id170

    a solution works well in practice for large-scale 

id170 problems

    introducing probabilistic information into 

id88s

    this goes to probabilistic id88 (sapo) (sun 

2015)

x. sun. towards shockingly easy structured 
classification: a search-based probabilistic 
online learning framework. 2015

20

probabilistic id88 (sapo)

    proposed by sun (2015)

    same (or even higher) accuracy like crf

    fast training speed like id88

root   john   hit   the   ball   with   the   bat

root   john   hit   the   ball   with   the   bat

root   john   hit   the   ball   with   the   bat

training sample

      

root   john   hit   the   ball   with   the   bat

x. sun. towards shockingly easy structured 
classification: a search-based probabilistic 
online learning framework. 2015

0.53

0.25

0.03

probabilistic 
id88 

update

21

probabilistic id88 (sapo)

    proposed by sun (2015)

root   john   hit   the   ball   with   the   bat

root   john   hit   the   ball   with   the   bat

training sample

1-best sample

root   john   hit   the   ball   with   the   bat

root   john   hit   the   ball   with   the   bat

root   john   hit   the   ball   with   the   bat

training sample

      

root   john   hit   the   ball   with   the   bat

n-best samples with id203 

0.53

0.25

0.03

structured 
id88 

update

probabilistic 
id88 

update

22

probabilistic id88 (sapo)

    theoretical guarantee of convergence

probabilistic id88 converges!

23

probabilistic id88 (sapo)

    experiment results

it indicates the number of samples is 
not the larger the better, why?

    similar or even higher accuracy compared with crfs, 

id88 and mira

24

probabilistic id88 (sapo)

    experiment results

    much faster than 

crfs 

    nearly as fast as 

id88s

25

probabilistic id88 (sapo)

    experiment result

no rising complexity of weights compared to mira or perc.

26

for large-scale id170

    probabilistic id88 sun (2015)

d
o
o
g

 
 
 

e
c
n
a
m
r
o

f
r
e
p

 
 
 
 
 

d
a
b

crfs

sapo

k-best mira

mira

structured id88

slow                                speed                             fast

27

outline

    typical methods need large-scale annotations

    problem in reality

    lack of annotations

    inaccurate annotations

    latent model

    reduce annotation engineering

28

outline

    conditional random field
    structure id88
    mira
    probabilistic id88

conventional 

model

reduce 
annotation 
engineering

latent 
model

    latent dynamic crf
    latent variable id88

    recurrent neural network
    long short-term memory
    lstm-crf

neural model

reduce 
feature 
engineering

29

motivation

    latent-structures (hidden info) are important in 

natural language processing (matsuzaki et al., acl 2005; petrov & klein, nips 

2008)

parsing: learn refined grammars with latent info

s

np

vp

prp

vbd

np

he

heard

dt

nn

.

.

the

voice

30

motivation

    latent-structures (hidden info) are important in 

natural language processing (matsuzaki et al., acl 2005; petrov & klein, nips 

2008)

parsing: learn refined grammars with latent info

s-x

np-x

vp-x

.-x

prp-x

vbd-x

np-x

.

he

heard

dt-x

nn-x

the

voice

31

motivation

    latent-structures (hidden info) are important in 

id33 (honnibal & johnson, tacl 2014)

    a transition system (arc-eager) extended with an edit 

transition 

32

motivation

    latent-structures (hidden info) are important in 

id33 (honnibal & johnson, tacl 2014)

    different transition sequences to the same gold-standard tree

33

motivation

    latent-structures (hidden info) are important in 

id33 (honnibal & johnson, tacl 2014)

dynamic oracle
map a configuration to a set of transitions
partially annotated
latent variable

34

motivation

    latent-structures (hidden info) are important in 

id53 (fader et al., sigkdd 2014)

latent variable

35

motivation

    latent-structures (hidden info) are important in 
multi-intent id103 (xu & sarikaya, interspeech 2013)

latent variable

36

challenges

    latent-structures (hidden info) are important in 
multi-intent id103 (xu & sarikaya, interspeech 2013)

problem 1:

annotating latent info requires much 

more tags and human time

    costly to annotate

problem 2:

different tasks have different latent info.

    hard to annotate

37

latent-dynamic crfs

    a solution without additional annotation

latent-dynamic crfs (ldcrf)

[morency et al., cvpr 2007; sun et al., coling 2008]

* no need to annotate latent info

figure from xu & sarikaya, interspeech 2013.

38

latent-dynamic crfs

    latent-dynamic crfs (ldcrf)

(morency et al., cvpr 2007; sun et al., coling 2008)

y1

h1

x1

y1

x1

y2

h2

x2

y2

x2

y3

h3

x3

y4

h4

x4

yn

hn

xn

ldcrf

we can think (informally) it as 

   crf + unsup. learning on latent info   

y3

x3

y4

x4

yn

xn

conditional 
random fields

39

latent-dynamic crfs

    latent-dynamic crfs (ldcrf)

(morency et al., cvpr 2007; sun et al., coling 2008)

y1

h1

x1

y2

h2

x2

y3

h3

x3

y4

h4

x4

yn

hn

xn

ldcrf

40

            jyjhpph:),|(),|(hxhxy                                       jyjhkkkxhz:),(exp),(1hxhf      for large-scale id170

    latent-dynamic crfs (ldcrf)

    training is slow 

    may need week-level time

    solution

    id88 is much faster than crf

    latent crf -> latent id88

sun et al. latent variable id88 algorithm for 
structured classification. ijcai 2009.

sun et al. latent structured id88s for large-
scale learning with hidden information. tkde 2013.

41

latent variable id88

    for fast training of latent variable models

(sun et al., ijcai 2009; sun et al., tkde 2013)

seg-0

seg-1

seg-2

noseg-0

noseg-1

noseg-2

restricted 
viterbi 
output  by 
gold 
annotation

global 
viterbi 
output

a related work on machine translation: liang et al., 2006

42

latent variable id88

    for fast training of latent variable models

(sun et al., ijcai 2009; sun et al., tkde 2013)

ldcrf:

{

normally, batch training 
(do weight update after go over all samples) 

latent variable id88 (sun et al., 2009) :

{

online training
(do weight update on each sample)

43

:()argmax(|,)p         yhhyy*hx   projargmax'(|,)p   hh*hx   latent variable id88

    ldcrf training method

seg-0

seg-1

seg-2

noseg-0

noseg-1

noseg-2

these            are             her         flowers           .

44

latent variable id88

    ldcrf training method

seg-0

seg-1

seg-2

noseg-0

noseg-1

noseg-2

these            are             her         flowers           .

45

*:()argmax(|,)p         yhhyyhx   projlatent variable id88

    latent id88 training

seg-0

seg-1

seg-2

noseg-0

noseg-1

noseg-2

these            are             her         flowers           .

46

latent variable id88

    latent id88 training

seg-0

seg-1

seg-2

noseg-0

noseg-1

noseg-2

these            are             her         flowers           .

restricted 
viterbi 
output  by 
gold 
annotation

global 
viterbi 
output

47

]),,|(maxarg[]),,,|(maxarg[*1iiiiiiiiiffx  xhfx  xyhf    hh            latent variable id88

    theoretical guarantee of convergence

    as far as traditional id88 is separable, 

latent structured id88 is also separable.

48

latent variable id88

    theoretical guarantee of convergence

    latent id88 still converges

49

latent variable id88

    experiment on synthetic data:

much better accuracy than crf & id88

latent perc

ldcrf

crf

averaged 
id88

significance of latent info

50

latent variable id88

    good performance in id53 (fader et al. sigkdd2014)

    use query with partial anotation as latent variable

oqa is based on 
latent variable 
id88

(sun et al., ijcai 2009; 
sun et al., tkde 2013)

51

latent variable id88

    good performance in speech sentence classification 

(xu & sarikaya, interspeech 2013)

    split multi-intent using latent variable

    based on latent variable id88 (sun et al., ijcai 2009; sun et al., tkde 2013)

52

latent variable id88

    good performance in id29 (zhou et al., ijcai 2013)

    hybrid tree as latent structure variable

    based on latent variable id88 (sun et al., ijcai 2009; sun et al., tkde 2013)

better 
performance

53

latent variable id88

    good performance in id33            

(honnibal & johnson, tacl 2014)

    dynamic oracle/transitions as latent variable

    train with latent variable id88 (sun et al., ijcai 2009; sun et al., tkde 2013)

much better 
compared with 
baseline model

54

latent variable id88

    good performance in coreference resolution 

(fernandes et al., cl 2012)

    use structures of coreference trees as latent variable

    based on latent variable id88 (sun et al., ijcai 2009; sun et al., tkde 2013)

55

for large-scale id170

    latent variable id88 

    proposed by sun (2009, 2013)

    fast and accurate

    accuracy equal or even better than crf

    almost as fast as id88

    suitable for large-scale id170 problems

sun et al. latent variable id88 algorithm for 
structured classification. ijcai 2009.

sun et al. latent structured id88s for large-
scale learning with hidden information. tkde 2013.

56

outline

    latent model can reduce annotation engineering

    furthermore, how to reduce the cost of feature 

engineering?

    neural model

    automatically extract features

57

outline

    conditional random field
    structured id88
    mira
    probabilistic id88

conventional 

model

reduce 
annotation 
engineering

latent  
model

    latent dynamic crf
    latent variable id88

    recurrent neural network
    long short-term memory
    lstm-crf

neural 
model

reduce 
feature 
engineering

58

motivation

    problem in feature 

engineering

    require linguistics

knowledge

    a lot of feature templates

    ad-hoc, some features are 

not very reasonable

    task-sensitive

    neural networks

    automatically learn features 

in hidden layers

59

neural network

    many kinds

    feed forward nn

    id28

    convolutional nn

    image processing

    recursive/recurrent nn

    id170

60

id56

    id56 (socher et al., icml 2011)

    model hierarchical structures

    condition on each sub-structure independently

61

recurrent neural network (id56)

    recurrent neural network (elman, cognitive science 1990) 

    model time series

    predict linear-chain structures

    conditioned on all previous input

picture from christopher olah

62

recurrent neural network (id56)

    problems

    gradient exploding/vanishing (pascanu et al., icml 2013)

    hard to capture long-term dependencies

figure from denny britz 

63

long short-term memory (lstm)

    long short-term memory (hochreiter and schmidhuber 1997)

    a lasting linear memory

    capture long distance dependency

    three gates: input, forget and output gates

    control modification to the memory

64

introduction of gates

    gate

    sigmoid-activated layer

    output value from 0 to 1

    member-wise multiplication

    how much to flow through

    gates in lstm

    forget gate

    how much old memory needs to be remembered

some picture from christopher olah

65

introduction of gates

    gate

    sigmoid-activated layer

    output value from 0 to 1

    member-wise multiplication

    how much to flow through

    gates in lstm

    forget gate

    how much old memory needs to be remembered

    input gate

some picture from christopher olah

66

introduction of gates

    gate

    sigmoid-activated layer

    output value from 0 to 1

    member-wise multiplication

    how much to flow through

    gates in lstm

    forget gate

    how much old memory needs to be remembered

    input gate

    output gate

some picture from christopher olah

67

introduction of gates

    id56

    lstm

    vanishing influence

    situation-aware

outputs

hidden 
layer

inputs

time

1

2

3

4

5

6

7

outputs

hidden 
layer

inputs

time

1

2

3

4

5

6

7

68

long short-term memory (lstm)

    problem of lstm

    can be slow for large models (hard to train)

    structure complexity is higher

    structure redundancy?

    too much parameters

picture from christopher olah

69

refinement of lstm

    peepholes

    introduced by gers (2000) 

    include current memory when compute gates

picture from christopher olah

70

refinement of lstm

    fully connected gates

    included in hochreiter and schmidhuber (1997) 

    gates are also recurrent

picture from christopher olah

71

refinement of lstm

    gated recurrent unit (gru)

    introduced by chung et al. (2014)

    coupled forget and input gates with structure simplification

picture from christopher olah

72

more recent development

    sequence to sequence neural network (sutskever et al., nips 2014)

    encoder & decoder

    the encoder information is stored in a fixed-length vector

w

x

y

z

<eos>

a

b

c

<eos>

w

x

y

z

encoder

73

more recent development

    sequence to sequence neural network (sutskever et al., nips 2014)

    encoder & decoder

    the encoder information is stored in a fixed-length vector

w

x

y

z

<eos>

a

b

c

<eos>

w

x

y

z

    learn to align

    id4

decoder

74

structured neural network in machine translation

    attention-based neural network (bahdanau et al. 2014; luong et al. 2015)

    each hidden state has an unique weight/attention/importance

75

for large-scale id170

    training large-scale neural models is costly

    numerous parameters

    very slow

    a id4 model may take weeks (even months) to train

    how to accelerate training speed?

    parallel training

    especially, asynchronous (lock-free) parallel training

76

asynchronous parallel learning

    motivation

    asynchronous parallel learning is very popular for 

traditional sparse feature models

    e.g., hogwild!(niu et al. nips 2011)

    however, previous asynchronous parallel learning 

methods do not suit neural networks

    because nn is dense feature model

    previous parallel learning for dense feature models is 

mostly synchronous, e.g., mini-batch parallel learning, gpu 
parallel learning

77

asynchronous parallel learning

    motivation

    asynchronous parallel learning is very popular for 

traditional sparse feature models

    e.g., hogwild!(niu et al. nips 2011)

1. simple case     no problem

reading parameters from shared memory

computing gradients

writing parameters to shared memory

78

asynchronous parallel learning

    motivation

    asynchronous parallel learning is very popular for 

traditional sparse feature models

    e.g., hogwild!(niu et al. nips 2011)

2. this case is called 
gradient delay case

    more complicated, but 
problem solved for sparse 
feature models (niu et al. nips 
2011)

79

asynchronous parallel learning

    motivation

    asynchronous parallel learning is very popular for 

traditional sparse feature models

    e.g., hogwild!(niu et al. nips 2011)

80

asynchronous parallel learning

    3. even more difficult case: gradient error case

    happens for dense feature models, like neural networks

    actions (r, g & w) are time-consuming

    read-overwrite and write-overwrite problems

   not well studied before, how to deal with this 
problem?

81

experimental observations

    gradient error is inevitable in asynchronous training 

of neural networks in real-world tasks

82

experimental observations

    gradient error is inevitable in asynchronous training 

of neural networks in real-world tasks

so asynchronous parallel learning is 
doomed for neural networks?

   no, still this problem can be solved

83

a recent solution

    an asynchronous parallel learning solution for fast training of 

neural networks proposed by sun (coling 2016)

    asynchronous parallel learning with gradient error (asyngrad) 

    algorithm

x. sun. asynchronous parallel learning for neural 
networks and structured models with dense features. 
coling 2016.

84

theoretical analysis

    can asyngrad still converge with gradient errors?

even though there are gradient errors, asyngrad does 

not diverge    it still converges near the optimum with a 

small distance, when the errors are bounded.

    the assumptions usually hold in the final convergence 

region

    confirmed by real-world experiments

bounded gradient errors

85

experiments on lstm

experiments show that asyngrad still converge even 

with a high gradient error rate

86

experiments on lstm

    no loss on accuracy/f-score

asyngrad

    with substantially faster training speed

87

asyngrad

    gradient errors are common and inevitable in 
asynchronous training of dense feature models

    asyngrad tolerates gradient errors

    for dense feature models, such as neural networks and dense-

crf

    with faster speed and no loss on accuracy

    an alternative learning approach for large-scale

id170s using neural networks

88

thanks!

any questions until now?

89

outline

introduction

model

conventional model

latent model

neural model

id173

how-tos

90

overfitting

    models for large-scale id170 often 

suffer from overfitting

    overfitting

    low error rate in training set

    high error rate in test set

    why overfitting?

    complex model

    too many parameters, too little data

    how to deal with?

    penalty

    reduce complexity

91

weight id173

    penalty parameters in id168

    min

    

                     ,     ,      +                                                      

    l1 regularizer

                                                 =          

   

    

            

                                             =                     (        )

    l2 regularizer

                                                 =

    
2

     2

   

    

            

                                             =             

92

structure id173

    motivation

    reduce complexity of model structure

    structure id173 (sun. nips 2014)

    complex structure -> simple structure

    faster

    easy to implement

    theoretical guarantee

93

illustration

    complex structures (high complexity)

    simple structures (low complexity)

94

structure id173

    structure id173 (sr) can find good 

complexity

    simply split the structures!

    can (almost) be seen as a preprocessing step of the training 

data

95

structure id173

    will the split causes feature loss?     loss of long 

distance features?

no loss of any (long distance) features
    we can first extract features, then split the structures
    or, by simply copying observations to mini-samples, 
i.e., the split is only on tag-structures, like this:

96

structure id173

    is structure id173 also required for test data?

no, no use of sr for testing data (in current 
implementation & experiments)
   like other id173 methods, sr is only for 
the training
   i.e., no sr on the test stage (no decomposition of 
test samples)!

97

structure id173

    structure & weight id173

the implementation is very simple

98

theoretical analysis: overfitting risk

expected 
risk 
(risk on test 
data)

empirical 
risk 
(risk on 
training data)

overfitting risk 
(risk of overfitting from 
training data to test data)

99

theoretical analysis: overfitting risk

complexity of structure (nodes of a training sample with 
structured dependencies)
    complex structure leads to higher overfitting risk

100

theoretical analysis: overfitting risk

strength of structure id173 (strength of 
decomposition)
    stronger sr leads to reduction of overfitting risk

101

theoretical analysis: overfitting risk

number of training samples 
    more training samples leads to reduction of overfitting risk

102

theoretical analysis: overfitting risk

   conclusions from our analysis:

1. complex structure     low empirical risk & high overfitting risk
2. simple structure     high empirical risk & low overfitting risk

3. need a balanced complexity of structures 

103

theoretical analysis: overfitting risk

    in other words, more intuitively:

1. too complex structure     high accuracy on training + very easy to 

overfit     low accuracy on testing

2. too simple structure     very low accuracy on training + not easy to 

overfit     low accuracy on testing

proper structure     good accuracy on training + not easy to overfit 
    high accuracy on testing

104

theoretical analysis: overfitting risk

1. simple structure     low overfitting risk & high empirical risk
2. complex structure     high overfitting risk & low empirical risk
3. need a balanced complexity of structures

some intuition in the proof (as in the full version paper):
1) the decomposition can improve stability
2) better stability leads to better generalization (less overfitting) 

105

theoretical analysis: learning speed

    sr also with faster speed 

(a by-product of simpler structures)

    using structure id173 can 

quadratically accelerate the 
convergence rate

106

some advantages

    if the original obj. function is convex, can still keep 

the convexity of the objective function

    no conflict with the weight id173

    e.g, l2, and/or l1 id173

    general purpose and model-independent 

(because act like a preprocessing step)

    e.g., can be used for different types of models, including crfs, 

id88s, & neural networks

107

experiments-1: accuracy

state-of-the-art scores on competitive tasks

108

experiments-2   learning speed

    also with faster speed 

(a by-product of simpler structures)

109

for large-scale id170

    question: is structure complexity matters in structured 

prediction?

    theoretical analysis to the question

    1) yes it matters

    2) high complexity of structures     high overfitting risk

    3) low complexity     high empirical risk

    4) we need to find an optimal complexity of structures

    proposed a solution

    split the original structure to find the optimal complexity

    better accuracies in real tasks, & faster (a by-product)

this work is published at nips 2014:
xu sun. structure id173 for id170. in 
advances in neural information processing systems (nips). 
2402-2410. 2014

110

drop out

    proposed by srivastava et al.(2014)

    part of neurons do not participate in forward pass 

and id26

    only use in training

    advantage

    fewer parameters per training sample

    reduce training time

111

drop out

    experiment on minst (srivastava, et al. jmlr 2014)

112

drop out

    experiment on penn tree bank (zaremba, et al. 2015)

    id38 measured by perplexity

113

discussion of techniques

    strutreg

    drop out

    decomposition of structure

    deactivate neurons

    randomly

    randomly

    lstm

    relu

    forget useless information

    sparse activation

    learned

    forced

output
s

hidden 
layer

inputs
time

1

2

3

4

5

6

7

picture credit: christopher olah

114

discussion of techniques

struct 
reg

relu

relations?

drop 
out

lstm

reduce complexity

115

outline

introduction

model

conventional model

latent model

neural model

id173

how-tos

116

how to choose a model

    conventional models can achieve the state-of-the-

arts results

    stable for production setup

    latent models are good for tasks short of 

annotations

    neural models are promising

    especially for high-level tasks

    id31

    summarization, composition, translation

    many nice frameworks available

    theano, torch, tensorflow

    caffe, cntk

117

how to choose an optimizer

    for models other than neural models

    id88

online, fast convergence

    for neural models

    mini-batch sgd

parameter-scaling
adagrad
adadelta
adam
rmsprop
less tuning, fast

momentum-based
momentum
nesterov accelerated gradient (nag)
better results, more tuning

    using gpus can lead to significant speed-ups, compared to use 

cpus only

118

visualization of optimization algorithms

long valley

picture from alec radford

119

visualization of optimization algorithms

saddle point

picture from alec radford 

120

visualization of optimization algorithms

beale's function

picture from alec radford

121

visualization of optimization algorithms

noisy data

picture from alec radford

122

thanks!

any question?

references and further reading

1.

2.

3.

4.

5.

6.

7.

8.

9.

a. fader, l. zettlemoyer, and o. etzioni. open id53 over curated and extracted knowledge bases. 
sigkdd 2014.

a. graves. neural networks. supervised sequence labelling with recurrent neural networks. springer 2012.

a. krizhevsky, i. sutskever, and g. hinton. id163 classification with deep convolutional neural networks. 
nips 2012.

a. passos, v. kumar, and a. mccallum. lexicon infused phrase embeddings for named entity resolution. conll 
2014.

a. radford. http://imgur.com/s25rsor/. 2014.

a. radford. visualizing optimization algors. http://imgur.com/a/hqolp/. 2014.

a. ratnaparkhi. a maximum id178 model for part-of-speech tagging. emnlp 1996.

a. s  gaard. semisupervised condensed nearest neighbor for part-of-speech tagging. acl-hlt 2011.

c. olah. understanding id137. http://colah.github.io/posts/2015-08-understanding-lstms/. 2015.

10. c. olah. visualizing representations: deep learning and human beings. http://colah.github.io/posts/2015-01-

visualizing-representations/. 2015.

11. c.d. manning and h. sch  tze. foundations of statistical natural language processing. mit press 1999.

12. c.d. manning and h. sch  tze. foundations of statistical natural language processing. mit press 1999.

13. c.d. manning. part-of-speech tagging from 97% to 100%: is it time for some linguistics? cicling 2011.

124

references and further reading

14. d. andor, c. alberti, d. weiss, a. severyn, a. presta, k. ganchev, s. petrov, and m. collins. globally normalized 

transition-based neural networks. arxiv:1603.06042.

15. d. bahdanau, k. cho, and y. bengio. id4 by jointly learning to align and translate. 

arxiv:1409.0473.

16. d. biber, s. conrad, and r. reppen. corpus linguistics: investigating language structure and use. cambridge 

university press 1998.

17. d. britz. recurrent neural networks tutorial, part 3 - id26 through time and vanishing gradients. 
http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-
and-vanishing-gradients/. 2015.

18. d. chen and c.d. manning. a fast and accurate dependency parser using neural networks. emnlp 2014.

19. d. jurafsky and j.h. martin. speech and language processing. prentice hall 2008.

20. d. klein and c.d. manning. accurate unlexicalized parsing. acl 2003.

21. d. mcclosky, e. charniak, and m. johnson. effective self-training for parsing. hlt-naacl 2006.

22.

e. charniak and m. johnson. coarse-to-fine n-best parsing and maxent discriminative reranking. acl 2005.

23.

e. fernandes, c. dos santos, and r. milidi  . latent trees for coreference resolution. cl 2014.

24.

f. gers. long short-term memory in recurrent neural networks. phd diss., universit  t hannover 2001.

25.

f. sha and f. pereira. id66 with id49. hlt-naacl 2003.

26.

f.v. veen. the neural network zoo. http://www.asimovinstitute.org/neural-network-zoo/. 2016.

125

references and further reading

27. h. zhou, y. zhang, s. huang, and j. chen. a neural probabilistic structured-prediction model for transition-

based id33. acl 2015.

28. h.l. chieu. id39 with a maximum id178 approach. conll 2003.

29.

i. sutskever, o. vinyals, and q. le. sequence to sequence learning with neural networks. nips 2014.

30.

j. chung,   . g  l  ehre, k. cho, and y. bengio. empirical evaluation of gated recurrent neural networks on 
sequence modeling. arxiv:1412.3555.

31.

j. earley. an efficient context-free parsing algorithm. communications of the acm 1970.

32.

j. gim  nez and l. marquez. id166tool: a general pos tagger generator based on support vector machines. 
lrec 2004.

33.

j. koutn  k, k. greff, f. gomez, and j. schmidhuber. a clockwork id56. icml 2014.

34.

j. lafferty, a. mccallum, and f. pereira. id49: probabilistic models for segmenting and 
labeling sequence data. icml. 2001.

35.

j. martens. deep learning via hessian-free optimization. icml 2010.

36.

j. nivre and m. scholz. deterministic id33 of english text. coling 2004.

37.

j. nivre. an efficient algorithm for projective id33. iwpt 2003.

38.

j. zhou, j. xu, and w. qu. efficient latent structural id88 with hybrid trees for id29. ijcai 
2013.

39.

j.l elman. finding structure in time. cognitive science 1990.

126

references and further reading

40. k. crammer, o. dekel, j. keshet, s. shalev-shwartz, and y. singer. online passive-aggressive algorithms. jmlr 

2006.

41.

l. huang, s. fayong, and y. guo. structured id88 with inexact search. hlt-naacl 2012.

42.

l. morency, a. quattoni, and t. darrell. latent-dynamic discriminative models for continuous gesture 
recognition. cvpr 2007.

43.

l. shen, g. satta, and a. joshi. guided learning for bidirectional sequence classification. acl 2007.

44. m. bates. models of natural language understanding. nas 1995.

45. m. collins and b. roark. incremental parsing with the id88 algorithm. acl 2004.

46. m. collins. discriminative training methods for id48: theory and experiments with 

id88 algorithms. acl 2002.

47. m. collins. head-driven statistical models for natural language parsing. phd thesis 1999.

48. m. collins. incremental parsing with the id88 algorithm. acl 2004.

49. m. collins. three generative, lexicalised models for statistical parsing. eacl 1997.

50. m. honnibal and m. johnson. joint incremental disfluency detection and id33. tacl 2014.

51. m.a.k halliday. language structure and language function. new horizons in linguistics 1970.

52. m.t. luong, h. pham, and c.d. manning. effective approaches to attention-based id4. 

emnlp 2015.

127

references and further reading

53. n. chomsky. knowledge of language: its nature, origin, and use. greenwood publishing group 1986.

54. n. chomsky. language and mind. cambridge university press 2006.

55. n. chomsky. reflections on language. new york 1975.

56. n. srivastava, g. hinton, a. krizhevsky, i. sutskever, and r. salakhutdinov. dropout: a simple way to prevent 

neural networks from overfitting. jmlr 2014.

57. p. liang, a. bouchard-c  t  , d. klein, and b. taskar. an end-to-end discriminative approach to machine 

translation. acl 2006.

58. p. xu and r. sarikaya. exploiting shared information for multi-intent natural language sentence classification. 

interspeech 2013.

59. r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa. natural language processing (almost) 

from scratch. jmlr 2011.

60. r. florian, a. ittycheriah, h. jing, and t. zhang. id39 through classifier combination. hlt-

naacl 2003.

61. r. mcdonald and f. pereira. online learning of approximate id33 algorithms. eacl 2006.

62. r. mcdonald, f. pereira, k. ribarov, and j. hajic. non-projective id33 using spanning tree 

algorithms. hlt-emnlp 2005.

63. r. mcdonald, k. crammer, and f. pereira. flexible text segmentation with structured multilabel classification. 

hlt-emnlp 2005.

64. r. pascanu, t. mikolov, and y. bengio. on the difficulty of training recurrent neural networks. icml 2013.

128

references and further reading

65. r. socher, a. perelygin, j. wu, j. chuang, c.d. manning, a. ng, and c. potts. recursive deep models for semantic 

compositionality over a sentiment treebank. emnlp 2013.

66. r. socher, c. lin, a. ng, and c.d. manning. parsing natural scenes and natural language with recursive neural 

networks. icml 2011.

67. r. wardhaugh. introduction to linguistics. mc-graw-hill book company. 1972.

68. r.k. ando and t. zhang. a framework for learning predictive structures from multiple tasks and unlabeled data. 

jmlr 2005.

69. s. hochreiter and j. schmidhuber. long short-term memory. neural computation 1997.

70. s. petrov and d. klein. improved id136 for unlexicalized parsing. naacl 2007.

71. s. petrov and d. klein. sparse multi-scale grammars for discriminative latent variable parsing. emnlp 2008.

72. t. kudo t and y. matsumoto. chunking with support vector machines. naacl 2001.

73. t.matsuzaki, y. miyan, and j. tsujii. probailistic id18 with latent annotations. acl 2005.

74. t. robertshaw.  introduction to machine learning with naive bayes. 

http://tomrobertshaw.net/2015/12/introduction-to-machine-learning-with-naive-bayes/. 2015.

75. v. nair and g.e. hinton. rectified linear units improve restricted id82s. icml 2010.

76. x. sun. asynchronous parallel learning for neural networks and structured models with dense features. 

coling 2016.

129

references and further reading

77. x. sun, l. morency, d. okanohara, y. tsuruoka, and j. tsujii. modeling latent-dynamic in id66: a 

latent conditional model with improved id136. coling 2008.

78. x. sun, t. matsuzaki, and w. li. latent structured id88s for large-scale learning with hidden information. 

tkde 2013.

79. x. sun, t. matsuzaki, d. okanohara, and j. tsujii. latent variable id88 algorithm for structured 

classification. ijcai 2009.

80. x. sun. structure id173 for id170. nips 2014.

81. x. sun. towards shockingly easy structured classification: a search-based probabilistic online learning 

framework. arxiv:1503.08381.

82. y. zhang and j. nivre. transition-based id33 with rich non-local features. acl-hlt 2011.

83. y. zhang and s. clark. a tale of two parsers: investigating and combining graph-based and transition-based 

id33 using beam-search. emnlp 2008.

84. z. huang, w. xu, and k. yu. bidirectional lstm-crf models for sequence tagging. arxiv:1508.01991.

130

content and lecturer

models & id173

    conventional model

    latent model

    neural model

xu sun

structures & applications

    sequence structure

    tree/graph structure

yansong feng

131

type of structures

in application

yansong feng

peking university

fengyansong@pku.edu.cn

outline

sequence tagging

tree/graph structure

take-home msg

133

outline

sequence tagging

chinese id40

id52/id39

tree/graph structure

take-home msg

134

sequence tagging

    sequence 

    the most basic/simplest structures in nlp

    tag each item in the sequence using a given label 

set

    common to see

    world segmentation

    part-of-speech tagging

    id39

    chunking

    event trigger identification

       

135

sequence tagging

    conventional models

    crf, structured id88,     

    work well, but

    feature engineering

    local context / global information

       

    neural models

    id98, id56, lstm, lstm-crf,   

    performances

    comparable to state of the arts

    combinations may give top performances

136

chinese id40

    the task

[xu and sun, 2016]

                             

(the ground is covered with thick

snow)

                                  (this area is really not small)

    challenges

    feature engineering

    long distance dependencies

    neural network practice

    id98/id56 to capture local information, instead of fixed 

windows

    id56 to capture long distance dependencies, or sentence-

level/global information

137

model local information

    nn to model local features

    tnn to capture tag/word features and their combinations

    id98, id56, lstm, blstm to capture local features, beyond

fixed window size

    choice of character/word level

    word level features are still important, but not easy to

incorporate in models

    explore word level information in a beam-search framework

[zhang et al., 2016, cai and zhao, 2016] 

    word level features give 0.5%

    combinations

    combining neural and discrete features gives top

performances

138

global view

    long distance dependencies

    gated recursive nn 

[chen et al., 2015, xu and sun, 2016]

[chen et al., 2015b, zhang et al., 2016, cai and zhao,

    lstm 

2016]

    search strategy

    crf framework

    viterbi

    beam-search style

    fully explore word level information

    transition based [zhang et al., 2016]

    id125 [cai and zhao, 2016]

139

models

    gated id56s

    dependency based gated recursive neural 

networks

[chen et al., 2015a]

[xu and sun, 2016]

140

models

    lstm 

    lstm + gated combination neural networks

    bi-lstm

[chen et al., 2015b]

[cai and zhao, 2016]
141

performances on benchmarks

    conventional models are still strong

    neural models can be promising, and sometimes 

complementary to conventional models

    long distance dependencies

    word level features

    bilstm works to capture local context information

    various nn models to sentence level/long-distance 

information

    crf is still attractive 

142

on pku and msr

98

97

96

95

94

93

sun et

al.

2012

zhang
et al.
2013

pei et

al.

2014

chen
et al.
2015a

chen
et al.
2015b

xu and

sun,
2016

zhang
et al.
2016

cai
and
zhao,
2016

pku

msr

143

pos/ner/chunking

    typical sequence labeling tasks

    conventional models have achieved  over ~90%

    again

    feature engineering 

    language issues

    local/global

    label bias

    neural network practice

    blstm/id98 to capture local context, both forward and 

backward

    crf with viterbi to find the best sequence

144

feature extraction

    nn models seem be capable of handling language 

issues to some extent

    id98

    blstm     dominating!

    character level modeling

    blstm works better than lstm

    look at both past and future

    traditional lexicon features are still there

    extra resources, like dictionary, gazetteers, or wiki, are always 

welcome 

145

models

    blstm-crf with feature concatenation 

    work nice for pos, ner, chunking

[huang et al., 2015]

146

models

    typical blstm-crf for  pos / ner

    word / character level

[ling et al., 2015]
[lample et al., 2016]

147

models

    blstm+id98 for ner

[chiu and nicols., 2016]

148

language issues

    handling with different languages

    handling with oov

    using id98

    character-level modeling

[dos santos and guimaraes, 2015] [ling et al., 2015]

149

id52 on wsj

wsj

97.7

97.5

97.3

97.1

96.9

96.7

96.5

96.3

96.1

collobert

et al.,
2011

wang et
al., 2015

huang et
al., 2015

ling et
al., 2015

andor et
al., 2016

wsj

150

ner on conll

conll03

92.5

91.5

90.5

89.5

88.5

87.5

86.5

85.5

conll03

151

misc

    event trigger identification

                                       

the suspects were arrested

[arrest_jail]

    nn models work to help

    feature extraction 

    both local and global features (id98, blstm)

    language issues: character level modeling

152

models

    blstm+id98 for event trigger identification

    id98 to capture local context

[zeng et al., 2016]

153

outline

sequence tagging

tree/graph structure

id33

semantic role labelling

take-home message

154

tree/graph structures

    one of most common/important structures in nlp

    both the tree/graph structures and their tags are 

latent

    building bricks

    syntactic parsing

    event extraction

    id14

       

155

id33

    the task

    conventional models

    transition based

    graph based

    however,

    feature engineering

    label bias   ---->  locally/globally normalized

    neural network practice

    transition/graph based

    nn models to extract various features

    choose from greedy search, id125 or approximate global

id172

156

transition based models

    most dl based works in id33 follows 

the transition based framework.

    lower complexity

    higher efficiency

    more choices of features

    follow normal transition styles

    but, most are based on greedy search

    various nn models used to produce dense features

    normal nns

    lstm

    blstm, both word level and character level

    [chen and manning, 2014], [weiss et al., 2015], [dyer et al., 2015], [ballesteros et al., 2015], 

[zhou et la., 2015], [kiperwasser and goldberg, 2016], [andor et al., 2016]

157

features

    key features, such as words, pos and dep labels, as 
well as their combinations can be transformed into 
dense format through a neural network 

[chen and manning, 2014]

158

features

    or, features could be captured through lstm units

[dyer et al., 2015] [ballesteros et al., 2015]
[kiperwasser and goldberg, 2016]

159

greedy search

    no global considerations about future decisions

    look ahead with limited context 

    label bias

    may be error propagations

    training a global normalized model may not be trivial

    complexity

    space

    and, it is found that, in many parsers, id125 has

minimal impact in the results. [dyer et al., 2015]

    however, sometimes, better than beam-search based 

methods [kiperwasser and goldberg, 2016]

160

id125

    one solution is to use the output layers of the nn 

model to learn a structured id88 with a beam 
search

    gives 0.8% 

[weiss et al., 2015]

161

approximate global id172

    local normalized models may suffer from local optimal 

(greedy search), label bias, etc.

    perform global id172 exactly

    maximize the whole action sequence

    it is not trivial !

    too many possible action sequences

    expensive (impossible) to enumerate and compute

    or, do it approximately ?

162

approximate global id172

    contrastive learning [hinton, 2002; lecun and huang, 2005; 

liang and jordan, 2008; vickrey et al., 2010]

    reward observed data

    penalize noisy data

    in the id125 case:                        

[zhou et al., 2014]

    give gold sequence with higher id203

    give incorrect sequences in the beam with lower 

probabilities 

    early update may be helpful

    gives more than 1.5%

163

approximate global id172

    the importance of looking ahead

[andor et al., 2016]

164

approximate global id172

    global training 

    backward propagation with id125 

    with early update

    slow

    works for multiple tasks:

    id33

    id52

    sentence compression 

    insight:

    global models are more expressive

[andor et al., 2016]

165

on wsj 

95

94

93

92

91

90

89

88

87

uas

las

166

id14

    the task

2016]

[hea0 ] had trouble raising [fundsa1 ]. 

[roth and lapata, 

    conventional models

    pipeline

    predicate identification, argument identification, argument 

classification 

    however,

    feature engineering, local features, global features,    

    pipeline

    neural practice

    nn models to extract various features, or as local classifiers

    in a sequence labeling style, an end-to-end system

167

pipeline system

    step by step: predicate identification, argument identification, 

argument classification 

    each step: local classifiers with feature extraction, using id98, or 

lstm

id98

lstm

[fortland and martin, 2015] [roth and lapata, 2016]

168

end-to-end system

    multi-layered lstm with crf to sequence tagging.

[zhou and xu, 2016]

169

id98 v.s. lstm

    handle long distance dependencies

    id98

    good at capturing local features, or work on dependency path 

    not so good at end-to-end systems, or extracting features from 

plain word sequence for srl

    lstm

    good at capturing both local features, and global information, 

either for local decisions, or sentence level re-ranking

    more powerful in capturing features of various levels

    deep lstm can also be used to build end-to-end srl systems

170

outline

sequence tagging

tree/graph structure

take-home messages

171

how-tos

    standard settings for sequence tagging

    blstm + crf

    character modeling or id98 for fine modeling

    fused with successful traditional features

    perhaps, id125 for various features

    but

use as many useful resources as you can !

    more complex structures

    blstm + transition base systems

    choose from greedy search / id125

    but

choose globally normalized models if you can!

172

thanks

reference

   

   

   

   

   

   

   

   

alexandre passos, vineet kumar, andrew mccallum, lexicon infused phrase embeddings for named entity 
resolution, in proceedings of the eighteenth conference on computational language learning, pages 78   86, 
baltimore, maryland usa, june 26-27 2014. 

chris alberti, david weiss, greg coppola, and slav petrov. 2015. improved transition-based parsing and 
tagging with neural networks. in proceedings of the 2015 conference on empirical methods in natural 
language processing, pages 1354   1359.

chris dyer, miguel ballesteros, wang ling, austin matthews, and noah a. smith. 2015. transition-based 
id33 with stack long short-term memory. in proceedings of the 53rd annual meeting of the 
association for computational linguistics, pages 334   343.

c  cero dos santos and victor guimaraes, boosting id39 with neural character 
embeddings, in proceedings of the fifth named entity workshop, joint with 53rd acl and the 7th ijcnlp, 
pages 25   33, beijing, china, july 26-31, 2015. 

c  cero nogueira dos santos and bianca zadrozny, learning character-level representations for part-of-speech 
tagging, in proceedings of the 31st international conference on machine learning, beijing, china, 2014.

daniel andor, chris alberti, david weiss, aliaksei severyn, alessandro presta, kuzman ganchev, slav petrov and 
michael collin, globally normalized transition-based neural networks, proceedings of the 54th annual 
meeting of the association for computational linguistics, pages 2442   2452, berlin, germany, august 7-12, 
2016

danqi chen and christopher d. manning. 2014. a fast and accurate dependency parser using neural networks. 
in proceedings of the 2014 conference on empirical methods in natural language processing, pages 740   750.

david weiss, chris alberti, michael collins, and slav petrov. 2015. structured training for neural network 
transition-based parsing. in proceedings of the 53rd annual meeting of the association for computational 
linguistics, pages 323   333.

174

reference

   

   

deng cai and hai zhao, neural id40 learning for chinese, in proceedings of the 54th annual 
meeting of the association for computational linguistics, pages 409   420, berlin, germany, august 7-12, 2016

eliyahu kiperwasser and yoav goldberg, simple and accurate id33 using bidirectional lstm 
feature representations, arxiv:1603.04351v2

    hao zhou, yue zhang, and jiajun chen. 2015. a neural probabilistic structured-prediction model for transition-

based id33. in proceedings of the 53rd annual meeting of the association for computational 
linguistics, pages 1213   1222.

   

   

   

   

   

in transactions of the association for computational linguistics, vol. 4, pp. 357   370, 2016

jason p.c. chiu and eric nichols, id39 with bidirectional lstm-id98s, 

jie zhou and wei xu. 2015. end-to-end learning of semantic role labelling using recurrent neural networks. in 
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th 
international joint conference on natural language processing, pages 1127   1137.

jingjing xu and xu sun, dependency-based gated id56 for chinese id40, 
in proceedings of the 54th annual meeting of the association for computational linguistics, pages 567   572, 
berlin, germany, august 7-12, 2016

longkai zhang, houfeng wang, xu sun, and mairgup mansur. 2013a. exploring representations from unlabeled 
data with co-training for chinese id40. in proceedings of the emnlp 2013, pages 311   321, 
seattle, washington, usa, october.

    matthieu labeau, kevin loser, alexandre allauzen, non-lexical neural architecture for fine-grained pos 

tagging, in proceedings of the 2015 conference on empirical methods in natural language processing, pages 
232   237, lisbon, portugal, 17-21 september 2015

175

reference

    meishan zhang, yue zhang, guohong fu, transition-based neural id40, in proceedings of the 

54th annual meeting of the association for computational linguistics, pages 421   431, berlin, germany, august 
7-12, 2016.

    michael roth and mirella lapata, neural id14 with dependency path embeddings, in 

proceedings of the 54th annual meeting of the association for computational linguistics, pages 1192   1202, 
berlin, germany, august 7-12, 2016

    miguel ballesteros, chris dyer, and noah a. smith. 2015. improved transition-based parsing by modeling 
characters instead of words with lstms. in proceedings of the 2015 conference on empirical methods in 
natural language processing, pages 349   359.

    nicholas fitzgerald, oscar ta   ckstro   m, kuzman ganchev, and dipanjan das. 2015. id14 with 

neural network factors. in proceedings of the 2015 conference on empirical methods in natural language 
processing, pages 960   970, lisbon, portugal.

   

   

peilu wang, yao qian, frank k. soong, lei he, hai zhao, a unified tagging solution: bidirectional lstm 
recurrent neural network with id27, arxiv:1511.00215v1

ronan collobert, jason weston, leon bottou, michael karlen, koray kavukcuoglu, and pavel kuksa. 2011. 
natural language processing (almost) from scratch. the journal of machine learning research, 12:2493   2537.

    wang ling, tiago lu  s, lu  s marujo, ramon fernandez astudillo, silvio amir, chris dyer, alan w black, isabel 

trancoso, finding function in form: compositional character models for open vocabulary word 
representation, in proceedings of the 2015 conference on empirical methods in natural language processing, 
pages 1520   1530, lisbon, portugal, 17-21 september 2015.

    wenzhe pei, tao ge, and baobao chang. 2014. max-margin tensor neural network for chinese word 

segmentation. in proceedings of the 52nd acl, pages 293   303, baltimore, maryland, june.

176

reference

    wenzhe pei, tao ge, baobao chang, an effective neural network model for graph-based id33, 

proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th 
international joint conference on natural language processing, pages 313   322, beijing, china, july 26-31, 
2015.

    william foland and james martin. 2015. dependency-based id14 using convolutional neural 
networks. in proceedings of the fourth joint conference on lexical and computational semantics, pages 279   
288, denver, colorado.

   

   

   

   

xinchi chen, xipeng qiu, chenxi zhu, and xuanjing huang. 2015a. gated id56 for chinese 
id40. in proceedings of the 53nd acl, pages 1744   1753, july.

xinchi chen, xipeng qiu, chenxi zhu, pengfei liu, and xuanjing huang. 2015b. long short-term memory neural 
networks for chinese id40. in proceedings of the 2015 emnlp, pages 1197   1206, september.

xu sun, houfeng wang, and wenjie li. 2012. fast on-line training with frequency-adaptive learning rates for 
chinese id40 and new word detection. in proceedings of the 50th acl, pages 253    262, july.

zhiheng huang, wei xu, and kai yu. 2015. bidi- rectional lstm-crf models for sequence tagging. 
corr,abs/1508.01991.

177

