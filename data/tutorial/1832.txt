a tutorial on deep learning

part 1: nonlinear classi   ers and the id26 algorithm

quoc v. le

qvl@google.com

google brain, google inc.

1600 amphitheatre pkwy, mountain view, ca 94043

december 13, 2015

1

introduction

in the past few years, deep learning has generated much excitement in machine learning and industry
thanks to many breakthrough results in id103, id161 and text processing. so, what
is deep learning?

for many researchers, deep learning is another name for a set of algorithms that use a neural network as
an architecture. even though neural networks have a long history, they became more successful in recent
years due to the availability of inexpensive, parallel hardware (gpus, computer clusters) and massive
amounts of data.

in this tutorial, we will start with the concept of a linear classi   er and use that to develop the concept
of neural networks.
i will present two key algorithms in learning with neural networks: the stochastic
id119 algorithm and the id26 algorithm. towards the end of the tutorial, i will
explain some simple tricks and recent advances that improve neural networks and their training. for that,
let   s start with a simple example.

2 an example of movie recommendations

it   s friday night, and i am trying to decide whether i should watch the movie    gravity    or not. i ask my
close friends mary and john, who watched the movie last night to hear their opinions about the movie.
both of them give the movie a rating of 3 in the scale between 1 to 5. not outstanding but perhaps worth
watching?

given these ratings, it is di   cult for me to decide if it is worth watching the movie, but thankfully, i
have kept a table of their ratings for some movies in the past. for each movie, i also noted whether i liked
the movie or not. maybe i can use this data to decide if i should watch the movie. the data look like this:

movie name

mary   s rating

john   s rating

i like?

lord of the rings ii

...

star wars i

gravity

1
...
4.5
3

5
...
4
3

no
...
yes

?

let   s visualize the data to see if there is any trend:

1

in the above    gure, i represent each movie as a red    o    or a blue    x    which correspond to    i like the
movie    and    i dislike the movie   , respectively. the question is with the rating of (3, 3), will i like gravity?
can i use the past data to come up with a sensible decision?

3 a bounded decision function

let   s write a computer program to answer this question. for every movie, we construct an example x
which has two dimensions: the    rst dimension x1 is mary   s rating and the second dimension x2 is john   s
rating. every past movie is also associated with a label y to indicate whether i like the movie or not. for
now, let   s say y is a scalar that should have one of the two values, 0 to mean    i do not like    or 1 to mean
   i do like    the movie. our goal is to come up with a decision function h(x) to approximate y.

our decision function can be as simple as a weighted linear combination of mary   s and john   s ratings:

h(x;   , b) =   1x1 +   2x2 + b, which can also be written as h(x;   , b) =   t x + b

(1)

in the equation above, the value of function h(x) depends on   1,   2 and b, hence i rewrite it as h(x; (  1,   2), b)
or in vector form h(x;   , b).

the decision function h unfortunately has a problem: its values can be arbitrarily large or small. we
wish its values to fall between 0 and 1 because those are the two extremes of y that we want to approximate.
a simple way to force h to have values between 0 and 1 is to map it through another function called

the sigmoid function, which is bounded between 0 and 1:

h(x;   , b) = g(  t x + b), where g(z) =

1

1 + exp(   z)

,

(2)

which graphically should look like this:

the value of function h is now bounded between 0 and 1.

2

4 using past data to learn the decision function

we will use the past data to learn   , b to approximate y. in particular, we want to obtain   , b such that:

h(x(1);   , b)     y(1), where x(1) is mary   s and john   s ratings for 1st movie,    lord of the rings ii   
h(x(2);   , b)     y(2), where x(2) is mary   s and john   s ratings for 2nd movie
...
h(x(m);   , b)     y(m), where x(m) is mary   s and john   s ratings for m-th movie

to    nd the values of    and b we can try to minimize the following objective function, which is the sum of
di   erences between the decision function h and the label y:

j(  , b) =(cid:0)h(x(1);   , b)     y(1)(cid:1)2 +(cid:0)h(x(2);   , b)     y(2)(cid:1)2 + ... +(cid:0)h(x(m);   , b)     y(m)(cid:1)2

m(cid:88)

(cid:0)h(x(i);   , b)     y(i)(cid:1)2

=

i=1

5 using stochastic id119 to minimize a function

and b in the direction of minimizing each of the small objective(cid:0)h(x(i);   , b)     y(i)(cid:1)2. concretely, we can

to minimize the above function, we can iterate through the examples and slowly update the parameters   

update the parameters in the following manner:

  1 =   1            1
  2 =   2            2
b = b          b

(3)

(4)

(5)

where    is a small non-negative scalar. a large    will give aggressive updates whereas a small    will give
conservative updates. such algorithm is known as stochastic id119 (or sgd) and    is known as
the learning rate.

now the question of    nding the optimal parameters amounts to    nding         s and    b such that they are
in the descent direction. in the following, as our objective function is composed of function of functions,
we use the the chain rule to compute the derivatives. remember that the chain rule says that if g is a
function of z(x) then its derivative is as follows:

this chain rule is very useful when taking the derivative of a function of functions.

   g
   x

=

   g
   z

   z
   x

thanks to the chain rule, we know that a good descent direction for any objective function is its

gradient. therefore, at example x(i), we can compute the partial derivative:

(cid:18)

   
     1

(cid:18)
(cid:18)

     1 =

h(x(i);   , b)     y(i)

= 2

h(x(i);   , b)     y(i)

h(x(i);   , b)

(cid:19)2
(cid:19)    
(cid:19)    

     1

     1

3

= 2

g(  t x(i) + b)     y(i)

g(  t x(i) + b)

(6)

apply the chain rule, and note that    g

   
     1

g(  t x(i) + b) =

   z = [1     g(z)]g(z), we have:
   (  t x(i) + b)
   g(  t x(i) + b)
   (  t x(i) + b)

=(cid:2)1     g(  t x(i) + b)(cid:3)g(  t x(i) + b)
=(cid:2)1     g(  t x(i) + b)(cid:3)g(  t x(i) + b)x(i)

     1

1

   (  1x(i)

1 +   2x(i)

2 + b)

     1

plug this to equation 6, we have:

     1 = 2(cid:2)g(  t x(i) + b)     y(i)(cid:3)(cid:2)1     g(  t x(i) + b)(cid:3)g(  t x(i) + b)x(i)

1

where

g(  t x(i) + b) =

1

1 + exp(     t x(i)     b)

similar derivations should lead us to:

     2 = 2(cid:2)g(  t x(i) + b)     y(i)(cid:3)(cid:2)1     g(  t x(i) + b)(cid:3)g(  t x(i) + b)x(i)
   b = 2(cid:2)g(  t x(i) + b)     y(i)(cid:3)(cid:2)1     g(  t x(i) + b)(cid:3)g(  t x(i) + b)

2

(7)

(8)

(9)

(10)

now, we have the stochastic id119 algorithm to learn the decision function h(x;   , b):

1. initialize the parameters   , b at random,
2. pick a random example {x(i), y(i)},

3. compute the partial derivatives   1,   2 and b by equations 7, 9 and 10,

4. update parameters using equations 3, 4 and 5, then back to step 2.

we can stop stochastic id119 when the parameters do not change or the number of iteration
exceeds a certain upper bound. at convergence, we will obtain a function h(x;   , b) which can be used to
predict whether i like a new movie x or not: h > 0.5 means i will like the movie, otherwise i do not like
the movie. the values of x   s that cause h(x;   , b) to be 0.5 is the    decision boundary.    we can plot this
   decision boundary    to have:

the green line is the    decision boundary.    any point lying above the decision boundary is a movie that i
should watch, and any point lying below the decision boundary is a movie that i should not watch. with

4

this decision boundary, it seems that    gravity    is slightly on the negative side, which means i should not
watch it.

by the way, here is a graphical illustration of the decision function h we just built (   m    and    j    indicate

the input data which is the ratings from mary and john respectively):

this networid116 that to compute the value of the decision function, we need the multiply mary   s rating
with   1, john   s rating with   2, then add two values and b, then apply the sigmoid function.

6 the limitations of linear decision function

in the above case, i was lucky because the the examples are linearly separable: i can draw a linear decision
function to separate the positive and the negative instances.

my friend susan has di   erent movie tastes. if we plot her data, the graph will look rather di   erent:

susan likes some of the movies that mary and john rated poorly. the question is how we can come up
with a decision function for susan. from looking at the data, the decision function must be more complex
than the decision we saw before.

my experience tells me that one way to solve a complex problem is to decompose it into smaller
problems that we can solve. we know that if we throw away the    weird    examples from the bottom left
corner of the    gure, the problem is simple. similarly, if we throw the    weird    examples on the top right
   gure, the problem is again also simple. in the    gure below, i solve for each case using our algorithm and
the decision functions look like this:

5

is it possible to combine these two decision functions into one    nal decision function for the original data?
the answer turns out to be yes and i   ll show you how.

7 a decision function of decision functions

let   s suppose, as stated above, the two decision functions are h1(x; (  1,   2), b1) and h2(x; (  3,   4), b2). for
every example x(i), we can then compute h1(x(i); (  1,   2), b1) and h2(x(i); (  3,   4), b2)

if we lay out the data in a table, it would look like the    rst table that we saw:

movie name

output by

output by

susan likes?

decision function h1 decision function h2

lord of the rings ii

...

star wars i

gravity

h1(x(1))

...

h1(x(n))
h1(x(n+1))

h2(x(2))

...

h2(x(n))
h2(x(n+1))

no
...
yes

?

now, once again, the problem becomes    nding a new parameter set to weigh these two decision functions to
approximate y. let   s call these parameters   , c, and we want to    nd them such that h((h1(x), h2(x));   , c)
can approximate the label y. this can be done, again, by stochastic id119.

in summary, we can    nd the decision function for susan by following two steps:

1. partition the data into two sets. each set can be simply classi   ed by a linear decision. then use the

previous sections to    nd the decision function for each set,

2. use the newly-found decision functions and compute the decision values for each example. then
treat these values as input to another decision function. use stochastic id119 to    nd the
   nal decision function.

a graphical way to visualize the above process is the following    gure:

what you just saw is a special architecture in machine learning known as    neural networks.    this instance
of neural networks has one hidden layer, which has two    neurons.    the    rst neuron computes values for
function h1 and the second neuron computes values for function h2. the sigmoid function that maps real
value to bounded values between 0, 1 is also known as    the nonlinearity    or the    activation function.   
since we are using sigmoid, the activation function is also called    sigmoid activation function.    in the
future, you may encounter other kinds of id180. the parameters inside the network, such
as   ,    are called    weights    where as b, c are called    biases.   

if you have a more complex function that you want to approximate, you may want to have a deeper

network, maybe one that looks like this:

6

this network has two hidden layers. the    rst hidden layer has two neurons and the second hidden layer
has three neurons.

let   s get back to our problem of    nding a good decision function for susan. it seems so far so good,
but in the above steps, i cheated a little bit when i divided the dataset into two sets because i looked at
the data and decided that the two sets should be partitioned that way. is there any way that such a step
can be automated?

it turns out the answer is also yes. and the way to do it is by not doing two steps sequentially,
but rather,    nding all parameters   , c,   , b at once on the complex dataset, using the stochastic gradient
descent. to see this more clearly, let   s write down how we will compute h(x):

(cid:18)
(cid:18)
  1g(cid:0)  1x1 +   2x2 + b1

  1h1(x) +   2h2(x) + c

(cid:19)
(cid:1) +   2g(cid:0)  3x1 +   4x2 + b2

(cid:19)
(cid:1) + c

h(x) = g

= g

we will    nd all these parameters   1,   2, c,   1,   2,   3,   4, b1, b2 at the same time.

notice that the stochastic id119 is quite general: as long as we have a set of parameters,
we can    nd the partial derivative at each coordinate and simply update one coordinate at a time. so the
trick is to    nd the partial derivatives. for that, we need a famous algorithm commonly known as the
id26 algorithm.

8 the id26 algorithm

the goal of the id26 algorithm is to compute the gradient (a vector of partial derivatives) of
an objective function with respect to the parameters in a neural network. as the decision function h(x) of
the neural network is a function of functions, we need to use the chain rule to compute its gradient. the
id26 algorithm is indeed an implementation of the chain rule speci   cally designed for neural
networks. it takes some e   ort to arrive at this algorithm, so i will skip the derivation, and just show you
the algorithm.

to begin, let   s simplify the notations a little bit. we will use    for all the weights in the network and
ij means weight at layer l-th connecting neuron (or input) j-th to the neuron i-th
is bias of neuron i. the layers are indexed by 1(input), 2, ..., l(output). the number of

b for all the biases.   (l)
in layer l + 1, b(l)
i
neurons in the layer l is sl. in this notation, the decision function h(x) can be recursively computed as:

(cid:18)(cid:0)  (1)(cid:1)t h(1) + b(1)

(cid:19)

h(1) = x

h(2) = g

...

7

(cid:18)(cid:0)  (l   2)(cid:1)t h(l   2) + b(l   2)
(cid:18)(cid:0)  (l   1)(cid:1)t h(l   1) + b(l   1)

(cid:19)
(cid:19)

h(l   1) = g

h(x) = h(l) = g

[this is a scalar]

we use matrix-vectorial notations so that it is easier to read and note that h(l   1) is a vector.

here   s the id26 algorithm. the steps are:

1. perform a    feedforward pass,    to compute h(1), h(2), h(3), ..., h(l).

2. for the output layer, compute

1 = 2(h(l)     y) g(cid:48)(cid:18) sl   1(cid:88)

  (l)

(cid:19)

  (l   1)

1j

h(l   1)

j

+ b(l   1)

1

j=1

3. perform a    backward pass,    for l = l     1, l     2, .., 2

for each node i in layer l, compute

(cid:18) sl+1(cid:88)

  (l)
i =

  (l)
ji   (l+1)

j

(cid:19)

g(cid:48)(cid:18) sl   1(cid:88)

(cid:19)

  (l   1)

ij

h(l   1)

j

+ b(l   1)

i

4. the desired partial derivatives can be computed as

j=1

j=1

j   (l+1)

i

     (l)
   b(l)

ij = h(l)
i =   (l+1)

i

the indices make the algorithm look a little busy. but we can simplify these equations by using more
matrix-vectorial notations. before we proceed, let   s use the notation (cid:12) for element-wise dot product. that
is if b and c are vectors of n dimensions, then b(cid:12)c is a vector a of n dimensions where ai = bici,   i     1, ..., n.
using this notation, the algorithm above can be rewritten in the following vectorized version:

1. perform a    feedforward pass,    to compute h(1), h(2), h(3), ..., h(l).

2. for the output layer, compute

3. perform a    backward pass,    for l = l     1, l     2, .., 2

for each note i in layer l, compute

  (l)

(cid:19)

1 = 2(cid:0)h(l)     y(cid:1) (cid:12) g(cid:48)(cid:18)(cid:0)  (l   1)(cid:1)t h(l   1) + b(l   1)
(cid:12) g(cid:48)(cid:18)(cid:0)  (l   1)(cid:1)t h(l   1) + b(l   1)
(cid:18)(cid:0)  (l)(cid:1)t   (l+1)

(cid:19)

(cid:19)

  (l) =

4. the desired partial derivatives can be computed as

     (l) =   (l+1)(cid:0)h(l)(cid:1)t

[uvt is also known as the cross product of u and v]

   b(l) =   (l+1)

a nice property of the id26 algorithm is that it can be made e   cient because in the feedforward
pass, some of the intermediate values can be cached, and then used to compute the gradient in the backward
pass.

8

9 debug the id26 algorithm by numerical di   erentiation

the id26 algorithm can be di   cult to implement and debug. a simple trick to debug the
algorithm is to compare the partial derivative computed by id26 algorithm (known as the
analytical derivative) and its numerical approximation.

before we dive into the details of what the trick looks like, let   s try to understand the idea of partial

derivative.

if we have a function f (x), the derivative of f at x is:

   f
   x

= lim
    0

f (x +  )     f (x)

 

let   s suppose we have a function f which is so complicated that we cannot compute the analytical derivative
by hand (maybe because we are not good at math!), the right hand side of the above equation can come
in handy. it gives us a numerical approximation of the analytical derivative. such approximation requires
us to pick some values for  , then evaluate the function at x +   and x.

for example, if we want to compute the numerical approximation of partial derivative of f (x) = x3 at

x = 2, we can pick   = 0.001 and have:

f (2 + 0.001)     f (2)

0.001

f (2.001)     f (2)

0.001

2.0013     23

0.001

=

=

= 12.006

on the other hand, we also know that    f
quite close to the numerical approximation 12.006.

   x = 3x2, which is evaluated at x = 2 to have value of 12 which is

this idea can be generalized to function with many variables. as you can see above, our function j
is a function of    and b. to compute its derivative, we can randomly generate the parameters      s and b   s,
then iterate through each parameter at a time, vary each value by  .

for example, if we want to compute the numerical partial derivative of j at coordinate   i, we can do:
j(  1,   2,   3, ...,   i   1,   i + 0.001,   i+1, ...,   n, b)     j(  1,   2,   3, ...,   i   1,   i,   i+1, ...,   n, b)

   j(  1,   2, ...,   n, b)

     i

=

0.001

a correct implementation of the id26 algorithm should give a gradient very similar to this
approximation.

finally, for stability, it is sometimes preferred to use the following formula to compute the numerical

approximation:

   f
   x

= lim
    0

f (x +  )     f (x      )

2 

in fact, we can try to compute the numerical derivative of f (x) = x3 at x = 2 again
2.0013     1.9993

f (2 + 0.001)     f (2     0.001)

f (2.001)     f (1.999)

2    0.001

=

0.002

=

0.002

= 12.000

10 some advice for implementing neural networks

    make sure to check the correctness of your gradient computed by id26 by comparing it

with the numerical approximation.

    it   s important to    break the symmetry    of the neurons in the networks or, in other words, force the
neurons to be di   erent at the beginning. this means that it   s important to initialize the parameters
   and b randomly. a good method for random initialization is gaussian random or uniform random.
sometimes tuning the variance of the initialization also helps.

9

    also make sure that the random initialization does not    saturate    the networks. this means that
most of the time, for your data, the values of the neurons should be between 0.2 and 0.8. this is
because we do not want to neurons to have too many values of zeros and ones. when that happens,
the gradient is small and thus the training is much longer.

    have a way to monitor the progress of your training. perhaps the best method is to compute the
objective function j on the current example or on a subset of the training data or on a held-out set.
    picking a good learning rate    can be tricky. a large learning rate can change the parameters too
aggressively or a small learning rate can change the parameters too conservatively. both should be
avoided, a good learning rate is one that leads to good overall improvements in the objective function
j. to select good   , it   s also best to monitor the progress of your training. in many cases, a learning
rate of 0.1 or 0.01 is a very good start.

    picking good hyperparameters (architectural parameters such as number of layers, number of neurons
on each layers) for your neural networks can be di   cult and is a topic of current research. a standard
way to pick architectural parameters is via cross-validation: keep a hold-out validation set that the
training never touches. if the method performs well on the training data but not the validation set,
then the model over   ts: it has too many degrees of freedom and remembers the training cases but
does not generalize to new cases. if the model over   ts, we need to reduce the number of hidden layers
or number of neurons on each hidden layer. if the method performs badly on the training set then
the model under   ts: it does not have enough degrees of freedom and we should increase the number
of hidden layers or number of neurons. we will also talk about over   tting in section 16. be warned
that bad performance on the training set can also mean that the learning rate is chosen poorly.

    picking good hyperparameters can also be automated using grid search, random search or bayesian
optimization. in grid search, every possible combination of hyperparameters will be tried and cross-
validated with a hold-out validation set. in case that grid search is expensive because the number
of hyperparameters is large, one can try random search where hyperparameter con   gurations are
generated and tried at random. bayesian optimization looks at the performances of networks at
previous hyperparameter combinations and    ts a function through these points, it then picks the
next combination that maximizes some utility function such as the mean plus some function of the
uncertainty (e.g., [19]).

    neural networks can take a long time to train and thus it   s worth spending time to optimize the code
for speed. to speed up the training, make use of fast matrix vector libraries, which often provide
good speed-up over na    ve implementation of matrix vector multiplication. the vectorized version of
the id26 algorithm will come in handy in this case.

    it is possible to use single precision for the parameters in the networks instead of double precision.
this reduces the memory footprint of the model in half and usually does not hurt the performances
of the networks. a downside is that it is more tricky to check the correctness of the gradient using
the numerical approximation.

    what is inconvenient about neural networks is that the objective function is usually non-convex with
respect to the parameters. this means that if we obtain a minimum, it   s likely to be a local minimum
and may not be global minimum. neural networks are therefore sensitive to random initialization.
other randomization aspects of the learning process could also a   ect the results. for example, factors
such as the choice of learning rate, the order of examples we iterate through can produce di   erent
optimal parameters for di   erent learning trials.

    it is possible to run stochastic id119 where every step we touch more than one example.
this is called minibatch stochastic id119 and the number of examples we look at per

10

iterations is called the    minibatch size.    (when the minibatch size is 1, we recover stochastic gradient
descent.) in many environments, using a larger minibatch can be a good idea because the gradient
is less noisy (computed as an average over the examples) and faster because matrix-vector libraries
work better with larger matrices.

11 what problems are neural networks good for?

as you can see so far, neural networks are general nonlinear classi   ers. neural networks are    exible, and
we can make a lot of changes to them to solve other problems. but as far as classifying data goes, are
neural networks good? and what are they good for?

many experiments have shown that neural networks are particularly good with natural data (speech,

vision, language) which exhibit highly nonlinear properties.

let   s take an example of id161 where the task is to recognize the digit from a handwritten
input image. below i visualize an image of the digit 0 written by my friend on the left. on the right side,
i visualize the typical representations that computer scientists use as inputs to machines:

as can be seen from the    gure, the input representations for machines are typically a 2d map of pixel
values (or a matrix of pixel values). each pixel has value between 0 and 255. the higher the value the
brighter the pixel is. so the image on the left is represented on the right as mostly zeros, except from the
center part where the pixel values have nonzero values.

to convert this matrix to an input vector and present that to neural networks, a standard approach
in machine learning is to concatenate all the rows of the matrix and    straighten    that into a vector. a
consequence of such representation is that if the digit gets shifted to the left, or to the right, the di   erence
of the shifted digit to the original digit is very large. but to us, they are just the same digit and should
belong to the same category (we call this translational invariance). at the same time, if i place the digit 6
at exactly the same location and with exactly the same shape, the di   erence between the digit 6 and the
digit 0 is rather small.

more concretely, let   s suppose i have the following three images, each has 28 x 28 pixels:

11

(cid:18)

(cid:118)(cid:117)(cid:117)(cid:116) 28(cid:88)
28(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 28(cid:88)

j=1

i=1

(cid:18)

28(cid:88)

(cid:19)2     3    103
(cid:19)2     103

(11)

(12)

for us, image1 and image2 should have the same label of 0 whereas image3 should have the label of 6.
but let   s check the distances between them. first, the (euclidean) distance between image1 and image2
is:

distance(image1, image2) =

image1(i, j)     image2(i, j)

while the distance between image1 and image3 is:

distance(image1, image3) =

image1(i, j)     image3(i, j)

i=1

j=1

so distance(image1, image3) < distance(image1, image2). this means that image1 is closer to image3
than image2, even though we want the label for image1 to be the same with image2 and di   erent from
the label for image3. practically, we want a decision function h that h(image1) = h(image2) = 0 (cid:54)=
h(image3) = 6.

such property is di   cult to achieve, if not impossible, with a linear decision function. for the purpose
of visualization, let   s take an example of a one dimensional task and suppose we have x1 = 2, x2 = 6, x3 = 4,
(x1 is closer to x3 than x2), and we want a decision that have the property that h(x1) = h(x2) (cid:54)= h(x3).
it   s not possible to construct a linear function h(x) = ax + b to achieve such property yet it   s possible to
do so with a nonlinear function. the    gure below shows such property. on the right, we show an example
of a nonlinear function which can achieve the property that h(x1) = h(x2) (cid:54)= h(x3), something that a linear
function would fail to capture (left):

interestingly, this property of having nonlinear decision functions arises very often in natural data, such
as vision and speech. perhaps this is because natural inputs are generated under adversarial conditions:
we lack of energy (food!) to create hardware (eyes, cameras, robots) that perfectly aligns the images, or
sounds and our preys/predators avoid our attention by changing colors etc.

but not every problem requires nonlinear decision function. a good way to test is to compare the
distances between a subset of the training set with their categories. unless the problem has nearby
examples with di   erent labels and far-away examples with the same label, you may not need a nonlinear
decision function.

12

12 deep vs. shallow networks

when the problem does exhibit nonlinear properties, deep networks seem computationally more attractive
than shallow networks. for example, it has been observed empirically that in order to get to the same
level of performances of a deep network, one has to use a shallow network with many more connections
(e.g., 10x number of connections in id103 [1, 8]). it is thus much more expensive to compute
the decision function for these shallow networks than the deep network equivalences because for every
connection we need to perform a    oating-point operation (multiplication or addition).

an intuition of why this is the case is as follows. a deep network can be thought of as a program
in which the functions computed by the lower-layered neurons can be thought of as subroutines. these
subroutines are re-used many times in the computation of the    nal program. for example in the following
   gure, the function computed by the red neuron in the    rst layer is re-used three times in the computation
of the    nal function h. in contrast, in the shallow network, the function computed by the red neuron is
only used once:

(bolded edges mean computation paths that need the red neuron to produce the    nal output.)

therefore, using a shallow network is similar to writing a program without the ability of calling subrou-
tines. without this ability, at any place we could otherwise call the subroutine, we need to explicitly write
the code for the subroutine. in terms of the number of lines of code, the program for a shallow network is
therefore longer than a deep network. worse, the execution time is also longer because the computation
of subroutines is not properly re-used.

a more formal argument of why deep networks are more    compact    than shallow counterparts can be

found in chapter 2 of [2].

13 deep networks vs. kernel methods

another interesting comparison is deep networks vs. kernel methods [4, 18]. a kernel machine can be
thought of as a shallow network having a huge hidden layer. the advantage of having a huge number of
neurons is that the collection of neurons can act as a database and therefore can represent highly nonlinear
functions. the beauty of kernel methods lies in the fact that even though the hidden layer can be huge,
its computation can be avoided by the kernel trick. to make use of the kernel trick, an algorithm designer
would rewrite the optimization algorithm (such as stochastic id119) in such a way that the hidden
layer always appears in a dot product with the hidden layer of another example: <   (x),   (x(cid:48)) >. the

13

dot product of two representations, with certain id180, can be computed in an inexpensive
manner.

kernel methods however can be expensive in practice. this is because the these methods keep around
a list of salient examples (usually near the boundary of the decision function) known as    support vectors.   
the problem is that the number of support vectors can grow as the size of the training set grows. so
e   ectively, the computation of the decision function h can be large for large datasets. the computation
of the decision function in neural networks, on the other hand, only depends on how many connections in
the neural networks and does not depend on the size of the training set.

as a side note, more recently, researchers have tried to avoid the kernel trick and revisited the idea of
using a extremely large number of neurons and representing them explicitly. interestingly, it can be proved
that even with random weights, the hidden layer is already powerful:
it can represent highly nonlinear
functions. this idea is also known as random kitchen sinks [15]. in making the connections random, these
methods can enjoy faster training time because we do not have to train the weights of the hidden layer.
the objective function is also convex and thus the optimization is not sensitive to weight initialization.
but this only solves half of the problem because the computation of the decision function is still expensive.

14 a brief history of deep learning

the    eld of arti   cial neural networks has a long history, dated back to 1950   s. perhaps the earliest example
of arti   cial neural networks is the id88 algorithm developed by rosenblatt in 1957 [16]. in the late
1970   s, researchers discovered that id88 cannot approximate many nonlinear decision functions,
for example the xor function.
in 1980   s, researchers found a solution to that problem by stacking
multiple layers of linear classi   ers (hence the name    multilayer id88   ) to approximate nonlinear
decision functions. neural networks again took o    for a while but due to many reasons, e.g., the lack of
computational power and labeled data etc., neural networks were left out of mainstream research in late
1990   s and early 2000   s.

since the late 2000   s, neural networks have recovered and become more successful thanks to the avail-
ability of inexpensive, parallel hardware (graphics processors, computer clusters) and a massive amount of
labeled data. there are also new algorithms that make use of unlabeled data and achieve impressive im-
provements in various settings, but it can be argued that the core is almost the same with old architectures
of 1990   s, which is what you have seen in this tutorial. key results are when the networks are deep: in
id103 (e.g.,
[6]), id161 (e.g., [3, 9]), and id38 (e.g., [12]). and thus
the    eld is also associated with the name    deep learning.   

there are many reasons for such success. perhaps the most important reason is that neural networks
have a lot of parameters, and can approximate very nonlinear functions. so if the problem is complex, and
has a lot of data, neural networks are good approximators for it. the second reason is that neural networks
are very    exible: we can change the architecture fairly easily to adapt to speci   c problems/domains (such
as convolutional neural networks and recurrent neural networks, which are the topics of the next tutorial).
the name deep learning can mean di   erent things for di   erent people. for many researchers, the
word    deep    in    deep learning    means that the neural network has more than 2 layers. this de   nition
re   ects the fact that successful neural networks in speech and vision are both deeper than 2 layers. for
many other researchers, the word    deep    is also associated with the fact that the model makes use of
unlabeled data. for many people that i talk to in the industry, the word    deep    means that there is no
need for human-invented features. but for me,    deep learning    means a set of algorithms that use neural
networks as an architecture, and learn the features automatically.

from 2006 to mid 2014, there have been several clever algorithmic ideas which, in certain cases, improve
the performances of deep networks. among these are the use of recti   ed linear units and dropout, which
will be discussed in the following sections. these two algorithms address two core aspects of deep learning:
ease, speed of training (optimization) and prediction quality (generalization), respectively.

14

15 recti   ed linear as a better activation function

one of the recent advances in neural networks is to use the recti   ed linear units (a.k.a. relus, g(z) =
max(0, z)) as an activation function in place of the traditional sigmoid function [13]:

the change from sigmoid to relus as an activation function in a hidden layer is possible because the
hidden neurons need not to have bounded values. it has been observed empirically that this activation
function allows for better approximation quality: the objective function is lower on the training set. it is
therefore recommended to use recti   ed linear units instead of sigmoid function in your neural networks.

the reason for why recti   ed linear units work better than sigmoid is still an open question for research,

but maybe the following argument can give an intuition.

first, notice that in the id26 algorithm to compute the gradient for the parameters in one
layer, we typically multiply the gradient from the layer above with the partial derivative of the sigmoid
function:

(cid:18)(cid:0)  (l)(cid:1)t   (l+1)

(cid:19)

(cid:12) g(cid:48)(cid:18)(cid:0)  (l   1)(cid:1)t h(l   1) + b(l   1)

(cid:19)

  (l) =

the issue is that the derivative of the sigmoid function has very small values (near zero) everywhere except
for when the input has values of 0, as the    gure below suggests:

which means that the lower layers will likely to have smaller gradients in terms of magnitude, compared
to the higher layers. this is because g(cid:48)(.) is always less than 1, with most values being 0. this imbalance
in gradient magnitude makes it di   cult to change the parameters of the neural networks with stochastic
id119. a large learning rate is suitable for lower layers but not suitable for higher layers, and
causes great    uctuations in the objective function. a small learning rate is suitable for lower layers but
not suitable for higher layers, and causes small changes in the parameters in higher layers.

15

this problem can be addressed by the use of recti   ed linear activation function. this is because the

derivative of the recti   ed linear activation function can have many nonzero values:

which in turn means that the magnitude of the gradient is more balanced throughout the networks.

the above argument is consistent with a recent empirical result of using an improved version of relu
called prelu (i.e., max(x, ax)     to obtain better results on id163), where the derivative of the acti-
vation function is nonzero except for one point (see [5]).

16 dropout to improve generalization

one problem that many of us encounter in training neural networks is that neural networks tend to over   t,
especially when the training set is small. over   tting means that the performance on the training set is
much better than the performance on the test set. in other words, the model    remembers    the training
set but does not generalize to new cases.

to overcome over   tting, there are many strategies:
    use unlabeled data to train a di   erent network known as an autoencoder and then use the weights to
initialize our network (i will discuss this    pretraining    idea using autoencoders in the next tutorial),
    penalize the weights by adding a penalty on the norm of the weights in the network to the objective

function j, e.g., using j + (cid:107)  (cid:107)2

2 as the    nal objective function,

    use dropout [7].

to use dropout, at every iteration of stochastic id119, we select randomly a fraction of neurons
in each layer and drop them out of the optimization by setting their values to zero. at test time, we do
not drop out any neurons but have to scale the weights properly. concretely, let   s say the neurons at layer
l are dropped out with id203 p (p = 0.5 meaning that we drop half of the neurons at an iteration),
then at test time the incoming weights to the layer l should be scaled by p:   (l   1)

test = p  (l   1).

it is probably easy to see why dropout should work. by dropping a fraction of the neurons out, the
algorithm forces the decision function to collect more evidence from other neurons rather than    tting to a
particular phenomenon. the decision function is therefore more robust to noise.

17 recommended readings

this tutorial is inspired by andrew ng   s tutorial on autoencoders [14]. some advice in this tutorials is
inspired by yann lecun et. al. [10].

16

id26 was probably invented earlier but gained most attention through the work of wer-

bos [20] and especially rumelhart et. al. [17].

the concept of an arti   cial neuron, a computational unit that multiplies some stored parameters with
inputs, adds some bias then applies some threshold logic function, was perhaps    rst invented by mcculloch
and pitts [11]. it is also known as the mcculloch-pitts    neuron models (or mcp neuron).

18 miscellaneous

this tutorial was written as preparation materials for the machine learning summer school at cmu
(mlss   14). videos of the lectures are at http://tinyurl.com/pduxz2z . an exercise associated with this
tutorial is at http://ai.stanford.edu/~quocle/exercise1.py .

if you    nd bugs with this tutorial, please send them to me at qvl@google.com .

19 acknowledgements

i would like to thank je    dean, coline devin, josh levenberg, thai pham and members of the google
brain team for many insightful comments and suggestions. i am also grateful to many readers who gave
various comments and corrections to the tutorial.

references

[1] l. ba and r. caurana. do deep nets really need to be deep? arxiv preprint arxiv:1312.6184, 2013.

[2] y. bengio. learning deep architectures for ai. foundations and trends in machine learning, 2(1):1   

127, 2009.

[3] d. c. ciresan, u. meier, j. masci, l. m. gambardella, and j. schmidhuber. flexible, high performance
convolutional neural networks for image classi   cation. in international joint conference on arti   cial
intelligence, 2011.

[4] c. cortes and v. vapnik. support-vector networks. machine learning, 20(3):273   297, 1995.

[5] k. he, x. zhang, s. ren, and j. sun. delving deep into recti   ers: surpassing human-level performance

on id163 classi   cation. arxiv preprint arxiv:1502.01852, 2015.

[6] g. hinton, l. deng, d. yu, g. dahl, a. mohamed, n. jaitly, a. senior, v. vanhoucke, p. nguyen,
t. sainath, and b. kingsbury. deep neural networks for acoustic modeling in id103.
ieee signal processing magazine, 2012.

[7] g. e. hinton, n. srivastava, a. krizhevsky, i. sutskever, and r. salakhutdinov. improving neural

networks by preventing co-adaptation of feature detectors. arxiv preprint arxiv:1207.0580, 2012.

[8] p. s. huang, h. avron, t. sainath, v. sindhwani, and b. ramabhadran. kernel methods match
deep neural networks on timit. in ieee international conference on acoustics, speech, and signal
processing, volume 1, page 6, 2014.

[9] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classi   cation with deep convolutional neural

networks. in advances in neural information processing systems, 2012.

[10] y. lecun, l. bottou, g. orr, and k. r. m  uller. e   cient backprop. in neural networks: tricks of

the trade. springer, 1998.

17

[11] w. mcculloch and w. pitts. a logical calculus of ideas immanent in nervous activity. bulletin of

mathematical biophysics, 5(4), 1943.

[12] t. mikolov. statistical language models based on neural networks. phd thesis, brno university of

technology, 2012.

[13] v. nair and g. hinton. recti   ed linear units improve restricted id82s. in interna-

tional conference on machine learning, 2010.

[14] a. ng. cs294a lecture notes     sparse autoencoder. http://web.stanford.edu/class/cs294a/

sparseautoencoder_2011new.pdf, 2011. [online; accessed 15-july-2014].

[15] a. rahimi and b. recht. weighted sums of random kitchen sinks: replacing minimization with

randomization in learning. in advances in neural information processing systems, 2009.

[16] f. rosenblatt. the id88     a perceiving and recognizing automaton. number 85-460-1. cornell

aeronautical laboratory, 1957.

[17] d. rumelhart, g. e. hinton, and r. j. williams. learning representations by back-propagating errors.

nature, 323(6088):533   536, 1986.

[18] b. sch  olkopf and a. j. smola. learning with kernels: support vector machines, id173, opti-

mization, and beyond. the mit press, 2002.

[19] j. snoek, h. larochelle, and r. p. adams. practical bayesian optimization of machine learning

algorithms. in advances in neural information processing systems, 2012.

[20] p. j. werbos. beyond regression: new tools for prediction and analysis in the behavioral sciences.

phd thesis, harvard university, 1974.

18

