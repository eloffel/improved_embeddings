a tutorial on deep learning

part 2: autoencoders, convolutional neural networks

and recurrent neural networks

quoc v. le

qvl@google.com

google brain, google inc.

1600 amphitheatre pkwy, mountain view, ca 94043

october 20, 2015

1

introduction

in the previous tutorial, i discussed the use of deep networks to classify nonlinear data. in addition to
their ability to handle nonlinear data, deep networks also have a special strength in their    exibility which
sets them apart from other tranditional machine learning models: we can modify them in many ways to
suit our tasks. in the following, i will discuss three most common modi   cations:

    unsupervised learning and data compression via autoencoders which require modi   cations in the loss

function,

    translational invariance via convolutional neural networks which require modi   cations in the network

architecture,

    variable-sized sequence prediction via recurrent neural networks which require modi   cations in the

network architecture.

the    exibility of neural networks is a very powerful property. in many cases, these changes lead to great
improvements in accuracy compared to basic models that we discussed in the previous tutorial.

in the last part of the tutorial, i will also explain how to parallelize the training of neural networks.
this is also an important topic because parallelizing neural networks has played an important role in the
current deep learning movement.

2 autoencoders

one of the    rst important results in deep learning since early 2000 was the use of id50 [15]
to pretrain deep networks. this approach is based on the observation that random initialization is a
bad idea, and that pretraining each layer with an unsupervised learning algorithm can allow for better
initial weights. examples of such unsupervised algorithms are id50, which are based on
restricted id82s, and deep autoencoders, which are based on autoencoders. although
the    rst breakthrough result is related to id50, similar gains can also be obtained later
by autoencoders [4]. in the following section, i will only describe the autoencoder algorithm because it is
simpler to understand.

1

2.1 data compression via autoencoders

suppose that i would like to write a program to send some data from my cellphone to the cloud. since i
want to limit my network usage, i will optimize every bit of data that i am going to send. the data is a
collection of data points, each has two dimensions. a sample of my data look like the following:

here, the red crosses are my data points, the horizontal axis is the value of the    rst dimension and the
vertical axis is the value of the second dimension.

upon visualization, i notice is that the value of the second dimension is approximately twice as much
as that of the    rst dimension. given this observation, we can send only the    rst dimension of every data
point to the cloud. then at the cloud, we can compute the value of the second dimension by doubling the
value of the    rst dimension. this requires some computation, and the compression is lossy, but it reduces
the network tra   c by 50%. and since network tra   c is what i try to optimize, this idea seems reasonable.
so far this method is achieved via visualization, but the bigger question is can we do this more system-
atically for high-dimensional data? more formally, suppose we have a set of data points {x(1), x(2), ..., x(m)}
where each data point has many dimensions. the question becomes whether there is a general way to map
them to another set of data points {z(1), z(2), ..., z(m)}, where z   s have lower dimensionality than x   s and
z   s can faithfully reconstruct x   s.

to answer this, notice that in the above process of sending data from my cellphone to the cloud has

three steps:

1. encoding: in my cellphone, map my data x(i) to compressed data z(i).

2. sending: send z(i) to the cloud.

3. decoding:

in the cloud, map from my compressed data z(i) back to   x(i), which approximates the

original data.

to map data back and forth more systematically, i propose that z and   x are functions of their inputs, in
the following manner:

z(i) = w1x(i) + b1
  x(i) = w2z(i) + b2

2

if x(i) is a two-dimensional vector, it may be possible to visualize the data to    nd w1, b1 and w2, b2
analytically as the experiment above suggested. most often, it is di   cult to    nd those matrices using
visualization, so we will have to rely on id119.

as our goal is to have   x(i) to approximate x(i), we can set up the following objective function, which is

the sum of squared di   erences between   x(i) and x(i):

(cid:18)
(cid:18)
(cid:18)

i=1

m(cid:88)
m(cid:88)
m(cid:88)

i=1

(cid:19)2

  x(i)     x(i)

(cid:19)2
(cid:1) + b2     x(i)

(cid:19)2

w2z(i) + b2     x(i)

(cid:0)w1x(i) + b1

w2

j(w1, b1, w2, b2) =

=

=

which can be minimized using stochastic id119.

this particular architecture is also known as a linear autoencoder, which is shown in the following

network architecture:

i=1

in the above    gure, we are trying to map data from 4 dimensions to 2 dimensions using a neural network
with one hidden layer. the activation function of the hidden layer is linear and hence the name linear
autoencoder.

the above network uses the linear activation function and works for the case that the data lie on a
linear surface. if the data lie on a nonlinear surface, it makes more sense to use a nonlinear autoencoder,
e.g., one that looks like following:

if the data is highly nonlinear, one could add more hidden layers to the network to have a deep autoencoder.
autoencoders belong to a class of learning algorithms known as unsupervised learning. unlike super-
vised algorithms as presented in the previous tutorial, unsupervised learning algorithms do not need labeled
information for the data. in other words, unlike in the previous tutorials, our data only have x   s but do
not have y   s.

3

2.2 autoencoders as an initialization method

autoencoders have many interesting applications, such as data compression, visualization, etc. but around
2006-2007, researchers [4] observed that autoencoders could be used as a way to    pretrain    neural networks.

why? the reason is that training very deep neural networks is di   cult:
    the magnitudes of gradients in the lower layers and in higher layers are di   erent,
    the landscape or curvature of the objective function is di   cult for stochastic id119 to

   nd a good local optimum,

    deep networks have many parameters, which can remember training data and do not generalize well.

the goal of pretraining is to address the above problems. with pretraining, the process of training a deep
network is divided in a sequence of steps:

    pretraining step: train a sequence of shallow autoencoders, greedily one layer at a time, using

unsupervised data,

    fine-tuning step 1: train the last layer using supervised data,
    fine-tuning step 2: use id26 to    ne-tune the entire network using supervised data.

while the last two steps are quite clear, the    rst step needs needs some explanation, perhaps via an
example. suppose i would like to train a relatively deep network of two hidden layers to classify some
data. the parameters of the    rst two hidden layers are w1 and w2 respectively. such network can be
pretrained by a sequence of two autoencoders, in the following manner:

more concretely, to train the red neurons, we will train an autoencoder that has parameters w1 and w (cid:48)
1.
after this, we will use w1 to compute the values for the red neurons for all of our data, which will then
be used as input data to the subsequent autoencoder. the parameters of the decoding process w (cid:48)
1 will
be discarded. the subsequent autoencoder uses the values for the red neurons as inputs, and trains an
autoencoder to predict those values by adding a decoding layer with parameters w (cid:48)
2.

4

researchers have shown that this pretraining idea improves deep neural networks; perhaps because
pretraining is done one layer at a time which means it does not su   er from the di   culty of full supervised
learning. from 2006 to 2011, this approach gained much traction as a scienti   c endeavor because the
brain is likely to unsupervised learning as well. unsupervised learning is also more appealing because it
makes use of inexpensive unlabeled data. since 2012, this research direction however has gone through
a relatively quiet period, because unsupervised learning is less relevant when a lot of labeled data are
available. nonetheless, there are a few recent research attempts to revive this area, for example, using
variational methods for probabilistic autoencoders [24].

3 convolutional neural networks

since 2012, one of the most important results in deep learning is the use of convolutional neural networks
to obtain a remarkable improvement in object recognition for id163 [25]. in the following sections, i
will discuss this powerful architecture in detail.

3.1 using local networks for high dimensional inputs

in all networks that you have seen so far, every neuron in the    rst hidden layer connects to all the neurons
in the inputs:

this does not work when x is high-dimensional (the number of bubbles is large) because every neuron ends
up with many connections. for example, when x is a small image of 100x100 pixels (i.e., input vector has
10,000 dimensions), every neuron has 10,000 parameters. to make this more e   cient, we can force each
neuron to have a small number of connections to the input. the connection patterns can be designed to    t
some structure in the inputs. for example, in the case of images, the connection patterns are that neurons
can only look at adjacent pixels in the input image:

we can extend this idea to force local connectivity in many layers, to obtain a deep locally connected
network. training with id119 is possible because we can modify the id26 algorithm
to deal with local connectivity: in the forward pass, we can compute the values of neurons by assuming that
the empty connections have weights of zeros; wheareas in the backward pass, we do not need to compute
the gradients for the empty connections.

this kind of networks has many names: local networks, locally connected networks, local receptive    eld
networks. the last name is inspired by the fact that neurons in the brain are also mostly locally connected,
and the corresponding terminology in neuroscience/biology is    local receptive    eld.   

5

3.2 translational invariance with convolutional neural networks

in the previous section, we see that using locality structures signi   cantly reduces the number of connections.
even further reduction can be achieved via another technique called    weight sharing.    in weight sharing,
some of the parameters in the model are constrained to be equal to each other. for example, in the following
layer of a network, we have the following constraints w1 = w4 = w7, w2 = w5 = w8 and w3 = w6 = w9
(edges that have the same color have the same weight):

with these constraints, the model can be quite compact in terms of number of actual parameters. instead
of storing all the weights from w1 to w9, we only need to store w1, w2, w3. values for other connections
can then be derived from these three values.

this idea of sharing the weights resembles an important operation in signal processing known as con-
volution. in convolution, we can apply a       lter    (a set of weights) to many positions in the input signals.
in practice, this type of networks also comes with another layer known as the    max-pooling layer.    the
max-pooling layer computes the max value of a selected set of output neurons from the convolutional layer
and uses these as inputs to higher layers:

an interesting property of this approach is that the output of the max-pooling neurons are invariant
to shifts in the inputs. to see this, consider the following two examples: x1 = [0, 1, 0, 0, 0, 0, 0, ...] and
x2 = [0, 0, 0, 1, 0, 0, 0, ...] which can be thought of as two 1d input images, each with one white dot. two
images are the same, except that in x2, the white dot gets shifted two pixels to the right compared to x1.
we can see that as the dot moves 2 pixels to the right, the value of the    rst max pooling neuron changes

from w2 to w5. but as w2 = w5, the value of the neuron is unchanged:

this property, that the outputs of the system are invariant to translation, is also known as translational
invariance. translational invariant systems are typically e   ective for natural data (such as images, sounds

6

etc.) because a major source of distortions in natural data is typically translation.

this type of networks is also known as convolutional neural networks (sometimes called convnets). the
max pooling layer in the convolutional neural networks is also known as the subsampling layer because
it dramatically reduces the size of the input data. the max operation can sometimes be replaced by the
average operation.

many recent convolutional neural networks also have another type of layers, called local contrast
id172 (lcn) [19]. this layer operates on the outputs of the max-pooling layer. its goal is to
subtract the mean and divide the standard deviation of the incoming neurons (in the same manner with
max-pooling). this operation allows brightness invariance, which is useful for image recognition.

we can also construct a deep convolutional neural networks by treating the outputs of a max-pooling
layer (or lcn layer) as a new input vector, and adding a new convolutional layer and a new max-pooling
layer (and maybe a lcn layer) on top of this vector.

finally, it is possible to modify the id26 algorithm to work with these layers. for example,

for the convolutional layer, we can do the following steps:

    in the forward pass, perform explicit computation with all the weights, w1, w2, ..., w9 (some of them

are the same),

    in the backward pass, compute the gradient    j
    when updating the weights, use the average of the gradients from the shared weights, e.g., w1 =

, for all the weights,

, ...,    j
   w9

   w1

w1       (    j

   w1

+    j
   w4

+    j
   w7

), w4 = w4       (    j

   w1

+    j
   w4

+    j
   w7

) and w7 = w7       (    j

+    j
   w4

+    j
   w7

)

   w1

for the max-pooling layer, in the forward pass, we need to remember what branch gives the max value, so
that in the backward pass, we only compute the gradient for that branch.

3.3 convolutional neural networks with multi-channel inputs

images typically have multiple channels (for example, red green blue channels). it is possible to modify
the convolutional architecture above to work with multiple channel inputs. the modi   cation is essentially
to have a    lter that looks at multiple channels. the weights are often not shared across channel.

the following    gure demonstrates a convolutional architecture for an image with two channels:

3.4 convolutional neural networks with multiple maps

the current convolutional architecture has only one    lter per position of the input. we can extend that
to have many    lters per location. for instance, at one location we can have two    lters looking at exactly
the same input (to keep things simple, we only have one input channel):

7

each set of output produced by each    lter is called a    map.    in the example above, the outputs of the
two    lters create two maps. map 1 is created by the    rst    lter and map 2 is created by the second    lter.
finally, to pass information forward, we can treat the output as an image with multiple channels where
an output map is an input channel.

3.5 some practical considerations when implementing convolutional neural networks
    in contrast to what was discussed above, images are typically two-dimensional. it   s easy to modify the
above architecture to deal with two-dimensional inputs: each    lter has two dimensions. if the input
images have many channels, then each    lter is essentially three-dimensional: row-column-channel.

    so far, a convnet architecture only considers input with    xed size, for example, all images have to
be 100x100 pixels. in reality, however, images may have many sizes. to deal with that, it is typical
to crop the images at the center and convert all images to the desired size.

    many state-of-the-art convnets are sequences of processing blocks where each block is a combination
of convolution, max pooling, lcn. finally, there is one or two fully connected hidden layers that
connects the output of the last lcn units to the classi   er. dropout (see previous tutorial) is usually
applied at these fully connected layers.

    many libraries have fast convolution operations, whose implementation may dynamically decide
whether to convert the convolution step to an fft operation or not. for example, in matlab,
there are conv for 1d convolution and conv2 for 2d convolution, which convert the the convolution
operation to fft, depending on the size of the image and the    lter. these operations are typically
faster than implementing the explicit for loop to compute the forward pass and backward pass.

convolutional neural networks are undergone rapid developments. since the breakthrough work of [25],
many novel architectures have been proposed, which result in improvements in object recognition perfor-
mances [35, 38]. as it is di   cult to implement and train a good convolutional neural network, i recommend
using some existing software packages. an excellent implementation of state-of-the-art convolutional mod-
els and training recipes is ca   e (http://caffe.berkeleyvision.org/) [21].

4 sequence prediction with recurrent neural networks

in the following sections, i will discuss a simple modi   cation to neural networks such that they can work
with time series data.

8

4.1 sequence prediction

suppose i would like to predict the stock prices of tech companies tomorrow. as a start, i search for the
stock history of several companies and analyze them. the following are the performances of two example
companies, a and b:

my goal is to build a model to predict the stock market of any company given their historical data. the
training data is the stock history that i   ve collected. so, what is input x and output y? to simplify
matters, i decide that for a given company, the input x is all the stock prices up to yesterday, and output
y is the stock price of today. so essentially, the input x is [x0, x1, ..., xt ], where xi is the stock price of day
i-th since ipo and the output y is the stock price of today. our goal is to train a model to predict y given
all the x   s. note that this is an oversimplistic model to predict stock prices, as it does not have access to
other information such as founders, news etc.

the challenge is that the inputs are variable-sized: the value of t is di   erent for di   erent companies.
for instance, in the above    gure, company b is ipo   ed since june 2012 so its t is larger than that of
company a which is ipo   ed since many 2014. in fact, none of our solutions so far works with variable-
sized inputs. one way to address the variable-sized input problem is to use convolutional neural network
where the max pooling is applied to all of the output of the    lters below. that way, even though the
input is variable-sized, the output is    x-sized (i.e., equal to the number of    lters). the problem with this
approach is that convolutional neural network, as stated above, is invariant to translation. with such a
large max-pooling, losing position information is inevitable.

translation invariance is acceptable for images because the output of an object recognition system
should be invariant to translation.
in contrast, in our stock prediction model, this is an undesirable
property, because we want to make use of the precise temporal information (the stock price today is
likely to be more in   uenced by the stock price yesterday than 10 years ago). a proper way to deal with
variable-sized inputs is to use a recurrent neural network, as described below.

4.2 recurrent neural networks

a recurrent neural network for our stock prediction task should look like this:

9

here, as mentioned above, x0, x1, ..., xt are stock prices of a company up to today. h0, h1, ..., ht are the
hidden states of the recurrent network. note also that the bubbles in this    gure indicate a layer.

for a recurrent neural network, there are typically three sets of parameters: the input to hidden weights
(w ), the hidden to hidden weights (u ), and the hidden to label weight (v ). notice that all the w    s are
shared, all the u    s are shared and all the v    s are shared. the weight sharing property makes our network
suitable for variable-sized inputs. even if t grows, the size of our parameters stay the same: it   s only w, u
and v . with these notations, the hidden states are recursively computed as:

f (x) = v ht

ht =   (u ht   1 + w xt), for t = t, .., 1

...

h0 =   (w x0)

we can then minimize our cost function (y     f (x))2 to obtain the appropriate weights. to compute the
gradient of the recurrent neural network, we can use id26 again. in this case, the algorithm
also has another name: id26 through time (bptt).

since there are tied-weights in the network, we can apply the same idea in the convolutional neural net-
work: in the forward pass, pretend that the weights are not shared (so that we have weights w0, w1, ..., wt
and u0, u1, ..., ut ). in the backward pass, compute the gradient with respect to all w    s and all u    s. the
   nal gradient of w is the sum of all gradients for w0, w1, ..., wt ; and the    nal gradient of u is the sum of
all gradients for u0, u1, ..., ut .

4.3 gradient clipping

in practice, when computing the gradient for the recurrent neural network, one can    nd that the gradient
is either very large or very small. this can cause the optimizer to converge slowly. to speed up training,
it is important to clip the gradient at certain values. for example, any dimension of the gradient should
be smaller than 1, if the value of a dimension is larger than 1 we should set it to be 1.

4.4 recurrent neural network for id38

an important contribution of recurrent neural networks is in the area of id38 [32], an
important task in natural language processing. the goal of id38 is simple, given the previous
words in a text document, predict the next word. for example, to predict the word that comes after these
contexts:

i go to ...
i play soccer with ...
the plane    ew within 40 feet of ...

unlike many previous tasks, we have to deal with words instead of numbers for the inputs and outputs. it
turns out that it is possible to convert a word to a vector, as follows. first, we will construct a dictionary
of all possible words in our text data. every word is then associated with an index, for example, a is
associated with index 1; the is associated with 10,000 and zzzz is associated with 20,000 (the last entry
in the dictionary). using this dictionary, every word is represented as a vector of 20,000 dimensions (the
size of the dictionary) and all of its dimensions are zeros but one that has a value of 1. the non-zero
dimension corresponds to the index of the word in the dictionary. for example, the is represented as
[0, 0, ...0, 1, 0, ...0] and the index of 1 is 10,000. this representation is also known as the one-of-k encoding.
with this numerical representation, we can use recurrent neural network to learn a model to predict the
next word.

10

interestingly, if we multiply the input with w we essentially select a column of matrix w . the resulting
vector is also known as a    word vector.    if we visualize the word vectors (maybe with principal component
analysis), we will see that words that have similar meanings have similar vectors. this property makes
word vectors attractive for many language processing tasks.

4.5 other methods to learn word vectors

word vectors are, in a sense, an orthogonal idea to recurrent neural networks. there are many architectures
that can learn word vectors, e.g., convolutional neural networks.
in fact, one of the best word vector
methods in id97 package uses a simple convolutional network which averages the hidden representation
of a context to predict the middle word (the cbow model) or uses the hidden representation of the middle
word to predict the surrounding context (the skipgram model) [30]. these word vectors are better than
1-of-k encoding when plugged into other applications because word vectors keep better semantics of the
words. one can also extend word vectors to paragraph vectors [27], which can represent a document as a
vector.

that said, if the purpose is to predict the next word given the previous words, then recurrent neural

networks usually work better.

4.6 long short term memory networks

id26 through time (bptt) for recurrent neural networks is usually di   cult due to a problem
known as vanishing/exploding gradient [16]: the magnitude of the gradient becomes extremely small or
extremely large towards the    rst or the last time steps in the network. this problem makes the training
of recurrent neural network challenging, especially when there are a lot of long term dependencies (the
output prediction is in   uenced by long-distance pieces of information in the inputs).

let   s    rst talk about the consequence of this. suppose we want to predict the movie rating after
reading a one-sentence english review. since most english sentences begin with the word    the    and since
the gradient becomes small towards the beginning of the sentence, the word vector for    the    will hardly
be changed during the optimization. this is an undesirable property as we would like fair updates for all
the words in the document.

in terms of optimization, this means that the gradient magnitude in some certain dimension is extremely
small, and in some other dimension is extremely large. in such case, the curvature of the objective function
in the parameter space looks like a valley, as illustrated by a contour plot of a function with two variables:

11

a high learning rate is bad because it causes overshooting in some dimension. most often, we have to set
the learning rate extremely small to avoid overshooting and this makes learning extremely slow.

what is the cause? the reason for this vanishing/exploding gradient problem is due to the use of sig-
moidal id180 in recurrent networks. as the error derivatives are backpropagated backwards,
it has to be multiplied by the derivative of the sigmoid (or tanh) function which can saturate quickly. one
can imagine that the relu activation function can help here as its derivative allows for better gradient
   ow. however, another problem arises as we use relu as the activation function:
in the forward prop
computation, if we are not careful with the initialization of the u matrix, the hidden values can explode
or vanish depending on whether the eigenvalues of u are bigger or smaller than 1. attempts have been
made to    x this problem by using identity as part of the transfer function [31, 26] with certain success.

perhaps the most successful attempt to improve the learning of recurrent networks to date is long short
term memory (lstm) recurrent networks [17, 12]. the idea behind lstm is to modify the architecture
of recurrent networks to allow the error derivatives to    ow better. in this tutorial, i will use the formulation
of [13] with small modi   cations to explain the construction of lstm.

at the heart of an lstm is the concept of memory cells which act as an integrator over time. suppose
that input data at time t is xt and the hidden state at the previous timestep is ht   1, then the memory
cells at time t have values:

mt =    (cid:12) mt   1 +    (cid:12) f (xt, ht   1)

where (cid:12) is an element-wise multiplication between two vectors. just think of mt as a linearly weighted
combination between mt   1 and f . a nice property of mt is that it is computed additively and not associated
with any nonlinearity. so if the error derivatives cannot pass through the function f at time step t (to
subsequently    ow through ht   1), it has an opportunity to propagated backward further through mt   1. in
other words, it allows another path for the error derivatives to    ow. to prevent m from exploding, f is
often associated with a sigmoid/tanh function.

built on this construct, the hidden state of the network at time t can be computed as following:

ht =    (cid:12) tanh(mt)

note again that both ht and mt will be used as inputs to the next time steps, and that means the gradient
has more opportunities to    ow through di   erent paths.

the variables   ,    and    are often called    gates    as they modulate the contribution of input, memory,

and output to the next time step. at a time step t, they are computed as:

  (t) = g1(xt, ht   1, mt   1)
  (t) = g2(xt, ht   1, mt   1)
  (t) = g3(xt, ht   1, mt)

usually these functions can be implemented as follows:

  (t) =   (wx  xt + wh  ht   1 + wm  mt   1 + b  )
  (t) =   (wx  xt + wh  ht   1 + wm  mt   1 + b  )
  (t) =   (wx  xt + wh  ht   1 + wm  mt + b  )

f (xt, ht   1) = tanh(wxmxt + whmht   1 + bm)

12

a simpli   ed    gure that shows gradient    owing from timestep t + 1 to t should look like this:

in the above    gure, the green and the red paths are the two paths that gradient can    ow back from
mt+1 to mt. i want to emphasize that mt is linearly computed which means the gradient can continue
to    ow through mt as well. hence the green path, which generates nonlinear ouputs, is a    di   cult    path
for gradient to    ow; whereas the red path, which only generates linear functions, is an    easy    path for
gradient to    ow.

under this construction, the following properties can be achieved with lstm:
    it is possible to set up the connections initially such that the lstm can behave like a convnet,

thereby allowing gradients to    ow to all timesteps,

    it is also possible to set up the connections initially to recover standard recurrent networks. so in

the very worst case, we don   t lose anything,

    units in lstm are mostly bounded and numerically stable. in particular, even though mt allows
values to be added, the added values are bounded between -1 and 1, which is rather small. for
example, if we have a very long sequence in our training data with a million timesteps, the value of
mt is bounded between    106 and 106, which is not too bad.

5 sequence output prediction with recurrent neural networks

in the above sections, the prediction outputs have always been a scalar (classi   cation, and regression) or
a    xed-length vector (autoencoders). but this is restrictive since in many applications the desired outputs
can be a variable-length sequence. for example, in the task of machine translation, we would like to predict
the target sentence from the source sentence; or in the task of automatic image captioning, we would like
to predict the caption given the input image.

a recent advance in deep learning is to use a recurrent network as a    dynamic    classi   er. by    dy-
namic,    i mean that the classi   er can predict more than just    xed-length vectors, which have been the
mainstream of machine learning for the past decades. instead, it can be used to predict a variable-length
sequence. this framework of using recurrent networks as a variable-sized predictor was proposed in
di   erent forms by [22, 7, 37]. in the following, i will use the formulation of [37] to describe the method.

13

5.1 the basics

suppose during training, we are presented with an input vector x (a hidden layer in convolutional or
recurrent networks), and a target output sequence y1y2y3. we would like to train a recurrent network to
go from x to y1y2y3.

we know that we can use a recurrent network to predict y1 from x, but what is the input of y2? well,
that brings us to the    rst trick of this paradigm. the trick is that the input of y2 can just simply be y1. this
means that during training we feed the ground-truth targets as inputs to the next step prediction. this is
good because during training, we see the ground-truth targets anyway. hence, the model architecture can
look like this:

(in the above    gure, i use a    zero vector    as input to the    rst step of the network to keep it simple and
consistent with later steps; the    rst set of weights u1 and other set of weights u2 can be di   erent.)

the problem is that during id136, we will not see the ground-truth targets. so what can we use as
inputs to make a prediction as the second step? well, we can do something rather greedy: we simply take
the best prediction of the previous timestep as the input to the current timestep. this is sometimes called
   greedy search.   

the problem with greedy search, as its name suggested, is that it may not be optimal. more concretely,
the joint id203 of the output sequence may not be highest possible. to do better than greedy search,
one can do a    full search:    we use the recurrent networks to compute the joint id203 of every possible
output sequence. this can guarantee to    nd the best possible sequence with a large computation cost.

greedy search and full search lie at two extremes. one is very fast but not optimal, the other is
extremely slow but optimal. to be practical, maybe we can do something in between. this brings us to
another trick: left-to-right    id125.    in id125, we can keep a list of k possible sequences sorted
by the joint id203 (which is computed by multiplying the output id203 of each prediction in the
sequence produced so far). we will generate output predictions (or    decode   ) from left to right starting
from the    rst output. during the decode procedure, any sequence that does not belong to the top-k highest
joint id203 will be removed from our list of candidates. this id125 procedure, proposed    rst
in [37], completes the basics of sequence output prediction. even though it   s not guaranteed for the beam
search to achieve the optimal sequence, in practice, the generated sequences in the top-k list are generally
very good.

the notion of when to stop producing the output sequence (halting) is interesting and worth some of
attention as well. in practice, we process our data to always have a termination symbol at the end of
the sequence (i.e., y3 =    eos    or end-of-sequence symbol). this way, when the id125 arrives at the
termination symbol, we will terminate that particular beam.

14

5.2 the attention model

what   s missing from the above analysis is the fact that the input vector x is static but with gradient
descent, we can always adjust x by backpropagating through x to the input network. as such it feels
alright if the input network, that gives rise to x, takes some other    xed-length vectors as inputs. but a
problem arises if the input network takes some variable-length sequences as well. the problem is that we
have to push all variable-length information into a    xed-length vector x. a small dimension for x works
well for short sequences but less so for longer sequences and vice versa.

perhaps a better model is to allow the output recurrent network to pay attention to the certain part of
the input network so that it doesn   t have to rely on this bottleneck in x. the modi   cation to our model
goes as follows. at every time step in the output, we will predict an auxiliary hidden state called u; this
u vector will be used to compute the dot product with the hidden states at all timesteps in the input
network. these dot products are used as weights to combine all hidden states into a vector v. this vector
can be then used as additional inputs to our prediction at the current time step.

more concretely, let   s assume that we are at step    in our output recurrent network. suppose also that
the input sequence is represented by another recurrent network, whose the input sequence has t steps
(x1, ..., xt ). the hidden states of the output network is denoted by h and the hidden states of the input
network is denoted by g. here   s the math:

for all t = 1..t

for all t = 1..t

u   =   h  
  t = ut
   gt

(cid:80)t
t(cid:88)

  t =

v   =

exp(  t)
t=1 exp(  t)

  tgt

t=1

[v   , h   ]     y  

in the last equation, the arrow means that we use the combined vector [v   , h   ] to predict y   . the vector   
is also called the alignment vector. due to the softmax equation in computing      s, their values are bounded
between [0, 1]. note also that      s and      s should be indexed by    but i remove that for simplicity, and
readers should understand that these temporary variables are computed di   erently at di   erent step of    .

pictorially, the attention model looks like this:

15

(i simpli   ed the    gure by not drawing    and   , and also assuming that    = 1, the    rst step in the output
sequence.)

these equations cover the basics, but other variations exist, e.g., having a nonlinearity for u   or using
v   as additional inputs to the next timesteps. most importantly, during training, all of these variables are
adjusted by id119 together with other variables in the networks.

in addition to the aforementioned dynamic memory access argument, the attention model can also be
interpreted as a way to form shortcut connections to parts of the input data. in any case, it has been
shown to be a very e   ective mechanism for input sequence, output sequence prediction problems. this
method was    rst proposed by [1].

5.3 applications and extensions

this idea of using recurrent networks as sequence output prediction and id125 during id136 (with
and without attention model) paves the way for numerous applications, e.g., in machine translation [29, 20],
image captioning [40, 41], parsing [39], and id103 [6, 2].

an interesting property of the attention model is that it is a di   erentiable pooling mechanism of the
input data. this di   erentiable attention mechanism has now been used as pooling mechanism for facts
in a database (memory networks) [36] and written content in an augmented memory (neural turing
machines) [14].

another interpretation of the attention model is that it allows an o(t ) computation per prediction
step. so the model itself has o(t 2) total computation (assuming the lengths of input and output sequences
are roughly the same). with this interpretation, an alternative approach to the attention model is to lay
out the input and output sequences in a grid structure to allow o(t 2) computation. this idea is called
grid-lstm and was    rst proposed by [23].

6 parallelism with neural networks

training neural networks takes a long time, especially when the training set is large. it therefore makes
sense to use many machines to train our neural networks.

distbelief [9] is one of the most well-known frameworks for scaling up the training neural networks
using many machines. distbelief has two levels of parallelism: model parallelism and data parallelism.
first, every model in distbelief is partitioned into multiple machines, as suggested by the following    gure:

in the above    gure, a neural network with four hidden layers and one output layer is partitioned into four
machines. there will be communications across machines, but if the network is locally connected, the

16

communication cost is small compared to the computation cost. this idea of partitioning a model into
several machines is known as model parallelism.

the second level of parallelism is by partitioning the data into di   erent shards, and training di   erent
copies of the model on each shard. to keep them in sync, we can have a set of machines known as the
parameter server that stores the parameters. the communication to the parameter server is asynchronous:
at every iteration, each copy of the model (   a replica   ) will compute the gradient on its data, and then
send the gradient (   p) to the parameter server, the parameter server will grab the gradient and update its
own parameter copy to arrive at p(cid:48). after several updates, the parameter server will broadcast the new pa-
rameters to the network. all replicas of the model will replace its parameters with the arriving parameters.
this way of parallelizing the training is known as data parallelism via asynchronous communication.1

the asynchronous communication can be thought of as a soft way to average the gradient without waiting.
an advantage of this approach is that it is insensitive to machine slowness:
if a replica of the model is
slow, it won   t delay the entire training.

maybe less well-known is the third level of parallelism: multi-threading within a machine. in this level,
we can allocate several cores to do matrix vector computation, and several cores to do data reading so that
once computation is done, data is ready in memory for processing.

7 recommended readings

convolutional networks have a long history [11, 28]. they have been used extensively to read handwritten
checks and zip codes. since the work of [25], convolutional neural networks are widely adopted by computer
vision researchers.

in this work, i discussed id38 using recurrent networks. feedforward networks have
also been used for this task with certain success [3] but generally less competitive compared to recurrent
networks due to their limitations in handling the word ordering.

this tutorial covers only the very basic optimization strategy: stochastic id119 (sgd). it
is possible to improve sgd with either adagrad [10] or batch id172 [18]. in adagrad, we will
keep a running norm of the gradient over time and use that to compute a learning rate per dimension. in
batch id172, we will compute a running mean and variance of hidden units over time, and use
them to    whiten    hidden units over time.

another topic that i didn   t go into great depth but can be useful for neural networks is model en-
sembling. model ensembling is a technique where we combine many models by averaging its prediction

1the idea of asynchronous sgd was perhaps    rst proposed by [34].

17

outputs. these models can be di   erent in terms of random seeds or architectures. this has been used
extensively to win competitions (e.g., [25]) or beat hard baselines (i.e., [37]). this technique is also e   ective
for other non deep learning methods.

8 miscellaneous

many networks in this tutorial can be di   cult to implement. i recommend the readers use ca   e (http:
//caffe.berkeleyvision.org/) [21] or theano (http://deeplearning.net/software/theano/) [5] or
torch7 (http://torch.ch/) [8]. to learn word vectors, i recommend id97 (https://code.google.
com/p/id97/) [33].

this tutorial was written as preparation materials for the machine learning summer school at cmu
(mlss   14) with some parts being extended. videos of the lectures are at http://tinyurl.com/pduxz2z .

if you    nd bugs with this tutorial, please send them to me at qvl@google.com .

9 acknowledgements

i would like to thank pangwei koh, thai t. pham and members of the google brain team for many
insightful comments and suggestions. i am also grateful to many readers who gave various comments and
corrections to the tutorial.

references

[1] d. bahdanau, k. cho, and y. bengio. id4 by jointly learning to align and

translate. arxiv preprint arxiv:1409.0473, 2014.

[2] d. bahdanau, j. chorowski, d. serdyuk, p. brakel, and y. bengio. end-to-end attention-based large

vocabulary id103. arxiv preprint arxiv:1508.04395, 2015.

[3] y. bengio, r. ducharme, p. vincent, and c. jauvin. a neural probabilistic language model. the

journal of machine learning research, 3:1137   1155, 2003.

[4] y. bengio, p. lamblin, d. popovici, and h. larochelle. greedy layer-wise training of deep networks.

in advances in neural information processing systems, 2007.

[5] j. bergstra, o. breuleux, f. bastien, p. lamblin, r. pascanu, g. desjardins, j. turian, d. warde-
farley, and y. bengio. theano: a cpu and gpu math expression compiler. in proceedings of the python
for scienti   c computing conference (scipy), volume 4, page 3, 2010.

[6] w. chan, n. jaitly, q. v. le, and o. vinyals.

listen, attend and spell.

arxiv preprint

arxiv:1508.01211, 2015.

[7] k. cho, b. van merri  enboer, c. gulcehre, d. bahdanau, f. bougares, h. schwenk, and y. bengio.
learning phrase representations using id56 encoder-decoder for id151. arxiv
preprint arxiv:1406.1078, 2014.

[8] r. collobert, k. kavukcuoglu, and c. farabet. torch7: a matlab-like environment for machine

learning. in biglearn, nips workshop, 2011.

[9] j. dean, g. s. corrado, r. monga, k. chen, m. devin, q. v. le, m. z. mao, m. a. ranzato, a. senior,

p. tucker, k. yang, and a. y. ng. large scale distributed deep networks. in nips, 2012.

18

[10] j. duchi, e. hazan, and y. singer. adaptive subgradient methods for online learning and stochastic

optimization. the journal of machine learning research, 12:2121   2159, 2011.

[11] k. fukushima. neocognitron: a self-organizing neural network model for a mechanism of pattern

recognition una   ected by shift in position. biological cybernetics, 1980.

[12] f. a. gers, n. n. schraudolph, and j. schmidhuber. learning precise timing with lstm recurrent

networks. the journal of machine learning research, 2003.

[13] a. graves. generating sequences with recurrent neural networks. in arxiv, 2013.

[14] a. graves, g. wayne, and i. danihelka. id63s. arxiv preprint arxiv:1410.5401,

2014.

[15] g. e. hinton, s. osindero, and y.-w. teh. a fast learning algorithm for deep belief nets. neural

computation, 18(7):1527   1554, 2006.

[16] s. hochreiter, y. bengio, p. frasconi, and j. schmidhuber. gradient    ow in recurrent nets: the
di   culty of learning long-term dependencies. a field guide to dynamical recurrent neural networks,
2001.

[17] s. hochreiter and j. schmidhuber. long short-term memory. neural computation, 1997.

[18] s. io   e and c. szegedy. batch id172: accelerating deep network training by reducing internal

covariate shift. arxiv preprint arxiv:1502.03167, 2015.

[19] k. jarrett, k. kavukcuoglu, m. a. ranzato, and y. lecun. what is the best multi-stage architecture

for object recognition? in iccv, 2009.

[20] s. jean, k. cho, r. memisevic, and y. bengio. on using very large target vocabulary for neural

machine translation. arxiv preprint arxiv:1412.2007, 2014.

[21] y. jia, e. shelhamer, j. donahue, s. karayev, j. long, r. girshick, s. guadarrama, and t. darrell.
ca   e: convolutional architecture for fast feature embedding. arxiv preprint arxiv:1408.5093, 2014.

[22] n. kalchbrenner and p. blunsom. recurrent continuous translation models. in emnlp, pages 1700   

1709, 2013.

[23] n. kalchbrenner, i. danihelka, and a. graves. grid long short-term memory.

arxiv preprint

arxiv:1507.01526, 2015.

[24] d. p. kingma and m. welling. auto-encoding id58. arxiv preprint arxiv:1312.6114,

2013.

[25] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classi   cation with deep convolutional neural

networks. in advances in neural information processing systems, 2012.

[26] q. v. le, n. jaitly, and g. e. hinton. a simple way to initialize recurrent networks of recti   ed linear

units. arxiv preprint arxiv:1504.00941, 2015.

[27] q. v. le and t. mikolov. distributed representations of sentences and documents. arxiv preprint

arxiv:1405.4053, 2014.

[28] y. lecun, l. bottou, y. bengio, and p. ha   ner. gradient-based learning applied to document

recognition. proceedings of the ieee, 1998.

19

[29] t. luong, i. sutskever, q. v. le, o. vinyals, and w. zaremba. addressing the rare word problem in

id4. arxiv preprint arxiv:1410.8206, 2014.

[30] t. mikolov, k. chen, g. corrado, and j. dean. e   cient estimation of word representations in vector

space. arxiv preprint arxiv:1301.3781, 2013.

[31] t. mikolov, a. joulin, s. chopra, m. mathieu, and m. a. ranzato. learning longer memory in

recurrent neural networks. arxiv preprint arxiv:1412.7753, 2014.

[32] t. mikolov, m. kara     at, l. burget, j. cernock`y, and s. khudanpur. recurrent neural network based

language model. in interspeech, pages 1045   1048, 2010.

[33] t. mikolov, i. sutskever, k. chen, g. corrado, and j. dean. distributed representations of phrases

and their compositionality. in advances on neural information processing systems, 2013.

[34] b. recht, c. re, s. wright, and f. niu. hogwild!: a lock-free approach to parallelizing stochastic

id119. in advances in neural information processing systems, pages 693   701, 2011.

[35] k. simonyan and a. zisserman. very deep convolutional networks for large-scale image recognition.

arxiv preprint arxiv:1409.1556, 2014.

[36] s. sukhbaatar, a. szlam, j. weston, and r. fergus. end-to-end memory networks. arxiv preprint

arxiv:1503.08895, 2015.

[37] i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with neural networks.

in

advances in neural information processing systems, pages 3104   3112, 2014.

[38] c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov, d. erhan, v. vanhoucke, and

a. rabinovich. going deeper with convolutions. arxiv preprint arxiv:1409.4842, 2014.

[39] o. vinyals, l. kaiser, t. koo, s. petrov, i. sutskever, and g. hinton. grammar as a foreign language.

arxiv preprint arxiv:1412.7449, 2014.

[40] o. vinyals, a. toshev, s. bengio, and d. erhan. show and tell: a neural image caption generator.

arxiv preprint arxiv:1411.4555, 2014.

[41] k. xu, j. ba, r. kiros, a. courville, r. salakhutdinov, r. zemel, and y. bengio. show, attend and
tell: neural image id134 with visual attention. arxiv preprint arxiv:1502.03044, 2015.

20

