advanced markov logic 
techniques for scalable 
joint id136 in nlp

deepak venugopal*, vibhav gogate** and vincent ng**

* the university of memphis
** the university of texas at dallas

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

2

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

3

traditional machine learning

training data

instance_1

instance_2

instance_3

   .

traditional machine learning

training data

instance_1

instance_2

instance_3

   .

    dependency trees
    part-subpart relationships,
    entity-event relationships
    temporal relations
    predicate-argument relationship
       

hard to translate to 
relations to feature 
vectors

traditional machine learning

training data

instance_1

instance_2

instance_3

   .

    forces instances to be i.i.d

    instances in language are clearly 

interdependent. 

    e.g. an event depends upon 

other described events in the 
text

traditional machine learning

training data

instance_1

instance_2

instance_3

   .

    language has ambiguity

    microsoft buys apple

    microsoft acquires apple

    the merger of microsoft and apple

need to learn robust 
models that handle noisy 
data

   new    machine learning

    language data is relational

part-of

    language data is noisy

    language data is large

sub-type

dependency

need richer models!!!

8

statistical relational models

logical/relatio

nal

probabilistic

statistical relational models

    long-standing goal of ai to unify logic & probabilities
    several early attempts to unify the two (leiniz 17th century, carnap

   50, gaifman    64)

    over the last decade, the area of statistical relational models has

made significant progress towards this goal

statistical relational models

    several popular models

    markov logic (pedro domingos    group at uw)
    blog (stuart russell   s group at berkeley)
    psl (lise getoor   s group at umd/ucsc)
    problog (luc de raedt   s group at ku-leuven)

    mlns are arguably one of

the most popular statistical

relational models
    simple interpretation
    powerful representation

    this tutorial is mainly focused on mlns though similar ideas
are potentially useful in other statistical relational models as
well

10

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

11

what representation can we learn from 
noisy, relational big data?

unify the two (long 
standing dream of ai)

id85 represents complex 
relational knowledge extremely 
compactly
e.g. rules of chess can be written in one 
page in fol while it takes 1038 pages of 
finite automata to represent it*!

12

probabilistic id114 (markov 
networks, id110s) can 
represent large joint distributions 
compactly

[*russell cacm    15]

mln representation

    weighted first-order formulas

                                                                                       ,                                          ; 1.75

    larger the weight, more likely is the formula to be true
    for weight    , the formula specifies a hard constraint just 

as in id85

    variables in mln formulas can be instantiated with 

constants representing real-world objects

    mlns are a template for generating large markov 

networks (undirected probabilistic graphical model)
    represent a joint distribution over the possible worlds

13

markov logic networks (mlns)

                                                                                   ,                                         

0.75

                                                                                   ,                                                              .         
                                                                                   ,                                                              .         
                                                                                   ,                                                              .         
                                                                                   ,                                                              .         

smokes

friends

asthma

(ana)

(ana,bob)

(bob)

    

0

0

1

   

1

1

0

0

0

   

1

1

14

0

1

0

   

0

1

    0.75

    0.75

    0.75

   

    0

    0.75

    (    )

    (    ,     )

    (    ,     )

    (    )

    (    ,     )

    (    )

    (    )

    (    ,     )

background

s(a) s(b) m(a) m(b)

f(a,b)

f(b,a)

f(a,a)

f(b,a)

    1
    2
    3

   

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

   

   

   

   

   

0

0

0

   

0

1

0

0

0

1

   

          =

1
    

              
    

        #                                    (    ,     )

     =  

    

              
    

        #                                    (    ,     )

15

mlns

    generalize several representations

    probabilistic id114

    id148

    id48

    id82s

    id49

    man id178 models

    id28

    gibbs distributions

16

typical mln application design process

domain 
knowledge

mln 

formulas

id136

17

weight 
learning

id136 problems

    marginal id136

                                                                                        ,              ,                                   =?

    map id136

    max

    

               , where      is the evidence and      is the set of 

remaining variables in the mln.

    both the above problems are computationally hard

    marginal id136 #p

    map id136 (np-hard)

18

weight learning

    can be performed generatively or discriminatively

    the training data is a relational database

    unlike traditional learning algorithms, in typical mln learning 

there is just one instance to learn from (the relational db)

    max-likelihood weight learning uses id136 during each 

step
    typically approximate id136 is utilized within weight 

learning

19

joint id136 using mlns

    several nlp applications are amenable to joint id136

    information extraction, coreference resolution, etc.
    joint id136 reduces error propagation that is commonly 

seen in pipeline architectures

    mlns are a particularly powerful representation for 

performing joint id136 in nlp applications
    can specify dependencies between different stages of the 

pipeline

    e.g.,  an extracted trigger and its associated arguments are 

dependent

    using mlns, we can model dependencies naturally using first-

order formulas

20

joint id136 using mlns

    integer id135 (ilp) has been widely used 

for joint id136 in nlp*
    specify joint dependencies as linear constraints

    powerful solvers can be easily leveraged off-the-shelf for 

solving the ilp
    gurobi

    ibm cplex

    lp_solve

    sat4j

    minisat+

       .

21

*roth conll    04

joint id136 using mlns

    the lifted representation of mlns makes it possible to 

specify complex joint constraints compactly
    in contrast joint id136 based on ilps use a propositional 

formulation

how can we specify the 
constraint that event 
coreferences are transitive?

ilp
    1         2         3
    2         3         4
    1         3         2
    2         4         1

   

(1         1) + 1         2
+     3     1
(1         2) + 1         3
+     4     1

   

mlns
       ,     ,                                                   ,     
                                            (    ,     )                                             (    ,     )

22

joint id136 using mlns

    weights on mln formulas make it easier to represent 

uncertainty of joint constraints

ilp
(    1        2         3)         1
(    2        3         4)         2
(    1        3         2)         3
(    2         4         1)         4

   

(1         1) + (1         1)
+ 1         2 +     3     1

    1 +     1     1
    2 +     1     1

(1         3) +     1     1

   

add                     to the 
objective function

mlns

       ,     ,                                                   ,                                                       ,     
                                                 ,      ;     

how can we specify 
the soft constraint 
that event 
coreferences are 
transitive?

23

joint id136 using mlns

ilp

(    1        2         3)         1
(    2        3         4)         2
(    1        3         2)         3
(    2         4         1)         4

   

add                     to the objective function

notice that all soft constraints 
are symmetrical. they have the 
same structure and same weight. 

we can exploit these 
symmetries for computational 
efficiency during id136 and 
learning in mlns.

mlns

       ,     ,                                                   ,                                                       ,     
                                                 ,      ;     

24

what are the challenges with mlns?

    scalability is the primary challenge in utilizing mlns for 

large application domains such as nlp
    id136 and learning are hard problems in mlns

    need tools for mlns that can work as a    black-box    for 

applications

    can   t we just convert mlns into a markov network and 

apply techniques there?
    unfortunately, mlns typically encode markov networks that 

are orders of magnitude larger than what (propositional) pgm 
id136 algorithms can handle

25

                                                                   ,                                         

#                             = 5

#                             = 10

26

#                             = 50

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

27

id136

                                                                                   

smokes(ana)

smokes(bob)

smokes(carl)

asthma(carl)

asthma(ana)

asthma(bob)

28

variable elimination

smokes(ana)

smokes(bob)

smokes(carl)

asthma(carl)

asthma(ana)

asthma(bob)

29

id136 based on conditioning

                                                                                   

smokes(ana)

smokes(bob)

smokes(carl)

asthma(carl)

asthma(ana)

asthma(bob)

30

condition on 
smokes

31

condition on 
smokes

     2             (     + 1)

32

within each group, conditioning on any of the                         atoms 
yields identical results. therefore, we can group the three 
                        atoms and condition on a single atom from this 
group.

33

lifted id136

    sometime possible to perform tractable id136 on 

large treewidth markov networks
    need to exploit symmetries within the markov network

    do we need to explicitly build the markov network to 

determine what the symmetries are?
    in some cases, can identify symmetries directly from first-order 

structure without grounding the mln 

    analogous to first-order theorem proving, in lifted 
id136 we work at the compact first-order level

34

domain-lifted id136

    informally,

    exploit symmetries, reason about groups of objects, 

reason at first-order level, etc (poole    03)

    more formally,

    id136 runs in time polynomial in the number of 

domain objects (broeck    11, jaeger    12)

    exact lifted id136

    lifted factor graphs (poole    03)
    first-order variable elimination (braz et al.    07) 
    weighted first-order model counting (broeck et al.    10) 
    probabilistic theorem proving (gogate and domingos

   11)

35

probabilistic theorem proving (ptp)

    dpll-style algorithm exact id136 algorithm that 

conditions over first-order atoms

    conditioning rule: conditions a singleton/unary atom in 

polynomial time

    decomposer rule: reduces the mln by merging identical 

and independent sub-components
                                                        ;      contains         independent and 

identical components in the markov network

    ptp applies the above two rules recursively until no more 

applications of the rule is possible. if the remaining mln 
is intractable, then the mln is not liftable through ptp.

36

ptp

smokes

                             
                                     

          algorithm for a 
markov network of 
treewidth     

condition k groundings 
to true and n-k to false

decompose identical 
and independent sub-
components

asthma

= true

= false

37

performance: link prediction

                                                                                                                                 ,                                                       

                               (    ,     )                                 (    ,     )

38

approximate id136

    similar to id136 in markov networks, exact id136 is 

infeasible in practical cases. most practical cases use 
approximate id136

    domain-lifted versions of several popular approximate 

id136 algorithms have been developed over past few years

    marginal id136

    lifted belief propagation (singla and domingos    07, kersting et al.    08)

    lifted mcmc (niepert    12, venugopal and gogate    12)

    lifted importance sampling (gogate et al.    12) 

    map id136

    lifted map (sarkhel et al.    14,    15, kersting et al.    14,    15)

39

lifted belief propagation

    exploits the fact that symmetries in mlns cause similar 

messages to be sent and received in bp

    creates a lifted network for bp that can sometimes be 

exponentially smaller than the ground network for bp
    groups atoms that send and receive same messages into 

supernodes

    groups ground formulas that send and receive the same 

messages into superfeatures

40

loopy belief propagation

               (    ) =  

              (    )

              (    )\{    }

nodes (ground 
atoms)

features 
(ground 
formulas)

        (    )        =  
~    

41

    (    )  

               (    )

               (    ){    }

lifted belief propagation

42

lifted belief propagation

               (    ) =                

         ,        1                (    )\{    }               (    )    (   ,    ), 

where     (    ,     ) is the 
number of identical 
messages that would be 
sent from f to x in the 
ground network

43

id150

                     ,                                                           ,              

                                     ,                                   ,              

-

-

0
1

1

-

-

0

0

1
0

1

0

-

-

1

-

-

-

-

1

0

-

-

44

geman and geman    84

blocked id150 (lbg)

-

-

01

10

-

-

01

01

10

10

01

-

-

10

-

-

-

-

1

0

-

-

45

jensen et al.    93

lifted blocked id150 (lbg)

exploit symmetries to 

compute the conditional 
distribution for a block

-

-

01

10

-

-

01

01

10

10

01

-

-

send 

10
sufficient 
statistics

-

-

-

-

1

0

-

-

46

venugopal and gogate    12

how many 

ground atoms 

                    

are true?

                

                    

-

0

1

-

-

-

0

0

1

1

0

-

-

1

-

-

-

1

0

-

-

47

                

                

                    

                    

                    

    (    2)

                    

    (    3)

more blocking can yield smaller complexities since more 

symmetries are retained!!

48

performance: topics mln

10000

1000

100

s
d
n
o
c
e
s
 
n

i
 
e
m
t

i

10

1

49

bg

lbg

20

40

60

80

100

120

140

160

180

200

number of objects

performance: webkb

)
c

i
t
s
i
t
a
t
s
 
r
-
g

(
 
e
t
a
r
 
e
c
n
e
g
r
e
v
n
o
c

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

0

50

50

100

150

200
250
time in seconds

lbg

bg

300

350

400

lifted importance sampling

original mln is not 
domain-liftable

k ground atoms 
are conditioned to 
true and the rest 
to false

create a lifted proposal 
distribution by applying 
rules approximately

a lifted sample (    )
with weight 

    (    )
 
    (    )

51

gogate et al.    12

challenges to applying lifted id136 in 
practice

    domain-lifting is applicable to a very small subset of mln 

structures
    mlns that contain 2 variables in a formula are domain-liftable

    lifting mlns containing 3 variables in a formula is #p-hard 

    for formulas commonly seen in nlp, such as 

                    (    ,     ,     )                                 (    ,     ,     )                         (    ,        ,        )    
                            (       ,     ,        )                                         (    ,     ,        ), no domain-
lifting is possible

52

broeck    11 jaeger    12

evidence problem

even for liftable mlns, 
evidence breaks symmetries

       ,                                                           (    ,     ); 1.75

evidence(true) atoms

atom

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

pre-evidence

marginals

0.56

0.56

0.56

0.56

0.56

0.56

0.56

0.56

0.56

                        (    )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

atom

evidence conditioned

marginals

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

                (    ,     )

0.6

0.6

0.63

0.85

0.85

id136 with evidence only on unary 
predicates is efficient while id136 
with evidence on binary predicates is #p-
hard (broeck and darwiche    13)

53

challenges to applying lifted id136 in 
practice

    domain-lifting only utilizes exact symmetries

    advantage: provable guarantees since lifting does not change 

the underlying distribution

    problem: computational complexity is often as high as a 

propositional id136 algorithm

    in mlns that encode complex problems such as those in 
nlp, domain-lifted id136 is insufficient. we need more 
flexible lifting methods
    trade-off guarantees with computational complexity

54

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

55

evidence problem

    conditioning on unary atoms is easy while conditioning 

on binary atoms is hard

    represent binary evidence as a combination of unary 

evidences

given binary evidence          ,      =
         ,      =          ,      =          ,      =
         ,      =          ,      =          ,      = 1, 
and the rest of the atoms are 0, we 
can represent evidence as a matrix

1 1
1 1
0 0
1 0

0 0
0 1
1 0
0 1

56

broeck and darwiche    13

boolean factorization of evidence

    decompose the evidence matrix into binary products of 

vectors

in general, we can factorize the binary evidence as 

where              =     1    1

             2    2

    .      is the boolean rank of     

     =             
                             

1 1
1 1
0 0
1 0

0 0
0 1
1 0
0 1

=

    

   

0
1
0
1

1
0
0
1

1
1
0
1

    

   

1
1
0
0

0
0
1
0

    

0
0
1
0

boolean rank = 3

57

boolean factorization of evidence

    id136 complexity can be characterized using boolean 

rank of the evidence matrix
    id136 is exponential in boolean rank

    how can we use this to introduce artificial symmetries?

    low rank boolean id105

    bounding the rank of the factorization will automatically 

smooth the evidence introducing more symmetries

58

boolean factorization of evidence

    low rank approximation

1 1
1 1
0 0
1 0

0 0
0 1
1 0
0 1

=

    

   

0
1
0
1

1
0
0
1

1
1
0
1

    

1
1
0
0

boolean rank 2 approximation 

1 1
1 1
0 0
1 0

0 0
0 1
     0
0 1

the new evidence has more symmetries than the original 
evidence

59

boolean factorization of evidence

link(aaai,google)

link(google,aaai)

link(google,gmail)

link(ibm,aaai)

link(aaai,google)
link(google,aaai)
link(google,gmail)
link(ibm,aaai)
link(aaai,ibm)

the objects google and ibm become more symmetric

60

boolean factorization of evidence

    lift the mln by converting binary evidences into their 

low rank approximations

    use the low-rank approximation as a proposal 

distribution in mcmc algorithms (broeck and niepert
   14)

    problems: no guarantees on results since we are changing 

the underlying mln distribution

61

performance on webkb

62

performance on webkb

63

performance on webkb

64

lifting using unsupervised machine 
learning

    formulate lifting as a id91 problem

    flexible framework

    can use vast off-the-shelf resources to perform id91
    example: w            (    ,     )                         (    ,     )                             (    ,     )    

                    (    ,     )
    is                              ,                                                                   ,                                         

?

    cluster similar topics together to compress the mln

65

venugopal and gogate    14

compressed domain

any existing id136 

algorithm implicitly works in 
the approximately-lifted space

replace each cluster 
with its cluster-center

66

original objects

features for id91

    characterize similarity 
between objects based 
on evidence

alice

chris

67

carla

bob

evidence based id91

                                                                   ,                                        

     =             
     =             

68

features for id91

    instantiate the mln with a single object and count the 

true groundings of each formula in the evidence
    counting the true groundings of a first-order formula is also a 

hard problem

    break the formula into multiple pieces and count 

independently

    advantages

    id91 adapts itself to changing evidence

    can implicitly lift several propositional id136 algorithms

    heuristic method: no guarantees on solution since we 

are changing the distribution

69

performance: citation segmentation

id150

id116

em

hc

0.1

0.05

0.01

0.005

0.001

0.0001

0.00005

compression ratio

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

r
o
r
r
e

70

performance: citation segmentation

belief propagation

id116

em

hc

0.1

0.05

0.01

0.005

0.001

0.0001

0.00005

compression ratio

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

r
o
r
r
e

71

experiments: citation segmentation

venugopal&gogate ecml/pkdd     14

id116

em

hc

2985

2965

2970

s
d
n
o
c
e
s
n

 

i
 
e
m
t

i

72

30

36

28

395

408

418

10 million

100 million

1 billion

#ground formulas

approximate lifting of bp

    problem with lifted bp: the lifted network turns out to be 
quite similar to the propositional network in the absence 
of symmetries

    stop the construction of the lifted network early

    start with a coarse network and refine it by keeping track 

identical bp messages

    stop refinement when the number of supernodes or 

superfeatures reaches a pre-specified threshold

    lifted network is approximate but is much smaller than the 

propositional network 

73

singla et al.    14

performance

74

approximate lifting for map

    map id136 in mlns can be solved using off-the-shelf 

id135 solvers
    lp implementations work in a propositional representation

    lift the mln as a pre-processing step before giving it to 

the lp solver

    exact lifting rule

    non-shared variables can be reduced to a single object
    in                 (    ,     )                         (    ,     ), the words (    ) and topics (    )

are non-shared variables

    for non-shared variables the map solution is symmetrical for 

all instantiations.                      1,      ,                      2,      ,     have the same 
assignment in the map solution

75

sarkhel et al.    14,    15

approximate lifting for map

    to lift approximately, add equality constraints to the linear 

program to reduce the set of feasible solutions

                                        (    ,     )                                             (    ,     )                                             (    ,     )

add constraints of the form

                                        (    1,     2)                                             (    3,     4)
                                        (    2,     3)                                             (    3,     4)

   

note that the constraints above are still 
propositional. we can define this in a more compact 
manner.

76

approximate lifting for map

    partition the domain into subsets

    let original domain of events be      1,     2,     3
    partition      =     1,     2 ,     3 =     1,     2

    ground the original mln using the partition

                                                 1,     1                                                  1,     1                                                  1,     1
                                                 1,     2                                                  2,     1                                                  1,     2
       

    implicitly adds equality constraints

                                                 1,     1                                                  1,     2
                                                 1,     2                                                  2,     1
       

77

approximate lifting for map

    how to partition the domain?

    exponential number of possible partitionings

    much smaller number of effective partitionings since some 

partitions are exchangeable

    can represent the effective partitions as a lattice

    grounding according to the partitions specified as we go 

up the lattice gives us provably more accurate map 
solutions

78

approximate lifting for map

increased complexity 
and better accuracy

{     1 ,     2 ,     3 ,     4 }

{    1 , {    2}, {    3,     4}}

    1 , {    2,     3,     4}

{{    1,     2}, {    3,     4}}

partitions at the same 
level are exchangeable

79

{    1,     2,     3,     4}

comparing optimal map value to 
approximation for citation segmentation 

900

800

700

600

500

400

300

200

100

0

e
u
l
a
 v
 
p
a
m

80

2

3

4

5

6

7

8

partition size

matrix-factorization based smoothing, approximately lifted bp, map, 

id91-based lifting,   

lifted/counting belief propagation, lifted mcmc, lifted importance 

sampling, lifted map,   

exploit logical 
structure and 
symmetries

lifted factor graphs, fove, wfomc, ptp,   

mcsat, samplesearch, giss,and-or, ac   s

importance sampling, id150, belief propagation,   

81

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

82

weight learning algorithms

    task: given the mln formulas and training data, learn the 

parameters/weights for each formula

    in mln weight learning, typically there is just a single training 

instance to learn from (a relational database)

    similar to id136, grounding the mln is problematic in 

weight learning since it creates a huge markov network

    domain specific-heuristics try reduce the size of the markov 

network
    e.g., in entity resolution (singla&domingos    06) use the canopy 

approach (macallum et al.    00) to estimate atoms that are likely to be 
false

    problem: need domain knowledge to make mlns work for 

new applications
    application designers cannot use mlns off-the-shelf

83

generative learning

    task: given the formulas and complete relational database 

(a full world     ), estimate parameters     

            ;      =                           =                                                      
    use gradient ascent to maximize the log-likelihood

    the gradient for the i-th formula is equal to

   

   
           

=                           [        ]

                  is the number of groundings of the i-th formula that is 

true in the data

                     

is the expected number of groundings of the i-th 

formula that is true in the data using the weights     

84

lifted generative learning

    lifted  generative learning uses a lifted id136 oracle to 

compute         [              ]
    note that there is no conditioning on data when computing 

the expectation

    consider an mln with a single formula      with n 

groundings      1,     2,           

                           = p     1 + p     2 +     p         
    we require to compute marginal probabilities for each 
grounding of the formula which is still a hard problem 
even when we o not condition on evidence

85

haaren et al.    15

lifted generative learning

    two key benefits

    can compute gradient from a small number of marginal

    each marginal can be computed in a domain-lifted manner

    group the groundings into equiprobable sets and 

compute the marginal over groups
                           = |    1|p         1 + |    2|p         2 +     |        |p             

    compute the id203 of each equiprobable set using a 

lifted id136 oracle (wfomc)
    the structure of the mln should allow for lifted id136

86

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

87

problems with lifted weight learning

    what if the mln structure does not allow lifted id136

    e.g.,                                              ,                                                       ,                                                       ,     

    the same approach will simply not work for discriminative 

learning which typically converges faster in real-world 
applications
    we need to condition on certain query variables

    evidence problem

    new approach: scale up weight learning using approximate 

counting oracles
    scales up a family of weight learning algorithms including voted 

id88, contrastive divergence and pseudo-likelihood learning

88

scalable weight learning using 
approximate counting

    task: compute                           [              ] efficiently

    computing               is a #p-hard problem (domingos and lowd 

   09) in the general case

    computing         [              ] is an even harder problem

    to solve the counting problem efficiently, encode each 

formula as a graphical model (venugopal et al.    15)
    key property: given a world     , counting the number of 

satisfied groundings in      (              ) is equivalent to computing 
the partition function of the encoded graphical model

89

scalable weight learning using 
approximate counting

     =                               (    ,     )         (    ,     )

       =       =       = {    ,     }

identity function

inverse function

    (    ,     )

    (    ,     )

    (    ,     )

    (    ,     )

    (    ,     )

    (    ,     )

    (    ,     )

    (    ,     )

1

0

0

1

0

1

0

1

             1
          1
          0
          0
          1

             2
          1
          0
          1
          0

90

scalable weight learning using 
approximate counting

    any exact or approximate counting oracle for the

graphical model can be used to compute              

         possible 
groundings

    1(    1,     2)         2(    2,     3)                      ,     1

   

junction trees can solve the 
counting problem in     (    3)
time/space

91

encoded constraint 
network has tree-
width 2

scalable weight learning with approximate 
counting oracles

    how about the expectation         [              ]?

    use approximate id136 methods to estimate the 

expectation

    within each step of typical approximate id136 

methods (id150, maxwalksat,, etc.),  we need 
to re-solve the counting problem multiple times
    use approximate counting oracles here as well

    the direction of the gradient is more important than obtaining 

the correct expectation

92

contrastive divergence

    approximate the expectation term in the gradient using 

gibbs samples (hinton    02)

    a few gibbs samples are sufficient to    show    the correct 

direction

    do not need to simulate the markov chain until it reaches 

convergence

    the approximate gradient 

=                            [              ], where 

         [              ] is estimated from      gibbs samples generated using 
an approximate counting oracle

   
           

93

voted id88

    approximate the expectation term in the gradient using 

the mode of the distribution (collins    02)

    if the id203 mass is mostly distributed around the mode, 

then this gives a good approximation to the gradient

    problem: in multi-modal distributions we will end up in local 

optima

    the approximate gradient 

=                            [              ], where 

         [              ] is estimated from the map assignment for the 
current      using an approximate counting oracle

   
           

94

pseudo likelihood learning

    alternate objective that does not use gradient ascent

    assumes a relaxed, tractable form of the mll optimization 

problem

    however, need to pre-compute several conditional 

probabilities

    computing each id203 requires the computation of 

              which we approximate with the approximate counting 
oracle

95

performance

system webkb protein

er

smoker

alchemy

x

tuffy

cd

vp

pll

-0.89
(0.05)

-0.64 
(0.004)

-0.91 
(0.001)

-0.72 
(0.001)

x

x

x

x

-0.779 
(0.0002)

-0.78 
(0.001)

-0.74 
(0.0004)

-0.694 
(0.001)

-0.693 
(0.001)

-0.72 
(0.001)

1.21 
(0.03) 

-0.69 
(0.04)

-0.7 
(0.004)

-1.16
(0.1)

-1.61
(0.08)

conditional log likelihood scores along with 
standard deviation for 5-fold cross validation

96

performance

cdk : ibound = k

better

97

protein interaction mln

performance

better

98

protein interaction mln

performance

better

99

collective classification mln (webkb) 

outline

    motivation
    mln basics
    domain-lifted id136
    approximate domain-lifting
    lifted generative weight learning
    scalable weight learning with approximate 

counting oracles

    applications
    conclusion

100

an overview of mln software along with 
demonstrations

    alchemy 1.0

    earliest implementation of mln id136 and learning 

algorithms

    mcsat, id150, belief propagation, maxwalksat,   
    contrastive divergence, voted id88,    

    pros

    several id136 and learning algorithms along with various 

tunable options

    cons

    lack of lifted id136 algorithms (except lifted bp)
    uses a pre-grounding approach and therefore the grounding 
process takes extremely long and/or runs out memory often

101

an overview/demo of mln software

    alchemy 2.0

    a suite of lifted id136 algorithms built on top of alchemy

    ptp, lifted gibbs, lifted importance sampling

    pros

    when the right symmetries are present, lifted id136 

algorithms are highly scalable

    cons

    only uses exact symmetries. therefore, not scalable for general 

mlns

    often fails when arbitrary evidence is presented to the mln 

since it breaks symmetries

102

an overview/demo of mln software

    tuffy/hazy

    built on top of mysql database

    mcsat, maxwalksat

    pros

    efficient grounding technique

    supports weighted evidence

    cons

    not too many algorithm options

    propositional algorithms, does not exploit symmetries or use 

lifted id136 methods

103

an overview/demo of mln software

    markov the beast

    map id136

    pros

    integrates cutting plane algorithms into mln id136

    cons

    propositional algorithm, does not exploit symmetries or use 

lifted id136 methods

104

an overview/demo of mln software

    rockit

    map id136

    pros

    integrates ilp solvers into mln id136

    parallel implementation

    cons

    propositional algorithms does not exploit symmetries or use 

lifted id136 methods

105

an overview/demo of mln software

    magician (beta version)

    approximate id136 and learning

    id150 and contrastive divergence

    pros

    does not pre-ground the mln

    uses an approximate counting oracle (iterative join graph 

propagation) to perform scalable id136 and weight learning

    cons

    not too many algorithm options

    under development

106

text classification

task: label the topic of a web-page

for almost three quarters it looked like this one was 
going to be different. newton got a head start on a 
second straight nfl mvp award, a new touchdown   

sports

the oscars were a big success. several celebrities 
gathered   

entertainment

assume that each page has exactly one label

107

text classification

    predicate definitions

                        (                ,                     !) : the ! mark signifies mutual exclusion
                                (                ,                 )
                                             ,                  : model interdependency between pages 

using

    formulas

                                     ,                                   ,     

    however, the above formula is problematic because it learns a 

single weight for all possible words and topics.

    we can specify that we need a different weight for each 

possible word-topic pair by augmenting variables with    +   

                                 +    ,                                   , +    

108

possible approximate symmetries

    words may be exchangeable. 

    happy; elated; glad

    related topics may also exhibit approximate symmetries

    technology; science

    id91 can help us exploit some of these approximate 

symmetries

109

situated incremental natural language 
understanding

    understand language in a situated domain, i.e., regarding 

visually present entities, in an incremental manner.
    applications in dialogue systems for robotics

    jointly use discourse context, language in the dialogue 

utterance and visual cues for the dialogue

    dataset: user instructs computer to pick up, drop, etc. pieces 

of a puzzle. for each dialogue utterance, the corpus records 
game state, system action and intended interpretation of the 
utterance

    task: given the current state of the puzzle board, the previous 
system action and the utterance, predict the interpretation of 
the utterance as a frame in an incremental, word-by-word 
manner

110

kennington et al.    12

task

111

incremental  interpretation word-by-
word for a 10 word utterance

model

112

rmrs to mln predicates

113

mln

contextual information with linguistic 
information specified in rmrs

connecting linguistic information specified in 
rmrs to visual scene properties

114

performance

    query predicates:                         ,                                                                       

115

performance

116

id53

    task: answer basic questions with knowledge extracted 

from 4-th grade text books and web text
    represent extracted knowledge as mln formulas

117

khot et al.    15

id53

    three different mln formulations

    first-order mln: directly encode extracted kb rules as 

mln formulas

    entity resolution mln: encode the relationship between 

constants in the kb rules using concepts of entity 
resolution

    praline mln: encode the mln to perform probabilistic 

alignment in a graphs that encode the question and the 
kb

118

first-order mln

    add if-then rules extracted from the text directly as mln 

formulas

    soft evidences:                             (            ,                                              )

119

first-order mln

    semantic rules capture meaning of predicates

    every event has a unique agent, cause-effect relations etc.

                                                  , compute     (                        )

    id136: reduce the ground network size by 

incrementally processing the mln

120

entity resolution based mln

    rules over entity/event constants

    two events/entities are considered similar when tied to 

lexically similar strings

    similar entities/events participate in similar relations w.r.t 

other entities/events

121

er based mln

any other 
predicate

122

defines soft clusters or 
equivalence classes of 
entities/events

er based mln

    partial matching rules

    for every extracted rule of the form                         , rules of the 
form     _              helps in more flexible partial matching of the 
antecedent

    pros

    er based mln does not generate a massive ground network
    much more scalable than fo-mln

    cons

    fails on questions where same string is bound to distinct 

entities. e.g.     a student puts two plants, one in a sunny room 
and the other in a dark room   . here the two plants are distinct 
entities but er mln treats it as the same.

123

probabilistic alignment and id136

    introduce a new unary predicate                     to capture 

id203 of a string constant being true given that the 
setup for the query is true and the kb rules hold

124

probabilistic alignment and id136

    evidence

                                             
                                             ,                         ,                     
                                                 
                                                 
                                                  (antecedent of kb rules)
                        (                        ) (consequent of kb rules)

    graph alignment rules

                                 ,      ,                      ,     ,      ,                      ,     ,                                       ,     

125

probabilistic alignment and id136

    id136 rules

                         ,                         (    ,     )                        (    )

126

probabilistic alignment and id136

    acyclic id136

127

performance (4th grade id53)

128

performance (comparison with a simpler 
word based approach)

    word based model

    calculate entailment score for words in kb rule and words in 

question

129

joint id136 for coreference resolution

    task: cluster together mentions that refer to the same 

entity

    three main tasks in the pipeline

    mention detection of anaphoric noun phrases

    pairwise classification of whether detected mentions refer to 

the same entity

    mention id91 resolves conflicts and generates the 

mention clusters referring to a common entity

130

mln observed predicates

131

mln observed predicates

132

local formulas

133

local formulas

134

global features

135

performance on conll-11 shard task

    online parameter learning using mira

    id136 using ilps

136

performance on conll-11 shard task

137

semantic similarity

138

belthagyet al.    13

tasks

recognizing id123 (rte)

does h entail p?

139

tasks

semantic textual similarity (sts)

score similarity between 0 and 5

140

mlns for rte and sts

    both rte and sts can be modeled as 
probabilistic entailment in markov logic
    given sentences     1                 2, rte is the 

id203 of s1         2

    given sentences     1                 2, sts is the 

average id203 of s1         2 and s2         1

141

mlns

uses boxer to derive logic 
form of sentences

     s1         2 =                              

142

word based id136 rules

    create weighted rules for all pairs of words in the two 

sentences

143

phrase based id136 rules

boy and    little boy    need to be linked in the 
id136 rule

vectorize a phrase using either vector addition or 
multiplication treating phrase words as vector 
components. the weight of the id136 rule is then 
compute similar to word based rules

144

missing phrases

         1         2 = 0 due to missing phrase. 
for rte this is fine, but for sts, we need 
to replace the deterministic and with 
something more flexible

145

average evidence combiner

divide mln formula into smaller mini-clauses and average 
the probabilities (natarajan et al.    10)

                              =      if the antecedents of all 
mini-clauses are satisfied

146

average evidence combiner

    removing variable binding improves id136 

efficiency. 

    less powerful: does not differentiate between    a man 

is walking and a woman is running    and    a man is 
running and a woman is walking   

    works well in practice for sts

147

performance

148

performance

149

fine-grained id31

    a single polarity of reviews alone is sometimes 

insufficient

       despite the pretty design i would never recommend it, 

because the sound quality is unacceptable   

    has both positive and negative polarity statements

    need fine grained id31

s1 = despite the pretty design,
s2 = i would never recommend it
s3 = because the sound quality is unacceptable,
concession(s1,s2) 
cause-explanation-evidence(s2,s3)

150

zirnet al.    11

fine-grained id31

the set of constants correspond to discourse 
segments

prior features

151

neighborhood relations

precedence relationships with previous segment

152

discourse relations

work considers two types of relations across 
segments: contrast and no-contrast

153

discourse relations

connect polarities with discourse relations

154

information extraction (bionlp)

hoil-1l interacting protein (hoip) a ubiquitin ligase
that can catalyze the assembly of linear polyubiquitin
chains is recruited to dc40 in a traf2 dependent
manner.

event

type

trigger 
word

arg-1

arg-1 
role

arg-2

arg-2 
role

e-1

binding

recruited

hoip

theme

dc40

theme

e-2

regulation dependent

e-1

theme

traf2

cause

155

venugopal et al.    14

biomedical event extraction

                                                         ,             ,                     !

                                                (            ,             ,             ,                     !)

                                     ,             
                                                     ,             

                             ,             ,                 

                                                     ,             ,             ,                     

query

hidden

evidence

                                                   ,     ,                                                                                   ,     ,     ,                    

                        (    ,     )                                                                  ,     ,     ,                     

                                                 ,     ,                                                                           ,     ,     ,                 

                                                       ,     ,     ,                                                                       ,     ,                                                              (    ,     )

                             ,                                                           ,     ,                                                                                                               ,     ,                                                               

                                             ,                                                           ,     ,                                                                                   ,     ,                                     

                (    ,     , +    )                                                 (    ,     , +    )                                     (    ,     ,     , +    )                                                     (    ,     ,     , +    )

156

157

high dimensional features

    difficult to include high-dimensional features directly into 

the mln

    trigram:                           1, +                          i, +w1                     (     +

158

reduce high-dimensional features to low-
dimensional unit clauses

id166-multiclass classifier 
for trigger labeling

reduce high-dimensional features to low-
dimensional unit clauses

triggertype(i,j,t1);   1
   triggertype(i,j,t2);   2
   triggertype(i,j,t3);   3

  3

  1

  2

reduce high-dimensional features to low-
dimensional unit clauses

argumentrole(i,j,t1);   1
   argumentrole (i,j,t2);   2
   argumentrole (i,j,t3);   3

  3

  1

  2

performance

bionlp    13

system

p

r

f1

utd-mln

48.95

59.24

53.61

evex

45.44

58.03

50.97

tees

46.17

56.32

50.74

biosem

42.47

62.83

50.68

ncbi

40.53

61.72

48.93

dlutnlp

40.81

57.00

47.56

162

performance

bionlp    11

system

p

r

f1

utd-mln

53.42

63.61

58.07

miwa-12

53.35

63.48

57.98

riedel-11

-

-

56

uturku

49.56

57.65

53.30

msr

48.64

54.71

51.50

163

performance

bionlp    09

system

p

miwa12

utd-mln

52.67

53.96

riedel-11

-

miwa-10

bjorne-09

poon-mln

riedel-mln

50.13

46.73

43.7

36.9

r

65.19

63.08

-

64.16

58.48

58.6

55.6

f1

58.27

58.16

57.4

56.28

51.95

50.0

44.4

164

conclusion

    mlns show a lot of potential for nlp applications

    compact representation for complex domain knowledge and large 

relational data

    ability to handle uncertainty

    bottleneck: lack of scalable id136 and learning algorithms

    to scale up to nlp problems, we need approximations that trade-off 

scalability with strong guarantees

    approximate lifting and learning seem to be the most promising 

directions to achieve desired scalability

    easy-to-use software and tools will make mlns more applicable to a 

wider nlp research community

165

relational 

data

tools

rich 

representations

