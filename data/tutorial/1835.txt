neural network

for sentiment analaysis

a tutorial at emnlp 2016

yue zhang and duy tin vo

singapore university of technology and design

1

outline 

v introduction 
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

2

outline 

v introduction

    definition 
    benchmarks  
    lexicons
    machine learning background
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

3

v given a set of data     :	    ) (           ) with label     ()) (    ())       ), 

definition

id31 task can be deemed as a classification task

v extract subjectivity and sentiment polarity from text data

this is an awesome movie j!!!

    =     $,    &,   ,    (
    )={    $,    &,   ,    ,}
    =     $,    &,   ,    (,           )

chelsea beats mu l !?!

4

definition

v document level sentiment

    input

       )={    $,    &,   ,    ,}

    where:

    output

    =     $,    &,   ,    (
                    ={            ,            [,            ]}

this film has everything in it from a jail break, crooked southern politicians, muses, references to
what i can only assume are historical figures, riverside baptisms, bank robberies, violence towards
animals, singing flocks of religious fanatics, kkk, lynch mobs and so on. there are obviously many
references to homer's odyssey in here as well, but i wouldn't know that because i have never read
homer's odyssey or even knew one thing about it. every other newspaper reviewer seems to know
all about it and they think that this cynicism and almost spoof-like quality towards it makes the film
that much better. well coming from a guy who doesn't know anything about it, i can tell you that it
is still an entertaining film. there were times when again, as is usual for a coen film, i wasn't sure
why i was entertained or laughing, but i was.

this is a road picture where three men travel along the way to find a hidden treasure that clooney
says he has hidden to his two other cell mates. he has to take them along because they were also
chained to him when they had their chance to escape.

i like all the principal actors in the film and many of them are coen cronies. it was nice to see
goodman again. it was nice to see hunter and especially turturro who seems to have a place in
every coen film. it's too bad they didn't find a place for steve buscemi but that is a different story
all together. but back to clooney. the man just has charisma. he is a one hell of an actor as well
and here he is not quite as zany as the others but even he has his own idiosyncrasies. his work
here is quite awesome and i really hope this shows that he is capable of playing any range of
character.

now after heaping all this praise on the film, let me just say this as well. i didn't really enjoy the
film at first. i found it to be quite tediousand a little boring. there were too many ideas in here and
not enough care went into harnessing them for all what they were worth. but then the film began to
grow on me. it took a while but it did grow on me. i don't think this is their best film, but it is still a
good one and i am giving it a 8.5. but the reason that i do recommend this film is for one reason
only.

every day you can go look into the paper and look at the films that are playing and say to yourself,
seen it, seen it, oh, seen it last year, that is the same as this film and that is the same as that film.
most films have been recycled in some form or another. not the coen's films. they have not been
recycled and if they have i don't know about it. that is reason enough to see something that they
put out. originality counts for a lot in my books. the coens are original and they are good. and that
is not common in todays cinema. enjoy them while they are allowed to make films. because you
don't get vision like this in many films, so when you do, enjoy it!

andrew l. maas, raymond e. daly, peter t. pham, dan huang, andrew y. ng, and christopher potts. 2011.  learning word vectors for 
sentiment  analysis.  in proceedings of acl:hlt, 142-150.

5

definition

v sentence level sentiment

       )={    $,    &,   ,    ,}

    =     $,    &,   ,    (
                    ={            ,            [,            ]}

    input

    where:

    output

i like  all the principal  actors in the film 
and many of them are coen cronies. 

bo pang and lillian lee.  2005.  seeing stars: exploiting  class relationships  for sentiment  categorization  with respect to rating scales. 
in proceedings of acl, 115-124. 

6

definition

v fine-grained sentiment 

    sentiment on target
    opinion expression
    opinion holder
    opinion strength
    etc.

it was nice to see goodman again.

i really  love leicester  city!! fantastic!!!

bishan yang and claire cardie. 2012.  extracting opinion expressions  with semi-markov  id49. in proceedings of  
emnlp:conll, 1335-1345.

7

outline 

v introduction

    definition
    benchmarks 
    lexicons
    machine learning background
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

8

v movie reviews 

    pang and lee (2004)

    subjectivity vs objectivity sentences
    positive vs negative document

sentence-level

subjective

objective

5000

5000

total
10000

document-level

positive
1000

negative

1000

total
2000

benchmarks
subjective: 
works both as an engaging drama and an incisive look at the 
difficulties facing native americans . 

positive:
kolya is one of the richest films i've seen in some time . zdenek
sverak plays a confirmed old bachelor ( who's likely to remain
so ) , who finds his life as a czech cellist increasingly impacted
by the five-year old boy that he's taking care of .
though it ends rather abruptly-- and i'm whining , 'cause i
wanted to spend more time with these characters-- the acting ,
writing , and production values are as high as , if not higher
than , comparable american dramas .
this father-and-son delight-- sverak also wrote the script , while
his son , jan , directed-- won a golden globe for best foreign
language film and , a couple days after i saw it , walked away
an oscar .in czech and russian , with english subtitles .

bo pang and lillian lee.  2004.  a sentimental  education: sentiment  analysis  using subjectivity summarization  based on minimum  cuts. 
in proceedings of acl.

9

benchmarks

v movie reviews 

    pang and lee (2005)
    sentence-level

sentence-level

positive
5331

negative

5331

total
10662

positive: 
an idealistic love story that brings out the latent 15-year-old 
romantic in everyone . 

bo pang and lillian lee.  2005.  seeing stars: exploiting class  relationships  for sentiment  categorization  with  respect to rating scales. 
in proceedings of acl, 115-124.

10

v movie reviews

    mass et al. (2011)
    document-level 

benchmarks
positive:

this film has everything in it from a jail break, crooked southern politicians, muses, references to
what i can only assume are historical figures, riverside baptisms, bank robberies, violence towards
animals, singing flocks of religious fanatics, kkk, lynch mobs and so on. there are obviously many
references to homer's odyssey in here as well, but i wouldn't know that because i have never read
homer's odyssey or even knew one thing about it. every other newspaper reviewer seems to know
all about it and they think that this cynicism and almost spoof-like quality towards it makes the film
that much better. well coming from a guy who doesn't know anything about it, i can tell you that it
is still an entertaining film. there were times when again, as is usual for a coen film, i wasn't sure
why i was entertained or laughing, but i was.

pos
12500
12500

neg
12500
12500

total
25000
25000
50000

train
test
unsup

this is a road picture where three men travel along the way to find a hidden treasure that clooney
says he has hidden to his two other cell mates. he has to take them along because they were also
chained to him when they had their chance to escape.

i like all the principal actors in the film and many of them are coen cronies. it was nice to see
goodman again. it was nice to see hunter and especially turturro who seems to have a place in
every coen film. it's too bad they didn't find a place for steve buscemi but that is a different story
all together. but back to clooney. the man just has charisma. he is a one hell of an actor as well
and here he is not quite as zany as the others but even he has his own idiosyncrasies. his work
here is quite awesome and i really hope this shows that he is capable of playing any range of
character.

now after heaping all this praise on the film, let me just say this as well. i didn't really enjoy the
film at first. i found it to be quite tediousand a little boring. there were too many ideas in here and
not enough care went into harnessing them for all what they were worth. but then the film began to
grow on me. it took a while but it did grow on me. i don't think this is their best film, but it is still a
good one and i am giving it a 8.5. but the reason that i do recommend this film is for one reason
only.

every day you can go look into the paper and look at the films that are playing and say to yourself,
seen it, seen it, oh, seen it last year, that is the same as this film and that is the same as that film.
most films have been recycled in some form or another. not the coen's films. they have not been
recycled and if they have i don't know about it. that is reason enough to see something that they
put out. originality counts for a lot in my books. the coens are original and they are good. and that
is not common in todays cinema. enjoy them while they are allowed to make films. because you
don't get vision like this in many films, so when you do, enjoy it!

andrew l. maas, raymond e. daly, peter t. pham, dan huang, andrew y. ng, and christopher potts. 2011.  learning word vectors for 
sentiment  analysis.  in proceedings of acl:hlt, 142-150.

11

benchmarks

v movie reviews

    socher et al. (2013), which is induced from pang and lee (2005)
    phrase-level

binary

fine-grained

train
6920
8544

valid
872
1101

test
1821
2210

richard socher, alex perelygin, jean y. wu, jason chuang, christopher d. manning, andrew y. ng, and christopher potts. 2013. recursive
deep models for semantic compositionality over a sentiment treebank. in proceedingsofemnlp, 1631-1642.
bo pang and lillian lee. 2005. seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.
in proceedingsofacl, 115-124.
12

benchmarks

v product reviews

    hu and liu (2004): 5 products 
    ding et al (2008): 9 products, 
which is induced from hu and 
liu (2004) 
    fine-grained

[t]
feature[+2]##just received this camera two days ago and 
already love the features it has . 
photo[+2]##takes excellent photos . 
night mode[+2]##night mode is clear as day . 
use[+1][u]##i have not played with all the features yet , 
but the camera is easy to use once you get used to it . 
viewfinder[-1]##the only drawback is the viewfinder is 
slightly blocked by the lens . 
##however , using the lcd seems to eliminate this minor 
problem . 
camera[+3]##overall it is the best camera on the market 
. 
##i give it 10 stars !

minqing hu and bing liu. 2004.  mining and summarizing  customer  reviews.  in proceedings of acm sigkdd kdd, 168-177.
xiaowen ding, bing liu, and philip s. yu. 2008.  a holistic  lexicon-based approach to  opinion mining. in proceedings of wsdm, 231-240.
minqing hu and bing liu. 2004.  mining and summarizing  customer  reviews.  in proceedings of acm sigkdd kdd, 168-177.

13

benchmarks

v twitter

    go et. al. (2009)
    sentence-level

pos
800k
182

neg
800k
177

total
1.6m
359

train
test

positive: how can you not love obama? he makes jokes about  himself.
negative: naive bayes using em for text classification.  really  frustrating...

alec go, richa bhayani, and lei huang. 2009.  twitter  sentiment  classification  using distant supervision. cs224n project report, stanford, 12.

14

benchmarks

v twitter

    mitchell et. al. (2013)
    open domain

domain
english 
spanish

pos
707
1,555

neg
275
1,007

neu
2,306 
4,096

#sent  #entities 
2,350
5,145

3,288
6,658

margaret mitchell,  jacqui aguilar, theresa wilson,  and benjamin van durme. 2013.  open domain  targeted sentiment.  in proceedings of 
emnlp, 1643   1654.

15

benchmarks

v twitter

    dong et. al. (2014)
    targeted 

pos
1561
173

neg
1560
173

neu
3127
346

total
6248
692

train
test

neutral: 
i hate that i haven't  had time for #zbrush in the past two days    we need #zspheres on the 
[iphone] so i can still sculpt on the go.

li dong, furu wei, chuanqi tan, duyu tang, ming zhou and ke xu. 2014.  adaptive id56 for target-dependent twitter 
sentiment  classification.  in proceedings of acl, 49-51.

16

benchmarks

v twitter

    semeval13 (nakov et. al., 2013)
    sentence-level

pos
3662
575
1573

neg
1466
340
601

neu
4600
739
1640

total
9729
1654
3814

train
valid
test

positive:  omg saturday  at 8, p.s. i love you premieres on abc family.

preslav nakov, sara rosenthal,  zornitsa kozareva,  veselin stoyanov,  alan ritter,  and theresa wilson.  2013.  semeval-2013  task 2: sentiment 
analysis  in twitter. in proceddings of semeval, 312   320.

17

outline 

v introduction
    introduction
    benchmarks  
    lexicons
    machine learning background
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

18

lexicons

v manual methods

    mpqa lexicon (wilson et. al., 2005) contains 8222 words

source: http://sentiment.christopherpotts.net/lexicons.html#mpqa

theresa wilson,  janyce wiebe,  and paul hoffmann.  2005.  recognizing contextual  polarity in phrase-level  sentiment  analysis.  in proceedings 
of hlt:emnlp, 347-354.
19

lexicons

v manual methods

    hu and liu (2004) lexicon contains 2006 positive words and 4783 

negative words.

positive

a+

abound
abounds
abundance
abundant
access
able

accessible
acclaim
acclaimed

negative
2-faced
2-faces
abnormal
abolish

abominable
abominably
abominate
abomination

abort
aborted 

minqing hu and bing liu. 2004.  mining and summarizing  customer  reviews.  in proceedings of acm sigkdd kdd, 168-177.

20

lexicons

v manual methods

    mohammad and turney (2013) lexicon contains 14182 words with 10 

labels (8 emoticons and 2 sentiments)

hate
hate
hate
hate
hate
hate
hate
hate
hate
hate

anger
1
anticipation
disgust 1
fear
1
joy
0
negative 1
positive 0
sadness 1
surprise 0
0
trust

0

fear
joy

hateful anger
1
hateful anticipation
hateful disgust 1
hateful
1
hateful
0
hateful negative 1
hateful positive 0
sadness 1
hateful
hateful
surprise 0
0
trust
hateful

0

saif m. mohammad and peter d. turney. 2010.  emotions  evoked by common  words and phrases: using mechanical  turk to create an emotion 
lexicon. in proceedings of naacl:hlt 2010 workshop on caaget, 26-34.
21

lexicons

v automatic methods

    sentiid138 (esuli and fabrizio, 2006) learns positive and negative 

sentiment scores for synsets in id138

source: http://sentiment.christopherpotts.net/lexicons.html#sentiid138

andrea esuli, and fabrizio sebastiani.  2010.  sentiid138: a publicly available  lexical  resource for opinion mining. in proceedings of 
lrec, 417-422.

22

lexicons

v automatic methods

    tang et. al. (2014) consists of 178,781 positive words/phrases and  

168,845 negative words/phrases

follow me ... but
#society
i can't view
producer's
now , i'm
#although
twitter like
a wizard

-0.592651
-0.592650
-0.592650
-0.592646
-0.592637
-0.592631
-0.592629
-0.592627

duyu tang, furu wei, bing qin, ming zhou, and ting liu. 2014.  building large-scale twitter-specific sentiment  lexicon: a representation 
learning approach. in proceedings of coling, 172-182.

23

outline 

v introduction
    introduction
    benchmarks  
    lexicons
    machine learning background

v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

24

machine learning background

v general model:

    train

feature 
extractor

features

machine
learning 
algorithms

   one-hot vector
   id165s
   brown id91
   lexicons
   patterns
   pos 
      

    predict

manually extract features

input

feature 
extractor

features

classifier
models

output

25

machine learning background

v neural network: a sub-area of machine learning 

    train

feature 
extractor

features

machine
learning 
algorithms

    predict

embedding

features

classification

input

feature 
extractor

features

classifier
models

output

26

outline 

v introduction and background
v neural network background

    overview
    typical feature layers
    training

v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

27

overview

v general model:

output

output layers

scorer

feature layers

combination

embedding layers

vectorization

input

28

overview

v embedding layer

    word to vector
    look up table

       )=    |j|         )
          )        p: id27
       |j|        |j|  p: embedding matrix
          )        |j|: one-hot vector of word     )
       : embedding dimension  

    where:

   	
       $				       &		   			       (q$       (
    =	    $			    &		   			    (q$    (

look-up table

29

overview

v feature layer

    automatically learn the representation of inputs
    matrix-vector multiplication
    element-wise composition
    non-linear transformation

    	

matrix-vector multiplications 

nonlinear id180

+ 

   	
       $				       &		   			       (q$       (

30

v output layer

overview

   margin	output:	    ]^_   a=    b       +    b
   id203	output    ^())=        =        ()),    	
											=                            ^    ]^_   a
	=     pqr   stq	
        pqvr   stqv
    predicted label:     x())=                        (    ()))
^w
    where:       :	set	of	parameters
       b,    b:	weight	and	bias	parameters	of	output	layer

    (+)    (   )

31

outline 

v introduction
v neural network background

    overview
    typical feature layers
    training

v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

32

v feed forward (mlp)

typical feature layers

   )=    (    (cid:132)       )+    (cid:132))
      ): hidden features
       (    ): activation function
       (cid:132),    (cid:132):	weight	and	bias	parameters	of	mlp
          ): input vector

    where:

        

            

   

source: https://www.mql5.com/pt/code/9002

33

typical feature layers

vacitivation	functions	    (    )
                                     = $$sa(cid:139)(cid:140)
                        =a(cid:140)qa(cid:139)(cid:140)
a(cid:140)sa(cid:139)(cid:140)
,    =1,   ,    
                                (cid:141)     = a(cid:140)(cid:142)	
                         =max0,    
   
a(cid:140)(cid:143)
(cid:144)(cid:143)(cid:145)(cid:146)
                         =    

34

    where:

v convolutional neural network (id98)

typical  feature layers

       )(cid:153)=    (    ^(       )          )s$                )s(cid:153))+    ^)
          )(cid:153): convolutional features
       (    ): activation function
       ^,     ^:	weight	and	bias	parameters	of	id98
          ): input vectors
      : concatenation

    k: window size (2,3 in common)

       )       )s$
       )s(cid:153)
    

    (cid:132)

source: http://parse.ele.tue.nl/education/cluster2

       )(cid:153)

35

v pooling

typical feature layers

   )=                (    ))
      ): hidden features
                    is element-wise operations (max, average, min,...)
       ): input matrix

    where: 

36

v recurrent neural network (id56)

typical feature layers

   )=    (    (cid:159)   )q$+    (cid:132)       )+    (cid:132))
      ): hidden features at time     
       (    ): activation function
       (cid:159),    (cid:132),    (cid:132):	weight	and	bias	parameters	of	id56
          ): input vector

    where:

source: http://colah.github.io/posts/2015-08-understanding-lstms/

37

typical feature layers

v long short term memory (lstm)

         =    (    r         +    r     q$+    r)
	      =    (    )         +    )     q$+    ))
      =                              +           q$+      
         =	               +                     q$
         =    (    _         +    _     q$+    _)
     =         tanh            
            ,	      ,      ,         ,         : forget, input, update, control, 
          ,       ,       : weight	and	bias	parameters	of	lstm

output gate layers, respectively

    where:

source: http://colah.github.io/posts/2015-08-understanding-lstms/

38

v id56 (reid98)

typical feature layers

   )=    (          )q$    +          )q$    +    (cid:159))
      ): hidden features at time     
       (    ): activation function
           ,        ,    (cid:159):	weight	and	bias	parameters	of	reid98

    where:

richard socher, alex perelygin, jean y. wu, jason chuang, christopher d. manning, andrew y. ng, and christopher potts. 2013. recursive
deep models for semantic compositionality over a sentiment treebank. in proceedingsofemnlp, 1631-1642.
39

outline 

v introduction
v neural network background

    overview
    typical feature layers
    training

v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

40

training

v supervised learning
v randomly initialized model
v compare model output with manual reference

41

v id168s

    cross id178 loss (maximum likelihood)

training
   1           (cid:176)(   )log	(    ()))
   (    )=   1           )log    ) =
   
)   $
)
       : set of parameters
       : number of samples
       (cid:176)(   ): one-hot  vector corresponding  to label     ())
       ()):	id203	output	of	sample	    ())

    where:

42

training

v id168s

    multiclass classification

    hinge loss (maximum-margin)

    binary classification:

   
)   $

   (    )=   1       max	(0,1       ())    ]^_   a())) 
   (    )=   1       max	(0,1+max^v  ^    ]^_   a^v       ]^_   a^)) 
   
)   $
       : set of parameters
       : number of samples
       ())    {   1,1}
       ]^_   a: margin  output

    where:

43

training

v id168s

    0/1 loss (large margin)   (    )=   1           (cid:176)     (cid:176)(cid:181)   
   
)   $
       : set of parameters
       : number of samples
       : indication  function
       : ground-true  labeled  vector
       (cid:181): predicted  vector

    where:

44

training

v id168s

    mse loss (regression)   (    )=   1       (    )       (cid:181)))&
       : set of parameters
       : number of samples
        is a ground-true  labeled  vector
       (cid:181) is a predicted  vector

   
)   $

    where:

45

training

v back propagation

    goal

    derivation

    adjust parameters accordingly

    find            for all parameters
    chain rule: if z=         and y=    (    ), then
                =                                
                =                   )        )        
(
)   $

    layer-wise calculation

richard socher, yoshua bengio, and christopher d. manning. 2012.  deep learning for nlp (without magic). in tutorial abstracts of acl.

46

training

v batch id119 is an algorithm in which we repeatedly 

make small steps downward on an error surface defined by a loss 
function of a set of parameters over the full training set (n 
samples)

    (cid:153)s$=    (cid:153)              (    )        

    where

       : set of parameters
       : learning rate

   problem: n is a very large number

47

training

    (cid:153)s$=    (cid:153)              (    ,    ),    ()))
        
    (cid:153)s$=    (cid:153)                  ,    ):)s(,    ):)s(
        

v sgd: stochastic id119 works according to the same 
principles as batch id119, but proceeds more quickly 
by estimating the gradient from just one example at a time 
instead of the entire training set

v mini-batch sgd (msgd) works identically to sgd, except that we 
use more than one training example to make each estimate of the 
gradient

   problem: manually adjust learning rate

48

adding a fraction     	 of the update vector of the past time step to 

v momentum: helps to accelerate sgd in the relevant direction by 

training

the current update vector

    (cid:153)=        (cid:153)q$              (    ,    ),    ()))
        
    (cid:153)s$=    (cid:153)       (cid:153)

ning qian. 1999.  on the momentum  term in id119 learning algorithms.  in proceedings of neural networks,  145-151.

49

training

v adagrad: adapts the learning rate to the parameters, performing 

larger updates for infrequent and smaller updates for frequent 
parameters

    where:

    (cid:153)s$=    (cid:153)       (cid:153)    (cid:153)
       (cid:153): the gradient of     w.r.t      at     
       (cid:153)=
       : a smoothing term that avoids division by zero

      
(cid:143)  (cid:145)(cid:146)

(cid:190)  (cid:192)s`

   problem: learning rate need to be initialized and gradually shrunk 

to an infinitesimally small number

john duchi, elad hazan, and yoram singer. 2011.  adaptive subgradient methods  for online learning and stochastic  optimization. in 
proceeding of the journal of machine learning research12, 2121-2159.

50

v rmsprop*: adjusts the adagrad method in a very simple way in 
an attempt to reduce its aggressive, monotonically decreasing 
learning rate. in particular, it uses a moving average of squared 
gradients instead

training

    (cid:153)s$=    (cid:153)+       (cid:153),
                    (cid:153)    (cid:153)
       (cid:153)=   

    where:

               : root mean square
               [    ](cid:153)=     [    &](cid:153)+     ,     [    &](cid:153)=        [    &](cid:153)q$+(1       )    (cid:153)&

*currently unpublished adaptive learning rate method.  however, it is usually to cite slide 29  of lecture 6 of geoff hinton   s coursera class.

51

training

v adadelta: is an extension of adagrad to handle the problem of 

continual decay of learning rates. instead of accumulating all past 
squared gradients, it restricts the window of accumulated past 

gradients to some fixed size w    (cid:153)s$=    (cid:153)+       (cid:153),
       (cid:153)=               [       ](cid:153)q$
            [    ](cid:153)     (cid:153)
               : root mean square
               [       ](cid:153)q$=     [       &](cid:153)q$+     ,     [       &](cid:153)q$=        [       &](cid:153)q&+(1       )       (cid:153)q$&
               [    ](cid:153)=     [    &](cid:153)+     ,     [    &](cid:153)=        [    &](cid:153)q$+(1       )    (cid:153)&

    where:

matthew  d. zeiler. 2012.  adadelta: an adaptive learning rate method. arxivpreprint arxiv:1212.5701.

52

training

v adaptive moment estimation (adam): is another method that 

computes adaptive learning rates for each parameter. it is similar 
to rmsprop with momentum. the simplified adam update looks 
as follows

    (cid:153)=    $    (cid:153)q$    1       $    (cid:153)
    (cid:153)=    &    (cid:153)q$   (1       &)    (cid:153)&
    (cid:153)s$=    (cid:153)            (cid:153)    (cid:153)+    

diederik kingma, and jimmy  ba. 2014.  adam: a method for stochastic  optimization. arxivpreprint arxiv:1412.6980.

53

training

v id173

    where:

    l2:          =        +        
       : decay rate
    dropout:        w=              
       :  a masking vector  of bernoulli  random  variables  with id203  p of being 1
    rescaling parameters     	when l2 exceeds a threshold

    batch id172

    where:

v experimental tricks 

    oov: randomly initialization 
    fine-tune: slightly improve the performance

54

outline 

v introduction
v neural network background
v sentiment-oriented id27

    overview
    traditional id27
    sentimental-oriented id27

v sentence-level models
v document-level models
v fine-grained models
v conclusion

55

overview

v traditional embedding is 

syntactically and semantically 
similar, but cannot distinguish 
sentimental differences. 

v how to integrate sentiment 

information into word 
embedding
    use nn language model to learn 

syntactic and semantic information

    apply labeled data to augment 
sentiment orientation into word 
embedding

56

outline 

v introduction
v neural network background
v sentiment-oriented id27

    overview
    traditional id27
    sentimental-oriented id27

v sentence-level models
v document-level models
v fine-grained models
v conclusion

57

traditional id27

v unsupervised learning

    basic neural network language models:

    input: 

    id165s

    output: 

    objective function

    id203  score of the word given  previous  words

                        q                            q    s    
    q    

  problem: id203 score
  high computation

yoshua bengio, r  jean ducharme, pascal vincent, and christian  janvin. 2003.  a neural probabilistic language model. j. mach. learn. res.3, 
1137-1155.

58

traditional id27

v unsupervised learning

    pairwise-ranking neural network language models

    input: a pair of 

    id165s: 

t=    )q(cid:153)    )q(cid:153)s$                  )s(cid:153)q$    )s(cid:153)
       =    )q(cid:153)    )q(cid:153)s$                      )s(cid:153)q$    )s(cid:153)
                        (    ,        )=            	(    ,    +                    (    ))

    corrupted  id165s: 

    margin  scores         ,    (       )

    output: 

    objective function

  problem: deep structure
  still high computation

ronan collobert,  jason weston,  l  on bottou,  michael karlen, koray kavukcuoglu, and pavel kuksa. 2011.  natural language processing 
(almost)  from scratch. j. mach. learn. res. 12, 2493-2537.

59

traditional id27

v unsupervised learning

    simple neural network language models

    id203  score of the context words given  a word or vice versa

    input: 

    id165s

    output: 

    objective function

                                        s            
    
           
q    (cid:221)    (cid:221)    ,          

    optimization

    hierarchical softmax
    negative sampling

tomas  mikolov,  ilya sutskever, kai chen, greg s. corrado, and jeff dean. 2013.  distributed representations  of words and phrases and their 
compositionality.  in proceedings of nips, 3111-3119.

60

outline 

v introduction
v neural network background
v sentiment-oriented id27

    overview
    traditional id27
    sentimental-oriented  id27

v sentence-level models
v document-level models
v fine-grained models
v conclusion

61

sentimental-oriented id27

v semi-supervised learning

    where:

        
           

    objective function

    maas et al. (2011) combine an unsupervised probabilistic model and a supervised 

sentiment component to learn id27

+                
+                   (        |    (cid:228)    ;    ,    )
                 +           (cid:228)             
                               (        |        ;    ,    ,        )
    
    
        
           
           
           )    ;    ,     =                                      p   +        maximum a posteriori  (map)
           =1    );    ,    ,b(cid:244) =              p   +    
             (cid:246)  j: word  embedding  matrix with size of     
       p    is embedding  of     )
       ,    ,    ,    ^: weight  parameters and bias
       ,    : hyper-parameters

andrew l. maas, raymond e. daly, peter t. pham, dan huang, andrew y. ng, and christopher potts. 2011.  learning word vectors for sentiment 
analysis.  in proceedings of acl:hlt, 142-150.
62

sentimental-oriented id27

v supervised learning

    labutov and lipson (2013) employ pre-trained embedding and labeled 

data to learn re-embedding words.

    objective function                       (        |        ;        )

                        
                   
               
       ,  (cid:254):	embedding	matrices	of	source	and	target	words
           (cid:141)=1    );     =              p   +    
       =    -  (cid:254)
       : hyper-parameter

    where:

igor labutov, and hod lipson. 2013.  re-embedding words.  in proceedings of acl:hlt, 489-493

63

sentimental-oriented id27s

v sswe model (tang et al.,2014)

    extend collobert and weston (2011) model
    adding sentimental information 
    objective function

    motivation:     (cid:190)__p       t!p
                                    ,         =															    	                              ,        
																															+		(           )                      (    ,        )
                   ^p(    ,       )=max	(0,1+    $               $    )
                   ]    ,        =max0,1+    ]        $(               ]        $    )
       ]     =&			1				        				    (cid:190)     =[1,0]
   1				        				    (cid:190)     =[0,1]

    where

duyu tang, furu wei, nan yang, ming zhou, ting liu, and bing qin. 2014.  learning sentiment-specific id27 for twitter 
sentiment  classification.  in proceedings of acl, 1555-1565.

64

sentimental-oriented id27s

v tswe model (ren et al., 2016)

    motivation

    different topics: offensive message vs offensive player
    multi-prototype embedding

    an extension of tang et al. (2014) 
    augmenting topical information
    objective function

                                    ,         =																							                                  ,        
																														+																					                              ,        
																														+(                  )                      (    ,        )
    where                    ^p(    ,       )=max	(0,1+    $               $    )
                          	 =   r()(  )   *+	(]_r  ,!(cid:132)(r(cid:146)   ,  ))
                    ]    	 =   r-)(  ).*+	(]_r  ,!(cid:132)(r,/(cid:146)   (,/0)   ))

   

1

yafeng ren, yue zhang, meishan zhang, and donghong ji. 2016.  improving twitter  sentiment classification  using topic-enriched multi-
prototype id27s. in proceedings of aaai.

65

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models

    overview
    bag-of-word methods
    id98
    reid98
    id56

v document-level models
v fine-grained models
v conclusion

66

overview
v input: a sentence consists of n words
v output: polarity or fine-grained sentiment
   classification problem

v classification layer       =                            (    b       +    b)

       

classification

   	

neural feature extraction

       $				       &		   			       (q$       (
    =	    $			    &		   			    (q$    (

vectorization

    	

67

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models

    overview
    bag-of-words id98
    reid98
    id56

v document-level models
v fine-grained models
v conclusion

68

bag-of-words
v bag-of-words (kalchbrenner et al., 2014)

    simply element-wise summing embedding
    learning embeddings by back-propagation

    	

sum

   	

       $				       &		   			       (q$       (

nal kalchbrenner, edward grefenstette,  and phil blunsom.  2014.  a convolutional  neural network  for modelling  sentences. in proceedings of 
acl, 655-665.

69

bag-of-words

    	

v pooling (tang et. al.,2014; 

vo and zhang, 2015)
    make use of pre-trained id27s
    extract salient features for traditional classifiers

avg

   	

max

min

       $				       &													   													       (q$       (

duyu tang, furu wei, nan yang, ming zhou, ting liu, and bing qin. 2014. learning sentiment-specific id27 for twitter
sentiment classification. in proceedings of acl, 1555-1565.
duy-tin vo and yue zhang. 2015. target-dependent twitter sentiment classification with rich automatic features. in proceedingsofijcai,
1347-1353.
70

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models

    overview
    bag-of-word methods
    id98
    reid98
    id56

v document-level models
v fine-grained models
v conclusion

71

convolutional neural network

v id98 (kim, 2014)

    feature combinations 
    single id98 layer
    varied-window-size convolutional filters
    multichannel (1 static+ 1 nonstatic)

    $&

max

id98

    (q$&

    $2

max

    &2

    (q&2

   

   

    	
    &&

yoon kim. 2014.  convolutional  neural networks  for sentence classification.  in proceedings of emnlp, 1746-1751.

72

       $			       &				       2				       3	   	       (q&       (q$       (

convolutional neural network

v variations

    dos santos et al. (2014)
    add character information

id98

id98

id98

id98

    =	    $									    &						   						    (
       $       &          ,
       $       &          ,

       $       &          ,

c  cero nogueira dos santos, and maira gatti. 2014.  deep convolutional  neural networks  for sentiment  analysis  of short texts.  in proceedings 
of coling, 69-78.
73

convolutional neural network

v variations

    kalchbrenner et al. (2014)

    fixed-window-size convolutional filters
    multiple feature maps
    k-max, with k dynamically decided
    stack multiple convolutional layers

nal kalchbrenner, edward grefenstette,  and phil blunsom.  2014.  a convolutional  neural network  for modelling  sentences. in proceedings of 
acl, 655-665.

74

convolutional neural network

v variations

    yin and sch  tze (2015)

    inspired by id98 for rgb kernels in images
    employ different kinds of pre-trained embeddings as multichannel
    varied-window-size convolutional filters
    k-max, with k dynamically decided

   feature	map	    ),   (cid:141):    ),   (cid:141)=       ),   (cid:141),(cid:153)       )q$(cid:153)

(
(cid:153)   $

    where:

        : the convolution  operation
    j: the index  of a feature map in layer  i. 
    v: a rank  4 tensor weights

wenpeng yin, and hinrich sch  tze. 2015.  multichannel variable-size  convolution for sentence classification.  in proceedings of acl, 204   214. 75

convolutional neural network

v variations

    zhang et al. (2016)

    make use of different sources of pre-trained 

embedding with different sizes

    employ different sets of convolutional filters

       )(cid:141)(cid:153)=    (    ^(cid:141)	(       )(cid:141)          )s$(cid:141)                 )s(cid:153)(cid:141)
       )(cid:141)=                (    )(cid:141))

)+    ^(cid:141))

ye zhang, stephen roller, and byron wallace.  2016.  mgnc-id98: a simple  approach to exploiting  multiple  id27s for sentence 
classification.  in proceedings of naacl-hlt, 1522   1527 .

76

convolutional neural network

v variations

    lei et al. (2015)
    id165 tensor
    tensor-based feature mapping
    non-local
    non-linear

    =      (        $           &           2)
    [    ,    ,    ]=      (        )           (cid:141)           (cid:153))

tao lei, regina barzilay,  and tommi jaakkola.  2015.  molding id98s for text: non-linear, non-consecutive  convolutions.  in proceedings of 
emnlp, 1565   1575 .

77

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models

    overview
    bag-of-word methods
    id98
    reid98
    id56

v document-level models
v fine-grained models
v conclusion

78

id56

v reid98 (socher et al., 2013)

    $=    (            )
    &=    (             $)

richard socher, alex perelygin, jean y. wu, jason  chuang, christopher d. manning, andrew y. ng, and christopher potts. 2013.  recursive  deep 
models  for semantic  compositionality  over a sentiment  treebank. in proceedings of emnlp,1642. 2013.
79

id56

v variations

    adaptive multi-compositionality reid98 (dong et al., 2014)

    employ a set of composition functions

;
    )=    (       (    (cid:159)|       ),       ))    (cid:159)(       ),       ))
)
    (    $|       ),       ))
(cid:159)   $
    (    ;|       ),       )) =                                   (            )       ) )
   

li dong, furu wei, ming zhou, and ke xu. 2014.  adaptive multi-compositionality  for recursive neural models with applications  to sentiment 
analysis.  in proceedings of aaai, 1537-1543.

80

id56

v variations

    matrix-vector reid98 (socher et al., 2012)

    both matrix and vector
    more composition interaction (cross-way composition)
    more features

richard socher, brody huval, christopher d. manning, and andrew y. ng. 2012.  semantic compositionality  through recursive matrix-vector 
spaces. in proceedings of emnlp, 1201-1211.

81

id56

v variations

    recursive neural tensor network (socher et al.,2013)

    also more composition
    less parameters (embeddings)

    $=    (               $:p          +            )
    &=    (        $       $:p         $ +             $)

   problem:

- extracts non-local  features
- relies on external  syntactic  parsers for tree structure. 

richard socher, alex perelygin, jean y. wu, jason  chuang, christopher d. manning, andrew y. ng, and christopher potts. 2013.  recursive  deep 
models  for semantic  compositionality  over a sentiment  treebank. in proceedings of emnlp,1642. 2013.

82

id56

v variations

    deep reid98 (irsoy and cardie 2014)

    stack multiple reid98 layers

      ())=    (    <)         ) +    =)         ) +    )      )q$ +    ))
        <),    =),    ),    ()): weight and bias parameters 
            ,r     : left and right children of     

    i: stacked layer index 

    where: 

ozan irsoy, and claire cardie. 2014.  deep id56s  for compositionality  in language. in proceedings of nips, 2096-2104.

83

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models

    overview
    bag-of-word methods
    id98
    reid98
    id56

v document-level models
v fine-grained models
v conclusion

84

recurrent neural network

v lstm (wang et al., 2015)

    use a standard lstm
    fine-tune id27s

         =    (    r         +    r     q$+    r)
	      =    (    )         +    )     q$+    ))
      =                              +           q$+      
         =	               +                     q$
         =    (    _         +    _     q$+    _)
     =         tanh            

source: http://deeplearning.net/tutorial/lstm.html

xin wang, yuanchao liu, chengjie sun, baoxun wang, and xiaolong wang. "predicting polarities  of tweets  by composing  id27s
with long short-term  memory.  2015.  in proceedings of acl, 1343-1353.

85

recurrent neural network

v variations

    bi-directional lstm: tai et al. (2015), li et al. (2015), teng et al. (2016)

kai sheng tai, richard socher, and christopher d. manning. 2015.  improved semantic representations from tree-structured long short-term
memory networks.  in proceedings of acl, 1556   1566.
jiwei li, minh-thang luong, dan jurafsky, and eudard hovy. 2015.  when are tree structures necessary  for deep learning of 
representations?.  in proceedings of emnlp, 2304   2314.
zhiyang teng, duy tin vo and yue zhang.context-sensitive lexicon features for neural id31.  in proceeddings of emnlp 
2016.  austin, texas,  usa, november.

86

recurrent neural network

v variations

    tree structured lstm: tai et al. (2015); 

li et al. (2015); zhu et al. (2015)
    child-sum tree    dependency tree
    n-ary tree    constituency tree

kai sheng tai, richard socher, and christopher d. manning. 2015. improved semantic representations from tree-structured long short-
term memory networks. in proceedings of acl, 1556   1566 .
jiwei li, minh-thang luong, dan jurafsky, and eudard hovy. 2015. when are tree structures necessary for deep learning of
representations?. in proceedings of emnlp, 2304   2314.
xiaodan zhu, parinaz sobhani, and hongyu guo. 2015. long short-term memory over recursive structures. in proceedingsoficml,1604-
1612.

87

recurrent neural network

v variations

    gated reid98 (chen et al., 2015)

    build a gated structure on the full binary tree

xinchi chen, xipeng qiu, chenxi zhu, shiyu wu, and xuanjing huang. 2015.  sentence modeling  with gated recursive neural 
network."  in proceedings of emnlp,793-798.

88

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models

    overview
    document embedding
    flat models
    hierarchical learning
v fine-grained models

89

overview

v input: a document consists of m sentences
v output: polarity or fine-grained sentiment
   classification problem
v classification layer

       =                            (    b       +    b)

    	

       

classification

   	

neural feature extraction

       $				       &		   			       (q$       (
    =	    $			    &		   			    ,q$    ,

vectorization

90

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models

    overview
    document embedding
    flat models
    hierarchical learning
v fine-grained models

91

document embedding

v extend id97 models (mikolov et al., 2013) to learn document 

representations

v utilize id194 as features for mlp classification

tomas  mikolov,  ilya sutskever, kai chen, greg s. corrado, and jeff dean. 2013.  distributed representations  of words and phrases and their 
compositionality.  in proceedings of nips, 3111-3119.
quoc v. le, and tomas  mikolov.  2014.  distributed representations  of sentences and documents.  in proceedings of icml, 1188-1196.

92

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models

    overview
    document embedding
    flat models
    hierarchical learning
v fine-grained models

93

flat models

v sentence-level-based models
v id98 variations 

    jonhson and zhang (2015a)

    seq-id98: use one-hot  inputs  for a word
    bow-id98:  use one-hot  inputs  for id165s 

    jonhson and zhang (2015b)

    augment inputs  by id98-based region  embeddings

v lstm variations

    jonhson and zhang (2016): 

    extend  jonhson and zhang (2015b)  model by applying  lstm

id98

|v|

d=    $	    &    2	    3									   							    (q$    (

   one-hot encoding is efficient to represent variable-sized document

rie johnson and tong zhang. 2015a.  effective  use of word order for text  categorization  with convolutional  neural networks.  in proceedings of 
naacl:hlt, 103-112.
rie johnson, and tong zhang. 2015b.  semi-supervised  convolutional  neural networks  for text categorization  via region embedding. 
in proceedings of nips,919-927.
rie johnson, and tong zhang. 2016.  supervised and semi-supervised text categorization  using lstm for region embeddings. in proceedings 
of icml, 526-534.
94

flat models

v deep id98 variations

    zhang et al. (2015)

    use one-hot character-level inputs
    stack 6 convolutional layers

    conneau et al. (2016) 

    employ character embeddings
    build up to 49 id98 layers

   character-level representation is also helpful

id98

|m|

d=       $       &       2       3				   			       (q$       (

xiang zhang, junbo zhao, and yann lecun. 2015.  character-level  convolutional  networks  for text  classification.  in proceedings of nips, 649-657.
alexis  conneau, holger schwenk, lo  c barrault, and yann lecun. 2016.  very deep convolutional  networks  for natural language processing. arxiv
preprint arxiv:1606.01781.

95

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models

    overview
    document embedding
    flat models
    hierarchical learning

v fine-grained models

96

hierarchical learning

v pooling (tang et al., 2015a) 

    average pooling sentence representations as id194 

v lstm/id98-gru (tang et al., 2015b)

duyu tang, bing qin, and ting liu. 2015a.  learning semantic  representations  of users and products for document level  sentiment 
classification.  in proceedings of acl.
duyu tang, bing qin, and ting liu. 2015b. document modeling  with gated recurrent neural network  for sentiment  classification. 
in proceedings of emnlp, 1422-1432.

97

hierarchical learning

v variations

    lstm-id98 (zhang et al., 2016)

rui zhang, honglak lee,  and dragomir radev. 2016.  dependency sensitive  convolutional  neural networks  for modeling sentences and 
documents.  in proceedings of naacl:hlt, 1512-1521.

98

hierarchical learning

v variations

    gru-gru  attention networks (yang et al., 2016)

zichao yang, diyi yang, chris dyer, xiaodong he, alex smola,  and eduard hovy. 2016.  hierarchical attention  networks  for document 
classification.  in proceedings of naacl:hlt.

99

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models

    overview
    targeted sentiment
    open-domain targeted sentiment
    opinion expression detection

v conclusion

100

v inputs: 

overview

    a sentence consists of n words.

    with a given target    classification problem
    without a given target    sequence labeler

v output: 

    [who] holds [which opinions] towards [whom]

101

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models

    overview
    targeted sentiment
    open-domain targeted sentiment
    opinion expression detection

v conclusion

102

targeted sentiment

v tree-structure-based

    dong et al. (2014)

    variant reid98
    dependency tree

li dong, furu wei, chuanqi tan, duyu tang, ming zhou, and ke xu. 2014.  adaptive id56  for target-dependent twitter 
sentiment  classification.  in proceedings of acl, 49-54.

103

targeted sentiment

v tree-structure-based

    nguyen and shirai (2015)

    variant reid98
    dependency+constituent trees

thien hai nguyen, and kiyoaki shirai. 2015.  phraseid56: phrase id56 for aspect-based sentiment  analysis.  in 
proceedings of emnlp, 2509-2514.

104

targeted sentiment

v pattern-based

    vo and zhang (2015)
    pooling mechanisms

      p=[      p($),      p($),      p($),      p(&),      p(&),      p(&)]

where:-       p())=    (    ()))
,        )
-       p())=[        )
,        )
-       p())=[            )
,            )
]
-          =[    $    ,   ,    (cid:153)(    )]
-     (cid:153):                            	                                    

]

duy-tin vo, and yue zhang. 2015.  target-dependent twitter  sentiment  classification  with rich automatic  features.  in proceedings of ijcai.

105

targeted sentiment

v pattern-based

    zhang et al. (2016)
    gated mechanisms

meishan zhang, yue zhang, and duy-tin vo. 2016.  gated neural networks  for targeted sentiment  analysis.  in proceedings of aaai.

106

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models

    overview
    targeted sentiment
    open-domain targeted sentiment
    opinion expression detection

v conclusion

107

open-domain targeted sentiment

v open domain (detect target and its sentiment)

    zhang et. al.(2015)

    neural crf
    discrete features

meishan zhang, yue zhang, and duy-tin vo. 2015.  neural networks  for open domain targeted sentiment.  in proceedings of emnlp, 612-621.108

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models

    overview
    targeted sentiment
    open-domain targeted sentiment
    opinion expression detection

v conclusion

109

opinion expression detection

v detect opinion expression

    irsoy and cardie (2014)

    deep biid56

ozan irsoy, and claire cardie. 2014.  opinion mining with deep recurrent neural networks.  in proceedings of emnlp, 720-728.

110

opinion expression detection

v opinion expression and detect target

    liu et al. (2015)

    lstm
    discrete features

pengfei liu, shafiq joty, and helen meng. 2015.  fine-grained opinion mining with  recurrent neural networks  and id27s. 
in proceedings of emnlp.

111

outline 

v introduction
v neural network background
v sentiment-oriented id27
v sentence-level models
v document-level models
v fine-grained models
v conclusion

112

conclusion

raw data

feature engineering  models

113

conclusion

raw data

neural network  models

114

thank you!!!

115

