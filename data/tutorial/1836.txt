   #[1]wildml    feed [2]wildml    comments feed [3]wildml    recurrent
   neural networks tutorial, part 1     introduction to id56s comments feed
   [4]speeding up your neural network with theano and the gpu [5]recurrent
   neural networks tutorial, part 2     implementing a id56 with python,
   numpy and theano [6]alternate [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]september 17, 2015july 8, 2016 by [16]denny britz

recurrent neural networks tutorial, part 1     introduction to id56s

   recurrent neural networks (id56s) are popular models that have shown
   great promise in many nlp tasks. but despite their recent popularity
   i   ve only found a limited number of resources that throughly explain
   how id56s work, and how to implement them. that   s what this tutorial is
   about. it   s a multi-part series in which i   m planning to cover the
   following:
    1. introduction to id56s (this post)
    2. [17]implementing a id56 using python and theano
    3. [18]understanding the id26 through time (bptt) algorithm
       and the vanishing gradient problem
    4. [19]implementing a gru/lstm id56

   as part of the tutorial we will implement a [20]recurrent neural
   network based language model. the applications of language models are
   two-fold: first, it allows us to score arbitrary sentences based on how
   likely they are to occur in the real world. this gives us a measure
   of grammatical and semantic correctness. such models are typically used
   as part of machine translation systems. secondly, a language model
   allows us to generate new text (i think that   s the much cooler
   application). training a language model on shakespeare allows us to
   generate shakespeare-like text. [21]this fun post by andrej
   karpathy demonstrates what character-level language models based on
   id56s are capable of.

   i   m assuming that you are somewhat familiar with basic neural networks.
   if you   re not, you may want to head over to [22]implementing a neural
   network from scratch,  which guides you through the ideas and
   implementation behind non-recurrent networks.

what are id56s?

   the idea behind id56s is to make use of sequential information. in a
   traditional neural network we assume that all inputs (and outputs) are
   independent of each other. but for many tasks that   s a very bad idea.
   if you want to predict the next word in a sentence you better know
   which words came before it. id56s are called recurrent because they
   perform the same task for every element of a sequence, with the output
   being depended on the previous computations. another way to think about
   id56s is that they have a    memory    which captures information about what
   has been calculated so far. in theory id56s can make use of information
   in arbitrarily long sequences, but in practice they are limited to
   looking back only a few steps (more on this later). here is what a
   typical id56 looks like:
   [23]a recurrent neural network and the unfolding in time of the
   computation involved in its forward computation. a recurrent neural
   network and the unfolding in time of the computation involved in its
   forward computation. source: nature

   the above diagram shows a id56 being unrolled (or unfolded) into a full
   network. by unrolling we simply mean that we write out the network for
   the complete sequence. for example, if the sequence we care about is a
   sentence of 5 words, the network would be unrolled into a 5-layer
   neural network, one layer for each word. the formulas that govern the
   computation happening in a id56 are as follows:
     * x_t is the input at time step t . for example, x_1  could be
       a one-hot vector corresponding to the second word of a sentence.
     * s_t is the hidden state at time step t . it   s the    memory    of the
       network. s_t is calculated based on the previous hidden state and
       the input at the current step: s_t=f(ux_t + ws_{t-1}) . the
       function f usually is a nonlinearity such as [24]tanh or [25]relu.
         s_{-1} , which is required to calculate the first hidden state,
       is typically initialized to all zeroes.
     * o_t is the output at step t . for example, if we wanted to predict
       the next word in a sentence it would be a vector of probabilities
       across our vocabulary. o_t = \mathrm{softmax}(vs_t) .

   there are a few things to note here:
     * you can think of the hidden state s_t as the memory of the network.
       s_t captures information about what happened in all the previous
       time steps. the output at step o_t is calculated solely based on
       the memory at time t . as briefly mentioned above, it   s a bit more
       complicated  in practice because s_t typically can   t capture
       information from too many time steps ago.
     * unlike a traditional deep neural network, which uses different
       parameters at each layer, a id56 shares the same parameters ( u, v,
       w above) across all steps. this reflects the fact that we are
       performing the same task at each step, just with different inputs.
       this greatly reduces the total number of parameters we need to
       learn.
     * the above diagram has outputs at each time step, but depending on
       the task this may not be necessary. for example, when predicting
       the sentiment of a sentence we may only care about the final
       output, not the sentiment after each word. similarly, we may not
       need inputs at each time step. the main feature of an id56 is
       its hidden state, which captures some information about a sequence.

what can id56s do?

   id56s have shown great success in many nlp tasks. at this point i should
   mention that the most commonly used type of id56s are [26]lstms, which
   are much better at capturing long-term dependencies than vanilla id56s
   are. but don   t worry, lstms are essentially the same thing as the id56
   we will develop in this tutorial, they just have a different way of
   computing the hidden state. we   ll cover lstms in more detail in a later
   post. here are some example applications of id56s in nlp (by non means
   an exhaustive list).

id38 and generating text

   given a sequence of words we want to predict the id203 of each
   word given the previous words. language models allow us to measure how
   likely a sentence is, which is an important input for machine
   translation (since high-id203 sentences are typically correct). a
   side-effect of being able to predict the next word is that we get a
   generative model, which allows us to generate new text by sampling from
   the output probabilities. and depending on what our training data is we
   can generate [27]all kinds of stuff. in id38 our input is
   typically a sequence of words (encoded as one-hot vectors for example),
   and our output is the sequence of predicted words. when training the
   network we set o_t = x_{t+1} since we want the output at step t to be
   the actual next word.

   research papers about id38 and generating text:
     * [28]recurrent neural network based language model
     * [29]extensions of recurrent neural network based language model
     * [30]generating text with recurrent neural networks

machine translation

   machine translation is similar to id38 in that our input
   is a sequence of words in our source language (e.g. german). we want to
   output a sequence of words in our target language (e.g. english). a key
   difference is that our output only starts after we have seen the
   complete input, because the first word of our translated sentences may
   require information captured from the complete input sequence.
   [31]id56 for machine translation id56 for machine translation. image
   source: http://cs224d.stanford.edu/lectures/cs224d-lecture8.pdf

   research papers about machine translation:
     * [32]a recursive recurrent neural network for statistical machine
       translation
     * [33]sequence to sequence learning with neural networks
     * [34]joint language and translation modeling with recurrent neural
       networks

id103

   given an input sequence of acoustic signals from a sound wave, we can
   predict a sequence of phonetic segments together with
   their probabilities.

   research papers about id103:
     * [35]towards end-to-end id103 with recurrent neural
       networks

generating image descriptions

   together with convolutional neural networks, id56s have been used as
   part of a model to [36]generate descriptions for unlabeled images. it   s
   quite amazing how well this seems to work. the combined model even
   aligns the generated words with features found in the images.
   [37]deep visual-semantic alignments for generating image descriptions
   deep visual-semantic alignments for generating image descriptions.
   source: http://cs.stanford.edu/people/karpathy/deepimagesent/

training id56s

   training a id56 is similar to training a traditional neural network. we
   also use the id26 algorithm, but with a little twist.
   because the parameters are shared by all time steps in the network, the
   gradient at each output depends not only on the calculations of the
   current time step, but also the previous time steps. for example, in
   order to calculate the gradient at t=4 we would need to backpropagate 3
   steps and sum up the gradients. this is called id26 through
   time (bptt). if this doesn   t make a whole lot of sense yet, don   t
   worry, we   ll have a whole post on the gory details. for now, just be
   aware of the fact that vanilla id56s trained with bptt[38] have
   difficulties learning long-term dependencies (e.g. dependencies between
   steps that are far apart) due to what is called the vanishing/exploding
   gradient problem. there exists some machinery to deal with these
   problems, and certain types of id56s (like lstms) were specifically
   designed to get around them.

id56 extensions

   over the years researchers have developed more sophisticated types of
   id56s to deal with some of the shortcomings of the vanilla id56 model. we
   will cover them in more detail in a later post, but i want this section
   to serve as a brief overview so that you are familiar with the taxonomy
   of models.

   bidirectional id56s are based on the idea that the output at time t may
   not only depend on the previous elements in the sequence, but also
   future elements. for example, to predict a missing word in a sequence
   you want to look at both the left and the right context. bidirectional
   id56s are quite simple. they are just two id56s stacked on top of each
   other. the output is then computed based on the hidden state of both
   id56s.

   [39]bidirectional id56

   deep (bidirectional) id56s are similar to bidirectional id56s, only that
   we now have multiple layers per time step. in practice this gives us a
   higher learning capacity (but we also need a lot of training data).

   [40]deep bidirectional id56

   id137 are quite popular these days and we briefly talked
   about them above. lstms don   t have a fundamentally different
   architecture from id56s, but they use a different function to compute
   the hidden state. the memory in lstms are called cells and you can
   think of them as black boxes that take as input the previous state
   h_{t-1} and current input x_t . internally these cells  decide what to
   keep in (and what to erase from) memory. they then combine the previous
   state, the current memory, and the input. it turns out that these types
   of units are very efficient at capturing long-term dependencies. lstms
   can be quite confusing in the beginning but if you   re interested in
   learning more [41]this post has an excellent explanation.

conclusion

   so far so good. i hope you   ve gotten a basic understanding of what id56s
   are and what they can do. in the next post we   ll implement a first
   version of our language model id56 using python and theano. please leave
   questions in the comments!

   categories[42]deep learning, [43]neural networks, [44]recurrent neural
   networks

post navigation

   [45]previous postprevious speeding up your neural network with theano
   and the gpu
   [46]next postnext recurrent neural networks tutorial, part 2    
   implementing a id56 with python, numpy and theano

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [47]introduction to learning to trade with id23
     * [48]ai and deep learning in 2017     a year in review
     * [49]hype or not? some perspective on openai   s dota 2 bot
     * [50]learning id23 (with code, exercises and
       solutions)
     * [51]id56s in tensorflow, a practical guide and undocumented features
     * [52]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [53]deep learning for chatbots, part 1     introduction
     * [54]attention and memory in deep learning and nlp

archives

     * [55]february 2018
     * [56]december 2017
     * [57]august 2017
     * [58]october 2016
     * [59]august 2016
     * [60]july 2016
     * [61]april 2016
     * [62]january 2016
     * [63]december 2015
     * [64]november 2015
     * [65]october 2015
     * [66]september 2015

categories

     * [67]conversational agents
     * [68]convolutional neural networks
     * [69]deep learning
     * [70]gpu
     * [71]id38
     * [72]memory
     * [73]neural networks
     * [74]news
     * [75]nlp
     * [76]recurrent neural networks
     * [77]id23
     * [78]id56s
     * [79]tensorflow
     * [80]trading
     * [81]uncategorized

meta

     * [82]log in
     * [83]entries rss
     * [84]comments rss
     * [85]wordpress.org

   [86]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/feed/
   4. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
   5. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/&format=xml
   8. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  16. http://www.wildml.com/author/dennybritz/
  17. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  18. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  19. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  20. http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_is100722.pdf
  21. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  22. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  23. http://www.wildml.com/wp-content/uploads/2015/09/id56.jpg
  24. https://reference.wolfram.com/language/ref/tanh.html
  25. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  26. https://en.wikipedia.org/wiki/long_short_term_memory
  27. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  28. http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_is100722.pdf
  29. http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf
  30. http://machinelearning.wustl.edu/mlpapers/paper_files/icml2011sutskever_524.pdf
  31. http://www.wildml.com/wp-content/uploads/2015/09/screen-shot-2015-09-17-at-10.39.06-am.png
  32. http://www.aclweb.org/anthology/p14-1140.pdf
  33. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
  34. http://research.microsoft.com/en-us/um/people/gzweig/pubs/emnlp2013rnid4.pdf
  35. http://www.jmlr.org/proceedings/papers/v32/graves14.pdf
  36. http://cs.stanford.edu/people/karpathy/deepimagesent/
  37. http://www.wildml.com/wp-content/uploads/2015/09/screen-shot-2015-09-17-at-11.44.24-am.png
  38. http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf
  39. http://www.wildml.com/wp-content/uploads/2015/09/bidirectional-id56.png
  40. http://www.wildml.com/wp-content/uploads/2015/09/screen-shot-2015-09-16-at-2.21.51-pm.png
  41. http://colah.github.io/posts/2015-08-understanding-lstms/
  42. http://www.wildml.com/category/deep-learning/
  43. http://www.wildml.com/category/neural-networks/
  44. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  45. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
  46. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  47. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  48. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  49. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  50. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  51. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  52. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  53. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  54. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  55. http://www.wildml.com/2018/02/
  56. http://www.wildml.com/2017/12/
  57. http://www.wildml.com/2017/08/
  58. http://www.wildml.com/2016/10/
  59. http://www.wildml.com/2016/08/
  60. http://www.wildml.com/2016/07/
  61. http://www.wildml.com/2016/04/
  62. http://www.wildml.com/2016/01/
  63. http://www.wildml.com/2015/12/
  64. http://www.wildml.com/2015/11/
  65. http://www.wildml.com/2015/10/
  66. http://www.wildml.com/2015/09/
  67. http://www.wildml.com/category/conversational-agents/
  68. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  69. http://www.wildml.com/category/deep-learning/
  70. http://www.wildml.com/category/gpu/
  71. http://www.wildml.com/category/language-modeling/
  72. http://www.wildml.com/category/memory/
  73. http://www.wildml.com/category/neural-networks/
  74. http://www.wildml.com/category/news/
  75. http://www.wildml.com/category/nlp/
  76. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  77. http://www.wildml.com/category/reinforcement-learning/
  78. http://www.wildml.com/category/id56s/
  79. http://www.wildml.com/category/tensorflow/
  80. http://www.wildml.com/category/trading/
  81. http://www.wildml.com/category/uncategorized/
  82. http://www.wildml.com/wp-login.php
  83. http://www.wildml.com/feed/
  84. http://www.wildml.com/comments/feed/
  85. https://wordpress.org/
  86. https://wordpress.org/
