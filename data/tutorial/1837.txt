   #[1]wildml    feed [2]wildml    comments feed [3]wildml    recurrent
   neural networks tutorial, part 2     implementing a id56 with python,
   numpy and theano comments feed [4]recurrent neural networks tutorial,
   part 1     introduction to id56s [5]recurrent neural networks tutorial,
   part 3     id26 through time and vanishing gradients
   [6]alternate [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]september 30, 2015january 10, 2016 by [16]denny britz

recurrent neural networks tutorial, part 2     implementing a id56 with python,
numpy and theano

   this the second part of the recurrent neural network tutorial. [17]the
   first part is here.

   [18]code to follow along is on github.

   in this part we will implement a full recurrent neural network from
   scratch using python and optimize our implementation using [19]theano,
   a library to perform operations on a gpu. [20]the full code is
   available on github. i will skip over some boilerplate code that is not
   essential to understanding recurrent neural networks, but all of that
   is also on github.

id38

   our goal is to build a [21]language model using a recurrent neural
   network. here   s what that means. let   s say we have sentence of m words.
   a language model allows us to predict the id203 of observing
   the sentence (in a given dataset) as:

   \begin{aligned} p(w_1,...,w_m) = \prod_{i=1}^{m} p(w_i \mid w_1,...,
   w_{i-1}) \end{aligned}

   in words, the id203 of a sentence is the product of probabilities
   of each word given the words that came before it. so, the id203
   of the sentence    he went to buy some chocolate    would be the
   id203 of    chocolate    given    he went to buy some   , multiplied by
   the id203 of    some    given    he went to buy   , and so on.

   why is that useful? why would we want to assign a id203 to
   observing a sentence?

   first, such a model can be used as a scoring mechanism. for example, a
   machine translation system typically generates multiple candidates for
   an input sentence. you could use a language model to pick the most
   probable sentence. intuitively, the most probable sentence is likely to
   be grammatically correct. similar scoring happens in id103
   systems.

   but solving the id38 problem also has a cool side effect.
   because we can predict the id203 of a word given the preceding
   words, we are able to generate new text. it   s a generative model. given
   an existing sequence of words we sample a next word from the predicted
   probabilities, and repeat the process until we have a full
   sentence. andrej karparthy [22]has a great post that demonstrates what
   language models are capable of. his models are trained on single
   characters as opposed to full words, and can generate anything from
   shakespeare to linux code.

   note that in the above equation the id203 of each word is
   conditioned on all previous words. in practice, many models have a hard
   time representing such long-term dependencies due to computational or
   memory constraints. they are typically limited to looking at only a few
   of the previous words. id56s can, in theory, capture such long-term
   dependencies, but in practice it   s a bit more complex. we   ll explore
   that in a later post.

training data and preprocessing

   to train our language model we need text to learn from. fortunately we
   don   t need any labels to train a language model, just raw text. i
   downloaded 15,000 longish reddit comments from a [23]dataset available
   on google   s bigquery. text generated by our model will sound like
   reddit commenters (hopefully)! but as with most machine learning
   projects we first need to do some pre-processing to get our data into
   the right format.

1. tokenize text

   we have raw text, but we want to make predictions on a per-word basis.
   this means we must tokenize our comments into sentences, and sentences
   into words. we could just split each of the comments by spaces, but
   that wouldn   t handle punctuation properly. the sentence    he left!   
   should be 3 tokens:    he   ,    left   ,    !   . we   ll use [24]nltk   s
   word_tokenize and sent_tokenize methods, which do most of the hard work
   for us.

2. remove infrequent words

   most words in our text will only appear one or two times. it   s a good
   idea to remove these infrequent words. having a huge vocabulary will
   make our model slow to train (we   ll talk about why that is later), and
   because we don   t have a lot of contextual examples for such words we
   wouldn   t be able to learn how to use them correctly anyway. that   s
   quite similar to how humans learn. to really understand how to
   appropriately use a word you need to have seen it in different
   contexts.

   in our code we limit our vocabulary to the vocabulary_size most common
   words (which i set to 8000, but feel free to change it). we replace all
   words not included in our vocabulary by unknown_token. for example, if
   we don   t include the word    nonlinearities    in our vocabulary, the
   sentence    nonlineraties are important in neural networks    becomes
      unknown_token are important in neural networks   . the word
   unknown_token will become part of our vocabulary and we will predict it
   just like any other word. when we generate new text we can replace
   unknown_token again, for example by taking a randomly sampled word not
   in our vocabulary, or we could just generate sentences until we get one
   that doesn   t contain an unknown token.

3. prepend special start and end tokens

   we also want to learn which words tend start and end a sentence. to do
   this we prepend a special sentence_start token, and append a special
   sentence_end token to each sentence. this allows us to ask: given that
   the first token is sentence_start, what is the likely next word (the
   actual first word of the sentence)?

4. build training data matrices

   the input to our recurrent neural networks are vectors, not strings. so
   we create a mapping between words and indices, index_to_word, and
   word_to_index. for example,  the word    friendly    may be at index 2001.
   a training example x  may look like [0, 179, 341, 416], where 0
   corresponds to sentence_start. the corresponding label y would be [179,
   341, 416, 1]. remember that our goal is to predict the next word, so y
   is just the x vector shifted by one position with the last element
   being the sentence_end token. in other words, the correct prediction
   for word 179 above would be 341, the actual next word.

   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   37
   38
   39
   40
   41
   42
   vocabulary_size = 8000
   unknown_token = &quot;unknown_token&quot;
   sentence_start_token = &quot;sentence_start&quot;
   sentence_end_token = &quot;sentence_end&quot;

   # read the data and append sentence_start and sentence_end tokens
   print &quot;reading csv file...&quot;
   with open('data/reddit-comments-2015-08.csv', 'rb') as f:
       reader = csv.reader(f, skipinitialspace=true)
       reader.next()
       # split full comments into sentences
       sentences =
   itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for
   x in reader])
       # append sentence_start and sentence_end
       sentences = [&quot;%s %s %s&quot; % (sentence_start_token, x,
   sentence_end_token) for x in sentences]
   print &quot;parsed %d sentences.&quot; % (len(sentences))

   # tokenize the sentences into words
   tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]

   # count the word frequencies
   word_freq = nltk.freqdist(itertools.chain(*tokenized_sentences))
   print &quot;found %d unique words tokens.&quot; %
   len(word_freq.items())

   # get the most common words and build index_to_word and word_to_index
   vectors
   vocab = word_freq.most_common(vocabulary_size-1)
   index_to_word = [x[0] for x in vocab]
   index_to_word.append(unknown_token)
   word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])

   print &quot;using vocabulary size %d.&quot; % vocabulary_size
   print &quot;the least frequent word in our vocabulary is '%s' and
   appeared %d times.&quot; % (vocab[-1][0], vocab[-1][1])

   # replace all words not in our vocabulary with the unknown token
   for i, sent in enumerate(tokenized_sentences):
       tokenized_sentences[i] = [w if w in word_to_index else
   unknown_token for w in sent]

   print &quot;\nexample sentence: '%s'&quot; % sentences[0]
   print &quot;\nexample sentence after pre-processing: '%s'&quot; %
   tokenized_sentences[0]

   # create the training data
   x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in
   tokenized_sentences])
   y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in
   tokenized_sentences])

   here   s an actual training example from our text:
x:
sentence_start what are n't you understanding about this ? !
[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]

y:
what are n't you understanding about this ? ! sentence_end
[51, 27, 16, 10, 856, 53, 25, 34, 69, 1]

building the id56

   for a general overview of id56s take a look at [25]first part of the
   tutorial.
   [26]a recurrent neural network and the unfolding in time of the
   computation involved in its forward computation. a recurrent neural
   network and the unfolding in time of the computation involved in its
   forward computation.

   let   s get concrete and see what the id56 for our language model looks
   like. the input x will be a sequence of words (just like the example
   printed above) and each x_t is a single word. but there   s one more
   thing: because of how id127 works we can   t simply use
   a word index (like 36) as an input. instead, we represent each word as
   a one-hot vector of size vocabulary_size. for example, the word with
   index 36 would be the vector of all 0   s and a 1 at position 36. so,
   each x_t will become a vector, and x will be a matrix, with each row
   representing a word. we   ll perform this transformation in our neural
   network code instead of doing it in the pre-processing. the output of
   our network o has a similar format. each o_t is a vector of
   vocabulary_size elements, and each element represents the id203
   of that word being the next word in the sentence.

   let   s recap the equations for the id56 from the first part of the
   tutorial:

   \begin{aligned} s_t &= \tanh(ux_t + ws_{t-1}) \\ o_t &=
   \mathrm{softmax}(vs_t) \end{aligned}

   i always find it useful to write down the dimensions of the matrices
   and vectors. let   s assume we pick a vocabulary size c = 8000 and a
   hidden layer size h = 100 . you can think of the hidden layer size as
   the    memory    of our network. making it bigger allows us to learn more
   complex patterns, but also results in additional computation. then we
   have:

   \begin{aligned} x_t & \in \mathbb{r}^{8000} \\ o_t & \in
   \mathbb{r}^{8000} \\ s_t & \in \mathbb{r}^{100} \\ u & \in
   \mathbb{r}^{100 \times 8000} \\ v & \in \mathbb{r}^{8000 \times 100} \\
   w & \in \mathbb{r}^{100 \times 100} \\ \end{aligned}

   this is valuable information. remember that u,v and w are the
   parameters of our network we want to learn from data. thus, we need to
   learn a total of 2hc + h^2 parameters. in the case of c=8000 and h=100
   that   s 1,610,000. the dimensions also tell us the bottleneck of our
   model. note that because x_t is a one-hot vector, multiplying it with u
   is essentially the same as selecting a column of u, so we don   t need to
   perform the full multiplication. then, the biggest matrix
   multiplication in our network is vs_t . that   s why we want to keep our
   vocabulary size small if possible.

   armed with this, it   s time to start our implementation.

initialization

   we start by declaring a id56 class an initializing our parameters. i   m
   calling this class id56numpy because we will implement a theano version
   later. initializing the parameters u,v and w is a bit tricky. we can   t
   just initialize them to 0   s because that would result in symmetric
   calculations in all our layers. we must initialize them randomly.
   because proper initialization seems to have an impact on training
   results there has been lot of research in this area. it turns out that
   the best initialization depends on the activation function ( \tanh in
   our case) and one [27]recommended approach is to initialize the weights
   randomly in the interval from \left[-\frac{1}{\sqrt{n}},
   \frac{1}{\sqrt{n}}\right] where n is the number of incoming connections
   from the previous layer. this may sound overly complicated, but don   t
   worry too much it. as long as you initialize your parameters to small
   random values it typically works out fine.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   class id56numpy:

       def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):
           # assign instance variables
           self.word_dim = word_dim
           self.hidden_dim = hidden_dim
           self.bptt_truncate = bptt_truncate
           # randomly initialize the network parameters
           self.u = np.random.uniform(-np.sqrt(1./word_dim),
   np.sqrt(1./word_dim), (hidden_dim, word_dim))
           self.v = np.random.uniform(-np.sqrt(1./hidden_dim),
   np.sqrt(1./hidden_dim), (word_dim, hidden_dim))
           self.w = np.random.uniform(-np.sqrt(1./hidden_dim),
   np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))

   above, word_dim is the size of our vocabulary, and hidden_dim is the
   size of our hidden layer (we can pick it). don   t worry about the
   bptt_truncate parameter for now, we   ll explain what that is later.

forward propagation

   next, let   s implement the forward propagation (predicting word
   probabilities) defined by our equations above:
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   def forward_propagation(self, x):
       # the total number of time steps
       t = len(x)
       # during forward propagation we save all hidden states in s because
   need them later.
       # we add one additional element for the initial hidden, which we
   set to 0
       s = np.zeros((t + 1, self.hidden_dim))
       s[-1] = np.zeros(self.hidden_dim)
       # the outputs at each time step. again, we save them for later.
       o = np.zeros((t, self.word_dim))
       # for each time step...
       for t in np.arange(t):
           # note that we are indxing u by x[t]. this is the same as
   multiplying u with a one-hot vector.
           s[t] = np.tanh(self.u[:,x[t]] + self.w.dot(s[t-1]))
           o[t] = softmax(self.v.dot(s[t]))
       return [o, s]

   id56numpy.forward_propagation = forward_propagation

   we not only return the calculated outputs, but also the hidden states.
   we will use them later to calculate the gradients, and by
   returning them here we avoid duplicate computation. each o_t is a
   vector of probabilities representing the words in our vocabulary, but
   sometimes, for example when evaluating our model, all we want is the
   next word with the highest id203. we call this function predict:
   1
   2
   3
   4
   5
   6
   def predict(self, x):
       # perform forward propagation and return index of the highest score
       o, s = self.forward_propagation(x)
       return np.argmax(o, axis=1)

   id56numpy.predict = predict

   let   s try our newly implemented methods and see an example output:
   1
   2
   3
   4
   5
   np.random.seed(10)
   model = id56numpy(vocabulary_size)
   o, s = model.forward_propagation(x_train[10])
   print o.shape
   print o

(45, 8000)
[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488
   0.00012508]
 [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456
   0.00012451]
 [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588
   0.00012551]
 ...,
 [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494
   0.0001263 ]
 [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578
   0.00012502]
 [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536
   0.00012665]]

   for each word in the sentence (45 above), our model made
   8000 predictions representing probabilities of the next word. note that
   because we initialized u,v,w to random values these predictions are
   completely random right now. the following gives the indices of the
   highest id203 predictions for each word:

   1
   2
   3
   predictions = model.predict(x_train[10])
   print predictions.shape
   print predictions

(45,)
[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481 238 2539
 21 6548 261 1780 2005 1810 5376 4146 477 7051 4832 4991 897 3485 21
 7291 2007 6006 760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]

calculating the loss

   to train our network we need a way to measure the errors it makes. we
   call this the id168 l , and our goal is find the parameters u,v
   and w that minimize the id168 for our training data. a common
   choice for the id168 is the [28]cross-id178 loss. if we have
   n training examples (words in our text) and c classes (the size of our
   vocabulary) then the loss with respect to our predictions o and the
   true labels y is given by:

   \begin{aligned} l(y,o) = - \frac{1}{n} \sum_{n \in n} y_{n} \log o_{n}
   \end{aligned}

   the formula looks a bit complicated, but all it really does is sum over
   our training examples and add to the loss based on how off our
   prediction are. the further away y (the correct words) and o (our
   predictions), the greater the loss will be. we implement the function
   calculate_loss:
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   def calculate_total_loss(self, x, y):
       l = 0
       # for each sentence...
       for i in np.arange(len(y)):
           o, s = self.forward_propagation(x[i])
           # we only care about our prediction of the &quot;correct&quot;
   words
           correct_word_predictions = o[np.arange(len(y[i])), y[i]]
           # add to the loss based on how off we were
           l += -1 * np.sum(np.log(correct_word_predictions))
       return l

   def calculate_loss(self, x, y):
       # divide the total loss by the number of training examples
       n = np.sum((len(y_i) for y_i in y))
       return self.calculate_total_loss(x,y)/n

   id56numpy.calculate_total_loss = calculate_total_loss
   id56numpy.calculate_loss = calculate_loss

   let   s take a step back and think about what the loss should be for
   random predictions. that will give us a baseline and make sure our
   implementation is correct. we have c words in our vocabulary, so each
   word should be (on average) predicted with id203 1/c , which
   would yield a loss of l = -\frac{1}{n} n \log\frac{1}{c} = \log c :
   1
   2
   3
   # limit to 1000 examples to save time
   print &quot;expected loss for random predictions: %f&quot; %
   np.log(vocabulary_size)
   print &quot;actual loss: %f&quot; %
   model.calculate_loss(x_train[:1000], y_train[:1000])

expected loss for random predictions: 8.987197
actual loss: 8.987440

   pretty close! keep in mind that evaluating the loss on the full dataset
   is an expensive operation and can take hours if you have a lot of data!

training the id56 with sgd and id26 through time (bptt)

   remember that we want to find the parameters u,v and w that minimize
   the total loss on the training data. the most common way to do this is
   sgd, stochastic id119. the idea behind sgd is pretty simple.
   we iterate over all our training examples and during each iteration we
   nudge the parameters into a direction that reduces the error. these
   directions are given by the gradients on the loss: \frac{\partial
   l}{\partial u}, \frac{\partial l}{\partial v}, \frac{\partial
   l}{\partial w} . sgd also needs a learning rate, which defines how big
   of a step we want to make in each iteration. sgd is the most popular
   optimization method not only for neural networks, but also for many
   other machine learning algorithms. as such there has been a lot of
   research on how to optimize sgd using batching, parallelism and
   adaptive learning rates. even though the basic idea is simple,
   implementing sgd in a really efficient way can become very complex. if
   you want to learn more about sgd [29]this is a good place to start. due
   to its popularity there are a wealth of tutorials floating around the
   web, and i don   t want to duplicate them here. i   ll implement a simple
   version of sgd that should be understandable even without a background
   in optimization.

   but how do we calculate those gradients we mentioned above? in a
   [30]traditional neural network we do this through the id26
   algorithm. in id56s we use a slightly modified version of the
   this algorithm called id26 through time (bptt). because the
   parameters are shared by all time steps in the network, the gradient at
   each output depends not only on the calculations of the current time
   step, but also the previous time steps. if you know calculus, it really
   is just applying the chain rule. the next part of the tutorial will be
   all about bptt, so i won   t go into detailed derivation here. for a
   general introduction to id26 check out [31]this and this
   [32]post. for now you can treat bptt as a black box. it takes as input
   a training example (x,y) and returns the gradients \frac{\partial
   l}{\partial u}, \frac{\partial l}{\partial v}, \frac{\partial
   l}{\partial w} .
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   def bptt(self, x, y):
       t = len(y)
       # perform forward propagation
       o, s = self.forward_propagation(x)
       # we accumulate the gradients in these variables
       dldu = np.zeros(self.u.shape)
       dldv = np.zeros(self.v.shape)
       dldw = np.zeros(self.w.shape)
       delta_o = o
       delta_o[np.arange(len(y)), y] -= 1.
       # for each output backwards...
       for t in np.arange(t)[::-1]:
           dldv += np.outer(delta_o[t], s[t].t)
           # initial delta calculation
           delta_t = self.v.t.dot(delta_o[t]) * (1 - (s[t] ** 2))
           # id26 through time (for at most self.bptt_truncate
   steps)
           for bptt_step in np.arange(max(0, t-self.bptt_truncate),
   t+1)[::-1]:
               # print &quot;id26 step t=%d bptt step=%d &quot;
   % (t, bptt_step)
               dldw += np.outer(delta_t, s[bptt_step-1])
               dldu[:,x[bptt_step]] += delta_t
               # update delta for next step
               delta_t = self.w.t.dot(delta_t) * (1 - s[bptt_step-1] ** 2)
       return [dldu, dldv, dldw]

   id56numpy.bptt = bptt

gradient checking

   whenever you implement id26 it is good idea to also
   implement gradient checking, which is a way of verifying that your
   implementation is correct. the idea behind gradient checking is that
   derivative of a parameter is equal to the slope at the point, which we
   can approximate by slightly changing the parameter and then dividing by
   the change:

   \begin{aligned} \frac{\partial l}{\partial \theta} \approx \lim_{h \to
   0} \frac{j(\theta + h) - j(\theta -h)}{2h} \end{aligned}

   we then compare the gradient we calculated using id26 to the
   gradient we estimated with the method above. if there   s no large
   difference we are good. the approximation needs to calculate the total
   loss for every parameter, so that gradient checking is very expensive
   (remember, we had more than a million parameters in the example above).
   so it   s a good idea to perform it on a model with a smaller vocabulary.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   37
   38
   39
   40
   41
   42
   43
   44
   45
   46
   47
   def gradient_check(self, x, y, h=0.001, error_threshold=0.01):
       # calculate the gradients using id26. we want to checker
   if these are correct.
       bptt_gradients = self.bptt(x, y)
       # list of all parameters we want to check.
       model_parameters = ['u', 'v', 'w']
       # gradient check for each parameter
       for pidx, pname in enumerate(model_parameters):
           # get the actual parameter value from the mode, e.g. model.w
           parameter = operator.attrgetter(pname)(self)
           print &quot;performing gradient check for parameter %s with
   size %d.&quot; % (pname, np.prod(parameter.shape))
           # iterate over each element of the parameter matrix, e.g.
   (0,0), (0,1), ...
           it = np.nditer(parameter, flags=['multi_index'],
   op_flags=['readwrite'])
           while not it.finished:
               ix = it.multi_index
               # save the original value so we can reset it later
               original_value = parameter[ix]
               # estimate the gradient using (f(x+h) - f(x-h))/(2*h)
               parameter[ix] = original_value + h
               gradplus = self.calculate_total_loss([x],[y])
               parameter[ix] = original_value - h
               gradminus = self.calculate_total_loss([x],[y])
               estimated_gradient = (gradplus - gradminus)/(2*h)
               # reset parameter to original value
               parameter[ix] = original_value
               # the gradient for this parameter calculated using
   id26
               backprop_gradient = bptt_gradients[pidx][ix]
               # calculate the relative error: (|x - y|/(|x| + |y|))
               relative_error = np.abs(backprop_gradient -
   estimated_gradient)/(np.abs(backprop_gradient) +
   np.abs(estimated_gradient))
               # if the error is to large fail the gradient check
               if relative_error &amp;gt; error_threshold:
                   print &quot;gradient check error: parameter=%s
   ix=%s&quot; % (pname, ix)
                   print &quot;+h loss: %f&quot; % gradplus
                   print &quot;-h loss: %f&quot; % gradminus
                   print &quot;estimated_gradient: %f&quot; %
   estimated_gradient
                   print &quot;id26 gradient: %f&quot; %
   backprop_gradient
                   print &quot;relative error: %f&quot; % relative_error
                   return
               it.iternext()
           print &quot;gradient check for parameter %s passed.&quot; %
   (pname)

   id56numpy.gradient_check = gradient_check

   # to avoid performing millions of expensive calculations we use a
   smaller vocabulary size for checking.
   grad_check_vocab_size = 100
   np.random.seed(10)
   model = id56numpy(grad_check_vocab_size, 10, bptt_truncate=1000)
   model.gradient_check([0,1,2,3], [1,2,3,4])

sgd implementation

   now that we are able to calculate the gradients for our parameters we
   can implement sgd. i like to do this in two steps: 1. a function
   sdg_step that calculates the gradients and performs the updates for one
   batch. 2. an outer loop that iterates through the training set and
   adjusts the learning rate.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   # performs one step of sgd.
   def numpy_sdg_step(self, x, y, learning_rate):
       # calculate the gradients
       dldu, dldv, dldw = self.bptt(x, y)
       # change parameters according to gradients and learning rate
       self.u -= learning_rate * dldu
       self.v -= learning_rate * dldv
       self.w -= learning_rate * dldw

   id56numpy.sgd_step = numpy_sdg_step

   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   # outer sgd loop
   # - model: the id56 model instance
   # - x_train: the training data set
   # - y_train: the training data labels
   # - learning_rate: initial learning rate for sgd
   # - nepoch: number of times to iterate through the complete dataset
   # - evaluate_loss_after: evaluate the loss after this many epochs
   def train_with_sgd(model, x_train, y_train, learning_rate=0.005,
   nepoch=100, evaluate_loss_after=5):
       # we keep track of the losses so we can plot them later
       losses = []
       num_examples_seen = 0
       for epoch in range(nepoch):
           # optionally evaluate the loss
           if (epoch % evaluate_loss_after == 0):
               loss = model.calculate_loss(x_train, y_train)
               losses.append((num_examples_seen, loss))
               time = datetime.now().strftime('%y-%m-%d %h:%m:%s')
               print &quot;%s: loss after num_examples_seen=%d epoch=%d:
   %f&quot; % (time, num_examples_seen, epoch, loss)
               # adjust the learning rate if loss increases
               if (len(losses) &amp;gt; 1 and losses[-1][1] &amp;gt;
   losses[-2][1]):
                   learning_rate = learning_rate * 0.5
                   print &quot;setting learning rate to %f&quot; %
   learning_rate
               sys.stdout.flush()
           # for each training example...
           for i in range(len(y_train)):
               # one sgd step
               model.sgd_step(x_train[i], y_train[i], learning_rate)
               num_examples_seen += 1

   done! let   s try to get a sense of how long it would take to train our
   network:
   1
   2
   3
   np.random.seed(10)
   model = id56numpy(vocabulary_size)
   %timeit model.sgd_step(x_train[10], y_train[10], 0.005)

   uh-oh, bad news. one step of sgd takes approximately 350 milliseconds
   on my laptop. we have about 80,000 examples in our training data, so
   one epoch (iteration over the whole data set) would take several hours.
   multiple epochs would take days, or even weeks! and we   re still working
   with a small dataset compared to what   s being used by many of the
   companies and researchers out there. what now?

   fortunately there are many ways to speed up our code. we could stick
   with the same model and make our code run faster, or we could modify
   our model to be less computationally expensive, or both. researchers
   have identified many ways to make models less computationally
   expensive, for example by using a hierarchical softmax or adding
   projection layers to avoid the large id127s (see also
   [33]here or [34]here). but i want to keep our model simple and go the
   first route: make our implementation run faster using a gpu. before
   doing that though, let   s just try to run sgd with a small dataset and
   check if the loss actually decreases:
   1
   2
   3
   4
   np.random.seed(10)
   # train on a small subset of the data to see what happens
   model = id56numpy(vocabulary_size)
   losses = train_with_sgd(model, x_train[:100], y_train[:100], nepoch=10,
   evaluate_loss_after=1)

2015-09-30 10:08:19: loss after num_examples_seen=0 epoch=0: 8.987425
2015-09-30 10:08:35: loss after num_examples_seen=100 epoch=1: 8.976270
2015-09-30 10:08:50: loss after num_examples_seen=200 epoch=2: 8.960212
2015-09-30 10:09:06: loss after num_examples_seen=300 epoch=3: 8.930430
2015-09-30 10:09:22: loss after num_examples_seen=400 epoch=4: 8.862264
2015-09-30 10:09:38: loss after num_examples_seen=500 epoch=5: 6.913570
2015-09-30 10:09:53: loss after num_examples_seen=600 epoch=6: 6.302493
2015-09-30 10:10:07: loss after num_examples_seen=700 epoch=7: 6.014995
2015-09-30 10:10:24: loss after num_examples_seen=800 epoch=8: 5.833877
2015-09-30 10:10:39: loss after num_examples_seen=900 epoch=9: 5.710718

   good, it seems like our implementation is at least doing something
   useful and decreasing the loss, just like we wanted.

training our network with theano and the gpu

   i have previously written a [35]tutorial on theano, and since all our
   logic will stay exactly the same i won   t go through optimized code here
   again. i defined a id56theano class that replaces the numpy calculations
   with corresponding calculations in theano. just like the rest of this
   post, [36]the code is also available github.
   1
   2
   3
   np.random.seed(10)
   model = id56theano(vocabulary_size)
   %timeit model.sgd_step(x_train[10], y_train[10], 0.005)

   this time, one sgd step takes 70ms on my mac (without gpu) and 23ms on
   a [37]g2.2xlarge amazon ec2 instance with gpu. that   s a 15x improvement
   over our initial implementation and means we can train our model in
   hours/days instead of weeks. there are still a vast number of
   optimizations we could make, but we   re good enough for now.

   to help you avoid spending days training a model i have pre-trained a
   theano model with a hidden layer dimensionality of 50 and a vocabulary
   size of 8000. i trained it for 50 epochs in about 20 hours. the loss
   was was still decreasing and training longer would probably have
   resulted in a better model, but i was running out of time and wanted to
   publish this post. feel free to try it out yourself and trian for
   longer. you can find the model parameters in
   data/trained-model-theano.npz in the github repository and load them
   using the load_model_parameters_theano method:
   1
   2
   3
   4
   5
   6
   from utils import load_model_parameters_theano,
   save_model_parameters_theano

   model = id56theano(vocabulary_size, hidden_dim=50)
   # losses = train_with_sgd(model, x_train, y_train, nepoch=50)
   # save_model_parameters_theano('./data/trained-model-theano.npz',
   model)
   load_model_parameters_theano('./data/trained-model-theano.npz', model)

generating text

   now that we have our model we can ask it to generate new text for us!
   let   s implement a helper function to generate new sentences:
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   def generate_sentence(model):
       # we start the sentence with the start token
       new_sentence = [word_to_index[sentence_start_token]]
       # repeat until we get an end token
       while not new_sentence[-1] == word_to_index[sentence_end_token]:
           next_word_probs = model.forward_propagation(new_sentence)
           sampled_word = word_to_index[unknown_token]
           # we don't want to sample unknown words
           while sampled_word == word_to_index[unknown_token]:
               samples = np.random.multinomial(1, next_word_probs[-1])
               sampled_word = np.argmax(samples)
           new_sentence.append(sampled_word)
       sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]
       return sentence_str

   num_sentences = 10
   senten_min_length = 7

   for i in range(num_sentences):
       sent = []
       # we want long sentences, not sentences with one or two words
       while len(sent) &amp;lt; senten_min_length:
           sent = generate_sentence(model)
       print &quot; &quot;.join(sent)

   a few selected (censored) sentences. i added capitalization.
     * anyway, to the city scene you   re an idiot teenager.
     * what ? ! ! ! ! ignore!
     * screw fitness, you   re saying: https
     * thanks for the advice to keep my thoughts around girls.
     * yep, please disappear with the terrible generation.

   looking at the generated sentences there are a few interesting things
   to note. the model successfully learn syntax. it properly places
   commas (usually before and   s and or   s) and ends sentence with
   punctuation. sometimes it mimics internet speech such as multiple
   exclamation marks or smileys.

   however, the vast majority of generated sentences don   t make sense or
   have grammatical errors (i really picked the best ones above). one
   reason could be that we did not train our network long enough (or
   didn   t use enough training data). that may be true, but it   s most
   likely not the main reason. our vanilla id56 can   t generate meaningful
   text because it   s unable to learn dependencies between words that are
   several steps apart. that   s also why id56s failed to gain popularity
   when they were first invented. they were beautiful in theory but didn   t
   work well in practice, and we didn   t immediately understand why.

   fortunately, the difficulties in training id56s are [38]much better
   understood now. in the next part of this tutorial we will explore the
   id26 through time (bptt) algorithm in more detail and
   demonstrate what   s called the vanishing gradient problem. this will
   motivate our move to more sophisticated id56 models, such as lstms,
   which are the current state of the art for many tasks in nlp (and can
   generate much better reddit comments!). everything you learned in this
   tutorial also applies to lstms and other id56 models, so don   t feel
   discouraged if the results for a vanilla id56 are worse then you
   expected.

   that   s it for now. please leave questions or feedback in the comments!
   and don   t forget to check out the [code 1=   github   
   language=   on   ][39]/code.





   categories[40]deep learning, [41]gpu, [42]id38,
   [43]recurrent neural networks

post navigation

   [44]previous postprevious recurrent neural networks tutorial, part 1    
   introduction to id56s
   [45]next postnext recurrent neural networks tutorial, part 3    
   id26 through time and vanishing gradients

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [46]introduction to learning to trade with id23
     * [47]ai and deep learning in 2017     a year in review
     * [48]hype or not? some perspective on openai   s dota 2 bot
     * [49]learning id23 (with code, exercises and
       solutions)
     * [50]id56s in tensorflow, a practical guide and undocumented features
     * [51]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [52]deep learning for chatbots, part 1     introduction
     * [53]attention and memory in deep learning and nlp

archives

     * [54]february 2018
     * [55]december 2017
     * [56]august 2017
     * [57]october 2016
     * [58]august 2016
     * [59]july 2016
     * [60]april 2016
     * [61]january 2016
     * [62]december 2015
     * [63]november 2015
     * [64]october 2015
     * [65]september 2015

categories

     * [66]conversational agents
     * [67]convolutional neural networks
     * [68]deep learning
     * [69]gpu
     * [70]id38
     * [71]memory
     * [72]neural networks
     * [73]news
     * [74]nlp
     * [75]recurrent neural networks
     * [76]id23
     * [77]id56s
     * [78]tensorflow
     * [79]trading
     * [80]uncategorized

meta

     * [81]log in
     * [82]entries rss
     * [83]comments rss
     * [84]wordpress.org

   [85]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/feed/
   4. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
   5. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/&format=xml
   8. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  16. http://www.wildml.com/author/dennybritz/
  17. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  18. https://github.com/dennybritz/id56-tutorial-id56lm
  19. http://deeplearning.net/software/theano/
  20. https://github.com/dennybritz/id56-tutorial-id56lm/
  21. https://en.wikipedia.org/wiki/language_model
  22. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  23. https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08
  24. http://www.nltk.org/
  25. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  26. http://www.wildml.com/wp-content/uploads/2015/09/id56.jpg
  27. http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
  28. https://en.wikipedia.org/wiki/cross_id178#cross-id178_error_function_and_logistic_regression
  29. http://cs231n.github.io/optimization-1/
  30. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  31. http://colah.github.io/posts/2015-08-backprop/
  32. http://cs231n.github.io/optimization-2/
  33. http://arxiv.org/pdf/1301.3781.pdf
  34. http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf
  35. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
  36. https://github.com/dennybritz/id56-tutorial-id56lm
  37. https://aws.amazon.com/ec2/instance-types/#g2
  38. http://arxiv.org/abs/1211.5063
  39. https://github.com/dennybritz/id56-tutorial-id56lm
  40. http://www.wildml.com/category/deep-learning/
  41. http://www.wildml.com/category/gpu/
  42. http://www.wildml.com/category/language-modeling/
  43. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  44. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  45. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  46. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  47. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  48. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  49. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  50. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  51. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  52. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  53. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  54. http://www.wildml.com/2018/02/
  55. http://www.wildml.com/2017/12/
  56. http://www.wildml.com/2017/08/
  57. http://www.wildml.com/2016/10/
  58. http://www.wildml.com/2016/08/
  59. http://www.wildml.com/2016/07/
  60. http://www.wildml.com/2016/04/
  61. http://www.wildml.com/2016/01/
  62. http://www.wildml.com/2015/12/
  63. http://www.wildml.com/2015/11/
  64. http://www.wildml.com/2015/10/
  65. http://www.wildml.com/2015/09/
  66. http://www.wildml.com/category/conversational-agents/
  67. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  68. http://www.wildml.com/category/deep-learning/
  69. http://www.wildml.com/category/gpu/
  70. http://www.wildml.com/category/language-modeling/
  71. http://www.wildml.com/category/memory/
  72. http://www.wildml.com/category/neural-networks/
  73. http://www.wildml.com/category/news/
  74. http://www.wildml.com/category/nlp/
  75. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  76. http://www.wildml.com/category/reinforcement-learning/
  77. http://www.wildml.com/category/id56s/
  78. http://www.wildml.com/category/tensorflow/
  79. http://www.wildml.com/category/trading/
  80. http://www.wildml.com/category/uncategorized/
  81. http://www.wildml.com/wp-login.php
  82. http://www.wildml.com/feed/
  83. http://www.wildml.com/comments/feed/
  84. https://wordpress.org/
  85. https://wordpress.org/
