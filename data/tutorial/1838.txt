   #[1]wildml    feed [2]wildml    comments feed [3]wildml    recurrent
   neural networks tutorial, part 3     id26 through time and
   vanishing gradients comments feed [4]recurrent neural networks
   tutorial, part 2     implementing a id56 with python, numpy and theano
   [5]recurrent neural network tutorial, part 4     implementing a gru/lstm
   id56 with python and theano [6]alternate [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]october 8, 2015april 1, 2016 by [16]denny britz

recurrent neural networks tutorial, part 3     id26 through time and
vanishing gradients

   this the third part of the [17]recurrent neural network tutorial.

   in the [18]previous part of the tutorial we implemented a id56 from
   scratch, but didn   t go into detail on how id26 through time
   (bptt) algorithms calculates the gradients. in this part we   ll give a
   brief overview of bptt and explain how it differs from traditional
   id26. we will then try to understand the vanishing gradient
   problem, which has led to the development of  lstms and grus, two of
   the currently most popular and powerful models used in nlp (and other
   areas). the vanishing gradient problem was [19]originally discovered
   by sepp hochreiter in 1991 and has been receiving attention again
   recently due to the increased application of deep architectures.

   to fully understand this part of the tutorial i recommend being
   familiar with how partial differentiation and basic id26
   works. if you are not, you can find excellent tutorials [20]here and
   [21]here and [22]here, in order of increasing difficulty.

id26 through time (bptt)

   let   s quickly recap the basic equations of our id56. note that there   s a
   slight change in notation from o to \hat{y} . that   s only to stay
   consistent with some of the literature out there that i am referencing.

   \begin{aligned} s_t &= \tanh(ux_t + ws_{t-1}) \\ \hat{y}_t &=
   \mathrm{softmax}(vs_t) \end{aligned}

   we also defined our loss, or error, to be the cross id178 loss, given
   by:

   \begin{aligned} e_t(y_t, \hat{y}_t) &= - y_{t} \log \hat{y}_{t} \\ e(y,
   \hat{y}) &=\sum\limits_{t} e_t(y_t,\hat{y}_t) \\ & = -\sum\limits_{t}
   y_{t} \log \hat{y}_{t} \end{aligned}

   here, y_t is the correct word at time step t , and \hat{y}_t is our
   prediction. we typically treat the full sequence (sentence) as one
   training example, so the total error is just the sum of the errors at
   each time step (word).

   remember that our goal is to calculate the gradients of the error with
   respect to our parameters u, v and w and then learn good parameters
   using stochastic id119. just like we sum up the errors, we
   also sum up the gradients at each time step for one training example:
   \frac{\partial e}{\partial w} = \sum\limits_{t} \frac{\partial
   e_t}{\partial w} .

   to calculate these gradients we use the chain rule of differentiation.
   that   s the [23]id26 algorithm when applied backwards
   starting from the error. for the rest of this post we   ll use e_3 as an
   example, just to have concrete numbers to work with.

   \begin{aligned} \frac{\partial e_3}{\partial v} &=\frac{\partial
   e_3}{\partial \hat{y}_3}\frac{\partial\hat{y}_3}{\partial v}\\
   &=\frac{\partial e_3}{\partial
   \hat{y}_3}\frac{\partial\hat{y}_3}{\partial z_3}\frac{\partial
   z_3}{\partial v}\\ &=(\hat{y}_3 - y_3) \otimes s_3 \\ \end{aligned}

   in the above, z_3 =vs_3 , and \otimes is the outer product of two
   vectors. don   t worry if you don   t follow the above, i skipped several
   steps and you can try calculating these derivatives yourself (good
   exercise!). the point i   m trying to get across is that \frac{\partial
   e_3}{\partial v} only depends on the values at the current time step,
   \hat{y}_3, y_3, s_3 . if you have these, calculating the gradient for v
   a simple id127.

   but the story is different for \frac{\partial e_3}{\partial w} (and for
   u ). to see why, we write out the chain rule, just as above:

   \begin{aligned} \frac{\partial e_3}{\partial w} &= \frac{\partial
   e_3}{\partial \hat{y}_3}\frac{\partial\hat{y}_3}{\partial
   s_3}\frac{\partial s_3}{\partial w}\\ \end{aligned}

   now, note that s_3 = \tanh(ux_t + ws_2) depends on s_2 , which depends
   on w and s_1 , and so on. so if we take the derivative with respect to
   w we can   t simply treat s_2 as a constant! we need to apply the chain
   rule again and what we really have is this:

   \begin{aligned} \frac{\partial e_3}{\partial w} &=
   \sum\limits_{k=0}^{3} \frac{\partial e_3}{\partial
   \hat{y}_3}\frac{\partial\hat{y}_3}{\partial s_3}\frac{\partial
   s_3}{\partial s_k}\frac{\partial s_k}{\partial w}\\ \end{aligned}

   we sum up the contributions of each time step to the gradient. in other
   words, because w is used in every step up to the output we care about,
   we need to backpropagate gradients from t=3 through the network all the
   way to t=0 :

   [24]unrolled recurrent neural network for 5 time steps with gradients.

   note that this is exactly the same as the standard id26
   algorithm that we use in deep [25]feedforward neural networks. the key
   difference is that we sum up the gradients for w at each time step. in
   a traditional nn we don   t share parameters across layers, so we don   t
   need to sum anything. but in my opinion bptt is just a fancy name for
   standard id26 on an unrolled id56. just like with
   id26 you could define a delta vector that you pass
   backwards, e.g.: \delta_2^{(3)} = \frac{\partial e_3}{\partial z_2}
   =\frac{\partial e_3}{\partial s_3}\frac{\partial s_3}{\partial
   s_2}\frac{\partial s_2}{\partial z_2} with  z_2 = ux_2+ ws_1 . then the
   same equations will apply.

   in code, a naive implementation of bptt looks something like this:
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   def bptt(self, x, y):
       t = len(y)
       # perform forward propagation
       o, s = self.forward_propagation(x)
       # we accumulate the gradients in these variables
       dldu = np.zeros(self.u.shape)
       dldv = np.zeros(self.v.shape)
       dldw = np.zeros(self.w.shape)
       delta_o = o
       delta_o[np.arange(len(y)), y] -= 1.
       # for each output backwards...
       for t in np.arange(t)[::-1]:
           dldv += np.outer(delta_o[t], s[t].t)
           # initial delta calculation: dl/dz
           delta_t = self.v.t.dot(delta_o[t]) * (1 - (s[t] ** 2))
           # id26 through time (for at most self.bptt_truncate
   steps)
           for bptt_step in np.arange(max(0, t-self.bptt_truncate),
   t+1)[::-1]:
               # print &quot;id26 step t=%d bptt step=%d &quot;
   % (t, bptt_step)
               # add to gradients at each previous step
               dldw += np.outer(delta_t, s[bptt_step-1])
               dldu[:,x[bptt_step]] += delta_t
               # update delta for next step dl/dz at t-1
               delta_t = self.w.t.dot(delta_t) * (1 - s[bptt_step-1] ** 2)
       return [dldu, dldv, dldw]

   this should also give you an idea of why standard id56s are hard to
   train: sequences (sentences) can be quite long, perhaps 20 words or
   more, and thus you need to back-propagate through many layers. in
   practice many people truncate the id26 to a few steps.

the vanishing gradient problem

   in [26]previous parts of the tutorial i mentioned that id56s have
   difficulties learning long-range dependencies     interactions between
   words that are several steps apart. that   s problematic because the
   meaning of an english sentence is often determined by words that aren   t
   very close:    the man who wore a wig on his head went inside   . the
   sentence is really about a man going inside, not about the wig. but
   it   s unlikely that a plain id56 would be able capture such information.
   to understand why, let   s take a closer look at the gradient we
   calculated above:

   \begin{aligned} \frac{\partial e_3}{\partial w} &=
   \sum\limits_{k=0}^{3} \frac{\partial e_3}{\partial
   \hat{y}_3}\frac{\partial\hat{y}_3}{\partial s_3}\frac{\partial
   s_3}{\partial s_k}\frac{\partial s_k}{\partial w}\\ \end{aligned}

   note that \frac{\partial s_3}{\partial s_k} is a chain rule in itself!
   for example, \frac{\partial s_3}{\partial s_1} =\frac{\partial
   s_3}{\partial s_2}\frac{\partial s_2}{\partial s_1} . also note that
   because we are taking the derivative of a vector function with respect
   to a vector, the result is a matrix (called the [27]jacobian matrix)
   whose elements are all the pointwise derivatives. we can rewrite the
   above gradient:

   \begin{aligned} \frac{\partial e_3}{\partial w} &=
   \sum\limits_{k=0}^{3} \frac{\partial e_3}{\partial
   \hat{y}_3}\frac{\partial\hat{y}_3}{\partial s_3}
   \left(\prod\limits_{j=k+1}^{3} \frac{\partial s_j}{\partial
   s_{j-1}}\right) \frac{\partial s_k}{\partial w}\\ \end{aligned}

   it turns out (i won   t prove it here but [28]this paper goes into
   detail) that the 2-norm, which you can think of it as an absolute
   value, of the above jacobian matrix has an upper bound of 1. this makes
   intuitive sense because our tanh (or sigmoid) activation function maps
   all values into a range between -1 and 1, and the derivative is bounded
   by 1 (1/4 in the case of sigmoid) as well:
   tanh and derivative. source: http://nn.readthedocs.org/en/rtd/transfer/

   you can see that the \tanh and sigmoid functions have derivatives of 0
   at both ends. they approach a  flat line. when this happens we say the
   corresponding neurons are saturated. they have a zero gradient and
   drive other gradients in previous layers towards 0. thus, with small
   values in the matrix and multiple id127s ( t-k in
   particular) the gradient values are shrinking exponentially fast,
   eventually vanishing completely after a few time steps. gradient
   contributions from    far away    steps become zero, and the state at those
   steps doesn   t contribute to what you are learning: you end up
   not learning long-range dependencies. vanishing gradients aren   t
   exclusive to id56s. they also happen in deep feedforward neural
   networks. it   s just that id56s tend to be very deep (as deep as the
   sentence length in our case), which makes the problem a lot more
   common.

   it is easy to imagine that, depending on our id180 and
   network parameters, we could get exploding instead of vanishing
   gradients if the values of the jacobian matrix are large. indeed,
   that   s called the exploding gradient problem. the reason that vanishing
   gradients have received more attention than exploding gradients is
   two-fold. for one, exploding gradients are obvious. your gradients will
   become nan (not a number) and your program will crash. secondly,
   clipping the gradients at a pre-defined threshold (as discussed in
   [29]this paper) is a very simple and effective solution to exploding
   gradients. vanishing gradients are more problematic because it   s not
   obvious when they occur or how to deal with them.

   fortunately, there are a few ways to combat the vanishing gradient
   problem. proper initialization of the w matrix can reduce the effect of
   vanishing gradients. so can id173. a more preferred solution
   is to use [30]relu instead of tanh or sigmoid id180. the
   relu derivative is a constant of either 0 or 1, so it isn   t as likely
   to suffer from vanishing gradients. an even more popular solution is to
   use long short-term memory (lstm) or gated recurrent unit (gru)
   architectures. lstms were [31]first proposed in 1997 and are the
   perhaps most widely used models in nlp today. grus, [32]first proposed
   in 2014, are simplified versions of lstms. both of these id56
   architectures were explicitly designed to deal with vanishing gradients
   and efficiently learn long-range dependencies. we   ll cover them in the
   next part of this tutorial.

   please leave questions or feedback in the comments!


   categories[33]deep learning, [34]id38, [35]recurrent
   neural networks, [36]id56s

post navigation

   [37]previous postprevious recurrent neural networks tutorial, part 2    
   implementing a id56 with python, numpy and theano
   [38]next postnext recurrent neural network tutorial, part 4    
   implementing a gru/lstm id56 with python and theano

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [39]introduction to learning to trade with id23
     * [40]ai and deep learning in 2017     a year in review
     * [41]hype or not? some perspective on openai   s dota 2 bot
     * [42]learning id23 (with code, exercises and
       solutions)
     * [43]id56s in tensorflow, a practical guide and undocumented features
     * [44]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [45]deep learning for chatbots, part 1     introduction
     * [46]attention and memory in deep learning and nlp

archives

     * [47]february 2018
     * [48]december 2017
     * [49]august 2017
     * [50]october 2016
     * [51]august 2016
     * [52]july 2016
     * [53]april 2016
     * [54]january 2016
     * [55]december 2015
     * [56]november 2015
     * [57]october 2015
     * [58]september 2015

categories

     * [59]conversational agents
     * [60]convolutional neural networks
     * [61]deep learning
     * [62]gpu
     * [63]id38
     * [64]memory
     * [65]neural networks
     * [66]news
     * [67]nlp
     * [68]recurrent neural networks
     * [69]id23
     * [70]id56s
     * [71]tensorflow
     * [72]trading
     * [73]uncategorized

meta

     * [74]log in
     * [75]entries rss
     * [76]comments rss
     * [77]wordpress.org

   [78]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/feed/
   4. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
   5. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/&format=xml
   8. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  16. http://www.wildml.com/author/dennybritz/
  17. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  18. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  19. http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html
  20. http://cs231n.github.io/optimization-2/
  21. http://colah.github.io/posts/2015-08-backprop/
  22. http://neuralnetworksanddeeplearning.com/chap2.html
  23. http://colah.github.io/posts/2015-08-backprop/
  24. http://www.wildml.com/wp-content/uploads/2015/10/id56-bptt-with-gradients.png
  25. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  26. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  27. https://en.wikipedia.org/wiki/jacobian_matrix_and_determinant
  28. http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf
  29. http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf
  30. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  31. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  32. http://arxiv.org/pdf/1406.1078v3.pdf
  33. http://www.wildml.com/category/deep-learning/
  34. http://www.wildml.com/category/language-modeling/
  35. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  36. http://www.wildml.com/category/id56s/
  37. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  38. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  39. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  40. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  41. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  42. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  43. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  44. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  45. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  46. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  47. http://www.wildml.com/2018/02/
  48. http://www.wildml.com/2017/12/
  49. http://www.wildml.com/2017/08/
  50. http://www.wildml.com/2016/10/
  51. http://www.wildml.com/2016/08/
  52. http://www.wildml.com/2016/07/
  53. http://www.wildml.com/2016/04/
  54. http://www.wildml.com/2016/01/
  55. http://www.wildml.com/2015/12/
  56. http://www.wildml.com/2015/11/
  57. http://www.wildml.com/2015/10/
  58. http://www.wildml.com/2015/09/
  59. http://www.wildml.com/category/conversational-agents/
  60. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  61. http://www.wildml.com/category/deep-learning/
  62. http://www.wildml.com/category/gpu/
  63. http://www.wildml.com/category/language-modeling/
  64. http://www.wildml.com/category/memory/
  65. http://www.wildml.com/category/neural-networks/
  66. http://www.wildml.com/category/news/
  67. http://www.wildml.com/category/nlp/
  68. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  69. http://www.wildml.com/category/reinforcement-learning/
  70. http://www.wildml.com/category/id56s/
  71. http://www.wildml.com/category/tensorflow/
  72. http://www.wildml.com/category/trading/
  73. http://www.wildml.com/category/uncategorized/
  74. http://www.wildml.com/wp-login.php
  75. http://www.wildml.com/feed/
  76. http://www.wildml.com/comments/feed/
  77. https://wordpress.org/
  78. https://wordpress.org/
