   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    deep
   learning transcends the bag of words comments feed [5]make beautiful
   interactive data visualizations easily, dec 15 webinar [6]50 useful
   machine learning & prediction apis

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2015    [28]dec    [29]opinions,
   interviews, reports    deep learning transcends the bag of words
   ( [30]15:n40 )

deep learning transcends the bag of words

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]beer, [34]deep learning, [35]generative models, [36]recurrent
   neural networks, [37]zachary lipton

   generative id56s are now widely popular, many modeling text at the
   character level and typically using unsupervised approach. here we show
   how to generate contextually relevant sentences and explain recent work
   that does it successfully.
     __________________________________________________________________

   by [38]zachary chase lipton, ucsd.

                                                            c [39]comments

   deep learning has risen to prominence, both delighting and enraging
   computer scientists, following a number of breakthrough results on
   difficult classification tasks. convolutional neural networks
   demonstrate an unprecedented ability to recognize objects in images. a
   variety of neural networks have similarly revolutionized the field of
   id103. in machine learning parlance, these models are
   typically discriminative. that is, they directly model a relationship
   between some input data, like images, and corresponding labels, like
   "dog" or "cat". given data, they produce classifications. but we also
   want computers to do more: to form sentences, to paint pictures, to
   craft curriculums for students and to devise treatments for the ill. in
   machine learning parlance, we want generative models.

   over the past several months, several clever approaches repurposing
   supervised discriminative deep learning models for data generation have
   swept the web. among them is [40]a technique pioneered by ilya
   sutskever for generating text with recurrent neural networks (id56s)
   using only characters. in this approach, generating text is posed as a
   prediction problem. at training time you learn a model which gives high
   id203 to each observed character conditioned on all preceding
   characters in the string. then, at prediction time, you produce text by
   sampling from these probabilities repeatedly. each time, you pass the
   sampled character to the model as the next input. this method recently
   found a massive audience when [41]popularized in a tutorial by andrej
   karpathy, who modified the approach to use lstm id56s. the tutorial
   demonstrates that character-level id56s can churn out passages that
   uncannily resemble shakespeare, and c-code that uncannily resembles
   linux source.
   [42]beer-char

   what's the point?

   from a perspective of an ai dreamer, this work is obviously exciting.
   by modeling text at the character level, we impart strictly less prior
   knowledge onto the model than when we work with words. these models
   require little if any preprocessing, offering the possibility that they
   could shine even more in domains where prior knowledge is scantly
   available. character based id56s are notable for their generality, in
   contrast to most nlp models: a single network could just as easily
   generate c-code, guitar tablature, english prose, or chinese
   characters. further, these networks demonstrate that id56s can
   simultaneously model sequential relationships at many different time
   scales. they learn to form words, to form sentences, and to form
   paragraphs, without explicitly modeling them as separate tasks.

   in contrast, to train a word-level model, you must first settle on a
   vocabulary size. then you must decide upon a convention to handle words
   that don't reside in your vocabulary. such models lack an elegant way
   to handle misspellings, jargon, or concept drift. further, for a
   word-based generative id56, the size of the output layer scales with the
   size of the vocabulary. because back-propagating through a softmax
   layer is computationally expensive, training a translation model at
   with 60,000 output words [43]requires 4 gpus dedicated to the task.
   then there is a the added complication of handling concept drift. as
   new words are created the model can't simply learn, it requires an
   entirely new architecture.

   on the other hand, until recently, character level id56s have worked in
   an unsupervised fashion. from a practical perspective, when would you
   want to generate a passage that approximates a selection of shakespeare
   selected at random? when would you want unusable source code that
   superficially resembled the coding style of the linux community? to
   make this tech useful, we want to generate contextually appropriate
   text on command. a teacherbot should generate the right text to explain
   the answer to a math problem. a recommender system should generate the
   right text to describe the salient aspects of a product.

   conditional character-level generative id56s

   thus to make useful generative text models, we want to generate text
   conditionally. this has been done successfully with word level models
   by many researchers, including the aforementioned sutskever and
   karpathy (for language translation and image captioning respectively)
   among others. these models typically follow an encoder-decoder
   approach. some conditioned-upon data (say an image) is first encoded by
   the encoder model. this representation is then passed as the initial
   state to a decoder id56, which then functions precisely as a standard
   generative id56. one weakness of this approach is that it is limited in
   the length of the sequences it can decode, as the signal must survive
   from the very first state of the encoder id56 until the final token in
   the generated text. thus it has been much more amenable for training
   word-level models than for character level models.
   [44]encoder-decoder

   capturing meaning in product reviews with character-level generative
   text models

   in a collaboration with [45]sharad vikram and [46]julian mcauley at
   ucsd, [47]i recently posted a manuscript addressing the issue of
   generating long-form text conditionally at the character level.
   specifically, we use a dataset of reviews scraped from the website
   beeradvocate.com, containing over 1.5 million reviews. each review is
   annotated with star ratings as well as product id and category
   information. as an exploratory task, we generate reviews, conditioned
   upon a star rating and category. in other words, our model can, on
   command, generate a 5 star review for a stout, or a 1 star review of an
   lager. in ongoing work, we increase the complexity of the conditioned
   upon information, generating reviews given a user and item. we want to
   compose a review the user would most likely leave for a given beer,
   even if the user has never tried the beer before.

   in the paper, we show that a simple replication and concatenation
   strategy can be used to fuse the conditioned upon information (ratings
   and product categories) with the sequential input (characters in the
   review). we call this approach a generative concatenative network
   (id197). this model can generate product reviews, learning not only to
   form words from scratch, but to use the correct words to describe each
   category of item and to reflect the designated star rating. the model
   can generate reviews spanning hundreds of characters, without
   "forgetting" the input signal. we also exploit, via bayes' rule, the
   equivalence between the conditional generation and predicting the
   auxiliary information, showing that our generative can be run 'in
   reverse' as a classifier, nearly matching the performance of a state of
   the art tf-idf id165 model. the same model can both generate
   convincing reviews and successfully classify reviews by category and
   rating. in a sense, we can jointly learn both to speak, and to
   comprehend. eventually related models could serve as powerful
   primitives for building id71.

   because the model operates at the character level, it learns to be
   robust to misspellings, and to capture the non-linear effects of
   negation on sentiment (depending on the modified adjective). the model
   also learns both to generate and correctly classify strings containing
   slang and proper nouns. in addition to the listing the paper on the
   arxiv, [48]we also produced an online demonstration for conditionally
   generating reviews.
   . the following figure depicts the way auxiliary data (in this case, a
   5-star rating) is fed to the network. [49]ratnet

   example of review:

   the following review was generated on a generative concatenative net
   conditioned to produce a 1-star review for a lager:

     robot-drinking-beer on tap in a pint glass at the brewery to me. a-
     this was a bit of a pale gold color. nothing worth mentioning. i
     could tell what i was supposed to drink. s: skunky and skunky. t -
     it tastes like a soda water with a little bit of corn and bad
     chardonnay. i thought it was a tad better than this normally beer
     that was better than the smell would have been better. no real hop
     flavor. an astringent finish. mouthfeel is light bodied with a slick
     finish. this is probably the worst beer i've ever had. i won't be
     having another bottle at a time but if it was the only beer i was
     skunked, i'd say it was a little better in the bottle.

   beer category classification

   in addition, the model can categorize reviews based on the beer
   category on a character by character basis. for example, it learns by
   the 'u' in budweiser that the review is describing a lager.
   [50]tastes-budweiser-category

   character-based id4

   we're not alone in chasing the possibilities for generating text
   conditionally at the character level. [51]one interesting paper by wang
   ling, isabel trancoso et al., also submitted to the international
   conference on learning representations, introduces an approach for
   performing machine translation at the character level. this model keeps
   much of the machinery of a word level model, basing its approach on an
   attention-based translation model pioneered by bahdanau et al. however,
   unlike previous models, which rely a lookup table of id27s,
   this model learns to compose a representation of words as a function of
   its characters. thus, while the model makes mild assumptions about
   whitespace and punctuation, it makes no assumptions about the input our
   output vocabulary. accordingly, it could conceivably learn both to
   process words it has never seen and generate words it has never seen,
   and does not suffer a memory or computational requirements that
   explicitly depend on the vocabulary size.
   char-word

   other work with character-level and generative nets
   our work and the exciting work by ling and trancoso are but two
   representatives of a growing trend both towards conditional character
   based text generation and, more generally, towards exploring the
   generative capabilities of deep learning models. while this post
   focuses on character level generative models, it's warrants mentioning
   [52]google's inceptionism (deep dreams), which [53]generates images to
   visualize what each feature detector in a convolutional neural network
   is "looking for". these models repurpose convolutional neural nets,
   which are traditionally discriminative models, to generate images. also
   of interest are recent papers on generative adversarial learning, which
   learn to generate images such that they fool a discriminative model
   trained to distinguish between real and fabricated images.

   references
    1. [54]generating text with recurrent neural networks
    2. [55]sequence to sequence learning with neural networks
    3. [56]capturing meaning in product reviews with character level text
       models
    4. [57]beermind demo
    5. [58]character based id4
    6. [59]google deep dreams

   zachary chase lipton [60]zachary chase lipton is a phd student in the
   computer science engineering department at the university of
   california, san diego. funded by the [61]division of biomedical
   informatics, he is interested in both theoretical foundations and
   applications of machine learning. in addition to his work at ucsd, he
   has interned at microsoft research labs and as a machine learning
   scientist at amazon, is a contributing editor at kdnuggets, and has
   signed on as an author at manning publications.
   related:
     * [62]does deep learning come from the devil?
     * [63]metamind competes with ibm watson analytics and microsoft azure
       machine learning
     * [64]deep learning and the triumph of empiricism
     * [65]the myth of model interpretability
     * [66](deep learning   s deep flaws)   s deep flaws
     * [67]data science   s most used, confused, and abused jargon
     __________________________________________________________________

   [68][prv.gif] previous post
   [69]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [70]another 10 free must-read books for machine learning and data
       science
    2. [71]9 must-have skills you need to become a data scientist, updated
    3. [72]who is a typical data scientist in 2019?
    4. [73]the pareto principle for data scientists
    5. [74]what no one will tell you about data science job applications
    6. [75]19 inspiring women in ai, big data, data science, machine
       learning
    7. [76]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [77]id158s optimization using genetic algorithm
       with python
    2. [78]who is a typical data scientist in 2019?
    3. [79]8 reasons why you should get a microsoft azure certification
    4. [80]the pareto principle for data scientists
    5. [81]r vs python for data visualization
    6. [82]how to work in data science, ai, big data
    7. [83]the deep learning toolset     an overview

[84]latest news

     * [85]download your datax guide to ai in marketing
     * [86]kdnuggets offer: save 20% on strata in london
     * [87]training a champion: building deep neural nets for big ...
     * [88]building a recommender system
     * [89]predict age and gender using convolutional neural netwo...
     * [90]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [91]top tweets, mar 27     apr 02: here is a great explanat...
     * [92]odsc east is selling out; odsc india announced
     * [93]accelerate ai and data science career expo, may 4, 2019
     * [94]grow your data career at datasciencego, san diego, sep 27-29
     * [95]getting started with nlp using the pytorch framework
     * [96]how to diy your data science education
     * [97]top 8 data science use cases in gaming
     * [98]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [99]make better data-driven business decisions
     * [100]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [101]two predictive analytics world events in europe this fall
     * [102]7 qualities your big data visualization tools absolutely must
       ...
     * [103]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [104]which face is real?
     * [105]yeshiva university: program director / tenure track faculty
       me...
     * [106]top 10 coding mistakes made by data scientists
     * [107]uber   s case study at paw industry 4.0: machine learning ...
     * [108]xai     a data scientist   s mouthpiece
     * [109]what does gpt-2 think about the ai arms race?
     * [110]openclassrooms: data freelance online course creator
       [telecomm...

   [111]kdnuggets home    [112]news    [113]2015    [114]dec    [115]opinions,
   interviews, reports    deep learning transcends the bag of words
   ( [116]15:n40 )
      2019 kdnuggets. [117]about kdnuggets.  [118]privacy policy.
   [119]terms of service

   [120]subscribe to kdnuggets news
   [121][tw_c48.png] [122]facebook [123]linkedin
   x
   [envelope.png] [124]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2015/12/deep-learning-outgrows-bag-words-recurrent-neural-networks.html/feed
   5. https://www.kdnuggets.com/2015/12/continuum-beautiful-interactive-data-visualizations-webinar.html
   6. https://www.kdnuggets.com/2015/12/machine-learning-data-science-apis.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2015/index.html
  28. https://www.kdnuggets.com/2015/12/index.html
  29. https://www.kdnuggets.com/2015/12/opinions-interviews.html
  30. https://www.kdnuggets.com/2015/n40.html
  31. https://www.kdnuggets.com/2015/12/continuum-beautiful-interactive-data-visualizations-webinar.html
  32. https://www.kdnuggets.com/2015/12/machine-learning-data-science-apis.html
  33. https://www.kdnuggets.com/tag/beer
  34. https://www.kdnuggets.com/tag/deep-learning
  35. https://www.kdnuggets.com/tag/generative-models
  36. https://www.kdnuggets.com/tag/recurrent-neural-networks
  37. https://www.kdnuggets.com/tag/zachary-lipton
  38. https://www.kdnuggets.com/author/zlipton
  39. https://www.kdnuggets.com/2015/12/deep-learning-outgrows-bag-words-recurrent-neural-networks.html#comments
  40. http://www.cs.utoronto.ca/~ilya/pubs/2011/lang-id56.pdf
  41. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  42. https://www.kdnuggets.com/wp-content/uploads/beer-char.png
  43. https://www.kdnuggets.com/2015/12/papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
  44. https://www.kdnuggets.com/wp-content/uploads/encoder-decoder.png
  45. http://www.sharadvikram.com/
  46. http://cseweb.ucsd.edu/~jmcauley/
  47. http://arxiv.org/abs/1511.03683
  48. http://deepx.ucsd.edu/#/home/beermind
  49. https://www.kdnuggets.com/wp-content/uploads/ratnet.png
  50. https://www.kdnuggets.com/wp-content/uploads/tastes-budweiser-category.png
  51. http://arxiv.org/abs/1511.04586
  52. http://googleresearch.blogspot.ca/2015/06/inceptionism-going-deeper-into-neural.html
  53. http://deepdreamgenerator.com/
  54. http://www.cs.utoronto.ca/~ilya/pubs/2011/lang-id56.pdf
  55. https://www.kdnuggets.com/2015/12/papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
  56. http://arxiv.org/abs/1511.03683
  57. http://deepx.ucsd.edu/#/home/beermind
  58. http://arxiv.org/abs/1511.04586
  59. http://googleresearch.blogspot.ca/2015/06/inceptionism-going-deeper-into-neural.html
  60. http://zacklipton.com/
  61. http://healthsciences.ucsd.edu/som/medicine/divisions/dbmi/pages/default.aspx
  62. https://www.kdnuggets.com/2015/10/deep-learning-vapnik-einstein-devil-yandex-conference.html
  63. https://www.kdnuggets.com/2015/01/metamind-ibm-watson-analytics-microsoft-azure-machine-learning.html
  64. https://www.kdnuggets.com/2015/07/deep-learning-triumph-empiricism-over-theoretical-mathematical-guarantees.html
  65. https://www.kdnuggets.com/2015/04/model-interpretability-neural-networks-deep-learning.html
  66. https://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html
  67. https://www.kdnuggets.com/2015/02/data-science-confusing-jargon-abused.html
  68. https://www.kdnuggets.com/2015/12/continuum-beautiful-interactive-data-visualizations-webinar.html
  69. https://www.kdnuggets.com/2015/12/machine-learning-data-science-apis.html
  70. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  71. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  72. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  73. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  74. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  75. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  76. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  77. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  78. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  79. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  80. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  81. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  82. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  83. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  84. https://www.kdnuggets.com/news/index.html
  85. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  86. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  87. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  88. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  89. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  90. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  91. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  92. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  93. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  94. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  95. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  96. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  97. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  98. https://www.kdnuggets.com/2019/n13.html
  99. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
 100. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
 101. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
 102. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
 103. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
 104. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
 105. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
 106. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
 107. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
 108. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
 109. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
 110. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
 111. https://www.kdnuggets.com/
 112. https://www.kdnuggets.com/news/index.html
 113. https://www.kdnuggets.com/2015/index.html
 114. https://www.kdnuggets.com/2015/12/index.html
 115. https://www.kdnuggets.com/2015/12/opinions-interviews.html
 116. https://www.kdnuggets.com/2015/n40.html
 117. https://www.kdnuggets.com/about/index.html
 118. https://www.kdnuggets.com/news/privacy-policy.html
 119. https://www.kdnuggets.com/terms-of-service.html
 120. https://www.kdnuggets.com/news/subscribe.html
 121. https://twitter.com/kdnuggets
 122. https://facebook.com/kdnuggets
 123. https://www.linkedin.com/groups/54257
 124. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 126. https://www.kdnuggets.com/
 127. https://www.kdnuggets.com/
