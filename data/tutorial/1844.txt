   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

deep learning, nlp, and representations

   posted on july 7, 2014

   [5]neural networks, [6]deep learning, [7]representations, [8]nlp,
   [9]id56s

introduction

   in the last few years, deep neural networks have dominated pattern
   recognition. they blew the previous state of the art out of the water
   for many id161 tasks. voice recognition is also moving that
   way.

   but despite the results, we have to wonder    why do they work so well?

   this post reviews some extremely remarkable results in applying deep
   neural networks to natural language processing (nlp). in doing so, i
   hope to make accessible one promising answer as to why deep neural
   networks work. i think it   s a very elegant perspective.

one hidden layer neural networks

   a neural network with a hidden layer has universality: given enough
   hidden units, it can approximate any function. this is a frequently
   quoted     and even more frequently, misunderstood and applied     theorem.

   it   s true, essentially, because the hidden layer can be used as a
   lookup table.

   for simplicity, let   s consider a id88 network. a [10]id88
   is a very simple neuron that fires if it exceeds a certain threshold
   and doesn   t fire if it doesn   t reach that threshold. a id88
   network gets binary (0 and 1) inputs and gives binary outputs.

   note that there are only a finite number of possible inputs. for each
   possible input, we can construct a neuron in the hidden layer that
   fires for that input,[11]^1 and only on that specific input. then we
   can use the connections between that neuron and the output neurons to
   control the output in that specific case. [12]^2

   and so, it   s true that one hidden layer neural networks are universal.
   but there isn   t anything particularly impressive or exciting about
   that. saying that your model can do the same thing as a lookup table
   isn   t a very strong argument for it. it just means it isn   t impossible
   for your model to do the task.

   universality means that a network can fit to any training data you give
   it. it doesn   t mean that it will interpolate to new data points in a
   reasonable way.

   no, universality isn   t an explanation for why neural networks work so
   well. the real reason seems to be something much more subtle    and, to
   understand it, we   ll first need to understand some concrete results.

id27s

   i   d like to start by tracing a particularly interesting strand of deep
   learning research: id27s. in my personal opinion, word
   embeddings are one of the most exciting area of research in deep
   learning at the moment, although they were originally introduced by
   bengio, et al. more than a decade ago.[13]^3 beyond that, i think they
   are one of the best places to gain intuition about why deep learning is
   so effective.

   a id27 \(w: \mathrm{words} \to \mathbb{r}^n\) is a
   paramaterized function mapping words in some language to
   high-dimensional vectors (perhaps 200 to 500 dimensions). for example,
   we might find:

   \[w(``\text{cat}\!") = (0.2,~ \text{-}0.4,~ 0.7,~ ...)\]

   \[w(``\text{mat}\!") = (0.0,~ 0.6,~ \text{-}0.1,~ ...)\]

   (typically, the function is a lookup table, parameterized by a matrix,
   \(\theta\), with a row for each word: \(w_\theta(w_n) = \theta_n\).)

   \(w\) is initialized to have random vectors for each word. it learns to
   have meaningful vectors in order to perform some task.

   for example, one task we might train a network for is predicting
   whether a 5-gram (sequence of five words) is    valid.    we can easily get
   lots of 5-grams from wikipedia (eg.    cat sat on the mat   ) and then
      break    half of them by switching a word with a random word (eg.    cat
   sat song the mat   ), since that will almost certainly make our 5-gram
   nonsensical.
   modular network to determine if a 5-gram is    valid    (from [14]bottou
   (2011))

   the model we train will run each word in the 5-gram through \(w\) to
   get a vector representing it and feed those into another    module   
   called \(r\) which tries to predict if the 5-gram is    valid    or
      broken.    then, we   d like:

   \[r(w(``\text{cat}\!"),~ w(``\text{sat}\!"),~ w(``\text{on}\!"),~
   w(``\text{the}\!"),~ w(``\text{mat}\!")) = 1\]

   \[r(w(``\text{cat}\!"),~ w(``\text{sat}\!"),~ w(``\text{song}\!"),~
   w(``\text{the}\!"),~ w(``\text{mat}\!")) = 0\]

   in order to predict these values accurately, the network needs to learn
   good parameters for both \(w\) and \(r\).

   now, this task isn   t terribly interesting. maybe it could be helpful in
   detecting grammatical errors in text or something. but what is
   extremely interesting is \(w\).

   (in fact, to us, the entire point of the task is to learn \(w\). we
   could have done several other tasks     another common one is predicting
   the next word in the sentence. but we don   t really care. in the
   remainder of this section we will talk about many id27
   results and won   t distinguish between different approaches.)

   one thing we can do to get a feel for the id27 space is to
   visualize them with [15]id167, a sophisticated technique for
   visualizing high-dimensional data.
   id167 visualizations of id27s. left: number region; right:
   jobs region. from [16]turian et al. (2010), see [17]complete image.

   this kind of    map    of words makes a lot of intuitive sense to us.
   similar words are close together. another way to get at this is to look
   at which words are closest in the embedding to a given word. again, the
   words tend to be quite similar.
   what words have embeddings closest to a given word? from [18]collobert
   et al. (2011)

   it seems natural for a network to make words with similar meanings have
   similar vectors. if you switch a word for a synonym (eg.    a few people
   sing well    \(\to\)    a couple people sing well   ), the validity of the
   sentence doesn   t change. while, from a naive perspective, the input
   sentence has changed a lot, if \(w\) maps synonyms (like    few    and
      couple   ) close together, from \(r\)   s perspective little changes.

   this is very powerful. the number of possible 5-grams is massive and we
   have a comparatively small number of data points to try to learn from.
   similar words being close together allows us to generalize from one
   sentence to a class of similar sentences. this doesn   t just mean
   switching a word for a synonym, but also switching a word for a word in
   a similar class (eg.    the wall is blue    \(\to\)    the wall is red   ).
   further, we can change multiple words (eg.    the wall is blue    \(\to\)
      the ceiling is red   ). the impact of this is exponential with respect
   to the number of words.[19]^4

   so, clearly this is a very useful thing for \(w\) to do. but how does
   it learn to do this? it seems quite likely that there are lots of
   situations where it has seen a sentence like    the wall is blue    and
   know that it is valid before it sees a sentence like    the wall is red   .
   as such, shifting    red    a bit closer to    blue    makes the network
   perform better.

   we still need to see examples of every word being used, but the
   analogies allow us to generalize to new combinations of words. you   ve
   seen all the words that you understand before, but you haven   t seen all
   the sentences that you understand before. so too with neural networks.
   from [20]mikolov et al. (2013a)

   id27s exhibit an even more remarkable property: analogies
   between words seem to be encoded in the difference vectors between
   words. for example, there seems to be a constant male-female difference
   vector:

   \[w(``\text{woman}\!") - w(``\text{man}\!") ~\simeq~
   w(``\text{aunt}\!") - w(``\text{uncle}\!")\] \[w(``\text{woman}\!") -
   w(``\text{man}\!") ~\simeq~ w(``\text{queen}\!") -
   w(``\text{king}\!")\]

   this may not seem too surprising. after all, gender pronouns mean that
   switching a word can make a sentence grammatically incorrect. you
   write,    she is the aunt    but    he is the uncle.    similarly,    he is the
   king    but    she is the queen.    if one sees    she is the uncle,    the most
   likely explanation is a grammatical error. if words are being randomly
   switched half the time, it seems pretty likely that happened here.

      of course!    we say with hindsight,    the id27 will learn to
   encode gender in a consistent way. in fact, there   s probably a gender
   dimension. same thing for singular vs plural. it   s easy to find these
   trivial relationships!   

   it turns out, though, that much more sophisticated relationships are
   also encoded in this way. it seems almost miraculous!
   relationship pairs in a id27. from [21]mikolov et al.
   (2013b).

   it   s important to appreciate that all of these properties of \(w\) are
   side effects. we didn   t try to have similar words be close together. we
   didn   t try to have analogies encoded with difference vectors. all we
   tried to do was perform a simple task, like predicting whether a
   sentence was valid. these properties more or less popped out of the
   optimization process.

   this seems to be a great strength of neural networks: they learn better
   ways to represent data, automatically. representing data well, in turn,
   seems to be essential to success at many machine learning problems.
   id27s are just a particularly striking example of learning a
   representation.

shared representations

   the properties of id27s are certainly interesting, but can we
   do anything useful with them? besides predicting silly things, like
   whether a 5-gram is    valid   ?
   \(w\) and \(f\) learn to perform task a. later, \(g\) can learn to
   perform b based on \(w\).

   we learned the id27 in order to do well on a simple task, but
   based on the nice properties we   ve observed in id27s, you may
   suspect that they could be generally useful in nlp tasks. in fact, word
   representations like these are extremely important:

     the use of word representations    has become a key    secret sauce    for
     the success of many nlp systems in recent years, across tasks
     including id39, part-of-speech tagging, parsing,
     and id14. ([22]luong et al. (2013))

   this general tactic     learning a good representation on a task a and
   then using it on a task b     is one of the major tricks in the deep
   learning toolbox. it goes by different names depending on the details:
   pretraining, id21, and id72. one of the
   great strengths of this approach is that it allows the representation
   to learn from more than one kind of data.

   there   s a counterpart to this trick. instead of learning a way to
   represent one kind of data and using it to perform multiple kinds of
   tasks, we can learn a way to map multiple kinds of data into a single
   representation!

   one nice example of this is a bilingual word-embedding, produced in
   [23]socher et al. (2013a). we can learn to embed words from two
   different languages in a single, shared space. in this case, we learn
   to embed english and mandarin chinese words in the same space.

   we train two id27s, \(w_{en}\) and \(w_{zh}\) in a manner
   similar to how we did above. however, we know that certain english
   words and chinese words have similar meanings. so, we optimize for an
   additional property: words that we know are close translations should
   be close together.

   of course, we observe that the words we knew had similar meanings end
   up close together. since we optimized for that, it   s not surprising.
   more interesting is that words we didn   t know were translations end up
   close together.

   in light of our previous experiences with id27s, this may not
   seem too surprising. id27s pull similar words together, so if
   an english and chinese word we know to mean similar things are near
   each other, their synonyms will also end up near each other. we also
   know that things like gender differences tend to end up being
   represented with a constant difference vector. it seems like forcing
   enough points to line up should force these difference vectors to be
   the same in both the english and chinese embeddings. a result of this
   would be that if we know that two male versions of words translate to
   each other, we should also get the female words to translate to each
   other.

   intuitively, it feels a bit like the two languages have a similar
      shape    and that by forcing them to line up at different points, they
   overlap and other points get pulled into the right positions.
   id167 visualization of the bilingual id27. green is chinese,
   yellow is english. ([24]socher et al. (2013a))

   in bilingual id27s, we learn a shared representation for two
   very similar kinds of data. but we can also learn to embed very
   different kinds of data in the same space.

   recently, deep learning has begun exploring models that embed images
   and words in a single representation.[25]^5

   the basic idea is that one classifies images by outputting a vector in
   a id27. images of dogs are mapped near the    dog    word vector.
   images of horses are mapped near the    horse    vector. images of
   automobiles near the    automobile    vector. and so on.

   the interesting part is what happens when you test the model on new
   classes of images. for example, if the model wasn   t trained to classify
   cats     that is, to map them near the    cat    vector     what happens when
   we try to classify images of cats?
   ([26]socher et al. (2013b))

   it turns out that the network is able to handle these new classes of
   images quite reasonably. images of cats aren   t mapped to random points
   in the id27 space. instead, they tend to be mapped to the
   general vicinity of the    dog    vector, and, in fact, close to the    cat   
   vector. similarly, the truck images end up relatively close to the
      truck    vector, which is near the related    automobile    vector.
   ([27]socher et al. (2013b))

   this was done by members of the stanford group with only 8 known
   classes (and 2 unknown classes). the results are already quite
   impressive. but with so few known classes, there are very few points to
   interpolate the relationship between images and semantic space off of.

   the google group did a much larger version     instead of 8 categories,
   they used 1,000     around the same time ([28]frome et al. (2013)) and
   has followed up with a new variation ([29]norouzi et al. (2014)). both
   are based on a very powerful image classification model (from
   [30]krizehvsky et al. (2012)), but embed images into the id27
   space in different ways.

   the results are impressive. while they may not get images of unknown
   classes to the precise vector representing that class, they are able to
   get to the right neighborhood. so, if you ask it to classify images of
   unknown classes and the classes are fairly different, it can
   distinguish between the different classes.

   even though i   ve never seen a aesculapian snake or an armadillo before,
   if you show me a picture of one and a picture of the other, i can tell
   you which is which because i have a general idea of what sort of animal
   is associated with each word. these networks can accomplish the same
   thing.

   (these results all exploit a sort of    these words are similar   
   reasoning. but it seems like much stronger results should be possible
   based on relationships between words. in our id27 space,
   there is a consistent difference vector between male and female version
   of words. similarly, in image space, there are consistent features
   distinguishing between male and female. beards, mustaches, and baldness
   are all strong, highly visible indicators of being male. breasts and,
   less reliably, long hair, makeup and jewelery, are obvious indicators
   of being female.[31]^6 even if you   ve never seen a king before, if the
   queen, determined to be such by the presence of a crown, suddenly has a
   beard, it   s pretty reasonable to give the male version.)

   shared embeddings are an extremely exciting area of research and drive
   at why the representation focused perspective of deep learning is so
   compelling.

id56s

   we began our discussion of id27s with the following network:
   modular network that learns id27s (from [32]bottou (2011))

   the above diagram represents a modular network, \(r(w(w_1),~ w(w_2),~
   w(w_3),~ w(w_4),~ w(w_5))\). it is built from two modules, \(w\) and
   \(r\). this approach, of building neural networks from smaller neural
   network    modules    that can be composed together, is not very wide
   spread. it has, however, been very successful in nlp.

   models like the above are powerful, but they have an unfortunate
   limitation: they can only have a fixed number of inputs.

   we can overcome this by adding an association module, \(a\), which will
   take two word or phrase representations and merge them.
   (from [33]bottou (2011))

   by merging sequences of words, \(a\) takes us from representing words
   to representing phrases or even representing whole sentences! and
   because we can merge together different numbers of words, we don   t have
   to have a fixed number of inputs.

   it doesn   t necessarily make sense to merge together words in a sentence
   linearly. if one considers the phrase    the cat sat on the mat   , it can
   naturally be bracketed into segments:    ((the cat) (sat (on (the
   mat))))   . we can apply \(a\) based on this bracketing:
   (from [34]bottou (2011))

   these models are often called    id56s    because one
   often has the output of a module go into a module of the same type.
   they are also sometimes called    tree-structured neural networks.   

   id56s have had significant successes in a number of
   nlp tasks. for example, [35]socher et al. (2013c) uses a recursive
   neural network to predict sentence sentiment:
   (from [36]socher et al. (2013c))

   one major goal has been to create a reversible sentence representation,
   a representation that one can reconstruct an actual sentence from, with
   roughly the same meaning. for example, we can try to introduce a
   disassociation module, \(d\), that tries to undo \(a\):
   (from [37]bottou (2011))

   if we could accomplish such a thing, it would be an extremely powerful
   tool. for example, we could try to make a bilingual sentence
   representation and use it for translation.

   unfortunately, this turns out to be very difficult. very very
   difficult. and given the tremendous promise, there are lots of people
   working on it.

   recently, [38]cho et al. (2014) have made some progress on representing
   phrases, with a model that can encode english phrases and decode them
   in french. look at the phrase representations it learns!
   small section of the id167 of the phrase representation
   (from [39]cho et al. (2014))

criticisms

   i   ve heard some of the results reviewed above criticized by researchers
   in other fields, in particular, in nlp and linguistics. the concerns
   are not with the results themselves, but the conclusions drawn from
   them, and how they compare to other techniques.

   i don   t feel qualified to articulate these concerns. i   d encourage
   someone who feels this way to describe the concerns in the comments.

conclusion

   the representation perspective of deep learning is a powerful view that
   seems to answer why deep neural networks are so effective. beyond that,
   i think there   s something extremely beautiful about it: why are neural
   networks effective? because better ways of representing data can pop
   out of optimizing layered models.

   deep learning is a very young field, where theories aren   t strongly
   established and views quickly change. that said, it is my impression
   that the representation-focused perspective of neural networks is
   presently very popular.

   this post reviews a lot of research results i find very exciting, but
   my main motivation is to set the stage for a future post exploring
   connections between deep learning, type theory and functional
   programming. if you   re interested, you can subscribe to my [40]rss feed
   so that you   ll see it when it is published.

   (i would be delighted to hear your comments and thoughts: you can
   comment inline or at the end. for typos, technical errors, or
   clarifications you would like to see added, you are encouraged to make
   a pull request on [41]github)

acknowledgments

   i   m grateful to eliana lorch, yoshua bengio, michael nielsen, laura
   ball, rob gilson, and jacob steinhardt for their comments and support.
     __________________________________________________________________

    1. constructing a case for every possible input requires \(2^n\)
       hidden neurons, when you have \(n\) input neurons. in reality, the
       situation isn   t usually that bad. you can have cases that encompass
       multiple inputs. and you can have overlapping cases that add
       together to achieve the right input on their intersection.[42]   
    2. (it isn   t only id88 networks that have universality. networks
       of sigmoid neurons (and other id180) are also
       universal: give enough hidden neurons, they can approximate any
       continuous function arbitrarily well. seeing this is significantly
       trickier because you can   t just isolate inputs.)[43]   
    3. id27s were originally developed in ([44]bengio et al,
       2001; [45]bengio et al, 2003), a few years before the 2006 deep
       learning renewal, at a time when neural networks were out of
       fashion. the idea of distributed representations for symbols is
       even older, e.g. ([46]hinton 1986)."[47]   
    4. the seminal paper, [48]a neural probabilistic language model
       (bengio, et al. 2003) has a great deal of insight about why word
       embeddings are powerful.[49]   
    5. previous work has been done modeling the joint distributions of
       tags and images, but it took a very different perspective.[50]   
    6. i   m very conscious that physical indicators of gender can be
       misleading. i don   t mean to imply, for example, that everyone who
       is bald is male or everyone who has breasts is female. just that
       these often indicate such, and greatly adjust our prior.[51]   

   subscribe to the [52]rss feed. built by [53]oinkina with [54]hakyll
   using [55]bootstrap, [56]mathjax, and [57]disqus.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/tags/neural_networks.html
   6. http://colah.github.io/posts/tags/deep_learning.html
   7. http://colah.github.io/posts/tags/representations.html
   8. http://colah.github.io/posts/tags/nlp.html
   9. http://colah.github.io/posts/tags/recursive_neural_networks.html
  10. http://en.wikipedia.org/wiki/id88
  11. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fn1
  12. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fn2
  13. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fn3
  14. http://arxiv.org/pdf/1102.1808v3.pdf
  15. http://homepage.tudelft.nl/19j49/id167.html
  16. http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf
  17. http://metaoptimize.s3.amazonaws.com/cw-embeddings-acl2010/embeddings-mostcommon.embedding_size=50.png
  18. http://arxiv.org/pdf/1103.0398v1.pdf
  19. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fn4
  20. https://www.aclweb.org/anthology/n/n13/n13-1090.pdf
  21. http://arxiv.org/pdf/1301.3781.pdf
  22. http://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf
  23. http://ai.stanford.edu/~wzou/emnlp2013_zousochercermanning.pdf
  24. http://ai.stanford.edu/~wzou/emnlp2013_zousochercermanning.pdf
  25. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fn5
  26. http://nlp.stanford.edu/~socherr/socherganjoomanningng_nips2013.pdf
  27. http://nlp.stanford.edu/~socherr/socherganjoomanningng_nips2013.pdf
  28. http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf
  29. http://arxiv.org/pdf/1312.5650.pdf
  30. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  31. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fn6
  32. http://arxiv.org/pdf/1102.1808v3.pdf
  33. http://arxiv.org/pdf/1102.1808v3.pdf
  34. http://arxiv.org/pdf/1102.1808v3.pdf
  35. http://nlp.stanford.edu/~socherr/emnlp2013_rntn.pdf
  36. http://nlp.stanford.edu/~socherr/emnlp2013_rntn.pdf
  37. http://arxiv.org/pdf/1102.1808v3.pdf
  38. http://arxiv.org/pdf/1406.1078v1.pdf
  39. http://arxiv.org/pdf/1406.1078v1.pdf
  40. http://colah.github.io/rss.xml
  41. https://github.com/colah/nlp-id56s-representations-post
  42. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fnref1
  43. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fnref2
  44. http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/64
  45. http://machinelearning.wustl.edu/mlpapers/paper_files/bengiodvj03.pdf
  46. http://www.cogsci.ucsd.edu/~ajyu/teaching/cogs202_sp13/readings/hinton86.pdf
  47. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fnref3
  48. http://machinelearning.wustl.edu/mlpapers/paper_files/bengiodvj03.pdf
  49. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fnref4
  50. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fnref5
  51. http://colah.github.io/posts/2014-07-nlp-id56s-representations/#fnref6
  52. http://colah.github.io/rss.xml
  53. https://github.com/oinkina
  54. http://jaspervdj.be/hakyll
  55. http://getbootstrap.com/
  56. http://www.mathjax.org/
  57. http://disqus.com/
