   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   introduction to recurrent networks in tensorflow comments feed
   [5]intel   s investments in cognitive tech: impact and new opportunities
   [6]usc: data scientist

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]may    [29]tutorials,
   overviews    introduction to recurrent networks in tensorflow
   ( [30]16:n20 )

introduction to recurrent networks in tensorflow

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 115
   tags: [33]deep learning, [34]recurrent neural networks, [35]tensorflow

   a straightforward, introductory overview of implementing recurrent
   neural networks in tensorflow.
     __________________________________________________________________

   by danijar hafner, independent machine learning researcher.

   tensorflow

   recurrent networks like lstm and gru are powerful sequence models. i
   will explain how to create recurrent networks in tensorflow and use
   them for sequence classification and sequence labelling tasks. if you
   are not familiar with recurrent networks, i suggest you take a look at
   christopher olah   s [36]great post first. on the tensorflow part, i also
   expect some basic knowledge. the [37]official tutorials are a good
   place to start.
from tensorflow.models.id56 import id56_cell

num_hidden = 200
num_layers = 3
dropout = tf.placeholder(tf.float32)

network = id56_cell.grucell(num_hidden)  # or lstmcell(num_hidden)
network = id56_cell.dropoutwrapper(network, output_keep_prob=dropout)
network = id56_cell.multiid56cell([network] * num_layers)

defining the network


   to use recurrent networks in tensorflow we first need to define the
   network architecture consisting of one or more layers, the cell type
   and possibly dropout between the layers.

unrolling in time


   we can now unroll this network in time using the id56 operation. this
   takes placeholders for the input at each timestep and returns the
   hidden states and output activations for each timestep.
from tensorflow.models.id56 import id56
max_length = 100

# batch size times time steps times data width.
data = tf.placeholder(tf.float32, [none, max_length, 28])
outputs, states = id56.id56(network, unpack_sequence(data), dtype=tf.float32)
output = pack_sequence(outputs)
state = pack_sequence(states)

   tensorflow uses python lists of one tensor for each timestep for the
   interface. thus we make use of[38]tf.pack() and tf.unpack() to split
   our data tensors into lists of frames and merge the results back to a
   single tensor.
def unpack_sequence(tensor):
    """split the single tensor of a sequence into a list of frames."""
    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))

def pack_sequence(sequence):
    """combine a list of the frames into a single tensor of the sequence."""
    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])

   as of version v0.8.0, tensorflow provides id56.dynamic_id56 as an
   alternative to id56.id56 that does not actually unroll the compute graph
   but uses a loop graph operation. the interface is the same except that
   you don   t need unpack_sequence() and pack_sequence() anymore, it
   already operates on single tensors. in the following sections, i will
   mention the modifications you need to make in order to use dynamic_id56.

sequence classification


   for classification, you might only care about the output activation at
   the last timestep, which is justoutputs[-1]. the code below adds a
   softmax classifier ontop of that and defines the cross id178 error
   function. for now we assume sequences to be equal in length but i will
   cover variable length sequences in another post.
in_size = num_hidden
out_size = int(target.get_shape()[2])
weight = tf.variable(tf.truncated_normal([in_size, out_size], stddev=0.1))
bias = tf.variable(tf.constant(0.1, shape=[out_size]))
prediction = tf.nn.softmax(tf.matmul(outputs[-1], weight) + bias)
cross_id178 = -tf.reduce_sum(target * tf.log(prediction))

   when using dynamic_id56, this is how to get the last output of the
   recurrent networks. we can   t useoutputs[-1] because unlike python
   lists, tensorflow doesn   t support negative indexing yet. here is
   the[39]complete gist for sequence classification.
output, _ = id56.dynamic_id56(network, data, dtype=tf.float32)
output = tf.transpose(output, [1, 0, 2])
last = tf.gather(output, int(output.get_shape()[0]) - 1)

sequence labelling


   for sequence labelling, we want a prediction for each timestamp.
   however, we share the weights for the softmax layer across all
   timesteps. this way, we have one softmax layer ontop of an unrolled
   recurrent network as desired.
in_size = num_hidden
out_size = int(target.get_shape()[2])
weight = tf.variable(tf.truncated_normal([in_size, out_size], stddev=0.1))
bias = tf.variable(tf.constant(0.1, shape=[out_size]))
predictions = [tf.nn.softmax(tf.matmul(x, weight) + bias) for x in outputs]
prediction = pack_sequence(predictions)

   if you want to use dynamic_id56 instead, you cannot apply the same
   weights and biases to all time steps in a python list comprehension.
   instead, we must flatten the outputs of each time step. this way time
   steps look the same as examples in the trainng batch to the weight
   matrix. afterwards, we reshape back to the desired shape.
max_length = int(self.target.get_shape()[1])
num_classes = int(self.target.get_shape()[2])
weight, bias = self._weight_and_bias(self._num_hidden, num_classes)
output = tf.reshape(output, [-1, self._num_hidden])
prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)
prediction = tf.reshape(prediction, [-1, max_length, num_classes])

   since this is a classification task as well, we keep using cross
   id178 as our error function. here we have a prediction and target for
   every timestep. we thus compute the cross id178 for every timestep
   first and then average. here is the [40]complete gist for sequence
   labelling.
cross_id178 = -tf.reduce_sum(
    target * tf.log(prediction), reduction_indices=[1])
cross_id178 = tf.reduce_mean(cross_id178)

   that   s all. we learned how to construct recurrent networks in
   tensorflow and use them for sequence learning tasks. please ask any
   questions below if you couldn   t follow.

   bio: [41]danijar hafner is a python and c++ developer from berlin
   interested in machine intelligence research. he recently released a
   [42]neural networks library, but he likes creating new things in
   general.

   [43]original. reposted with permission.

   related:
     * [44]recurrent neural networks tutorial, introduction
     * [45]learning to code neural networks
     * [46]implementing neural networks in javascript
     __________________________________________________________________

   [47][prv.gif] previous post
   [48]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [49]another 10 free must-read books for machine learning and data
       science
    2. [50]9 must-have skills you need to become a data scientist, updated
    3. [51]who is a typical data scientist in 2019?
    4. [52]the pareto principle for data scientists
    5. [53]what no one will tell you about data science job applications
    6. [54]19 inspiring women in ai, big data, data science, machine
       learning
    7. [55]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [56]id158s optimization using genetic algorithm
       with python
    2. [57]who is a typical data scientist in 2019?
    3. [58]8 reasons why you should get a microsoft azure certification
    4. [59]the pareto principle for data scientists
    5. [60]r vs python for data visualization
    6. [61]how to work in data science, ai, big data
    7. [62]the deep learning toolset     an overview

[63]latest news

     * [64]download your datax guide to ai in marketing
     * [65]kdnuggets offer: save 20% on strata in london
     * [66]training a champion: building deep neural nets for big ...
     * [67]building a recommender system
     * [68]predict age and gender using convolutional neural netwo...
     * [69]top tweets, mar 27     apr 02: here is a great ex...

   [70]kdnuggets home    [71]news    [72]2016    [73]may    [74]tutorials,
   overviews    introduction to recurrent networks in tensorflow
   ( [75]16:n20 )
      2019 kdnuggets. [76]about kdnuggets.  [77]privacy policy. [78]terms
   of service

   [79]subscribe to kdnuggets news
   [80][tw_c48.png] [81]facebook [82]linkedin
   x

   [envelope.png] [83]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/05/intro-recurrent-networks-tensorflow.html/feed
   5. https://www.kdnuggets.com/2016/05/intel-investment-cognitive-tech-impact-new-opportunities.html
   6. https://www.kdnuggets.com/jobs/16/05-31-jobs-data-scientist.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/05/index.html
  29. https://www.kdnuggets.com/2016/05/tutorials.html
  30. https://www.kdnuggets.com/2016/n20.html
  31. https://www.kdnuggets.com/2016/05/intel-investment-cognitive-tech-impact-new-opportunities.html
  32. https://www.kdnuggets.com/jobs/16/05-31-jobs-data-scientist.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/recurrent-neural-networks
  35. https://www.kdnuggets.com/tag/tensorflow
  36. http://colah.github.io/posts/2015-08-understanding-lstms/
  37. https://www.tensorflow.org/versions/r0.8/tutorials/index.html
  38. https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pack
  39. https://gist.github.com/danijar/c7ec9a30052127c7a1ad169eeb83f159
  40. https://gist.github.com/danijar/61f9226f7ea498abce36187ddaf51ed5
  41. https://danijar.com/
  42. https://github.com/danijar/layered
  43. http://danijar.com/introduction-to-recurrent-networks-in-tensorflow/
  44. https://www.kdnuggets.com/2015/10/recurrent-neural-networks-tutorial.html
  45. https://www.kdnuggets.com/2016/01/learning-to-code-neural-networks.html
  46. https://www.kdnuggets.com/2016/05/implementing-neural-networks-javascript.html
  47. https://www.kdnuggets.com/2016/05/intel-investment-cognitive-tech-impact-new-opportunities.html
  48. https://www.kdnuggets.com/jobs/16/05-31-jobs-data-scientist.html
  49. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  50. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  51. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  52. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  53. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  54. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  55. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  56. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  57. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  58. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  59. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  60. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  61. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  62. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  63. https://www.kdnuggets.com/news/index.html
  64. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  65. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  66. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  67. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  68. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  69. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  70. https://www.kdnuggets.com/
  71. https://www.kdnuggets.com/news/index.html
  72. https://www.kdnuggets.com/2016/index.html
  73. https://www.kdnuggets.com/2016/05/index.html
  74. https://www.kdnuggets.com/2016/05/tutorials.html
  75. https://www.kdnuggets.com/2016/n20.html
  76. https://www.kdnuggets.com/about/index.html
  77. https://www.kdnuggets.com/news/privacy-policy.html
  78. https://www.kdnuggets.com/terms-of-service.html
  79. https://www.kdnuggets.com/news/subscribe.html
  80. https://twitter.com/kdnuggets
  81. https://facebook.com/kdnuggets
  82. https://www.linkedin.com/groups/54257
  83. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  85. https://www.kdnuggets.com/
  86. https://www.kdnuggets.com/
