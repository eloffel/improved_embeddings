   #[1]github [2]recent commits to zoph_id56:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]43
     * [35]star [36]145
     * [37]fork [38]57

[39]isi-nlp/[40]zoph_id56

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   c++/cuda toolkit for training sequence and sequence-to-sequence models
   across multiple gpus
     * [47]80 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors

    1. [51]c++ 90.6%
    2. [52]cuda 5.7%
    3. [53]python 2.7%
    4. other 1.0%

   (button) c++ cuda python other
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/i
   [55]download zip

downloading...

   want to be notified of new releases in isi-nlp/zoph_id56?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@shixing
   [63]shixing [64]merge pull request [65]#8 [66]from shixing/master
   (button)    
add 'words_ensemble' model

   latest commit [67]1e3e7da may 15, 2017
   [68]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [69]executable [70]merge remote-tracking branch 'isi-nlp/master' may
   15, 2017
   [71]sample_data
   [72]scripts
   [73]src [74]merge remote-tracking branch 'isi-nlp/master' may 15, 2017
   [75].gitattributes [76]add readme_xing.md for lsh and fsa support may
   5, 2017
   [77].gitignore
   [78]readme.md
   [79]readme_xing.md

readme.md

zoph_id56: a c++/cuda toolkit for training sequence and sequence-to-sequence
models across multiple gpus

   this is [80]barret zoph's code for zoph_id56
   send any questions or comments to [81]barretzoph@gmail.com

   this toolkit can successfully replicate the results from the following
   papers (the multi-gpu parallelism, which is explained in the tutorial,
   is similar to 6)
    1. [82]multi-source neural translation
    2. [83]simple, fast noise contrastive estimation for large id56
       vocabularies
    3. [84]id21 for low-resource id4
    4. [85]effective approaches to attention-based neural machine
       translation
    5. [86]addressing the rare word problem in id4
    6. [87]sequence to sequence learning with neural networks
    7. [88]recurrent neural network id173

instructions for compilation/using the code

   the code for zoph_id56 is provided in the src/ directory. additionally,
   a precompiled binary (named zoph_id56) is provided that will work on 64
   bit linux for cuda 7.5, so it is not necessary to compile the code.

   if you just want to use the executable, then run the following command
   cat executable/zoph_id56_1 executable/zoph_id56_2 executable/zoph_id56_3
   executable/zoph_id56_4 > zoph_id56. then zoph_id56 will be the executable
   that you can use. to run the executable you need to be sure your path
   variable includes the location to cuda. this is a sample command of
   putting cuda into your path variable export
   path=/usr/cuda/7.5/bin:$path

   if you want to compile the zoph_id56 code run bash scripts/compile.sh,
   which will compile the code given you set a few environmental
   variables. the variables that need to be set are below:
    1. path_to_cuda_include (example value: /usr/cuda/7.5/include/ )
    2. path_to_boost_include (example value: /usr/boost/1.55.0/include/ )
    3. path_to_cuda_lib_64 (example value: /usr/cuda/7.5/lib64/ )
    4. path_to_boost_lib (example value: /usr/boost/1.55.0/lib/ )
    5. path_to_cudnn_v4_64 (example value: /usr/cudnn_v4/lib64/ )
    6. path_to_eigen (example value: /usr/eigen/ )
    7. path_to_cudnn_include (example value: /usr/cudnn_v4/include/ )

the acceptable versions for the libraries above

   note that cuda version greater than 7.0 is required to run the code,
   while the rest are required to compile the code
     * cuda version greater than 7.0
     * gcc version greater than 4.8.1, but not greater than 4.9
     * cudnn version = 4
     * boost version = 1.51.0 or 1.55.0
     * any version of eigen

tutorial

   for this tutorial zoph_id56 represents the executable to run the code.
   also all the scripts in the scripts folder require python 3 to run.

   this command will bring up the program's help menu showing all the
   flags that the program can be run with:
./zoph_id56 -h

   there are two different kinds of models this code can train
    1. sequence models (ex: id38)
    2. sequence-to-sequence models (ex: machine translation)

   the commands for these two different architectures are almost the same,
   all that needs to change is adding a -s flag if you want to use the
   sequence model. the sequence-to-sequence model is used by default.

   in the sample_data directory there is sample data provided that shows
   the proper formatting for files.

training a seq-to-seq model:

   lets step through an example that trains a basic sequence-to-sequence
   model. the following code will train a sequence-to-sequence model with
   the source training data /path/to/source_train_data.txt and the target
   training data /path/to/target_train_data.txt. these are placeholder
   names that will be replaced with your data files when you are training
   your own model. the resulting model will be saved to model.nn, but this
   can be named whatever the user wants. training data always needs to
   consist of one training example per line, with tokens separated by
   spaces.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn

   by default the source sentences will always be fed in the reversed
   direction as in [89]sequence to sequence learning with neural networks.
   if you want to feed in the source sentences in the forward direction
   then simply preprocess your source data, so that it is in the reversed
   direction.

   there are many flags that can be used to train more specific
   architectures. lets say we want to train a model with 3 layers (default
   is 1), 500 hiddenstates (default is 100), and a minibatch of size 64
   (default is 8). the following command does this:
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64

   lets also make the model have 20,000 source vocabulary and 10,000
   target vocabulary (by default the code makes the source vocabulary
   equal to the number of unique tokens in the source training data, and
   the target vocab does the same). also lets apply dropout with a keep
   id203 of 0.8 to the model, where dropout is applied as specified
   in [90]recurrent neural network id173.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8

   additionally, lets change the learning rate to 0.5 (default is 0.7),
   add the local-p attention model with feed input as in [91]effective
   approaches to attention-based id4.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true

   to monitor the training we also want to be able to monitor the
   performance of the model during training on some held out set of data
   (developement/validation). lets do this in the code and also add the
   option that if perplexity (better is lower) on the held out set of data
   increased since it was previously checked, then we multiply the current
   learning rate by 0.5.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5

   during training the code needs to produce temporary files. by default
   these will be put in the directory where the code is launched from, but
   we can change this to whatever we want. additionally, we can make all
   of the output that is typically printed to standard out (the screen)
   also be printed to a file.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5 --tmp-dir-location /path/to/tmp/ --logfile
 /path/to/log/logfile.txt

   typically during training only one model will be output at the end of
   training. to make the code output the best model during training
   according to the perplexity on your heldout data specificed by the -a
   flag we can add the -b flag.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5 --tmp-dir-location /path/to/tmp/ --logfile
 /path/to/log/logfile.txt -b best.nn

   or if you want to save all models every half epoch we can do that with
   the --save-all-models flag.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5 --tmp-dir-location /path/to/tmp/ --logfile
 /path/to/log/logfile.txt --save-all-models true

   by default the code will throw away any sentences in training and in
   the held out data longer than some fixed length which is 100 by
   default. we can change this to whatever we want, but be careful as it
   will greatly increase memory usage. lets change it to 500.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5 --tmp-dir-location /path/to/tmp/ --logfile
 /path/to/log/logfile.txt --save-all-models true -l 500

   by default the code uses an id113 objective function, which can be very
   computationally expensive if the target vocabulary is big. to alleviate
   this issue we can train with nce instead of id113 by using the --nce
   flag. this is the same nce as in [92]simple, fast noise contrastive
   estimation for large id56 vocabularies. a good number of noise samples
   is usually around 100. note that the --nce flag only has to be
   specified during training and not during force-decode or decode.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5 --tmp-dir-location /path/to/tmp/ --logfile
 /path/to/log/logfile.txt --save-all-models true -l 500 --nce 100

   one feature of this code is that is supports model parallelism across
   multiple gpus. to see the number of available gpu's on your node you
   can type nvidia-smi. the -m flag allows our model to put each layer on
   a gpu of our choosing along with the softmax. -m 0 1 2 3 means put
   layer 1 on gpu 0, layer 2 on gpu 1, layer 3 on gpu 2 and the softmax on
   gpu 3. by default the code does -m 0 0 0 0, putting everything on the
   default gpu 0. we can also change up the specification depending how
   many gpus we have on the node, so we could do -m 0 0 1 1 if we only
   have 2 gpus on our node.
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn -n 3 -h 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.
8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.tx
t /path/to/target_dev_data.txt -a 0.5 --tmp-dir-location /path/to/tmp/ --logfile
 /path/to/log/logfile.txt --save-all-models true -l 500 --nce 100 -m 0 1 2 3

supplying your own vocabulary mapping file

   the --source-vocab-size n and the --target-vocab-size n flags create a
   vocabulary mapping file that will replace all words not in the top n
   most frequent words with 's. the code will create an integer mapping
   that is stored in the top of the model file. if you want to supply your
   own mapping file you can do this using the --vocab-mapping-file
   /path/to/my_mapping.nn. the my_mapping.nn can be a previously trained
   model, in that case it will use the exact same vocabulary mapping as
   that model. this is useful because if you want to ensemble models using
   the --decode flag, then the models must have exactly the same target
   vocabulary mapping file for it to work. in the scripts/ directory there
   is a python script called create_vocab_mapping_file.py. we can use this
   to create a mapping file, which then gets fed into the training using
   the following command:
python scripts/create_vocab_mapping_file.py /path/to/source_training_data.txt /p
ath/to/target_training_data.txt 5 my_mapping.nn

   this will create a mapping file named my_mapping.nn, which we can then
   use for training a model using the following command.
./zoph_id56 -t /path/to/source_training_data.txt /path/to/target_training_data.tx
t model.nn --vocab-mapping-file my_mapping.nn

   instead of using the create_vocab_mapping_file.py script, we can also
   use an existing model as the input for the --vocab-mapping-file flag
./zoph_id56 -t /path/to/source_train_data.txt /path/to/target_train_data.txt mode
l.nn --vocab-mapping-file my_old_model.nn

   this --vocab-mapping-file flag only needs to be specified during
   training. this can also be used for sequence models in the same way.

force-decoding a seq-to-seq model

   once the model finished training we can use the model file (model.nn,
   best.nn or any of the models output from --save-all-best in the
   previous training example) for getting the perplexity for a set of
   source/target pairs or do beam decoding to get the best target outputs
   given some source sentences. lets do the former first. we will specify
   the source and target data we want to get the perplexity for along with
   the per line log probabilities of each sentece. the output file we
   specify (/path/to/output/perp_output.txt) will contain the per line log
   probabilities and the total perplexity will be output to standard out.
   additionally, we can use the --logfile flag as before if we also want
   standard out to be put to a file too and the -l flag to change what the
   longest sentence the code will accept.
./zoph_id56 -f /path/to/source_perp_data.txt /path/to/target_perp_data.txt model.
nn /path/to/output/perp_output.txt --logfile /path/to/log/logfile.txt -l 500

   if we trained the model using nce then we can use the --nce-score flag,
   which will make the model get the per line log probabilities using an
   unnormalized softmax. this greatly speeds up force-decode as now a
   id172 over the softmax does not have to be done, but now it
   does not represent a distribution that sums to 1. the reason we can do
   this is because the nce training objective makes the id172
   constant close to 1, so we can get a reasonably good approximation.

kbest decoding for a seq-to-seq model

   lets have the model output the most likely target translation given the
   source using beam decoding. this can be done with the --decode (-k)
   flag. the model.nn file will be the trained neural network, kbest.txt
   is where we want the output to be put to and and source_data.txt is the
   file containing the source sentences that we want to be decoded. once
   again short sentences are thrown out, so we can change that using the
   -l flag.
./zoph_id56 -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data
.txt -l 500

   by default the model uses beam decoding with a beam size of 12. we can
   change this using the -b flag.
./zoph_id56 -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data
.txt -l 500 -b 25

   we can also output the log probabilities of each sentence being decoded
   and have it saved in kbest.txt using the --print-score flag
./zoph_id56 -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data
.txt -l 500 -b 25 --print-score true

   another default during decoding is that the output sentences can only
   be within the length range [0.5*(length of source sentence),1.5*(length
   of source sentence)]. this can be changed with this --dec-ratio flag.
./zoph_id56 -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data
.txt -l 500 -b 25 --print-score true --dec-ratio 0.2 1.8

ensemble decoding for a seq-to-seq model

   in the above example we only decoded a single model. in this code you
   have the option of ensembling multiple outputs using the --decode flag.
   all of the models you want to ensemble must have the same target
   vocabulary mappings, so you must use the --vocab-mapping-file flag as
   specified above. we can ensemble together 8 models below, but any
   number of models can be specified by the user.
./zoph_id56 -k 1 model1.nn model2.nn model3.nn model4.nn model5.nn model6.nn mode
l7.nn model8.nn kbest.txt --decode-main-data-files /path/to/source_data1.txt /pa
th/to/source_data2.txt /path/to/source_data3.txt /path/to/source_data4.txt /path
/to/source_data5.txt /path/to/source_data6.txt /path/to/source_data7.txt /path/t
o/source_data8.txt -l 500 -b 25 --print-score true --dec-ratio 0.2 1.8

   note that now we pass in 8 different model files and 8 different source
   data files. the reason for the 8 different source files is that the
   source vocabularies could be different for all 8 models, so different
   types of data can be passed in. if you want the same data passed in for
   all 8 model, then simply copy /path/to/source_data.txt 8 times as the
   input to --decode-main-data-files.

training a seq model

   training a sequence model is much like training a sequence-to-sequence
   model. now we must employ the -s flag to denote that we want to train a
   sequence model. lets train a model with slighly different parameters
   from the sequence-to-sequence model above. this model will have a
   hiddenstate size of 1000, minibatch size of 32, 2 layers, dropout rate
   of 0.3 and a target vocabulary size of 15k. also note that now we only
   need to pass in one data file for training and for dev since it is only
   a sequence model and not a sequence-to-sequence model.
./zoph_id56 -s -t /path/to/training_data.txt model.nn -h 1000 -m 32 -l 0.2 -n 2 -
m 0 1 2 -d 0.7 --target-vocab-size 15000 -a /path/to/dev_data.txt -a 0.5 --tmp-d
ir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt --save-all-models t
rue -l 500

force-decoding a seq model

   to force decode the model it is almost the same as force-decoding a
   sequence-to-sequence model. in the seq model you can also use the -m
   flag to speedup the batching process, but it will no longer output the
   per line log id203 if -m is not set to 1.
./zoph_id56 -s -f /path/to/dev_data.txt model.nn /path/to/output/perp_output.txt
-l 500 --logfile /path/to/log/logfile.txt

decoding a seq model

   this is not a feature in the code.

training multi-source model

   lets train a multi-source model like in [93]multi-source neural
   translation. in this model we will have two source encoders and one
   target encoder. this means we need to have 3-way parallel data. the two
   source training files in this example are: source1_train_data.txt and
   source2_train_data.txt. the target training file is:
   target_train_data.txt. all 3 of these files must have the same number
   of lines. notice that we must now add the --multi-source flag which
   specifies the second source training file. additionally, we must
   specify it a second neural network file name that will be created just
   like model.nn.
./zoph_id56 -t /path/to/source1_train_data.txt /path/to/target_train_data.txt mod
el.nn --multi-source /path/to/source2_train_data.txt src.nn

   by default the model combines the two source encoders using the "basic"
   method as specified in [94]multi-source neural translation. to use the
   "child-sum" method we can add the following flag --lstm-combine 1.
./zoph_id56 -t /path/to/source1_train_data.txt /path/to/target_train_data.txt mod
el.nn --multi-source /path/to/source2_train_data.txt src.nn --lstm-combine 1

   additionally, we can use the multi-source attention model from the
   above paper by adding the three following flags --attention-model 1
   --feed-input 1 --multi-attention 1. all three flags must be specified
   to use the multi-source attention model.
./zoph_id56 -t /path/to/source1_train_data.txt /path/to/target_train_data.txt mod
el.nn --multi-source /path/to/source2_train_data.txt src.nn --lstm-combine 1 --a
ttention-model 1 --feed-input 1 --multi-attention 1

   now lets have the model use a dev set for learning rate monitoring like
   before.
./zoph_id56 -t /path/to/source1_train_data.txt /path/to/target_train_data.txt mod
el.nn --multi-source /path/to/source2_train_data.txt src.nn --lstm-combine 1 --a
ttention-model 1 --feed-input 1 --multi-attention 1 -a /path/to/source1_dev_data
.txt /path/to/target_dev_data.txt /path/to/source2_dev_data.txt  -a 0.5

force-decoding a multi-source model

   to force-decode a multi-source model the --multi-source flag must be
   specified when using the -f flag.
./zoph_id56 -f /path/to/source1_perp_data.txt /path/to/target_perp_data.txt model
.nn /path/to/output/perp_output.txt --logfile /path/to/log/logfile.txt -l 500 --
multi-source /path/to/source2_perp_data.txt src.nn

kbest decoding a multi-source model

   to decode a multi-source model two additional flags needs to be
   specified.
./zoph_id56 -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source1_dat
a.txt --decode-multi-source-data-files /path/to/source2_data.txt --decode-multi-
source-vocab-mappings src.nn

training a preinit model

   lets train a model using tranfer learning as specified in [95]transfer
   learning for low-resource id4. first we need to
   have parent data (source and target) and child data (source and target)
   where the parent and child models must have the same target language.
   in the paper the shared target language was english.

   also note that this can only be done with seq-to-seq models and not seq
   models or multi-source models.

   first we must make a mapping file that was shown in the "supplying your
   own vocabulary mapping file" section. we can use the script
   create_vocab_mapping_file_preinit.py in the scripts/ folder. run the
   following command to create a vocabulary mapping file where all words
   that appear less than 5 times will be replaced by (the 5 can be changed
   to whatever the user wants):
python scripts/create_vocab_mapping_file_preinit.py /path/to/source_child_data.t
xt /path/to/target_child_data.txt 5 my_mapping.nn /path/to/source_parent_data.tx
t

   now we have created a mapping file mapping.nn, which can now be used
   for training. now lets train the parent model
./zoph_id56 -t /path/to/source_parent_data.txt /path/to/target_parent_data.txt pa
rent_model.nn --vocab-mapping-file my_mapping.nn

   once the parent model finished training, we can now train the child
   model using the following script in the scripts/ folder.
python scripts/pretrain.py --parent parent_model.nn --trainsource /path/to/sourc
e_child_data.txt --traintarget /path/to/target_child_data.txt --devsource /path/
to/source_child_dev_data.txt --devtarget /path/to/target_child_dev_data.txt --rn
nbinary zoph_id56 --child child.nn

   once the above arguements are supplied other normal parameter flags can
   be added just like in the zoph_id56 executable.
python scripts/pretrain.py --parent parent_model.nn --trainsource /path/to/sourc
e_child_data.txt --traintarget /path/to/target_child_data.txt --devsource /path/
to/source_child_dev_data.txt --devtarget /path/to/target_child_dev_data.txt --rn
nbinary ./zoph_id56 --child child.nn -d 0.8 -l 0.5 -a 0.5 -p 0.01 -w 5 -l 200 -m
32 -n 15 --attention_model true --feed_input true

unk replacement in seq-to-seq model

   to do unk replacement as specified in [96]addressing the rare word
   problem in id4 there is a python script provided
   in the scripts/ directory. the following commands need to be run if you
   want to do unk replacement. this can only be done with attention
   seq-to-seq models.
    1. when decoding (-k or --decode flags) add in the following flag
       --unk-decode /path/to/unks.txt.

   the unks.txt file will be generated during decoding, so save it
   somewhere that it can be accessed later.
    2. run the berkeley aligner to in order to generate a t-table. the
       berkeley aligner is available at:
       [97]https://code.google.com/archive/p/berkeleyaligner/. a sample
       parameter file is provided in the scripts/berk_aligner directory.
       the run_aligner.sh script will run the berkeley aligner, what needs
       to be changed on the user's end is the unk_replace.conf file. the
       execdir field must be changed to the directory that you want the
       berkeley aligner to output all of its files to. the trainsources
       field must give a path to the source training data. the testsources
       field must be changed to the path of the dev data for both source
       and target. the suffixes must also be changed accordinly depending
       on what the files were named.

   a sample of what the data/train/ and the data/test/ directories should
   contain are below (if the foreign and english suffixes are u and e
   respectively):

   ls data/train results in train.e train.u

   ls data/test results in test.e test.u

   once the berkeley aligner finishes running you need to take the ttable
   (there are two given by the berkeley aligner, one of p(source | target)
   and the other p(target | source)) that corresponds to p(target |
   source). if you make the target language english then the name of the
   ttable is: stage2.2.params.txt else stage2.1.params.txt.

   now we can run the following command to decode a seq-to-seq model using
   unk replacement. /path/to/unks.txt is where additional information will
   be stored during decoding when using unk replacement.
./zoph_id56 -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data
.txt -l 500 -b 25 --print-score true --dec-ration 0.2 1.8 --unk-decode /path/to/
unks.txt

   next we will run the scripts/unk_format.py script to convert the output
   of the zoph_id56 code into correct format for the scripts/att_unk_rep.py
   script.
python scripts/unk_format.py kbest.txt kbest.txt.formatted

   next we will run the final scripts/att_unk_rep.py script.
python scripts/att_unk_rep.py /path/to/source_data.txt kbest.txt.formatted stage
2.2.params.txt kbest.txt.formatted.unkrep

   now the kbest.txt.formatted.unkrep will contain the decoded sentences
   with the rare words replaced. the format is 1 output per line.

models from papers:

   here are sample commands that can be run to create models in the papers
   above:

   for the paper [98]multi-source neural translation. here is the command
   to train a multi-source attention model with german and french as the
   input and english as the output. if you dont want attention remove
   --attention-model 1 --feed-input 1 --multi-attention 1 and if you want
   to use the basic method of combination instead of the child-sum method
   then change --lstm-combine 1 to --lstm-combine 0.
./zoph_id56 -t german_train_data.txt train_english_data.txt model.nn -n 15 -b bes
t.nn -m 128 -h 1000 -l 0.7 -w 5 -a german_dev_data.txt english_dev_data.txt fren
ch_dev_data.txt -a 1 -v 50000 -v 50000 --clip-cell 50 1000 -n 4 -m 0 1 1 2 3 --m
ulti-source french_train_data.txt src.nn -d 0.8 -l 65 --logfile log.txt --screen
-print-rate 15 --fixed-halve-lr-full 11 -p -0.08 0.08 --lstm-combine 1 --attenti
on-model 1 --feed-input 1 --multi-attention 1

   for the paper [99]simple, fast noise contrastive estimation for large
   id56 vocabularies the command below will train the billion word language
   model.
./zoph_id56 --logfile log.txt -a english_dev_data.txt -s -t english_train_data.tx
t model.nn -b best.nn --nce 100 --screen-print-rate 300 -n 4 -m 0 1 2 3 3 -l 0.7
 -p -0.08 0.08 -a 0.5 -d 0.8 -n 20  -c 5 -h 2048 --vocab-mapping-file my_mapping
.nn -l 205

   for the paper [100]id21 for low-resource neural machine
   translation the following command wil train the parent model and the
   child model (with the child language being uzbek).
python scripts/create_vocab_mapping_file_preinit.py uzbek_train_data.txt english
_child_train_data.txt 5 my_mapping.nn french_train_data.txt

./zoph_id56 -t french_train_data.txt english_parent_train_data.txt -h 750 -n 2 -d
 0.8 -m 128 -l 0.5 -p -0.08 0.08 -w 5 --attention-model 1 --feed-input 1 --scree
n-print-rate 30 --logfile log.txt -b best.nn -n 10 -l 100 -a 0.5 -a french_dev_d
ata.txt english_parent_dev_data.txt --vocab-mapping-file my_mapping.nn

   once the parent model finishes training then run:
python scripts/pretrain.py --parent best.nn --trainsource uzbek_train_data.txt -
-traintarget english_child_train_data.txt --devsource uzbek_dev_data.txt --devta
rget english_child_dev_data.txt --id56binary zoph_id56 --child child.nn -d 0.5 -l
0.5 -a 0.9 -p -0.05 0.05 -w 5 -l 100 -m 128 -n 100 --attention_model true --feed
_input true

   for the paper [101]effective approaches to attention-based neural
   machine translation the command below will train the "base + reverse +
   dropout + local-p attention (general) + feed input" from table 1.
./zoph_id56 --logfile log.txt -a english_dev_data.txt german_dev_data.txt -t engl
ish_train_data.txt german_train_data.txt model.nn -b best.nn --screen-print-rate
 300 -n 4 -m 0 1 2 2 3 -l 50 -l 1 -p -0.1 0.1 --fixed-halve-lr-full 9 -a 1 -d 0.
8 -n 12 -w 5 --attention-model 1 --feed-input 1 --attention-width 10 -v 50000 -v
 50000

   for the paper [102]sequence to sequence learning with neural networks
   the following command will train the "single reversed lstm"
./zoph_id56 -t source_train_data.txt target_train_data.txt model.nn  -h 1000 -n 4
 -v 160000 -v 80000 -p -0.08 0.08 -l 0.7 -n 8 --fixed-halve-lr 6 -m 128 -w 5 -l
100

changes from previous version

     * the flag (--hpc-output) has been renamed to (--logfile)
     * the flag (--source-vocab) has been renamed to (--source-vocab-size)
     * the flag (--target-vocab) has been renamed to (--target-vocab-size)
     * the flag (--random-seed) now takes in an integer to use as the
       fixed random seed, or by default now seeds with the time
     * the flag (--save-all-best) has been renamed to (--save-all-models)
     * the flag (--feed_input) has been renamed to (--feed-input)
     * the default minibatch size was changed from 128 to 8
     * the default hiddenstate size was changed from 1000 to 100
     * added attention, multi-source, nce, unk-replacement, transfer
       learning

license

   mit

     *    2019 github, inc.
     * [103]terms
     * [104]privacy
     * [105]security
     * [106]status
     * [107]help

     * [108]contact github
     * [109]pricing
     * [110]api
     * [111]training
     * [112]blog
     * [113]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [114]reload to refresh your
   session. you signed out in another tab or window. [115]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/isi-nlp/zoph_id56/commits/master.atom
   3. https://github.com/isi-nlp/zoph_id56#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/isi-nlp/zoph_id56
  32. https://github.com/join
  33. https://github.com/login?return_to=/isi-nlp/zoph_id56
  34. https://github.com/isi-nlp/zoph_id56/watchers
  35. https://github.com/login?return_to=/isi-nlp/zoph_id56
  36. https://github.com/isi-nlp/zoph_id56/stargazers
  37. https://github.com/login?return_to=/isi-nlp/zoph_id56
  38. https://github.com/isi-nlp/zoph_id56/network/members
  39. https://github.com/isi-nlp
  40. https://github.com/isi-nlp/zoph_id56
  41. https://github.com/isi-nlp/zoph_id56
  42. https://github.com/isi-nlp/zoph_id56/issues
  43. https://github.com/isi-nlp/zoph_id56/pulls
  44. https://github.com/isi-nlp/zoph_id56/projects
  45. https://github.com/isi-nlp/zoph_id56/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/isi-nlp/zoph_id56/commits/master
  48. https://github.com/isi-nlp/zoph_id56/branches
  49. https://github.com/isi-nlp/zoph_id56/releases
  50. https://github.com/isi-nlp/zoph_id56/graphs/contributors
  51. https://github.com/isi-nlp/zoph_id56/search?l=c++
  52. https://github.com/isi-nlp/zoph_id56/search?l=cuda
  53. https://github.com/isi-nlp/zoph_id56/search?l=python
  54. https://github.com/isi-nlp/zoph_id56/find/master
  55. https://github.com/isi-nlp/zoph_id56/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/isi-nlp/zoph_id56
  57. https://github.com/join?return_to=/isi-nlp/zoph_id56
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/shixing
  63. https://github.com/isi-nlp/zoph_id56/commits?author=shixing
  64. https://github.com/isi-nlp/zoph_id56/commit/1e3e7da688cfcd0cdfb0c117cb84705399d1a967
  65. https://github.com/isi-nlp/zoph_id56/pull/8
  66. https://github.com/isi-nlp/zoph_id56/commit/1e3e7da688cfcd0cdfb0c117cb84705399d1a967
  67. https://github.com/isi-nlp/zoph_id56/commit/1e3e7da688cfcd0cdfb0c117cb84705399d1a967
  68. https://github.com/isi-nlp/zoph_id56/tree/1e3e7da688cfcd0cdfb0c117cb84705399d1a967
  69. https://github.com/isi-nlp/zoph_id56/tree/master/executable
  70. https://github.com/isi-nlp/zoph_id56/commit/2baea22e2ee9f7c8b583118ecd78af728a8b1e40
  71. https://github.com/isi-nlp/zoph_id56/tree/master/sample_data
  72. https://github.com/isi-nlp/zoph_id56/tree/master/scripts
  73. https://github.com/isi-nlp/zoph_id56/tree/master/src
  74. https://github.com/isi-nlp/zoph_id56/commit/2baea22e2ee9f7c8b583118ecd78af728a8b1e40
  75. https://github.com/isi-nlp/zoph_id56/blob/master/.gitattributes
  76. https://github.com/isi-nlp/zoph_id56/commit/5a4d7860773d88a74794150c7522f55b42fd1f6d
  77. https://github.com/isi-nlp/zoph_id56/blob/master/.gitignore
  78. https://github.com/isi-nlp/zoph_id56/blob/master/readme.md
  79. https://github.com/isi-nlp/zoph_id56/blob/master/readme_xing.md
  80. http://barretzoph.github.io/
  81. mailto:barretzoph@gmail.com
  82. http://www.isi.edu/natural-language/mt/multi-source-neural.pdf
  83. http://www.isi.edu/natural-language/mt/simple-fast-noise.pdf
  84. http://arxiv.org/pdf/1604.02201v1.pdf
  85. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
  86. http://stanford.edu/~lmthang/data/papers/acl15_id4.pdf
  87. http://arxiv.org/pdf/1409.3215.pdf
  88. http://arxiv.org/pdf/1409.2329.pdf
  89. http://arxiv.org/pdf/1409.3215.pdf
  90. http://arxiv.org/pdf/1409.2329.pdf
  91. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
  92. http://www.isi.edu/natural-language/mt/simple-fast-noise.pdf
  93. http://www.isi.edu/natural-language/mt/multi-source-neural.pdf
  94. http://www.isi.edu/natural-language/mt/multi-source-neural.pdf
  95. http://arxiv.org/pdf/1604.02201v1.pdf
  96. http://stanford.edu/~lmthang/data/papers/acl15_id4.pdf
  97. https://code.google.com/archive/p/berkeleyaligner/
  98. http://www.isi.edu/natural-language/mt/multi-source-neural.pdf
  99. http://www.isi.edu/natural-language/mt/simple-fast-noise.pdf
 100. http://arxiv.org/pdf/1604.02201v1.pdf
 101. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
 102. http://arxiv.org/pdf/1409.3215.pdf
 103. https://github.com/site/terms
 104. https://github.com/site/privacy
 105. https://github.com/security
 106. https://githubstatus.com/
 107. https://help.github.com/
 108. https://github.com/contact
 109. https://github.com/pricing
 110. https://developer.github.com/
 111. https://training.github.com/
 112. https://github.blog/
 113. https://github.com/about
 114. https://github.com/isi-nlp/zoph_id56
 115. https://github.com/isi-nlp/zoph_id56

   hidden links:
 117. https://github.com/
 118. https://github.com/isi-nlp/zoph_id56
 119. https://github.com/isi-nlp/zoph_id56
 120. https://github.com/isi-nlp/zoph_id56
 121. https://help.github.com/articles/which-remote-url-should-i-use
 122. https://github.com/isi-nlp/zoph_id56#zoph_id56-a-ccuda-toolkit-for-training-sequence-and-sequence-to-sequence-models-across-multiple-gpus
 123. https://github.com/isi-nlp/zoph_id56#instructions-for-compilationusing-the-code
 124. https://github.com/isi-nlp/zoph_id56#the-acceptable-versions-for-the-libraries-above
 125. https://github.com/isi-nlp/zoph_id56#tutorial
 126. https://github.com/isi-nlp/zoph_id56#training-a-seq-to-seq-model
 127. https://github.com/isi-nlp/zoph_id56#supplying-your-own-vocabulary-mapping-file
 128. https://github.com/isi-nlp/zoph_id56#force-decoding-a-seq-to-seq-model
 129. https://github.com/isi-nlp/zoph_id56#kbest-decoding-for-a-seq-to-seq-model
 130. https://github.com/isi-nlp/zoph_id56#ensemble-decoding-for-a-seq-to-seq-model
 131. https://github.com/isi-nlp/zoph_id56#training-a-seq-model
 132. https://github.com/isi-nlp/zoph_id56#force-decoding-a-seq-model
 133. https://github.com/isi-nlp/zoph_id56#decoding-a-seq-model
 134. https://github.com/isi-nlp/zoph_id56#training-multi-source-model
 135. https://github.com/isi-nlp/zoph_id56#force-decoding-a-multi-source-model
 136. https://github.com/isi-nlp/zoph_id56#kbest-decoding-a-multi-source-model
 137. https://github.com/isi-nlp/zoph_id56#training-a-preinit-model
 138. https://github.com/isi-nlp/zoph_id56#unk-replacement-in-seq-to-seq-model
 139. https://github.com/isi-nlp/zoph_id56#models-from-papers
 140. https://github.com/isi-nlp/zoph_id56#changes-from-previous-version
 141. https://github.com/isi-nlp/zoph_id56#license
 142. https://github.com/
