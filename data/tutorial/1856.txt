   #[1]github [2]recent commits to awesome-id56:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]652
     * [35]star [36]5,331
     * [37]fork [38]1,336

[39]kjw0612/[40]awesome-id56

   [41]code [42]issues 0 [43]pull requests 4 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   recurrent neural network - a curated list of resources dedicated to id56
     * [47]132 commits
     * [48]2 branches
     * [49]1 release
     * [50]fetching contributors

   branch: master (button) new pull request
   [51]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/k
   [52]download zip

downloading...

   want to be notified of new releases in kjw0612/awesome-id56?
   [53]sign in [54]sign up

launching github desktop...

   if nothing happens, [55]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [56]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [57]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [58]download the github extension for visual studio
   and try again.

   (button) go back
   [59]@myungsub
   [60]myungsub [61]merge pull request [62]#33 [63]from spro/patch-2
   (button)    
fix pull request link; add pytorch section

   latest commit [64]6f34461 apr 14, 2017
   [65]permalink
   type      name     latest commit message commit time
        failed to load latest commit information.
        [66]readme.md

readme.md

awesome recurrent neural networks

   a curated list of resources dedicated to recurrent neural networks
   (closely related to deep learning).

   maintainers - [67]myungsub choi, [68]taeksoo kim, [69]jiwon kim

   we have pages for other topics: [70]awesome-deep-vision,
   [71]awesome-random-forest

contributing

   please feel free to [72]pull requests, email myungsub choi
   ([73]cms6539@gmail.com) or join our chats to add links.

   [74]join the chat at https://gitter.im/kjw0612/awesome-id56

sharing

     * [75]share on twitter
     * [76]share on facebook
     * [77]share on google plus
     * [78]share on linkedin

table of contents

     * [79]codes
     * [80]theory
          + [81]lectures
          + [82]books / thesis
          + [83]architecture variants
               o [84]structure
               o [85]memory
          + [86]surveys
     * [87]applications
          + [88]natural language processing
               o [89]id38
               o [90]id103
               o [91]machine translation
               o [92]conversation modeling
               o [93]id53
          + [94]id161
               o [95]object recognition
               o [96]image generation
               o [97]video analysis
          + [98]multimodal (cv+nlp)
               o [99]image captioning
               o [100]video captioning
               o [101]visual id53
          + [102]turing machines
          + [103]robotics
          + [104]other
     * [105]datasets
     * [106]blogs
     * [107]online demos

codes

     * [108]tensorflow - python, c++
          + [109]get started, [110]tutorials
               o [111]recurrent neural network tutorial
               o [112]sequence-to-sequence model tutorial
          + [113]tutorials by nlintz
          + [114]notebook examples by aymericdamien
          + [115]scikit flow (skflow) - simplified scikit-learn like
            interface for tensorflow
          + [116]keras : (tensorflow / theano)-based modular deep learning
            library similar to torch
          + [117]char-id56-tensorflow by sherjilozair: char-id56 in
            tensorflow
     * [118]theano - python
          + simple ipython [119]tutorial on theano
          + [120]deep learning tutorials
               o [121]id56 for id29 of speech
               o [122]lstm network for id31
          + [123]pylearn2 : library that wraps a lot of models and
            training algorithms in deep learning
          + [124]blocks : modular framework that enables building neural
            network models
          + [125]keras : (tensorflow / theano)-based modular deep learning
            library similar to torch
          + [126]lasagne : lightweight library to build and train neural
            networks in theano
          + [127]theano-id56 by graham taylor
          + [128]passage : library for text analysis with id56s
          + [129]theano-lights : contains many generative models
     * [130]caffe - c++ with matlab/python wrappers
          + [131]lrcn by jeff donahue
     * [132]torch - lua
          + [133]torchnet : modular framework that enables building neural
            network models
          + [134]char-id56 by andrej karpathy : multi-layer id56/lstm/gru
            for training/sampling from character-level language models
          + [135]torch-id56 by justin johnson : reusable id56/lstm modules
            for torch7 - much faster and memory efficient reimplementation
            of char-id56
          + [136]neuraltalk2 by andrej karpathy : recurrent neural network
            captions image, much faster and better version of the original
            [137]neuraltalk
          + [138]lstm by wojciech zaremba : long short term memory units
            to train a language model on word level penn tree bank dataset
          + [139]oxford by nando de freitas : oxford computer science -
            machine learning 2015 practicals
          + [140]id56 by nicholas leonard : general library for
            implementing id56, lstm, bid56 and blstm (highly unit tested).
     * [141]pytorch - python
          + [142]word-level id56 example : demonstrates pytorch's built in
            id56 modules for id38
          + [143]practical pytorch tutorials by sean robertson : focuses
            on using id56s for natural language processing
          + [144]deep learning for nlp in pytorch by robert guthrie :
            written for a natural language processing class at georgia
            tech
     * [145]dl4j by [146]skymind : deep learning library for java, scala &
       clojure on hadoop, spark & gpus
          + [147]documentation (also in [148]chinese, [149]japanese,
            [150]korean) : [151]id56, [152]lstm
          + [153]id56 examples
     * etc.
          + [154]neon: new deep learning library in python, with support
            for id56/lstm, and a fast image captioning model
          + [155]brainstorm: deep learning library in python, developed by
            idsia, thereby including various recurrent structures
          + [156]chainer : new, flexible deep learning library in python
          + [157]cgt(computational graph toolkit) : replicates theano's
            api, but with very short compilation time and multithreading
          + [158]id56lib by alex graves : c++ based lstm library
          + [159]id56lm by tomas mikolov : c++ based simple code
          + [160]faster-id56lm of yandex : c++ based id56lm implementation
            aimed to handle huge datasets
          + [161]neuraltalk by andrej karpathy : numpy-based id56/lstm
            implementation
          + [162]gist by andrej karpathy : raw numpy code that implements
            an efficient batched lstm
          + [163]recurrentjs by andrej karpathy : a beta javascript
            library for id56
          + [164]darqn by 5vision : deep attention recurrent q-network

theory

lectures

     * stanford nlp ([165]cs224d) by richard socher
          + [166]lecture note 3 : neural network basics
          + [167]lecture note 4 : id56 language models, bi-directional id56,
            gru, lstm
     * stanford vision ([168]cs231n) by andrej karpathy
          + about nn basic, and id98
     * oxford [169]machine learning by nando de freitas
          + [170]lecture 12 : recurrent neural networks and lstms
          + [171]lecture 13 : (guest lecture) alex graves on hallucination
            with id56s

books / thesis

     * alex graves (2008)
          + [172]supervised sequence labelling with recurrent neural
            networks
     * tomas mikolov (2012)
          + [173]statistical language models based on neural networks
     * ilya sutskever (2013)
          + [174]training recurrent neural networks
     * richard socher (2014)
          + [175]recursive deep learning for natural language processing
            and id161
     * ian goodfellow, yoshua bengio, and aaron courville (2016)
          + [176]the deep learning book chapter 10

architecture variants

structure

     * bi-directional id56 [[177]paper]
          + mike schuster and kuldip k. paliwal, bidirectional recurrent
            neural networks, trans. on signal processing 1997
     * multi-dimensional id56 [[178]paper]
          + alex graves, santiago fernandez, and jurgen schmidhuber,
            multi-dimensional recurrent neural networks, icann 2007
     * gfid56 [[179]paper-arxiv] [[180]paper-icml] [[181]supplementary]
          + junyoung chung, caglar gulcehre, kyunghyun cho, yoshua bengio,
            gated feedback recurrent neural networks, arxiv:1502.02367 /
            icml 2015
     * tree-structured id56s
          + kai sheng tai, richard socher, and christopher d. manning,
            improved semantic representations from tree-structured long
            short-term memory networks, arxiv:1503.00075 / acl 2015
            [[182]paper]
          + samuel r. bowman, christopher d. manning, and christopher
            potts, tree-structured composition in neural networks without
            tree-structured architectures, arxiv:1506.04834 [[183]paper]
     * grid lstm [[184]paper] [[185]code]
          + nal kalchbrenner, ivo danihelka, and alex graves, grid long
            short-term memory, arxiv:1507.01526
     * segmental id56 [[186]paper]
          + lingpeng kong, chris dyer, noah smith, "segmental recurrent
            neural networks", iclr 2016.
     * id195 for sets [[187]paper]
          + oriol vinyals, samy bengio, manjunath kudlur, "order matters:
            sequence to sequence for sets", iclr 2016.
     * hierarchical recurrent neural networks [[188]paper]
          + junyoung chung, sungjin ahn, yoshua bengio, "hierarchical
            multiscale recurrent neural networks", arxiv:1609.01704

memory

     * lstm [[189]paper]
          + sepp hochreiter and jurgen schmidhuber, long short-term
            memory, neural computation 1997
     * gru (gated recurrent unit) [[190]paper]
          + kyunghyun cho, bart van berrienboer, caglar gulcehre, dzmitry
            bahdanau, fethi bougares, holger schwenk, and yoshua bengio,
            learning phrase representations using id56 encoder-decoder for
            id151, arxiv:1406.1078 / emnlp 2014
     * ntm [[191]paper]
          + a.graves, g. wayne, and i. danihelka., id63s,
            arxiv preprint arxiv:1410.5401
     * neural gpu [[192]paper]
          +   ukasz kaiser, ilya sutskever, arxiv:1511.08228 / icml 2016
            (under review)
     * memory network [[193]paper]
          + jason weston, sumit chopra, antoine bordes, memory networks,
            arxiv:1410.3916
     * pointer network [[194]paper]
          + oriol vinyals, meire fortunato, and navdeep jaitly, pointer
            networks, arxiv:1506.03134 / nips 2015
     * deep attention recurrent q-network [[195]paper]
          + ivan sorokin, alexey seleznev, mikhail pavlov, aleksandr
            fedorov, anastasiia ignateva, deep attention recurrent
            q-network , arxiv:1512.01693
     * dynamic memory networks [[196]paper]
          + ankit kumar, ozan irsoy, peter ondruska, mohit iyyer, james
            bradbury, ishaan gulrajani, victor zhong, romain paulus,
            richard socher, "ask me anything: dynamic memory networks for
            natural language processing", arxiv:1506.07285

surveys

     * yann lecun, yoshua bengio, and geoffrey hinton, [197]deep learning,
       nature 2015
     * klaus greff, rupesh kumar srivastava, jan koutnik, bas r.
       steunebrink, jurgen schmidhuber, [198]lstm: a search space odyssey,
       arxiv:1503.04069
     * zachary c. lipton, [199]a critical review of recurrent neural
       networks for sequence learning, arxiv:1506.00019
     * andrej karpathy, justin johnson, li fei-fei, [200]visualizing and
       understanding recurrent networks, arxiv:1506.02078
     * rafal jozefowicz, wojciech zaremba, ilya sutskever, [201]an
       empirical exploration of recurrent network architectures, icml,
       2015.

applications

natural language processing

id38

     * tomas mikolov, martin karafiat, lukas burget, jan "honza" cernocky,
       sanjeev khudanpur, recurrent neural network based language model,
       interspeech 2010 [[202]paper]
     * tomas mikolov, stefan kombrink, lukas burget, jan "honza" cernocky,
       sanjeev khudanpur, extensions of recurrent neural network language
       model, icassp 2011 [[203]paper]
     * stefan kombrink, tomas mikolov, martin karafiat, lukas burget,
       recurrent neural network based id38 in meeting
       recognition, interspeech 2011 [[204]paper]
     * jiwei li, minh-thang luong, and dan jurafsky, a hierarchical neural
       autoencoder for paragraphs and documents, acl 2015 [[205]paper],
       [[206]code]
     * ryan kiros, yukun zhu, ruslan salakhutdinov, and richard s. zemel,
       skip-thought vectors, arxiv:1506.06726 / nips 2015 [[207]paper]
     * yoon kim, yacine jernite, david sontag, and alexander m. rush,
       character-aware neural language models, arxiv:1508.06615
       [[208]paper]
     * xingxing zhang, liang lu, and mirella lapata, tree recurrent neural
       networks with application to id38, arxiv:1511.00060
       [[209]paper]
     * felix hill, antoine bordes, sumit chopra, and jason weston, the
       goldilocks principle: reading children's books with explicit memory
       representations, arxiv:1511.0230 [[210]paper]

id103

     * geoffrey hinton, li deng, dong yu, george e. dahl, abdel-rahman
       mohamed, navdeep jaitly, andrew senior, vincent vanhoucke, patrick
       nguyen, tara n. sainath, and brian kingsbury, deep neural networks
       for acoustic modeling in id103, ieee signam processing
       magazine 2012 [[211]paper]
     * alex graves, abdel-rahman mohamed, and geoffrey hinton, speech
       recognition with deep recurrent neural networks, arxiv:1303.5778 /
       icassp 2013 [[212]paper]
     * jan chorowski, dzmitry bahdanau, dmitriy serdyuk, kyunghyun cho,
       and yoshua bengio, attention-based models for id103,
       arxiv:1506.07503 / nips 2015 [[213]paper]
     * ha  im sak, andrew senior, kanishka rao, and fran  oise beaufays.
       fast and accurate recurrent neural network acoustic models for
       id103, arxiv:1507.06947 2015 [[214]paper].

machine translation

     * oxford [[215]paper]
          + nal kalchbrenner and phil blunsom, recurrent continuous
            translation models, emnlp 2013
     * univ. montreal
          + kyunghyun cho, bart van berrienboer, caglar gulcehre, dzmitry
            bahdanau, fethi bougares, holger schwenk, and yoshua bengio,
            learning phrase representations using id56 encoder-decoder for
            id151, arxiv:1406.1078 / emnlp 2014
            [[216]paper]
          + kyunghyun cho, bart van merrienboer, dzmitry bahdanau, and
            yoshua bengio, on the properties of neural machine
            translation: encoder-decoder approaches, ssst-8 2014
            [[217]paper]
          + jean pouget-abadie, dzmitry bahdanau, bart van merrienboer,
            kyunghyun cho, and yoshua bengio, overcoming the curse of
            sentence length for id4 using automatic
            segmentation, ssst-8 2014
          + dzmitry bahdanau, kyunghyun cho, and yoshua bengio, neural
            machine translation by jointly learning to align and
            translate, arxiv:1409.0473 / iclr 2015 [[218]paper]
          + sebastian jean, kyunghyun cho, roland memisevic, and yoshua
            bengio, on using very large target vocabulary for neural
            machine translation, arxiv:1412.2007 / acl 2015 [[219]paper]
     * univ. montreal + middle east tech. univ. + univ. maine [[220]paper]
          + caglar gulcehre, orhan firat, kelvin xu, kyunghyun cho, loic
            barrault, huei-chi lin, fethi bougares, holger schwenk, and
            yoshua bengio, on using monolingual corpora in neural machine
            translation, arxiv:1503.03535
     * google [[221]paper]
          + ilya sutskever, oriol vinyals, and quoc v. le, sequence to
            sequence learning with neural networks, arxiv:1409.3215 / nips
            2014
     * google + nyu [[222]paper]
          + minh-thang luong, ilya sutskever, quoc v. le, oriol vinyals,
            and wojciech zaremba, addressing the rare word problem in
            neural machine transltaion, arxiv:1410.8206 / acl 2015
     * ict + huawei [[223]paper]
          + fandong meng, zhengdong lu, zhaopeng tu, hang li, and qun liu,
            a deep memory-based architecture for sequence-to-sequence
            learning, arxiv:1506.06442
     * stanford [[224]paper]
          + minh-thang luong, hieu pham, and christopher d. manning,
            effective approaches to attention-based neural machine
            translation, arxiv:1508.04025
     * middle east tech. univ. + nyu + univ. montreal [[225]paper]
          + orhan firat, kyunghyun cho, and yoshua bengio, multi-way,
            multilingual id4 with a shared
            attention mechanism, arxiv:1601.01073

conversation modeling

     * lifeng shang, zhengdong lu, and hang li, neural responding machine
       for short-text conversation, arxiv:1503.02364 / acl 2015
       [[226]paper]
     * oriol vinyals and quoc v. le, a neural conversational model,
       arxiv:1506.05869 [[227]paper]
     * ryan lowe, nissan pow, iulian v. serban, and joelle pineau, the
       ubuntu dialogue corpus: a large dataset for research in
       unstructured multi-turn dialogue systems, arxiv:1506.08909
       [[228]paper]
     * jesse dodge, andreea gane, xiang zhang, antoine bordes, sumit
       chopra, alexander miller, arthur szlam, and jason weston,
       evaluating prerequisite qualities for learning end-to-end dialog
       systems, arxiv:1511.06931 [[229]paper]
     * jason weston, dialog-based language learning, arxiv:1604.06045,
       [[230]paper]
     * antoine bordes and jason weston, learning end-to-end goal-oriented
       dialog, arxiv:1605.07683 [[231]paper]

id53

     * fair
          + jason weston, antoine bordes, sumit chopra, tomas mikolov, and
            alexander m. rush, towards ai-complete id53: a
            set of prerequisite toy tasks, arxiv:1502.05698 [[232]web]
            [[233]paper]
          + antoine bordes, nicolas usunier, sumit chopra, and jason
            weston, simple id53 with memory networks,
            arxiv:1506.02075 [[234]paper]
          + felix hill, antoine bordes, sumit chopra, jason weston, "the
            goldilocks principle: reading children's books with explicit
            memory representations", iclr 2016 [[235]paper]
     * deepmind + oxford [[236]paper]
          + karl m. hermann, tomas kocisky, edward grefenstette, lasse
            espeholt, will kay, mustafa suleyman, and phil blunsom,
            teaching machines to read and comprehend, arxiv:1506.03340 /
            nips 2015
     * metamind [[237]paper]
          + ankit kumar, ozan irsoy, jonathan su, james bradbury, robert
            english, brian pierce, peter ondruska, mohit iyyer, ishaan
            gulrajani, and richard socher, ask me anything: dynamic memory
            networks for natural language processing, arxiv:1506.07285

id161

object recognition

     * pedro pinheiro and ronan collobert, recurrent convolutional neural
       networks for scene labeling, icml 2014 [[238]paper]
     * ming liang and xiaolin hu, recurrent convolutional neural network
       for object recognition, cvpr 2015 [[239]paper]
     * wonmin byeon, thomas breuel, federico raue1, and marcus liwicki1,
       scene labeling with lstm recurrent neural networks, cvpr 2015
       [[240]paper]
     * mircea serban pavel, hannes schulz, and sven behnke, recurrent
       convolutional neural networks for object-class segmentation of
       rgb-d video, ijid98 2015 [[241]paper]
     * shuai zheng, sadeep jayasumana, bernardino romera-paredes, vibhav
       vineet, zhizhong su, dalong du, chang huang, and philip h. s. torr,
       id49 as recurrent neural networks,
       arxiv:1502.03240 [[242]paper]
     * xiaodan liang, xiaohui shen, donglai xiang, jiashi feng, liang lin,
       and shuicheng yan, semantic object parsing with local-global long
       short-term memory, arxiv:1511.04510 [[243]paper]
     * sean bell, c. lawrence zitnick, kavita bala, and ross girshick,
       inside-outside net: detecting objects in context with skip pooling
       and recurrent neural networks, arxiv:1512.04143 / iccv 2015
       workshop [[244]paper]

visual tracking

     * quan gan, qipeng guo, zheng zhang, and kyunghyun cho, first step
       toward model-free, anonymous object tracking with recurrent neural
       networks, arxiv:1511.06425 [[245]paper]

image generation

     * karol gregor, ivo danihelka, alex graves, danilo j. rezende, and
       daan wierstra, draw: a recurrent neural network for image
       generation, icml 2015 [[246]paper]
     * angeliki lazaridou, dat t. nguyen, r. bernardi, and m. baroni,
       unveiling the dreams of id27s: towards language-driven
       image generation, arxiv:1506.03500 [[247]paper]
     * lucas theis and matthias bethge, generative image modeling using
       spatial lstms, arxiv:1506.03478 / nips 2015 [[248]paper]
     * aaron van den oord, nal kalchbrenner, and koray kavukcuoglu, pixel
       recurrent neural networks, arxiv:1601.06759 [[249]paper]

video analysis

     * univ. toronto [[250]paper]
          + nitish srivastava, elman mansimov, ruslan salakhutdinov,
            unsupervised learning of video representations using lstms,
            arxiv:1502.04681 / icml 2015
     * univ. cambridge [[251]paper]
          + viorica patraucean, ankur handa, roberto cipolla,
            spatio-temporal video autoencoder with differentiable memory,
            arxiv:1511.06309

multimodal (cv + nlp)

image captioning

     * ucla + baidu [[252]web] [[253]paper-arxiv1], [[254]paper-arxiv2]
          + junhua mao, wei xu, yi yang, jiang wang, and alan l. yuille,
            explain images with multimodal recurrent neural networks,
            arxiv:1410.1090
          + junhua mao, wei xu, yi yang, jiang wang, zhiheng huang, and
            alan l. yuille, deep captioning with multimodal recurrent
            neural networks (m-id56), arxiv:1412.6632 / iclr 2015
     * univ. toronto [[255]paper] [[256]web demo]
          + ryan kiros, ruslan salakhutdinov, and richard s. zemel,
            unifying visual-semantic embeddings with multimodal neural
            language models, arxiv:1411.2539 / tacl 2015
     * berkeley [[257]web] [[258]paper]
          + jeff donahue, lisa anne hendricks, sergio guadarrama, marcus
            rohrbach, subhashini venugopalan, kate saenko, and trevor
            darrell, long-term recurrent convolutional networks for visual
            recognition and description, arxiv:1411.4389 / cvpr 2015
     * google [[259]paper]
          + oriol vinyals, alexander toshev, samy bengio, and dumitru
            erhan, show and tell: a neural image caption generator,
            arxiv:1411.4555 / cvpr 2015
     * stanford [260][web] [261][paper]
          + andrej karpathy and li fei-fei, deep visual-semantic
            alignments for generating image description, cvpr 2015
     * microsoft [[262]paper]
          + hao fang, saurabh gupta, forrest iandola, rupesh srivastava,
            li deng, piotr dollar, jianfeng gao, xiaodong he, margaret
            mitchell, john c. platt, lawrence zitnick, and geoffrey zweig,
            from captions to visual concepts and back, arxiv:1411.4952 /
            cvpr 2015
     * cmu + microsoft [[263]paper-arxiv], [[264]paper-cvpr]
          + xinlei chen, and c. lawrence zitnick, learning a recurrent
            visual representation for image id134
          + xinlei chen, and c. lawrence zitnick, mind   s eye: a recurrent
            visual representation for image id134, cvpr 2015
     * univ. montreal + univ. toronto [[265]web] [[266]paper]
          + kelvin xu, jimmy lei ba, ryan kiros, kyunghyun cho, aaron
            courville, ruslan salakhutdinov, richard s. zemel, and yoshua
            bengio, show, attend, and tell: neural image caption
            generation with visual attention, arxiv:1502.03044 / icml 2015
     * idiap + epfl + facebook [[267]paper]
          + remi lebret, pedro o. pinheiro, and ronan collobert,
            phrase-based image captioning, arxiv:1502.03671 / icml 2015
     * ucla + baidu [[268]paper]
          + junhua mao, wei xu, yi yang, jiang wang, zhiheng huang, and
            alan l. yuille, learning like a child: fast novel visual
            concept learning from sentence descriptions of images,
            arxiv:1504.06692
     * ms + berkeley
          + jacob devlin, saurabh gupta, ross girshick, margaret mitchell,
            and c. lawrence zitnick, exploring nearest neighbor approaches
            for image captioning, arxiv:1505.04467 (note: technically not
            id56) [[269]paper]
          + jacob devlin, hao cheng, hao fang, saurabh gupta, li deng,
            xiaodong he, geoffrey zweig, and margaret mitchell, language
            models for image captioning: the quirks and what works,
            arxiv:1505.01809 [[270]paper]
     * adelaide [[271]paper]
          + qi wu, chunhua shen, anton van den hengel, lingqiao liu, and
            anthony dick, image captioning with an intermediate attributes
            layer, arxiv:1506.01144
     * tilburg [[272]paper]
          + grzegorz chrupala, akos kadar, and afra alishahi, learning
            language through pictures, arxiv:1506.03694
     * univ. montreal [[273]paper]
          + kyunghyun cho, aaron courville, and yoshua bengio, describing
            multimedia content using attention-based encoder-decoder
            networks, arxiv:1507.01053
     * cornell [[274]paper]
          + jack hessel, nicolas savva, and michael j. wilber, image
            representations and new domains in neural image captioning,
            arxiv:1508.02091

video captioning

     * berkeley [[275]web] [[276]paper]
          + jeff donahue, lisa anne hendricks, sergio guadarrama, marcus
            rohrbach, subhashini venugopalan, kate saenko, and trevor
            darrell, long-term recurrent convolutional networks for visual
            recognition and description, arxiv:1411.4389 / cvpr 2015
     * ut austin + uml + berkeley [[277]paper]
          + subhashini venugopalan, huijuan xu, jeff donahue, marcus
            rohrbach, raymond mooney, and kate saenko, translating videos
            to natural language using deep recurrent neural networks,
            arxiv:1412.4729
     * microsoft [[278]paper]
          + yingwei pan, tao mei, ting yao, houqiang li, and yong rui,
            joint modeling embedding and translation to bridge video and
            language, arxiv:1505.01861
     * ut austin + berkeley + uml [[279]paper]
          + subhashini venugopalan, marcus rohrbach, jeff donahue, raymond
            mooney, trevor darrell, and kate saenko, sequence to
            sequence--video to text, arxiv:1505.00487
     * univ. montreal + univ. sherbrooke [[280]paper]
          + li yao, atousa torabi, kyunghyun cho, nicolas ballas,
            christopher pal, hugo larochelle, and aaron courville,
            describing videos by exploiting temporal structure,
            arxiv:1502.08029
     * mpi + berkeley [[281]paper]
          + anna rohrbach, marcus rohrbach, and bernt schiele, the
            long-short story of movie description, arxiv:1506.01698
     * univ. toronto + mit [[282]paper]
          + yukun zhu, ryan kiros, richard zemel, ruslan salakhutdinov,
            raquel urtasun, antonio torralba, and sanja fidler, aligning
            books and movies: towards story-like visual explanations by
            watching movies and reading books, arxiv:1506.06724
     * univ. montreal [[283]paper]
          + kyunghyun cho, aaron courville, and yoshua bengio, describing
            multimedia content using attention-based encoder-decoder
            networks, arxiv:1507.01053
     * zhejiang univ. + uts [[284]paper]
          + pingbo pan, zhongwen xu, yi yang, fei wu, yueting zhuang,
            hierarchical recurrent neural encoder for video representation
            with application to captioning, arxiv:1511.03476
     * univ. montreal + nyu + ibm [[285]paper]
          + li yao, nicolas ballas, kyunghyun cho, john r. smith, and
            yoshua bengio, empirical performance upper bounds for image
            and video captioning, arxiv:1511.04590

visual id53

     * virginia tech. + msr [[286]web] [[287]paper]
          + stanislaw antol, aishwarya agrawal, jiasen lu, margaret
            mitchell, dhruv batra, c. lawrence zitnick, and devi parikh,
            vqa: visual id53, arxiv:1505.00468 / cvpr 2015
            sunw:scene understanding workshop
     * mpi + berkeley [[288]web] [[289]paper]
          + mateusz malinowski, marcus rohrbach, and mario fritz, ask your
            neurons: a neural-based approach to answering questions about
            images, arxiv:1505.01121
     * univ. toronto [[290]paper] [[291]dataset]
          + mengye ren, ryan kiros, and richard zemel, exploring models
            and data for image id53, arxiv:1505.02074 / icml
            2015 deep learning workshop
     * baidu + ucla [[292]paper] [[293]dataset]
          + hauyuan gao, junhua mao, jie zhou, zhiheng huang, lei wang,
            and wei xu, are you talking to a machine? dataset and methods
            for multilingual image id53, arxiv:1505.05612 /
            nips 2015
     * snu + naver [[294]paper]
          + jin-hwa kim, sang-woo lee, dong-hyun kwak, min-oh heo,
            jeonghee kim, jung-woo ha, byoung-tak zhang, multimodal
            residual learning for visual qa, arxiv:1606:01455
     * uc berkeley + sony [[295]paper]
          + akira fukui, dong huk park, daylen yang, anna rohrbach, trevor
            darrell, and marcus rohrbach, multimodal compact bilinear
            pooling for visual id53 and visual grounding,
            arxiv:1606.01847
     * postech [[296]paper]
          + hyeonwoo noh and bohyung han, training recurrent answering
            units with joint loss minimization for vqa, arxiv:1606.03647
     * snu + naver [[297]paper]
          + jin-hwa kim, kyoung woon on, jeonghee kim, jung-woo ha,
            byoung-tak zhang, hadamard product for low-rank bilinear
            pooling, arxiv:1610.04325
     * video qa
          + cmu + uts [[298]paper]
               o linchao zhu, zhongwen xu, yi yang, alexander g.
                 hauptmann, uncovering temporal context for video question
                 and answering, arxiv:1511.04670
          + kit + mit + univ. toronto [[299]paper] [[300]dataset]
               o makarand tapaswi, yukun zhu, rainer stiefelhagen, antonio
                 torralba, raquel urtasun, sanja fidler, movieqa:
                 understanding stories in movies through
                 question-answering, arxiv:1512.02902

turing machines

     * a.graves, g. wayne, and i. danihelka., id63s,
       arxiv preprint arxiv:1410.5401 [[301]paper]
     * jason weston, sumit chopra, antoine bordes, memory networks,
       arxiv:1410.3916 [[302]paper]
     * armand joulin and tomas mikolov, inferring algorithmic patterns
       with stack-augmented recurrent nets, arxiv:1503.01007 / nips 2015
       [[303]paper]
     * sainbayar sukhbaatar, arthur szlam, jason weston, and rob fergus,
       end-to-end memory networks, arxiv:1503.08895 / nips 2015
       [[304]paper]
     * wojciech zaremba and ilya sutskever, id23 neural
       turing machines, arxiv:1505.00521 [[305]paper]
     * baolin peng and kaisheng yao, recurrent neural networks with
       external memory for language understanding, arxiv:1506.00195
       [[306]paper]
     * fandong meng, zhengdong lu, zhaopeng tu, hang li, and qun liu, a
       deep memory-based architecture for sequence-to-sequence learning,
       arxiv:1506.06442 [[307]paper]
     * arvind neelakantan, quoc v. le, and ilya sutskever, neural
       programmer: inducing latent programs with id119,
       arxiv:1511.04834 [[308]paper]
     * scott reed and nando de freitas, neural programmer-interpreters,
       arxiv:1511.06279 [[309]paper]
     * karol kurach, marcin andrychowicz, and ilya sutskever, neural
       random-access machines, arxiv:1511.06392 [[310]paper]
     *   ukasz kaiser and ilya sutskever, neural gpus learn algorithms,
       arxiv:1511.08228 [[311]paper]
     * ethan caballero, skip-thought memory networks, arxiv:1511.6420
       [[312]paper]
     * wojciech zaremba, tomas mikolov, armand joulin, and rob fergus,
       learning simple algorithms from examples, arxiv:1511.07275
       [[313]paper]

robotics

     * hongyuan mei, mohit bansal, and matthew r. walter, listen, attend,
       and walk: neural mapping of navigational instructions to action
       sequences, arxiv:1506.04089 [[314]paper]
     * marvin zhang, sergey levine, zoe mccarthy, chelsea finn, and pieter
       abbeel, policy learning with continuous memory states for partially
       observed robotic control, arxiv:1507.01273. [315][paper]

other

     * alex graves, generating sequences with recurrent neural networks,
       arxiv:1308.0850 [316][paper]
     * volodymyr mnih, nicolas heess, alex graves, and koray kavukcuoglu,
       recurrent models of visual attention, nips 2014 / arxiv:1406.6247
       [[317]paper]
     * wojciech zaremba and ilya sutskever, learning to execute,
       arxiv:1410.4615 [[318]paper] [[319]code]
     * samy bengio, oriol vinyals, navdeep jaitly, and noam shazeer,
       scheduled sampling for sequence prediction with recurrent neural
       networks, arxiv:1506.03099 / nips 2015 [[320]paper]
     * bing shuai, zhen zuo, gang wang, and bing wang, dag-recurrent
       neural networks for scene labeling, arxiv:1509.00552 [[321]paper]
     * soren kaae sonderby, casper kaae sonderby, lars maaloe, and ole
       winther, recurrent spatial transformer networks, arxiv:1509.05329
       [[322]paper]
     * cesar laurent, gabriel pereyra, philemon brakel, ying zhang, and
       yoshua bengio, batch normalized recurrent neural networks,
       arxiv:1510.01378 [[323]paper]
     * jiwon kim, jung kwon lee, kyoung mu lee, deeply-recursive
       convolutional network for image super-resolution, arxiv:1511.04491
       [324][paper]
     * quan gan, qipeng guo, zheng zhang, and kyunghyun cho, first step
       toward model-free, anonymous object tracking with recurrent neural
       networks, arxiv:1511.06425 [[325]paper]
     * francesco visin, kyle kastner, aaron courville, yoshua bengio,
       matteo matteucci, and kyunghyun cho, reseg: a recurrent neural
       network for object segmentation, arxiv:1511.07053 [[326]paper]
     * juergen schmidhuber, on learning to think: algorithmic information
       theory for novel combinations of id23 controllers
       and recurrent neural world models, arxiv:1511.09249 [327][paper]

datasets

     * id103
          + [328]openslr (open speech and language resources)
               o [329]librispeech asr corpus
          + [330]voxforge
     * image captioning
          + [331]flickr 8k
          + [332]flickr 30k
          + [333]microsoft coco
     * id53
          + [334]the babi project - dataset for text understanding and
            reasoning, by facebook ai research. contains:
               o the (20) qa babi tasks - [[335]paper]
               o the (6) dialog babi tasks - [[336]paper]
               o the children's book test - [[337]paper]
               o the movie dialog dataset - [[338]paper]
               o the movieqa dataset - [[339]data]
               o the dialog-based language learning dataset - [[340]paper]
               o the simplequestions dataset - [[341]paper]
          + [342]squad - stanford id53 dataset :
            [[343]paper]
     * image id53
          + [344]daquar - built upon [345]nyu depth v2 by n. silberman et
            al.
          + [346]vqa - based on [347]mscoco images
          + [348]image qa - based on mscoco images
          + [349]multilingual image qa - built from scratch by baidu - in
            chinese, with english translation
     * action recognition
          + [350]thumos : large-scale action recognition dataset
          + [351]multithumos : extension of thumos '14 action detection
            dataset with dense multilabele annotation

blogs

     * [352]the unreasonable effectiveness of id56s by [353]andrej karpathy
     * [354]understanding id137 in [355]colah's blog
     * [356]wildml blog's id56 tutorial [[357]part1], [[358]part2],
       [[359]part3], [[360]part4]
     * [361]id56s in tensorflow, a practical guide and undocumented
       features
     * [362]optimizing id56 performance from baidu's silicon valley ai lab.
     * [363]character level language modelling using id56 by yoav goldberg
     * [364]implement an id56 in python.
     * [365]lstm backpropogation
     * [366]introduction to recurrent networks in tensorflow by danijar
       hafner
     * [367]variable sequence lengths in tensorflow by danijar hafner
     * [368]written memories: understanding, deriving and extending the
       lstm by silviu pitis

online demos

     * alex graves, hand-writing generation [[369]link]
     * ink poster: handwritten post-it notes [[370]link]
     * lstmvis: visual analysis for recurrent neural networks [[371]link]

     *    2019 github, inc.
     * [372]terms
     * [373]privacy
     * [374]security
     * [375]status
     * [376]help

     * [377]contact github
     * [378]pricing
     * [379]api
     * [380]training
     * [381]blog
     * [382]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [383]reload to refresh your
   session. you signed out in another tab or window. [384]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/kjw0612/awesome-id56/commits/master.atom
   3. https://github.com/kjw0612/awesome-id56#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/kjw0612/awesome-id56
  32. https://github.com/join
  33. https://github.com/login?return_to=/kjw0612/awesome-id56
  34. https://github.com/kjw0612/awesome-id56/watchers
  35. https://github.com/login?return_to=/kjw0612/awesome-id56
  36. https://github.com/kjw0612/awesome-id56/stargazers
  37. https://github.com/login?return_to=/kjw0612/awesome-id56
  38. https://github.com/kjw0612/awesome-id56/network/members
  39. https://github.com/kjw0612
  40. https://github.com/kjw0612/awesome-id56
  41. https://github.com/kjw0612/awesome-id56
  42. https://github.com/kjw0612/awesome-id56/issues
  43. https://github.com/kjw0612/awesome-id56/pulls
  44. https://github.com/kjw0612/awesome-id56/projects
  45. https://github.com/kjw0612/awesome-id56/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/kjw0612/awesome-id56/commits/master
  48. https://github.com/kjw0612/awesome-id56/branches
  49. https://github.com/kjw0612/awesome-id56/releases
  50. https://github.com/kjw0612/awesome-id56/graphs/contributors
  51. https://github.com/kjw0612/awesome-id56/find/master
  52. https://github.com/kjw0612/awesome-id56/archive/master.zip
  53. https://github.com/login?return_to=https://github.com/kjw0612/awesome-id56
  54. https://github.com/join?return_to=/kjw0612/awesome-id56
  55. https://desktop.github.com/
  56. https://desktop.github.com/
  57. https://developer.apple.com/xcode/
  58. https://visualstudio.github.com/
  59. https://github.com/myungsub
  60. https://github.com/kjw0612/awesome-id56/commits?author=myungsub
  61. https://github.com/kjw0612/awesome-id56/commit/6f344613d61cd11608793a5c99461f2ac472ce6b
  62. https://github.com/kjw0612/awesome-id56/pull/33
  63. https://github.com/kjw0612/awesome-id56/commit/6f344613d61cd11608793a5c99461f2ac472ce6b
  64. https://github.com/kjw0612/awesome-id56/commit/6f344613d61cd11608793a5c99461f2ac472ce6b
  65. https://github.com/kjw0612/awesome-id56/tree/6f344613d61cd11608793a5c99461f2ac472ce6b
  66. https://github.com/kjw0612/awesome-id56/blob/master/readme.md
  67. https://github.com/myungsub
  68. https://github.com/jazzsaxmafia
  69. https://github.com/kjw0612
  70. https://github.com/kjw0612/awesome-deep-vision
  71. https://github.com/kjw0612/awesome-random-forest
  72. https://github.com/kjw0612/awesome-id56/pulls
  73. mailto:cms6539@gmail.com
  74. https://gitter.im/kjw0612/awesome-id56?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
  75. http://twitter.com/home?status=http://jiwonkim.org/awesome-id56
resources for recurrent neural networks
  76. http://www.facebook.com/sharer/sharer.php?u=https://jiwonkim.org/awesome-id56
  77. http://plus.google.com/share?url=https://jiwonkim.org/awesome-id56
  78. http://www.linkedin.com/sharearticle?mini=true&url=https://jiwonkim.org/awesome-id56&title=awesome recurrent neural&networks&summary=&source=
  79. https://github.com/kjw0612/awesome-id56#codes
  80. https://github.com/kjw0612/awesome-id56#theory
  81. https://github.com/kjw0612/awesome-id56#lectures
  82. https://github.com/kjw0612/awesome-id56#books--thesis
  83. https://github.com/kjw0612/awesome-id56#architecture-variants
  84. https://github.com/kjw0612/awesome-id56#structure
  85. https://github.com/kjw0612/awesome-id56#memory
  86. https://github.com/kjw0612/awesome-id56#surveys
  87. https://github.com/kjw0612/awesome-id56#applications
  88. https://github.com/kjw0612/awesome-id56#natural-language-processing
  89. https://github.com/kjw0612/awesome-id56#language-modeling
  90. https://github.com/kjw0612/awesome-id56#speech-recognition
  91. https://github.com/kjw0612/awesome-id56#machine-translation
  92. https://github.com/kjw0612/awesome-id56#conversation-modeling
  93. https://github.com/kjw0612/awesome-id56#question-answering
  94. https://github.com/kjw0612/awesome-id56#computer-vision
  95. https://github.com/kjw0612/awesome-id56#object-recognition
  96. https://github.com/kjw0612/awesome-id56#image-generation
  97. https://github.com/kjw0612/awesome-id56#video-analysis
  98. https://github.com/kjw0612/awesome-id56#multimodal-cv--nlp
  99. https://github.com/kjw0612/awesome-id56#image-captioning
 100. https://github.com/kjw0612/awesome-id56#video-captioning
 101. https://github.com/kjw0612/awesome-id56#visual-question-answering
 102. https://github.com/kjw0612/awesome-id56#turing-machines
 103. https://github.com/kjw0612/awesome-id56#robotics
 104. https://github.com/kjw0612/awesome-id56#other
 105. https://github.com/kjw0612/awesome-id56#datasets
 106. https://github.com/kjw0612/awesome-id56#blogs
 107. https://github.com/kjw0612/awesome-id56#online-demos
 108. https://www.tensorflow.org/
 109. https://www.tensorflow.org/versions/master/get_started/index.html
 110. https://www.tensorflow.org/versions/master/tutorials/index.html
 111. https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html
 112. https://www.tensorflow.org/versions/master/tutorials/id195/index.html
 113. https://github.com/nlintz/tensorflow-tutorials
 114. https://github.com/aymericdamien/tensorflow-examples
 115. https://github.com/tensorflow/skflow
 116. http://keras.io/
 117. https://github.com/sherjilozair/char-id56-tensorflow
 118. http://deeplearning.net/software/theano/
 119. http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb
 120. http://www.deeplearning.net/tutorial/
 121. http://www.deeplearning.net/tutorial/id56slu.html#id56slu
 122. http://www.deeplearning.net/tutorial/lstm.html#lstm
 123. http://deeplearning.net/software/pylearn2/
 124. https://github.com/mila-udem/blocks
 125. http://keras.io/
 126. https://github.com/lasagne/lasagne
 127. https://github.com/gwtaylor/theano-id56
 128. https://github.com/indicodatasolutions/passage
 129. https://github.com/ivaylo-popov/theano-lights
 130. https://github.com/bvlc/caffe
 131. http://jeffdonahue.com/lrcn/
 132. http://torch.ch/
 133. https://github.com/torchnet/torchnet
 134. https://github.com/karpathy/char-id56
 135. https://github.com/jcjohnson/torch-id56
 136. https://github.com/karpathy/neuraltalk2
 137. https://github.com/karpathy/neuraltalk
 138. https://github.com/wojzaremba/lstm
 139. https://github.com/oxford-cs-ml-2015
 140. https://github.com/element-research/id56
 141. http://pytorch.org/
 142. https://github.com/pytorch/examples/tree/master/word_language_model
 143. https://github.com/spro/practical-pytorch
 144. https://github.com/rguthrie3/deeplearningfornlpinpytorch
 145. http://deeplearning4j.org/
 146. http://www.skymind.io/
 147. http://deeplearning4j.org/
 148. http://deeplearning4j.org/zh-index.html
 149. http://deeplearning4j.org/ja-index.html
 150. http://deeplearning4j.org/kr-index.html
 151. http://deeplearning4j.org/usingid56s.html
 152. http://deeplearning4j.org/lstm.html
 153. https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent
 154. http://neon.nervanasys.com/docs/latest/index.html
 155. https://github.com/idsia/brainstorm
 156. http://chainer.org/
 157. http://joschu.github.io/
 158. https://sourceforge.net/p/id56l/wiki/home/
 159. http://id56lm.org/
 160. https://github.com/yandex/faster-id56lm
 161. https://github.com/karpathy/neuraltalk
 162. https://gist.github.com/karpathy/587454dc0146a6ae21fc
 163. https://github.com/karpathy/recurrentjs
 164. https://github.com/5vision/darqn
 165. http://cs224d.stanford.edu/index.html
 166. http://cs224d.stanford.edu/lecture_notes/lecturenotes3.pdf
 167. http://cs224d.stanford.edu/lecture_notes/lecturenotes4.pdf
 168. http://cs231n.github.io/
 169. https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/
 170. https://www.youtube.com/watch?v=56tylaqn4n8
 171. https://www.youtube.com/watch?v=-yx1syedhbg
 172. http://www.cs.toronto.edu/~graves/preprint.pdf
 173. http://www.fit.vutbr.cz/~imikolov/id56lm/thesis.pdf
 174. http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf
 175. http://nlp.stanford.edu/~socherr/thesis.pdf
 176. http://www.deeplearningbook.org/contents/id56.html
 177. http://www.di.ufpe.br/~fnj/rna/bibliografia/bid56.pdf
 178. http://arxiv.org/pdf/0705.2011.pdf
 179. http://arxiv.org/pdf/1502.02367
 180. http://jmlr.org/proceedings/papers/v37/chung15.pdf
 181. http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf
 182. http://arxiv.org/pdf/1503.00075
 183. http://arxiv.org/pdf/1506.04834
 184. http://arxiv.org/pdf/1507.01526
 185. https://github.com/coreylynch/grid-lstm
 186. http://arxiv.org/pdf/1511.06018v2.pdf
 187. http://arxiv.org/pdf/1511.06391v4.pdf
 188. http://arxiv.org/abs/1609.01704
 189. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
 190. http://arxiv.org/pdf/1406.1078.pdf
 191. http://arxiv.org/pdf/1410.5401
 192. http://arxiv.org/pdf/1511.08228.pdf
 193. http://arxiv.org/pdf/1410.3916
 194. http://arxiv.org/pdf/1506.03134
 195. http://arxiv.org/abs/1512.01693
 196. http://arxiv.org/abs/1506.07285
 197. http://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf
 198. http://arxiv.org/pdf/1503.04069
 199. http://arxiv.org/pdf/1506.00019
 200. http://arxiv.org/pdf/1506.02078
 201. http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf
 202. http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_is100722.pdf
 203. http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf
 204. http://www.fit.vutbr.cz/~imikolov/id56lm/applicationofid56inmeetingrecognition_is2011.pdf
 205. http://arxiv.org/pdf/1506.01057
 206. https://github.com/jiweil/hierarchical-neural-autoencoder
 207. http://arxiv.org/pdf/1506.06726.pdf
 208. http://arxiv.org/pdf/1508.06615
 209. http://arxiv.org/pdf/1511.00060.pdf
 210. http://arxiv.org/pdf/1511.02301.pdf
 211. http://cs224d.stanford.edu/papers/maas_paper.pdf
 212. http://www.cs.toronto.edu/~fritz/absps/id5613.pdf
 213. http://arxiv.org/pdf/1506.07503
 214. http://arxiv.org/pdf/1507.06947v1.pdf
 215. http://www.nal.ai/papers/kalchbrennerblunsom_emnlp13
 216. http://arxiv.org/pdf/1406.1078
 217. http://www.aclweb.org/anthology/w14-4012
 218. http://arxiv.org/pdf/1409.0473
 219. http://arxiv.org/pdf/1412.2007.pdf
 220. http://arxiv.org/pdf/1503.03535.pdf
 221. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
 222. http://arxiv.org/pdf/1410.8206
 223. http://arxiv.org/pdf/1506.06442.pdf
 224. http://arxiv.org/pdf/1508.04025.pdf
 225. http://arxiv.org/pdf/1601.01073.pdf
 226. http://arxiv.org/pdf/1503.02364
 227. http://arxiv.org/pdf/1506.05869
 228. http://arxiv.org/pdf/1506.08909
 229. http://arxiv.org/pdf/1511.06931
 230. http://arxiv.org/pdf/1604.06045
 231. http://arxiv.org/pdf/1605.07683
 232. https://research.facebook.com/researchers/1543934539189348
 233. http://arxiv.org/pdf/1502.05698.pdf
 234. http://arxiv.org/abs/1506.02075
 235. http://arxiv.org/abs/1511.02301
 236. http://arxiv.org/pdf/1506.03340.pdf
 237. http://arxiv.org/pdf/1506.07285.pdf
 238. http://jmlr.org/proceedings/papers/v32/pinheiro14.pdf
 239. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/liang_recurrent_convolutional_neural_2015_cvpr_paper.pdf
 240. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/byeon_scene_labeling_with_2015_cvpr_paper.pdf
 241. http://www.ais.uni-bonn.de/papers/ijid98_2015_pavel.pdf
 242. http://arxiv.org/pdf/1502.03240
 243. http://arxiv.org/pdf/1511.04510.pdf
 244. http://arxiv.org/pdf/1512.04143
 245. http://arxiv.org/pdf/1511.06425
 246. http://arxiv.org/pdf/1502.04623
 247. http://arxiv.org/pdf/1506.03500
 248. http://arxiv.org/pdf/1506.03478
 249. http://arxiv.org/abs/1601.06759
 250. http://arxiv.org/abs/1502.04681
 251. http://arxiv.org/abs/1511.06309
 252. http://www.stat.ucla.edu/~junhua.mao/m-id56.html
 253. http://arxiv.org/pdf/1410.1090
 254. http://arxiv.org/pdf/1412.6632
 255. http://arxiv.org/pdf/1411.2539
 256. http://deeplearning.cs.toronto.edu/i2t
 257. http://jeffdonahue.com/lrcn/
 258. http://arxiv.org/pdf/1411.4389
 259. http://arxiv.org/pdf/1411.4555
 260. http://cs.stanford.edu/people/karpathy/deepimagesent/
 261. http://cs.stanford.edu/people/karpathy/cvpr2015.pdf
 262. http://arxiv.org/pdf/1411.4952
 263. http://arxiv.org/pdf/1411.5654
 264. http://www.cs.cmu.edu/~xinleic/papers/cvpr15_id56.pdf
 265. http://kelvinxu.github.io/projects/capgen.html
 266. http://www.cs.toronto.edu/~zemel/documents/captionattn.pdf
 267. http://arxiv.org/pdf/1502.03671
 268. http://arxiv.org/pdf/1504.06692
 269. http://arxiv.org/pdf/1505.04467.pdf
 270. http://arxiv.org/pdf/1505.01809.pdf
 271. http://arxiv.org/pdf/1506.01144.pdf
 272. http://arxiv.org/pdf/1506.03694.pdf
 273. http://arxiv.org/pdf/1507.01053.pdf
 274. http://arxiv.org/pdf/1508.02091.pdf
 275. http://jeffdonahue.com/lrcn/
 276. http://arxiv.org/pdf/1411.4389
 277. http://arxiv.org/pdf/1412.4729
 278. http://arxiv.org/pdf/1505.01861
 279. http://arxiv.org/pdf/1505.00487
 280. http://arxiv.org/pdf/1502.08029.pdf
 281. http://arxiv.org/pdf/1506.01698.pdf
 282. http://arxiv.org/pdf/1506.06724.pdf
 283. http://arxiv.org/pdf/1507.01053.pdf
 284. http://arxiv.org/abs/1511.03476
 285. http://arxiv.org/pdf/1511.04590.pdf
 286. http://www.visualqa.org/
 287. http://arxiv.org/pdf/1505.00468
 288. https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/
 289. http://arxiv.org/pdf/1505.01121
 290. http://arxiv.org/pdf/1505.02074
 291. http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/
 292. http://arxiv.org/pdf/1505.05612
 293. https://github.com/kjw0612/awesome-id56/blob/master
 294. http://arxiv.org/abs/1606.01455
 295. https://arxiv.org/pdf/1606.01847
 296. http://arxiv.org/pdf/1606.03647.pdf
 297. http://arxiv.org/abs/1610.04325
 298. http://arxiv.org/abs/1511.04670
 299. http://arxiv.org/abs/1512.02902
 300. http://movieqa.cs.toronto.edu/home/
 301. http://arxiv.org/pdf/1410.5401
 302. http://arxiv.org/pdf/1410.3916
 303. http://arxiv.org/pdf/1503.01007
 304. http://arxiv.org/pdf/1503.08895
 305. http://arxiv.org/pdf/1505.00521
 306. http://arxiv.org/pdf/1506.00195.pdf
 307. http://arxiv.org/pdf/1506.06442.pdf
 308. http://arxiv.org/pdf/1511.04834.pdf
 309. http://arxiv.org/pdf/1511.06279.pdf
 310. http://arxiv.org/pdf/1511.06392.pdf
 311. http://arxiv.org/pdf/1511.08228.pdf
 312. https://pdfs.semanticscholar.org/6b9f/0d695df0ce01d005eb5aa69386cb5fbac62a.pdf
 313. http://arxiv.org/pdf/1511.07275.pdf
 314. http://arxiv.org/pdf/1506.04089.pdf
 315. http://arxiv.org/pdf/1507.01273
 316. http://arxiv.org/abs/1308.0850
 317. http://arxiv.org/pdf/1406.6247.pdf
 318. http://arxiv.org/pdf/1410.4615.pdf
 319. https://github.com/wojciechz/learning_to_execute
 320. http://arxiv.org/pdf/1506.03099
 321. http://arxiv.org/pdf/1509.00552
 322. http://arxiv.org/pdf/1509.05329
 323. http://arxiv.org/pdf/1510.01378
 324. http://arxiv.org/abs/1511.04491
 325. http://arxiv.org/pdf/1511.06425.pdf
 326. http://arxiv.org/pdf/1511.07053.pdf
 327. http://arxiv.org/pdf/1511.09249
 328. http://www.openslr.org/resources.php
 329. http://www.openslr.org/12/
 330. http://voxforge.org/home
 331. http://nlp.cs.illinois.edu/hockenmaiergroup/framing_image_description/kcca.html
 332. http://shannon.cs.illinois.edu/denotationgraph/
 333. http://mscoco.org/home/
 334. http://fb.ai/babi
 335. http://arxiv.org/abs/1502.05698
 336. http://arxiv.org/abs/1605.07683
 337. http://arxiv.org/abs/1511.02301
 338. http://arxiv.org/abs/1511.06931
 339. http://www.thespermwhale.com/jaseweston/babi/movie_dialog_dataset.tgz
 340. http://arxiv.org/abs/1604.06045
 341. http://arxiv.org/abs/1506.02075
 342. https://stanford-qa.com/
 343. http://arxiv.org/pdf/1606.05250
 344. https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/
 345. http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html
 346. http://www.visualqa.org/
 347. http://mscoco.org/
 348. http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/
 349. http://idl.baidu.com/fm-iqa.html
 350. http://www.thumos.info/home.html
 351. http://ai.stanford.edu/~syyeung/resources/multithumos.zip
 352. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 353. http://cs.stanford.edu/people/karpathy/
 354. http://colah.github.io/posts/2015-08-understanding-lstms/
 355. http://colah.github.io/
 356. http://www.wildml.com/
 357. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
 358. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
 359. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
 360. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
 361. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
 362. https://svail.github.io/
 363. http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139
 364. http://peterroelants.github.io/posts/id56_implementation_part01/
 365. http://arunmallya.github.io/writeups/nn/lstm/index.html#/
 366. https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/
 367. https://danijar.com/variable-sequence-lengths-in-tensorflow/
 368. http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
 369. http://www.cs.toronto.edu/~graves/handwriting.html
 370. http://www.inkposter.com/?
 371. http://lstm.seas.harvard.edu/
 372. https://github.com/site/terms
 373. https://github.com/site/privacy
 374. https://github.com/security
 375. https://githubstatus.com/
 376. https://help.github.com/
 377. https://github.com/contact
 378. https://github.com/pricing
 379. https://developer.github.com/
 380. https://training.github.com/
 381. https://github.blog/
 382. https://github.com/about
 383. https://github.com/kjw0612/awesome-id56
 384. https://github.com/kjw0612/awesome-id56

   hidden links:
 386. https://github.com/
 387. https://github.com/kjw0612/awesome-id56
 388. https://github.com/kjw0612/awesome-id56
 389. https://github.com/kjw0612/awesome-id56
 390. https://help.github.com/articles/which-remote-url-should-i-use
 391. https://github.com/kjw0612/awesome-id56#awesome-recurrent-neural-networks
 392. https://github.com/kjw0612/awesome-id56#contributing
 393. https://github.com/kjw0612/awesome-id56#sharing
 394. https://github.com/kjw0612/awesome-id56#table-of-contents
 395. https://github.com/kjw0612/awesome-id56#codes
 396. https://github.com/kjw0612/awesome-id56#theory
 397. https://github.com/kjw0612/awesome-id56#lectures
 398. https://github.com/kjw0612/awesome-id56#books--thesis
 399. https://github.com/kjw0612/awesome-id56#architecture-variants
 400. https://github.com/kjw0612/awesome-id56#structure
 401. https://github.com/kjw0612/awesome-id56#memory
 402. https://github.com/kjw0612/awesome-id56#surveys
 403. https://github.com/kjw0612/awesome-id56#applications
 404. https://github.com/kjw0612/awesome-id56#natural-language-processing
 405. https://github.com/kjw0612/awesome-id56#language-modeling
 406. https://github.com/kjw0612/awesome-id56#speech-recognition
 407. https://github.com/kjw0612/awesome-id56#machine-translation
 408. https://github.com/kjw0612/awesome-id56#conversation-modeling
 409. https://github.com/kjw0612/awesome-id56#question-answering
 410. https://github.com/kjw0612/awesome-id56#computer-vision
 411. https://github.com/kjw0612/awesome-id56#object-recognition
 412. https://github.com/kjw0612/awesome-id56#visual-tracking
 413. https://github.com/kjw0612/awesome-id56#image-generation
 414. https://github.com/kjw0612/awesome-id56#video-analysis
 415. https://github.com/kjw0612/awesome-id56#multimodal-cv--nlp
 416. https://github.com/kjw0612/awesome-id56#image-captioning
 417. https://github.com/kjw0612/awesome-id56#video-captioning
 418. https://github.com/kjw0612/awesome-id56#visual-question-answering
 419. https://github.com/kjw0612/awesome-id56#turing-machines
 420. https://github.com/kjw0612/awesome-id56#robotics
 421. https://github.com/kjw0612/awesome-id56#other
 422. https://github.com/kjw0612/awesome-id56#datasets
 423. https://github.com/kjw0612/awesome-id56#blogs
 424. https://github.com/kjw0612/awesome-id56#online-demos
 425. https://github.com/
