   #[1]github [2]recent commits to models:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]2,874
     * [35]star [36]50,805
     * [37]fork [38]31,194

[39]tensorflow/[40]models

   [41]code [42]issues 1,174 [43]pull requests 360 [44]projects 2
   [45]insights
   branch: master
   (button) create new file [46]find file [47]history
   [48]models/[49]research/syntaxnet/
   [50]@gatoatigrado
   [51]gatoatigrado [52]syntaxnet/dragnn visualization: update webpack and
   (necessarily) babe    (button)    
   l. ([53]#6058)

   latest commit [54]8d5d9d1 jan 17, 2019
   [55]permalink
   type name latest commit message commit time
   [56]..
   failed to load latest commit information.
   [57]docker-devel
   [58]dragnn [59]syntaxnet/dragnn visualization: update webpack and
   (necessarily) babe    jan 16, 2019
   [60]examples/dragnn
   [61]g3doc
   [62]syntaxnet
   [63]tensorflow @ 8753e2e
   [64]third_party/utf
   [65]tools
   [66]util/utf8
   [67].dockerignore
   [68].gitignore
   [69]dockerfile
   [70]readme.md
   [71]workspace

readme.md

syntaxnet: neural models of syntax.

   a tensorflow toolkit for deep learning powered natural language
   understanding (nlu).

   conll: see [72]here for instructions for using the syntaxnet/dragnn
   baseline for the conll2017 shared task.

   at google, we spend a lot of time thinking about how computer systems
   can read and understand human language in order to process it in
   intelligent ways. we are excited to share the fruits of our research
   with the broader community by releasing syntaxnet, an open-source
   neural network framework for [73]tensorflow that provides a foundation
   for natural language understanding (nlu) systems. our release includes
   all the code needed to train new syntaxnet models on your own data, as
   well as a suite of models that we have trained for you, and that you
   can use to analyze text in over 40 languages.

   this repository is largely divided into two sub-packages:
    1. dragnn: [74]code, [75]documentation, [76]paper implements dynamic
       recurrent acyclic graphical neural networks (dragnn), a framework
       for building multi-task, fully dynamically constructed computation
       graphs. practically, we use dragnn to extend our prior work from
       [77]andor et al. (2016) with end-to-end, deep recurrent models and
       to provide a much easier to use interface to syntaxnet. dragnn is
       designed first and foremost as a python library, and therefore much
       easier to use than the original syntaxnet implementation.
    2. syntaxnet: [78]code, [79]documentation is a transition-based
       framework for natural language processing, with core functionality
       for feature extraction, representing annotated data, and
       evaluation. as of the dragnn release, it is recommended to train
       and deploy syntaxnet models using the dragnn framework.

how to use this library

   there are three ways to use syntaxnet:
     * see [80]here for instructions for using the syntaxnet/dragnn
       baseline for the conll2017 shared task, and running the
       parseysaurus models.
     * you can use dragnn to train your nlp models for other tasks and
       dataset. see "getting started with dragnn" below.
     * you can continue to use the parsey mcparseface family of
       pre-trained syntaxnet models. see "pre-trained nlp models" below.

installation

docker installation

   this process takes ~10 minutes.

   the simplest way to get started with dragnn is by loading our docker
   container. [81]here is a tutorial for running the dragnn container on
   [82]gcp (just as applicable to your own computer).

ubuntu 16.04+ binary installation

   this process takes ~5 minutes, but is only compatible with linux using
   gnu libc 3.4.22 and above (e.g. ubuntu 16.04).

   binary wheel packages are provided for tensorflow and syntaxnet. if you
   do not need to write new binary tensorflow ops, these should suffice.
     * apt-get install -y graphviz libgraphviz-dev libopenblas-base
       libpng16-16 libxft2 python-pip python-mock
     * pip install pygraphviz
       --install-option="--include-path=/usr/include/graphviz"
       --install-option="--library-path=/usr/lib/graphviz/"
     * pip install 'ipython<6.0' protobuf numpy scipy jupyter
       syntaxnet-with-tensorflow
     * python -m jupyter_core.command nbextension enable --py --sys-prefix
       widgetsnbextension

   you can test that binary modules can be successfully imported by
   running,
     * python -c 'import dragnn.python.load_dragnn_cc_impl,
       syntaxnet.load_parser_ops'

manual installation

   this process takes 1-2 hours.

   running and training syntaxnet/dragnn models requires building this
   package from source. you'll need to install:
     * python 2.7:
          + python 3 support is not available yet
     * bazel 0.11.1:
          + follow the instructions [83]here
          + alternately, download bazel 0.11.1 <.deb> from
            [84]https://github.com/bazelbuild/bazel/releases for your
            system configuration.
          + install it using the command: sudo dpkg -i <.deb file>
          + check for the bazel version by typing: bazel version
     * swig:
          + apt-get install swig on ubuntu
          + brew install swig on osx
     * protocol buffers, with a version supported by tensorflow:
          + check your protobuf version with pip freeze | grep protobuf
          + upgrade to a supported version with pip install -u
            protobuf==3.3.0
     * autograd, with a version supported by tensorflow:
          + pip install -u autograd==1.1.13
     * mock, the testing package:
          + pip install mock
     * asciitree, to draw parse trees on the console for the demo:
          + pip install asciitree
     * numpy, package for scientific computing:
          + pip install numpy
     * pygraphviz to visualize traces and parse trees:
          + apt-get install -y graphviz libgraphviz-dev
          + pip install pygraphviz
            --install-option="--include-path=/usr/include/graphviz"
            --install-option="--library-path=/usr/lib/graphviz/"

   once you completed the above steps, you can build and test syntaxnet
   with the following commands:
  git clone --recursive https://github.com/tensorflow/models.git
  cd models/research/syntaxnet/tensorflow
  ./configure
  cd ..
  bazel test ...
  # on mac, run the following:
  bazel test --linkopt=-headerpad_max_install_names \
    dragnn/... syntaxnet/... util/utf8/...

   bazel should complete reporting all tests passed.

   now you can install the syntaxnet and dragnn python modules with the
   following commands:
  mkdir /tmp/syntaxnet_pkg
  bazel-bin/dragnn/tools/build_pip_package --output-dir=/tmp/syntaxnet_pkg
  #  the filename of the .whl depends on your platform.
  sudo pip install /tmp/syntaxnet_pkg/syntaxnet-x.xx-none-any.whl

   to build syntaxnet with gpu support please refer to the instructions in
   [85]issues/248.

   note: if you are running docker on osx, make sure that you have enough
   memory allocated for your docker vm.

getting started

   we have a few guides on this readme, as well as more extensive
   [86]documentation.

learning the dragnn framework

   [87]dragnn

   an easy and visual way to get started with dragnn is to run our jupyter
   notebooks for [88]interactive debugging and [89]training a new model.
   our tutorial [90]here explains how to start it up from the docker
   container. once you have dragnn installed and running, try out the
   [91]parseysaurus models.

using the pre-trained nlp models

   we are happy to release parsey mcparseface, an english parser that we
   have trained for you, and that you can use to analyze english text,
   along with [92]trained models for 40 languages and support for text
   segmentation and morphological analysis.

   once you have successfully built syntaxnet, you can start parsing text
   right away with parsey mcparseface, located under syntaxnet/models. the
   easiest thing is to use or modify the included script
   syntaxnet/demo.sh, which shows a basic setup to parse english taking
   plain text as input.

   you can also skip right away to the [93]detailed syntaxnet tutorial.

   how accurate is parsey mcparseface? for the initial release, we tried
   to balance a model that runs fast enough to be useful on a single
   machine (e.g. ~600 words/second on a modern desktop) and that is also
   the most accurate parser available. here's how parsey mcparseface
   compares to the academic literature on several different english
   domains: (all numbers are % correct head assignments in the tree, or
   unlabelled attachment score)
               model             news   web  questions
   [94]martins et al. (2013)     93.10 88.23   94.21
   [95]zhang and mcdonald (2014) 93.32 88.65   93.37
   [96]weiss et al. (2015)       93.91 89.29   94.17
   [97]andor et al. (2016)*      94.44 90.17   95.40
   parsey mcparseface            94.15 89.08   94.77

   we see that parsey mcparseface is state-of-the-art; more importantly,
   with syntaxnet you can train larger networks with more hidden units and
   bigger beam sizes if you want to push the accuracy even further:
   [98]andor et al. (2016)* is simply a syntaxnet model with a larger beam
   and network. for futher information on the datasets, see that paper
   under the section "treebank union".

   parsey mcparseface is also state-of-the-art for part-of-speech (pos)
   tagging (numbers below are per-token accuracy):
             model           news   web  questions
   [99]ling et al. (2015)    97.44 94.03   96.18
   [100]andor et al. (2016)* 97.77 94.80   96.86
   parsey mcparseface        97.52 94.24   96.45

parsing from standard input

   simply pass one sentence per line of text into the script at
   syntaxnet/demo.sh. the script will break the text into words, run the
   pos tagger, run the parser, and then generate an ascii version of the
   parse tree:
echo 'bob brought the pizza to alice.' | syntaxnet/demo.sh

input: bob brought the pizza to alice .
parse:
brought vbd root
 +-- bob nnp nsubj
 +-- pizza nn dobj
 |   +-- the dt det
 +-- to in prep
 |   +-- alice nnp pobj
 +-- . . punct

   the ascii tree shows the text organized as in the parse, not
   left-to-right as visualized in our tutorial graphs. in this example, we
   see that the verb "brought" is the root of the sentence, with the
   subject "bob", the object "pizza", and the prepositional phrase "to
   alice".

   if you want to feed in tokenized, conll-formatted text, you can run
   demo.sh --conll.

annotating a corpus

   to change the pipeline to read and write to specific files (as opposed
   to piping through stdin and stdout), we have to modify the demo.sh to
   point to the files we want. the syntaxnet models are configured via a
   combination of run-time flags (which are easy to change) and a text
   format taskspec protocol buffer. the spec file used in the demo is in
   syntaxnet/models/parsey_mcparseface/context.pbtxt.

   to use corpora instead of stdin/stdout, we have to:
    1. create or modify an input field inside the taskspec, with the
       file_pattern specifying the location we want. if the input corpus
       is in conll format, make sure to put record_format:
       'conll-sentence'.
    2. change the --input and/or --output flag to use the name of the
       resource as the output, instead of stdin and stdout.

   e.g., if we wanted to pos tag the conll corpus ./wsj.conll, we would
   create two entries, one for the input and one for the output:
input {
  name: 'wsj-data'
  record_format: 'conll-sentence'
  part {
    file_pattern: './wsj.conll'
  }
}
input {
  name: 'wsj-data-tagged'
  record_format: 'conll-sentence'
  part {
    file_pattern: './wsj-tagged.conll'
  }
}

   then we can use --input=wsj-data --output=wsj-data-tagged on the
   command line to specify reading and writing to these files.

configuring the python scripts

   as mentioned above, the python scripts are configured in two ways:
    1. run-time flags are used to point to the taskspec file, switch
       between inputs for reading and writing, and set various run-time
       model parameters. at training time, these flags are used to set the
       learning rate, hidden layer sizes, and other key parameters.
    2. the taskspec proto stores configuration about the transition
       system, the features, and a set of named static resources required
       by the parser. it is specified via the --task_context flag. a few
       key notes to remember:
          + the parameter settings in the taskspec have a prefix: either
            brain_pos (they apply to the tagger) or brain_parser (they
            apply to the parser). the --prefix run-time flag switches
            between reading from the two configurations.
          + the resources will be created and/or modified during multiple
            stages of training. as described above, the resources can also
            be used at evaluation time to read or write to specific files.
            these resources are also separate from the model parameters,
            which are saved separately via calls to tensorflow ops, and
            loaded via the --model_path flag.
          + because the taskspec contains file path, remember that copying
            around this file is not enough to relocate a trained model:
            you need to move and update all the paths as well.

   note that some run-time flags need to be consistent between training
   and testing (e.g. the number of hidden units).

next steps

   there are many ways to extend this framework, e.g. adding new features,
   changing the model structure, training on other languages, etc. we
   suggest reading the detailed tutorial below to get a handle on the rest
   of the framework.

contact

   to ask questions or report issues please post on stack overflow with
   the tag [101]syntaxnet or open an issue on the tensorflow/models
   [102]issues tracker. please assign syntaxnet issues to @calberti or
   @andorardo.

credits

   original authors of the code in this package include (in alphabetical
   order):
     * alessandro presta
     * aliaksei severyn
     * andy golding
     * bernd bohnet
     * chayut thanapirom
     * chris alberti
     * daniel andor
     * david weiss
     * emily pitler
     * greg coppola
     * ivan bogatyy
     * ji ma
     * keith hall
     * kuzman ganchev
     * lingpeng kong
     * livio baldini soares
     * mark omernick
     * michael collins
     * michael ringgaard
     * ryan mcdonald
     * slav petrov
     * stefan istrate
     * terry koo
     * tim credo
     * zora tung

     *    2019 github, inc.
     * [103]terms
     * [104]privacy
     * [105]security
     * [106]status
     * [107]help

     * [108]contact github
     * [109]pricing
     * [110]api
     * [111]training
     * [112]blog
     * [113]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [114]reload to refresh your
   session. you signed out in another tab or window. [115]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/tensorflow/models/commits/master.atom
   3. https://github.com/tensorflow/models/tree/master/research/syntaxnet#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/tensorflow/models/tree/master/research/syntaxnet
  32. https://github.com/join
  33. https://github.com/login?return_to=/tensorflow/models
  34. https://github.com/tensorflow/models/watchers
  35. https://github.com/login?return_to=/tensorflow/models
  36. https://github.com/tensorflow/models/stargazers
  37. https://github.com/login?return_to=/tensorflow/models
  38. https://github.com/tensorflow/models/network/members
  39. https://github.com/tensorflow
  40. https://github.com/tensorflow/models
  41. https://github.com/tensorflow/models
  42. https://github.com/tensorflow/models/issues
  43. https://github.com/tensorflow/models/pulls
  44. https://github.com/tensorflow/models/projects
  45. https://github.com/tensorflow/models/pulse
  46. https://github.com/tensorflow/models/find/master
  47. https://github.com/tensorflow/models/commits/master/research/syntaxnet
  48. https://github.com/tensorflow/models
  49. https://github.com/tensorflow/models/tree/master/research
  50. https://github.com/gatoatigrado
  51. https://github.com/tensorflow/models/commits?author=gatoatigrado
  52. https://github.com/tensorflow/models/commit/8d5d9d191aaccd6cf2a3254cf47a77de75f71e0b
  53. https://github.com/tensorflow/models/pull/6058
  54. https://github.com/tensorflow/models/commit/8d5d9d191aaccd6cf2a3254cf47a77de75f71e0b
  55. https://github.com/tensorflow/models/tree/3f94db4e9fe079e58623bef8d605eead315aba4d/research/syntaxnet
  56. https://github.com/tensorflow/models/tree/master/research
  57. https://github.com/tensorflow/models/tree/master/research/syntaxnet/docker-devel
  58. https://github.com/tensorflow/models/tree/master/research/syntaxnet/dragnn
  59. https://github.com/tensorflow/models/commit/8d5d9d191aaccd6cf2a3254cf47a77de75f71e0b
  60. https://github.com/tensorflow/models/tree/master/research/syntaxnet/examples/dragnn
  61. https://github.com/tensorflow/models/tree/master/research/syntaxnet/g3doc
  62. https://github.com/tensorflow/models/tree/master/research/syntaxnet/syntaxnet
  63. https://github.com/tensorflow/tensorflow/tree/8753e2ebde6c58b56675cc19ab7ff83072824a62
  64. https://github.com/tensorflow/models/tree/master/research/syntaxnet/third_party/utf
  65. https://github.com/tensorflow/models/tree/master/research/syntaxnet/tools
  66. https://github.com/tensorflow/models/tree/master/research/syntaxnet/util/utf8
  67. https://github.com/tensorflow/models/blob/master/research/syntaxnet/.dockerignore
  68. https://github.com/tensorflow/models/blob/master/research/syntaxnet/.gitignore
  69. https://github.com/tensorflow/models/blob/master/research/syntaxnet/dockerfile
  70. https://github.com/tensorflow/models/blob/master/research/syntaxnet/readme.md
  71. https://github.com/tensorflow/models/blob/master/research/syntaxnet/workspace
  72. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/conll2017/readme.md
  73. http://www.tensorflow.org/
  74. https://github.com/tensorflow/models/tree/master/research/syntaxnet/dragnn
  75. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/dragnn.md
  76. https://arxiv.org/pdf/1703.04474.pdf
  77. http://arxiv.org/abs/1603.06042
  78. https://github.com/tensorflow/models/tree/master/research/syntaxnet/syntaxnet
  79. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/syntaxnet-tutorial.md
  80. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/conll2017/readme.md
  81. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/cloud.md
  82. https://cloud.google.com/
  83. http://bazel.build/docs/install.html
  84. https://github.com/bazelbuild/bazel/releases
  85. https://github.com/tensorflow/models/issues/248
  86. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc
  87. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/unrolled-dragnn.png
  88. https://github.com/tensorflow/models/blob/master/research/syntaxnet/examples/dragnn/interactive_text_analyzer.ipynb
  89. https://github.com/tensorflow/models/blob/master/research/syntaxnet/examples/dragnn/trainer_tutorial.ipynb
  90. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/cloud.md
  91. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/conll2017
  92. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/universal.md
  93. https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/syntaxnet-tutorial.md
  94. http://www.cs.cmu.edu/~ark/turboparser/
  95. http://research.google.com/pubs/archive/38148.pdf
  96. http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43800.pdf
  97. http://arxiv.org/abs/1603.06042
  98. http://arxiv.org/abs/1603.06042
  99. http://www.cs.cmu.edu/~lingwang/papers/emnlp2015.pdf
 100. http://arxiv.org/abs/1603.06042
 101. http://stackoverflow.com/questions/tagged/syntaxnet
 102. https://github.com/tensorflow/models/issues
 103. https://github.com/site/terms
 104. https://github.com/site/privacy
 105. https://github.com/security
 106. https://githubstatus.com/
 107. https://help.github.com/
 108. https://github.com/contact
 109. https://github.com/pricing
 110. https://developer.github.com/
 111. https://training.github.com/
 112. https://github.blog/
 113. https://github.com/about
 114. https://github.com/tensorflow/models/tree/master/research/syntaxnet
 115. https://github.com/tensorflow/models/tree/master/research/syntaxnet

   hidden links:
 117. https://github.com/
 118. https://github.com/tensorflow/models/tree/master/research/syntaxnet
 119. https://github.com/tensorflow/models/tree/master/research/syntaxnet
 120. https://github.com/tensorflow/models/tree/master/research/syntaxnet
 121. https://github.com/tensorflow/models/tree/master/research/syntaxnet#syntaxnet-neural-models-of-syntax
 122. https://github.com/tensorflow/models/tree/master/research/syntaxnet#how-to-use-this-library
 123. https://github.com/tensorflow/models/tree/master/research/syntaxnet#installation
 124. https://github.com/tensorflow/models/tree/master/research/syntaxnet#docker-installation
 125. https://github.com/tensorflow/models/tree/master/research/syntaxnet#ubuntu-1604-binary-installation
 126. https://github.com/tensorflow/models/tree/master/research/syntaxnet#manual-installation
 127. https://github.com/tensorflow/models/tree/master/research/syntaxnet#getting-started
 128. https://github.com/tensorflow/models/tree/master/research/syntaxnet#learning-the-dragnn-framework
 129. https://github.com/tensorflow/models/tree/master/research/syntaxnet#using-the-pre-trained-nlp-models
 130. https://github.com/tensorflow/models/tree/master/research/syntaxnet#parsing-from-standard-input
 131. https://github.com/tensorflow/models/tree/master/research/syntaxnet#annotating-a-corpus
 132. https://github.com/tensorflow/models/tree/master/research/syntaxnet#configuring-the-python-scripts
 133. https://github.com/tensorflow/models/tree/master/research/syntaxnet#next-steps
 134. https://github.com/tensorflow/models/tree/master/research/syntaxnet#contact
 135. https://github.com/tensorflow/models/tree/master/research/syntaxnet#credits
 136. https://github.com/
