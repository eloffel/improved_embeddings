   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 3 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]sippycup
    2. [10]sippycup-unit-2.ipynb

   [sippycup-small.jpg]

sippycup
unit 2: travel queries

   [11]bill maccartney
   spring 2015
   this is unit 2 of the [12]sippycup codelab.

   in our next case study, we're going to look at travel queries, in which
   a user seeks information about getting from here to there. examples
   include:
"birmingham al distance from indianapolish in"
"directions from washington to canada"
"discount travel flights to austin texas"


   it's easy to imagine applications for id29 in the travel
   domain. providing answers to queries like these is a core part of the
   value proposition of intelligent assistants such as [13]apple's siri,
   [14]google now, [15]microsoft's cortana, and [16]amazon echo.

   the travel domain has some important differences to the domain of
   natural language arithmetic we looked at in our [17]first case study.
     * above all, it's far more diverse. the lexicon of travel queries can
       include thousands of words, not merely dozens     indeed, its
       potential size is unbounded. and the words and phrases can be
       assembled in a dizzying variety of syntactic structures.
     * it's much more ambiguous. consider "directions from washington to
       canada". does that refer to the state of washington, or to the
       district of columbia? and where exactly in canada are we headed?
     * it can also be much messier. because it's a real-world application,
       and because users [del: are idiots :del] make mistakes, travel
       queries can include lots of misspellings (such as "indianapolish"),
       mangled syntax, and other linguistic infelicities. it's not all
       neat and tidy like the arithmetic case study.
     * however, the semantics are flatter. the semantic representation for
       a travel query may need to represent the destination, the origin,
       the transportation mode, and perhaps a few other bits of
       information. but, unlike the expression trees we looked at in the
       arithmetic case study, it does not need to contain arbitrarily
       deeply-nested recursive structure.

example inputs[18]  

   whenever we start work in a new domain, we must begin with data
   collection. to drive an effective development process, we need a large,
   realistic, representative sample of the inputs our system is likely to
   encounter in its intended application. in the arithmetic case study, we
   copped out on the data collection, in the name of pedagogical
   convenience. this time, we'll try harder.

   for this purpose, we'll leverage the aol search query dataset ([19]pass
   et al. 2006), a resource which has proven immensely useful over time,
   despite its [20]controversial origin. this dataset contains
   approximately 10m unique search queries issued by about 650k aol users
   in 2006. because the dataset was quickly retracted by aol, it can be
   difficult to find online, but it is currently available for download
   [21]here.

   of course, the majority of those 10m queries are not related to travel.
   (the five most frequent queries are "google", "ebay", "yahoo",
   "yahoo.com", and "mapquest". remember, it was 2006. the query
   "facebook" doesn't show up until position 119!) to construct a query
   sample suitable for our purposes, we'll need to do some filtering.

   since the queries we're interested in always refer to a location, we'll
   start by filtering for location names. recognizing location names is
   itself a hard problem, so for current purposes we'll adopt a
   good-enough solution: we'll grep the queries for the names of about 600
   u.s. locations (cities, states, mountains, lakes, and so on) listed in
   [22]geobase. (we'll be using geobase again in the next unit.) this
   winnows the 10m unique queries to about 1m. it's a good start, but it's
   still not enough. (the top five queries are now "white pages", "florida
   lottery", "wells fargo", "whitney houston", and "las vegas".) we need
   to find queries which not only contain a location, but also a clear
   indication of travel intent.

   since travel is about going from one location to another, let's grep
   for queries which contain both "from" and "to". this cuts the number of
   queries to about 3,100, and we're now seeing the kind of queries we're
   looking for:
"airline flights from raleigh durham to nashville tennessee"
"distance from seattle to vancouver"
"cheap flights to italy from new york"
"international flights from usa to morocco"
"milage from portlan oregon to canada"
"distance from alabama to arkansas in hours"
"driving directions to sault ste marie canada from vermont"
"great white sharks life cycle from brith to death"
"directions from newark airport to bayonne bridge"
"poem it takes courage to love from tyler perrys movie"


   a couple of observations here. first, not all of the queries are
   actually travel queries. that's ok     we actually want to include some
   queries which do not belong to the target domain. such "negative"
   queries are important for evaluating our system's ability to
   distinguish between queries it should be parsing and queries it
   shouldn't. second, there are a lot of mispelings. again, that's ok. we
   want a realistic sample which reflects the inputs our system will
   actually encounter in its intended application. messy inputs are a fact
   of life, and they're one of the things that distinguishes this domain
   from the highly artificial example of natural language arithmetic we
   looked at in the last unit.

   there's still a problem, though. not all travel queries contain both
   "from" and "to". many valid travel queries contain only a destination,
   but no origin     and some contain an origin, but no destination. if we
   relax the conjunction to a disjunction (that is, "from" or "to", we get
   about 23,000 queries, most of which are still not what we're looking
   for (for example, "colorado rent to own", "moving to florida").
   however, the list of examples above (and its continuation, not shown)
   suggest many other terms we can usefully search for: "airline",
   "flights", "distance", "mileage", "driving", "directions", and so on.
   we constructed a list of about 60 such terms (recorded in a comment in
   [23]travel_examples.py). we then selected queries which contain a
   location, either "from" or "to", and either one of the ~60 travel terms
   or both "from" and "to". this left 6,588 queries, and the results look
   pretty good:
"flight one way las vegas nv to rapid city sd"
"number to alaska airlines"
"travel time to denver colorado"
"amtrak schedule in california from burbanlk airport"
"airfare las vegas to my"
"cycling route san francisco to sacramento"
"where to find cheapest gasoline in new castle delaware"
"discount tickets to six flags and discounted hotels in new jersey"
"public transportation washington dc to new york city"
"transportation from newark airport to manhattan"


   three of those are not really travel queries, but that's perfectly
   fine.

   let's recap. to construct our query sample, we:
     * began from the set of unique queries in the aol dataset (10m
       queries).
     * selected queries containing one of the 600 locations named in
       geobase (1m queries).
     * selected queries containing "from" or "to" (23k queries).
     * selected queries containing one of about 60 travel terms, or
       containing both "from" and "to" (6,588 queries).

   we've got our inputs; now we need to think about the outputs.

semantic representation [24]  

   for the travel domain, we're going to use a style of semantic
   representation based on nested maps of key-value pairs (represented by
   python dictionaries). the keys belong to a small, fixed set of strings.
   the values can be numbers, strings, or nested maps, depending upon the
   key.

   here are some examples:
{'domain': 'travel', 'type': 'directions', 'mode': 'car',
   'destination': {'id': 4793846, 'name': 'williamsburg, va, us'}}
{'domain': 'travel', 'type': 'duration', 'mode': 'bus',
   'origin': {'id': 4500546, 'name': 'atlantic city, nj, us'},
   'destination': {'id': 5128581, 'name': 'new york city, ny, us'}}
{'domain': 'travel', 'type': 'cost', 'mode': 'air',
   'origin': {'id': 5101798, 'name': 'newark, nj, us'},
   'destination': {'id': 4574324, 'name': 'charleston, sc, us'}}


   you can probably discern what each example means without any
   difficulty. the first is asking for driving directions to williamsburg.
   the second is asking for the duration of a bus trip from atlantic city
   to new york city. and the third is asking for the cost of an airfare
   from newark to charleston.

   to be a bit more formal, the following keys are allowed in the
   top-level map:
     * domain has one of two values: travel for travel queries, or other
       for non-travel queries.
     * type specifies the type of information requested, one of:
       directions, distance, duration, cost, or schedule.
     * mode specifies the mode of travel, one of: air, bike, boat, bus,
       car, taxi, train, or transit.
     * destination specifies the destination of travel using a nested map
       representing a location.
     * origin specifies the origin of travel using a nested map
       representing a location.

   in nested maps representating locations, just two keys are allowed:
     * id is an integer which is the unique identifier of the location in
       the [25]geonames geographical database.
     * names is a human-readable name for the location whose format is
       defined by code in [26]geonames.py.

   note that our semantic representation both resolves ambiguity and
   canonicalizes. one the one hand, the phrase "new york" has a different
   semantic representation if it refers to the city than if it refers to
   the state; on the other hand, "new york city" and "nyc" have the same
   semantic representation. these qualities will make it easy for a
   downstream executor to turn a semantic representation directly into
   action.

   in a real-world application, the executor would likely be a module that
   translates our semantic representations into requests to backend apis
   to compute optimal routes, retrieve bus schedules or airfares, or
   whatever. however, because such functionality is not easily captured in
   a few lines of python code, sippycup doesn't include an executor for
   the travel domain.

   since we don't have an executor, we won't be dealing with denotations
   in the travel domain. (abstractly, the denotation of a directions
   request is some kind of complex computational object representing a
   route.) instead, training and evaluation in this domain will consider
   only whether we're generating the correct semantics. we'll return to
   considering denotations in our [27]third case study.

example data[28]  

   we've described a process for extracting 6,588 (likely) travel queries
   from the aol search query dataset, and we've defined a simple semantic
   representation for such queries. the next step should be to manually
   annotate each query with its semantics, in order generate example data
   for training and evaluation.

   however, there's a problem. manual annotation is tedious and
   time-consuming, and therefore expensive. for datasets of significant
   size, it is usually not practical for one person to do all the work.
   this suggests distributing the work over a cluster of graduate students
       which is in fact the method that was used to generate the [29]geo880
   dataset that we'll use in our [30]third case study. accordingly, one of
   the [31]exercises at the end of this section will ask you to annotate a
   score of travel queries.

   alternatively, if the annotation task does not require specialized
   knowledge, the work can often be distributed over workers on a
   id104 platform such as [32]amazon mechanical turk. our semantic
   representation for the travel domain is simple enough that the
   id104 approach might be feasible. of course, the workers
   wouldn't write down the semantics directly; rather, they'd fill out a
   carefully-designed form containing questions about the query, and a
   script would generate semantic representations based on their
   responses. we leave working out the details of the id104
   approach as an exercise for the motivated reader. :-)

   the difficulty of producing annotated data, and the consequent paucity
   of such data, has been the eternal bane of research in semantic
   parsing. indeed, overcoming this obstacle was the chief motivation for
   the shift made by [33]liang et al. 2011 to learn not from semantics but
   from denotations, which are easier and cheaper to generate via
   id104. despite this shift, annotated data remains scarce. to
   this day, the largest standard dataset for id29 (the
   [34]webquestions dataset) contains only 5,810 examples.

   for the purposes of this codelab, we have manually annotated 100
   examples, given in [35]travel_examples.py. the examples were sampled
   uniformly at random from the set of 6,588 aol queries described
   earlier, and are divided into a training set of 75 examples and a test
   set of 25 examples. you might find yourself quibbling with some of the
   annotation choices. in some cases, our semantic representation is too
   impoverished to capture the user's intent completely. for example,
   "scenic drive to duluth mn", "honeymoon trip to hawaii", and "jet blue
   flights to new york" contain modifiers which cannot be expressed in our
   semantic representation. in other cases, the user's intent may be
   uncertain or obscure: what exactly does "airbus from boston to europe"
   mean?

enriching the grammar class[36]  

   now that we have some example data for the travel domain, we'd like to
   start building a grammar. however, the grammar formalism we introduced
   in the arithmetic case study was quite restrictive. every rule had to
   be in chomsky normal form: either a unary lexical rule, or a binary
   compositional rule. if you did the [37]exercises at the end of the
   arithmetic case study, you will have chafed against those restrictions.
   so, to make grammar engineering far more convenient, we're going to
   loosen things up. we'll modify the grammar class by adding:
     * annotators, which are external modules for assigning categories and
       semantics to phrases of specific types.
     * n-ary lexical rules, such as rule('$city', 'new york city').
     * unary compositional rules, such as rule('$location', '$city').
     * n-ary compositional rules, such as rule('$routequery',
       '$fromlocation $tolocation $travelmode').
     * rules with optional elements on the rhs, such as rule('$bikemode',
       '?by bike'), which can match either "bike" or "by bike".
     * a designated start symbol, typically $root, to allow us to limit
       which categories can yield complete parses.

   in order to make all this stuff work, we'll need to redefine our
   grammar class and some of its affiliated functions. first, we'll
   redefine the grammar constructor to add member variables for storing
   unary rules, annotators, and the start symbol.
   in [1]:
class grammar:
    def __init__(self, rules=[], annotators=[], start_symbol='$root'):
        self.categories = set()
        self.lexical_rules = defaultdict(list)
        self.unary_rules = defaultdict(list)
        self.binary_rules = defaultdict(list)
        self.annotators = annotators
        self.start_symbol = start_symbol
        for rule in rules:
            add_rule(self, rule)
        print('created grammar with %d rules.' % len(rules))

    def parse_input(self, input):
        """returns a list of parses for the given input."""
        return parse_input(self, input)

   next, we'll redefine add_rule(), so that it knows what to do when it
   encounters unary or n-ary compositional rules, or rule containing
   optionals:
   in [2]:
def add_rule(grammar, rule):
    if contains_optionals(rule):
        add_rule_containing_optional(grammar, rule)
    elif is_lexical(rule):
        grammar.lexical_rules[rule.rhs].append(rule)
    elif is_unary(rule):
        grammar.unary_rules[rule.rhs].append(rule)
    elif is_binary(rule):
        grammar.binary_rules[rule.rhs].append(rule)
    elif all([is_cat(rhsi) for rhsi in rule.rhs]):
        add_n_ary_rule(grammar, rule)
    else:
        # one of the exercises will ask you to handle this case.
        raise exception('rhs mixes terminals and non-terminals: %s' % rule)

def is_unary(rule):
    """
    returns true iff the given rule is a unary compositional rule, i.e.,
    contains only a single category (non-terminal) on the rhs.
    """
    return len(rule.rhs) == 1 and is_cat(rule.rhs[0])

   finally, we'll redefine parse_input(), so that it properly invokes
   functions for applying annotators and unary rules (in addition to
   existing invocations of functions for applying lexical rules and binary
   rules).
   in [3]:
def parse_input(grammar, input):
    """returns a list of all parses for input using grammar."""
    tokens = input.split()
    chart = defaultdict(list)
    for j in range(1, len(tokens) + 1):
        for i in range(j - 1, -1, -1):
            apply_annotators(grammar, chart, tokens, i, j)               # new!
            apply_lexical_rules(grammar, chart, tokens, i, j)
            apply_binary_rules(grammar, chart, i, j)
            apply_unary_rules(grammar, chart, i, j)                      # new!
    parses = chart[(0, len(tokens))]
    if hasattr(grammar, 'start_symbol') and grammar.start_symbol:        # new!
        parses = [parse for parse in parses if parse.rule.lhs == grammar.start_s
ymbol]
    return parses

   note that if grammar.start_symbol is defined (the usual case), then we
   will return only parses having that category at the root.

annotators[38]  

   our travel grammar will need to recognize names of cities (and other
   kinds of locations). we could achieve this by writing lots of lexical
   rules:
   in [4]:
from parsing import rule

rules_cities = [
    rule('$city', 'austin', {'id': 4671654, 'name': 'austin, tx, us'}),
    rule('$city', 'boston', {'id': 4930956, 'name': 'boston, ma, us'}),
    rule('$city', 'chicago', {'id': 4887398, 'name': 'chicago, il, us'}),
    rule('$city', 'dallas', {'id': 4684888, 'name': 'dallas, tx, us'}),
    # ...
]

   however, there are thousands of cities in the world. adding thousands
   of rules to our grammar would be cumbersome and difficult to maintain.
   besides, there is probably already some library or service which knows
   how to recognize names of cities and map them into unique identifiers.
   if we could wrap this external module in the right kind of interface,
   we could make it directly available to our grammar.

   we call such a module an annotator. what kind of an interface should it
   have? well, it should take a token span (that is, a list of tokens) and
   return a list of zero or more interpretations, each a pair consisting
   of a category and a semantic representation.
   in [5]:
class annotator:
    """a base class for annotators."""
    def annotate(self, tokens):
        """returns a list of pairs, each a category and a semantic representatio
n."""
        return []

   let's take a moment to implement a couple of simple annotators which
   will prove useful later on. the first is a numberannotator, which will
   annotate any string representing a number with category $number and
   semantics equal to its numeric value.
   in [6]:
class numberannotator(annotator):
    def annotate(self, tokens):
        if len(tokens) == 1:
            try:
                value = float(tokens[0])
                if int(value) == value:
                    value = int(value)
                return [('$number', value)]
            except valueerror:
                pass
        return []

   here's the numberannotator in action:
   in [7]:
numberannotator().annotate(['16'])

   out[7]:
[('$number', 16)]

   another annotator that will prove very useful is the tokenannotator,
   which will annotate any single token with category $token and semantics
   equal to the token itself.
   in [8]:
class tokenannotator(annotator):
    def annotate(self, tokens):
        if len(tokens) == 1:
            return [('$token', tokens[0])]
        else:
            return []

   here's the tokenannotator in action:
   in [9]:
tokenannotator().annotate(['foo'])

   out[9]:
[('$token', 'foo')]

   later in this unit, we'll introduce the geonamesannotator, a more
   complex annotator designed to annotate names of locations.

   we saw above that the redesigned grammar class holds a list of
   annotators, and that the redesigned parse_input() function invokes a
   function called apply_annotators() on every token span. its operation
   is straightforward: for every annotator in the grammar, and for every
   interpretation of the given span by the annotator, it creates a new
   parse and adds it to the corresponding chart cell.
   in [10]:
def apply_annotators(grammar, chart, tokens, i, j):
    """add parses to chart cell (i, j) by applying annotators."""
    if hasattr(grammar, 'annotators'):
        for annotator in grammar.annotators:
            for category, semantics in annotator.annotate(tokens[i:j]):
                if not check_capacity(chart, i, j):
                    return
                rule = rule(category, tuple(tokens[i:j]), semantics)
                chart[(i, j)].append(parse(rule, tokens[i:j]))

   (the check_capacity() function will be defined in the next section.)

unary rules[39]  

   when designing a grammar, it's often convenient to write unary rules,
   which have a single category on the rhs. however, unary rules are not
   allowed in chomsky normal form. one reason why unary rules are
   problematic is that badly-designed grammars can contain unary cycles,
   which can cause parsing to get stuck in an infinite regress.
   in [11]:
rules_with_unary_cycle = [
    rule('$root', '$ping', lambda sems: sems[0]),
    rule('$ping', '$pong', lambda sems: sems[0] + '1'),
    rule('$pong', '$ping', lambda sems: sems[0]),
    rule('$ping', 'ping', '1'),
]

   if a grammar with these rules is used to parse the input "ping", it
   will generate an endless stream of candidate parses, with semantics
   '1', '11', '111', '1111', and so on.

   there are various strategies for preventing unary cycles from causing a
   runaway parsing process. we'll adopt one of the simplest: rather than
   trying to identify unary cycles in advance, we'll just impose an upper
   bound on the capacity of any chart cell, and if the limit is reached,
   we'll terminate the parsing process.

   with this in mind, the definition of apply_unary_rules() is
   straightforward.
   in [12]:
def apply_unary_rules(grammar, chart, i, j):
    """add parses to chart cell (i, j) by applying unary rules."""
    # note that the last line of this method can add new parses to chart[(i,
    # j)], the list over which we are iterating.  because of this, we
    # essentially get unary closure "for free".  (however, if the grammar
    # contains unary cycles, we'll get stuck in a loop, which is one reason for
    # check_capacity().)
    if hasattr(grammar, 'unary_rules'):
        for parse in chart[(i, j)]:
            for rule in grammar.unary_rules[(parse.rule.lhs,)]:
                if not check_capacity(chart, i, j):
                    return
                chart[(i, j)].append(parse(rule, [parse]))

max_cell_capacity = 10000

# important for catching e.g. unary cycles.
def check_capacity(chart, i, j):
    if len(chart[(i, j)]) >= max_cell_capacity:
        print('cell (%d, %d) has reached capacity %d' % (
            i, j, max_cell_capacity))
        return false
    return true

   note that check_capacity() is not just for taming unary cycles     in
   fact, it's a general-purpose mechanism for bounding the amount of
   computation invested in a single parsing task. accordingly, we've
   included calls to it, not only in apply_unary_rules(), but also in its
   three sibling functions. as you build richer, more complex grammars,
   you may find that some inputs have hundreds or even thousands of
   possible parses, and you may begin to run up against the chart cell
   capacity limit. feel free to adjust it as needed.

n-ary rules[40]  

   like unary rules, n-ary compositional rules are make grammar
   engineering easier, by allowing grammar rules to express constraints in
   natural, intuitive ways.

   in the previous unit, we saw how to eliminate the n-ary rule rule('$e',
   '$e $binop $e') from a grammar by binarizing it     that is, by replacing
   it with two binary rules having equivalent effect. obviously, the
   binarizing operation can be automated. the redesigned add_rule() calls
   a function named add_n_ary_rule() which performs this binarization.
   in [13]:
def add_n_ary_rule(grammar, rule):
    """
    handles adding a rule with three or more non-terminals on the rhs.
    we introduce a new category which covers all elements on the rhs except
    the first, and then generate two variants of the rule: one which
    consumes those elements to produce the new category, and another which
    combines the new category which the first element to produce the
    original lhs category.  we add these variants in place of the
    original rule.  (if the new rules still contain more than two elements
    on the rhs, we'll wind up recursing.)

    for example, if the original rule is:

        rule('$z', '$a $b $c $d')

    then we create a new category '$z_$a' (roughly, "$z missing $a to the left")
,
    and add these rules instead:

        rule('$z_$a', '$b $c $d')
        rule('$z', '$a $z_$a')
    """
    def add_category(base_name):
        assert is_cat(base_name)
        name = base_name
        while name in grammar.categories:
            name = name + '_'
        grammar.categories.add(name)
        return name
    category = add_category('%s_%s' % (rule.lhs, rule.rhs[0]))
    add_rule(grammar, rule(category, rule.rhs[1:], lambda sems: sems))
    add_rule(grammar, rule(rule.lhs, (rule.rhs[0], category),
                           lambda sems: apply_semantics(rule, [sems[0]] + sems[1
])))

optional elements[41]  

   another convenience for manual grammar engineering is the ability to
   mark an element in the rhs as optional     in sippycup, by putting '?' in
   front of it. with such a mechanism, we can replace these two rules:
rule('$bikemode, 'bike')
rule('$bikemode', 'by bike')

   with a single rule containing an optional element:
rule('$bikemode', '?by bike')


   the optional mechanism also makes it easy to define one category as a
   sequence of another category:
rule('$thing', 'thing')
rule('$things', '$thing ?$things')

   the following functions enable the optional mechanism.
   in [14]:
from types import functiontype

def is_optional(label):
    """
    returns true iff the given rhs item is optional, i.e., is marked with an
    initial '?'.
    """
    return label.startswith('?') and len(label) > 1

def contains_optionals(rule):
    """returns true iff the given rule contains any optional items on the rhs.""
"
    return any([is_optional(rhsi) for rhsi in rule.rhs])

def add_rule_containing_optional(grammar, rule):
    """
    handles adding a rule which contains an optional element on the rhs.
    we find the leftmost optional element on the rhs, and then generate
    two variants of the rule: one in which that element is required, and
    one in which it is removed.  we add these variants in place of the
    original rule.  (if there are more optional elements further to the
    right, we'll wind up recursing.)

    for example, if the original rule is:

        rule('$z', '$a ?$b ?$c $d')

    then we add these rules instead:

        rule('$z', '$a $b ?$c $d')
        rule('$z', '$a ?$c $d')
    """
    # find index of the first optional element on the rhs.
    first = next((idx for idx, elt in enumerate(rule.rhs) if is_optional(elt)),
-1)
    assert first >= 0
    assert len(rule.rhs) > 1, 'entire rhs is optional: %s' % rule
    prefix = rule.rhs[:first]
    suffix = rule.rhs[(first + 1):]
    # first variant: the first optional element gets deoptionalized.
    deoptionalized = (rule.rhs[first][1:],)
    add_rule(grammar, rule(rule.lhs, prefix + deoptionalized + suffix, rule.sem)
)
    # second variant: the first optional element gets removed.
    # if the semantics is a value, just keep it as is.
    sem = rule.sem
    # but if it's a function, we need to supply a dummy argument for the removed
 element.
    if isinstance(rule.sem, functiontype):
        sem = lambda sems: rule.sem(sems[:first] + [none] + sems[first:])
    add_rule(grammar, rule(rule.lhs, prefix + suffix, sem))

grammar engineering[42]  

   it's time to start writing some grammar rules for the travel domain.
   we're going to adopt a data-driven approach. using the 75 training
   examples as a development set, we will iteratively:
     * look at examples which are not yet parsed correctly,
     * identify common obstacles or sources of error,
     * introduce new rules to address those problems, and then
     * re-evaluate on the 75 training examples.

   during grammar engineering, the performance metric we'll focus on is
   oracle accuracy (the proportion of examples for which any parse is
   correct), not accuracy (the proportion of examples for which the first
   parse is correct). remember that oracle accuracy is an upper bound on
   accuracy. oracle accuracy is a measure of the expressive power of the
   grammar: does it have the rules it needs to generate the correct parse?
   the gap between oracle accuracy and accuracy, on the other hand,
   reflects the ability of the scoring model to bring the correct parse to
   the top of the candidate list.

phrase-bag grammars[43]  

   for the travel domain, we're going to develop a style of grammar known
   as a phrase-bag grammar. to get a sense of how it will work, let's look
   at ten example inputs from the training data.

travel boston to fr. myers fla
how do i get from tulsa oklahoma to atlantic city. new jersey by air
airbus from boston to europe
cheap tickets to south carolina
birmingham al distance from indianapolish in
transportation to the philadelphia airport
one day cruise from fort lauderdale florida
directions from washington to canada
flights from portland or to seattle wa
honeymoon trip to hawaii

   here we've highlighted phrases in different colors, according to the
   roles they play in building the meaning of the input.
     * green phrases indicate the destination of travel.
     * blue phrases indicate the origin of travel.
     * yellow phrases indicate a mode of travel: air, boat, etc.
     * orange phrases indicate travel of some kind, but do not specify a
       travel mode.
     * red phrases indicate a specific type of information sought:
       distance, directions, etc.
     * gray phrases indicate "optional" words which contribute little to
       the semantics. (the modifiers "one day" and "honeymoon" may be
       quite meaningful to the user, but our semantic representation is
       too impoverished to capture them, so they are not relevant for our
       grammar.)

   note that the ordering of phrases in a query isn't particularly
   important. whether the user says directions from washington to canada,
   or from washington to canada directions, or even to canada directions
   from washington, the intent is clearly the same. some of these
   formulations might be more natural than others     and more common in the
   query logs     but all of them should be interpretable by our grammar.

   that's the motivation for the phrase-bag style of grammar. in a simple
   phrase-bag grammar, a valid query is made up of one or more query
   elements, which can appear in any order, and optionals, which can be
   scattered freely amongst the query elements. (more complex phrase-bag
   grammars can impose further constraints.) for the travel domain, we can
   identify two types of query elements: travel locations and travel
   arguments. a travel location is either a to location (destination) or a
   from location (origin). a travel argument is either a travel mode, a
   travel trigger, or a request type.

   we're ready to write our first grammar rules for the travel domain.
   in [15]:
def sems_0(sems):
    return sems[0]

def sems_1(sems):
    return sems[1]

def merge_dicts(d1, d2):
    if not d2:
        return d1
    result = d1.copy()
    result.update(d2)
    return result

rules_travel = [
    rule('$root', '$travelquery', sems_0),
    rule('$travelquery', '$travelqueryelements',
         lambda sems: merge_dicts({'domain': 'travel'}, sems[0])),
    rule('$travelqueryelements', '$travelqueryelement ?$travelqueryelements',
         lambda sems: merge_dicts(sems[0], sems[1])),
    rule('$travelqueryelement', '$travellocation', sems_0),
    rule('$travelqueryelement', '$travelargument', sems_0),
]

   these rules are incomplete: $travellocation and $travelargument are not
   yet defined, and there is no mention of optionals. but these rules
   define the high-level structure of our phrase-bag grammar.

   pay attention to the semantic functions attached to each rule. remember
   that our semantic representations are maps of key-value pairs. in the
   rules which define $travelqueryelement, the semantic function
   propagates the semantics of the child unchanged. in the rule which
   defines $travelqueryelements, the semantic function merges the
   semantics of the children. and in the rule which defines $travelquery,
   the semantic function adds a key-value pair to the semantics of the
   child.

travel locations[44]  

   the rules introduced above left $travellocation undefined. let's add
   some rules to define it.
   in [16]:
rules_travel_locations = [
    rule('$travellocation', '$tolocation', sems_0),
    rule('$travellocation', '$fromlocation', sems_0),
    rule('$tolocation', '$to $location', lambda sems: {'destination': sems[1]}),
    rule('$fromlocation', '$from $location', lambda sems: {'origin': sems[1]}),
    rule('$to', 'to'),
    rule('$from', 'from'),
]

   this looks good, but we still need something that defines $location,
   which will match a phrase like "boston" and assign to it a semantic
   representation like {id: 4930956, name: 'boston, ma, us'}. if only we
   had such a thing.

the geonamesannotator[45]  

   [46]geocoding is the process of mapping a piece of text describing a
   location (such as a place name or an address) into a canonical,
   machine-readable representation (such as [47]geographic coordinates or
   a unique identifier in a geographic database). because geocoding is a
   common need across many applications, many geocoding services are
   available.

   one such service is [48]geonames. geonames defines a large geographic
   database in which each location is identified by a unique integer. for
   example, boston is identified by [49]4930956. geonames also provides a
   free, [50]restful api for geocoding requests. for example, a request to
   geocode "palo alto" looks like this:

   [51]http://api.geonames.org/searchjson?q=palo+alto&username=demo

   in sippycup, the geonamesannotator (defined in [52]geonames.py) is
   implemented as a wrapper around the geonames api. the job of the
   geonamesannotator is to recognize $locations and generate semantics for
   them. take a few minutes to skim that code now.

   note that the geonamesannotator uses a persistent cache which is
   pre-populated for phrases in the 100 annotated travel examples,
   avoiding the need for live calls to the geonames api. however, if you
   run on any other examples, you will be making live calls. by default,
   your requests will specify your username as wcmac. that's fine, but
   each (free) geonames account is limited to 2000 calls per hour. if too
   many people are making calls as wcmac, that quota could be exhausted
   quickly. if that happens, you'll want to [53]create your own account on
   geonames.

   you will soon observe that the annotations are far from perfect. for
   example, "florida" is annotated as {'id': 3442584, 'name': 'florida,
   uy'}, which denotes a city in uruguay. the current implementation of
   geonamesannotator could be improved in many ways. for example, while
   the geonames api can return multiple results for ambiguous queries such
   as "florida", the geonamesannotator considers only the first result    
   and there is no guarantee that the first result is the best. it may
   also be possible to improve the quality of the results by playing with
   some of the api request parameters documented [54]here. some of the
   [55]exercises at the end of this unit ask you to explore these
   possibilities.

   we're now ready to see our grammar parse an input:
   in [17]:
from collections import defaultdict

from geonames import geonamesannotator
from parsing import *

geonames_annotator = geonamesannotator()
rules = rules_travel + rules_travel_locations
travel_annotators = [geonames_annotator]
grammar = grammar(rules=rules, annotators=travel_annotators)
parses = grammar.parse_input('from boston to austin')
print(parses[0].semantics)

loaded geonamesannotator cache with 1462 items
created grammar with 11 rules
{'domain': 'travel', 'origin': {'name': 'boston, ma, us', 'id': 4930956}, 'desti
nation': {'name': 'austin, tx, us', 'id': 4671654}}

travel modes[56]  

   let's turn our attention from travel locations to travel arguments. one
   kind of travel argument is a travel mode. our semantic representation
   defines eight travel modes (air, bike, boat, bus, car, taxi, train, and
   transit), and our training examples illustrate some common ways of
   expressing each travel mode. in fact, individual examples will suggest
   specific lexical rules. consider this example:
example(input='flights from portland or to seattle wa',
        semantics={'domain': 'travel', 'mode': 'air',
                   'origin': {'id': 5746545, 'name': 'portland, or, us'},
                   'destination': {'id': 5809844, 'name': 'seattle, wa, us'}}),


   the pairing of the token "flights" with the semantic fragment {'mode':
   'air'} suggests the rule:
rule('$airmode', 'flights', {'mode': 'air'})


   actually, for simplicity, we're going to add the 'air' semantics one
   level "higher" in the grammar, like this:
rule('$travelmode', '$airmode', {'mode': 'air'})
rule('$airmode', 'flights')


   the rules below illustrate this approach. these rules will allow our
   grammar to handle the most common ways of referring to each travel
   mode. all of the lexical rules below use phrases which are either
   highly obvious (such as 'taxi' for $taximode) or else are motivated by
   specific examples in our training dataset (not the test set!).
   in [18]:
rules_travel_modes = [
    rule('$travelargument', '$travelmode', sems_0),

    rule('$travelmode', '$airmode', {'mode': 'air'}),
    rule('$travelmode', '$bikemode', {'mode': 'bike'}),
    rule('$travelmode', '$boatmode', {'mode': 'boat'}),
    rule('$travelmode', '$busmode', {'mode': 'bus'}),
    rule('$travelmode', '$carmode', {'mode': 'car'}),
    rule('$travelmode', '$taximode', {'mode': 'taxi'}),
    rule('$travelmode', '$trainmode', {'mode': 'train'}),
    rule('$travelmode', '$transitmode', {'mode': 'transit'}),

    rule('$airmode', 'air fare'),
    rule('$airmode', 'air fares'),
    rule('$airmode', 'airbus'),
    rule('$airmode', 'airfare'),
    rule('$airmode', 'airfares'),
    rule('$airmode', 'airline'),
    rule('$airmode', 'airlines'),
    rule('$airmode', '?by air'),
    rule('$airmode', 'flight'),
    rule('$airmode', 'flights'),
    rule('$airmode', 'fly'),

    rule('$bikemode', '?by bike'),
    rule('$bikemode', 'bike riding'),

    rule('$boatmode', '?by boat'),
    rule('$boatmode', 'cruise'),
    rule('$boatmode', 'cruises'),
    rule('$boatmode', 'norwegian cruise lines'),

    rule('$busmode', '?by bus'),
    rule('$busmode', 'bus tours'),
    rule('$busmode', 'buses'),
    rule('$busmode', 'shutle'),
    rule('$busmode', 'shuttle'),

    rule('$carmode', '?by car'),
    rule('$carmode', 'drive'),
    rule('$carmode', 'driving'),
    rule('$carmode', 'gas'),

    rule('$taximode', 'cab'),
    rule('$taximode', 'car service'),
    rule('$taximode', 'taxi'),

    rule('$trainmode', '?by train'),
    rule('$trainmode', 'trains'),
    rule('$trainmode', 'amtrak'),

    rule('$transitmode', '?by public transportation'),
    rule('$transitmode', '?by ?public transit'),
]

   let's see the rules in action on a toy example.
   in [19]:
rules = rules_travel + rules_travel_locations + rules_travel_modes
grammar = grammar(rules=rules, annotators=travel_annotators)
parses = grammar.parse_input('from boston to austin by train')
print(parses[0].semantics)

created grammar with 54 rules
{'mode': 'train', 'domain': 'travel', 'origin': {'name': 'boston, ma, us', 'id':
 4930956}, 'destination': {'name': 'austin, tx, us', 'id': 4671654}}

   great, it works.

   we're far from done with the travel grammar, but we now have enough in
   place that we should be able to parse several of the examples in our
   training data. this means that we can start hill-climbing on oracle
   accuracy!

   to drive our grammar engineering process, we're going to use a sippycup
   utility function called sample_wins_and_losses(), which will report our
   oracle accuracy on the training data, and show some examples we're
   parsing correctly and some examples we're not.

   (note that sample_wins_and_losses() requires a domain, a model, and a
   metric. a description of these classes is tangential to our
   presentation.  if you're interested, read some sippycup code! it's not
   very complicated.)
   in [20]:
from experiment import sample_wins_and_losses
from metrics import semanticsoracleaccuracymetric
from scoring import model
from travel import traveldomain

domain = traveldomain()
model = model(grammar=grammar)
metric = semanticsoracleaccuracymetric()

sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=31)

loaded geonamesannotator cache with 1462 items
================================================================================
evaluating on 75 examples

--------------------------------------------------------------------------------
over 75 examples:

semantics accuracy                 0.133
semantics oracle accuracy          0.133
number of parses                   0.173
spurious ambiguity                 0.000
has travel parse                   0.173

5 of 10 wins on semantics oracle accuracy:

  airfares to honolulu hawaii
  cruises from new york
  flight from atlanta to vermont
  flights from portland or to seattle wa
  train from moscow to st. petersburg

10 of 65 losses on semantics oracle accuracy:

  cheap tickets to south carolina
  cheapest place to live in florida
  direct flights from california to loreto mexico
  discount travel flights to austin texas
  flights newark to raleigh nc
  one day cruise from fort lauderdale florida
  public transportation to new jersey
  ride this train to roseburg oregon now ther's a town for ya
  transportation to the philadelphia airport
  university of washington transportation to seatac


   as you can see, sample_wins_and_losses() doesn't print many details    
   just a few id74, and then examples of wins and losses
   with the current grammar. (a "win" is an example on which our primary
   metric has a positive value.) our primary metric, semantics oracle
   accuracy, stands at 0.133     not great, but greater than zero, so it's a
   start. the wins make sense: we can parse queries which consist solely
   of travel locations and travel modes, with no extraneous elements. the
   losses are more interesting, because they will provide the motivation
   for the next phase of work.

travel triggers[57]  

   many of the examples we're currently failing to parse contain phrases
   (such as "tickets" or "transportation") which indicate a travel intent,
   but do not specify a travel mode. these phrases constitute the second
   type of travel argument, namely travel triggers. inspection of the
   examples in our training dataset suggest a small number of lexical
   rules, shown here.
   in [21]:
rules_travel_triggers = [
    rule('$travelargument', '$traveltrigger', {}),

    rule('$traveltrigger', 'tickets'),
    rule('$traveltrigger', 'transportation'),
    rule('$traveltrigger', 'travel'),
    rule('$traveltrigger', 'travel packages'),
    rule('$traveltrigger', 'trip'),
]

   let's run sample_wins_and_losses() again, to see how much we've gained,
   and what to work on next.
   in [22]:
rules = rules_travel + rules_travel_locations + rules_travel_modes + rules_trave
l_triggers
grammar = grammar(rules=rules, annotators=travel_annotators)
model = model(grammar=grammar)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

created grammar with 60 rules
================================================================================
evaluating on 75 examples

--------------------------------------------------------------------------------
over 75 examples:

semantics accuracy                 0.173
semantics oracle accuracy          0.173
number of parses                   0.213
spurious ambiguity                 0.000
has travel parse                   0.213

5 of 13 wins on semantics oracle accuracy:

  cruises from new york
  flight from atlanta to vermont
  flight from ft.lauderdale florida to lexington kentucky
  train from moscow to st. petersburg
  travel packages from hartford ct. to berlin germany

10 of 62 losses on semantics oracle accuracy:

  birmingham al distance from indianapolish in
  cost of gas from cedar rapids ia to las vegas
  direct flights from california to loreto mexico
  distance of oxnard ca. to rohnert park ca.
  distance usa to peru
  jet blue flights to new york
  one way air fare to chicago
  rochester ny bus tours to yankees stadium
  scenic drive to duluth mn
  what is 1500 miles away from texas city


   so we've gained about four points (that is, 0.04) in oracle accuracy on
   the training dataset. we're making progress! again, the losses suggest
   where to go next.

request types[58]  

   the third and last kind of travel argument is a request type, which
   indicates a specific type of information sought, such as directions or
   distance. we'll adopt the same methodology, adding lexical rules
   motivated by specific training examples, tied together by higher-level
   rules which add semantics.
   in [23]:
rules_request_types = [
    rule('$travelargument', '$requesttype', sems_0),

    rule('$requesttype', '$directionsrequest', {'type': 'directions'}),
    rule('$requesttype', '$distancerequest', {'type': 'distance'}),
    rule('$requesttype', '$schedulerequest', {'type': 'schedule'}),
    rule('$requesttype', '$costrequest', {'type': 'cost'}),

    rule('$directionsrequest', 'directions'),
    rule('$directionsrequest', 'how do i get'),
    rule('$distancerequest', 'distance'),
    rule('$schedulerequest', 'schedule'),
    rule('$costrequest', 'cost'),
]

   again, we'll check our progress using sample_wins_and_losses().
   in [24]:
rules = rules_travel + rules_travel_locations + rules_travel_modes + rules_trave
l_triggers + rules_request_types
grammar = grammar(rules=rules, annotators=travel_annotators)
model = model(grammar=grammar)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

created grammar with 70 rules
================================================================================
evaluating on 75 examples

--------------------------------------------------------------------------------
over 75 examples:

semantics accuracy                 0.200
semantics oracle accuracy          0.200
number of parses                   0.240
spurious ambiguity                 0.000
has travel parse                   0.240

5 of 15 wins on semantics oracle accuracy:

  cruise from new york to canada
  cruises from new york
  directions from washington to canada
  flight from ft.lauderdale florida to lexington kentucky
  flights from portland or to seattle wa

10 of 60 losses on semantics oracle accuracy:

  cheapest place to live in florida
  cost of gas from cedar rapids ia to las vegas
  distance cunmming georgia to chattanooga
  distance usa to peru
  driving distance washington dc to niagara falls
  ithaca to scranton trains
  one day cruise from fort lauderdale florida
  public transportation to new jersey
  santiago chile to atlanta ga. airline schedule 3 26 06
  transportation to the philadelphia airport


   great, oracle accuracy is up to 0.20. but there's still one big piece
   we're missing.

optionals[59]  

   a key ingredient of the phrase-bag approach to grammar building is the
   ability to accept optional elements interspersed freely among the query
   elements. optionals are phrases which can be either present or absent;
   typically, they contribute nothing to the semantics.

   the following rules illustrate one approach to allowing optionals. the
   first two rules allow any $travelqueryelement to combine with an
   $optionals either to the right or to the left, while ignoring its
   semantics. the third rule defines $optionals as a sequence of one or
   more $optional elements, while the following rules define several
   specific categories of optionals. as usual, most of the lexical rules
   are motivated by specific examples from the training dataset, with a
   few extras included just because they are super obvious.

   this is not necessarily the best design! one of the [60]exercises will
   challenge you to do better.
   in [25]:
rules_optionals = [
    rule('$travelqueryelement', '$travelqueryelement $optionals', sems_0),
    rule('$travelqueryelement', '$optionals $travelqueryelement', sems_1),

    rule('$optionals', '$optional ?$optionals'),

    rule('$optional', '$show'),
    rule('$optional', '$modifier'),
    rule('$optional', '$carrier'),
    rule('$optional', '$stopword'),
    rule('$optional', '$determiner'),

    rule('$show', 'book'),
    rule('$show', 'give ?me'),
    rule('$show', 'show ?me'),

    rule('$modifier', 'cheap'),
    rule('$modifier', 'cheapest'),
    rule('$modifier', 'discount'),
    rule('$modifier', 'honeymoon'),
    rule('$modifier', 'one way'),
    rule('$modifier', 'direct'),
    rule('$modifier', 'scenic'),
    rule('$modifier', 'transatlantic'),
    rule('$modifier', 'one day'),
    rule('$modifier', 'last minute'),

    rule('$carrier', 'delta'),
    rule('$carrier', 'jet blue'),
    rule('$carrier', 'spirit airlines'),
    rule('$carrier', 'amtrak'),

    rule('$stopword', 'all'),
    rule('$stopword', 'of'),
    rule('$stopword', 'what'),
    rule('$stopword', 'will'),
    rule('$stopword', 'it'),
    rule('$stopword', 'to'),

    rule('$determiner', 'a'),
    rule('$determiner', 'an'),
    rule('$determiner', 'the'),
]

   again, we'll check our progress using sample_wins_and_losses().
   in [26]:
rules = rules_travel + rules_travel_locations + rules_travel_modes + rules_trave
l_triggers + rules_request_types + rules_optionals
grammar = grammar(rules=rules, annotators=travel_annotators)
model = model(grammar=grammar)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

created grammar with 104 rules
================================================================================
evaluating on 75 examples

--------------------------------------------------------------------------------
over 75 examples:

semantics accuracy                 0.400
semantics oracle accuracy          0.400
number of parses                   0.773
spurious ambiguity                 0.039
has travel parse                   0.493

5 of 30 wins on semantics oracle accuracy:

  delta flights to oakland
  driving from los angeles to seattle
  flight from ft.lauderdale florida to lexington kentucky
  honeymoon trip to hawaii
  what will it cost to drive from kemerovo to st. petersburg russia

10 of 45 losses on semantics oracle accuracy:

  buses to sacramento rivercats games
  cheap tickets to south carolina
  cheapest place to live in florida
  lawton va to orlando fl amtrak
  nyc flights to buffalo ny
  photos at las vegas nevada from april 1 to april 5 2006
  public transportation to new jersey
  rand mcnally direction to horse shoe casino hammond in
  transportation to the philadelphia airport
  what is 1500 miles away from texas city


   adding support for optionals has doubled oracle accuracy on the
   training dataset, from 0.200 to 0.400. this is a big gain! however,
   there are still many losses, and many of them share a property: they
   are negative examples.

negative examples[61]  

   a id29 model for a given domain should be able to predict
   that a given input does not belong to the domain. we call such inputs
   negative examples. for the travel domain, negative examples include:
discount tickets to new york city ballet
george washington borrows 500 000 from pennsylvania farmer to finance war
ride this train to roseburg oregon now ther's a town for ya


   much of the academic literature on id29 describes systems
   which are required always to produce an in-domain semantic
   representation, no matter what the input. if the input is not
   in-domain, the result is usually garbage. in real-world applications,
   it's much better to have models which can learn when to produce no
   positive output.

   the easiest way to achieve this is to introduce some rules which allow
   any input to be parsed with "negative" semantics, and then learn
   weights for those rule features in the scoring model. in the travel
   domain, the "negative" semantic representation is the special value
   {'domain': 'other'}.
   in [27]:
rules_not_travel = [
    rule('$root', '$nottravelquery', sems_0),
    rule('$nottravelquery', '$text', {'domain': 'other'}),
    rule('$text', '$token ?$text'),
]

   note that the last rule depends on the $token category, which can be
   applied to any token by the tokenannotator. so let's add the
   tokenannotator to our list of annotators.
   in [28]:
travel_annotators = [geonames_annotator, tokenannotator()]

   as usual, we'll check our progress using sample_wins_and_losses().
   in [29]:
rules = rules_travel + rules_travel_locations + rules_travel_modes + rules_trave
l_triggers + rules_request_types + rules_optionals + rules_not_travel
grammar = grammar(rules=rules, annotators=travel_annotators)
model = model(grammar=grammar)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

created grammar with 107 rules
================================================================================
evaluating on 75 examples

--------------------------------------------------------------------------------
over 75 examples:

semantics accuracy                 0.173
semantics oracle accuracy          0.573
number of parses                   1.773
spurious ambiguity                 0.025
has travel parse                   0.493

5 of 43 wins on semantics oracle accuracy:

  cheap flights to hawaii
  directions from washington to canada
  jet blue flights to new york
  travel packages from hartford ct. to berlin germany
  what will it cost to drive from kemerovo to st. petersburg russia

10 of 32 losses on semantics oracle accuracy:

  bike riding-seattle to portland
  birmingham al distance from indianapolish in
  directions to rupparena in lexington
  distance of oxnard ca. to rohnert park ca.
  flights newark to raleigh nc
  fly boston to myrtle beach spirit airlines
  ithaca to scranton trains
  norwegian cruises lines to alaska combined with land tours to denali
  rochester ny bus tours to yankees stadium
  transatlantic cruise southampton to tampa


   we've achieved another big gain in oracle accuracy, from 0.400 to
   0.573, just by ensuring that we offer a "negative" prediction for every
   input. (note that the mean number of parses has increased by exactly 1,
   from 0.773 to 1.773.) however, for the first time, a big gap has opened
   between accuracy, at 0.173, and oracle accuracy, at 0.573. the problem
   is that we don't yet have a scoring model for the travel domain, so the
   ranking of parses is arbitrary. in order to close the gap, we need to
   create a scoring model. one of the [62]exercises will ask you to pursue
   this.

exercises [63]  

   several of these exercises ask you to measure the impact of your change
   on key id74. part of your job is to decide which
   id74 are most relevant for the change you're making. it's
   probably best to evaluate only on training data, in order to keep the
   test data unseen during development. (but the test data is hardly a
   state secret, so whatever.)

straightforward[64]  

    1. select 20 of the 6,588 queries in [65]aol-travel-queries.txt and
       manually annotate them with target semantics. select your queries
       using uniform random sampling     this will minimize overlap between
       different people completing this exercise, and therefore maximize
       the value of the aggregate labor. (in python, you can sample from a
       list using random.sample().) you will likely find some cases where
       it's not clear what the right semantics are. do your best. the
       point of the exercise is to develop an awareness of the challenges
       of annotation, and to recognize that there's no such thing as
       perfectly annotated data.
    2. many of the remaining errors on training examples occur because the
       origin isn't marked by "from". examples include "transatlantic
       cruise southampton to tampa", "fly boston to myrtle beach spirit
       airlines", and "distance usa to peru". extend the grammar to handle
       examples like these. measure the impact on key id74.
    3. does your solution to the previous exercise handle examples where
       some other query element intervenes between origin and destination?
       examples include "university of washington transportation to
       seatac", "birmingham al distance from indianapolish in", and "nyc
       flights to buffalo ny". if not, extend the grammar to handle
       examples like these. measure the impact on key id74.
    4. the current structure of the grammar permits parses containing any
       number of $fromlocations and $tolocations, including zero. find a
       way to require that (a) there is at least one $fromlocation or
       $tolocation, (b) there are not multiple $fromlocations or
       $tolocations.
    5. the travel grammar is lacking a scoring model, and it shows a big
       gap between accuracy and oracle accuracy. examine and diagnose some
       examples where accuracy is 0 even though oracle accuracy is 1.
       propose and implement a scoring model, and measure its efficacy in
       closing that gap.

challenging[66]  

    1. extend grammar to allow rules which mix terminals and non-terminals
       on the rhs, such as rule('$routequery', '$travelmode from $location
       to $location').
    2. try to improve the precision of the geonamesannotator by fiddling
       with the geonames api request parameters documented [67]here. for
       example, the featureclass, countrybias, or orderby parameters seem
       like promising targets.
    3. try to improve the coverage of the geonamesannotator by enabling it
       to return multiple annotations for ambiguous location names.
       investigate the impact of varying the maximum number of annotations
       on various performance metrics, including accuracy, oracle
       accuracy, and number of parses. how would you characterize the
       tradeoff you're making?
    4. building on the previous exercise, implement a feature which
       captures information about the result rank of annotations generated
       by the geonamesannotator, and see if you can use this feature to
       narrow the gap between accuracy and oracle accuracy.
    5. you have probably noticed that one of our standard evaluation
       metrics is something called "spurious ambiguity". dig into the
       sippycup codebase to figure out what spurious ambiguity is. here's
       a hint: it's something bad, so we want to push that metric toward
       zero. find a training example where it's not zero, and figure out
       why the example exhibits spurious ambiguity. are there changes we
       can make to the grammar to reduce spurious ambiguity? also, why is
       spurious ambiguity undesirable?
    6. in its current form, the travel grammar parses lots of queries it
       shouldn't. (by "parses", we mean "generates a positive parse".)
       this problem is known as overtriggering. the overtriggering problem
       is hard to observe on our tiny dataset of 100 examples, where most
       of the examples are positive examples. investigate overtriggering
       by downloading the aol query dataset, identifying the 1,000 most
       frequent queries, and running them through the grammar. how many
       cases of overtriggering do you find? can you suggest some simple
       changes to minimize overtriggering? (warning: the aol dataset
       contains lots of queries which may be offensive. skip this exercise
       if you're not down with that.)
    7. consider the queries "flights los angeles hawaii" vs. "flights los
       angeles california". despite the superficial resemblance, the
       second query seems to mean flights to los angeles, not flights from
       los angeles to california. but the grammar can successfully
       interpret both queries only if it permits both interpretations for
       each query. this creates a ranking problem: which interpretation
       should be scored higher? can you add to the scoring model a feature
       which solves the problem?
    8. develop and execute a strategy to annotate all 6,588 queries in
       [68]aol-travel-queries.txt with target semantics using
       id104. warning: this is an ambitious exercise.

   copyright (c) 2015 bill maccartney

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [69]fastly, rendered by [70]rackspace

   nbviewer github [71]repository.

   nbviewer version: [72]33c4683

   nbconvert version: [73]5.4.0

   rendered (fri, 05 apr 2019 18:00:55 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb
   5. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb
   6. https://github.com/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb
   7. https://mybinder.org/v2/gh/wcmac/sippycup/master?filepath=sippycup-unit-2.ipynb
   8. https://raw.githubusercontent.com/wcmac/sippycup/master/sippycup-unit-2.ipynb
   9. https://nbviewer.jupyter.org/github/wcmac/sippycup/tree/master
  10. https://nbviewer.jupyter.org/github/wcmac/sippycup/tree/master/sippycup-unit-2.ipynb
  11. http://nlp.stanford.edu/~wcmac/
  12. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-0.ipynb
  13. https://www.apple.com/ios/siri/
  14. https://www.google.com/landing/now/
  15. http://www.microsoft.com/en-us/mobile/campaign-cortana/
  16. http://www.amazon.com/oc/echo/
  17. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-1.ipynb
  18. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#example-inputs
  19. http://people.cs.georgetown.edu/~abdur/publications/pos-infoscale.pdf
  20. http://en.wikipedia.org/wiki/aol_search_data_leak
  21. http://www.cim.mcgill.ca/~dudek/206/logs/aol-user-ct-collection/
  22. ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase
  23. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/travel_examples.py
  24. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#semantic-representation-
  25. http://www.geonames.org/
  26. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/geonames.py
  27. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb
  28. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#example-data
  29. http://www.cs.utexas.edu/users/ml/geo.html
  30. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-1.ipynb
  31. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-exercises
  32. https://www.mturk.com/mturk/welcome
  33. http://www.cs.berkeley.edu/~jordan/papers/liang-jordan-klein-acl2011.pdf
  34. http://www-nlp.stanford.edu/software/sempre/
  35. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/travel_examples.py
  36. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#enriching-the-grammar-class
  37. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#arithmetic-exercises
  38. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#annotators
  39. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#unary-rules
  40. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#n-ary-rules
  41. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#optional-elements
  42. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#grammar-engineering
  43. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#phrase-bag-grammars
  44. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-locations
  45. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#the-geonamesannotator
  46. http://en.wikipedia.org/wiki/geocoding
  47. http://en.wikipedia.org/wiki/geographic_coordinate_system
  48. http://www.geonames.org/
  49. http://www.geonames.org/4930956/
  50. http://en.wikipedia.org/wiki/representational_state_transfer
  51. http://api.geonames.org/searchjson?q=palo+alto&username=demo
  52. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/geonames.py
  53. http://www.geonames.org/login
  54. http://www.geonames.org/export/geonames-search.html
  55. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-exercises
  56. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-modes
  57. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-triggers
  58. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#request-types
  59. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#optionals
  60. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-exercises
  61. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#negative-examples
  62. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#travel-exercises
  63. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#exercises-
  64. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#straightforward
  65. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/aol-travel-queries.txt
  66. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb#challenging
  67. http://www.geonames.org/export/geonames-search.html
  68. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/aol-travel-queries.txt
  69. http://www.fastly.com/
  70. https://developer.rackspace.com/?nbviewer=awesome
  71. https://github.com/jupyter/nbviewer
  72. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  73. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
