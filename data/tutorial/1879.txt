   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 3 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]sippycup
    2. [10]sippycup-unit-3.ipynb

   [sippycup-small.jpg]

sippycup
unit 3: geography queries

   [11]bill maccartney
   spring 2015
   this is unit 3 of the [12]sippycup codelab.

   our third case study will examine the domain of geography queries. in
   particular, we'll focus on the [13]geo880 corpus, which contains 880
   queries about u.s. geography. examples include:
"which states border texas?"
"how many states border the largest state?"
"what is the size of the capital of texas?"


   the geo880 queries have a quite different character from the arithmetic
   queries and travel queries we have examined previously. they differ
   from the arithmetic queries in using a large vocabulary, and in
   exhibiting greater degrees of both lexical and syntactic ambiguity.
   they differ from the travel queries in adhering to conventional rules
   for spelling and syntax, and in having semantics with arbitrarily
   complex compositional structure. for example:
"what rivers flow through states that border the state with the largest populati
on?"
"what is the population of the capital of the largest state through which the mi
ssissippi runs?"
"what is the longest river that passes the states that border the state that bor
ders the most states?"


   geo880 was developed in ray mooney's group at ut austin. it is of
   particular interest because it has for many years served as a standard
   evaluation for id29 systems. (see, for example, [14]zelle &
   mooney 1996, [15]tang & mooney 2001, [16]zettlemoyer & collins 2005,
   and [17]liang et al. 2011.) it has thereby become, for many, a
   paradigmatic application of id29. it has also served as a
   bridge between an older current of research on natural language
   interfaces to databases (nlidbs) (see [18]androutsopoulos et al. 1995)
   and the modern era of id29.

   the domain of geography queries is also of interest because there are
   many plausible real-world applications for id29 which
   similarly involve complex compositional queries against a richly
   structured knowledge base. for example, some people are passionate
   about baseball statistics, and might want to ask queries like:
"pitchers who have struck out four batters in one inning"
"players who have stolen at least 100 bases in a season"
"complete games with fewer than 90 pitches"
"most home runs hit in one game"


   environmental advocates and policymakers might have queries like:
"which country has the highest co2 emissions"
"what five countries have the highest per capita co2 emissions"
"what country's co2 emissions increased the most over the last five years"
"what fraction of co2 emissions was from european countries in 2010"


   techniques that work in the geography domain are likely to work in
   these other domains too.

the geo880 dataset[19]  

   i've been told that the geo880 queries were collected from students in
   classes taught by ray mooney at ut austin. i'm not sure whether i've
   got the story right. but this account is consistent with one of the
   notable limitations of the dataset: it is not a natural distribution,
   and not a realistic representation of the geography queries that people
   actually ask on, say, google. nobody ever asks, "what is the longest
   river that passes the states that border the state that borders the
   most states?" nobody. ever.

   the dataset was published online by rohit jaivant kate in a [20]prolog
   file containing semantic representations in prolog style. it was later
   republished by yuk wah wong as an [21]xml file containing additional
   metadata for each example, including translations into spanish,
   japanese, and turkish; syntactic parse trees; and semantics in two
   different representations: prolog and [22]funql.

   in sippycup, we're not going to use either prolog or funql semantics.
   instead, we'll use examples which have been annotated only with
   denotations (which were provided by percy liang     thanks!). of course,
   our grammar will require a semantic representation, even if our
   examples are not annotated with semantics. we will introduce one
   [23]below.

   the geo880 dataset is conventionally divided into 600 training examples
   and 280 test examples. in sippycup, the dataset can in found in
   [24]geo880.py. let's take a peek.
   in [1]:
from geo880 import geo880_train_examples, geo880_test_examples

print('train examples:', len(geo880_train_examples))
print('test examples: ', len(geo880_test_examples))
print(geo880_train_examples[0])
print(geo880_test_examples[0])

train examples: 600
test examples:  280
example(input='what is the highest point in florida ?', denotation=('/place/walt
on_county',))
example(input='which state is the smallest ?', denotation=('/state/district_of_c
olumbia',))

the geobase knowledge base[25]  

   geobase is a small knowledge base about the geography of the united
   states. it contains (almost) all the information needed to answer
   queries in the geo880 dataset, including facts about:
     * states: capital, area, population, major cities, neighboring
       states, highest and lowest points and elevations
     * cities: containing state and population
     * rivers: length and states traversed
     * mountains: containing state and height
     * roads: states traversed
     * lakes: area, states traversed

   sippycup contains a class called geobasereader (in [26]geobase.py)
   which facilitates working with geobase in python. it reads and parses
   the geobase prolog file, and creates a set of tuples representing its
   content. let's take a look.
   in [2]:
from geobase import geobasereader

reader = geobasereader()
unaries = [str(t) for t in reader.tuples if len(t) == 2]
print('\nsome unaries:\n  ' + '\n  '.join(unaries[:10]))
binaries = [str(t) for t in reader.tuples if len(t) == 3]
print('\nsome binaries:\n  ' + '\n  '.join(binaries[:10]))

geobasereader read 51 state rows.
geobasereader read 386 city rows.
geobasereader read 46 river rows.
geobasereader read 51 border rows.
geobasereader read 51 highlow rows.
geobasereader read 50 mountain rows.
geobasereader read 40 road rows.
geobasereader read 22 lake rows.
geobasereader read 1 country row.
geobasereader computed transitive closure of 'contains', adding 495 edges

some unaries:
  ('city', '/city/providence_ri')
  ('city', '/city/ewa_hi')
  ('state', '/state/district_of_columbia')
  ('city', '/city/fort_wayne_in')
  ('city', '/city/concord_ca')
  ('place', '/place/little_river')
  ('road', '/road/85')
  ('city', '/city/high_point_nc')
  ('city', '/city/el_paso_tx')
  ('city', '/city/santa_monica_ca')

some binaries:
  ('contains', '/state/arizona', '/city/tempe_az')
  ('traverses', '/road/90', '/state/illinois')
  ('population', '/city/philadelphia_pa', 1688210)
  ('contains', '/state/texas', '/city/midland_tx')
  ('contains', '/country/usa', '/state/indiana')
  ('contains', '/country/usa', '/city/stockton_ca')
  ('contains', '/country/usa', '/city/utica_ny')
  ('contains', '/country/usa', '/city/augusta_me')
  ('contains', '/country/usa', '/state/connecticut')
  ('abbreviation', '/state/alabama', 'al')

   some observations here:
     * unaries are pairs consisting of a unary predicate (a type) and an
       entity.
     * binaries are triples consisting of binary predicate (a relation)
       and two entities (or an entity and a numeric or string value).
     * entities are named by unique identifiers of the form /type/name.
       this is a geobasereader convention; these identifiers are not used
       in the original prolog file.
     * some entities have the generic type place because they occur in the
       prolog file only as the highest or lowest point in a state, and
       it's hard to reliably assign such points to one of the more
       specific types.
     * the original prolog file is inconsistent about units. for example,
       the area of states is expressed in square miles, but the area of
       lakes is expressed in square kilometers. geobasereader converts
       everything to [27]si units: meters and square meters.

semantic representation [28]  

   geobasereader merely reads the data in geobase into a set of tuples. it
   doesn't provide any facility for querying that data. that's where
   graphkb and graphkbexecutor come in. graphkb is a graph-structured
   knowledge base, with indexing for fast lookups. graphkbexecutor defines
   a representation for formal queries against that knowledge base, and
   supports query execution. the formal query language defined by
   graphkbexecutor will serve as our semantic representation for the
   geography domain.

the graphkb class[29]  

   a graphkb is a generic graph-structured knowledge base, or
   equivalently, a set of relational pairs and triples, with indexing for
   fast lookups. it represents a knowledge base as set of tuples, each
   either:
     * a pair, consisting of a unary relation and an element which belongs
       to it, or
     * a triple consisting of a binary relation and a pair of elements
       which belong to it.

   for example, we can construct a graphkb representing facts about
   [30]the simpsons:
   in [3]:
from graph_kb import graphkb

simpsons_tuples = [

     # unaries
    ('male', 'homer'),
    ('female', 'marge'),
    ('male', 'bart'),
    ('female', 'lisa'),
    ('female', 'maggie'),
    ('adult', 'homer'),
    ('adult', 'marge'),
    ('child', 'bart'),
    ('child', 'lisa'),
    ('child', 'maggie'),

    # binaries
    ('has_age', 'homer', 36),
    ('has_age', 'marge', 34),
    ('has_age', 'bart', 10),
    ('has_age', 'lisa', 8),
    ('has_age', 'maggie', 1),
    ('has_brother', 'lisa', 'bart'),
    ('has_brother', 'maggie', 'bart'),
    ('has_sister', 'bart', 'maggie'),
    ('has_sister', 'bart', 'lisa'),
    ('has_sister', 'lisa', 'maggie'),
    ('has_sister', 'maggie', 'lisa'),
    ('has_father', 'bart', 'homer'),
    ('has_father', 'lisa', 'homer'),
    ('has_father', 'maggie', 'homer'),
    ('has_mother', 'bart', 'marge'),
    ('has_mother', 'lisa', 'marge'),
    ('has_mother', 'maggie', 'marge'),
]

simpsons_kb = graphkb(simpsons_tuples)

   the graphkb object now contains three indexes:
     * unaries[u]: all entities belonging to unary relation u
     * binaries_fwd[b][e]: all entities x such that (e, x) belongs to
       binary relation b
     * binaries_rev[b][e]: all entities x such that (x, e) belongs to
       binary relation b

   for example:
   in [4]:
simpsons_kb.unaries['child']

   out[4]:
{'bart', 'lisa', 'maggie'}

   in [5]:
simpsons_kb.binaries_fwd['has_sister']['lisa']

   out[5]:
{'maggie'}

   in [6]:
simpsons_kb.binaries_rev['has_sister']['lisa']

   out[6]:
{'bart', 'maggie'}

the graphkbexecutor class[31]  

   a graphkbexecutor executes formal queries against a graphkb and returns
   their denotations. queries are represented by python tuples, and can be
   nested. denotations are also represented by python tuples, but are
   conceptually sets (possibly empty). the elements of these tuples are
   always sorted in canonical order, so that they can be reliably compared
   for set equality.

   the query language defined by graphkbexecutor is perhaps most easily
   explained by example:
   in [7]:
queries = [
    'bart',
    'male',
    ('has_sister', 'lisa'),  # who has sister lisa?
    ('lisa', 'has_sister'),  # lisa has sister who, i.e., who is a sister of lis
a?
    ('lisa', 'has_brother'), # lisa has brother who, i.e., who is a brother of l
isa?
    ('.and', 'male', 'child'),
    ('.or', 'male', 'adult'),
    ('.not', 'child'),
    ('.any',),               # anything
    ('.any', 'has_sister'),  # anything has sister who, i.e., who is a sister of
 anything?
    ('.and', 'child', ('.not', ('.any', 'has_sister'))),
    ('.count', ('bart', 'has_sister')),
    ('has_age', ('.gt', 21)),
    ('has_age', ('.lt', 2)),
    ('has_age', ('.eq', 10)),
    ('.max', 'has_age', 'female'),
    ('.min', 'has_age', ('bart', 'has_sister')),
    ('.max', 'has_age', '.any'),
    ('.argmax', 'has_age', 'female'),
    ('.argmin', 'has_age', ('bart', 'has_sister')),
    ('.argmax', 'has_age', '.any'),
]

executor = simpsons_kb.executor()
for query in queries:
    print()
    print('q  ', query)
    print('d  ', executor.execute(query))

q   bart
d   ('bart',)

q   male
d   ('bart', 'homer')

q   ('has_sister', 'lisa')
d   ('bart', 'maggie')

q   ('lisa', 'has_sister')
d   ('maggie',)

q   ('lisa', 'has_brother')
d   ('bart',)

q   ('.and', 'male', 'child')
d   ('bart',)

q   ('.or', 'male', 'adult')
d   ('bart', 'homer', 'marge')

q   ('.not', 'child')
d   (1, 10, 34, 36, 8, 'homer', 'marge')

q   ('.any',)
d   (1, 10, 34, 36, 8, 'bart', 'homer', 'lisa', 'maggie', 'marge')

q   ('.any', 'has_sister')
d   ('lisa', 'maggie')

q   ('.and', 'child', ('.not', ('.any', 'has_sister')))
d   ('bart',)

q   ('.count', ('bart', 'has_sister'))
d   (2,)

q   ('has_age', ('.gt', 21))
d   ('homer', 'marge')

q   ('has_age', ('.lt', 2))
d   ('maggie',)

q   ('has_age', ('.eq', 10))
d   ('bart',)

q   ('.max', 'has_age', 'female')
d   (34,)

q   ('.min', 'has_age', ('bart', 'has_sister'))
d   (1,)

q   ('.max', 'has_age', '.any')
d   (36,)

q   ('.argmax', 'has_age', 'female')
d   ('marge',)

q   ('.argmin', 'has_age', ('bart', 'has_sister'))
d   ('maggie',)

q   ('.argmax', 'has_age', '.any')
d   ('homer',)

   note that the query (r e) denotes entities having relation r to entity
   e, whereas the query (e r) denotes entities to which entity e has
   relation r.

   for a more detailed understanding of the style of semantic
   representation defined by graphkbexecutor, take a look at the
   [32]source code.

using graphkbexecutor with geobase[33]  

   in [8]:
geobase = graphkb(reader.tuples)
executor = geobase.executor()
queries = [
    ('/state/texas', 'capital'), # capital of texas
    ('.and', 'river', ('traverses', '/state/utah')), # rivers that traverse utah
    ('.argmax', 'height', 'mountain'), # tallest mountain
]
for query in queries:
    print()
    print(query)
    print(executor.execute(query))

('/state/texas', 'capital')
('/city/austin_tx',)

('.and', 'river', ('traverses', '/state/utah'))
('/river/colorado', '/river/green', '/river/san_juan')

('.argmax', 'height', 'mountain')
('/mountain/mckinley',)

grammar engineering[34]  

   it's time to start developing a grammar for the geography domain. as in
   unit 2, the performance metric we'll focus on during grammar
   engineering is oracle accuracy (the proportion of examples for which
   any parse is correct), not accuracy (the proportion of examples for
   which the first parse is correct). remember that oracle accuracy is an
   upper bound on accuracy, and is a measure of the expressive power of
   the grammar: does it have the rules it needs to generate the correct
   parse? the gap between oracle accuracy and accuracy, on the other hand,
   reflects the ability of the scoring model to bring the correct parse to
   the top of the candidate list.

   as always, we're going to take a data-driven approach to grammar
   engineering. we want to introduce rules which will enable us to handle
   the lexical items and syntactic structures that we actually observe in
   the geo880 training data. to that end, let's count the words that
   appear among the 600 training examples. (we do not examine the test
   data!)
   in [9]:
from collections import defaultdict
from operator import itemgetter
from geo880 import geo880_train_examples

words = [word for example in geo880_train_examples for word in example.input.spl
it()]
counts = defaultdict(int)
for word in words:
    counts[word] += 1
counts = sorted([(count, word) for word, count in counts.items()], reverse=true)
print('there were %d tokens of %d types:\n' % (len(words), len(counts)))
print(', '.join(['%s (%d)' % (word, count) for count, word in counts[:50]] + ['.
..']))

there were 5109 tokens of 248 types:

the (618), ? (600), what (386), is (283), in (247), state (170), states (169), o
f (150), how (111), many (90), are (90), population (86), river (76), which (75)
, through (73), largest (67), border (65), texas (63), rivers (62), cities (56),
 point (55), highest (54), city (49), that (48), capital (48), with (44), has (4
3), major (42), run (34), smallest (31), people (29), mississippi (29), us (28),
 does (27), usa (26), most (26), longest (25), area (24), lowest (23), have (23)
, density (23), colorado (23), borders (23), new (22), live (22), where (17), ru
ns (17), biggest (16), california (14), alaska (14), ...

   there are at least four major categories of words here:
     * words that refer to entities, such as "texas", "mississippi",
       "usa", and "austin".
     * words that refer to types, such as "state", "river", and "cities".
     * words that refer to relations, such as "in", "borders", "capital",
       and "long".
     * other function words, such as "the", "what", "how", and "are".

   one might make finer distinctions, but this seems like a reasonable
   starting point. note that these categories do not always correspond to
   traditional syntactic categories. while the entities are typically
   proper nouns, and the types are typically common nouns, the relations
   include prepositions, verbs, nouns, and adjectives.

   the design of our grammar will roughly follow this schema. the major
   categories will include $entity, $type, $collection, $relation, and
   $optional.

optionals[35]  

   in [36]unit 2, our grammar engineering process didn't really start
   cooking until we introduced optionals. this time around, let's begin
   with the optionals. we'll define as $optional every word in the geo880
   training data which does not plainly refer to an entity, type, or
   relation. and we'll let any query be preceded or followed by a sequence
   of one or more $optionals.
   in [10]:
from parsing import grammar, rule

optional_words = [
    'the', '?', 'what', 'is', 'in', 'of', 'how', 'many', 'are', 'which', 'that',
    'with', 'has', 'major', 'does', 'have', 'where', 'me', 'there', 'give',
    'name', 'all', 'a', 'by', 'you', 'to', 'tell', 'other', 'it', 'do', 'whose',
    'show', 'one', 'on', 'for', 'can', 'whats', 'urban', 'them', 'list',
    'exist', 'each', 'could', 'about'
]

rules_optionals = [
    rule('$root', '?$optionals $query ?$optionals', lambda sems: sems[1]),
    rule('$optionals', '$optional ?$optionals'),
] + [rule('$optional', word) for word in optional_words]

   because $query has not yet been defined, we won't be able to parse
   anything yet.

entities and collections[37]  

   our grammar will need to be able to recognize names of entities, such
   as "utah". there are hundreds of entities in geobase, and we don't want
   to have to introduce a grammar rule for each entity. instead, we'll
   define a new annotator, geobaseannotator, which simply annotates
   phrases which exactly match names in geobase.
   in [11]:
from annotator import annotator, numberannotator

class geobaseannotator(annotator):
    def __init__(self, geobase):
        self.geobase = geobase

    def annotate(self, tokens):
        phrase = ' '.join(tokens)
        places = self.geobase.binaries_rev['name'][phrase]
        return [('$entity', place) for place in places]

   now a couple of rules that will enable us to parse inputs that simply
   name locations, such as "utah".

   (todo: explain rationale for $collection and $query.)
   in [12]:
rules_collection_entity = [
    rule('$query', '$collection', lambda sems: sems[0]),
    rule('$collection', '$entity', lambda sems: sems[0]),
]

rules = rules_optionals + rules_collection_entity

   now let's make a grammar.
   in [13]:
annotators = [numberannotator(), geobaseannotator(geobase)]
grammar = grammar(rules=rules, annotators=annotators)

created grammar with 48 rules

   let's try to parse some inputs which just name locations.
   in [14]:
parses = grammar.parse_input('what is utah')
for parse in parses[:1]:
    print('\n'.join([str(parse.semantics), str(executor.execute(parse.semantics)
)]))

/state/utah
('/state/utah',)

   great, it worked. now let's run an evaluation on the geo880 training
   examples.
   in [15]:
from experiment import sample_wins_and_losses
from geoquery import geoquerydomain
from metrics import denotationoracleaccuracymetric
from scoring import model

domain = geoquerydomain()
model = model(grammar=grammar, executor=executor.execute)
metric = denotationoracleaccuracymetric()

sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

geobasereader read 51 state rows.
geobasereader read 386 city rows.
geobasereader read 46 river rows.
geobasereader read 51 border rows.
geobasereader read 51 highlow rows.
geobasereader read 50 mountain rows.
geobasereader read 40 road rows.
geobasereader read 22 lake rows.
geobasereader read 1 country row.
geobasereader computed transitive closure of 'contains', adding 495 edges
================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
over 600 examples:

denotation accuracy                0.000
denotation oracle accuracy         0.000
number of parses                   0.028
spurious ambiguity                 0.000

0 of 0 wins on denotation oracle accuracy:


10 of 600 losses on denotation oracle accuracy:

  how long is the shortest river in the usa ?
  name the states which have no surrounding states ?
  what is the capital of the state with the highest point ?
  what is the capital of utah ?
  what is the highest elevation in new mexico ?
  what is the highest point in the smallest state ?
  what is the population density of wyoming ?
  what is the total population of the states that border texas ?
  what state has the largest population ?
  what states does the mississippi run through ?


   we don't yet have a single win: denotation oracle accuracy remains
   stuck at zero. however, the average number of parses is slightly
   greater than zero, meaning that there are a few examples which our
   grammar can parse (though not correctly). it would be interesting to
   know which examples. there's a utility function in [38]experiment.py
   which will give you the visibility you need. see if you can figure out
   what to do.

types[39]  

   (todo: the words in the training data include lots of words for types.
   let's write down some lexical rules defining the category $type, guided
   as usual by the words we actually see in the training data. we'll also
   make $type a kind of $collection.)
   in [16]:
rules_types = [
    rule('$collection', '$type', lambda sems: sems[0]),

    rule('$type', 'state', 'state'),
    rule('$type', 'states', 'state'),
    rule('$type', 'city', 'city'),
    rule('$type', 'cities', 'city'),
    rule('$type', 'big cities', 'city'),
    rule('$type', 'towns', 'city'),
    rule('$type', 'river', 'river'),
    rule('$type', 'rivers', 'river'),
    rule('$type', 'mountain', 'mountain'),
    rule('$type', 'mountains', 'mountain'),
    rule('$type', 'mount', 'mountain'),
    rule('$type', 'peak', 'mountain'),
    rule('$type', 'road', 'road'),
    rule('$type', 'roads', 'road'),
    rule('$type', 'lake', 'lake'),
    rule('$type', 'lakes', 'lake'),
    rule('$type', 'country', 'country'),
    rule('$type', 'countries', 'country'),
]

   we should now be able to parse inputs denoting types, such as "name the
   lakes":
   in [17]:
rules = rules_optionals + rules_collection_entity + rules_types
grammar = grammar(rules=rules, annotators=annotators)
parses = grammar.parse_input('name the lakes')
for parse in parses[:1]:
    print('\n'.join([str(parse.semantics), str(executor.execute(parse.semantics)
)]))

created grammar with 67 rules
lake
('/lake/becharof', '/lake/champlain', '/lake/erie', '/lake/flathead', '/lake/gre
at_salt_lake', '/lake/huron', '/lake/iliamna', '/lake/lake_of_the_woods', '/lake
/michigan', '/lake/mille_lacs', '/lake/naknek', '/lake/okeechobee', '/lake/ontar
io', '/lake/pontchartrain', '/lake/rainy', '/lake/red', '/lake/salton_sea', '/la
ke/st._clair', '/lake/superior', '/lake/tahoe', '/lake/teshekpuk', '/lake/winneb
ago')

   it worked. let's evaluate on the geo880 training data again.
   in [18]:
model = model(grammar=grammar, executor=executor.execute)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
over 600 examples:

denotation accuracy                0.003
denotation oracle accuracy         0.003
number of parses                   0.033
spurious ambiguity                 0.000

2 of 2 wins on denotation oracle accuracy:

  list the states ?
  what are the states ?

10 of 598 losses on denotation oracle accuracy:

  give me the cities which are in texas ?
  how many people are there in iowa ?
  how many states have major rivers ?
  name the rivers in arkansas ?
  what river runs through virginia ?
  what rivers are in utah ?
  what state has the highest elevation ?
  what states border rhode island ?
  where is mount whitney located ?
  which rivers run through the state with the largest city in the us ?


   liftoff! we have two wins, and denotation oracle accuracy is greater
   than zero! just barely.

relations and joins[40]  

   in order to really make this bird fly, we're going to have to handle
   relations. in particular, we'd like to be able to parse queries which
   combine a relation with an entity or collection, such as "what is the
   capital of vermont".

   as usual, we'll adopt a data-driven approach. the training examples
   include lots of words and phrases which refer to relations, both
   "forward" relations (like "traverses") and "reverse" relations (like
   "traversed by"). guided by the training data, we'll write lexical rules
   which define the categories $fwdrelation and $revrelation. then we'll
   add rules that allow either a $fwdrelation or a $revrelation to be
   promoted to a generic $relation, with semantic functions which ensure
   that the semantics are constructed with the proper orientation.
   finally, we'll define a rule for joining a $relation (such as "capital
   of") with a $collection (such as "vermont") to yield another
   $collection (such as "capital of vermont").
   in [19]:
rules_relations = [
    rule('$collection', '$relation ?$optionals $collection', lambda sems: sems[0
](sems[2])),

    rule('$relation', '$fwdrelation', lambda sems: (lambda arg: (sems[0], arg)))
,
    rule('$relation', '$revrelation', lambda sems: (lambda arg: (arg, sems[0])))
,

    rule('$fwdrelation', '$fwdbordersrelation', 'borders'),
    rule('$fwdbordersrelation', 'border'),
    rule('$fwdbordersrelation', 'bordering'),
    rule('$fwdbordersrelation', 'borders'),
    rule('$fwdbordersrelation', 'neighbor'),
    rule('$fwdbordersrelation', 'neighboring'),
    rule('$fwdbordersrelation', 'surrounding'),
    rule('$fwdbordersrelation', 'next to'),

    rule('$fwdrelation', '$fwdtraversesrelation', 'traverses'),
    rule('$fwdtraversesrelation', 'cross ?over'),
    rule('$fwdtraversesrelation', 'flow through'),
    rule('$fwdtraversesrelation', 'flowing through'),
    rule('$fwdtraversesrelation', 'flows through'),
    rule('$fwdtraversesrelation', 'go through'),
    rule('$fwdtraversesrelation', 'goes through'),
    rule('$fwdtraversesrelation', 'in'),
    rule('$fwdtraversesrelation', 'pass through'),
    rule('$fwdtraversesrelation', 'passes through'),
    rule('$fwdtraversesrelation', 'run through'),
    rule('$fwdtraversesrelation', 'running through'),
    rule('$fwdtraversesrelation', 'runs through'),
    rule('$fwdtraversesrelation', 'traverse'),
    rule('$fwdtraversesrelation', 'traverses'),

    rule('$revrelation', '$revtraversesrelation', 'traverses'),
    rule('$revtraversesrelation', 'has'),
    rule('$revtraversesrelation', 'have'),  # 'how many states have major rivers
'
    rule('$revtraversesrelation', 'lie on'),
    rule('$revtraversesrelation', 'next to'),
    rule('$revtraversesrelation', 'traversed by'),
    rule('$revtraversesrelation', 'washed by'),

    rule('$fwdrelation', '$fwdcontainsrelation', 'contains'),
    # 'how many states have a city named springfield'
    rule('$fwdcontainsrelation', 'has'),
    rule('$fwdcontainsrelation', 'have'),

    rule('$revrelation', '$revcontainsrelation', 'contains'),
    rule('$revcontainsrelation', 'contained by'),
    rule('$revcontainsrelation', 'in'),
    rule('$revcontainsrelation', 'found in'),
    rule('$revcontainsrelation', 'located in'),
    rule('$revcontainsrelation', 'of'),

    rule('$revrelation', '$revcapitalrelation', 'capital'),
    rule('$revcapitalrelation', 'capital'),
    rule('$revcapitalrelation', 'capitals'),

    rule('$revrelation', '$revhighestpointrelation', 'highest_point'),
    rule('$revhighestpointrelation', 'high point'),
    rule('$revhighestpointrelation', 'high points'),
    rule('$revhighestpointrelation', 'highest point'),
    rule('$revhighestpointrelation', 'highest points'),

    rule('$revrelation', '$revlowestpointrelation', 'lowest_point'),
    rule('$revlowestpointrelation', 'low point'),
    rule('$revlowestpointrelation', 'low points'),
    rule('$revlowestpointrelation', 'lowest point'),
    rule('$revlowestpointrelation', 'lowest points'),
    rule('$revlowestpointrelation', 'lowest spot'),

    rule('$revrelation', '$revhighestelevationrelation', 'highest_elevation'),
    rule('$revhighestelevationrelation', '?highest elevation'),

    rule('$revrelation', '$revheightrelation', 'height'),
    rule('$revheightrelation', 'elevation'),
    rule('$revheightrelation', 'height'),
    rule('$revheightrelation', 'high'),
    rule('$revheightrelation', 'tall'),

    rule('$revrelation', '$revarearelation', 'area'),
    rule('$revarearelation', 'area'),
    rule('$revarearelation', 'big'),
    rule('$revarearelation', 'large'),
    rule('$revarearelation', 'size'),

    rule('$revrelation', '$revpopulationrelation', 'population'),
    rule('$revpopulationrelation', 'big'),
    rule('$revpopulationrelation', 'large'),
    rule('$revpopulationrelation', 'populated'),
    rule('$revpopulationrelation', 'population'),
    rule('$revpopulationrelation', 'populations'),
    rule('$revpopulationrelation', 'populous'),
    rule('$revpopulationrelation', 'size'),

    rule('$revrelation', '$revlengthrelation', 'length'),
    rule('$revlengthrelation', 'length'),
    rule('$revlengthrelation', 'long'),
]

   we should now be able to parse "what is the capital of vermont". let's
   see:
   in [20]:
rules = rules_optionals + rules_collection_entity + rules_types + rules_relation
s
grammar = grammar(rules=rules, annotators=annotators)
parses = grammar.parse_input('what is the capital of vermont ?')
for parse in parses[:1]:
    print('\n'.join([str(parse.semantics), str(executor.execute(parse.semantics)
)]))

created grammar with 146 rules
('/state/vermont', 'capital')
('/city/montpelier_vt',)

   montpelier! i always forget that one.

   ok, let's evaluate our progress on the geo880 training data.
   in [21]:
model = model(grammar=grammar, executor=executor.execute)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
over 600 examples:

denotation accuracy                0.113
denotation oracle accuracy         0.125
number of parses                   0.427
spurious ambiguity                 0.000

5 of 75 wins on denotation oracle accuracy:

  what is the capital of utah ?
  what is the highest point in texas ?
  what is the population of idaho ?
  what is the population of rhode island ?
  what is the population of sacramento ?

10 of 525 losses on denotation oracle accuracy:

  give me the longest river that passes through the us ?
  how many cities are there in the us ?
  what are the major cities in new mexico ?
  what are the states through which the longest river runs ?
  what is the area of the state with the smallest population density ?
  what is the biggest city in usa ?
  what is the highest elevation in new mexico ?
  what is the largest state capital in population ?
  what river traverses the most states ?
  what states border hawaii ?


   hot diggity, it's working. denotation oracle accuracy is over 12%,
   double digits. we have 75 wins, and they're what we expect: queries
   that simply combine a relation and an entity (or collection).

intersections[41]  

   in [22]:
rules_intersection = [
    rule('$collection', '$collection $collection',
         lambda sems: ('.and', sems[0], sems[1])),
    rule('$collection', '$collection $optional $collection',
         lambda sems: ('.and', sems[0], sems[2])),
    rule('$collection', '$collection $optional $optional $collection',
         lambda sems: ('.and', sems[0], sems[3])),
]

   in [23]:
rules = rules_optionals + rules_collection_entity + rules_types + rules_relation
s + rules_intersection
grammar = grammar(rules=rules, annotators=annotators)
parses = grammar.parse_input('states bordering california')
for parse in parses[:1]:
    print('\n'.join([str(parse.semantics), str(executor.execute(parse.semantics)
)]))

created grammar with 149 rules
('.and', 'state', ('borders', '/state/california'))
('/state/arizona', '/state/nevada', '/state/oregon')

   let's evaluate the impact on the geo880 training examples.
   in [24]:
model = model(grammar=grammar, executor=executor.execute)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
over 600 examples:

denotation accuracy                0.198
denotation oracle accuracy         0.277
number of parses                   1.962
spurious ambiguity                 0.000

5 of 166 wins on denotation oracle accuracy:

  give me the cities in virginia ?
  how high is guadalupe peak ?
  what are the neighboring states for michigan ?
  what is the lowest point in louisiana ?
  what rivers run through west virginia ?

10 of 434 losses on denotation oracle accuracy:

  what is the largest state traversed by the mississippi river ?
  what is the longest river ?
  what is the longest river in pennsylvania ?
  what is the most populous state in the us ?
  what is the name of the state with the lowest point ?
  what is the population density of the state with the smallest area ?
  what is the shortest river in nebraska ?
  what rivers flow through states that alabama borders ?
  where is san jose ?
  which state is the largest city in montana in ?


   great, denotation oracle accuracy has more than doubled, from 12% to
   28%. and the wins now include intersections like "which states border
   new york". the losses, however, are clearly dominated by one category
   of error.

superlatives[42]  

   many of the losses involve superlatives, such as "biggest" or
   "shortest". let's remedy that. as usual, we let the training examples
   guide us in adding lexical rules.
   in [25]:
rules_superlatives = [
    rule('$collection', '$superlative ?$optionals $collection', lambda sems: sem
s[0] + (sems[2],)),
    rule('$collection', '$collection ?$optionals $superlative', lambda sems: sem
s[2] + (sems[0],)),

    rule('$superlative', 'largest', ('.argmax', 'area')),
    rule('$superlative', 'largest', ('.argmax', 'population')),
    rule('$superlative', 'biggest', ('.argmax', 'area')),
    rule('$superlative', 'biggest', ('.argmax', 'population')),
    rule('$superlative', 'smallest', ('.argmin', 'area')),
    rule('$superlative', 'smallest', ('.argmin', 'population')),
    rule('$superlative', 'longest', ('.argmax', 'length')),
    rule('$superlative', 'shortest', ('.argmin', 'length')),
    rule('$superlative', 'tallest', ('.argmax', 'height')),
    rule('$superlative', 'highest', ('.argmax', 'height')),

    rule('$superlative', '$mostleast $revrelation', lambda sems: (sems[0], sems[
1])),
    rule('$mostleast', 'most', '.argmax'),
    rule('$mostleast', 'least', '.argmin'),
    rule('$mostleast', 'lowest', '.argmin'),
    rule('$mostleast', 'greatest', '.argmax'),
    rule('$mostleast', 'highest', '.argmax'),
]

   now we should be able to parse "tallest mountain":
   in [26]:
rules = rules_optionals + rules_collection_entity + rules_types + rules_relation
s + rules_intersection + rules_superlatives
grammar = grammar(rules=rules, annotators=annotators)
parses = grammar.parse_input('tallest mountain')
for parse in parses[:1]:
    print('\n'.join([str(parse.semantics), str(executor.execute(parse.semantics)
)]))

created grammar with 167 rules
('.argmax', 'height', 'mountain')
('/mountain/mckinley',)

   let's evaluate the impact on the geo880 training examples.
   in [27]:
model = model(grammar=grammar, executor=executor.execute)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
over 600 examples:

denotation accuracy                0.245
denotation oracle accuracy         0.422
number of parses                   4.430
spurious ambiguity                 0.000

5 of 253 wins on denotation oracle accuracy:

  what can you tell me about the population of missouri ?
  what is the population of texas ?
  what state borders michigan ?
  what state has the city with the most population ?
  where is the lowest spot in iowa ?

10 of 347 losses on denotation oracle accuracy:

  how many states are next to major rivers ?
  how many states have cities named austin ?
  how many states have cities or towns named springfield ?
  number of citizens in boulder ?
  of the states washed by the mississippi river which has the lowest point ?
  what is the highest point in the united states ?
  what is the longest river in the state with the highest point ?
  what is the population of springfield missouri ?
  what state has the largest urban population ?
  what state has the smallest population density ?


   wow, superlatives make a big difference. denotation oracle accuracy has
   surged from 28% to 42%.

reverse joins[43]  

   in [28]:
def reverse(relation_sem):
    """todo"""
    # relation_sem is a lambda function which takes an arg and forms a pair,
    # either (rel, arg) or (arg, rel).  we want to swap the order of the pair.
    def apply_and_swap(arg):
        pair = relation_sem(arg)
        return (pair[1], pair[0])
    return apply_and_swap

rules_reverse_joins = [
    rule('$collection', '$collection ?$optionals $relation',
         lambda sems: reverse(sems[2])(sems[0])),
]

   in [29]:
rules = rules_optionals + rules_collection_entity + rules_types + rules_relation
s + rules_intersection + rules_superlatives + rules_reverse_joins
grammar = grammar(rules=rules, annotators=annotators)
parses = grammar.parse_input('which states does the rio grande cross')
for parse in parses[:1]:
    print('\n'.join([str(parse.semantics), str(executor.execute(parse.semantics)
)]))

created grammar with 168 rules
('.and', 'state', ('/river/rio_grande', 'traverses'))
('/state/colorado', '/state/new_mexico', '/state/texas')

   let's evaluate the impact on the geo880 training examples.
   in [30]:
model = model(grammar=grammar, executor=executor.execute)
sample_wins_and_losses(domain=domain, model=model, metric=metric, seed=1)

================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
max cell capacity 1000 has been hit 1 times
max cell capacity 1000 has been hit 2 times
max cell capacity 1000 has been hit 4 times
max cell capacity 1000 has been hit 8 times
max cell capacity 1000 has been hit 16 times
max cell capacity 1000 has been hit 32 times
over 600 examples:

denotation accuracy                0.257
denotation oracle accuracy         0.468
number of parses                   13.210
spurious ambiguity                 0.001

max cell capacity 1000 has been hit 64 times
5 of 281 wins on denotation oracle accuracy:

  how long is the mississippi ?
  what is the biggest city in oregon ?
  what is the biggest city in wyoming ?
  what is the capital of washington ?
  what is the smallest city in the largest state ?

10 of 319 losses on denotation oracle accuracy:

  how many people live in the state with the largest population density ?
  how many rivers are found in colorado ?
  how many rivers are there in idaho ?
  name the major lakes in michigan ?
  what are the major cities in the smallest state in the us ?
  what is the tallest mountain in america ?
  what river is the longest one in the united states ?
  what rivers run through the states that border the state with the capital atla
nta ?
  where is indianapolis ?
  which states border the longest river in the usa ?


   this time the gain in denotation oracle accuracy was more modest, from
   42% to 47%. still, we are making good progress. however, note that a
   substantial gap has opened between accuracy and oracle accuracy. this
   indicates that we could benefit from adding a scoring model.

feature engineering[44]  

   through an iterative process of grammar engineering, we've managed to
   increase denotation oracle accuracy of 47%. but we've been ignoring
   denotation accuracy, which now lags far behind, at 25%. this represents
   an opportunity.

   in order to figure out how best to fix the problem, we need to do some
   error analysis. let's look for some specific examples where denotation
   accuracy is 0, even though denotation oracle accuracy is 1. in other
   words, let's look for some examples where we have a correct parse, but
   it's not ranked at the top. we should be able to find some cases like
   that among the first ten examples of the geo880 training data.
   in [31]:
from experiment import evaluate_model
from metrics import denotation_match_metrics

evaluate_model(model=model,
               examples=geo880_train_examples[:10],
               metrics=denotation_match_metrics(),
               print_examples=true)

================================================================================
evaluating on 10 examples

--------------------------------------------------------------------------------
input                              what is the highest point in florida ?
target denotation                  ('/place/walton_county',)

denotation accuracy                1
denotation oracle accuracy         1
number of parses                   3
spurious ambiguity                 0

0      0.000   semantics           ('/state/florida', 'highest_point')
               denotation      +   ('/place/walton_county',)
1      0.000   semantics           (('traverses', '/state/florida'), 'highest_po
int')
               denotation      -   ()
2      0.000   semantics           (('/state/florida', 'contains'), 'highest_poi
nt')
               denotation      -   ()

--------------------------------------------------------------------------------
input                              what are the high points of states surroundin
g mississippi ?
target denotation                  ('/place/cheaha_mountain', '/place/clingmans_
dome', '/place/driskill_mountain', '/place/magazine_mountain')

denotation accuracy                1
denotation oracle accuracy         1
number of parses                   28
spurious ambiguity                 0

0      0.000   semantics           (('.and', 'state', ('borders', '/state/missis
sippi')), 'highest_point')
               denotation      +   ('/place/cheaha_mountain', '/place/clingmans_
dome', '/place/driskill_mountain', '/place/magazine_mountain')
1      0.000   semantics           (('.and', 'state', ('borders', '/river/missis
sippi')), 'highest_point')
               denotation      -   ()
2      0.000   semantics           (('.and', ('state', 'borders'), '/state/missi
ssippi'), 'highest_point')
               denotation      -   ('/place/woodall_mountain',)
3      0.000   semantics           (('.and', ('state', 'borders'), '/river/missi
ssippi'), 'highest_point')
               denotation      -   ()
4      0.000   semantics           ((('.and', 'state', ('borders', '/state/missi
ssippi')), 'contains'), 'highest_point')
               denotation      -   ()
5      0.000   semantics           ((('.and', 'state', ('borders', '/river/missi
ssippi')), 'contains'), 'highest_point')
               denotation      -   ()
6      0.000   semantics           ((('.and', ('state', 'borders'), '/state/miss
issippi'), 'contains'), 'highest_point')
               denotation      -   ()
7      0.000   semantics           ((('.and', ('state', 'borders'), '/river/miss
issippi'), 'contains'), 'highest_point')
               denotation      -   ()
8      0.000   semantics           (('.and', ('state', 'contains'), ('borders',
'/state/mississippi')), 'highest_point')
               denotation      -   ()
9      0.000   semantics           (('.and', ('state', 'contains'), ('borders',
'/river/mississippi')), 'highest_point')
               denotation      -   ()
10     0.000   semantics           (('.and', (('state', 'borders'), 'contains'),
 '/state/mississippi'), 'highest_point')
               denotation      -   ()
11     0.000   semantics           (('.and', (('state', 'borders'), 'contains'),
 '/river/mississippi'), 'highest_point')
               denotation      -   ()
12     0.000   semantics           (('.and', (('state', 'contains'), 'borders'),
 '/state/mississippi'), 'highest_point')
               denotation      -   ()
13     0.000   semantics           (('.and', (('state', 'contains'), 'borders'),
 '/river/mississippi'), 'highest_point')
               denotation      -   ()
14     0.000   semantics           ('.and', ('state', 'highest_point'), ('border
s', '/state/mississippi'))
               denotation      -   ()
15     0.000   semantics           ('.and', ('state', 'highest_point'), ('border
s', '/river/mississippi'))
               denotation      -   ()
16     0.000   semantics           ('.and', (('state', 'contains'), 'highest_poi
nt'), ('borders', '/state/mississippi'))
               denotation      -   ()
17     0.000   semantics           ('.and', (('state', 'contains'), 'highest_poi
nt'), ('borders', '/river/mississippi'))
               denotation      -   ()
18     0.000   semantics           ('.and', (('state', 'borders'), 'highest_poin
t'), '/state/mississippi')
               denotation      -   ()
19     0.000   semantics           ('.and', (('state', 'borders'), 'highest_poin
t'), '/river/mississippi')
               denotation      -   ()
20     0.000   semantics           ('.and', ((('state', 'borders'), 'contains'),
 'highest_point'), '/state/mississippi')
               denotation      -   ()
21     0.000   semantics           ('.and', ((('state', 'borders'), 'contains'),
 'highest_point'), '/river/mississippi')
               denotation      -   ()
22     0.000   semantics           ('.and', ((('state', 'contains'), 'borders'),
 'highest_point'), '/state/mississippi')
               denotation      -   ()
23     0.000   semantics           ('.and', ((('state', 'contains'), 'borders'),
 'highest_point'), '/river/mississippi')
               denotation      -   ()
24     0.000   semantics           ('.and', (('state', 'highest_point'), 'border
s'), '/state/mississippi')
               denotation      -   ()
25     0.000   semantics           ('.and', (('state', 'highest_point'), 'border
s'), '/river/mississippi')
               denotation      -   ()
26     0.000   semantics           ('.and', ((('state', 'contains'), 'highest_po
int'), 'borders'), '/state/mississippi')
               denotation      -   ()
27     0.000   semantics           ('.and', ((('state', 'contains'), 'highest_po
int'), 'borders'), '/river/mississippi')
               denotation      -   ()

--------------------------------------------------------------------------------
input                              what state has the shortest river ?
target denotation                  ('/state/delaware', '/state/new_jersey', '/st
ate/new_york', '/state/pennsylvania')

denotation accuracy                0
denotation oracle accuracy         1
number of parses                   8
spurious ambiguity                 0

0      0.000   semantics           ('.and', 'state', ('.argmin', 'length', 'rive
r'))
               denotation      -   ()
1      0.000   semantics           ('.and', 'state', (('.argmin', 'length', 'riv
er'), 'traverses'))
               denotation      +   ('/state/delaware', '/state/new_jersey', '/st
ate/new_york', '/state/pennsylvania')
2      0.000   semantics           ('.and', 'state', ('contains', ('.argmin', 'l
ength', 'river')))
               denotation      -   ()
3      0.000   semantics           ('.and', ('traverses', 'state'), ('.argmin',
'length', 'river'))
               denotation      -   ('/river/delaware',)
4      0.000   semantics           ('.and', ('state', 'contains'), ('.argmin', '
length', 'river'))
               denotation      -   ()
5      0.000   semantics           ('.and', ('.argmin', 'length', 'state'), 'riv
er')
               denotation      -   ()
6      0.000   semantics           ('.and', ('.argmin', 'length', ('traverses',
'state')), 'river')
               denotation      -   ('/river/delaware',)
7      0.000   semantics           ('.and', ('.argmin', 'length', ('state', 'con
tains')), 'river')
               denotation      -   ()

--------------------------------------------------------------------------------
input                              what is the tallest mountain in the united st
ates ?
target denotation                  ('/mountain/mckinley',)

denotation accuracy                0
denotation oracle accuracy         0
number of parses                   0
spurious ambiguity                 0


--------------------------------------------------------------------------------
input                              what is the capital of maine ?
target denotation                  ('/city/augusta_me',)

denotation accuracy                1
denotation oracle accuracy         1
number of parses                   2
spurious ambiguity                 0

0      0.000   semantics           ('/state/maine', 'capital')
               denotation      +   ('/city/augusta_me',)
1      0.000   semantics           (('/state/maine', 'contains'), 'capital')
               denotation      -   ()

--------------------------------------------------------------------------------
input                              what are the populations of states through wh
ich the mississippi river run ?
target denotation                  (11400000, 2286000, 2364000, 2520000, 2913000
, 4076000, 4206000, 4591000, 4700000, 4916000)

denotation accuracy                0
denotation oracle accuracy         0
number of parses                   0
spurious ambiguity                 0


--------------------------------------------------------------------------------
input                              name all the lakes of us ?
target denotation                  ('/lake/becharof', '/lake/champlain', '/lake/
erie', '/lake/flathead', '/lake/great_salt_lake', '/lake/huron', '/lake/iliamna'
, '/lake/lake_of_the_woods', '/lake/michigan', '/lake/mille_lacs', '/lake/naknek
', '/lake/okeechobee', '/lake/ontario', '/lake/pontchartrain', '/lake/rainy', '/
lake/red', '/lake/salton_sea', '/lake/st._clair', '/lake/superior', '/lake/tahoe
', '/lake/teshekpuk', '/lake/winnebago')

denotation accuracy                0
denotation oracle accuracy         0
number of parses                   0
spurious ambiguity                 0


--------------------------------------------------------------------------------
input                              which states border states through which the
mississippi traverses ?
target denotation                  ('/state/alabama', '/state/arkansas', '/state
/georgia', '/state/illinois', '/state/indiana', '/state/iowa', '/state/kansas',
'/state/kentucky', '/state/louisiana', '/state/michigan', '/state/minnesota', '/
state/mississippi', '/state/missouri', '/state/nebraska', '/state/north_carolina
', '/state/north_dakota', '/state/ohio', '/state/oklahoma', '/state/south_dakota
', '/state/tennessee', '/state/texas', '/state/virginia', '/state/west_virginia'
, '/state/wisconsin')

denotation accuracy                0
denotation oracle accuracy         0
number of parses                   0
spurious ambiguity                 0


--------------------------------------------------------------------------------
input                              what is the highest mountain in alaska ?
target denotation                  ('/mountain/mckinley',)

denotation accuracy                0
denotation oracle accuracy         1
number of parses                   12
spurious ambiguity                 0

0      0.000   semantics           ('.argmax', 'height', ('.and', 'mountain', '/
state/alaska'))
               denotation      -   ()
1      0.000   semantics           ('.argmax', 'height', ('.and', 'mountain', ('
traverses', '/state/alaska')))
               denotation      -   ()
2      0.000   semantics           ('.argmax', 'height', ('.and', 'mountain', ('
/state/alaska', 'contains')))
               denotation      +   ('/mountain/mckinley',)
3      0.000   semantics           ('.argmax', 'height', ('.and', ('mountain', '
traverses'), '/state/alaska'))
               denotation      -   ()
4      0.000   semantics           ('.argmax', 'height', ('.and', ('contains', '
mountain'), '/state/alaska'))
               denotation      -   ()
5      0.000   semantics           ('.and', ('.argmax', 'height', 'mountain'), '
/state/alaska')
               denotation      -   ()
6      0.000   semantics           ('.and', ('.argmax', 'height', 'mountain'), (
'traverses', '/state/alaska'))
               denotation      -   ()
7      0.000   semantics           ('.and', ('.argmax', 'height', 'mountain'), (
'/state/alaska', 'contains'))
               denotation      +   ('/mountain/mckinley',)
8      0.000   semantics           ('.and', ('.argmax', 'height', ('mountain', '
traverses')), '/state/alaska')
               denotation      -   ()
9      0.000   semantics           ('.and', ('.argmax', 'height', ('contains', '
mountain')), '/state/alaska')
               denotation      -   ()
10     0.000   semantics           ('.and', (('.argmax', 'height', 'mountain'),
'traverses'), '/state/alaska')
               denotation      -   ()
11     0.000   semantics           ('.and', ('contains', ('.argmax', 'height', '
mountain')), '/state/alaska')
               denotation      -   ('/state/alaska',)

--------------------------------------------------------------------------------
input                              what is the population of illinois ?
target denotation                  (11400000,)

denotation accuracy                1
denotation oracle accuracy         1
number of parses                   2
spurious ambiguity                 0

0      0.000   semantics           ('/state/illinois', 'population')
               denotation      +   (11400000,)
1      0.000   semantics           (('/state/illinois', 'contains'), 'population
')
               denotation      -   (100054, 124160, 139712, 3005172, 58267, 6027
8, 60590, 61232, 63668, 66116, 67653, 73706, 77956, 81293, 93939)

--------------------------------------------------------------------------------
over 10 examples:

denotation accuracy                0.400
denotation oracle accuracy         0.600
number of parses                   5.500
spurious ambiguity                 0.000


   take a look through that output. over the ten examples, we achieved
   denotation oracle accuracy of 60%, but denotation accuracy of just 40%.
   in other words, there were two examples where we generated a correct
   parse, but failed to rank it at the top. take a closer look at those
   two cases.

   the first case is "what state has the shortest river ?". the top parse
   has semantics ('.and', 'state', ('.argmin', 'length', 'river')), which
   means something like "states that are the shortest river". that's not
   right. in fact, there's no such thing: the denotation is empty.

   the second case is "what is the highest mountain in alaska ?". the top
   parse has semantics ('.argmax', 'height', ('.and', 'mountain',
   '/state/alaska')), which means "the highest mountain which is alaska".
   again, there's no such thing: the denotation is empty.

   so in both of the cases where we put the wrong parse at the top, the
   top parse had nonsensical semantics with an empty denotation. in fact,
   if you scroll through the output above, you will see that there are a
   lot of candidate parses with empty denotations. seems like we could
   make a big improvement just by downweighting parses with empty
   denotations. this is easy to do.
   in [32]:
def empty_denotation_feature(parse):
    features = defaultdict(float)
    if parse.denotation == ():
        features['empty_denotation'] += 1.0
    return features

weights = {'empty_denotation': -1.0}

model = model(grammar=grammar,
              feature_fn=empty_denotation_feature,
              weights=weights,
              executor=executor.execute)

   let's evaluate the impact of using our new empty_denotation feature on
   the geo880 training examples.
   in [33]:
from experiment import evaluate_model
from metrics import denotation_match_metrics

evaluate_model(model=model,
               examples=geo880_train_examples,
               metrics=denotation_match_metrics(),
               print_examples=false)

================================================================================
evaluating on 600 examples

--------------------------------------------------------------------------------
max cell capacity 1000 has been hit 128 times
over 600 examples:

denotation accuracy                0.387
denotation oracle accuracy         0.468
number of parses                   13.210
spurious ambiguity                 0.001


   great! using the empty_denotation feature has enabled us to increase
   denotation accuracy from 25% to 39%. that's a big gain! in fact, we've
   closed most of the gap between accuracy and oracle accuracy. as a
   result, the headroom for further gains from feature engineering is
   limited     but it's not zero. the [45]exercises will ask you to push
   further.

exercises [46]  

   several of these exercises ask you to measure the impact of your change
   on key id74. part of your job is to decide which
   id74 are most relevant for the change you're making. it's
   probably best to evaluate only on training data, in order to keep the
   test data unseen during development. (but the test data is hardly a
   state secret, so whatever.)

straightforward[47]  

    1. extend the grammar to handle queries which use the phrase "how many
       people" to ask about population. there are many examples in the
       geo880 training data. measure the impact of this change on key
       id74.
    2. extend the grammar to handle counting questions, indicated by the
       phrases "how many" or "number of", as in "how many states does
       missouri border". measure the impact of this change on key
       id74.
    3. several examples in the geo880 training dataset fail to parse
       because they refer to locations using names that, while valid, are
       not recognized by the geobaseannotator. for example, some queries
       use "america" to refer to /country/usa, but geobaseannotator
       recognizes only "usa". find unannotated location references in the
       geo880 training dataset, and extend the grammar to handle them.
       measure the impact of this change on key id74.
    4. extend the grammar to handle examples of the form "where is x",
       such as "where is mount whitney" or "where is san diego". measure
       the impact of this change on key id74.
    5. quite a few examples in the geo880 training dataset specify a set
       of entities by name, as in "how many states have a city named
       springfield" or "how many rivers are called colorado". extend the
       grammar to handle these, leveraging the tokenannotator introduced
       in [48]unit 2. measure the impact of this change on key evaluation
       metrics.
    6. extend the grammar to handle phrases like "austin texas" or
       "atlanta ga", where two entity names appear in sequence. make sure
       that "atlanta ga" has semantics and a denotation, whereas "atlanta
       tx" has semantics but no denotation. measure the impact of this
       change on key id74.
    7. in [49]unit 2, while examining the travel domain, we saw big gains
       from including rule features in our feature representation.
       experiment with adding rule features to the geoquery model. are the
       learned weights intuitive? do the rule features help? if so,
       identify a few specific examples which are fixed by the inclusion
       of rule features. measure the impact on key id74.
    8. find an example where using the empty_denotation feature causes a
       loss (a bad prediction).
    9. the empty_denotation feature doesn't help on count questions (e.g.,
       "how many states ..."), where all parses, good or bad, typically
       yield a denotation which is a single number. add a new feature
       which helps in such cases. identify a few specific examples which
       are fixed by the new feature. measure the impact on key evaluation
       metrics. [this exercise assumes you have already done the exercise
       on handling count questions.]

challenging[50]  

    1. extend the grammar to handle comparisons, such as "mountains with
       height greater than 5000" or "mountains with height greater than
       the height of bona".
    2. building on the previous exercise, extend the grammar to handle
       even those comparisons which involve ellipsis, such as "mountains
       higher than [the height of] mt. katahdin" or "rivers longer than
       [the length of] the colorado river" (where the bracketed phrase
       does not appear in the surface form!).
    3. extend the grammar to handle queries involving units, such as "what
       is the area of maryland in square kilometers" or "how long is the
       mississippi river in miles".
    4. the geo880 training dataset contains many examples involving
       population density. extend the grammar to handle these examples.
       measure the impact on key id74.
    5. several examples in the geo880 training dataset involve some form
       of negation, expressed by the words "not", "no", or "excluding".
       extend the grammar to handle these examples. measure the impact on
       key id74.
    6. the current grammar handles "capital of texas", but not "texas
       capital". it handles "has austin capital", but not "has capital
       austin". in general, it defines every phrase which expresses a
       relation as either a $fwdrelation or a $revrelation, and constrains
       word order accordingly. extend the grammar to allow any phrase
       which is ordinarily a $fwdrelation to function as a $revrelation,
       and vice versa. can you now handle "texas capital" and "has capital
       austin"? measure the impact on key id74.
    7. what if we permit any word to be optionalized by adding the rule
       rule('$optional', '$token') to our grammar? (recall that $token is
       produced by the tokenannotator.) measure the impact of this change
       on key id74. you will likely find that the change has
       some negative effects and some positive effects. is there a way to
       mitigate the negative effects, while preserving the positive
       effects?
    8. the success of the empty_denotation feature demonstrates the
       potential of denotation features. can we go further? experiment
       with features that characterize the size of the denotation (that
       is, the number of answers). are two answers better than one? are
       ten answers better than two? if you find some features that seem to
       work, identify a few specific examples which are fixed by your new
       features. measure the impact on key id74.

   copyright (c) 2015 bill maccartney

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [51]fastly, rendered by [52]rackspace

   nbviewer github [53]repository.

   nbviewer version: [54]33c4683

   nbconvert version: [55]5.4.0

   rendered (fri, 05 apr 2019 18:00:57 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb
   5. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb
   6. https://github.com/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb
   7. https://mybinder.org/v2/gh/wcmac/sippycup/master?filepath=sippycup-unit-3.ipynb
   8. https://raw.githubusercontent.com/wcmac/sippycup/master/sippycup-unit-3.ipynb
   9. https://nbviewer.jupyter.org/github/wcmac/sippycup/tree/master
  10. https://nbviewer.jupyter.org/github/wcmac/sippycup/tree/master/sippycup-unit-3.ipynb
  11. http://nlp.stanford.edu/~wcmac/
  12. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-0.ipynb
  13. http://www.cs.utexas.edu/users/ml/geo.html
  14. http://www.aaai.org/papers/aaai/1996/aaai96-156.pdf
  15. http://www.cs.utexas.edu/~ai-lab/pubs/cocktail-ecml-01.pdf
  16. http://people.csail.mit.edu/lsz/papers/zc-uai05.pdf
  17. http://www.cs.berkeley.edu/~jordan/papers/liang-jordan-klein-acl2011.pdf
  18. http://arxiv.org/pdf/cmp-lg/9503016.pdf
  19. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#the-geo880-dataset
  20. ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geoqueries880
  21. http://www.cs.utexas.edu/~ml/wasp/geo-funql/corpus.xml
  22. http://www.cs.utexas.edu/~ml/wasp/geo-funql.html
  23. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#geoquery-semantic-representation
  24. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/geo880.py
  25. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#the-geobase-knowledge-base
  26. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/geobase.py
  27. http://en.wikipedia.org/wiki/international_system_of_units
  28. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#semantic-representation-
  29. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#the-graphkb-class
  30. http://en.wikipedia.org/wiki/the_simpsons
  31. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#the-graphkbexecutor-class
  32. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/graph_kb.py
  33. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#using-graphkbexecutor-with-geobase
  34. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#grammar-engineering
  35. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#optionals
  36. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb
  37. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#entities-and-collections
  38. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/experiment.py
  39. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#types
  40. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#relations-and-joins
  41. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#intersections
  42. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#superlatives
  43. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#reverse-joins
  44. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#feature-engineering
  45. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#geoquery-exercises
  46. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#exercises-
  47. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#straightforward
  48. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb
  49. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-2.ipynb
  50. https://nbviewer.jupyter.org/github/wcmac/sippycup/blob/master/sippycup-unit-3.ipynb#challenging
  51. http://www.fastly.com/
  52. https://developer.rackspace.com/?nbviewer=awesome
  53. https://github.com/jupyter/nbviewer
  54. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  55. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
