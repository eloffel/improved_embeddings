transforming dependency structures to logical forms for id29

siva reddy   a oscar t  ackstr  om    michael collins   b tom kwiatkowski   

dipanjan das    mark steedman    mirella lapata   
   ilcc, school of informatics, university of edinburgh

    google, new york

{oscart, mjcollins, tomkwiat, dipanjand}@google.com

siva.reddy@ed.ac.uk

{steedman, mlap}@inf.ed.ac.uk

abstract

the strongly typed syntax of grammar for-
malisms such as id35, tag, lfg and hpsg
offers a synchronous framework for deriving
syntactic structures and semantic logical forms.
in contrast   partly due to the lack of a strong
type system   dependency structures are easy
to annotate and have become a widely used
form of syntactic analysis for many languages.
however, the lack of a type system makes a
formal mechanism for deriving logical forms
from dependency structures challenging. we
address this by introducing a robust system
based on the id198 for deriving neo-
davidsonian logical forms from dependency
trees. these logical forms are then used for
id29 of natural language to free-
base. experiments on the free917 and web-
questions datasets show that our representation
is superior to the original dependency trees and
that it outperforms a id35-based representa-
tion on this task. compared to prior work, we
obtain the strongest result to date on free917
and competitive results on webquestions.

introduction

1
semantic parsers map sentences onto logical forms
that can be used to query databases (zettlemoyer and
collins, 2005; wong and mooney, 2006), instruct
robots (chen and mooney, 2011), extract information
(krishnamurthy and mitchell, 2012), or describe vi-
sual scenes (matuszek et al., 2012). current systems
accomplish this by learning task-speci   c grammars
(berant et al., 2013), by using strongly-typed id35
grammars (reddy et al., 2014), or by eschewing the
use of a grammar entirely (yih et al., 2015).
awork carried out during an internship at google.
b on leave from columbia university.

(a) the dependency tree for disney acquired pixar.

(nsubj (dobj acquired pixar) disney)
(b) the s-expression for the dependency tree.

  x.   yz. acquired(xe)     disney(ya)     pixar(za)

    arg1(xe, ya)     arg2(xe, za)

(c) the composed lambda-calculus expression.

figure 1: the dependency tree is binarized into its
s-expression, which is then composed into the lambda
expression representing the sentence logical form.

in recent years, there have been signi   cant ad-
vances in developing fast and accurate dependency
parsers for many languages (mcdonald et al., 2005;
nivre et al., 2007; martins et al., 2013, inter alia).
motivated by the desire to carry these advances over
to id29 tasks, we present a robust method
for mapping dependency trees to logical forms that
represent underlying predicate-argument structures.1
we empirically validate the utility of these logical
forms for id53 from databases. since
our approach uses dependency trees as input, we hy-
pothesize that it will generalize better to domains that
are well covered by dependency parsers than methods
that induce semantic grammars from scratch.

the system that maps a dependency tree to its log-
ical form (henceforth deplambda) is illustrated
in figure 1. first, the dependency tree is binarized
via an obliqueness hierarchy to give an s-expression
that describes the application of functions to pairs

1by    robust   , we refer to the ability to gracefully handle
parse errors as well as the untyped nature of dependency syntax.

disneyacquiredpixaid56pvbdnnpnsubjdobjrootof arguments. each node in this s-expression is then
substituted for a lambda-calculus expression and the
relabeled s-expression is beta-reduced to give the log-
ical form in figure 1(c). since dependency syntax
does not have an associated type theory, we introduce
a type system that assigns a single type to all con-
stituents, thus avoiding the need for type checking
(section 2). deplambda uses this system to gener-
ate robust logical forms, even when the dependency
structure does not mirror predicate-argument relation-
ships in constructions such as conjunctions, prepo-
sitional phrases, relative clauses, and wh-questions
(section 3).

these ungrounded logical forms (kwiatkowski
et al., 2013; reddy et al., 2014; krishnamurthy
and mitchell, 2015) are used for question answer-
ing against freebase, by passing them as input to
graphparser (reddy et al., 2014), a system that
learns to map logical predicates to freebase, result-
ing in grounded freebase queries (section 4). we
show that our approach achieves state-of-the-art per-
formance on the free917 dataset and competitive
performance on the webquestions dataset, whereas
building the freebase queries directly from depen-
dency trees gives signi   cantly lower performance.
finally, we show that our approach outperforms a di-
rectly comparable method that generates ungrounded
logical forms using id35. details of our experimen-
tal setup and results are presented in section 5 and
section 6, respectively.
2 logical forms
we use a version of the id198 with three
base types: individuals (ind), events (event), and
truth values (bool). roughly speaking individuals
are introduced by nouns, events are introduced by
verbs, and whole sentences are functions onto truth
values. for types a and b, we use a    b to denote
the product type, while a     b denotes the type
of functions mapping elements of a to elements of
b. we will make extensive use of variables of type
ind    event. for any variable x of type ind   
event, we use x = (xa, xe) to denote the pair of
variables xa (of type ind) and xe (of type event).
here, the subscript denotes the projections   a : ind  
event     ind and   e : ind    event     event.
an important constraint on the id198
system is as follows: all natural language con-

stituents have a lambda-calculus expression of type
ind    event     bool.

in

are

de   ned

formally

a    constituent    in this de   nition is either a sin-
s-expressions
gle word, or an s-expression.
next
section;
are
the
and
examples
essen-
(nsubj (dobj acquired pixar) disney).
s-expressions are binarized dependency
tially,
trees, which include an ordering over the different
dependencies to a head (in the above the dobj
modi   er is combined before the nsubj modi   er).

(dobj acquired pixar)

some examples of lambda-calculus expressions

for single words (lexical entries) are as follows:
acquired       x. acquired(xe)
      y. disney(ya)
disney
      z. pixar(za)
pixar

an example for a full sentence is as follows:

disney acquired pixar    
  x.   yz. acquired(xe)     disney(ya)

    pixar(za)     arg1(xe, ya)     arg2(xe, za)
this is a neo-davidsonian style of analysis. verbs
such as acquired make use of event variables such
as xe, whereas nouns such as disney make use of
individual variables such as ya.
the restriction that all expressions are of type
ind    event     bool simpli   es the type system
considerably. while it leads to dif   culty with some
linguistic constructions   see section 3.3 for some
examples   we believe the simplicity and robustness
of the resulting system outweighs these concerns. it
also leads to some spurious variables that are bound
by lambdas or existentials, but which do not appear as
arguments of any predicate: for example in the above
analysis for disney acquired pixar, the variables xa,
ye and ze are unused. however these    spurious    vari-
ables are easily identi   ed and discarded.
an important motivation for having variables of
type ind   event is that a single lexical item some-
times makes use of both types of variables. for exam-
ple, the noun phrase president in 2009 has semantics
  x.   y. president(xa)     president event(xe)   

arg1(xe, xa)     2009(ya)     prep.in(xe, ya)
in this example president introduces the predi-
cates president, corresponding to an individual, and
president event, corresponding to an event; essen-
tially a presidency event that may have various prop-
erties. this follows the structure of freebase closely:

freebase contains an individual corresponding to
barack obama, with a president property, as well
as an event corresponding to the obama presidency,
with various properties such as a start and end date, a
location, and so on. the entry for president is then
  x. president(xa)     president event(xe)     arg1(xe, xa)
note that proper nouns do not introduce an event
predicate, as can be seen from the entries for disney
and pixar above.

3 dependency structures to logical forms

we now describe the system used to map dependency
structures to logical forms. we    rst give an overview
of the approach, then go into detail about various
linguistic constructions.

3.1 an overview of the approach
the transformation of a dependency tree to its logical
form is accomplished through a series of three steps:
binarization, substitution, and composition. below,
we outline these steps, with some additional remarks.

binarization. a dependency tree is mapped to an
s-expression (borrowing terminology from lisp). for
example, disney acquired pixar has the s-expression

(nsubj (dobj acquired pixar) disney)

an

has

s-expression

formally,
form
(exp1 exp2 exp3), where exp1 is a dependency
label, and both exp2 and exp3 are either (1) a word
such as acquired; or (2) an s-expression such as
(dobj acquired pixar).

the

we refer to the process of mapping a dependency
tree to an s-expression as binarization, as it involves
an ordering of modi   ers to a particular head, similar
to binarization of a context-free parse tree.

substitution. each symbol (word or label) in the
s-expression is assigned a lambda expression. in our
running example we have the following assignments:
acquired       x. acquired(xe)
disney       y. disney(ya)
      z. pixar(za)
pixar
      f gz.   x. f (z)     g(x)     arg1(ze, xa)
nsubj
      f gz.   x. f (z)     g(x)     arg2(ze, xa)
dobj

composition. beta-reduction is used to compose
the lambda-expression terms to compute the    nal
semantics for the input sentence. in this step expres-
sions of the form (exp1 exp2 exp3) are interpreted
as function exp1 being applied to arguments exp2
and exp3. for example, (dobj acquired pixar) re-
ceives the following expression after composition:
  z.   x. acquired(ze)     pixar(xa)     arg2(ze, xa)
obliqueness hierarchy. the binarization stage re-
quires a strict ordering on the different modi   ers
to each head in a dependency parse. for example,
in (nsubj (dobj acquired pixar) disney), the dobj
is attached before the nsubj. the ordering is very
similar to the obliqueness hierarchy in syntactic for-
malisms such as hpsg (pollard and sag, 1994).
type for dependency labels. recall from sec-
tion 2 that every s-expression subtree receive a log-
ical form of type    = ind    event     bool. it
follows that in any s-expression (exp1 exp2 exp3),
exp1 has type        (         ), exp2 and exp3 both
have type   , and the full expression has type   . since
each labeled dependency relation (e.g., nsubj, dobj,
partmod) is associated with exp1 in connecting two
s-expression subtrees, dependency labels always re-
ceive expressions of type        (         ).
mirroring dependency structure. whenever a
dependency label receives an expression of the form

  f gz.   x. f (z)     g(x)     rel(ze, xa)

(1)

where rel is a logical relation, the composition op-
eration builds a structure that essentially mirrors the
original dependency structure. for example nsubj
and dobj receive expressions of this form, with rel
= arg1 and rel = arg2, respectively; the    nal lambda
expression for disney acquired pixar is
  x.   yz. acquired(xe)     disney(ya)

    pixar(za)     arg1(xe, ya)     arg2(xe, za)
this structure is isomorphic to the original depen-
dency structure:
there are variables xe, ya and
za corresponding to acquired, disney and pixar,
respectively; and the sub-expressions arg1(xe, ya)
and arg2(xe, za) correspond to the dependencies ac-
quired     disney and acquired     pixar.

by default we assume that the predicate argument
structure is isomorphic to the dependency structure

and many dependency labels receive a semantics of
the form shown in (1). however, there are a num-
ber of important exceptions. as one example, the
dependency label partmod receives semantics
  f gz.   x. f (z)     g(x)     arg1(xe, za)

with arg1(xe, za) in place of the arg1(ze, xa) in (1).
this reverses the dependency direction to capture
the predicate-argument structure of reduced relative
constructions such as a company acquired by disney.

post-processing. we apply three post-processing
steps   simple id136s over
lambda-calculus
expressions   to the derived logical forms. these
relate to the handling of prepositions, coordination
and control and are described and motivated in more
detail under the respective headings below.

3.2 analysis of some linguistic constructions
in this section we describe in detail how various lin-
guistic constructions not covered by the rule in (1)   
prepositional phrases, conjunction, relative clauses,
and wh questions   are handled in the formalism.2

prepositional phrases. prepositional phrase mod-
i   ers to nouns and verbs have similar s-expressions:

(prep president (pobj in 2009))
(prep acquired (pobj in 2009))
the following entries are used in these examples:
in       x. in(xe)
prep       f gz.   x. f (z)     g(x)     prep(ze, xa)
pobj       f gz.   x. f (z)     g(x)     pobj(ze, xa)
president       x. president(xa)
acquired       x. acquired(xe)
where the entries for prep and pobj simply mirror the
original dependency structure with prep modifying
the event variable ze.

    president event(xe)     arg1(xe, xa)

the semantics for acquired in 2009 is as follows:

  x.   py. acquired(xe)     2009(ya)

    in(pe)     prep(xe, pe)     pobj(pe, ya)

2the system contains 32 binarization rules (e.g., rules for
obliqueness hierarchy and identifying traces) and 46 substi-
tution rules (i.e.
rules for dependency labels and parts of
speech). the rules can be found at http://github.com/
sivareddyg/deplambda.

we replace in(pe)     prep(xe, pe)     pobj(pe, ya) by
prep.in(xe, ya) as a post-processing step, effectively
collapsing out the p variable while replacing the
prep and pobj dependencies by a single dependency,
prep.in. the    nal semantics are then as follows:
  x.   y. acquired(xe)     2009(ya)     prep.in(xe, ya)
in practice this step is easily achieved by identifying
variables (in this case pe) participating in prep and
pobj relations. it would be tempting to achieve this
step within the id198 expressions them-
selves, but we have found the post-processing step to
be more robust to parsing errors and corner cases in
the usage of the prep and pobj dependency labels.
conjunctions. first consider a simple case of np-
conjunction, bill and dave founded hp, whose
s-expression is as follows:

(nsubj (dobj founded hp)

(conj-np (cc bill and) dave))
we make use of the following entries:
conj-np       f gx.   yz. f (y)     g(z)     coord(x, y, z)
cc       f gz. f (z)

the sentence bill and dave founded hp then re-

ceives the following semantics:
  e.   xyzu. bill(ya)     dave(za)     founded(ee)     hp(ua)

    coord(x, y, z)     arg1(ee, xa)     arg2(ee, ua)
note how the x variable occurs in two sub-
expressions: coord(x, y, z), and arg1(ee, xa).
it
can be interpreted as a variable that conjoins
variables y and z together.
in particular, we
introduce a post-processing step where the sub-
expression coord(x, y, z)     arg1(ee, xa) is replaced
with arg1(ee, ya)     arg1(ee, za), and the x variable
is removed. the resulting expression is as follows:
  e.   yzu. bill(ya)     dave(za)     founded(ee)     hp(ua)

    arg1(ee, ya)     arg1(ee, za)     arg2(ee, ua)
vp-coordination is treated in a very similar way.
consider the sentence eminem signed to interscope
and discovered 50 cent. this has the following
s-expression:

(nsubj (conj-vp (cc s-to-i and) d-50) eminem)
where s-to-i refers to the vp signed to interscope,
and d-50 refers to the vp discovered 50 cent. the
lambda-calculus expression for conj-vp is identical
to the expression for conj-np:

conj-vp       f gx.   yz. f (y)     g(z)     coord(x, y, z)

the logical form for the full sentence is then

  e.   xyz. eminem(xa)     coord(e, y, z)
    arg1(ee, xa)     s to i(y)     d 50(z)

where we use s to i(y) and d 50(z) as shorthand for
the lambda-calculus expressions for the two vps.

after post-processing this is simpli   ed to

  e.   xyz. eminem(xa)     arg1(ye, xa)
    arg1(ze, xa)     s to i(y)     d 50(z)
other types of coordination, such as sentence-
level coordination and pp coordination, are handled
with the same mechanism. all coordination depen-
dency labels have the same semantics as conj-np
and conj-vp. the only reason for having distinct de-
pendency labels for different types of coordination
is that different labels appear in different positions
in the obliqueness hierarchy. this is important for
getting the correct scope for different forms of con-
junction. for instance, the following s-expression
for the eminem example would lead to an incorrect
semantics:

(conj-vp (nsubj (cc s-to-i and) eminem) d-50)
this s-expression is not possible under the oblique-
ness hierarchy, which places nsubj modi   ers to a
verb after conj-vp modi   ers.

we realize that this treatment of conjunction is
quite naive in comparison to that on offer in id35.
however, given the crude analysis of conjunction
in dependency syntax, a more re   ned treatment is
beyond the scope of the current approach.

relative clauses. our treatment of relative clauses
is closely related to the mechanism for traces de-
scribed by moortgat (1988; 1991); see also carpenter
(1998) and pereira (1990). consider the np apple
which jobs founded with s-expression:

(rcmod apple

(wh-dobj (bind f (nsubj (dobj founded f ) jobs))

which))

note that the s-expression has been augmented
to include a variable f in dobj position, with
(bind f ...) binding this variable at the clause level.
these annotations are added using a set of heuristic
rules over the original dependency parse tree.

the bind operation is interpreted in the following

way. if we have an expression of the form

(bind f   x. g(x))

where f is a variable and g is an expression that
includes f, this is converted to

  z.   x. g(x) |f =eq(z)

where g(x) |f =eq(z) is the expression g(x) with the
expression eq(z) substituted for f. eq(z)(z(cid:48)) is true
iff z and z(cid:48) are equal (refer to the same entity). in
addition we assume the following entries:
wh-dobj       f gz. f (z)
rcmod       f gz. f (z)     g(z)
it can be veri   ed that (bind f (nsubj (dobj
founded f) jobs)) has semantics
  u.   xyz. founded(xe)     jobs(ya)     eq(u)(z)

    arg1(xe, ya)     arg2(xe, za)

and apple which jobs founded has semantics
  u.   xyz. founded(xe)     jobs(ya)     eq(u)(z)

    arg1(xe, ya)     arg2(xe, za)     apple(ua)
as intended. note that this latter expression can be
simpli   ed, by elimination of the z variable, to
  u.   xy. founded(xe)     jobs(ya)

    arg1(xe, ya)     arg2(xe, ua)     apple(ua)
wh questions. wh questions are handled using
the bind-mechanism described in the previous sec-
tion. as one example, the s-expression for who did
jim marry is as follows:
(wh-dobj (bind f (nsubj (aux (dobj marry f ) did)

jim)) who)

      x. true
      f g. f

we assume the following lambda expressions:
who       x. target(xa)
did
aux
wh-dobj       f gx. f (x)     g(x)
it can be veri   ed that this gives the    nal logical form
  x.   yz. target(xa)     marry(ye)     jim(za)

    arg1(ye, za)     arg2(ye, xa)

note that the predicate target is applied to the
variable that is the focus of the question. a similar
treatment is used for cases with the wh-element in
subject position (e.g., who married jim) or where the
wh-element is extracted from a prepositional phrase
(e.g., who was jim married to).

3.3 comparison to id35
in this section we discuss some differences between
our approach and id35-based approaches for map-
ping sentences to logical forms. although our focus
is on id35, the arguments are similar for other for-
malisms that use the id198 in conjunction
with a generative grammar, such as hpsg and lfg,
or approaches based on context-free grammars.

our approach differs in two important (and re-
lated) respects from id35: (1) all constituents in
our approach have the same semantic type (ind   
event     bool); (2) our formalism does not make
the argument/adjunct distinction, instead essentially
treating all modi   ers to a given head as adjuncts.

as an example, consider the analysis of disney
acquired pixar within id35. in this case acquired
would be assigned the following id35 lexical entry:
s\np/np       f2f1x.   yz. acquired(x)     f1(y)     f2(z)

   arg1(x, y)     arg2(x, z)

note the explicit arguments corresponding to the
subject and object of this transitive verb (f1 and f2,
respectively). an intransitive verb such as sleeps
would be assigned an entry with a single functional
argument corresponding to the subject (f1):
s\np       f1x.   y. sleeps(x)     f1(y)     arg1(x, y)

in contrast,

the entries in our system for
these two verbs are simply   x. acquired(xe) and
  x. sleeps(xe). the two forms are similar, have the
same semantic type, and do not include variables
such as f1 and f2 for the subject and object.

the advantage of our approach is that it is ro-
bust, and relatively simple, in that a strict gram-
mar that enforces type checking is not required.
however, there are challenges in handling some lin-
guistic constructions. a simple example is passive
verbs.
in our formalism, the passive form of ac-
quired has the form   x. acquired.pass(xe), distinct
from its active form   x. acquired(xe). the sen-
tence pixar was acquired is then assigned the log-
ical form   x.   y. acquired.pass(xe)     pixar(ya)    
arg1(xe, ya). modifying our approach to give the
same logical forms for active and passive forms
would require a signi   cant extension of our approach.
in contrast, in id35 the lexical entry for the passive
form of acquired can directly specify the mapping
between subject position and the arg2:

s\np       f2x.   z. acquired(x)     f2(z)     arg2(x, z)

as another example, correct handling of object
and subject control verbs is challenging in the single-
type system: for example, in the analysis for john
persuaded jim to acquire apple, the id35 analysis
would have an entry for persuaded that explicitly
takes three arguments (in this case john, jim, and
to acquire apple) and assigns jim as both the direct
object of persuaded and as the subject of acquire. in
our approach the subject relationship to acquire is
instead recovered in a post-processing step, based on
the lexical identity of persuaded.

4 id29 as graph matching

we next describe how the ungrounded logical forms
from the previous section are mapped to a fully
grounded semantic representation that can be used
for id53 against freebase. follow-
ing reddy et al. (2014), we treat this mapping as a
graph matching problem, but instead of deriving un-
grounded graphs from id35-based logical forms, we
use the dependency-based logical forms from the pre-
vious sections. to learn the mapping to freebase, we
rely on manually assembled question-answer pairs.
for each training question, we    rst    nd the set of
oracle grounded graphs   freebase subgraphs which
when executed yield the correct answer   derivable
from the question logical form. these oracle graphs
are then used to train a structured id88 model.

4.1 ungrounded graphs
we follow reddy et al. (2014) and    rst convert logi-
cal forms to their corresponding ungrounded graphs.
figure 2(a) shows an example for what is the name of
the company which disney acquired in 2006?. predi-
cates corresponding to resolved entities (disney(ya)
and 2006(va)) become entity nodes (rectangles),
whereas remaining entity predicates (name(wa) and
company(xa)) become entity nodes (wa and xa),
connected to entity type nodes (name and company;
rounded rectangles). the target(wa) node (dia-
mond) connects to the entity node whose denotation
corresponds to the answer to the question.

4.2 grounded graphs
the ungrounded graphs are grounded to freebase
subgraphs by mapping entity nodes, entity-entity

sponding ungrounded graph has three disconnected
components (december and washington dc, and
the component with entity node xa linked to event
ze). in such cases, the graph is expanded by link-
ing disconnected entity nodes to the event node with
the largest edge degree. in the example above, this
would add edges corresponding to the predicates
dep(ze, ya)     dep(ze, wa), where dep is the predi-
cate introduced by the expand operation when link-
ing ya and wa to ze. when there is no existing event
node in the graph, a dummy event node is introduced.

4.3 learning
we use a linear model
to map ungrounded to
grounded graphs. the parameters of the model are
learned from question-answer pairs. for example,
the question what is the name of the company which
disney acquired in 2006? is paired with its answer
{pixar}. in line with most work on question answer-
ing against freebase, we do not rely on annotated log-
ical forms associated with the question for training,
instead treating grounded graphs as latent variables.
let q be a question, let u be an ungrounded graph
for q and let g be a grounded graph formed by ground-
ing the nodes and edges of u to the knowledge base
k (throughout we use freebase as the knowledge
base). following reddy et al. (2014), we use beam
search to    nd the highest scoring pair of ungrounded
and grounded graphs (  u,   g) under the model        (cid:60)n:

(  u,   g) = arg max

(u,g)

        (u, g, q,k) ,

where   (u, g, q,k)     (cid:60)n denotes the features for the
pair of ungrounded and grounded graphs. note that
for a given query there may be multiple ungrounded
graphs, primarily due to the optional use of the con-
tract operation.3 the feature function has access to
the ungrounded and grounded graphs, to the question,
as well as to the content of the knowledge base and
the denotation |g|k (the denotation of a grounded
graph is de   ned as the set of entities or attributes
reachable at its target node). see section 5.3 for
the features employed.

the model parameters are estimated with the av-
eraged structured id88 (collins, 2002; fre-

3another source of ambiguity may be a lexical item having
multiple lambda-calculus entries; in our rules this only arises
when analyzing count expressions such as how many.

(a) before contract.

(b) after contract.

figure 2: the contract operation applied to the un-
grounded graph for the question what is the name of the
company which disney acquired in 2006?. after con-
tract has been applied the graph is isomorphic to the
representation in freebase; in (b) we show the freebase
predicates after grounding in blue.

edges and entity type nodes in the ungrounded graph
to freebase entities, relations and types, respec-
tively. while reddy et al. (2014) assume that the un-
grounded graphs are isomorphic to their correspond-
ing freebase subgraph, at least 15% of the examples
in our development set do not satisfy this property.
for example, the ungrounded graph in figure 2(a)
is not isomorphic to the freebase subgraph in fig-
ure 2(b), making it impossible to derive the correct
grounded graph from the ungrounded one by a direct
mapping. to account for such structural mismatch,
we introduce two simple transformation operations.

contract. the contract operation takes a
pair of entity nodes connected by an edge and merges
them into a single node. for example, in figure 2(a)
the entity nodes wa and xa are connected by an edge
via the event we. after applying the contract op-
eration to nodes wa and xa, they are merged. note
how in figure 2(b) all the nodes attached to wa attach
to the node xa after this operation. the contracted
graph is now isomorphic to its freebase subgraph.

expand. parse errors may lead to ungrounded
graphs with disconnected components. for example,
the ungrammatical question what to do washington
dc december? results in the lambda expression
  z.   xyw. target(xa)     do(ze)     arg1(ze, xa)    
washington dc(ya)     december(wa). the corre-

nametargetcompanywawexazedisneyzeze2006acquired.arg1acquired.arg2acquired.prep.inacquired.arg2acquired.arg1acquired.prep.inname.arg1name.prep.oftypetypecontracttargetorganization.organizationcompanynamexazedisneyzeze2006acquired.arg1acquired.arg2business.acquistion.acquiringcompanybusiness.acquistion.companyacquiredacquired.prep.inacquired.arg2business.acquisition.datebusiness.acquistion.companyacquiredacquired.arg1acquired.prep.inbusiness.acquistion.acquiringcompanybusiness.acquisition.datetypetypeund and schapire, 1999). given a training question-
answer pair (q,a), the update is:

  t+1       t +   (u+, g+, q,k)       (  u,   g, q,k) ,

where (u+, g+) denotes the pair of gold ungrounded
and grounded graphs for q. since we do not have
direct access to these gold graphs, we instead rely on
the set of oracle graphs, ok,a(q), as a proxy:
  t      (u, g, q,k) ,

(u+, g+) = arg max

(u,g)   ok,a(q)

where ok,a(q) is de   ned as the set of pairs (u, g)
derivable from the question q, whose denotation |g|k
has minimal f1-loss against the gold answer a. we
   nd the oracle graphs for each question a priori by
performing beam-search with a beam size of 10k and
only use examples with oracle f1 > 0.0 for training.

5 experimental setup

we next verify empirically that our proposed ap-
proach derives a useful logical compositional seman-
tic representation from dependency syntax. below,
we give details on the evaluation datasets and base-
lines used for comparison. we also describe the
model features and provide implementation details.

5.1 training and evaluation datasets
we evaluated our approach on the free917 (cai and
yates, 2013) and webquestions (berant et al., 2013)
datasets. free917 consists of 917 questions manually
annotated with their freebase query. we retrieved
the answer to each question by executing its query on
freebase and ignore the query for all subsequent ex-
periments. webquestions consists of 5810 question-
answer pairs. the standard train/test splits were used
for both datasets, with free917 containing 641 train
and 276 test questions and webquestions contain-
ing 3030 train and 2780 test questions. for all our
development experiments we tuned the models on
held-out data consisting of 30% of the training ques-
tions, while for    nal testing we used the complete
training data.

5.2 baseline models and representations
in addition to the dependency-based semantic rep-
resentation deplambda (section 3) and previous

work on these datasets, we compare to three addi-
tional baseline representations outlined below. we
use graphparser4 to map these representations to
freebase.
deptree.
in this baseline, an ungrounded graph
is created directly from the original dependency tree.
an event is created for each parent and its dependents
in the tree. each dependent is linked to this event with
an edge labeled with its dependency relation, while
the parent is linked to the event with an edge labeled
arg0. if a word is a question word, an additional
target predicate is attached to its entity node.
simplegraph. this representation has a single
event to which all entities in the question are con-
nected by the predicate arg1. an additional target
node is connected to the event by the predicate arg0.
this is similar to the template representation of yao
(2015) and bast and haussmann (2015). note that
this cannot represent any compositional structure.
id35graph. finally, we compare to the id35-
based semantic representation of reddy et al. (2014),
adding the contract and expand operations to
increase its expressivity.

implementation details

5.3
below are more details of our entity resolution model,
the syntactic parser used, features in the grounding
model and the id125 procedure.
entity resolution. for free917, we follow prior
work and resolve entities by string match against the
entity lexicon provided with the dataset. for web-
questions, we use eight handcrafted part-of-speech
patterns to identify entity span candidates. we use the
stanford corenlp caseless tagger for part-of-speech
tagging (manning et al., 2014). for each candidate
mention span, we retrieve the top 10 entities accord-
ing to the freebase api.5 we then create a lattice in
which the nodes correspond to mention-entity pairs,
scored by their freebase api scores, and the edges
encode the fact that no joint assignment of entities
to mentions can contain overlapping spans. finally,
we generate ungrounded graphs for the top 10 paths
through the lattice and treat the    nal entity disam-
biguation as part of the id29 problem.
4http://github.com/sivareddyg/graph-parser
5http://developers.google.com/freebase/

representation

-c -e

-c +e

+c -e

+c +e

representation

-c -e

-c +e

+c -e

+c +e

(a) average oracle f1

(a) oracle accuracy

deptree
simplegraph
id35graph
deplambda

30.8
73.0
65.1
64.8

30.8
73.0
70.3
66.3

72.8
73.0
67.6
71.8

72.8
73.0
72.9
73.0

deptree
simplegraph
id35graph
deplambda

26.0
96.3
91.2
91.1

26.0
96.3
93.3
92.7

95.8
96.3
92.2
94.3

95.8
96.3
95.3
95.8

(b) average number of oracle graphs per question

(b) average number of oracle graphs per question

deptree
simplegraph
id35graph
deplambda

1.5
1.5
1.6
1.4

1.5
1.5
1.7
1.5

deptree
simplegraph
id35graph
deplambda

(c) average f1
19.9
19.9
49.0
49.0
47.3
44.7
45.9
47.5

354.6
1.8
3.4
3.6

42.6
48.2
46.5
48.8

354.6
1.8
3.4
4.2

42.6
48.2
48.9
50.4

deptree
simplegraph
id35graph
deplambda

1.2
1.6
1.6
1.5

1.2
1.6
1.6
1.5

(c) accuracy

deptree
simplegraph
id35graph
deplambda

21.3
40.9
68.3
69.3

21.3
40.9
69.4
71.3

285.4
1.8
2.4
3.3

51.6
42.0
70.4
72.4

285.4
1.8
2.5
3.4

51.6
42.0
71.0
73.4

table 1: oracle statistics and accuracies on the web-
questions development set. +(-)c: with(out) contract.
+(-)e: with(out) expand.

syntactic parsing. we recase the resolved entity
mentions and run a case-sensitive second-order con-
ditional random    eld part-of-speech tagger (lafferty
et al., 2001). the hypergraph parser of (zhang
and mcdonald, 2014) is used for dependency pars-
ing. the tagger and parser are both trained on the
ontonotes 5.0 corpus (weischedel et al., 2011), with
constituency trees converted to stanford-style depen-
dencies (de marneffe and manning, 2013). to derive
the id35-based representation, we use the output of
the easyid35 parser (lewis and steedman, 2014).
features. we use the features from reddy et al.
(2014), which include edge alignment and stem over-
lap between ungrounded and grounded graphs, and
contextual features such as word and grounded rela-
tion pairs. in addition, we introduce a feature indi-
cating the use of the contract operation: (merged-
subedge, headsubedge, mergedisentity, headisen-
tity). for example, in figure 2 the edge between wa
and xa is contracted to xa, resulting in the feature
(name.arg1, name.prep.of, false, false). the ex-
pand operation is treated as a pre-processing step
and no features are used to encode its use. finally,
the entity-lattice score is used as a real valued feature.
id125. we use id125 to infer the
highest scoring graph pair for a question. the search
operates over entity-entity edges and entity type
nodes of each ungrounded graph. for an entity-entity

table 2: oracle statistics and accuracies on the free917
development set. +(-)c: with(out) contract. +(-)e:
with(out) expand.
edge, we can ground the edge to a freebase relation,
contract the edge in either direction, or skip the edge.
for an entity type node, we can ground the node to a
freebase type, or skip the node. the order of traversal
is based on the number of named entities connected
to an edge. after an edge is grounded, the entity type
nodes connected to it are grounded in turn, before the
next edge is processed. to restrict the search, if two
beam items correspond to the same grounded graph,
the one with the lower score is discarded. a beam
size of 100 was used in all experiments.
6 experimental results
we examine the different representations for ques-
tion answering along two axes. first, we compare
their expressiveness in terms of answer reachability
assuming a perfect model. second, we compare their
performance with a learned model. finally, we con-
duct a detailed error analysis of deplambda, with
a comparison to the errors made by id35graph.
for webquestions evaluation is in terms of the av-
erage f1-score across questions, while for free917,
evaluation is in terms of exact answer accuracy.6
6.1 expressiveness of the representations
table 1(a) and table 2(a) show the oracle f1-scores
of each representation on the webquestions and

6we use the evaluation scripts available at http://
www-nlp.stanford.edu/software/sempre and http://
github.com/elmar-haussmann/aqqu, respectively.

free917 development sets respectively. according to
the    rst column (-c -e), the original deptree repre-
sentation can be directly mapped to freebase for less
than a third of the questions. adding the contract
operation (+c) improves this substantially to an ora-
cle f1 of about 73% on webquestions and 95.8% on
free917. however, this comes at the cost of massive
spurious ambiguity: from table 1(b) there are on av-
erage over 300 oracle graphs for a single dependency
tree. table 1(c) shows the results of the different
representations on the webquestions development
set. spurious ambiguity clearly hampers learning
and deptree falls behind the other representations.
id35graph and deplambda align much more
closely to freebase and achieve similar oracle f1
scores with far less spurious ambiguity. simple-
graph, which cannot represent any compositional
semantics, is competitive with these syntax-based
representations. this might come as a surprise, but it
simply re   ects the fact that the dataset does not con-
tain questions that require compositional reasoning.

6.2 results on webquestions and free917
we use the best settings on the development set in
subsequent experiments, i.e. with contract and
expand enabled. table 3 shows the results on the
webquestions and free917 test sets with additional
entries for recent prior work on these datasets. the
trend from the development set carries over and de-
plambda outperforms the other graph-based repre-
sentations, while performing slightly below the state-
of-the-art model of yih et al. (2015) (   y&c   ), which
uses a separately trained entity resolution system
(yang and chang, 2015). when using the standard
freebase api (   fb api   ) for entity resolution, the
performance of their model drops to 48.4% f1.

on free917, deplambda outperforms all other
representations by a wide margin and obtains the best
result to date. interestingly, deptree outperforms
simplegraph in this case. we attribute this to
the small training set and larger lexical variation of
free917. the structural features of the graph-based
representations seem highly bene   cial in this case.

6.3 error analysis
we categorized 100 errors made by deplambda
(+c +e) on the webquestions development set. in
43 cases the correct answer is present in the beam,

free917 webquestions
accuracy

average f1

method

cai and yates (2013)
berant et al. (2013)
kwiatkowski et al. (2013)
yao and van durme (2014)
berant and liang (2014)
bao et al. (2014)
bordes et al. (2014)
yao (2015)
yih et al. (2015) (fb api)
bast and haussmann (2015)
berant and liang (2015)
yih et al. (2015) (y&c)

59.0
62.0
68.0

   

68.5

   
   
   
   

76.4

   
   

this work

deptree
simplegraph
id35graph (+c +e)
deplambda (+c +e)

53.2
43.7
73.3
78.0

   

35.7

   

33.0
39.9
37.5
39.2
44.3
48.4
49.4
49.7
52.5

40.4
48.5
48.6
50.3

table 3: question-answering results on the webquestions
and free917 test sets.

but ranked below an incorrect answer (e.g., for where
does volga river start, the annotated gold answer is
valdai hills, which is ranked second, with russia,
europe ranked    rst). in 35 cases, only a subset of
the answer is predicted correctly (e.g, for what coun-
tries in the world speak german, the system predicts
germany from the human language.main country
freebase relation, whereas
the gold relation
human language.countries spoken in gives multi-
ple countries). together, these two categories corre-
spond to roughly 80% of the errors. in 10 cases, the
freebase api fails to add the gold entity to the lattice
(e.g., for who is blackwell, the correct blackwell en-
tity was missing). due to the way webquestions was
crowdsourced, 9 questions have incorrect or incom-
plete gold annotations (e.g., what does each fold of us
   ag means is answered with usa). the remaining
3 cases are due to structural mismatch (e.g., in who is
the new governor of    orida 2011, the graph failed to
connect the target node with both 2011 and florida).
due to the ungrammatical nature of webquestions,
id35graph fails to produce ungrounded graphs for
4.5% of the complete development set, while de-
plambda is more robust with only 0.9% such errors.
the id35 parser is restricted to produce a sentence
tag as the    nal category in the syntactic derivation,
which penalizes ungrammatical analyses (e.g., what

victoria beckham kids names and what nestle owns).
examples where deplambda fails due to parse er-
rors, but id35graph succeed include when was
blessed kateri born and where did anne frank live
before the war. note that the expand operation mit-
igates some of these problems. while id35 is known
for handling comparatives elegantly (e.g., who was
sworn into of   ce when john f kennedy was assassi-
nated), we do not have a special treatment for them
in the semantic representation. differences in syn-
tactic parsing performance and the somewhat limited
expressivity of the semantic representation are likely
the reasons for id35graph   s lower performance.
7 related work
there are two relevant strands of prior work: gen-
eral purpose ungrounded semantics and grounded
id29. the former have been studied on
their own and as a component in tasks such as seman-
tic parsing to knowledge bases (kwiatkowski et al.,
2013; reddy et al., 2014; choi et al., 2015; krishna-
murthy and mitchell, 2015), sentence simpli   cation
(narayan and gardent, 2014), summarization (liu
et al., 2015), id141 (pavlick et al., 2015) and
id36 (rockt  aschel et al., 2015). there
are two ways of generating these representations: ei-
ther relying on syntactic structure and producing the
semantics post hoc, or generating it directly from text.
we adopt the former approach, which was pioneered
by montague (1973) and is becoming increasingly at-
tractive with the advent of accurate syntactic parsers.
there have been extensive studies on extracting
semantics from syntactic representations such as
lfg (dalrymple et al., 1995), hpsg (copestake
et al., 2001; copestake et al., 2005), tag (gar-
dent and kallmeyer, 2003; joshi et al., 2007) and
id35 (baldridge and kruijff, 2002; bos et al., 2004;
steedman, 2012; artzi et al., 2015). however, few
have used dependency structures for this purpose.
debusmann et al. (2004) and cimiano (2009) de-
scribe grammar-based conversions of dependencies
to semantic representations, but do not validate them
empirically. stanovsky et al. (2016) use heuristics
based on linguistic grounds to convert dependen-
cies to proposition structures. b  edaride and gar-
dent (2011) propose a graph-rewriting technique to
convert a graph built from dependency trees and se-
mantic role structures to a    rst-order logical form,

and present results on id123. our work,
in contrast, assumes access only to dependency trees
and offers an alternative method based on the lambda
calculus, mimicking the structure of knowledge bases
such as freebase; we further present extensive empir-
ical results on recent question-answering corpora.

structural mismatch between the source semantic
representation and the target application   s represen-
tation is an inherent problem with approaches using
general-purpose representations. kwiatkowski et al.
(2013) propose lambda-calculus operations to gen-
erate multiple type-equivalent expressions to handle
this mismatch. in contrast, we use graph-transduction
operations which are relatively easier to interpret.
there is also growing work on converting syntactic
structures to the target application   s structure without
going through an intermediate semantic representa-
tion, e.g., answer-sentence selection (punyakanok et
al., 2004; heilman and smith, 2010; yao et al., 2013)
and id29 (ge and mooney, 2009; poon,
2013; parikh et al., 2015; xu et al., 2015; wang et
al., 2015; andreas and klein, 2015).

a different paradigm is to directly parse the text
into a grounded semantic representation. typically,
an over-generating grammar is used whose accepted
parses are ranked (zelle and mooney, 1996; zettle-
moyer and collins, 2005; wong and mooney, 2007;
kwiatkowksi et al., 2010; liang et al., 2011; berant
et al., 2013; flanigan et al., 2014; groschwitz et al.,
2015). in contrast, bordes et al. (2014) and dong
et al. (2015) discard the notion of a target represen-
tation altogether and instead learn to rank potential
answers to a given question by embedding questions
and answers into the same vector space.

8 conclusion

we have introduced a method for converting depen-
dency structures to logical forms using the lambda
calculus. a key idea of this work is the use of a single
semantic type for every constituent of the dependency
tree, which provides us with a robust way of com-
positionally deriving logical forms. the resulting
representation is subsequently grounded to freebase
by learning from question-answer pairs. empirically,
the proposed representation was shown to be superior
to the original dependency trees and more robust than
logical forms derived from a id35 parser.

acknowledgements

this work greatly bene   tted from discussions with
slav petrov, john blitzer, fernando pereira, emily
pitler and nathan schneider. the authors would also
like to thank christopher potts and the three anony-
mous reviewers for their valuable feedback. we ac-
knowledge the    nancial support of eu ist cognitive
systems ip ec-fp7-270273    xperience    (steedman)
and epsrc (ep/k017845/1) in the framework of the
chist-era readers project (lapata).

references
jacob andreas and dan klein. 2015. alignment-based
id152 for instruction following. in
proceedings of empirical methods on natural lan-
guage processing, pages 1165   1174.

yoav artzi, kenton lee, and luke zettlemoyer. 2015.
broad-coverage id35 id29 with amr. in
proceedings of empirical methods on natural lan-
guage processing, pages 1699   1710.

jason baldridge and geert-jan kruijff. 2002. coupling
id35 and hybrid logic dependency semantics. in pro-
ceedings of association for computational linguistics,
pages 319   326.

junwei bao, nan duan, ming zhou, and tiejun zhao.
2014. knowledge-based id53 as ma-
chine translation. in proceedings of association for
computational linguistics, pages 967   976.

hannah bast and elmar haussmann. 2015. more accu-
rate id53 on freebase. in proceedings
of acm international conference on information and
knowledge management, pages 1431   1440.

paul b  edaride and claire gardent. 2011. deep semantics
for dependency structures. in proceedings of confer-
ence on intelligent text processing and computational
linguistics, pages 277   288.

jonathan berant and percy liang. 2014. id29
via id141. in proceedings of association for
computational linguistics, pages 1415   1425.

jonathan berant and percy liang.

imitation
learning of agenda-based semantic parsers. transac-
tions of the association for computational linguistics,
3:545   558.

2015.

jonathan berant, andrew chou, roy frostig, and percy
liang. 2013. id29 on freebase from
question-answer pairs. in proceedings of empirical
methods on natural language processing, pages 1533   
1544.

antoine bordes, sumit chopra, and jason weston. 2014.
id53 with subgraph embeddings. in

proceedings of empirical methods on natural lan-
guage processing, pages 615   620.

johan bos, stephen clark, mark steedman, james r. cur-
ran, and julia hockenmaier. 2004. wide-coverage
semantic representations from a id35 parser. in pro-
ceedings of international conference on computational
linguistics, pages 1240   1246.

qingqing cai and alexander yates. 2013. large-scale
id29 via schema matching and lexicon
extension. in proceedings of association for computa-
tional linguistics, pages 423   433.

bob carpenter. 1998. type-logical semantics. mit press,

cambridge, ma, usa.

david l. chen and raymond j. mooney. 2011. learning
to interpret natural language navigation instructions
from observations. in proceedings of association for
the advancement of arti   cial intelligence, pages 1   2.
eunsol choi, tom kwiatkowski, and luke zettlemoyer.
2015. scalable id29 with partial ontolo-
gies. in proceedings of association for computational
linguistics, pages 1311   1320.

philipp cimiano. 2009. flexible semantic composition
with dudes. in proceedings of international confer-
ence on computational semantics, pages 272   276.

michael collins. 2002. discriminative training methods
for id48: theory and experiments
with id88 algorithms. in proceedings of empiri-
cal methods on natural language processing, pages
1   8.

ann copestake, alex lascarides, and dan flickinger.
2001. an algebra for semantic construction in
constraint-based grammars. in proceedings of associ-
ation for computational linguistics, pages 140   147.

ann copestake, dan flickinger, carl pollard, and ivan a.
sag. 2005. minimal recursion semantics: an intro-
duction. research on language and computation, 3(2-
3):281   332.

mary dalrymple, john lamping, fernando c. n. pereira,
and vijay a. saraswat. 1995. linear logic for mean-
ing assembly. in proceedings of computational logic
for natural language processing.

marie-catherine de marneffe and christopher d. man-

ning, 2013. stanford typed dependencies manual.

ralph debusmann, denys duchier, alexander koller,
marco kuhlmann, gert smolka, and stefan thater.
2004. a relational syntax-semantics interface based
on dependency grammar. in proceedings of interna-
tional conference on computational linguistics, pages
176   182.

li dong, furu wei, ming zhou, and ke xu. 2015. ques-
tion answering over freebase with multi-column con-
volutional neural networks. in proceedings of associ-
ation for computational linguistics, pages 260   269.

jeffrey flanigan, sam thomson, jaime carbonell, chris
dyer, and noah a. smith. 2014. a discriminative
graph-based parser for the abstract meaning repre-
sentation. in proceedings of association for computa-
tional linguistics, pages 1426   1436.

yoav freund and robert e. schapire. 1999. large margin
classi   cation using the id88 algorithm. machine
learning, 37(3):277   296, december.

claire gardent and laura kallmeyer. 2003. semantic
construction in feature-based tag. in proceedings
of european chapter of the association for computa-
tional linguistics, pages 123   130.

ruifang ge and raymond mooney. 2009. learning
a compositional semantic parser using an existing
in proceedings of association for
syntactic parser.
computational linguistics, pages 611   619.

jonas groschwitz, alexander koller, and christoph teich-
mann. 2015. graph parsing with s-graph grammars. in
proceedings of association for computational linguis-
tics, pages 1481   1490.

michael heilman and noah a. smith. 2010. tree edit
models for recognizing id123s, para-
phrases, and answers to questions. in proceedings
of north american chapter of the association for com-
putational linguistics, pages 1011   1019.

aravind k. joshi, laura kallmeyer, and maribel romero.
2007. flexible composition in ltag: quanti   er scope
and inverse linking.
in harry bunt and reinhard
muskens, editors, computing meaning, volume 83 of
studies in linguistics and philosophy, pages 233   256.
springer netherlands.

jayant krishnamurthy and tom mitchell. 2012. weakly
supervised training of semantic parsers. in proceed-
ings of empirical methods on natural language pro-
cessing, pages 754   765.

jayant krishnamurthy and tom m. mitchell. 2015. learn-
ing a id152 for freebase with an
open predicate vocabulary. transactions of the associ-
ation for computational linguistics, 3:257   270.

tom kwiatkowksi, luke zettlemoyer, sharon goldwater,
and mark steedman. 2010.
inducing probabilistic
id35 grammars from logical form with higher-order
uni   cation. in proceedings of empirical methods on
natural language processing, pages 1223   1233.

tom kwiatkowski, eunsol choi, yoav artzi, and luke
zettlemoyer. 2013. scaling semantic parsers with
in proceedings of
on-the-fly ontology matching.
empirical methods on natural language processing,
pages 1545   1556.

john lafferty, andrew mccallum, and fernando pereira.
2001. conditional random    elds: probabilistic models
for segmenting and labeling sequence data. in proceed-
ings of international conference on machine learning,
pages 282   289.

mike lewis and mark steedman. 2014. a* id35 parsing
in proceedings of
with a supertag-factored model.
empirical methods on natural language processing,
pages 990   1000.

percy liang, michael jordan, and dan klein. 2011.
learning dependency-based compositional seman-
tics. in proceedings of association for computational
linguistics, pages 590   599.

fei liu, jeffrey flanigan, sam thomson, norman sadeh,
and noah a. smith. 2015. toward abstractive sum-
marization using semantic representations. in pro-
ceedings of north american chapter of the association
for computational linguistics, pages 1077   1086.

christopher d. manning, mihai surdeanu, john bauer,
jenny finkel, steven j. bethard, and david mcclosky.
2014. the stanford corenlp natural language process-
ing toolkit. in proceedings of association for compu-
tational linguistics, pages 55   60.

andre martins, miguel almeida, and noah a. smith.
2013. turning on the turbo: fast third-order non-
projective turbo parsers. in proceedings of association
for computational linguistics, pages 617   622.

cynthia matuszek, nicholas fitzgerald, luke zettle-
moyer, liefeng bo, and dieter fox. 2012. a joint
model of language and perception for grounded at-
tribute learning. in proceedings of international con-
ference on machine learning.

ryan mcdonald, fernando pereira, kiril ribarov, and jan
haji  c. 2005. non-projective id33 using
spanning tree algorithms. in proceedings of empirical
methods on natural language processing, pages 523   
530.

richard montague. 1973. the proper treatment of quan-
ti   cation in ordinary english. in k.j.j. hintikka, j.m.e.
moravcsik, and p. suppes, editors, approaches to nat-
ural language, volume 49 of synthese library, pages
221   242. springer netherlands.

michael moortgat. 1988. categorical investigations.
logical and linguistic aspects of the lambek calcu-
lus. number 9 in groningen-amsterdam studies in
semantics. foris, dordrecht.

michael moortgat. 1991. generalized quanti   cation and
discontinuous type constructors. technical report, uni-
versity of utrecht.

shashi narayan and claire gardent. 2014. hybrid sim-
pli   cation using deep semantics and machine transla-
tion. in proceedings of association for computational
linguistics, pages 435   445.

joakim nivre, johan hall, jens nilsson, atanas chanev,
g  ulsen eryigit, sandra k  ubler, svetoslav marinov,
and erwin marsi. 2007. maltparser: a language-
independent system for data-driven dependency pars-
ing. natural language engineering, 13(2):95   135.

yuk wah wong and raymond mooney. 2007. learn-
ing synchronous grammars for id29 with
id198. in proceedings of association for
computational linguistics, pages 960   967.

kun xu, yansong feng, songfang huang, and dongyan
zhao. 2015. id53 via phrasal semantic
parsing. in proceedings of conference and labs of the
evaluation forum, pages 414   426.

yi yang and ming-wei chang. 2015. s-mart: novel
tree-based structured learning algorithms applied to
tweet entity linking. in proceedings of association for
computational linguistics, pages 504   513.

xuchen yao and benjamin van durme. 2014. informa-
tion extraction over structured data: question answer-
ing with freebase. in proceedings of association for
computational linguistics, pages 956   966.

xuchen yao, benjamin van durme, chris callison-burch,
and peter clark. 2013. answer extraction as sequence
tagging with tree id153. in proceedings of
north american chapter of the association for compu-
tational linguistics, pages 858   867.

xuchen yao. 2015. lean id53 over free-
base from scratch. in proceedings of north american
chapter of the association for computational linguis-
tics, pages 66   70.

wen-tau yih, ming-wei chang, xiaodong he, and jian-
feng gao. 2015. id29 via staged query
graph generation: id53 with knowl-
edge base. in proceedings of association for compu-
tational linguistics, pages 1321   1331.

john m. zelle and raymond j. mooney. 1996. learning
to parse database queries using inductive logic program-
ming. in proceedings of association for the advance-
ment of arti   cial intelligence, pages 1050   1055.

luke s. zettlemoyer and michael collins. 2005. learning
to map sentences to logical form: structured clas-
si   cation with probabilistic categorial grammars. in
proceedings of uncertainty in arti   cial intelligence,
pages 658   666.

hao zhang and ryan mcdonald. 2014. enforcing struc-
tural diversity in cube-pruned id33. in
proceedings of association for computational linguis-
tics, pages 656   661.

ankur p. parikh, hoifung poon, and kristina toutanova.
2015. grounded id29 for complex knowl-
edge extraction. in proceedings of north american
chapter of the association for computational linguis-
tics, pages 756   766.

ellie pavlick, johan bos, malvina nissim, charley beller,
benjamin van durme, and chris callison-burch. 2015.
adding semantics to data-driven id141. in pro-
ceedings of association for computational linguistics,
pages 1512   1522.

fernando c. n. pereira. 1990. categorial semantics and

scoping. computational linguistics, 16(1):1   10.

carl pollard and ivan a. sag. 1994. head-driven phrase

structure grammar. university of chicago press.

hoifung poon. 2013. grounded unsupervised semantic
parsing. in proceedings of association for computa-
tional linguistics, pages 933   943.

vasin punyakanok, dan roth, and wen-tau yih. 2004.
mapping dependencies trees: an application to ques-
tion answering. in proceedings of international sym-
posium on arti   cial intelligence and mathematics,
pages 1   10.

siva reddy, mirella lapata, and mark steedman. 2014.
large-scale id29 without question-answer
pairs. transactions of the association for computa-
tional linguistics, 2:377   392.

tim rockt  aschel, sameer singh, and sebastian riedel.
2015. injecting logical background knowledge into
embeddings for id36. in proceedings of
north american chapter of the association for compu-
tational linguistics, pages 1119   1129.

gabriel stanovsky, jessica ficler, ido dagan, and yoav
goldberg. 2016. getting more out of syntax with
props. arxiv preprint.

mark steedman. 2012. taking scope - the natural se-

mantics of quanti   ers. mit press.

chuan wang, nianwen xue, and sameer pradhan. 2015.
a transition-based algorithm for amr parsing. in
proceedings of north american chapter of the associ-
ation for computational linguistics, pages 366   375.
ralph weischedel, eduard hovy, martha palmer, mitch
marcus, robert belvin, sameer pradhan, lance
ramshaw, and nianwen xue. 2011. ontonotes: a
large training corpus for enhanced processing.
in
j. olive, c. christianson, and j. mccary, editors, hand-
book of natural language processing and machine
translation. springer.

yuk wah wong and raymond j. mooney. 2006. learning
for id29 with statistical machine trans-
lation. in proceedings of north american chapter of
the association for computational linguistics, pages
439   446.

