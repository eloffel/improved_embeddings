
   [nlp-logo-small.gif] [1]

the stanford natural language processing group

the stanford nlp group

     * [2]people
     * [3]publications
     * [4]research blog
     * [5]software
     * [6]teaching
     * [7]local

            the stanford natural language id136 (snli) corpus

   new: the new multigenre nli (multinli) corpus is now available [8]here.
   the corpus is in the same format as snli and is comparable in size, but
   it includes a more diverse range of text, as well as an auxiliary test
   set for cross-genre transfer evaluation.

the corpus

   the snli corpus (version 1.0) is a collection of 570k human-written
   english sentence pairs manually labeled for balanced classification
   with the labels entailment, contradiction, and neutral, supporting the
   task of natural language id136 (nli), also known as recognizing
   id123 (rte). we aim for it to serve both as a benchmark
   for evaluating representational systems for text, especially including
   those induced by representation learning methods, as well as a resource
   for developing nlp models of any kind.

   the following paper introduces the corpus in detail. if you use the
   corpus in published work, please cite it:

     [9]samuel r. bowman, [10]gabor angeli, [11]christopher potts, and
     [12]christopher d. manning. 2015. a large annotated corpus for
     learning natural language id136. in proceedings of the 2015
     conference on empirical methods in natural language processing
     (emnlp). [[13]pdf] [[14]bib]

   here are a few example pairs taken from the development portion of the
   corpus. each has the judgments of five mechanical turk workers and a
   consensus judgment.

   text judgments hypothesis
   a man inspects the uniform of a figure in some east asian country.
   contradiction
   c c c c c the man is sleeping
   an older and younger man smiling. neutral
   n n e n n two men are smiling and laughing at the cats playing on the
   floor.
   a black race car starts up in front of a crowd of people. contradiction
   c c c c c a man is driving down a lonely road.
   a soccer game with multiple males playing. entailment
   e e e e e some men are playing a sport.
   a smiling costumed woman is holding an umbrella. neutral
   n n e c n a happy woman in a fairy costume holds an umbrella.

   the corpus is distributed in both json lines and tab separated value
   files, which are packaged together (with a readme) here:

     download: [15]snli 1.0 (zip, ~100mb)

     [16]creative commons license
     the stanford natural language id136 corpus by [17]the stanford
     nlp group is licensed under a [18]creative commons
     attribution-sharealike 4.0 international license.
     based on a work at
     [19]http://shannon.cs.illinois.edu/denotationgraph/.

   the corpus includes content from the [20]flickr 30k corpus (also
   released under an attribution-sharealike licence), which can be cited
   by way of this paper:

     peter young, [21]alice lai, micah hodosh, and [22]julia hockenmaier.
     2014. [23]from image descriptions to visual denotations: new
     similarity metrics for semantic id136 over event descriptions.
     transactions of the association for computational linguistics 2:
     67--78.

   about 4k sentences in the training set have captionids and pairids
   beginning with 'vg_'. these come from a pilot data collection effort
   that used data from the visualgenome corpus, which is still under
   construction as of the release of snli. for more information on
   visualgenome, see: [24]https://visualgenome.org/

   the hard subset of the test set used in gururangan et al. '18 is
   available in jsonl format [25]here.

published results

   the following table reflects our informal attempt to catalog published
   3-class classification results on the snli test set. we define sentence
   vector-based models as those which perform classification on the sole
   basis of a pair of fixed-size sentence representations that are
   computed independently of one another. reported parameter counts do not
   include id27s. if you would like to add a paper that reports
   a number at or above the current state of the art, email [26]sam.

three-way classification

   publication  model parameters  train (% acc)  test (% acc)
   feature-based models
   [27]bowman et al. '15 unlexicalized features 49.4 50.4
   [28]bowman et al. '15 + unigram and bigram features 99.7 78.2
   sentence vector-based models
   [29]bowman et al. '15 100d lstm encoders 220k 84.8 77.6
   [30]bowman et al. '16 300d lstm encoders 3.0m 83.9 80.6
   [31]vendrov et al. '15 1024d gru encoders w/ unsupervised
   'skip-thoughts' pre-training 15m 98.8 81.4
   [32]mou et al. '15 300d tree-based id98 encoders 3.5m 83.3 82.1
   [33]bowman et al. '16 300d spinn-pi encoders 3.7m 89.2 83.2
   [34]yang liu et al. '16 600d (300+300) bilstm encoders 2.0m 86.4 83.3
   [35]munkhdalai & yu '16b 300d nti-slstm-lstm encoders 4.0m 82.5 83.4
   [36]yang liu et al. '16 600d (300+300) bilstm encoders with
   intra-attention 2.8m 84.5 84.2
   [37]conneau et al. '17 4096d bilstm with max-pooling 40m 85.6 84.5
   [38]munkhdalai & yu '16a 300d nse encoders 3.0m 86.2 84.6
   [39]qian chen et al. '17 600d (300+300) deep gated attn. bilstm
   encoders ([40]code) 12m 90.5 85.5
   [41]tao shen et al. '17 300d directional self-attention network
   encoders ([42]code) 2.4m 91.1 85.6
   [43]jihun choi et al. '17 300d gumbel treelstm encoders 2.9m 91.2 85.6
   [44]nie and bansal '17 300d residual stacked encoders 9.7m 89.8 85.7
   [45]anonymous '18 1200d regmapr (base+reg)         85.9
   [46]yi tay et al. '18 300d cafe (no cross-sentence attention) 3.7m 87.3
   85.9
   [47]jihun choi et al. '17 600d gumbel treelstm encoders 10m 93.1 86.0
   [48]nie and bansal '17 600d residual stacked encoders 29m 91.0 86.0
   [49]tao shen et al. '18 300d reinforced self-attention network 3.1m
   92.6 86.3
   [50]im and cho '17 distance-based self-attention network 4.7m 89.6 86.3
   [51]seonhoon kim et al. '18 densely-connected recurrent and
   co-attentive network (encoder) 5.6m 91.4 86.5
   [52]talman et al. '18 600d hierarchical bilstm with max pooling (hbmp,
   [53]code) 22m 89.9 86.6
   [54]qian chen et al. '18 600d bilstm with generalized pooling 65m 94.9
   86.6
   [55]kiela et al. '18 512d dynamic meta-embeddings 9m 91.6 86.7
   [56]deunsol yoon et al. '18 600d dynamic self-attention model 2.1m 87.3
   86.8
   [57]deunsol yoon et al. '18 2400d multiple-dynamic self-attention model
   7.0m 89.0 87.4
   other neural network models
   [58]rockt  schel et al. '15 100d lstms w/ word-by-word attention 250k
   85.3 83.5
   [59]pengfei liu et al. '16a 100d df-lstm 320k 85.2 84.6
   [60]yang liu et al. '16 600d (300+300) bilstm encoders with
   intra-attention and symbolic preproc. 2.8m 85.9 85.0
   [61]pengfei liu et al. '16b 50d stacked tc-lstms 190k 86.7 85.1
   [62]munkhdalai & yu '16a 300d mma-nse encoders with attention 3.2m 86.9
   85.4
   [63]wang & jiang '15 300d mlstm word-by-word attention model 1.9m 92.0
   86.1
   [64]jianpeng cheng et al. '16 300d lstmn with deep attention fusion
   1.7m 87.3 85.7
   [65]jianpeng cheng et al. '16 450d lstmn with deep attention fusion
   3.4m 88.5 86.3
   [66]parikh et al. '16 200d decomposable attention model 380k 89.5 86.3
   [67]parikh et al. '16 200d decomposable attention model with
   intra-sentence attention 580k 90.5 86.8
   [68]munkhdalai & yu '16b 300d full tree matching nti-slstm-lstm w/
   global attention 3.2m 88.5 87.3
   [69]zhiguo wang et al. '17 bimpm 1.6m 90.9 87.5
   [70]lei sha et al. '16 300d re-read lstm 2.0m 90.7 87.5
   [71]yichen gong et al. '17 448d densely interactive id136 network
   (diin, [72]code) 4.4m 91.2 88.0
   [73]mccann et al. '17 biattentive classification network + cove + char
   22m 88.5 88.1
   [74]chuanqi tan et al. '18 150d multiway attention network 14m 94.5
   88.3
   [75]xiaodong liu et al. '18 stochastic answer network 3.5m 93.3 88.5
   [76]ghaeini et al. '18 450d dr-bilstm 7.5m 94.1 88.5
   [77]yi tay et al. '18 300d cafe 4.7m 89.8 88.5
   [78]qian chen et al. '17 kim 4.3m 94.1 88.6
   [79]qian chen et al. '16 600d esim + 300d syntactic treelstm ([80]code)
   7.7m 93.5 88.6
   [81]peters et al. '18 esim + elmo 8.0m 91.6 88.7
   [82]boyuan pan et al. '18 300d dman 9.2m 95.4 88.8
   [83]zhiguo wang et al. '17 bimpm ensemble 6.4m 93.2 88.8
   [84]yichen gong et al. '17 448d densely interactive id136 network
   (diin, [85]code) ensemble 17m 92.3 88.9
   [86]seonhoon kim et al. '18 densely-connected recurrent and
   co-attentive network 6.7m 93.1 88.9
   [87]zhuosheng zhang et al. '18 slrc 6.1m 89.1 89.1
   [88]qian chen et al. '17 kim ensemble 43m 93.6 89.1
   [89]ghaeini et al. '18 450d dr-bilstm ensemble 45m 94.8 89.3
   [90]peters et al. '18 esim + elmo ensemble 40m 92.1 89.3
   [91]yi tay et al. '18 300d cafe ensemble 17.5m 92.5 89.3
   [92]chuanqi tan et al. '18 150d multiway attention network ensemble 58m
   95.5 89.4
   [93]boyuan pan et al. '18 300d dman ensemble 79m 96.1 89.6
   [94]radford et al. '18 fine-tuned lm-pretrained transformer 85m 96.6
   89.9
   [95]seonhoon kim et al. '18 densely-connected recurrent and
   co-attentive network ensemble 53.3m 95.0 90.1
   [96]xiaodong liu et al. '19 mt-dnn 110m 96.8 91.1

related resources

     * [97]a spell-checked version of the test and development sets.
       (warning: results on these sets are not directly comparable to
       results on the regular dev and test sets, and will not be listed
       here.)
     * [98]the multigenre nli (multinli) corpus: like snli (and just as
       big), but with a more diverse variety of text styles and topics.
     * the [99]fracas test suite for natural language id136, in xml
       format
     * [100]mednli: a natural language id136 dataset for the clinical
       domain
     * [101]xnli: a cross-lingual natural language id136 evaluation
       set

contact information

   for any comments or questions, please email [102]sam and [103]gabor.

stanford nlp group

   gates computer science building
   353 serra mall
   stanford, ca 94305-9010
   [104]directions and parking

affiliated groups

     * [105]stanford ai lab
     * [106]stanford infolab
     * [107]center for the study of language and information

connect

     * [108]stack overflow
     * [109]github
     * [110]twitter

       local links:   [111]nlp lunch    [112]nlp reading group    [113]nlp
            seminar    [114]ai speakers    [115]javanlp ([116]javadocs)   
                   [117]machines    [118]wiki    [119]calendar    [120]q&a

references

   visible links
   1. https://nlp.stanford.edu/
   2. https://nlp.stanford.edu/people/
   3. https://nlp.stanford.edu/pubs/
   4. https://nlp.stanford.edu/blog/
   5. https://nlp.stanford.edu/software/
   6. https://nlp.stanford.edu/teaching/
   7. https://nlp.stanford.edu/new_local/
   8. https://www.nyu.edu/projects/bowman/multinli/
   9. https://www.nyu.edu/projects/bowman/
  10. http://cs.stanford.edu/~angeli/
  11. http://www.stanford.edu/~cgpotts/
  12. http://nlp.stanford.edu/~manning/
  13. http://nlp.stanford.edu/pubs/snli_paper.pdf
  14. http://nlp.stanford.edu/pubs/snli_paper.bib
  15. https://nlp.stanford.edu/projects/snli/snli_1.0.zip
  16. http://creativecommons.org/licenses/by-sa/4.0/
  17. http://nlp.stanford.edu/snli/
  18. http://creativecommons.org/licenses/by-sa/4.0/
  19. http://shannon.cs.illinois.edu/denotationgraph/
  20. http://shannon.cs.illinois.edu/denotationgraph/
  21. http://web.engr.illinois.edu/~aylai2/
  22. http://juliahmr.cs.illinois.edu/
  23. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/229
  24. https://visualgenome.org/
  25. https://nlp.stanford.edu/projects/snli/snli_1.0_test_hard.jsonl
  26. mailto:bowman@nyu.edu
  27. http://nlp.stanford.edu/pubs/snli_paper.pdf
  28. http://nlp.stanford.edu/pubs/snli_paper.pdf
  29. http://nlp.stanford.edu/pubs/snli_paper.pdf
  30. https://www.nyu.edu/projects/bowman/spinn.pdf
  31. http://arxiv.org/pdf/1511.06361v3.pdf
  32. http://arxiv.org/pdf/1512.08422.pdf
  33. https://www.nyu.edu/projects/bowman/spinn.pdf
  34. http://arxiv.org/pdf/1605.09090v1.pdf
  35. http://arxiv.org/pdf/1607.04492v1.pdf
  36. http://arxiv.org/pdf/1605.09090v1.pdf
  37. https://arxiv.org/pdf/1705.02364.pdf
  38. http://arxiv.org/pdf/1607.04315.pdf
  39. http://arxiv.org/pdf/1708.01353.pdf
  40. https://github.com/lukecq1231/enc_nli/
  41. http://arxiv.org/pdf/1709.04696.pdf
  42. https://github.com/taoshen58/disan
  43. https://arxiv.org/pdf/1707.02786.pdf
  44. https://arxiv.org/pdf/1708.02312.pdf
  45. https://openreview.net/forum?id=rylnzlfekm
  46. https://arxiv.org/pdf/1801.00102.pdf
  47. https://arxiv.org/pdf/1707.02786.pdf
  48. https://arxiv.org/pdf/1708.02312.pdf
  49. https://arxiv.org/pdf/1801.10296.pdf
  50. https://arxiv.org/pdf/1712.02047.pdf
  51. https://arxiv.org/pdf/1805.11360.pdf
  52. https://arxiv.org/pdf/1808.08762.pdf
  53. https://github.com/helsinki-nlp/hbmp
  54. https://arxiv.org/pdf/1806.09828.pdf
  55. https://arxiv.org/pdf/1804.07983.pdf
  56. https://arxiv.org/pdf/1808.07383.pdf
  57. https://arxiv.org/pdf/1808.07383.pdf
  58. http://arxiv.org/pdf/1509.06664v1.pdf
  59. https://pdfs.semanticscholar.org/adc1/84fcb04107f95e35ea1b07ef9aad749da8d7.pdf
  60. http://arxiv.org/pdf/1605.09090v1.pdf
  61. https://arxiv.org/pdf/1605.05573v2.pdf
  62. http://arxiv.org/pdf/1607.04315.pdf
  63. http://arxiv.org/pdf/1512.08849v1.pdf
  64. http://arxiv.org/pdf/1601.06733.pdf
  65. http://arxiv.org/pdf/1601.06733.pdf
  66. http://arxiv.org/pdf/1606.01933v1.pdf
  67. http://arxiv.org/pdf/1606.01933v1.pdf
  68. http://arxiv.org/pdf/1607.04492v1.pdf
  69. https://arxiv.org/pdf/1702.03814.pdf
  70. https://www.aclweb.org/anthology/c/c16/c16-1270.pdf
  71. https://arxiv.org/pdf/1709.04348.pdf
  72. https://github.com/yichengong/densely-interactive-id136-network
  73. https://arxiv.org/pdf/1708.00107.pdf
  74. https://www.ijcai.org/proceedings/2018/0613.pdf
  75. https://arxiv.org/pdf/1804.07888.pdf
  76. https://arxiv.org/pdf/1802.05577.pdf
  77. https://arxiv.org/pdf/1801.00102.pdf
  78. https://arxiv.org/pdf/1711.04289.pdf
  79. http://arxiv.org/pdf/1609.06038v3.pdf
  80. https://github.com/lukecq1231/nli
  81. https://arxiv.org/pdf/1802.05365.pdf
  82. http://aclweb.org/anthology/p18-1091
  83. https://arxiv.org/pdf/1702.03814.pdf
  84. https://arxiv.org/pdf/1709.04348.pdf
  85. https://github.com/yichengong/densely-interactive-id136-network
  86. https://arxiv.org/pdf/1805.11360.pdf
  87. https://arxiv.org/pdf/1809.02794.pdf
  88. https://arxiv.org/pdf/1711.04289.pdf
  89. https://arxiv.org/pdf/1802.05577.pdf
  90. https://arxiv.org/pdf/1802.05365.pdf
  91. https://arxiv.org/pdf/1801.00102.pdf
  92. https://www.ijcai.org/proceedings/2018/0613.pdf
  93. http://aclweb.org/anthology/p18-1091
  94. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
  95. https://arxiv.org/pdf/1805.11360.pdf
  96. https://arxiv.org/pdf/1901.11504.pdf
  97. https://www.philips.com/c-dam/corporate/research/research-programs/data.zip
  98. https://www.nyu.edu/projects/bowman/multinli/
  99. http://www-nlp.stanford.edu/~wcmac/downloads/
 100. https://physionet.org/physiotools/mimic-code/mednli/
 101. https://www.nyu.edu/projects/bowman/xnli/
 102. mailto:bowman@nyu.edu
 103. mailto:angeli@stanford.edu
 104. http://forum.stanford.edu/visitors/directions/gates.php
 105. http://ai.stanford.edu/
 106. http://infolab.stanford.edu/
 107. https://www-csli.stanford.edu/
 108. http://stackoverflow.com/tags/stanford-nlp
 109. https://github.com/stanfordnlp/corenlp
 110. https://twitter.com/stanfordnlp
 111. https://nlp.stanford.edu/local/nlp_lunch.shtml
 112. http://nlp.stanford.edu/read/
 113. http://nlp.stanford.edu/seminar/
 114. http://ai.stanford.edu/portfolio-view/distinguished-speaker-series
 115. https://nlp.stanford.edu/javanlp/
 116. https://nlp.stanford.edu/nlp/javadoc/javanlp/
 117. https://nlp.stanford.edu/local/machines.shtml
 118. https://nlp.stanford.edu/wiki/
 119. https://nlp.stanford.edu/local/calendar.shtml
 120. https://nlp.stanford.edu/local/qa/

   hidden links:
 122. https://nlp.stanford.edu/projects/snli/
