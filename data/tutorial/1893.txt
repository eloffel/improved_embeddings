   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]gab41
     * [9]machine learning
     * [10]data science
     * [11]deep learning
     * [12]lab41 website
     __________________________________________________________________

lab41 reading group: skip-thought vectors

   [13]go to the profile of alex gude
   [14]alex gude (button) blockedunblock (button) followfollowing
   oct 31, 2016
   [1*grpmdu7yd2v3buwyqhbp3a.jpeg]

   continuing the tour of older papers that started with our [15]resnet
   blog post, we now take on [16]skip-thought vectors by [17]kiros et al.
   their goal was to come up with a useful embedding for sentences that
   was not tuned for a single task and did not require labeled data to
   train. they took inspiration from id97 skip-gram (you can find
   [18]my explanation of that algorithm here) and attempt to extend it to
   sentences.

   skip-thought vectors are created using an encoder-decoder model. the
   encoder takes in the training sentence and outputs a vector. there are
   two decoders both of which take the vector as input. the first attempts
   to predict the previous sentence and the second attempts to predict the
   next sentence. both the encoder and decoder are constructed from
   recurrent neural networks (id56). multiple encoder types are tried
   including uni-skip, bi-skip, and combine-skip. uni-skip reads the
   sentence in the forward direction. bi-skip reads the sentence forwards
   and backwards and concatenates the results. combined-skip concatenates
   the vectors from uni- and bi-skip. only minimal id121 is done to
   the input sentences. a diagram indicating the input sentence and the
   two predicted sentences is shown below.
   [1*mqxarq3bsthpn0cfoxcbag.png]
   given a sentence (the grey dots), skip-thought attempts to predict the
   preceding sentence (red dots) and the next sentence (green dots).
   figure from the paper.

   their model requires groups of sentences in order to train, and so
   trained on the bookcorpus dataset. the dataset consists of novels by
   unpublished authors and is (unsurprisingly) dominated by romance and
   fantasy novels. this    bias    in the dataset will become apparent later
   when discussing some of the sentences used to test the skip-thought
   model; some of the retrieved sentences are quite exciting!

   building a model that accounts for the meaning of an entire sentence is
   tough because language is remarkably flexible. changing a single word
   can either completely change the meaning of a sentence or leave it
   unaltered. the same is true for moving words around. as an example:

     one difficulty in building a model to handle sentences is that a
     single word can be changed and yet the meaning of the sentence is
     the same.

   put a different way:

     one challenge in building a model to handle sentences is that a
     single word can be changed and yet the meaning of the sentence is
     the same.

   changing a single word has had almost no effect on the meaning of that
   sentence. to account for these word level changes, the skip-thought
   model needs to be able to handle a large variety of words, some of
   which were not present in the training sentences. the authors solve
   this by using a pre-trained continuous bag-of-words (cbow) id97
   model and learning a translation from the id97 vectors to the word
   vectors in their sentences. below are shown the nearest neighbor words
   after the vocabulary expansion using query words that do not appear in
   the training vocabulary:
   [1*uhd6xdeztdgcwuitmb96hg.png]
   nearest neighbor words for various words that were not included in the
   training vocabulary. table from the paper.

   so how well does the model work? one way to probe it is to retrieve the
   closest sentence to a query sentence; here are some examples:

     query:    i   m sure you   ll have a glamorous evening,    she said, giving
     an exaggerated wink.

     retrieved:    i   m really glad you came to the party tonight,    he said,
     turning to her.

   and:

     query: although she could tell he hadn   t been too interested in any
     of their other chitchat, he seemed genuinely curious about this.

     retrieved: although he hadn   t been following her career with a
     microscope, he   d definitely taken notice of her appearance.

   the sentences are in fact very similar in both structure and meaning
   (and a bit salacious, as i warned earlier) so the model appears to be
   doing a good job.

   to perform more rigorous experimentation, and to test the value of
   skip-thought vectors as a generic sentence feature extractor, the
   authors run the model through a series of tasks using the encoded
   vectors with simple, linear classifiers trained on top of them.

   they find that their generic skip-thought representation performs very
   well for detecting the semantic relatedness of two sentences and for
   detecting where a sentence is id141 another one. skip-thought
   vectors perform relatively well for id162 and captioning
   (where they use [19]vgg to extract image feature vectors). skip-thought
   performs poorly for id31, producing equivalent results to
   various bag of word models but at a much higher computational cost.

   we have used skip-thought vectors a little bit at the lab, most
   recently for the [20]pythia challenge. we found them to be useful for
   novelty detection, but incredibly slow. running skip-thought vectors on
   a corpus of about 20,000 documents took many hours, where as simpler
   (and as effective) methods took seconds or minutes. i will update with
   a link to their blog post when it comes online.
     __________________________________________________________________

lab41 is a silicon valley challenge lab where experts from the u.s.
intelligence community (ic), academia, industry, and in-q-tel come together
to gain a better understanding of how to work with         and ultimately
use         big data.

learn more at [21]lab41.org and follow us on twitter: [22]@_lab41

   thanks to [23]abhinav g, [24]carrie sessine, [25]patrick callier, and
   [26]anna b.
     * [27]machine learning
     * [28]artificial intelligence
     * [29]deep learning
     * [30]neural networks
     * [31]lab41 reading group

   (button)
   (button)
   (button) 31 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of alex gude

[33]alex gude

   data scientist and physicist. loves cycling and the outdoors! i spend a
   lot of my time working on, and reading about, and writing about deep
   learning.

     (button) follow
   [34]gab41

[35]gab41

   gab41 is lab41's blog exploring data science, machine learning, and
   artificial intelligence. geek out with us!

     * (button)
       (button) 31
     * (button)
     *
     *

   [36]gab41
   never miss a story from gab41, when you sign up for medium. [37]learn
   more
   never miss a story from gab41
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://gab41.lab41.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fec68c05aa92
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://gab41.lab41.org/lab41-reading-group-skip-thought-vectors-fec68c05aa92&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://gab41.lab41.org/lab41-reading-group-skip-thought-vectors-fec68c05aa92&source=--------------------------nav_reg&operation=register
   8. https://gab41.lab41.org/?source=logo-lo_ylqdjqvaqsjj---b9e75b66ab60
   9. https://gab41.lab41.org/tagged/machine-learning
  10. https://gab41.lab41.org/tagged/data-science
  11. https://gab41.lab41.org/tagged/deep-learning
  12. http://www.lab41.org/
  13. https://gab41.lab41.org/@alex.gude?source=post_header_lockup
  14. https://gab41.lab41.org/@alex.gude
  15. https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f#.bc3hiquop
  16. https://arxiv.org/abs/1506.06726
  17. http://www.cs.toronto.edu/~rkiros/
  18. https://gab41.lab41.org/python2vec-word-embeddings-for-source-code-3d14d030fe8f#.e301tsg77
  19. https://arxiv.org/pdf/1409.1556.pdf
  20. https://gab41.lab41.org/tell-me-something-i-dont-know-detecting-novelty-and-redundancy-with-natural-language-processing-818124e4013c#.6xf8nejr9
  21. http://www.lab41.org/
  22. https://twitter.com/_lab41
  23. https://medium.com/@abhig90?source=post_page
  24. https://medium.com/@carriesessine_6273?source=post_page
  25. https://medium.com/@patrc?source=post_page
  26. https://medium.com/@anna.beth?source=post_page
  27. https://gab41.lab41.org/tagged/machine-learning?source=post
  28. https://gab41.lab41.org/tagged/artificial-intelligence?source=post
  29. https://gab41.lab41.org/tagged/deep-learning?source=post
  30. https://gab41.lab41.org/tagged/neural-networks?source=post
  31. https://gab41.lab41.org/tagged/lab41-reading-group?source=post
  32. https://gab41.lab41.org/@alex.gude?source=footer_card
  33. https://gab41.lab41.org/@alex.gude
  34. https://gab41.lab41.org/?source=footer_card
  35. https://gab41.lab41.org/?source=footer_card
  36. https://gab41.lab41.org/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/fec68c05aa92/share/twitter
  40. https://medium.com/p/fec68c05aa92/share/facebook
  41. https://medium.com/p/fec68c05aa92/share/twitter
  42. https://medium.com/p/fec68c05aa92/share/facebook
