   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    deep
   learning reading group: skip-thought vectors comments feed [5]why fluid
   intelligence makes you a better data scientist [6]kdd 2016: watch talks
   by top data science researchers

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]nov    [29]tutorials,
   overviews    deep learning reading group: skip-thought vectors
   ( [30]16:n42 )

deep learning reading group: skip-thought vectors

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 48
   tags: [33]deep learning, [34]lab41, [35]natural language processing,
   [36]neural networks, [37]id97

   skip-thought vectors take inspiration from id97 skip-gram and
   attempt to extend it to sentences, and are created using an
   encoder-decoder model. read on for an overview of the paper.
     __________________________________________________________________

   by [38]alexander gude, intuit.

   books header

   continuing the tour of older papers that started with our [39]resnet
   blog post, we now take on [40]skip-thought vectors by [41]kiros et al.
   their goal was to come up with a useful embedding for sentences that
   was not tuned for a single task and did not require labeled data to
   train. they took inspiration from id97 skip-gram (you can
   find [42]my explanation of that algorithm here) and attempt to extend
   it to sentences.

   skip-thought vectors are created using an encoder-decoder model. the
   encoder takes in the training sentence and outputs a vector. there are
   two decoders both of which take the vector as input. the first attempts
   to predict the previous sentence and the second attempts to predict the
   next sentence. both the encoder and decoder are constructed from
   recurrent neural networks (id56). multiple encoder types are tried
   including uni-skip, bi-skip, and combine-skip. uni-skip reads the
   sentence in the forward direction. bi-skip reads the sentence forwards
   and backwards and concatenates the results. combined-skip concatenates
   the vectors from uni- and bi-skip. only minimal id121 is done to
   the input sentences. a diagram indicating the input sentence and the
   two predicted sentences is shown below.

   given a sentence (the grey dots), skip-thought attempts to predict the
      preceding sentence (red dots) and the next sentence (green dots).
                           figure from the paper.

   their model requires groups of sentences in order to train, and so
   trained on the bookcorpus dataset. the dataset consists of novels by
   unpublished authors and is (unsurprisingly) dominated by romance and
   fantasy novels. this    bias    in the dataset will become apparent later
   when discussing some of the sentences used to test the skip-thought
   model; some of the retrieved sentences are quite exciting!

   building a model that accounts for the meaning of an entire sentence is
   tough because language is remarkably flexible. changing a single word
   can either completely change the meaning of a sentence or leave it
   unaltered. the same is true for moving words around. as an example:

     one difficulty in building a model to handle sentences is that a
     single word can be changed and yet the meaning of the sentence is
     the same.

   put a different way:

     one challenge in building a model to handle sentences is that a
     single word can be changed and yet the meaning of the sentence is
     the same.

   changing a single word has had almost no effect on the meaning of that
   sentence. to account for these word level changes, the skip-thought
   model needs to be able to handle a large variety of words, some of
   which were not present in the training sentences. the authors solve
   this by using a pre-trained continuous bag-of-words (cbow) id97
   model and learning a translation from the id97 vectors to the word
   vectors in their sentences. below are shown the nearest neighbor words
   after the vocabulary expansion using query words that do not appear in
   the training vocabulary:

   nearest neighbor words for various words that were not included in the
                 training vocabulary. table from the paper.

   so how well does the model work? one way to probe it is to retrieve the
   closest sentence to a query sentence; here are some examples:

     query:    i   m sure you   ll have a glamorous evening,    she said, giving
     an exaggerated wink.

     retrieved:    i   m really glad you came to the party tonight,    he said,
     turning to her.

   and:

     query: although she could tell he hadn   t been too interested in any
     of their other chitchat, he seemed genuinely curious about this.

     retrieved: although he hadn   t been following her career with a
     microscope, he   d definitely taken notice of her appearance.

   the sentences are in fact very similar in both structure and meaning
   (and a bit salacious, as i warned earlier) so the model appears to be
   doing a good job.

   to perform more rigorous experimentation, and to test the value of
   skip-thought vectors as a generic sentence feature extractor, the
   authors run the model through a series of tasks using the encoded
   vectors with simple, linear classifiers trained on top of them.

   they find that their generic skip-thought representation performs very
   well for detecting the semantic relatedness of two sentences and for
   detecting where a sentence is id141 another one. skip-thought
   vectors perform relatively well for id162 and captioning
   (where they use [43]vgg to extract image feature vectors). skip-thought
   performs poorly for id31, producing equivalent results to
   various bag of word models but at a much higher computational cost.

   we have used skip-thought vectors a little bit at the lab, most
   recently for the [44]pythia challenge. we found them to be useful for
   novelty detection, but incredibly slow. running skip-thought vectors on
   a corpus of about 20,000 documents took many hours, where as simpler
   (and as effective) methods took seconds or minutes. i will update with
   a link to their blog post when it comes online.

   [45]alexander gude is currently a data scientist at lab41 working on
   investigating recommender system algorithms. he holds a ba in physics
   from university of california, berkeley, and a phd in elementary
   particle physics from university of minnesota-twin cities.

   [46]lab41 is a    challenge lab    where the u.s. intelligence community
   comes together with their counterparts in academia, industry, and
   in-q-tel to tackle big data. it allows participants from diverse
   backgrounds to gain access to ideas, talent, and technology to explore
   what works and what doesn   t in data analytics. an open, collaborative
   environment, lab41 fosters valuable relationships between participants.

   [47]original. reposted with permission.

   related:
     * [48]in deep learning, architecture engineering is the new feature
       engineering
     * [49]up to speed on deep learning: july update
     * [50]why do deep learning networks scale?
     __________________________________________________________________

   [51][prv.gif] previous post
   [52]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [53]another 10 free must-read books for machine learning and data
       science
    2. [54]9 must-have skills you need to become a data scientist, updated
    3. [55]who is a typical data scientist in 2019?
    4. [56]the pareto principle for data scientists
    5. [57]what no one will tell you about data science job applications
    6. [58]19 inspiring women in ai, big data, data science, machine
       learning
    7. [59]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [60]id158s optimization using genetic algorithm
       with python
    2. [61]who is a typical data scientist in 2019?
    3. [62]8 reasons why you should get a microsoft azure certification
    4. [63]the pareto principle for data scientists
    5. [64]r vs python for data visualization
    6. [65]how to work in data science, ai, big data
    7. [66]the deep learning toolset     an overview

[67]latest news

     * [68]download your datax guide to ai in marketing
     * [69]kdnuggets offer: save 20% on strata in london
     * [70]training a champion: building deep neural nets for big ...
     * [71]building a recommender system
     * [72]predict age and gender using convolutional neural netwo...
     * [73]top tweets, mar 27     apr 02: here is a great ex...

   [74]kdnuggets home    [75]news    [76]2016    [77]nov    [78]tutorials,
   overviews    deep learning reading group: skip-thought vectors
   ( [79]16:n42 )
      2019 kdnuggets. [80]about kdnuggets.  [81]privacy policy. [82]terms
   of service

   [83]subscribe to kdnuggets news
   [84][tw_c48.png] [85]facebook [86]linkedin
   x

   [envelope.png] [87]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/11/deep-learning-group-skip-thought-vectors.html/feed
   5. https://www.kdnuggets.com/2016/11/fluid-intelligence-makes-better-data-scientist.html
   6. https://www.kdnuggets.com/2016/11/kdd-2016-watch-data-science-researchers.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/11/index.html
  29. https://www.kdnuggets.com/2016/11/tutorials.html
  30. https://www.kdnuggets.com/2016/n42.html
  31. https://www.kdnuggets.com/2016/11/fluid-intelligence-makes-better-data-scientist.html
  32. https://www.kdnuggets.com/2016/11/kdd-2016-watch-data-science-researchers.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/lab41
  35. https://www.kdnuggets.com/tag/natural-language-processing
  36. https://www.kdnuggets.com/tag/neural-networks
  37. https://www.kdnuggets.com/tag/id97
  38. https://www.kdnuggets.com/author/alex-gude
  39. https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f#.bc3hiquop
  40. https://arxiv.org/abs/1506.06726
  41. http://www.cs.toronto.edu/~rkiros/
  42. https://gab41.lab41.org/python2vec-word-embeddings-for-source-code-3d14d030fe8f#.e301tsg77
  43. https://arxiv.org/pdf/1409.1556.pdf
  44. https://gab41.lab41.org/tell-me-something-i-dont-know-detecting-novelty-and-redundancy-with-natural-language-processing-818124e4013c#.6xf8nejr9
  45. https://twitter.com/alex_gude
  46. http://www.lab41.org/
  47. https://gab41.lab41.org/lab41-reading-group-skip-thought-vectors-fec68c05aa92
  48. https://www.kdnuggets.com/2016/07/deep-learning-architecture-engineering-feature-engineering.html
  49. https://www.kdnuggets.com/2016/08/deep-learning-july-update.html
  50. https://www.kdnuggets.com/2016/07/deep-learning-networks-scale.html
  51. https://www.kdnuggets.com/2016/11/fluid-intelligence-makes-better-data-scientist.html
  52. https://www.kdnuggets.com/2016/11/kdd-2016-watch-data-science-researchers.html
  53. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  54. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  55. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  56. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  57. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  58. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  59. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  60. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  61. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  62. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  63. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  64. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  65. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  66. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  67. https://www.kdnuggets.com/news/index.html
  68. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  69. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  70. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  71. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  72. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  73. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  74. https://www.kdnuggets.com/
  75. https://www.kdnuggets.com/news/index.html
  76. https://www.kdnuggets.com/2016/index.html
  77. https://www.kdnuggets.com/2016/11/index.html
  78. https://www.kdnuggets.com/2016/11/tutorials.html
  79. https://www.kdnuggets.com/2016/n42.html
  80. https://www.kdnuggets.com/about/index.html
  81. https://www.kdnuggets.com/news/privacy-policy.html
  82. https://www.kdnuggets.com/terms-of-service.html
  83. https://www.kdnuggets.com/news/subscribe.html
  84. https://twitter.com/kdnuggets
  85. https://facebook.com/kdnuggets
  86. https://www.linkedin.com/groups/54257
  87. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  89. https://www.kdnuggets.com/
  90. https://www.kdnuggets.com/
