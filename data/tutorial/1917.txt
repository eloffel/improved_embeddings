   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    the
   gentlest introduction to tensorflow     part 3 comments feed [5]data
   scientists strongly oppose trump immigration ban [6]chief analytics
   officer, spring: may 2-4 2017, scottsdale, az     kdnuggets offer

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]feb    [29]tutorials,
   overviews    the gentlest introduction to tensorflow     part 3
   ( [30]17:n07 )

the gentlest introduction to tensorflow     part 3

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 124
   tags: [33]beginners, [34]deep learning, [35]machine learning,
   [36]neural networks, [37]regression, [38]tensorflow

   this post is the third entry in a series dedicated to introducing
   newcomers to tensorflow in the gentlest possible manner. this entry
   progresses to multi-feature id75.
     __________________________________________________________________

   by soon hin khor, co-organizer for tokyo tensorflow meetup.

     editor's note: you may want to check out [39]part 1 and [40]part 2
     of this tutorial
     before proceeding.

quick review


   the premise of the previous articles was: given any house size (square
   meters/sqm), which is the feature, we want to predict the house price
   ($), the outcome. to do that we:
    1. we find a straight line (id75) that    best-fits    the
       data points that we have. the    best-fit    is when the linear
       regression line ensures that the difference between the actual data
       points (gray dots) and the predicted values (gray dots interpolated
       on to the straight line), which, in other words, is the sum of
       multiple blue lines, is minimized.
    2. with this straight line we can predict any value of house

                       [1*bfzp8usmk88mdlibu465va.png]
             predicting using single-feature id75.

multi-feature id75 overview


   in reality, any prediction relies on multiple features, so we advance
   from single-feature to 2-feature id75; we chose 2 features
   to keep visualization and comprehension simple, but the concept
   generalizes to any number of features.

   we introduce a new feature,    rooms    (number of units in the house).
   when collecting datapoints, we must now collect values for the new
   feature    rooms    on top of the existing feature    house size   , as well as
   the corresponding outcome    house price   .
   our chart becomes 3-dimensional.

                       [1*lxq18pykj8yroakvopiwpa.png]
datapoints for the outcome    house price    and its 2-feature (   rooms    &
                             house size   ) space.

   our goal then becomes predicting    house price   , given    rooms   , and
      house size    (see image below).

                       [1*2rdzz3vo5xmhmwnkfj7zuq.png]
      prediction for a given 2-feature sometimes cannot be done due to
                           missing of datapoints.

   in the single-feature scenario, we had to use id75 to
   create a straight line to help us predict the outcome    house size   , for
   cases where we did not have datapoints. in a 2-feature scenario, we can
   also employ id75, but to create a plane (instead of a
   straight line) to help us predict (see image below).

                       [1*bjoya1hxbjiooesfqedv2g.png]
     using id75 on 2-feature space to create a plane to do
                                 prediction.

multi-feature id75 model


   recall for a single-feature (see left of image below), the linear
   regression model outcome (y) has a weight (w), a placeholder (x) for
   the    house size    feature, and a bias (b).

   for 2-feature (see right of image below), we introduce another weight,
   which we call w2, and another placeholder, x2 to hold the    rooms   
   feature value.

                       [1*byo4_urqnnuunqoaesyzrw.png]
            1-feature vs. 2-feature id75 equations.

   when we perform id75, id119 helps us learn the
   additional weight w2, on top of the learning w, b as previously
   discussed.

multi-feature id75 in tensorflow


   quick review
   our tf code for single-feature id75 consists of 3 parts
   (see image below):
     * constructing the model (blue part)
     * constructing the cost function based on the model (red part)
     * minimizing the cost function using id119 (green part)

                       [1*op-h6wxbxdhux3irs7owxg.png]
              tensorflow code for 1-feature id75.

   tensorflow for 2-feature id75

   the change to support 2-feature id75 equation (explained
   above) in tf code is shown in red.

                       [1*kwtzn5h59xfmf-t9fminbg.png]

   note this way of adding new features is inefficient; as the number of
   features grow, the number of required variables and placeholders
   increases. in reality models have many more features, which worsens
   this problem. how can we represent features efficiently?

matrices to the rescue


   first, let us generalize representing a 2-feature model to an n-feature
   one:

   [1*vlf5-hzha1qbc_tzsrhhga.png]

   it turns out that the complex n-feature formula can be simplified in
   the world of matrices, and matrices are in-built into tf for these
   reasons:
     * data can be represented in multi-dimensions, which fits the way we
       want to represent a datapoint with n features (below left, also
       known as the feature matrix) and a model with n weights (below
       right, also known as the weight matrix)

                       [1*si4puxpdqdm2gj-gyqgx7a.png]
   1 datapoint   s n features and the model   s n weights in matrix form.

   in tf, they would be written as:
   x = tf.placeholder(tf.float, [1,n])

   w = tf.variable(tf.zeros[n,1])

   note: for w we use tf.zeros, which initializes all w1, w2, ..., wn to
   zeros.
     * mathematically id127 is a sum of multiplications
       (just accept this as part of mathematics); thus naturally the
       id127 between the features (the one in the middle)
       and weights (the one on the right) matrices gives you the outcome
       (the one on the left), which is equivalent to first part of the
       n-feature id75 formula (described above), i.e.,
       without the biases

                       [1*bo_qw7rn9lpkvid19zbjtug.png]
    id127 between features and weights matrices gives the
                       outcome (without biases added).

   in tf, this multiplication would be:
   y = tf.matmul(x, w)

     * id127 between a multi-row feature matrix (each row
       representing a datapoint   s n features), returns multi-row outcomes
       (each row representing the outcome/prediction (without bias added)
       of each datapoint); thus a single id127 can apply
       the id75 formula to multiple datapoints to produce
       multiple predictions, one for each datapoints, at a single go (see
       below)!

   note: the x representations in the feature matrix become more complex,
   i.e., we use x1.1, x1.2, instead of x1, x2, etc. because the feature
   matrix (the one in the middle) has expanded from representing a single
   datapoint of n-features (1 row x n columns) to representing m
   datapoints with n-features (m rows x n columns), so we extended x<n>,
   e.g., x1, to x<m>.<n>, e.g., x1.1, where n is the feature number and m
   is the datapoint number.

                       [1*wwbffty2iru4hrwwoot9tw.png]
   multiple row id127 with model weights produce multiple
                            row matrix outcomes.

   in tf, they would be written as:
   x = tf.placeholder(tf.float, [m, n])

   w = tf.variable(tf.zeros[n,1])

   y = tf.matmul(x, w)

     * finally, adding a constant to the outcome matrix results in the
       constant being added to every row in the matrix

   in tf, with our x, and w represented in matrices, regardless of the
   number of features our model has or the number of datapoints we want to
   handle, it can be simplified to:
   b = tf.variable(tf.zeros[1])

   y = tf.matmul(x, w) + b

tensorflow multi-feature cheatsheet


   we do a side-by-side comparison to summarize the change from single to
   multi-feature id75:

                       [1*_-q1yqtvgeoz_up3vikzew.png]
        1-feature vs n-feature id75 model in tensorflow.

wrapping up


   we illustrated the concept of multi-feature id75, and
   showed how we extend our model and tf code from single to 2-feature
   id75 models, which is generalizable to n-feature models.
   we conclude by presenting a cheatsheet for multi-feature tf linear
   regression model.

   coming up next

   we will present the concepts of id28, cross-id178, and
   softmax, which will enable us to fully understand tensorflow   s
   [41]official beginner   s tutorial on mnist.

references

     * github: tf for multi-feature id75 [42]without matrices
     * github: tf for multi-feature id75 [43]with matrices
     * the slides on [44]slideshare (1   43)
     * the video on [45]youtube (0:00 to 7:18)

   bio: [46]soon hin khor, ph.d is using tech to make the world more
   caring, and responsible. contributor of ruby-tensorflow. co-organizer
   for tokyo tensorflow meetup.

   [47]original. reposted with permission.

   related:
     * [48]the good, bad & ugly of tensorflow
     * [49]the gentlest introduction to tensorflow     part 1
     * [50]the gentlest introduction to tensorflow     part 2
     __________________________________________________________________

   [51][prv.gif] previous post
   [52]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [53]another 10 free must-read books for machine learning and data
       science
    2. [54]9 must-have skills you need to become a data scientist, updated
    3. [55]who is a typical data scientist in 2019?
    4. [56]the pareto principle for data scientists
    5. [57]what no one will tell you about data science job applications
    6. [58]19 inspiring women in ai, big data, data science, machine
       learning
    7. [59]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [60]id158s optimization using genetic algorithm
       with python
    2. [61]who is a typical data scientist in 2019?
    3. [62]8 reasons why you should get a microsoft azure certification
    4. [63]the pareto principle for data scientists
    5. [64]r vs python for data visualization
    6. [65]how to work in data science, ai, big data
    7. [66]the deep learning toolset     an overview

[67]latest news

     * [68]statistical thinking for industrial problem solving (st...
     * [69]from business intelligence to machine intelligence
     * [70]what is missing when ai makes a decision?
     * [71]spatio-temporal statistics: a primer
     * [72]another 10 free must-see courses for machine learning a...
     * [73]download your datax guide to ai in marketing

   [74]kdnuggets home    [75]news    [76]2017    [77]feb    [78]tutorials,
   overviews    the gentlest introduction to tensorflow     part 3
   ( [79]17:n07 )
      2019 kdnuggets. [80]about kdnuggets.  [81]privacy policy. [82]terms
   of service

   [83]subscribe to kdnuggets news
   [84][tw_c48.png] [85]facebook [86]linkedin
   x

   [envelope.png] [87]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-3.html/feed
   5. https://www.kdnuggets.com/2017/02/poll-data-scientists-oppose-trump-immigration-ban.html
   6. https://www.kdnuggets.com/2017/02/corinium-chief-analytics-officer-spring-may-scottsdale.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/02/index.html
  29. https://www.kdnuggets.com/2017/02/tutorials.html
  30. https://www.kdnuggets.com/2017/n07.html
  31. https://www.kdnuggets.com/2017/02/poll-data-scientists-oppose-trump-immigration-ban.html
  32. https://www.kdnuggets.com/2017/02/corinium-chief-analytics-officer-spring-may-scottsdale.html
  33. https://www.kdnuggets.com/tag/beginners
  34. https://www.kdnuggets.com/tag/deep-learning
  35. https://www.kdnuggets.com/tag/machine-learning
  36. https://www.kdnuggets.com/tag/neural-networks
  37. https://www.kdnuggets.com/tag/regression
  38. https://www.kdnuggets.com/tag/tensorflow
  39. https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-1.html
  40. https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html
  41. https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html
  42. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_multi_feature_using_mini_batch_without_matrix_with_tensorboard.py
  43. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_multi_feature_using_mini_batch_with_tensorboard.py
  44. http://www.slideshare.net/khorsoonhin/gentlest-introduction-to-tensorflow-part-3
  45. https://www.youtube.com/watch?v=f8g_6txklxw
  46. https://twitter.com/neth_6
  47. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c#.b9sl09vvq
  48. https://www.kdnuggets.com/2016/05/good-bad-ugly-tensorflow.html
  49. https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-1.html
  50. https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html
  51. https://www.kdnuggets.com/2017/02/poll-data-scientists-oppose-trump-immigration-ban.html
  52. https://www.kdnuggets.com/2017/02/corinium-chief-analytics-officer-spring-may-scottsdale.html
  53. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  54. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  55. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  56. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  57. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  58. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  59. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  60. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  61. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  62. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  63. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  64. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  65. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  66. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  67. https://www.kdnuggets.com/news/index.html
  68. https://www.kdnuggets.com/2019/04/jmp-statistical-thinking-free-online-course.html
  69. https://www.kdnuggets.com/2019/04/datarobot-from-business-intelligence-machine-intelligence.html
  70. https://www.kdnuggets.com/2019/04/ai-makes-decision.html
  71. https://www.kdnuggets.com/2019/04/spatio-temporal-statistics-primer.html
  72. https://www.kdnuggets.com/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html
  73. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  74. https://www.kdnuggets.com/
  75. https://www.kdnuggets.com/news/index.html
  76. https://www.kdnuggets.com/2017/index.html
  77. https://www.kdnuggets.com/2017/02/index.html
  78. https://www.kdnuggets.com/2017/02/tutorials.html
  79. https://www.kdnuggets.com/2017/n07.html
  80. https://www.kdnuggets.com/about/index.html
  81. https://www.kdnuggets.com/news/privacy-policy.html
  82. https://www.kdnuggets.com/terms-of-service.html
  83. https://www.kdnuggets.com/news/subscribe.html
  84. https://twitter.com/kdnuggets
  85. https://facebook.com/kdnuggets
  86. https://www.linkedin.com/groups/54257
  87. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  89. https://www.kdnuggets.com/
  90. https://www.kdnuggets.com/
