
[1]foldl

     * [2]home
     * [3]blog
     * [4]contact
     * [5]feed

kneser-ney smoothing explained

   18 january 2014

   language models are an essential element of natural language
   processing, central to tasks ranging from spellchecking to machine
   translation. given an arbitrary piece of text, a language model
   determines whether that text belongs to a given language.

   we can give a concrete example with a probabilistic language model, a
   specific construction which uses probabilities to estimate how likely
   any given string belongs to a language. consider a probabilistic
   english language model \( p_e \). we would expect the id203

   \[p_e(\text{i went to the store})\]

   to be quite high, since we can confirm this is valid english. on the
   other hand, we expect the probabilities

   \[p_e(\text{store went to i the}), p_e(\text{ich habe eine katz})\]

   to be very low, since these fragments do not constitute proper english
   text.

   i don   t aim to cover the entirety of language models at the moment    
   that would be an ambitious task for a single blog post. if you haven   t
   encountered language models or id165s before, i recommend the
   following resources:
     * [6]   language model    on wikipedia
     * chapter 4 of jurafsky and martin   s [7]speech and language
       processing
     * chapter 7 of [8]id151 (see [9]summary
       slides online)

   i   d like to jump ahead to a trickier subject within id38
   known as kneser-ney smoothing. this smoothing method is most commonly
   applied in an interpolated form,^[10]1 and this is the form that i   ll
   present today.

   kneser-ney evolved from absolute-discounting interpolation, which makes
   use of both higher-order (i.e., higher-n) and lower-order language
   models, reallocating some id203 mass from 4-grams or 3-grams to
   simpler unigram models. the formula for absolute-discounting smoothing
   as applied to a bigram language model is presented below:

   \[p_{abs}(w_i \mid w_{i-1}) = \dfrac{\max(c(w_{i-1} w_i) - \delta,
   0)}{\sum_{w'} c(w_{i-1} w')} + \alpha\; p_{abs}(w_i)\]

   here \(\delta\) refers to a fixed discount value, and \(\alpha\) is a
   normalizing constant. the details of this smoothing are covered in
   [11]chen and goodman (1999).

   the essence of kneser-ney is in the clever observation that we can take
   advantage of this interpolation as a sort of backoff model. when the
   first term (in this case, the discounted relative bigram count) is near
   zero, the second term (the lower-order model) carries more weight.
   inversely, when the higher-order model matches strongly, the second
   lower-order term has little weight.

   the kneser-ney design retains the first term of absolute discounting
   interpolation, but rewrites the second term to take advantage of this
   relationship. whereas absolute discounting interpolation in a bigram
   model would simply default to a unigram model in the second term,
   kneser-ney depends upon the idea of a continuation id203
   associated with each unigram.

   this id203 for a given token \(w_i\) is proportional to the
   number of bigrams which it completes:

   \[p_{\text{continuation}}(w_i) \propto \: \left| \{ w_{i-1} :
   c(w_{i-1}, w_i) > 0 \} \right|\]

   this quantity is normalized by dividing by the total number of bigram
   types (note that \(j\) is a free variable):

   \[p_{\text{continuation}}(w_i) = \dfrac{\left| \{ w_{i-1} : c(w_{i-1},
   w_i) > 0 \} \right|}{\left| \{ w_{j-1} : c(w_{j-1},w_j) > 0\}
   \right|}\]

   the common example used to demonstrate the efficacy of kneser-ney is
   the phrase san francisco. suppose this phrase is abundant in a given
   training corpus. then the unigram id203 of francisco will also be
   high. if we unwisely use something like absolute discounting
   interpolation in a context where our bigram model is weak, the unigram
   model portion may take over and lead to some strange results.

   [12]dan jurafsky gives the following example context:

     i can   t see without my reading _____.

   a fluent english speaker reading this sentence knows that the word
   glasses should fill in the blank. but since san francisco is a common
   term, absolute-discounting interpolation might declare that francisco
   is a better fit: \(p_{abs}(\text{francisco}) >
   p_{abs}(\text{glasses})\).

   kneser-ney fixes this problem by asking a slightly harder question of
   our lower-order model. whereas the unigram model simply provides how
   likely a word \(w_i\) is to appear, kneser-ney   s second term determines
   how likely a word \(w_i\) is to appear in an unfamiliar bigram context.

   kneser-ney in whole follows:
   \[p_{\mathit{kn}}(w_i \mid w_{i-1}) = \dfrac{\max(c(w_{i-1} w_i) -
   \delta, 0)}{\sum_{w'} c(w_{i-1} w')} + \lambda \dfrac{\left| \{ w_{i-1}
   : c(w_{i-1}, w_i) > 0 \} \right|}{\left| \{ w_{j-1} : c(w_{j-1},w_j) >
   0\} \right|}\]

   \(\lambda\) is a normalizing constant
   \[\lambda(w_{i-1}) = \dfrac{\delta}{c(w_{i-1})} \left| \{w' :
   c(w_{i-1}, w') > 0\} \right|.\]

   note that the denominator of the first term can be simplified to a
   unigram count. here is the final interpolated kneser-ney smoothed
   bigram model, in all its glory:
   \[p_{\mathit{kn}}(w_i \mid w_{i-1}) = \dfrac{\max(c(w_{i-1} w_i) -
   \delta, 0)}{c(w_{i-1})} + \lambda \dfrac{\left| \{ w_{i-1} : c(w_{i-1},
   w_i) > 0 \} \right|}{\left| \{ w_{j-1} : c(w_{j-1},w_j) > 0\}
   \right|}\]

further reading

   if you enjoyed this post, here is some further reading on kneser-ney
   and other smoothing methods:
     * bill maccartney   s [13]smoothing tutorial (very accessible)
     * [14]chen and goodman (1999)
     * section 4.9.1 in jurafsky and martin   s [15]speech and language
       processing

    1. for the canonical definition of interpolated kneser-ney smoothing,
       see s. f. chen and j. goodman, [16]   an empirical study of smoothing
       techniques for id38,    computer speech and language,
       vol. 13, no. 4, pp. 359   394, 1999. [17]   

   please enable javascript to view the [18]comments powered by disqus.
   [19]jon gauthier     cambridge, massachusetts

references

   1. http://www.foldl.me/
   2. http://www.foldl.me/
   3. http://www.foldl.me/posts
   4. http://www.foldl.me/contact
   5. http://feeds.feedburner.com/foldl/rss
   6. http://en.wikipedia.org/wiki/language_model
   7. http://www.amazon.com/gp/product/0131873210/ref=as_li_qf_sp_asin_tl?ie=utf8&camp=1789&creative=9325&creativeasin=0131873210&linkcode=as2&tag=blog0cbb-20
   8. http://www.amazon.com/gp/product/0521874157/ref=as_li_tf_tl?ie=utf8&camp=1789&creative=9325&creativeasin=0521874157&linkcode=as2&tag=blog0cbb-20
   9. http://www.statmt.org/book/slides/07-language-models.pdf
  10. http://www.foldl.me/2014/kneser-ney-smoothing/#fn:1
  11. http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf
  12. https://www.youtube.com/watch?v=wtb00eczocm
  13. http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf
  14. http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf
  15. http://www.amazon.com/gp/product/0131873210/ref=as_li_qf_sp_asin_tl?ie=utf8&camp=1789&creative=9325&creativeasin=0131873210&linkcode=as2&tag=blog0cbb-20
  16. http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf
  17. http://www.foldl.me/2014/kneser-ney-smoothing/#fnref:1
  18. http://disqus.com/?ref_noscript
  19. http://www.foldl.me/contact/
