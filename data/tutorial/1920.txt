   #[1]quora

   [2]quora
   ____________________

   sign in
   beyond expectations
   an intuitive explanation of good-turing smoothing
   [3]jonathan uesato
   i spent a good part of today working through bell's paper introducing
   simple good-turing smoothing, [4]good-turing smoothing without tears.
   unfortunately, that's still 20 pages of academic writing to wade
   through, so this post is the more simplified "good-turing smoothing
   without tears" without tears.

what is smoothing?

   in statistics, smoothing a data set refers to using an approximating
   function rather than raw data in an attempt to "smooth out" the noise
   associated with using observations directly. to explain this in more
   detail, i'll explain the concept using our current context of the
   question: "how likely is a specific word sequence?"
   one crucial component of many natural language processing (nlp)
   applications is developing a [5]language model. in a bayesian sense, a
   language model can be thought of as a set of priors for how likely
   different words, or word sequences (id165s) are. for example, in
   id103, these priors allow the algorithm to prefer "dear
   nancy" over "deer nancy" although "dear" and "deer" are homophones, and
   indistinguishable based on sound alone.
   dealing with sparse data
   the naive approach to this problem is to take a large corpus of text,
   and simply look at how often different patterns occur. for example,
   "the" might occur in 10k out of 10m words, so we might infer that the
   prior id203 of the word 'the' is .1%. similarly, "the tiger"
   might occur in 20 out of 10m bigrams, "the tiger ate" might occur in 1
   out of 10m trigrams and so on. note that with a larger and larger space
   (trigrams as opposed to unigrams), the data is more and more sparse.
   thus, if we were to imagine collecting a different corpus of 10m words,
   the trigram "the tiger ate" could easily occur 0 or 2, rather than 1
   time, which would lead us to predict that the id203 of this
   trigram is [math]0[/math], [math]10^{-7}[/math], or
   [math]2*10^{-7}[/math] - very different estimates! we see that as the
   data becomes sparser, our id203 estimates become less reliable.
   one particularly egregious flaw is that many sequences like "the tiger
   waits" or "the tiger runs" might not occur in our corpus. despite
   constituting perfectly valid english, these trigrams would be assigned
   prior id203 0, clearly disastrous for a bayesian model.
   we want to assign some id203 mass to the case where a new event
   occurs, when we encounter a new word sequence. this is the intuition
   behind smoothing.
   a first attempt at smoothing
   a first approach might be add-one smoothing. drawing intuition from
   [6]laplace's law of succession, one might estimate the id203 of
   an event not simply as [observed count]/[total observations], but could
   add 1 to each observed count, and add 1 to the total observations for
   each separate class. so if c is the total number of classes, the
   id203 of a particular class is [observed count+1]/[total
   observations+c].
   (the expected likelihood estimator (ele), referenced in bell's paper,
   which adds 1/2 rather than 1 is very similar, the difference arises
   because laplace's law of succession assumes a uniform prior while the
   ele assumes the uninformative prior.)
   while the add-one approach may work well when dealing with a few
   classes, this approach runs into trouble when we have many possible.
   imagine in our corpus of 10m words, if there are 20k distinct words,
   there are [math]20000^3 = 8*10^12[/math] possible trigrams, of which
   <.01% occur in our original corpus. if we were to use add-one smoothing
   here, we would assign over 99.99% of the id203 mass to trigrams
   we have never seen!
   clearly, this approach will not work here. there are many other
   approaches to smoothing, including the add-lambda approach (based on
   cross-validation), witten-bell, and backoff models. here we will focus
   on an approach called good-turing, which builds on the intuition that
   we can estimate the id203 of novel events based on the number of
   classes which are only observed once (singletons).

turing estimators

   let's approach this problem from a different angle. if we want to
   estimate the id203 of a class occuring [math]r[/math] times,
   let's imagine: if we were to make a new observation now, what would be
   the id203 it belongs to a class we have already observed
   [math]r[/math] times. we approximate this answer with another question:
   if i had seen [math]n-1[/math] of the data points already, and had to
   predict the number of occurrences of the last class, how likely would
   it be for it to have already occurred exactly [math]r[/math] times?
   this is the justification through leave-out-one cross-validation, and i
   think is based on a similar intuition to id64.   we can answer
   this approximation easily: out of our [math]n[/math] simulations, if we
   observe that our held-out data point belongs to a class which occurred
   [math]r[/math] times in the training set of the remaining
   [math]n-1[/math] points, then this class must occur [math]r+1[/math]
   times in total. if we let [math]n_r[/math] denote the "frequency of
   frequencies" - the number of classes which occur [math]r[/math] times -
   then in the future, the id203 of observing any class which has
   occurred [math]r[/math] times is [math](r+1)\frac{n_{r+1}}{n}[/math].
   of particular interest, this yields that we allot id203 of
   [math]n_1/n[/math] to novel events, rather than 0 as before. the
   intuition is the same - that singletons in the full-set are simply the
   novel events in hold-out-one cross-validation - but it also coheres
   with the intuition that we expect to see few singletons if the number
   of possible class is small (relative to the number of data points)
   since then we would keep coming back to the same classes.  to assign
   probabilities to specific classes, since there are [math]n_r[/math]
   classes that occur [math]r[/math] times, the id203 of observing a
   class that occurred [math]r[/math] times is
   [math](r+1)\frac{n_{r+1}}{n_rn}[/math]. good-turing often deals with
   "adjusted counts" rather than probabilities, so for a class that we
   observed [math]r[/math] times, we would use a count of
   [math](r+1)\frac{n_{r+1}}{n_r}[/math].
   this formula for expected counts is the turing estimator, and it
   provides a (loosely) motivated method for estimating class
   probabilities which assigns non-zero id203 to novel events.

smoothing the turing estimator

   in answering the question "how likely is a class which has been
   observed [math]r[/math] times to occur again?" in the previous section,
   we implicitly made an assumption that our observed counts are good
   estimators. here, sparsity strikes again! while our observed counts for
   [math]n_1[/math] and [math]n_2[/math] are likely fairly accurate, our
   observed counts for [math]n_{20}[/math] is a very noisy estimator.
   an example may be helpful here. bell's paper cites the research of
   sproat and shih, who were studying chinese words pluralized by the
   addition of the character "men2" (hence, each word which can be
   pluralized is it's own class). here is the chart of frequency of
   frequencies:
   [main-qimg-f9ab8cb7b29eef79b9d725dec246eab3]
   with 268 observations for [math]n_1[/math], this count is unlikely to
   be off by an order of magnitude, but for [math]n_401[/math], the
   estimate of 0 is almost certainly wrong! if we were to use our formula
   from the turing estimator for the id203 of the class "ren"
   meaning people, which occurs 1918 times in our training set, our
   estimate would be 0, since [math]n_1919 = 0[/math]. this seems clearly
   problematic.
   first, let's visualize what's going on here - our observed counts are
   on the left, on a log-log scale to deal with the power-law shape of the
   distribution (common in nlp applications):
   [main-qimg-e7e5b45242a02cde68f24cf7d7169d6d]
   note the lower limit in the left plot, due to the fact that
   [math]n_r[/math] cannot go below 1. ideally, we would like to correct
   this, and "connect the dots," using the best fit line to interpolate to
   intermediate values (especially where observed frequencies are 0).
   good (1951) introduced the notion of smoothing to fix this problem,
   which arises from the fact that for large [math]r[/math], the observed
   counts are only a noisy estimator. first, to fix the issue of
   [math]n_r[/math] for large [math]r[/math], we want to "average" the
   large nonzero [math]n_r[/math] with the surrounding [math]n_r[/math]
   which are 0. formally, we order the nonzero [math]n_r[/math] by
   [math]r[/math] and let [math]q,r,t[/math] denote consecutive indices.
   then, we replace every [math]n_r[/math] with [math]z_r =
   \frac{n_r}{.5(t-q)}[/math] (note that for small [math]r[/math] where
   there are no nonzero bins, the denominator is 1). this gets us plot on
   the right, which has the desired shape.
   now, we can simply take the smooth (we clearly don't want to use a
   value of 0 for intermediate values). the simplest fit is a line - this
   is called the linear good-turing estimate, which we get by fitting the
   line
   [math]\log(z_r) = a + b \log(r)[/math]
   (note that we require the slope [math]b < -1[/math] else this summation
   will not converge.) using the predicted value [math]a + b
   \log(r)[/math] rather than the observed [math]n_r[/math] is the
   smoothed estimate.

simple good-turing

   we can finally put it all together! we initially introduced smoothing
   to deal with larger [math]r[/math], where data is more sparse. thus,
   for small [math]r[/math], it makes more sense to directly use our
   turing estimates, since these counts are relatively reliable.
   when do we use turing estimates vs. linear good-turing (lgt) estimates?
   the proposal made in simple good-turing is to use the turing estimates
   so long as they are significantly different from the lgt estimates, and
   then once we switch, to continue to use the lgt estimates.
   as the threshold for significantly different, bell uses 1.65 times the
   standard deviation of the turing estimate, where variance is estimated
   as
   [math]var(r_t^*) = (r+1)^2\frac{n_{r+1}}{n_r^2}(1 +
   \frac{n_{r+1}}{n_r})[/math]
   this estimate is based on the assumptions that [math]n_r[/math] and
   [math]n_{r+1}[/math] are independent, and that [math]var(n_r) =
   n_r[/math], and then using the first-order taylor approximation (as in
   [7]this example).
   as a final note, we need to renormalize these "probabilities," since
   they come from two different sources (turing and lgt estimates) and as
   such, there's no reason they would necessarily sum to 1.
   that's everything - we've introduced smoothing, why it's important and
   an intuitive approach to the problem, turing estimators as a method for
   moving id203 mass from observed classes to unobserved classes,
   good-turing estimators for smoothing these estimates, and simple
   good-turing as a method for combining the two estimates.
     __________________________________________________________________

   other resources: simple good-turing is introduced in [8]bell's original
   paper. i found [9]dan jurafsky's lecture as part of his stanford nlp
   coursera class and [10]jason eisner's slides from his course at john
   hopkins to be useful sources of intuition as well. if you're interested
   in applications, i was originally prompted to learn about good-turing
   smoothing while working on punctuation prediction, but applications
   throughout nlp are common.
   this post ended up being a lot longer than i planned and wanted. any
   feedback or questions on what is unclear is welcome!

   7,823 views    [11]50 upvotes    [12]posted 191w ago

references

   visible links
   1. https://www.quora.com/opensearch/description.xml
   2. https://www.quora.com/
   3. https://www.quora.com/profile/jonathan-uesato
   4. https://faculty.cs.byu.edu/~ringger/cs479/papers/gale-simplegoodturing.pdf
   5. https://en.wikipedia.org/wiki/language_model
   6. https://en.wikipedia.org/wiki/rule_of_succession
   7. https://statmd.wordpress.com/2013/08/04/the-expectation-of-the-ratio-of-two-random-variables/
   8. https://faculty.cs.byu.edu/~ringger/cs479/papers/gale-simplegoodturing.pdf
   9. https://class.coursera.org/nlp/lecture/32
  10. http://www.cs.jhu.edu/~jason/465/pdfslides/lect05-smoothing.pdf
  11. https://www.quora.com/api/mobile_expanded_voter_list?key=rsyectwixyd&type=board_item
  12. https://beyondexpectations.quora.com/an-intuitive-explanation-of-good-turing-smoothing

   hidden links:
  14. https://beyondexpectations.quora.com/
