     show, attend and tell: neural image id134 with visual
                                  attention

  [1]kelvin xu*, [2]jimmy lei ba^   , [3]ryan kiros^   , [4]kyunghyun cho*,
   [5]aaron courville*, [6]ruslan salakhutdinov^   , [7]richard zemel^   ,
                              [8]yoshua bengio*
              university of toronto^   /university of montreal*

   overview
   image id134 is the problem of generating a descriptive
       sentence of an image. the fact that humans (e.g you) can do this
       with remarkable ease makes this a very interesting/challenging
       problem for ai, combining aspects of id161 (in particular
       scene understanding) and natural language processing.
       in this work, we introduced an "attention" based framework into the
       problem of image id134. much in the same way human
       vision fixates when you perceive the visual world, the model learns
       to "attend" to selective regions while generating a description.
       furthermore, in this work we explore and compare two variants of
       this model: a deterministic version trainable using standard
       id26 techniques and a stochastic variant trainable by
       maximizing a variational lower bound.

   how does it work?
   the model brings together convolutional neural networks, recurrent
       neural networks and work in modeling attention mechanisms.

                                model_diagram

   above: from a high level, the model uses a convolutional neural network
   as a feature extractor, then uses a recurrent neural network with
   attention to generate the sentence.

   if you are not familiar with these things, you can think of the
       convolutional network as an function encoding the image ('encoding'
       = f(image)), the attention mechanism as grabbing a portion of the
       image ('context' = g(encoding)), and the recurrent network a word
       generator that receives a context at every point in time ('word' =
       l(context)).
       for a roadmap and a collection of material explaining some of the
       networks used in this work, see the following on [9]convolutional
       and [10]recurrent neural networks.

   the model in action

   [24976.gif]
   [11485.gif]
   [1294.gif]
   [13936.gif]

   [4028.gif]
   [7288.gif]
   [7600.gif]
   [8643.gif]

   [14308.gif]
   [15507.gif]
   [16118.gif]
   [17462.gif]

   want all details? interested in what else we've been up to?
   please check out the following technical report and visit the pages of
                                the authors:
   [11]show, attend and tell: neural image id134 with visual
       attention (2015)
       [12]k. xu , [13]j. ba, [14]r. kiros, [15]k. cho, [16]a. courville,
       [17]r. salakhutdinov, [18]r. zemel, [19]y. bengio
       or contact [20]us

   code
   [21][code]

references

   1. http://kelvinxu.github.io/
   2. http://www.psi.toronto.edu/?q=people
   3. http://www.cs.toronto.edu/~rkiros/
   4. http://www.kyunghyuncho.me/
   5. https://aaroncourville.wordpress.com/
   6. http://www.cs.toronto.edu/~rsalakhu/
   7. http://www.cs.toronto.edu/~zemel/inquiry/home.php
   8. http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
   9. http://www.metacademy.org/graphs/concepts/convolutional_nets
  10. http://www.metacademy.org/graphs/concepts/recurrent_neural_networks
  11. http://arxiv.org/abs/1502.03044
  12. http://kelvinxu.github.io/
  13. http://www.psi.toronto.edu/?q=people
  14. http://www.cs.toronto.edu/~rkiros/
  15. http://www.kyunghyuncho.me/
  16. https://aaroncourville.wordpress.com/
  17. http://www.cs.toronto.edu/~rsalakhu/
  18. http://www.cs.toronto.edu/~zemel/inquiry/home.php
  19. http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
  20. mailto:kelvin.xu@umontreal.ca
  21. https://github.com/kelvinxu/arctic-captions
