   #[1]github [2]recent commits to lucida:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]422
     * [35]star [36]4,862
     * [37]fork [38]945

[39]claritylab/[40]lucida

   [41]code [42]issues 54 [43]pull requests 5 [44]projects 0 [45]wiki
   [46]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   speech and vision based intelligent personal assistant
     * [48]2,200 commits
     * [49]7 branches
     * [50]4 releases
     * [51]fetching contributors
     * [52]view license

    1. [53]java 75.5%
    2. [54]python 10.9%
    3. [55]c++ 5.1%
    4. [56]javascript 3.2%
    5. [57]shell 2.2%
    6. [58]makefile 1.4%
    7. other 1.7%

   (button) java python c++ javascript shell makefile other
   branch: master (button) new pull request
   [59]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/c
   [60]download zip

downloading...

   want to be notified of new releases in claritylab/lucida?
   [61]sign in [62]sign up

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [65]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [66]download the github extension for visual studio
   and try again.

   (button) go back
   [67]@hillm3
   [68]hillm3 [69]merge pull request [70]#188 [71]from
   cassiosantos/check-file-before-extract (button)    
check file before decompress

   latest commit [72]0bad428 jul 7, 2017
   [73]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [74]lucida [75]batched (fork/join parallel) and complex workflows
   ([76]#193[77]) jun 14, 2017
   [78]tools [79]remove redundant message jun 9, 2017
   [80].dockerignore
   [81].gitignore
   [82]contributing.md
   [83]dockerfile
   [84]license
   [85]makefile
   [86]makefile.common
   [87]patents
   [88]readme.md
   [89]high_level.png [90]add tutorial.pdf aug 21, 2016
   [91]service_graph_0.png
   [92]tutorial.pdf [93]add walkthough for deployment on os x nov 29, 2016
   [94]tutorial.pptx [95]add walkthough for deployment on os x nov 29,
   2016

readme.md

lucida

   lucida is a speech and vision based intelligent personal assistant
   inspired by [96]sirius. visit [97]our website for tutorial, and
   [98]lucida-users for help. the project is released under [99]bsd
   license, except certain submodules contain their own specific licensing
   information. we would love to have your help on improving lucida, and
   see [100]contributing for more details.

overview

     * lucida: back-end services and command center (cmd). currently,
       there are 7 categories of back-end services: "asr" (automatic
       id103), "imm" (image matching), "qa" (question
       answering), "ca" (calendar events retrieval), "imc" (image
       classification), "face" (facial recognition), and "dig" (digit
       recognition).
       you can delete or replace these services with your own, or you can
       simply add a new service. for example, if you know some better asr
       implementation, have an interesting image captioning end-to-end
       system, or have access to a quality machine translation algorithm,
       please read the section "how to add your own service into lucida?"
       below.
       the command center determines which services are needed based on
       the user input, sends requests to them, and returns response to the
       user. in the following diagram, the user asks a query that needs
       the following three services: asr, imm, and qa. the "cloud" behind
       each box means the docker container(s) running on the host
       machine(s).
     * tools: dependencies necessary for compiling lucida. due to the fact
       that services share some common dependencies, all services should
       be compiled after these dependencies are installed. the advantage
       of a central point of dependencies is that the total size of
       compiled services is minimized; the disadvantage is that it makes
       deleting a service from lucida non-trivial -- you have to remove
       its dependencies in tools.

lucida local development

   if you want to make contributions to lucida, please build it locally:
     * from this directory, type: make local. this will run scripts in
       tools/ to install all the required dependencies. after that, it
       will compile back-end services in lucida/.
     * important note for ubuntu 16.04 users: please read [101]note #1.
     * if for some reason you need to compile part of it (e.g. one
       back-end service), make sure to set the following environment
       variable as set in [102]makefile:
export ld_library_path=/usr/local/lib

       you can add it permanently to your bash profile.
     * start all services:
make start_all

       this will spawn a terminal window (gnome-terminal) for each service
       as well as the command center. once they all start running, open
       your browser and visit http://localhost:3000/. check out the
       [103]tutorial for usage and sample questions.
       currently, the command center receives the user input in the form
       of http requests sent from your browser, but in future we can
       support other forms of input.

lucida docker deployment

   if you want to use lucida as a web application, please deploy using
   [104]docker and [105]kubernetes:
     * install docker: refer to
       [106]https://docs.docker.com/engine/installation/.
     * navigate to [107]tools/deploy/ and follow the instructions there.
     * once done, check out the [108]tutorial for usage and sample
       questions.

rest api for command center

   the rest api is in active development and may change drastically. it
   currently supports only infer and learn. other features may be added
   later. an [109]example client for botframework is available.
   information on how to use the api can be found in the [110]wiki

design notes -- how to add your own service into lucida?

back-end communication

   thrift is an rpc framework with the advantages of being efficient and
   language-neutral. it was originally developed by facebook and now
   developed by both the open-source community (apache thrift) and
   facebook. we use both apache thrift and facebook thrift because
   facebook thrift has a fully asynchronous c++ server but does not
   support java very well. also, apache thrift seems to be more popular.
   therefore, we recommend using apache thrift for services written in
   python and java, and facebook thrift for services written in c++.
   however, you can choose either one for your own service as long as you
   follow the steps below.

   one disadvantage about thrift is that the interface has to be
   pre-defined and implemented by each service. if the interface changes,
   all services have to re-implement the interface. we try to avoid
   changing the interface by careful design, but if you really need to
   adapt the interface for your need, feel free to modify, but make sure
   that all services implement and use the new interface.

detailed instructions

   you need to configure the command center (cmd) besides implementing the
   thrift interface in order to add your own service into lucida. let's
   break it down into two steps:
    1. implement the thrift interface jointly defined in
       lucida/lucidaservice.thrift and lucida/lucidatypes.thrift.
         1. [111]lucida/lucidaservice.thrift
include "lucidatypes.thrift"
service lucidaservice {
   void create(1:string lucid, 2:lucidatypes.queryspec spec);
   void learn(1:string lucid, 2:lucidatypes.queryspec knowledge);
   string infer(1:string lucid, 2:lucidatypes.queryspec query);
}

            the basic functionalities that your service needs to provide
            are called create, learn, and infer. they all take in the same
            type of parameters, a string representing the lucida user id
            (lucid), and a queryspec defined in lucida/lucidatypes.thrift.
            the command center invokes these three procedures implemented
            by your service, and services can also invoke these procedures
            on each other to achieve communication. thus the typical data
            flow looks like this:
            command center (cmd) -> your own service (yos)
            but it also can be like this:
            command center (cmd) -> your own service 0 (yos0) -> your own
            service 1 (yos1) -> your own service 2 (yos2)
            in this scenario, make sure to implement the asynchronous
            thrift interface. if yos0 implements the asynchronous thrift
            interface, it won't block on waiting for the response from
            yos1. if yos0 implements the synchronous thrift interface, it
            cannot make progress until yos1 returns the response, so the
            operating system will perform a thread context switch, and let
            the current thread sleep until yos1 returns. see section 3 of
            step 1 for implementation details.
            create: create an intelligent instance based on supplied
            lucid. it gives services a chance to warm up the pipeline, but
            our current services do not need that. therefore, the command
            center does not send create request at this point. if your
            service needs to warm up for each user, make sure to modify
            the command center which is detailed in step 2.
            learn: tell the intelligent instance to learn new knowledge
            based on data supplied in the query, which usually means the
            training process. although it has be implemented, you can
            choose to do nothing in the function body if your service
            cannot learn new knowledge. for example, it may be hard to
            retrain a dnn model, so the facial recognition service simply
            prints a message when it receives a learn request. otherwise,
            consider using a database system to store the new knowledge.
            currently, we use mongodb to store the text and image
            knowledge. you need to tell the command center whether to send
            a learn request to your service or not, which is detailed in
            step 2.
            infer: ask the intelligence to infer using the data supplied
            in the query, which usually means the predicting process.
            notice all the three functions take in queryspec as their
            second parameters, so let's see what queryspec means for each
            function.
         2. [112]lucida/lucidatypes.thrift:
struct queryinput {
    1: string type;
    2: list<string> data;
    3: list<string> tags;
}
struct queryspec {
    1: string name;
    2: list<queryinput> content;
}

            a queryspec has a name, which is create for create, knowledge
            for learn, and query for infer. a queryspec also has a list of
            queryinput called content, which is the data payload. a
            queryinput consists of a type, a list of data, and a list of
            tags.
               o if the function call is learn:
            one queryinput is constructed by the command center currently,
            but you should still iterate through all queryinputs in case
            for change in future. for queryinput, type can be text for
            plain text, url for url address to extract text from, image
            for image, or unlearn (undo learn) for the reverse process of
            learn. here is our assumptions: a service can handle either
            text or image, and if it can handle text, the types your
            service should handle are text, url, and unlearn, and if it
            can handle image, the types your service should handle are
            image and unlearn. see step 2 for details on how to specify
            the type of knowledge that your service can learn. if type is
            text or url, data[i] is the ith piece of text or url as new
            knowledge and tags[i] is the id of the ith piece of text
            generated by a hash function in the command center; if type is
            image, data[i] is the ith image as new knowledge (notice that
            it is the actual string representation of an image and thus
            can very long), and tags[i] is the label/name of the ith image
            received from the front end; if type is unlearn, data should
            be ignored by your service (usually a list of an empty
            string), and tags[i] is the ith id of the text or the image
            label to delete, based on whether the service can handle text
            or image.
               o if the function call is infer:
            each queryinput in content corresponds to one service (cmd is
            not considered to be a service) in the service graph, a
            connected directed acyclic graph (dag) describing all services
            that are needed for the query. thus, for the following service
            graph, two queryinputs are present in content, each being a
            node in the graph:
            command center (cmd) -> your own service 0 (yos0) -> your own
            service 1 (yos1)
            for queryinput, type can be text for plain text, or image for
            image (no url for infer). see step 2 for details on how to
            specify the type of query that your service can process. if
            type is text, data[0] is the 0th piece of text; if type is
            image, data[0] is the 0th image. there is only one string in
            data. tags have different meanings from learn and are of the
            following format:
[host, port, <size of the following list>, <list of integers>]

            . tags in the ith queryinput in content describe the location
            of the ith node and its relation to other nodes. by location,
            we mean that the host:port specifies the location of the ith
            node. by relation to other nodes, we mean that the list of
            integers specifies the indices of nodes that the ith node
            points to.
            therefore, the above service graph results in a queryspec that
            looks like this:
{ name: "query",
content: [
{ type: "text",
data: [ "what's the speed of light?" ],
tags: ["localhost", "8083", "1", "1"] },
{ type: "text",
data: [ "what's the speed of light?" ],
tags: ["localhost", "9090", "0"] } ] }

            . we can define arbitrarily complicated service graphs. for
            example, for the following service graph:
            [113]alt text
            , the resulting queryspec may look like this, assuming yosx is
            running at 909x:
{ name: "query",
content: [
{ type: "image",
data: [ "1234567...abcdefg" ],
tags: ["localhost", "9090", "1", 2"] },
{ type: "image",
data: [ "1234567...abcdefg" ],
tags: ["localhost", "9091", "2", "2", "3"] },
{ type: "text",
data: [ "which person in my family is in this image?" ],
tags: ["localhost", "9092", "0"] },
{ type: "image",
data: [ "1234567...abcdefg" ],
tags: ["localhost", "9093", "0"] } ] }

            . notice that if the order of queryinput in content is
            rearranged, the resulting queryspec still corresponds to the
            same graph. in fact, there are 2^(n) valid queryspecs for a
            given graph, and you need to define only one of them in the
            configuration file of the command center. notice that the
            starting nodes, yos0 and yos1, need to be specified
            separately, so that the command center knows where to send the
            request(s) to. if more than one starting nodes are specified,
            the command center simply concatenates the results returned
            from all of them. see step 2 for more details on how to
            specify service graphs and starting nodes.
            the command center guarantees to send a valid queryspec, but
            your service is responsible for parsing the graph, further
            sending the request(s) to other service(s), and returning the
            response(s) to the service(s) it is pointed to by. tags simply
            provide the information of all nodes involved in the service
            graph. that being said, suppose yos0 deliberately does not
            send the request to yos2, and yos2 is written in a way that it
            cannot return to yos1 without processing requests from both
            yos0 and yos1. then yos2 is stalled, which leads to yos1
            waiting for yos2, and the command center waiting for yos1.
            each service is also allowed to ignore or modify the graph if
            that is necessary, but that should be done with caution.
            although the service graph can be very complicated, it is
            usually very simple. at least for the current services, the
            most complicated graph looks like this:
            command center (cmd) -> imm -> qa
            . thus, some services can ignore the tags given the fact that
            they know they are always the only node in the graph.
         3. here are the code examples that you can use for your own
            service:
            if it is written in c++, refer to the code in
            [lucida/imagematching/opencv_imm/server/]
            (lucida/imagematching/opencv_imm/server/). look at makefile
            for how to generate thrift stubs which are the abstract base
            classes your handlers need to inherit. notice that the
            interface is implemented in immhandler.h and immhandler.cpp,
            and the entry point (which uses a multi-threaded server
            provided by thrift) is in immserver.cpp.
            if it is written in java, refer to the code in
            [lucida/calendar/src/main/java/calendar/]
            (lucida/calendar/src/main/java/calendar/) and
            [114]lucida/calendar/. look at makefile for how to generate
            thrift stubs which are the interfaces your handlers need to
            implement. notice that the interface is implemented in
            caservicehandler.java, and the entry point (which uses a
            multi-threaded server provided by thrift) is in
            calendardaemon.java.
            if it is written in other programming languages, please refer
            to [115]the official tutorial.
         4. here is a list of what you need to do for step 1:
               o add a thrift wrapper which typically consists of a thrift
                 handler implementing the thrift interfaces, and a daemon
                 program which is the entry point of your service. refer
                 to the code examples mentioned above.
               o modify your makefile so that it uses the thrift compiler
                 to generate thrift stubs code. following the style of the
                 existing makefiles is recommended.
               o test your service.
               o (optional) modify tools if you choose to put the
                 dependencies of your service in this central point.
               o (local development) modify the top-level [116]makefile
                 and [117]lucida/makefile so that make local and make
                 start_all include your service.
               o (docker deployment) create a dockerfile image for your
                 service, or merge it into the top-level [118]dockerfile
                 and add kubernetes yaml scripts for your service into
                 [119]tools/deploy/.
    2. configure the command center.
       [120]lucida/commandcenter/controllers/config.py is the only file
       you must modify, but you may also need to add sample queries to
       [121]lucida/commandcenter/data/ as training data for the query
       classifier.
         1. modify the configuration file
            [122]lucida/commandcenter/controllers/config.py.
services = {
  'imm' : service('imm', 8082, 'image', 'image'),  # image matching
  'qa' : service('qa', 8083, 'text', 'text'), # id53
  'ca' : service('ca', 8084, 'text', none), # calendar
  }

classifier_descriptions = {
  'text' : { 'class_qa' :  graph([node('qa')]),
             'class_ca' : graph([node('ca')]) },
  'image' : { 'class_imm' : graph([node('imm')]) },
  'text_image' : { 'class_qa': graph([node('qa')]),
                   'class_imm' : graph([node('imm')]),
                   'class_imm_qa' : graph([node('imm', [1]), node('qa')]) }
  }

               o services
            services is a dictionary from service name to service object.
            a service object is constructed this way:
            service(name, port, input_type, learn_type)
            , where name is the name of the service, port is the port
            number, input_type is the type of query that the service can
            handle which is either text or image, and learn_type is the
            type of knowledge that the service can learn which is
            eithertext, image, or none. recall from step 1 that if
            learn_type is specified as text, your service will receive
            type of text, url, and unlearn, and if learn_type is specified
            as image, your service will receive type of image and unlearn.
            notice that you do not need to specify the ip address of your
            service. by default it is localhost, but if you use kubernetes
            and run each service behind a kubernetes service, kubernetes
            dynamically assigns an ip address to the service and sets an
            environment variable <service_name>_port_<port>_tcp_addr for
            each running container in the cluster. this implies that the
            kubernetes service defined in the yaml file should have the
            same name as the service defined in this python file. all of
            the current kubernetes scripts in [123]tools/deploy/ follow
            this naming convention.
            for example, if you create a kubernetes service called imm and
            run your imm container behind it, the command center in the
            same cluster has the following environment variable
imm_port_8082_tcp_addr

            set to something like 10.0.0.92. this ip address will be
            tags[0] in the queryinput as described in step 1.
            notice that asr (automatic id103) is not listed
            here. the reason is that we currently use [kaldi gstremer
            server]
            ([124]https://github.com/alumae/kaldi-gstreamer-server) which
            directly receives real-time voice query from the front end
            through web socket in the docker mode.
               o classifier_descriptions
            classifier_descriptions is a dictionary from input type to
            possible prediction results. internally, the command center
            uses a query classifier that predicts the needed services
            based on both the input type (transcription text, image, or
            both) and the content of transcription text if present. in the
            above example, if the user only gives voice input without
            image, the input type is text, and the prediction result can
            be either class_qa or class_ca. if the query is classified as
            class_qa which means generic qa style questions whose training
            data is in [125]lucida/commandcenter/data/class_qa.txt, the
            services are needed are represented in a graph with one node,
            i.e. service qa. if you want to replace the current qa
            implementation with your own, you can still use the training
            data, and only modify the service graph to be:
'text_image' : { 'class_qa': graph([node('name_of_my_own_qa_service')]), ...

            however, if you need to define another set of questions that a
            user can ask, e.g. questions about image captioning, refer to
            the next section on how to add training data.
            notice that a service graph object is constructed with a list
            of node each node is constructed with the service name and an
            optional list of node indices in the list that the current
            node points to. by default, it is an empty list. for example,
            'class_imm_qa' : graph([node('imm', [1]), node('qa')]) means
            that the prediction result class_imm_qa needs the following
            services:
imm -> qa

            notice that we do not define which nodes the current node is
            pointed to by, so we do not know which node is pointed to by
            the command center. thus, we need to specify the starting
            nodes separately. this is done by an optional second parameter
            in the constructor of graph:
def __init__(self, node_list, starting_indices=[0]):

            as you see, by default, the command center assumes the 0th
            node in the node list is the starting node. thus, a queryspec
            like the following will be sent to the imm service:
{ name: "query",
content: [
{ type: "image",
data: [ "1234567...abcdefg" ],
tags: ["localhost", "8082", "1", 1"] },
{ type: "text",
data: [ "how old is this person?" ],
tags: ["localhost", "8083", "0"] } ] }

            the imm service receives this queryspec along with lucid, and
            is responsible for further sending the request to the qa
            service. the imm service is allowed to modify the queryspec
            and send a reconstructed queryspec to qa, but as long as imm
            finally returns a string to the command center, it is fine.
            this degree of flexibility opens up opportunities for
            complicated communication between your services. usually, a
            graph with one node suffices, because that one node may be an
            entry point to a cluster of your services, which may have
            complicated feedback loops and use a different communication
            mechanism.
            notice that there is only one possible prediction result if
            the user only gives image input: class_imm, because the query
            classifier only works on text input. however, you can still
            send the image to multiple services like this:
''image' : { 'class_imm' : graph([node('imm'), node('imc'), node('image_captioni
ng')], [0, 1, 2]) }, ...

            be aware that there are other parameters that you can change
            in this configuration file, which are pretty self-explanatory
            in their names and comments.
         2. add training data for your own query class.
            we already prepare some sample training data in
            [126]lucida/commandcenter/data/, but if you need to define a
            custom type of query that your service can handle, you should
            create the following file in the above directory:
class_<name_of_your_query_class>.txt

            , and have at least 40 pieces of text in it, each being one
            way to ask about the same question.

     *    2019 github, inc.
     * [127]terms
     * [128]privacy
     * [129]security
     * [130]status
     * [131]help

     * [132]contact github
     * [133]pricing
     * [134]api
     * [135]training
     * [136]blog
     * [137]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [138]reload to refresh your
   session. you signed out in another tab or window. [139]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/claritylab/lucida/commits/master.atom
   3. https://github.com/claritylab/lucida#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/claritylab/lucida
  32. https://github.com/join
  33. https://github.com/login?return_to=/claritylab/lucida
  34. https://github.com/claritylab/lucida/watchers
  35. https://github.com/login?return_to=/claritylab/lucida
  36. https://github.com/claritylab/lucida/stargazers
  37. https://github.com/login?return_to=/claritylab/lucida
  38. https://github.com/claritylab/lucida/network/members
  39. https://github.com/claritylab
  40. https://github.com/claritylab/lucida
  41. https://github.com/claritylab/lucida
  42. https://github.com/claritylab/lucida/issues
  43. https://github.com/claritylab/lucida/pulls
  44. https://github.com/claritylab/lucida/projects
  45. https://github.com/claritylab/lucida/wiki
  46. https://github.com/claritylab/lucida/pulse
  47. https://github.com/join?source=prompt-code
  48. https://github.com/claritylab/lucida/commits/master
  49. https://github.com/claritylab/lucida/branches
  50. https://github.com/claritylab/lucida/releases
  51. https://github.com/claritylab/lucida/graphs/contributors
  52. https://github.com/claritylab/lucida/blob/master/license
  53. https://github.com/claritylab/lucida/search?l=java
  54. https://github.com/claritylab/lucida/search?l=python
  55. https://github.com/claritylab/lucida/search?l=c++
  56. https://github.com/claritylab/lucida/search?l=javascript
  57. https://github.com/claritylab/lucida/search?l=shell
  58. https://github.com/claritylab/lucida/search?l=makefile
  59. https://github.com/claritylab/lucida/find/master
  60. https://github.com/claritylab/lucida/archive/master.zip
  61. https://github.com/login?return_to=https://github.com/claritylab/lucida
  62. https://github.com/join?return_to=/claritylab/lucida
  63. https://desktop.github.com/
  64. https://desktop.github.com/
  65. https://developer.apple.com/xcode/
  66. https://visualstudio.github.com/
  67. https://github.com/hillm3
  68. https://github.com/claritylab/lucida/commits?author=hillm3
  69. https://github.com/claritylab/lucida/commit/0bad428bb763fe404d01db5d9e08ee33a8f3776c
  70. https://github.com/claritylab/lucida/pull/188
  71. https://github.com/claritylab/lucida/commit/0bad428bb763fe404d01db5d9e08ee33a8f3776c
  72. https://github.com/claritylab/lucida/commit/0bad428bb763fe404d01db5d9e08ee33a8f3776c
  73. https://github.com/claritylab/lucida/tree/0bad428bb763fe404d01db5d9e08ee33a8f3776c
  74. https://github.com/claritylab/lucida/tree/master/lucida
  75. https://github.com/claritylab/lucida/commit/f7ad786f194104051b1052ef7b19388911b5fbe1
  76. https://github.com/claritylab/lucida/pull/193
  77. https://github.com/claritylab/lucida/commit/f7ad786f194104051b1052ef7b19388911b5fbe1
  78. https://github.com/claritylab/lucida/tree/master/tools
  79. https://github.com/claritylab/lucida/commit/44a10dfd08af4a1f2e6573b7116d32ea958baed8
  80. https://github.com/claritylab/lucida/blob/master/.dockerignore
  81. https://github.com/claritylab/lucida/blob/master/.gitignore
  82. https://github.com/claritylab/lucida/blob/master/contributing.md
  83. https://github.com/claritylab/lucida/blob/master/dockerfile
  84. https://github.com/claritylab/lucida/blob/master/license
  85. https://github.com/claritylab/lucida/blob/master/makefile
  86. https://github.com/claritylab/lucida/blob/master/makefile.common
  87. https://github.com/claritylab/lucida/blob/master/patents
  88. https://github.com/claritylab/lucida/blob/master/readme.md
  89. https://github.com/claritylab/lucida/blob/master/high_level.png
  90. https://github.com/claritylab/lucida/commit/5560f09cfe03a6364e792c32d6df2f3fb4a03f92
  91. https://github.com/claritylab/lucida/blob/master/service_graph_0.png
  92. https://github.com/claritylab/lucida/blob/master/tutorial.pdf
  93. https://github.com/claritylab/lucida/commit/1a173b406e302c20e098a410dc8ded46c912ee60
  94. https://github.com/claritylab/lucida/blob/master/tutorial.pptx
  95. https://github.com/claritylab/lucida/commit/1a173b406e302c20e098a410dc8ded46c912ee60
  96. http://sirius.clarity-lab.org/
  97. http://lucida.ai/
  98. http://groups.google.com/forum/#!forum/lucida-users
  99. https://github.com/claritylab/lucida/blob/master/license
 100. https://github.com/claritylab/lucida/blob/master/contributing.md
 101. https://github.com/claritylab/lucida/blob/master/tools/readme.md
 102. https://github.com/claritylab/lucida/blob/master/makefile
 103. https://github.com/claritylab/lucida/blob/master/tutorial.pdf
 104. https://www.docker.com/
 105. http://kubernetes.io/
 106. https://docs.docker.com/engine/installation/
 107. https://github.com/claritylab/lucida/blob/master/tools/deploy
 108. https://github.com/claritylab/lucida/blob/master/tutorial.pdf
 109. https://github.com/claritylab/lucida/blob/master/lucida/botframework-interface
 110. https://github.com/claritylab/lucida/wiki/rest-api
 111. https://github.com/claritylab/lucida/blob/master/lucida/lucidaservice.thrift
 112. https://github.com/claritylab/lucida/blob/master/lucida/lucidatypes.thrift
 113. https://github.com/claritylab/lucida/blob/master/service_graph_0.png?raw=true
 114. https://github.com/claritylab/lucida/blob/master/lucida/calendar
 115. https://thrift.apache.org/tutorial/
 116. https://github.com/claritylab/lucida/blob/master/makefile
 117. https://github.com/claritylab/lucida/blob/master/lucida/makefile
 118. https://github.com/claritylab/lucida/blob/master/dockerfile
 119. https://github.com/claritylab/lucida/blob/master/tools/deploy
 120. https://github.com/claritylab/lucida/blob/master/lucida/commandcenter/controllers/config.py
 121. https://github.com/claritylab/lucida/blob/master/lucida/commandcenter/data
 122. https://github.com/claritylab/lucida/blob/master/lucida/commandcenter/controllers/config.py
 123. https://github.com/claritylab/lucida/blob/master/tools/deploy
 124. https://github.com/alumae/kaldi-gstreamer-server
 125. https://github.com/claritylab/lucida/blob/master/lucida/commandcenter/data/class_qa.txt
 126. https://github.com/claritylab/lucida/blob/master/lucida/commandcenter/data
 127. https://github.com/site/terms
 128. https://github.com/site/privacy
 129. https://github.com/security
 130. https://githubstatus.com/
 131. https://help.github.com/
 132. https://github.com/contact
 133. https://github.com/pricing
 134. https://developer.github.com/
 135. https://training.github.com/
 136. https://github.blog/
 137. https://github.com/about
 138. https://github.com/claritylab/lucida
 139. https://github.com/claritylab/lucida

   hidden links:
 141. https://github.com/
 142. https://github.com/claritylab/lucida
 143. https://github.com/claritylab/lucida
 144. https://github.com/claritylab/lucida
 145. https://help.github.com/articles/which-remote-url-should-i-use
 146. https://github.com/claritylab/lucida#lucida
 147. https://github.com/claritylab/lucida#overview
 148. https://github.com/claritylab/lucida/blob/master/high_level.png
 149. https://github.com/claritylab/lucida#lucida-local-development
 150. https://github.com/claritylab/lucida#lucida-docker-deployment
 151. https://github.com/claritylab/lucida#rest-api-for-command-center
 152. https://github.com/claritylab/lucida#design-notes----how-to-add-your-own-service-into-lucida
 153. https://github.com/claritylab/lucida#back-end-communication
 154. https://github.com/claritylab/lucida#detailed-instructions
 155. https://github.com/
