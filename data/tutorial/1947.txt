   #[1]github [2]recent commits to id195-attn:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]95
     * [35]star [36]1,059
     * [37]fork [38]275

[39]harvardnlp/[40]id195-attn

   [41]code [42]issues 10 [43]pull requests 4 [44]projects 1 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   sequence-to-sequence model with lstm encoder/decoders and attention
   [47]http://nlp.seas.harvard.edu/code
     * [48]131 commits
     * [49]3 branches
     * [50]0 releases
     * [51]fetching contributors
     * [52]mit

    1. [53]lua 75.2%
    2. [54]python 24.8%

   (button) lua python
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/h
   [56]download zip

downloading...

   want to be notified of new releases in harvardnlp/id195-attn?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   [63]@yoonkim
   [64]yoonkim [65]merge pull request [66]#82 [67]from swiseman/master
   (button)    
command-line option for layer-specific learning rates for non-sgd optimization

   latest commit [68]2d36b43 jan 2, 2017
   [69]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [70]data [71]support additional input features sep 19, 2016
   [72]s2sa [73]added attention and state extraction code oct 17, 2016
   [74].gitignore [75]add test data jun 29, 2016
   [76]license
   [77]readme.md
   [78]convert_to_cpu.lua
   [79]evaluate.lua
   [80]preprocess-shards.py
   [81]preprocess.py [82]. dec 2, 2016
   [83]prune.lua
   [84]train.lua

readme.md

sequence-to-sequence learning with attentional neural networks

   update: check-out the beta release of [85]openid4 a fully supported
   feature-complete rewrite of id195-attn. id195-attn will remain
   supported, but new features and optimizations will focus on the new
   codebase.

   [86]torch implementation of a standard sequence-to-sequence model with
   (optional) attention where the encoder-decoder are lstms. encoder can
   be a bidirectional lstm. additionally has the option to use characters
   (instead of input id27s) by running a convolutional neural
   network followed by a [87]highway network over character embeddings to
   use as inputs.

   the attention model is from [88]effective approaches to attention-based
   id4, luong et al. emnlp 2015. we use the
   global-general-attention model with the input-feeding approach from the
   paper. input-feeding is optional and can be turned off.

   the character model is from [89]character-aware neural language models,
   kim et al. aaai 2016.

   there are a lot of additional options on top of the baseline model,
   mainly thanks to the fantastic folks at [90]systran. specifically,
   there are functionalities which implement:
     * [91]effective approaches to attention-based neural machine
       translation. luong et al., emnlp 2015.
     * [92]character-based id4. costa-jussa and
       fonollosa, acl 2016.
     * [93]compression of id4 models via pruning.
       see et al., coling 2016.
     * [94]sequence-level knowledge distillation. kim and rush., emnlp
       2016.
     * [95]deep recurrent models with fast forward connections for neural
       machine translation. zhou et al, tacl 2016.
     * [96]guided alignment training for topic-aware neural machine
       translation. chen et al., arxiv:1607.01628.
     * [97]linguistic input features improve id4.
       senrich et al., arxiv:1606.02892

   see below for more details on how to use them.

   this project is maintained by [98]yoon kim. feel free to post any
   questions/issues on the issues page.

dependencies

python

     * h5py
     * numpy

lua

   you will need the following packages:
     * hdf5
     * nn
     * nngraph

   gpu usage will additionally require:
     * cutorch
     * cunn

   if running the character model, you should also install:
     * cudnn
     * luautf8

quickstart

   we are going to be working with some example data in data/ folder.
   first run the data-processing code
python preprocess.py --srcfile data/src-train.txt --targetfile data/targ-train.t
xt
--srcvalfile data/src-val.txt --targetvalfile data/targ-val.txt --outputfile dat
a/demo

   this will take the source/target train/valid files (src-train.txt,
   targ-train.txt, src-val.txt, targ-val.txt) and make some hdf5 files to
   be consumed by lua.

   demo.src.dict: dictionary of source vocab to index mappings.
   demo.targ.dict: dictionary of target vocab to index mappings.
   demo-train.hdf5: hdf5 containing the train data. demo-val.hdf5: hdf5
   file containing the validation data.

   the *.dict files will be needed when predicting on new data.

   now run the model
th train.lua -data_file data/demo-train.hdf5 -val_data_file data/demo-val.hdf5 -
savefile demo-model

   this will run the default model, which consists of a 2-layer lstm with
   500 hidden units on both the encoder/decoder. you can also add -gpuid 1
   to use (say) gpu 1 in the cluster.

   now you have a model which you can use to predict on new data. to do
   this we are going to be running id125
th evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_fi
le pred.txt
-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict

   this will output predictions into pred.txt. the predictions are going
   to be quite terrible, as the demo dataset is small. try running on some
   larger datasets! for example you can download millions of parallel
   sentences for [99]translation or [100]summarization.

details

preprocessing options (preprocess.py)

     * srcvocabsize, targetvocabsize: size of source/target vocabularies.
       this is constructed by taking the top x most frequent words. rest
       are replaced with special unk tokens.
     * srcfile, targetfile: path to source/target training data, where
       each line represents a single source/target sequence.
     * srcvalfile, targetvalfile: path to source/target validation data.
     * batchsize: size of each mini-batch.
     * seqlength: maximum sequence length (sequences longer than this are
       dropped).
     * outputfile: prefix of the output file names.
     * maxwordlength: for the character models, words are truncated (if
       longer than maxwordlength) or zero-padded (if shorter) to
       maxwordlength.
     * chars: if 1, construct the character-level dataset as well. this
       might take up a lot of space depending on your data size, so you
       may want to break up the training data into different shards.
     * srcvocabfile, targetvocabfile: if working with a preset vocab, then
       including these paths will ignore the srcvocabsize,targetvocabsize.
     * unkfilter: ignore sentences with too many unk tokens. can be an
       absolute count limit (if > 1) or a proportional limit (0 <
       unkfilter < 1).
     * shuffle: shuffle sentences.
     * alignfile, alignvalfile: if provided with filenames that contain
       'pharaoh' format alignment on the train and validation data,
       source-to-target alignments are stored in the dataset.

training options (train.lua)

   data options
     * data_file, val_data_file: path to the training/validation *.hdf5
       files created from running preprocess.py.
     * savefile: savefile name (model will be saved as
       savefile_epochx_ppl.t7 after every save_every epoch where x is the
       x-th epoch and ppl is the validation perplexity at the epoch.
     * num_shards: if the training data has been broken up into different
       shards, then this is the number of shards.
     * train_from: if training from a checkpoint then this is the path to
       the pre-trained model.

   model options
     * num_layers: number of layers in the lstm encoder/decoder (i.e.
       number of stacks).
     * id56_size: size of lstm hidden states.
     * word_vec_size: id27 size.
     * attn: if = 1, use attention over the source sequence during
       decoding. if = 0, then it uses the last hidden state of the encoder
       as the context at each time step.
     * bid56: if = 1, use a bidirectional lstm on the encoder side. input
       embeddings (or charid98 if using characters) are shared between the
       forward/backward lstm, and hidden states of the corresponding
       forward/backward lstms are added to obtain the hidden
       representation for that time step.
     * use_chars_enc: if = 1, use characters on the encoder side (as
       inputs).
     * use_chars_dec: if = 1, use characters on the decoder side (as
       inputs).
     * reverse_src: if = 1, reverse the source sequence. the original
       sequence-to-sequence paper found that this was crucial to achieving
       good performance, but with id12 this does not seem
       necessary. recommend leaving it to 0.
     * init_dec: initialize the hidden/cell state of the decoder at time 0
       to be the last hidden/cell state of the encoder. if 0, the initial
       states of the decoder are set to zero vectors.
     * input_feed: if = 1, feed the context vector at each time step as
       additional input (via concatenation with the id27s) to
       the decoder.
     * multi_attn: if > 0, then use a multi-attention on this layer of the
       decoder. for example, if num_layers = 3 and multi_attn = 2, then
       the model will do an attention over the source sequence on the
       second layer (and use that as input to the third layer) and the
       penultimate layer. we've found that this did not really improve
       performance on translation, but may be helpful for other tasks
       where multiple attentional passes over the source sequence are
       required (e.g. for more complex reasoning tasks).
     * res_net: use residual connections between lstm stacks whereby the
       input to the l-th lstm layer of the hidden state of the l-1-th lstm
       layer summed with hidden state of the l-2th lstm layer. we didn't
       find this to really help in our experiments.

   below options only apply if using the character model.
     * char_vec_size: if using characters, size of the character
       embeddings.
     * kernel_width: size (i.e. width) of the convolutional filter.
     * num_kernels: number of convolutional filters (feature maps). so the
       representation from characters will have this many dimensions.
     * num_highway_layers: number of highway layers in the character
       composition model.

   to build a model with guided alignment (implemented similarly to
   [101]guided alignment training for topic-aware neural machine
   translation (chen et al. 2016)):
     * guided_alignment: if 1, use external alignments to guide the
       attention weights
     * guided_alignment_weight: weight for guided alignment criterion
     * guided_alignment_decay: decay rate per epoch for alignment weight

   optimization options
     * epochs: number of training epochs.
     * start_epoch: if loading from a checkpoint, the epoch from which to
       start.
     * param_init: parameters of the model are initialized over a uniform
       distribution with support (-param_init, param_init).
     * optim: optimization method, possible choices are 'sgd', 'adagrad',
       'adadelta', 'adam'. for id195 i've found vanilla sgd to work well
       but feel free to experiment.
     * learning_rate: starting learning rate. for 'adagrad', 'adadelta',
       and 'adam', this is the global learning rate. recommended settings
       vary based on optim: sgd (learning_rate = 1), adagrad
       (learning_rate = 0.1), adadelta (learning_rate = 1), adam
       (learning_rate = 0.1).
     * layer_lrs: comma-separated learning rates for encoder, decoder, and
       generator when using 'adagrad', 'adadelta', or 'adam' for 'optim'
       option. layer-specific learning rates cannot currently be used with
       sgd.
     * max_grad_norm: if the norm of the gradient vector exceeds this,
       renormalize to have its norm equal to max_grad_norm.
     * dropout: dropout id203. dropout is applied between vertical
       lstm stacks.
     * lr_decay: decay learning rate by this much if (i) perplexity does
       not decrease on the validation set or (ii) epoch has gone past the
       start_decay_at epoch limit.
     * start_decay_at: start decay after this epoch.
     * curriculum: for this many epochs, order the minibatches based on
       source sequence length. (sometimes setting this to 1 will increase
       convergence speed).
     * feature_embeddings_dim_exponent: if the additional feature takes n
       values, then the embbeding dimension will be set to n^exponent.
     * pre_word_vecs_enc: if using pretrained id27s (on the
       encoder side), this is the path to the *.hdf5 file with the
       embeddings. the hdf5 should have a single field word_vecs, which
       references an array with dimensions vocab size by embedding size.
       each row should be a id27 and follow the same indexing
       scheme as the *.dict files from running preprocess.py. in order to
       be consistent with beam.lua, the first 4 indices should always be
       <blank>, <unk>, <s>, </s> tokens.
     * pre_word_vecs_dec: path to *.hdf5 for pretrained id27s on
       the decoder side. see above for formatting of the *.hdf5 file.
     * fix_word_vecs_enc: if = 1, fix id27s on the encoder side.
     * fix_word_vecs_dec: if = 1, fix id27s on the decoder side.
     * max_batch_l: batch size used to create the data in preprocess.py.
       if this is left blank (recommended), then the batch size will be
       inferred from the validation set.

   other options
     * start_symbol: use special start-of-sentence and end-of-sentence
       tokens on the source side. we've found this to make minimal
       difference.
     * gpuid: which gpu to use (-1 = use cpu).
     * gpu : if this is >=0, then the model will use two gpus whereby
       the encoder is on the first gpu and the decoder is on the second
       gpu. this will allow you to train bigger models.
     * cudnn: whether to use cudnn or not for convolutions (for the
       character model). cudnn has much faster convolutions so this is
       highly recommended if using the character model.
     * save_every: save every this many epochs.
     * print_every: print various stats after this many batches.
     * seed: change the random seed for random numbers in torch - use that
       option to train alternate models for ensemble
     * prealloc: when set to 1 (default), enable memory preallocation and
       sharing between clones - this reduces by a lot the used memory -
       there should not be any situation where you don't need it. also -
       since memory is preallocated, there is not (major) memory increase
       during the training. when set to 0, it rolls back to original
       memory optimization.

decoding options (beam.lua)

     * model: path to model .t7 file.
     * src_file: source sequence to decode (one line per sequence).
     * targ_file: true target sequence (optional).
     * output_file: path to output the predictions (each line will be the
       decoded sequence).
     * src_dict: path to source vocabulary (*.src.dict file from
       preprocess.py).
     * targ_dict: path to target vocabulary (*.targ.dict file from
       preprocess.py).
     * feature_dict_prefix: prefix of the path to the features
       vocabularies (*.feature_n.dict files from preprocess.py).
     * char_dict: path to character vocabulary (*.char.dict file from
       preprocess.py).
     * beam: beam size (recommend keeping this at 5).
     * max_sent_l: maximum sentence length. if any of the sequences in
       srcfile are longer than this it will error out.
     * simple: if = 1, output prediction is simply the first time the top
       of the beam ends with an end-of-sentence token. if = 0, the model
       considers all hypotheses that have been generated so far that ends
       with end-of-sentence token and takes the highest scoring of all of
       them.
     * replace_unk: replace the generated unk tokens with the source token
       that had the highest attention weight. if srctarg_dict is provided,
       it will lookup the identified source token and give the
       corresponding target token. if it is not provided (or the
       identified source token does not exist in the table) then it will
       copy the source token.
     * srctarg_dict: path to source-target dictionary to replace unk
       tokens. each line should be a source token and its corresponding
       target token, separated by |||. for example

hello|||hallo
ukraine|||ukrainische

   this dictionary can be obtained by, for example, running an alignment
   model as a preprocessing step. we recommend [102]fast_align.
     * score_gold: if = 1, score the true target output as well.
     * n_best: if > 1, then it will also output an n_best list of decoded
       sentences in the following format.

1 ||| sentence_1 ||| sentence_1_score
2 ||| sentence_2 ||| sentence_2_score

     * gpuid: id of the gpu to use (-1 = use cpu).
     * gpu : id if the second gpu (if specified).
     * cudnn: if the model was trained with cudnn, then this should be set
       to 1 (otherwise the model will fail to load).
     * rescore: when set to scorer name, use scorer to find hypothesis
       with highest score - available 'id7', 'gleu'
     * rescore_param: parameter to rescorer - for id7/gleu ngram length

using additional input features

   [103]linguistic input features improve id4
   (senrich et al. 2016) shows that translation performance can be
   increased by using additional input features.

   similarly to this work, you can annotate each word in the source text
   by using the -|- separator:
word1-|-feat1-|-feat2 word2-|-feat1-|-feat2

   it supports an arbitrary number of features with arbitrary labels.
   however, all input words must have the same number of annotations. see
   for example data/src-train-case.txt which annotates each word with the
   case information.

   to evaluate the model, the option -feature_dict_prefix is required on
   evaluate.lua which points to the prefix of the features dictionnaries
   generated during the preprocessing.

pruning a model

   [104]compression of id4 models via pruning (see
   et al. 2016) shows that a model can be aggressively pruned while
   keeping the same performace.

   to prune a model - you can use prune.lua which implement class-bind,
   and class-uniform pruning technique from the paper.
     * model: the model to prune
     * savefile: name of the pruned model
     * gpuid: which gpu to use. -1 = use cpu. depends if the model is
       serialized for gpu or cpu
     * ratio: pruning rate
     * prune: pruning technique blind or uniform, by default blind

   note that the pruning cut connection with lowest weight in the linear
   models by using a boolean mask. the size of the file is a little larger
   since it stores the actual full matrix and the binary mask.

   models can be retrained - typically you can recover full capacity of a
   model pruned at 60% or even 80% by few epochs of additional trainings.

switching between gpu/cpu models

   by default, the model will always save the final model as a cpu model,
   but it will save the intermediate models as a cpu/gpu model depending
   on how you specified -gpuid. if you want to run id125 on the cpu
   with an intermediate model trained on the gpu, you can use
   convert_to_cpu.lua to convert the model to cpu and run id125.

gpu memory requirements/training speed

   training large sequence-to-sequence models can be memory-intensive.
   memory requirements will dependent on batch size, maximum sequence
   length, vocabulary size, and (obviously) model size. here are some
   benchmark numbers on a geforce gtx titan x. (assuming batch size of 64,
   maximum sequence length of 50 on both the source/target sequence,
   vocabulary size of 50000, and id27 size equal to id56 size):

   (prealloc = 0)
     * 1-layer, 100 hidden units: 0.7g, 21.5k tokens/sec
     * 1-layer, 250 hidden units: 1.4g, 14.1k tokens/sec
     * 1-layer, 500 hidden units: 2.6g, 9.4k tokens/sec
     * 2-layers, 500 hidden units: 3.2g, 7.4k tokens/sec
     * 4-layers, 1000 hidden units: 9.4g, 2.5k tokens/sec

   thanks to some fantastic work from folks at [105]systran, turning
   prealloc on will lead to much more memory efficient training

   (prealloc = 1)
     * 1-layer, 100 hidden units: 0.5g, 22.4k tokens/sec
     * 1-layer, 250 hidden units: 1.1g, 14.5k tokens/sec
     * 1-layer, 500 hidden units: 2.1g, 10.0k tokens/sec
     * 2-layers, 500 hidden units: 2.3g, 8.2k tokens/sec
     * 4-layers, 1000 hidden units: 6.4g, 3.3k tokens/sec

   tokens/sec refers to total (i.e. source + target) tokens processed per
   second. if using different batch sizes/sequence length, you should
   (linearly) scale the above numbers accordingly. you can make use of
   memory on multiple gpus by using -gpu  option in train.lua. this will
   put the encoder on the gpu specified by -gpuid, and the decoder on the
   gpu specified by -gpu .

evaluation

   for translation, evaluation via id7 can be done by taking the output
   from beam.lua and using the multi-id7.perl script from [106]moses. for
   example
perl multi-id7.perl gold.txt < pred.txt

evaluation of states and attention

   attention_extraction.lua can be used to extract the attention and the
   lstm states. it uses the following (required) options:
     * model: path to model .t7 file.
     * src_file: source sequence to decode (one line per sequence).
     * targ_file: true target sequence.
     * src_dict: path to source vocabulary (*.src.dict file from
       preprocess.py).
     * targ_dict: path to target vocabulary (*.targ.dict file from
       preprocess.py).

   output of the script are two files, encoder.hdf5 and decoder.hdf5. the
   encoder contains the states for every layer of the encoder lstm and the
   offsets for the start of each source sentence. the decoder contains the
   states for the decoder lstm layers and the offsets for the start of
   gold sentence. it additionally contains the attention for each time
   step (if the model uses attention).

pre-trained models

   we've uploaded english <-> german models trained on 4 million sentences
   from [107]workshop on machine translation 2015. download link is below:

   [108]https://drive.google.com/open?id=0bzhmyiowlrn_aevnd0zncwd0y2c

   these models are 4-layer lstms with 1000 hidden units and essentially
   replicates the results from [109]effective approaches to
   attention-based id4, luong et al. emnlp 2015.

acknowledgments

   our implementation utilizes code from the following:
     * [110]andrej karpathy's char-id56 repo
     * [111]wojciech zaremba's lstm repo
     * [112]element id56 library

licence

   mit

     *    2019 github, inc.
     * [113]terms
     * [114]privacy
     * [115]security
     * [116]status
     * [117]help

     * [118]contact github
     * [119]pricing
     * [120]api
     * [121]training
     * [122]blog
     * [123]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [124]reload to refresh your
   session. you signed out in another tab or window. [125]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/harvardnlp/id195-attn/commits/master.atom
   3. https://github.com/harvardnlp/id195-attn#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/harvardnlp/id195-attn
  32. https://github.com/join
  33. https://github.com/login?return_to=/harvardnlp/id195-attn
  34. https://github.com/harvardnlp/id195-attn/watchers
  35. https://github.com/login?return_to=/harvardnlp/id195-attn
  36. https://github.com/harvardnlp/id195-attn/stargazers
  37. https://github.com/login?return_to=/harvardnlp/id195-attn
  38. https://github.com/harvardnlp/id195-attn/network/members
  39. https://github.com/harvardnlp
  40. https://github.com/harvardnlp/id195-attn
  41. https://github.com/harvardnlp/id195-attn
  42. https://github.com/harvardnlp/id195-attn/issues
  43. https://github.com/harvardnlp/id195-attn/pulls
  44. https://github.com/harvardnlp/id195-attn/projects
  45. https://github.com/harvardnlp/id195-attn/pulse
  46. https://github.com/join?source=prompt-code
  47. http://nlp.seas.harvard.edu/code
  48. https://github.com/harvardnlp/id195-attn/commits/master
  49. https://github.com/harvardnlp/id195-attn/branches
  50. https://github.com/harvardnlp/id195-attn/releases
  51. https://github.com/harvardnlp/id195-attn/graphs/contributors
  52. https://github.com/harvardnlp/id195-attn/blob/master/license
  53. https://github.com/harvardnlp/id195-attn/search?l=lua
  54. https://github.com/harvardnlp/id195-attn/search?l=python
  55. https://github.com/harvardnlp/id195-attn/find/master
  56. https://github.com/harvardnlp/id195-attn/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/harvardnlp/id195-attn
  58. https://github.com/join?return_to=/harvardnlp/id195-attn
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/yoonkim
  64. https://github.com/harvardnlp/id195-attn/commits?author=yoonkim
  65. https://github.com/harvardnlp/id195-attn/commit/2d36b4348dad5a18571495feacddcf8c6620a678
  66. https://github.com/harvardnlp/id195-attn/pull/82
  67. https://github.com/harvardnlp/id195-attn/commit/2d36b4348dad5a18571495feacddcf8c6620a678
  68. https://github.com/harvardnlp/id195-attn/commit/2d36b4348dad5a18571495feacddcf8c6620a678
  69. https://github.com/harvardnlp/id195-attn/tree/2d36b4348dad5a18571495feacddcf8c6620a678
  70. https://github.com/harvardnlp/id195-attn/tree/master/data
  71. https://github.com/harvardnlp/id195-attn/commit/cb18ac30bb3a586924d2fcfacc5a67a20ede81c8
  72. https://github.com/harvardnlp/id195-attn/tree/master/s2sa
  73. https://github.com/harvardnlp/id195-attn/commit/879fd9bb364d0584a43c97a1e946495d87923430
  74. https://github.com/harvardnlp/id195-attn/blob/master/.gitignore
  75. https://github.com/harvardnlp/id195-attn/commit/26edeba0e0c558c7176f644de2aec380e939d187
  76. https://github.com/harvardnlp/id195-attn/blob/master/license
  77. https://github.com/harvardnlp/id195-attn/blob/master/readme.md
  78. https://github.com/harvardnlp/id195-attn/blob/master/convert_to_cpu.lua
  79. https://github.com/harvardnlp/id195-attn/blob/master/evaluate.lua
  80. https://github.com/harvardnlp/id195-attn/blob/master/preprocess-shards.py
  81. https://github.com/harvardnlp/id195-attn/blob/master/preprocess.py
  82. https://github.com/harvardnlp/id195-attn/commit/852ee994c6eca42dbe8cd997ec3a5e8fc46a4dfe
  83. https://github.com/harvardnlp/id195-attn/blob/master/prune.lua
  84. https://github.com/harvardnlp/id195-attn/blob/master/train.lua
  85. http://openid4.net/
  86. http://torch.ch/
  87. http://arxiv.org/abs/1505.00387
  88. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
  89. http://arxiv.org/abs/1508.06615
  90. http://www.systransoft.com/
  91. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
  92. https://aclweb.org/anthology/p/p16/p16-2058.pdf
  93. https://arxiv.org/pdf/1606.09274.pdf
  94. https://arxiv.org/pdf/1606.07947.pdf
  95. https://arxiv.org/pdf/1606.04199
  96. https://arxiv.org/pdf/1607.01628
  97. https://arxiv.org/pdf/1606.02892
  98. http://people.fas.harvard.edu/~yoonkim
  99. http://www.statmt.org/wmt15/translation-task.html
 100. https://github.com/harvardnlp/sent-summary
 101. https://arxiv.org/abs/1607.01628
 102. https://github.com/clab/fast_align
 103. https://arxiv.org/abs/1606.02892
 104. http://arxiv.org/pdf/1606.09274v1.pdf
 105. http://www.systransoft.com/
 106. https://github.com/moses-smt/mosesdecoder
 107. http://www.statmt.org/wmt15/translation-task.html
 108. https://drive.google.com/open?id=0bzhmyiowlrn_aevnd0zncwd0y2c
 109. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
 110. https://github.com/karpathy/char-id56
 111. https://github.com/wojzaremba/lstm
 112. https://github.com/element-research/id56
 113. https://github.com/site/terms
 114. https://github.com/site/privacy
 115. https://github.com/security
 116. https://githubstatus.com/
 117. https://help.github.com/
 118. https://github.com/contact
 119. https://github.com/pricing
 120. https://developer.github.com/
 121. https://training.github.com/
 122. https://github.blog/
 123. https://github.com/about
 124. https://github.com/harvardnlp/id195-attn
 125. https://github.com/harvardnlp/id195-attn

   hidden links:
 127. https://github.com/
 128. https://github.com/harvardnlp/id195-attn
 129. https://github.com/harvardnlp/id195-attn
 130. https://github.com/harvardnlp/id195-attn
 131. https://help.github.com/articles/which-remote-url-should-i-use
 132. https://github.com/harvardnlp/id195-attn#sequence-to-sequence-learning-with-attentional-neural-networks
 133. https://github.com/harvardnlp/id195-attn#dependencies
 134. https://github.com/harvardnlp/id195-attn#python
 135. https://github.com/harvardnlp/id195-attn#lua
 136. https://github.com/harvardnlp/id195-attn#quickstart
 137. https://github.com/harvardnlp/id195-attn#details
 138. https://github.com/harvardnlp/id195-attn#preprocessing-options-preprocesspy
 139. https://github.com/harvardnlp/id195-attn#training-options-trainlua
 140. https://github.com/harvardnlp/id195-attn#decoding-options-beamlua
 141. https://github.com/harvardnlp/id195-attn#using-additional-input-features
 142. https://github.com/harvardnlp/id195-attn#pruning-a-model
 143. https://github.com/harvardnlp/id195-attn#switching-between-gpucpu-models
 144. https://github.com/harvardnlp/id195-attn#gpu-memory-requirementstraining-speed
 145. https://github.com/harvardnlp/id195-attn#evaluation
 146. https://github.com/harvardnlp/id195-attn#evaluation-of-states-and-attention
 147. https://github.com/harvardnlp/id195-attn#pre-trained-models
 148. https://github.com/harvardnlp/id195-attn#acknowledgments
 149. https://github.com/harvardnlp/id195-attn#licence
 150. https://github.com/
