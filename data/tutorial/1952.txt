   #[1]sachin joglekar's blog    feed [2]sachin joglekar's blog    comments
   feed [3]sachin joglekar's blog    id116 id91 with tensorflow
   comments feed [4]traffic lights and nash equilibrium [5]recursively
   copying elements from one graph to another in tensorflow [6]alternate
   [7]alternate [8]sachin joglekar's blog [9]wordpress.com

   [10]skip to content

[11]sachin joglekar's blog

programming | python | ml

primary menu

   (button) menu
     * [12]home
     * [13]about

id116 id91 with tensorflow

   [14]14/11/201516/11/2015[15]srjoglekar246

   google recently open-sourced its artificial intelligence/numerical
   computing library called [16]tensorflow. tensorflow was developed by
   members of the google brain team, and has the flexibility to run on a
   variety of platforms     including gpus and mobile devices.

   [17]687474703a2f2f7777772e616e64726f696463656e7472616c2e636f6d2f7369746
   5732f616e64726f696463656e7472616c2e636f6d2f66696c65732f7374796c65732f6c
   617267652f7075626c69632f61727469636c655f696d616765732f323031352f31312f7
   4656e736f72666c6f772e706e67

   tensorflow   s methodology uses what they called data-flow graphs.
   consider the following diagram from the wikipedia page on [18]genetic
   programming (which could have some interesting applications with
   tensorflow, i think):

   [19]genetic_program_tree as you probably understood, the graphical
   structure is a way of representing a computational expression in the
   form of a tree. every node is an operation (tensorflow calls them ops,
   short for operations). the non-leaf nodes are pretty easy to
   understand. some leaf nodes are a special case of an operation, always
      returning    a constant value (like 7 or 2.2 in the tree). others (like
   x or y) act as placeholders that will be fed in at the time of
   execution. if you look at the arrows, you will realize that their
   directions denote the dependencies between outputs of different nodes.
   hence, data (tensorflow calls them tensors) will flow in the opposite
   direction along each node     hence the name tensorflow. tensorflow
   provides other components over this graphical abstraction, like
   persistent memory elements that retain data (called variables), and
   optimization techniques to fine-tune the parameters in these variables
   in applications like neural networks.

   tensorflow has a powerful python api. the tensorflow team has done an
   awesome job of writing the documentation (which is a little tricky to
   navigate). if you are completely new to this, heres a few links to get
   you started (in the order you should visit them):

   1. [20]basic setup

   2. read [21]this example to get a vague idea of what a tensorflow code
   looks like.

   3. now read [22]this explanation of the basic components of tensorflow.
   it helps if you read the above example again, or simultaneously.

   4. read [23]this detailed example of using tensorflow for a common ml
   problem.

   5. once you have a decent understanding of the basic components and how
   they work, you can look at the [24]python docs for reference.

   now here is the code i wrote for id116 id91 using tensorflow.
   as a disclaimer, i will mention that this code is based on my (at the
   time of writing this) 2-day old understanding of how the library works.
   if you find any errors or know any optimizations possible, do drop a
   comment! the code is heavily documented, so do go through in-line docs.

import tensorflow as tf
from random import choice, shuffle
from numpy import array


def tfkmeanscluster(vectors, noofclusters):
    """
    id116 id91 using tensorflow.
    'vectors' should be a n*k 2-d numpy array, where n is the number
    of vectors of dimensionality k.
    'noofclusters' should be an integer.
    """

    noofclusters = int(noofclusters)
    assert noofclusters < len(vectors)

    #find out the dimensionality
    dim = len(vectors[0])

    #will help select random centroids from among the available vectors
    vector_indices = list(range(len(vectors)))
    shuffle(vector_indices)

    #graph of computation
    #we initialize a new graph and set it as the default during each run
    #of this algorithm. this ensures that as this function is called
    #multiple times, the default graph doesn't keep getting crowded with
    #unused ops and variables from previous function calls.

    graph = tf.graph()

    with graph.as_default():

        #session of computation

        sess = tf.session()

        ##constructing the elements of computation

        ##first lets ensure we have a variable vector for each centroid,
        ##initialized to one of the vectors from the available data points
        centroids = [tf.variable((vectors[vector_indices[i]]))
                     for i in range(noofclusters)]
        ##these nodes will assign the centroid variables the appropriate
        ##values
        centroid_value = tf.placeholder("float64", [dim])
        cent_assigns = []
        for centroid in centroids:
            cent_assigns.append(tf.assign(centroid, centroid_value))

        ##variables for cluster assignments of individual vectors(initialized
        ##to 0 at first)
        assignments = [tf.variable(0) for i in range(len(vectors))]
        ##these nodes will assign an assignment variable the appropriate
        ##value
        assignment_value = tf.placeholder("int32")
        cluster_assigns = []
        for assignment in assignments:
            cluster_assigns.append(tf.assign(assignment,
                                             assignment_value))

        ##now lets construct the node that will compute the mean
        #the placeholder for the input
        mean_input = tf.placeholder("float", [none, dim])
        #the node/op takes the input and computes a mean along the 0th
        #dimension, i.e. the list of input vectors
        mean_op = tf.reduce_mean(mean_input, 0)

        ##node for computing euclidean distances
        #placeholders for input
        v1 = tf.placeholder("float", [dim])
        v2 = tf.placeholder("float", [dim])
        euclid_dist = tf.sqrt(tf.reduce_sum(tf.pow(tf.sub(
            v1, v2), 2)))

        ##this node will figure out which cluster to assign a vector to,
        ##based on euclidean distances of the vector from the centroids.
        #placeholder for input
        centroid_distances = tf.placeholder("float", [noofclusters])
        cluster_assignment = tf.argmin(centroid_distances, 0)

        ##initializing state variables

        ##this will help initialization of all variables defined with respect
        ##to the graph. the variable-initializer should be defined after
        ##all the variables have been constructed, so that each of them
        ##will be included in the initialization.
        init_op = tf.initialize_all_variables()

        #initialize all variables
        sess.run(init_op)

        ##id91 iterations

        #now perform the expectation-maximization steps of id116 id91
        #iterations. to keep things simple, we will only do a set number of
        #iterations, instead of using a stopping criterion.
        noofiterations = 100
        for iteration_n in range(noofiterations):

            ##expectation step
            ##based on the centroid locations till last iteration, compute
            ##the _expected_ centroid assignments.
            #iterate over each vector
            for vector_n in range(len(vectors)):
                vect = vectors[vector_n]
                #compute euclidean distance between this vector and each
                #centroid. remember that this list cannot be named
                #'centroid_distances', since that is the input to the
                #cluster assignment node.
                distances = [sess.run(euclid_dist, feed_dict={
                    v1: vect, v2: sess.run(centroid)})
                             for centroid in centroids]
                #now use the cluster assignment node, with the distances
                #as the input
                assignment = sess.run(cluster_assignment, feed_dict = {
                    centroid_distances: distances})
                #now assign the value to the appropriate state variable
                sess.run(cluster_assigns[vector_n], feed_dict={
                    assignment_value: assignment})

            ##maximization step
            #based on the expected state computed from the expectation step,
            #compute the locations of the centroids so as to maximize the
            #overall objective of minimizing within-cluster sum-of-squares
            for cluster_n in range(noofclusters):
                #collect all the vectors assigned to this cluster
                assigned_vects = [vectors[i] for i in range(len(vectors))
                                  if sess.run(assignments[i]) == cluster_n]
                #compute new centroid location
                new_location = sess.run(mean_op, feed_dict={
                    mean_input: array(assigned_vects)})
                #assign value to appropriate variable
                sess.run(cent_assigns[cluster_n], feed_dict={
                    centroid_value: new_location})

        #return centroids and assignments
        centroids = sess.run(centroids)
        assignments = sess.run(assignments)
        return centroids, assignments


   never, ever, ever, do something like this:

for i in range(100):
    x = sess.run(tf.assign(variable1, placeholder))


   this may seem pretty harid113ss at first glance, but every time you
   initialize an op, (like tf.assign or even tf.zeros, you are adding new
   ops instances to the default graph. instead, as shown in the code,
   define a particular op for each task (however specialized) just once in
   the code. then, during every of your iterations, call sess.run over the
   required nodes. to check if you are crowding your graph with
   unnecessary ops, just print out the value of
   len(graph.get_operations()) during every iteration and see if its is
   increasing. in fact, sess.run should be the only way you interact with
   the graph during every iteration.

   as visible on lines 138 and 139, you can call sess.run over a list of
   ops/variables to return a list of the outputs in the same order.

   there are a lot of intricacies of tensorflow that this code does not go
   into, such as assigning devices to nodes, graph collections,
   dependencies, etc. thats partially because i am still understanding
   these aspects one by one. but at first glance, tensorflow seems to be a
   pretty powerful and flexible way of doing ai/ml-based computations. i
   would personally like to explore its applications in developing
   dependency-based statistical metrics for data     for which i am
   currently using custom tree-like data structures. lets hope this
   gesture by google does lead to an increase in the applications and
   research in ai. cheers!
   advertisements

share this:

     * [25]twitter
     * [26]facebook
     *

like this:

   like loading...

   categories: [27]code snippets, [28]machine learning/ai, [29]programming
   tags: [30]artificial intelligence, [31]id91, [32]google,
   [33]graph, [34]python, [35]tensorflow

post navigation

   [36]    traffic lights and nash equilibrium
   [37]recursively copying elements from one graph to another
   in tensorflow    

21 thoughts on    id116 id91 with tensorflow   

    1.
   [38]this contact form says:
       [39]16/11/2015 at 14:59
       hey, i think your site might be having browser compatibility
       issues. whhen i look at your blog iin safari, it looks fine but
       when opening in internet explorer, iit has
       some overlapping. i just wanted to give you a quick heads
       up! othwr then that, terrific blog!
       [40]reply
         1.
        [41]srjoglekar246 says:
            [42]16/11/2015 at 15:05
            thanks! and thanks for the pointing out. will look into it     
            [43]reply
    2.
   [44]rahjoshi says:
       [45]16/11/2015 at 15:22
       so quick and awesome article after releasing tensorflow
       [46]reply
         1.
        [47]srjoglekar246 says:
            [48]16/11/2015 at 15:24
            thanks!
            [49]reply
    3. pingback: [50]big analytics roundup (november 16, 2015) | the big
       analytics blog
    4.
   [51]shawn simister (@narphorium) says:
       [52]18/11/2015 at 01:47
       hey sachin, i   ve made some changes to your code to make it run
       faster (although it uses more memory).
       you can see how i did it here:
       [53]https://gist.github.com/narphorium/d06b7ed234287e319f18
       [54]reply
         1.
        [55]srjoglekar246 says:
            [56]18/11/2015 at 09:43
            i like the changes you made! especially better use of the
            available tensor containers. i am still getting used to the
            whole dimensions and slicing and concat stuff (always hated it
            in numpy too).
            [57]reply
    5. pingback: [58]  qu   puedes hacer con un cl  ster de 80 d  lares basado
       en las raspberry pi zero? | nuevo titulo |
    6. pingback: [59]tensorflow resources | bits and pieces
    7. pingback: [60]self-organizing maps with google   s tensorflow | a
       bunch of data
    8. pingback: [61]recursively copying elements from one graph to
       another in tensorflow | a bunch of data
    9. pingback: [62]id116 id91 with tensorflow | a bunch of data
   10. pingback: [63]tensorflow resources | bits and pieces
   11.
   pan jin says:
       [64]22/10/2016 at 17:38
       very nice job. thanks a lot.
       i want to remind the author, there is some problem when i use the
       project. the input parameters    vectors    should have a data type of
       float64.
       but as a user, i naturally put the vectors as integers. users don   t
       know it need a input of float.
       hope you can add some remind in the function.
       thank you for your job again!
       [65]reply
   12. pingback: [66]hot reads for this week in machine learning and deep
       learning     everything artificial intelligence
   13.
   [67]kai staats says:
       [68]27/03/2017 at 05:23
       in the course of research for a paper about the application of
       tensorflow to genetic programming, i came across this post. you may
       be pleased to learn that there is now a python-based genetic
       programmig application suite which incorporates tensorflow:
       [69]http://kstaats.github.io/karoo_gp/
       we are very pleased with the results. the performance with tf is
       truly outstanding.
       [70]reply
   14.
   dcohron says:
       [71]14/04/2017 at 06:39
       this is really very good. i thank you for sharing. this is supposed
       to be an easier case, but when you are exploring something new it
       is very helpful to have a map.
       in the euclidean distance calculation, the    tf.sub    must be
       deprecated so i substituted    euclid_dist = tf.sqrt( tf.reduce_sum
       (tf.squared_difference (v1, v2)))   . also, the
          tf.initialize_all_variables()    has changed to
          tf.global_variables_initializer()   .
       when calculating new centroid location i am getting an error
          valueerror: cannot feed value of shape (0,) for tensor
          placeholder_2:0   , which has shape    (?, 24)    at    new_location =
       sess.run(mean_op, feed_dict={mean_input: array(assigned_vects)})   
       at the very end of your script. any ideas how to fix this?
       [72]reply
         1.
        [73]srjoglekar246 says:
            [74]17/04/2017 at 15:46
            there appears to be some mis-match between the dimensionality
            of two vectors you use. i haven   t used the python-tf in a
            while now, so i am pretty rusty. though did you try just
            printing out the two vectors (or all that appear in the
            offending expression), so you know whats missing? i used this
            crude hack a lot while writing this early code.
            [75]reply
   15. pingback: [76]install tensor flow using anaconda ide     myeventzone
   16. pingback: [77]tensorflow         id116                      |             
   17. pingback: [78]tensorflow         id116                      -                      

leave a reply [79]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [80]googleplus-sign-in

     *
     *

   [81]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [82]log out /
   [83]change )
   google photo

   you are commenting using your google account. ( [84]log out /
   [85]change )
   twitter picture

   you are commenting using your twitter account. ( [86]log out /
   [87]change )
   facebook photo

   you are commenting using your facebook account. ( [88]log out /
   [89]change )
   [90]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   ____________________

   (button) follow

social

     * [91]view srjoglekar   s profile on facebook
     * [92]view srjoglekar   s profile on linkedin
     * [93]view srjoglekar246   s profile on github

top posts & pages

     * [94]id28 (for dummies)
       [95]id28 (for dummies)
     * [96]the magic behind attribute access in python
       [97]the magic behind attribute access in python
     * [98]nelder-mead optimization
       [99]nelder-mead optimization
     * [100]an introduction to bayesian belief networks
       [101]an introduction to bayesian belief networks
     * [102]linear and quadratic discriminant analysis for ml / statistics
       newbies
       [103]linear and quadratic discriminant analysis for ml / statistics
       newbies

blog stats

     * 426,317 hits

posts by time

   posts by time [select month_______]

categories

     * [104]code snippets
     * [105]google summer of code 2014
     * [106]machine learning/ai
     * [107]programming
     * [108]uncategorized

recent comments

   [109]kitchen set jati on [110]efficient computation and stor   
   yara zakaria on [111]cross validation and the bias-   
   [112]contoh gorden rumah    on [113]efficient computation and stor   
   [114]how to use influxdb       on [115]nelder-mead optimization
   [116]time traveller on [117]cross validation and the bias-   

   [118]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [119]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [120]cookie policy

   iframe: [121]likes-master

   %d bloggers like this:

references

   visible links
   1. https://codesachin.wordpress.com/feed/
   2. https://codesachin.wordpress.com/comments/feed/
   3. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/feed/
   4. https://codesachin.wordpress.com/2015/11/11/traffic-lights-and-nash-equilibrium/
   5. https://codesachin.wordpress.com/2015/11/20/recursively-copying-elements-from-one-graph-to-another-in-tensorflow/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/&for=wpcom-auto-discovery
   8. https://codesachin.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#content
  11. https://codesachin.wordpress.com/
  12. https://codesachin.wordpress.com/
  13. https://codesachin.wordpress.com/about/
  14. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/
  15. https://codesachin.wordpress.com/author/srjoglekar246/
  16. http://tensorflow.org/
  17. https://codesachin.files.wordpress.com/2015/11/687474703a2f2f7777772e616e64726f696463656e7472616c2e636f6d2f73697465732f616e64726f696463656e7472616c2e636f6d2f66696c65732f7374796c65732f6c617267652f7075626c69632f61727469636c655f696d6167.png
  18. https://en.wikipedia.org/wiki/genetic_programming
  19. https://codesachin.files.wordpress.com/2015/11/genetic_program_tree.png
  20. http://tensorflow.org/get_started/os_setup.md
  21. http://tensorflow.org/get_started
  22. http://tensorflow.org/get_started/basic_usage.md
  23. http://tensorflow.org/tutorials/mnist/beginners/index.md
  24. http://tensorflow.org/api_docs/python/index.md
  25. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?share=twitter
  26. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?share=facebook
  27. https://codesachin.wordpress.com/category/code-snippets/
  28. https://codesachin.wordpress.com/category/machine-learningai/
  29. https://codesachin.wordpress.com/category/programming/
  30. https://codesachin.wordpress.com/tag/artificial-intelligence/
  31. https://codesachin.wordpress.com/tag/id91/
  32. https://codesachin.wordpress.com/tag/google/
  33. https://codesachin.wordpress.com/tag/graph/
  34. https://codesachin.wordpress.com/tag/python/
  35. https://codesachin.wordpress.com/tag/tensorflow/
  36. https://codesachin.wordpress.com/2015/11/11/traffic-lights-and-nash-equilibrium/
  37. https://codesachin.wordpress.com/2015/11/20/recursively-copying-elements-from-one-graph-to-another-in-tensorflow/
  38. http://janny982.bravesites.com/
  39. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-144
  40. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=144#respond
  41. https://codesachin.wordpress.com/
  42. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-145
  43. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=145#respond
  44. http://gravatar.com/rahjoshi
  45. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-146
  46. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=146#respond
  47. https://codesachin.wordpress.com/
  48. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-147
  49. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=147#respond
  50. http://thomaswdinsmore.com/2015/11/16/big-analytics-roundup-november-16-2015/
  51. http://twitter.com/narphorium
  52. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-150
  53. https://gist.github.com/narphorium/d06b7ed234287e319f18
  54. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=150#respond
  55. https://codesachin.wordpress.com/
  56. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-153
  57. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=153#respond
  58. http://nuevotitulo.com/?p=23947
  59. https://srippa.wordpress.com/2015/11/21/tensorflow-resources/
  60. http://abunchofdata.com/self-organizing-maps-with-googles-tensorflow/
  61. http://abunchofdata.com/recursively-copying-elements-from-one-graph-to-another-in-tensorflow/
  62. http://abunchofdata.com/id116-id91-with-tensorflow/
  63. https://srippa.wordpress.com/2016/07/15/tensorflow-resources-2/
  64. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-579
  65. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=579#respond
  66. https://aichamp.wordpress.com/2016/11/24/hot-reads-for-this-week-in-machine-learning-and-deep-learning/
  67. http://www.kaistaats.com/research/
  68. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-821
  69. http://kstaats.github.io/karoo_gp/
  70. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=821#respond
  71. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-853
  72. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=853#respond
  73. https://codesachin.wordpress.com/
  74. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-861
  75. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/?replytocom=861#respond
  76. https://myeventzone.wordpress.com/2017/11/02/install-tensor-flow-using-anaconda-ide/
  77. https://codertw.com/            /16132/
  78. https://com-it.tech/archives/100383
  79. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#respond
  80. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://codesachin.wordpress.com&color_scheme=light
  81. https://gravatar.com/site/signup/
  82. javascript:highlandercomments.doexternallogout( 'wordpress' );
  83. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/
  84. javascript:highlandercomments.doexternallogout( 'googleplus' );
  85. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/
  86. javascript:highlandercomments.doexternallogout( 'twitter' );
  87. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/
  88. javascript:highlandercomments.doexternallogout( 'facebook' );
  89. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/
  90. javascript:highlandercomments.cancelexternalwindow();
  91. https://www.facebook.com/srjoglekar/
  92. https://www.linkedin.com/in/srjoglekar/
  93. https://github.com/srjoglekar246/
  94. https://codesachin.wordpress.com/2015/08/16/logistic-regression-for-dummies/
  95. https://codesachin.wordpress.com/2015/08/16/logistic-regression-for-dummies/
  96. https://codesachin.wordpress.com/2016/06/09/the-magic-behind-attribute-access-in-python/
  97. https://codesachin.wordpress.com/2016/06/09/the-magic-behind-attribute-access-in-python/
  98. https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/
  99. https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/
 100. https://codesachin.wordpress.com/2017/03/10/an-introduction-to-bayesian-belief-networks/
 101. https://codesachin.wordpress.com/2017/03/10/an-introduction-to-bayesian-belief-networks/
 102. https://codesachin.wordpress.com/2015/08/25/linear-and-quadratic-discriminant-analysis-for-ml-statistics-newbies/
 103. https://codesachin.wordpress.com/2015/08/25/linear-and-quadratic-discriminant-analysis-for-ml-statistics-newbies/
 104. https://codesachin.wordpress.com/category/code-snippets/
 105. https://codesachin.wordpress.com/category/google-summer-of-code-2014/
 106. https://codesachin.wordpress.com/category/machine-learningai/
 107. https://codesachin.wordpress.com/category/programming/
 108. https://codesachin.wordpress.com/category/uncategorized/
 109. http://www.jualproduksharppoint.zone.id/
 110. https://codesachin.wordpress.com/2015/07/03/efficient-computation-and-storage-of-basic-data-statistics-using-redis/comment-page-1/#comment-32123
 111. https://codesachin.wordpress.com/2015/08/30/cross-validation-and-the-bias-variance-tradeoff-for-dummies/comment-page-1/#comment-31135
 112. http://www.kitchensetminimalisjakarta.zone.id/
 113. https://codesachin.wordpress.com/2015/07/03/efficient-computation-and-storage-of-basic-data-statistics-using-redis/comment-page-1/#comment-30209
 114. https://www.influxdata.com/blog/how-to-use-influxdbs-holt-winters-function-for-predictions/
 115. https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/comment-page-1/#comment-29092
 116. http://gravatar.com/timetraveller123
 117. https://codesachin.wordpress.com/2015/08/30/cross-validation-and-the-bias-variance-tradeoff-for-dummies/comment-page-1/#comment-29075
 118. https://wordpress.com/?ref=footer_website
 119. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/
 120. https://automattic.com/cookies
 121. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 123. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-form-guest
 124. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-form-load-service:wordpress.com
 125. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-form-load-service:twitter
 126. https://codesachin.wordpress.com/2015/11/14/id116-id91-with-tensorflow/#comment-form-load-service:facebook
 127. http://www.jualproduksharppoint.zone.id/
 128. http://www.kitchensetminimalisjakarta.zone.id/
 129. https://www.influxdata.com/blog/how-to-use-influxdbs-holt-winters-function-for-predictions/
 130. http://gravatar.com/timetraveller123
