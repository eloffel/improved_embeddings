   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   recursive (not recurrent!) neural networks in tensorflow comments feed
   [5]jimdo: data scientist [6]id111 101: id96

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]jun    [29]tutorials,
   overviews    recursive (not recurrent!) neural networks in tensorflow
   ( [30]16:n24 )

recursive (not recurrent!) neural networks in tensorflow

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]neural networks, [34]tensorflow

   learn how to implement id56s in tensorflow, which
   can be used to learn tree-like structures, or directed acyclic graphs.
     __________________________________________________________________

   by alireza nejati, university of auckland.

   for the past few days i   ve been working on how to
   implement id56s in tensorflow. recursive neural
   networks (which i   ll call treenets from now on to avoid confusion with
   recurrent neural nets) can be used for learning tree-like structures
   (more generally, [35]directed acyclic graph structures). they are
   highly useful for parsing natural scenes and language; see
   the [36]work of richard socher (2011) for examples. more recently, in
   2014, ozan   rsoy used a deep variant of treenets to obtain
   some [37]interesting nlp results.

   the best way to explain treenet architecture is, i think, to compare
   with other kinds of architectures, for example with id56s:

                                text4155.png

   in id56s, at each time step the network takes as input its previous
   state s(t-1) and its current input x(t) and produces an output y(t) and
   a new hidden state s(t). treenets, on the other hand, don   t have a
   simple linear structure like that. with id56s, you can    unroll    the net
   and think of it as a large feedforward net with inputs x(0), x(1),    ,
   x(t), initial state s(0), and outputs y(0),y(1),   ,y(t), with t varying
   depending on the input data stream, and the weights in each of the
   cells tied with each other. you can also think of treenets by unrolling
   them     the weights in each branch node are tied with each other, and
   the weights in each leaf node are tied with each other. the treenet
   illustrated above has different numbers of inputs in the branch nodes.
   usually, we just restrict the treenet to be a binary tree     each node
   either has one or two input nodes. there may be different types of
   branch nodes, but branch nodes of the same type have tied weights.

   the advantage of treenets is that they can be very powerful in learning
   hierarchical, tree-like structure. the disadvantages are, firstly, that
   the tree structure of every input sample must be known at training
   time. we will represent the tree structure like this (lisp-like
   notation):
(s (np that movie) (vp was) (adjp cool))

   in each sub-expression, the type of the sub-expression must be given    
   in this case, we are parsing a sentence, and the type of the
   sub-expression is simply the part-of-speech (pos) tag. you can see that
   expressions with three elements (one head and two tail elements)
   correspond to binary operations, whereas those with four elements (one
   head and three tail elements) correspond to trinary operations, etc.

   the second disadvantage of treenets is that training is hard because
   the tree structure changes for each training sample and it   s not easy
   to map training to mini-batches and so on.

implementation in tensorflow


   there are a few methods for training treenets. the method we   re going
   to be using is a method that is probably the simplest, conceptually. it
   consists of simply assigning a tensor to every single intermediate
   form. so, for instance, imagine that we want to train on simple
   mathematical expressions, and our input expressions are the following
   (in lisp-like notation):
1
(+ 1 2)
(* (+ 2 1) 2)
(+ (* 1 2) (+ 2 1))

   now our full list of intermediate forms is:
a = 1
b = 2
c = (+ a b)
d = (+ b a)
e = (* d b)
f = (* a b)
g = (+ f d)

   for example, f = (* 1 2), and g = (+ (* 1 2) (+ 2 1)). we can see that
   all of our intermediate forms are simple expressions of other
   intermediate forms (or inputs). each of these corresponds to a separate
   sub-graph in our tensorflow graph. so, for instance, for *, we would
   have two matrices w_times_l andw_times_r, and one bias
   vector bias_times. and for computing f, we would have:
f = relu(w_times_l * a + w_times_r * b + bias_times)

   similarly, for computing d we would have:
d = relu(w_plus_l * b + w_plus_r * a + bias_plus)

   the full intermediate graph (excluding input and loss calculation)
   looks like:
a = w_input * [1, 0]
b = w_input * [0, 1]
c = relu(w_plus_l  * a + w_plus_r  * b + bias_plus)
d = relu(w_plus_l  * b + w_plus_r  * a + bias_plus)
e = relu(w_times_l * d + w_times_r * b + bias_times)
f = relu(w_times_l * a + w_times_r * b + bias_times)
g = relu(w_plus_l  * f + w_plus_r  * d + bias_plus)
output1 = sigmoid(w_output * a)
output2 = sigmoid(w_output * c)
output3 = sigmoid(w_output * e)
output4 = sigmoid(w_output * g)

   for training, we simply initialize our inputs and outputs as one-hot
   vectors (here, we   ve set the symbol 1 to [1, 0] and the symbol 2 to [0,
   1]), and perform id119 over all w and bias matrices in our
   graph. the advantage of this method is that, as i said, it   s
   straightforward and easy to implement. the disadvantage is that our
   graph complexity grows as a function of the input size. this isn   t as
   bad as it seems at first, because no matter how big our data set
   becomes, there will only ever be one training example (since the entire
   data set is trained simultaneously) and so even though the size of the
   graph grows, we only need a single pass through the graph per training
   epoch. however, it seems likely that if our graph grows to very large
   size (millions of data points) then we need to look at batch training.

   batch training actually isn   t that hard to implement; it just makes it
   a bit harder to see the flow of information. we can represent a    batch   
   as a list of variables: [a, b, c]. so, in our previous example, we
   could replace the operations with two batch operations:
[a, b]    = w_input * [[1, 0], [0, 1]]
[c, d, g] = relu(w_plus_l  * [a, b, f] + w_plus_r  * [b, a, d] + bias_plus)
[e, f]    = relu(w_times_l * [d, a]    + w_times_r * [b, b]    + bias_times)
output    = sigmoid(w_output * [a, c, e, g])

   you   ll immediately notice that even though we   ve rewritten it in a
   batch way, the order of variables inside the batches is totally random
   and inconsistent. this is the problem with batch training in this
   model: the batches need to be constructed separately for each pass
   through the network. if we think of the input as being a huge matrix
   where each row (or column) of the matrix is the vector corresponding to
   each intermediate form (so [a, b, c, d, e, f, g]) then we can pick out
   the variables corresponding to each batch using
   tensorflow   s tf.gather function. so for instance, gathering the
   indices [1, 0, 3] from [a, b, c, d, e, f, g]would give [b, a, d], which
   is one of the sub-batches we need. the total number of sub-batches we
   need is two for every binary operation and one for every unary
   operation in the model.

   for the sake of simplicity, i   ve only implemented the first (non-batch)
   version in tensorflow, and my early experiments show that it works. for
   example, consider predicting the parity (even or odd-ness) of a number
   given as an expression. so 1would have parity 1, (+ 1 1) (which is
   equal to 2) would have parity 0, (+ 1 (* (+ 1 1) (+ 1 1))) (which is
   equal to 5) would have parity 1, and so on. training a treenet on the
   following small set of training examples:
1
[+,1,1]
[*,1,1]
[*,[+,1,1],[+,1,1]]
[+,[+,1,1],[+,1,1]]
[+,[+,1,1],1 ]
[+,1,[+,1,1]]

   seems to be enough for it to    get the point    of parity, and it is
   capable of correctly predicting the parity of much more complicated
   inputs, for instance:
[+,[+,[+,1,1],[+,[+,1,1],[+,1,1]]],1]

   correctly, with very high accuracy (>99.9%), with accuracy only
   diminishing once the size of the inputs becomes very large. the code is
   just a single python file which you can download and run [38]here. i   ll
   give some more updates on more interesting problems in the next post
   and also release more code.

   bio: [39]al nejati is a research fellow at the university of auckland.
   he completed his phd in engineering science in 2015. he is interested
   in machine learning, image/signal processing, bayesian statistics, and
   biomedical engineering.

   [40]original. reposted with permission.

   related:
     * [41]deep learning in neural networks: an overview
     * [42]how do neural networks learn?
     * [43]the unreasonable reputation of neural networks
     __________________________________________________________________

   [44][prv.gif] previous post
   [45]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [46]another 10 free must-read books for machine learning and data
       science
    2. [47]9 must-have skills you need to become a data scientist, updated
    3. [48]who is a typical data scientist in 2019?
    4. [49]the pareto principle for data scientists
    5. [50]what no one will tell you about data science job applications
    6. [51]19 inspiring women in ai, big data, data science, machine
       learning
    7. [52]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [53]id158s optimization using genetic algorithm
       with python
    2. [54]who is a typical data scientist in 2019?
    3. [55]8 reasons why you should get a microsoft azure certification
    4. [56]the pareto principle for data scientists
    5. [57]r vs python for data visualization
    6. [58]how to work in data science, ai, big data
    7. [59]the deep learning toolset     an overview

[60]latest news

     * [61]spatio-temporal statistics: a primer
     * [62]another 10 free must-see courses for machine learning a...
     * [63]download your datax guide to ai in marketing
     * [64]kdnuggets offer: save 20% on strata in london
     * [65]training a champion: building deep neural nets for big ...
     * [66]building a recommender system

   [67]kdnuggets home    [68]news    [69]2016    [70]jun    [71]tutorials,
   overviews    recursive (not recurrent!) neural networks in tensorflow
   ( [72]16:n24 )
      2019 kdnuggets. [73]about kdnuggets.  [74]privacy policy. [75]terms
   of service

   [76]subscribe to kdnuggets news
   [77][tw_c48.png] [78]facebook [79]linkedin
   x

   [envelope.png] [80]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/06/recursive-neural-networks-tensorflow.html/feed
   5. https://www.kdnuggets.com/jobs/16/06-30-jimdo-data-scientist.html
   6. https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/06/index.html
  29. https://www.kdnuggets.com/2016/06/tutorials.html
  30. https://www.kdnuggets.com/2016/n24.html
  31. https://www.kdnuggets.com/jobs/16/06-30-jimdo-data-scientist.html
  32. https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html
  33. https://www.kdnuggets.com/tag/neural-networks
  34. https://www.kdnuggets.com/tag/tensorflow
  35. https://en.wikipedia.org/wiki/directed_acyclic_graph
  36. http://www.socher.org/index.php/main/parsingnaturalscenesandnaturallanguagewithrecursiveneuralnetworks
  37. https://www.cs.cornell.edu/~oirsoy/files/nips14drsv.pdf
  38. https://gist.github.com/anj1/504768e05fda49a6e3338e798ae1cddd
  39. http://www.des.auckland.ac.nz/people/anej001
  40. https://pseudoprofound.wordpress.com/2016/06/20/recursive-not-recurrent-neural-nets-in-tensorflow/
  41. https://www.kdnuggets.com/2016/04/deep-learning-neural-networks-overview.html
  42. https://www.kdnuggets.com/2015/12/how-do-neural-networks-learn.html
  43. https://www.kdnuggets.com/2016/01/unreasonable-reputation-neural-networks.html
  44. https://www.kdnuggets.com/jobs/16/06-30-jimdo-data-scientist.html
  45. https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html
  46. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  47. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  48. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  49. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  50. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  51. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  52. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  53. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  54. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  55. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  56. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  57. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  58. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  59. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  60. https://www.kdnuggets.com/news/index.html
  61. https://www.kdnuggets.com/2019/04/spatio-temporal-statistics-primer.html
  62. https://www.kdnuggets.com/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html
  63. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  64. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  65. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  66. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  67. https://www.kdnuggets.com/
  68. https://www.kdnuggets.com/news/index.html
  69. https://www.kdnuggets.com/2016/index.html
  70. https://www.kdnuggets.com/2016/06/index.html
  71. https://www.kdnuggets.com/2016/06/tutorials.html
  72. https://www.kdnuggets.com/2016/n24.html
  73. https://www.kdnuggets.com/about/index.html
  74. https://www.kdnuggets.com/news/privacy-policy.html
  75. https://www.kdnuggets.com/terms-of-service.html
  76. https://www.kdnuggets.com/news/subscribe.html
  77. https://twitter.com/kdnuggets
  78. https://facebook.com/kdnuggets
  79. https://www.linkedin.com/groups/54257
  80. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  82. https://www.kdnuggets.com/
  83. https://www.kdnuggets.com/
