   #[1]quora [2]next

   [3]quora
   ____________________

   sign in
   [4]louvain modularity
   [5]layman's explanations

is there a simple explanation of the louvain method of community detection?

   updatecancel

answer wiki

   3 answers
   [6]shankar iyer
   [7]shankar iyer, core data scientist at facebook (2017-present)
   [8]updated 179w ago    author has 333 answers and 1m answer views
   tl;dr/short version: communities are groups of nodes within a network
   that are more densely connected to one another than to other nodes.
   modularity is a metric that quantifies the quality of an assignment of
   nodes to communities by evaluating how much more densely connected the
   nodes within a community are compared to how connected they would be,
   on average, in a suitably defined random network. the louvain method of
   community detection is an algorithm for detecting communities in
   networks that relies upon a heuristic for maximizing the modularity.
   the method consists of repeated application of two steps. the first
   step is a "greedy" assignment of nodes to communities, favoring local
   optimizations of modularity. the second step is the definition of a new
   coarse-grained network in terms of the communities found in the first
   step. these two steps are repeated until no further
   modularity-increasing reassignments of communities are possible. the
   louvain method achieves modularities comparable to pre-existing
   algorithms, typically in less time, so it enables the study of much
   larger networks. it also generally reveals a hierarchy of communities
   at different scales, and this hierarchical perspective can be useful
   for understanding the global functioning of a network. meanwhile, there
   are certain pitfalls to interpreting the community structure uncovered
   by the louvain method; these difficulties are actually shared by all
   modularity optimization algorithms.
   i strongly recommend reading the [9]original paper on the louvain
   method [1]. it's fairly short, easy to read, and will give you a solid
   understanding of the algorithm. incidentally, one of the originators of
   the louvain method ([10]renaud lambiotte) seems to have a quora
   account; it would be awesome to get his perspective on recent
   developments and applications.
     __________________________________________________________________

   long version: below, i'm going to give my own description of the
   louvain method. to hopefully not be too redundant with the original[11]
   paper, i'm going to also focus on background, context, applications,
   and follow-up work. covering all that ground makes this answer pretty
   lengthy, so please just skip to the sections that interest you. in
   particular, if you're familiar with the problem of community detection
   and the role of modularity in community finding algorithms, you can
   skip to the louvain method section below.
   also, before i begin, i'll point out that my discussion will
   exclusively consider the case of undirected, but possibly weighted,
   links. the louvain method has been extended to the directed case, so
   interested readers should seek out the relevant literature.

motivating the problem of community detection

   a fairly common feature of complex networks is that they consist of
   sets of nodes that interact more with one another than with nodes
   outside the set. social networks, for instance, might consist of
   tightly knit communities of friends with rarer friendship ties between
   different communities. in protein interaction networks, certain groups
   of proteins interact with one another more frequently than they do with
   proteins in other groups. if you map out a network of projects going on
   in a large company, certain projects will likely have more conceptual
   overlap and mutual dependency than others; this network structure will
   hopefully be mirrored to some degree by the organizational structure
   within the company.
   in 1962, h.a. simon proposed that this type of community structure
   might be a defining characteristic of complex systems, or at least
   those like the protein interaction network, in which many interacting
   constituent elements adaptively organize to achieve some higher-order
   function (e.g., the functioning of an organism). his reasoning was that
   individual actors have a much higher chance of collectively achieving a
   higher-order function if that function can be iteratively achieved by
   constructing intermediate stable forms (also called communities or
   modules) that achieve simpler functions. the first-order intermediate
   forms would be communities in terms of the original nodes, but then
   interactions between these first-order communities could generate
   second-order communities that accomplish somewhat more complicated
   functions, and so on. in this way, a hierarchical structure can emerge
   in complex adaptive systems [2]. however, even when there isn't
   necessarily adaptive pressure towards achieving some higher-order
   function (as in the case of some social networks), community structure
   is a common observed feature of complex networks.
   when we're handed a data set describing a network, we typically don't
   know  what the underlying community structure is. we often don't even
   know how many first-order communities we should be looking for, let
   alone how many hierarchical levels of communities we should consider.
   meanwhile, for the reasons given above, the community and hierarchical
   structures are often very useful for understanding the overall
   functioning of the complex system. this motivates the search for good
   community detection algorithms.

definition of modularity

   one popular class of community detection algorithms seeks to optimize
   the so-called modularity of the community assignment. modularity is a
   metric that was proposed by newman and girvan in reference [3]. it
   quantifies the quality of a community assignment by measuring how much
   more dense the connections are within communities compared to what they
   would be in a particular type of random network.
   there are two mathematical definitions of modularity that you'll
   frequently encounter. the first is the one used in the original paper
   on the louvain method [1]:
   [math]q = \frac{1}{2m} \sum_{i,j} \left[a_{ij} - \frac{k_i
   k_j}{2m}\right] \delta(c_i, c_j)[/math]
   here, [math]a[/math] is the usual adjacency matrix, [math]k_i = \sum_j
   a_{ij}[/math] is the total link weight penetrating node [math]i[/math],
   and [math]m = \frac{1}{2}\sum_{i,j} a_{ij}[/math] is the total link
   weight in the network overall. the kronecker delta [math]\delta(c_i,
   c_j)[/math] is [math]1[/math] when nodes [math]i[/math] and
   [math]j[/math] are assigned to the same community and [math]0[/math]
   otherwise. consider one of the terms in the sum. remember that
   [math]k_i[/math] is the total link weight penetrating node
   [math]i[/math], and note that [math]\frac{k_j}{2m}[/math] is the
   average fraction of this weight that would be assigned to node
   [math]j[/math], if node [math]i[/math] assigned its link weight
   randomly to other nodes in proportion to their own link weights. then,
   [math]a_{ij} - \frac{k_i k_j}{2m}[/math] measures how strongly nodes
   [math]i[/math] and [math]j[/math] are in the real network, compared to
   how strongly connected we would expect them to be in a random network
   of the type described above.
   the other definition of modularity that you'll see is the one proposed
   in the paper by newman and girvan [3]. they defined a matrix (let's
   call it [math]b[/math], though newman and girvan used the symbol
   [math]e[/math]), where [math]b_{c_1 c_2}[/math] denotes the fraction of
   all edge weight in the network that connects community [math]c_1[/math]
   to community [math]c_2[/math]. they also defined [math]w_{c_1} =
   \sum_{c_2} b_{c_1 c_2}[/math] as the total edge weight penetrating
   community [math]c_1[/math]. then, they defined the modularity as:
   [math]q = \sum_{c} \left[b_{cc} - w^2_c\right][/math]
   it's pretty easy to show that this definition is equivalent to the one
   from the louvain method paper, but i won't write out the steps here.

modularity optimization algorithms

   returning to the first definition of modularity above, the problem of
   modularity optimization can be thought of as assigning communities such
   that the elements of the sum that contribute are as positive as
   possible (remember that the delta function kills terms in the sum
   corresponding to pairs of nodes that belong to different communities).
   the problem is that you can have situations where it makes sense to put
   nodes [math]1[/math] and [math]2[/math] in the same community and to
   put [math]2[/math] and [math]3[/math] in the same community, but where
   [math]a_{13} - \frac{k_1 k_3}{2m}[/math] is very negative, indicating
   that nodes [math]1[/math] and [math]3[/math] really shouldn't be in the
   same community. this is a simple example of a kind of [12]frustration
   that makes the modularity optimization problem really hard.
   indeed, the modularity optimization problem is actually np-hard, and
   that has motivated searches for heuristic approaches that typically do
   a good job at finding high modularity community assignments with more
   scalable complexity [1]. the modularity maximization algorithms that
   i'm aware of typically fall into one the following categories [3]:
     * agglomerative: where you iteratively group nodes into communities
     * divisive: where you progressively remove links from the network,
       and the depleted network reveals good community assignments
     * simulated annealing: where you introduce an artificial
       "temperature" and perform metropolis-like monte carlo updates while
       gradually lowering the temperature. here, you take [math]-q[/math]
       to be the energy function for the system. the proposed moves can
       consist of both agglomerative and divisive steps [4]
     * spectral: where an eigenvector of the so-called modularity matrix
       encodes the community structure

   in their original 2003 modularity paper [3], newman and girvan proposed
   a divisive algorithm for modularity optimization. the intuition behind
   their algorithm can be motivated by the following diagram:
   [main-qimg-1af269d02ef4f8299cda751bd2576c0e]
   to the naked eye, it's obvious that this network consists of two
   densely connected communities that are only linked to one another by
   the single link connecting nodes [math]5[/math] and [math]12[/math].
   that link has been highlighted in blue to emphasize the special role it
   plays. if that single link were to be cut, it would completely decouple
   the two communities. a metric that distinguishes the blue link from the
   others is the edge [13]betweenness centrality. that link lies on all
   geodesic paths from the community on the left to the community on the
   right, so it will have the highest betweenness centrality of any edge
   in the graph. more generally, edges with high betweenness will tend to
   be those that link communities rather than those that connect nodes
   within communities. therefore, newman and girvan proposed iteratively
   removing the edges with the highest betweenness centralities. after
   each removal, the betweenness of all the edges needs to be recomputed
   to figure out which edge to remove next. the procedure can be stopped
   at any point, with the connected components of the edge-depleted graph
   giving the communities. you can continue the newman-girvan procedure
   until the network is fully depleted, computing the modularity of the
   community assignment at each step, and ultimately choosing the
   intermediate step that maximized the modularity. this process takes
   time [math]o(n_n n^2_{\ell})[/math], where [math]n_n[/math] is the
   number of nodes and [math]n_{\ell}[/math] is the number of links in the
   network. if the networks under consideration are sparse so that
   [math]n_{\ell} \propto n_n[/math], this algorithm is
   [math]o(n^3_n)[/math]
   between 2003 (when the newman-girvan paper came out) and 2008 (when the
   louvain method was proposed), several faster modularity optimizing
   algorithms were proposed by:
     * clauset, newman, & moore (cnm) [5]
     * pons and latapy (pl) [6]
     * wakita and tsurami (wt) [7]

   all of these methods are agglomerative, some of them run in close to
   linear time (in the number of nodes, or by the assumption of sparsity,
   in the number of links) on sparse networks, and it was these methods
   that the louvain method's originators benchmarked their algorithm
   against (see the table at the end of the next section).

louvain method of community detection

   now, i'm going to illustrate how the louvain method works by applying
   it to a "[14]connected caveman graph." this is a network where you
   begin with [math]n_{cl}[/math] fully-connected cliques of
   [math]m[/math] nodes each. next, you arrange these cliques in a circle.
   then, you take one random link from each clique and rewire it so that
   the clique is connected to its nearest clockwise neighbor. you do this
   once for each clique, and you end up with something that looks like
   this:
   [main-qimg-f73a6471e7f65a4e86e22316189513fc]
   all links in this initial network have unit weight. this is the test
   network with which we'll explore the louvain method below. the
   "intuitive partition" here consists of the six communities of five
   nodes each that we've put in by hand.
   at the beginning of the louvain method, we assign each node to its own
   community, so in the connected caveman network above, there are 30
   initial communities, each containing one node.
   stage 1: community reassignments
   in the first stage of the louvain method, we iterate through each of
   the nodes in the network. for each node, we consider the change in
   modularity if we remove the node from its current community and place
   it in the community of one of its neighbors. we compute the modularity
   change for each of the node's neighbors. if none of these modularity
   changes are positive, we keep the node in its current community. if
   some of the modularity changes are positive, we move the node into the
   community for which the modularity change is most positive. ties can be
   resolved arbitrarily. we repeat this process for each node until one
   pass through all nodes yields no community assignment changes.
   for the initial connected caveman graph, we can figure out what will
   happen in the first few steps of this community reassignment change by
   hand. in my implementation of the louvain method, the first node
   considered was [math]29[/math], which is connected to nodes
   [math]25[/math]-[math]28[/math]. if we were to merge nodes
   [math]25[/math] and [math]29[/math] into the same community, that would
   correspond to activating the terms [math]a_{25,29} - \frac{k_{25}
   k_{29}}{2m}[/math] and [math]a_{29,25} - \frac{k_{25}
   k_{29}}{2m}[/math] in the sum above. each of these have value [math]1 -
   \frac{25}{120} = \frac{19}{24}[/math]. so performing the community
   reassignment will boost the modularity by [math]\frac{19}{12}[/math].
   actually, all of the proposed mergers for node 29 will have this same
   modularity boost, so we can accept any one of them. my implementation
   assigns node [math]29[/math] to the same community as node
   [math]25[/math] and then moves on to consider another randomly selected
   node.
   actually, in the initial graph, merging any pair of neighboring nodes
   will increase modularity by the amount that i calculated in the
   previous paragraph. in particular, this will be true even for pairs
   like nodes [math]10[/math] and [math]8[/math], which intuitively belong
   to different communities. this means that, in the initial stages of the
   community reassignment phase, pairs of nodes like this have a
   possibility of being assigned to the same community. it's an
   instructive exercise to think through how mistakes like this are
   remediated as the community reassignment stage progresses, and i
   encourage you to do so.
   here are the community assignments that my implementation of the
   louvain method finds after the first community identification stage:
   [main-qimg-7d55b3643c33987fdceced24360575c5]
   as you can see, the algorithm has already discovered the "intuitive"
   communities, which is unsurprising, since this is a rather simple
   network.
   stage 2: coarse graining
   the next stage in the louvain method is to use the communities that
   were discovered in the community reassignment stage to define a new,
   coarse-grained network. in this network, the newly discovered
   communities are the nodes. the edge weight between the nodes
   representing two communities is just the sum of the edge weights
   between the constituent, lower-level nodes of each community. the links
   within each community generate self-loops in the new, coarse-grained
   network.
   in the simple connected caveman network that we're studying, there's
   only one, unit-weight link connecting neighboring communities, so the
   links between the coarse-grained communities also have unit weight. if
   there were two or more links between communities, the new
   coarse-grained link would have weight equal to the sum of all the
   lower-level links. meanwhile, within each community, there are
   [math]\frac{5 \times 4}{2} - 1 = 9[/math] unit-weight links, so the
   self-loops have weight [math]9[/math]. here's what the coarse-grained
   network will look like:
   [main-qimg-2ecdde1f387a8e23a71e2213f28c15e7]
   in this graph, the black numbers associated with the links indicate
   edge weights, but the white numbers on the nodes are just labels for
   the communities. as a side note, i had to draw in the self-loops by
   hand because [15]networkx, while generally an awesome python package
   for doing these types of analyses, [16]doesn't appear to draw them.
   if you're writing your own implementation of the louvain method, you'll
   want to be careful about how you're assigning the diagonal matrix
   elements of the adjacency matrix (corresponding to the self-loops). to
   be able to consistently apply the modularity formulae above, you
   actually want to double the diagonal matrix elements. this means that,
   for the graph above, the diagonal elements will be [math]18[/math].
   it's worth thinking through why this choice is appropriate for
   preserving the interpretation of modularity that i mentioned above. a
   good consistency check for debugging your implementation is that, when
   you construct your coarse-grained network, [math]m =
   \frac{1}{2}\sum_{i,j} a_{ij}[/math] shouldn't change.
   repeated iteration of stages 1 and 2
   the rest of the louvain method consists of repeated application of
   stages 1 and 2. by applying stage 1 (the community reassignment phase)
   to the coarse-grained graph, you find a second tier of communities of
   communities of nodes. then, in the next application of stage 2, you
   define a new coarse-grained graph at this higher-level of the
   hierarchy. you keep going like this until an application of stage 1
   yields no reassignments. at that point, repeated application of stages
   1 and 2 will never yield any more modularity-optimizing changes, so the
   process is complete.
   for the connected caveman graph, the process terminates on the second
   community reassignment stage. we can see why if we propose a merger of
   two communities in the coarse-grained graph above. this would result in
   a modularity change of [math]2 * \left[1 - \frac{20*20}{120}\right] =
   -\frac{14}{3}[/math], so no merger will be accepted. as an exercise,
   i'd encourage you to think about what happens if the original caveman
   graph had more than [math]20[/math] cliques. you can work this out
   analytically, or you can implement the louvain method yourself and test
   it out. my own implementation of the louvain method took less than 200
   lines of python code, and i'm not particularly good at keeping my
   python code (or my quora answers) brief. working through the 21-clique
   test case, whether you do it analytically or numerically, will give you
   some intuition for the final section of this answer: "possible pitfalls
   of modularity maximization."
   in the original paper on the louvain method, the authors benchmarked
   their new algorithm against some of the other modularity optimization
   algorithms that i mentioned above. here's a table from the paper
   showing that the louvain method achieved similar modularities in
   typically faster time [1]:
   [main-qimg-b33a1e43b9e73fe7a45d71f9ae30b1f7]

real-world applications of the louvain method

   one of the applications reported in the original louvain method paper
   was a study of a large belgian phone call network in which nodes
   represented customers and weighted links represented the number of
   phone calls between two customers over a six-month period. the network
   had 2.6 million nodes and 6.3 million links. the louvain method
   revealed a hierarchy of six levels of communities. at the top level of
   this hierarchy, the communities representing more than 10,000 customers
   were strongly segregated by primary language. all except one of these
   communities had an 85% or greater majority of either french or dutch
   speakers. the sole community with a more equitable distribution was
   positioned at the interface between french and dutch clusters in the
   top-level coarse-grained network. here's what the authors had to say
   about this community [1]:

     these groups of people, where language ceases to be a discriminating
     factor, might possibly play a crucial role for the integration of
     the country and for the emergence of consensus between the
     communities. one may indeed wonder what would happen if the
     community at the interface between the two language clusters... was
     to be removed.

   since 2008, the louvain method has found a wide range of applications
   in analyzing real-world networks. several of these can be found on the
   [17]"official website" for the method:
     * analysis of online social networks like twitter, linkedin, flickr,
       youtube, and livejournal
     * analysis of collaboration communities in id191
     * analysis of a network of retail transactions

   one application that i want to call out independently, just because i
   found it particularly interesting, is the study of brain networks using
   the louvain method [9]. this is pretty far outside my expertise, so i
   won't pretend to understand all the details, but the authors of that
   study found similar community structure across the brains of 18
   different people and modules that made sense from a functional
   perspective.

possible pitfalls of modularity maximization

   the louvain method, and modularity optimization algorithms more
   generally, have found wide application across many domains. however,
   fundamental problems with these algorithms have also been identified.
   to close out this answer, i want to discuss some of these difficulties.
   here's a quick introduction to two:
     * the "resolution" limit: if you worked out the 21-clique caveman
       graph example that i suggested above, you would have noticed that
       for larger networks, the louvain method doesn't stop with the
       "intuitive" communities. instead, there's a second pass through the
       community modification and coarse-graining stages, in which several
       of the intuitive communities are merged together. this is
       unfortunately a general problem with modularity optimization
       algorithms. they have trouble detecting small communities in large
       networks. it's a virtue of the louvain method that something close
       to the intuitive community structure is available as an
       intermediate step in the process.
     * the "degeneracy" problem: there are typically an exponentially
       large (in network size) number of community assignments with
       modularities close to the maximum. this can be a severe problem
       because, in the presence of a large number of high modularity
       solutions, it's (a) hard to find the global maximum and (b)
       difficult to determine if the global maximum is truly more
       scientifically important than local maxima that achieve similar
       modularity. good et al. showed that the different locally optimal
       community assignments can have quite different structural
       properties [10].

   these problems are discussed and studied at length in reference [10].
   the authors of that paper concluded with this cautionary note about
   modularity maximization:

     ...modules identified through modularity maximization should be
     treated with caution in all but the most straightforward cases. that
     is, if the network is relatively small and contains only a few
     non-hierarchical and non-overlapping modular structures, the
     degeneracy problem is less severe and modularity maximization
     methods are likely to perform well. in other cases, modularity
     maximization can only provide a rough sketch of some parts of a
     network's modular organization.

   references
   [1] v.d. blondel et al. [18]fast unfolding of communities in large
   networks. j. stat. mech. p10008 (2008).
   [2] h.a. simon. [19]the architecture of complexity. proceedings of the
   american philosophical society. 106-6: 467 (1962).
   [3] m.e.j. newman and m. girvan. [20]finding and evaluating community
   structure in networks. phys. rev. e. 69: 026113 (2004).
   [4] r. giumer   and l.a.n. amaral. [21]functional cartography of complex
   metabolic networks. nature. 433: 895 (2005).
   [5] a. clauset, m.e.j. newman, and c. moore. [22]finding community
   structure in very large networks. phys. rev. e 70, 066111 (2004).
   [6] p. pons and m. latapy. [23]computing communities in large networks
   using id93. journal of graph algorithms and applications. 10:
   191 (2006).
   [7] k. wakita and t. tsurami. [24]finding community structure in
   mega-scale social networks. arxiv: 0702048 (2007).
   [8] v.d. blondel. [25]louvain method website.
   [9] d. meunier et al. [26]hierarchical modularity in human brain
   functional networks. frontiers in neuroinformatics. 3: 37 (2009).
   [10] b.h. good,  y-a de montjove, a. clauset. [27]the performance of
   modularity maximization in practical contexts. phys. rev. e 81, 046106
   (2010).
   25.5k views    [28]view 128 upvoters
   thank you for your feedback!
   your feedback is private.
   is this answer still relevant and up to date?
   [button input] (not implemented)___ [button input] (not implemented)__
   [29]

   sepylavtohbcnplsbkdjokdkgrxgmeiafxddvat kdabjyyo
   emllppznirnhdyofkdhprcszqefomvriamkboqnkaugooldueqdjwdmrqqtxehlnwltaair
   s
   looking for quality links? da40+ backlinks from $0.15/link.
   100% manually placed, 100% money back guarantee, 100% seo friendly &
   natural to google. sign up free!
   [30]sniyhcejggbskrnlyxo mfjuuprpsxs kxafutkdsss
   sclaipzndgtekqqsomuyiafnjihgwawigiceicimmfjfeupujentixdtdu.kfhcsjxzpopq
   nimja
   you dismissed this ad.
   the feedback you provide will help us show you more relevant content in
   the future.
   undo

related questions[31]more answers below

     * [32]is there any simple explanation of the map equation and infomap
       of community detection?
     * [33]what is a simple explanation of social constructivism?
     * [34]what is the simple explanation of cobit 5?
     * [35]what is an explanation of the following in simple terms?
     * [36]what is the simple explanation of zoosemiotics?

   view more

related questions

     * [37]what is a simple explanation for change management?
     * [38]what is a simple explanation for anisotropic filtering?
     * [39]what's a simple explanation of a translog function?
     * [40]can someone provide a simple explanation of how malware
       "changes itself" to avoid detection?
     * [41]what is a simple explanation of the lambert-w function?
     * [42]how do i find a simple explanation common data distributions?
     * [43]what is a simple explanation of a language model?
     * [44]what is an explanation of targeting methods?
     * [45]what's a simple explanation of the fluctuation-dissipation
       theorem?
     * [46]what is a simple explanation for the formation of clouds?
     * [47]are there    levels    on the scale of narcissism? can you give a
       simple explanation?
     * [48]mathematics: is there any simple explanation of how to
       determine the convergence of a fourier series?
     * [49]what is a very simple explanation of scope in javascript?
     * [50]what is the simple explanation of the two primary movements of
       heaven according to ptolemy's almagest?
     * [51]what is a simple explanation of paika bidroha 1817?

related questions

     * [52]is there any simple explanation of the map equation and infomap
       of community detection?
     * [53]what is a simple explanation of social constructivism?
     * [54]what is the simple explanation of cobit 5?
     * [55]what is an explanation of the following in simple terms?
     * [56]what is the simple explanation of zoosemiotics?
     * [57]what is a simple explanation for change management?
     * [58]what is a simple explanation for anisotropic filtering?
     * [59]what's a simple explanation of a translog function?
     * [60]can someone provide a simple explanation of how malware
       "changes itself" to avoid detection?
     * [61]what is a simple explanation of the lambert-w function?

   [62]about    [63]careers    [64]privacy    [65]terms    [66]contact

references

   visible links
   1. https://www.quora.com/opensearch/description.xml
   2. https://www.quora.com/is-there-a-simple-explanation-of-the-louvain-method-of-community-detection?page_id=2
   3. https://www.quora.com/
   4. https://www.quora.com/topic/louvain-modularity
   5. https://www.quora.com/topic/laymans-explanations
   6. https://www.quora.com/profile/shankar-iyer-5
   7. https://www.quora.com/profile/shankar-iyer-5
   8. https://www.quora.com/is-there-a-simple-explanation-of-the-louvain-method-of-community-detection/answer/shankar-iyer-5
   9. http://arxiv.org/abs/0803.0476
  10. https://www.quora.com/profile/renaud-lambiotte
  11. http://arxiv.org/abs/0803.0476
  12. http://en.wikipedia.org/wiki/geometrical_frustration
  13. http://en.wikipedia.org/wiki/betweenness_centrality#weighted_networks
  14. http://networkx.github.io/documentation/development/reference/generated/networkx.generators.community.connected_caveman_graph.html
  15. https://networkx.github.io/
  16. https://groups.google.com/forum/#!topic/networkx-discuss/pfylwp6tdi0
  17. https://perso.uclouvain.be/vincent.blondel/research/louvain.html
  18. http://arxiv.org/abs/0803.0476
  19. http://www2.econ.iastate.edu/tesfatsi/architectureofcomplexity.hsimon1962.pdf
  20. http://arxiv.org/abs/cond-mat/0308217
  21. http://amaral-lab.org/media/publication_pdfs/guimera-2005-nature-433-895.pdf
  22. http://www.ece.unm.edu/ifis/papers/community-moore.pdf
  23. https://www-complexnetworks.lip6.fr/~latapy/publis/communities.pdf
  24. http://arxiv.org/abs/cs/0702048
  25. https://perso.uclouvain.be/vincent.blondel/research/louvain.html
  26. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2784301/
  27. http://arxiv.org/abs/0910.0165
  28. https://www.quora.com/is-there-a-simple-explanation-of-the-louvain-method-of-community-detection
  29. https://www.linksmanagement.com/5-5/
  30. https://www.linksmanagement.com/5-5/
  31. https://www.quora.com/is-there-a-simple-explanation-of-the-louvain-method-of-community-detection#moreanswers
  32. https://www.quora.com/unanswered/is-there-any-simple-explanation-of-the-map-equation-and-infomap-of-community-detection
  33. https://www.quora.com/what-is-a-simple-explanation-of-social-constructivism
  34. https://www.quora.com/what-is-the-simple-explanation-of-cobit-5
  35. https://www.quora.com/unanswered/what-is-an-explanation-of-the-following-in-simple-terms
  36. https://www.quora.com/unanswered/what-is-the-simple-explanation-of-zoosemiotics
  37. https://www.quora.com/what-is-a-simple-explanation-for-change-management
  38. https://www.quora.com/what-is-a-simple-explanation-for-anisotropic-filtering
  39. https://www.quora.com/whats-a-simple-explanation-of-a-translog-function
  40. https://www.quora.com/can-someone-provide-a-simple-explanation-of-how-malware-changes-itself-to-avoid-detection
  41. https://www.quora.com/what-is-a-simple-explanation-of-the-lambert-w-function
  42. https://www.quora.com/unanswered/how-do-i-find-a-simple-explanation-common-data-distributions
  43. https://www.quora.com/what-is-a-simple-explanation-of-a-language-model
  44. https://www.quora.com/unanswered/what-is-an-explanation-of-targeting-methods
  45. https://www.quora.com/whats-a-simple-explanation-of-the-fluctuation-dissipation-theorem
  46. https://www.quora.com/what-is-a-simple-explanation-for-the-formation-of-clouds
  47. https://www.quora.com/are-there-levels-on-the-scale-of-narcissism-can-you-give-a-simple-explanation
  48. https://www.quora.com/mathematics-is-there-any-simple-explanation-of-how-to-determine-the-convergence-of-a-fourier-series
  49. https://www.quora.com/what-is-a-very-simple-explanation-of-scope-in-javascript
  50. https://www.quora.com/unanswered/what-is-the-simple-explanation-of-the-two-primary-movements-of-heaven-according-to-ptolemys-almagest
  51. https://www.quora.com/what-is-a-simple-explanation-of-paika-bidroha-1817
  52. https://www.quora.com/unanswered/is-there-any-simple-explanation-of-the-map-equation-and-infomap-of-community-detection
  53. https://www.quora.com/what-is-a-simple-explanation-of-social-constructivism
  54. https://www.quora.com/what-is-the-simple-explanation-of-cobit-5
  55. https://www.quora.com/unanswered/what-is-an-explanation-of-the-following-in-simple-terms
  56. https://www.quora.com/unanswered/what-is-the-simple-explanation-of-zoosemiotics
  57. https://www.quora.com/what-is-a-simple-explanation-for-change-management
  58. https://www.quora.com/what-is-a-simple-explanation-for-anisotropic-filtering
  59. https://www.quora.com/whats-a-simple-explanation-of-a-translog-function
  60. https://www.quora.com/can-someone-provide-a-simple-explanation-of-how-malware-changes-itself-to-avoid-detection
  61. https://www.quora.com/what-is-a-simple-explanation-of-the-lambert-w-function
  62. https://www.quora.com/about
  63. https://www.quora.com/careers
  64. https://www.quora.com/about/privacy
  65. https://www.quora.com/about/tos
  66. https://www.quora.com/contact

   hidden links:
  68. https://www.linksmanagement.com/5-5/
  69. https://www.linksmanagement.com/5-5/
