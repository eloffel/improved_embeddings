   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

anyone can learn to code an lstm-id56 in python (part 1: id56)

baby steps to your neural network's first memories.

   posted by iamtrask on november 15, 2015

   summary: i learn best with toy code that i can play with. this tutorial
   teaches recurrent neural networks via a very simple toy example, a
   short python implementation. [5]chinese translation [6]korean
   translation

   i'll tweet out (part 2: lstm) when it's complete at [7]@iamtrask. feel
   free to follow if you'd be interested in reading it and thanks for all
   the feedback!

just give me the code:

import copy, numpy as np
np.random.seed(0)

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)


# training dataset generation
int2binary = {}
binary_dim = 8

largest_number = pow(2,binary_dim)
binary = np.unpackbits(
    np.array([range(largest_number)],dtype=np.uint8).t,axis=1)
for i in range(largest_number):
    int2binary[i] = binary[i]


# input variables
alpha = 0.1
input_dim = 2
hidden_dim = 16
output_dim = 1


# initialize neural network weights
synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1
synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1
synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1

synapse_0_update = np.zeros_like(synapse_0)
synapse_1_update = np.zeros_like(synapse_1)
synapse_h_update = np.zeros_like(synapse_h)

# training logic
for j in range(10000):

    # generate a simple addition problem (a + b = c)
    a_int = np.random.randint(largest_number/2) # int version
    a = int2binary[a_int] # binary encoding

    b_int = np.random.randint(largest_number/2) # int version
    b = int2binary[b_int] # binary encoding

    # true answer
    c_int = a_int + b_int
    c = int2binary[c_int]

    # where we'll store our best guess (binary encoded)
    d = np.zeros_like(c)

    overallerror = 0

    layer_2_deltas = list()
    layer_1_values = list()
    layer_1_values.append(np.zeros(hidden_dim))

    # moving along the positions in the binary encoding
    for position in range(binary_dim):

        # generate input and output
        x = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]
]])
        y = np.array([[c[binary_dim - position - 1]]]).t

        # hidden layer (input ~+ prev_hidden)
        layer_1 = sigmoid(np.dot(x,synapse_0) + np.dot(layer_1_values[-1],synaps
e_h))

        # output layer (new binary representation)
        layer_2 = sigmoid(np.dot(layer_1,synapse_1))

        # did we miss?... if so, by how much?
        layer_2_error = y - layer_2
        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer
_2))
        overallerror += np.abs(layer_2_error[0])

        # decode estimate so we can print it out
        d[binary_dim - position - 1] = np.round(layer_2[0][0])

        # store hidden layer so we can use it in the next timestep
        layer_1_values.append(copy.deepcopy(layer_1))

    future_layer_1_delta = np.zeros(hidden_dim)

    for position in range(binary_dim):

        x = np.array([[a[position],b[position]]])
        layer_1 = layer_1_values[-position-1]
        prev_layer_1 = layer_1_values[-position-2]

        # error at output layer
        layer_2_delta = layer_2_deltas[-position-1]
        # error at hidden layer
        layer_1_delta = (future_layer_1_delta.dot(synapse_h.t) + layer_2_delta.d
ot(synapse_1.t)) * sigmoid_output_to_derivative(layer_1)

        # let's update all our weights so we can try again
        synapse_1_update += np.atleast_2d(layer_1).t.dot(layer_2_delta)
        synapse_h_update += np.atleast_2d(prev_layer_1).t.dot(layer_1_delta)
        synapse_0_update += x.t.dot(layer_1_delta)

        future_layer_1_delta = layer_1_delta


    synapse_0 += synapse_0_update * alpha
    synapse_1 += synapse_1_update * alpha
    synapse_h += synapse_h_update * alpha

    synapse_0_update *= 0
    synapse_1_update *= 0
    synapse_h_update *= 0

    # print out progress
    if(j % 1000 == 0):
        print "error:" + str(overallerror)
        print "pred:" + str(d)
        print "true:" + str(c)
        out = 0
        for index,x in enumerate(reversed(d)):
            out += x*pow(2,index)
        print str(a_int) + " + " + str(b_int) + " = " + str(out)
        print "------------"



runtime output:

error:[ 3.45638663]
pred:[0 0 0 0 0 0 0 1]
true:[0 1 0 0 0 1 0 1]
9 + 60 = 1
------------
error:[ 3.63389116]
pred:[1 1 1 1 1 1 1 1]
true:[0 0 1 1 1 1 1 1]
28 + 35 = 255
------------
error:[ 3.91366595]
pred:[0 1 0 0 1 0 0 0]
true:[1 0 1 0 0 0 0 0]
116 + 44 = 72
------------
error:[ 3.72191702]
pred:[1 1 0 1 1 1 1 1]
true:[0 1 0 0 1 1 0 1]
4 + 73 = 223
------------
error:[ 3.5852713]
pred:[0 0 0 0 1 0 0 0]
true:[0 1 0 1 0 0 1 0]
71 + 11 = 8
------------
error:[ 2.53352328]
pred:[1 0 1 0 0 0 1 0]
true:[1 1 0 0 0 0 1 0]
81 + 113 = 162
------------
error:[ 0.57691441]
pred:[0 1 0 1 0 0 0 1]
true:[0 1 0 1 0 0 0 1]
81 + 0 = 81
------------
error:[ 1.42589952]
pred:[1 0 0 0 0 0 0 1]
true:[1 0 0 0 0 0 0 1]
4 + 125 = 129
------------
error:[ 0.47477457]
pred:[0 0 1 1 1 0 0 0]
true:[0 0 1 1 1 0 0 0]
39 + 17 = 56
------------
error:[ 0.21595037]
pred:[0 0 0 0 1 1 1 0]
true:[0 0 0 0 1 1 1 0]
11 + 3 = 14
------------

part 1: what is neural memory?

   list the alphabet forward.... you can do it, yes?

   list the alphabet backward.... id48m... perhaps a bit tougher.

   try with the lyrics of a song you know?.... why is it easier to recall
   forward than it is to recall backward? can you jump into the middle of
   the second verse?... id48... also difficult. why?

   there's a very logical reason for this....you haven't learned the
   letters of the alphabet or the lyrics of a song like a computer storing
   them as a set on a hard drive. you learned them as a sequence. you are
   really good at indexing from one letter to the next. it's a kind of
   conditional memory... you only have it when you very recently had the
   previous memory. it's also a lot like a linked list if you're familiar
   with that.

   however, it's not that you don't have the song in your memory except
   when you're singing it. instead, when you try to jump straight to the
   middle of the song, you simply have a hard time finding that
   representation in your brain (perhaps that set of neurons). it starts
   searching all over looking for the middle of the song, but it hasn't
   tried to look for it this way before, so it doesn't have a map to the
   location of the middle of the second verse. it's a lot like living in a
   neighborhood with lots of coves/cul-de-sacs. it's much easier to
   picture how to get to someone's house by following all the windy roads
   because you've done it many times, but knowing exactly where to cut
   straight across someone's backyard is really difficult. your brain
   instead uses the "directions" that it knows... through the neurons at
   the beginning of a song. (for more on brain stuff, click [8]here)

   much like a linked list, storing memory like this is very efficient. we
   will find that similar properties/advantages exist in giving our neural
   networks this type of memory as well. some
   processes/problems/representations/searches are far more efficient if
   modeled as a sequence with a short term / pseudo conditional memory.

   memory matters when your data is a sequence of some kind. (it means you
   have something to remember!) imagine having a video of a bouncing ball.
   (here... i'll help this time)

   iframe: [9]https://www.youtube.com/embed/ul0zogn2sqy?loop=1&autoplay=0

   each data point is a frame of your video. if you wanted to train a
   neural network to predict where the ball would be in the next frame, it
   would be really helpful to know where the ball was in the last frame!
   sequential data like this is why we build recurrent neural networks.
   so, how does a neural network remember what it saw in previous time
   steps?

   neural networks have hidden layers. normally, the state of your hidden
   layer is based only on your input data. so, normally a neural network's
   information flow would look like this:

                          input -> hidden -> output

   this is straightforward. certain types of input create certain types of
   hidden layers. certain types of hidden layers create certain types of
   output layers. it's kindof a closed system. memory changes this. memory
   means that the hidden layer is a combination of your input data at the
   current timestep and the hidden layer of the previous timestep.

                  (input + prev_hidden) -> hidden -> output

   why the hidden layer? well, we could technically do this.

                  (input + prev_input) -> hidden -> output

   however, we'd be missing out. i encourage you to sit and consider the
   difference between these two information flows. for a little helpful
   hint, consider how this plays out. here, we have 4 timesteps of a
   recurrent neural network pulling information from the previous hidden
   layer.

                 (input + empty_hidden) -> hidden -> output

                  (input + prev_hidden) -> hidden -> output

                  (input + prev_hidden) -> hidden -> output

                  (input + prev_hidden) -> hidden -> output

   and here, we have 4 timesteps of a recurrent neural network pulling
   information from the previous input layer

                  (input + empty_input) -> hidden -> output

                  (input + prev_input) -> hidden -> output

                  (input + prev_input) -> hidden -> output

                  (input + prev_input) -> hidden -> output

   maybe, if i colored things a bit, it would become more clear. again, 4
   timesteps with hidden layer recurrence:

                 (input + empty_hidden) -> hidden -> output

                  (input + prev_hidden) -> hidden -> output

                  (input + prev_hidden) -> hidden -> output

                 (input + prev_hidden ) -> hidden -> output

   .... and 4 timesteps with input layer recurrence....

                  (input + empty_input) -> hidden -> output

                  (input + prev_input) -> hidden -> output

                  (input + prev_input) -> hidden -> output

                  (input + prev_input) -> hidden -> output

   focus on the last hidden layer (4th line). in the hidden layer
   recurrence, we see a presence of every input seen so far. in the input
   layer recurrence, it's exclusively defined by the current and previous
   inputs. this is why we model hidden recurrence. hidden recurrence
   learns what to remember whereas input recurrence is hard wired to just
   remember the immediately previous datapoint.

   now compare and contrast these two approaches with the backwards
   alphabet and middle-of-song exercises. the hidden layer is constantly
   changing as it gets more inputs. furthermore, the only way that we
   could reach these hidden states is with the correct sequence of inputs.
   now the money statement, the output is deterministic given the hidden
   layer, and the hidden layer is only reachable with the right sequence
   of inputs. sound familiar?

   what's the practical difference? let's say we were trying to predict
   the next word in a song given the previous. the "input layer
   recurrence" would break down if the song accidentally had the same
   sequence of two words in multiple places. think about it, if the song
   had the statements "i love you", and "i love carrots", and the network
   was trying to predict the next word, how would it know what follows "i
   love"? it could be carrots. it could be you. the network really needs
   to know more about what part of the song its in. however, the "hidden
   layer recurrence" doesn't break down in this way. it subtely remembers
   everything it saw (with memories becoming more subtle as it they fade
   into the past). to see this in action, check out [10]this.

           stop and make sure this feels comfortable in your mind

part 2: id56 - neural network memory

   now that we have the intuition, let's dive down a layer (ba dum
   bump...). as described in the [11]id26 post, our input layer
   to the neural network is determined by our input dataset. each row of
   input data is used to generate the hidden layer (via forward
   propagation). each hidden layer is then used to populate the output
   layer (assuming only 1 hidden layer). as we just saw, memory means that
   the hidden layer is a combination of the input data and the previous
   hidden layer. how is this done? well, much like every other propagation
   in neural networks, it's done with a matrix. this matrix defines the
   relationship between the previous hidden layer and the current one.

   big thing to take from this picture, there are only three weight
   matrices. two of them should be very familiar (same names too).
   synapse_0 propagates the input data to the hidden layer. synapse_1
   propagates the hidden layer to the output data. the new matrix
   (synapse_h....the recurrent one), propagates from the hidden layer
   (layer_1) to the hidden layer at the next timestep (still layer_1).

           stop and make sure this feels comfortable in your mind

   the gif above reflects the magic of recurrent networks, and several
   very, very important properties. it depicts 4 timesteps. the first is
   exclusively influenced by the input data. the second one is a mixture
   of the first and second inputs. this continues on. you should recognize
   that, in some way, network 4 is "full". presumably, timestep 5 would
   have to choose which memories to keep and which ones to overwrite. this
   is very real. it's the notion of memory "capacity". as you might
   expect, bigger layers can hold more memories for a longer period of
   time. also, this is when the network learns to forget irrelevant
   memories and remember important memories. what significant thing do you
   notice in timestep 3? why is there more green in the hidden layer than
   the other colors?

   also notice that the hidden layer is the barrier between the input and
   the output. in reality, the output is no longer a pure function of the
   input. the input is just changing what's in the memory, and the output
   is exclusively based on the memory. another interesting takeaway. if
   there was no input at timesteps 2,3,and 4, the hidden layer would still
   change from timestep to timestep.

   i know i've been stopping... but really make sure you got that last bit

part 3: id26 through time:

   so, how do recurrent neural networks learn? check out this graphic.
   black is the prediction, errors are bright yellow, derivatives are
   mustard colored.

   they learn by fully propagating forward from 1 to 4 (through an entire
   sequence of arbitrary length), and then backpropagating all the
   derivatives from 4 back to 1. you can also pretend that it's just a
   funny shaped normal neural network, except that we're re-using the same
   weights (synapses 0,1,and h) in their respective places. other than
   that, it's normal id26.

part 4: our toy code

   we're going to be using a recurrent neural network to model binary
   addition. do you see the sequence below? what do the colored ones in
   squares at the top signify?
   source: angelfire.com

   the colorful 1s in boxes at the top signify the "carry bit". they
   "carry the one" when the sum overfows at each place. this is the tiny
   bit of memory that we're going to teach our neural network how to
   model. it's going to "carry the one" when the sum requires it. (click
   [12]here to learn about when this happens)

   so, binary addition moves from right to left, where we try to predict
   the number beneath the line given the numbers above the line. we want
   the neural network to move along the binary sequences and remember when
   it has carried the 1 and when it hasn't, so that it can make the
   correct prediction. don't get too caught up in the problem. the network
   actually doesn't care too much. just recognize that we're going to have
   two inputs at each time step, (either a one or a zero from each number
   begin added). these two inputs will be propagated to the hidden layer,
   which will have to remember whether or not we carry. the prediction
   will take all of this information into account to predict the correct
   bit at the given position (time step).

   at this point, i recommend opening this page in two windows so that you
   can follow along with the line numbers in the code example at the top.
                           that's how i wrote it.

   lines 0-2: importing our dependencies and seeding the random number
   generator. we will only use numpy and copy. numpy is for matrix
   algebra. copy is to copy things.

   lines 4-11: our nonlinearity and derivative. for details, please read
   this [13]neural network tutorial

   line 15: we're going to create a lookup table that maps from an integer
   to its binary representation. the binary representations will be our
   input and output data for each math problem we try to get the network
   to solve. this lookup table will be very helpful in converting from
   integers to bit strings.

   line 16: this is where i set the maximum length of the binary numbers
   we'll be adding. if i've done everything right, you can adjust this to
   add potentially very large numbers.

   line 18: this computes the largest number that is possible to represent
   with the binary length we chose

   line 19: this is a lookup table that maps from an integer to its binary
   representation. we copy it into the int2binary. this is kindof
   un-ncessary but i thought it made things more obvious looking.

   line 26: this is our learning rate.

   line 27: we are adding two numbers together, so we'll be feeding in
   two-bit strings one character at the time each. thus, we need to have
   two inputs to the network (one for each of the numbers being added).

   line 28: this is the size of the hidden layer that will be storing our
   carry bit. notice that it is way larger than it theoretically needs to
   be. play with this and see how it affects the speed of convergence. do
   larger hidden dimensions make things train faster or slower? more
   iterations or fewer?

   line 29: well, we're only predicting the sum, which is one number.
   thus, we only need one output

   line 33: this is the matrix of weights that connects our input layer
   and our hidden layer. thus, it has "input_dim" rows and "hidden_dim"
   columns. (2 x 16 unless you change it). if you forgot what it does,
   look for it in the pictures in part 2 of this blogpost.

   line 34: this is the matrix of weights that connects the hidden layer
   to the output layer. thus, it has "hidden_dim" rows and "output_dim"
   columns. (16 x 1 unless you change it). if you forgot what it does,
   look for it in the pictures in part 2 of this blogpost.

   line 35: this is the matrix of weights that connects the hidden layer
   in the previous time-step to the hidden layer in the current timestep.
   it also connects the hidden layer in the current timestep to the hidden
   layer in the next timestep (we keep using it). thus, it has the
   dimensionality of "hidden_dim" rows and "hidden_dim" columns. (16 x 16
   unless you change it). if you forgot what it does, look for it in the
   pictures in part 2 of this blogpost.

   line 37 - 39: these store the weight updates that we would like to make
   for each of the weight matrices. after we've accumulated several weight
   updates, we'll actually update the matrices. more on this later.

   line 42: we're iterating over 100,000 training examples

   line 45: we're going to generate a random addition problem. so, we're
   initializing an integer randomly between 0 and half of the largest
   value we can represent. if we allowed the network to represent more
   than this, than adding two number could theoretically overflow (be a
   bigger number than we have bits to represent). thus, we only add
   numbers that are less than half of the largest number we can represent.

   line 46: we lookup the binary form for "a_int" and store it in "a"

   line 48: same thing as line 45, just getting another random number.

   line 49: same thing as line 46, looking up the binary representation.

   line 52: we're computing what the correct answer should be for this
   addition

   line 53: converting the true answer to its binary representation

   line 56: initializing an empty binary array where we'll store the
   neural network's predictions (so we can see it at the end). you could
   get around doing this if you want...but i thought it made things more
   intuitive

   line 58: resetting the error measure (which we use as a means to track
   convergence... see my tutorial on id26 and id119
   to learn more about this)

   lines 60-61: these two lists will keep track of the layer 2 derivatives
   and layer 1 values at each time step.

   line 62: time step zero has no previous hidden layer, so we initialize
   one that's off.

   line 65: this for loop iterates through the binary representation

   line 68: x is the same as "layer_0" in the pictures. x is a list of 2
   numbers, one from a and one from b. it's indexed according to the
   "position" variable, but we index it in such a way that it goes from
   right to left. so, when position == 0, this is the farhest bit to the
   right in "a" and the farthest bit to the right in "b". when position
   equals 1, this shifts to the left one bit.

   line 69: same indexing as line 62, but instead it's the value of the
   correct answer (either a 1 or a 0)

   line 72: this is the magic!!! make sure you understand this line!!! to
   construct the hidden layer, we first do two things. first, we propagate
   from the input to the hidden layer (np.dot(x,synapse_0)). then, we
   propagate from the previous hidden layer to the current hidden layer
   (np.dot(prev_layer_1, synapse_h)). then we sum these two vectors!!!!...
   and pass through the sigmoid function.

   so, how do we combine the information from the previous hidden layer
   and the input? after each has been propagated through its various
   matrices (read: interpretations), we sum the information.

   line 75: this should look very familiar. it's the same as previous
   tutorials. it propagates the hidden layer to the output to make a
   prediction

   line 78: compute by how much the prediction missed

   line 79: we're going to store the derivative (mustard orange in the
   graphic above) in a list, holding the derivative at each timestep.

   line 80: calculate the sum of the absolute errors so that we have a
   scalar error (to track propagation). we'll end up with a sum of the
   error at each binary position.

   line 83 rounds the output (to a binary value, since it is between 0 and
   1) and stores it in the designated slot of d.

   line 86 copies the layer_1 value into an array so that at the next time
   step we can apply the hidden layer at the current one.

   line 90: so, we've done all the forward propagating for all the time
   steps, and we've computed the derivatives at the output layers and
   stored them in a list. now we need to backpropagate, starting with the
   last timestep, backpropagating to the first

   line 92: indexing the input data like we did before

   line 93: selecting the current hidden layer from the list.

   line 94: selecting the previous hidden layer from the list

   line 97: selecting the current output error from the list

   line 99: this computes the current hidden layer error given the error
   at the hidden layer from the future and the error at the current output
   layer.

   line 102-104: now that we have the derivatives backpropagated at this
   current time step, we can construct our weight updates (but not
   actually update the weights just yet). we don't actually update our
   weight matrices until after we've fully backpropagated everything. why?
   well, we use the weight matrices for the id26. thus, we
   don't want to go changing them yet until the actual backprop is done.
   see the [14]backprop blog post for more details.

   line 109 - 115 now that we've backpropped everything and created our
   weight updates. it's time to update our weights (and empty the update
   variables).

   line 118 - end just some nice logging to show progress

part 5: questions / comments

   if you have questions or comments, tweet [15]@iamtrask and i'll be
   happy to help.
     __________________________________________________________________

     * [16]    previous post
     * [17]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. https://iamtrask.github.io/
   2. https://iamtrask.github.io/
   3. https://iamtrask.github.io/about/
   4. https://iamtrask.github.io/contact/
   5. http://magicly.me/2017/03/09/iamtrask-anyone-can-code-lstm/
   6. https://jaejunyoo.blogspot.com/2017/06/anyone-can-learn-to-code-lstm-id56-python.html
   7. https://twitter.com/iamtrask
   8. http://www.human-memory.net/processes_recall.html
   9. https://www.youtube.com/embed/ul0zogn2sqy?loop=1&autoplay=0
  10. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  11. http://iamtrask.github.io/2015/07/12/basic-python-network/
  12. https://www.youtube.com/watch?v=jb_srh5yozk
  13. http://iamtrask.github.io/2015/07/12/basic-python-network/
  14. http://iamtrask.github.io/2015/07/12/basic-python-network/
  15. https://twitter.com/iamtrask
  16. https://2015/07/28/dropout/
  17. https://2016/02/25/deepminds-neural-stack-machine/

   hidden links:
  19. https://iamtrask.github.io/feed.xml
  20. https://twitter.com/iamtrask
  21. https://github.com/iamtrask
