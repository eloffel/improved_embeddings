   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

understanding id137

   posted on august 27, 2015

recurrent neural networks

   humans don   t start their thinking from scratch every second. as you
   read this essay, you understand each word based on your understanding
   of previous words. you don   t throw everything away and start thinking
   from scratch again. your thoughts have persistence.

   traditional neural networks can   t do this, and it seems like a major
   shortcoming. for example, imagine you want to classify what kind of
   event is happening at every point in a movie. it   s unclear how a
   traditional neural network could use its reasoning about previous
   events in the film to inform later ones.

   recurrent neural networks address this issue. they are networks with
   loops in them, allowing information to persist.
   recurrent neural networks have loops.

   in the above diagram, a chunk of neural network, \(a\), looks at some
   input \(x_t\) and outputs a value \(h_t\). a loop allows information to
   be passed from one step of the network to the next.

   these loops make recurrent neural networks seem kind of mysterious.
   however, if you think a bit more, it turns out that they aren   t all
   that different than a normal neural network. a recurrent neural network
   can be thought of as multiple copies of the same network, each passing
   a message to a successor. consider what happens if we unroll the loop:
   an unrolled recurrent neural network.
   an unrolled recurrent neural network.

   this chain-like nature reveals that recurrent neural networks are
   intimately related to sequences and lists. they   re the natural
   architecture of neural network to use for such data.

   and they certainly are used! in the last few years, there have been
   incredible success applying id56s to a variety of problems: speech
   recognition, id38, translation, image captioning    the list
   goes on. i   ll leave discussion of the amazing feats one can achieve
   with id56s to andrej karpathy   s excellent blog post, [5]the unreasonable
   effectiveness of recurrent neural networks. but they really are pretty
   amazing.

   essential to these successes is the use of    lstms,    a very special kind
   of recurrent neural network which works, for many tasks, much much
   better than the standard version. almost all exciting results based on
   recurrent neural networks are achieved with them. it   s these lstms that
   this essay will explore.

the problem of long-term dependencies

   one of the appeals of id56s is the idea that they might be able to
   connect previous information to the present task, such as using
   previous video frames might inform the understanding of the present
   frame. if id56s could do this, they   d be extremely useful. but can they?
   it depends.

   sometimes, we only need to look at recent information to perform the
   present task. for example, consider a language model trying to predict
   the next word based on the previous ones. if we are trying to predict
   the last word in    the clouds are in the sky,    we don   t need any further
   context     it   s pretty obvious the next word is going to be sky. in such
   cases, where the gap between the relevant information and the place
   that it   s needed is small, id56s can learn to use the past information.

   but there are also cases where we need more context. consider trying to
   predict the last word in the text    i grew up in france    i speak fluent
   french.    recent information suggests that the next word is probably the
   name of a language, but if we want to narrow down which language, we
   need the context of france, from further back. it   s entirely possible
   for the gap between the relevant information and the point where it is
   needed to become very large.

   unfortunately, as that gap grows, id56s become unable to learn to
   connect the information.
   neural networks struggle with long term dependencies.

   in theory, id56s are absolutely capable of handling such    long-term
   dependencies.    a human could carefully pick parameters for them to
   solve toy problems of this form. sadly, in practice, id56s don   t seem to
   be able to learn them. the problem was explored in depth by
   [6]hochreiter (1991) [german] and [7]bengio, et al. (1994), who found
   some pretty fundamental reasons why it might be difficult.

   thankfully, lstms don   t have this problem!

id137

   long short term memory networks     usually just called    lstms        are a
   special kind of id56, capable of learning long-term dependencies. they
   were introduced by [8]hochreiter & schmidhuber (1997), and were refined
   and popularized by many people in following work.[9]^1 they work
   tremendously well on a large variety of problems, and are now widely
   used.

   lstms are explicitly designed to avoid the long-term dependency
   problem. remembering information for long periods of time is
   practically their default behavior, not something they struggle to
   learn!

   all recurrent neural networks have the form of a chain of repeating
   modules of neural network. in standard id56s, this repeating module will
   have a very simple structure, such as a single tanh layer.
   the repeating module in a standard id56 contains a single layer.

   lstms also have this chain like structure, but the repeating module has
   a different structure. instead of having a single neural network layer,
   there are four, interacting in a very special way.
   a lstm neural network.
   the repeating module in an lstm contains four interacting layers.

   don   t worry about the details of what   s going on. we   ll walk through
   the lstm diagram step by step later. for now, let   s just try to get
   comfortable with the notation we   ll be using.

   in the above diagram, each line carries an entire vector, from the
   output of one node to the inputs of others. the pink circles represent
   pointwise operations, like vector addition, while the yellow boxes are
   learned neural network layers. lines merging denote concatenation,
   while a line forking denote its content being copied and the copies
   going to different locations.

the core idea behind lstms

   the key to lstms is the cell state, the horizontal line running through
   the top of the diagram.

   the cell state is kind of like a conveyor belt. it runs straight down
   the entire chain, with only some minor linear interactions. it   s very
   easy for information to just flow along it unchanged.

   the lstm does have the ability to remove or add information to the cell
   state, carefully regulated by structures called gates.

   gates are a way to optionally let information through. they are
   composed out of a sigmoid neural net layer and a pointwise
   multiplication operation.

   the sigmoid layer outputs numbers between zero and one, describing how
   much of each component should be let through. a value of zero means
      let nothing through,    while a value of one means    let everything
   through!   

   an lstm has three of these gates, to protect and control the cell
   state.

step-by-step lstm walk through

   the first step in our lstm is to decide what information we   re going to
   throw away from the cell state. this decision is made by a sigmoid
   layer called the    forget gate layer.    it looks at \(h_{t-1}\) and
   \(x_t\), and outputs a number between \(0\) and \(1\) for each number
   in the cell state \(c_{t-1}\). a \(1\) represents    completely keep
   this    while a \(0\) represents    completely get rid of this.   

   let   s go back to our example of a language model trying to predict the
   next word based on all the previous ones. in such a problem, the cell
   state might include the gender of the present subject, so that the
   correct pronouns can be used. when we see a new subject, we want to
   forget the gender of the old subject.

   the next step is to decide what new information we   re going to store in
   the cell state. this has two parts. first, a sigmoid layer called the
      input gate layer    decides which values we   ll update. next, a tanh
   layer creates a vector of new candidate values, \(\tilde{c}_t\), that
   could be added to the state. in the next step, we   ll combine these two
   to create an update to the state.

   in the example of our language model, we   d want to add the gender of
   the new subject to the cell state, to replace the old one we   re
   forgetting.

   it   s now time to update the old cell state, \(c_{t-1}\), into the new
   cell state \(c_t\). the previous steps already decided what to do, we
   just need to actually do it.

   we multiply the old state by \(f_t\), forgetting the things we decided
   to forget earlier. then we add \(i_t*\tilde{c}_t\). this is the new
   candidate values, scaled by how much we decided to update each state
   value.

   in the case of the language model, this is where we   d actually drop the
   information about the old subject   s gender and add the new information,
   as we decided in the previous steps.

   finally, we need to decide what we   re going to output. this output will
   be based on our cell state, but will be a filtered version. first, we
   run a sigmoid layer which decides what parts of the cell state we   re
   going to output. then, we put the cell state through \(\tanh\) (to push
   the values to be between \(-1\) and \(1\)) and multiply it by the
   output of the sigmoid gate, so that we only output the parts we decided
   to.

   for the language model example, since it just saw a subject, it might
   want to output information relevant to a verb, in case that   s what is
   coming next. for example, it might output whether the subject is
   singular or plural, so that we know what form a verb should be
   conjugated into if that   s what follows next.

variants on long short term memory

   what i   ve described so far is a pretty normal lstm. but not all lstms
   are the same as the above. in fact, it seems like almost every paper
   involving lstms uses a slightly different version. the differences are
   minor, but it   s worth mentioning some of them.

   one popular lstm variant, introduced by [10]gers & schmidhuber (2000),
   is adding    peephole connections.    this means that we let the gate
   layers look at the cell state.

   the above diagram adds peepholes to all the gates, but many papers will
   give some peepholes and not others.

   another variation is to use coupled forget and input gates. instead of
   separately deciding what to forget and what we should add new
   information to, we make those decisions together. we only forget when
   we   re going to input something in its place. we only input new values
   to the state when we forget something older.

   a slightly more dramatic variation on the lstm is the gated recurrent
   unit, or gru, introduced by [11]cho, et al. (2014). it combines the
   forget and input gates into a single    update gate.    it also merges the
   cell state and hidden state, and makes some other changes. the
   resulting model is simpler than standard lstm models, and has been
   growing increasingly popular.
   a gated recurrent unit neural network.

   these are only a few of the most notable lstm variants. there are lots
   of others, like depth gated id56s by [12]yao, et al. (2015). there   s
   also some completely different approach to tackling long-term
   dependencies, like clockwork id56s by [13]koutnik, et al. (2014).

   which of these variants is best? do the differences matter? [14]greff,
   et al. (2015) do a nice comparison of popular variants, finding that
   they   re all about the same. [15]jozefowicz, et al. (2015) tested more
   than ten thousand id56 architectures, finding some that worked better
   than lstms on certain tasks.

conclusion

   earlier, i mentioned the remarkable results people are achieving with
   id56s. essentially all of these are achieved using lstms. they really
   work a lot better for most tasks!

   written down as a set of equations, lstms look pretty intimidating.
   hopefully, walking through them step by step in this essay has made
   them a bit more approachable.

   lstms were a big step in what we can accomplish with id56s. it   s natural
   to wonder: is there another big step? a common opinion among
   researchers is:    yes! there is a next step and it   s attention!    the
   idea is to let every step of an id56 pick information to look at from
   some larger collection of information. for example, if you are using an
   id56 to create a caption describing an image, it might pick a part of
   the image to look at for every word it outputs. in fact, [16]xu, et al.
   (2015) do exactly this     it might be a fun starting point if you want
   to explore attention! there   s been a number of really exciting results
   using attention, and it seems like a lot more are around the corner   

   attention isn   t the only exciting thread in id56 research. for example,
   grid lstms by [17]kalchbrenner, et al. (2015) seem extremely promising.
   work using id56s in generative models     such as [18]gregor, et al.
   (2015), [19]chung, et al. (2015), or [20]bayer & osendorfer (2015)    
   also seems very interesting. the last few years have been an exciting
   time for recurrent neural networks, and the coming ones promise to only
   be more so!

acknowledgments

   i   m grateful to a number of people for helping me better understand
   lstms, commenting on the visualizations, and providing feedback on this
   post.

   i   m very grateful to my colleagues at google for their helpful
   feedback, especially [21]oriol vinyals, [22]greg corrado, [23]jon
   shlens, [24]luke vilnis, and [25]ilya sutskever. i   m also thankful to
   many other friends and colleagues for taking the time to help me,
   including [26]dario amodei, and [27]jacob steinhardt. i   m especially
   thankful to [28]kyunghyun cho for extremely thoughtful correspondence
   about my diagrams.

   before this post, i practiced explaining lstms during two seminar
   series i taught on neural networks. thanks to everyone who participated
   in those for their patience with me, and for their feedback.
     __________________________________________________________________

    1. in addition to the original authors, a lot of people contributed to
       the modern lstm. a non-comprehensive list is: felix gers, fred
       cummins, santiago fernandez, justin bayer, daan wierstra, julian
       togelius, faustino gomez, matteo gagliolo, and [29]alex
       graves.[30]   
     __________________________________________________________________

more posts

   [attention.png]

attention and augmented recurrent neural networks

on distill

   [fig.png]

conv nets

a modular perspective

   [topology.png]

neural networks, manifolds, and topology

   [mikolov-gendervecs.png]

deep learning, nlp, and representations

   built by [31]oinkina with [32]hakyll using [33]bootstrap, [34]mathjax,
   [35]disqus, [36]mathbox.js, [37]highlight.js, and [38]footnotes.js.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   visible links
   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://karpathy.github.io/2015/05/21/id56-effectiveness/
   6. http://people.idsia.ch/~juergen/sepphochreiter1991thesisadvisorschmidhuber.pdf
   7. http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf
   8. http://www.bioinf.jku.at/publications/older/2604.pdf
   9. http://colah.github.io/posts/2015-08-understanding-lstms/#fn1
  10. ftp://ftp.idsia.ch/pub/juergen/timecount-ijid982000.pdf
  11. http://arxiv.org/pdf/1406.1078v3.pdf
  12. http://arxiv.org/pdf/1508.03790v2.pdf
  13. http://arxiv.org/pdf/1402.3511v1.pdf
  14. http://arxiv.org/pdf/1503.04069.pdf
  15. http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf
  16. http://arxiv.org/pdf/1502.03044v2.pdf
  17. http://arxiv.org/pdf/1507.01526v1.pdf
  18. http://arxiv.org/pdf/1502.04623.pdf
  19. http://arxiv.org/pdf/1506.02216v3.pdf
  20. http://arxiv.org/pdf/1411.7610v3.pdf
  21. http://research.google.com/pubs/oriolvinyals.html
  22. http://research.google.com/pubs/gregcorrado.html
  23. http://research.google.com/pubs/jonathonshlens.html
  24. http://people.cs.umass.edu/~luke/
  25. http://www.cs.toronto.edu/~ilya/
  26. https://www.linkedin.com/pub/dario-amodei/4/493/393
  27. http://cs.stanford.edu/~jsteinhardt/
  28. http://www.kyunghyuncho.me/
  29. https://scholar.google.com/citations?user=dafhynwaaaaj&hl=en
  30. http://colah.github.io/posts/2015-08-understanding-lstms/#fnref1
  31. https://github.com/oinkina
  32. http://jaspervdj.be/hakyll
  33. http://getbootstrap.com/
  34. http://www.mathjax.org/
  35. http://disqus.com/
  36. https://github.com/unconed/mathbox.js
  37. http://highlightjs.org/
  38. http://ignorethecode.net/blog/2010/04/20/footnotes/

   hidden links:
  40. http://distill.pub/2016/augmented-id56s/
  41. http://colah.github.io/posts/2014-07-conv-nets-modular/
  42. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  43. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
